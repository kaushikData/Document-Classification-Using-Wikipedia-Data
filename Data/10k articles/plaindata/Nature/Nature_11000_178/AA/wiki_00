{"id": "10980356", "url": "https://en.wikipedia.org/wiki?curid=10980356", "title": "2006 RH120", "text": "2006 RH120\n\nUntil given a minor planet designation on 18 February 2008, the object was known as 6R10DB9, an internal identification number assigned by the Catalina Sky Survey.\n\n was discovered on 14 September 2006 by the Schmidt camera of the Catalina Sky Survey in Arizona. \"6R10DB9\" was the Catalina Sky Survey's own discovery designation for this object, which usually would only be used on the MPC's NEO Confirmation Page (NEOCP) until an IAU designation was applied, if the object was classified as a minor object. It was added on 14 September to the NEOCP and subsequently removed with the explanation that it \"was not a minor planet\". Preliminary orbital calculations indicated it was captured by Earth's gravity from solar orbit of a period of about 11 months, which is similar to that of many spent rocket boosters dating to the Apollo program of the 1960s and early 1970s. 6R10DB was assigned the designation on 18 February 2008.\n\nSome controversy existed regarding the origin of the object. Upon discovery, it was not given a formal name because its size, orbit, and spectrum were consistent with the white titanium-oxide paint used on Saturn V rockets, which meant it could be an artificial object. Precedents for this exist: J002E3 is currently thought to be the third-stage Saturn S-IVB booster from Apollo 12 and was in an almost identical orbit, and 6Q0B44E, discovered a month earlier, was also thought to be artificial. Its status as a satellite was also debated, with A. W. Harris of the Space Science Institute commenting, \"Claiming some bit of fluff in a temporary looping orbit to be a ‘satellite,’ with all the baggage that term carries, is mere hype\".\n\nAnalysis has shown that solar-radiation pressure is perturbing its motion perceptibly. However, Paul Chodas in JPL's Solar System Dynamics Group suspects that the perturbations are consistent with expectations for a rocky object but not with old flight hardware. One hypothesis is that the object is a piece of lunar rock ejected by an impact. On 18 February 2008, the object was given a minor planet designation.\n\nThe object made four Earth orbits of about three months each before being ejected after the June 2007 perigee. At that perigee, it dipped inside the Moon's orbit to a distance of . became an Apollo-class asteroid in June 2007.\n\nOn 14 June 2007, made its fourth and last perigee of the most recent Earth encounter. It was 0.7 lunar distances at closest, with an apparent magnitude of 18.5–19.0. Astronomers at JPL Goldstone in California made radar astrometry measurements on 12, 14 and 17 June 2007.\n\n is now in solar orbit as an Amor-class asteroid with an orbit completely outside of Earth's orbit. As of 2017, this object is 2 AU from Earth on the other side of the Sun. Systematic residuals in the positions of the object probably indicate that the model used to determine solar radiation pressure may be too simple to adequately describe its motion over a long period of time. The next near-Earth encounter is in August 2028 when the object will pass Earth at a relatively low speed of . For comparison, on 13 April 2029, asteroid 99942 Apophis will pass Earth at a relative speed of . is listed as part of the Near-Earth Object Human Space Flight Accessible Targets Study (NHATS).\n\n\n"}
{"id": "42126823", "url": "https://en.wikipedia.org/wiki?curid=42126823", "title": "Advances in Atmospheric Sciences", "text": "Advances in Atmospheric Sciences\n\nAdvances in Atmospheric Sciences is a bimonthly peer-reviewed scientific journal co-published by Springer Science+Business Media and Science Press. It covers research on the dynamics, physics and chemistry of the atmosphere and oceans, including weather systems, numerical weather prediction, climate dynamics and variability, and satellite meteorology. It was established in 1984. The editors-in-chief are D. Lü, H. Wang (Chinese Academy of Sciences), and M. Xue (University of Oklahoma. According to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 1.338.\n"}
{"id": "57494841", "url": "https://en.wikipedia.org/wiki?curid=57494841", "title": "Alan Nebauer", "text": "Alan Nebauer\n\nAlan Nebauer is an Australian yachtsman who competed in the single-handed round-the-world 1994–95 BOC Challenge race aboard the yacht \"Newcastle Australia\". Nebauer won two awards for his seamanship during the race, one of them for rescuing his British competitor Josh Hall, whose vessel foundered during the first leg from Charleston to Cape Town.\n\nNebauer wrote a book about his adventures in the race () and a documentary film was also produced.\n\nAlan Nebauer and his wife, Cindy, currently run a charter business in Jervis Bay, Australia.. Alan also remains involved with various yachting projects as a consultant and contract skipper.\n"}
{"id": "298975", "url": "https://en.wikipedia.org/wiki?curid=298975", "title": "Audion", "text": "Audion\n\nThe Audion was an electronic detecting or amplifying vacuum tube invented by American electrical engineer Lee de Forest in 1906. It was the first triode, consisting of an evacuated glass tube containing three electrodes: a heated filament, a grid, and a plate. It is important in the history of technology because it was the first widely used electronic device which could amplify; a small electrical signal applied to the grid could control a larger current flowing from the filament to plate.\n\nThe original triode Audion had more residual gas in the tube than later versions and vacuum tubes; the extra residual gas limited the dynamic range and gave the Audion nonlinear characteristics and erratic performance. Originally developed as a radio receiver detector by adding a grid electrode to the Fleming valve, it found little use until its amplifying ability was recognized around 1912 by several researchers, who used it to build the first amplifying radio receivers and electronic oscillators. The many practical applications for amplification motivated its rapid development, and the original Audion was superseded within a few years by improved versions with higher vacuum.\n\nIt had been known since the middle of the 19th century that gas flames were electrically conductive, and early wireless experimenters had noticed that this conductivity was affected by the presence of radio waves. De Forest found that gas in a partial vacuum heated by a conventional lamp filament behaved much the same way, and that if a wire were wrapped around the glass housing, the device could serve as a detector of radio signals. In his original design, a small metal plate was sealed into the lamp housing, and this was connected to the positive terminal of a 22 volt battery via a pair of headphones, the negative terminal being connected to one side of the lamp filament. When wireless signals were applied to the wire wrapped around the outside of the glass, they caused disturbances in the current which produced sounds in the headphones.\n\nThis was a significant development as existing commercial wireless systems were heavily protected by patents; a new type of detector would allow De Forest to market his own system. He eventually discovered that connecting the antenna circuit to a third electrode placed directly in the current path greatly improved the sensitivity; in his earliest versions, this was simply a piece of wire bent into the shape of a grid-iron (hence \"grid\").\n\nThe Audion provided power gain; with other detectors, all of the power to operate the headphones had to come from the antenna circuit itself. Consequently, weak transmitters could be heard at greater distances.\n\nDe Forest and everybody else at the time greatly underestimated the potential of his original device, imagining it to be limited to mostly military applications. It is significant that he apparently never saw its potential as a telephone repeater amplifier, even though crude electromechanical \"note magnifiers\" had been the bane of the telephone industry for at least two decades. (Ironically, in the years of patent disputes leading up to World War I, it was only this \"loophole\" that allowed vacuum triodes to be manufactured at all, since none of De Forest's patents specifically mentioned this application).\n\nDe Forest was granted a patent for his early two-electrode version of the Audion on November 13, 1906 (), but the \"triode\" (three electrode) version was patented in 1908 (). De Forest continued to claim that he developed the Audion independently from John Ambrose Fleming's earlier research on the thermionic valve (for which Fleming received Great Britain patent 24850 and the American Fleming valve patent ), and De Forest became embroiled in many radio-related patent disputes. De Forest was famous for saying that he \"didn't know why it worked, it just did\".\n\nHe always referred to the vacuum triodes developed by other researchers as \"Oscillaudions\", although there is no evidence that he had any significant input to their development. It is true that after the invention of the true vacuum triode in 1913 (see below), De Forest continued to manufacture various types of radio transmitting and receiving apparatus, (examples of which are illustrated on this page). However, although he routinely described these devices as using \"Audions\", they actually used high-vacuum triodes, using circuitry very similar to that developed by other experimenters.\n\nIn 1914, Columbia University student Edwin Howard Armstrong worked with professor John Harold Morecroft to document the electrical principles of the Audion. Armstrong published his explanation of the Audion in \"Electrical World\" in December 1914, complete with circuit diagrams and oscilloscope graphs. In March and April 1915, Armstrong spoke to the Institute of Radio Engineers in New York and Boston, respectively, presenting his paper \"Some Recent Developments in the Audion Receiver\", which was published in September. A combination of the two papers was reprinted in other journals such as the \"Annals of the New York Academy of Sciences\". When Armstrong and De Forest later faced each other in a dispute over the regeneration patent, Armstrong was able to demonstrate conclusively that De Forest still had no idea how it worked.\n\nThe problem was that (possibly to distance his invention from the Fleming valve) De Forest's original patents specified that low-pressure gas inside the Audion was essential to its operation (Audion being a contraction of \"Audio-Ion\"), and in fact early Audions had severe reliability problems due to this gas being adsorbed by the metal electrodes. The Audions sometimes worked extremely well; at other times they would barely work at all.\n\nAs well as De Forest himself, numerous researchers had tried to find ways to improve the reliability of the device by stabilizing the partial vacuum. Much of the research that led to the development of true vacuum tubes was carried out by Irving Langmuir in the General Electric (GE) research laboratories.\n\nLangmuir had long suspected that certain assumed limitations on the performance of various low-pressure and vacuum electrical devices, might not be fundamental physical limitations at all, but simply due to contamination and impurities in the manufacturing process.\n\nHis first success was in demonstrating that, contrary to what Edison and others had long asserted, incandescent lamps could function more efficiently and with longer life if the glass envelope was filled with low-pressure inert gas rather than a complete vacuum. However, this only worked if the gas used was meticulously 'scrubbed\" of all traces of oxygen and water vapor.\nHe then applied the same approach to producing a rectifier for the newly developed \"Coolidge\" X-ray tubes. Again contrary to what had been widely believed to be possible, by virtue of meticulous cleanliness and attention to detail, he was able to produce versions of the Fleming Diode that could rectify hundreds of thousands of volts. His rectifiers were called \"Kenotrons\" from the Greek \"keno\" (empty, contains nothing, as in a vacuum) and \"tron\" (device, instrument).\n\nHe then turned his attention to the Audion tube, again suspecting that its notoriously unpredictable behaviour might be tamed with more care in the manufacturing process.\n\nHowever he took a somewhat unorthodox approach. Instead of trying to stabilize the partial vacuum, he wondered if it was possible to make the Audion function with the total vacuum of a Kenotron, since that was somewhat easier to stabilize.\n\nHe soon realized that his \"vacuum\" Audion had markedly different characteristics from the De Forest version, and was really a quite different device, capable of linear amplification and at much higher frequencies. To distinguish his device from the Audion he named it the \"Pliotron\", from the Greek \"plio\" (more or extra, in this sense meaning gain, more signal coming out than went in).\n\nEssentially, he referred to all his vacuum tube designs as Kenotrons, the Pliotron basically being a specialized type of Kenotron. However, because Pliotron and Kenotron were registered trademarks, technical writers tended to use the more generic term \"vacuum tube\". By the mid-1920s, the term \"Kenotron\" had come to exclusively refer to vacuum tube rectifiers, while the term \"Pliotron\" had fallen into disuse. Ironically, in popular usage, the sound-alike brands \"Radiotron\" and \"Ken-Rad\" outlasted the original names.\n\nDe Forest continued to manufacture and supply Audions to the US Navy up until the early 1920s, for maintenance of existing equipment, but elsewhere they were regarded as well and truly obsolete by then. It was the vacuum triode that made practical radio broadcasts a reality.\n\nPrior to the introduction of the Audion, radio receivers had used a variety of detectors including coherers, barretters, and crystal detectors. The most popular crystal detector consisted of a small piece of galena crystal probed by a fine wire commonly referred to as a \"cat's-whisker detector\". They were very unreliable, requiring frequent adjustment of the cat's whisker and offered no amplification. Such systems usually required the user to listen to the signal through headphones, sometimes at very low volume, as the only energy available to operate the headphones was that picked up by the antenna. For long distance communication huge antennas were normally required, and enormous amounts of electrical power had to be fed into the transmitter.\n\nThe Audion was a considerable improvement on this, but the original devices could not provide any subsequent amplification to what was produced in the signal detection process. The later vacuum triodes allowed the signal to be amplified to any desired level, typically by feeding the amplified output of one triode into the grid of the next, eventually providing more than enough power to drive a full-sized speaker. Apart from this, they were able to amplify the incoming radio signals prior to the detection process, making it work much more efficiently.\n\nVacuum tubes could also be used to make superior radio transmitters. The combination of much more efficient transmitters and much more sensitive receivers revolutionized radio communication during World War I.\n\nBy the late 1920s such \"tube radios\" began to become a fixture of most Western world households, and remained so until long after the introduction of transistor radios in the mid-1950s.\n\nIn modern electronics, the vacuum tube has been largely superseded by solid state devices such as the transistor, invented in 1947 and implemented in integrated circuits in 1959, although vacuum tubes remain to this day in such applications as high-powered transmitters, guitar amplifiers and some high fidelity audio equipment.\n\n\n"}
{"id": "56083630", "url": "https://en.wikipedia.org/wiki?curid=56083630", "title": "Bayou Bridge Pipeline", "text": "Bayou Bridge Pipeline\n\nThe Bayou Bridge Pipeline (BBP) is a 162.5-mile crude oil pipeline planned from Lake Charles, Louisiana to St. James, Louisiana by Bayou Bridge, LLC, a subsidiary of Energy Transfer Partners. Communities directly impacted by the pipeline have asked the Army Corps of Engineers for an Environmental Impact Statement. The Corps refused to do so and approved the project on 15 December 2017.\n\nThe Bayou Bridge Pipeline (BBP), is a crude oil pipeline project through Louisiana´s Atchafalaya Basin and across 11 parishes. It is to connect an oil-and-gas hub in Texas with oil refineries in Louisiana. The planned 162 miles of pipeline have been estimated to cost $670 million.\n\nThe Bayou Bridge pipeline is to deliver heavy and light crude oil from the Phillips 66 and Sunoco Logistics terminals in Nederland, Texas into Lake Charles, Louisiana other than by ship or rail. Greg Garland, chairman of Phillips 66 said it complemented other pipeline projects to deliver Bakken crude oil to the Gulf Coast.\n\nIn August 2015, Energy Transfer Partners LP announced a joint venture with Phillips 66 (40%) and Sunoco Logistics Partners LP (30%) to construct the Bayou Bridge pipeline, in which they would have a 30% interest.\n\nThe proposed 162-mile Bayou Bridge Pipeline project is located in Calcasieu, Jefferson Davis Parish, Louisiana, Acadia Parish, Louisiana, Vermilion Parish, Louisiana, Lafayette Parish, Louisiana, Iberia Parish, Louisiana, St. Martin Parish, Louisiana, Iberville Parish, Louisiana, Ascension Parish, Louisiana, Assumption Parish, Louisiana, and St. James Parish, Louisiana. Two pump stations have been proposed in Jefferson Davis and St. Martin parishes. There are eight watersheds within the project: Lower Calcasieu, Mermentau, Vermilion, Bayou Teche, Atchafalaya, Lower Grand, West Central Louisiana Coastal, and East Central Louisiana Coastal.\n\nAn important context of the route is the fact that marshes and wetlands have suffered long-term erosion from levees, which are keeping the Mississippi River from flooding but also from re-depositing sediments.\n\nWith the 2015 joint venture, Phase I of the project began, the construction of a 30-inch pipeline from Nederland, Texas to Lake Charles, Louisiana. In April 2016, this pipeline went into service. \n\nPhase II of the pipeline entails a 24” pipe from Lake Charles to St. James, Louisiana, which is expected to be completed in the first few months of 2018.\nInformational meetings were held in January 2017. By May 2017, pipes had already been manufactured. \nIn August 2017, St James residents opposed the pipeline. and the parish council delayed its vote, but 10 days later, it approved the land use for the pipeline by a margin of 4-3 so it could run through the Vacherie, Louisiana area. The vote was along racial lines, with the white majority prevailing.\n\nIn December 2017, as Bayou Bridge LLC was to expropriate property, environmental activists demanded to see project records, including internal company communications, per Louisiana’s public records law.\n\nTwo government permits are needed for the pipeline: one from the U.S. Army Corps of Engineers and one from the Louisiana Department of Natural Resources. the latter had approved the project.\n\nSt James residents have stated that the oil industry is hurting their health and their town in what is known as cancer alley.\n\nA recent explosion of an oil platform on Lake Pontchartrain on 6 December 2017 illustrates the dangers involved. \n\nOil spills are a realistic concern. In May 2017, Energy transfer pipelines in Ohio had a series of leaks.On December 1 it was reported that a pipeline leaked oil into Louisiana marsh.At a meeting of the St. Martin Parish Council the Louisiana Crawfish Producers Association-West noted that many pipelines and their spill banks underneath the Atchafalaya Basin running east to west have changed the water flow to such a degree that it no longer flows throughout the Basin, creating \"dead water\" or low-oxygen swamp water. The Corps of Engineers has required pipeline companies to maintain water flow, but is not enforcing the requirement.\nCrawfishermen have been impacted by the oil industry, particularly since the 2010 BP Oil Spill. This comes on the background of deteriorating health of the basin, an increase of the dead zones. \n\nResidents who have been impacted by the oil industry have heard companies´ promises of protecting and restoring the coast but not to come true.\n\nAs Energy Transfer´s militaristic tactics of dealing with protest at its Dakota Access pipeline have become public, including the employment of security companies for aerial surveillance, radio eavesdropping and infiltration of camps as counterterrorism measures,\nculminating in the attempt to build a conspiracy lawsuit, it has demonstrated how it can scare protestors from further activism.\n\nIn June 2017, the pastor of St. James led a lawsuit opposing the pipeline to protect the community. A prayer and resistance camp was set up. and indigenous leaders had joined the resistance by August.\n\nProtesters have been demanding an environmental impact statement since at least September. On Halloween, people went to the Louisiana Capitol demanding that Governor John Bel Edwards should require an environmental impact statement for the pipeline. In November, resistance was increasing and opponents of the project filed petition to intervene in a hearing of the Louisiana State Board of Private Security Examiners regarding the application of private security firm TigerSwan, hired by Energy Transfer Partners.\n\nIn December an activist bought land in the path of the proposed pipeline.\n\n\n"}
{"id": "25986521", "url": "https://en.wikipedia.org/wiki?curid=25986521", "title": "Blue Marsh National Recreation Area", "text": "Blue Marsh National Recreation Area\n\nBlue Marsh National Recreation Area is an artificial lake located northwest of the city of Reading, Pennsylvania, USA and managed by the U.S. Army Corps of Engineers Philadelphia District. It is in western Berks County, fed into by the Tulpehocken Creek. The main span of the lake is along the border between Bern and Lower Heidelberg Townships. However, the northwesternmost portions lie in the more sparsely populated North Heidelberg and Penn Townships. In the middle of the lake is a large, uninhabited island (claimed by Bern Township). The lake is a popular recreation area in the summer, where people can fish, swim, and boat.\n\nBlue Marsh was the name of the village that was located where the lake now is. It was the first settlement in Lower Heidelberg Township. The land was very fertile. It was also a heavily forested area with abundance of wildlife. There were many Farms, and 18th and 19th century homes. The village also had a church, schoolhouse, post office, and Pyles General Store. Some buildings were spared, such as Gruber Wagon Works, which was originally in Blue Marsh but was moved a safe distance away before the dam was created. Others include Old Dry Rd. Farm, which most of its buildings were relocated out of the village. Residents were put under eminent domain and had no choice but to move out. Initial authorization for the reservoir was granted by the Flood Control Act of 1962 that started eight projects on the Delaware River basin. The Pennsylvania Project 70 Land Acquisition and Borrowing Act provided funding and permitted the eminent domain acquisition of the land that would later become the lake with the governor's permission in 1969. The United States Army Corps of Engineers began constructing the lake in March 1974 with the impoundment of the Tulpehocken Creek and was completed in September 1979.\n\n"}
{"id": "86227", "url": "https://en.wikipedia.org/wiki?curid=86227", "title": "Cessair", "text": "Cessair\n\nCessair or Cesair (spelt Ceasair in modern Irish; anglicized Kesair, Pron: KAH-seer, Meaning: Sorrow, Affliction) is a character from the \"Lebor Gabála Érenn\", a medieval Christian pseudo-history of Ireland. According to the \"Lebor Gabála\", she was the leader of the first inhabitants of Ireland, before the Biblical Flood. The tale may be an attempt to Christianize an earlier pagan myth, but may alternatively be the product of post-conversion pseudohistory.\n\nAccording to the \"LGÉ\", Cessair was daughter of Noah's (non-Biblical) son Bith and his wife Birren. Cessair's father's name Bith is derived from Proto-Celtic \"Bitu\"-, which was a common prefix in Gaulish and Insular king names and has the meaning of \"World\", \"Life\", or \"Age\" in Old Irish (cf. Bituitus). In some versions of the tale, Noah tells them to go to the western edge of the world to escape the oncoming Flood. In other versions, when their people are denied a place on Noah's Ark, Cessair tells them to make an idol to advise them. This idol tells them to escape the Flood by sailing to Ireland. They set out in three ships and reach Ireland after a long journey. However, when they attempt to land, two of the ships are lost. The only survivors are Cessair, forty-nine other women, and three men: Fintan mac Bóchra, Bith and Ladra. They land in Ireland at Dún na mBarc (on Bantry Bay) forty days before the Flood, in Age of the World 2242 according to the Annals of the Four Masters, or 2361 BC according to Seathrún Céitinn's chronology.\n\nThe men are shared out evenly among the women. Each also takes one as her primary husband: Cessair takes Fintán, Bairrfhind takes Bith and Alba takes Ladra. However, Bith and Ladra soon die (Ladra becoming the first man buried in Ireland). Fintán is left with all the women but is unable to cope and so he flees. When the Flood comes, Fintán is the only one to survive. He becomes a salmon and later an eagle and a hawk, living for 5,500 years after the Flood, whence he becomes a man again and recounts Ireland's history. According to legend, Cessair died at \"Cúil Ceasra(ch)\" in Connacht and a cairn, \"Carn Ceasra(ch)\", was raised over her body. It has been said that this cairn is near Boyle in County Roscommon, or alternatively that it is Cnoc Meadha in County Galway (Lynch, 2006).\n\nAn earlier version of the tale, apparently found in the Cín Dromma Snechtai, says that it was Banba who first came to Ireland with her two sisters, three men and fifty women. Banba, Fódla and Ériu were a trio of land goddesses and their husbands were Mac Cuill (son of hazel), Mac Cecht (son of the plough) and Mac Gréine (son of the Sun). It is likely that Cessair, Bairrfhind and Alba are a Christianized replacement for the three goddesses and Fintán, Bith and Ladra a replacement for the three gods. Fintán/Mac Cuill may also be linked to the Salmon of Knowledge, which gains all the world's knowledge after eating nine hazelnuts that fall into a well. The women who accompany Cessair appear by their names to represent the world's ancestral mothers; they included German (Germans), Espa (Spanish), Alba (British), Traige (Thracians), Gothiam (Goths), and so forth. Thus \"their arrival can be read as creating a microcosm of the whole world's population in Ireland\". Several other companions echo the names of ancient Irish goddesses.\n\nSeathrún Céitinn also refers to a legend that three fishermen from Iberia—Capa, Laigne and Luasad—were driven to Ireland by a storm a year before the Flood. They liked it, so they went home to get their wives, returned shortly before the Flood, and were drowned.\n\nAccording to another medieval Irish legend, Cessair is also the name of a daughter of the king of Gallia who married the High King of Ireland, Úgaine Mór, in the 6th or 5th century BC.\n\nMallory has a slightly different version. There were 150 women and 3 men. The first man died and was buried in Wexford. The second man now had 100 women and soon died of exhaustion. The 150 women then chased the remaining man who saved himself by jumping into the sea and turning into a salmon.\n\n"}
{"id": "747259", "url": "https://en.wikipedia.org/wiki?curid=747259", "title": "Dependency theory", "text": "Dependency theory\n\nDependency theory is the notion that resources flow from a \"periphery\" of poor and underdeveloped states to a \"core\" of wealthy states, enriching the latter at the expense of the former. It is a central contention of dependency theory that poor states are impoverished and rich ones enriched by the way poor states are integrated into the \"world system\".\n\nThe theory arose as a reaction to modernization theory, an earlier theory of development which held that all societies progress through similar stages of development, that today's underdeveloped areas are thus in a similar situation to that of today's developed areas at some time in the past, and that, therefore, the task of helping the underdeveloped areas out of poverty is to accelerate them along this supposed common path of development, by various means such as investment, technology transfers, and closer integration into the world market. Dependency theory rejected this view, arguing that underdeveloped countries are not merely primitive versions of developed countries, but have unique features and structures of their own; and, importantly, are in the situation of being the weaker members in a world market economy.\n\nDependency theory no longer has many proponents as an overall theory, though some writers have argued for its continuing relevance as a conceptual orientation to the global division of wealth.\n\nDependency theory originates with two papers published in 1949 – one by Hans Singer, one by Raúl Prebisch – in which the authors observe that the terms of trade for underdeveloped countries relative to the developed countries had deteriorated over time: the underdeveloped countries were able to purchase fewer and fewer manufactured goods from the developed countries in exchange for a given quantity of their raw materials exports. This idea is known as the Prebisch–Singer thesis. Prebisch, an Argentine economist at the United Nations Commission for Latin America (UNCLA), went on to conclude that the underdeveloped nations must employ some degree of protectionism in trade if they were to enter a self-sustaining development path. He argued that import-substitution industrialisation (ISI), not a trade-and-export orientation, was the best strategy for underdeveloped countries. The theory was developed from a Marxian perspective by Paul A. Baran in 1957 with the publication of his \"The Political Economy of Growth\". Dependency theory shares many points with earlier, Marxist, theories of imperialism by Rosa Luxemburg and Vladimir Lenin, and has attracted continued interest from Marxists. Some authors identify two main streams in dependency theory: the Latin American Structuralist, typified by the work of Prebisch, Celso Furtado, and Aníbal Pinto at the United Nations Economic Commission for Latin America (ECLAC, or, in Spanish, CEPAL); and the American Marxist, developed by Paul A. Baran, Paul Sweezy, and Andre Gunder Frank.\n\nUsing the Latin American dependency model, the Guyanese Marxist historian Walter Rodney, in his book \"How Europe Underdeveloped Africa\", described in 1972 an Africa that had been consciously exploited by European imperialists, leading directly to the modern underdevelopment of most of the continent.\n\nThe theory was popular in the 1960s and 1970s as a criticism of modernization theory, which was falling increasingly out of favor because of continued widespread poverty in much of the world. It was used to explain the causes of overurbanization, a theory that urbanization rates outpaced industrial growth in several developing countries.\n\nThe Latin American Structuralist and the American Marxist schools had significant differences but agreed on some basic points:[B]oth groups would agree that at the core of the dependency relation between center and periphery lays [lies] the inability of the periphery to develop an autonomous and dynamic process of technological innovation. Technology – the Promethean force unleashed by the Industrial Revolution – is at the center of stage. The Center countries controlled the technology and the systems for generating technology. Foreign capital could not solve the problem, since it only led to limited transmission of technology, but not the process of innovation itself. Baran and others frequently spoke of the international division of labour – skilled workers in the center; unskilled in the periphery – when discussing key features of dependency.\n\nBaran placed surplus extraction and capital accumulation at the center of his analysis. Development depends on a population's producing more than it needs for bare subsistence (a surplus). Further, some of that surplus must be used for capital accumulation – the purchase of new means of production – if development is to occur; spending the surplus on things like luxury consumption does not produce development. Baran noted two predominant kinds of economic activity in poor countries. In the older of the two, plantation agriculture, which originated in colonial times, most of the surplus goes to the landowners, who use it to emulate the consumption patterns of wealthy people in the developed world; much of it thus goes to purchase foreign-produced luxury items –automobiles, clothes, etc. – and little is accumulated for investing in development. The more recent kind of economic activity in the periphery is industry—but of a particular kind. It is usually carried out by foreigners, although often in conjunction with local interests. It is often under special tariff protection or other government concessions. The surplus from this production mostly goes to two places: part of it is sent back to the foreign shareholders as profit; the other part is spent on conspicuous consumption in a similar fashion to that of the plantation aristocracy. Again, little is used for development. Baran thought that political revolution was necessary to break this pattern.\n\nIn the 1960s, members of the Latin American Structuralist school argued that there is more latitude in the system than the Marxists believed. They argued that it allows for partial development or \"dependent development\"–development, but still under the control of outside decision makers. They cited the partly successful attempts at industrialisation in Latin America around that time (Argentina, Brazil, Mexico) as evidence for this hypothesis. They were led to the position that dependency is not a relation between commodity exporters and industrialised countries, but between countries with different degrees of industrialisation. In their approach, there is a distinction made between the economic and political spheres: economically, one may be developed or underdeveloped; but even if (somewhat) economically developed, one may be politically autonomous or dependent. More recently, Guillermo O'Donnell has argued that constraints placed on development by neoliberalism were lifted by the military coups in Latin America that came to promote development in authoritarian guise (O'Donnell, 1982).\n\nThe importance of multinational corporations and state promotion of technology were emphasised by the Latin American Structuralists.\n\nFajnzybler has made a distinction between systemic or authentic competitiveness, which is the ability to compete based on higher productivity, and spurious competitiveness, which is based on low wages.\n\nThe third-world debt crisis of the 1980s and continued stagnation in Africa and Latin America in the 1990s caused some doubt as to the feasibility or desirability of \"dependent development\".\n\nThe \"sine qua non\" of the dependency relationship is not the difference in technological sophistication, as traditional dependency theorists believe, but rather the difference in financial strength between core and peripheral countries–particularly the inability of peripheral countries to borrow in their own currency. He believes that the hegemonic position of the United States is very strong because of the importance of its financial markets and because it controls the international reserve currency – the US dollar. He believes that the end of the Bretton Woods international financial agreements in the early 1970s considerably strengthened the United States' position because it removed some constraints on their financial actions.\n\n\"Standard\" dependency theory differs from Marxism, in arguing against internationalism and any hope of progress in less developed nations towards industrialization and a liberating revolution. Theotonio dos Santos described a \"new dependency\", which focused on both the internal and external relations of less-developed countries of the periphery, derived from a Marxian analysis. Former Brazilian President Fernando Henrique Cardoso (in office 1995–2002) wrote extensively on dependency theory while in political exile during the 1960s, arguing that it was an approach to studying the economic disparities between the centre and periphery. Cardoso summarized his version of dependency theory as follows:\n\nThe analysis of development patterns in the 1990s and beyond is complicated by the fact that capitalism develops not smoothly, but with very strong and self-repeating ups and downs, called cycles. Relevant results are given in studies by Joshua Goldstein, Volker Bornschier, and Luigi Scandella.\n\nWith the economic growth of India and some East Asian economies, dependency theory has lost some of its former influence. It still influences some NGO campaigns, such as Make Poverty History and the fair trade movement.\n\nTwo other early writers relevant to dependency theory were François Perroux and Kurt Rothschild. Other leading dependency theorists include Herb Addo, Walden Bello, Ruy Mauro Marini, Enzo Faletto, Armando Cordova, Ernest Feder, Pablo González Casanova, Keith Griffin, Kunibert Raffer, Paul Israel Singer, and Osvaldo Sunkel. Many of these authors focused their attention on Latin America; the leading dependency theorist in the Islamic world is the Egyptian economist Samir Amin.\n\nTausch, based on works of Amin from 1973 to 1997, lists the following main characteristics of periphery capitalism:\n\nThe American sociologist Immanuel Wallerstein refined the Marxist aspect of the theory and expanded on it, to form world-systems theory. This postulates a third category of countries, the \"semi-periphery\", intermediate between the core and periphery. Wallerstein believed in a tri-modal rather than a bi-modal system because he viewed the world-systems as more complicated than a simplistic classification as either core or periphery nations. To Wallerstein, many nations do not fit into one of these two categories, so he proposed the idea of a semi-periphery as an in between state within his model. In this model, the semi-periphery is industrialized, but with less sophistication of technology than in the core; and it does not control finances. The rise of one group of semi-peripheries tends to be at the cost of another group, but the unequal structure of the world economy based on unequal exchange tends to remain stable. Tausch traces the beginnings of world-systems theory to the writings of the Austro-Hungarian socialist Karl Polanyi after the First World War, but its present form is usually associated with the work of Wallerstein.\n\nDependency theory has also been associated with Johan Galtung's structural theory of imperialism.\n\nDependency theorists hold that short-term spurts of growth notwithstanding, long-term growth in the periphery will be imbalanced and unequal, and will tend towards high negative current account balances. Cyclical fluctuations also have a profound effect on cross-national comparisons of economic growth and societal development in the medium and long run. What seemed like spectacular long-run growth may in the end turn out to be just a short run cyclical spurt after a long recession. Cycle time plays an important role. Giovanni Arrighi believed that the logic of accumulation on a world scale shifts over time, and that the 1980s and beyond once more showed a deregulated phase of world capitalism with a logic, characterized - in contrast to earlier regulatory cycles - by the dominance of financial capital.\n\nIt is argued that, at this stage, the role of unequal exchange in the entire relationship of dependency cannot be underestimated. Unequal exchange is given if double factorial terms of trade of the respective country are < 1.0 (Raffer, 1987, Amin, 1975).\n\nThe former ideological head of the Blekingegade Gang and political activist Torkil Lauesen argues in his book \"The Global Perspective\" that political theory and practice stemming from dependency theory are more relevant than ever. He postulates that the conflict between countries in the core and countries in the periphery has been ever-intensifying and that the world is at the onset of a resolution of the core-periphery contradiction – that humanity is \"in for an economic and political rollercoaster ride\".\n\nEconomic policies based on dependency theory has been criticized by free-market economists such as Peter Bauer and Martin Wolf and others:\n\n\nMarket economists cite a number of examples in their arguments against dependency theory. The improvement of India's economy after it moved from state-controlled business to open trade is one of the most often cited (\"see also\" economy of India, \"The Commanding Heights\"). India's example seems to contradict dependency theorists' claims concerning comparative advantage and mobility, as much as its economic growth originated from movements such as outsourcing – one of the most mobile forms of capital transfer. South Korea and North Korea provide another example of trade-based development vs. autocratic self-sufficiency. Following the Korean War, North Korea pursued a policy of import substitution industrialization as suggested by dependency theory, while South Korea pursued a policy of export-oriented industrialization as suggested by comparative advantage theory. In 2013, South Korea's per capita GDP was 18 times that of North Korea. In Africa, states which have emphasized import-substitution development, such as Zimbabwe, have typically been among the worst performers, while the continent's most successful non-oil based economies, such as Egypt, South Africa, and Tunisia, have pursued trade-based development.\n\nAccording to economic historian Robert C. Allen, dependency theory's claims are \"debatable\", and that the protectionism that was implemented in Latin America as a solution ended up failing. The countries incurred too much debt and Latin America went into a recession. One of the problems was that the Latin American countries simply had too small national markets to be able to efficiently produce complex industrialized goods, such as automobiles.\n\n\n\n"}
{"id": "298818", "url": "https://en.wikipedia.org/wiki?curid=298818", "title": "Effects of the car on societies", "text": "Effects of the car on societies\n\nSince the start of the twentieth century, the role of the car has become highly important though controversial. It is used throughout the world and has become the most popular mode of transport in the more developed countries. In developing countries, the effects of the car on society are not as visible, however they are significant. The development of the car built upon the transport sector first started by railways. This has introduced sweeping changes in employment patterns, social interactions, infrastructure and the distribution of goods.\n\nNonetheless the positive effects on access to remote places and mobility comfort provided by the automobile, allowing people to geographically increase their social and economic interactions, the negative effects of the car on everyday life are not negligible. Although the introduction of the mass-produced car represented a revolution in industry and convenience, creating job demand and tax revenue, the high motorisation rates also brought severe consequences to the society and to the environment. The modern negative consequences of heavy automotive use include the use of non-renewable fuels, a dramatic increase in the rate of accidental death, the disconnection of local community, the decrease of local economy, the rise in obesity and cardiovascular diseases, the emission of air and noise pollution, the emission of greenhouse gases, generation of urban sprawl and traffic, segregation of pedestrians and other active mobility means of transport, decrease in the railway network, urban decay and the high cost per unit-distance on which the car paradigm is based.\n\nIn the early 20th century, cars entered mass production. The United States produced 45,000 cars in 1907, but 28 years later, in 1935, this had increased nearly 90-fold to 3,971,000. This increase in production required a new large work force. In 1913 there were 13,623 people working for Ford Motor Company, but by 1915 this had increased to 18,028. Bradford DeLong, author of The Roaring Twenties, tells us that \"Many more lined up outside the Ford factory for chances to work at what appeared to them to be (and, for those who did not mind the pace of the assembly line much, was) an incredible boondoggle of a job.\" There was a surge in the need for workers at big, new high-technology companies such as Ford. Employment increased greatly.\n\nThough, when the motor age arrived in western countries at the beginning of the 20th century, many conservative intellectuals opposed the increase in motor vehicles on the roads. These increases removed space for pedestrians, and brought a tremendous increase in pedestrian deaths caused by car collisions.\n\nW.S. Gilbert, a famous British librettist, wrote to \"The Times\" on 3 June 1903:\n\nTen years later, Alfred Godley wrote a more elaborate protest, \"The Motor Bus\", a poem which cleverly combined a lesson in Latin grammar with an expression of distaste for innovative motor transport.\n\nWorldwide, the car has allowed easier access to remote places. However, average journey times to regularly visited places have increased in large cities, as a result of widespread car adoption and urban sprawl, as well as the decommissioning of older tram systems. This is due to traffic congestion and the increased distances between home and work brought about by urban sprawl.\n\nExamples of car access issues in underdeveloped countries are for example the paving of Mexican Federal Highway 1 through Baja California, completing the connection of Cabo San Lucas to California. In Madagascar, another example, about 30% of the population does not have access to reliable all-weather roads and in China, 184 towns and 54,000 villages have no motor road (or roads at all).\n\nCertain developments in retail are partially due to car use, such as drive-thru fast food purchasing and Gasoline station grocery shopping.\n\nThe development of the car has contributed to changes in employment distribution, shopping patterns, social interactions, manufacturing priorities and city planning; increasing use of cars has reduced the roles of walking, horses and railroads.\n\nIn addition to money for roadway construction, car use was also encouraged in many places through new zoning laws that allowed any new business to construct a certain amount of parking based on the size and type of facility. The effect was to create many free parking spaces, and business places further back from the road. In aggregate, this led to less dense settlements and made a carless lifestyle increasingly unattractive.\n\nMany new shopping centers and suburbs did not install sidewalks, making pedestrian access dangerous. This had the effect of encouraging people to drive, even for short trips that might have been walkable, thus increasing and solidifying American auto-dependency. As a result of this change, employment opportunities for people who were not wealthy enough to own a car and for people who could not drive, due to age or physical disabilities, became severely limited.\n\nIn countries with major car manufacturers, such as USA or Germany, a certain degree of car dependency might be positive for the economy at a macroeconomic level, since it demands automobile production, therefore resulting also in job demand and tax revenue. These economic conditions were particularly valid during the 1920s when the number of automobiles, worldwide, had a substantial annual average increase, but also during the post–World War II economic expansion. Notwithstanding the growing effects provided by the automobile on the economy of some countries, several other auto-dependent countries, deprived from automobile industry and oil resources, have to allocate substantial economic assets, to satisfy its mobility policies, affecting then their commercial balance. This situation is broadly valid in the majority of the European countries, since, disregarding some few exceptions such as Norway, Europe is largely dependent on imports for its fossil fuels. Furthermore, just few European countries, such as Germany or France, have car manufacturers productive enough to satisfy their country's internal demand for cars. All these factors related to high motorisation rates, affect therefore the economic growth in the majority of the European countries. Most African countries are also dependant from imported cars, usually second-hand from Western countries, some of those vehicles being in a very worn-out state. Finally, even countries with oil ressources could be deprived of refineries, such as Nigeria which has to import fuel even though it is a major oil producer.\n\nAs of 2009 the U.S. motor vehicle manufacturing industry employed 880,000 workers, or approximately 6.6% of the U.S. manufacturing workforce.\n\nCycling steadily became more important in Europe over the first half of the twentieth century, but it dropped off dramatically in the United States between 1900 and 1910. Automobiles became the preferred means of transportation. Over the 1920s, bicycles gradually became considered children's toys, and by 1940 most bicycles in the United States were made for children.\nFrom the early 20th century until after World War II, the roadster constituted most adult bicycles sold in the United Kingdom and in many parts of the British Empire. For many years after the advent of the motorcycle and automobile, they remained a primary means of adult transport. In several places - both high and low income - bicycles have retained or regained this position. In Denmark, cycling policies were adopted as a direct consequence of the 1973 oil crisis whereas bike advocacy in the Netherlands started in earnest with a campaign against traffic deaths called \"stop child murder\". Today both countries have high modal shares of cycling despite high car ownership rates.\n\nPrior to the appearance of the automobile, horses, walking and streetcars were the major modes of transportation within cities. Horses require a large amount of care, and were therefore kept in public facilities that were usually far from residences. The wealthy could afford to keep horses for private use, hence the term carriage trade referred to elite patronage. Horse manure left on the streets also created a sanitation problem.\n\nThe motorcycle made regular medium-distance travel more convenient and affordable and after World War I the automobile too, especially in areas without railways. Because cars did not require rest, were faster than horse-drawn conveyances, and soon had a lower total cost of ownership, more people were routinely able to travel farther than in earlier times. The construction of highways in the 1950s continued this. Some experts suggest that many of these changes began during the earlier Golden age of the bicycle, from 1880 to 1915.\n\nBeginning in the 1940s, most urban environments in the United States lost their streetcars, cable cars, and other forms of light rail, to be replaced by diesel-burning motor coaches or buses. Many of these have never returned, but some urban communities eventually installed rapid transit.\n\nAnother change brought about by the car is that modern urban pedestrians must be more alert than their ancestors. In the past, a pedestrian had to worry about relatively slow-moving streetcars or other obstacles of travel. With the proliferation of the car, a pedestrian has to anticipate safety risks of automobiles traveling at high speeds because they can cause serious injuries to a human and can be fatal, unlike in previous times when traffic deaths were usually due to horses escaping control.\n\nAccording to many social scientists, the loss of pedestrian-scale villages has also disconnected communities. Many people in developed countries have less contact with their neighbors and rarely walk unless they place a high value on exercise. \nIn the decades following World War II, the auto united in the United States with the single family dwelling to form suburbs. Suburban affluence led to a baby boomer generation far removed from the hardships of their parents. Community standards of the past, driven by scarcity and the need to share public resources, gave way to new credos of self-exploration. As the economy of the fifties and sixties boomed, car sales grew steadily, from 6 million units sold per year in the U.S. to 10 million. Married women entered into the economy and two car households became commonplace. In the seventies, however, the comparative economic stagnation then experienced was accompanied by societal self-reflection on the changes the auto brought. Critics of automotive society found little positive choice in the decision to move to the suburbs; the physical movement was looked upon as flight. The auto industry was also under attack from bureaucratic fronts, and new emission and CAFÉ regulations began to hamper Big Three profit margins.\n\nKenneth Schneider in Autokind vs Mankind (1971) called for a war against the auto, derided it for being a destroyer of cities, and likened its proliferation to a disease. Renowned social critic Vance Packard in A Nation of Strangers (1972) blamed the geographic mobility enabled by the auto for loneliness and social isolation. Auto sales peaked in 1973, at 14.6 million units sold, and were not to reach comparable levels for another decade. The 1973 Arab-Israeli War was followed by the OPEC oil embargo, leading to an explosion of prices, long gas lines, and talk of rationing.\n\nWhile it may appear clear, in retrospect, that the automotive/suburban culture would continue to flourish, as it did in the 1950s and 1960s, no such certainty existed at the time when British architect Martin Pawley authored his seminal work, The Private Future (1973). Pawley called the automobile “the shibboleth of privatization; the symbol and the actuality of withdrawal from the community” and perceived that, in spite of its momentary misfortunes, its dominance in North American society would continue. The car was a private world that allowed for fantasy and escape, and Pawley forecasted that it would grow in size, and in technological capacities. He saw no pathology in consumer behavior grounded in freedom of expression.\n\nImproved transport accelerated the outward growth of cities and the development of suburbs beyond an earlier era's streetcar suburbs. Until the advent of the car, factory workers lived either close to the factory or in high density communities farther away, connected to the factory by streetcar or rail. The car and the federal subsidies for roads and suburban development that supported car culture allowed people to live in low density residential areas even farther from the city center and integrated city neighborhoods. were Industrial suburbs being few, due in part to single use zoning, they created few local jobs and residents commuted longer distances to work each day as the suburbs continued to expand.\n\nThe car had a significant effect on the culture of the United States. As other vehicles had been, cars were incorporated into artworks including music, books and movies. Between 1905 and 1908, more than 120 songs were written in which the automobile was the subject. Although authors such as Booth Tarkington decried the automobile age in books including \"The Magnificent Ambersons\" (1918), novels celebrating the political effects of motorization included \"Free Air\" (1919) by Sinclair Lewis, which followed in the tracks of earlier bicycle touring novels. Some early 20th century experts doubted the safety and suitability of allowing female automobilists. Dorothy Levitt was among those eager to lay such concerns to rest, so much so that a century later only one country had a women to drive movement. Where 19th-century mass media had made heroes of Casey Jones, Allan Pinkerton and other stalwart protectors of public transport, new road movies offered heroes who found freedom and equality, rather than duty and hierarchy, on the open road.\n\nGeorge Monbiot writes that widespread car culture has shifted voter's preference to the right-wing of the political spectrum, and thinks that car culture has contributed to an increase in individualism and fewer social interactions between members of different socioeconomic classes. The American Motor League had promoted the making of more and better cars since the early days of the car, and the American Automobile Association joined the good roads movement begun during the earlier bicycle craze; when manufacturers and petroleum fuel suppliers were well established, they also joined construction contractors in lobbying governments to build public roads.\n\nAs tourism became motorized, individuals, families and small groups were able to vacation in distant locations such as national parks. Roads including the Blue Ridge Parkway were built specifically to help the urban masses experience natural scenery previously seen only by a few. Cheap restaurants and motels appeared on favorite routes and provided wages for locals who were reluctant to join the trend to rural depopulation.\n\nRoad building was sometimes also influenced by Keynesian-style political ideologies. In Europe, massive freeway building programs were initiated by a number of social democratic governments after World War II, in an attempt to create jobs and make the car available to the working classes. From the 1970s, promotion of the automobile increasingly became a trait of some conservatives. Margaret Thatcher mentioned a \"great car economy\" in the paper on Roads for Prosperity. The 1973 oil crisis and with it fuel rationing measures brought to light for the first time in a generation, what cities without cars might look like, reinvigorating or creating environmental consciousness in the process. Green parties emerged in several European countries in partial response to car culture, but also as the political arm of the anti-nuclear movement.\n\nThe rise of car culture during the twentieth century, played an important cultural role in cinema, mainly through blockbusters. Important characters such as James Bond, or the ones performed by James Dean, were always provided on scene with powerful automobiles, which through time, have become cultural icons.\n\nOver time, the car has evolved beyond being a means of transportation or status symbol and into a subject of interest and a cherished lifestyle amongst many people in the world, who appreciate cars for their craftsmanship, their performance, as well as the vast arrays of activities one can take part in with his/her car. People who have a keen interest in cars and/or participate in the car hobby are known as \"Car Enthusiasts\".\n\nOne major aspect of the hobby is collecting. Cars, especially classic vehicles, are appreciated by their owners as having aesthetic, recreational and historic value. Such demand generates investment potential and allows some cars to command extraordinarily high prices and become financial instruments in their own right.\n\nA second major aspect of the car hobby is vehicle modification, as many car enthusiasts modify their cars to achieve performance improvements or visual enhancements. Many subcultures exist within this segment of the car hobby, for example, those building their own custom vehicles, primarily appearance-based on original examples or reproductions of pre-1948 US car market designs and similar designs from the World War II era and earlier from elsewhere in the world, are known as hot rodders, while those who believe cars should stay true to their original designs and not be modified are known as \"Purists\".\n\nIn addition, motorsport (both professional and amateur) as well as casual driving events, where enthusiasts from around the world gather to drive and display their cars, are important pillars of the car hobby as well. Notable examples such events are the annual Mille Miglia classic car rally and the Gumball 3000 supercar race.\n\nMany car clubs have been set up to facilitate social interactions and companionships amongst those who take pride in owning, maintaining, driving and showing their cars. Many prestigious social events around the world today are centered around the hobby, a notable example is the Pebble Beach Concours d'Elegance classic car show.\n\nMotor vehicle accidents account for 37.5% of accidental deaths in the United States, making them the country's leading cause of accidental death. Though travelers in cars suffer fewer deaths per journey, or per unit time or distance, than most other users of private transport such as bicyclers or pedestrians , cars are also more used, making automobile safety an important topic of study. For those aged 5–34 in the United States, motor vehicle crashes are the leading cause of death, claiming the lives of 18,266 Americans each year.\n\nIt is estimated that motor vehicle collisions caused the death of around 60 million people during the 20th century around the same number of World War II casualties. Just in 2010 alone, 1.23 million people were killed due to traffic collisions.\n\nNotwithstanding the high number of fatalities, the trend of motor vehicle collision is showing a decrease. Road toll figures in developed nations show that car collision fatalities have declined since 1980. Japan is an extreme example, with road deaths decreasing to 5,115 in 2008, which is 25% of the 1970 rate per capita and 17% of the 1970 rate per vehicle distance travelled. In 2008, for the first time, more pedestrians than vehicle occupants were killed in Japan by cars. Besides improving general road conditions like lighting and separated walkways, Japan has been installing intelligent transportation system technology such as stalled-car monitors to avoid crashes.\n\nIn developing nations, statistics may be grossly inaccurate or hard to get. Some nations have not significantly reduced the total death rate, which stands at 12,000 in Thailand in 2007, for example. In the United States, twenty-eight states had reductions in the number of automobile crash fatalities between 2005 and 2006. 55% of vehicle occupants 16 years or older in 2006 were not using seat belts when they crashed. Road fatality trends tend to follow Smeed's law, an empirical schema that correlates increased fatality rates per capita with traffic congestion.\n\nAccording to the \"Handbook on estimation of external costs in the transport sector\" made by the Delft University and which is the main reference in European Union for assessing the externalities of cars, the main external costs of driving a car are:\n\n\nUse of cars for transportation creates barriers by reducing the landscape required for walking and cycling. It may look like a minor problem initially but in the long run, it poses a threat to children and the elderly. Transport is a major land use, leaving less land available for other purposes.\n\nCars also contribute to pollution of air and water. Though a horse produces more waste, cars are cheaper, thus far more numerous in urban areas than horses ever were. Emissions of harmful gases like carbon monoxide, ozone, carbon dioxide, benzene and particulate matter can damage living organisms and the environment. The emissions from cars cause disabilities, respiratory diseases, and ozone depletion. Noise pollution from cars can also potentially result in hearing disabilities, headaches, and stress to those frequently exposed to it.\n\nIn countries such as the United States the infrastructure that makes car use possible, such as highways, roads and parking lots is funded by the government and supported through zoning and construction requirements. Fuel taxes in the United States cover about 60% of highway construction and repair costs, but little of the cost to construct or repair local roads. Payments by motor-vehicle users fall short of government expenditures tied to motor-vehicle use by 20–70 cents per gallon of gas. Zoning laws in many areas require that large, free parking lots accompany any new buildings. Municipal parking lots are often free or do not charge a market rate. Hence, the cost of driving a car in the US is subsidized, supported by businesses and the government who cover the cost of roads and parking. This is in addition to other external costs car users do not pay like accidents or pollution. Even in countries with higher gas taxes like Germany motorists don't fully pay for the external costs they create.\n\nThis government support of the automobile through subsidies for infrastructure, the cost of highway patrol enforcement, recovering stolen cars, and many other factors makes public transport a less economically competitive choice for commuters when considering Out-of-pocket expenses. Consumers often make choices based on those costs, and underestimate the indirect costs of car ownership, insurance and maintenance. However, globally and in some US cities, tolls and parking fees partially offset these heavy subsidies for driving. Transportation planning policy advocates often support tolls, increased fuel taxes, congestion pricing and market-rate pricing for municipal parking as a means of balancing car use in urban centers with more efficient modes such as buses and trains.\n\nWhen cities charge market rates for parking, and when bridges and tunnels are tolled, driving becomes less competitive in terms of out-of-pocket costs. When municipal parking is underpriced and roads are not tolled, most of the cost of vehicle usage is paid for by general government revenue, a subsidy for motor vehicle use. The size of this subsidy dwarfs the federal, state, and local subsidies for the maintenance of infrastructure and discounted fares for public transportation.\n\nBy contrast, although there are environmental and social costs for rail, there is a very small impact.\n\nWalking or cycling often have net positive impacts on society as they help reduce health costs and produce virtually no pollution.\n\nCompared to other popular modes of passenger transportation, especially buses or trains, the car has a relatively high cost per passenger-distance travelled. Motorists in the United Kingdom seem to spend on their cars an average of roughly 1/3 of their average net income, while motorists in Portugal seem to spend 1/2 of their net income. For the average car owner, depreciation constitutes about half the cost of running a car, nevertheless the typical motorist underestimates this fixed cost by a big margin, or even ignores it altogether.\n\nIn the United States, out of pocket expenses for car ownership can vary considerably based on the state in which you live. In 2013, annual car ownership costs including repair, insurance, gas and taxes were highest in Georgia ($4,233) and lowest in Oregon ($2,024) with a national average of $3,201. Furthermore, the IRS considers, for tax deduction calculations, that the automobile has a total cost for drivers in the USA, of 0.55 USD/mile, around 0.26 EUR/km. Data provided by the American Automobile Association indicates that the cost of ownership for an automobile in the United States is rising about 2% per year. 2013 data provided by the Canadian Automobile Association concludes that the cost of ownership for a compact car in Canada, including depreciation, insurance, borrowing costs, maintenance, licensing, etc. was CA $9500 per year, or about US $7300.\n\nThe Austrian philosopher Ivan Illich, a critic of the modern society habits, was one of the first thinkers to establish the so-called consumer speed concept. He wrote in his book \"Energy and Equity\" published in 1974:\n\nIt is known by classical mechanics that the average kinetic speed formula_1 of an automobile and its passengers is simply the amount of space the car travels, divided by the elapsed time, i.e.:\nwhere formula_3 is the distance travelled by the car and formula_4 is the travelled time, i.e., the time elapsed during the travel.\n\nThough, to assess the consumer speed, we must sum the amount of time the car owner strictly allocates to work to afford such travelled distance. Then the consumer speed formula_5 is:\nwhere formula_7 is the time the driver needs to work, to afford doing that specific travelled distance formula_3 using such car.\n\nJames (an example), a common car owner and driver who takes his car to get to work, spends totally (standing and running costs) on his car an average of €5000 per year. Considering James just uses his car to get to work and that one year has around 250 business days, James pays on average €20 per working day to afford his car. Consider the James' average net salary is €10 per hour; then James needs to work 2 hours per day just to afford his mean of transport to get to work, time strictly allocated to pay his car bills.\n\nIf he lives 20 km away from his workplace and he gets there in half an hour, then he makes 40 km per day during one hour (round trip). His kinetic average speed would then be:\nThough, James needs on average 2 hours per day just to afford his car, working time budget strictly allocated for paying his car bills, so his consumer speed would be:\njust 1/3 of his kinetic speed.\n\nGeneral:\nAlternatives:\nEffects:\nPlanning response:\n"}
{"id": "5976318", "url": "https://en.wikipedia.org/wiki?curid=5976318", "title": "Elephant (science book)", "text": "Elephant (science book)\n\nElephant is a 1964 science book by L. Sprague de Camp, published by Pyramid Books as part of The Worlds of Science series.\n\nThe book treats its subject comprehensively, covering elephants in captivity and the wild, their use in ancient warfare, modern conflicts between elephants and farmers, and preservation efforts, among other topics. It is \"[d]esigned for the general reader and student, about the various aspects of the world's largest land animal, from fossils to captive elephants.\"\n\nWhile a decent study, the book is important more for its insight into the mind of the author than in its own right, elephants being a lifelong interest of de Camp's that figures in many of his other literary works. In his early time travel novel \"Lest Darkness Fall\" his protagonist Martin Padway pens a similar monograph, while in his historical novel \"An Elephant for Aristotle\" details the difficulties in transporting an elephant from India to Greece during ancient times. De Camp also wrote a number of articles about elephants, a few of which appeared, together with a chapter selected from the present work, in his later collection \"The Fringe of the Unknown\" (1983).\n"}
{"id": "9888003", "url": "https://en.wikipedia.org/wiki?curid=9888003", "title": "Emesh", "text": "Emesh\n\nEmesh is a Sumerian god of vegetation. He was created, alongside the god Enten, at the wish of Enlil to take responsibility on earth for woods, fields, sheep folds, and stables. He is identified with the abundance of the earth and with summer. \n\nMichael Jordan, Encyclopedia of Gods, Kyle Cathie Limited, 2002\n"}
{"id": "57166788", "url": "https://en.wikipedia.org/wiki?curid=57166788", "title": "Energy in Malawi", "text": "Energy in Malawi\n\nBurning of charcoal and wood fuel provides approximately 94 percent of the energy in Malawi. Much of the renewable hydroelectric potential of the country is untapped. As of 2009, the national electrification rate in Malawi was 10%, with 37% of the urban population and only 2% of the rural population having access to electricity.\n\nIn March 2018, Malawi's installed electricity-generating capacity was , of which 93.3 percent was hydroelectric.\n\nWith a population of 19 million people in 2018, the country's per capita consumption of electrical energy is still low, estimated at 93 kWh per year compared with an average of 432 kWh for Sub-Saharan Africa and 2167 kWh per year for the World average. There is urgency for Malawi to reach the critical threshold of 500 kWh per year.\n\nThe un-bundling of the former electricity monopoly Electric Supply Commission of Malawi (ESCOM), with the formation of the Electricity Generation Company Malawi Limited (Egenco), is a welcome development. With the government of Malawi continuing to reform the energy sector, strong investor interest has developed with Independent Power Producers (IPPs) willing to enter the Malawi energy market.\n\nThe majority of the country's hydroelectric power stations are situated in the Southern Region of the country, specifically on the Shire River. Total grid-connected hydro-power is 351 megawatts. This is expected to increase to 369 megawatts, when the 18 megawatts Tedzani IV Hydroelectric Power Station is completed in 2020.\n\nWith peak demand of 350 megawatts and growing at about 6 percent annually, the Malawian grid has very little flexibility. This has exposed the country to severe, recurrent load-shedding. It has also stunted big industrial and manufacturing projects, especially in the areas of mining and processing, slowing national development and curtailing job creation.\n\nThe hydro-potential of the Shire River is estimated at about 600 MW, and another 400 megawatts of potential exists on smaller rivers including Songwe River, South Rukuru River, Dwangwa River and \"Bua River\". Smaller off-grid untapped potential exists on smaller rivers scattered around the county.\n\nMalawi has coal reserves estimated at 22 billion tons. Coal can also be imported from neighboring Mozambique, via railroad. In 2014, Malawi decided to go ahead with plans to build Kammwamba Thermal Power Station, a planned , coal-fired power station in Neno District. The plans call for starting with a 300 megawatts development, expandable in the future to 1,000 megawatts. The primary source of the coal to fire the power station, was identified as the Moatize coalfields in neighboring Mozambique. The imported coal would be hauled to the site via the nearby railway line.\n\nThe government, through the \"Electricity Supply Commission of Malawi\", has established, on a temporary basis, several diesel fuel power stations in various locations around the country.\n\nAs of April 2018, Malawi has no proven oil and/or gas reserves.\n\nSeveral IPPs are in the process of setting up grid-ready photo-voltaic solar power plants, to be ready by December 2019. The sites under consideration include Salima, Lilongwe, Dedza and Nkhotakota. A total of 40-70 megawatts is expected from these sites. Many off-grid solar projects dot the countryside, as the hope of universal grid coverage is increasingly growing doubtful.\n\n\n"}
{"id": "28562741", "url": "https://en.wikipedia.org/wiki?curid=28562741", "title": "European Energy Centre", "text": "European Energy Centre\n\nThe European Energy Centre (EEC) was incorporated in 2010 in Scotland as a private limited company; it has no official links to the European Union. Although its website makes a claim that it was created in 1975, this relates to the Centro Studi Galileo, an affiliated (but legally unconnected) company in Italy that runs a biennial conference on air-conditioning and refrigeration. EEC's focus is on renewable energy training and conference delivery to help develop a workforce capable of installing, repairing and maintaining renewable energy equipment.\n\nThe European Energy Centre and Centro Studi Galileo work with the United Nations Environment Programme (UNEP) the Intergovernmental International Institute of Refrigeration and Centro Studi Galileo, along with Universities such as Edinburgh Napier University and Heriot-Watt University in promoting the use of renewable energy technologies across the United Kingdom.\n\nThe European Energy Centre is also active European-wide with conferences in refrigeration, air conditioning and renewable energy, specifically Heating and Cooling technologies, see the 14th European Conference at Heriot-Watt University, Edinburgh.\n\nThe European Energy Centre is also active in India with training courses and conferences with its Indian Partner TERRE Policy Centre.\n\n"}
{"id": "48803", "url": "https://en.wikipedia.org/wiki?curid=48803", "title": "Gamma-ray burst", "text": "Gamma-ray burst\n\nIn gamma-ray astronomy, gamma-ray bursts (GRBs) are extremely energetic explosions that have been observed in distant galaxies. They are the brightest electromagnetic events known to occur in the universe. Bursts can last from ten milliseconds to several hours. After an initial flash of gamma rays, a longer-lived \"afterglow\" is usually emitted at longer wavelengths (X-ray, ultraviolet, optical, infrared, microwave and radio).\n\nThe intense radiation of most observed GRBs is thought to be released during a supernova or superluminous supernova as a high-mass star collapses to form a neutron star or a black hole.\n\nA subclass of GRBs (the \"short\" bursts) appear to originate from a kilonova (the merger of binary neutron stars). The cause of the precursor burst observed in some of these short events may be the development of a resonance between the crust and core of such stars as a result of the massive tidal forces experienced in the seconds leading up to their collision, causing the entire crust of the star to shatter.\n\nThe sources of most GRBs are billions of light years away from Earth, implying that the explosions are both extremely energetic (a typical burst releases as much energy in a few seconds as the Sun will in its entire 10-billion-year lifetime) and extremely rare (a few per galaxy per million years). All observed GRBs have originated from outside the Milky Way galaxy, although a related class of phenomena, soft gamma repeater flares, are associated with magnetars within the Milky Way. It has been hypothesized that a gamma-ray burst in the Milky Way, pointing directly towards the Earth, could cause a mass extinction event.\n\nGRBs were first detected in 1967 by the Vela satellites, which had been designed to detect covert nuclear weapons tests; this was declassified and published in 1973. Following their discovery, hundreds of theoretical models were proposed to explain these bursts, such as collisions between comets and neutron stars. Little information was available to verify these models until the 1997 detection of the first X-ray and optical afterglows and direct measurement of their redshifts using optical spectroscopy, and thus their distances and energy outputs. These discoveries, and subsequent studies of the galaxies and supernovae associated with the bursts, clarified the distance and luminosity of GRBs, definitively placing them in distant galaxies.\n\nGamma-ray bursts were first observed in the late 1960s by the U.S. Vela satellites, which were built to detect gamma radiation pulses emitted by nuclear weapons tested in space. The United States suspected that the Soviet Union might attempt to conduct secret nuclear tests after signing the Nuclear Test Ban Treaty in 1963. On July 2, 1967, at 14:19 UTC, the Vela 4 and Vela 3 satellites detected a flash of gamma radiation unlike any known nuclear weapons signature. Uncertain what had happened but not considering the matter particularly urgent, the team at the Los Alamos National Laboratory, led by Ray Klebesadel, filed the data away for investigation. As additional Vela satellites were launched with better instruments, the Los Alamos team continued to find inexplicable gamma-ray bursts in their data. By analyzing the different arrival times of the bursts as detected by different satellites, the team was able to determine rough estimates for the sky positions of sixteen bursts and definitively rule out a terrestrial or solar origin. The discovery was declassified and published in 1973.\n\nMost early theories of gamma-ray bursts posited nearby sources within the Milky Way Galaxy. From 1991, the Compton Gamma Ray Observatory (CGRO) and its Burst and Transient Source Explorer (BATSE) instrument, an extremely sensitive gamma-ray detector, provided data that showed the distribution of GRBs is isotropic—not biased towards any particular direction in space. If the sources were from within our own galaxy they would be strongly concentrated in or near the galactic plane. The absence of any such pattern in the case of GRBs provided strong evidence that gamma-ray bursts must come from beyond the Milky Way. However, some Milky Way models are still consistent with an isotropic distribution.\n\nIn October 2018, astronomers reported that GRB 150101B, a gamma-ray burst event detected in 2015, may be directly related to the historic GW170817, a gravitational wave event detected in 2017, and associated with the merger of two neutron stars. The similarities between the two events, in terms of gamma ray, optical and x-ray emissions, as well as to the nature of the associated host galaxies, are \"striking\", suggesting the two separate events may both be the result of the merger of neutron stars, and both may be a kilonova, which may be more common in the universe than previously understood, according to the researchers.\n\nFor decades after the discovery of GRBs, astronomers searched for a counterpart at other wavelengths: i.e., any astronomical object in positional coincidence with a recently observed burst. Astronomers considered many distinct classes of objects, including white dwarfs, pulsars, supernovae, globular clusters, quasars, Seyfert galaxies, and BL Lac objects. All such searches were unsuccessful, and in a few cases particularly well-localized bursts (those whose positions were determined with what was then a high degree of accuracy) could be clearly shown to have no bright objects of any nature consistent with the position derived from the detecting satellites. This suggested an origin of either very faint stars or extremely distant galaxies. Even the most accurate positions contained numerous faint stars and galaxies, and it was widely agreed that final resolution of the origins of cosmic gamma-ray bursts would require both new satellites and faster communication.\n\nSeveral models for the origin of gamma-ray bursts postulated that the initial burst of gamma rays should be followed by slowly fading emission at longer wavelengths created by collisions between the burst ejecta and interstellar gas. This fading emission would be called the \"afterglow\". Early searches for this afterglow were unsuccessful, largely because it is difficult to observe a burst's position at longer wavelengths immediately after the initial burst. The breakthrough came in February 1997 when the satellite BeppoSAX detected a gamma-ray burst (GRB 970228) and when the X-ray camera was pointed towards the direction from which the burst had originated, it detected fading X-ray emission. The William Herschel Telescope identified a fading optical counterpart 20 hours after the burst. Once the GRB faded, deep imaging was able to identify a faint, distant host galaxy at the location of the GRB as pinpointed by the optical afterglow.\n\nBecause of the very faint luminosity of this galaxy, its exact distance was not measured for several years. Well before then, another major breakthrough occurred with the next event registered by BeppoSAX, GRB 970508. This event was localized within four hours of its discovery, allowing research teams to begin making observations much sooner than any previous burst. The spectrum of the object revealed a redshift of \"z\" = 0.835, placing the burst at a distance of roughly 6 billion light years from Earth. This was the first accurate determination of the distance to a GRB, and together with the discovery of the host galaxy of 970228 proved that GRBs occur in extremely distant galaxies. Within a few months, the controversy about the distance scale ended: GRBs were extragalactic events originating within faint galaxies at enormous distances. The following year, GRB 980425 was followed within a day by a bright supernova (SN 1998bw), coincident in location, indicating a clear connection between GRBs and the deaths of very massive stars. This burst provided the first strong clue about the nature of the systems that produce GRBs.\nBeppoSAX functioned until 2002 and CGRO (with BATSE) was deorbited in 2000. However, the revolution in the study of gamma-ray bursts motivated the development of a number of additional instruments designed specifically to explore the nature of GRBs, especially in the earliest moments following the explosion. The first such mission, HETE-2, launched in 2000 and functioned until 2006, providing most of the major discoveries during this period. One of the most successful space missions to date, Swift, was launched in 2004 and as of 2016 is still operational. Swift is equipped with a very sensitive gamma ray detector as well as on-board X-ray and optical telescopes, which can be rapidly and automatically slewed to observe afterglow emission following a burst. More recently, the Fermi mission was launched carrying the Gamma-Ray Burst Monitor, which detects bursts at a rate of several hundred per year, some of which are bright enough to be observed at extremely high energies with Fermi's Large Area Telescope. Meanwhile, on the ground, numerous optical telescopes have been built or modified to incorporate robotic control software that responds immediately to signals sent through the Gamma-ray Burst Coordinates Network. This allows the telescopes to rapidly repoint towards a GRB, often within seconds of receiving the signal and while the gamma-ray emission itself is still ongoing.\n\nNew developments since the 2000s include the recognition of short gamma-ray bursts as a separate class (likely from merging neutron stars and not associated with supernovae), the discovery of extended, erratic flaring activity at X-ray wavelengths lasting for many minutes after most GRBs, and the discovery of the most luminous (GRB 080319B) and the former most distant (GRB 090423) objects in the universe. The most distant known GRB, GRB 090429B, is now the most distant known object in the universe.\n\nThe light curves of gamma-ray bursts are extremely diverse and complex. No two gamma-ray burst light curves are identical, with large variation observed in almost every property: the duration of observable emission can vary from milliseconds to tens of minutes, there can be a single peak or several individual subpulses, and individual peaks can be symmetric or with fast brightening and very slow fading. Some bursts are preceded by a \"precursor\" event, a weak burst that is then followed (after seconds to minutes of no emission at all) by the much more intense \"true\" bursting episode. The light curves of some events have extremely chaotic and complicated profiles with almost no discernible patterns.\n\nAlthough some light curves can be roughly reproduced using certain simplified models, little progress has been made in understanding the full diversity observed. Many classification schemes have been proposed, but these are often based solely on differences in the appearance of light curves and may not always reflect a true physical difference in the progenitors of the explosions. However, plots of the distribution of the observed duration for a large number of gamma-ray bursts show a clear bimodality, suggesting the existence of two separate populations: a \"short\" population with an average duration of about 0.3 seconds and a \"long\" population with an average duration of about 30 seconds. Both distributions are very broad with a significant overlap region in which the identity of a given event is not clear from duration alone. Additional classes beyond this two-tiered system have been proposed on both observational and theoretical grounds.\n\nEvents with a duration of less than about two seconds are classified as short gamma-ray bursts. These account for about 30% of gamma-ray bursts, but until 2005, no afterglow had been successfully detected from any short event and little was known about their origins. Since then, several dozen short gamma-ray burst afterglows have been detected and localized, several of which are associated with regions of little or no star formation, such as large elliptical galaxies and the central regions of large galaxy clusters. This rules out a link to massive stars, confirming that short events are physically distinct from long events. In addition, there has been no association with supernovae.\n\nThe true nature of these objects was initially unknown, and the leading hypothesis was that they originated from the mergers of binary neutron stars or a neutron star with a black hole. Such mergers were theorized to produce kilonovae, and evidence for a kilonova associated with GRB 130603B was seen. The mean duration of these events of 0.2 seconds suggests (because of causality) a source of very small physical diameter in stellar terms; less than 0.2 light-seconds (about 60,000 km or 37,000 miles—four times the Earth's diameter). The observation of minutes to hours of X-ray flashes after a short gamma-ray burst is consistent with small particles of a primary object like a neutron star initially swallowed by a black hole in less than two seconds, followed by some hours of lesser energy events, as remaining fragments of tidally disrupted neutron star material (no longer neutronium) remain in orbit to spiral into the black hole, over a longer period of time. A small fraction of short gamma-ray bursts are probably produced by giant flares from soft gamma repeaters in nearby galaxies.\n\nThe origin of short GRBs in kilonovae was confirmed when short GRB 170817A was detected only 1.7 s after the detection of gravitational wave GW170817, which was a signal from the merger of two neutron stars.\n\nMost observed events (70%) have a duration of greater than two seconds and are classified as long gamma-ray bursts. Because these events constitute the majority of the population and because they tend to have the brightest afterglows, they have been observed in much greater detail than their short counterparts. Almost every well-studied long gamma-ray burst has been linked to a galaxy with rapid star formation, and in many cases to a core-collapse supernova as well, unambiguously associating long GRBs with the deaths of massive stars. Long GRB afterglow observations, at high redshift, are also consistent with the GRB having originated in star-forming regions.\n\nThese events are at the tail end of the long GRB duration distribution, lasting more than 10,000 seconds. They have been proposed to form a separate class, caused by the collapse of a blue supergiant star, a tidal disruption event or a new-born magnetar. Only a small number have been identified to date, their primary characteristic being their gamma ray emission duration. The most studied ultra-long events include GRB 101225A and GRB 111209A. The low detection rate may be a result of low sensitivity of current detectors to long-duration events, rather than a reflection of their true frequency. A 2013 study, on the other hand, shows that the existing evidence for a separate ultra-long GRB population with a new type of progenitor is inconclusive, and further multi-wavelength observations are needed to draw a firmer conclusion.\n\nGamma-ray bursts are very bright as observed from Earth despite their typically immense distances. An average long GRB has a bolometric flux comparable to a bright star of our galaxy despite a distance of billions of light years (compared to a few tens of light years for most visible stars). Most of this energy is released in gamma rays, although some GRBs have extremely luminous optical counterparts as well. GRB 080319B, for example, was accompanied by an optical counterpart that peaked at a visible magnitude of 5.8, comparable to that of the dimmest naked-eye stars despite the burst's distance of 7.5 billion light years. This combination of brightness and distance implies an extremely energetic source. Assuming the gamma-ray explosion to be spherical, the energy output of GRB 080319B would be within a factor of two of the rest-mass energy of the Sun (the energy which would be released were the Sun to be converted entirely into radiation).\n\nGamma-ray bursts are thought to be highly focused explosions, with most of the explosion energy collimated into a narrow jet. The approximate angular width of the jet (that is, the degree of spread of the beam) can be estimated directly by observing the achromatic \"jet breaks\" in afterglow light curves: a time after which the slowly decaying afterglow begins to fade rapidly as the jet slows and can no longer beam its radiation as effectively. Observations suggest significant variation in the jet angle from between 2 and 20 degrees.\n\nBecause their energy is strongly focused, the gamma rays emitted by most bursts are expected to miss the Earth and never be detected. When a gamma-ray burst is pointed towards Earth, the focusing of its energy along a relatively narrow beam causes the burst to appear much brighter than it would have been were its energy emitted spherically. When this effect is taken into account, typical gamma-ray bursts are observed to have a true energy release of about 10 J, or about 1/2000 of a Solar mass () energy equivalent—which is still many times the mass-energy equivalent of the Earth (about 5.5 × 10 J). This is comparable to the energy released in a bright type Ib/c supernova and within the range of theoretical models. Very bright supernovae have been observed to accompany several of the nearest GRBs. Additional support for focusing of the output of GRBs has come from observations of strong asymmetries in the spectra of nearby type Ic supernova and from radio observations taken long after bursts when their jets are no longer relativistic.\n\nShort (time duration) GRBs appear to come from a lower-redshift (i.e. less distant) population and are less luminous than long GRBs. The degree of beaming in short bursts has not been accurately measured, but as a population they are likely less collimated than long GRBs or possibly not collimated at all in some cases.\n\nBecause of the immense distances of most gamma-ray burst sources from Earth, identification of the progenitors, the systems that produce these explosions, is challenging. The association of some long GRBs with supernovae and the fact that their host galaxies are rapidly star-forming offer very strong evidence that long gamma-ray bursts are associated with massive stars. The most widely accepted mechanism for the origin of long-duration GRBs is the collapsar model, in which the core of an extremely massive, low-metallicity, rapidly rotating star collapses into a black hole in the final stages of its evolution. Matter near the star's core rains down towards the center and swirls into a high-density accretion disk. The infall of this material into a black hole drives a pair of relativistic jets out along the rotational axis, which pummel through the stellar envelope and eventually break through the stellar surface and radiate as gamma rays. Some alternative models replace the black hole with a newly formed magnetar, although most other aspects of the model (the collapse of the core of a massive star and the formation of relativistic jets) are the same.\n\nThe closest analogs within the Milky Way galaxy of the stars producing long gamma-ray bursts are likely the Wolf–Rayet stars, extremely hot and massive stars, which have shed most or all of their hydrogen to radiation pressure. Eta Carinae and WR 104 have been cited as possible future gamma-ray burst progenitors. It is unclear if any star in the Milky Way has the appropriate characteristics to produce a gamma-ray burst.\n\nThe massive-star model probably does not explain all types of gamma-ray burst. There is strong evidence that some short-duration gamma-ray bursts occur in systems with no star formation and no massive stars, such as elliptical galaxies and galaxy halos. The favored theory for the origin of most short gamma-ray bursts is the merger of a binary system consisting of two neutron stars. According to this model, the two stars in a binary slowly spiral towards each other because gravitational radiation releases energy until tidal forces suddenly rip the neutron stars apart and they collapse into a single black hole. The infall of matter into the new black hole produces an accretion disk and releases a burst of energy, analogous to the collapsar model. Numerous other models have also been proposed to explain short gamma-ray bursts, including the merger of a neutron star and a black hole, the accretion-induced collapse of a neutron star, or the evaporation of primordial black holes.\n\nAn alternative explanation proposed by Friedwardt Winterberg is that in the course of a gravitational collapse and in reaching the event horizon of a black hole, all matter disintegrates into a burst of gamma radiation.\n\nThis new class of GRB-like events was first discovered through the detection of GRB 110328A by the Swift Gamma-Ray Burst Mission on 28 March 2011. This event had a gamma-ray duration of about 2 days, much longer than even ultra-long GRBs, and was detected in X-rays for many months. It occurred at the center of a small elliptical galaxy at redshift z = 0.3534. There is an ongoing debate as to whether the explosion was the result of stellar collapse or a tidal disruption event accompanied by a relativistic jet, although the latter explanation has become widely favoured.\n\nA tidal disruption event of this sort is when a star interacts with a supermassive black hole shredding the star, and in some cases creating a relativistic jet which produces bright emission of gamma ray radiation. The event GRB 110328A (also denoted Swift J1644+57) was initially argued to be produced by the disruption of a main sequence star by a black hole of several million times the mass of the Sun, although it has subsequently been argued that the disruption of a white dwarf by a black hole of mass about 10 thousand times the Sun may be more likely.\n\nThe means by which gamma-ray bursts convert energy into radiation remains poorly understood, and as of 2010 there was still no generally accepted model for how this process occurs. Any successful model of GRB emission must explain the physical process for generating gamma-ray emission that matches the observed diversity of light curves, spectra, and other characteristics. Particularly challenging is the need to explain the very high efficiencies that are inferred from some explosions: some gamma-ray bursts may convert as much as half (or more) of the explosion energy into gamma-rays. Early observations of the bright optical counterparts to GRB 990123 and to GRB 080319B, whose optical light curves were extrapolations of the gamma-ray light spectra, have suggested that inverse Compton may be the dominant process in some events. In this model, pre-existing low-energy photons are scattered by relativistic electrons within the explosion, augmenting their energy by a large factor and transforming them into gamma-rays.\n\nThe nature of the longer-wavelength afterglow emission (ranging from X-ray through radio) that follows gamma-ray bursts is better understood. Any energy released by the explosion not radiated away in the burst itself takes the form of matter or energy moving outward at nearly the speed of light. As this matter collides with the surrounding interstellar gas, it creates a relativistic shock wave that then propagates forward into interstellar space. A second shock wave, the reverse shock, may propagate back into the ejected matter. Extremely energetic electrons within the shock wave are accelerated by strong local magnetic fields and radiate as synchrotron emission across most of the electromagnetic spectrum. This model has generally been successful in modeling the behavior of many observed afterglows at late times (generally, hours to days after the explosion), although there are difficulties explaining all features of the afterglow very shortly after the gamma-ray burst has occurred.\n\nGamma ray bursts can have harmful or destructive effects on life. Considering the universe as a whole, the safest environments for life similar to that on Earth are the lowest density regions in the outskirts of large galaxies. Our knowledge of galaxy types and their distribution suggests that life as we know it can only exist in about 10% of all galaxies. Furthermore, galaxies with a redshift, \"z\", higher than 0.5 are unsuitable for life as we know it, because of their higher rate of GRBs and their stellar compactness.\n\nAll GRBs observed to date have occurred well outside the Milky Way galaxy and have been harmless to Earth. However, if a GRB were to occur within the Milky Way and its emission were beamed straight towards Earth, the effects could be harmful and potentially devastating for the ecosystems. Currently, orbiting satellites detect on average approximately one GRB per day. The closest observed GRB as of March 2014 was GRB 980425, located away (z=0.0085) in an SBc-type dwarf galaxy. GRB 980425 was far less energetic than the average GRB and was associated with the Type Ib supernova SN 1998bw.\n\nEstimating the exact rate at which GRBs occur is difficult; for a galaxy of approximately the same size as the Milky Way, estimates of the expected rate (for long-duration GRBs) can range from one burst every 10,000 years, to one burst every 1,000,000 years. Only a small percentage of these would be beamed towards Earth. Estimates of rate of occurrence of short-duration GRBs are even more uncertain because of the unknown degree of collimation, but are probably comparable.\n\nSince GRBs are thought to involve beamed emission along two jets in opposing directions, only planets in the path of these jets would be subjected to the high energy gamma radiation.\n\nAlthough nearby GRBs hitting Earth with a destructive shower of gamma rays are only hypothetical events, high energy processes across the galaxy have been observed to affect the Earth's atmosphere.\n\nEarth's atmosphere is very effective at absorbing high energy electromagnetic radiation such as x-rays and gamma rays, so these types of radiation would not reach any dangerous levels at the surface during the burst event itself. The immediate effect on life on Earth from a GRB within a few parsecs would only be a short increase in ultraviolet radiation at ground level, lasting from less than a second to tens of seconds. This ultraviolet radiation could potentially reach dangerous levels depending on the exact nature and distance of the burst, but it seems unlikely to be able to cause a global catastrophe for life on Earth.\n\nThe long-term effects from a nearby burst are more dangerous. Gamma rays cause chemical reactions in the atmosphere involving oxygen and nitrogen molecules, creating first nitrogen oxide then nitrogen dioxide gas. The nitrogen oxides cause dangerous effects on three levels. First, they deplete ozone, with models showing a possible global reduction of 25–35%, with as much as 75% in certain locations, an effect that would last for years. This reduction is enough to cause a dangerously elevated UV index at the surface. Secondly, the nitrogen oxides cause photochemical smog, which darkens the sky and blocks out parts of the sunlight spectrum. This would affect photosynthesis, but models show only about a 1% reduction of the total sunlight spectrum, lasting a few years. However, the smog could potentially cause a cooling effect on Earth's climate, producing a \"cosmic winter\" (similar to an impact winter, but without an impact), but only if it occurs simultaneously with a global climate instability. Thirdly, the elevated nitrogen levels in the atmosphere would wash out and produce nitric acid rain. Nitric acid is toxic to a variety of organisms, including amphibian life, but models predict that it would not reach levels that would cause a serious global effect. The nitrates might in fact be of benefit to some plants.\n\nAll in all, a GRB within a few parsecs, with its energy directed towards Earth, will mostly damage life by raising the UV levels during the burst itself and for a few years thereafter. Models show that the destructive effects of this increase can cause up to 16 times the normal levels of DNA damage. It has proved difficult to assess a reliable evaluation of the consequences of this on the terrestrial ecosystem, because of the uncertainty in biological field and laboratory data.\n\nGRBs close enough to affect life in some way might occur once every five million years or so — around a thousand times since life on Earth began.\n\nThe major Ordovician–Silurian extinction events 450 million years ago may have been caused by a GRB. The late Ordovician species of trilobites that spent portions of their lives in the plankton layer near the ocean surface were much harder hit than deep-water dwellers, which tended to remain within quite restricted areas. This is in contrast to the usual pattern of extinction events, wherein species with more widely spread populations typically fare better. A possible explanation is that trilobites remaining in deep water would be more shielded from the increased UV radiation associated with a GRB. Also supportive of this hypothesis is the fact that during the late Ordovician, burrowing bivalve species were less likely to go extinct than bivalves that lived on the surface.\n\nA case has been made that the 774–775 carbon-14 spike was the result of a short GRB, though a very strong solar flare is another possibility.\n\nA Wolf–Rayet star in WR 104, about away, is considered a nearby GRB candidate that could have destructive effects on terrestrial life. It is expected to explode in a core-collapse-supernova at some point within the next 500,000 years and it is possible that this explosion will create a GRB. If that happens, there is a small chance that Earth will be in the path of its gamma ray jet.\n\nNo gamma-ray burst from within our own galaxy, the Milky Way, has been observed, and the question of whether one has ever occurred remains unresolved. In light of evolving understanding of gamma-ray bursts and their progenitors, the scientific literature records a growing number of local, past, and future GRB candidates. Long duration GRBs are related to superluminous supernovae, or hypernovae, and most luminous blue variables (LBVs), and rapidly spinning Wolf–Rayet stars are thought to end their life cycles in core-collapse supernovae with an associated long-duration GRB. Knowledge of GRBs, however, is from metal-poor galaxies of former epochs of the universe's evolution, and it is impossible to directly extrapolate to encompass more evolved galaxies and stellar environments with a higher metallicity, such as the Milky Way.\n\n\n\n\n"}
{"id": "284969", "url": "https://en.wikipedia.org/wiki?curid=284969", "title": "Graeme Gibson", "text": "Graeme Gibson\n\nGraeme C. Gibson, (born 9 August 1934) is a Canadian novelist. He is a Member of the Order of Canada (1992) and was one of the organizers of the Writer's Union of Canada (chair, 1974–75). He is also a founder of the Writers' Trust of Canada, a non-profit literary organization that seeks to encourage Canada's writing community. He had two sons, Matt and Grae, with publisher Shirley Gibson.\n\nGibson has a long-term relationship with novelist and poet Margaret Atwood which began in 1973. They moved to a farm near Alliston, Ontario, where their daughter Eleanor Jess Atwood Gibson was born in 1976. The family returned to Toronto in 1980.\n\nHe is best known for his 1973 book \"Eleven Canadian Novelists\", a non-fiction work in which he conducted extended interviews about literature and writing with Atwood, Austin Clarke, Matt Cohen, Marian Engel, Timothy Findley, Dave Godfrey, Margaret Laurence, Jack Ludwig, Alice Munro, Mordecai Richler and Scott Symons.\n\nIn 1996 he decided to stop writing novels. At the time he was working on a novel titled \"Moral Disorder\". Atwood borrowed the title for her collection of short stories published in 2006. He is a former council member of World Wildlife Fund Canada and is chairman of Pelee Island Bird Observatory.\n\nHe was awarded, jointly with Jacob Verhoef and Margaret Atwood, the 2015 Gold Medal of the Royal Canadian Geographical Society.\n\n\"The New Yorker\" magazine reported in its April 17, 2017 edition that Gibson has been diagnosed with early signs of dementia.\n\n"}
{"id": "2829162", "url": "https://en.wikipedia.org/wiki?curid=2829162", "title": "Green wave", "text": "Green wave\n\nA green wave occurs when a series of traffic lights (usually three or more) are coordinated to allow continuous traffic flow over several intersections in one main direction.\n\nAny vehicle travelling along with the green wave (at an approximate speed decided upon by the traffic engineers) will see a progressive cascade of green lights, and not have to stop at intersections. This allows higher traffic loads, and reduces noise and energy use (because less acceleration and braking is needed). In practical use, only a group of cars (known as a \"platoon\", the size of which is defined by the signal times) can use the \"green wave\" before the time band is interrupted to give way to other traffic flows.\n\nThe coordination of the signals is sometimes done dynamically, according to sensor data of currently existing traffic flows - otherwise it is done statically, by the use of timers. Under certain circumstances, \"green waves\" can be interwoven with each other, but this increases their complexity and reduces usability, so in conventional set-ups only the roads and directions with the heaviest loads get this preferential treatment.\n\nIn 2011, a study modeled the implementation of green waves during the night in a busy Manchester suburb (Chorlton-cum-Hardy) using S-Paramics microsimulation and the AIRE emissions module. The results showed using green wave signal setups on a network have the potential to:\n\n\nA green wave in both directions may be possible with different speed recommendations for each direction, otherwise traffic coming from one direction may reach the traffic light faster than from the other direction if the distance from the previous traffic light is not mathematically a multiple of the opposite direction. Alternatively a dual carriageway may be suitable for green waves in both directions if there is sufficient space in the central reservation to allow pedestrians to wait and separate pedestrian crossing stages for each side of the road.\n\nGreen waves are sometimes used to facilitate bicycle traffic. Copenhagen, Amsterdam, San Francisco, and other cities may synchronize traffic signals to provide a green light for a flow of cyclists. On San Francisco's Valencia Street, the signals were retimed in early 2009 to provide a green wave in both directions, possibly the first street in the world with a two-way green wave for cyclists. In Copenhagen, a green wave on the arterial street Nørrebrogade facilitates 30,000 cyclists to maintain a 12 mph (19.3 km/h) speed for 2.5 kilometers. In Amsterdam, cyclists riding at a speed of 15 to 18 km/h will be able to travel without being stopped by a red signal. Tests show that public transport can benefit as well and cars may travel slightly slower.\n\nIn Vienna, Austria a stretch of cycle path on Lassellestrasse in the 2nd district has a display that tells cyclists their speed and the speed they must maintain to make the next green light.\n\nFrederiksberg, a part of Copenhagen, the capital of Denmark, has implemented a green wave for emergency vehicles to improve the public services.\n\nIn the UK, in 2009, it was revealed that the Department for Transport had previously discouraged green waves as they reduced fuel usage, and thus less revenue was raised from fuel taxes. Despite this government Webtag documents were only updated in 2011. It is still unclear if the economic appraisal software used to apply these guidelines has also been updated and if the new guidelines are being applied to new projects.\n\nIn a more limited sense, the term Green wave has also been applied to railroad travel. For several years starting in the 1960s, the German Federal Railway maintained an advertising campaign featuring the slogan \"garantiert grüne Welle\" (Guaranteed Green Wave), which communicated the notion of speed, limited delays and open track blocks to potential customers choosing between train and automobile travel, and was featured prominently in promotional materials ranging from posters to radio jingles.\n\n\n"}
{"id": "6634008", "url": "https://en.wikipedia.org/wiki?curid=6634008", "title": "Joel Hedgpeth", "text": "Joel Hedgpeth\n\nJoel Walker Hedgpeth (September 29, 1911 – July 28, 2006) was a marine biologist, environmentalist and author. He was an expert on the marine arthropods known as sea spiders (Pycnogonida), and on the seashore plant and animal life of southern California. He was a spokesperson for care for the floral and faunal diversity of the California coastline.\n\nHedgpeth was born on September 29, 1911 in Oakland, California. He married Florence Warrens in 1944, and the couple would have two children. He obtained his PhD (on the distribution and ecology of invertebrates along the Texas and Louisiana coasts) from the University of California, Berkeley in 1952. While at Berkeley, he studied under two of the most important marine biologists of the era, S.F. Light and Ralph I. Smith.\n\nHedgpeth met and corresponded with Edward F. Ricketts (1897–1948), a charismatic researcher of West Coast marine biology and the real-life model for the character \"Doc\" in John Steinbeck's novel, Cannery Row. Hedgpeth himself may have been the model for the character, \"Old Jay\" in Steinbeck's novel, Sweet Thursday [Schram and Newman 2007]. Hedgpeth later was the editor of several editions of Ricketts' \"Between Pacific Tides,\" a classic in marine biology, describing marine life along the coasts of California, Oregon and Washington. Hedgpeth edited more of Ricketts' writings in two volumes of \"The Outer Shores.\"\n\nHis publications included the massive Volume 1 of the \"Treatise on Maine Ecology & Paleoecology\" (1957); and \"Introduction to Seashore Life of the San Francisco Bay Region\" (1962). His teaching posts included the Scripps Institution of Oceanography at University of California, San Diego. He was also director of the Pacific Marine Station, a University of the Pacific research facility at Dillon Beach, California, from 1957 to 1965. He was director of the Yaquina Biological Laboratories of the Marine Science Center, Oregon State University, Newport, Oregon, from 1965 to 1973. He retired as Professor of Oceanography in September, 1973. He and his wife moved to Santa Rosa, California during retirement. He died July 28, 2006 in Hillsboro, Oregon. His archives are housed at Scripps Institution of Oceanography in San Diego, California.\n\nThe nudibranch \"Polycera hedgpethi\" was named in his honor by Ernst Marcus, a marine biologist who taught at the University of São Paulo in Brazil.\n\nHedgpeth was an iconoclast and an early environmentalist. He spoke Latin, German, Welsh, and Russian. He founded the \"Society for the Prevention of Progress\" and was its sole member, under a pseudonym, Jerome Tichenor (Schram and Newman 2007). Under the same pseudonym, he published \"Poems in Contempt of Progress\" and vocally opposed a nuclear power plant once proposed at Bodega Head, California (Carlton 2006). He was influential in the developing West Coast environmental movement in the 1970s. His influence was instrumental in getting the California freshwater shrimp, \"Syncaris pacifica\", listed as an endangered species (Schram and Newman 2007).\n\n\nHedgpeth (1911–2006). The Quarterly Review of Biology 82(2):93-96.\n"}
{"id": "51114484", "url": "https://en.wikipedia.org/wiki?curid=51114484", "title": "Knudsen paradox", "text": "Knudsen paradox\n\nThe Knudsen Paradox has been observed in experiments of channel flow with varying channel width or equivalently different pressures. If the normalized mass flux through the channel is plotted over the Knudsen number based on the channel width a distinct minimum is observed around formula_1. This is a paradoxical behaviour because, based on the Navier–Stokes equations, one would expect the mass flux to decrease with increasing the Knudsen number. The minimum can be understood intuitively by considering the two extreme cases of very small and very large Knudsen number. For very small Kn the viscosity vanishes and a fully developed steady state channel flow shows infinite flux. On the other hand, the particles stop interacting for large Knudsen numbers. Because of the constant acceleration due to the external force, the steady state again will show infinite flux.\n\n"}
{"id": "50003334", "url": "https://en.wikipedia.org/wiki?curid=50003334", "title": "List of National Waterways in India", "text": "List of National Waterways in India\n\nThere are 111 officially notified Inland National Waterways (NWs) in India identified for the purposes of inland water transport,\nas per The National Waterways Act, 2016. Out of the 111 NWs, 106 were created in 2016. The NW network covers around 20,275.5 km. NW-1, 2, & 3 are already operational. Cargo as well as passenger / cruise vessels are plying on these waterways. Detailed Project Report(DPR) for development of NW-4 & 5 was completed in 2010. The DPR of NW 5 was updated in 2014. For the newly declared 106 NWs, techno-economic feasibility studies have been initiated.\n\n"}
{"id": "5872075", "url": "https://en.wikipedia.org/wiki?curid=5872075", "title": "List of Sites of Special Scientific Interest in South Yorkshire", "text": "List of Sites of Special Scientific Interest in South Yorkshire\n\nSouth Yorkshire's geography can be split into different types. The very west of South Yorkshire is part of Dark Peak which is part of the Peak District National Park and lies to the west of Sheffield. This extensive moorland is one of the largest semi-natural areas in England and has broad plateaus with rocky outcrops interspersed with valleys. Moving east the land elevation drops with a transition from the peak district to coal fields. Much of this area of transition has seen urban development with Sheffield being a good example.\n\nThe central region, to the north of Sheffield is largely dependent on the presence of coal measures in the areas geology. This is reflected by ancient woodlands, valley wetlands and large arable fields where there is no urban development. To the east of the coal measures is a strip of Magnesian Limestone which runs north to south between Sheffield and Doncaster. This open landscape is characterised by ancient woodlands and limestone grasslands and often has historic limestone monuments. However the light and dry soils are ideal for cultivation which means little of the original habitat remains. In the very east of South Yorkshire the Humberhead Levels dominate, with the area being relatively flat and dominated by the areas river systems. The area is predominately covered with small fields or areas of peatland.\n\n"}
{"id": "10834670", "url": "https://en.wikipedia.org/wiki?curid=10834670", "title": "List of Superfund sites in Arizona", "text": "List of Superfund sites in Arizona\n\nThis is a list of Superfund sites in Arizona designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of May 1, 2010, there were nine Superfund sites on the National Priorities List in Arizona. Three other sites have been cleaned up and removed from it; no sites are currently proposed for addition to the NPL.\n\n\n"}
{"id": "10085128", "url": "https://en.wikipedia.org/wiki?curid=10085128", "title": "List of animal classes", "text": "List of animal classes\n\nThe following is a list of the classes in each phylum of the kingdom Animalia. There are 107 classes of animals in 33 phyla in this list. However, different sources give different numbers of classes and phyla. For e.g, Protura, Diplura, and Collembola are often considered to be the three orders in the class Entognatha. This list should by no means be considered complete and authoritative and should be used carefully.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "41567502", "url": "https://en.wikipedia.org/wiki?curid=41567502", "title": "List of asteroid close approaches to Earth in 2014", "text": "List of asteroid close approaches to Earth in 2014\n\nBelow is the list of asteroid close approaches to Earth in 2014.\n\nA list of known near-Earth asteroid close approaches less than 1 lunar distance (384,400 km or 0.00256 AU) from Earth in 2014.\n\nFor reference, the radius of Earth is approximately or 0.0166 Lunar distances.<br>The orbit of geosynchronous satellites, however, is or 0.110 Lunar distances. This year, 3 asteroids were detected which traveled nearer than this, most notably 2014 AA, which impacted. This list does not include any of the other 32 objects that collided with earth in 2014, none of which were discovered in advance, but were recorded by sensors designed to detect detonation of nuclear devices (of the 33 objects detected 5 had an impact energy greater than that of a 1 kiloton device) .\n\nThe number of asteroids listed here are significantly less than those of asteroids approaching Earth for several reasons:\n\nThese factors combined severely limit the amount of Moon-approaching asteroids, to a level many times lower than the detected asteroids to pass just as close to Earth instead.\n\nAn example list of near-Earth asteroids that passed more than 1 lunar distance (384,400 km or 0.00256 AU) from Earth in 2014.\n\n"}
{"id": "28344340", "url": "https://en.wikipedia.org/wiki?curid=28344340", "title": "List of countries without rivers", "text": "List of countries without rivers\n\nThis is a list of countries that do not have any rivers.\n\n\n\n\n"}
{"id": "9179093", "url": "https://en.wikipedia.org/wiki?curid=9179093", "title": "List of kampo herbs", "text": "List of kampo herbs\n\nKampō (or \"Kanpō\", 漢方) medicine is the Japanese study and adaptation of traditional Chinese medicine. In 1967, the Japanese Ministry of Health, Labour and Welfare approved four \"kampo\" medicines for reimbursement under the National Health Insurance (NHI) program. In 1976, 82 kampo medicines were approved by the Ministry of Health, Labour and Welfare. Currently, 148 \"kampo\" medicines are approved for reimbursement. \n\nThe 14th edition of the \"Japanese Pharmacopoeia\" (JP) (日本薬局方 Nihon yakkyokuhō) lists 165 herbal ingredients that are approved to be used in \"kampo\" remedies. The following are the most common herbs used in it:\n\nTsumura (ツムラ) is the leading maker; they make 128 of the 148 \"kampo]] medicines. The \"count\" column shows in how many of these 128 formulae the herb is found. The most common herb is Glycyrrhizae Radix (Chinese liquorice root). It is in 94 of the 128 Tsumura formulae. Other common herbs are Zingiberis Rhizoma (ginger) (51 of 128 formulae) and Paeoniae Radix (Chinese peony root) (44 of 128 formulae).\n\n\n\n\n"}
{"id": "48173", "url": "https://en.wikipedia.org/wiki?curid=48173", "title": "List of national parks of the Netherlands", "text": "List of national parks of the Netherlands\n\nNational parks in the Netherlands were defined in the 1960s as areas of at least 10 km² consisting of natural terrains, water and/or forests, with a special landscape and flora and fauna.\n\nThe first two national parks were founded in the 1930s by private organisations. The first official national park, Schiermonnikoog National Park, was not established until 1989. The most recent national park to have been established is the Nieuw Land National Park, which is established in 2018.\n\nIn 2011, the government decided to make the provinces responsible for the national parks. \n\n"}
{"id": "13016885", "url": "https://en.wikipedia.org/wiki?curid=13016885", "title": "List of nuclear power stations", "text": "List of nuclear power stations\n\nThe following page lists all nuclear power stations that are larger than in current net capacity. Those power stations that are smaller than , and those that are only at a planning or proposal stage, may be found in regional lists at the end of the page or in the list of nuclear reactors. The list is based on figures from PRIS (Power Reactor Information System) maintained by International Atomic Energy Agency.\n\nThis table lists all currently operational power stations with current net capacity over . Some of these may have reactors under construction, but only current net capacity is listed. Capacity of permanently shut-down reactors is not included, but capacity of long-term shut-down reactors is included. Power stations with past net capacity over and current net capacity under are listed in third table.\n\nThis table lists stations under construction or operational stations with under-construction reactors and current net capacity under . Planned connection column indicate connection of first reactor, not thus whole capacity.\n\n! Power station !! # units !! Net capacity under construction (MW) !! Construction start !! Planned connection !! Country !! Location\n<!-- on-hold |-\n"}
{"id": "38973180", "url": "https://en.wikipedia.org/wiki?curid=38973180", "title": "List of parson-naturalists", "text": "List of parson-naturalists\n\nParson-naturalists were ministers of religion who also studied natural history. The archetypical parson-naturalist was a priest in the Church of England in charge of a country parish, who saw the study of science as an extension of his religious work. The philosophy entailed the belief that God, as the Creator of all things, wanted man to understand his Creations and thus to study them through scientific techniques. They often collected and preserved natural artefacts such as leaves, flowers, birds' eggs, birds, insects, and small mammals to classify and study. Some wrote books or kept nature diaries.\n\n"}
{"id": "36764873", "url": "https://en.wikipedia.org/wiki?curid=36764873", "title": "List of rivers of Eswatini", "text": "List of rivers of Eswatini\n\nThis is a list of rivers in Swaziland. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n"}
{"id": "19931209", "url": "https://en.wikipedia.org/wiki?curid=19931209", "title": "List of rivers of Malaysia", "text": "List of rivers of Malaysia\n\nThis is an incomplete list of rivers that are at least partially in Malaysia. The rivers are grouped by strait or sea. The rivers flowing into the sea are sorted along the coast. Rivers flowing into other rivers are listed by the rivers they flow into. The rivers that have their mouths in Malaysia are given in italics. The same river may be found in more than one state as many rivers cross state borders.\n\n\n"}
{"id": "8591841", "url": "https://en.wikipedia.org/wiki?curid=8591841", "title": "List of stars in Scorpius", "text": "List of stars in Scorpius\n\nThis is the list of notable stars in the constellation Scorpius, sorted by decreasing brightness.\n\n\n"}
{"id": "2578091", "url": "https://en.wikipedia.org/wiki?curid=2578091", "title": "List of the vascular plants of Britain and Ireland 5", "text": "List of the vascular plants of Britain and Ireland 5\n\nList of the vascular plants of Britain and Ireland #5 — this page's list covers the dicotyledon family Rosaceae.\n\nStatus key: \"*\" indicates an introduced species and \"e\" indicates an extinct species.\n\nGo to: \n"}
{"id": "42468723", "url": "https://en.wikipedia.org/wiki?curid=42468723", "title": "List of tornadoes by calendar day", "text": "List of tornadoes by calendar day\n\nThe following is a list notable tornado events (including tornado outbreaks and tornado outbreak sequences) occurring throughout the year.\n\nIn the United States tornadoes have been recorded on every day of the year and destructive tornadoes occurred during every month of the calendar. The average date of first occurrence in the U.S. is January 11. The earliest recorded tornado in a year (since 1950) was at 12:02 am on January 1, 2011 in Attala County, Mississippi (2 minutes into the year) and the latest occurrence of the first tornado was on February 15, 2003 in Marengo County, Alabama (45 days into the year).\n\n North America\n\n\n\n"}
{"id": "14625493", "url": "https://en.wikipedia.org/wiki?curid=14625493", "title": "Lists of World War I topics", "text": "Lists of World War I topics\n\nThis is a list of World War I-related lists:\n\n"}
{"id": "40426826", "url": "https://en.wikipedia.org/wiki?curid=40426826", "title": "María Elena Solar Power Plant", "text": "María Elena Solar Power Plant\n\nMaría Elena Solar Power Plant (Spanish: Planta Termosolar María Elena) is a concentrated solar power plant with a molten-salt technology system that is currently under construction in the commune of María Elena in the Antofagasta Region of Chile.\n\nThe María Elena complex will be made up of four separate 100 MW plants, for a total capacity of 400 MW, and will supply the northern SING (Sistema Interconectado del Norte Gande) grid via a 17.5 km, 220 kV double circuit transmission line connected to the Encuentro substation. \nThe $3.6 billion thermo solar plant will be the largest plant of its kind in the world.\nThe project is being developed by Ibereólica Solar Atacama, a subsidiary of Madrid-based Grupo Ibereólica. \nIt will employ 1,500 workers during the construction stage and 200 as permanent staff when in operation. \nConstruction will take 27 months so the plant will be operational in July 2016 at the earliest.\n\n\n"}
{"id": "4361793", "url": "https://en.wikipedia.org/wiki?curid=4361793", "title": "Millennium Live", "text": "Millennium Live\n\nMillennium Live is a cancelled international television special, which was an unsuccessful attempt to broadcast an international celebration of the beginning of the Year 2000, or the so-called Millennium. Reports claimed that the show was to have involved broadcasters in up to 130 nations. \"Millennium Live: Humanity's Broadcast\" was going to compete against the \"2000 Today\" international broadcast that was supported by ABC in the United States, and led by BBC in the UK.\n\nThe programme was called off on 28 December 1999 when its organizers, the Millennium Television Network (MTN), announced that it had failed to obtain sufficient financing for the broadcast. MTN reportedly was not paying production and satellite companies for their services prior to the cancellation before MTN shelved their efforts to establish a global broadcast for the following New Year's Eve.\n\nThe Millennium Television Network (MTN) was formed by Live Aid's American producer Hal Uplinger to prepare and conduct the broadcast. A series of Early planning activities among international representatives reportedly occurred in Cannes in October 1998. \"Millennium Live\" was planned as a 24- or 25-hour broadcast from 11:00 UTC 31 December 1999.\n\nPax TV (now known as ION Television) of the United States had the exclusive rights to broadcast the show which they were billed as \"Pax Millennium Live: A New World's Eve\".\n\n\"Millennium Live\" was competing against the 2000 Today international broadcast that was supported by PBS and ABC in the United States, and led by BBC in the UK.\n\nScheduled musical guests included Aerosmith, Bee Gees, Blondie, Chicago, Phil Collins, Destiny's Child, Ricky Martin, 'N Sync, The Pretenders, Sting, Santana and 10,000 Maniacs. Bryan Adams, Simply Red and the Spice Girls were also sought as featured artists.\n\nNew Year's Eve celebrations from various worldwide locations were to have been seen on the show. Angelica Castro (Chile), Carmen Electra (US), Ramzi Malouki (France), and Zam Nkosi (South Africa) were also scheduled as a program hosts for the represented nations. The studios of a television special were to have been hosted in Los Angeles on a set contained in a 90-foot geodesic dome at Manhattan Beach in the United States.\n\nThe program was cancelled on 28 December 1999 with an announcement that MTN had failed to obtain sufficient financing for the broadcast. MTN reportedly was not paying production and satellite companies for their services prior to the cancellation. Pax aired a series of movies in \"Millennium Live's\" place, and MTN never materialised it's reported plans to establish a global broadcast for the following New Year's Eve on the real Millennium (2001).\n\nThe following broadcasters were reported as participants in \"Millennium Live\":\nNote: RAI was the only broadcaster that moved to the \"2000 Today\" broadcast\n\n\n"}
{"id": "23579", "url": "https://en.wikipedia.org/wiki?curid=23579", "title": "Photoelectric effect", "text": "Photoelectric effect\n\nThe photoelectric effect is the emission of electrons or other free carriers when light shines on a material. Electrons emitted in this manner can be called \"photo electrons\". This phenomenon is commonly studied in electronic physics, as well as in fields of chemistry, such as quantum chemistry or electrochemistry.\n\nAccording to classical electromagnetic theory, this effect can be attributed to the transfer of energy from the light to an electron. From this perspective, an alteration in the intensity of light would induce changes in the kinetic energy of the electrons emitted from the metal. Furthermore, according to this theory, a sufficiently dim light would be expected to show a time lag between the initial shining of its light and the subsequent emission of an electron. However, the experimental results did not correlate with either of the two predictions made by classical theory.\n\nInstead, electrons are dislodged only by the impingement of photons when those photons reach or exceed a threshold frequency (energy). Below that threshold, no electrons are emitted from the material regardless of the light intensity or the length of time of exposure to the light. (Rarely, an electron will escape by absorbing two or more quanta. However, this is extremely rare because by the time it absorbs enough quanta to escape, the electron will probably have emitted the rest of the quanta.) To make sense of the fact that light can eject electrons even if its intensity is low, Albert Einstein proposed that a beam of light is not a wave propagating through space, but rather a collection of discrete wave packets (photons), each with energy \"hν\". This shed light on Max Planck's previous discovery of the Planck relation () linking energy (\"E\") and frequency (\"ν\") as arising from quantization of energy. The factor \"h\" is known as the Planck constant.\n\nIn 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1900, while studying black-body radiation, the German physicist Max Planck suggested that the energy carried by electromagnetic waves could only be released in \"packets\" of energy. In 1905, Albert Einstein published a paper advancing the hypothesis that light energy is carried in discrete quantized packets to explain experimental data from the photoelectric effect. This model contributed to the development of quantum mechanics. In 1914, Millikan's Experiment supported Einstein's model of the photoelectric effect. Einstein was awarded the Nobel Prize in 1921 for \"his discovery of the law of the photoelectric effect\", and Robert Millikan was awarded the Nobel Prize in 1923 for \"his work on the elementary charge of electricity and on the photoelectric effect\".\n\nThe photoelectric effect requires photons with energies approaching zero (in the case of negative electron affinity) to over 1 MeV for core electrons in elements with a high atomic number. Emission of conduction electrons from typical metals usually requires a few electron-volts, corresponding to short-wavelength visible or ultraviolet light. Study of the photoelectric effect led to important steps in understanding the quantum nature of light and electrons and influenced the formation of the concept of wave–particle duality. Other phenomena where light affects the movement of electric charges include the photoconductive effect (also known as photoconductivity or photoresistivity), the photovoltaic effect, and the photoelectrochemical effect.\n\nPhotoemission can occur from any material, but it is most easily observable from metals or other conductors because the process produces a charge imbalance, and if this charge imbalance is not neutralized by current flow (enabled by conductivity), the potential barrier to emission increases until the emission current ceases. It is also usual to have the emitting surface in a vacuum, since gases impede the flow of photoelectrons and make them difficult to observe. Additionally, the energy barrier to photoemission is usually increased by thin oxide layers on metal surfaces if the metal has been exposed to oxygen, so most practical experiments and devices based on the photoelectric effect use clean metal surfaces in a vacuum.\n\nWhen the photoelectron is emitted into a solid rather than into a vacuum, the term \"internal photoemission\" is often used, and emission into a vacuum distinguished as \"external photoemission\".\nThe photons of a light beam have a characteristic energy proportional to the frequency of the light. In the photoemission process, if an electron within some material absorbs the energy of one photon and acquires more energy than the work function (the electron binding energy) of the material, it is ejected. If the photon energy is too low, the electron is unable to escape the material. Since an increase in the intensity of low-frequency light will only increase the number of low-energy photons sent over a given interval of time, this change in intensity will not create any single photon with enough energy to dislodge an electron. Thus, the energy of the emitted electrons does not depend on the intensity of the incoming light, but only on the energy (equivalent frequency) of the individual photons. It is an interaction between the incident photon and the outermost electrons.\n\nElectrons can absorb energy from photons when irradiated, but they usually follow an \"all or nothing\" principle. All of the energy from one photon must be absorbed and used to liberate one electron from atomic binding, or else the energy is re-emitted. If the photon energy is absorbed, some of the energy liberates the electron from the atom, and the rest contributes to the electron's kinetic energy as a free particle.\n\nThe theory of the photoelectric effect must explain the experimental observations of the emission of electrons from an illuminated metal surface.\n\nFor a given metal surface, there exists a certain minimum frequency of incident radiation below which no photoelectrons are emitted. This frequency is called the \"threshold frequency\". Increasing the frequency of the incident beam, keeping the number of incident photons fixed (this would result in a proportionate increase in energy) increases the maximum kinetic energy of the photoelectrons emitted. Thus the stopping voltage increases. The number of electrons also changes because of the probability that each photon results in an emitted electron are a function of photon energy. If the intensity of the incident radiation of a given frequency is increased, there is no effect on the kinetic energy of each photoelectron.\n\nAbove the threshold frequency, the maximum kinetic energy of the emitted photoelectron depends on the frequency of the incident light, but is independent of the intensity of the incident light so long as the latter is not too high.\n\nFor a given metal and frequency of incident radiation, the rate at which photoelectrons are ejected is directly proportional to the intensity of the incident light. An increase in the intensity of the incident beam (keeping the frequency fixed) increases the magnitude of the photoelectric current, although the stopping voltage remains the same.\n\nThe time lag between the incidence of radiation and the emission of a photoelectron is very small, less than 10 second.\n\nThe direction of distribution of emitted electrons peaks in the direction of polarization (the direction of the electric field) of the incident light, if it is linearly polarized.\n\nIn 1905, Einstein proposed an explanation of the photoelectric effect using a concept first put forward by Max Planck that light waves consist of tiny bundles or packets of energy known as photons or quanta. \nThe maximum kinetic energy formula_1 of an ejected electron is given by\n\nwhere formula_3 is the Planck constant and formula_4 is the frequency of the incident photon. The term formula_5 is the work function (sometimes denoted formula_6, or formula_7), which gives the minimum energy required to remove a delocalised electron from the surface of the metal. The work function satisfies\n\nwhere formula_9 is the threshold frequency for the metal. The maximum kinetic energy of an ejected electron is then\n\nKinetic energy is positive, so we must have formula_11 for the photoelectric effect to occur.\n\nThe relation between current and applied voltage illustrates the nature of the photoelectric effect. For discussion, a light source illuminates a plate P, and another plate electrode Q collects any emitted electrons. We vary the potential between P and Q and measure the current flowing in the external circuit between the two plates.\n\nIf the frequency and the intensity of the incident radiation are fixed, the photoelectric current increases gradually with an increase in the positive potential on the collector electrode until all the photoelectrons emitted are collected. The photoelectric current attains a saturation value and does not increase further for any increase in the positive potential. The saturation current increases with the increase of the light intensity. It also increases with greater frequencies due to a greater probability of electron emission when collisions happen with higher energy photons.\n\nIf we apply a negative potential to the collector plate Q with respect to the plate P and gradually increase it, the photoelectric current decreases, becoming zero at a certain negative potential. The negative potential on the collector at which the photoelectric current becomes zero is called the \"stopping potential\" or \"cut off\" potential\n\ni. For a given frequency of incident radiation, the stopping potential is independent of its intensity.\n\nii. For a given frequency of incident radiation, the stopping potential is determined by the maximum kinetic energy formula_1 of the photoelectrons that are emitted. If \"q\" is the charge on the electron and formula_13 is the stopping potential, then the work done by the retarding potential in stopping the electron is formula_14, so we have\n\nRecalling\n\nwe see that the stopping voltage varies linearly with frequency of light, but depends on the type of material. For any particular material, there is a threshold frequency that must be exceeded, independent of light intensity, to observe any electron emission.\n\nIn the X-ray regime, the photoelectric effect in crystalline material is often decomposed into three steps:\n\nIn the three-step model, an electron can take multiple paths through these three steps. All paths can interfere in the sense of the path integral formulation.\nFor surface states and molecules the three-step model does still make some sense as even most atoms have multiple electrons which can scatter the one electron leaving.\n\nWhen a surface is exposed to electromagnetic radiation above a certain threshold frequency (typically visible light for alkali metals, near ultraviolet for other metals, and extreme ultraviolet for non-metals), the radiation is absorbed and electrons are emitted.\nLight, and especially ultra-violet light, discharges negatively electrified bodies with the production of rays of the same nature as cathode rays. Under certain circumstances it can directly ionize gases. The first of these phenomena was discovered by Hertz and Hallwachs in 1887. The second was announced first by Philipp Lenard in 1900.\n\nThe ultra-violet light to produce these effects may be obtained from an arc lamp, or by burning magnesium, or by sparking with an induction coil between zinc or cadmium terminals, the light from which is very rich in ultra-violet rays. Sunlight is not rich in ultra-violet rays, as these have been absorbed by the atmosphere, and it does not produce nearly so large an effect as the arc-light. Many substances besides metals discharge negative electricity under the action of ultraviolet light: lists of these substances will be found in papers by G. C. Schmidt and O. Knoblauch.\n\nIn 1839, Alexandre Edmond Becquerel discovered the photovoltaic effect while studying the effect of light on electrolytic cells. Though not equivalent to the photoelectric effect, his work on photovoltaics was instrumental in showing a strong relationship between light and electronic properties of materials. In 1873, Willoughby Smith discovered photoconductivity in selenium while testing the metal for its high resistance properties in conjunction with his work involving submarine telegraph cables.\n\nJohann Elster (1854–1920) and Hans Geitel (1855–1923), students in Heidelberg, developed the first practical photoelectric cells that could be used to measure the intensity of light. Elster and Geitel had investigated with great success the effects produced by light on electrified bodies.\n\nIn 1887, Heinrich Hertz observed the photoelectric effect and the production and reception of electromagnetic waves. He published these observations in the journal Annalen der Physik. His receiver consisted of a coil with a spark gap, where a spark would be seen upon detection of electromagnetic waves. He placed the apparatus in a darkened box to see the spark better. However, he noticed that the maximum spark length was reduced when in the box. A glass panel placed between the source of electromagnetic waves and the receiver absorbed ultraviolet radiation that assisted the electrons in jumping across the gap. When removed, the spark length would increase. He observed no decrease in spark length when he replaced the glass with quartz, as quartz does not absorb UV radiation. Hertz concluded his months of investigation and reported the results obtained. He did not further pursue the investigation of this effect.\n\nThe discovery by Hertz in 1887 that the incidence of ultra-violet light on a spark gap facilitated the passage of the spark, led immediately to a series of investigations by Hallwachs, Hoor, Righi and Stoletow on the effect of light, and especially of ultra-violet light, on charged bodies. It was proved by these investigations that a newly cleaned surface of zinc, if charged with negative electricity, rapidly loses this charge however small it may be when ultra-violet light falls upon the surface; while if the surface is uncharged to begin with, it acquires a positive charge when exposed to the light, the negative electrification going out into the gas by which the metal is surrounded; this positive electrification can be much increased by directing a strong airblast against the surface. If however the zinc surface is positively electrified it suffers no loss of charge when exposed to the light: this result has been questioned, but a very careful examination of the phenomenon by Elster and Geitel has shown that the loss observed under certain circumstances is due to the discharge by the light reflected from the zinc surface of negative electrification on neighbouring conductors induced by the positive charge, the negative electricity under the influence of the electric field moving up to the positively electrified surface.\n\nWith regard to the \"Hertz effect\", the researchers from the start showed a great complexity of the phenomenon of photoelectric fatigue — that is, the progressive diminution of the effect observed upon fresh metallic surfaces. According to an important research by Wilhelm Hallwachs, ozone played an important part in the phenomenon. However, other elements enter such as oxidation, the humidity, the mode of polish of the surface, etc. It was at the time not even sure that the fatigue is absent in a vacuum.\n\nIn the period from February 1888 and until 1891, a detailed analysis of photoeffect was performed by Aleksandr Stoletov with results published in 6 works; four of them in \"Comptes Rendus\", one review in \"Physikalische Revue\" (translated from Russian), and the last work in \"Journal de Physique\". First, in these works Stoletov invented a new experimental setup which was more suitable for a quantitative analysis of photoeffect. Using this setup, he discovered the direct proportionality between the intensity of light and the induced photo electric current (the first law of photoeffect or Stoletov's law). One of his other findings resulted from measurements of the dependence of the intensity of the electric photo current on the gas pressure, where he found the existence of an optimal gas pressure P corresponding to a maximum photocurrent; this property was used for a creation of solar cells.\n\nIn 1899, J. J. Thomson investigated ultraviolet light in Crookes tubes. Thomson deduced that the ejected particles were the same as those previously found in the cathode ray, later called electrons, which he called \"corpuscles\". In the research, Thomson enclosed a metal plate (a cathode) in a vacuum tube, and exposed it to high-frequency radiation. It was thought that the oscillating electromagnetic fields caused the atoms' field to resonate and, after reaching a certain amplitude, caused a subatomic \"corpuscle\" to be emitted, and current to be detected. The amount of this current varied with the intensity and color of the radiation. Larger radiation intensity or frequency would produce more current.\n\nThe discovery of the ionization of gases by ultra-violet light was made by Philipp Lenard in 1900. As the effect was produced across several centimeters of air and made very great positive and small negative ions, it was natural to interpret the phenomenon, as did J. J. Thomson, as a \"Hertz effect\" upon the solid or liquid particles present in the gas.\n\nIn 1902, Lenard observed that the energy of individual emitted electrons increased with the frequency (which is related to the color) of the light.\n\nThis appeared to be at odds with Maxwell's wave theory of light, which predicted that the electron energy would be proportional to the intensity of the radiation.\n\nLenard observed the variation in electron energy with light frequency using a powerful electric arc lamp which enabled him to investigate large changes in intensity, and that had sufficient power to enable him to investigate the variation of potential with light frequency. His experiment directly measured potentials, not electron kinetic energy: he found the electron energy by relating it to the maximum stopping potential (voltage) in a phototube. He found that the calculated maximum electron kinetic energy is determined by the frequency of the light. For example, an increase in frequency results in an increase in the maximum kinetic energy calculated for an electron upon liberation – ultraviolet radiation would require a higher applied stopping potential to stop current in a phototube than blue light. However, Lenard's results were qualitative rather than quantitative because of the difficulty in performing the experiments: the experiments needed to be done on freshly cut metal so that the pure metal was observed, but it oxidized in a matter of minutes even in the partial vacuums he used. The current emitted by the surface was determined by the light's intensity, or brightness: doubling the intensity of the light doubled the number of electrons emitted from the surface.\n\nThe researches of Langevin and those of Eugene Bloch have shown that the greater part of the Lenard effect is certainly due to this 'Hertz effect'. The Lenard effect upon the gas itself nevertheless does exist. Refound by J. J. Thomson and then more decisively by Frederic Palmer, Jr., it was studied and showed very different characteristics than those at first attributed to it by Lenard.\n\nIn 1905, Albert Einstein solved this apparent paradox by describing light as composed of discrete quanta, now called photons, rather than continuous waves. Based upon Max Planck's theory of black-body radiation, Einstein theorized that the energy in each quantum of light was equal to the frequency multiplied by a constant, later called Planck's constant. A photon above a threshold frequency has the required energy to eject a single electron, creating the observed effect. This discovery led to the quantum revolution in physics and earned Einstein the Nobel Prize in Physics in 1921. By wave-particle duality the effect can be analyzed purely in terms of waves though not as conveniently.\n\nAlbert Einstein's mathematical description of how the photoelectric effect was caused by absorption of quanta of light was in one of his 1905 papers, named \"On a Heuristic Viewpoint Concerning the Production and Transformation of Light\". This paper proposed the simple description of \"light quanta\", or photons, and showed how they explained such phenomena as the photoelectric effect. His simple explanation in terms of absorption of discrete quanta of light explained the features of the phenomenon and the characteristic frequency.\n\nThe idea of light quanta began with Max Planck's published law of black-body radiation (\"On the Law of Distribution of Energy in the Normal Spectrum\") by assuming that Hertzian oscillators could only exist at energies \"E\" proportional to the frequency \"f\" of the oscillator by \"E\" = \"hf\", where \"h\" is Planck's constant. By assuming that light actually consisted of discrete energy packets, Einstein wrote an equation for the photoelectric effect that agreed with experimental results. It explained why the energy of photoelectrons was dependent only on the \"frequency\" of the incident light and not on its \"intensity\": a low-intensity, the high-frequency source could supply a few high energy photons, whereas a high-intensity, the low-frequency source would supply no photons of sufficient individual energy to dislodge any electrons. This was an enormous theoretical leap, but the concept was strongly resisted at first because it contradicted the wave theory of light that followed naturally from James Clerk Maxwell's equations for electromagnetic behavior, and more generally, the assumption of infinite divisibility of energy in physical systems. Even after experiments showed that Einstein's equations for the photoelectric effect were accurate, resistance to the idea of photons continued since it appeared to contradict Maxwell's equations, which were well understood and verified.\n\nEinstein's work predicted that the energy of individual ejected electrons increases linearly with the frequency of the light. Perhaps surprisingly, the precise relationship had not at that time been tested. By 1905 it was known that the energy of photoelectrons increases with increasing \"frequency\" of incident light and is independent of the \"intensity\" of the light. However, the manner of the increase was not experimentally determined until 1914 when Robert Andrews Millikan showed that Einstein's prediction was correct.\n\nThe photoelectric effect helped to propel the then-emerging concept of wave–particle duality in the nature of light. Light simultaneously possesses the characteristics of both waves and particles, each being manifested according to the circumstances. The effect was impossible to understand in terms of the classical wave description of light, as the energy of the emitted electrons did not depend on the intensity of the incident radiation. Classical theory predicted that the electrons would 'gather up' energy over a period of time, and then be emitted.\n\nThese are extremely light-sensitive vacuum tubes with a photocathode coated onto part (an end or side) of the inside of the envelope. The photocathode contains combinations of materials such as cesium, rubidium, and antimony specially selected to provide a low work function, so when illuminated even by very low levels of light, the photocathode readily releases electrons. By means of a series of electrodes (dynodes) at ever-higher potentials, these electrons are accelerated and substantially increased in number through secondary emission to provide a readily detectable output current. Photomultipliers are still commonly used wherever low levels of light must be detected.\n\nVideo camera tubes in the early days of television used the photoelectric effect, for example, Philo Farnsworth's \"Image dissector\" used a screen charged by the photoelectric effect to transform an optical image into a scanned electronic signal.\n\nGold-leaf electroscopes are designed to detect static electricity. Charge placed on the metal cap spreads to the stem and the gold leaf of the electroscope. Because they then have the same charge, the stem and leaf repel each other. This will cause the leaf to bend away from the stem.\n\nAn electroscope is an important tool in illustrating the photoelectric effect. For example, if the electroscope is negatively charged throughout, there is an excess of electrons and the leaf is separated from the stem. If high-frequency light shines on the cap, the electroscope discharges, and the leaf will fall limp. This is because the frequency of the light shining on the cap is above the cap's threshold frequency. The photons in the light have enough energy to liberate electrons from the cap, reducing its negative charge. This will discharge a negatively charged electroscope and further charge a positive electroscope. However, if the electromagnetic radiation hitting the metal cap does not have a high enough frequency (its frequency is below the threshold value for the cap), then the leaf will never discharge, no matter how long one shines the low-frequency light at the cap.\n\nSince the energy of the photoelectrons emitted is exactly the energy of the incident photon minus the material's work function or binding energy, the work function of a sample can be determined by bombarding it with a monochromatic X-ray source or UV source, and measuring the kinetic energy distribution of the electrons emitted.\n\nPhotoelectron spectroscopy is usually done in a high-vacuum environment, since the electrons would be scattered by gas molecules if they were present. However, some companies are now selling products that allow photoemission in air. The light source can be a laser, a discharge tube, or a synchrotron radiation source.\n\nThe concentric hemispherical analyzer is a typical electron energy analyzer and uses an electric field to change the directions of incident electrons, depending on their kinetic energies. For every element and core (atomic orbital) there will be a different binding energy. The many electrons created from each of these combinations will show up as spikes in the analyzer output, and these can be used to determine the elemental composition of the sample.\n\nThe photoelectric effect will cause spacecraft exposed to sunlight to develop a positive charge. This can be a major problem, as other parts of the spacecraft are in shadow which will result in the spacecraft developing a negative charge from nearby plasmas. The imbalance can discharge through delicate electrical components. The static charge created by the photoelectric effect is self-limiting, because a higher charged object doesn't give up its electrons as easily as a lower charged object does.\n\nLight from the sun hitting lunar dust causes it to become charged with the photoelectric effect. The charged dust then repels itself and lifts off the surface of the Moon by electrostatic levitation. This manifests itself almost like an \"atmosphere of dust\", visible as a thin haze and blurring of distant features, and visible as a dim glow after the sun has set. This was first photographed by the Surveyor program probes in the 1960s. It is thought that the smallest particles are repelled kilometers from the surface and that the particles move in \"fountains\" as they charge and discharge.\n\nPhotons hitting a thin film of alkali metal or semiconductor material such as gallium arsenide in an image intensifier tube cause the ejection of photoelectrons due to the photoelectric effect. These are accelerated by an electrostatic field where they strike a phosphor coated screen, converting the electrons back into photons. Intensification of the signal is achieved either through acceleration of the electrons or by increasing the number of electrons through secondary emissions, such as with a micro-channel plate. Sometimes a combination of both methods is used. Additional kinetic energy is required to move an electron out of the conduction band and into the vacuum level. This is known as the electron affinity of the photocathode and is another barrier to photoemission other than the forbidden band, explained by the band gap model. Some materials such as Gallium Arsenide have an effective electron affinity that is below the level of the conduction band. In these materials, electrons that move to the conduction band are all of the sufficient energy to be emitted from the material and as such, the film that absorbs photons can be quite thick. These materials are known as negative electron affinity materials.\n\nThe photoelectric effect is one interaction mechanism between photons and atoms. It is one of 12 theoretically possible interactions.\n\nAt the high photon energies comparable to the electron rest energy of , Compton scattering, another process, may take place. Above twice this () pair production may take place. Compton scattering and pair production are examples of two other competing mechanisms.\n\nIndeed, even if the photoelectric effect is the favoured reaction for a particular single-photon bound-electron interaction, the result is also subject to statistical processes and is not guaranteed, albeit the photon has certainly disappeared and a bound electron has been excited (usually K or L shell electrons at gamma ray energies). The probability of the photoelectric effect occurring is measured by the cross-section of interaction, σ. This has been found to be a function of the atomic number of the target atom and photon energy. A crude approximation, for photon energies above the highest atomic binding energy, is given by:\n\nHere \"Z\" is atomic number and \"n\" is a number which varies between 4 and 5. (At lower photon energies a characteristic structure with edges appears, K edge, L edges, M edges, etc.) The obvious interpretation follows that the photoelectric effect rapidly decreases in significance, in the gamma-ray region of the spectrum, with increasing photon energy, and that photoelectric effect increases steeply with atomic number. The corollary is that high-\"Z\" materials make good gamma-ray shields, which is the principal reason that lead (\"Z\" = 82) is a preferred and ubiquitous gamma radiation shield.\n\n\n\n\"Applets\"\n"}
{"id": "37882012", "url": "https://en.wikipedia.org/wiki?curid=37882012", "title": "Polar forests of the Cretaceous", "text": "Polar forests of the Cretaceous\n\nCretaceous polar forests were temperate forests that grew at polar latitudes during the final period of the Mesozoic Era, known as the Cretaceous Period 145–66 Ma. During this period, global average temperature was about higher and carbon dioxide (CO) levels were approximately 1000 parts per million (ppm), 2.5 times the current concentration in Earth's atmosphere. The abundance of atmospheric carbon dioxide had a very significant impact on global climate and Earth's natural systems as its concentration is considered one of the main factors in the development of a pronounced greenhouse Earth during the Cretaceous with a very low average global temperature gradient. As a consequence, high paleolatitudes in both hemispheres were much warmer than at present. This temperature gradient was partly responsible for the lack of continental ice sheets in polar regions.\n\nAs a response to elevated global temperatures, the Earth's hydrologic cycle was significantly enhanced due to greater volume of moisture evaporation from the surface of the ocean. In turn, the absolute sea level during this time period stood at elevations much higher than the present level. Continental encroachment of seawater formed widespread shallow seas, including expanses of epeiric seas.\n\nAn increase in surface area between shallow, warm epeiric seawater and the atmosphere permits higher evaporation rates and more precipitation at various latitudes, producing a more temperate global climate. A widespread temperate climate also had significant effects on high latitude ecosystems.\n\nDuring the Cretaceous, temperate forests thrived at polar latitudes, as there was a notable difference from current conditions at high latitudes during the Cretaceous polar seasons. The duration of summer sunlight and winter darkness lasted for approximately 5 months each. This variation in light is thought to have played a critical role in the composition and evolution of polar forests. Fossilized flora evidence suggests the presence of paleoforests up to latitudes of 85° in both Northern and Southern hemispheres. The dominant forms of vegetation at these high latitudes during the previous 100 million years were rapidly evolving and ultimately being replaced during a time known as the Cretaceous Terrestrial Revolution. During the Cretaceous Terrestrial Revolution, conifers, cycads and ferns were selectively replaced by angiosperms and gymnosperms, becoming the main species dominating the high paleolatitudes. In this Cretaceous greenhouse world, Arctic conifer forests were considered predominantly deciduous, while those that grew on Antarctica contained a significantly greater proportion of evergreens.\n\nIn the early Cretaceous, approximately 130 million years ago, there was a major diversification of angiosperms that set in motion a large evolutionary change in high paleolatitude forest composition. The diversification of angiosperms is in close connection with pollen and nectar collecting insects. It is thought that the diversification of these insects would have a substantial impact on the rate of angiosperm speciation. Whatever the mechanism for the diversification, the early Cretaceous angiosperm \"takeover\" denotes an important transition of the ecosystem. By the end of the Cretaceous, the composition of polar forest regions had diversified by approximately 50-80%. This transition from conifers, cycads and ferns to predominantly angiosperms reflects an interesting evolutionary adaptation to the regional polar climate and quite possibly numerous other factors like sea-floor spreading rates, eustatic sea level and high global temperatures.\n\nPoleward displacement of the temperate zone during the Cretaceous significantly elevated terrestrial forest primary productivity. At high to mid paleolatitudes, forest productivity was estimated to be twice as much relative to lower paleolatitudes. Terrestrial productivity in the high paleolatitudes is strongly linked to elevated atmospheric carbon dioxide concentrations. Results from the experiments on deciduous and evergreen tree growth under various carbon dioxide concentrations show differing impacts.\n\nThere are four main factors that contribute to net forest productivity: carbon dioxide concentration, root respiration rates, temperature and photosynthesis. Carbon dioxide alone tends to decrease leaf and root respiration by lowering the light compensation point of photosynthesis, allowing for a net positive gain in carbon intake during the course of a day. The reduction of root respiration tends to initiate root growth and ultimately results in an improvement in nutrient and water uptake efficiency. When photosynthesis is added to the effects of carbon dioxide, depending on regional temperature, forest productivity is drastically increased. The combination of all four physiological factors results in a significant net increase in forest productivity. According to experimental results, tree species with long lived evergreen foliage tend to benefit the greatest in a carbon dioxide rich environment because of their longer growing season and adaptations like canopy development that allow them to thrive in the temperate polar paleolatitudes of the Cretaceous.\n\nThe composition and structure of high latitude Cretaceous forests was composed primarily of deciduous conifers, ferns, angiosperms and gymnosperms. The most abundant and globally widespread plant taxa were the araucarioid and podocarpoid conifers, extending approximately 80° into both hemispheres and composing more than 90% of the canopy generating evergreen vegetation. Other types of conifers, although abundant in occurrence, were restricted to mid and low latitudes in both hemispheres, confined mainly by regional climates. As global climate evolved, the rise of angiosperms began to put pressure on conifers at higher latitudes by growing taller and ultimately winning the battle for sunlight. The rapid evolution of diverse angiosperm species 25 million years later eventually became the dominant tree type by the mid-Cretaceous. By the Late Cretaceous, a temperate climate in both the Northern and Southern hemisphere was ideal for the rapid diversification and distribution of various angiosperms and to a lesser extent, conifers. Studies on the mid-Cretaceous paleorecord conclude that forest compositions in Northern hemisphere high paleolatitudes were mainly populated by mixed evergreen and deciduous tree types. In contrast, the Southern hemisphere was composed primarily of evergreens.\n\nA paleoclimate indicator, also known as a paleoclimate proxy, can reveal important information about what global climate may have been like in the past. Paleoclimate studies on tree growth rings, deep sea cores, ice cores and paleosols are just a few of the many common proxies used to evaluate the major forcings on paleoclimates.\n\nOne of the most important and valuable tools for paleothermometry reconstruction is the analysis of isotope-ratio mass spectrometry data on stable isotopes like those of hydrogen and oxygen. Studies on marine (planktonic/benthic) foraminifera and bulk carbonate isotope ratios during the mid-Cretaceous suggest a continual warming period from ~100 Ma to 66 Ma. During this period, the southern high latitudes were as cool as and as warm as . Paleotemperatures of Cretaceous northern high latitudes were deduced from oxygen isotope analysis of well-preserved brachiopod and molluscan shells. Results from studies show temperature fluctuations that correspond to seasonal variation ranging from .\n\nGrowth ring measurements during the Cretaceous can also provide details of what the climate might have been like in various geographic locations on Earth. Pattern analysis of tree rings or growth rings from Cretaceous fossil woods are mainly used to make inferences into paleoclimate and forest productivity. One very useful scientific method used for tree growth ring dating is dendrochronology. However, most of the studies conducted on fossilized wood rely on the idea that processes related to tree growth rates that operated in the past are identical to the processes that operate in the present, uniformitarianism. On this basis, forest productivity can be inferred from the analysis of growth rings in Cretaceous trees. Analysis of forest productivity from the Cretaceous shows that annual tree growth rates at low paleolatitudes were significantly elevated relative to the present. In the polar paleolatitudes, growth rate analysis also indicates elevated productivity, but even more significantly improved relative to today. Dendrochronology of fossilized wood growth rings from high paleolatitudes suggests the presence of greenhouse-like climatic conditions on a global scale during this time period.\n\n"}
{"id": "28072640", "url": "https://en.wikipedia.org/wiki?curid=28072640", "title": "Political globalization", "text": "Political globalization\n\nPolitical globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene. The creation and existence of the United Nations has been called one of the classic examples of political globalization.\n\nPolitical globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.\n\nWilliam R. Thompson has defined it as \"the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed\". Valentine M. Moghadam defined it as \"an increasing trend toward multilateralism (in which the United Nations plays a key role), toward an emerging 'transnational state apparatus,' and toward the emergence of national and international nongovernmental organizations that act as watchdogs over governments and have increased their activities and influence\". Manfred B. Steger in turn wrote that it \"refers to the intensification and expansion of political interrelations across the globe\". The longer definition by Colin Crouch goes as follows: \"Political globalization refers to the growing power of institutions of global governance such as the World Bank, the International Monetary Fund (IMF) and the World Trade Organization (WTO). But it also refers to the spread and influence of international non-governmental organizations, social movement organizations and transnational advocacy networks operating across borders and constituting a kind of global civil society.\" Finally, Gerard Delanty and Chris Rumford define it as \"a tension between three processes which interact to produce the complex field of global politics: global geopolitics, global normative culture and polycentric networks.\"\n\nSalvatore Babones discussing sources used by scholars for studying political globalizations noted the usefulness of Europa World Year Book for data on diplomatic relationships between countries, publications of International Institute for Strategic Studies such as \"The Military Balance\" for matters of military, and US government publication \"Patterns of Global Terrorism\" for matters of terrorism.\n\nPolitical globalization is measured by aggregating and weighting data on the number of embassies and high commissioners in a country, the number of the country's membership in international organization, its participation in the UN peacekeeping missions, and the number of international treaties signed by said country. This measure has been used by Axel Dreher, Noel Gaston, Pim Martens Jeffrey Haynes and is available from the KOF institute at ETH Zurich\n\nLike globalization itself, political globalization has several dimensions and lends itself to a number of interpretations. It has been discussed in the context of new emancipatory possibilities, as well as in the context of loss of autonomy and fragmentation of the social world. Political globalization can be seen in changes such as democratization of the world, creation of the global civil society, and moving beyond the centrality of the nation-state, particularly as the sole actor in the field of politics. Some of the questions central to the discussion of the political globalization are related to the future of the nation-state, whether its importance is diminishing and what are the causes for those changes; and understanding the emergence of the concept of global governance. The creation and existence of the United Nations has been called one of the classic examples of political globalization. Political actions by non-governmental organizations and social movements, concerned about various topics such as environmental protection, is another example.\n\nDavid Held has proposed that continuing political globalization may lead to the creation of a world government-like cosmopolitan democracy, though this vision has also been criticized as too idealistic.\n\nThere is a heated debate over Political Globalization and Nation State. The question arises whether or not political globalization signifies the decline of the nation-state. Hyper globalists argue that globalization has engulfed today's world in such a way that state boundaries are beginning to lose significance. However, skeptics disregard this as naiveté, believing that the nation-state remains the supreme actor in international relations. \n\n\n\n"}
{"id": "5307853", "url": "https://en.wikipedia.org/wiki?curid=5307853", "title": "QMR effect", "text": "QMR effect\n\nQuadratic magnetic rotation (also known as QMR or QMR effect) is a type of magneto-optic effect, discovered in the mid 1980s by a team of Ukrainian physicists. QMR, like the Faraday effect, establishes a relationship between the magnetic field and rotation of polarization of the plane of linearly polarized light. In contrast to the Faraday effect, QMR originates from the quadratic proportionality between the angle of the rotation of the plane of polarization and the strength of the magnetic field. Mostly QMR can be observed in the transverse geometry when the vector of the magnetic field strength is perpendicular to the direction of light propagation.\n\nThe first evidence of QMR effect was obtained in the antiferromagnetic crystal of cobalt fluoride in 1985.\n\nConsiderations of the symmetry of the media, light and axial vector of the magnetic field forbid QMR in non-magnetic or magnetically disordered media. Onsager's reciprocal relations generalized for magnetically ordered media eliminate symmetry restrictions for QMR in the media which have lost the center of anti-inversion as an operation of symmetry at an ordering of its magnetic subsystem. Despite the fact that some crystal groups of symmetry are devoid of the center of anti-inversion, they also don’t have QMR because of action of other operators of symmetry. They are eleven groups without the center of anti-inversion 432, 43'm, m3m, 422, 4mm, 4'2m, 4/mmm, 622, 6mm, 6'm2 and 6/mmm. Accordingly, the rest of groups of crystal symmetry where QMR can be observed constitutes 27 antiferromagnetic and 31 pyromagnetic crystal classes.\n\nQMR is described by fourth-order c-tensor which is antisymmetrical as to the first two indices.\n\n"}
{"id": "52565777", "url": "https://en.wikipedia.org/wiki?curid=52565777", "title": "Q Puppis", "text": "Q Puppis\n\nThe Bayer designations q Puppis and Q Puppis are distinct. Due to , both designations link here. For the star\n\n\n"}
{"id": "616293", "url": "https://en.wikipedia.org/wiki?curid=616293", "title": "Soft matter", "text": "Soft matter\n\nSoft matter or soft condensed matter is a subfield of condensed matter comprising a variety of physical systems that are deformed or structurally altered by thermal or mechanical stress of the magnitude of thermal fluctuations. They include liquids, colloids, polymers, foams, gels, granular materials, liquid crystals, and a number of biological materials. These materials share an important common feature in that predominant physical behaviors occur at an energy scale comparable with room temperature thermal energy. At these temperatures, quantum aspects are generally unimportant.\nPierre-Gilles de Gennes, who has been called the \"founding father of soft matter,\" received the Nobel Prize in physics in 1991 for discovering that methods developed for studying order phenomena in simple systems can be generalized to the more complex cases found in soft matter, in particular, to the behaviors of liquid crystals and polymers.\n\nInteresting behaviors arise from soft matter in ways that cannot be predicted, or are difficult to predict, directly from its atomic or molecular constituents. Materials termed soft matter exhibit this property due to a shared propensity of these materials to self-organize into \"mesoscopic\" physical structures. By way of contrast, in hard condensed matter physics it is often possible to predict the overall behavior of a material because the molecules are organized into a crystalline lattice with no changes in the pattern at any mesoscopic scale.\n\nOne defining characteristic of soft matter is the mesoscopic scale\nof physical structures. The structures are much larger than the microscopic scale (the arrangement of atoms and molecules), and yet are much smaller than the macroscopic (overall) scale of the material. The properties and interactions of these mesoscopic structures may determine the macroscopic behavior of the material. For example, the turbulent vortices that naturally occur within a flowing liquid are much smaller than the overall quantity of liquid and yet much larger than its individual molecules, and the emergence of these vortices control the overall flowing behavior of the material. Also, the bubbles that comprise a foam are mesoscopic because they individually consist of a vast number of molecules, and yet the foam itself consists of a great number of these bubbles, and the overall mechanical stiffness of the foam emerges from the combined interactions of the bubbles. \n\nA second common feature of soft matter is the importance of thermal fluctuations. Typical bond energies in soft matter structures are of similar scale as thermal energies. Therefore, the structures are constantly affected by thermal fluctuations, undergoing Brownian motion.\n\nFinally, a third distinctive feature of soft matter system is self-assembly. The characteristic complex behavior and hierarchical structures arise spontaneously as the system evolves towards equilibrium.\n\nSoft materials are important in a wide range of technological applications. They may appear as structural and packaging materials, foams and adhesives, detergents and cosmetics, paints, food additives, lubricants and fuel additives, rubber in tires, etc. In addition, a number of biological materials (blood, muscle, milk, yogurt, jello) are classifiable as soft matter. Liquid crystals, another category of soft matter, exhibit a responsivity to electric fields that make them very important as materials in display devices (LCDs). In spite of the various forms of these materials, many of their properties have common physicochemical origins, such as a large number of internal degrees of freedom, weak interactions between structural elements, and a delicate balance between entropic and enthalpic contributions to the free energy. These properties lead to large thermal fluctuations, a wide variety of forms, sensitivity of equilibrium structures to external conditions, macroscopic softness, and metastable states. Active liquid crystals are another example of soft materials, where the constituent elements in liquid crystals can self propel. Soft matters, such as polymers and lipids have found applications in nanotechnology as well.\n\nThe realization that soft matter contains innumerable examples of symmetry breaking, generalized elasticity, and many fluctuating degrees of freedom has re-invigorated classical fields of physics such as fluids (now generalized to non-Newtonian and structured media) and elasticity (membranes, filaments, and anisotropic networks are all important and have common aspects).\n\nAn important part of soft condensed matter research is biophysics. Soft condensed matter biophysics may be diverging into two distinct directions: a physical chemistry approach and a complex systems approach.\n\n\n\n"}
{"id": "161657", "url": "https://en.wikipedia.org/wiki?curid=161657", "title": "Soft power", "text": "Soft power\n\nSoft power is the ability to attract and co-opt, rather than coerce (hard power). Soft power is the ability to shape the preferences of others through appeal and attraction. A defining feature of soft power is that it is non-coercive; the currency of soft power is culture, political values, and foreign policies. Recently, the term has also been used in changing and influencing social and public opinion through relatively less transparent channels and lobbying through powerful political and non-political organizations. In 2012, Joseph Nye of Harvard University explained that with soft power, \"the best propaganda is not propaganda\", further explaining that during the Information Age, \"credibility is the scarcest resource.\"\n\nNye coined the term in a 1990 book, \"Bound to Lead: The Changing Nature of American Power\". In this book, he wrote: “when one country gets other countries to want what it wants-might be called co-optive or soft power in contrast with the hard or command power of ordering others to do what it wants.” He further developed the concept in his 2004 book, \"Soft Power: The Means to Success in World Politics\". The term is now widely used in international affairs by analysts and statesmen. For example, US Secretary of Defense Robert Gates spoke of the need to enhance American soft power by \"a dramatic increase in spending on the civilian instruments of national security – diplomacy, strategic communications, foreign assistance, civic action and economic reconstruction and development.\" In 2011, Xi Jinping was preparing to take power from General Secretary Hu Jintao, the 17th Central Committee of the Chinese Communist Party devoted a whole plenary session to the issue of culture, with the final Communiqué declaring that it was a national goal to \"build our country into a socialist cultural superpower.\" And in 2014, Xi announced, \"We should increase China's soft power, give a good Chinese narrative, and better communicate China's messages to the world.\"\n\nAccording to the Soft Power 30, an annual index published by Portland Communications and the USC Center on Public Diplomacy for 2018, the United Kingdom is the leading sovereign state in soft power. Other leading countries in soft power include France, Germany, the United States, Canada, Japan, Switzerland, Australia, Sweden, and the Netherlands. According to the 2016/17 \"Monocle\" Soft Power Survey, ranks the United States as the leading country in soft power. The Elcano Global Presence Report scores the European Union highest for soft presence when considered as a whole, and ranks the United States first among sovereign states.\n\nJoseph Nye introduced the concept of \"soft power\" in the late 1980s. For Nye, power is the ability to influence the behavior of others to get the outcomes you want. There are several ways one can achieve this: you can coerce them with threats; you can induce them with payments; or you can attract and co-opt them to want what you want. This soft power – getting others to want the outcomes you want – co-opts people rather than coerces them.\n\nIt can be contrasted with 'hard power', which is the use of coercion and payment. Soft power can be wielded not just by states but also by all actors in international politics, such as NGOs or international institutions. It is also considered the \"second face of power\" that indirectly allows you to obtain the outcomes you want. A country's soft power, according to Nye, rests on three resources: \"its culture (in places where it is attractive to others), its political values (when it lives up to them at home and abroad), and its foreign policies (when others see them as legitimate and having moral authority).\"\n\nSoft power resources are the assets that produce attraction which often leads to acquiescence. Nye asserts that, \"Seduction is always more effective than coercion, and many values like democracy, human rights, and individual opportunities are deeply seductive.\" Angelo Codevilla observed that an often overlooked essential aspect of soft power is that different parts of populations are attracted or repelled by different things, ideas, images, or prospects. Soft power is hampered when policies, culture, or values repel others instead of attracting them.\n\nIn his book, Nye argues that soft power is a more difficult instrument for governments to wield than hard power for two reasons: many of its critical resources are outside the control of governments, and soft power tends to \"work indirectly by shaping the environment for policy, and sometimes takes years to produce the desired outcomes.\" The book identifies three broad categories of soft power: \"culture\", \"political values\", and \"policies.\"\n\nIn \"The Future of Power\" (2011), Nye reiterates that soft power is a descriptive, rather than a normative, concept. Therefore, soft power can be wielded for nefarious purposes. \"Hitler, Stalin, and Mao all possessed a great deal of soft power in the eyes of their acolytes, but that did not make it good. It is not necessarily better to twist minds than to twist arms.\" Nye also claims that soft power does not contradict the international relations theory of realism. \"Soft power is not a form of idealism or liberalism. It is simply a form of power, one way of getting desired outcomes.\"\n\nSoft power has been criticized as being ineffective by authors such as Niall Ferguson in the preface to \"Colossus\". Neorealist and other rationalist and neorationalist authors (with the exception of Stephen Walt) dismiss soft power out of hand as they assert that actors in international relations respond to only two types of incentives: economic incentives and force.\n\nAs a concept, it can be difficult to distinguish between soft power from hard power. For example, Janice Bially Mattern argues that George W. Bush's use of the phrase \"you are either with us or with the terrorists\" was in fact an exercise of hard power. Though military and economic force was not used to pressure other states to join its coalition, a kind of force – representational force – was used. This kind of force threatens the identity of its partners, forcing them to comply or risk being labelled as evil. This being the case, soft power is therefore not so soft. However, rationalist authors would merely see this as an 'implied threat', and that direct economic or military sanctions would likely follow from being 'against us'.\n\nThe first attempt to measure soft power through a composite index was created and published by the Institute for Government and the media company \"Monocle\" in 2010. The IfG-Monocle Soft Power Index combined a range of statistical metrics and subjective panel scores to measure the soft power resources of 26 countries. The metrics were organized according to a framework of five sub-indices including culture, diplomacy, education, business/innovation, and government. The index is said to measure the soft power resources of countries, and does not translate directly into ability influence. \"Monocle\" has published an annual \"Soft Power Survey\" since then. As of 2016/17, the list is calculated using around 50 factors that indicate the use of soft power, including the number of cultural missions (primarily language schools), Olympic medals, the quality of a country's architecture and business brands.\n\n\"The Soft Power 30\", which includes a foreword by Joseph Nye, is a ranking of countries' soft power produced and published by the media company \"Portland\" in 2015. The ranking is based on \"the quality of a country’s political institutions, the extent of their cultural appeal, the strength of their diplomatic network, the global reputation of their higher education system, the attractiveness of their economic model, and a country’s digital engagement with the world.\"\n\nThe \"Elcano Global Presence Report\" scores the European Union first for soft presence when its member states are excluded and the EU is considered as a whole.\n\nSoft power, then, represents the third behavioral way of getting the outcomes you want. Soft power is contrasted with hard power, which has historically been the predominant realist measure of national power, through quantitative metrics such as population size, concrete military assets, or a nation's gross domestic product. But having such resources does not always produce the desired outcomes, as the United States discovered in the Vietnam War. The extent of attraction can be measured by public opinion polls, by elite interviews, and case studies.\n\nNye argues that soft power is more than influence, since influence can also rest on the hard power of threats or payments. And soft power is more than just persuasion or the ability to move people by argument, though that is an important part of it. It is also the ability to attract, and attraction often leads to acquiescence.\n\nIn international relations, soft power is generated only in part by what the government does through its policies and public diplomacy. The generation of soft power is also affected in positive (and negative) ways by a host of non-state actors within and outside the country. Those actors affect both the general public and governing elites in other countries, and create an enabling or disabling environment for government policies.\n\nIn some cases, soft power enhances the probability of other elites adopting policies that allow one to achieve preferred outcomes. In other cases, where being seen as friendly to another country is seen as a local political kiss of death, the decline or absence of soft power will prevent a government from obtaining particular goals. But even in such instances, the interactions of civil societies and non-state actors may help to further general milieu goals such as democracy, liberty, and development. Soft power is not the possession of any one country or actor.\n\nThe success of soft power heavily depends on the actor's reputation within the international community, as well as the flow of information between actors. Thus, soft power is often associated with the rise of globalization and neoliberal international relations theory. Popular culture and mass media are regularly identified as a source of soft power, as is the spread of a national language or a particular set of normative structures; a nation with a large amount of soft power and the good will that engenders it inspire others to acculturate, avoiding the need for expensive hard power expenditures. More particularly, international news was found crucial in shaping the image and reputation of foreign countries. The high prominence of the US in international news, for example, has been linked to its soft power. Positive news coverage was associated with positive international views, while negative news coverage with negative views.\n\nBecause soft power has appeared as an alternative to raw power politics, it is often embraced by ethically-minded scholars and policymakers. But soft power is a descriptive rather than a normative concept. Like any form of power, it can be wielded for good or bad purposes. While soft power can be used with bad intentions and wreak horrible consequences, it differs in terms of means. It is on this dimension that one might construct a normative preference for greater use of soft power.\n\nAcademics have engaged in several debates around soft power. These have included:\n\nOne study finds that a state's soft power has a measurable effect on its exports. Countries that are admired for their positive global influence export more, holding other things constant.\n\nThe Soviet Union competed with the U.S. for influence throughout the Cold War. The Soviets were engaged in a broad campaign to convince the world of the attractiveness of its Communist system. In 1945, the Soviet Union was very effective in attracting many in Europe from its resistance to Hitler, and in colonized areas around the world because of its opposition to European imperialism. The Soviets also employed a substantially large public diplomacy program that included: promoting their high culture, broadcasting, disseminating disinformation about the West, and sponsoring nuclear protests, peace movements, and youth organizations. Despite all of this, the Soviets' closed system and lack of popular culture impeded the ability of the Soviet Union to compete with the U.S. in terms of soft power.\n\nA number of non-democratic governments have attempted to use migration as an instrument of soft power: Egypt under the rule of Gamal Abdel Nasser trained and dispatched thousands of teachers across the Arab world in an effort to spread ideas of anti-colonialism and anti-Zionism. In Cuba, the Fidel Castro regime's medical internationalism programme has dispatched thousands of medical professionals abroad for cultural diplomacy purposes. The Chinese-sponsored Confucius Institutes across the world rely on Chinese teachers in order to strengthen the country's soft power abroad. More recently, Turkey's migration diplomacy involves sponsoring the short-term emigration of imams across Europe and North America. \n\nAfter Pope John Paul II visited Poland in 1979, some political commentators said his visit influenced events against Poland's communist government, the Soviet Union, and ultimately communism, which promoted atheism.\n\nBesides the Pope's visit, American-government broadcasting and propaganda into Soviet-occupied Europe, particularly Poland, contributed to the rise of the Solidarity movement and to the collapse of the Soviet-backed governments there and in the rest of the Warsaw Pact alliance.\n\nThe United States and Europe have consistently been sources of influence and soft power. European culture's art, literature, music, design, fashion, and even food have been global magnets for some time. Europe and the U.S. have often claimed to support human rights and international law throughout the world. Unlike the U.S., Europeans' love of football enhances their soft power globally, whereas the primary sports of the U.S. such as American football and baseball are largely unpopular on the world stage. In 2012, the European Union was awarded the Nobel Peace Prize \"for over six decades [it has] contributed to the advancement of peace and reconciliation, democracy and human rights in Europe.\" The U.S. has the largest diplomatic network in the world, the largest number of foreign journalists based in the country, and is the most popular destination for international students. American films, among other influences, have contributed to the Americanization of other cultures.\n\nAsia and more recently China have been working to use the potential soft power assets that are present in the admiration of their ancient cultures, arts, fashion and cuisine. China is presenting itself as a defender of national sovereignty, which became an issue after the NATO air campaign to oust Colonel Muammar Gaddafi and NATO's support of the rebels in Libya. The Chinese are also competing with the United States to gain influence throughout the South Pacific, however some commentators have said their recent assertiveness in this region has created an appeal for nations in this region to align with the United States thus increasing U.S. soft power in this area.\n\nSoft power extends beyond the operations of government, to the activities of the private sector and to society and culture at large. Soft power has gained more influence because it addresses the underlying dispositions of the people who have increasingly become more active in their governments. This is true even in authoritarian countries where people and institutions are increasingly able to shape the debate.\n\nThe information age has also led to the rise of soft power resources for non-state actors, Primarily, through the use of global media, and to a greater extent the internet using tools such as the World Wide Web, non-state actors have been able to increase their soft power and put pressure on governments that can ultimately affect policy outcomes. Instead of front organizations, non-state actors can create cyber advocacy organizations to recruit members and project their voice on the global stage.\n\nChina's traditional culture has been a source of attraction, building on which it has created several hundred Confucius Institutes around the world to teach its language and culture. The enrollment of foreign students in China has increased from 36,000 a decade before to at least 240,000 in 2010. China's Asian Infrastructure Investment Bank has attracted many western countries to join. In 2017, China had the second largest diplomatic network in the world.\n\nA spring 2014 Global Attitudes survey from Pew Research Center states China receives mostly positive reviews in the sub-Saharan African nations polled, although South Africans are closely divided (45% favorable, 40% unfavorable). China's increasing soft power can be explained by looking at China's economic growth and regarding economic engagement with many African countries. China's expansion of trade and investment on the African continent and the spread of Chinese-led infrastructure projects gives positive impressions of China to people in Africa. China's economic engagement in African countries is considered as much more pragmatic and in consistency with the priorities of many African countries. Moreover, China's increasing role as a global superpower seems appealing and this drives a desire to tie African economies more closely to China's economy.\n\nChina has made a systematic effort to expand and give greater profile to its soft-power policies in Africa ever since the first Forum on China-Africa Cooperation in 2000. The commitments of China's soft power ranges from health, humanitarian assistance to academic, professional and cultural exchange. China's assistance to Africa, however, is not near the U.S. assistance in Africa.\n\nCultural exchange between China and Africa can be a representative example of how China has been spreading its soft power.\nIn 2005, the first Confucius Institute was established in Africa. The institute is funded by the Chinese government and it provides Chinese language and cultural programming to the public. There are 19 institutes today in Africa and China has planned to spend 20 million renminbi for education projects in South Africa, including the teaching of Mandarin in 50 local high schools.\n\nFurthermore, there is an increasing support for cultural visitors programs which gained momentum in 2004 when the African Cultural Visitors Program was established. There is a rising number of African entrepreneurs who choose to move to China and there are also diaspora communities in many Chinese cities that have been found.\n\nOutside of Africa, Chinese soft power extends to countries like Barbados. Barbadian Prime Minister David Thompson expressed admiration for the Chinese economic model and sought to emulate the way Chinese state controlled banks guided development.\n\nFrance has long exerted a great amount of soft power. The country and its culture have for centuries been admired in many parts of the world; so much so that Thomas Jefferson is famously quoted as saying \"Every man has two countries, his own and France.\" The very term \"culture\" comes from France. In 2017, France had the third largest diplomatic network in the world.\n\nFrance was a focal point of the Age of Enlightenment; its attachment to the ideals of liberty, equality, tolerance and reason was notably embodied in the writing and publishing of the \"Encyclopédie\". The French Revolution was one of the most significant events in European and world history. France has since then been instrumental in spreading Republican ideals. The Napoleonic Code, which influenced much of the rest of Europe and beyond, is regarded as one of the most important law document of the modern era.\n\nThe French language has for centuries been an important diplomatic language. For example, French has to be used – on par with English – for all documents issued by the United Nations Treaty Series, ensuring that all UN treaties are equally valid in their English and French versions.\n\nFrance has also followed for decades a very active diplomatic and cultural policy. The \"Alliance Française\", whose aim is to promote the French language and French culture, was created as early as 1883. In \"Monocle\"'s 2015 \"Soft Power 30\" report, France was ranked first in the \"engagement\" criteria, which is intended to measure \"the reach of states’ diplomatic network and their commitment to major challenges like development and the environment.\" \"Monocle\" further noted that \"In terms of influential reach, France is the best networked state in the world and is member of more multi-lateral organisations than any other country.\" Overall, France ranked fourth in that study.\n\nFrance's laïcité, secularism, policy has inspired some countries over time. For instance, France was Atatürk's main role model for Westernization as part of the major reform efforts that he spearheaded when he was President of Turkey.\n\nFrance, and in particular Paris, has long been considered one of the most romantic places to be. France was in 2014 the most visited country in the world.\n\nThe annual soft power rankings by \"Monocle\" magazine and the Institute for Government ranks 30 countries which “best attract favor from other nations through culture, sport, cuisine, design, diplomacy and beyond.”\n\"Monocle\" magazine said: “Merkel may be painted as a stern taskmaster but it seems she has a softer side, or the country she leads does.”\nIt said Germany’s rise as a soft power should not come as a surprise.\n“The country is traditionally excellent at pursuing its ideas, values and aims using diplomatic, cultural and economic tools,\" it said. “By quietly doing the simple things well it is a country that has become a global power and the rest of us can feel comfortable with that.”\nGermans had been understandably wary about depicting a dominant image abroad, the magazine added, but it said that the country’s rise should not make everyone else feel uncomfortable. In 2017, Germany had the eighth largest diplomatic network in the world.\n\nThe famous elements of Italian soft culture are its art, music, fashion, and iconic food. Italy was the birthplace of opera, and for generations the language of opera was Italian, irrespective of the nationality of the composer. Popular tastes in drama in Italy have long favored comedy; the improvisational style known as the \"Commedia dell'arte\" began in Italy in the mid-16th century and is still performed today. Before being exported to France, the famous Ballet dance genre also originated in Italy.\nThe country boasts several world-famous cities. Rome was the ancient capital of the Roman Empire and seat of the Pope of the Catholic Church. Florence was the heart of the Renaissance, a period of great achievements in the arts that ended the Dark Ages. Other important cities include Turin, which used to be the capital of Italy, and is now one of the world's great centers of automobile engineering. Milan is a fashion capital of the World. Venice, with its intricate canal system, attracts tourists from all over the world especially during the Venetian Carnival and the Biennale.\nItaly is home to the greatest number of UNESCO World Heritage Sites (51) to date, and according to one estimate the country is home to half the world's great art treasures. The nation has, overall, an estimated 100,000 monuments of any sort (churches, cathedrals, archaeological sites, houses and statues).\n\nItaly is considered the birthplace of Western civilization and a cultural superpower. In 2017, Italy had the eleventh largest diplomatic network in the world.\n\n\"Cool Japan\" is a concept coined in 2002 as an expression of Japan’s popular culture. The concept has been adopted by the Japanese government as well as trade bodies seeking to exploit the commercial capital of the country’s culture industry. It has been described as a form of soft power, \"the ability to indirectly influence behavior or interests through cultural or ideological means.\"\nIn a 2002 article in the journal \"Foreign Policy\" titled “Japan’s Gross National Cool”, Douglas McGray wrote of Japan “reinventing superpower” as its cultural influence expanded internationally despite the economic and political problems of the “lost decade.” Surveying youth culture and the role of J-pop, manga, anime, fashion, film, consumer electronics, architecture, and cuisine, McGray highlighted Japan’s considerable soft power, posing the question of what message the country might project. He also argued that Japan’s recession may even have boosted its national cool, due to the partial discrediting of erstwhile rigid social hierarchies and big-business career paths. In 2017, Japan had the fifth largest diplomatic network in the world.\n\nRussia has been developing its soft power by investing in various public diplomacy instruments throughout the 2000s but the term was first used in an official document in 2010 as President Medvedev approved an Addendum to the national Foreign Policy Concept. The term was not defined but it was described as related to cultural diplomacy. In 2013, the term appeared in a new version of the Foreign Policy Concept where the soft power was defined as \"a comprehensive toolkit for achieving foreign policy objectives building on civil society potential, information, cultural and other methods and technologies alternative to traditional diplomacy.\" In 2007, Russian President Vladimir Putin was named \"Time\" Person of the Year. In 2013, he was named most powerful person by Forbes magazine. In 2015, Russia led the creation of the Eurasian Economic Union. In 2017, Russia had the fourth largest diplomatic network in the world. In the wake of the poisoning of Sergei and Yulia Skripal in 2018, the BBC reported that \"Its extensive diplomatic network reflects both its imperial history as a great power in the 19th Century, as well as its Cold War posture. It has a multitude of posts in Eastern Europe and former communist allies including China, Vietnam, Cuba and Angola, as well as legacies of the former USSR in Africa and Asia. The size of its network reflects the extent of its undiminished global ambition.\"\n\n\"Hallyu\", also known as the \"Korean Wave\", is a neologism referring to the spread of South Korean culture since the late 1990s. According to a \"Washington Post\" reporter, the spread of South Korean entertainment has led to higher sales of other goods and services such as food, clothing, and Korean language classes. Besides increasing the amount of exports, the \"Korean Wave\" is used by the government as a soft power tool to engage with the masses of young people all over the world, and to try reduce anti-Korean sentiment.\n\nIn 2012, the BBC's country rating poll revealed that public opinion of South Korea has been improving every year since the first rating poll for the country was conducted in 2009. In several countries such as Russia, India, China and France, public opinion of South Korea turned from slightly negative to generally positive. The report cited culture and tradition as among the most important factors contributing to positive perceptions of South Korea. This comes alongside a rapid growth in the total value of cultural exports which rose to US$4.2 billion in 2011.\n\nFirst driven by the spread of Korean dramas televised across East, South and Southeast Asia during its initial stages, the \"Korean Wave\" evolved from a regional development into a global phenomenon due to the proliferation of Korean pop (K-pop) music videos on YouTube. Currently, the spread of the \"Korean Wave\" to other regions of the world is most visibly seen among teenagers and young adults in Latin America, the Middle East, North Africa, and immigrant enclaves of the Western world.\n\nSince the period of \"Pax Britannica\" the United Kingdom has held significant soft power. Today it remains one of the most influential countries in the world, coming first in the 2015 Portland Group, Comres, Facebook report, and the \"Monocle\" survey of global soft power in 2012.\n\nThe UK has strong diplomatic relations with countries around the world, particularly countries in the Commonwealth of Nations and many others in Europe, Asia, the Middle-east, Africa and the United States. Diplomatic missions between Commonwealth countries are known as High Commissions rather than Embassies to indicate the closeness of the relationship. The UK exerts influence on countries within the European Union, and has the seventh largest global network of diplomatic missions as of 2017. Many countries around the world use the British form of democracy and government known as the Westminster system.\n\nThe influence of British culture and sports are widespread, particularly notable during the British Invasion, Cool Britannia, and more recently the Diamond Jubilee and 2012 Summer Olympics. The opening and closing ceremonies celebrated British culture and achievements with the world. London was the first city to host the modern Olympics three times. British media is broadcast internationally, notably the \"BBC World Service\", \"BBC World News\" and \"The Economist\" magazine. British film and literature have international appeal, and British theatre helps make London one of the most visited cities in the world. Schools and universities in Britain are popular destinations for students of other nations.\n\nAlongside the English language, English contract law is the most important and most used contract law in international business. London is the headquarters for four of the world's six largest law firms. The UK and more specifically London is a centre of international finance where foreign participants in financial markets come to deal with one another. It is headquarters for major international corporations, many of which choose to be listed on the London Stock Exchange.\n\nFollowing the poisoning of Sergei and Yulia Skripal in 2018, the UK responded with bilateral and multilateral diplomatic efforts that led to nations around the world expelling one hundred and fifty Russian diplomats, described by CNN as a \"remarkable diplomatic coup for Britain\". British prime minister Theresa May stated in parliament that the coordinated global response was the \"largest collective expulsion of Russian intelligence officers in history.\" Subsequently, Russia attempted to attribute some of the influence to the United States, this was denied by the various states as well as the entirety of the European Union.\n\nThe United States has long had a great deal of soft power. Examples include Franklin D. Roosevelt's four freedoms in Europe at the end of World War II, young people behind the Iron Curtain listening to the government's foreign propaganda arm Radio Free Europe, Chinese students symbolizing their protests in Tiananmen Square by creating a replica of the Statue of Liberty that they called \"Goddess of Democracy\", newly liberated Afghans in 2001 asking for a copy of the Bill of Rights and young Iranians today surreptitiously watching banned American videos and satellite television broadcasts in the privacy of their homes. America's early commitment to religious toleration, for example, was a powerful element of its overall appeal to potential immigrants; and American aid in the reconstruction of Europe after World War II was a propaganda victory to show off the prosperity and the generosity of the people of the United States.\n\nStudies of American broadcasting into the Soviet bloc, and testimonials from Czech President Václav Havel, Polish President Lech Wałęsa, and Russian President Boris Yeltsin support that soft power efforts of the United States and its allies during the Cold War were ultimately successful in creating the favorable conditions that led to the collapse of the Soviet Union.\n\n\n"}
{"id": "14135401", "url": "https://en.wikipedia.org/wiki?curid=14135401", "title": "Stephen Thomas (professor)", "text": "Stephen Thomas (professor)\n\nStephen Thomas is a professor at the University of Greenwich Business School, working in the area of energy policy. Before moving to the University of Greenwich in 2001, Thomas worked for twenty-two years at the University of Sussex.\n\nStephen Thomas is professor at the University of Greenwich Business School, and has been a researcher in the area of energy policy for over twenty-five years. He specialises in the economics and policy of nuclear power, liberalisation and privatisation of the electricity and gas industries, and trade policy on network energy industries. Professor Thomas serves on the editorial boards several periodicals including \"Energy Policy, Utilities Policy, Energy and Environment\", and \"International Journal of Regulation and Governance\".\n\nBefore moving to the University of Greenwich in 2001, Thomas worked for twenty-two years at the Science Policy Research Unit (SPRU) at the University of Sussex.\n\nThomas has been critical of the idea that the nuclear power industry is undergoing a \"nuclear renaissance\". In May 2009 he said:\n\nWe've been waiting in vain on a 'Nuclear Renaissance' in Europe since the early 1990s. Even before the recent collapse in energy prices and the financial downturn, it was clear that all of the talk of a new resurgence in the prospects for nuclear reactors was just that: talk. \nIn terms of the nuclear power industry in the USA, Thomas said, in May 2009:\n\nU.S. efforts to revive nuclear ordering, begun in 2002, were originally expected to get a new plant on-line by 2010. Even if there are no further delays, the earliest a new plant could be on-line is now looking closer to 2020. In the meantime, the nuclear industry has upped its demands for taxpayer-backed loan guarantees to build demonstration plants from about $5 billion to more than $100 billion. The prospect that these demonstration plants would lead to unsubsidized ordering now looks fanciful.\nRegarding the UK nuclear power industry, on 20 November 2014, he told BBC Radio 4's Today programme:\n\n\"If someone wants to make losses on [building nuclear] plants in Britain, well, good luck to them.\" The UK \"lost its nuclear industry probably 35 years ago, and I'm not sure we should be too worried about that – it's not an industry that's done well for us... The last reactors we exported were more than 50 years ago. ... We should be focusing on energy efficiency and renewable energy, which is becoming cheaper while nuclear is becoming more expensive.\" \n\n"}
{"id": "44677226", "url": "https://en.wikipedia.org/wiki?curid=44677226", "title": "The Orchid Album", "text": "The Orchid Album\n\nThe Orchid Album is a horticultural work by Robert Warner and Benjamin Samuel Williams of eleven volumes published between 1872 and 1897 and illustrated by John Nugent Fitch.\n\n</div>\n\n"}
{"id": "2651057", "url": "https://en.wikipedia.org/wiki?curid=2651057", "title": "Theta Sagittarii", "text": "Theta Sagittarii\n\nThe Bayer designation Theta Sagittarii (θ Sagittarii) is shared by two stars, θ Sagittarii and θ Sagittarii, in the constellation Sagittarius. The pair are separated by 0.58° in the sky.\n\n"}
{"id": "44039518", "url": "https://en.wikipedia.org/wiki?curid=44039518", "title": "Tree box filter", "text": "Tree box filter\n\nTree box filters are typically filtration systems installed under a street or sidewalk which have trees planted in them. The systems collect stormwater runoff through methods such as permeable pavers or curb cuts. The runoff is then filtered by soil, microbes and vegetation before either being utilized by the tree or discharged into a storm drain system.\n\nTree box filters have the potential to improve the quality and health of street trees by providing larger areas for roots to stabilize the tree and more availability of water and nutrients. Additionally, the filters can delay the peak flow and improve the water quality of water entering a storm drain system. It is a practical management strategy for stormwater issues in urban settings. \n"}
{"id": "30468705", "url": "https://en.wikipedia.org/wiki?curid=30468705", "title": "UCI World Ranking", "text": "UCI World Ranking\n\nFor the 2016 season the UCI revamped the points system used to rank men's road cycling riders. The following table summarises the new rankings, how points are scored towards them and how points are scaled. \n\nThe following is a list of riders and nations that achieved the number one position at the end of the year:\n\nSource:\n\n"}
