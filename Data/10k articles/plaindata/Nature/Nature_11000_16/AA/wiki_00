{"id": "1426202", "url": "https://en.wikipedia.org/wiki?curid=1426202", "title": "100 Famous Japanese Mountains", "text": "100 Famous Japanese Mountains\n\nThe complete list (sorted into regions from northeast to southwest) is below.\n\nThe selection of celebrated mountains had been done since the Edo period. Tani Bunchō praised 90 mountains as celebrated mountains in 日本名山図会 (\"A collection of maps and pictures of famous Japanese mountains\"), but among these were included such small mountains as Mount Asama in Ise, Mie and Mount Nokogiri on the Boso Peninsula. Unsatisfied with this selection, Fukuda, who had climbed many mountains in Japan, selected 100 celebrated Japanese mountains based on a combination of grace, history, and individuality, moreover excluding mountains with an altitude of less than .\n\nThough it was at first unknown other than to some hiking-lovers and avid readers, reports that the list was one of the Prince's favorite books increased its profile. The Crown Prince is a mountain enthusiast to the extent that he has even belonged to an alpine club, and it has been reported that it is a dream of his to reach the summit of every mountain on the list.\n\nSince the 1980s, there has been a climbing boom amongst the middle-aged. It is not rock climbing that has been popularised, but rather hiking or trekking. However, due to the creation of more mountain lodges and trails, and the improvement of mountaineering technology, it became possible to climb mountains which had previously been considered very rugged.\n\nThe list became widely read, and people who choose mountains from the book to climb have increased. In imitation of Prince Naruhito, many people have also set the goal of reaching every summit on the list.\n\nMountaineering programs on NHK helped popularize the list. The station televised a documentary about taking up the mountains on the list one by one, and Rambō Minami's mountaineering primer for the middle-aged. These gained wide popularity, and the list became widely known. Since then, lists of 200 and 300 mountains, lists of hundreds of mountains in various localities, and a list of 100 floral mountains have appeared.\n\nIn 2002, a new record traversing all of the mountains in 66 days was established. This was superseded in 2007, with a new record of 48 continuous days.\n\nEnglish translation by Martin Hood is published as \"One Hundred Mountains of Japan\" (University of Hawaii Press 2014）.\n\nCompared to other modern essays on Japanese mountains such as \"Mountaineering and Exploration in the Japanese Alps\" by Walter Weston, the book is short. Fukuda writes about the history of the mountains, especially the origins of their names. It is not a text that people can read to vicariously experience climbing or nature. Some think that the reason the list has been widely well received is that it put into focus 100 mountains which were already well known.\n\nFukada selected 100 mountains from those which are 1,500 meter or higher and he had climbed up ever, according to three criteria: grace, history and individuality. He softened the altitude limit on some exceptions, like Mount Tsukuba and Mount Kaimon. \n\nThere have been many varying opinions about the criteria for selection. It is often pointed out that the list emphasises mountains in the Chūbu region. It has been reported that Fukada, who was from Ishikawa Prefecture, was brought up looking at Mt. Haku, but he only selected 13 further west. \n\nHowever, grace and individuality are in the eye of the beholder, and throughout history, many legends have been circulated about mountains throughout the Kinki region. Moreover, many mountain-lovers have argued that since Mount Tsukuba, with an altitude of 877 meters (876 at the time), was selected, certain mountains in other localities should have been selected.\n\n\n\n\n\n\n\n\n\n\n\"One Hundred Mountains of Japan\"\n"}
{"id": "17440610", "url": "https://en.wikipedia.org/wiki?curid=17440610", "title": "Alberta electricity policy", "text": "Alberta electricity policy\n\nIn 1996, Alberta began to restructure its electricity market away from traditional regulation to a market-based system. The market now includes a host of buyers and sellers, and an increasingly diverse infrastructure.\n\nConsumers range from residential buyers to huge industrial consumers mining the oil sands, operating pipelines and milling forest products. On the supply side, generators range from wind farms east of Crowsnest Pass to huge coal-fired plants near Edmonton. The diversity of Alberta's electricity supply has increased substantially. To a large extent because of deregulation, the province has more technology, fuels, locations, ownership, and maintenance diversity than in the past. The system's reliability, its cost structure and Alberta's collective exposure to risk are now met by a complex system based on diverse power sources, mainly coal and natural gas, with some wind and hydroelectric energy sources.\n\nThe Alberta Government passed the Electric Utilities Act(1996) effective January 1, 1996, which created Power Pool of Alberta, a wholesale market clearing entity. The Power Pool was a not for profit entity that operated the \"competitive wholesale market including dispatch of generation.\" The Electric Utilities Act stipulated all electric energy bought and sold in Alberta had to be exchanged through the Power Pool which \"served as an independent, central, open access pool.\" It functioned as a \"spot market intending to match the demand with the lowest cost supply and establish an hourly pool price.\" \nAlberta was the first Canadian province to implement a deregulated electricity market. Competitive wholesale markets were being fostered in the 1990s as part of the liberalization process of the 1990s changing some parameters such as the unbundling of generation, transmission and distribution functions of incumbent utilities. Local distribution utilities, either investor- or municipally owned, retained the obligation to supply and the 6 largest utilities were assigned a share of the output of existing generators at a fixed price. The province moved to full retail access in 2001. In 2003 The Alberta Electric System Operator was established under the provisions of the Electric Utilities Act, and through the AESO, a spot market was created. After consumers complained about high prices in 2000, the government implemented a Regulated Rate Option (RRO), as a means to shield consumers from price volatility.\n\nThe generation sector in Alberta is dominated by TransAlta (formerly Calgary Power), ENMAX, and Capital Power Corporation, a spin-off of Edmonton's municipally owned company EPCOR. Utility companies in Alberta also include the wind generating Bullfrog Power, TransAlta Corporation, Alberta Power limited, AltaLink, ATCO Power and FortisAlberta. Although 5,700 megawatts of new generation was added and 1,470 of old plants were retired between 1998 and 2009, coal still accounted for 73.8% of utility-generated power in 2007, followed by natural gas, with 20.6%.\n\nInstalled capacity reached 12,834 megawatts in 2009, with coal (5,692 MW) and natural gas (5,189 MW) representing the bulk of the province's generation fleet. As of 2008, Alberta's electricity sector was the most carbon-intensive of all Canadian provinces and territories, with total emissions of 55.9 million tonnes of equivalent in 2008, accounting for 47% of all Canadian emissions in the electricity and heat generation sector.\n\nIn 2013, electrical generation consisted of coal (55%), natural gas (35%), renewable and alternative sources (11%). \n\nBy 2010 wind capacity had reached 657 MW and hydroelectric capacity produced 900 MW. In June 2010 the federal government announced tougher new emission measures. Alberta will likely remain dependent on coal generated electricity into 2050.\n\nAlberta's electricity market consists of five fundamental components and features.\n\nWhile generation companies (e.g., EPCOR) continue to own both generation and transmission in Alberta, the Alberta Electric System Operator (AESO) which is \"independent of any industry affiliations and owns no transmission or market assets\" has transmission control. The Alberta Minister of Energy appoints the members of AESO's board. \"[It] is governed by an independent board, which has a diverse background in finance, business, electricity, oil and gas, energy management, regulatory affairs and technology. The Board's governance strategy is founded on balancing the interests of a diverse set of stakeholders, while at the same time, providing benefit for the overall industry stakeholder needs. (AESO cited in Brennan 2008:9).\"\n\nAlberta and neighbouring British Columbia are buyers and sellers of each other's power. Historically, Albertans buy from B.C. during peak hours. Similarly, B.C. buys from Alberta frequently during off-peak periods (weekends, evenings, or statutory holidays when demand in Alberta diminishes). This arrangement confers benefits on both provinces.\n\nThe power-exchanging relationship between the two provinces is based on geography. Alberta has coal and natural gas, while B.C. has big mountains, long valleys and historically an abundance of water resources. As a result, B.C. based its system on hydroelectric power while Alberta constructed one that primarily burns hydrocarbons, mostly gas and coal. Over the years the two provinces have evolved an interdependent relationship.\n\nAlberta's electrical demand varies throughout the day and across the seasons. When individuals are fixing supper and using home appliances, demand for power goes up, as it does during heat waves and cold snaps. It tapers off during spring and fall. Like other mechanical devices, generators fail from time to time. If they are wind-powered, their output varies with the wind.\n\nWhether for reasons of temporary high demand, short supply or both, Alberta buys electricity from its western neighbour through AESO. By contrast, Alberta might sell electricity to British Columbia during off-peak periods. During that period, B.C. uses that power to reduce its hydroelectric generation.\n\nAlberta buys electricity from B.C. during periods of peak consumption, on unusually cold or hot days or when a larger-than-normal number of generators are down for maintenance. British Columbia buys electricity from Alberta during off-peak periods. This arrangement enables both provinces to make use of their generating and storage capacity and use assets more efficiently. Also, it keeps power prices lower in both provinces than they would otherwise be.\n\nThis arrangement evolved because of physical differences between the two electrical systems. It depends little on differences in the two market models.\n\nThe differences between Alberta's and British Columbia's market models represent the two extremes in use within Canada. Alberta has developed a system in which markets determine prices and the pace of investment. B.C. has a conventional cost of service regulated power system (that exists in most of Canada and most of the United States). \n\nDespite the vast differences in market design and because of large differences in the mix of generation assets, the electricity systems of Alberta and British Columbia enjoy a unique symbiotic relationship. B.C. provides a market for Alberta's off-peak surplus and a peaking supply for Alberta's crunch periods. The investment climate in Alberta has attracted a steady stream of private investors-funded generation projects since 1996. This is one of the reasons Alberta's electricity system has provided reliable, sustainable power even during periods of rapid economic growth.\n\nIn April 2013, Calgary ranked third (with an average monthly payment of $216 based on a monthly consumption of 1,000 kWh) and Edmonton fourth ($202 a month) in Canada compared to other cities in terms of high electricity bills. Halifax placed first and worst in Canada at $225 a month. Compared to other cities in North America, Calgary and Edmonton placed seventh and eighth in terms of highest power costs. Vancouver, BC was among the least expensive ($130 a month).\n\nThe unit price of electricity in Calgary in April 2013 was 14.81 cents per kWh, compared to 6.87 cents per kWh in Montreal, 15.45 in Halifax.\n\nKeith Provost, former senior vice-president of Alberta Power Ltd. (now ATCO Power), with decades of experience in the electrical utilities business, argued that instead of marketing electricity contracts for future deliveries in a regulated market, AESO has their own system that is open to manipulation and is not a free-market system. According to AESO, \"The pool price is the arithmetic average of the 60 one-minute system marginal prices. Only those offers accepted generate power and receive the AESO pool price. All offers accepted receive the same price, the pool price, not the price offered.\" It is this deregulated system that causes volatility in the price of electricity, keeps consumer prices high while maximizing profits to generating companies.\n\n\n"}
{"id": "47249", "url": "https://en.wikipedia.org/wiki?curid=47249", "title": "Amalthea (mythology)", "text": "Amalthea (mythology)\n\nIn Greek mythology, Amaltheia () is the most-frequently mentioned foster-mother of Zeus.\n\nThe name \"Amaltheia\", in Greek \"tender goddess\", is clearly an epithet, signifying the presence of an earlier nurturing goddess, whom the Hellenes, whose myths we know, knew to be located in Crete, where Minoans may have called her a version of \"Dikte\".\n\nThere were different traditions regarding Amaltheia.\nAmaltheia is sometimes represented as the goat who nurtured the infant-god in a cave in Cretan Mount Aigaion (\"Goat Mountain\"), sometimes as a goat-tending nymph of uncertain parentage (the daughter of Oceanus, Helios, Haemonius, or—according to Lactantius—Melisseus), The possession of multiple and uncertain mythological parents indicates wide worship of a deity in many cultures having varying local traditions. Other names, like Adrasteia, Ide, the nymph of Mount Ida, or Adamanthea, which appear in mythology handbooks, are simply duplicates of Amaltheia.\n\nIn the tradition represented by Hesiod's \"Theogony\", Cronus swallowed all of his children immediately after birth. The mother goddess Rhea, Zeus' mother, deceived her brother consort Cronus by giving him a stone wrapped to look like a baby instead of Zeus. Since she instead gave the infant Zeus to Adamanthea to nurse in a cave on a mountain in Crete, it is clear that Adamanthea is a doublet of Amaltheia. In many literary references, the Greek tradition relates that in order that Cronus should not hear the wailing of the infant, Amaltheia gathered about the cave the Kuretes or the Korybantes to dance, shout, and clash their spears against their shields. Still a baby, Zeus would have broken off one of Amalthea's horns while playing, which would become the cornucopia, and turning Amalthea into the first unicorn, a reference used by Peter S. Beagle in his novel \"The Last Unicorn\", as Schmendrick, the magician, called the main character \"Lady Amalthea\", without giving any explanation to his choice.\n\nAmaltheia's skin, or that of her goat, taken by Zeus in honor of her when she died, became the protective aegis in some traditions.\n\n\"Amaltheia was placed amongst the stars as the constellation Capra—the group of stars surrounding Capella on the arm (\"ôlenê\") of Auriga the Charioteer.\" \"Capra\" simply means \"she-goat\" and the star-name \"Capella\" is the \"little goat\", but some modern readers confuse her with the male sea-goat of the Zodiac, Capricorn, who bears no relation to Amaltheia, no connection in a Greek or Latin literary source nor any ritual or inscription to join the two. Hyginus describes this catasterism in the \"Poetic Astronomy\", in speaking of Auriga, the Charioteer:\n\n<poem>Parmeniscus says that a certain Melisseus was king in Crete, and to his daughters Jove was brought to nurse. Since they did not have milk, they furnished him a she-goat, Amaltheia by name, who is said to have reared him. She often bore twin kids, and at the very time that Jove was brought to her to nurse, had borne a pair. And so because of the kindness of the mother, the kids, too were placed among the constellations. Cleostratus of Tenedos is said to have first pointed out these kids among the stars.\n\nBut Musaeus says Jove was nursed by Themis and the nymph Amaltheia, to whom he was given by Ops, his mother. Now Amaltheia had as a pet a certain goat which is said to have nursed Jove.</poem>\n\n\n"}
{"id": "23356578", "url": "https://en.wikipedia.org/wiki?curid=23356578", "title": "Batokunku", "text": "Batokunku\n\nBatokunku (also spelled \"Batukunku\") is a village located in Kombo South, one of the nine districts of The Gambia's Western Division. In January 2009, the village became notable as the location of the first wind turbine erected in West Africa. The 150 kilowatt turbine, a second-hand machine originally built by the Danish wind energy manufacturer Bonus, currently provides electrical power for the entire village. The windmill is currently serviced/maintained by Global Energy (generator service) based in the nearby village of Tujereng in collaboration with Windstrom SH from Germany.\n"}
{"id": "56758762", "url": "https://en.wikipedia.org/wiki?curid=56758762", "title": "Born Free (book)", "text": "Born Free (book)\n\nBorn Free is a book by Joy Adamson. Released in 1960 by Pantheon Books, it describes Adamson's experiences raising a lion cub named Elsa. It was translated into several languages, and made into an Academy Award-winning film of the same name.\n\nThe book was rereleased in 2017 by Pan Books as part of their Pan 70th anniversary collection, celebrating their best-loved, best-selling stories.\n"}
{"id": "21783926", "url": "https://en.wikipedia.org/wiki?curid=21783926", "title": "Carbon process management", "text": "Carbon process management\n\nCarbon Process Management (CPM) is a management process which promotes environmental effectiveness in organizations. It is designed to maximize efficiencies in the consumption of resources that contribute to climate change. When implemented effectively, CPM techniques can reduce operating costs, realizing gains in brand equity, competitive advantage and stakeholder value. Initially introduced by First Carbon Solutions, CPM uses Japanese kaizen philosophy which continuously improves workplace practices to reduce wastage, this is combined business process management (BPM) which increases efficiency. Governments who resorts to legal mechanisms and regulation to deal with the risks of climate change, techniques such as CPM are directed towards a corporate approach in helping reduce greenhouse gas emissions.\n"}
{"id": "20445347", "url": "https://en.wikipedia.org/wiki?curid=20445347", "title": "Centre for Renewable Energy", "text": "Centre for Renewable Energy\n\nCentre for Renewable Energy (short: SFFE, from Norwegian \"Senter for fornybar energi\") is a virtual research centre owned by Norwegian University of Science and Technology (NTNU), SINTEF, Institute for Energy Technology (IFE) and University of Oslo (UiO). SFFE was established in 2004, initially as a unifying organ for SINTEF and NTNU. IFE and UiO became co-owners of the Centre in 2005 and 2011, respectively. The goal of SFFE is to increase the quality, efficiency and scope of education, research, development and innovation within renewable energy in Norway. SFFE works to coordinate the available competence and the research and education activities localized at its member institutions. In 2010, the internal network in the member institutions included more than 400 scientists (including doctoral students) working on renewable energy. The current leader of the Centre is Gabriella Tranell (associate professor at Department of Materials Science and Engineering, NTNU).\n\nTogether, the owners of SFFE represent Norway's largest resource in renewable energy research. Research fields at the Centre include small scale hydropower, wind energy, solar energy, wave energy, and bio-energy as well as the social dimensions of energy use. Competence has recently been developed in ocean energy fields, especially research on offshore wind power and tidal power.\n\n"}
{"id": "1533140", "url": "https://en.wikipedia.org/wiki?curid=1533140", "title": "Dalton Minimum", "text": "Dalton Minimum\n\nThe Dalton Minimum was a period of low sunspot count, representing low solar activity, named after the English meteorologist John Dalton, lasting from about 1790 to 1830 or 1796 to 1820, corresponding to the period solar cycle 4 to solar cycle 7.\n\nLike the Maunder Minimum and Spörer Minimum, the Dalton Minimum coincided with a period of lower-than-average global temperatures. During that period, there was a variation of temperature of about 1 °C in Germany.\n\nThe cause of the lower-than-average temperatures and their possible relation to the low sunspot count are not well understood. Recent papers have suggested that a rise in volcanism was largely responsible for the cooling trend.\n\nWhile the Year Without a Summer, in 1816, occurred during the Dalton Minimum, the prime reason for that year's cool temperatures was the highly explosive 1815 eruption of Mount Tambora in Indonesia, which was one of the two largest eruptions in the past 2000 years. One must also consider that the rise in volcanism may have been triggered by lower levels of solar output as there is a weak but statistically significant link between decreased solar output and an increase in volcanism.\n\n\n"}
{"id": "5994564", "url": "https://en.wikipedia.org/wiki?curid=5994564", "title": "David Henry Lewis", "text": "David Henry Lewis\n\nDavid Henry Lewis, DCNZM (1917 – 23 October 2002) was a sailor, adventurer, doctor, and Polynesian scholar. He is best known for his studies on the traditional systems of navigation used by the Pacific Islanders. His studies, published in the book \"We, the Navigators\", made these navigational methods known to a wide audience and helped to inspire a revival of traditional voyaging methods in the South Pacific.\n\nDavid was born in Plymouth, England and raised in New Zealand and Rarotonga. He was sent to the Polynesian school in Rarotonga, where he apparently developed his appreciation for Polynesian identity and culture. He remained a New Zealander throughout his life, though he eventually retired to Queensland.\n\nAfter an adventurous childhood and teenage years including mountaineering and skiing in New Zealand, and a multi-hundred mile kayak journey, he traveled to England in 1938 for medical training at the University of Leeds, and served in the British army as a medical officer. After the war, he worked as a doctor in London, and was involved in setting up the National Health Service.\n\nWith the announcement in 1960 of the first single-handed trans-Atlantic yacht race (from Plymouth, UK to the US East Coast), Lewis decided to enter in a small 25-foot boat. Following a series of accidents, including a dismasting shortly after leaving, he finished third (Francis Chichester came first), as described in his book \"The Ship Would Not Travel Due West\". \n\nHe later decided to sail around the world with his second wife and two small daughters, and built the ocean cruising catamaran \"Rehu Moana\", for this purpose. After an initial voyage towards Greenland, he entered the 1964 single-handed trans-Atlantic race and picked up his family in the United States. They circumnavigated by way of the Strait of Magellan, the South Pacific and the Cape of Good Hope. (See his book \"Daughters of the Wind\".) This was the world’s first circumnavigation by multihull.\n\nFollowing his longstanding interest in old navigational methods used to explore and populate the Pacific, he employed similar techniques for the Tahiti-New Zealand leg of the \"Rehu Moana\" voyage without using a compass, sextant or marine chronometer.\n\nIn 1967, Lewis acquired another boat, \"Isbjorn\", to embark on further field studies of traditional Polynesian navigation. With a research grant from the Australian National University and with his second wife, two daughters and 19-year-old son, he set out for the Pacific again to study traditional navigation techniques. While there, he was welcomed into the cultures of various Pacific Islanders such as Hipour, who taught him their navigational lore, heretofore largely unrecognized by those outside Polynesia. Lewis chronicled this voyage and research in various articles and in his books \"We, the Navigators\" and \"The Voyaging Stars\". \nLewis’ voyages and resulting books gave inspiration to the revival in traditional Polynesian canoe building and voyaging, which was essentially extinct in many parts of the Pacific. \n\nIn 1976, Lewis joined Polynesian Voyaging Society's first experimental voyage from Hawaii to Tahiti on \"Hokule'a\". But this voyage was marred by a clash of egos between David and the Hawaiian navigators. Nevertheless, the team successfully navigated using traditional methods to Tahiti. Lewis departed from Hokule'a in Tahiti and went on to work in his own research. \n\nAlong with Dr. Marianne (Mimi) George, he proposed that original Polynesian navigation is still alive in the Polynesian outlier Taumako.\n\nLewis’ next adventure in 1972 was an attempt at circumnavigating Antarctica single-handed. For this he acquired a small steel yacht, named \"Ice Bird\". Facing treacherous conditions in the Southern Ocean after departing, Lewis was not heard from for 13 weeks but eventually managed to sail the \"Ice Bird\" to the Antarctic Peninsula under a jury rig after dismasting. Lewis was rescued by personnel from the Antarctic research outpost Palmer Station, who subsequently repaired the Ice Bird while Lewis spent the Antarctic winter in Australia.\n\nAfter returning eight months later, Lewis left Palmer station to complete the voyage, but that very same day was caught in a heavy ice field and had to be towed to open water by the R.V. Hero. Later, Lewis capsized again and eventually brought the boat to Cape Town, South Africa. Edited aspects of these events are described in his bestseller book, \"Ice Bird\".\nHis son, Barry, sailed the yacht back to Sydney from South Africa where it underwent extensive work to prevent further corrosion. In 1982, Dr. Lewis donated \"Ice Bird\" to the Powerhouse Museum in Sydney, Australia.\n\nAfter the \"Ice Bird\" voyage, Lewis was involved in setting up the Oceanic Research Foundation with the aim of sending private expeditions to the Antarctic. In a 17.4 metre (57 ft) Alan Payne designed steel yacht named \"Solo\" with seven other crew, Lewis made a summer expedition to Antarctica and wintered over there, 1977-78. Lewis spent some of his later years conducting research into traditional navigation techniques of the Inuit on the Bering Strait region. One obituary said of Lewis that he “always brought his crews home intact. He was a typical Polynesian sailor, getting into trouble through haste and neglect, then, with near superhuman courage and seamanship, fighting his way out of it.”\n\nFollowing this, he retired to New Zealand to write his autobiography, \"Shapes on the Wind\"; one of 12 books he wrote. In recognition of his various academic, adventure, sailing and anthropological endeavours, he was made a Distinguished Companion of the New Zealand Order of Merit. He finally retired to Australia, and died at Gympie, Queensland.\n\n\n"}
{"id": "11003959", "url": "https://en.wikipedia.org/wiki?curid=11003959", "title": "Eastern miombo woodlands", "text": "Eastern miombo woodlands\n\nThe Eastern miombo woodlands (AT0706) are an ecoregion of grassland and woodland in southern Tanzania and northern Mozambique.\n\nThese species-rich savanna ecosystems cover wide areas of gentle hills and low valleys containing rivers and dambo wetlands. The region is located on the East African Plateau, extending from inland south-eastern Tanzania to cover the northern half of Mozambique, with small areas in neighbouring Malawi. They are a section of the belt of miombo woodland that crosses Africa south of the Congo rain forests and the savannas of East Africa. The ecoregion covers an area of . It is bounded by the Northern and Southern Zanzibar-Inhambane coastal forest mosaic to the east along the Indian Ocean, and by the Zambezian and mopane woodlands in the Zambezi lowlands to the southwest, and by Lake Malawi to the west. To the north and northwest, the forested Eastern Arc Mountains separate the eastern miombo woodlands from the Southern Acacia-Commiphora bushlands and thickets of central Tanzania.\n\nThe region has a hot, tropical climate with a wet summer from November to March and a long winter drought. The woodlands are vulnerable to fire, particularly at the start of the summer.\n\nThe predominant tree is miombo (\"Brachystegia\" spp.), along with Baikiaea woodland.\n\nDespite the low rainfall and relatively nutrient-poor soil the woodland is home to many species. The miombo and other vegetation in and around the region have historically a variety of food and cover for several miombo specialist endemic bird and lizard species as well as more widespread mammals including herds of African elephant \"(Loxodonta africana)\", giraffe, Burchell's zebra \"(Equus burchelli)\", wildebeest \"(Connochaetes taurinus)\" and hippopotamus \"(Hippopotamus amphibius)\" and antelopes including greater kudu \"(Tragelaphus strepsiceros)\", eland \"(Taurotragus oryx)\", impala \"(Aepyceros melampus)\", Roosevelt sable antelope \"(Hippotragus niger)\" and Lichtenstein's hartebeest \"(Sigmoceros lichtensteinii)\" (Campbell 1996). The roan antelope is mysteriously absent from the Eastern Miombo woodlands, and the long dry season and poor soils do not support the large herds of herbivores found further north in Tanzania. Large carnivores in the region include lion (\"Panthera leo\"), leopard (\"Panthera pardus\"), cheetah (\"Acinonyx jubatus\"), spotted hyena (\"Crocuta crocuta\") and side-striped jackal (\"Canis adustus\"). The African wild dog (\"Lycaon pictus\") population of the Selous Game Reserve is the largest known population on the continent\n\nThe region is also rich in bird life, including the near-endemic Stierling's woodpecker as well as other species like vultures, fish eagle, kingfisher, yellow billed stork and plover.\n\nReptiles include common crocodiles while the endemics are the two sub-species of the spotted flat lizard, and the chameleon \"Chamaeleon tornieri\" (although the validity of this last species classification has been questioned).\n\nThe ecoregion is thinly populated by humans, partly due to tsetse fly and the Mozambique Civil War, but the miombo woodlands are important to the livelihoods of the rural people, who depend on the resources available from the woodland. The wide variety of species provides non-timber products such as fruits, honey, fodder for livestock and fuelwood. In Tanzania however a large portion of the ecoregion is covered by Selous Game Reserve, the largest protected area in Africa. There are areas of Eastern Miombo woodland in Tanzania south of Selous, in the regions of Ruvuma and Lindi. In Mozambique the region is contained within the sparsely populated Niassa and the inland areas of Cabo Delgado, Nampula and Zambezia.\n\nSelous Game Reserve is a major element of conservation in this ecoregion along with Niassa Reserve and other national parks in Mozambique, which suffered during the civil war and are in a state of reconstruction.\n\nEven outside protected areas the woodlands have remained comparatively intact due to the sparse human population. However the woodlands are being slowly cleared for farmland and pasture throughout. There is little commercial logging except for the African blackwood (Dalbergia melanoxylon), whose timber is highly valuable. Poaching of elephant and rhino are a threat, especially in Mozambique.\n\nMany areas of miombo woodland are still managed in traditional ways, with slash and burn farming systems dominating, but in some areas alternative land management practices are being promoted. One such example is the N'hambita Community Carbon Project in the Sofala Province of Mozambique. This was developed as a result of the increasing concern about global climate change, and the recent evolution of carbon markets, paired with a need for poverty reduction and alternative livelihoods where rural communities lack the resources to prevent environmental degradation. These carbon markets are part of the 'Payments for Ecosystem services' (PES) system and in this case finance from the sale of carbon offsets is used to incentivise and sustain activities which increase carbon sequestration and protect existing carbon stocks in forests.\n\nThe N'hambita project was launched in 2003 as a collaboration between the environmental company Envirotrade Ltd. and the University of Edinburgh. To date (Jan 2009) the project has engaged 1350 farmers or 'producer's in agroforestry and woodland restoration and conservation activities. Those involved have benefited from staged payments, continued technical support and have been encouraged to become involved in other micro-finance initiatives, such as beekeeping and carpentry, using miombo tree species planted within the project.\n\nThe Plan Vivo System is used, which was pioneered, and has run successfully in Mexico for over 10 years in the Scolel Te project. Plan Vivo projects are registered and reviewed under standards developed by the Plan Vivo Foundation. Producers who sign up to the scheme must agree to manage their land according to their \"plan vivo\", a long term land management plan evaluated and registered by the local project manager.\n\n\n"}
{"id": "9632", "url": "https://en.wikipedia.org/wiki?curid=9632", "title": "Ecosystem", "text": "Ecosystem\n\nAn ecosystem is a community made up of living organisms and nonliving components such as air, water, and mineral soil. Ecosystems can be studied in two different ways. They can be thought of as interdependent collections of plants and animals, or as structured systems and communities governed by general rules. The living (biotic) and non-living (abiotic) components interact through nutrient cycles and energy flows. Ecosystems include interactions among organisms, and between organisms and their environment. Ecosystems can be of any size but each ecosystem has a specific, limited space. Some scientists view the entire planet as one ecosystem.\n\nEnergy, water, nitrogen and soil minerals are essential abiotic components of an ecosystem. The energy used by ecosystems comes primarily from the sun, via photosynthesis. Photosynthesis uses energy from the sun and also captures carbon dioxide from the atmosphere. Animals also play an important role in the movement of matter and energy through ecosystems. They influence the amount of plant and microbial biomass that lives in the system. As organic matter dies, carbon is released back into the atmosphere. This process also facilitates nutrient cycling by converting nutrients stored in dead biomass back to a form that can be used again by plants and other microbes. \n\nEcosystems are controlled by both external and internal factors. External factors such as climate, the parent material that forms the soil, topography and time each affect ecosystems. However, these external factors are not themselves influenced by the ecosystem. Ecosystems are dynamic: they are subject to periodic disturbances and are often in the process of recovering from past disturbances and seeking balance. Internal factors are different: They not only control ecosystem processes but are also controlled by them. Another way of saying this is that internal factors are subject to feedback loops.\n\nHumans operate within ecosystems and can influence both internal and external factors. Global warming is an alleged example of a cumulative effect of human activities. Ecosystems provide benefits, called \"ecosystem services\", which people depend on for their livelihood. Ecosystem management is more efficient than trying to manage individual species.\n\nThere is no single definition of what constitutes an ecosystem. German ecologist Ernst-Detlef Schulze and coauthors defined an ecosystem as an area which is \"uniform regarding the biological turnover, and contains all the fluxes above and below the ground area under consideration.\" They explicitly reject Gene Likens' use of entire river catchments as \"too wide a demarcation\" to be a single ecosystem, given the level of heterogeneity within such an area. Other authors have suggested that an ecosystem can encompass a much larger area, even the whole planet. Schulze and coauthors also rejected the idea that a single rotting log could be studied as an ecosystem because the size of the flows between the log and its surroundings are too large, relative to the proportion cycles within the log. Philosopher of science Mark Sagoff considers the failure to define \"the kind of object it studies\" to be an obstacle to the development of theory in ecosystem ecology.\n\nEcosystems can be studied in a variety of ways. Those include theoretical studies or more practical studies that monitor specific ecosystems over long periods of time or look at differences between ecosystems to better understand how they work. Some studies involve experimenting with direct manipulation of the ecosystem. Studies can be carried out at a variety of scales, ranging from whole-ecosystem studies to studying or mesocosms (simplified representations of ecosystems). American ecologist Stephen R. Carpenter has argued that microcosm experiments can be \"irrelevant and diversionary\" if they are not carried out in conjunction with field studies done at the ecosystem scale. Microcosm experiments often fail to accurately predict ecosystem-level dynamics.\n\nThe Hubbard Brook Ecosystem Study started in 1963 to study the White Mountains in New Hampshire. It was the first successful attempt to study an entire watershed as an ecosystem. The study used stream chemistry as a means of monitoring ecosystem properties, and developed a detailed biogeochemical model of the ecosystem. Long-term research at the site led to the discovery of acid rain in North America in 1972. Researchers documented the depletion of soil cations (especially calcium) over the next several decades.\n\nTerrestrial ecosystems (found on land) and aquatic ecosystems (found in water) are concepts related to ecosystems. Aquatic ecosystems are split into marine ecosystems and freshwater ecosystems.\n\nEcosystems are controlled both by external and internal factors. External factors, also called state factors, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. The most important of these is climate. Climate determines the biome in which the ecosystem is embedded. Rainfall patterns and seasonal temperatures influence photosynthesis and thereby determine the amount of water and energy available to the ecosystem. \n\nParent material determines the nature of the soil in an ecosystem, and influences the supply of mineral nutrients. Topography also controls ecosystem processes by affecting things like microclimate, soil development and the movement of water through a system. For example, ecosystems can be quite different if situated in a small depression on the landscape, versus one present on an adjacent steep hillside.\n\nOther external factors that play an important role in ecosystem functioning include time and potential biota. Similarly, the set of organisms that can potentially be present in an area can also significantly affect ecosystems. Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present. The introduction of non-native species can cause substantial shifts in ecosystem function.\n\nUnlike external factors, internal factors in ecosystems not only control ecosystem processes but are also controlled by them. Consequently, they are often subject to feedback loops. While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. Other factors like disturbance, succession or the types of species present are also internal factors. \n\nPrimary production is the production of organic matter from inorganic carbon sources. This mainly occurs through photosynthesis. The energy incorporated through this process supports life on earth, while the carbon makes up much of the organic matter in living and dead biomass, soil carbon and fossil fuels. It also drives the carbon cycle, which influences global climate via the greenhouse effect.\n\nThrough the process of photosynthesis, plants capture energy from light and use it to combine carbon dioxide and water to produce carbohydrates and oxygen. The photosynthesis carried out by all the plants in an ecosystem is called the gross primary production (GPP). About 48–60% of the GPP is consumed in plant respiration. \n\nThe remainder, that portion of GPP that is not used up by respiration, is known as the net primary production (NPP). \n\nEnergy and carbon enter ecosystems through photosynthesis, are incorporated into living tissue, transferred to other organisms that feed on the living and dead plant matter, and eventually released through respiration.\n\nThe carbon and energy incorporated into plant tissues (net primary production) is either consumed by animals while the plant is alive, or it remains uneaten when the plant tissue dies and becomes detritus. In terrestrial ecosystems, roughly 90% of the net primary production ends up being broken down by decomposers. The remainder is either consumed by animals while still alive and enters the plant-based trophic system, or it is consumed after it has died, and enters the detritus-based trophic system. \n\nIn aquatic systems, the proportion of plant biomass that gets consumed by herbivores is much higher.\nIn trophic systems photosynthetic organisms are the primary producers. The organisms that consume their tissues are called primary consumers or secondary producers—herbivores. Organisms which feed on microbes (bacteria and fungi) are termed microbivores. Animals that feed on primary consumers—carnivores—are secondary consumers. Each of these constitutes a trophic level. \n\nThe sequence of consumption—from plant to herbivore, to carnivore—forms a food chain. Real systems are much more complex than this—organisms will generally feed on more than one form of food, and may feed at more than one trophic level. Carnivores may capture some prey which are part of a plant-based trophic system and others that are part of a detritus-based trophic system (a bird that feeds both on herbivorous grasshoppers and earthworms, which consume detritus). Real systems, with all these complexities, form food webs rather than food chains.\n\nEcosystem ecology studies \"the flow of energy and materials through organisms and the physical environment\". It seeks to understand the processes which govern the stocks of material and energy in ecosystems, and the flow of matter and energy through them. The study of ecosystems can cover 10 orders of magnitude, from the surface layers of rocks to the surface of the planet.\n\nThe carbon and nutrients in dead organic matter are broken down by a group of processes known as decomposition. This releases nutrients that can then be re-used for plant and microbial production and returns carbon dioxide to the atmosphere (or water) where it can be used for photosynthesis. In the absence of decomposition, the dead organic matter would accumulate in an ecosystem, and nutrients and atmospheric carbon dioxide would be depleted. Approximately 90% of terrestrial net primary production goes directly from plant to decomposer.\n\nDecomposition processes can be separated into three categories—leaching, fragmentation and chemical alteration of dead material. \n\nAs water moves through dead organic matter, it dissolves and carries with it the water-soluble components. These are then taken up by organisms in the soil, react with mineral soil, or are transported beyond the confines of the ecosystem (and are considered lost to it). Newly shed leaves and newly dead animals have high concentrations of water-soluble components and include sugars, amino acids and mineral nutrients. Leaching is more important in wet environments and much less important in dry ones.\n\nFragmentation processes break organic material into smaller pieces, exposing new surfaces for colonization by microbes. Freshly shed leaf litter may be inaccessible due to an outer layer of cuticle or bark, and cell contents are protected by a cell wall. Newly dead animals may be covered by an exoskeleton. Fragmentation processes, which break through these protective layers, accelerate the rate of microbial decomposition. Animals fragment detritus as they hunt for food, as does passage through the gut. Freeze-thaw cycles and cycles of wetting and drying also fragment dead material.\n\nThe chemical alteration of the dead organic matter is primarily achieved through bacterial and fungal action. Fungal hyphae produce enzymes which can break through the tough outer structures surrounding dead plant material. They also produce enzymes which break down lignin, which allows them access to both cell contents and to the nitrogen in the lignin. Fungi can transfer carbon and nitrogen through their hyphal networks and thus, unlike bacteria, are not dependent solely on locally available resources.\n\nDecomposition rates vary among ecosystems. The rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture, and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself. Temperature controls the rate of microbial respiration; the higher the temperature, the faster microbial decomposition occurs. It also affects soil moisture, which slows microbial growth and reduces leaching. Freeze-thaw cycles also affect decomposition—freezing temperatures kill soil microorganisms, which allows leaching to play a more important role in moving nutrients around. This can be especially important as the soil thaws in the spring, creating a pulse of nutrients which become available.\n\nDecomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth.\n\nEcosystems continually exchange energy and carbon with the wider environment. Mineral nutrients, on the other hand, are mostly cycled back and forth between plants, animals, microbes and the soil. Most nitrogen enters ecosystems through biological nitrogen fixation, is deposited through precipitation, dust, gases or is applied as fertilizer. \n\nSince most terrestrial ecosystems are nitrogen-limited, nitrogen cycling is an important control on ecosystem production.\n\nUntil modern times, nitrogen fixation was the major source of nitrogen for ecosystems. Nitrogen-fixing bacteria either live symbiotically with plants or live freely in the soil. The energetic cost is high for plants which support nitrogen-fixing symbionts—as much as 25% of gross primary production when measured in controlled conditions. Many members of the legume plant family support nitrogen-fixing symbionts. Some cyanobacteria are also capable of nitrogen fixation. These are phototrophs, which carry out photosynthesis. Like other nitrogen-fixing bacteria, they can either be free-living or have symbiotic relationships with plants. Other sources of nitrogen include acid deposition produced through the combustion of fossil fuels, ammonia gas which evaporates from agricultural fields which have had fertilizers applied to them, and dust. Anthropogenic nitrogen inputs account for about 80% of all nitrogen fluxes in ecosystems.\n\nWhen plant tissues are shed or are eaten, the nitrogen in those tissues becomes available to animals and microbes. Microbial decomposition releases nitrogen compounds from dead organic matter in the soil, where plants, fungi, and bacteria compete for it. Some soil bacteria use organic nitrogen-containing compounds as a source of carbon, and release ammonium ions into the soil. This process is known as nitrogen mineralization. Others convert ammonium to nitrite and nitrate ions, a process known as nitrification. Nitric oxide and nitrous oxide are also produced during nitrification. Under nitrogen-rich and oxygen-poor conditions, nitrates and nitrites are converted to nitrogen gas, a process known as denitrification.\n\nOther important nutrients include phosphorus, sulfur, calcium, potassium, magnesium and manganese. Phosphorus enters ecosystems through weathering. As ecosystems age this supply diminishes, making phosphorus-limitation more common in older landscapes (especially in the tropics). Calcium and sulfur are also produced by weathering, but acid deposition is an important source of sulfur in many ecosystems. Although magnesium and manganese are produced by weathering, exchanges between soil organic matter and living cells account for a significant portion of ecosystem fluxes. Potassium is primarily cycled between living cells and soil organic matter.\n\nBiodiversity plays an important role in ecosystem functioning. The reason for this is that ecosystem processes are driven by the number of species in an ecosystem, the exact nature of each individual species, and the relative abundance organisms within these species. Ecosystem processes are broad generalizations that actually take place through the actions of individual organisms. The nature of the organisms—the species, functional groups and trophic levels to which they belong—dictates the sorts of actions these individuals are capable of carrying out and the relative efficiency with which they do so. \n\nEcological theory suggests that in order to coexist, species must have some level of limiting similarity—they must be different from one another in some fundamental way, otherwise one species would competitively exclude the other. Despite this, the cumulative effect of additional species in an ecosystem is not linear—additional species may enhance nitrogen retention, for example, but beyond some level of species richness, additional species may have little additive effect. \n\nThe addition (or loss) of species which are ecologically similar to those already present in an ecosystem tends to only have a small effect on ecosystem function. Ecologically distinct species, on the other hand, have a much larger effect. Similarly, dominant species have a large effect on ecosystem function, while rare species tend to have a small effect. Keystone species tend to have an effect on ecosystem function that is disproportionate to their abundance in an ecosystem. Similarly, an ecosystem engineer is any organism that creates, significantly modifies, maintains or destroys a habitat.\n\nEcosystems are dynamic entities. They are subject to periodic disturbances and are in the process of recovering from some past disturbance. When a perturbation occurs, an ecoystem responds by moving away from its initial state. The tendency of an ecosystem to remain close to its equilibrium state, despite that disturbance, is termed its resistance. On the other hand, the speed with which it returns to its initial state after disturbance is called its resilience. Time plays a role in the development of soil from bare rock and the recovery of a community from disturbance.\n\nFrom one year to another, ecosystems experience variation in their biotic and abiotic environments. A drought, an especially cold winter and a pest outbreak all constitute short-term variability in environmental conditions. Animal populations vary from year to year, building up during resource-rich periods and crashing as they overshoot their food supply. These changes play out in changes in net primary production decomposition rates, and other ecosystem processes. Longer-term changes also shape ecosystem processes—the forests of eastern North America still show legacies of cultivation which ceased 200 years ago, while methane production in eastern Siberian lakes is controlled by organic matter which accumulated during the Pleistocene.\n\nDisturbance also plays an important role in ecological processes. F. Stuart Chapin and coauthors define disturbance as \"a relatively discrete event in time and space that alters the structure of populations, communities, and ecosystems and causes changes in resources availability or the physical environment\". This can range from tree falls and insect outbreaks to hurricanes and wildfires to volcanic eruptions. Such disturbances can cause large changes in plant, animal and microbe populations, as well soil organic matter content. Disturbance is followed by succession, a \"directional change in ecosystem structure and functioning resulting from biotically driven changes in resources supply.\"\n\nThe frequency and severity of disturbance determine the way it affects ecosystem function. A major disturbance like a volcanic eruption or glacial advance and retreat leave behind soils that lack plants, animals or organic matter. Ecosystems that experience such disturbances undergo primary succession. A less severe disturbance like forest fires, hurricanes or cultivation result in secondary succession and a faster recovery. More severe disturbance and more frequent disturbance result in longer recovery times. \n\nClassifying ecosystems into ecologically homogeneous units is an important step towards effective ecosystem management. There is no single, agreed-upon way to do this. A variety of systems exist, based on vegetation cover, remote sensing, and bioclimatic classification systems. \n\nEcological land classification is a cartographical delineation or regionalisation of distinct ecological areas, identified by their geology, topography, soils, vegetation, climate conditions, living species, habitats, water resources, and sometimes also anthropic factors.\n\nHuman activities are important in almost all ecosystems. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.\n\nEcosystems provide a variety of goods and services upon which people depend. Ecosystem goods include the \"tangible, material products\" of ecosystem processes such as food, construction material, medicinal plants. They also include less tangible items like tourism and recreation, and genes from wild plants and animals that can be used to improve domestic species. \n\nEcosystem services, on the other hand, are generally \"improvements in the condition or location of things of value\". These include things like the maintenance of hydrological cycles, cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research. While ecosystem goods have traditionally been recognized as being the basis for things of economic value, ecosystem services tend to be taken for granted.\n\nWhen natural resource management is applied to whole ecosystems, rather than single species, it is termed ecosystem management. Although definitions of ecosystem management abound, there is a common set of principles which underlie these definitions. A fundamental principle is the long-term sustainability of the production of goods and services by the ecosystem; \"intergenerational sustainability [is] a precondition for management, not an afterthought\". \n\nWhile ecosystem management can be used as part of a plan for wilderness conservation, it can also be used in intensively managed ecosystems (see, for example, agroecosystem and close to nature forestry).\n\nAs human population and per capita consumption grow, so do the resource demands imposed on ecosystems and the effects of the human ecological footprint. Natural resources are vulnerable and limited. The environmental impacts of anthropogenic actions are becoming more apparent. Problems for all ecosystems include: environmental pollution, climate change and biodiversity loss. For terrestrial ecosystems further threats include air pollution, soil degradation, and deforestation. For aquatic ecosystems threats include also unsustainable exploitation of marine resources (for example overfishing of certain species), marine pollution, microplastics pollution, water pollution, and building on coastal areas. \n\nSociety is increasingly becoming aware that ecosystem services are not only limited but also that they are threatened by human activities. The need to better consider long-term ecosystem health and its role in enabling human habitation and economic activity is urgent. To help inform decision-makers, many ecosystem services are being assigned economic values, often based on the cost of replacement with anthropogenic alternatives. The ongoing challenge of prescribing economic value to nature, for example through biodiversity banking, is prompting transdisciplinary shifts in how we recognize and manage the environment, social responsibility, business opportunities, and our future as a species.\n\nThe term \"ecosystem\" was first used in 1935 in a publication by British ecologist Arthur Tansley. Tansley devised the concept to draw attention to the importance of transfers of materials between organisms and their environment. He later refined the term, describing it as \"The whole system, ... including not only the organism-complex, but also the whole complex of physical factors forming what we call the environment\". Tansley regarded ecosystems not simply as natural units, but as \"mental isolates\". Tansley later defined the spatial extent of ecosystems using the term ecotope.\n\nG. Evelyn Hutchinson, a limnologist who was a contemporary of Tansley's, combined Charles Elton's ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky. As a result, he suggested that mineral nutrient availability in a lake limited algal production. This would, in turn, limit the abundance of animals that feed on algae. Raymond Lindeman took these ideas further to suggest that the flow of energy through a lake was the primary driver of the ecosystem. Hutchinson's students, brothers Howard T. Odum and Eugene P. Odum, further developed a \"systems approach\" to the study of ecosystems. This allowed them to study the flow of energy and material through ecological systems.\n\n\n\n"}
{"id": "40256773", "url": "https://en.wikipedia.org/wiki?curid=40256773", "title": "Energy in Israel", "text": "Energy in Israel\n\nMost energy in Israel comes from hydrocarbon fuels. The country's total primary energy demand is significantly higher than its total primary energy production, relying heavily on imports to meet its energy needs. Total primary energy consumption was in 2011, or 24.5 Mtoe (million tonne of oil equivalent).\n\nElectricity consumption in Israel was 59.83 TWh in 2014, while production was 64.44 TWh, with net exports of 4.94 TWh. The installed generating capacity was about 16.25 GW in the same year, almost all from hydrocarbon fuel plants, mostly coal and gas fueled. Renewable energy accounted for a minor share of electricity production, with a small photovoltaic installed capacity. However, there are a total of over 1.3 million solar water heaters installed as a result of mandatory solar water heating regulations.\n\nThroughout Israel's history, securing the energy supply had been a major concern of Israeli policymakers. Today, Israel Electric Corporation, which traces its history to 1923, is the main electricity generator and distributor in Israel. Petroleum exploration began in 1947 on a surface feature in the Heletz area in the southern coastal plain. The first discovery, Heletz-I, was completed in 1955, followed by the discovery and development of a few small wells in Kokhav, Brur, Ashdod and Zuk Tamrur in 1957. The combined Heletz-Brur-Kokhav field produced a total of 17.2 million barrels, a negligible amount compared with national consumption. Since the early 1950s, 480 oil and gas wells, land and offshore were drilled in Israel, most of which did not result in commercial success. In 1958–1961, several small gas fields were discovered in the southern Judean desert. From the Six-Day War until the Egyptian Separation Treaty in 1975, Israel produced large quantities of petroleum from the Abu Rodes oil field in Sinai.\n\nIn 1951, the Arab states accused American oil interests in Saudi Arabia of selling oil to Central American governments who circumvented the Arab blockade against Israel by selling the oil back to the refinery in Haifa.\n\nSince Israel’s creation in 1948, it has been dependent on energy imports from other countries. Specifically, Israel produced 7 billion cubic meters of natural gas in 2013, and imported 720 million cubic meters in 2011. \nHistorically, Israel has imported natural gas through the Arish-Ashkelon pipeline from Egypt. \nEgypt is the second-largest natural gas producer in North Africa. In 2005 Egypt signed a 2.5 billion-dollar deal to supply Israel with 57 billion cubic feet of gas per year for fifteen years. Under this arrangement, Egypt supplies 40 percent of Israel's natural gas demand. \nThe Israeli Electric Corporation (IEC) controls more than 95% of the electricity sector in Israel, and controls production, distribution, and transmission of electricity. The IEC has a natural gas distribution law which regulates the distribution of natural gas in Israel to empower market competition. \n\nThe discoveries of the Tamar gas field in 2009 and the Leviathan gas field in 2010 off the coast of Israel were important. The natural gas reserves in these two fields (Leviathan has around 19 trillion cubic feet) could make Israel energy secure for more than 50 years. In 2013 Israel began commercial production of natural gas from the Tamar field.\n\nEnergy Minister Yuval Steinitz said “For many decades, the Arabs used the fact that they’re supplying Europe with oil and natural gas in order to try to pressure Israel...And now we will have something to balance and influence.”\n\nIn 2015, energy consumption in Israel was 52.86 TWh, or 6,562 kWh per capita. The Israel Electric Corporation (IEC) is the main producer of electricity in Israel, with a production capacity of 11,900 megawatts. In 2016, IEC's share of the electricity market was 71%.\n\nMost electricity in Israel comes from hydrocarbon fuels from the following IEC power plants:\n\nThe following power plants belong to independent power producers and, although connected to the IEC’s distribution grid, are not operated by the IEC:\n\nRenewable energy in Israel is produced in solar fields, such as Energix Renewable Energies' Neot Hovav and Ketura Sun and from biogas (11 MW), hydroelectricity (6.6 MW) and wind power in the Golan Heights Wind Farm (6 MW). \n\nDespite getting more than 300 days of sunshine per year, , less than 2% of Israel‘s electricity comes from renewable sources. According to the Green Energy Association of Israel, the number of solar energy companies in the country has fallen from about 130 in 2010 to 60 in 2015. The Association says that discoveries of large amounts of natural gas since 2009 have dimmed the government's interest in renewable energy. Officially, however, Israel maintains it will achieve its goal of 10% of its energy from renewable sources by 2020.\n\nAlthough there is a fully functional heavy water nuclear reactor at Negev Nuclear Research Center, Israel has no nuclear power plants. However, in January 2007, Israeli Infrastructure Minister Binyamin Ben-Eliezer said his country should consider producing nuclear power for civilian purposes.\n\nAs a result of the nuclear emergencies at Japan's Fukushima I Nuclear Power Plant, Prime Minister Benjamin Netanyahu said on March 17, 2011, \"I don't think we're going to pursue civil nuclear energy in the coming years.\"\n\nIsrael is one of the world leaders in the use of solar thermal energy per capita.\nAs of the early 1990s, all new residential buildings were required by the government to install solar water-heating systems, and Israel's National Infrastructure Ministry estimates that solar panels for water-heating satisfy 4% of the country's total energy demand. \nIsrael and Cyprus are the per-capita leaders in the use of solar hot water systems with over 90% of homes using them.\nThe Ministry of National Infrastructures estimates solar water heating saves Israel of oil a year.\n\n"}
{"id": "33757693", "url": "https://en.wikipedia.org/wiki?curid=33757693", "title": "Energy in Slovakia", "text": "Energy in Slovakia\n\nEnergy in Slovakia describes energy and electricity production, consumption and import in Slovak Republic. Energy policy of Slovakia will describe the energy policy in the politics of Slovakia more in detail. Electricity sector in Slovakia is the main article of electricity in Slovakia.\n\nPrimary energy use in Slovakia was 194 TWh and 36 TWh per million persons in 2009.\n\nSlovakia is net energy importer. The share of import was 63% of the primary energy use in 2009. (IEA Key stats 2010 pages 52)\n\nSlovnaft is the largest oil refinery in Slovakia.\n\nSlovenský plynárenský priemysel (Slovak Gas Industry) is the main natural gas supplier in Slovakia.\n\nFour operating reactors in two power plants (Bohunice Nuclear Power Plant and Mochovce Nuclear Power Plant) produced 14,42 TW·h in 2014. Three other reactors at Bohunice are in permanent shutdown. Two other reactors are under construction at Mochovce.\n\nIn the end of 2010 wind power capacity in Slovakia 3 MW was the lowest of the EU countries, except Malta and Slovenia. EWEA’s targets are to produce 14-17% of the EU’s electricity with wind power in 2020, and save €28 billion a year in fuel costs in Europe.\n\nEmissions of carbon dioxide in total, per capita in 2007 were 6.8 tons CO compared to EU 27 average 7.9 tons . Emission change 2007/1990 (%) was -35.1%. In Europe in 2007 the Slovak emissions of carbon dioxide per capita (6.8 tons CO) were higher than in Hungary 5.4, Sweden 5.1, Portugal 5.2 or Switzerland 5.6 and lower than in Czech Republic 11.8, Luxembourg 22.4, Finland 12.2, Netherlands 11.1, Germany 9.7 or Ireland 10.1\n\n1990 emissions were 74 Mt . The Kyoto protocol target is reduction of 6 Mt (-8%).\n\n"}
{"id": "11588698", "url": "https://en.wikipedia.org/wiki?curid=11588698", "title": "Energy policy of China", "text": "Energy policy of China\n\nEnsuring adequate energy supply to sustain economic growth has been a core concern of the Chinese government since 1949. \nPrimary energy use in China was 26,250 TWh and 20 TWh per million persons in 2009.\nAccording to IEA the primary energy use grew 40% and electricity use 70% from 2004 to 2009.\n\nThe energy import was three times bigger in 2009 compared to 2004. \nThe share of energy import of the primary energy use was 12% in 2009.\n\nThe country is the world's largest emitter of greenhouse gases, \nwith carbon emission growth of 44% from 2004 to 2009.\nHowever, from 2010 to 2015 China reduced energy consumption per unit of GDP by 18%, and CO emissions per unit of GDP by 20%. On a per-capita basis, it was the world's 42nd largest emitter of greenhouse gases in 2014.\n\nChina is also the world's largest renewable energy producer.\nChina is the largest producer of hydroelectricity, solar power and wind power in the world.\n\nEnergy policymaking in China is largely decentralised. \nThere has been no Ministry of Energy since it was dissolved in 1993, and it was only in 2002–2003 that serious governmental dialogue on re-centralising energy administration began. \nChina’s energy industries are governed by a number of ministries and commissions, and companies with varying levels of power and influence, such as the China National Petroleum Corporation (CNPC) and the China Petroleum and Chemical Corporation (Sinopec). \nBoth of these companies originally comprised one ministry before being converted to state companies in the 1980s. \nThey have retained the same hierarchical rank as government ministries, putting them higher than the sub-ministerial bureau charged with supervising them. \nThese complicated organisational structures and interrelationships complicate any efforts to change the way energy is priced and billed.\n\nOn June 19, 2007, the Netherlands Environmental Assessment Agency announced that a preliminary study had indicated that China's greenhouse gas emissions for 2006 had exceeded those of the United States for the first time. The agency calculated that China’s CO emissions from fossil fuels increased by 9% in 2006, while those of the United States fell by 1.4%, compared to 2005. The study used energy and cement production data from British Petroleum which they believed to be 'reasonably accurate', while warning that statistics for rapidly changing economies such as China are less reliable than data on OECD countries.\n\nThe Initial National Communication on Climate Change of the People's Republic of China calculated that carbon dioxide emissions in 2004 had risen to approximately 5.05 billion metric tons, with total greenhouse gas emissions reaching about 6.1 billion metric tons carbon dioxide equivalent.\n\nIn 2002, China ranked 2nd (after the United States) in the list of countries by carbon dioxide emissions, with emissions of 3.3 billion metric tons, representing 14.5% of the world total. However, due to its huge population size (the largest in the world), it only ranked 43 in the list of countries by carbon dioxide emissions per capita, with emissions of 7.6 metric tons per person (compared to 16.4 metric tons per person in the United States). In addition, it has been estimated that around a third of China's carbon emissions in 2005 were due to manufacturing exported goods.\n\nSince 2006, China has overtaken the USA, producing 8% more emissions than the US to become the worlds biggest emitter of pollution.\n\nIn the industrial sector, six industries – electricity generation, steel, non-ferrous metals, construction materials, oil processing and chemicals – account for nearly 70% of energy use.\n\nIn the construction materials sector, China produced about 44% of the world's cement in 2006. Cement production produces more carbon emissions than any other industrial process, accounting for around 4% of global carbon emissions.\n\nChina has been taking action on climate change for some years, with the publication on June 4, 2007 of China's first National Action Plan on Climate Change, and in that year China became the first developing country to publish a national strategy addressing global warming. The plan did not include targets for carbon dioxide emission reductions, but it has been estimated that, if fully implemented, China's annual emissions of greenhouse gases would be reduced by 1.5 billion tons of carbon dioxide equivalent by 2010. Other commentators, however, put the figure at 0.950 billion metric tons.\n\nThe publication of the strategy was officially announced during a meeting of the State Council, which called on governments and all sectors of the economy to implement the plan, and for the launch of a public environmental protection awareness campaign.\n\nThe National Action Plan includes increasing the proportion of electricity generation from renewable energy sources and from nuclear power, increasing the efficiency of coal-fired power stations, the use of cogeneration, and the development of coal-bed and coal-mine methane.\n\nIn addition, the one child policy in China has successfully slowed down the population increase, preventing 300 million births, the equivalent of 1.3 billion tons of CO emissions based on average world per capita emissions of 4.2 tons at 2005 level.\n\nIn January 2012, as part of its 12th Five-year Plan, China published a report \"12th Five-year Plan on Greenhouse Emission Control (guofa [2011] No. 41)\", which establishes goals of reducing carbon intensity by 17% by 2015, compared with 2010 levels and raising energy consumption intensity by 16%, relative to GDP. More demanding targets were set for the most developed regions and those with most heavy industry, including Guangdong, Shanghai, Jiangsu, Zhejiang and Tianjin. China also plans to meet 11.4% of its primary energy requirements from non-fossil sources by 2015.\n\nThe plan will also pilot the construction of a number of low-carbon Development Zones and low-carbon residential communities, which it hopes will result in a cluster effect among businesses and consumers.\n\nIn addition, the Government will in future include data on greenhouse emissions in its official statistics.\n\nIn a separate development, on January 13, 2012, the National Development and Reform Commission announced that the cities of Beijing, Tianjin, Shanghai, Chongqing and Shenzhen, and the provinces of Hubei and Guangdong would become the first to participate in a pilot carbon cap and trade scheme that would operate in a similar way to the European Union Emission Trading Scheme. The development follows an unsuccessful experiment with voluntary carbon exchanges that was set up in 2009 in Beijing, Shanghai and Tianjin.\n\nCoal remains the foundation of the Chinese energy system, covering close to 70 percent of the country’s primary energy needs and representing 80 percent of the fuel used in electricity generation. Coal has grown rapidly in importance in recent years along with the rising demand for electricity. In 2011, according to the IEA, Chinese coal production was equivalent to 3,576 Mt*0.522 toe/Mt*11.630 TWh/toe = 21,709 TWh. Assuming the same caloric value for the imported coal, the net coal energy available would be 22,784 TWh. Equally calculated available coal was 17,000 TWh in 2008 and 22,800 TWh in 2011, with increase of 5,800 TWh in three years. Total renewable energy in China was 3,027 TWh in 2008 and 2,761 TWh in 2005, with increase of 266 TWh in three years. Same period from 2005 to 2008 annual coal use increased 3,341 TWh.\n\nChina's coal supply was in 2009 18,449 TWh which was 47% of the world coal supply.\n\nChina is the world's top coal producer and ranks third in the amounts of coal reserves. It is approximately self-sufficient in coal, with a production of 2.38 billion ton and a consumption of 2.37 billion tons in 2006.\n\nTop 10 hard and brown coal producers in 2010 (2009) were (Mt): China 3,162 (2,971), United States 997 (985), India 571 (561), Australia 420 (399), Indonesia 336 (301), Russia 324 (297), South Africa 255 (247), Kazakhstan 111 (101), Poland 134 (135) and Colombia 74 (73).\n\nIn 2010 China was second highest hard coal importer (157 Mt).\n\nChina consumes more coal than any other country. Its share of the world coal production was 48% in 2009 and 28% in 2000. Coal use in the world increased 48% from 2000 to 2009. In practice, the majority of this growth occurred in China and the rest in other parts of Asia. In China, Coal usage double between 2003 and 2007.\n\nAnalysis in 2016 shows that China's coal consumption appears to have peaked in 2014.\n\nChina's oil supply was 4,855 TWh in 2009 which represented 10% of the world's supply.\n\nAlthough China is still a major crude oil producer, it became an oil importer in the 1990s. China became dependent on imported oil for the first time in its history in 1993 due to demand rising faster than domestic production. In 2002, annual crude petroleum production was 1,298,000,000 barrels, and annual crude petroleum consumption was 1,670,000,000 barrels. In 2006, it imported 145 million tons of crude oil, accounting for 47% of its total oil consumption. By 2014 China was importing approximately 7 mil. barrels of oil per day. Three state-owned oil companies – Sinopec, CNPC, and CNOOC – dominate its domestic market.\n\nChina announced on June 20, 2008 plans to raise petrol, diesel and aviation kerosene prices. This decision appeared to reflect a need to reduce the unsustainably high level of subsidies these fuels attract, given the global trend in the price of oil.\n\nTop oil producers were in 2010: Russia 502 Mt (13%), Saudi Arabia 471 Mt (12%), US 336 Mt (8%), Iran 227 Mt (6%), China 200 Mt (5%), Canada 159 Mt (4%), Mexico 144 Mt (4%), UAE 129 Mt (3%). The world oil production increased from 2005 to 2010 1.3% and from 2009 to 2010 3.4%.\n\nChina's natural gas supply was 1,015 TWh in 2009 that was 3% of the world supply.\n\nCNPC, Sinopec, and CNOOC are all active in the upstream gas sector, as well as in LNG import, and in midstream pipelines. Branch pipelines and urban networks are run by city gas companies including China Gas Holdings, ENN Energy, Towngas China, Beijing Enterprises Holdings and Kunlun Energy.\n\nIssued by China’s State Council in September 2013, China’s Action Plan for the Prevention and Control of Air Pollution illustrates government desire to increase the share of natural gas in China’s energy mix. In May 2014 China signed a 30-year deal with Russia to deliver 38 billion cubic metres of natural gas each year.\n\nChina was top seventh in natural gas production in 2010.\n\nIn 2013, China's total annual electricity output was 5.398 trillion kWh and the annual consumption was 5.380 trillion kWh with an installed capacity of 1247 GW (all the largest in the world).\nThis is an increase from 2009, when China's total annual electricity output was 3.71465 trillion kWh, and the annual consumption was 3.6430 trillion kWh (second largest in the world). In the same year, the total installed electricity generating capacity was 874 GW. China is undertaking substantial long distance transmission projects with record breaking capacities, and has the goal of achieving an integrated nationwide grid in the period between 2015 and 2020.\n\nIn 2015, China generated 73% of its electricity from coal-fired power stations, which has been dropping from a peak of 81% in 2007. \n\nChina is the world's leading renewable energy producer, with an installed capacity of 152 GW. China has been investing heavily in the renewable energy field in recent years. In 2007, the total renewable energy investment was $12 billion USD, second only to Germany. In 2012, China invested $65.1 billion USD in clean energy (20% more than in 2011), fully 30% of the total investment by the G-20, including 25% ($31.2 billion USD) of global solar energy investment, 37% percent ($27.2 billion USD) of global wind energy investment, and 47% ($6.3 billion USD) of global investment in \"other renewable energy\" (small hydro, geothermal, marine, and biomass); 23 GW of clean generation capacity was installed.\n\nChina is also the largest producer of wind turbines and solar panels. Approximately 7% of China's energy was from renewable sources in 2006, a figure targeted to rise to 10% by 2010 and to 16% by 2020. The major renewable energy source in China is hydropower. Total hydro-electric output in China in 2009 was 615.64 TWh, constituting 16.6% of all electricity generated. The country already has the most hydro-electric capacity in the world, and the Three Gorges Dam is currently the largest hydro-electric power station in the world, with a total capacity of 22.5 GW. It has been in full operation since May 2012.\n\nIn 2012, China had 15 nuclear power units with a total electric capacity of 11 GW and total output of 54.8 billion kWh, accounting for 1.9% country's total electricity output. This rose to 17 reactors in 2013. By 2016 the number of operating nuclear reactors was 32 with 22 under construction and other dozen to start construction this year. There are plans to increase nuclear power capacity and nuclear power percentage, bringing the total electricity output to 86 GW and 4% respectively by 2020. Plans are to increase this to 200 GWe by 2030, and 400 GWe by 2050. China has set an end-of-the-Century goal 1500GWs of nuclear energy, most of this from fast reactors. China has 32 reactors under construction, the highest number in the world.\n\nFollowing the completion of the similar Township Electrification Program in 2005, the Village Electrification Program plans to provide renewable electricity to 3.5 million households in 10,000 villages by 2010. This is to be followed by full rural electrification using renewable energy by 2015.\n\nAlthough a majority of the renewable energy in China is from hydropower, other renewable energy sources are in rapid development. In 2006, a total of 10 billion US dollars had been invested in renewable energy, second only to Germany.\n\nIn 2006, 16 million tons of corn have been used to produce ethanol. However, because food prices in China rose sharply during 2007, China has decided to ban the further expansion of the corn ethanol industry.\n\nOn February 7, a spokesman for the State Forestry Administration announced that would be devoted to biofuel production. Under an agreement reached with PetroChina in January 2007, 400 square kilometres of \"Jatropha curcas\" is to be grown for biodiesel production. Local governments are also developing oilseed projects. There are concerns that such developments may lead to serious environmental damage.\n\nChina has become the world's largest consumer of solar energy. It is the largest producer of solar water heaters, accounting for 60 percent of the world’s solar hot water heating capacity, and the total installed heaters is estimated at 30 million households. Solar PV production in China is also in rapid development. In 2007, 0.82 GW of Solar PV was produced, second only to Japan.\n\nAs part of the stimulus plan of \"Golden Sun\", announced by the government in 2009, several developments and projects became part of the milestones for the development of solar technology in China. These include the agreement signed by LDK for a 500MW solar project, a new thin film solar plant developed by Anwell Technologies in Henan province using its own proprietary solar technology and the solar power plant project in a desert, headed by First Solar and Ordos City. The effort to drive the renewable energy use in China was further assured after the speech by the Chinese President, given at the UN climate summit on 22 Sept 2009 in New York, pledging that China will plan to have 15% of its energy from renewable sources within a decade. China is using solar power in houses, buildings, and cars.\n\nChina's total wind power capacity reached 2.67 gigawatts (GW) in 2006, 6.05 GW by 2007, 12.2 GW by 2008, 25 GW by 2009, and 44.7 GW by 2010, making China the world leader in installed wind power generation capacity.\n\nOfficials were warned that violating energy conservation and environmental protection laws would lead to criminal proceedings, while failure to achieve targets would be taken into account in the performance assessment of officials and business leaders.\n\nAfter achieving less than half the 4% reduction in energy intensity targeted for 2006, all companies and local and national government were asked to submit detailed plans for compliance before June 30, 2007.\n\nDuring the first four years of the plan, energy intensity improved by 14.4%, but dropped sharply in the first quarter of 2010. In August 2010, China announced the closing of 2,087 steel mills, cement works and other energy-intensive factories by September 30, 2010. The factory closings were made more palatable by a labor shortage in much of China making it easier for workers to find other jobs.\n\nA State Council circular issued on June 3, 2007, restricts the temperature of air conditioning in public buildings to no lower than 26 °C in summer (78.8 °F), and of heating to no higher than 20 °C (68 °F) in winter. The sale of inefficient air conditioning units has also been outlawed.\n\nChinese billionaires in energy business by Forbes included in 2013 Wang Yusuo & family ($2.4 B) the chairman of ENN Group, one of China's largest non-government-controlled energy businesses and Huo Qinghua ( $1.1 B) chairman of China Kingho Energy Group, one of the country's largest privately held mining and energy companies, with operations in China, Africa and Mongolia. and in Hong Kong Sit Kwong Lam ($1.35 B) the founder and chairman of Hong Kong-listed Brightoil Petroleum.\n\nThe Chinese results from the 1st Annual World Environment Review, published on June 5, 2007 revealed that, in a sample of 1024 people (50% male):\n\n\nAnother survey published in August 2007 by China Youth Daily and the British Council sampled 2,500 Chinese people with an average age of 30.1. It showed that 80% of young Chinese are concerned about global warming.\n\nIn December 2011 in Haimen, Guangdong, a coastal town of about 120,000 people, residents have protested ongoing for three days (22.12.2011) against plans for another coal-fired power plant. Police were armed with batons and shields and fired teargas to break up demonstrations.\n\n\n"}
{"id": "8909138", "url": "https://en.wikipedia.org/wiki?curid=8909138", "title": "Energy policy of Russia", "text": "Energy policy of Russia\n\nRussia’s energy policy is set out in the government’s \"Energy Strategy\" document, first approved in 2000, which sets out the government’s policy to 2020. The Energy Strategy outlines several key priorities: an increase in energy efficiency, reducing the impact on the environment, sustainable development, energy development and technological development, as well as improved effectiveness and competitiveness.\n\nIn July 2008 Russia's president signed a law allowing the government to allocate strategic oil and gas deposits on the continental shelf without an auction procedure. On 17 February 2011, Russia signed a deal with China, stating that in return for $25 billion in Chinese loans to Russian oil companies, Russia will supply China with large quantities of crude oil via new pipelines for the next 20 years.\n\nAs of 2014, oil and gas comprise over 60% of Russia's exports and account for over 30% of the country's gross domestic product (GDP). Russian energy policy of pumping 10.6 million barrels of oil a day is nearly 4 billion barrels annually. Russia’s proven oil reserves are 100 billion barrels. It’s available hydrocarbon potential will be able to provide the nation’s growing economy for 30 years. Russia's energy policy and a finite, depleting amount of oil and pumping more oil at a low oil price, creates issues for Russia's economy today and in the future.\n\nAs percentages of the world's total reserves, Russia holds 45% of the gas, 23% of the coal, 14% of the uranium, and 13% of the oil. Russian oil production and export had increased significantly since 2000, and in 2006 temporarily exceeded Saudi Arabia’s production. Since 2016, Russia is the top crude oil producer. Russia is also the world’s largest energy producer. \n\nRussia is not a member of OPEC (Organization of Petroleum Exporting Countries) and presents itself as an alternative to Middle Eastern energy resources, asserting that it is in fact a \"reliable energy supplier and that it only seeks to use its position as an important supplier to enhance global energy security\".\n\nThe Russian economy is heavily dependent on the export of natural resources such as oil and natural gas, and Russia has used these resources to its political advantage. Meanwhile, the US and other Western countries have worked to lessen the dependency of Europe on Russia and its resources. Starting in the mid-2000s, Russia and Ukraine had several disputes in which Russia threatened to cut off the supply of gas. As a great deal of Russia's gas is exported to Europe through the pipelines crossing Ukraine, those disputes affected several other European countries. Under Putin, special efforts were made to gain control over the European energy sector. Russian influence played a major role in canceling the construction of the Nabucco pipeline, which would have supplied natural gas from Azerbaijan, in favor of South Stream (though South Stream itself was also later canceled). Russia has also sought to create a Eurasian Economic Union consisting of itself and other post-Soviet countries.\n\nThe economy of the Union of Soviet Socialist Republics was based on a system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning. The economy was characterized by state control of investment, and public ownership of industrial assets. The Soviet Union invested heavily into infrastructure projects including the electrification of vast areas, and the construction and maintenance of natural gas and oil pipelines that stretch out of Russia and into every constituent nations of the USSR. This type of investment set the stage for Russia to become an energy superpower.\n\nThe concept of a Russian national energy policy was approved by the Russian government in 1992, and the government decided to develop the Energy Strategy. For this purpose the Interagency Commission was established.\n\nIn December 1994, the \"Energy Strategy of Russia (Major Provisions)\" was approved by the government, followed by the presidential decree of 7 May 1995 that outlined the first post-Soviet Russian energy strategy \"On the Main Directions of Energy Policy and Restructuring of the Fuel and Energy Industry of the Russian Federation for the Period up to the Year 2010\", and the government′s decision of 13 October 1995 that approved the \"Main provisions for the Energy Strategy of the Russian Federation\".\n\nThe strategy was amended under the presidency of Vladimir Putin. On 23 November 2000, the government approved the main provisions of the Russian energy strategy to 2020. On 28 May 2002, the Russian Ministry of Energy gave an elaboration on the main provisions. Based on these documents, the new Russian energy strategy up to 2020 was approved on 23 May 2003 and confirmed by the government on 28 August 2003. The main objective of the energy strategy was defined as reaching a better quality of fuel and energy mix and enhancing the competitiveness of Russian energy production and services in the world market. To that end, the long-term energy policy was to concentrate on energy safety, energy effectiveness, budget effectiveness and ecological energy security.\n\nThe Energy Strategy defines the main priority of Russian energy strategy as an increase in energy efficiency (meaning decreasing of energy intensity in production and energy supply expenditures), reducing impact on the environment, sustainable development, energy development and technological development, as well as an improvement of effectiveness and competitiveness.\n\nThe main natural gas producers in Russia are gas companies Gazprom, Novatek, Itera, Northgas and Rospan, and vertically integrated oil and gas companies Surgutneftegaz, TNK-BP, Rosneft and LUKOIL. \n\nMajority state-owned Gazprom has a monopoly of natural gas pipelines and has the exclusive right to export natural gas, granted by the Federal Law \"On Gas Export\", which came into force on 20 July 2006. Gazprom also has control over all gas pipelines out of Central Asia, and thus controls access to the European market. Russia has used Central Asia's gas, primarily that from Turkmenistan, on occasions where it has found itself unable to meet all its delivery obligations from its own production. For example, in 2000 Gazprom allowing Turkmenistan to use its pipelines to supply gas to the Russian domestic market, to enable Gazprom to fulfil its obligations to European customers. \n\nHistorically, the Medvezhye, Urengoy and Yamburg gas fields have made up the bulk of Gazprom's production. However, in the coming 10–20 years an increasing share of Gazprom's production will have to come from new fields. Recent developments such as Yen-Yakhinskoe, Yuzhno-Russkoye and West Pestsovoe in the Nadym-Pur-Taz area, which have all come on-stream since 2005, are relatively cost-efficient, being located close to the existing pipeline grid and other infrastructure. But they are not large enough to compensate for the decline in Gazprom's three core assets. Thus, the much larger Shtokman and Yamal developments will have to provide the bulk of new production capacity, by adding 70 and 200 BCM per year, respectively. Investments in the development of Shtokman and Yamal are forecast to account for over 40% of Gazprom's total expected capital expenditure over the next 20 years. Although Shtokman has been shelved at least for the time being due to relatively low gas prices and high costs, the project may still be resuscitated during the coming decades, depending on developments in unconventional gas and the supply–demand picture. Meanwhile, work on the largest field on the Yamal Peninsula, Bovanenkovo, is forging ahead. In any case, the complexity of these projects drives high field development costs, which in turn require a high gas price to be profitable.\n\nThe Energy Strategy foresees non-Gazprom production rising from a share of 17% in 2008 to 25–30% by 2030, implying growth from 114 to about 245 BCM/year.\n\nThe main export markets of Russian natural gas are the European Union and the CIS. Russia supplies a quarter of the EU gas consumption, mainly via transit through Ukraine (Soyuz, Brotherhood) and Belarus (Yamal-Europe pipeline). The main importers are Germany (where links were developed as a result of Germany's \"Ostpolitik\" during the 1970s, and also Ukraine, Belarus, Italy, Turkey, France and Hungary.\n\nIn September 2009, Prime Minister Vladimir Putin said Russia would try to liberalize the domestic gas market in the near future but would maintain Gazprom's export monopoly in the medium term.\n\nEnergy was the backbone of the Soviet economy. The 1973 oil embargo marked a turning point in Soviet society. The increase in the price of oil around the world prompted the USSR to begin exporting oil in exchange for money and Western technology. Increasing Western reliance on Russian resources further bolstered the importance of Russia’s energy sector to the overall economy.\n\nAs the Arctic ice cap shrinks due to global warming, the prospect of oil exploration in the Arctic Ocean is thought to be an increasing possibility. On 20 December 2001, Russia submitted documents to the UN Commission on the Limits of the Continental Shelf claiming expanded limits to Russian continental shelf beyond the previous 200 mile zone within the Russian Arctic sector. In 2002 the UN Commission recommended that Russia should carry out additional research, which commenced in 2007. It is thought that the area may contain 10bn tonnes of gas and oil deposits.\n\nThe Russian electricity market is dominated by Inter RAO and Gazprom Energoholding, the power generation subsidiary of Gazprom. While production and retail sale is open to competition, transmission and distribution remains under state control.\n\nIn recent years there have been several blackouts, notably the 2005 Moscow power blackouts.\n\nVladimir Putin approved the Kyoto Protocol on 4 November 2004 and Russia officially notified the United Nations of its ratification on 18 November 2004. The issue of Russian ratification was particularly closely watched in the international community, as the accord was brought into force 90 days after Russian ratification (16 February 2005).\n\nPresident Putin had earlier decided in favour of the protocol in September 2004, along with the Russian cabinet, against the opinion of the Russian Academy of Sciences, of the Ministry for Industry and Energy and of the then president's economic advisor, Andrey Illarionov, and in exchange to EU's support for the Russia's admission in the WTO. As anticipated after this, ratification by the lower (22 October 2004) and upper house of parliament did not encounter any obstacles.\n\nThe Kyoto Protocol limits emissions to a percentage increase or decrease from their 1990 levels. Russia did not face mandatory cuts since its greenhouse-gas emissions fell well below the 1990 baseline due to a drop in economic output after the breakup of the Soviet Union. Because of this, despite its growing economy, by 2012 Russia will by no means exceed the level of emissions in 1990, which is the Kyoto Protocol's year of departure.\n\nIt is debatable whether Russia will benefit from selling emissions credits to other countries in the Kyoto Protocol, although Gazprom has already entered the market. \"Russia is the Saudi Arabia of carbon [carbon emissions credits],\" said its representative. \"There is a tremendous bank there\".\n\nRenewable energy in Russia is relatively underdeveloped due to the lack of a conducive government policy framework and lack of clear policy signals. The abundance of energy and subsidies for natural gas, electricity and heating have also hampered growth of renewable energy in the country.\n\nThe Organization of the Petroleum Exporting Countries has unsuccessfully asked Russia to become a member several times. In 2008, with falling oil prices, Russia announced that it would work with OPEC to coordinate a reduction in output. \n\nIn 2013, Saudi Arabia was reported to have urged Russia to join OPEC, but Russia declined the offer. Russia has expressed its desire to become an observer to OPEC, which could lead to greater communication that Russia has sought since the oil price crash in 2014. In 2015, it was estimated that Russia working with OPEC would boost the cartel’s clout by nearly a third.\n\nRussia and OPEC have made several oil production cutback agreements to raise the price of oil since March 1999, when a deal was reached as part of an agreement between OPEC and non-OPEC oil producers to lift crude prices off their lows. \n\nIn June 2015, Russian president Vladimir Putin received deputy crown prince Mohammad bin Salman along with the Saudi minister of petroleum and mineral resources Ali al-Naimi; the latter spoke of ″creating a petroleum alliance between the two countries for the benefit of the international oil market as well as producing countries and stabilizing and improving the market″. In late November 2016, Russia agreed to join OPEC nations to reduce oil output, with cuts taking effect from 1 January 2017 to last for six months.\n\nIn terms of the Russian energy demand structure, domestic production greatly exceeds domestic demand, making Russia the world’s leading net energy exporter. The Federal Tariff Service sets gas and wholesale electricity prices, the Regional Energy Commissions set co-generated electricity and heating prices, and municipalities set prices for heat transmission and heat generation by municipal boilers. Heavily subsidised district heating—the distribution of heat from a central locale to subsidiary commercial or residential areas—plays a major role, providing over a third of energy requirements for industry and close to half those of the commercial and household sectors. Almost 50 percent of primary energy consumption in Russia is used for heat generation, transmission and distribution. Domestic gas prices generally are barely 15–20 percent of the market rate at which Russia’s gas is sold to Germany.\n\nRussia's energy superpower status became a hot topic in the European Union in 2006. Russia's large reserves of natural gas have helped give it the title without much debate.\n\nRussia has identified natural gas as a key strategic asset, and since 20 July 2006 Gazprom has had the exclusive right to export natural gas. The Russian government is the largest shareholder of Gazprom, and has been accused of manipulating prices for political reasons, particularly in CIS nations.\n\nAfter Russia's annexation of Crimea and involvement in the War in Eastern Ukraine in 2014, Western countries imposed sanctions targeting the Russian oil and gas sector. The sanctions did not cause the Russian economy to collapse, but due to the long time-lag on the development of new oil and gas fields, could have a longer-term impact on Russian oil production. \n\nRussia has been accused in the West (i.e. Europe and the United States) of using its natural resources as a policy tool to be wielded against offending states like Georgia, Ukraine, and other states it perceives as hindrances to its power. According to one estimate, since 1991 there have been more than 55 energy incidents, of which more than 30 had political underpinnings. Only 11 incidents had no political connections. On the other hand, Russian officials like to remind their Western partners that even at the height of the Cold War the Soviet Union never disrupted energy supplies to the West. And yet, Russia's ability to use energy as a foreign policy tool is constrained by many factors.\n\nRussia, in turn, accuses the West of applying double-standards relating to market principles, pointing out that it has been supplying gas to the states in question at prices that were significantly below world market levels, and in some cases remain so even after price hikes. Russia argues that it is not obligated to effectively subsidize the economies of post-Soviet states by supplying them with resources at below-market prices.\n\nThere is still a risk of supply interruptions for the states of the Former Soviet Union. Depending on bilateral relations and the present context, the risk for partial and/or short-duration cut-offs is high. Since 1991, the energy lever has been used for putting political or economic pressure on Estonia, Latvia, Lithuania, Ukraine, Belarus, Moldova, Georgia that subsequently affected most of Europe. The number of incidents, i.e. cut-offs, take-overs, coercive price policy, blackmail or threats, is over fifty in total (of which about forty are cut-offs). Incidents appear to be equally divided between the Yeltsin and Putin eras, but the number of cut-offs has decreased by half during Putin. The immediate reasons for Russia’s coercive policy appear to be political concession in ongoing negotiations, infrastructure take-over, and execution of economically favorable deals or to make political statements. There are economic underpinnings in the majority of the cases and Russian demands for payments of debts are legitimate. However, there are also political underpinnings in more than half of the incidents, and in a few cases explicit political demands are evident.\n\nStarting 1 January 2007 Gazprom increased the price of natural gas to Azerbaijan from US$110 to $235 per thousand cubic metres. (At the time, Gazprom charged the EU US$250.) Azerbaijan refused to pay this price and the gas supply to Azerbaijan stopped. On its side, Azerbaijan stopped oil exports to and via Russia. \n\nA year earlier, pro-Russian Armenia was hit with the same 100% price increase as Western-oriented Georgia, Vladimir Socor has observed.\n\nThe Russia-Belarus energy dispute began when Russian state-owned gas supplier Gazprom demanded an increase in gas prices paid by Belarus, which has been closely allied with Moscow and forms a loose union state with Russia. It escalated on 8 January 2007, when the Russian state-owned pipeline company Transneft stopped pumping oil into the Druzhba pipeline which runs through Belarus.\nTransneft has accused Belarus of forcing the shutdown by stealing oil from the pipeline and halted the oil transport.\nOn 10 January, Transneft resumed oil exports through the pipeline after Belarus ended the tariff that sparked the shutdown, despite differing messages from the parties on the state of negotiations.\n\nOn 9 July 2008, after signing an agreement between the United States and the Czech Republic to host a tracking radar for an antiballistic missile system, the flow of Russian oil through the Druzhba pipeline to the Czech Republic started to reduce. Although officially the linkage between reduction of oil supplies and the radar agreement was not claimed, it was suspected. Transneft denied any connections with radar agreement, saying that reduction was purely commercial as Tatneft and Bashneft started to refine more oil at their own refineries. Although Prime Minister Putin asked Deputy Prime Minister Igor Sechin to 'work with all partners to make sure there are no disruptions', in reality the supplies were reduced to 50%.\n\nIn the January 2006 alleged North Ossetia sabotage, two simultaneous explosions occurred on the main branch and a reserve branch of the Mozdok-Tbilisi pipeline in the Russian border region of North Ossetia. The electricity transmission line in Russia's southern region of Karachayevo-Cherkessiya near the Georgian border was brought down by an explosion just hours later. Georgian president Mikhail Saakashvili blamed Russia for putting pressure on Georgia's energy system at the time of the coldest weather.\n\nOn 1 November 2006 Gazprom announced that it will construct a direct gas pipeline to Georgia's breakaway region of South Ossetia. The work on the pipeline started just before South Ossetia's 12 November referendum on separating from Georgia.\nStarting 1 January 2007 Gazprom increased natural gas prices to Georgia following an international incident in an alleged effort to strongly influence the Georgian leadership's defiance of Moscow. The current price is 235 USD per thousand cubic meter, which is the highest among the CIS countries.\n\nThe August 2008 military conflict between Georgia and Russia over the autonomous region of South Ossetia, which has been de facto independent from Georgia since the early 1990s, is likely to shift the balance of power between the main players involved in the formation of the future of the Caspian and Central Asian energy sector, including:\n\n• Producer and transit countries: Azerbaijan, Georgia, Kazakhstan, Turkmenistan, Uzbekistan, Turkey and Iran;\n\n• Foreign corporations operating in the region's hydrocarbon sector;\n\n• Major external players: China, Russia, the European Union and the United States.\n\nThe volatility of these transit routes is likely to shape investment decisions of international oil companies involved in the development of Central Asian and Caspian hydrocarbons and their transportation to global markets. Governments of these resource-rich countries are bound to have serious concerns about the safety of BTC, WREP and BTE pipelines, the railway networks and the oil terminals at the Georgian Black Sea ports of Batumi, Kulevi and Poti, all of which were halted by the Georgian-Russian hostilities. Although, the pipelines were only temporarily shut down for security reasons and were not targeted or damaged in the conflict, their future expansion and the construction of related new pipeline projects, such as the Kazakh-Caspian Transportation System, the Trans-Caspian gas pipeline and Nabucco are now uncertain. In this situation, Central Asian and Caspian producers may opt for traditional exports via Russia (providing Moscow successfully expands the capacity of its oil and gas export routes) and the new export pipelines to China.\n\nOn 29 July 2006 Russia shut down oil export to Mažeikių oil refinery in Lithuania after an oil spill on the Druzhba pipeline system occurred in Russia’s Bryansk oblast, near the point where a line to Belarus and Lithuania branches off the main export pipeline. Transneft said it would need one year and nine months to repair the damaged section. Although Russia cited technical reasons for stopping oil deliveries to Lithuania, Lithuania claims that the oil supply was stopped because Lithuania sold the Mažeikių refinery to Polish company PKN Orlen in an effort to avoid the refinery and infrastructure being bought out by Russian interests. Russian crude oil is now being transshipped via the Būtingė Marine Terminal.\n\nThere has been rapprochement with Tusk's government in Warsaw, after two years of tensions with the conservative government of Kaczynski. The cooperation on the Yamal-Europe pipeline has continued without serious problems. Nevertheless, some disagreements concerning control of the Yamal-Europe pipeline and transit pricing remain. Despite attempts to relieve tensions, consecutive Polish governments strongly oppose the Nord Stream project bypassing Poland and favour further development of overland alternatives. It remains a contentious issue that as a result of the Russian-Ukrainian gas dispute in 2009, Polish PGNIG gas company did not receive contracted supplies of Russian gas from Ukraine.\n\nAt the beginning of 2006 Russia greatly increased the price of gas for Ukraine to bring it in line with market values. The dispute between Russian state-owned gas supplier Gazprom and Ukraine over natural gas prices started in March 2005 (over the price of natural gas and prices for the transition of Gazprom's gas to Europe). The two parties were unable to reach an agreement to resolve the dispute, and Russia cut gas exports to Ukraine on 1 January 2006 at 10:00 MSK. The supply was restored on 4 January, when a preliminary agreement between two gas companies was settled. Other disputes arose in October 2007 and in January 2009, this dispute again resulted in 18 European countries reporting major falls or cut-offs of their gas supplies from Russia transported through Ukraine. Gas supplies restarted on 20 January 2009 and were fully restored on 21 January 2009.\n\nThe EU-Russia Energy Dialogue was launched at the EU-Russia Summit in Paris in October 2000. François Lamoureux, Director general for Energy and Transport at the European Commission and Viktor Khristenko, Vice-Prime Minister of the Russian Federation took up the responsibility as sole interlocutors. Christian Cleutinx, then Head of Unit at the European Commission was designated as the Coordinator of the dialogue. At the working level the Energy Dialogue consists three thematic working groups. The Energy Dialogue involves the EU Member States, energy industry and the international financial institutions.\nProjected gas pipelines originating in Russia and supplying Europe.\n\nRussia signed the Energy Charter Treaty in 1994, but flatly refused to ratify its current revision. Russia's main objections to the ratification revolve around the proviso about the third party access to the pipelines and transit fees. Notwithstanding the fact that Russia didn't ratify the treaty, Ivan Materov, State Secretary and Deputy Minister of Industry and Energy of the Russian Federation, serves as the vice-chairman of the Energy Charter Conference, and Andrei Konoplyanik as the Deputy Secretary General.\n\nRussia and the European Union have also failed to finalize the negotiations on the Energy Charter Protocol on Transit. The main issue remain open is how, and to what extent, the Protocol will include mechanisms for establishment long term transit arrangements. Also the third party access to its pipeline infrastructure has remained Russia's main objection to the Protocol.\n\nAccording to the estimation of Swedish economist Anders Åslund in 2008, 50% of the state-owned Gazprom's investments were lost through corrupt practices. For instance, the Russian section of Blue Stream pipeline was three times more expensive to construct per kilometer than the Turkish section of the pipeline.\n\nExperts believe Bill Browder's \"visa problem\" is related to that he made questions about Gazprom's murky intermediates which receive money from Gazprom.\n\nThe Russian government and Russian energy companies were beneficiaries in the Oil-for-Food Programme.\n\nCrude oil prices have decline from over a 100 dollars a barrel in 2014 to below 50 US dollars in 2015. Russia tried and failed to get OPEC support for production cutbacks and is now ramping up its oil production to reduce the drop in oil revenues. OPEC's oil glut supply policy has affected the Russian economy and energy policy.\n\nThere are controversies about the reasons for OPEC's policy for reducing the price of oil. Russia has responded to OPEC's policy by increasing dialogue with OPEC.\n\n\n\n"}
{"id": "14099722", "url": "https://en.wikipedia.org/wiki?curid=14099722", "title": "Episcopal palace, Oradea", "text": "Episcopal palace, Oradea\n\nThe Episcopal palace () of the city of Oradea in Bihor county, Romania dates to the Baroque times.\n\nIt was founded in 1762 by the Baron Bishop Adam Patačić, as bishopric palace of the Roman Catholic Diocese of Magnovaradimum. Illustrious Viennese architect Franz Anton Hillebrandt, designer of many Austrian palaces and one of Europe's 18th century best, designed the palace and planned the city's posh side as Baroque quarter, while engineer A.J. Neumann was in charge of the palace's massive construction, complete with its 365 exterior windows resembling the days of the year and 120 large, extravagant rooms distributed on three floor plans.\n\nThe architecture of the palace is of late Austrian Baroque style, a more sober and practical type compared to the overly ornamented French Baroque, for example. The building was meant to resemble on a smaller scale the famous Royal Belvedere (palace) of Vienna, which likely was one of the reasons along with other religious conflicts that made Empress Maria Theresa of Austria repudiate the founder, Adam Patachich, a Croatian nobleman and the bishop of Oradea between 1759 and 1776; he was then sent to another diocese, in Kalocsa, Hungary. Nevertheless, the baron was a charismatic, highly educated humanist and an illuminated patron of arts, who is mostly remembered for the fine music and musicians he surrounded himself with: this is where Michael Haydn, famous composer and Joseph Haydn's brother, worked as a Kapellmeister in the bishop's orchestra. The bishop also employed at the court other famous European composers and violinists like Wenzel Pichl and Carl Ditters von Dittersdorf, who between 1765-1769 served as a Musikdirektor.\n\nFinally, in 1771, the Holy Roman Empress (jure uxori), Maria Theresa of Austria, together her son, future Joseph II, Holy Roman Emperor, arrived here to visit and make peace with a place whose project she did not initially fancy. In 1773 the palace unfortunately burned down entirely in a mysterious fire, but was reconstructed immediately by the next appointed bishop, after its original plans.\n\nIn the year 1855, a new side and entrance was added graciously in tone and respect with the initial building, with grand double stairways. Later in time, after Romania gained possession of Transylvania, it remained under the church's patronage but during the socialist regime, it was seized as state property.\n\nOn January 17, 1971, the Baroque Palace became a county museum hosting many large and fine archeological, historical, natural history, ethnographic and art collections under the name of \"Muzeul Ţării Crişurilor\" (\"Museum of the Three Rivers Land\"). The museum has approximately 400 000 pieces divided under four main collections: History and Archeology, Ethnography, Art and Natural History. Famous for its world-class Neolithic and Bronze Age collection, the museum also boasts treasures from Ancient Egypt and Greece. The ethnography section has probably the best of western Transylvanian folk exhibits anywhere, including a large selection of traditional costumes, peasant house appliances, pottery and painted Easter eggs.\n\nThe highlights of the Natural History section are the prehistoric animals including cave bears, giant elk, different mammoth types or dinosaurs (like Iguanodons, Valdosaurus or Camptosaurus). The interior courtyard is dotted by a long row of Romanian monarch busts added during the museum years.\n\nThe front courtyard is an artistic park with large old bronze and marble statues of historical figures and also home to a famous Baroque parish church erected in 1752 even before the palace, a work of the Italian architect Giovanni Battista Ricca modeled after the mother church of the Jesuits, Church of the Gesu in Rome. The basilica contains the relics of King Saint Ladislaus, born in year 1040, a splint of his skull being kept here in a gold box. In 1992, Pope John Paul II through the Holy See's decree, raised the church to a holy basilica rank.\n\nIn 2003, like many other edifices, The Baroque Palace of Oradea was restored to the Roman Catholic Diocese of Oradea Mare by the Government of Romania, but the building is still being used as a museum until further negotiations are made. \n\n\n"}
{"id": "481170", "url": "https://en.wikipedia.org/wiki?curid=481170", "title": "Fiddlehead fern", "text": "Fiddlehead fern\n\nFiddleheads or fiddlehead greens are the furled fronds of a young fern, harvested for use as a vegetable.\n\nLeft on the plant, each fiddlehead would unroll into a new frond (circinate vernation). As fiddleheads are harvested early in the season before the frond has opened and reached its full height, they are cut fairly close to the ground.\n\nFiddleheads have antioxidant activity, are a source of omega-3 and omega-6 fatty acids, and are high in iron and fibre. Certain varieties of fiddleheads have been shown to be carcinogenic. (See bracken poisoning)\n\nThe fiddlehead resembles the curled ornamentation (called a \"scroll\") on the end of a stringed instrument, such as a violin. It is also called a \"crozier\", after the curved staff used by bishops, which has its origins in the shepherd's crook.\n\nThe fiddleheads of certain ferns are eaten as a cooked leaf vegetable. The most popular of these are:\n\nFiddleheads' ornamental value makes them very expensive in the temperate regions where they are not abundant.\n\nAvailable seasonally, fiddleheads are both foraged and commercially harvested in spring. When picking fiddleheads, it is recommended to take only half the tops per plant/cluster for sustainable harvest. Each plant produces several tops that turn into fronds; repeated over-picking will eventually kill the plant. Maintaining sustainable harvesting methods is important in the propagation of any non-farmed food species. \n\nFiddleheads have been part of traditional diets in much of Northern France since the beginning of the Middle Ages, across Asia, and also among Native Americans for centuries. They are also part of the diet in the Russian Far East where they are often picked in the wild in autumn, preserved in salt over winter, and then consumed in spring.\n\nIn Indonesia, young fiddlehead ferns are cooked in a rich coconut sauce spiced with chili pepper, galangal, lemongrass, turmeric leaves and other spices. This dish is called \"gulai pakis\" or \"gulai paku\", and originated from the Minangkabau ethnic group of Indonesia. In the Philippines, fiddlehead fern or \"pakô\" is a delicacy often made into a salad with tomato, salted egg slices, and a simple vinaigrette dressing.\n\nIn East Asia, fiddleheads of bracken (\"Pteridium aquilinum\") are eaten as a vegetable, called \"warabi\" () in Japan, \"gosari\" () in Korea, and \"juécài\" () in China and Taiwan. In Korea, a typical \"banchan\" (small side dish) is \"gosari-namul\" (), which consists of prepared fernbrake fiddleheads that have been sauteed. It is also a component of the popular dish \"bibimbap\". In Japan, bracken fiddleheads are a prized dish, and roasting the fiddleheads is reputed to neutralize any toxins in the vegetable.\n\nIn Japan, fiddleheads of flowering fern (\"Osmunda japonica\"), known as \"zenmai\" (), as well as those of the ostrich fern (\"Matteuccia struthiopteris\"), known as \"kogomi\" (), are commonly eaten in springtime. Fiddleheads in Japan are considered \"sansai\", or wild vegetables.\n\nIn the Indian subcontinent, it is found in the Himalayan states of North and Northeast India.\n\nIn the Kullu Valley in Himachal Pradesh, it is known locally as \"lingri\" and is famously used to make a pickle \"lingri ka achaar\". In the Kangra Valley it is called \"lungdu\" in the Kangri dialect.In Chamba it is known as \"Kasrod\".\n\nIn Garhwal division of Uttarakhand, it is called \"languda\" and eaten as a vegetable.\n\nIn Darjeeling and Sikkim regions, it is called \"niyuro\" (नियुरो) and is common as a vegetable side dish, often mixed with local cheese. It is also pickled.\nIn Assam, it is known as \"dhekia xak\" (); there it is a popular side dish.\nIn the area of Jammu in Jammu and Kashmir, it's known as \"kasrod\" (कसरोड). The most famous Dogra dish is \"kasrod ka achaar\" (fiddlehead fern pickle). It's also cooked as a dry vegetable side dish to be eaten with rotis or parathas.\n\nIn Nepal, it is a seasonal food called \"niyuro\" (नियुरो) and is served as a vegetable side dish, often cooked in local clarified butter. It is also pickled.\n\nOstrich ferns (\"Matteuccia struthiopteris\"), known locally as \"fiddlehead ferns\", grow wild in wet areas of northeastern North America in spring. The Maliseet, Mi'kmaq, and Penobscot peoples of Eastern Canada and Maine have traditionally harvested fiddleheads, and the vegetable was introduced first to the Acadian settlers in the early 18th century, and later to United Empire Loyalist colonists as they began settling in New Brunswick in the 1780s. Fiddleheads remain a traditional dish in these regions, with most commercial harvesting occurring in New Brunswick, Quebec and Maine. North America's largest grower, packer and distributor of wild fiddleheads established Ontario's first commercial fiddlehead farm in Port Colborne in 2006. Fiddlehead-producing areas are also located in Nova Scotia, Vermont and New Hampshire. The Canadian village of Tide Head, New Brunswick, bills itself as the \"Fiddlehead Capital of the World.\"\n\nFiddleheads are sold fresh and frozen. Fresh fiddleheads are available in the market for only a few weeks in springtime, and are fairly expensive. Pickled and frozen fiddleheads, however, can be found in some shops year-round. The vegetable is typically steamed, boiled and/or sautéed before being eaten hot, with hollandaise sauce, butter, lemon, vinegar and/or garlic, or chilled in salad or with mayonnaise.\n\nTo cook fiddleheads, it is advised to remove the brown papery husk before washing in several changes of cold water, then boil or steam them. Boiling reduces the bitterness and the content of tannins and toxins. The Centers for Disease Control and Prevention associated a number of food-borne illness cases with fiddleheads in the early 1990s. Although they did not identify a toxin in the fiddleheads, the findings of that case suggest that fiddleheads should be cooked thoroughly before eating. The cooking time recommended by health authorities is 15 minutes if boiled and 10 to 12 minutes if steamed. The cooking method recommended by gourmets is to spread a thin layer in a steam basket and steam lightly, just until tender crisp.\n\nFiddleheads contain various vitamins and minerals, as well as omega-3 and omega-6 fatty acids. They are a source of antioxidants and dietary fiber. They are low in sodium, but rich in potassium, which may make them suitable for people who need a low-sodium diet.\n\nFiddleheads may harbour microbes, and should be washed and cooked before eating.\n\nMany ferns also contain the enzyme thiaminase, which breaks down thiamine. This can lead to beriberi, if consumed in extreme excess.\n\nFurther, there is some evidence that certain varieties of fiddleheads, e.g. bracken (\"Pteridium genus\"), are carcinogenic. It is recommended to fully cook fiddleheads to destroy the shikimic acid. Ostrich fern (\"Matteuccia struthiopteris\") is not thought to cause cancer, although there is evidence it contains a toxin unidentified as yet.\n\n\n"}
{"id": "1770184", "url": "https://en.wikipedia.org/wiki?curid=1770184", "title": "Jovian Chronicles", "text": "Jovian Chronicles\n\nJovian Chronicles is a science fiction game setting published by Dream Pod 9 since 1992. It introduces a complete universe for role-playing and wargaming space combat, featuring mecha, giant spacecraft and epic space battles.\n\nThe setting was originally published as a pair of licensed supplements for the \"Mekton\" role-playing game; IANVS Games released the two volumes (\"Jovian Chronicles\" and \"Europa Incident\") in 1992. This first edition is known amongst fans as the \"Green Edition\" because of the color scheme of its cover design.\n\nThe game was re-published as a full-fledged game line in 1997 by Dream Pod 9, this time using their own in-house rule system. This edition, and all subsequent ones, are known as the \"White Editions,\" though several of the supplemental books feature a dark blue starfield cover.\n\nThe game world shows one possible future, a time in which Mankind has expanded and settled the solar system, going as far as terraforming Venus and Mars. The colonists living around Jupiter are at odds with the government of Earth, driving much of the series' intrigue. The game features a blend of action/adventure and hard-science space colonization facts. The books are based on plausible technical details to maintain a realistic hard science fiction setting.\n\n\nMany recent Western cartoon shows and games have Japanese anime styling. \"Jovian Chronicles\" also features anime-inspired artwork and narrative elements, though the various technical elements (ships, stations, equipment, etc.) are original works illustrated with computer art.\n\nExo-armors are one of the main hooks of the series, these one-crew mecha consist of big armored space suits talling 15 metres or more with weapons capable of devastating even small warships. The main source of inspiration for the game was the various iterations of the Gundam Universal Century series up to the early 90's, with strong influence from Gundam 0083 Stardust Memories and Gundam 0080 War in the Pocket. Hints of other influences such as Patlabor and to some extent, Macross and Mospeada, are present, although no transformable mecha appear to date. \"Jovian Chronicles\" can be placed in the Real Robot anime genre.\n\nThe series art and mechanical designs were created by John Moscato and Ghislain Barbe, the latter who was also responsible for the artistic style which characterized all of the Dream Pod 9's lines including \"Heavy Gear\" and \"Tribe 8\".\n\nThe concept has its origins on a campaign played by the authors Mark A. Vezina and Etienne Gagnon since 1987, using R. Talsorian's Mekton system. After 5 years of development, IANVS plublications licensed the Mekton II rules and published the \"Jovian Chronicles\" Campaign Setting, a 112-page letter sized book which describes the JC universe, timeline, technologies and introduces players to the setting thru a 12 episode campaign. The JC campaign works as an extension of the Mekton rules set, with minimum changes required and then only for character status and wealth, however, compared to other Mecha depicted in Mekton, JC Exo-armors tend to be less resilient and their weapons are less powerful. It is also of note that at this stage, \"Exos\" tend to be more balanced with little differences among them, while on later editions, \"grunt\" exos are intentionally inferior to \"hero\" machines, for example, a CEGA Syreen and a JC Pathfinder are equally balanced in power, armor and weapons, and similar in capabilities to larger machines.\n\nMecha designs also vary on this edition, being more complex and elaborated in design than on latter iterations, during this release, there were also a number of metal miniatures for six of the exo-armors in the setting, these were produced by Canadian manufacturer RAFM and were based on the campaign book technical designs by John Moscato. Miniatures were in 1/325 scale due to the fictional size of the exo-armors and presented one of the finest and most dynamic mecha sculptings of the era.\n\nThe second release of Jovian Chronicles, was the first one published by DP9, adapts the \"Silhouette game system\", a streamlined set of rules already tested and proved in \"Heavy Gear\", Dream Pod 9's other successful science fiction game. It can be played as either a role-playing game, a tactical wargame, or an integration of both. There are major changes on this release, such as the aforementioned distinction among the different exo-armors and other equipment as well as the focus on the character creation.\n\nBoth the RPG and miniature games are built on the same basic rule mechanics. \"Silhouette\" which defines characters in terms of 10 base attributes (agility, knowledge, etc.), 5 derived attributes (health, etc.), and a variety of skills. Skill rolls make up the backbone of the system, which focuses on effect-based speed of play over grainy detail. The core mechanic involves rolling a number of 6-sided dice, taking the highest result, adding in modifiers for attributes and/or situation, and then comparing it to a set threshold number. If the result is higher than the threshold the test is a success; if it is lower the test is a failure. The margin by which the test succeeded (Margin of Success, MoS) or failed (Margin of Failure, MoF) helps to determine the final outcome. Combat is handled by the same system, with characters taking penalty-inflicting wounds rather than depleting a set number of health points. As a result, the system can be lethal, especially on inexperienced characters.\n\nNew miniatures were also released for this edition, for both, exo-armor/fighters as well as spaceships. The new line of miniatures covered a much broader range of subjects, at the expense of size, being scaled at 1/500 for the exo armor/fighters which were near half the size of the original ones, this edition also marks the appearance of spaceship miniatures, which were scaled to 1/5000.\n\nDuring this stage, the game system was splintered into the \"Silhouette Core\" RPG rules and the \"Lightning Strike\" system for tactical scenarios. The latter, is a more abstract version of the original tactical system which allow for combined arms maneuvers.\n\nA third edition Jovian Wars was put on kickstarter during March-April 2017 and is in the process of delivering to its backers, the new edition is a fleet scale tactical game, whose rules are distributed for free at the DP9 website and changes (again) the scale of the miniatures to 1/1000 to exo-armors and fighters and 1/4000 for the spaceships.\n\nThis change in scale intends to represent more realistically the magnitude of the conflicts depicted, making combat units smaller and warships larger. It features a new turning template for ship maneuvers which is promoted as the major innovation in the system and due to the scale of the exo-armors, they now operate in squadrons of 3 miniatures each. With the new scale, each exo/fighter is around 15mm in height, compared to 30mm for Lightning strike minis and an average of 54mm for RAFM miniatures. The RPG aspect of the game is all but left aside.\n\nThe project was funded, however it is uncertain if besides backers, other potential or new players will accept the scale change which has turned the exo-armors, an emblematic symbol of the series and marketing hook (first edition depicted a full page illustration of a Jovian Exo-armor on cover which was also used on the RAFM blister's card) into a sort of battle counter (size matters... sometime), rather focusing on ship to ship maneuvers.\n\n"}
{"id": "20813830", "url": "https://en.wikipedia.org/wiki?curid=20813830", "title": "Kanguk Formation", "text": "Kanguk Formation\n\nThe Kanguk Formation is a geological formation in the Northwest Territories of Canada whose strata date back to the Late Cretaceous. Dinosaur remains are among the fossils that have been recovered from the formation.\n\nIt was first described in the Kanguk Peninsula of the Axel Heiberg Island, along the shore of the Stand Fiord by Souther in 1963. The formation occurs throughout the Sverdrup Basin and the southern Queen Elizabeth Islands.\n\nThe Kanguk Formation is composed of dark shale and siltstone with interbeds of sandstone, bentonite and tuff. Thicker sandstone and conglomerate beds occur in the western reaches in Eglinton Island.\n\n"}
{"id": "234132", "url": "https://en.wikipedia.org/wiki?curid=234132", "title": "Klystron", "text": "Klystron\n\nA klystron is a specialized linear-beam vacuum tube, invented in 1937 by American electrical engineers Russell and Sigurd Varian, which is used as an amplifier for high radio frequencies, from UHF up into the microwave range. Low-power klystrons are used as oscillators in terrestrial microwave relay communications links, while high-power klystrons are used as output tubes in UHF television transmitters, satellite communication, radar transmitters, and to generate the drive power for modern particle accelerators.\n\nIn a klystron, an electron beam interacts with radio waves as it passes through resonant cavities, metal boxes along the length of a tube. The electron beam first passes through a cavity to which the input signal is applied. The energy of the electron beam amplifies the signal, and the amplified signal is taken from a cavity at the other end of the tube. The output signal can be coupled back into the input cavity to make an electronic oscillator to generate radio waves. The gain of klystrons can be high, 60 dB (one million) or more, with output power up to tens of megawatts, but the bandwidth is narrow, usually a few percent although it can be up to 10% in some devices.\n\nA \"reflex klystron\" is an obsolete type in which the electron beam was reflected back along its path by a high potential electrode, used as an oscillator.\n\nThe name \"klystron\" comes from the Greek verb κλύζω (\"klyzo\") referring to the action of waves breaking against a shore, and the suffix -τρον (\"tron\") meaning the place where the action happens. The name \"klystron\" was suggested by Hermann Fränkel, a professor in the classics department at Stanford University when the klystron was under development.\n\nThe klystron was the first significantly powerful source of radio waves in the microwave range; before its invention the only sources were the Barkhausen-Kurz tube and split anode magnetron, which were limited to very low power. It was invented by the brothers Russell and Sigurd Varian at Stanford University. Their prototype was completed and demonstrated successfully on August 30, 1937. Upon publication in 1939, news of the klystron immediately influenced the work of US and UK researchers working on radar equipment. The Varians went on to found Varian Associates to commercialize the technology (for example, to make small linear accelerators to generate photons for external beam radiation therapy). Their work was preceded by the description of velocity modulation by A. Arsenjewa-Heil and Oskar Heil (wife and husband) in 1935, though the Varians were probably unaware of the Heils' work.\n\nThe work of physicist W.W. Hansen was instrumental in the development of the klystron and was cited by the Varian brothers in their 1939 paper. His resonator analysis, which dealt with the problem of accelerating electrons toward a target, could be used just as well to decelerate electrons (i.e., transfer their kinetic energy to RF energy in a resonator). During the second World War, Hansen lectured at the MIT Radiation labs two days a week, commuting to Boston from Sperry Gyroscope Company on Long Island. His resonator was called a \"rhumbatron\" by the Varian brothers. Hansen died of beryllium disease in 1949 as a result of exposure to beryllium oxide (BeO).\n\nDuring the Second World War, the Axis powers relied mostly on (then low-powered and long wavelength) klystron technology for their radar system microwave generation, while the Allies used the far more powerful but frequency-drifting technology of the cavity magnetron for much shorter-wavelength one centimeter microwave generation. Klystron tube technologies for very high-power applications, such as synchrotrons and radar systems, have since been developed.\n\nRight after the war, AT&T used 4 watt klystrons in its brand new network of microwave relay links that covered the US continent. The network provided long distance telephone service and also carried television signals for the major TV networks. Western Union Telegraph Company also built point-to-point microwave communication links using intermediate repeater stations at about 40 mile intervals at that time, using 2K25 reflex klystrons in both the transmitters and receivers.\n\nKlystrons amplify RF signals by converting the kinetic energy in a DC electron beam into radio frequency power. In a vacuum, a beam of electrons is emitted by a thermionic cathode (a heated pellet of low work function material), and accelerated by high-voltage electrodes (typically in the tens of kilovolts). \n\nThis beam passes through an input cavity resonator. RF energy has been fed into the input cavity at, or near, its resonant frequency, creating standing waves, which produce an oscillating voltage, which acts on the electron beam. The electric field causes the electrons to \"bunch\": electrons that pass through when the electric field opposes their motion are slowed, while electrons which pass through when the electric field is in the same direction are accelerated, causing the previously continuous electron beam to form bunches at the input frequency. \n\nTo reinforce the bunching, a klystron may contain additional \"buncher\" cavities. \n\nThe beam then passes through a \"drift\" tube, in which the faster electrons catch up to the slower ones, creating the \"bunches\", then through a \"catcher\" cavity. \n\nIn the output \"catcher\" cavity, each bunch enters the cavity at the time in the cycle when the electric field opposes the electrons' motion, decelerating them. Thus the kinetic energy of the electrons is converted to potential energy of the field, increasing the amplitude of the oscillations. The oscillations excited in the catcher cavity are coupled out through a coaxial cable or waveguide. \n\nThe spent electron beam, with reduced energy, is captured by a collector electrode.\n\nTo make an oscillator, the output cavity can be coupled to the input cavity(s) with a coaxial cable or waveguide. Positive feedback excites spontaneous oscillations at the resonant frequency of the cavities.\n\nThe simplest klystron tube is the two-cavity klystron. In this tube there are two microwave cavity resonators, the \"catcher\" and the \"buncher\". When used as an amplifier, the weak microwave signal to be amplified is applied to the buncher cavity through a coaxial cable or waveguide, and the amplified signal is extracted from the catcher cavity.\n\nAt one end of the tube is the hot cathode which produces electrons when heated by a filament. The electrons are attracted to and pass through an anode cylinder at a high positive potential; the cathode and anode act as an electron gun to produce a high velocity stream of electrons. An external electromagnet winding creates a longitudinal magnetic field along the beam axis which prevents the beam from spreading.\n\nThe beam first passes through the \"buncher\" cavity resonator, through grids attached to each side. The buncher grids have an oscillating AC potential across them, produced by standing wave oscillations within the cavity, excited by the input signal at the cavity's resonant frequency applied by a coaxial cable or waveguide. The direction of the field between the grids changes twice per cycle of the input signal. Electrons entering when the entrance grid is negative and the exit grid is positive encounter an electric field in the same direction as their motion, and are accelerated by the field. Electrons entering a half-cycle later, when the polarity is opposite, encounter an electric field which opposes their motion, and are decelerated.\n\nBeyond the buncher grids is a space called the \"drift space\". This space is long enough so that the accelerated electrons catch up with electrons that were accelerated at an earlier time, forming \"bunches\" longitudinally along the beam axis. Its length is chosen to allow maximum bunching at the resonant frequency, and may be several feet long.\n\nThe electrons then pass through a second cavity, called the \"catcher\", through a similar pair of grids on each side of the cavity. The function of the \"catcher grids\" is to absorb energy from the electron beam. The bunches of electrons passing through excite standing waves in the cavity, which has the same resonant frequency as the buncher cavity. Each bunch of electrons passes between the grids at a point in the cycle when the exit grid is negative with respect to the entrance grid, so the electric field in the cavity between the grids opposes the electrons motion. The electrons thus do work on the electric field, and are decelerated, their kinetic energy is converted to electric potential energy, increasing the amplitude of the oscillating electric field in the cavity. Thus the oscillating field in the catcher cavity is an amplified copy of the signal applied to the buncher cavity. The amplified signal is extracted from the catcher cavity through a coaxial cable or waveguide.\n\nAfter passing through the catcher and giving up its energy, the lower energy electron beam is absorbed by a \"collector\" electrode, a second anode which is kept at a small positive voltage.\n\nAn electronic oscillator can be made from a klystron tube, by providing a feedback path from output to input by connecting the \"catcher\" and \"buncher\" cavities with a coaxial cable or waveguide. When the device is turned on, electronic noise in the cavity is amplified by the tube and fed back from the output catcher to the buncher cavity to be amplified again. Because of the high Q of the cavities, the signal quickly becomes a sine wave at the resonant frequency of the cavities.\n\nIn all modern klystrons, the number of cavities exceeds two. Additional \"buncher\" cavities added between the first \"buncher\" and the \"catcher\" may be used to increase the gain of the klystron, or to increase the bandwidth.\n\nThe residual kinetic energy in the electron beam when it hits the collector electrode represents wasted energy, which is dissipated as heat, which must be removed by a cooling system. Some modern klystrons include depressed collectors, which recover energy from the beam before collecting the electrons, increasing efficiency. Multistage depressed collectors enhance the energy recovery by \"sorting\" the electrons in energy bins.\n\nThe reflex klystron (also known as a Sutton tube after one of its inventors, Robert Sutton) was a low power klystron tube with a single cavity, which functioned as an oscillator. It was used as a local oscillator in some radar receivers and a modulator in microwave transmitters the 1950s and 60s, but is now obsolete, replaced by semiconductor microwave devices.\n\nIn the reflex klystron the electron beam passes through a single resonant cavity. The electrons are fired into one end of the tube by an electron gun. After passing through the resonant cavity they are reflected by a negatively charged reflector electrode for another pass through the cavity, where they are then collected. The electron beam is velocity modulated when it first passes through the cavity. The formation of electron bunches takes place in the drift space between the reflector and the cavity. The voltage on the reflector must be adjusted so that the bunching is at a maximum as the electron beam re-enters the resonant cavity, thus ensuring a maximum of energy is transferred from the electron beam to the RF oscillations in the cavity. The reflector voltage may be varied slightly from the optimum value, which results in some loss of output power, but also in a variation in frequency. This effect is used to good advantage for automatic frequency control in receivers, and in frequency modulation for transmitters. The level of modulation applied for transmission is small enough that the power output essentially remains constant. At regions far from the optimum voltage, no oscillations are obtained at all.\n\nThere are often several regions of reflector voltage where the reflex klystron will oscillate; these are referred to as modes. The electronic tuning range of the reflex klystron is usually referred to as the variation in frequency between half power points—the points in the oscillating mode where the power output is half the maximum output in the mode.\n\nModern semiconductor technology has effectively replaced the reflex klystron in most applications.\n\nSome klystrons have cavities that are tunable. By adjusting the frequency of individual cavities, the technician can change the operating frequency, gain, output power, or bandwidth of the amplifier. No two klystrons are exactly identical (even when comparing like part/model number klystrons). Each unit has manufacturer-supplied calibration values for its specific performance characteristics. Without this information the klystron would not be properly tunable, and hence not perform well, if at all.\n\nTuning a klystron is delicate work which, if not done properly, can cause damage to equipment or injury to the technician due to the very high voltages that could be produced. The technician must be careful not to exceed the limits of the graduations, or damage to the klystron can result. Other precautions taken when tuning a klystron include using nonferrous tools. Some klystrons employ permanent magnets. If a technician uses ferrous tools (which are ferromagnetic) and comes too close to the intense magnetic fields that contain the electron beam, such a tool can be pulled into the unit by the intense magnetic force, smashing fingers, injuring the technician, or damaging the unit. Special lightweight nonmagnetic (or rather very weakly diamagnetic) tools made of beryllium alloy have been used for tuning U.S. Air Force klystrons.\n\nPrecautions are routinely taken when transporting klystron devices in aircraft, as the intense magnetic field can interfere with magnetic navigation equipment. Special overpacks are designed to help limit this field \"in the field,\" and thus allow such devices to be transported safely.\n\nThe technique of amplification used in the klystron is also being applied experimentally at optical frequencies in a type of laser called the free-electron laser (FEL); these devices are called \"optical klystrons\". Instead of microwave cavities, these use devices called undulators. The electron beam passes through an undulator, in which a laser light beam causes bunching of the electrons. Then the beam passes through a second undulator, in which the electron bunches cause oscillation to create a second, more powerful light beam.\n\nThe floating drift tube klystron has a single cylindrical chamber containing an electrically isolated central tube. Electrically, this is similar to the two cavity oscillator klystron with a lot of feedback between the two cavities. Electrons exiting the source cavity are velocity modulated by the electric field as they travel through the drift tube and emerge at the destination chamber in bunches, delivering power to the oscillation in the cavity. This type of oscillator klystron has an advantage over the two-cavity klystron on which it is based, in that it needs only one tuning element to effect changes in frequency. The drift tube is electrically insulated from the cavity walls, and DC bias is applied separately. The DC bias on the drift tube may be adjusted to alter the transit time through it, thus allowing some electronic tuning of the oscillating frequency. The amount of tuning in this manner is not large and is normally used for frequency modulation when transmitting.\n\nKlystrons can produce far higher microwave power outputs than solid state microwave devices such as Gunn diodes. In modern systems, they are used from UHF (hundreds of megahertz) up to hundreds of gigahertz (as in the Extended Interaction Klystrons in the CloudSat satellite). Klystrons can be found at work in radar, satellite and wideband high-power communication (very common in television broadcasting and EHF satellite terminals), medicine (radiation oncology), and high-energy physics (particle accelerators and experimental reactors). At SLAC, for example, klystrons are routinely employed which have outputs in the range of 50 MW (pulse) and 50 kW (time-averaged) at 2856 MHz. The Arecibo Planetary Radar uses two klystrons that provide a total power output of 1 MW (continuous) at 2380 MHz.\n\nPopular Science's \"Best of What's New 2007\" described a company, Global Resource Corporation, currently defunct, using a klystron to convert the hydrocarbons in everyday materials, automotive waste, coal, oil shale, and oil sands into natural gas and diesel fuel.\n\n"}
{"id": "24927248", "url": "https://en.wikipedia.org/wiki?curid=24927248", "title": "Liquid resistor", "text": "Liquid resistor\n\nA liquid resistor is an electrical resistor in which the resistive element is a liquid. Fixed-value liquid resistors are typically used where very high power dissipation is required. They are used in the rotor circuits of large slip ring induction motors to control starting current, torque and to limit large electrical fault currents (while other protection systems operate to clear or isolate the fault). They typically have electrodes made of welded steel plate (galvanised to reduce corrosion), suspended by insulated connections in a conductive chemical solution held in a tank - which may be open or enclosed. The tank body is normally solidly grounded or earthed. A typical unit can be rated for continuous use, or for short periods when used for current limitation in protection systems.\n\nA common use in the electrical power generating and distribution industry is as a fault current limiter in the common neutral leg of large three-phase transformers and generators. In the UK they are known as Liquid Neutral Earthing Resistors (LNERs).\n\nA rating of 0.5 megawatt for 30 seconds would not be unusual to protect the winding of a 660 MW generator. In this application where a neutral leg current flows due to a current imbalance between the generator or transformer windings due to a fault, it is limited in magnitude by the resistor.\n\nA permanently installed current transformer (CT) fixed around the neutral leg feeder to the LNER senses the current. The CT sends a signal current to an external sensing circuit (the protection system) which then sends a signal to the relevant circuit breaker to open and disconnect the supply or isolate the generator (or transformer) from the fault.The ohmic value of the resistor is calculated as a function of the permissible fault current and the system voltage to earth.\n\nThe electrolyte is normally a custom solution of sodium carbonate (soda ash) mixed on-site for the specific job.\n\nThis salt is alkaline and electrode corrosion effects on galvanised steel tanks is manageable if re-galvanisation is possible.\n\nThe resistance of the bulk electrolyte is a function of temperature and the concentration of the salt governed by the formula:\n\nwhere \"θ\" is the temperature (in Celsius) and \"R\" is the standardised or datum calibration temperature.\n\nA specialised technique is required to accurately calibrate an LNER electrolyte bulk resistance value and it is normally carried out by experienced power engineers and technicians. This task must be done off-load at routine intervals (roughly 2–4 years depending on results from routine monitoring) to maintain the resistance value within tolerance and facilitate inspection of the LNER.\n\n"}
{"id": "446790", "url": "https://en.wikipedia.org/wiki?curid=446790", "title": "List of Betula species", "text": "List of Betula species\n\nSubgenera of genus \"Betula\" (birch), are;\n\nBark on twigs rich in methyl salicylate (oil of wintergreen). Female catkins erect.\n\nBark on twigs contains some methyl salicylate. Female catkins pendulous.\n\nBark on twigs without methyl salicylate. Female catkins erect.\n\nBark on twigs without methyl salicylate. Female catkins pendulous.\n\nSmall shrubs with small rounded leaves. Female catkins pendulous.\n\nThere is no consensus at all on species limits in \"Betula\", with different authors differing wildly in what species they accept, from under 30 species, to over 60. The above (incomplete) list was compiled from the references cited below. Birches will hybridise very freely, particularly in cultivation but also in the wild where conditions and species present permit. While differing chromosome number (diploid, tetraploid, etc.) may reduce interbreeding, it is not an absolute bar to it. Many botanists regard differing chromosome number as a specific discriminant, though not all do so (e.g. some include \"B. cordifolia\" and \"B. neoalaskana\" as varieties within \"B. papyrifera\").\n\n\n"}
{"id": "2445144", "url": "https://en.wikipedia.org/wiki?curid=2445144", "title": "List of Idaho state forests", "text": "List of Idaho state forests\n\nThe following is a list of Idaho state forests.\n\n"}
{"id": "6185965", "url": "https://en.wikipedia.org/wiki?curid=6185965", "title": "List of Sites of Special Scientific Interest in Western Isles South", "text": "List of Sites of Special Scientific Interest in Western Isles South\n\nThe following is a list of Sites of Special Scientific Interest in the Western Isles South Area of Search. For Western Isles North see List of SSSIs in Western Isles North. For SSSIs elsewhere in Scotland, see List of SSSIs by Area of Search.\n\n"}
{"id": "42797562", "url": "https://en.wikipedia.org/wiki?curid=42797562", "title": "List of invasive species in Colombia", "text": "List of invasive species in Colombia\n\nColombia's governmental organization that oversees and manages natural parks within its national borders, \"Parques Nacionales Naturales de Colombia\", has provided an official list of species that are considered to be invasive under the following resolutions:\n\n\n\n\n\n\n"}
{"id": "28162", "url": "https://en.wikipedia.org/wiki?curid=28162", "title": "List of nearest stars and brown dwarfs", "text": "List of nearest stars and brown dwarfs\n\nThere are 52 stellar systems beyond our own Solar system that currently lie within of the Sun. These systems contain a total of 63 stars, of which 50 are red dwarfs, by far the most common type of star in the Milky Way. Much more massive stars, such as our own, make up the remaining 13. In addition to these \"true\" stars, there are 11 brown dwarfs (objects not quite massive enough to fuse hydrogen), and 4 white dwarfs (extremely dense objects that remain after stars such as our Sun exhaust all fusable hydrogen in their core and slowly shed their outer layers while only the collapsed core remains). Despite the relative proximity of these objects to Earth, only nine (not including the Sun) are brighter than 6.5 apparent magnitude, the dimmest magnitude visible to the naked eye from Earth. All of these objects are located in the Local Bubble, a region within the Orion–Cygnus Arm of the Milky Way.\nBased on results from the Gaia telescope's second data release from April 2018, an estimated 694 stars will possibly approach the Solar system to less than over the next 15 million years. Of these, 26 have a good probability to come within and another 7 within . This number is likely much higher, due to the sheer number of stars needed to be surveyed; a star approaching the Solar System 10 million years ago, moving at a typical Sun-relative 20–200 kilometers per second, would be 600–6,000 light years from the Sun at present day, with millions of stars closer to the Sun. The closest encounter to the Sun so far predicted is the low-mass orange dwarf star Gliese 710 / HIP 89825 with roughly 60% the mass of the Sun. It is currently predicted to pass from the Sun in million years from the present, close enough to significantly disturb our Solar System's Oort cloud.\nThe easiest way to determine stellar distance to the Sun for objects at these distances is parallax, which measures how much stars appear to move against background objects over the course of Earth's orbit around the Sun. As a parsec (parallax-second) is defined by the distance of an object that would appear to move exactly one second of arc against background objects, stars less than 5 parsecs away will have measured parallaxes of over 0.2 arcseconds, or 200 milliarcseconds. Determining past and future positions relies on accurate astrometric measurements of their parallax and total proper motions (how far they move across the sky due to their actual velocity relative to the Sun), along with spectroscopically determined radial velocities (their speed directly towards or away from us, which combined with proper motion defines their true movement through the sky relative to the Sun). Both of these measurements are subject to increasing and significant errors over very long time spans, especially over the several thousand-year time spans it takes for stars to noticeably move relative to each other.\n\nThe classes of the stars and brown dwarfs are shown in the color of their spectral types (these colors are derived from conventional names for the spectral types and do not represent the star's observed color). Many brown dwarfs are not listed by visual magnitude but are listed by near-infrared J band apparent magnitude due to how dim (and often invisible) they are in visible colors. Some of the parallax and distance results are preliminary measurements.\n\nOver long periods of time, the slow independent motion of stars change in both relative position and in their distance from the observer. This can cause other currently distant stars to fall within a stated range, which may be readily calculated and predicted using accurate astrometric measurements of parallax and total proper motions, along with spectroscopically determined radial velocities. Although predictions can be extrapolated back into the past or forward into the future, they are subject to increasing significant cumulative errors over very long periods. Inaccuracies of these measured parameters make determining the true minimum distances of any encountering stars or brown dwarfs fairly difficult.\n\nOne of the first stars known to approach the Sun particularly close is Gliese 710. The star, whose mass is roughly half that of the Sun, is currently 62 light-years from the Solar System. It was first noticed in 1999 using data from the Hipparcos satellite, and was estimated to pass less than from the Sun in 1.4 million years. With the release of Gaia's observations of the star, it has since been refined to a much closer , close enough to significantly disturb objects in the Oort cloud, which extends out to from the Sun.\n\nThe second closest object known to approach the Sun was only discovered in 2018 after Gaia's second data release, known as 2MASS J0610-4246. Its approach has not been fully described due to it being a distant binary star with a red dwarf, but almost certainly passed less than 1 light year from the Solar System roughly 1.16 million years ago.\n\n\n"}
{"id": "9125092", "url": "https://en.wikipedia.org/wiki?curid=9125092", "title": "List of rivers of Denmark", "text": "List of rivers of Denmark\n\nRivers of Denmark.\n\n"}
{"id": "8591869", "url": "https://en.wikipedia.org/wiki?curid=8591869", "title": "List of stars in Telescopium", "text": "List of stars in Telescopium\n\nThis is the list of notable stars in the constellation Telescopium, sorted by decreasing brightness.\n\n\n"}
{"id": "34542671", "url": "https://en.wikipedia.org/wiki?curid=34542671", "title": "MCACEA", "text": "MCACEA\n\nMCACEA (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) is a general framework that uses a single evolutionary algorithm (EA) per agent sharing their optimal solutions to coordinate the evolutions of the EAs populations using cooperation objectives. This framework can be used to optimize some characteristics of multiple cooperating agents in mathematical optimization problems. More specifically, due to its nature in which both individual and cooperation objectives are optimize, MCACEA is used in multi-objective optimization problems.\n\nMCACEA, uses multiple EAs (one per each agent) that evolve their own populations to find the best solution for its associated problem according to their individual and cooperation constraints and objective indexes. Each EA is an optimization problem that runs in parallel and that exchanges some information with the others during its evaluation step. This information is needed to let each EA to measure the coordination objectives of the solutions encoded in its own population, taking into account the possible optimal solutions of the remaining populations of the other EAs. With this purpose, each single EA receives information related to the best solutions of the remaining ones before evaluating the cooperative objectives of each possible solution of its own population.\n\nAs the cooperation objective values depend on the best solutions of the other populations and the optimality of a solution depends both on the individual and cooperation objectives, it is not really possible to select and send the best solution of \neach planner to the others. However, MCACEA divides the evaluation step inside each EA in three parts: In the first part, the \nEAs identify the best solution considering only its individual objective values and send it to the others EAs; in the second part, the cooperation objective values of all solutions are calculated taking into account the received information; and in the third part, the EAs calculate the fitness of the solutions considering all the individual and cooperation objective values. \n\nAlthough each population can only offer a unique optimal solution, each EA maintains a pareto set of optimal solutions and selects the unique optimal solution at the end, when the last population has already been obtained. Therefore, to be able to determine a unique optimal solution according with the individual objectives in each generation (and so, using it with the MCACEA framework), a step in charge of selecting the final optimal solution must also be included in the evaluation step of each EA.\n\nThe complete evaluation phase of the individual cooperating EAs is divided in six steps. When searching for the solution of a single EA, only the first two steps of this new evaluation process are used. MCACEA extends this process from these two only steps to the next six:\n\n1. Evaluating the individual objectives of each solution.\n\n2. Calculating the fitness of each solution with the single evaluation function (containing only the individual objectives).\n\n3. Finding the best solution of the population.\n\n4. Sending (and receiving) the best solution to (of) the other single EAs.\n\n5. Calculating the cooperation objectives taking into account the received information from the other EAs.\n\n6. Calculating the fitness of each solution with the complete evaluation function (containing both the individual and the cooperation objectives), which have been obtained in steps 1 and 5.\n\nAlthough MCACEA may look similar to the habitual parallelization of EAs, in this case, instead of distributing the solutions of the whole problem between different EAs that share their solutions periodically, the algorithm is dividing the problem into smaller problems that are solved simultaneously by each EA taking into account the solutions of the part of the problems that the other EAs are obtaining.\n\nAnother possibility, is to send the best completely evaluated solutions of the previous generation to the other EAs instead of our current best only individual objective evaluated one. Nevertheless, that approach introduces a bias toward outdated completely evaluated trajectories, while MCACEA does it toward currently good individual objective evaluated ones.\n\nMCACEA has been used for finding and optimizing unmanned aerial vehicles (UAVs) trajectories when flying simultaneously in the same scenario.\n\n\nL. de la Torre, J. M. de la Cruz, and B. Andrés-Toro. \"Evolutionary trajectory planner for multiple UAVs in realistic scenarios\". IEEE Transactions on Robotics, vol. 26, no. 4, pp. 619–634, August 2010.\n"}
{"id": "2088962", "url": "https://en.wikipedia.org/wiki?curid=2088962", "title": "Maffia", "text": "Maffia\n\nMaffia (\"Maffie\" or \"Mafie\" in Czech) was a secret organization acting during World War I. It was founded after emigration of Tomáš Garrigue Masaryk in 1914 by Czech politician Edvard Beneš, who later became second president of Czechoslovakia, and others main anti-royalist (Karel Kramář, Alois Rašín, Josef Scheiner and Přemysl Šámal). Maffia was based on the principles of Sicilian Mafia (name \"Maffia\"), it was central part of the First Czechoslovak Resistance and its main objective was to overthrow the Emperor of Austria and the disintegration of his country. Maffia plot against Austria-Hungary and Central Powers was revealed by police in 1915, some members of Maffia (Karel Kramář, Alois Rašín, Vincent Červinka and Josef Zamazal) were arrested and sentenced to death (they were later amnestied by emperor Charles I.) and Edvard Beneš escaped from Austria-Hungary to Switzerland, but subversive activities of Maffia continued, under the leadership of Přemysl Šámal, until the end of the First World War. Maffia had over 200 members and it was supported from abroad (US, Italy etc.).\n"}
{"id": "206284", "url": "https://en.wikipedia.org/wiki?curid=206284", "title": "Mare Insularum", "text": "Mare Insularum\n\nMare Insularum (the \"sea of islands\") is a lunar mare located in the Insularum basin just south of Mare Imbrium. The basin material is of the Lower Imbrian epoch, with the mare material of the Upper Imbrian epoch. The mare is bordered by the craters Copernicus on the east, and Kepler on the west. Oceanus Procellarum joins the mare to the southwest.\n\nCopernicus is one of the most noticeable craters on the Moon. The rays from both Kepler and Copernicus protrude into the mare. It is located near the crater Fra Mauro, the site of the Apollo 14 landing. Sinus Aestuum forms a northeastern extension to the mare.\n\nThe name was suggested by lunar geologist Don E. Wilhelms.\n"}
{"id": "19673093", "url": "https://en.wikipedia.org/wiki?curid=19673093", "title": "Matter", "text": "Matter\n\nIn the classical physics and in the chemistry of things of everyday life, matter is any substance that has mass and takes up space by having volume. All everyday objects that we can touch are ultimately composed of atoms, which are made up of interacting subatomic particles, and in everyday as well as scientific usage, \"matter\" generally includes atoms and anything made up of them, and any particles (or combination of particles) that act as if they have both rest mass and volume. However it does not include massless particles such as photons, or other energy phenomena or waves such as light or sound. Matter exists in various \"states\" (also known as \"phases\"). These include classical everyday phases such as solid, liquid, and gas – for example water exists as ice, liquid water, and gaseous steam – but other states are possible, including plasma, Bose–Einstein condensates, fermionic condensates, and quark–gluon plasma.\n\nUsually atoms can be imagined as a nucleus of protons and neutrons, and a surrounding \"cloud\" of orbiting electrons which \"take up space\". However this is only somewhat correct, because subatomic particles and their properties are governed by their quantum nature, which means they do not act as everyday objects appear to act – they can act like waves as well as particles and they do not have well-defined sizes or positions. In the Standard Model of particle physics, matter is not a fundamental concept because the elementary constituents of atoms are quantum entities which do not have an inherent \"size\" or \"volume\" in any everyday sense of the word. Due to the exclusion principle and other fundamental interactions, some \"point particles\" known as fermions (quarks, leptons), and many composites and atoms, are effectively forced to keep a distance from other particles under everyday conditions; this creates the property of matter which appears to us as matter taking up space.\n\nFor much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks, the so-called \"particulate theory of matter\", was first put forward by the Greek philosophers Leucippus (~490 BC) and Democritus (~470–380 BC).\n\nMatter should not be confused with mass, as the two are not the same in modern physics. Matter is a general term describing any 'physical substance'. By contrast, mass is not a substance but rather a quantitative \"property\" of matter and other substances or systems; various types of mass are defined within physics – including but not limited to rest mass, inertial mass, relativistic mass, mass–energy. \n\nWhile there are different views on what should be considered matter, the mass of a substance has exact scientific definitions. Another difference is that matter has an \"opposite\" called antimatter, but mass has no opposite—there is no such thing as \"anti-mass\" or negative mass, so far as is known, although scientists do discuss the concept. Antimatter has the same (i.e. positive) mass property as its normal matter counterpart.\n\nDifferent fields of science use the term matter in different, and sometimes incompatible, ways. Some of these ways are based on loose historical meanings, from a time when there was no reason to distinguish mass from simply a quantity of matter. As such, there is no single universally agreed scientific meaning of the word \"matter\". Scientifically, the term \"mass\" is well-defined, but \"matter\" can be defined in several ways. Sometimes in the field of physics \"matter\" is simply equated with particles that exhibit rest mass (i.e., that cannot travel at the speed of light), such as quarks and leptons. However, in both physics and chemistry, matter exhibits both wave-like and particle-like properties, the so-called wave–particle duality.\n\nA definition of \"matter\" based on its physical and chemical structure is: \"matter is made up of atoms\". Such \"atomic matter\" is also sometimes termed \"ordinary matter\". As an example, deoxyribonucleic acid molecules (DNA) are matter under this definition because they are made of atoms. This definition can be extended to include charged atoms and molecules, so as to include plasmas (gases of ions) and electrolytes (ionic solutions), which are not obviously included in the atoms definition. Alternatively, one can adopt the \"protons, neutrons, and electrons\" definition.\n\nA definition of \"matter\" more fine-scale than the atoms and molecules definition is: \"matter is made up of what atoms and molecules are made of\", meaning anything made of positively charged protons, neutral neutrons, and negatively charged electrons. This definition goes beyond atoms and molecules, however, to include substances made from these building blocks that are \"not\" simply atoms or molecules, for example electron beams in an old cathode ray tube television, or white dwarf matter—typically, carbon and oxygen nuclei in a sea of degenerate electrons. At a microscopic level, the constituent \"particles\" of matter such as protons, neutrons, and electrons obey the laws of quantum mechanics and exhibit wave–particle duality. At an even deeper level, protons and neutrons are made up of quarks and the force fields (gluons) that bind them together, leading to the next definition.\n\nAs seen in the above discussion, many early definitions of what can be called \"ordinary matter\" were based upon its structure or \"building blocks\". On the scale of elementary particles, a definition that follows this tradition can be stated as: \n\"ordinary matter is everything that is composed of quarks and leptons\", or \"ordinary matter is everything that is composed of any elementary fermions except antiquarks and antileptons\". The connection between these formulations follows.\n\nLeptons (the most famous being the electron), and quarks (of which baryons, such as protons and neutrons, are made) combine to form atoms, which in turn form molecules. Because atoms and molecules are said to be matter, it is natural to phrase the definition as: \"ordinary matter is anything that is made of the same things that atoms and molecules are made of\". (However, notice that one also can make from these building blocks matter that is \"not\" atoms or molecules.) Then, because electrons are leptons, and protons, and neutrons are made of quarks, this definition in turn leads to the definition of matter as being \"quarks and leptons\", which are two of the four types of elementary fermions (the other two being antiquarks and antileptons, which can be considered antimatter as described later). Carithers and Grannis state: \"Ordinary matter is composed entirely of first-generation particles, namely the [up] and [down] quarks, plus the electron and its neutrino.\" (Higher generations particles quickly decay into first-generation particles, and thus are not commonly encountered.)\n\nThis definition of ordinary matter is more subtle than it first appears. All the particles that make up ordinary matter (leptons and quarks) are elementary fermions, while all the force carriers are elementary bosons. The W and Z bosons that mediate the weak force are not made of quarks or leptons, and so are not ordinary matter, even if they have mass. In other words, mass is not something that is exclusive to ordinary matter.\n\nThe quark–lepton definition of ordinary matter, however, identifies not only the elementary building blocks of matter, but also includes composites made from the constituents (atoms and molecules, for example). Such composites contain an interaction energy that holds the constituents together, and may constitute the bulk of the mass of the composite. As an example, to a great extent, the mass of an atom is simply the sum of the masses of its constituent protons, neutrons and electrons. However, digging deeper, the protons and neutrons are made up of quarks bound together by gluon fields (see dynamics of quantum chromodynamics) and these gluons fields contribute significantly to the mass of hadrons. In other words, most of what composes the \"mass\" of ordinary matter is due to the binding energy of quarks within protons and neutrons. For example, the sum of the mass of the three quarks in a nucleon is approximately , which is low compared to the mass of a nucleon (approximately ). The bottom line is that most of the mass of everyday objects comes from the interaction energy of its elementary components.\n\nThe Standard Model groups matter particles into three generations, where each generation consists of two quarks and two leptons. The first generation is the \"up\" and \"down\" quarks, the \"electron\" and the \"electron neutrino\"; the second includes the \"charm\" and \"strange\" quarks, the \"muon\" and the \"muon neutrino\"; the third generation consists of the \"top\" and \"bottom\" quarks and the \"tau\" and \"tau neutrino\". The most natural explanation for this would be that quarks and leptons of higher generations are excited states of the first generations. If this turns out to be the case, it would imply that quarks and leptons are composite particles, rather than elementary particles.\n\nThis quark–lepton definition of matter also leads to what can be described as \"conservation of (net) matter\" laws—discussed later below. Alternatively, one could return to the mass–volume–space concept of matter, leading to the next definition, in which antimatter becomes included as a subclass of matter.\n\nA common or traditional definition of matter is \"anything that has mass and volume (occupies space)\". For example, a car would be said to be made of matter, as it has mass and volume (occupies space).\n\nThe observation that matter occupies space goes back to antiquity. However, an explanation for why matter occupies space is recent, and is argued to be a result of the phenomenon described in the Pauli exclusion principle, which applies to fermions. Two particular examples where the exclusion principle clearly relates matter to the occupation of space are white dwarf stars and neutron stars, discussed further below.\n\nThus, matter can be defined as everything composed of elementary fermions. Although we don't encounter them in everyday life, antiquarks (such as the antiproton) and antileptons (such as the positron) are the antiparticles of the quark and the lepton, are elementary fermions as well, and have essentially the same properties as quarks and leptons, including the applicability of the Pauli exclusion principle which can be said to prevent two particles from being in the same place at the same time (in the same state), i.e. makes each particle \"take up space\". This particular definition leads to matter being defined to include anything made of these antimatter particles as well as the ordinary quark and lepton, and thus also anything made of mesons, which are unstable particles made up of a quark and an antiquark.\n\nIn the context of relativity, mass is not an additive quantity, in the sense that one can not add the rest masses of particles in a system to get the total rest mass of the system. Thus, in relativity usually a more general view is that it is not the sum of rest masses, but the energy–momentum tensor that quantifies the amount of matter. This tensor gives the rest mass for the entire system. \"Matter\" therefore is sometimes considered as anything that contributes to the energy–momentum of a system, that is, anything that is not purely gravity. This view is commonly held in fields that deal with general relativity such as cosmology. In this view, light and other massless particles and fields are all part of \"matter\".\n\nIn particle physics, fermions are particles that obey Fermi–Dirac statistics. Fermions can be elementary, like the electron—or composite, like the proton and neutron. In the Standard Model, there are two types of elementary fermions: quarks and leptons, which are discussed next.\n\nQuarks are particles of spin-, implying that they are fermions. They carry an electric charge of − e (down-type quarks) or + e (up-type quarks). For comparison, an electron has a charge of −1 e. They also carry colour charge, which is the equivalent of the electric charge for the strong interaction. Quarks also undergo radioactive decay, meaning that they are subject to the weak interaction. Quarks are massive particles, and therefore are also subject to gravity.\n\nBaryons are strongly interacting fermions, and so are subject to Fermi–Dirac statistics. Amongst the baryons are the protons and neutrons, which occur in atomic nuclei, but many other unstable baryons exist as well. The term baryon usually refers to triquarks—particles made of three quarks. Also, \"exotic\" baryons made of four quarks and one antiquark are known as pentaquarks, but their existence is not generally accepted.\n\nBaryonic matter is the part of the universe that is made of baryons (including all atoms). This part of the universe does not include dark energy, dark matter, black holes or various forms of degenerate matter, such as compose white dwarf stars and neutron stars. Microwave light seen by Wilkinson Microwave Anisotropy Probe (WMAP), suggests that only about 4.6% of that part of the universe within range of the best telescopes (that is, matter that may be visible because light could reach us from it), is made of baryonic matter. About 26.8% is dark matter, and about 68.3% is dark energy.\n\nAs a matter of fact, the great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 per cent of the ordinary matter contribution to the mass–energy density of the universe.\n\nHadronic matter can refer to 'ordinary' baryonic matter, made from hadrons (Baryons and mesons), or quark matter (a generalisation of atomic nuclei), ie. the 'low' temperature QCD matter. It includes degenerate matter and the result of high energy heavy nuclei collisions. Distinct from dark matter.\n\nIn physics, \"degenerate matter\" refers to the ground state of a gas of fermions at a temperature near absolute zero. The Pauli exclusion principle requires that only two fermions can occupy a quantum state, one spin-up and the other spin-down. Hence, at zero temperature, the fermions fill up sufficient levels to accommodate all the available fermions—and in the case of many fermions, the maximum kinetic energy (called the \"Fermi energy\") and the pressure of the gas becomes very large, and depends on the number of fermions rather than the temperature, unlike normal states of matter.\n\nDegenerate matter is thought to occur during the evolution of heavy stars. The demonstration by Subrahmanyan Chandrasekhar that white dwarf stars have a maximum allowed mass because of the exclusion principle caused a revolution in the theory of star evolution.\n\nDegenerate matter includes the part of the universe that is made up of neutron stars and white dwarfs.\n\n\"Strange matter\" is a particular form of quark matter, usually thought of as a \"liquid\" of up, down, and strange quarks. It is contrasted with nuclear matter, which is a liquid of neutrons and protons (which themselves are built out of up and down quarks), and with non-strange quark matter, which is a quark liquid that contains only up and down quarks. At high enough density, strange matter is expected to be color superconducting. Strange matter is hypothesized to occur in the core of neutron stars, or, more speculatively, as isolated droplets that may vary in size from femtometers (strangelets) to kilometers (quark stars).\n\nIn particle physics and astrophysics, the term is used in two ways, one broader and the other more specific.\n\nLeptons are particles of spin-, meaning that they are fermions. They carry an electric charge of −1 e (charged leptons) or 0 e (neutrinos). Unlike quarks, leptons do not carry colour charge, meaning that they do not experience the strong interaction. Leptons also undergo radioactive decay, meaning that they are subject to the weak interaction. Leptons are massive particles, therefore are subject to gravity.\n\nIn bulk, matter can exist in several different forms, or states of aggregation, known as \"phases\", depending on ambient pressure, temperature and volume. A phase is a form of matter that has a relatively uniform chemical composition and physical properties (such as density, specific heat, refractive index, and so forth). These phases include the three familiar ones (solids, liquids, and gases), as well as more exotic states of matter (such as plasmas, superfluids, supersolids, Bose–Einstein condensates, ...). A \"fluid\" may be a liquid, gas or plasma. There are also paramagnetic and ferromagnetic phases of magnetic materials. As conditions change, matter may change from one phase into another. These phenomena are called phase transitions, and are studied in the field of thermodynamics. In nanomaterials, the vastly increased ratio of surface area to volume results in matter that can exhibit properties entirely different from those of bulk material, and not well described by any bulk phase (see nanomaterials for more details).\n\nPhases are sometimes called \"states of matter\", but this term can lead to confusion with thermodynamic states. For example, two gases maintained at different pressures are in different \"thermodynamic states\" (different pressures), but in the same \"phase\" (both are gases).\n\nIn particle physics and quantum chemistry, \"antimatter\" is matter that is composed of the antiparticles of those that constitute ordinary matter. If a particle and its antiparticle come into contact with each other, the two annihilate; that is, they may both be converted into other particles with equal energy in accordance with Einstein's equation . These new particles may be high-energy photons (gamma rays) or other particle–antiparticle pairs. The resulting particles are endowed with an amount of kinetic energy equal to the difference between the rest mass of the products of the annihilation and the rest mass of the original particle–antiparticle pair, which is often quite large. Depending on which definition of \"matter\" is adopted, antimatter can be said to be a particular subclass of matter, or the opposite of matter.\n\nAntimatter is not found naturally on Earth, except very briefly and in vanishingly small quantities (as the result of radioactive decay, lightning or cosmic rays). This is because antimatter that came to exist on Earth outside the confines of a suitable physics laboratory would almost instantly meet the ordinary matter that Earth is made of, and be annihilated. Antiparticles and some stable antimatter (such as antihydrogen) can be made in tiny amounts, but not in enough quantity to do more than test a few of its theoretical properties.\n\nThere is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter (in the sense of quarks and leptons but not antiquarks or antileptons), and whether other places are almost entirely antimatter (antiquarks and antileptons) instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called CP (charge-parity) symmetry violation, which can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.\n\nFormally, antimatter particles can be defined by their negative baryon number or lepton number, while \"normal\" (non-antimatter) matter particles have positive baryon or lepton number. These two classes of particles are the antiparticle partners of one another.\n\nIn October 2017, scientists reported further evidence that matter and antimatter, equally produced at the Big Bang, are identical, should completely annihilate each other and, as a result, the universe should not exist. This implies that there must be something, as yet unknown to scientists, that either stopped the complete mutual destruction of matter and antimatter in the early forming universe, or that gave rise to an imbalance between the two forms.\n\nTwo quantities that can define an amount of matter in the quark–lepton sense (and antimatter in an antiquark–antilepton sense), baryon number and lepton number, are conserved in the Standard Model. A baryon such as the proton or neutron has a baryon number of one, and a quark, because there are three in a baryon, is given a baryon number of 1/3. So the net amount of matter, as measured by the number of quarks (minus the number of antiquarks, which each have a baryon number of −1/3), which is proportional to baryon number, and number of leptons (minus antileptons), which is called the lepton number, is practically impossible to change in any process. Even in a nuclear bomb, none of the baryons (protons and neutrons of which the atomic nuclei are composed) are destroyed—there are as many baryons after as before the reaction, so none of these matter particles are actually destroyed and none are even converted to non-matter particles (like photons of light or radiation). Instead, nuclear (and perhaps chromodynamic) binding energy is released, as these baryons become bound into mid-size nuclei having less energy (and, equivalently, less mass) per nucleon compared to the original small (hydrogen) and large (plutonium etc.) nuclei. Even in electron–positron annihilation, there is no net matter being destroyed, because there was zero net matter (zero total lepton number and baryon number) to begin with before the annihilation—one lepton minus one antilepton equals zero net lepton number—and this net amount matter does not change as it simply remains zero after the annihilation. So the only way to really \"destroy\" or \"convert\" ordinary matter is to pair it with the same amount of antimatter so that their \"matterness\" cancels out—but in practice there is almost no antimatter generally available in the universe (see baryon asymmetry and leptogenesis) with which to do so.\n\nOrdinary matter, in the quarks and leptons definition, constitutes about 4% of the energy of the observable universe. The remaining energy is theorized to be due to exotic forms, of which 23% is dark matter and 73% is dark energy.\n\nIn astrophysics and cosmology, \"dark matter\" is matter of unknown composition that does not emit or reflect enough electromagnetic radiation to be observed directly, but whose presence can be inferred from gravitational effects on visible matter. Observational evidence of the early universe and the Big Bang theory require that this matter have energy and mass, but is not composed ordinary baryons (protons and neutrons). The commonly accepted view is that most of the dark matter is non-baryonic in nature. As such, it is composed of particles as yet unobserved in the laboratory. Perhaps they are supersymmetric particles, which are not Standard Model particles, but relics formed at very high energies in the early phase of the universe and still floating about.\n\nIn cosmology, \"dark energy\" is the name given to source of the repelling influence that is accelerating the rate of expansion of the universe. Its precise nature is currently a mystery, although its effects can reasonably be modeled by assigning matter-like properties such as energy density and pressure to the vacuum itself.\nExotic matter is a concept of particle physics, which may include dark matter and dark energy but goes further to include any hypothetical material that violates one or more of the properties of known forms of matter. Some such materials might possess hypothetical properties like negative mass.\n\nThe pre-Socratics were among the first recorded speculators about the underlying nature of the visible world. Thales (c. 624 BC–c. 546 BC) regarded water as the fundamental material of the world. Anaximander (c. 610 BC–c. 546 BC) posited that the basic material was wholly characterless or limitless: the Infinite (\"apeiron\"). Anaximenes (flourished 585 BC, d. 528 BC) posited that the basic stuff was \"pneuma\" or air. Heraclitus (c. 535–c. 475 BC) seems to say the basic element is fire, though perhaps he means that all is change. Empedocles (c. 490–430 BC) spoke of four elements of which everything was made: earth, water, air, and fire. Meanwhile, Parmenides argued that change does not exist, and Democritus argued that everything is composed of minuscule, inert bodies of all shapes called atoms, a philosophy called atomism. All of these notions had deep philosophical problems.\n\nAristotle (384 BC – 322 BC) was the first to put the conception on a sound philosophical basis, which he did in his natural philosophy, especially in \"Physics\" book I. He adopted as reasonable suppositions the four Empedoclean elements, but added a fifth, aether. Nevertheless, these elements are not basic in Aristotle's mind. Rather they, like everything else in the visible world, are composed of the basic \"principles\" matter and form.\n\nThe word Aristotle uses for matter, ὕλη (\"hyle\" or \"hule\"), can be literally translated as wood or timber, that is, \"raw material\" for building. Indeed, Aristotle's conception of matter is intrinsically linked to something being made or composed. In other words, in contrast to the early modern conception of matter as simply occupying space, matter for Aristotle is definitionally linked to process or change: matter is what underlies a change of substance. For example, a horse eats grass: the horse changes the grass into itself; the grass as such does not persist in the horse, but some aspect of it—its matter—does. The matter is not specifically described (e.g., as atoms), but consists of whatever persists in the change of substance from grass to horse. Matter in this understanding does not exist independently (i.e., as a substance), but exists interdependently (i.e., as a \"principle\") with form and only insofar as it underlies change. It can be helpful to conceive of the relationship of matter and form as very similar to that between parts and whole. For Aristotle, matter as such can only \"receive\" actuality from form; it has no activity or actuality in itself, similar to the way that parts as such only have their existence \"in\" a whole (otherwise they would be independent wholes).\n\nRené Descartes (1596–1650) originated the modern conception of matter. He was primarily a geometer. Instead of, like Aristotle, deducing the existence of matter from the physical reality of change, Descartes arbitrarily postulated matter to be an abstract, mathematical substance that occupies space:\nFor Descartes, matter has only the property of extension, so its only activity aside from locomotion is to exclude other bodies: this is the mechanical philosophy. Descartes makes an absolute distinction between mind, which he defines as unextended, thinking substance, and matter, which he defines as unthinking, extended substance. They are independent things. In contrast, Aristotle defines matter and the formal/forming principle as complementary \"principles\" that together compose one independent thing (substance). In short, Aristotle defines matter (roughly speaking) as what things are actually made of (with a \"potential\" independent existence), but Descartes elevates matter to an actual independent thing in itself.\n\nThe continuity and difference between Descartes' and Aristotle's conceptions is noteworthy. In both conceptions, matter is passive or inert. In the respective conceptions matter has different relationships to intelligence. For Aristotle, matter and intelligence (form) exist together in an interdependent relationship, whereas for Descartes, matter and intelligence (mind) are definitionally opposed, independent substances.\n\nDescartes' justification for restricting the inherent qualities of matter to extension is its permanence, but his real criterion is not permanence (which equally applied to color and resistance), but his desire to use geometry to explain all material properties. Like Descartes, Hobbes, Boyle, and Locke argued that the inherent properties of bodies were limited to extension, and that so-called secondary qualities, like color, were only products of human perception.\n\nIsaac Newton (1643–1727) inherited Descartes' mechanical conception of matter. In the third of his \"Rules of Reasoning in Philosophy\", Newton lists the universal qualities of matter as \"extension, hardness, impenetrability, mobility, and inertia\". Similarly in \"Optics\" he conjectures that God created matter as \"solid, massy, hard, impenetrable, movable particles\", which were \"...even so very hard as never to wear or break in pieces\". The \"primary\" properties of matter were amenable to mathematical description, unlike \"secondary\" qualities such as color or taste. Like Descartes, Newton rejected the essential nature of secondary qualities.\n\nNewton developed Descartes' notion of matter by restoring to matter intrinsic properties in addition to extension (at least on a limited basis), such as mass. Newton's use of gravitational force, which worked \"at a distance\", effectively repudiated Descartes' mechanics, in which interactions happened exclusively by contact.\n\nThough Newton's gravity would seem to be a \"power\" of bodies, Newton himself did not admit it to be an \"essential\" property of matter. Carrying the logic forward more consistently, Joseph Priestley (1733–1804) argued that corporeal properties transcend contact mechanics: chemical properties require the \"capacity\" for attraction. He argued matter has other inherent powers besides the so-called primary qualities of Descartes, et al.\n\nSince Priestley's time, there has been a massive expansion in knowledge of the constituents of the material world (viz., molecules, atoms, subatomic particles), but there has been no further development in the \"definition\" of matter. Rather the question has been set aside. Noam Chomsky (born 1928) summarizes the situation that has prevailed since that time:\nSo matter is whatever physics studies and the object of study of physics is matter: there is no independent general definition of matter, apart from its fitting into the methodology of measurement and controlled experimentation. In sum, the boundaries between what constitutes matter and everything else remains as vague as the demarcation problem of delimiting science from everything else.\n\nIn the 19th century, following the development of the periodic table, and of atomic theory, atoms were seen as being the fundamental constituents of matter; atoms formed molecules and compounds.\n\nThe common definition in terms of occupying space and having mass is in contrast with most physical and chemical definitions of matter, which rely instead upon its structure and upon attributes not necessarily related to volume and mass. At the turn of the nineteenth century, the knowledge of matter began a rapid evolution.\n\nAspects of the Newtonian view still held sway. James Clerk Maxwell discussed matter in his work \"Matter and Motion\". He carefully separates \"matter\" from space and time, and defines it in terms of the object referred to in Newton's first law of motion.\n\nHowever, the Newtonian picture was not the whole story. In the 19th century, the term \"matter\" was actively discussed by a host of scientists and philosophers, and a brief outline can be found in Levere. A textbook discussion from 1870 suggests matter is what is made up of atoms:Three divisions of matter are recognized in science: masses, molecules and atoms. A Mass of matter is any portion of matter appreciable by the senses. A Molecule is the smallest particle of matter into which a body can be divided without losing its identity. An Atom is a still smaller particle produced by division of a molecule. \n\nRather than simply having the attributes of mass and occupying space, matter was held to have chemical and electrical properties. In 1909 the famous physicist J. J. Thomson (1856–1940) wrote about the \"constitution of matter\" and was concerned with the possible connection between matter and electrical charge.\n\nThere is an entire literature concerning the \"structure of matter\", ranging from the \"electrical structure\" in the early 20th century, to the more recent \"quark structure of matter\", introduced today with the remark: \"Understanding the quark structure of matter has been one of the most important advances in contemporary physics.\" In this connection, physicists speak of \"matter fields\", and speak of particles as \"quantum excitations of a mode of the matter field\". And here is a quote from de Sabbata and Gasperini: \"With the word \"matter\" we denote, in this context, the sources of the interactions, that is spinor fields (like quarks and leptons), which are believed to be the fundamental components of matter, or scalar fields, like the Higgs particles, which are used to introduced mass in a gauge theory (and that, however, could be composed of more fundamental fermion fields).\"\n\nIn the late 19th century with the discovery of the electron, and in the early 20th century, with the discovery of the atomic nucleus, and the birth of particle physics, matter was seen as made up of electrons, protons and neutrons interacting to form atoms. Today, we know that even protons and neutrons are not indivisible, they can be divided into quarks, while electrons are part of a particle family called leptons. Both quarks and leptons are elementary particles, and are currently seen as being the fundamental constituents of matter.\n\nThese quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.\n\nThe modern conception of matter has been refined many times in history, in light of the improvement in knowledge of just \"what\" the basic building blocks are, and in how they interact.\nThe term \"matter\" is used throughout physics in a bewildering variety of contexts: for example, one refers to \"condensed matter physics\", \"elementary matter\", \"partonic\" matter, \"dark\" matter, \"anti\"-matter, \"strange\" matter, and \"nuclear\" matter. In discussions of matter and antimatter, normal matter has been referred to by Alfvén as \"koinomatter\" (Gk. \"common matter\"). It is fair to say that in physics, there is no broad consensus as to a general definition of matter, and the term \"matter\" usually is used in conjunction with a specifying modifier.\n\nThe history of the concept of matter is a history of the fundamental \"length scales\" used to define matter. Different building blocks apply depending upon whether one defines matter on an atomic or elementary particle level. One may use a definition that matter is atoms, or that matter is hadrons, or that matter is leptons and quarks depending upon the scale at which one wishes to define matter.\n\nThese quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton).\n\nAntimatter\n\nCosmology\nDark matter\n\nPhilosophy\nOther\n\nMy Pals Are Here! Science P3&4 Cycless-Chapter 3(Matter)\n\n"}
{"id": "10465505", "url": "https://en.wikipedia.org/wiki?curid=10465505", "title": "National Development and Reform Commission", "text": "National Development and Reform Commission\n\nThe National Development and Reform Commission of the People's Republic of China (NDRC), formerly State Planning Commission and State Development Planning Commission, is a macroeconomic management agency under the Chinese State Council, which has broad administrative and planning control over the Chinese economy. The candidate for the chairperson of the NDRC is nominated by the Premier of the People's Republic of China and approved by the National People's Congress. Since February 2017 the Commission has been headed by He Lifeng.\n\nThe NDRC's functions are to study and formulate policies for economic and social development, maintain the balance of economic development, and to guide restructuring of China's economic system. The NDRC has twenty-six functional departments/bureaus/offices with an authorized staff size of 890 civil servants.\n\n\n\nNEA was established in August 2008, replacing the National Energy Bureau (NEB; 国家能源局) which attempted to reform China’s highly dispersed energy management.\n\n\n"}
{"id": "149415", "url": "https://en.wikipedia.org/wiki?curid=149415", "title": "Neptunism", "text": "Neptunism\n\nNeptunism is a superseded scientific theory of geology proposed by Abraham Gottlob Werner (1749–1817) in the late 18th century, proposing that rocks formed from the crystallisation of minerals in the early Earth's oceans.\n\nThe theory took its name from Neptune, the ancient Roman god of the sea. There was considerable debate between its proponents (neptunists) and those favouring a rival theory known as plutonism which gave a significant role to volcanic origins, and which in modified form replaced neptunism in the early 19th century as the principle of uniformitarianism was shown to fit better with the geological facts as they became better known.\n\nModern geology acknowledges many different forms of rock formation, and explains the formation of sedimentary rock through processes very similar to those described by neptunism.\n\nIn the mid-eighteenth century as the investigation of geology found evidence such as fossils, naturalists developed new ideas which diverged from the Genesis creation narrative. Georges de Buffon proposed that the Earth was over 75,000 years old, possibly much older, and showed signs of historical development in a series of distinct epochs.\n\nAbraham Gottlob Werner was the inspector of mines and professor of mining and mineralogy at the Mining Academy in Freiberg (Saxony) which became dominant in late eighteenth-century geology. His \"Short Classification and Description of Rocks\" of 1787 and his lectures set out a classification of rocks on the basis of their age based on the sequence of layers of differing material, rather than by the types of minerals as had been previous practice.\n\nHe based his historical sequence of rock formation on the theory that the Earth had originally consisted of water. According to this account, the water contained material which settled out of suspension in a process of sedimentation to form the core of the planet and the continents as a series of layers, the oldest and hardest being granite while newer layers showed an increasing number of fossils. Volcanos had a minor effect, modifying the continents and adding more sediment as well as some volcanic rocks, and successive lesser floods added more layers, so that most rocks resulted from precipitates settling out of water. There is no indication that any of the floods in Werner's cosmogony were Noah's flood.\n\nA rival theory known as plutonism (or vulcanism) held that rocks were formed in fire. This was originally proposed by Abbé Anton Moro (1687–1750) with reference to his studies of volcanic islands, and was taken up by James Hutton who put forward a uniformitarian theory of a rock cycle extending over infinite time in which rocks were worn away by weathering and erosion, then were re-formed and uplifted by heat and pressure.\n\nNeptunists differed from the plutonists in holding that basalt was a sedimentary deposit which included fossils and so could not be of volcanic origin. Hutton correctly asserted that basalt never contained fossils and was always insoluble, hard, and crystalline. He found geological formations in which basalt cut through layers of other rocks, supporting his theory that it originated from molten rock under the Earth's crust.\n\nThe debate was not just between scientists. Johann Wolfgang von Goethe, one of the most respected authors of the day, took sides with the neptunists. The fourth act of his famous work \"Faust\" contains a dialogue between a neptunist and a plutonist, the latter being Mephistopheles, the antagonist of the play who is a devil. Doing so he implicitly expressed his favour for the neptunist theory, though he also did so explicitly and sometimes even harshly elsewhere.\n\nThe controversy lasted into the early years of the 19th century, but the works of Charles Lyell in the 1830s gradually won over support for the uniformitarian ideas of Hutton and the plutonists. However, sedimentary rocks such as limestone are considered to have resulted from processes like those described by the neptunists, and so modern theory can be seen as a synthesis of the two approaches.\n\n\nThe theory, and its intellectual context, are treated in Daniel Kehlmann's fictionalised account of the travels of Alexander von Humboldt, \"Die Vermessung der Welt\" (\"Measuring the World\") of 2006.\n\n"}
{"id": "4153951", "url": "https://en.wikipedia.org/wiki?curid=4153951", "title": "Oceanarium", "text": "Oceanarium\n\nAn oceanarium can be either a marine mammal park, such as MarineLand, or a large-scale aquarium, such as the Lisbon Oceanarium, presenting an ocean habitat with marine animals, especially large ocean dwellers such as sharks.\n\nMarineland of Florida, one of the first theme parks in Florida, United States, started in 1938, claims to be \"the world's first oceanarium\"\n\nMarineland of Florida was developed as \"Marine Studios\" near St. Augustine in Marineland, Florida, which was followed in Florida by Miami Seaquarium, opened in 1955 and in California by Marineland of the Pacific, opened in 1954 near Los Angeles, and Marine World, Africa USA, opened in 1968 near San Francisco.\n\nSeaWorld San Diego was opened in 1964, developed by four fraternity brothers Milt Shedd, Ken Norris, David DeMott and George Millay.\nSeaWorld Aurora opened in 1970 near Cleveland, Ohio.\nSeaWorld Orlando was opened in 1973.\nSeaWorld (San Diego, Aurora, Orlando) was sold to Harcourt Brace Jovanovich (a publishing company listed on the New York Stock Exchange) in 1976. \nThey purchased Marineland of the Pacific in 1986 and closed the park.\nThey had opened SeaWorld San Antonio in 1988.\nIn 1989 they sold SeaWorld (San Diego, Aurora, Orlando, San Antonio) to Anheuser-Busch, the world's largest brewer and owner of the Busch Gardens Safari Parks, for US$1.1 billion. \nIn 2001, Anheuser-Busch sold the Ohio park which finally ceased its activities in 2004.\n\nWhen the 170,000-square-foot Oceanarium at the Shedd Aquarium in Chicago opened on April 27, 1991, it debuted as the largest indoor marine mammal facility in the world. The position as world's oceanarium has since shifted repeatedly in recent years. From 2005 to 2012 it was the Georgia Aquarium in the United States with an initial total water volume of , later it expanded to , and home to 100–120,000 animals of 700 species. In 2012 it was surpassed by Marine Life Park in Singapore with a total water volume of and over 100,000 animals of more than 800 species. In 2014, the Singapore park was surpassed by the Chimelong Ocean Kingdom in China, the current record holder, with a total water volume of .\n\nModern marine aquariums try to create natural environments. A host of marine animals swim together in the four-story cylindrical tank of the New England Aquarium in Boston, which opened in 1969.\nAt the National Aquarium in Baltimore, which opened in 1981, a walkway spirals up through the center of two gigantic cylindrical tanks, the Atlantic Coral Reef and the Shark Alley, which display sharks, sawfish, and other sea creatures.\nSince then, many new aquariums have sought even greater realism, often concentrating on local environments. The richly endowed Monterey Bay Aquarium in California, which opened in 1984, is an outstanding example.\n\nThe Afrykarium is the only themed oceanarium devoted solely to exhibiting the fauna of Africa and located in Wrocław, Poland. A part of the Wrocław Zoo, the idea behind the Afrykarium is to comprehensively present selected ecosystems from the continent of Africa. Housing over 10 thousand animals, its breadth extends from housing insects such cockroaches to the large mammals like the elephants on an area of over 33 hectares .\n\nAstana, the capital of Kazakhstan is home to the only Oceanarium in Central Asia.\n\n\n\n"}
{"id": "148443", "url": "https://en.wikipedia.org/wiki?curid=148443", "title": "Operation Argus", "text": "Operation Argus\n\nOperation Argus was a series of United States low-yield, high-atmosphere nuclear weapons tests and missile tests secretly conducted during August and September 1958 over the South Atlantic Ocean. The ARGUS tests took 11 days from start to finish with the first launch on August 27 and the final launch on September 6. They were performed by the Defense Nuclear Agency, in conjunction with the Explorer 4 space mission. Operation Argus was conducted between the nuclear test series \"Operation Hardtack I\" and \"Operation Hardtack II\". Contractors from Lockheed Aircraft Corporation as well as a few personnel and contractors from the U.S. Atomic Energy Commission were on hand as well.\n\nThe tests were proposed by Nicholas Christofilos in an unpublished paper of what was then the Livermore branch of the Lawrence Radiation Laboratory (now Lawrence Livermore National Laboratory) as a means to verify the Christofilos effect, which argued that high-altitude nuclear detonations would create a radiation belt in the extreme upper regions of the Earth's atmosphere. Such belts would be similar in effect to the Van Allen radiation belts. \"Such radiation belts were viewed as having possible tactical use in war, including degradation of radio and radar transmissions, damage or destruction of the arming and fuzing mechanisms of ICBM warheads, and endangering the crews of orbiting space vehicles that might enter the belt.\" Prior to Argus, Hardtack Teak had shown disruption of radio communications from a nuclear blast, though this was not due to the creation of radiation belts.\n\n\"Argus\" was implemented rapidly after inception due to forthcoming bans on atmospheric and exoatmospheric testing in October 1958. Consequently, the tests were conducted within a mere half-year of conception (whereas \"normal\" testing took one to two years). Because nuclear testing during this time was bending the rules, the military borrowed International Geophysical Year equipment to cover up the nuclear tests.\n\n\nOriginally \"Argus\" was designated \"Hardtack-Argus\", and later \"Floral\". For reasons of security, both names were dropped in favor of the independent name \"Argus\".\n\nFunding was provided by the Armed Forces Special Weapons Project (AFSWP), the predecessor of today's Defense Threat Reduction Agency (DTRA). Total funds allotted for the project were US$ 9,023,000.\n\nThe United States Navy Task Force 88 (or TF-88), was formed April 28, 1958. TF-88 was organized solely to conduct \"Operation Argus\". Once \"Argus\" was completed, the task force was dissolved, and its records dispersed. Some of these records have been destroyed or lost in the intervening time period. Of particular note among the missing documents were the film records (which recorded radiation levels during the \"Argus\" tests). This has proved contentious due to the higher-than-normal number of leukemia claims among TF-88 participants to the Veterans Administration. Because of this, it has been difficult to resolve just how much radiation participants were exposed to.\n\n\"USS Norton Sound\" was a United States Navy-guided missile ship responsible for missile-launching functions. She also served as a training facility for crews involved in the testing. The X-17A missiles to be used in the test were unfamiliar to those conducting the tests. Exercises including assembly and repair of dummy missiles were conducted aboard \"Norton Sound\". She also carried a 27-MHz COZI radar, which was operated by Air Force Cambridge Research Center, which was used to monitor effects of the shots. She was responsible for the launching of three low-yield nuclear warheads into the high atmosphere.\n\n\"USS Albemarle\", fresh out of an overhaul, was not listed on the TF-88 order. She set out to the Atlantic, supposedly on shakedown. She, too, mounted a COZI radar and other instrumentation for detecting man-made ionization. This instrumentation included IGY radiometers, receivers, radar, and optical equipment. After this equipment was added, she sailed to the ocean around the area of the Azores to record data at the conjugate point, as the rest of task force 88 headed to the South Atlantic to conduct the tests.\n\n\"USS Tarawa\" served as overall command of the operation, with her commander serving as Task Group Commander. She carried an Air Force MSQ-1A radar and communication system for missile tracking. She also housed VS-32 aircraft for search and security operations as well as scientific measurement, photographic, and observer missions for each shot. HS-5 was also aboard and provided intra-task-force transportation for personnel and cargo.\n\n\"USS Warrington\", in conjunction with \"Bearss\", \"Hammerberg\", and \"Courtney\" maintained a weather picket 463 km west of the task force, provided a plane guard for \"Tarawa\" during flight operations, and carried out standard destroyer functions (such as surface security and search and rescue). \"Warrington\" also carried equipment for launching Loki Dart rockets.\n\n\"USS Neosho\" refueled task force ships during the operation. She was also outfitted with Air Force MSQ-1A radar. Her commanding officer also served as the flagship for TG 88.3, the Mobile Logistics Group, consisted of: \"Neosho\", equipped with USAF MSQ-1 radar and communication vans, USS \"Salamonie\" (AO-26), and assigned destroyers.\n\n\"USS Salamonie\" returned to the United States upon arrival at TF-88, and did not participate in any shots.\n\nTwo satellite launches were attempted in order to obtain data from these high-altitude tests. Explorer 4 was successfully launched on July 26. Explorer 4 Successfully rode an Army Jupiter-C missile to orbit from Cape Canaveral. The satellite contained enough battery power to function for sixty days. This was long enough for the satellite to track and measure ARGUS. Explorer 5 suffered a launch failure on August 24.\n\nThere were many tracking systems used by the task force along with these satellites along with many organizations that helped track these missiles. \"These included the Naval Research Laboratory, the Army Signal Research and Development Laboratory, the Smithsonian Astrophysical Laboratory, the Army Map Service, the Naval Ordnance Test Station, and the Ballistic Research Laboratory along with ground tracking stations from the Aleutian Islands through the Azores from academic, industrial, and military organizations.\" \n\nTo prepare for the launch of the ARGUS missiles, many tests and preparations were conducted. As the east coast units of TF 88 were heading towards the South Atlantic, they participated in countdown, launch, and missile- tracking drills using Loki/Dart high-altitude, antiaircraft rockets fired from the USS Warrington. Fourteen of these Loki launches were conducted from August 12 to 22. These tests were conducted to test equipment and procedures, and to train personnel in specialized assignments. Some of these assignments necessary for the ARGUS missile launchings were \"stationing of ships, MSQ-1A radar tracking by the \"USS Neosho\" and the USS Tarawa, communications, positioning of sky-camera S2F aircraft, and area surveillance S2F aircraft.\"\n\nAbout 1800 km southwest of Cape Town, South Africa, USS \"Norton Sound\" launched three modified X-17A missiles armed with 1.7 kt W-25 nuclear warheads into the upper atmosphere, where high altitude nuclear explosions took place. Due to the South Atlantic Anomaly, the Van Allen radiation belt is closer to the Earth's surface at that location. The (extreme) altitude of the tests was chosen so as to prevent personnel involved in the test from being exposed to any ionizing radiation. Even with the very low threat of radiation exposure, precautions were taken to prevent radiological exposure. The task force commander and his staff had laid out a series of precautionary radiation safe measures to be followed in each stage of the operation. Even though the chance of exposure to radiation from these missiles was so minute, the safety measures were still carried out as directed by the commander by the crew of task force 88.\n\nCoordinated measurement programs involving satellite, rocket, aircraft, and surface stations were employed by the services as well as other government agencies and various contractors worldwide.\n\nThe Argus explosions created artificial electron belts resulting from the β-decay of fission fragments. These lasted for several weeks. Such radiation belts affect radio and radar transmissions, damage or destroy arming and fusing mechanisms of intercontinental ballistic missile warheads, and endanger crews of orbiting space vehicles. It was found after running these tests that the explosions did in fact degrade the reception and transmission of radar signals, another proof that Christofilos was correct about the Christofilos effect.\n\n\"Argus\" proved the validity of Christofilos' theory: the establishment of an electron shell derived from neutron and β-decay of fission products and ionization of device materials in the upper atmosphere was demonstrated. It not only provided data on military considerations, but produced a \"great mass\" of geophysical data.\n\nThe tests were first reported by Hanson Baldwin and Walter Sullivan of the \"New York Times\" on March 19, 1959, headlining it as the \"greatest scientific experiment ever conducted.\" This was an unauthorized publication that caused an uproar in the scientific community because many of them were unaware of the presence of artificial particles in the Earth's atmosphere. Approximately nine ships and 4,500 people participated in the operation. After the completion of testing, the task force returned to the United States via Rio de Janeiro, Brazil.\n\nThe tests were announced the following year, but the full results and documentation of the tests were not declassified until April 30, 1982.\n\n\n\n"}
{"id": "2870736", "url": "https://en.wikipedia.org/wiki?curid=2870736", "title": "Oslo Graben", "text": "Oslo Graben\n\nThe Oslo Graben or Oslo Rift is a graben formed during a geologic rifting event in Permian time, the last phase of the Variscan orogeny. The main graben forming period began in the late Carboniferous, which culminated with rift formation and volcanism, with associated rhomb porphyry lava flows. This activity was followed by uplifting, and ended with intrusions about 65 million years after the onset of the formation. It is located in the area around the Norwegian capital Oslo.\n\nThe lava production was high when the rhomb porphyry lavas were deposited. The lavas reflect a period of abundant earthquake-related movements, when strong forces tore the crust apart.\n\nIn the Vestfold district, one lava flow was deposited on average every 250,000 years, resulting in a 3000-metre thick sequence of mainly volcanic material. In the Oslo area, lavas were deposited on average every 800,000 years. Only a few plant remains have been found between these lavas. The bedrock in this area, roughly from Skien to Oslo and Mjøsa, results in soil rich in nutrients important for plant growth.\n\nSince the Permian, erosion has removed the volcanic peaks and indeed most of the lava layer and laid bare the magma chambers and volcanic pipes deep below, allowing scientist a rare view of what goes on beneath a rift valley. Several of the old magma plumes are now quarried, the rich black Larvikite (named from Larvik, a town south of Oslo) being one.\n\nThe Särna alkaline complex in western Sweden, also of Late Carboniferous age, is thought to be related to the Oslo Graben as it is aligned to it.\n\n"}
{"id": "610773", "url": "https://en.wikipedia.org/wiki?curid=610773", "title": "Per-unit system", "text": "Per-unit system\n\nIn the power systems analysis field of electrical engineering, a per-unit system is the expression of system quantities as fractions of a defined base unit quantity. Calculations are simplified because quantities expressed as per-unit do not change when they are referred from one side of a transformer to the other. This can be a pronounced advantage in power system analysis where large numbers of transformers may be encountered. Moreover, similar types of apparatus will have the impedances lying within a narrow numerical range when expressed as a per-unit fraction of the equipment rating, even if the unit size varies widely. Conversion of per-unit quantities to volts, ohms, or amperes requires a knowledge of the base that the per-unit quantities were referenced to. The per-unit system is used in power flow, short circuit evaluation, motor starting studies etc.\n\nThe main idea of a per unit system is to absorb large differences in absolute values into base relationships. Thus, representations of elements in the system with per unit values become more uniform.\n\nA per-unit system provides units for power, voltage, current, impedance, and admittance. With the exception of impedance and admittance, any two units are independent and can be selected as base values; power and voltage are typically chosen. All quantities are specified as multiples of selected base values. For example, the base power might be the rated power of a transformer, or perhaps an arbitrarily selected power which makes power quantities in the system more convenient. The base voltage might be the nominal voltage of a bus. Different types of quantities are labeled with the same symbol (pu); it should be clear whether the quantity is a voltage, current, or other unit of measurement.\n\nThere are several reasons for using a per-unit system:\nThe per-unit system was developed to make manual analysis of power systems easier. Although power-system analysis is now done by computer, results are often expressed as per-unit values on a convenient system-wide base.\n\nGenerally base values of power and voltage are chosen. The base power may be the rating of a single piece of apparatus such as a motor or generator. If a system is being studied, the base power is usually chosen as a convenient round number such as 10 MVA or 100 MVA. The base voltage is chosen as the nominal rated voltage of the system. All other base quantities are derived from these two base quantities. Once the base power and the base voltage are chosen, the base current and the base impedance are determined by the natural laws of electrical circuits. The base value should only be a magnitude, while the per-unit value is a phasor. The phase angles of complex power, voltage, current, impedance, etc., are not affected by the conversion to per unit values.\n\nThe purpose of using a per-unit system is to simplify conversion between different transformers. Hence, it is appropriate to illustrate the steps for finding per-unit values for voltage and impedance. First, let the base power (S_base) of each end of a transformer become the same. Once every S is set on the same base, the base voltage and base impedance for every transformer can easily be obtained. Then, the real numbers of impedances and voltages can be substituted into the per-unit calculation definition to get the answers for the per-unit system. If the per-unit values are known, the real values can be obtained by multiplying by the base values.\n\nBy convention, the following two rules are adopted for base quantities:\n\n\nWith these two rules, a per-unit impedance remains unchanged when referred from one side of a transformer to the other. This allows the ideal transformer to be eliminated from a transformer model.\n\nThe relationship between units in a per-unit system depends on whether the system is single-phase or three-phase.\n\nAssuming that the independent base values are power and voltage, we have:\n\nAlternatively, the base value for power may be given in terms of reactive or apparent power, in which case we have, respectively,\n\nor\n\nThe rest of the units can be derived from power and voltage using the equations formula_6, formula_7, formula_8 and formula_9 (Ohm's law), formula_10 being represented by formula_11. We have:\n\nPower and voltage are specified in the same way as single-phase systems. However, due to differences in what these terms usually represent in three-phase systems, the relationships for the derived units are different. Specifically, power is given as total (not per-phase) power, and voltage is line-to-line voltage.\nIn three-phase systems the equations formula_7 and formula_8 also hold. The apparent power formula_17 now equals formula_18\n\nAs an example of how per-unit is used, consider a three-phase power transmission system that deals with powers of the order of 500 MW and uses a nominal voltage of 138 kV for transmission. We arbitrarily select formula_22, and use the nominal voltage 138 kV as the base voltage formula_23. We then have:\n\nIf, for example, the actual voltage at one of the buses is measured to be 136 kV, we have:\n\nThe following tabulation of per-unit system formulas is adapted from Beeman's \"Industrial Power Systems Handbook\".\nIt can be shown that voltages, currents, and impedances in a per-unit system will have the same values whether they are referred to primary or secondary of a transformer.\n\nFor instance, for voltage, we can prove that the per unit voltages of two sides of the transformer, side 1 and side 2, are the same. Here, the per-unit voltages of the two sides are E and E respectively.\n\nformula_28\n\n‘E and E are the voltages of sides 1 and 2 in volts. N is the number of turns the coil on side 1 has. N is the number of turns the coil on side 2 has. V and V are the base voltages on sides 1 and 2. formula_29\n\nFor current, we can prove that the per-unit currents of the two sides are the same below.\nformula_30\n\nwhere I and I are the per-unit currents of sides 1 and 2 respectively. In this, the base currents I and I are related in the opposite way that V and V are related, in that\n\nformula_31\n\nThe reason for this relation is for power conservation\nS = S\n\nThe full load copper loss of a transformer in per-unit form is equal to the per-unit value of its resistance:\n\nformula_32\n\nformula_33\n\nTherefore, it may be more useful to express the resistance in per-unit form as it also represents the full-load copper loss.\n\nAs stated above, there are two degrees of freedom within the per unit system that allow the engineer to specify any per unit system. The degrees of freedom are the choice of the base voltage (V) and the base power (S). By convention, a single base power (S) is chosen for both sides of the transformer and its value is equal to the rated power of the transformer. By convention, there are actually two different base voltages that are chosen, V and V which are equal to the rated voltages for either side of the transformer. By choosing the base quantities in this manner, the transformer can be effectively removed from the circuit as described above. For example:\n\nTake a transformer that is rated at 10 kVA and 240/100 V. The secondary side has an impedance equal to 1∠0° ohms. The base impedance on the secondary side is equal to:\n\nformula_34\n\nThis means that the per unit impedance on the secondary side is 1∠0° ohm / 1 ohm = 1∠0° p.u. When this impedance is referred to the other side, the impedance becomes:\n\nformula_35\n\nThe base impedance for the primary side is calculated the same way as the secondary:\n\nformula_36\n\nThis means that the per unit impedance is 5.76∠0° ohm / 5.76 ohm = 1∠0° p.u. which is the same as when calculated from the other side of the transformer, as would be expected.\n\nAnother useful tool for analyzing transformers is to have the base change formula that allows the engineer to go from a base impedance with one set of a base voltage and base power to another base impedance for a different set of a base voltage and base power. This becomes especially useful in real life applications where a transformer with a secondary side voltage of 1.2 kV might be connected to the primary side of another transformer whose rated voltage is 1 kV. The formula is as shown below.\n\nformula_37\n\n"}
{"id": "17732554", "url": "https://en.wikipedia.org/wiki?curid=17732554", "title": "Pindus Mountains mixed forests", "text": "Pindus Mountains mixed forests\n\nThe Pindus Mountains mixed forests constitute a terrestrial ecoregion of Europe according to both the WWF and Digital Map of European Ecological Regions by the European Environment Agency. It belongs to the biome of Mediterranean forests, woodlands, and scrub, and to the Palearctic ecozone.\n\nThe Pindus Mountains mixed forests are situated in the montane parts of the southern Balkans in the wide altitudinal range above 300–500 m. They cover Taygetus on the Peloponnesus in the south, occur in the mountain ranges of Central Greece (including the Pindus), eastern Albania and the southwestern part of the Republic of Macedonia, extend to the Drin River valley in the north and occupy 39,500 km² (15,300 sq. mi) in the three countries.\n\nThe ecoregion is landlocked and surrounded by the Aegean and Western Turkey sclerophyllous and mixed forests (in Greece), Illyrian deciduous forests (in Greece and Albania), Dinaric Mountains mixed forests (in Albania to the north of the Drin) and Balkan mixed forests (in Kosovo, Macedonia and Greece).\n\nThe climate of the ecoregion is mostly of Köppen's Mediterranean type with hot summers (Csa). \n\nDue to the wide altitudinal range of this ecoregion the highest elevations (above 1,000-1,400 m) are covered with coniferous forests, with a mixed broadleaf zone occurring lower. The coniferous forests are dominated by \"Pinus nigra\" subsp. \"nigra\" var. \"pallasiana\", \"Abies cephalonica\" and \"A. borisii-regis\", with deciduous European Beech in the north. \"Juniperus foetidissima\" occurs widely near the tree line. The dominant species on the lower elevations are remarkably diverse, including \"Aesculus hippocastanum\" (in more damp places) and various deciduous oaks (\"Quercus frainetto\", \"Q. pubescens\", \"Q. cerris\", \"Q. trojana\", \"Q. petraea\"). Evergreen oaks, mainly \"Q. calliprinos\", and other Mediterranean sclerophyll shrubland species are abundant on dry and rocky south-facing slopes.\n\nPhytogeographically, the ecoregion is shared between the East Mediterranean province of the Mediterranean Region and the Illyrian province of the Circumboreal Region within the Holarctic Kingdom (Armen Takhtajan's delineation).\n\n"}
{"id": "44007076", "url": "https://en.wikipedia.org/wiki?curid=44007076", "title": "Provenance (geology)", "text": "Provenance (geology)\n\nProvenance in geology, is the reconstruction of the origin of sediments. The Earth is a dynamic planet, and all rocks are subject to transition between the three main rock types: sedimentary, metamorphic, and igneous rocks (the rock cycle). Rocks exposed to the surface are sooner or later broken down into sediments. Sediments are expected to be able to provide evidence of the erosion history of their parent source rocks. The purpose of provenance study is to restore the tectonic, paleo-geographic and paleo-climatic history.\n\nIn the modern geological lexicon, \"sediment provenance\" specifically refers to the application of compositional analyses to determine the origin of sediments. This is often used in conjunction with the study of exhumation histories, interpretation of drainage networks and their evolution, and forward-modelling of paleo-earth systems. In combination, these help to characterise the \"source to sink\" journey of clastic sediments from hinterland to sedimentary basin.\n\nProvenance (from the French provenir, \"to come from\"), is the place of origin or earliest known history of something. In geology (specifically, in sedimentary petrology), the term provenance deals with the question where sediments originate from. The purpose of sedimentary provenance studies is to reconstruct and to interpret the history of sediment from parent rocks at a source area to detritus at a burial place. The ultimate goal of provenance studies is to investigate the characteristics of a source area by analyzing the composition and texture of sediments. The studies of provenance involve the following aspects: \"(1) the source(s) of the particles that make up the rocks, (2) the erosion and transport mechanisms that moved the particles from source areas to depositional sites, (3) the depositional setting and depositional processes responsible for sedimentation of the particles (the depositional environment), and (4) the physical and chemical conditions of the burial environment and diagenetic changes that occur in siliciclastic sediment during burial and uplift\". Provenance studies are conducted to investigate many scientific questions, for example, the growth history of continental crust, collision time of Indian and Asian plates, Asian monsoon intensity, and Himalayan exhumation Meanwhile, the provenance methods are widely used in the oil and gas industry. \"Relations between provenance and basin are important for hydrocarbon exploration because sand frameworks of contrasting detrital compositions respond differently to diagenesis, and thus display different trends of porosity reduction with depth of burial.\"\n\nAll rock exposed at the Earth's surface is subjected to physical or chemical weathering and broken down into finer grained sediment. All three types of rocks (igneous, sedimentary and metamorphic rocks) can be the source of detritus.\n\nRocks are transported downstream from higher elevation to lower elevation. Source rocks and detritus are transported by gravity, water, wind or glacial movement. The transportation process breaks rocks into smaller particles by physical abrasion, from big boulder size into sand or even clay size. At the same time minerals within the sediment can also be changed chemically, only minerals that are more resistant to chemical weathering can survive (e.g. ultrastable minerals zircon, tourmaline and rutile). During the transportation, minerals can be sorted by their density, and as a result, light minerals like quartz and mica can be moved faster and further than heavy minerals (like zircon and tourmaline).\n\nAfter a certain distance of transportation, detritus reaches a sedimentary basin and accumulates in one place. With the accumulation of sediments, sediments are buried to a deeper level and go through diagenesis, which turns separate sediments into sedimentary rocks (i.e. conglomerate, sandstone, mudrocks, limestone etc.) and some metamorphic rocks (such as quartzite) which were derived from sedimentary rocks. After sediments are weathered and eroded from mountain belts, they can be carried by stream and deposited along rivers as river sands. Detritus can also be transported and deposited in foreland basins and at offshore fans. The detrital record can be collected from all these places and can be used in provenance studies.\n\nAfter detritus are eroded from source area, they are transported and deposited in river, foreland basin or flood plain. Then the detritus can be eroded and transported again when flooding or other kinds of eroding events occur. This process is called as reworking of detritus. And this process could be problematic to provenance studies. For example, U-Pb zircon ages are generally considered to reflect the time of zircon crystallization at about 750° Celsius and zircon is resistant to physical abrasion and chemical weathering. So zircon grains can survive from multiple cycles of reworking. This means if the zircon grain is reworked (re-eroded) from a foreland basin (not from original mountain belt source area) it will lose information of reworking (detrital record will not indicate the foreland basin as a source area but will indicate the earlier mountain belt as a source area). To avoid this problem, samples can be collected close to the mountain front, upstream from which there is no significant sediment storage.\n\nThe study of sedimentary provenance involves several geological disciplines, including mineralogy, geochemistry, geochronology, sedimentology, igneous and metamorphic petrology. The development of provenance methods are heavily dependent on the development of these mainstream geological disciplines. The earliest provenance studies were primarily based on paleocurrent analysis and petrographic analysis (composition and texture of sandstone and conglomerate). Since the 1970s, provenance studies shifted to interpret tectonic settings (i.e. magmatic arcs, collision orogens and continental blocks) using sandstone composition. Similarly, bulk rock geochemistry techniques are applied to interpret provenance linking geochemical signatures to source rocks and tectonic settings. Later, with the development of chemical and isotopic micro-analysis methods and geochronological techniques(e.g. ICP-MS, SHRIMP), provenance researches shifted to analyze single mineral grains. The following table has examples of where provenance study samples are collected.\n\nGenerally, provenance methods can be sorted into two categories, which are petrological methods and geochemical methods. Examples of petrological methods include QFL ternary diagram, heavy mineral assemblages (apatite–tourmaline index, garnet zircon index), clay mineral assemblages and illite crystallinity, reworked fossils and palynomorphs, and stock magnetic properties. Examples of geochemical methods include zircon U-Pb dating (plus Hf isotope), zircon fission track, apatite fission track, bulk sediment Nd and Sr isotopes, garnet chemistry, pyroxene chemistry, amphibole chemistry and so on. There is a more detailed list below with references to various types of provenance methods.\n\nThis method is widely used in provenance studies and it has the ability to link sandstone composition to tectonic setting. This method is described in the Dickinson and Suczek 1979 paper. Detrital framework modes of sandstone suites from different kinds of basins are a function of provenance types governed by plate tectonics. (1)Quartzose sands from continental cratons are widespread within interior basins, platform successions, miogeoclinal wedges, and opening ocean basins. (2)Arkosic sands from uplifted basement blocks are present locally in rift troughs and in wrench basins related to transform ruptures. (3)Volcaniclastic lithic sand and more complex volcano-plutonic sands derived from magmatic arcs are present in trenches, forearc basins and marginal seas. (4) Recycled orogenic sands, rich in quartz or chert plus other lithic fragments and derived from subduction complexes, collision orogens, and foreland uplifts, are present in closing ocean basins. Triangular diagrams showing framework proportions of quartz, the two feldspars, polycrystalline quartzose lithics, and unstable lithics of volcanic and sedimentary parentage successfully distinguish the key provenance types.\"\n\nGeochronology and thermochronology are more and more applied to solve provenance and tectonic problems. Detrital minerals used in this method include zircons, monazites, white micas and apatites. The age dated from these minerals indicate timing of crystallization and multiple tectono-thermal events. This method is base on the following considerations: \"(1) the source areas are characterized by rocks with different tectonic histories recorded by distinctive crystallization and cooling ages; (2) the source rocks contain the selected mineral;\" (3) Detrital mineral like zircon is ultra-stable which means it is capable of surviving multiple phases of physical and chemical weathering, erosion and deposition. This property make these detrital mineral ideal to record long history of crystallization of tectonically complex source area.\n\nThe figure to the right is an example of U–Pb relative age probability diagram. The upper plot shows foreland basin detrital zircon age distribution. The lower plot shows hinterland (source area) zircon age distribution. In the plots, n is the number of analyzed zircon grains. So for foreland basin Amile formation, 74 grains are analyzed. For source area (divided into 3 tectonic level, Tethyan Himalaya, Greater Himalaya and Lesser Himalaya), 962, 409 and 666 grains are analyzed respectively. To correlate hinterland and foreland data, let's see the source area record first, Tethyan sequence have age peak at ~500 Myr, 1000 Myr and 2600 Myr, Greater Himalaya has age peaks at ~1200 Myr and 2500 Myr, and Lesser Himalaya sequence has age peaks at ~1800 Ma and 2600 Ma. By simply comparing the foreland basin record with source area record, we cam see that Amile formation resemble age distribution of Lesser Himalaya. It has about 20 grains with age ~1800 Myr (Paleoproterozoic) and about 16 grains yield age of ~2600 Myr (Archean). Then we can interpret that sediments of Amile formation are mainly derived from the Lesser Himalaya, and rocks yield ago of Paleoproterozoic and Archean are from the Indian craton. So the story is: Indian plate collide with Tibet, rocks of Indian craton deformed and involved into Himalayan thrust belt (e.g. Lesser Himalaya sequence), then eroded and deposited at foreland basin.\n\nU–Pb geochronology of zircons was conducted by laser ablation multicollector inductively coupled plasma mass spectrometry (LA-MC-ICPMS).\n\nDepend on properties of Sm–Nd radioactive isotope system can provide age estimation of sedimentary source rocks. It has been used in provenance studies. Nd is produced by α decay of Sm and has a half life of 1.06×10 years. Variation of Nd/Nd is caused by decay of Sm. Now Sm/Nd rato of the mantle is higher than that of the crust and Nd/Nd ratio is also higher than in the mantle than in the crust. Nd/Nd ratio is expressed in εNd notation (DePaolo and Wasserbur 1976). formula_1. CHUR refer to Chondritic Uniform Reservoir. So ϵNd is a function of T (time). Nd isotope evolution in mantle and crust in shown in the figure to the right. The upper plot (a), bold line shows the evolution of the bulk earth or CHUR(chondritic uniform reservoir). The lower plot (b) shows evolution of bulk earth (CHUR) crust and mantle, 143Nd/144Nd is transformed to εNd. Normally, the most rocks have εNd values in the range of -20 to +10. Calculated εNd value of rocks can be correlated to source rocks to perform provenance studies.What's more, Sr and Nd isotopes have been used to study both provenance and weathering intensity. Nd is mainly unaffected by weathering process but 87Sr/86Sr value is more affected by chemical weathering.\n\nTo pick suitable lab data acquisition to sediment provenance, grain size should be taken into consideration. For conglomerates and boulders, as original mineral paragenesisis preserved, almost all analytical methods can be used to study the provenance. For finer grained sediments, as they always lose paragenetic information, only a limited range of analytical methods can be used.\n\nLab data acquisition approaches for provenance study fall into the following three categories: (1) analyzing bulk composition to extract petrographic, mineralogical and chemical information. (2) analyzing specific groups of minerals such as heavy minerals and (3) analyzing single mineral grains about morphological, chemical and isotopic properties.\n\nFor bulk composition analysis, samples are crushed, powdered and disintegrated or melted. Then measurement of major and trace and rare-earth (REE) elements are conducted by using instruments like atomic absorption spectroscopy (AAS), X-ray fluorescence(XRF), neutron activation analysis (NAA) etc.\n\nSand-sized sediments are able to be analyzed by single-grain methods. Single-grain methods can be divided into the following three groups: (1) Microscopic-morphological techniques, which are used to observe shape, color and internal structures in minerals. For example, scanning electron microscope (SEM) and cathodoluminescence (CL) detector. (2) Single grain geochemical techniques, which are used to acquire chemical composition and variations within minerals. For example, laser-ablation inductively coupled plasma mass spectrometry (ICP-MS). (3) Radiometric dating of single grain mineral, which can determine the geochronological and thermochronological properties of minerals. For example, U/Pb SHRIMP dating and 40Ar/39Ar laser-probe dating.\n\nFor more information about instruments, see\n\nDuring the pathway of detritus transported from source area to basin, the detritus is subject to weathering, transporting, mixing, deposition, diagenesis and recycling. The complicated process can modify parents lithology both compositionally and textually. All these factors pose certain limits on our capability to restore the characteristics of source rocks from the properties of the produced detrital record. The following paragraphs briefly introduce major problems and limitations of provenance studies.\n\nTo correlate sediments (detrital record) to source area, several possible source area need to be chosen for comparison. In this process, possible source area where sediment is from may be missed and not chosen as a candidate source area. This could cause misinterpretation in correlation sediment to source later.\n\nGrain size could cause misinterpretation of provenance studies. During transportation and deposition, detritus is subject to mechanical breakdown, chemical alternation and sorting. This always results in a preferential enrichment of specific materials in a certain range of grain-size, and sediment composition tends to be a function of grain size. For instance, SiO/AlO ratios decrease with decreasing of grain size because Al-rich phyllosilicate enriches at the expense of Si-rich phase in fine-grained detritus. This means the changing of composition of detrital record could reflect effect of sorting of grain size and not only changing of provenance. To minimize the influence of sedimentary sorting on provenance method (like Sr-Nd isotopic method), only very fine-grained to fine-grained sandstones are collected as samples but medium-grained sandstones can be used when alternatives are unavailable.\n\nMixing of detritus from multiple sources may cause problems with correlating the final detrital record to source rocks, especially when dispersal pathways are complex and involve recycling of previously deposited sediments. For example, in a detrital record, there are zircon grains with the age of 1.0 billion years, but there are two source areas upstream that yield 1.0 billion years old zircon and rivers drained through both area. Then we couldn't determine which area the detritus is derived from.\n\nDiagenesis could be a problem when analyzing detrital records especially when dealing with ancient sediments which are always lithified. Variation of clay minerals in detrital record may not reflect variation of provenance rock, but burial effect. For example, clay minerals become unstable at great depth, kaolinite and smectite become illte. If there is a downward increasing trend of illite components in a drilling core, we can not conclude that early detrital record indicate more illite-yield source rock but possibly as a result of burial and alternation of minerals\n\nAs a provenance study tries to correlate detrital record (which is stored in basins) to hinterland stratigraphy, and hinterland stratigraphy is structurally controlled by fault systems, so hinterland structural setting is important to interpretation of the detrital record. Hinterland structural setting is estimated by field mapping work. Geologists work along river valleys and traverse mountain belts (thrust belt), locate major faults and describe major stratigraphy bounded by faults in the area. A geologic map is the product of field mapping work, and cross sections can be constructed by interpreting a geologic map. However, a lot of assumptions are made during this process, so the hinterland structural settings are always assumptions. And these assumptions can affect interpretation of detrital record. Here is an example, the right figure shows a classic thrust belt and foreland basin system, the thrust fault carries overlying rocks to the surface and rocks of various lithology are eroded and transported to deposit at the foreland basin. In structural assumption 1, the pink layer is assumed to exist above thrust 2 and thrust 3, but in the 2nd assumption, the pink layer is only carried by thrust 2. Detrital records are stored in foreland basin stratigraphy. Within the stratigraphy, the pink layer is correlated to the hinterland pink layer. If we use structural assumption 2, we can interpret that thrust 2 was active about 12 and 5 million years ago. But when using the other assumption, we couldn't know if the pink layer record indicates activity of thrust 2 or 3.\n\nA combination usage of multiple provenance methods (e.g.petrography, heavy mineral analysis, mineral geochemistry, wholerock geochemistry, geochronology and drainage capture analysis)can provide valuable insights to all stages of hydrocarbon exploration and production. In exploration stage, provenance studies can enhance the understanding of reservoir distribution and reservoir quality. These will affect chance of success of exploration project; In development stage, mineralogical and chemical techniques are widely used to estimate reservoir zonation and correlation of stratigraphy. At the same time, these provenance techniques are also used in production stage. For example, they are used to assess permeability variations and well decline rate resulting from spatial variability in diagenesis and depositional facies \n\n\n"}
{"id": "31357305", "url": "https://en.wikipedia.org/wiki?curid=31357305", "title": "Relativistic runaway electron avalanche", "text": "Relativistic runaway electron avalanche\n\nA relativistic runaway electron avalanche (RREA) is an avalanche growth of a population of relativistic electrons driven through a material (typically air) by an electric field. RREA has been hypothesized to be related to lightning initiation, terrestrial gamma-ray flashes, sprite lightning, and spark development. RREA is unique as it can occur at electric fields an order of magnitude lower than the dielectric strength of the material.\n\nWhen an electric field is applied to a material, free electrons will drift slowly through the material as described by the electron mobility. For low-energy electrons, faster drift velocities result in more interactions with surrounding particles. These interactions create a form of friction that slow the electrons down. Thus, for low-energy cases, the electron velocities tend to stabilize.\n\nAt higher energies, above about 100 keV, these collisional events become less common as the mean free path of the electron rises. These higher-energy electrons thus see less frictional force as their velocity increases. In the presence of the same electric field, these electrons will continue accelerating, \"running away\".\n\nAs runaway electrons gain energy from an electric field, they occasionally collide with atoms in the material, knocking off secondary electrons. If the secondary electrons also have high enough energy to run away, they too accelerate to high energies, produce further secondary electrons, etc. As such, the total number of energetic electrons grows exponentially in an avalanche.\n\nThe RREA mechanism above only describes the growth of the avalanche. An initial energetic electron is needed to start the process. In ambient air, such energetic electrons typically come from cosmic rays. In very strong electric fields, stronger than the maximum frictional force experienced by electrons, even low-energy (\"cold\" or \"thermal\") electrons can accelerate to relativistic energies, a process dubbed \"thermal runaway.\"\n\nRREA avalanches generally move opposite the direction of the electric field. As such, after the avalanches leave the electric field region, frictional forces dominate, the electrons lose energy, and the process stops. There is the possibility, however, that photons or positrons produced by the avalanche will wander back to where the avalanche began and can produce new seeds for a second generation of avalanches. If the electric field region is large enough, the number of second-generation avalanches will exceed the number of first-generation avalanches and the number of avalanches itself grows exponentially. This avalanche of avalanches can produce extremely large populations of energetic electrons. This process eventually leads to the decay of the electric field below the level at which feedback is possible and therefore acts as a limit to the large-scale electric field strength.\n\nThe large population of energetic electrons produced in RREA will produce a correspondingly large population of energetic photons by bremsstrahlung. These photons are proposed as the source of terrestrial gamma-ray flashes. Large RREA events in thunderstorms may also contribute rare but large radiation doses to commercial airline flights. The American physicist Joseph Dwyer coined the term \"dark lightning\" for this phenomenon, which is still the subject of research.\n"}
{"id": "17948474", "url": "https://en.wikipedia.org/wiki?curid=17948474", "title": "Roger A. Caras", "text": "Roger A. Caras\n\nRoger Andrew Caras (May 24, 1928 – February 18, 2001) was an American wildlife photographer, writer, wildlife preservationist and television personality.\n\nKnown as the host of the annual Westminster Kennel Club Dog Show, Caras was the author of more than 70 books, a veteran of network television programs including \"Nightline\", \"ABC World News Tonight\" and \"20/20\" before devoting himself to work as president of the American Society for the Prevention of Cruelty to Animals.\n\nBorn May 24, 1928, in the rural town of Methuen, Massachusetts, Caras was raised in a family that encouraged love of animals. His parents allowed him to foster a menagerie of pets, and during the Depression he went to work at the age of 10 to help pay for his pets' upkeep. His first job, working in the stables of an SPCA shelter, was his first experience with animal rescue in the shelter's haven for abused horses. He completed his education at Boston's Huntington Preparatory School and immediately enlisted in the U.S. Army near the end of World War II.\n\nCaras returned to Boston after his tour of duty and then enrolled as a zoology major at Northwestern University. In 1950, he transferred to Case Western Reserve University, Cleveland, Ohio, but interrupted his education for military service again, this time in the Korean War from 1950 to 1952.\n\nCaras returned to civilian life as a West Coast resident, attending the University of Southern California, where he earned a degree, not in zoology but in cinema, and stepped from academic life to executive-level work in the motion picture industry. During 15 years in the film world, Caras held a number of assignments, including serving as press secretary for actress Joan Crawford, and from 1965 to 1969 as vice president of Stanley Kubrick's production company, Hawk Films, working with Kubrick and Arthur C. Clarke on the science fiction epic \"\". During his Hollywood years, Caras also launched his writing career, contributing articles on animal and environmental issues to such periodicals as \"Audubon\" and publishing his first book, \"Antarctica: Land of Frozen Time,” in 1962.\n\nIn 1964, Caras made his broadcasting debut on the NBC News program \"The Today Show\", spending nearly a decade as the program's \"house naturalist.\" His skills in broadcasting, research, biology, and zoology led to his acceptance as one of the media's best-regarded animal authorities. He was sought out by the Walt Disney conglomerate as a consultant on their Florida Animal Kingdom park.\n\nActing as a special correspondent, Caras reported from around the globe on a variety of animal and environmental issues that ranged from exposes on laboratory animals to the plight of the endangered Giant Panda in China and to investigation of the black market commerce in exotic animals and poaching.\n\nCaras spent from 1975 to 1992 as a regularly featured reporter on ABC Evening News (later World News Tonight with Peter Jennings,) as well as contributing to \"Nightline\", \"20/20\", and \"Good Morning America\". He also hosted radio programs, including \"Pets and Wildlife\" on CBS, \"Report from the World of Animals\" on NBC, and the ABC series \"The Living World\".\n\nCaras won an Emmy Award for his reporting. His books include \"The Bond\" and his last book, \"Going for the Blue: Inside the World of Show Dogs and Dog Shows,\" which was published in time for the 2001 Westminster competition.\n\nCaras’s work with and on behalf of animals led to his 1991 election as the 14th president of the American Society for the Prevention of Cruelty to Animals, the oldest humane-treatment-of-animals organization in the United States. During his tenure, the ASPCA expanded its care, protection and education programs, and adopted a number of internal practices to improve its work. Caras retired in 1999 and became president emeritus, acting as a consultant and public speaker for the organization.\n\nCaras made his home in Freeland, Baltimore County, Maryland, where he and his wife, Jill Langdon Barclay, maintained a farm that became home for a variety of animals. In 2001, in the last year of his life, Caras shared his farm with 12 dogs (7 Greyhounds, 3 retrievers and 2 hounds), nine cats, all of mixed origin, five horses, two cows, a pair of alpacas and a llama. After his death, his wife, his son, Dr. Barclay Caras, and daughter, Pamela Caras, requested that people wishing to honor his memory donate memorial contributions to the ASPCA in his name.\n\n\n\n"}
{"id": "1518956", "url": "https://en.wikipedia.org/wiki?curid=1518956", "title": "Salalah", "text": "Salalah\n\nSalalah ( transliterated \"Ṣalālah\"), is the capital and largest city of the southern Omani governorate of Dhofar. Its population in 2009 was about 197,169.\n\nSalalah is the second-largest city in the Sultanate of Oman, and the largest city in the Dhofar Province. Salalah is the birthplace of the current sultan, Qaboos bin Said. Salalah attracts many people from other parts of Oman and the Persian Gulf region during the monsoon/\"khareef\" season, which spans from July to September.\nThe climate of the region and the monsoon allows the city to grow some vegetables and fruits like coconut and bananas. There are many gardens within the city where these vegetables and fruits grow.\n\nSalalah was the traditional capital of Dhofar, which reached the peak of prosperity in the 13th century thanks to the incense trade. Later it decayed, and in the 19th century it was absorbed by the Sultanate of Muscat. Between 1932 and 1970, Salalah was the capital of the Sultanate of Muscat and Oman under Said bin Taimur. After the latter's death, his son Qaboos decided to move the capital of Oman to Muscat.\n\nThe Sultan traditionally lives in Salalah rather than in Muscat, the capital and largest city in Oman; Qaboos has bucked this trend, and has lived in Muscat since he ascended to the throne in 1970. He does, however, visit Salalah fairly regularly to meet with influential tribal and local leaders; his last visit was in 2010 and before that he visited in 2006.\n\nIn 2010, during the 40th anniversary of Sultan Qaboos' taking the throne, he decided to spend his time in Salalah. The 40th anniversary celebrations consisted of a massive parade. It lasted several hours and had an estimated 100,000 attendees.\nIn 2011 the city hosted peaceful protests after the domino effect from the Arab Spring which lasted several months. Of the many requests filed from the protesters, some included the expulsion of the current ministers, job opportunities, salary increases, a solution to the increasing cost of living, and the establishment of Islamic banks. \n\n\nThe city has a hot desert climate (Köppen climate classification \"BWh\"), although summers are cooler than in more northern or inland parts of Oman. Salalah is very cloudy during the monsoon months of July and August, even though relatively little rain falls. Khareef means \"autumn\" in Arabic but it refers to monsoon when describing the region around Salalah. During this time, the brown landscape of Salalah and its surroundings is completely transformed to a beautiful and lush greenery. \n\nCyclone Mekunu, which originated over the Arabian Sea, became an extremely severe cyclone before hitting the Salalah city on 25 May 2018. 200 kmph was the recorded windspeed and the city of Salalah was pounded with over 617 mm of rainfall, which is almost 5 years of Oman's average rainfall.\n\nThe city, like many other in Arab states of the Arabian peninsula, has a relatively large expatriate community, mainly from India, Pakistan, Bangladesh and Philippines.\n\nThe majority of the Omani population in Salalah is Muslim. There is also a considerable population of Hindus, Christians, Buddhists and Sikhs in the expatriate community.\n\nArabic is the official language and the most spoken one. The unofficial, unwritten language known as Jeballi is the second most spoken language and the mother tongue of many in Salalah and its surrounding areas, with 25,000 estimated speakers as of 1993.\n\nEnglish is the official foreign language and the most spoken language of the expats. Malayalam is another popular language and together with Tamil, Telugu, Hindi/Urdu it is the most widely spoken language among expatriates.\n\nAPM Terminals, part of the A. P. Moller-Maersk Group of Denmark, manages the Port of Salalah; one of the largest ports on the Arabian Peninsula which is an important transshipment hub for container shipping in the area. The Port of Salalah is also one of the most vital ports on the peninsula connecting together Africa, the Middle East, and Asia. But the port is outside the city, to the south. It is also the largest private employer in the Dhofar region. The Salalah Free Zone, situated right beside the port, is emerging as a new center for heavy industries in the Middle East.\n\nSalalah's economy is also based on Tourism. During Khareef season (July to Sept), there are many tourists visit from Middle East. There are many places to visit in Salalah during this season as the mountains turns green and the \"rain causes many waterfalls in the mountains\" mainly Wadi Darbat, Ain Athum, Ain Tubrook, and Ain Khor. \n\nThere are four prophet tombs located here: Nabi Imran, Nabi Ayoob, Nabi Houd, and Nabi Salih. The city received more than 600,000 tourists during khareef season in 2017 \n\nSalalah is known as the home of some of the best football clubs in Oman. In total, Salalah has 4 sport clubs based in the city: Salalah Club, Al-Ittihad, Al-Nasr, and Dhofar.\n\nDhofar F.C. have been nicknamed as \"Al-Zaeem\", or \"The Leaders\", due to their enormous success in both the Omani League, and in the Sultan Qaboos Cup. Dhofar have also have an adequate number of trophies in sports like volleyball, and handball. Al-Nasr have also been known for their great success in football, winning the Omani League 5 times, and the Sultan Qaboos Cup 4 times. Al-Nasr, like Dhofar, have also been successful in other sports such as hockey, basketball, volleyball, and handball.\n\nSalalah currently has 2 stadiums, the Salalah Sports Complex (also known as the \"Youth Sports Complex\"), which is the only multi-purpose stadium in Salalah. The newer, Al-Saadah Stadium is the newly built stadium in Al-Saada district of Salalah devoted to football. Incorporated in the walls of the sports complex apart from the football stadium is a hockey field, tennis court, olympic swimming pool, and indoor volleyball/basketball court. Al-Saada Stadium is the venue where Saudi national football team, and the Omani national team first met in Salalah on August 12, 2009.\n\nThe most popular sport played among the youth is by far football. It is very normal to see a group of boys and young men from around the area playing in makeshift fields in parking lots, or in a large open area. Beach football is also a common sight to see along the beach in the Al-Haffa district. Another popular sport in Salalah is volleyball. Although not as popular as football the game is frequently played.\n\nCurrently Salalah has two colleges, the Salalah College of Technology and Salalah College of Applied Science, both of which are government owned and sponsored.\n\nThe Salalah College of Applied Sciences incorporates an English Department. Its aim is to offer students a solid grasp of the English language so that they may go on to complete further studies in important sectors such as I.T. and Communication and Design.\n\nSalalah is also home to a private university, Dhofar University which is one of the largest in the region. It has significant shares owned by Mustahil Al-Mashani, uncle of Sultan Qaboos bin Said. Recently the university has constructed a new campus worth 25 million OR.\n\nThe Indian School Salalah is an Indian-run, self-financing, co-educational institution, primarily established to meet the academic needs of children of Indian expatriates working in Salalah. The Indian School Salalah is the largest English-medium school in Salalah and it also admits children of other nationalities. The school is located in the North Dahariz area, of Salalah town.\n\nPakistan School Salalah is a Pakistani Co-educational High school which was established in the year 1982. The school is situated adjacent to the Indian School in Salalah Dahariz north.\n\nBritish School Salalah was founded in 1970. The school follows the National Curriculum of England and Wales, and offers schooling to children from Reception to Year 7. It is also situated in Dahariz next to the Indian School and Pakistani School.\n\n\"For the main article see: Salalah International Airport\"\n\nSalalah International Airport mainly caters to domestic flights from Muscat and some International flights from India and regional Arab countries such as Qatar, U.A.E, and Saudi Arabia. Oman Air, the national airline operates 5 flights daily from Salalah to Muscat, the capital city and also 2 flights to Dubai weekly. Oman Air introduced Oman Air Pass for regular travelers between Salalah and Muscat. Qatar Airways has four flights a week from Salalah to Doha connecting to over 130 destinations worldwide since May 2013. Very convenient connections are available to destinations in Europe, the Americas, Africa, Asia and Australia. There is also a direct weekly flight from and to Kochi, Kozhikode (Calicut) & Thiruvananthapuram operated by Air India Express for the Malayalee expatriates. During the Khareef Season (Monsoons) there are weekly flights to other international destinations including Sweden and Turkey. There are also transit flights to almost all countries. The new International airport opened in November 2014 and the old Airport has since then been converted into a Domestic and emergency Airport.\n\nSalalah does not have a public transportation system within the city limits. However long distance air-conditioned buses are operated daily from Salalah to Haima, Muscat, Nizwa, Al-Buraimi, Dubai, Al-Ain, Al-Ghaydah, Mukalla, and Seiyun, as well as PDO locations such as Marmul.\n\nOther forms of other public transport popular in Salalah are taxis. Generally fares vary from half a Rial to 2 Rials depending on the distance to destination. Taxis are color-coded orange and white and provide semi-personal transportation in the form of both individual hire and the same opportunistic roadway service as Baisa buses, which are not as popular in the city.\n\nBaisa buses, also colour-coded orange and white, and like taxis are unmetered after several government initiatives to introduce meters were rejected. The fare is set by way of negotiation, although drivers usually adhere to certain unwritten rules for fares within the city. One should always find out the normally accepted fare for one's journey from one's hotel or host before looking for a taxi.\n\n\"For the main article see : Port of Salalah\"\n\nPort of Salalah is one of the deepwater ports in Oman and also 11th busiest transshipment port in the world, second busiest port in middle east, which is located at Port Raysut(Salalah). It can accommodate large vessels up to 18m draft. It is the main Container Tran-shipment Terminal of the region.\nThis port is operated and managed by Salalah Port Services Company (S.A.O.G.). The port also welcomes cruise liners & luxury ships.\n\nThe English language radio stations are Hi FM and Merge, which are the only English language stations in Oman. There are also a number of Arabic radio channels. In Salalah almost all TV channels are available. \n\n\"For the main article see : Project GreenWorld International\"\n\nProject GreenOman, a project initiated to develop Eco-friendliness in Oman by Mast.Hridith Sudev, a student of Indian School Salalah and his younger brother, Samved Shaji won the World Environment Day Global School Contest conducted by United Nations Conference on Sustainable Development and Ecology & Environment Inc. This success brought glory to Indian School Salalah and this city making Salalah more popular as a green city. In 2014, International Energy Globe Award, Austria declared Project GreenOman as Oman's best project for sustainability.\n\n\n"}
{"id": "7527410", "url": "https://en.wikipedia.org/wiki?curid=7527410", "title": "Satellite television", "text": "Satellite television\n\nSatellite television is a service that delivers television programming to viewers by relaying it from a communications satellite orbiting the Earth directly to the viewer's location. The signals are received via an outdoor parabolic antenna commonly referred to as a satellite dish and a low-noise block downconverter.\n\nA satellite receiver then decodes the desired television programme for viewing on a television set. Receivers can be external set-top boxes, or a built-in television tuner. Satellite television provides a wide range of channels and services. It is usually the only television available in many remote geographic areas without terrestrial television or cable television service.\n\nModern systems signals are relayed from a communications satellite on the K band frequencies (12–18 GHz) requiring only a small dish less than a meter in diameter. The first satellite TV systems were an obsolete type now known as television receive-only. These systems received weaker analog signals transmitted in the C-band (4–8 GHz) from FSS type satellites, requiring the use of large 2–3-meter dishes. Consequently, these systems were nicknamed \"big dish\" systems, and were more expensive and less popular.\n\nEarly systems used analog signals, but modern ones use digital signals which allow transmission of the modern television standard high-definition television, due to the significantly improved spectral efficiency of digital broadcasting. As of 2018, Star One C2 from Brazil is the only remaining satellite broadcasting in analog signals, as well as one channel (C-SPAN) on AMC-11 from the United States.\n\nDifferent receivers are required for the two types. Some transmissions and channels are unencrypted and therefore free-to-air or free-to-view, while many other channels are transmitted with encryption (pay television), requiring the viewer to subscribe and pay a monthly fee to receive the programming.\n\nThe satellites used for broadcasting television are usually in a geostationary orbit above the earth's equator. The advantage of this orbit is that the satellite's orbital period equals the rotation rate of the Earth, so the satellite appears at a fixed position in the sky. Thus the satellite dish antenna which receives the signal can be aimed permanently at the location of the satellite, and does not have to track a moving satellite. A few systems instead use a highly elliptical orbit with inclination of +/−63.4 degrees and orbital period of about twelve hours, known as a Molniya orbit.\n\nSatellite television, like other communications relayed by satellite, starts with a transmitting antenna located at an uplink facility. Uplink satellite dishes are very large, as much as 9 to 12 meters (30 to 40 feet) in diameter. The increased diameter results in more accurate aiming and increased signal strength at the satellite. The uplink dish is pointed toward a specific satellite and the uplinked signals are transmitted within a specific frequency range, so as to be received by one of the transponders tuned to that frequency range aboard that satellite. The transponder re-transmits the signals back to Earth at a different frequency (a process known as translation, used to avoid interference with the uplink signal), typically in the C-band (4–8 GHz), K-band (12–18 GHz), or both. The leg of the signal path from the satellite to the receiving Earth station is called the downlink.\n\nA typical satellite has up to 32 K-band or 24 C-band transponders, or more for K/C hybrid satellites. Typical transponders each have a bandwidth between 27 and 50 MHz. Each geostationary C-band satellite needs to be spaced 2° longitude from the next satellite to avoid interference; for K the spacing can be 1°. This means that there is an upper limit of 360/2 = 180 geostationary C-band satellites or 360/1 = 360 geostationary K-band satellites. C-band transmission is susceptible to terrestrial interference while K-band transmission is affected by rain (as water is an excellent absorber of microwaves at this particular frequency). The latter is even more adversely affected by ice crystals in thunder clouds.\n\nOn occasion, sun outage will occur when the sun lines up directly behind the geostationary satellite to which the receiving antenna is pointed.\nThe downlink satellite signal, quite weak after traveling the great distance (see inverse-square law), is collected with a parabolic receiving dish, which reflects the weak signal to the dish's focal point. Mounted on brackets at the dish's focal point is a device called a feedhorn or collector. The feedhorn is a section of waveguide with a flared front-end that gathers the signals at or near the focal point and conducts them to a probe or pickup connected to a low-noise block downconverter (LNB). The LNB amplifies the signals and downconverts them to a lower block of intermediate frequencies (IF), usually in the L-band.\n\nThe original C-band satellite television systems used a low-noise amplifier (LNA) connected to the feedhorn at the focal point of the dish. The amplified signal, still at the higher microwave frequencies, had to be fed via very expensive low-loss 50-ohm impedance gas filled hardline coaxial cable with relatively complex N-connectors to an indoor receiver or, in other designs, a downconverter (a mixer and a voltage-tuned oscillator with some filter circuitry) for downconversion to an intermediate frequency. The channel selection was controlled typically by a voltage tuned oscillator with the tuning voltage being fed via a separate cable to the headend, but this design evolved.\n\nDesigns for microstrip-based converters for amateur radio frequencies were adapted for the 4 GHz C-band. Central to these designs was concept of block downconversion of a range of frequencies to a lower, more easily handled IF.\n\nThe advantages of using an LNB are that cheaper cable can be used to connect the indoor receiver to the satellite television dish and LNB, and that the technology for handling the signal at L-band and UHF was far cheaper than that for handling the signal at C-band frequencies. The shift to cheaper technology from the hardline and N-connectors of the early C-band systems to the cheaper and simpler 75-ohm cable and F-connectors allowed the early satellite television receivers to use, what were in reality, modified UHF television tuners which selected the satellite television channel for down conversion to a lower intermediate frequency centered on 70 MHz, where it was demodulated. This shift allowed the satellite television DTH industry to change from being a largely hobbyist one where only small numbers of systems costing thousands of US dollars were built, to a far more commercial one of mass production.\n\nIn the United States, service providers use the intermediate frequency ranges of 950–2150 MHz to carry the signal from the LNBF at the dish down to the receiver. This allows for transmission of UHF signals along the same span of coaxial wire at the same time. In some applications (DirecTV AU9-S and AT-9), ranges of the lower B-band and 2250–3000 MHz, are used. Newer LNBFs in use by DirecTV, called SWM (Single Wire Multiswitch), are used to implement single cable distribution and use a wider frequency range of 2–2150 MHz.\n\nThe satellite receiver or set-top box demodulates and converts the signals to the desired form (outputs for television, audio, data, etc.). Often, the receiver includes the capability to selectively unscramble or decrypt the received signal to provide premium services to some subscribers; the receiver is then called an integrated receiver/decoder or IRD. Low-loss cable (e.g. RG-6, RG-11, etc.) is used to connect the receiver to the LNBF or LNB. RG-59 is not recommended for this application as it is not technically designed to carry frequencies above 950 MHz, but may work in some circumstances, depending on the quality of the coaxial wire, signal levels, cable length, etc.\n\nA practical problem relating to home satellite reception is that an LNB can basically only handle a single receiver. This is because the LNB is translating two different circular polarizations (right-hand and left-hand) and, in the case of K-band, two different frequency bands (lower and upper) to the same frequency range on the cable. Depending on which frequency and polarization a transponder is using, the satellite receiver has to switch the LNB into one of four different modes in order to receive a specific \"channel\". This is handled by the receiver using the DiSEqC protocol to control the LNB mode. If several satellite receivers are to be attached to a single dish, a so-called multiswitch will have to be used in conjunction with a special type of LNB. There are also LNBs available with a multiswitch already integrated. This problem becomes more complicated when several receivers are to use several dishes (or several LNBs mounted in a single dish) pointing to different satellites.\n\nA common solution for consumers wanting to access multiple satellites is to deploy a single dish with a single LNB and to rotate the dish using an electric motor. The axis of rotation has to be set up in the north-south direction and, depending on the geographical location of the dish, have a specific vertical tilt. Set up properly the motorized dish when turned will sweep across all possible positions for satellites lined up along the geostationary orbit directly above the equator. The disk will then be capable of receiving any geostationary satellite that is visible at the specific location, i.e. that is above the horizon. The DiSEqC protocol has been extended to encompass commands for steering dish rotors.\n\nThere are five major components in a satellite system: the programming source, the broadcast center, the satellite, the satellite dish, and the receiver. \"Direct broadcast\" satellites used for transmission of satellite television signals are generally in geostationary orbit above the earth's equator. The reason for using this orbit is that the satellite circles the Earth at the same rate as the Earth rotates, so the satellite appears at a fixed point in the sky. Thus satellite dishes can be aimed permanently at that point, and don't need a tracking system to turn to follow a moving satellite. A few satellite TV systems use satellites in a Molniya orbit, a highly elliptical orbit with inclination of +/-63.4 degrees and orbital period of about twelve hours.\n\nSatellite television, like other communications relayed by satellite, starts with a transmitting antenna located at an uplink facility. Uplink facilities transmit the signal to the satellite over a narrow beam of microwaves, typically in the C-band frequency range due to its resistance to rain fade. Uplink satellite dishes are very large, often as much as 9 to 12 metres (30 to 40 feet) in diameter to achieve accurate aiming and increased signal strength at the satellite, to improve reliability. The uplink dish is pointed toward a specific satellite and the uplinked signals are transmitted within a specific frequency range, so as to be received by one of the transponders tuned to that frequency range aboard that satellite. The transponder then converts the signals to K band, a process known as \"translation,\" and transmits them back to earth to be received by home satellite stations.\n\nThe downlinked satellite signal, weaker after traveling the great distance (see inverse-square law), is collected by using a rooftop parabolic receiving dish (\"satellite dish\"), which reflects the weak signal to the dish's focal point. Mounted on brackets at the dish's focal point is a feedhorn which passes the signals through a waveguide to a device called a low-noise block converter (LNB) or low noise converter (LNC) attached to the horn. The LNB amplifies the weak signals, filters the block of frequencies in which the satellite television signals are transmitted, and converts the block of frequencies to a lower frequency range in the L-band range. The signal is then passed through a coaxial cable into the residence to the satellite television receiver, a set-top box next to the television.\n\nThe reason for using the LNB to do the frequency translation at the dish is so that the signal can be carried into the residence using cheap coaxial cable. To transport the signal into the house at its original K band microwave frequency would require an expensive waveguide, a metal pipe to carry the radio waves. The cable connecting the receiver to the LNB are of the low loss type RG-6, quad shield RG-6, or RG-11. RG-59 is not recommended for this application as it is not technically designed to carry frequencies above 950 MHz, but will work in many circumstances, depending on the quality of the coaxial wire. The shift to more affordable technology from the 50ohm impedance cable and N-connectors of the early C-band systems to the cheaper 75ohm technology and F-connectors allowed the early satellite television receivers to use, what were in reality, modified UHF television tuners which selected the satellite television channel for down conversion to another lower intermediate frequency centered on 70 MHz where it was demodulated.\n\nAn LNB can only handle a single receiver. This is due to the fact that the LNB is mapping two different circular polarisations – right hand and left hand – and in the case of the K-band two different reception bands – lower and upper – to one and the same frequency band on the cable, and is a practical problem for home satellite reception. Depending on which frequency a transponder is transmitting at and on what polarisation it is using, the satellite receiver has to switch the LNB into one of four different modes in order to receive a specific desired program on a specific transponder. The receiver uses the DiSEqC protocol to control the LNB mode, which handles this. If several satellite receivers are to be attached to a single dish a so-called multiswitch must be used in conjunction with a special type of LNB. There are also LNBs available with a multiswitch already integrated. This problem becomes more complicated when several receivers use several dishes or several LNBs mounted in a single dish are aimed at different satellites.\n\nThe set-top box selects the channel desired by the user by filtering that channel from the multiple channels received from the satellite, converts the signal to a lower intermediate frequency, decrypts the encrypted signal, demodulates the radio signal and sends the resulting video signal to the television through a cable. To decrypt the signal the receiver box must be \"activated\" by the satellite company. If the customer fails to pay his monthly bill the box is \"deactivated\" by a signal from the company, and the system will not work until the company reactivates it. Some receivers are capable of decrypting the received signal itself. These receivers are called integrated receiver/decoders or IRDs.\n\nAnalog television which was distributed via satellite was usually sent scrambled or unscrambled in NTSC, PAL, or SECAM television broadcast standards. The analog signal is frequency modulated and is converted from an FM signal to what is referred to as baseband. This baseband comprises the video signal and the audio subcarrier(s). The audio subcarrier is further demodulated to provide a raw audio signal.\n\nLater signals were digitized television signal or multiplex of signals, typically QPSK. In general, digital television, including that transmitted via satellites, is based on open standards such as MPEG and DVB-S/DVB-S2 or ISDB-S.\n\nThe conditional access encryption/scrambling methods include NDS, BISS, Conax, Digicipher, Irdeto, Cryptoworks, DG Crypt, Beta digital, SECA Mediaguard, Logiways, Nagravision, PowerVu, Viaccess, Videocipher, and VideoGuard. Many conditional access systems have been compromised.\n\nAn event called sun outage occurs when the sun lines up directly behind the satellite in the field of view of the receiving satellite dish. This happens for about a 10-minute period daily around midday, twice every year for a two-week period in the spring and fall around the equinox. During this period, the sun is within the main lobe of the dish's reception pattern, so the strong microwave noise emitted by the sun on the same frequencies used by the satellite's transponders drowns out reception.\n\nDirect-To-Home (DTHTV) can either refer to the communications satellites themselves that deliver service or the actual television service. Most satellite television customers in developed television markets get their programming through a direct broadcast satellite provider. Signals are transmitted using K band and are completely digital which means it has high picture and stereo sound quality.\n\nProgramming for satellite television channels comes from multiple sources and may include live studio feeds. The broadcast center assembles and packages programming into channels for transmission and, where necessary, encrypts the channels. The signal is then sent to the uplink where it is transmitted to the satellite. With some broadcast centers, the studios, administration and up-link are all part of the same campus. The satellite then translates and broadcasts the channels.\n\nMost systems use the DVB-S standard for transmission. With pay television services, the datastream is encrypted and requires proprietary reception equipment. While the underlying reception technology is similar, the pay television technology is proprietary, often consisting of a conditional-access module and smart card. This measure assures satellite television providers that only authorized, paying subscribers have access to pay television content but at the same time can allow free-to-air channels to be viewed even by the people with standard equipment available in the market.\n\nSome countries operate satellite television services which can be received for free, without paying a subscription fee. This is called free-to-air satellite television. Germany is likely the leader in free-to-air with approximately 250 digital channels (including 83 HDTV channels and various regional channels) broadcast from the Astra 19.2°E satellite constellation. These are not marketed as a DBS service, but are received in approximately 18 million homes, as well as in any home using the \"Sky Deutschland\" commercial DBS system. All German analogue satellite broadcasts ceased on 30 April 2012.\n\nThe United Kingdom has approximately 160 digital channels (including the regional variations of BBC channels, ITV channels, Channel 4 and Channel 5) that are broadcast without encryption from the Astra 28.2°E satellite constellation, and receivable on any DVB-S receiver (a DVB-S2 receiver is required for certain high definition television services). Most of these channels are included within the Sky EPG, and an increasing number within the Freesat EPG.\n\nIndia's national broadcaster, Doordarshan, promotes a free-to-air DBS package as \"DD Free Dish\", which is provided as in-fill for the country's terrestrial transmission network. It is broadcast from GSAT-15 at 93.5°E and contains about 80 FTA channels.\n\nWhile originally launched as backhaul for their digital terrestrial television service, a large number of French channels are free-to-air on satellites at 5°W, and have recently been announced as being official in-fill for the DTT network.\n\nIn North America (United States, Canada and Mexico) there are over 80 FTA digital channels available on Galaxy 19 (with the majority being ethnic or religious in nature). Other FTA satellites include AMC-4, AMC-6, Galaxy 18, and Satmex 5. A company called GloryStar promotes FTA religious broadcasters on Galaxy 19.\n\nThe term Television receive-only, or TVRO, arose during the early days of satellite television reception to differentiate it from commercial satellite television uplink and downlink operations (transmit and receive). This was the primary method of satellite television transmissions before the satellite television industry shifted, with the launch of higher powered DBS satellites in the early 1990s which transmitted their signals on the K band frequencies. Satellite television channels at that time were intended to be used by cable television networks rather than received by home viewers. Early satellite television receiver systems were largely constructed by hobbyists and engineers. These early TVRO systems operated mainly on the C-band frequencies and the dishes required were large; typically over 3 meters (10 ft) in diameter. Consequently, TVRO is often referred to as \"big dish\" or \"Big Ugly Dish\" (BUD) satellite television.\n\nTVRO systems were designed to receive analog and digital satellite feeds of both television or audio from both C-band and K-band transponders on FSS-type satellites. The higher frequency K-band systems tend to resemble DBS systems and can use a smaller dish antenna because of the higher power transmissions and greater antenna gain. TVRO systems tend to use larger rather than smaller satellite dish antennas, since it is more likely that the owner of a TVRO system would have a C-band-only setup rather than a K band-only setup. Additional receiver boxes allow for different types of digital satellite signal reception, such as DVB/MPEG-2 and 4DTV.\n\nThe narrow beam width of a normal parabolic satellite antenna means it can only receive signals from a single satellite at a time. Simulsat or the Vertex-RSI TORUS, is a quasi-parabolic satellite earthstation antenna that is capable of receiving satellite transmissions from 35 or more C- and K-band satellites simultaneously.\n\nIn 1945 British science fiction writer Arthur C. Clarke proposed a worldwide communications system which would function by means of three satellites equally spaced apart in earth orbit. This was published in the October 1945 issue of the Wireless World magazine and won him the Franklin Institute's Stuart Ballantine Medal in 1963.\n\nThe first public satellite television signals from Europe to North America were relayed via the Telstar satellite over the Atlantic ocean on 23 July 1962, although a test broadcast had taken place almost two weeks earlier on 11 July. The signals were received and broadcast in North American and European countries and watched by over 100 million. Launched in 1962, the \"Relay 1\" satellite was the first satellite to transmit television signals from the US to Japan. The first geosynchronous communication satellite, Syncom 2, was launched on 26 July 1963.\n\nThe world's first commercial communications satellite, called Intelsat I and nicknamed \"Early Bird\", was launched into geosynchronous orbit on April 6, 1965. The first national network of television satellites, called Orbita, was created by the Soviet Union in October 1967, and was based on the principle of using the highly elliptical Molniya satellite for rebroadcasting and delivering of television signals to ground downlink stations. The first commercial North American satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972. ATS-6, the world's first experimental educational and direct broadcast satellite (DBS), was launched on 30 May 1974. It transmitted at 860 MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent but experimenters were able to receive the signal in Western Europe using home constructed equipment that drew on UHF television design techniques already in use.\n\nThe first in a series of Soviet geostationary satellites to carry direct-to-home television, Ekran 1, was launched on 26 October 1976. It used a 714 MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.\n\nThe satellite television industry developed first in the US from the cable television industry as communication satellites were being used to distribute television programming to remote cable television headends. Home Box Office (HBO), Turner Broadcasting System (TBS), and Christian Broadcasting Network (CBN, later The Family Channel) were among the first to use satellite television to deliver programming. Taylor Howard of San Andreas, California became the first person to receive C-band satellite signals with his home-built system in 1976.\n\nIn the US, PBS, a non-profit public broadcasting service, began to distribute its television programming by satellite in 1978.\nIn 1979, Soviet engineers developed the Moskva (or Moscow) system of broadcasting and delivering of TV signals via satellites. They launched the Gorizont communication satellites later that same year. These satellites used geostationary orbits. They were equipped with powerful on-board transponders, so the size of receiving parabolic antennas of downlink stations was reduced to 4 and 2.5 metres. On October 18, 1979, the Federal Communications Commission (FCC) began allowing people to have home satellite earth stations without a federal government license. The front cover of the 1979 Neiman-Marcus Christmas catalogue featured the first home satellite TV stations on sale for $36,500. The dishes were nearly in diameter and were remote controlled. The price went down by half soon after that, but there were only eight more channels. The Society for Private and Commercial Earth Stations (SPACE), an organisation which represented consumers and satellite TV system owners, was established in 1980.\n\nEarly satellite television systems were not very popular due to their expense and large dish size. The satellite television dishes of the systems in the late 1970s and early 1980s were in diameter, made of fibreglass or solid aluminum or steel, and in the United States cost more than $5,000, sometimes as much as $10,000. Programming sent from ground stations was relayed from eighteen satellites in geostationary orbit located above the Earth.\n\nBy 1980, satellite television was well established in the USA and Europe. On 26 April 1982, the first satellite channel in the UK, Satellite Television Ltd. (later Sky1), was launched. Its signals were transmitted from the ESA's Orbital Test Satellites. Between 1981 and 1985, TVRO systems' sales rates increased as prices fell. Advances in receiver technology and the use of gallium arsenide FET technology enabled the use of smaller dishes. Five hundred thousand systems, some costing as little as $2000, were sold in the US in 1984. Dishes pointing to one satellite were even cheaper. People in areas without local broadcast stations or cable television service could obtain good-quality reception with no monthly fees. The large dishes were a subject of much consternation, as many people considered them eyesores, and in the US most condominiums, neighborhoods, and other homeowner associations tightly restricted their use, except in areas where such restrictions were illegal. These restrictions were altered in 1986 when the Federal Communications Commission ruled all of them illegal. A municipality could require a property owner to relocate the dish if it violated other zoning restrictions, such as a setback requirement, but could not outlaw their use. The necessity of these restrictions would slowly decline as the dishes got smaller.\n\nOriginally, all channels were broadcast in the clear (ITC) because the equipment necessary to receive the programming was too expensive for consumers. With the growing number of TVRO systems, the program providers and broadcasters had to scramble their signal and develop subscription systems.\n\nIn October 1984, the U.S. Congress passed the Cable Communications Policy Act of 1984, which gave those using TVRO systems the right to receive signals for free unless they were scrambled, and required those who did scramble to make their signals available for a reasonable fee. Since cable channels could prevent reception by big dishes, other companies had an incentive to offer competition. In January 1986, HBO began using the now-obsolete VideoCipher II system to encrypt their channels. Other channels used less secure television encryption systems. The scrambling of HBO was met with much protest from owners of big-dish systems, most of which had no other option at the time for receiving such channels, claiming that clear signals from cable channels would be difficult to receive. Eventually HBO allowed dish owners to subscribe directly to their service for $12.95 per month, a price equal to or higher than what cable subscribers were paying, and required a descrambler to be purchased for $395. This led to the attack on HBO's transponder Galaxy 1 by John R. MacDougall in April 1986. One by one, all commercial channels followed HBO's lead and began scrambling their channels. The Satellite Broadcasting and Communications Association (SBCA) was founded on December 2, 1986 as the result of a merger between SPACE and the Direct Broadcast Satellite Association (DBSA).\n\nVideocipher II used analog scrambling on its video signal and Data Encryption Standard–based encryption on its audio signal. VideoCipher II was defeated, and there was a black market for descrambler devices which were initially sold as \"test\" devices.\n\nThe necessity for better satellite television programming than TVRO arose in the 1980s. Satellite television services, first in Europe, began transmitting K band signals in the late 1980s. On 11 December 1988 Luxembourg launched Astra 1A, the first satellite to provide medium power satellite coverage to Western Europe. This was one of the first medium-powered satellites, transmitting signals in K band and allowing reception with small(90 cm) dishes for the first time ever. The launch of Astra beat the winner of the UK's state Direct Broadcast Satellite licence, British Satellite Broadcasting, to the market, and accelerated its demise.\n\nBy 1987, nine channels were scrambled, but 99 others were available free-to-air. While HBO initially charged a monthly fee of $19.95, soon it became possible to unscramble all channels for $200 a year. Dish sales went down from 600,000 in 1985 to 350,000 in 1986, but pay television services were seeing dishes as something positive since some people would never have cable service, and the industry was starting to recover as a result. Scrambling also led to the development of pay-per-view events. On November 1, 1988, NBC began scrambling its C-band signal but left its K band signal unencrypted in order for affiliates to not lose viewers who could not see their advertising. Most of the two million satellite dish users in the United States still used C-band. ABC and CBS were considering scrambling, though CBS was reluctant due to the number of people unable to receive local network affiliates. The piracy on satellite television networks in the US led to the introduction of the Cable Television Consumer Protection and Competition Act of 1992. This legislation enabled anyone caught engaging in signal theft to be fined up to $50,000 and to be sentenced to a maximum of two years in prison. A repeat offender can be fined up to $100,000 and be imprisoned for up to five years.\n\nSatellite television had also developed in Europe but it initially used low power communication satellites and it required dish sizes of over 1.7 metres. On 11 December 1988 Luxembourg launched Astra 1A, the first satellite to provide medium power satellite coverage to Western Europe. This was one of the first medium-powered satellites, transmitting signals in K band and allowing reception with small dishes (90 cm). The launch of Astra beat the winner of the UK's state Direct Broadcast Satellite licence holder, British Satellite Broadcasting, to the market.\n\nIn the US in the early 1990s, four large cable companies launched PrimeStar, a direct broadcasting company using medium power satellites. The relatively strong transmissions allowed the use of smaller (90 cm) dishes. Its popularity declined with the 1994 launch of the Hughes DirecTV and Dish Network satellite television systems.\n\nOn March 4, 1996 EchoStar introduced Digital Sky Highway (Dish Network) using the EchoStar 1 satellite. EchoStar launched a second satellite in September 1996 to increase the number of channels available on Dish Network to 170. These systems provided better pictures and stereo sound on 150–200 video and audio channels, and allowed small dishes to be used. This greatly reduced the popularity of TVRO systems. In the mid-1990s, channels began moving their broadcasts to digital television transmission using the DigiCipher conditional access system.\n\nIn addition to encryption, the widespread availability, in the US, of DBS services such as PrimeStar and DirecTV had been reducing the popularity of TVRO systems since the early 1990s. Signals from DBS satellites (operating in the more recent K band) are higher in both frequency and power (due to improvements in the solar panels and energy efficiency of modern satellites) and therefore require much smaller dishes than C-band, and the digital modulation methods now used require less signal strength at the receiver than analog modulation methods. Each satellite also can carry up to 32 transponders in the K band, but only 24 in the C band, and several digital subchannels can be multiplexed (MCPC) or carried separately (SCPC) on a single transponder. Advances in noise reduction due to improved microwave technology and semiconductor materials have also had an effect. However, one consequence of the higher frequencies used for DBS services is rain fade where viewers lose signal during a heavy downpour. C-band satellite television signals are less prone to rain fade.\n\nIn a return to the older (but proven) technologies of satellite communication, the current DBS-based satellite providers in the USA (Dish Network and DirecTV) are now utilizing additional capacity on the K-band transponders of existing FSS-class satellites, in addition to the capacity on their own existing fleets of DBS satellites in orbit. This was done in order to provide more channel capacity for their systems, as required by the increasing number of High-Definition and simulcast local station channels. The reception of the channels carried on the K-band FSS satellite's respective transponders has been achieved by both DirecTV & Dish Network issuing to their subscribers dishes twice as big in diameter (36\") than the previous 18\" (& 20\" for the Dish Network \"Dish500\") dishes the services used initially, equipped with 2 circular-polarized LNBFs (for reception of 2 native DBS satellites of the provider, 1 per LNBF), and 1 standard linear-polarized LNB for reception of channels from an FSS-type satellite. These newer DBS/FSS-hybrid dishes, marketed by DirecTV and Dish Network as the \"SlimLine\" and \"SuperDish\" models respectively, are now the current standard for both providers, with their original 18\"/20\" single or dual LNBF dishes either now obsolete, or only used for program packages, separate channels, or services only broadcast over the providers' DBS satellites.\n\nOn 29 November 1999 US President Bill Clinton signed the Satellite Home Viewer Improvement Act (SHVIA). The act allowed Americans to receive local broadcast signals via direct broadcast satellite systems for the first time.\n\nSatellite Television for the Asian Region (STAR), a service based in Mumbai and Hong Kong which now provides satellite TV coverage to Asia and Australia, introduced satellite TV to the Asian region in the early 1990s. It began broadcasting signals using the AsiaSat 1 satellite on 1 January 1991.\n\n"}
{"id": "7037353", "url": "https://en.wikipedia.org/wiki?curid=7037353", "title": "Satyrion", "text": "Satyrion\n\nIn Greek mythology, Satyrion is the name of a nymph, perhaps from the region of Taranto, Italy. Her union with the god Poseidon produced Taras.\n\nSatyrions is a former name for orchids from their connection to satyrs. Orchis was the son of a satyr and a nymph, who was killed for insulting a priestess of Bacchus, and was turned into the flower that bears his name. It was believed that orchids were the food of the satyrs, and incited them to excesses.\n\nSatyrion is also a name for ragwort and ancient aphrodisiac made from it. Though it may have been named after the nymph, it more likely derives from the mythical and lustful satyrs. This aphrodisiac is mentioned twice in the Satyricon of Petronius.\n"}
{"id": "18895331", "url": "https://en.wikipedia.org/wiki?curid=18895331", "title": "Telescope Array Project", "text": "Telescope Array Project\n\nThe Telescope Array project is an international collaboration involving research and educational institutions in Japan, The United States, Russia, South Korea, and Belgium. The experiment is designed to observe air showers induced by ultra-high-energy cosmic ray using a combination of ground array and air-fluorescence techniques. It is located in the high desert in Millard County, Utah (USA) at about above sea level.\n\nThe Telescope Array observatory is a hybrid detector system consisting of both an array of 507 scintillation surface detectors (SD) which measure the distribution of charged particles at the Earth's surface, and three fluorescence stations which observe the night sky above the SD array. Each fluorescence station is also accompanied by a LIDAR system for atmospheric monitoring. The SD array is much like that of the AGASA group, but covers an area that is nine times larger. The hybrid setup of the Telescope Array project allows for simultaneous observation of both the longitudinal development and the lateral distribution of the air showers. When a cosmic ray passes through the earth's atmosphere and triggers an air shower, the fluorescence telescopes measure the scintillation light generated as the shower passes through the gas of the atmosphere, while the array of scintillator surface detectors samples the footprint of the shower when it reaches the Earth's surface.\n\nAt the center of the ground array is the Central Laser Facility which is used for atmospheric monitoring and calibrations.\n\nThe surface detectors that make up the ground array are activated when ionizing particles from an extensive air shower pass through them. When these particles pass through the plastic scintillator within the detector, it induces photo electrons which are then gathered by wavelength-shifting fibers and sent to a photomultiplier tube. The electronic components within the detectors then filter the results, giving the detectors comparable accuracy to the AGASA experiment.\n\nThe surface detectors are evenly distributed across a 762 km grid array with 1.2 km between each unit. Each surface detector has an assembled weight of 250 kg and consists of a power supply, two layers of scintillation detectors and electronics. Power is generated by a 120W solar panel and stored in a sealed lead-acid battery. The system has the capacity to operate for one week in complete darkness. Each scintillation detector layer is made of extruded plastic scintillator that is 1.2 cm thick and has an area of 3m. The photo multiplier tube is connected to the scintillator via 96 wavelength-shifting fibers.\n\nThe Telescope Array has three fluorescence detector (FD) telescope stations. As in the previous Fly's Eye and High Resolution Fly's Eye (HiRes) experiments, these detectors work by measuring the air fluorescence light emitted by an extensive air shower. Each FD telescope consists of a primary mirror (made up of 18 smaller hexagonal mirror segments) and a camera. The cameras are made up of 256 PMTs (photomultiplier tubes) which are sensitive to the ultra violet light generated by a cosmic ray air shower.\n\nThe stations are located on a triangle about 35 km apart from one another. Each station has 12-14 telescopes viewing the range from 3 to 33 degrees in elevation. The three sites are named \"Black Rock Mesa\" (BRM), \"Long Ridge\" (LR), and \"Middle Drum\" (MD). By combining the data from the three sites, it is possible to determine the primary energy, the arrival direction, and the maximum\npoint of longitudinal development for an air shower.\n\nThe Lon and Mary Watson Millard County Cosmic Ray Center was dedicated on March 20, 2006. The center is located at 648 West Main Street in Delta. The building serves as a headquarters and data processing center for the Telescope Array Project.\n\nIn October 2011, a new visitor center was opened at the Cosmic Ray Center. It features displays about the history of cosmic ray research in Utah and about the Telescope Array, which is spread across the desert west of Delta. The center also includes a display about the nearby Topaz internment camp, where U.S. citizens of Japanese descent were imprisoned during World War II.\n\nTALE is the Telescope Array Low Energy extension. It is designed to observe cosmic rays with energies between 3×10eV and 10eV. TALE adds 10 new telescopes to the Middle Drum observatory site (24 total telescopes) extending the vertical field of view so that it now extends from 3 to 59 degrees in elevation. This allows the station to see the shower development including shower maximum for lower energy events. This is critical when trying to determine the chemical composition of the incident cosmic ray particle.\n\nThe TALE project also has a graded infill array of scintillator stations spaced 400m and 600m apart. It then connects to the main Telescope Array scintillator array where the scintillator detectors are 1200m apart. These stations measure charged particle densities (the shower footprint) at the Earth's surface for lower energy events approaching 3x10eV\n\nThe Telescope Array RADAR (TARA) Project is an effort to overcome some of the problems inherent to current cosmic ray detection techniques. Due to sun, moon and weather, fluorescence telescopes are usually limited to a ten percent duty cycle. Ground arrays can run during the day, but require a large amount of land, making it necessary to build them in remote locations. The goal of the TARA Project is to develop a bistatic radar detection system that is able to maintain a 24-hour duty cycle at a fraction of the cost of conventional detection systems.\n\nIn September 2012, the W. M. Keck Foundation awarded researchers at the University of Utah a $1 million grant to develop a bistatic radar detection system. This system will be built alongside the existing Telescope Array and will use analog television transmitters and digital receivers to observe the range, direction and strength of cosmic rays in order to trace them back to their point of origin. Once completed, this new facility will be known as the W.M. Keck Radar Observatory\n"}
{"id": "173838", "url": "https://en.wikipedia.org/wiki?curid=173838", "title": "Thorne–Żytkow object", "text": "Thorne–Żytkow object\n\nA Thorne–Żytkow object (TŻO or TZO) is a conjectured type of star wherein a red giant or supergiant contains a neutron star at its core, formed from the collision of the giant with the neutron star. Such objects were hypothesized by Kip Thorne and Anna Żytkow in 1977. In 2014, it was discovered that the star HV 2112 was a strong candidate but this has recently been called into question.\n\nA Thorne–Żytkow object is formed when a neutron star collides with a star, typically a red giant or supergiant. The colliding objects can simply be wandering stars. This is only likely to occur in extremely crowded globular clusters. Alternatively, the neutron star could form in a binary system after one of the two stars went supernova. Because no supernova is perfectly symmetric, and because the binding energy of the binary changes with the mass lost in the supernova, the neutron star will be left with some velocity relative to its original orbit. This kick may cause its new orbit to intersect with its companion, or, if its companion is a main-sequence star, it may be engulfed when its companion evolves into a red giant.\n\nOnce the neutron star enters the red giant, drag between the neutron star and the outer, diffuse layers of the red giant causes the binary star system's orbit to decay, and the neutron star and core of the red giant spiral inward toward one another. Depending on their initial separation, this process may take hundreds of years. When the two finally collide, the neutron star and red giant core will merge. If their combined mass exceeds the Tolman-Oppenheimer-Volkoff limit then the two will collapse into a black hole, resulting in a supernova that disperses the outer layers of the star. Otherwise, the two will coalesce into a single neutron star.\n\nIf a neutron star and a white dwarf merge, this could form a Thorne–Żytkow object with the properties of an R Coronae Borealis variable.\n\nThe surface of the neutron star is very hot, with temperatures exceeding 10 K: hotter than the cores of all but the most massive stars. This heat is dominated either by nuclear fusion in the accreting gas or by compression of the gas by the neutron star's gravity. Because of the high temperature, unusual nuclear processes may take place as the envelope of the red giant falls onto the neutron star's surface. Hydrogen may fuse to produce a different mixture of isotopes than it does in ordinary stellar nucleosynthesis, and some astronomers have proposed that the rapid proton nucleosynthesis that occurs in X-ray bursts also takes place inside Thorne–Żytkow objects.\n\nObservationally, a Thorne–Żytkow object may resemble a red supergiant, or, if it is hot enough to blow off the hydrogen-rich surface layers, a nitrogen-rich Wolf–Rayet star (type WN8).\n\nA TŻO has an estimated lifespan of 10–10 years. Given this lifespan, it is possible that between 20 and 200 Thorne-Żytkow objects currently exist in the Milky Way.\n\nIt has been theorized that the evolution of TŻOs will result in neutron stars with massive accretion discs, as mass loss will end the TŻO stage, and the remaining envelope converts to being a disc. These neutron stars may form the population of isolated pulsars with accretion discs. The massive accretion disc may also result in the collapse of a star, becoming a stellar companion to the neutron star. The neutron star may also accrete sufficient material to collapse into a black hole.\n\nAs of 2014, the most recent candidate, star HV 2112, has been observed to have some unusual properties that suggest that it may be a Thorne–Żytkow object. The discovering team, with Emily Levesque being the lead author, noted that HV 2112 displays some chemical characteristics that don't quite match theoretical models, but emphasize that the theoretical predictions for a Thorne–Żytkow object are quite old and theoretical improvements have been made since it was originally conceptualized.\n\nRecent work reappraising the properties of HV 2112 however has shown that star is unlikely to be a Thorne-Żytkow object, and it is more likely an intermediate mass AGB star. \n\n"}
{"id": "5122698", "url": "https://en.wikipedia.org/wiki?curid=5122698", "title": "Vertebrate Palaeontology (Benton)", "text": "Vertebrate Palaeontology (Benton)\n\nVertebrate Palaeontology is a basic textbook on vertebrate paleontology by Michael J. Benton, published by Blackwell's. It has so far appeared in four editions, published in 1990, 1997, 2005, and 2014. It is designed for paleontology graduate courses in biology and geology as well as for the interested layman.\n\nThe book is widely used, and has received excellent reviews: \n\nThe book gives an overall account of every major group of living and fossil vertebrate. At the rear of the book is a phylogenetic classification which combines both the Linnaean hierarchy and the cladistic arrangement, and has been used as a guideline for the Wikipedia pages on living and extinct vertebrate taxa. However, some of Benton's classification differs from that in the Tree of Life Web Project, especially as regards the relationship of early amphibian groups (Batrachomorpha and Reptiliomorpha).\n\n"}
{"id": "28506776", "url": "https://en.wikipedia.org/wiki?curid=28506776", "title": "Waterpod", "text": "Waterpod\n\nThe Waterpod was a community-based public art project in New York City in 2009. Open to the public, an ecosystem on a barge called the Waterpod visited the five boroughs at eight different piers for two weeks at a time during the summer of 2009, hosting a series of events. It was designed as a futuristic habitat and an experimental platform for assessing the design and efficacy of autonomous marine living systems in preparation for an assumed future.\nA multinational team including artists, designers, marine engineers, and civic activists led by the artist Mary Mattingly alongside the New York City Office of the Mayor Special Projects, the United States Coast Guard, and the New York City SBS Dockmaster Unit, this was a cross-disciplinary project that took place in the waterways of New York City in 2009.\n\nThe Waterpod Project route:\n\n\"...to fortify against the possibility of widespread climate change, desertification, and provide solutions for overpopulation and rising sea levels, the Waterpod offered a pathway to sustainable survival, mobility, and community building through a free, participatory project and event space that visited the five boroughs and Governors Island, for a voyage lasting from June to October 2009. The Waterpod’s mission has been to prepare, inform, and offer alternatives to current and future living spaces...\"\n\n\"...The Waterpod provides space for: (I) community and artistic activity; (II) eco-initiatives including food grown with filtered rainwater; and (III) living space. It provides a model for mobile vessels that can provide relief to cities and countries struck by natural disasters, as well as a model for reshaping suburban landscapes to be a self-sustaining living system. The methods that make up the Waterpod provide people with necessary systems for rotational food supply, seasonal seed collection and soil-renewing compost, potable water, and mobile shelter with minimal upkeep...\"\n\nThe Waterpod was an experiment in creatively using available, local reused materials from the New York Waste Stream:\n\"...The dome covers were constructed Waterpod’s from repurposed billboard vinyl...The soil was made in the Bronx from compost and sand, tested and donated from the New York City Department of Parks and Recreation... Construction materials also included salvaged pieces of sunken vessels raised from the rivers bottom in the Rockaway and other areas.\"\n\n\n"}
