{"id": "24637684", "url": "https://en.wikipedia.org/wiki?curid=24637684", "title": "1991 VG", "text": "1991 VG\n\n1991 VG is a very small near-Earth object of the Apollo group, approximately in diameter. It was first observed by American astronomer James Scotti on 6 November 1991, using the Spacewatch telescope on Kitt Peak National Observatory near Tucson, Arizona, in the United States.\n\nOn 6 November 1991, Scotti discovered a faint object which was designated \"1991 VG\" soon after discovery. The object's heliocentric orbit was found to be very similar to Earth's orbit and it was calculated that it would make a close approach to Earth in the month after discovery at 1.2 lunar distances or on 5 December 1991. \"1991 VG\" also passed 0.0568 AU from Earth on 7 August 2017. Given such an Earth-like orbit, the dynamical lifetime of such an object is relatively short with the object quickly either impacting Earth or being perturbed by Earth onto a different orbit. The similarity of its orbit with Earth was also very difficult to explain from natural sources, with ejecta from a recent Lunar impact or non-gravitational perturbations such as the Yarkovsky effect having been suggested. The first Earth Trojan asteroid, 2010 TK7, was later identified and such objects could well be a source for objects like \"1991 VG\". This near-Earth asteroid has been a transient co-orbital of the horseshoe type in the past and it will return as such in the future; it was a natural satellite of our planet for about a month back in 1992. This temporary capture may have taken place multiple times in the past and it is expected to repeat again in the future. It had an eccentricity of less than 1 with respect to the Earth from 23 February to 21 March 1992.\n\nSince the discovery of \"1991 VG\", about 80% of small asteroids with absolute magnitudes fainter than 22.0 (corresponding to sizes smaller than about 200 meters) which have had their lightcurve measured have rotation periods under 2 hours. The so-called fast rotators are typically monolithic bodies or, alternatively, welded conglomerates with a sufficient intrinsic strength to counteract centrifugal forces. More slowly rotating asteroids are sometimes gravitationally bound aggregates.\n\nThe uncertainty of the object's origin, combined with rapid variation in the object's brightness in images obtained during its close passage with Earth in early December 1991, led to some speculation that \"1991 VG\" might be a spent rocket fuel tank. There was speculation that it could be a rocket body from a satellite launched in the early 1970s, or from Apollo 12 mission. A detailed analysis of the available evidence confirms that there is no compelling reason to believe that \"1991 VG\" is not natural.\n\nAs of 2018, with a highly accurate orbit, it is highly unlikely that \"1991 VG\" is artificial, as it did not approach anywhere near Earth at any point since 1900.\n\n\"1991 VG\" was not observed between 1992 and 2017. But after 26 years, \"1991 VG\" had returned to the vicinity of Earth. As part of a program by ESA and ESO to secure the orbit of faint but potentially threatening Near-Earth Objects, \"1991 VG\" was recovered by the ESO VLT on 30 May 2017, at magnitude 25. With this recovery, the orbit of \"1991 VG\" is now determined with a high precision. It was removed from the Sentry Risk Table on 1 June 2017. The fact that it has stayed on a stable orbit for many years indicates not only that it is an inert object (i.e. not an active spacecraft), but also that its density is fairly high: an empty fuel tank, for instance, would have its orbit slightly changing in time due to the radiation pressure from the Sun. This strengthens the probability that \"1991 VG\" is a near Earth asteroid. It was last observed 1 June 2017.\n\nThe Near-Earth Asteroid Scout is a planned mission by NASA to develop a low-cost CubeSat solar sail spacecraft capable of encountering near-Earth asteroids and taking high resolution images. It is expected to launch in 2019, and \"1991 VG\" is the tentatively planned target.\n\n"}
{"id": "42017288", "url": "https://en.wikipedia.org/wiki?curid=42017288", "title": "2014 in arthropod paleontology", "text": "2014 in arthropod paleontology\n\nThis list of fossil arthropods described in 2014 is a list of new taxa of trilobites, fossil insects, crustaceans, arachnids and other fossil arthropods of every kind that have been described during the year 2014. The list only includes taxa at the level of genus or species.\n"}
{"id": "177766", "url": "https://en.wikipedia.org/wiki?curid=177766", "title": "88 modern constellations by area", "text": "88 modern constellations by area\n\nThe table below ranks the 88 modern constellations by the solid angle that they subtend in the sky, measured in square degrees and millisteradians.\n\nThese solid angles depend on arbitrary boundaries between the constellations: the list below is based on constellation boundaries drawn up by Eugène Delporte in 1930 on behalf of the IAU and published in \"Délimitation scientifique des constellations\" (Cambridge University Press). Before Delporte's work, there was no standard list of the boundaries of each constellation.\n\nDelporte drew the boundaries along vertical and horizontal lines of right ascension and declination; however, he did so for the epoch B1875.0, which means that due to precession of the equinoxes, the borders on a modern star map (e.g., for epoch J2000) are already somewhat skewed and no longer perfectly vertical or horizontal. This skew will increase over the years and centuries to come. However, this does not change the solid angle of any constellation.\n\n\n"}
{"id": "5308146", "url": "https://en.wikipedia.org/wiki?curid=5308146", "title": "Animals in Buddhism", "text": "Animals in Buddhism\n\nThe position and treatment of animals in Buddhism is important for the light it sheds on Buddhists' perception of their own relation to the natural world, on Buddhist humanitarian concerns in general, and on the relationship between Buddhist theory and Buddhist practice.\n\nAnimals have always been regarded in Buddhist thought as sentient beings. Furthermore, animals possess Buddha nature (according to the Mahāyāna school) and therefore potential for enlightenment. Moreover, the doctrine of rebirth held that any human could be reborn as animal, and any animal could be reborn as a human. An animal might be a reborn dead relative, and anybody who looked far enough back through their series of lives might come to believe every animal to be a distant relative. The Buddha expounded that sentient beings currently living in the animal realm have been our mothers, brothers, sisters, fathers, children, friends in past rebirths. One could not, therefore, make a hard distinction between moral rules applicable to animals and those applicable to humans; ultimately humans and animals were part of a single family. They are all interconnected.\n\nIn cosmological terms, the animals were believed to inhabit a distinct \"world\", separated from humans not by space but by state of mind. This world was called \"Tiryagyoni\" in Sanskrit, \"Tiracchānayoni\" in Pāli. Rebirth as an animal was considered to be one of the unhappy rebirths, usually involving more than human suffering. Buddhist commentarial texts depict many sufferings associated with the animal world: even where no human beings are present, they are attacked and eaten by other animals or live in fear of it, they endure extreme changes of environment throughout the year, and they have no security of habitation. Those that live among humans are often slaughtered for their bodies, or taken and forced to work with many beatings until they are slaughtered at the end of their lives. On top of this, they suffer from ignorance, not knowing or understanding with any clarity what is happening to them and unable to do much about it, acting primarily on instinct.\n\nThe Chinese scholar Tiantai taught the principle of the Mutual Possession of the Ten Worlds. This meant that all living beings have buddha-nature 'in their present form'. In the 'Devadatta chapter of the Lotus Sutra the Dragon King's Daughter attains Buddhahood in her present form, thus opening the way for both women and animals to attain Buddhahood.\n\nThe Jātaka stories which tell of past lives of the Buddha in folktale fashion, frequently involve animals as peripheral or main characters, and it is not uncommon for the Bodhisattva (the past-life Buddha) to appear as an animal as well. The stories sometimes involve animals alone, and sometimes involve conflicts between humans and animals; in the latter cases, the animals often exhibit characteristics of kindness and generosity that are absent in the humans.\n\nAlso recorded in the Jatakas is how, in a past life as King Shibi, Shakyamuni sacrificed himself to save a dove from a hawk. Recorded in the Golden Light Sutra, is how Shakyamuni in a past life, as Prince Sattva, came across a starving tigress and her cubs, he fed himself to them so that they would survive.\n\nThe first of the five precepts bans the taking of life. As most narrowly interpreted, it applies primarily to the killing of human beings; however, the broader interpretation is that it applies to all sentient beings, which includes those in the animal realm in its broadest sense, i.e., not just mammals, but all animal taxa including insects, and invertebrates. From the beginnings of Buddhism, there were regulations intended to prevent the harming of sentient beings in the animal realm for various reasons.\n\nThe Buddha taught that all sentient beings, including those in the animal realm, possess Buddha nature and therefore can attain enlightenment and that from infinite rebirths, all animals have been our past relatives, sisters, mothers, brothers, fathers and children. Therefore, it is against the first precept to harm, kill or eat sentient beings as it is the same as harming, killing or eating the flesh of our own child or mother. Monks were forbidden from intentionally killing an animal, or drinking water with living creatures (such as larvae) in it.\n\nConcern for animals is attested back to the beginnings of Buddhist history. The first Buddhist monarch of India, Ashoka, includes in his Edicts an expression of concern for the number of animals that had been killed for his meals, and expresses an intention to put an end to this killing. He also includes animals with humans as the beneficiaries of his programs for obtaining medicinal plants, planting trees and digging wells. In his fifth Pillar Edict, Ashoka decrees the protection of a large number of animals that were not in common use as livestock; protects from slaughter young animals and mother animals still milking their young; protects forests from being burned, expressly to protect the animals living in them; and bans a number of other practices hurtful to animals. In this Ashoka was carrying out the advice to the Cakravartin king given in the \"Cakkavattisīhanāda-sutta\" (DN.26) that a good king should extend his protection not merely to different classes of people equally, but also to beasts and birds.\n\nA basic precept in Buddhism is that of non-harm. Actions which result in the taking of life, directly or indirectly, contradict this basic Buddhist precept.\n\nMany Buddhists in many countries, including monks, are not vegetarians. In recent years, however, some people's attitudes are changing. In December 2007, the 17th Karmapa, Ogyen Trinley Dorje (The identification of the 17th Karmapa is disputed, see Karmapa controversy), reflected on the subject:\n\n\"Last year on the final day of the Kagyu Monlam, I said a few things on the subject of giving up eating meat. Almost all of you probably already know this. It seems some people did not completely understand what I said. For example, some foreign students seemed to think it meant that once you become a student of the Kagyu, meat is not allowed to pass your lips. They told all the meat-eating Kagyupas, “You can’t be a Kagyupa if you eat meat.” I did not say anything that inflammatory. If a Mahayana practitioner, who considers all sentient beings to be like their father or mother, eats the flesh of another being out of carelessness and without any compassion, that is not good. So we need to think about this and pay attention to it. All of us Mahayana practitioners, who accept that all sentient beings have been our mothers and fathers, need to think about this. For that reason, it would be good to decrease the amount of meat that we eat. That is what I said.\n\nI certainly did not say that you are not allowed to eat meat at all. That would be difficult. Whether it is because of previous karma or their present circumstances, some people cannot do without meat. This is how it is, and there’s nothing to do about it. It’s not a problem.\"\n\nThere has been some contention about interpretations of the sūtras. One interpretation is that eating of meat is not explicitly prohibited in the suttas and Vinaya of the Pāli canon which encourage monks to accept whatever food they are given. However, monks are forbidden from accepting animal flesh if they know, believe or suspect that the animal in question was killed especially for them, i.e., if the visits of begging monks have become an occasion for the slaughter of animals.\n\nIn the Mahāyāna & sutras, the Buddha explicitly prohibits the eating of meat, fish and any animal products which are the result of harming and killing of any sentient being. The Buddha states the only time it is acceptable for a monastic to accept and eat the flesh of sentient beings is for medicinal purposes only if the animal died in accordance with the Dharma, meaning the animal died of natural causes.\n\nIn Mahāyāna Chinese Buddhism and in those countries to which Chinese Buddhism has spread (such as Korea, Vietnam), Buddhist monks are more strictly vegetarian. One of the scriptural sources for this prohibition is the Mahāyāna Sūtra. This sūtra condemns meat-eating in the strongest terms; among several other reasons, it is stated that it should be avoided because the presence of a meat-eater causes terror in animals, who believe them to be likely to kill them.\n\nAlthough vegetarianism is not expressly commanded in the Pāli canon, it is evidently viewed as an ideal state from which human beings have fallen; the Aggañña Sutta (DN.27) explains how human beings, originally sustained on various kinds of vegetable matter (compare Gen.1:29-30), as the result of increasing wickedness began to live by hunting, which was originally thought of as a demeaning occupation.\n\nIn East Asian Buddhism and particularly in Tibet and China, the release of animals, particularly birds or fish, into their natural environment became an important way of demonstrating Buddhist pity. In Tibetan Buddhism it is known as \"Tsethar\"; whilst in China it was known as \"放生 (Fàngshēng)\". This practice is based on a passage in the Mahāyāna Sūtra of Brahma's Net (Ch: Fanwang Jing), which states that \"...all the beings in the six paths of existence are my parents. If I should kill and eat them, it is the same as killing my own parents. ... Since to be reborn into one existence after another is the permanent and unalterable law, we should teach people to release sentient beings.\" In the later Ming dynasty, societies \"for releasing life\" were created, which built ponds in which to release fish that were redeemed from fishermen for this purpose. They also bought other animals which were sold in the markets and released them.\n\nIt is increasingly recognized that animal release has the potential for negative environmental impacts, including as a pathway for the introduction of invasive species into non-native environments. This may lead to biodiversity loss over time. Further, some animals are captured for the explicit purpose of being released.\n\n"}
{"id": "11292509", "url": "https://en.wikipedia.org/wiki?curid=11292509", "title": "Aquarius (SAC-D instrument)", "text": "Aquarius (SAC-D instrument)\n\nAquarius was a NASA instrument aboard the Argentine SAC-D spacecraft. Its mission was to measure global sea surface salinity to better predict future climate conditions.\n\nAquarius was shipped to Argentina on June 1, 2009 to be mounted in the INVAP built SAC-D satellite. It came back to Vandenberg Air Force Base on March 31, 2011.\n\nFor the joint mission, Argentina provided the SAC-D spacecraft and additional science instruments, while NASA provided the Aquarius salinity sensor and the rocket launch platform. The National Aeronautics and Space Administration (NASA)'s Jet Propulsion Laboratory in Pasadena, California, managed the Aquarius Mission development for NASA's Earth Science Enterprise based in Washington, D.C., and NASA's Goddard Spaceflight Center in Greenbelt, Maryland, is managing the mission after launch.\n\nThe observatory was successfully launched from Vandenberg Air Force Base on June 10, 2011. After its launch aboard a Delta II from Vandenberg Air Force Base in California, SAC-D was carried into a 657 km (408 mi) sun-synchronous orbit to begin its 3-year mission.\n\nOn June 7, 2015, the SAC-D satellite carrying Aquarius suffered a power supply failure, ending the mission.\n\nThe spacecraft’s mission is a joint program between the National Aeronautics and Space Administration (NASA) and Argentina’s space agency, Comisión Nacional de Actividades Espaciales (CONAE). The Aquarius sensors are flown on the (now inoperative) Satélite de Applicaciones Científicas (SAC)-D spacecraft 657 kilometers (408 miles) above earth in a sun-synchronous, polar orbit that repeats itself once a week. Its instrument resolution was 150 kilometers (93 miles).\n\nAquarius objective was to provide insight into the effect of salt on the Earth’s weather and climate systems by making the first space based observations of variations in salinity and creating global ocean salinity distribution maps. Data from the instrument will be able to show changes in the ocean’s salinity on monthly, yearly and seasonal time scales.\n\nOceanographers use the Practical Salinity Scale (PSS) to measure salinity based on measurements of temperature, pressure and seawater conductivity and create a ratio. The PSS compares the conductivity ratio of a sample of seawater to a standard KCl solution. The oceanic average sea surface salinity is about 35 PSS (or 3.5% salt) and varies globally from 32 to 37 PSS. The Aquarius mission goal is to measure changes in salinity of 0.2 PSS.\n\nAquarius measured sea surface salinity by using radiometers to detect changes in the oceans microwave thermal emissions frequencies due to salinity. Aquarius’ three radiometers have antenna reflectors 2.5 meters (8.2 feet) in diameter that are able to scan a 390 kilometer (242 mile) wide swath of the ocean’s surface collectively. The radiometers on Aquarius are the most accurate ever and were able to sense at a frequency of 1.4 GHz.\n\nRaw data records from the Aquarius instrument was transmitted from CONAE to the ground station at NASA’s Goddard Space Flight Center (GSFC) in Greenbelt, MD. The CONAE ground station is located in Córdoba, Argentina, where mission operations are conducted, data is processed and instrument operations are held. The processed data will create salinity related data products that will be archived for use by NASA’s Physical Oceanography Distributed Active Archive Center (PO.DAAC) in Pasadena, CA’s Jet Propulsion Laboratory (JPL).\n\nAfter less than one month in operation, Aquarius produced the first map showing the varying degrees of salinity across the \nocean's surface produced by NASA. The first salinity maps from space were provided by the European Space Agency satellite SMOS (Soil Moisture and Ocean Salinity) that was launched in November 2009. Previous satellites enabled measurement of ocean currents, sea surface temperature and winds, and ocean color. Aquarius adds the ability to measure another ocean variable – the salt content. Measuring sea surface salinity (SSS) will supplement other satellite observations of the global water cycle: precipitation, evaporation, soil moisture, atmospheric water vapor, and sea ice extent.\n\nIn the past, salinity measurements have been taken using instruments in buoys and on ships, however the measurements are inconsistent and don’t provide accurate data over large temporal and spatial regions. Aquarius’ ability to consistently map the oceans enables scientists to create more advanced computer models to study sea surface salinity and potentially forecast future climate conditions.\n\n"}
{"id": "5801630", "url": "https://en.wikipedia.org/wiki?curid=5801630", "title": "Ball clay", "text": "Ball clay\n\nBall clays are kaolinitic sedimentary clays that commonly consist of 20–80% kaolinite, 10–25% mica, 6–65% quartz. Localized seams in the same deposit have variations in composition, including the quantity of the major minerals, accessory minerals and carbonaceous materials such as lignite. They are fine-grained and plastic in nature, and, unlike most earthenware clays, produce a fine quality white-coloured pottery body when fired, which is the key to their popularity with potters.\n\nBall clays are relatively scarce deposits due to the combination of geological factors needed for their formation and preservation. They are mined in parts of the Eastern United States and from three sites in Devon and Dorset in South West England. They are commonly used in the construction of many ceramic articles, where their primary role, apart from their white colour, is to either to impart plasticity or to aid rheological stability during the shaping processes.\n\nThe name \"ball clay\" is believed to derive from the time when the clay was mined by hand. It was cut into 15 to 17-kilogram cubes and during transport the corners of the cubes became rounded off leaving \"balls\".\n\nThe ceramic use of ball clays in Britain dates back to at least the Roman era. More recent trade began when a clay was needed to construct tobacco pipes in the 16th and 17th century. In 1771 Josiah Wedgwood signed a contract for 1400 tons a year of ball clay with Thomas Hyde of Purbeck, enabling him to fire thinner-walled ceramics.\n\n"}
{"id": "36072163", "url": "https://en.wikipedia.org/wiki?curid=36072163", "title": "Beyond Coal", "text": "Beyond Coal\n\nThe Beyond Coal movement is a campaign by environmental group the Sierra Club to promote renewable energy instead of coal. Their primary objective is to close coal power plants in the United States, including at least one-third of the country's more than 500 coal plants by 2020, and to replace them with renewable energy sources. The campaign is also active in other countries; for example they are trying to prevent the construction of the Kosovo C thermal power plant near Pristina, Kosovo; to this end they have collaborated with academic and Obama administration climate advisor Dan Kammen. Other objectives include keeping coal in the ground, specifically in Appalachia and the Powder River Basin, where the majority of American coal reserves are located, and preventing coal from being exported from America.\n\nThe campaign has received at least $80 million from Michael Bloomberg and his philanthropic foundation, Bloomberg Philanthropies. During the early Presidency of George W. Bush, an energy task force convened by Dick Cheney advocated the construction of 200 new coal plants in the United States, warning that if they were not built the entire country would face load shedding as California had just seen. During the Bush administration, the Beyond Coal campaign prevented 170 of the 200 plants from being built.\n\nIn November 2017, a similar campaign, called Europe Beyond Coal, was launched in Europe. This campaign is inspired by the US Beyond Coal campaign but is independent from it. Europe Beyond Coal is an alliance of civil society groups working to catalyse the closures of coal mines and power plants, prevent the building of any new coal projects and hasten the just transition to clean, renewable energy and energy efficiency. Over 30 NGOs, including Greenpeace, WWF, EEB, Climate Action Network Europe and many others, take part in the European campaign.\n\n"}
{"id": "54247835", "url": "https://en.wikipedia.org/wiki?curid=54247835", "title": "Biosolarization", "text": "Biosolarization\n\nBiosolarization is an alternative technology to soil fumigation used in agriculture. It is closely related to biofumigation and soil solarization, or the use of solar power to control nematodes, bacteria, fungi and other pests that damage crops. In solarization, the soil is mulched and covered with a tarp to trap solar radiation and heat the soil to a temperature that kills pests. Biosolarization adds the use of organic amendments or compost to the soil before it is covered with plastic, which speeds up the solarization process by decreasing the soil treatment time through increased microbial activity. Research conducted in Spain on the use of biosolarization in strawberry fruit production has shown it to be a sustainable and cost effective option. The practice of biosolarization is being used among small agricultural operations in California. Biosolarization is a growing practice in response to the need for methods for organic soil solarization. The option for more widespread use of biosolarization is being studied by researchers at the Western Center for Agricultural Health and Safety at the University of California at Davis in order to validate the effectiveness of biosolarization in commercial agriculture in California, where it has the potential to greatly reduce the use of conventional fumigants.\n"}
{"id": "1200509", "url": "https://en.wikipedia.org/wiki?curid=1200509", "title": "Burl", "text": "Burl\n\nA burl (American English) or bur or burr (UK English) is a tree growth in which the grain has grown in a deformed manner. It is commonly found in the form of a rounded outgrowth on a tree trunk or branch that is filled with small knots from dormant buds.\n\nA burl results from a tree undergoing some form of stress. It may be caused by an injury, virus or fungus. Most burls grow beneath the ground, attached to the roots as a type of malignancy that is generally not discovered until the tree dies or falls over. Such burls sometimes appear as groups of bulbous protrusions connected by a system of rope-like roots. Almost all burl wood is covered by bark, even if it is underground. Insect infestation and certain types of mold infestation are the most common causes of this condition.\n\nIn some tree species, burls can grow to great size. The largest, at , occur in coast redwoods (\"Sequoia sempervirens\") and can encircle the entire trunk; when moisture is present, these burls can grow new redwood trees. The world's second-largest burls can be found in Port McNeill, British Columbia. One of the largest burls known was found around 1984 in the small town of Tamworth, New South Wales. It stands tall, with an odd shape resembling a trombone. In January 2009, this burl was controversially removed from its original location, and relocated to a public school in the central New South Wales city of Dubbo.\n\nBurls yield a very peculiar and highly figured wood, prized for its beauty and rarity. It is sought after by furniture makers, artists, and wood sculptors. There are a number of well-known types of burls (each from a particular species); these are highly valued and sliced into veneers for furniture, inlay in doors, picture frames, household objects, automobile interior paneling and trim, musical instruments, and woodturning. The famous birdseye maple of the sugar maple (\"Acer saccharum\") superficially resembles the wood of a burl but is something else entirely. Burl wood is very hard to work with hand tools or on a lathe because its grain is twisted and interlocked, causing it to chip and shatter unpredictably. This \"wild grain\" makes burl wood extremely dense and resistant to splitting, which made it valued for bowls, mallets, mauls and \"beetles\" or \"beadles\" for hammering chisels and driving wooden pegs.\n\nBurls are harvested with saws or axes for smaller specimens and timber felling chainsaws and tractors for massive ones.\n\nBecause of the value of burls, ancient redwoods in National Parks in Western United States have recently been poached by thieves for their burls, including at Redwood National and State Parks. Poachers often cut off the burls from the sides of the trunks using chainsaws, which exposes the tree to infection and disease, or fell the entire tree to steal burls higher up. Because of risk of poaching, Jeff Denny, the state park’s redwood coast sector supervisor, encourages those buying burl to inquire where it came from and to ensure it was obtained legally. Legal acquisition methods for burl include trees from private land cleared for new development and from lumber companies with salvage permits.\n\nAmboyna burl is a particularly expensive type of burl, much more than bigleaf maple burl, for example. It comes from padauk trees (\"Pterocarpus\" spp.) of Southeast Asia. Padauk trees are quite common but burl wood is rare. The amboyna is usually a deep red, although the more rare moudui burl is the same species but the color is from golden yellow to yellow-orange. The sapwood is creamy white with brown streaks. The common use for amboyna is interiors for luxury vehicles, turnery, cabinets, veneer, and furniture.\n\n\n\n"}
{"id": "56572911", "url": "https://en.wikipedia.org/wiki?curid=56572911", "title": "Cañadón Calcáreo Formation", "text": "Cañadón Calcáreo Formation\n\nThe Cañadón Calcáreo Formation is a Oxfordian to Kimmeridgian-aged geologic formation, from the Cañadón Asfalto Basin in Chubut Province, Argentina, a rift basin that started forming since the earliest Jurassic. It was formerly thought to date into the Cretaceous, but the age has been revised with Uranium Lead dating as likely being solely Late Jurassic in age.\n\nIt is a subunit of the Loco Trapial Group, close to the city Cerro Condor in the Chubut Province of northwestern Patagonia, in southern Argentina. The formation is composed primarily of fluvial sandstones alongside shales and volcanic tuffites\n\nThe formation preserves fishes, crocodylomorphs and some dinosaur taxa, as well as conifers.\n\nThe Cañadón Calcáreo Formation is composed mainly of fluvial deposits, that are found close to the Cañadón Asfalto Formation. While originally thought to be part of the Cañadón Asfalto, there is a lack of calcareous rocks, and the geologic faulting and folding is weaker. The Cañadón Calcáreo Formation is overlain by the Chubut Group, the rocks of which lack synsedimentary deformation. The area of the Cañadón Calcáreo also includes lacustrine and palustrine rocks, including shales, pelites, psammites and coarse clastics.\n\nThe age of the Cañadón Calcáreo Formation was originally presumed to be Late Jurassic, probably Kimmeridgian to Tithonian in age. However, it has also been suggested to span the entire Late Jurassic, beginning in the Oxfordian, although this could also be because of the uncertainty of the youngest age Cañadón Asfalto Formation. A 2009 study of the palynology from a section of the formation revealed an age that was Hauterivian, which suggests that the formation extends from the Late Jurassic into the Early Cretaceous.\n\nAn unnamed brachiosaurid sauropod is also known from the Cañadón Calcáreo Formation. It is known from a single humerus.\n\n\n"}
{"id": "9504117", "url": "https://en.wikipedia.org/wiki?curid=9504117", "title": "Charge density wave", "text": "Charge density wave\n\nA charge density wave (CDW) is an ordered quantum fluid of electrons in a linear chain compound or layered crystal. The electrons within a CDW form a standing wave pattern and sometimes collectively carry an electric current. The electrons in such a CDW, like those in a superconductor, can flow through a linear chain compound en masse, in a highly correlated fashion. Unlike a superconductor, however, the electric CDW current often flows in a jerky fashion, much like water dripping from a faucet due to its electrostatic properties. In a CDW, the combined effects of pinning (due to impurities) and electrostatic interactions (due to the net electric charges of any CDW kinks) likely play critical roles in the CDW current's jerky behavior, as discussed in sections 4 & 5 below.\n\nMost CDW's in metallic crystals form due to the wave-like nature of electrons – a manifestation of quantum mechanical wave-particle duality – causing the electronic charge density to become spatially modulated, i.e., to form periodic \"bumps\" in charge. This standing wave affects each electronic wave function, and is created by combining electron states, or wavefunctions, of opposite momenta. The effect is somewhat analogous to the standing wave in a guitar string, which can be viewed as the combination of two interfering, traveling waves moving in opposite directions (see interference (wave propagation)).\n\nThe CDW in electronic charge is accompanied by a periodic distortion – essentially a superlattice – of the atomic lattice. The metallic crystals look like thin shiny ribbons (e.g., quasi-1-D NbSe crystals) or shiny flat sheets (e.g., quasi-2-D, 1T-TaS crystals). The CDW's existence was first predicted in the 1930s by Rudolf Peierls. He argued that a 1-D metal would be unstable to the formation of energy gaps at the Fermi wavevectors ±\"k\", which reduce the energies of the filled electronic states at ±\"k\" as compared to their original Fermi energy \"E\". The temperature below which such gaps form is known as the Peierls transition temperature, \"T\".\n\nThe electron spins are spatially modulated to form a standing spin wave in a spin density wave (SDW). A SDW can be viewed as two CDWs for the spin-up and spin-down subbands, whose charge modulations are 180° out-of-phase.\n\nIn 1954, Herbert Fröhlich proposed a microscopic theory, in which energy gaps at ±\"k\" would form below a transition temperature as a result of the interaction between the electrons and phonons of wavevector \"Q\"=2\"k\". Conduction at high temperatures is metallic in a quasi-1-D conductor, whose Fermi surface consists of fairly flat sheets perpendicular to the chain direction at ±\"k\". The electrons near the Fermi surface couple strongly with the phonons of 'nesting' wave number \"Q\" = 2\"k\". The 2\"k\" mode thus becomes softened as a result of the electron-phonon interaction. The 2\"k\" phonon mode frequency decreases with decreasing temperature, and finally goes to zero at the Peierls transition temperature. Since phonons are bosons, this mode becomes macroscopically occupied at lower temperatures, and is manifested by a static periodic lattice distortion. At the same time, an electronic CDW forms, and the Peierls gap opens up at ±\"k\". Below the Peierls transition temperature, a complete Peierls gap leads to thermally activated behavior in the conductivity due to normal uncondensed electrons.\n\nHowever, a CDW whose wavelength is incommensurate with the underlying atomic lattice, i.e., where the CDW wavelength is not an integer multiple of the lattice constant, would have no preferred position, or phase \"φ\", in its charge modulation \"ρ\" + \"ρ\"cos[2\"kx – φ\"]. Fröhlich thus proposed that the CDW could move and, moreover, that the Peierls gaps would be displaced in momentum space along with the entire Fermi sea, leading to an electric current proportional to \"dφ/dt\". However, as discussed in subsequent sections, even an incommensurate CDW cannot move freely, but is pinned by impurities. Moreover, interaction with normal carriers leads to dissipative transport, unlike a superconductor.\n\nSeveral quasi-2-D systems, including layered transition metal dichalcogenides, undergo Peierls transitions to form quasi-2-D CDWs. These result from multiple nesting wavevectors coupling different flat regions of the Fermi surface. The charge modulation can either form a honeycomb lattice with hexagonal symmetry or a checkerboard pattern. A concomitant periodic lattice displacement accompanies the CDW and has been directly observed in 1T-TaS using cryogenic electron microscopy. In 2012, evidence for competing, incipient CDW phases were reported for layered cuprate high-temperature superconductors such as YBCO.\n\nEarly studies of quasi-1-D conductors were motivated by a proposal, in 1964, that certain types of polymer chain compounds could exhibit superconductivity with a high critical temperature \"T\". The theory was based on the idea that pairing of electrons in the BCS theory of superconductivity could be mediated by interactions of conducting electrons in one chain with nonconducting electrons in some side chains. (By contrast, electron pairing is mediated by phonons, or vibrating ions, in the BCS theory of conventional superconductors.) Since light electrons, instead of heavy ions, would lead to the formation of Cooper pairs, their characteristic frequency and, hence, energy scale and \"T\" would be enhanced. Organic materials, such as TTF-TCNQ were measured and studied theoretically in the 1970s. These materials were found to undergo a metal-insulator, rather than superconducting, transition. It was eventually established that such experiments represented the first observations of the Peierls transition.\n\nThe first evidence for CDW transport in inorganic linear chain compounds, such as transition metal trichalcogenides, was reported in 1976 by Monceau et al., who observed enhanced electrical conduction at increased electric fields in NbSe. The nonlinear contribution to the electrical conductivity \"σ\" vs. field \"E\" was fit to a Landau-Zener tunneling characteristic ~ exp[-\"E\"/\"E\"] (see Landau–Zener formula), but it was soon realized that the characteristic Zener field \"E\" was far too small to represent Zener tunneling of normal electrons across the Peierls gap. Subsequent experiments showed a sharp threshold electric field, as well as peaks in the noise spectrum (narrow band noise) whose fundamental frequency scales with the CDW current. These and other experiments (e.g.,) confirm that the CDW collectively carries an electric current in a jerky fashion above the threshold field.\n\nLinear chain compounds exhibiting CDW transport have CDW wavelengths \"λ\" = \"π/k\" incommensurate with (i.e., not an integer multiple of) the lattice constant. In such materials, pinning is due to impurities that break the translational symmetry of the CDW with respect to \"φ\". The simplest model treats the pinning as a sine-Gordon potential of the form \"u\"(\"φ\") = \"u\"[1 – cos\"φ\"], while the electric field tilts the periodic pinning potential until the phase can slide over the barrier above the classical depinning field. Known as the overdamped oscillator model, since it also models the damped CDW response to oscillatory (ac) electric fields, this picture accounts for the scaling of the narrow-band noise with CDW current above threshold.\n\nHowever, since impurities are randomly distributed throughout the crystal, a more realistic picture must allow for variations in optimum CDW phase \"φ\" with position - essentially a modified sine-Gordon picture with a disordered washboard potential. This is done in the Fukuyama-Lee-Rice (FLR) model, in which the CDW minimizes its total energy by optimizing both the elastic strain energy due to spatial gradients in \"φ\" and the pinning energy. Two limits that emerge from FLR include weak pinning, typically from isoelectronic impurities, where the optimum phase is spread over many impurities and the depinning field scales as \"n\" (\"n\" being the impurity concentration) and strong pinning, where each impurity is strong enough to pin the CDW phase and the depinning field scales linearly with \"n\". Variations of this theme include numerical simulations that incorporate random distributions of impurities (random pinning model).\n\nEarly quantum models included a soliton pair creation model by Maki and a proposal by John Bardeen that condensed CDW electrons tunnel coherently through a tiny pinning gap, fixed at ±\"k\" unlike the Peierls gap. Maki's theory lacked a sharp threshold field and Bardeen only gave a phenomenological interpretation of the threshold field. However, a 1985 paper by Krive and Rozhavsky pointed out that nucleated solitons and antisolitons of charge ±\"q\" generate an internal electric field \"E*\" proportional to \"q/ε\". The electrostatic energy (1/2)\"ε\"[\"E\" ± \"E*\"] prevents soliton tunneling for applied fields \"E\" less than a threshold \"E\" = \"E*\"/2 without violating energy conservation. Although this Coulomb blockade threshold can be much smaller than the classical depinning field, it shows the same scaling with impurity concentration since the CDW's polarizability and dielectric response \"ε\" vary inversely with pinning strength.\n\nBuilding on this picture, as well as a 2000 article on time-correlated soliton tunneling, a more recent quantum model proposes Josephson-like coupling (see Josephson effect) between complex order parameters associated with nucleated droplets of charged soliton dislocations on many parallel chains. Following Richard Feynman in The Feynman Lectures on Physics, Vol. III, Ch. 21, their time-evolution is described using the Schrödinger equation as an emergent classical equation. The narrow-band noise and related phenomena result from the periodic buildup of electrostatic charging energy and thus do not depend on the detailed shape of the washboard pinning potential. Both a soliton pair-creation threshold and a higher classical depinning field emerge from the model, which views the CDW as a sticky quantum fluid or deformable quantum solid with dislocations, a concept discussed by Philip Warren Anderson.\n\nThe first evidence for phenomena related to the Aharonov–Bohm effect in CDWs was reported in a 1997 paper, which described experiments showing oscillations of period \"h\"/2\"e\" in CDW (not normal electron) conductance versus magnetic flux through columnar defects in NbSe. Later experiments, including some reported in 2012, show oscillations in CDW current versus magnetic flux, of dominant period \"h\"/2\"e\", through TaS rings up to 85 µm in circumference above 77 K. This behavior is similar to that of the superconducting quantum interference device (see SQUID), lending credence to the idea that CDW electron transport is fundamentally quantum in nature (see quantum mechanics).\n\n\n"}
{"id": "20265376", "url": "https://en.wikipedia.org/wiki?curid=20265376", "title": "Domestic energy assessor", "text": "Domestic energy assessor\n\nA Domestic Energy Assessor (DEA) is an accredited position in the UK that is approved by the Ministry for Housing, Communities and Local Government (MHCLG).\nUnder the UK's Energy Performance of Buildings Regulations, an Energy Performance Certificate is required for the sale or rent of a domestic dwelling. DEAs are qualified and accredited for the production of RdSAP EPCs - these are specifically for use on existing buildings, which would include an dwelling built after 2008 which had already received an on-construction SAP EPC.\nAs of November 2018, DEAs had produced 16,849,375 RdSAP EPCs since the inception of the industry.\n\n\"If you’re a DEA you must:\n\nDomestic energy assessors will be commissioned by householders or their agents to visit homes, initially those that are about to come onto the market. The owners will be obliged to make EPCs and advisory reports available to buyers, and later to tenants.\nIn this way, DEAs will be a key part of helping homebuyers, homeowners and tenants contribute to the “green agenda”. Research confirms that householders want to know about the energy efficiency of their homes, and about how they can make improvements.\n\nThese are defined in the National Occupational Standards (NOS). The NOS are a specification of both the technical knowledge and the skills required to be a competent DEA They include a requirement to understand the legal background to the role, to possess relevant interpersonal skills and make accurate judgments consistently, aided by the software, on the recommendations for cost effective improvement measures. They may be downloaded free of charge from the Asset Skills Energy Assessors website.\n\nDEA qualifications will be provided by three awarding bodies–the Awarding Body for the Built Environment (ABBE), City and Guilds and the National Association of Estate Agents (NAEA). Each will include a multiple choice examination and detailed assessment of not less than five EPCs.\nMany providers in both the public (usually Further Education Colleges) and private sector are offering or planning to offer training to become a DEA. Professional bodies and private providers will also do the same, whilst larger companies will almost certainly\nprovide the learning in-house. Asset Skills, awarding bodies and professional associations can all give advice about how to locate\ntraining relevant to your needs and where you live.\nThe amount of training you require will depend on\nyour current level of skills and knowledge–obviously\nmuch less for an experienced practitioner than\nsomeone entering the world of Energy Assessment\nand buildings for the first time.\n\nSome funding support may be available in certain circumstances–approach a local or regional Learning and Skills Council (LSC) or in Wales, the Welsh Assembly Government.\n\nIn order to practise, DEAs need to join an accreditation body. This will entail a check that the individual is fit and proper and a commitment to maintaining appropriate\nProfessional Indemnity, updating your skills and knowledge regularly (CPD), participating in the\naccrediting body’s quality assurance scheme, and abiding by their advice and guidance.\n\nDomestic Energy Assessors are able to join any of the following accreditation schemes to be able to practice. Multiple registrations are common within the Industry.\n"}
{"id": "11075027", "url": "https://en.wikipedia.org/wiki?curid=11075027", "title": "Eastern Caribbean Gas Pipeline", "text": "Eastern Caribbean Gas Pipeline\n\nThe Eastern Caribbean Gas Pipeline is a proposed natural gas pipeline from Tobago to other eastern Caribbean islands.\n\nThe idea had its genesis with Patrick Manning, Prime Minister of Trinidad and Tobago, who announced in 2002 that his country was going to undertake one of the largest civil engineering projects in the Caribbean region.\n\nThe project suffered a brief setback when Venezuelan leader Hugo Chavez petitioned intensely that the pipeline should instead originate from Venezuela and reach further into the north, including Cuba and the United States.\n\nIn March 2010 the Barbados indicated after a two-year hiatus that it would seek to move toward the negotiations stage for the first stage of the pipeline from Tobago to Barbados.\n\nThe overall pipeline would be long, including shore approaches and the lateral line to Barbados. The first long stage would start from the Cove Point Estate in Tobago and run to Barbados. The second stage it would be expanded to Saint Lucia, Dominica, Martinique, and Guadeloupe.\n\nThe project is developed by the Eastern Caribbean Gas Pipeline Company Limited (ECGPC). 60% of the company is jointly owned by \nthe United States companies Beowulf Energy LLC and First Reserve Energy International Fund. Rest is owned by the Trinidad and Tobago companies Guardian Holdings (15%), Unit Trust Corporation (15%), and the National Gas Company of Trinidad and Tobago (10%).\n\n\n"}
{"id": "48566230", "url": "https://en.wikipedia.org/wiki?curid=48566230", "title": "Energy in Nepal", "text": "Energy in Nepal\n\nNepal had a total primary energy supply (TPES) of 10.29 Mtoe in 2012. Electricity consumption was 3.57 TWh. \nMost of this primary energy (about 80%) represents solid biofuels used in the residential sector (for heating, cooking etc.).\nAbout 23% of the electricity is imported, with the rest almost completely supplied by hydroelectricity.\n\nNepal has no known major oil, gas, or coal reserves, and its position in the Himalayas makes it hard to reach remote communities. Consequently, most Nepali citizens have historically met their energy needs with biomass, human labor, imported kerosene, and/or traditional vertical axis water mills. Energy consumption per capita is thus low, at one-third the average for Asia as a whole and less than one-fifth of the world average. The country has considerable hydroelectricity potential. The commercially viable potential is estimated at about 44 GW from 66 hydropower sites.\nIn 2010, the electrification rate was only 53% (leaving 12.5 million people without electricity) and 76% depended on wood for cooking. \n\nWith about 1 toe for every $1,000 of GDP, Nepal has the poorest energy intensity among all south Asian countries. The country has therefore very large energy efficiency potential.\n\nPetroleum is the second largest energy fuel in Nepal after firewood and accounts for 11% of primary energy consumption in the country. All petroleum products are imported from India. \nAt the moment, the import of petroleum products is transacted exclusively between the Nepal Oil Corporation and the Indian Oil Corporation. 75% of the imports are diesel, kerosene and gasoline. Due to the high energy demand in the country, the dependence on petroleum imports is increasing. \nMore than 62% of the petroleum products are used in the transportation sector. Besides that, petroleum products constitute important energy sources for cooking purposes in households.\n\nBiomass is by far the most important primary energy source in Nepal. Biomass comprises wood, agricultural residues and dung. About 95% of the biomass is used for cooking and heating purposes in households.\n\nFarming system in Nepal is heavily dependent on livestock, with at least 1.2 million households owning cattle and buffalo. The technical biogas potential is therefore high and is estimated to be at least one million household-size plants, 57 percent located in the Terai plains, 37 percent in the hills and 6 percent in remote hills.\n\nAccording to Nepal's \"Alternative Energy Promotion Centre\", as of July 2011, 241,920 biogas plants were installed in more than 2,800 Village Development Committees and in all 75 Districts under their Biogas Support Program.\n\n"}
{"id": "45252075", "url": "https://en.wikipedia.org/wiki?curid=45252075", "title": "Exmouth Plateau", "text": "Exmouth Plateau\n\nThe Exmouth Plateau is an elongate northeast striking extensional passive margin located in the Indian Ocean roughly 3,000 meters offshore from western and northwestern Western Australia.\n\nThe plateau makes up the westernmost structural unit of the Northern Carnarvon Basin, which comprises the Exmouth, Barrow, Dampier, and Beagle Sub-basins, and the Rankin Platform. The Exmouth Plateau was once a part of the northern shore of eastern of Gondwanaland until it broke away during Late Jurassic to Early Cretaceous, leaving behind the oceanic crust of the Argo, Cuvier, and Gascoyne abyssal plains that now surround the distal margins of Exmouth Plateau.\n\nIn the Late Jurassic, Gondwanaland begins to break apart creating Western Gondwana, which was composed of the South American and African continental land masses, and Eastern Gondwana. The Eastern Gondwanian continent was made Madagascar, Greater India, Antarctica, and Australia. During this period of time Australia shared its southern margin with Antarctica and the western margin (now the Exmouth Plateau) with Greater India. The formation of the Exmouth's northern margin, the Argo Abyssal Plain, was not initiated until 155 million years ago when Australia broke apart from a continental fragment of the Burma plate that's present location is argued to be subsumed under Asia. It wasn't until 20 million years later that the Greater Indian land mass broke from western Australia, forming the central and southern margins of the Exmouth Plateau now known as the Gascoyne and Cuvier Abyssal Plains. As Australia continued to diverge away from the Antarctic land mass, it migrated in a northeastern direction and rotated counterclockwise to it present location, leaving the Exmouth Plateau along the continent's western margin.\n\nIn the begin of the late Triassic, high volumes of sediments accumulate off of the shoreline of western Australia to the northern extend of the Exmouth Plateau by the Mungaroo Deltas. The Carnian (237-228 Ma) to Norian (228-209 Ma) aged fluviodeltaic sediments deposited were siliciclastic claystones and sandstones, and detritus which would late make up the coals found in Mungaroo Formation. As extensional rifting between Greater Indian and the Australian continued, magmatic intrusion along the westernmost section of the Exmouth Plateau caused further rifting to the outer margins. By the end of the Late Triassic (209-201 Ma) tectonic activity had relatively slowed down and less deltaic sediments were deposited compared to the Carnian and Norian. More marine sedimentary deposit such as carbonates are found during this time period.\n\nDuring the early Jurassic, extension at the west Australian margin initiated simple shear mechanics creating a system of listric normal faults near the eastern region of the Exmouth Plateau. These listric faults were a product from the development of a major low angle detachment fault between a sedimentary base of Permian-Triassic upper crust and the mid-crustal horizon. As a result, brittle deformation and crustal thinning in the upper crust of the eastern Exmouth Plateau occurred, while in the west, the lower crystalline crust and lithosphere experienced shear stress and thinning. The formation of the fault system closer to the coast caused the initial development of the Exmouth, Barrow, and Dampier sub-basins of the Northern Carnarvon Basin. Carbonate marine sediment, primarily marls, continued to be deposited at this time on the central and western portions of the plateau. Closer to the shoreline siliciclastic mud and silt were deposited from marine and deltaic environments.\n\nAs extension continued in the mid to late Jurassic, multiple pull-apart basins and oblique right-lateral strike-slip faulting in the eastern margin of the Exmouth Plateau continued to dominate. The simple shear stress of the Exmouth detachment fault between the base of the upper crust, and the lower crust had been reduced in the east plateau. This reduction in simple shear was in part caused by the deformation that developed series negative flower structures and half-graben systems in the west plateau. By this time, the lithospheric thinning that had been initiated in during the early Jurassic was now considerably thinner. At this period, a magmatic intrusion between the lower crystalline crest and the lithosphere been introduce, underplating this region.\n\nIn Early Cretaceous, pure shear deformation at the ocean-continental boundary completed the final continental breakup and sea-floor spreading. By this time the overall structural morphology of the Exmouth Plateau had taken shape, aside from the post-breakup subsidence that occurred afterward from the Late Cretaceous to present day. Activity at the eastern Exmouth Plateau's detachment system had likely ceased which is reason for the completion of the plateau's morphology.\n\nThe Locker Shale and Mungaroo formation are associated with the syn-rift and post-rift sequences of the Permian extension that occurred just prior to the late Triassic extension event across the northwestern margin of Australia. Lying atop unconformable unit of Paleozoic sediments, the Locker Shale is composed of marine sediments deposited in a transgressional environment during the Late Permian extension. These marine sediments were deposited at the base of the Late Triassic and continued up until the end of Anisian age when it transitioned in the Mungaroo Formation. The Mungaroo Formation is an intricate fluvial environment of meandering and braided stream deposits laid down in the Ladinian (~241 Mya) to Norian ages (~209 Mya) during the pre-rift active margin of the Northwestern Australian Shelf. It is one of the thickest formations in the region and becomes progressively thicker (approximately 3,000 m thick) in the distal portions of the Northern Carnarvon Basin furthest offshore. One of the primary gas source for the Exmouth Plateau and Northern Carnarvon Basin, the Mungaroo Formation contains thick successions of siltstone, sandstone, and coal.\n\nThe Mungaroo Formation is capped by thin transgressive sequence of shallow marine claystone and limestone called the Brigadier Formation. This is a much thinner formation than the Mungaroo laid down only for a short period of time between the beginning of the Rhaetian to the Early Hettangian. Do to the thickness of fluvial deltaic deposits from the Mungaroo Formation, the transgressional sequence of the Brigadier Formation is correlated with the subsidence of sub-basins across region during the Middle and Late Triassic.\n\nThe distribution of sediment packages throughout the Northern Carnarvon Basin and Exmouth Plateau varies with location during the Jurassic. By the Pliensbachian, the general beginning structure of the Northern Carnarvon Basin was formed, creating the Exmouth, Barrow, and Dampier sub-basins along the proximal end of the northwestern Australian margin. Rapid subsidence of the Northern Carnarvon sub-basins ensued the deposition of the Dingo Claystones. These Claystones are thick marine shale deposits separated into two sequence, the Upper and Lower Dingo Claystones. The Lower Dingo sequence cannot be found in the Exmouth Plateau, because deposition During the Early to Middle Jurassic was restricted to sediments filling in the empty troughs of the Northern Carnarvon sub-basins. As deposition continued through the Middle and Late Jurassic, the troughs of sub-basins filled allowing more of the Dingo sediment overflow into the distal margins of the region. This depositional sequence characterizes the Upper Dingo Claystones, which are found as thick proximal sequences in the Northern Carnarvon sub-basins and thinner distal sequences in the Exmouth Plateau.\nDuring the late Tithonian to early Valanginian there were major changes in depositional patterns, specifically associated with the Barrow Group. At this period of time there was large uplift of the Cape Range Fracture Zone that provided sediments from the Barrow Delta prograde northward past Barrow Island and across the Exmouth Plateau. The sediments of the Barrow Group are composed of interbedded shale and fluvial-deltiac sands that were extensively deposited into Exmouth and Barrow sub-basins.\n\nEventually, deposition of the Barrow Delta's sediment ceased during the early or middle Valanginian with due the continental break up at the southern margin of the Exmouth Plateau. This was followed seafloor spreading at the Gascoyne and Cuvier Abyssal Plains and erosion at the surface of the Exmouth, Dampier and Barrow sub-basins and an unconformity to be position at the top of the early to middle Valanginian. By the middle to late Valanginian and through the Cretaceous major transgression occurred with deposit the Winning Group atop the Valanginian unconformity. The Winning Group consists of Muderong Shale, Windalia Radiolarite, and Gearle Siltsone. The Muderong Shale actually considered a siltstone and was deposited in a low energy, offshore-marine environment. The deposition of these sediments makes good seal for gas buildups in the Exmouth Plateau and the surrounding sub-basins of the Northern Carnvon Basin. The Windalia Radiolarite are primarily carbonate marine deposits commonly containing poorly preserved radiolarians and forminiferas. The Gearle Siltstone is a low-energy, offshore-marine deposit in a period of that input of the terrigenous sediments to the offshore was low minimal.\n"}
{"id": "11582749", "url": "https://en.wikipedia.org/wiki?curid=11582749", "title": "Free convective layer", "text": "Free convective layer\n\nIn atmospheric sciences, the free convective layer (FCL) is the layer of conditional or potential instability in the troposphere. It is a layer of positive buoyancy (PBE) and is the layer where deep, moist convection (DMC) can occur. On an atmospheric sounding, it is the layer between the level of free convection (LFC) and the equilibrium level (EL). The FCL is important to a variety of convective processes and to severe thunderstorm forecasting.\n\nIt is the layer of instability, the \"positive area\" on thermodynamic diagrams where an ascending air parcel is warmer than its environment. Integrating buoyant energy from the LFC to the EL gives the convective available potential energy (CAPE), an estimate of the maximum energy available to convection. The depth of the FCL is expressed by the formula:\n\nor \n\nDeep, moist convection is essentially a thunderstorm, it is cumulus congestus clouds or cumulonimbus clouds. An air parcel ascending from the near surface layer (mixed layer or boundary layer) must work through the stable layer of convective inhibition (CIN) when present. This work comes from increasing instability in the low levels by raising the temperature or dew point, or by mechanical lift. Without the aid of mechanical forcing, a parcel must reach its convective temperature (T) before moist convection (cloud) begins near the convective condensation level (CCL}, whereas with dynamic lift, cloud base begins near the lifted condensation level (LCL). This will remain as shallow, moist convection (small cumulus clouds) until breaking through the convective inhibition layer, after which DMC ensues as a parcel hits the LFC and enters the FCL, if thermal or mechanical forcing continues. At the level of neutral buoyancy (the EL), a parcel is cooler than the environment and is stable, so it slow down, eventually ceasing at the maximum parcel level (MPL). \n\n\n\n"}
{"id": "4370125", "url": "https://en.wikipedia.org/wiki?curid=4370125", "title": "Grand unification epoch", "text": "Grand unification epoch\n\nIn physical cosmology, assuming that nature is described by a Grand Unified Theory, the grand unification epoch was the period in the evolution of the early universe following the Planck epoch, starting at about 10 seconds after the Big Bang, in which the temperature of the universe was comparable to the characteristic temperatures of grand unified theories. If the grand unification energy is taken to be 10 GeV, this corresponds to temperatures higher than 10 K. During this period, three of the four fundamental interactions—electromagnetism, the strong interaction, and the weak interaction—were unified as the electronuclear force. Gravity had separated from the electronuclear force at the end of the Planck era. During the grand unification epoch, physical characteristics such as mass, charge, flavour and colour charge were meaningless.\n\nThe grand unification epoch ended at approximately 10 seconds after the Big Bang. At this point several key events took place. The strong force separated from the other fundamental forces. \nIt is possible that some part of this decay process violated the conservation of baryon number and gave rise to a small excess of matter over antimatter (see baryogenesis). This phase transition is also thought to have triggered the process of cosmic inflation that dominated the development of the universe during the following inflationary epoch.\n\n"}
{"id": "32524961", "url": "https://en.wikipedia.org/wiki?curid=32524961", "title": "Hap Wilson", "text": "Hap Wilson\n\nDavid \"Hap\" Wilson is a Canadian naturalist, canoe tripper, author, illustrator and photographer. He has published numerous Canadian canoe route guides and books about wilderness life.\n\n\n"}
{"id": "54673286", "url": "https://en.wikipedia.org/wiki?curid=54673286", "title": "Harmankaya Canyon Nature Park", "text": "Harmankaya Canyon Nature Park\n\nHarmankaya Canyon Nature Park () is a canyon in Bilecik Province, western Turkey, which was declared a nature park.\n\nThe canyon is located between Karahasanlar village in Yenipazar district and Harmanköy village in İnhisar of Bilecik. Its distance to Karahasanlar village is and to Harmanköy about . The canyon's northern entrance is at an elevation of , the southern entrance is at while the elevation in the canyon mid measures about . It is about long, and its entrance is around wide. The canyon cliffs are high. There are two waterfalls of about height, and many others of height. The area covering was declared as the country's 184th nature park by the Ministry of Environment and Forest on December 26, 2012.\n\nThe vegetation of the nature park consists of low trees and shrubs like oak (\"Quercus\"), juniper (\"Juniperus\"), rowan (\"Sorbus\"), terebinth (\"Pistacia terebinthus\"), willow (\"Salix,\") and blackthorn (\"Prunus spinosa\").\n"}
{"id": "5500901", "url": "https://en.wikipedia.org/wiki?curid=5500901", "title": "Indus-Yarlung suture zone", "text": "Indus-Yarlung suture zone\n\nThe Indus-Yarlung suture zone or the Indus-Yarlung Tsangpo suture is a tectonic suture in southern Tibet and across the north margin of the Himalayas which resulted from the collision between the Indian plate and the Eurasian plate starting about 52 Ma. The north side of the suture zone is the Ladakh Batholith of the Karakoram-Lhasa Block. The rocks of the suture zone consist of an ophiolite mélanges composed of Neotethys oceanic crustal flyschs and ophiolites; the Dras Volcanics: which are basalts, dacites and minor radiolarian cherts - the remains of a mid to late Mesozoic volcanic island arc; and the Indus Molasse which are an Eocene or later continental clastic sediments.\n\nThe Ophiolith which can be found here is not a remnant of a very big ocean, but of a small so called Back-arc basin structure.\n"}
{"id": "261352", "url": "https://en.wikipedia.org/wiki?curid=261352", "title": "Joint European Torus", "text": "Joint European Torus\n\nJET, the Joint European Torus, is the world's largest operational magnetically confined plasma physics experiment, located at Culham Centre for Fusion Energy in Oxfordshire, UK. Based on a tokamak design, the fusion research facility is a joint European project with a main purpose of opening the way to future nuclear fusion grid energy.\n\nJET was one of a number of tokamak reactors built in the early 1980s that tested new design concepts. It was one of only two designed to work with a real deuterium-tritium fuel mix, the other being the US-built TFTR. Both were built with the ultimate goal of reaching \"scientific breakeven\", the point where the energy created by the fusion reactions is greater than the energy being fed into it to keep it hot.\n\nJET began operation in 1983 and spent most of the next decade increasing its performance in a lengthy series of experiments and upgrades. In 1991 the first experiments including tritium were made, making JET the first reactor in the world to run on the production fuel of a 50-50 mix of tritium and deuterium. In 1997, using this fuel, JET set the current world record for fusion output at 16 MW from an input of 24 MW of heating and a total input of 700-800 MW of electrical power. This is also the world record for \"Q\", at 0.67. A \"Q\" of 1 is scientific breakeven, a point JET was not ultimately able to reach.\n\nJET was built by an international consortium, which formed the nucleus for the European Union's contribution to the International Thermonuclear Experimental Reactor. JET has been described as a \"little ITER\" as the designs are very similar in many ways. In recent years, JET has been used to test a number of features from the ITER design.\n\nBy the early 1960s, the fusion research community was in the \"doldrums\". Many initially promising experimental paths had all failed to produce useful results, and the latest experiments suggested performance was stalled out at the Bohm diffusion limit, far below what would be needed for a practical fusion generator.\n\nIn 1968 the Soviets held the periodic meeting of fusion researchers in Novosibirsk, where they introduced data from their T-3 tokamak. This represented a dramatic leap in fusion performance, at least 10 times what the best machines in the world had produced to that point. The results were so good that some dismissed them as faulty measurements. To counter this, the Soviets invited a team from the UK to independently test their machine. Their 1969 report confirmed the Soviet results, resulting in a \"veritable stampede\" of tokamak construction around the world.\n\nA key issue in tokamak designs was that they did not generate enough of an electrical current in their plasma to provide enough heating to bring the fuel to fusion conditions. Some sort of external heating would be required. There was no shortage of ideas for this, and in the mid-1970s a series of machines were built around the world to explore these concepts. One of these, the Princeton Large Torus (PLT) demonstrated that neutral beam injection was a workable concept, using it to reach record temperatures well over the 50 million K that is the minimum needed for a practical reactor.\n\nWith the PLTs success, the path to breakeven finally appeared possible after decades of effort. Breakeven is the point where the energy being given off by the fusion reactions is equal to the amount of energy used to run the reactor. Once breakeven is achieved, even small improvements from that point begin to rapidly increase the amount of net energy being released. Teams around the world began planning for a new generation of machines combining PLTs injectors with superconducting magnets and vacuum vessels that could hold deuterium-tritium fuel instead of the test fuels containing pure deuterium or hydrogen that had been used up to that point.\n\nIn 1971, the Council of the European Community decided in favour of a robust fusion programme and provided the necessary legal framework for a European fusion device to be developed. In 1975, the first proposals for the JET machine were completed. Detailed design took three years. At the end of 1977, after a long debate, Culham was chosen as the host site for the new design. Funding was approved on 1 April 1978 as the \"JET Joint Undertaking\" legal entity.\n\nThe reactor was built at a new site next to the Culham Centre for Fusion Energy, the UK's fusion research laboratory which opened in 1965. The construction of the buildings was undertaken by Tarmac Construction, starting in 1978 with the Torus Hall. The Hall was completed in January 1982 and construction of the JET machine itself began immediately after the completion of the Torus Hall. The cost was 198.8 Million European Units of Account (predecessor to the Euro) or 438 million in 2014 US dollars. \n\nJET achieved its first plasma on 25 June 1983. It was officially opened on 9 April 1984 by Queen Elizabeth II. On 9 November 1991, JET performed the world's first deuterium-tritium experiment. This beat the US's machine, TFTR, by a full two years.\n\nAlthough very successful, JET and its counterpart TFTR failed to reach breakeven. This was due to a variety of effects that had not been seen in previous machines operating at lower densities and pressures. Based on these results, and a number of advances in plasma shaping and diverter design, a new tokamak layout emerged, sometimes known as an \"advanced tokamak\". An advanced tokamak capable of reaching breakeven would have to be very large and very expensive, which led to the international effort ITER.\n\nIn the early 1990s, it was decided to add such a diverter design to JET, which occurred between 1991 and 1993. Performance was significantly improved, allowing JET to set many records in terms of confinement time, temperature and fusion triple product. It set the record for the closest approach to breakeven, reaching \"Q\" = 0.67 in 1997, producing 16 MW of fusion energy while consuming 24 MW to heat the fuel. This is also the record for greatest fusion power produced.\n\nIn 1998 JET's engineers developed a remote handling system with which, for the first time, it was possible to exchange certain components using artificial hands only. A \"Remote Handling\" system is, in general, an essential tool for any subsequent fusion power plant and especially for the International Thermonuclear Experimental Reactor (ITER). This Remote Handling system was later to lead on to become RACE (Remote Applications in Challenging Environments).\n\nIn 1999, the European Fusion Development Agreement (EFDA) was established with responsibility for the future collective use of JET.\n\nJET operated throughout 2003, with the year culminating in experiments using small amounts of tritium.\n\nIn October 2009, a 15-month shutdown period was started to rebuild many parts of the JET to test concepts from the ITER design. This including replacing carbon components in the vacuum vessel with tungsten and beryllium ones. Heating power was also increased by 50%, bringing the neutral beam power available to the plasma up to 34MW, and diagnostic and control capabilities were improved. In total, over 86,000 components were changed in the torus during the shutdown.\n\nIn mid-May 2011, the shutdown reached its end. The first experimental campaign after the installation of the “ITER-Like Wall” started on 2 September 2011.\n\nOn 14 July 2014, the European Commission signed a contract worth €283m for another 5-year extension so more advanced higher energy research can be performed at JET. Under this plan, a new run of deuterium-tritium experiments, a “dress rehearsal” for ITER, would be carried out in 2019. \n\nBrexit has thrown the plans for JET in doubt. As part of their plan to leave the EU, the UK will also leave Euratom, which provides the funding for JET. Talks on the funding after 2018, when the current 5-year plan expires, were underway and a new agreement to extend JET's operation until 2019 or 2020 appeared to be largely complete. These talks were put on hold after the Brexit announcement, and are currently stalled pending the Brexit outcome.\n\nJET has a major radius of 3 meters, and the D-shaped vacuum chamber is 2.5 meters wide and 4.2 meters high. The total plasma volume within it is 100 cubic meters, about 100 times larger than the largest machine in production when the JET design began.\n\nJET was one of the first tokamaks to be designed to use a D-shaped vacuum chamber. This was initially considered as a way to improve the safety factor, but during the design, it was also noticed that this would make it much easier to build the system mechanically, as it reduced the net forces across the chamber that are trying to force the torus towards the center of the major axis. Ideally, the magnets surrounding the chamber should be more curved at the top and bottom and less on the inside and outsides in order to support these forces, which leads to something like an oval shape that the D closely approximated. The flatter shape on the inside edge was also easier to support due to the larger, flatter surface.\n\nWhile exploring the stability of various plasma shapes on a computer, the team noticed that non-circular plasmas did not exactly cancel out the vertical drift that the twisted fields have originally been introduced to solve. If the plasma was displaced up or down, it would continue travelling in that direction. However, the simulations demonstrated that the drift rate was slow enough that it could be counteracted using additional magnets and an electronic feedback system.\n\nThe primary magnetic field in a tokamak is supplied by a series of magnets ringing the vacuum chamber. In JET, these are a series of 32 copper-wound magnets, each one weighing 12 tonnes. In total, they carry a current of 51 MA, and as they had to do so for periods of tens of seconds, they are water cooled. When operating, the coil is attempting to expand with a force of 6 MN, there is a net field towards the center of the major axis of 20 MN, and a further twisting force because the poloidal field inside the plasma is in different directions on the top and bottom. All of these forces are borne on the external structure.\n\nSurrounding the entire assembly is the 2,600 tonne eight-limbed transformer which is used to induce a current into the plasma. The primary purpose of this current is to generate a poloidal field that mixes with the one supplied by the toroidal magnets to produce the twisted field inside the plasma. The current also serves the secondary purpose of ionizing the fuel and providing some of the heating while the plasma before other systems take over.\n\nThe main source of heating in JET is provided by two systems, neutral beam injection and ion cyclotron resonance heating. The former uses small particle accelerators to shoot fuel atoms into the plasma, where collisions cause the atoms to ionize and become trapped with the rest of the fuel. These collisions deposit the kinetic energy of the accelerators into the plasma. Ion cyclotron resonance heating is essentially the plasma equivalent of a microwave oven, using radio waves to pump energy into the ions directly by matching their cyclotron frequency. JET was designed so it would initially be built with a few megawatts of both sources, and then later be expanded to as much of 25 MW of neutral beams and 15 MW of cyclotron heating.\n\nJET's power requirements during the plasma pulse are around 500 MW with peak in excess of 1000 MW. Because power draw from the main grid is limited to 575 MW, two large flywheel generators were constructed to provide this necessary power. Each 775-ton flywheel can spin up to 225 rpm and store 3.75 GJ. Each flywheel uses 8.8 MW to spin up and can generate 400 MW (briefly).\n\n\n\n\n"}
{"id": "16759956", "url": "https://en.wikipedia.org/wiki?curid=16759956", "title": "Karlıova Triple Junction", "text": "Karlıova Triple Junction\n\nThe Karlıova Triple Junction is a geologic triple junction of three tectonic plates: the Anatolian Plate, the Eurasian Plate and the Arabian Plate.\n\n"}
{"id": "444937", "url": "https://en.wikipedia.org/wiki?curid=444937", "title": "Lincoln Near-Earth Asteroid Research", "text": "Lincoln Near-Earth Asteroid Research\n\nThe Lincoln Near-Earth Asteroid Research (LINEAR) project is a collaboration of the United States Air Force, NASA, and the MIT's Lincoln Laboratory for the systematic detection and tracking of near-Earth objects. LINEAR was responsible for the majority of asteroid discoveries from 1998 until it was overtaken by the Catalina Sky Survey in 2005. , LINEAR had detected 231,082 new small Solar System bodies, of which at least 2,423 were near-Earth asteroids and 279 were comets. The instruments used by the LINEAR program are located at Lincoln Laboratory's Experimental Test Site (ETS) on the White Sands Missile Range (WSMR) near Socorro, New Mexico.\n\nIn the late 1970s, the new \"Lincoln Laboratory's Experimental Test Site\" facility (Lincoln Lab ETS, observatory code 704) was built at White Sands Missile Range. The project's prototype used low-light video cameras. In 1994 a new proposal was made for automated detection of asteroids, this time using newer digital detector technology. The LINEAR project began operating a near-Earth object discovery facility in 1996 using a aperture telescope designed for the Air Force Space Command's Ground-based Electro-Optical Deep Space Surveillance (GEODSS). The wide-field Air Force telescopes were designed for optical observation of Earth-orbiting spacecraft. Initial field tests used a 1024 × 1024 pixel charge-coupled device (CCD) detector. While this CCD detector filled only about one fifth of the telescope's field of view, four near-earth objects were discovered. A 1960 × 2560 pixel CCD which covered the telescope's two-square degree field of view was then installed, and both detectors were used in later tests.\n\nThe first LINEAR telescope became fully operational in March 1998. Beginning in October 1999, a second 1.0 m telescope was added to the search effort. In 2002, a telescope equipped with the original CCD was brought on-line to provide follow-up observations for the discoveries made by the two search telescopes. This allowed about 20% more of the sky to be searched each night. Data recorded by the telescopes is sent to a Lincoln Laboratory facility at Hanscom Air Force Base in Lexington, Massachusetts for processing. Detections are then forwarded to the Minor Planet Center.\n\nIn addition to discovering more than 140,000 minor planets, LINEAR is also credited with the discovery, or co-discovery, or rediscovery of several periodic comets, including 11P/Tempel–Swift–LINEAR, 158P/Kowal-LINEAR, 160P/LINEAR (LINEAR 43), 165P/LINEAR (LINEAR 10), and 176P/LINEAR (LINEAR 52, 118401 LINEAR: one of only five objects classified both as comets and asteroids). Other objects discovered include , , and 2004 FH.\n\n\n"}
{"id": "5893140", "url": "https://en.wikipedia.org/wiki?curid=5893140", "title": "List of Sites of Special Scientific Interest in Suffolk", "text": "List of Sites of Special Scientific Interest in Suffolk\n\nSuffolk is a county in East Anglia. It is bounded by Norfolk to the north, Cambridgeshire to the west, Essex to the south and the North Sea to the east. With an area of , it is the eighth largest county in England, and in mid-2016 the population was 745,000. At the top level of local government is Suffolk County Council, and below it are seven borough and district councils: Babergh, Forest Heath, Ipswich, Mid Suffolk, St Edmundsbury, Suffolk Coastal and Waveney. Much of the coast consists of the estuaries of the Orwell, Stour, Alde, Deben and Blyth rivers, with large areas of wetlands and marshes. Agriculture and shipping play a major role in the county's economy.\n\nIn England, Sites of Special Scientific Interest (SSSIs) are designated by Natural England, a non-departmental public body which is responsible for protecting England's natural environment. Designation as an SSSI gives legal protection to the most important wildlife and geological sites. As of October 2017 there are 142 SSSIs in Suffolk, of which 109 are biological, 28 geological and 5 are designated under both criteria.\n\nOne site is in the Dedham Vale, an Area of Outstanding Natural Beauty (AONB), and thirty-six are in another AONB, Suffolk Coast and Heaths. There are thirty-three Geological Conservation Review sites, twenty-three Nature Conservation Review sites, twenty Special Areas of Conservation, thirty Special Protection Areas under the European Union Directive on the Conservation of Wild Birds, eight Ramsar internationally important wetland sites, seven National Nature Reserves and 4 contain Scheduled Monuments. Six sites are Local Nature Reserves, twenty-seven are managed by the Suffolk Wildlife Trust, five by the Royal Society for the Protection of Birds and one by the National Trust. The largest is Breckland Forest at , which is partly in Norfolk and has several invertebrates on the IUCN Red List of Threatened Species, and the smallest is a meadow in London Road Industrial Estate, Brandon, which has the largest known wild population in Britain of the nationally rare sunflower \"Artemisia campestris\".\n\n\n"}
{"id": "5691247", "url": "https://en.wikipedia.org/wiki?curid=5691247", "title": "List of oldest stars", "text": "List of oldest stars\n\nThe age of the oldest known stars approaches the age of the universe, about 13.8 billion years. These are recognized as among the oldest:\nSome of these are among the first stars from reionization (the stellar dawn), ending the Dark Ages (cosmology) about 370,000 years after Big Bang\n"}
{"id": "7648907", "url": "https://en.wikipedia.org/wiki?curid=7648907", "title": "List of plants of the Amazon rainforest of Brazil", "text": "List of plants of the Amazon rainforest of Brazil\n\nThis is a list of plants found in the wild in Amazon Rainforest vegetation of Brazil. The estimates from useful plants suggested that there are 800 plant species of economic or social value in this forest, \naccording to Giacometti (1990). Additions are currently being made to this list.\nFlora definition\nFlora is the scientific word for plant.\n\nPlant unknown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "36309503", "url": "https://en.wikipedia.org/wiki?curid=36309503", "title": "Maritime domain awareness", "text": "Maritime domain awareness\n\nMaritime domain awareness (MDA) is defined by the International Maritime Organization as the effective understanding of anything associated with the maritime domain that could impact the security, safety, economy, or environment. \nThe maritime domain is defined as all areas and things of, on, under, relating to, adjacent to, or bordering on a sea, ocean, or other navigable waterway, including all maritime-related activities, infrastructure, people, cargo, and vessels and other conveyances.\n\nIn the United States, the Secretary of the Navy is the DoD Executive Agent for maritime domain awareness.\n\nFor private ports, The Mariner Group's CommandBridge platform is the market leader in Maritime Domain Awareness Systems. \n\nIn Canada, the 2004 National Security Policy resulted in the establishment of Marine Security Operations Centers (MSOCs) responsible for supporting a national response to maritime security threat. The East Coast MSOC is in Halifax, the West Coast MSOC is in Victoria, and the Great Lakes and St. Lawrence Seaway MSOC is in Niagara.\n\nThe European Union took a decision in 2008 to improve the integration and interoperability of member states' maritime safety, security, border control, environmental protection, fisheries and law enforcement systems in order to create a Common Information Sharing Environment for the EU maritime domain.\n\nIn the Philippines, the National Coast Watch System (NCWS) was originally designed to improve maritime domain awareness in the Sulu and Celebes Seas, but has been extended over the entire island country's territory.\n\nIn March 2012, the South African Navy announce the establishment of two MDA centers, one in Cape Town for the west coast and one in Durban to cover the east coast.\n\nNEW DELHI: The Narendra Modi government will establish a National Maritime Authority (NMA) to ensure cohesive policy-making and effective coordination on coastal security among the multiple authorities dealing with maritime issues in the country. \n\nIt was the NMA that caught the eye of many in the defence establishment. The concept of an apex federal maritime body is not new. It was strongly recommended after the 26/11 Mumbai terror attacks in 2008, and even figured on the agenda of the Cabinet Committee on Security in the shape of a maritime security advisory board (MSAB) with a maritime security advisor as its chief. \n\nBut the then UPA government had junked it due to turf-wars, among other reasons. Even the 2001 GoM report on reforming the national security system had underlined the need for \"an apex body for management of maritime affairs for institutionalised linkages among the Navy, Coast Guard and ministries of central and state governments\". \n\nThough a slew of coastal security measures have been taken since 26/11, from a fledgling coastal radar network to state marine police stations, experts feel port and coastal security in India is still far from becoming impregnable. \n\nThe 15 or more agencies involved, ranging from Navy, Coast Guard, customs, intelligence agencies and port authorities to the home and shipping ministries, state governments and fisheries departments, often work at cross-purposes. \"A full-time federal body like NMA is needed to clear the clutter,\" said an official. \n\nAfter 26/11, the UPA regime in August 2009 did constitute the national committee on strengthening maritime and coastal security against threats from the sea (NCSMCS), with the cabinet secretary as its chairman. \"While NCSMCS has done some good work in monitoring implementation of coastal security measures as well as coordination with different stakeholders, it has met barely 9-10 times since then... much more needs to be done,\" said a top official. \n\nThe national maritime domain awareness (NMDA) project, basically an integrated intelligence grid to detect and tackle threats emanating from the sea in real-time, for instance, is yet to take concrete shape. Its aim was to generate a \"common operational picture\" of activities at sea through an institutionalised mechanism for collecting, fusing and analysing information from technical and other sources like coastal surveillance network radars, space-based automatic identification systems, vessel traffic management systems, fishing vessel registration and fishermen biometric identity databases..\n"}
{"id": "44657", "url": "https://en.wikipedia.org/wiki?curid=44657", "title": "Metis (mythology)", "text": "Metis (mythology)\n\nMetis (; Greek: Μῆτις - \"wisdom,\" \"skill,\" or \"craft\"), in ancient Greek religion, was a mythical Titaness belonging to the second generation of Titans. \n\nBy the era of Greek philosophy in the 5th century BC, Metis had become the mother of wisdom and deep thought, but her name originally connoted \"magical cunning\" and was as easily equated with the trickster powers of Prometheus as with the \"royal \"metis\"\" of Zeus. The Stoic commentators allegorised Metis as the embodiment of \"prudence\", \"wisdom\" or \"wise counsel\", in which form she was inherited by the Renaissance.\n\nThe Greek word \"metis\" meant a quality that combined wisdom and cunning. This quality was considered to be highly admirable in the Mycenean era, with the hero Odysseus being the embodiment of it. \n\nMetis was an Oceanid, the daughters of Oceanus and his sister Tethys, who were three thousand in number, and was of an earlier age than Zeus and his siblings. Metis was the first great spouse of Zeus, and also his cousin. Zeus is himself titled \"Mêtieta\" (\"the wise counsellor\"), in the Homeric poems.\n\nIn the Classical era, it was regarded by Athenians as one of the notable characteristics of the Athenian character. Metis was the one who gave Zeus a potion to cause Cronus to vomit out Zeus' siblings. \n\nMetis was both a threat to Zeus and an indispensable aid:\"Zeus lay with Metis but immediately feared the consequences. It had been prophesied that Metis would bear extremely powerful children: the first, Athena and the second, a son more powerful than Zeus himself, who would eventually overthrow Zeus.\"In order to forestall these dire consequences, Zeus tricked her into turning herself into a fly and promptly swallowed her. He was too late: Metis had already conceived a child. In time she began making a helmet and robe for her fetal daughter. The hammering as she made the helmet caused Zeus great pain, and Hephaestus either clove Zeus's head with an axe, or hit it with a hammer at the river Triton, giving rise to Athena's birth. Athena leaped from Zeus's head, fully grown, armed, and armoured, and Zeus was none the worse for the experience.\n\nThe similarities between Zeus swallowing Metis and Cronus swallowing his children have been noted by several scholars. This also caused some controversy in regard to reproduction myths and the lack of a need for women as a means of reproduction.\n\nHesiod's account is followed by Acusilaus and the Orphic tradition, which enthroned Metis side by side with Eros as primal cosmogenic forces. Plato makes \"Poros\", or \"creative ingenuity\", the child of Metis. Stephen Fry mentions this at length in his popular book Mythos.\n\n\n"}
{"id": "40984195", "url": "https://en.wikipedia.org/wiki?curid=40984195", "title": "Mode water", "text": "Mode water\n\nMode water is defined as a particular type of water mass, which is nearly vertically homogeneous. Its vertical homogeneity is caused by the deep vertical convection in winter. The first term to describe this phenomenon is \"18° water\", which is used by L.V. Worthington to describe the isothermal layer in the northern Sargasso Sea cool to a temperature of about 18 °C each winter. Then Masuzawa introduced the \"subtropical mode water\" concept to describe the thick layer of temperature 16–18 °C in the northwestern North Pacific subtropical gyre, on the southern side of the Kuroshio Extension. The terminology \"mode water\" was extended to the thick near-surface layer north of the Subantarctic Front by McCartney, who identified and mapped the properties of the Subantarctic mode water (SAMW). After that, McCartney and Talley then applied the term \"subpolar mode water\" (SPMW) to the thick near-surface mixed layers in the North Atlantic’s subpolar gyre.\nDifferent mode waters have different formation and erosion mechanisms. The subtropical mode water (STMW) is formed mainly by subduction, the SPMW is formed mainly by other processes. SAMW is due to the combination of subduction and other processes. erosion mechanism of SPMW is a combination of turbulent mixing and air-sea flux. The erosion mechanism of STMW is likely air-sea flux. For SAMW erosion turbulent mixing may be the main factor.\n\nMode water formation areas are generally characterized by wintertime mixed layers that are relatively thick compared with other mixed layers in the same geographical region. North Atlantic, Southern eastern Indian Oceans and Ocean in the Pacific have the thickest mixed layers, so these thick layers are associated with the North Atlantic’s subpolar mode water and the Southern Ocean’s subantarctic mode water. Relatively thick mixed layers are also found in the subtropical mode water areas near the separated western boundary currents.\n\nOne prominent feature of mode waters is they are stable in properties and locations, so that researchers can use data set from all decades to map the approximate core properties. The stability of properties is linked to the largest spatial-scale, longest time-scale wind and buoyancy forcing. It is not to say there is no variation in properties of the mode water. These variations in these near-surface water masses, in temperature, salinity, density and thickness, are linked to surface forcing changes, although in some cases the connection is not yet obvious. For example, Suga and Hanawa show as the seasons progress, mode water moves away from the formation area and sometimes becomes permanently capped.\n\nTo detect the mode water we can use the minimum value in the vertical gradient of potential density, or equivalently in the Brunt–Väisälä frequency. Since temperature profiles are more abundant and both salinity and temperature are relatively homogeneous in mode water, vertical temperature gradients are sometimes used instead of potential vorticity or the vertical gradient of potential density to identify the core of the mode water. There is no specific values of those gradients to define the boundaries of a given mode water.\n\nMode waters have a big impact on nutrients distribution as they prevent deep-ocean nutrients from upwelling to the euphotic zone. Further more, they will control the biological pump, which plays an important role in carbon dioxide uptake. Dynamically, mode waters also control potential vorticity and baroclinic in the subtropical North Atlantic.\n"}
{"id": "3736541", "url": "https://en.wikipedia.org/wiki?curid=3736541", "title": "Nancy Newhall", "text": "Nancy Newhall\n\nNancy Wynne Newhall (May 9, 1908 – July 7, 1974) was an American photography critic. She is best known for writing the text to accompany photographs by Ansel Adams and Edward Weston, but was also a widely published writer on photography, conservation, and American culture. \n\nNewhall was born Nancy Wynne in Lynn, Massachusetts, and attended Smith College in that state. She married Beaumont Newhall, the curator of photography at the Museum of Modern Art in New York City, and substituted for him in that role during his military service in World War II. During the 1940s she wrote essays on popular art and culture for small magazines and journals, in which she called for a society more attuned to art, and particularly to visual art. Newhall was always more interested in a popular audience than an academic one; in a 1940 essay, she explores the possibilities of the new medium of television for popularizing the visual arts, suggesting techniques for teaching art and photography on camera:\nIn another, she argues for the centrality of photography for understanding and teaching American history (\"Research\"). Newhall became close to photographer Edward Weston during this period, championing his early work and regarding his controversial 1940s work, which juxtaposed still lifes and nudes of considerable beauty and delicacy with wartime items such as gas masks, with some anxiety.\n\nIn 1945, Newhall wrote the text for a book of photographs, \"Time in New England,\" by Paul Strand. The work would begin a new phase for her career, in which she became a vocal proponent and a central pioneer of the genre of oversized photography collections. The best known and most influential of these is \"This Is the American Earth\", a collaboration with Ansel Adams, published in 1960. Like Adams, Newhall was involved with the Sierra Club, and wrote often about issues of conservation. Newhall was sometimes accused of political heavy-handedness on that subject—one uncharitable review of \"American Earth\" calls her prose \"so full of Message that there is no room for poetry\" (Deevey)—but her explication of the political context and motivation of Adams' work has been important for the Sierra Club and the conservation movement in general.\n\nNancy and Beaumont spent three summers at Black Mountain College beginning in 1946. In addition to lecturing and teaching, the Newhalls photographed the college campus and its people, taking portraits of Leo Amino, Ilya Bolotowsky, Gwendolyn Knight, Jacob Lawrence, and Buckminster Fuller's venetian-blind experiment. Some of Nancy and Beaumont Newhall's work is archived at the Center for Creative Photography at the University of Arizona in Tucson, Arizona and at the Getty Research Institute in Los Angeles, California.\n\nShe died on July 7, 1974 at St. Johns Hospital in Jackson Hole, Wyoming from injuries received in an accident which occurred on the Snake River of Grand Teton National Park.\n\n\n\n"}
{"id": "86622", "url": "https://en.wikipedia.org/wiki?curid=86622", "title": "Nemetona", "text": "Nemetona\n\nNemetona, or 'she of the sacred grove', is a Celtic goddess with roots in northeastern Gaul. She is thought to have been the eponymous deity of the Germano-Celtic people known as the Nemetes; evidence of her veneration is found in their former territory along the Middle Rhine as well in the Altbachtal sanctuary in present-day Trier, Germany. She is also attested in Bath, England, where an altar to her was dedicated by a man of the Gallic Treveri people.\n\nHer name is derived from the Celtic root \"nemeto-\", referring to consecrated religious spaces, particularly sacred groves. She has thus been taken to be a guardian goddess of open-air places of worship. The same root is found in the names of the Romano-British goddess Arnemetia and the Matres Nemetiales (known from an inscription in Grenoble).\n\nSurviving inscriptions often associate Nemetona with Mars (sometimes given the Celtic name Loucetius). She is paired with \"Loucetius Mars\" in the inscription at Bath, and with \"Mars\" at Trier and Altrip. Separate inscriptions to Nemetona and to Loucetius have been recovered from the same site in Klein-Winternheim near Mainz. The Altrip site was further notable for yielding a terra cotta depiction of the goddess.\n\nOne inscription from Eisenberg appears to identify Nemetona with Victoria:\n\nNoémie Beck considers the identification of Nemetona with Nemain to be \"inaccurate and irrelevant\".\n\n"}
{"id": "37802425", "url": "https://en.wikipedia.org/wiki?curid=37802425", "title": "North Pacific Intermediate Water", "text": "North Pacific Intermediate Water\n\nNorth Pacific Intermediate Water (NPIW) is cold, moderately low salinity water mass that originates in the mixed water region (MWR) between the Kuroshio and Oyashio waters just east of Japan. Examination of NPIW at stations just east of the MWR indicates that the mixed waters in the MWR are the origin of the newest NPIW. The new NPIW ‘‘formed’’ in the MWR is a mixture of relatively fresh, recently ventilated Oyashio water coming from the subpolar gyre, and more saline, older Kuroshio water. The mixing process results in a salinity minimum and also in rejuvenation of the NPIW layer in the subtropical gyre due to the Oyashio input.\n\nThe density of the NPIW in the MWR, about 26.6 – 26.9 σ, is slightly higher than the apparent later winter surface density of the subpolar water. The low salinity and high oxygen at the density of the NPIW are attained in the subpolar gyre, through vertical diffusion in the open North Pacific and direct ventilation in the Okhotsk Sea as a result of sea ice formation.\n\nAccording to recent studies, the NPIW is formed along three paths between the Kuroshio Extension and the subarctic front. The formation time scale is estimated to be 1 – 1.5 years based on observations by floating instruments and the residence time of NPIW is estimated to be about 20 years. Since the formation time is much less than the residence time, the properties of the NPIW may change on an annual basis as a result of the large changes in the Oyashio transport.\n\n"}
{"id": "42783929", "url": "https://en.wikipedia.org/wiki?curid=42783929", "title": "OSCAR 1", "text": "OSCAR 1\n\nOSCAR I (aka OSCAR 1) is the first amateur radio satellite launched by Project OSCAR into Low Earth Orbit. OSCAR I was launched December 12, 1961, by a Thor-DM21 Agena B launcher from Vandenberg Air Force Base, Lompoc, California. The satellite, a rectangular box (30 x 25 x 12 cm) weighing 10 kg., was launched as a secondary payload (ballast) for Corona 9029, also known as Discoverer 36, the eighth and final launch of a KH-3 satellite.\n\nThe satellite had a battery-powered 140 mW transmitter operating in the 2-meter band (144.983 MHz), employed a monopole transmitting antenna 60 cm long extended from the center of the convex surface, but had no attitude control system. \nLike Sputnik 1, Oscar 1 carried only a simple beacon. For three weeks it transmitted its Morse Code message \"HI\". To this day, many organizations identify their Morse-transmitting satellites with \"HI\", which also indicates laughter in amateur telegraphy.\n\nOSCAR I lasted 22 days ceasing operation on January 3, 1962, and re-entered January 31, 1962.\n\nThe uniqueness of the OSCAR-1 spacecraft was not only that it was built by amateurs, only about four years after the launch of Sputnik-1, but that it was the world’s first piggyback satellite and the world’s first private non-government spacecraft.\n\nImmediately following the launch of OSCAR-1, United States vice president, Lyndon B. Johnson, honored it with a congratulatory telegram to the group sponsoring this momentous event in the history of Amateur Radio. It read:\n“For me this project is symbolic of the type of freedom for which this country stands — freedom of enterprise and freedom of participation on the part of individuals throughout the world.”\n\nThe original backup of OSCAR-1 has been restored and is fully operational, running off AC power. As of 2011 it is on display at ARRL HQ in Newington, Connecticut and continues to broadcast \"HI\" in Morse Code at 145MHz. \n\nProject OSCAR Inc. started in 1960 with the radio amateurs from the TRW Radio Club of Redondo Beach, California, many who worked at TRW, California defense industries, and Foothill College to investigate the possibility of putting an amateur satellite in orbit. Mr. Projoscar of Foothill College served as the Project Manager for Project OSCAR. Project OSCAR was responsible for the construction of the first Amateur Radio Satellite OSCAR-1, that was successfully launched from Vandenberg AFB in California. OSCAR-1 orbited the earth for 22 days, transmitting the “HI” greeting. Project Oscar was responsible for launching the next 3 amateur radio satellites during the 1960s: OSCAR 2, OSCAR 3, and OSCAR 4.\n\nSince that beginning, the group has focused on supporting and promoting amateur radio satellite related projects. Some current members take part in university satellite programs as advisors. The Project Oscar club has become more active since the start of the AMSAT Eagle project, with a renewed effort to build hardware and educate hams on the advantages of satellite operation.\n\nIn 1969 The Radio Amateur Satellite Organization (AMSAT) was founded by radio amateurs working at NASA's Goddard Space Flight Center and living in the Baltimore-Washington DC region, to continue the efforts begun by Project OSCAR. Its first project was to coordinate the launch of Australis-OSCAR 5, constructed by students at the University of Melbourne.\n\nToday, more than fifty years later, Project OSCAR's mission is “To initiate and support activities that promote the Satellite Amateur Radio Hobby”. Project Oscar's primary goal is to reach out and provide logistical support, training and in some cases equipment to amateur radio associations, schools and the public at large.\n\n\n"}
{"id": "1298274", "url": "https://en.wikipedia.org/wiki?curid=1298274", "title": "Ondol", "text": "Ondol\n\nOndol (Hangul: 온돌, ; from Korean \"ondol\" ) in Korean traditional architecture, is underfloor heating that uses direct heat transfer from wood smoke to heat the underside of a thick masonry floor. In modern usage it refers to any type of underfloor heating, or to a hotel or a sleeping room in Korean (as opposed to Western) style.\n\nThe main components of the traditional \"ondol\" are an \"agungi\" (firebox or stove) accessible from an adjoining room (typically kitchen or master bedroom), a raised masonry floor underlain by horizontal smoke passages, and a vertical, freestanding chimney on the opposite exterior wall providing a draft. The heated floor, supported by stone piers or baffles to distribute the smoke, is covered by stone slabs, clay and an impervious layer such as oiled paper.\n\nUse of the \"ondol\" has been found at archaeological sites in present-day North Korea. A Bronze Age archaeological site, circa 1000 BC, discovered in Unggi, Hamgyeongbuk-do, in present-day North Korea, shows a clear vestige of gudeul in the excavated dwelling ().\n\nEarly \"ondol\"s began as \"gudeul\" that provided the heating for a home and for cooking. When a fire was lit in the furnace to cook rice for dinner, the flame would extend horizontally because the flue entry was beside the furnace. This arrangement was essential, as it would not allow the smoke to travel upward, which would cause the flame to go out too soon. As the flame would pass through the flue entrance, it would be guided through the network of passages with the smoke. Entire rooms would be built on the furnace flue to create ondol floored rooms.\n\nThe term \"gudeul\" used colloquially for the modern day ondol in Korea for over two thousand years, and called by many alternate names (\"janggaeng\" (), \"hwagaeng\" (), \"nandol\" (), \"yeondol\" ()); the term \"ondol\" was introduced around the end of the 19th century. According to a Korean folkloric historian Son Jintae (1900 - missing during the 1950-53 Korean War), \"gudeul\" originated from \"guun-dol\" (Korean), which means \"warm stone\", and its pronunciation has undergone some change from \"gudol\" or \"gudul\" to finally take the form of \"gudeul\". \"Ondol\" was first written in Hanja by modern-day writers.\n\nOndol had traditionally been used as a living space for sitting, eating, sleeping and other pastimes in most Korean homes before the 1960s. Koreans are accustomed to sitting and sleeping on the floor, and working and eating at low tables instead of raised tables with chairs. The furnace burned mainly rice paddy straws, agricultural crop waste, biomass or any kind of dried firewood. For short-term cooking, rice paddy straws or crop waste was preferred, while long hours of cooking and floor heating needed longer-burning firewood. Unlike modern-day water heaters, the fuel was either sporadically or regularly burned (two to five times a day), depending on frequency of cooking and seasonal weather conditions.\n\nWith the traditional ondol heating, the floor closer to the furnace was normally warm enough, and the warmest spots reserved for elders and honored guests. Ondol had problems such as environmental pollution and carbon monoxide poisoning resulting from burning coal briquettes. Thus, other technology heats modern Korean homes.\n\nThe famous American architect, Frank Lloyd Wright, was building a hotel in Japan, and was invited to a Japanese family's house. The homeowner had experienced the ondol in Korea, and he built an ondol room in his house. Wright reportedly was so impressed that he invented radiant floor heating which uses hot water as the heating medium. Some of Wright's buildings employed this system. \n\nInstead of ondol-hydronic radiant floor heating, modern-day houses such as high-rise apartments have a modernized version of the ondol system. Many architects know the advantages and benefits of ondol, and they are using ondol in modern houses. Since the ondol has been introduced to many countries, it is beginning to be considered as one of the systems of home heating. Modern ondol are not same as the original version. Almost all Koreans use modern versions, so it is hard to find the traditional ondol system in Korean houses.\n\nOne of the advantages of an Ondol is that it can maintain heat for an extended period. In a traditional Korean house, people usually extinguish the fire before going to sleep at night, since it can stay warm until the morning. An ondol conducts heat evenly throughout the whole room, although the part of the room closest to the \"agungi\" is much warmer. Comparing the Ondol with the Western radiator: the heat from the radiator rises towards the ceiling, but Ondol, keeps both the floor and the air in the room warm. The advantage of the Ondol is that people do not have to worry about breakdown and repair of the Ondol. The Ondol is part of the house, therefore, it is less likely to run into problems. Any combustible materials can be used as fuel for the Ondol; there are no special fuel requirements. In contrast to heaters, such as fireplaces or charcoal-based heaters that leave ash in the room, Ondol does not cause pollution in the room leaving it clean and warm. \n\nThe Ondol has some disadvantages. Mud and stones are one of the main materials that make up the Ondol. Such materials take quite a long time to heat up, therefore the room takes a long time to warm up. In addition, it is difficult to adjust the temperature of the room.\n\nIn the 1970s and 80s, soldiers stationed at Camp Casey, Korea, were allowed to have off-post facilities after written approval of the local commander. But, the facilities had to pass carbon monoxide inspections every six months if they were heated by ondol (charcoal or wood). An non-Commissioned Officer from the soldier's unit would prepare a film canister to hold a diesel soaked piece of cloth. The NCO would place the fuel-soaked rag on the top of the charcoal and close the heater. The black smoke would travel under the floor and exit the flue. However, any leaks in the floor would allow black smoke to seep into the room. This would be the same pathway that the deadly carbon monoxide would get into the living quarters, putting the occupants at risk. Certification of safe testing was the minimum due diligence on the part of command.\n\nThe \"dol bed\", or stone bed, is a manufactured bed that has the same heating effect as \"ondol\" and is purported to have health benefits. The \"dol bed\" industry is estimated to be worth 100 billion Korean won, comprising 30 to 40 percent of the entire bed industry in South Korea; \"dol beds\" are most popular with middle-aged people in their 40s and 50s.\n\n"}
{"id": "42324882", "url": "https://en.wikipedia.org/wiki?curid=42324882", "title": "Pivdenno-Holitsynske gas field", "text": "Pivdenno-Holitsynske gas field\n\nThe Pivdenno-Holitsynske gas field natural gas field located on the continental shelf of the Black Sea. It was discovered in 1981 and developed by Chornomornaftogaz. It started commercial production in 1982. The total proven reserves of the Pivdenno-Holitsynske gas field are around , and production is slated to be around in 2015.\n"}
{"id": "48515156", "url": "https://en.wikipedia.org/wiki?curid=48515156", "title": "Powerline river crossings in the United Kingdom", "text": "Powerline river crossings in the United Kingdom\n\nThere are a number of substantial overhead power line river crossings in the United Kingdom.\n\nThe tallest electricity pylons in the UK are those of the 400 kV Thames Crossing, at West Thurrock, which are 190 m (630 ft) high. These were constructed by BICC in 1965. The cables stretch 1300 m (4,500 ft) across the River Thames and have a minimum clearance of 76 m (250 ft).\n\nThe longest powerline river crossing in the UK is the Aust Severn Powerline Crossing over the River Severn at Aust, stretching 1700 m (5,310 ft) between towers 148 m (488 ft) high. This pylon crossing is paralleled by the Severn-Wye Cable Tunnel beneath it, at almost the same location.\n\nThe 275 kV Forth Crossing, just upriver of the Kincardine Bridge, in Scotland, has a span of about 1130 m (3,700 ft) over the River Forth.\n\nA crossing of the River Tyne, located near Jarrow has a height of 128 meters, and a span of around 800 meters. \n"}
{"id": "44865976", "url": "https://en.wikipedia.org/wiki?curid=44865976", "title": "Premixed turbulent flames", "text": "Premixed turbulent flames\n\nIn a premixed turbulent flame, fuel and oxidizer are being mixed by turbulence during a sufficiently long time before combustion is initiated. The deposition of energy from the spark generates a flame kernel that grows at first by laminar, then by turbulent flame propagation. And in which the oxidizer has been mixed with the fuel before it reaches the flame front. This creates a thin flame front as all of the reactants are readily available.\n\n"}
{"id": "48264022", "url": "https://en.wikipedia.org/wiki?curid=48264022", "title": "Pursuit predation", "text": "Pursuit predation\n\nPursuit predation is a form of predation in which predators give chase to fleeing prey. The chase can be initiated either by the predator or by the prey, should the prey be alerted to a predator's presence and attempt to flee before the predator gives chase. The chase ends when either the predator captures and consumes the prey, or the prey escapes. Pursuit predation is typically observed in carnivorous species within the kingdom Animalia, with some iconic examples being cheetahs, lions, and wolves.\n\nPursuit predation is an alternate predation strategy to ambush predation. While pursuit predators use a detection and pursuit phase in order to obtain prey, ambush predators use stealth to capture prey. Strength and speed are important to pursuit predators, whereas ambush predators ignore these in favor of surprise from a typically concealed location. While the two patterns of predation are not mutually exclusive, morphological differences in body plan can create a bias in an organism towards each type of predation.\n\nThere is still uncertainty as to whether predators behave with a general tactic or strategy while preying. However, among pursuit predators, there are several common behaviors. Often, predators will scout potential prey, assessing prey quantity and density prior to engaging in a pursuit. Certain predators choose to pursue prey primarily in a group of conspecifics; such animals are known as pack hunters or group pursuers. Other species choose to hunt alone. These two behaviors are typically due to differences in hunting success, where some groups are very successful in groups and others are more successful alone. Pursuit predators may also choose to either exhaust their metabolic resources rapidly or pace themselves during a chase. This choice can be influenced by prey species, seasonal settings, or temporal settings. Predators that rapidly exhaust their metabolic resources during a chase tend to first stalk their prey, slowly approaching their prey to decrease chase distance and time. When the predator is at a closer distance (one that would lead to easier prey capture), it finally gives chase. Pacing pursuit is more commonly seen in group pursuit, as individual animals do not need to exert as much energy to capture prey. However, this type of pursuit requires group coordination, which may have varying degrees of success. Since groups can engage in longer chases, they often focus on separating a weaker or slower prey item during pursuit. Morphologically speaking, while ambush predation requires stealth, pursuit predation requires speed; pursuit predators are proportionally long-limbed and equipped with cursorial adaptations. Current theories suggest that this proportionally long-limbed approach to body plan was an evolutionary countermeasure to prey adaptation.\n\nGroup pursuers hunt with a collection of conspecifics. Group pursuit is usually seen in species of relatively high sociality; in vertebrates, individuals often seem to have defined roles in pursuit.\n\nAfrican wild dog (\"Lycaon pictus\") packs have been known to split into several smaller groups while in pursuit; one group initiates the chase, while the other travels ahead of the prey's escape path. The group of chase initiators coordinate their chase to lead the prey towards the location of the second group, where the prey's escape path will be effectively cut off. Bottlenose dolphins (\"Tursiops\") have been shown exhibiting similar behaviors of pursuit role specialization. One group within the dolphin pod, known as the drivers, give chase to the fish - forcing the fish into a tight circle formation, while the other group of the pod, the barriers, approach the fish from the opposite direction. This two-pronged attack leaves the fish with only the option of jumping out of the water to escape the dolphins. However, the fish are completely vulnerable in the air; it is at this point when the dolphins leap out and catch the fish. In lion (\"Panthera leo\") pack hunting, each member of the hunting group is assigned a position, from left wing to right wing, in order to better obtain prey. Such specializations in roles within the group are thought to increase sophistication in technique; lion wing members are faster, and will drive prey toward the center where the larger, stronger, killing members of the pride will take down the prey. Many observations of group pursuers note an optimal hunting size in which certain currencies (mass of prey killed or number of prey killed) are maximized with respect to costs (kilometers covered or injuries sustained). Groups size is often dependent on aspects of the environment: number of prey, prey density, number of competitors, seasonal changes, etc.\n\nWhile birds are generally believed to be individual hunters, there are a few examples of birds that cooperate during pursuits. Harris's hawks (\"Parabuteo unicinctus\") have two cooperative strategies for hunting. One strategy involves a group of hawks surrounding prey hidden under some form of cover, while another hawk attempts to penetrate the prey's cover. The penetration attempt flushes the prey out from its cover where it is swiftly killed by one of the surrounding hawks. The second strategy is less commonly used. It involves a \"relay attack\" in which a group of hawks, led by a \"lead\" hawk, engage in a long chase for prey. The \"lead\" hawk will dive in order to kill the prey. If the dive is unsuccessful, the role of the \"lead\" shifts to another hawk who will then dive in another attempt to kill the prey. During one observed relay attack, 20 dives and hence 20 lead switches were exhibited.\n\nAs in vertebrates, there are many species of invertebrates which actively pursue prey in groups and exhibit task specialization, but while the vertebrates change their behavior based on their role in hunting, invertebrate task delegation is usually based on actual morphological differences. The vast majority of eusocial insects have castes within a population which tend to differ in size and have specialized structures for different tasks. This differentiation is taken to the extreme in the groups isoptera and hymenoptera, or termites and ants, bees, and wasps respectively.\n\nTermite-hunting ants of the genus \"Pachycondyla\", also known as Matabele ants, form raiding parties consisting of ants of different castes, such as soldier ants and worker ants. Soldier ants are much larger than worker ants, with more powerful mandibles and more robust exoskeletons, and so they make up the front lines of raiding parties and are responsible for killing prey. Workers usually butcher and carry off the killed prey, while supporting the soldiers. The raiding parties are highly mobile and move aggressively into the colonies of termites, often breaking through their outer defenses and entering their mounds. The ants do not completely empty the mound of termites, instead they only take a few, allowing the termites to recover their numbers so that the ants have a steady stream of prey.\n\nAsian giant hornets, \"Vespa mandarinia\", form similar raiding parties to hunt their prey, which usually consists of honeybees. The giant hornets group together and as a team can decimate an entire honeybee colony, especially those of non-native European honeybees. Alone, the hornets are subject to attack by the smaller bees, who swarm the hornet and vibrate their abdomens to generate heat, collectively cooking the hornet until it dies. By hunting in groups, the hornets avoid this problem.\n\nWhile most big cat species are individual, ambush predators, Cheetahs (\"Acinonyx jubatus\") are pursuit predators. Widely known as the fastest terrestrial animal, with speeds reaching 61–64 miles per hour, cheetahs take advantage of their speed during chases. However, their speed and acceleration also have disadvantages, as both can only be sustained for short periods of time. Studies show that cheetahs can maintain maximum speed for a distance of approximately 500 yards. Due to these limitations, cheetahs are often observed running at moderate speeds during chases. There are claims that the key to cheetahs' pursuit success may not be just their speed. Cheetahs are extremely agile, able to maneuver and change directions at very high speeds in very short amounts of time. This extensive maneuverability can make up for unsustainable high speed pursuit, as it allows cheetahs to quickly close the distance between prey without decreasing their speed when prey change direction.\n\nThe Painted redstart (\"Myioborus pictus\") is one of the most well documented flush pursuers. When flies, prey for redstarts, are alerted of the presence of predators, they respond by fleeing. Redstarts take advantage of this anti-predator response by spreading and orienting their easily noticeable wings and tails, alerting the flies, but only when they are in a position where the flies' escape path intersects with the redstart's central field of vision. When prey's path are in this field of vision, the redstart's prey capture rate is at it maximum. Once the flies begin to flee, the redstart begins to chase. It has been proposed that redstarts exploit two aspects of the visual sensitivity of their prey: sensitivity to the location of the stimulus in the prey's visual field and sensitivity to the direction of stimulus environment. The effectiveness of this pursuit can also be explained by \"rare enemy effect\", an evolutionary consequence of multi-species predator-prey interactions.\n\nDragonflies are skilled aerial pursuers; they have a 97% success rate for prey capture. This success rate is a consequence of the \"decision\" oon which prey to pursue, based on initial conditions. Observations of several species of perching dragonflies show more pursuit initiations at larger starting distances for larger size prey species than for much smaller prey. Further evidence points to a potential bias towards larger prey, due to more substantial metabolic rewards. This bias is in spite of the fact that larger prey are typically faster and choosing them results in less successful pursuits. Dragonflies high success rate for prey capture may also be due to their interception foraging method. Unlike classical pursuit, in which the predator aims for the current position of their prey, dragonflies predict the prey's direction of motion, as in parallel navigation. Perching dragonflies (Libellulidae family), have been observed \"staking out\" high density prey spots prior to pursuit. There are no noticeable distinctions in prey capture efficiency between males and females. Further, percher dragonflies are bound by their visual range. They are more likely to engage in pursuit when prey come within a subtended angle of around 1-2 degrees. Angles greater than this are outside of a dragonflies visual range.\n\nCurrent theory on the evolution of pursuit predation suggests that the behavior is an evolutionary countermeasure to prey adaptation. Prey animals vary in their likelihood to avoid predation, and it is predation failure that drives evolution of both prey and predator. Predation failure rates vary wildly across the animal kingdom; raptorial birds can fail anywhere from 20% to 80% of the time in predation, while predatory mammals usually fail more than half the time. Prey adaptation drives these low rates in three phases: the detection phase, the pursuit phase, and the resistance phase. The pursuit phase drove the evolution of distinct behaviors for pursuit predation. \nAs selective pressure on prey is higher than on predators adaptation usually occurs in prey long before the reciprocal adaptations in predators. Evidence in the fossil record supports this, with no evidence of modern pursuit predators until the late Tertiary period. Certain adaptations, like long limbs in ungulates, that were thought to be adaptive for speed against predatory behavior have been found to predate predatory animals by over 20 million years. Because of this, modern pursuit predation is an adaptation that may have evolved separately and much later as a need for more energy in colder and more arid climates. Longer limbs in predators, the key morphological adaptation required for lengthy pursuit of prey, is tied in the fossil record to the late Tertiary. It is now believed that modern pursuit predators like the wolf and lion evolved this behavior around this time period as a response to ungulates increasing feeding range. As ungulate prey moved into a wider feeding range to discover food in response to changing climate, predators evolved the longer limbs and behavior necessary to pursue prey across larger ranges. In this respect, pursuit predation is not co-evolutionary with prey adaptation, but a direct response to prey. Prey's adaptation to climate is the key formative reason for evolving the behavior and morphological necessities of pursuit predation.\n\nIn addition to serving as a countermeasure to prey adaptation, pursuit predation has evolved in some species as an alternative, facultative mechanism for foraging. For example, polar bears typically act as specialized predators of seal pups and operate in a manner closely predicted by the optimal foraging theory. However, they have been seen to occasionally employ more energy-inefficient pursuit predation tactics on flightless geese. This alternative predatory strategy may serve as a back-up resource when optimal foraging is circumstantially impossible, or may even be a function of filling dietary needs.\n\nPursuit predation revolves around a distinct movement interaction between predator and prey; as prey move to find new foraging areas, predators should move with them. Predators congregate in areas of high prey density, and prey should therefore avoid these areas. However, dilution factor may be a reason to stay in areas of high density due to a decreased risk of predation. Given the movements of predators over ranges in pursuit predation, though, dilution factor seems a less important cause for predation avoidance. Because of these interactions, spatial patterns of predators and prey are important in preserving population size. Attempts by prey to avoid predation and find food are coupled with predator attempts to hunt and compete with other predators. These interactions act to preserve populations. Models of spatial patterns and synchrony of predator-prey relationships can be used as support for the evolution of pursuit predation as one mechanism to preserve these population mechanics. By pursuing prey over long distances, predators actually improve longterm survival of both their own population and prey population through population synchrony. Pursuit predation acts to even out population fluctuations by moving predatory animals from areas of high predator density to low predator density, and low prey density to high prey density. This keeps migratory populations in synchrony, which increases metapopulation persistence. Pursuit predation’s effect on population persistence is more marked over larger travel ranges. Predator and prey levels are usually more synchronous in predation over larger ranges, as population densities have more ability to even out. Pursuit predation can then be supported as an adaptive mechanism for not just individual feeding success but also metapopulation persistence.\n\nJust as the evolutionary arms race has led to the development of pursuit behavior of predators, so too has it led to the anti-predator adaptations of prey. Alarm displays such as eastern swamphen's tail flicking, white-tailed deer's tail flagging, and Thomson's gazelles' stotting have been observed deterring pursuit. These tactic are believed to signal that a predator's presence is known and, therefore, pursuit will be much more difficult. These displays are more frequent when predators are at an intermediate distance away. Alarm displays are used more often when prey believe predators are more prone to change their decision to pursue. For instance, cheetahs, common predators of Thomson's gazelles, are less likely to change their choice to pursue. As such, gazelles stott less when cheetahs are present than when other predators are present. In addition to behavioral adaptations, there are also morphological anti-predator adaptations to pursuit predators. For example, many birds have evolved rump feathers that fall off with much less force than the feathers of their other body parts. This allows for easier escape from predator birds, as avian predators often approach prey from their rump.\n\nIn many species that fall prey to pursuit predation, gregariousness on a massive scale has evolved as a protective behavior. Such herds can be conspecific (all individuals are of one species) or heterospecific. This is primarily due to the confusion effect, which states that if prey animals congregate in large groups, predators will have more difficulty identifying and tracking specific individuals. This effect has greater influence when individuals are visually similar and less distinguishable. In groups where individuals are visually similar, there is a negative correlation between group size and predator success rates. This may mean that the overall number of attacks decreases with larger group size or that the number of attacks per kill increases with larger group size. This is especially true in open habitats, such as grasslands or open ocean ecosystems, where view of the prey group is unobstructed, in contrast to a forest or reef. Prey species in these open environments tend to be especially gregarious, with notable examples being starlings and sardines. When individuals of the herd are visually dissimilar, however, the success rate of predators increases dramatically. In one study, wildebeest on the African Savannah were selected at random and had their horns painted white. This introduced a distinction, or oddity, into the population; researchers found that the wildebeest with white horns were preyed upon at substantially higher rates. By standing out, individuals are not as easily lost in the crowd, and so predators are able to track and pursue them with higher fidelity. This has been proposed as the reason why many schooling fish show little to no sexual dimorphism, and why many species in heterospecific schools bear a close resemblance to other species in their school.\n"}
{"id": "4286589", "url": "https://en.wikipedia.org/wiki?curid=4286589", "title": "Quarry-faced stone", "text": "Quarry-faced stone\n\nQuarry-faced stone is stone with a rough, unpolished surface, straight from the quarry. Many residential and public buildings in Jerusalem, Israel are built from quarry-faced Jerusalem stone.\n"}
{"id": "40046610", "url": "https://en.wikipedia.org/wiki?curid=40046610", "title": "Semiembossed film", "text": "Semiembossed film\n\nSemiembossed film is used as a liner to the calendared rubber to retain the properties of rubber and also to prevent dust and other foreign matters from sticking to the rubber while calendaring and during storage. It is manufactured with 100% virgin low-density polyethylene. The raw material is extruded and cast on the embossed roll and cooled. It can be of any color. Milky white, French blue, red and yellow are standard colors. The diamond-shaped embossing in the film helps in the easy removal of air between the film and the rubber. Semiembossed film is used by tyre manufacturers, tread and bonding gum manufacturers, conveyor belt manufacturers and other rubber coated fabric manufacturers.\n\nFilm embossing is a mechanical process in which a flat film is transformed into an embossed product. During the process, thermal and stress fields are applied to the polymer, causing changes in the microstructure and physical dimensions of the material. The engineering analysis of the process requires the study of various aspects relating to the characterization of the microstructure before and after embossing, A variety of techniques were employed to characterize the properties and microstructure of the embossed film in relation to crystallinity, orientation, mechanical properties, and dimensions of the embossed films. The thermal treatment of the polymer film was shown to be the most significant factor in the process. By controlling the thermal treatment of the film, it is possible to manipulate the properties and dimensions of the embossed film. The important aspects: influencing thermal treatment include the radiation heater temperature, preheat roll temperature, line velocity, and film thickness. The initial film orientation and embossing pressure have a minor effect on the final properties of the embossed film. The main effect of the embossing pressure is on the bulk thickness of the embossed film.\n"}
{"id": "5894781", "url": "https://en.wikipedia.org/wiki?curid=5894781", "title": "Standard solar model", "text": "Standard solar model\n\nThe standard solar model (SSM) is a mathematical treatment of the Sun as a spherical ball of gas (in varying states of ionisation, with the hydrogen in the deep interior being a completely ionised plasma). This model, technically the spherically symmetric quasi-static model of a star, has stellar structure described by several differential equations derived from basic physical principles. The model is constrained by boundary conditions, namely the luminosity, radius, age and composition of the Sun, which are well determined. The age of the Sun cannot be measured directly; one way to estimate it is from the age of the oldest meteorites, and models of the evolution of the Solar System. The composition in the photosphere of the modern-day Sun, by mass, is 74.9% hydrogen and 23.8% helium. All heavier elements, called \"metals\" in astronomy, account for less than 2 percent of the mass. The SSM is used to test the validity of stellar evolution theory. In fact, the only way to determine the two free parameters of the stellar evolution model, the helium abundance and the mixing length parameter (used to model convection in the Sun), are to adjust the SSM to \"fit\" the observed Sun.\n\nA star is considered to be at zero age (protostellar) when it is assumed to have a homogeneous composition and to be just beginning to derive most of its luminosity from nuclear reactions (so neglecting the period of contraction from a cloud of gas and dust). To obtain the SSM, a one solar mass () stellar model at zero age is evolved numerically to the age of the Sun. The abundance of elements in the zero age solar model is estimated from primordial meteorites. Along with this abundance information, a reasonable guess at the zero-age luminosity (such as the present-day Sun's luminosity) is then converted by an iterative procedure into the correct value for the model, and the temperature, pressure and density throughout the model calculated by solving the equations of stellar structure numerically assuming the star to be in a steady state. The model is then evolved numerically up to the age of the Sun. Any discrepancy from the measured values of the Sun's luminosity, surface abundances, etc. can then be used to refine the model. For example, since the Sun formed, some of the helium and heavy elements have settled out of the photosphere by diffusion. As a result, the Solar photosphere now contains about 87% as much helium and heavy elements as the protostellar photosphere had; the protostellar Solar photosphere was 71.1% hydrogen, 27.4% helium, and 1.5% metals. A measure of heavy-element settling by diffusion is required for a more accurate model.\n\nThe differential equations of stellar structure, such as the equation of hydrostatic equilibrium, are integrated numerically. The differential equations are approximated by difference equations. The star is imagined to be made up of spherically symmetric shells and the numerical integration carried out in finite steps making use of the equations of state, giving relationships for the pressure, the opacity and the energy generation rate in terms of the density, temperature and composition.\n\nNuclear reactions in the core of the Sun change its composition, by converting hydrogen nuclei into helium nuclei by the proton-proton chain and (to a lesser extent in the Sun than in more massive stars) the CNO cycle. This increases the mean molecular weight in the core of the Sun, which should lead to a decrease in pressure. This does not happen as instead the core contracts. By the virial theorem half of the gravitational potential energy released by this contraction goes towards raising the temperature of the core, and the other half is radiated away. This increase in temperature also increases the pressure and restores the balance of hydrostatic equilibrium. The luminosity of the Sun is increased by the temperature rise, increasing the rate of nuclear reactions. The outer layers expand to compensate for the increased temperature and pressure gradients, so the radius also increases.\n\nNo star is completely static, but stars stay on the main sequence (burning hydrogen in the core) for long periods. In the case of the Sun, it has been on the main sequence for roughly 4.6 billion years, and will become a red giant in roughly 6.5 billion years for a total main sequence lifetime of roughly 11 billion (10) years. Thus the assumption of steady state is a very good approximation. For simplicity, the stellar structure equations are written without explicit time dependence, with the exception of the luminosity gradient equation:\n\nHere L is the luminosity, ε is the nuclear energy generation rate per unit mass and ε is the luminosity due to neutrino emission (see below for the other quantities). The slow evolution of the Sun on the main sequence is then determined by the change in the nuclear species (principally hydrogen being consumed and helium being produced). The rates of the various nuclear reactions are estimated from particle physics experiments at high energies, which are extrapolated back to the lower energies of stellar interiors (the Sun burns hydrogen rather slowly). Historically, errors in the nuclear reaction rates have been one of the biggest sources of error in stellar modelling. Computers are employed to calculate the varying abundances (usually by mass fraction) of the nuclear species. A particular species will have a rate of production and a rate of destruction, so both are needed to calculate its abundance over time, at varying conditions of temperature and density. Since there are many nuclear species, a computerised reaction network is needed to keep track of how all the abundances vary together.\n\nAccording to the Vogt-Russell theorem, the mass and the composition structure throughout a star uniquely determine its radius, luminosity, and internal structure, as well as its subsequent evolution (though this \"theorem\" was only intended to apply to the slow, stable phases of stellar evolution and certainly does not apply to the transitions between stages and rapid evolutionary stages).\nThe information about the varying abundances of nuclear species over time, along with the equations of state, is sufficient for a numerical solution by taking sufficiently small time increments and using iteration to find the unique internal structure of the star at each stage.\n\nThe SSM serves two purposes:\n\nLike the Standard Model of particle physics and the standard cosmology model the SSM changes over time in response to relevant new theoretical or experimental physics discoveries.\n\nAs described in the Sun article, the Sun has a radiative core and a convective outer envelope. In the core, the luminosity due to nuclear reactions is transmitted to outer layers principally by radiation. However, in the outer layers the temperature gradient is so great that radiation cannot transport enough energy. As a result, thermal convection occurs as thermal columns carry hot material to the surface (photosphere) of the Sun. Once the material cools off at the surface, it plunges back downward to the base of the convection zone, to receive more heat from the top of the radiative zone.\n\nIn a solar model, as described in stellar structure, one considers the density formula_2, temperature T(r), total pressure (matter plus radiation) P(r), luminosity l(r) and energy generation rate per unit mass ε(r) in a spherical shell of a thickness dr at a distance r from the center of the star.\n\nRadiative transport of energy is described by the radiative temperature gradient equation:\nwhere κ is the opacity of the matter, σ is the Stefan-Boltzmann constant, and the Boltzmann constant is set to one.\n\nConvection is described using mixing length theory and the corresponding temperature gradient equation (for adiabatic convection) is:\nwhere γ = c / c is the adiabatic index, the ratio of specific heats in the gas. (For a fully ionized ideal gas, γ = 5/3.)\n\nNear the base of the Sun's convection zone, the convection is adiabatic, but near the surface of the Sun, convection is not adiabatic.\n\nA more realistic description of the uppermost part of the convection zone is possible through detailed three-dimensional and time-dependent hydrodynamical simulations, taking into account radiative transfer in the atmosphere. Such simulations successfully reproduce the observed surface structure of solar granulation, as well as detailed profiles of lines in the solar radiative spectrum, without the use of parametrized models of turbulence. The simulations only cover a very small fraction of the solar radius, and are evidently far too time-consuming to be included in general solar modeling. Extrapolation of an averaged simulation through the adiabatic part of the convection zone by means of a model based on the mixing-length description, demonstrated that the adiabat predicted by the simulation was essentially consistent with the depth of the solar convection zone as determined from helioseismology. An extension of mixing-length theory, including effects of turbulent pressure and kinetic energy, based on numerical simulations of near-surface convection, has been developed.\n\nThis section is adapted from the Christensen-Dalsgaard review of helioseismology, Chapter IV.\n\nThe numerical solution of the differential equations of stellar structure requires equations of state for the pressure, opacity and energy generation rate, as described in stellar structure, which relate these variables to the density, temperature and composition.\n\nHelioseismology is the study of the wave oscillations in the Sun. Changes in the propagation of these waves through the Sun reveal inner structures and allow astrophysicists to develop extremely detailed profiles of the interior conditions of the Sun. In particular, the location of the convection zone in the outer layers of the Sun can be measured, and information about the core of the Sun provides a method, using the SSM, to calculate the age of the Sun, independently of the method of inferring the age of the Sun from that of the oldest meteorites. This is another example of how the SSM can be refined.\n\nHydrogen is fused into helium through several different interactions in the Sun. The vast majority of neutrinos are produced through the pp chain, a process in which four protons are combined to produce two protons, two neutrons, two positrons, and two electron neutrinos. Neutrinos are also produced by the CNO cycle, but that process is considerably less important in the Sun than in other stars.\n\nMost of the neutrinos produced in the Sun come from the first step of the pp chain but their energy is so low (<0.425 MeV) they are very difficult to detect. A rare side branch of the pp chain produces the \"boron-8\" neutrinos with a maximum energy of roughly 15 MeV, and these are the easiest neutrinos to detect. A very rare interaction in the pp chain produces the \"hep\" neutrinos, the highest energy neutrinos predicted to be produced by the Sun. They are predicted to have a maximum energy of about 18 MeV.\n\nAll of the interactions described above produce neutrinos with a spectrum of energies. The electron capture of Be produces neutrinos at either roughly 0.862 MeV (~90%) or 0.384 MeV (~10%).\n\nThe weakness of the neutrino's interactions with other particles means that most neutrinos produced in the core of the Sun can pass all the way through the Sun without being absorbed. It is possible, therefore, to observe the core of the Sun directly by detecting these neutrinos.\n\nThe first experiment to successfully detect cosmic neutrinos was Ray Davis's chlorine experiment, in which neutrinos were detected by observing the conversion of chlorine nuclei to radioactive argon in a large tank of perchloroethylene. This was a reaction channel expected for neutrinos, but since only the number of argon decays was counted, it did not give any directional information, such as where the neutrinos came from. The experiment found about 1/3 as many neutrinos as were predicted by the standard solar model of the time, and this problem became known as the solar neutrino problem.\n\nWhile it is now known that the chlorine experiment detected neutrinos, some physicists at the time were suspicious of the experiment, mainly because they did not trust such radiochemical techniques. Unambiguous detection of solar neutrinos was provided by the Kamiokande-II experiment, a water Cerenkov detector with a low enough energy threshold to detect neutrinos through neutrino-electron elastic scattering. In the elastic scattering interaction the electrons coming out of the point of reaction strongly point in the direction that the neutrino was travelling, away from the Sun. This ability to \"point back\" at the Sun was the first conclusive evidence that the Sun is powered by nuclear interactions in the core. While the neutrinos observed in Kamiokande-II were clearly from the Sun, the rate of neutrino interactions was again suppressed compared to theory at the time. Even worse, the Kamiokande-II experiment measured about 1/2 the predicted flux, rather than the chlorine experiment's 1/3.\n\nThe solution to the solar neutrino problem was finally experimentally determined by the Sudbury Neutrino Observatory (SNO). The radiochemical experiments were only sensitive to electron neutrinos, and the signal in the water Cerenkov experiments was dominated by the electron neutrino signal. The SNO experiment, by contrast, had sensitivity to all three neutrino flavours. By simultaneously measuring the electron neutrino and total neutrino fluxes the experiment demonstrated that the suppression was due to the MSW effect, the conversion of electron neutrinos from their pure flavour state into the second neutrino mass eigenstate as they passed through a resonance due to the changing density of the Sun. The resonance is energy dependent, and \"turns on\" near 2MeV. The water Cerenkov detectors only detect neutrinos above about 5MeV, while the radiochemical experiments were sensitive to lower energy (0.8MeV for chlorine, 0.2MeV for gallium), and this turned out to be the source of the difference in the observed neutrino rates at the two types of experiments.\n\nAll neutrinos from the proton–proton chain reaction (PP neutrinos) have been detected except hep neutrinos (next point). Three techniques have been adopted: The radiochemical technique, used by Homestake, Gallex, GNO and SAGE allowed to measure the neutrino flux above a minimum energy. The detector SNO used scattering on deuterium that allowed to measure the energy of the events, thereby identifying the single components of the predicted SSM neutrino emission. Finally, Kamiokande, Super-Kamiokande, SNO, Borexino and KamLAND used elastic scattering on electrons, which allows the measurement of the neutrino energy. Boron8 neutrinos have been seen by Kamiokande, Super-Kamiokande, SNO, Borexino, KamLAND. Beryllium7, pep, and PP neutrinos have been seen only by Borexino to date.\n\nThe highest energy neutrinos have not yet been observed due to their small flux compared to the boron-8 neutrinos, so thus far only limits have been placed on the flux. No experiment yet has had enough sensitivity to observe the flux predicted by the SSM.\n\nNeutrinos from the CNO cycle of solar energy generation – i.e., the CNO-neutrinos – are also expected to provide observable events below 1 MeV. They have not yet been observed due to experimental noise (background). Ultra-pure scintillator detectors have the potential to probe the flux predicted by the SSM. This detection could be possible already in Borexino; the next scientific occasions will be in SNO+ and, on the longer term, in LENA and JUNO, three detectors that will be larger but will use the same principles of Borexino.\n\nWhile radiochemical experiments have in some sense observed the pp and Be7 neutrinos they have measured only integral fluxes. The \"holy grail\" of solar neutrino experiments would detect the Be7 neutrinos with a detector that is sensitive to the individual neutrino energies. This experiment would test the MSW hypothesis by searching for the turn-on of the MSW effect. Some exotic models are still capable of explaining the solar neutrino deficit, so the observation of the MSW turn on would, in effect, finally solve the solar neutrino problem.\n\nThe flux of boron-8 neutrinos is highly sensitive to the temperature of the core of the Sun, formula_5. For this reason, a precise measurement of the boron-8 neutrino flux can be used in the framework of the standard solar model as a measurement of the temperature of the core of the Sun. This estimate was performed by Fiorentini and Ricci after the first SNO results were published, and they obtained a temperature of formula_6 from a determined neutrino flux of 5.2·10/cm·s.\n\nStellar models of the Sun's evolution predict the solar surface chemical abundance pretty well except for lithium (Li).\nThe surface abundance of Li on the Sun is 140 times less than the protosolar value (i.e. the primordial abundance at the Sun's birth), yet the temperature at the base of the surface convective zone is not hot enough to burn – and hence deplete – Li. This is known as the solar lithium problem. A large range of Li abundances is observed in solar-type stars of the same age, mass, and metallicity as the Sun. Observations of an unbiased sample of stars of this type with or without observed planets (exoplanets) showed that the known planet-bearing stars have less than one per cent of the primordial Li abundance, and of the remainder half had ten times as much Li. It is hypothesised that the presence of planets may increase the amount of mixing and deepen the convective zone to such an extent that the Li can be burned. A possible mechanism for this is the idea that the planets affect the angular momentum evolution of the star, thus changing the rotation of the star relative to similar stars without planets; in the case of the Sun slowing its rotation. More research is needed to discover where and when the fault in the modelling lies. Given the precision of helioseismic probes of the interior of the modern-day Sun, it is likely that the modelling of the protostellar Sun needs to be adjusted.\n\n\n"}
{"id": "53181058", "url": "https://en.wikipedia.org/wiki?curid=53181058", "title": "Surface flotillas of the Kriegsmarine", "text": "Surface flotillas of the Kriegsmarine\n\nSurface flotillas of the Kriegsmarine were organizational groupings of German naval vessels during World War II based on class of vessel and geographical location. Surface flotillas were not operationally deployed units, but functioned through the administrative command chain of the Kriegsmarine.\n\nSurface flotillas were first formed in 1938 from preexisting surface divisions, some of which had existed since before the First World War. The surface flotillas were primarily used for mid to smaller class vessels, such as destroyers and minesweepers, while capital ships and heavy cruisers were considered \"stand alone\" vessels answering directly to a vessel type commander. Surface flotilla commanders reported to a type commander for their particular class of vessel. The Fleet commander of the Kriegsmarine, to which the vessel type commanders answered, was the highest authority for all flotillas.\n\nSubmarine flotillas were unique in that they were considered operationally deployed commands and answered directly to the Commander of Submarines, oftentimes bypassing other chains of command, including the Fleet commander of the Kriegsmarine. Intermediary regional commands also existed for most U-boat flotillas. Miniature submarines were under the command of the German Navy's special operations branch.\n\nLate in World War II, the German Navy began consolidating several types of harbor defense units, along with their associated flotillas, into a series of naval security divisions. In most cases, the original surface flotillas were then placed as subordinate units to the security divisions.\n\nGerman destroyer flotillas were the most recognizable surface units as they were organized along classic naval lines with a designated flotilla flagship and commander. Unlike most of the other surface flotilla commands, which operated out of shore offices, destroyer flotillas commanders were embarked onboard flagship vessels (known as \"Die Führerzerstörer\") and would occasionally put to sea along with the vessels of their command. The destroyer \"Leberecht Maass\" (Z1) was the first flotilla's designated flagship in 1939, later replaced by the \"Wilhelm Heidkamp\" (Z21) and then the Z28 in 1943.\n\nThere were eight destroyer flotillas authorized by the Kriegsmarine during the Second World War, of which seven were eventually established. The standard rank for a destroyer flotilla commander (\"Flottillenchef\") was \"Kapitän zur See\", although some flotillas were commanded by officers ranked as \"Fregattenkapitän\". Each flotilla staff also contained an engineering officer (\"Die Flottilleningenieure\") who was normally a \"Korvettenkapitän\". In tactical situations, destroyer flotillas commanders were often designated as small group leaders, as was the case during the invasion of Norway.\n\nList of Destroyer flotillas\n\nDuring the Second World War, the Kriegsmarine authorized the formation of ten flotillas for the \"auxiliary destroyers\", or German torpedo boats. The torpedo boat flotillas were formed from preexisting commands of the Reichsmarine, known as \"Torpedobootsflottille\", several of which had been founded during or before World War I. Of the ten authorized World War II torpedo boat flotillas, nine were commissioned (\"8.Torpedobootsflottille\" was authorized but never formed).\n\nList of Torpedo-boat flotillas\n\nEscort flotillas were the smallest number of surface units to be created, due in part to the Kriegsmarine limited use of the destroyer escort type vessel. The Kriegsmarine would only construct ten F-class escort ships which were originally grouped into two escort flotillas (\"1. und 2. Geleitflottille\") formed in 1937 and 1938 respectively. In September 1942, the Kriegsmarine authorized two additional escort flotillas and formed the \"31. Geleitflottille\" to which were assigned fifteen \"auxiliary patrol ships\", with hull numbers G-3101 through G-3115. In the summer of 1943, the Kriegsmarine further formed the \"30. Geleitflottille\" to which were assigned thirteen auxiliary escort ships (mostly converted civilian craft) with hull designations \"D\", \"FZ\", and \"MR\".\n\nIn March 1943, the Kriegsmarine overhauled the escort flotillas and ordered the formation of five new flotillas (1 through 5 \"Geleitflottille\") utilizing foreign boats of the \"Torpedoboot Ausland\" program, \"G class\" auxiliary patrol ships, as well as auxiliary cutter ships designated by hull number \"SG\". The original F class escort ships were interspersed into the new flotillas, with the bulk of these ships assigned to the \"5. Geleitflottille\".\n\nThe final order of battle for the escort flotillas was:\n\n\nMinesweeper flotillas were organized into over forty flotillas for the regular minesweepers, as well as thirty five auxiliary minesweeper units. A third category of flotilla existed for the \"Sperrbrecher\" \"mine barrage\" craft. The first minesweeper flotilla of the Kriegsmarine was formed in 1936 from pre-existing units of the Reichsmarine which had maintained two minesweeper and one auxiliary minesweeper flotilla during the inter-war years. The first auxiliary minesweeper flotilla was formed shortly afterwards in October 1937.\n\nMinesweeper flotillas were considered administrative units, operating from shore offices, and did not maintain a standard flagship as was the case with the destroyer flotillas. The standard rank for a minesweeper flotillas commander was that of \"Korvettenkapitän\".\n\nThe Kriegsmarine authorized twenty five \"Schnellboot\" flotillas for formation during the Second World War, of which fourteen were eventually established. The German Navy also maintained an attack boat training division (\"Schnellboots-Lehr-Division\") consisting of four training flotillas. In November 1943, the 3rd, 7th, 21st, 22nd, and 24th S-boat flotillas were consolidated into the 1st S-Boat Division (\"1. Schnellbootsdivision\").\n\nThe standard S-boat flotilla consisted of six attack boats, although some flotillas operated in excess of twelve. Flotillas were commanded by officers ranked as \"Kapitänleutnant\" while S-boat divisions were commanded by those ranked \"Fregattenkapitän\".\n\n\"Vorpostenboote\" were auxiliary naval vessels, often converted civilian craft for use as harbor patrol and sentry vessels. These patrol boats were organized into thirty five primary flotillas covering the various German naval ports. An additional two \"security flotillas\" (the 7th and 13th \"Sicherungsflotille\") were formed in early 1943 for general port security duties.\n\nPatrol boat flotillas were strictly administrative and operated from shore offices, most often commanded by an officer ranked as either \"Kapitänleutnant\" or \"Korvettenkapitän\".\n\nSubmarine chasers were designated as \"UJ\" class vessels (\"U-Bootjäger\") and were augmented by \"service war craft\" (KT class vessels) which were usually smaller converted ships utilized as ASW trawlers. Beginning in 1940, German minelayers (\"Minenschiffe\") were also administratively attached to the submarine chaser flotillas. The standard rank for a submarine chayser flotilla commander was \"Korvettenkapitän\".\n\nList of submarine chaser flotillas\n\n\nIn 1943, a \"13th UJ Group\" (\"13. UJ-Gruppe\") had also been formed from vessels of the smaller flotilla units.\n\nMinelayers attached to submarine chaser units\n\n\nKnown as \"Netzsperrflottille\", these units were composed of small craft designed to mount submarine netting within German harbors. Submarine net flotillas were typically commanded by an officer ranked as \"Korvettenkapitän\". A typical flotilla would be composited of between three and five \"net layer\" vessels and upwards of twelve to fifteen \"net tender\" vessels. Submarine net flotillas were designated by geographical regions; for larger regions, flotillas could be sub-divided into smaller submarine net groups (\"Netzsperrgruppe\"). Some of the large submarine netting groups were further subdivided into \"Arbeitsgebiet\" (Working Areas) to cover smaller ports and harbors.\n\nList of submarine net flotillas\n\n\nIn March 1943, a new unit known as \"Netzsperrgruppe Dänemark\" was formed strictly for submarine netting activities in Denmark. Units of the command were reallocated from \"Netzsperrflottille Mitte\".\n\nTransport flotillas were administrative units designed to oversee the maintenance and deployment of German Navy water transport craft, such as ferries and barges. The standard rank for a transport flotilla commander was \"Kapitänleutnant\".\n\nList of Transport Flotillas\n\n\nIn 1944, three additional transport flotillas were formed to administrative waterborne transport and landing craft for specific geographic regions. \"Transportflottille Niederlande\" was the first to be formed in September 1944 for transport craft in the Netherlands. This was followed by \"Transportflottille Niederrhein\" and \"Transportflottille Ruhr\". In the summer of 1944, the Kriegsmarine also formed a Baltic Sea transport unit known as \"Transportflottille der KMD Danzig\". The final transport unit formed was \"Fährflottille Waal\", created in September 1944, to oversee barges and ferries on the Waal River.\n\nGerman Navy landing craft (\"Marinefährprahm\") were grouped into twenty separate flotillas (\"Landungsflottillen\") during the Second World War with the first landing craft flotilla established in November 1941 for operations in the Black Sea. In tactical situations, transport craft were deployed by the Navy groups as part of landing operations. Each landing craft flotilla was assigned between twenty and thirty separate craft (designated as \"F\" hull numbered vessels), although some early flotillas had as little as eight to ten assigned vessels. The standard landing craft flotiulla commander rank was either \"Oberleutnant zur See\" or \"Kapitänleutnant\".\n\nGerman motorboats and inland waterway craft were placed under Kriegsmarine naval control in 1941 and formed into three flotillas. The original units were the \"Donauflottille\", \"Rheinflottille\", and the \"Flußräumflottille Niederlande\". These units administrated German motorboats on the Danube, the Rhine and within the territorial waters of the Netherlands. The units were slightly reorganized in 1942 when the Danube flotilla was renamed as the \"Maasflottille\". The standard rank for a service craft flotilla commander was \"Kapitänleutnant\" with the flotilla commanders answering to the Leader of Motorboats.\n\nGerman artillery craft, which were essentially manned artillery and anti-aircraft guns mounted on ocean and river barges, were grouped into eight separate flotillas (\"Artillerieträger-Flottillen\") during the Second World War. These units had originally been formed in 1940 as two separate \"Flakjäger-Gruppe\" before being re-designated as \"Flakjäger-Flottille\" in 1941. The following year, the units were again reorganized into artillery barge flotillas. \n\nThe standard rank for an artillery barge flotilla commander was \"Kapitänleutnant\" with the flotilla command administrative in nature and operating from shore offices. The standard compliment of an artillery barge flotilla was between fifteen and eighteen vessels, designated as \"AF\" craft.\n\nIn February 1943, a command for Black Sea artillery barges was formed in Konstanza known as the \"3. Marineartillerieleichter-Flottille\". The flotilla was attached eight \"marine artillery ships\", designated by the hull prefix MAL. By 1943, the command had been renamed as the \"Artillerieträger-Flottille Asowsches Meer\" (Artillery Barge Flotilla Sea of Azov) and now contained flak artillery vessels (designated hull number \"F\") as well as \"security artillery vessels\" designated with the hull prefix \"SAT\". In January 1945, the flotilla gained an additional water security craft listed as \"Motorprahm D 154\".\n\n\"Black Sea Artillery Flotilla Vessels\"\n\n"}
{"id": "843592", "url": "https://en.wikipedia.org/wiki?curid=843592", "title": "Terran Federation (Starship Troopers)", "text": "Terran Federation (Starship Troopers)\n\nThe Terran Federation is the fictional government of Earth and her space colonies in Robert A. Heinlein's 1959 science fiction novel \"Starship Troopers\". The \"Starship Troppers\"]] film adaptation uses the United Citizen Federation in its place.\n\nPassages in the book give some details on how the Terran Federation originated:\n\nAt the end of 20th century, national governments of the world collapsed due to the failure of \"unlimited democracies\", civil unrest and social workers and child psychologists, a \"pre-scientific pseudo-professional class\", banning corporal punishment, resulting in crime reaching endemic proportions.\n\nIllegal activity which took place all around the world, including in Russia and in the United Kingdom, brought down the North American Republic. In 1987, the resulting Russo-Anglo-American alliance became engaged in a war with the Chinese Hegemony. Shortly before the war's end, the \"Revolt of the Scientists\" tried to create a utopia through a coup d'état but soon failed. The war ended in 2130 with the humiliating Treaty of New Delhi, which made large concessions to the Hegemony. This treaty freed prisoners captured by the Russo-Anglo-American Alliance, but left 65,000 civilians (Japanese, Filipino and Russian) and two divisions of British Paratroopers (sentenced for political crimes) in Chinese incarceration, leaving them escape as the only way to freedom. The loss of the war (or rather, a negotiated peace on extremely unfavorable terms, somewhat like the Treaty of Versailles that ended World War I) left the West and Russia on the brink of anarchy.\n\nAfter the collapse of national governments, a group of veterans in Aberdeen, Scotland, formed a vigilante group to stop rioting and looting. They hanged a few people (including two veterans) and decided to only allow veterans to join their committee due to a mistrust of politicians. This contingency plan became routine after a couple of generations, and this group of vigilantes originated the Terran Federation. It is expressly stated that this was never intended to be a \"coup d'état\" and was more comparable to the Russian Revolution: one system collapsed on its own and another rose to fill its place.\n\nThe Terran Federation has a multicultural society that votes for a global leader, similar to a representative democracy. However people of higher levels of authority also have to suffer tougher repercussions of their actions: e.g. a lieutenant could hang for making a mistake that a private would merely be dismissed and maybe lashed for. Corporal and capital punishment are practiced by the government as well as spanking children being standard use amongst the population.\n\nA \"History and Moral philosophy\" instructor in the Army says \"personal freedom for all is [the] greatest in history, laws are few, taxes are low, living standards are as high as productivity permits, crime is at its lowest ebb.\"\n\nThe people of the Terran Federation are either \"Citizens\" or \"Civilians\". Everyone is born a \"Civilian\", and at age 18 every \"Civilian\" has the right to enroll for a minimal 2-year term of \"Federal Service\". After completing a term of Federal Service \"Civilians\" become \"Citizens\" and gain the right to vote.\n\nIn theory a completed term of Federal Service ensures a \"Citizen\" is willing to put the needs of the community before their own personal well-being. This is because Federal Service is tough and dangerous (by design). It can involve joining the military, being a human guinea pig, testing survival equipment, or manual labour. The Federation makes it quite easy to quit a term of service before completion (even during war-time), but once someone has quit they are never allowed to enroll again. This is to ensure that all volunteers are dedicated, whilst also discouraging people from leaving.\n\nThe Federation makes the opportunity of Federal Service open to everyone, able-bodied or not. A doctor giving a medical examination says \"if you came in here in a wheelchair and blind in both eyes and were silly enough to insist on enrolling, they would find you something silly to match. Counting the fuzz on a caterpillar by touch, maybe.\" The only impediment that can render one ineligible for federal service is if a psychiatrist determines that one cannot understand the oath of service.\n\n\"Civilians\" are neither discriminated against, nor deprived of legal rights other than that of the ballot. Several examples from the book bear this out, particularly the fact that Juan Rico's family is prosperous and lacks for nothing save the right to vote (which Rico's father regards as \"useless\" anyway).\n\nThe Terran Federation's military appears to be divided into an Army and a Navy (that uses spacecraft instead of ships and doubles as an air force). The dozens of non-combat and support branches include units for logistics, biological/chemical weapons development, and terraforming. The \"Sky Marshal\" commands the entire military. To be eligible for the post of Sky Marshal, an officer must reach certain high ranks in both the Army and the Navy.\n\nOnce in the military a volunteer has the choice to \"go career\", choosing to devote 20 years of service to the Federation instead of the usual two years required to gain Citizenship. After these 20 years they can then leave and get a \"reserved job\", for example in the police. If they quit before then, after choosing the career path, the Federation is less supportive to them in the Civilian world. A minimal 2-year service can be extended if the Federation deems it necessary, as explicitly stated in the \"Service Oath\" taken upon enrollment.\n\nDistinctive military units include:\n\n\nThe Navy has powerful weapons capable of destroying planets (e.g. \"Nova Bombs\").\nA majority of ship-board ranks are filled by females. The primary role of the Navy involves providing transportation for infantry forces.\n\nHeinlein suggests that \"revolution is impossible\" in the Terran Federation: stability ensues from arming aggressive types as \"sheepdogs\" while \"the sheep will never give you any trouble\".\nAccording to Franklin, \"[t]he underlying premise of the new social order is that the only people fit to govern the state are those willing to sacrifice their lives for the state\".\n\nDonald M. Hassler and Clyde Wilcox describe the Terran Federation as \"Heinlein's last attempt to articulate a perfect government\" and as \"the ultimate embrace of both military and democratic ideas within a single state. This fantasy utopia lends itself to a historical comparison of the positive effects of the military on liberty and personal freedom. Such a comparison will also highlight the inherent potential threat to liberty the military poses simply by its existence. If, in this idealized state militarism can be recognized as an ever present menace, then, \"a fortiori\", the danger it represents is even more critical for existing democracies. Broadly drawn, the Terran Federation is a liberal, representative democracy.\"\n\nStephen E. Andrews and Nick Rennison note that the \"Terran Federation, which Heinlein clearly expects us to admire, is said by his most extreme detractors to be analogous to Nazi Germany.\"\n\nIn the film adaptation, the United Citizen Federation (UCF) was used in place of the Terran Federation and has many of the hallmarks of a fascist government, including heavy-handed propaganda, the demonization of an external foe, the centralized organization of society for total war, military (or militarized) and not civilian leadership, and omnipresent uniforms.\n\nThe formation of the UCF is not fully explained in the movie. However, dialogue at the beginning of the film suggests its formation was similar to the Terran Federation (albeit more bloody).\n\nIn order to vote in the UCF (and its novel counterpart), one must provide \"Federal Service\" by serving in the military or other service. The social structure of the world is divided between citizens (people who have served) and civilians (people who haven't served).\n\n"}
{"id": "27656682", "url": "https://en.wikipedia.org/wiki?curid=27656682", "title": "Thermo-dielectric effect", "text": "Thermo-dielectric effect\n\nThe thermo-dielectric effect is the production of electric currents and charge separation during phase transition.\n\nThis interesting effect was discovered by Joaquim da Costa Ribeiro in 1944. The Brazilian physicist observed that solidification and melting of many dielectrics are accompanied by charge separation. A thermo-dielectric effect was demonstrated with carnauba wax, naphthalene and paraffin. Charge separation in ice was also expected. This effect was observed during water freezing period, electrical storm effects can be caused by this strange phenomenon. Effect was measured by many researches - Bernhard Gross, Armando Dias Tavares, Sergio Mascarenhas etc. César Lattes (co-discoverer of the pion) supposed that this was the only effect ever to be discovered entirely in Brasil.\n\n\n"}
{"id": "36997969", "url": "https://en.wikipedia.org/wiki?curid=36997969", "title": "Thyristor switched capacitor", "text": "Thyristor switched capacitor\n\nA thyristor switched capacitor (TSC) is a type of equipment used for compensating reactive power in electrical power systems. It consists of a power capacitor connected in series with a bidirectional thyristor valve and, usually, a current limiting reactor (inductor). The thyristor switched capacitor is an important component of a Static VAR Compensator (SVC), where it is often used in conjunction with a thyristor controlled reactor (TCR). Static VAR compensators are a member of the Flexible AC transmission system (FACTS) family.\n\nA TSC is usually a three-phase assembly, connected either in a delta or a star arrangement. Unlike the TCR, a TSC generates no harmonics and so requires no filtering. For this reason, some SVCs have been built with only TSCs . This can lead to a relatively cost-effective solution where the SVC only requires capacitive reactive power, although a disadvantage is that the reactive power output can only be varied in steps. Continuously variable reactive power output is only possible where the SVC contains a TCR or another variable element such as a STATCOM.\n\nUnlike the TCR, the TSC is only ever operated fully on or fully off. An attempt to operate a TSC in ‘’phase control’’ would result in the generation of very large amplitude resonant currents, leading to overheating of the capacitor bank and thyristor valve, and harmonic distortion in the AC system to which the SVC is connected.\n\nWhen the TSC is on, or ‘’deblocked’’, the current leads the voltage 90° (as with any capacitor). The rms current is given by:\n\nformula_1\n\nWhere:\n\nformula_2\n\nV is the rms value of the line-to-line busbar voltage to which the SVC is connected\n\nC is the total TSC capacitance per phase\n\nL is the total TSC inductance per phase\n\nf is the frequency of the AC system\n\nThe TSC forms an inductor-capacitor (LC) resonant circuit with a characteristic frequency of :\n\nformula_3\n\nThe tuned frequency is usually chosen to be in the range 150-250Hz on 60Hz systems or 120-210Hz on 50Hz systems. It is an economic choice between the size of the TSC reactor (which increases with decreasing frequency) and the need to protect the thyristor valve from excessive oscillatory currents when the TSC is turned on at an incorrect point of wave (‘’misfiring’’).\n\nThe TSC is usually tuned to a non-integer harmonic of the mains frequency so as to avoid the risk of the TSC being overloaded by harmonic currents flowing into it from the AC system.\n\nWhen the TSC is switched off, or ‘’blocked’’, no current flows and the voltage is supported by the thyristor valve. After the TSC has been switched off for a long time (hours) the capacitor will be fully discharged, and the thyristor valve will experience only the AC voltage of the SVC busbar. However, when the TSC turns off, it does so at zero current, corresponding to peak capacitor voltage. The capacitor only discharges very slowly, so the voltage experienced by the thyristor valve will reach a peak of more than twice the peak AC voltage, about half a cycle after blocking. The thyristor valve needs to contain enough thyristors in series to withstand this voltage safely.\n\nWhen the TSC is turned on (\"deblocked\") again, care must be taken to choose the correct instant in order to avoid creating very large oscillatory currents. Since the TSC is a resonant circuit, any sudden shock excitation will produce a high frequency ringing effect which could damage the thyristor valve. \n\nThe optimum time to turn on a TSC is when the capacitor is still charged to its normal peak value and the turn-on command is sent at the minimum of valve voltage. If the TSC is deblocked at this point, the transition back into the conducting state will be smooth.\n\nSometimes, however, the TSC may turn on at an incorrect instant (as a result of a control or measurement fault), or the capacitor may become charged to a voltage above the normal value so that even at the minimum of valve voltage, a large transient current results. The current in the TSC will then consist of a fundamental-frequency component (50 Hz or 60 Hz) superimposed on a much larger current at the tuned frequency of the TSC. This transient current can take hundreds of milliseconds to die away, during which time the cumulative heating in the thyristors may be excessive.\n\nA TSC normally comprises three main items of equipment: the main capacitor bank, the thyristor valve and a current-limiting reactor, which is usually air-cored.\n\nThe largest item of equipment in a TSC, the capacitor bank, is constructed from rack-mounted outdoor capacitor units, each unit typically having a rating in the range 500 – 1000 kilovars (kVAr).\n\nThe function of the TSC reactor is to limit the peak current and rate of rise of current (di/dt) when the TSC turns on at an incorrect time. The reactor is usually an air-cored reactor, similar to that of a TCR, but smaller. The size and cost of the TSC reactor is heavily influenced by the tuning frequency of the TSC, lower frequencies requiring larger reactors. \n\nThe TSC reactor is usually located outside, close to the main capacitor bank.\n\nThe thyristor valve typically consists of 10-30 inverse-parallel-connected pairs of thyristors connected in series. The inverse-parallel connection is needed because most commercially available thyristors can conduct current in only one direction. The series connection is needed because the maximum voltage rating of commercially available thyristors (up to approximately 8.5 kV) is insufficient for the voltage at which the TCR is connected. For some low-voltage applications, it may be possible to avoid the series-connection of thyristors; in such cases the thyristor valve is simply an inverse-parallel connection of two thyristors.\n\nIn addition to the thyristors themselves, each inverse-parallel pair of thyristors has a resistor–capacitor ‘’snubber’ circuit connected across it, to force the voltage across the valve to divide uniformly amongst the thyristors and to damp the \"commutation overshoot\" which occurs when the valve turns off. \n\nThe thyristor valve for a TSC is very similar to that of a TCR, but (for a given AC voltage) generally has between 1.5 and 2 times as many thyristors connected in series because of the need to withstand both the AC voltage and the trapped capacitor voltage after blocking.\n\nThe thyristor valve is usually installed in a purpose-built, ventilated building, or a modified shipping container. Cooling for the thyristors and snubber resistors is usually provided by deionised water.\n\nSome TSCs have been built with the capacitor and inductor arranged not as a simple tuned LC circuit but rather as a damped filter. This type of arrangement is useful when the power system to which the TSC is connected contains significant levels of background harmonic distortion, or where there is a risk of resonance between the power system and the TSC.\n\nIn several “Relocatable SVCs” built for National Grid (Great Britain), three TSCs of unequal size were provided, in each case with the capacitor and inductor arranged as a “C-type” damped filter. In a C-type filter, the capacitor is split into two series-connected sections. A damping resistor is connected across one of the two capacitor sections and the inductor, the tuned frequency of this section being equal to the grid frequency. In this way, damping is provided for harmonic frequencies but the circuit incurs no power loss at grid frequency.\n\n"}
{"id": "39924092", "url": "https://en.wikipedia.org/wiki?curid=39924092", "title": "Wenlock epoch", "text": "Wenlock epoch\n\nThe Wenlock (sometimes referred to as the \"Wenlockian\") is the second series of the Silurian. It is preceded by the Llandovery series and followed by the Ludlow. Radiometric dates constrain the Wenlockian between and million years ago.<ref name=\"Stratchart 2013/01\"></ref>\n\nThe Wenlock is named after Wenlock Edge, an outcrop of rocks near the town of Much Wenlock in Shropshire (West Midlands, United Kingdom). The name was first used in the term \"Wenlock and Dudley rocks\" by Roderick Murchison in 1834 to refer to the limestones and underlying shales that underlay what he termed the \"Ludlow rocks\". He later modified this term to simply the \"Wenlock rocks\" in his book, \"The Silurian System\" in 1839.\n\nThe Wenlock's beginning is defined by the lower boundary (or GSSP) of the Sheinwoodian. The end is defined as the base (or GSSP) of the Gorstian.\n\nThe Wenlock is divided into the older Sheinwoodian and the younger Homerian stage. The Sheinwoodian lasted from to million years ago. The Homerian lasted from to million years ago.\n"}
{"id": "1228152", "url": "https://en.wikipedia.org/wiki?curid=1228152", "title": "Wet season", "text": "Wet season\n\nThe monsoon season is the time of year when most of a region's average annual rainfall occurs. Generally the season lasts at least a month. The term \"green season\" is also sometimes used as a euphemism by tourist authorities. Areas with wet seasons are dispersed across portions of the tropics and subtropics.\n\nUnder the Köppen climate classification, for tropical climates, a wet season month is defined as a month where average precipitation is or more. In contrast to areas with savanna climates and monsoon regimes, Mediterranean climates have wet winters and dry summers. Dry and rainy months are characteristic of tropical seasonal forests: in contrast to tropical rainforests, which do not have dry or wet seasons, since their rainfall is equally distributed throughout the year. Some areas with pronounced rainy seasons will see a break in rainfall mid-season, when the intertropical convergence zone or monsoon trough moves to higher latitudes in the middle of the warm season.\n\nWhen the wet season occurs during a warm season, or summer, precipitation falls mainly during the late afternoon and early evening. In the wet season, air quality improves, fresh water quality improves, and vegetation grows substantially, leading to crop yields late in the season. Rivers overflow their banks, and some animals retreat to higher ground. Soil nutrients diminish and erosion increases. The incidence of malaria increases in areas where the rainy season coincides with high temperatures, particularly in tropical areas. Some animals have adaptation and survival strategies for the wet season. Often, the previous dry season leads to food shortages in the wet season, as the crops have yet to mature.\n\nIn areas where the heavy rainfall is associated with a wind shift, the wet season is known as the monsoon. Rainfall in the wet season is mainly due to daytime heating which leads to diurnal thunderstorm activity within a pre-existing moist airmass, so the rain mainly falls in late afternoon and early evening in savannah and monsoon regions. Further, much of the total rainfall each day occurs in the first minutes of the downpour, before the storms mature into their stratiform stage. Most places have only one wet season, but areas of the tropics can have two wet seasons, because the monsoon trough, or Intertropical Convergence Zone, can pass over locations in the tropics twice per year. However, since rain forests have rainfall spread evenly through the year, they do not have a wet season.\n\nIt is different for places with a Mediterranean climate. In the western United States, during the cold season from September–May, extratropical cyclones from the Pacific Ocean move inland into the region due to a southward migration of the jet stream during the cold season. This shift in the jet stream brings much of the annual precipitation to the region, and sometimes also brings heavy rain and strong low pressure systems. The peninsula of Italy has weather very similar to the western United States in this regard.\n\nAreas with a savanna climate in Sub-Saharan Africa, such as Ghana, Burkina Faso, Darfur, Eritrea, Ethiopia, and Botswana have a distinct rainy season. Also within the savanna climate regime, Florida and South Texas have a rainy season. Monsoon regions include the Indian subcontinent, Southeast Asia (including Indonesia and Philippines), northern sections of Australia's North, Polynesia, Central America, western and southern Mexico, the Desert Southwest of the United States, southern Guyana, portions of northeast Brazil.\n\nNorthern Guyana has two wet seasons: one in early spring and the other in early winter. In western Africa, there are two rainy seasons across southern sections, but only one across the north. Within the Mediterranean climate regime, the west coast of the United States and the Mediterranean coastline of Italy, Greece, and Turkey experience a wet season in the winter months. Similarly, the wet season in the Negev desert of Israel extends from October through May. At the boundary between the Mediterranean and monsoon climates lies the Sonoran desert, which receives the two rainy seasons associated with each climate regime.\n\nThe wet season is known by many different local names throughout the world. For example, in Mexico it is known as \"storm season\". Different names are given to the various short \"seasons\" of the year by the Aboriginal tribes of Northern Australia: the wet season typically experienced there from December to March is called \"Gudjewg\". The precise meaning of the word is disputed, although it is widely accepted to relate to the severe thunderstorms, flooding, and abundant vegetation growth commonly experienced at this time.\n\nIn tropical areas, when the monsoon arrives, high daytime high temperatures drop and overnight low temperatures increase, thus reducing diurnal temperature variation. During the wet season, a combination of heavy rainfall and, in some places such as Hong Kong, an onshore wind, improve air quality. In Brazil, the wet season is correlated with weaker trade winds off the ocean. The pH level of water becomes more balanced due to the charging of local aquifers during the wet season. Water also softens, as the concentration of dissolved materials reduces during the rainy season. Erosion is also increased during rainy periods. Arroyos that are dry at other times of the year fill with runoff, in some cases with water as deep as . Leaching of soils during periods of heavy rainfall depletes nutrients. The higher runoff from land masses affects nearby ocean areas, which are more stratified, or less mixed, due to stronger surface currents forced by the heavy rainfall runoff.\n\nHigh rainfall can cause widespread flooding, which can lead to landslides and mudflows in mountainous areas. Such floods cause rivers to burst their banks and submerge homes. The Ghaggar-Hakra River, which only flows during India's monsoon season, can flood and severely damage local crops. Floods can be exacerbated by fires that occurred during the previous dry season, which cause soils which are sandy or composed of loam to become hydrophobic, or water repellent. In various ways governments may help people deal with wet season floods. Flood plain mapping identifies which areas are more prone to flooding. Instructions on controlling erosion through outreach are also provided by telephone or the internet.\n\nThe wet season is the main period of vegetation growth within the Savanna climate regime. However, this also means that wet season is a time for food shortages before crops reach their full maturity. This causes seasonal weight changes for people in developing countries, with a drop occurring during the wet season until the time of the first harvest, when weights rebound. Malaria incidence increases during periods of high temperature and heavy rainfall.\n\nCows calve, or give birth, at the beginning of the wet season. The onset of the rainy season signals the departure of the monarch butterfly from Mexico. Tropical species of butterflies show larger dot markings on their wings to fend off possible predators and are more active during the wet season than the dry season. Within the tropics and warmer areas of the subtropics, decreased salinity of near shore wetlands due to the rains causes an increase in crocodile nesting. Other species, such as the arroyo toad, spawn within the couple of months after the seasonal rains. Armadillos and rattlesnakes seek higher ground.\n\n"}
{"id": "21495990", "url": "https://en.wikipedia.org/wiki?curid=21495990", "title": "Zamzam Well", "text": "Zamzam Well\n\nThe Well of Zamzam () is a well located within the Masjid al-Haram in Mecca, Saudi Arabia, east of the Kabah, the holiest place in Islam. According to Islamic mythology, it is a miraculously generated source of water from God, which sprang thousands of years ago when Ibrahim's infant son ʾIsmaʿil was left with his mother Hajar in the desert, where he was thirsty and kept crying. Millions of pilgrims visit the well each year while performing the \"Hajj\" or \"Umrah\" pilgrimages in order to drink its water. The safety of Zamzam water for drinking has been the subject of debate.\n\nIslamic tradition states that the Zamzam Well was revealed to Hajar, the second wife of Ibrahim's and mother of ʾIsmaʿil. By the instruction of God, Ibrahim left his wife and son at a spot in the desert and walked away. She was desperately seeking water for her infant son, but she could not find any, as Mecca is located in a hot dry valley with few sources of water. Hajar ran seven times back and forth in the scorching heat between the two hills of Safa and Marwah, looking for water. Getting thirstier by the second, the infant Ishmael scraped the land with his feet, where suddenly water sprang out. There are other versions of the story involving God sending his angel, Gabriel (Jibra'il), who kicked the ground with his heel (or wing), and the water rose. A similar story about a well is also mentioned in the Bible.\n\nThe name of the well comes from the phrase \"Zomë Zomë\", meaning \"stop flowing\", a command repeated by Hagara during her attempt to contain the spring water.\n\nAccording to Islamic tradition, Abraham rebuilt the \"Bayt Allah\" (\"House of God\", cognate of the Hebrew-derived place name Bethel) near the site of the well, a building which had been originally constructed by Adam (Adem), and today is called the Kaaba, a building toward which Muslims around the world face in prayer, five times each day. The Zamzam Well is located approximately east of the Kaaba. In other Islamic tradition, Muhammad's heart was extracted from his body, washed with the water of Zamzam, and then was restored in its original position, after which it was filled with faith and wisdom.\n\nThe well originally had two cisterns in the first era, one for drinking and one for ablution. At that time, it was a simple well surrounded by a fence of stones. Then in the era of the Abbasid caliph Al-Mansur 771 AD (154/155 AH) a dome was built above the well, and it was tiled with marble. \n\nIn 775 AD (158/159 AH), Al-Mahdi rebuilt the well during his caliphate, and built a dome of teak which was covered with mosaic. One small dome covered the well, and a larger dome covered the room for the pilgrims. In 835 AD (220 AH) there was further restoration, and the dome was covered with marble during the caliphate of Al-Mu'tasim.\n\nIn 1417 (819/820 AH), during the time of the Mamluks, the mosque was damaged by fire, and required restoration. Further restoration occurred in 1430 (833/834 AH), and again in 1499 (904/95 AH) during the time of Sultan Qaitbay, when the marble was replaced.\n\nIn modern times, the most extensive restoration took place to the dome during the era of the Ottoman Sultan Abdul Hamid II in 1915 (1333/1334 AH). To facilitate crowd control, the building housing the Zamzam was moved away from its original location, to get it out of the way of the \"Tawaf\", when millions of pilgrims would circumambulate the Kaaba. The water of the well is now pumped to the eastern part of the mosque, where it was made available in separate locations for men and women.\n\nThe Zamzam well was excavated by hand, and is about deep and in diameter. It taps groundwater from the wadi alluvium and some from the bedrock. Originally water from the well was drawn via ropes and buckets, but today the well itself is in a basement room where it can be seen behind glass panels (visitors are not allowed to enter). Electric pumps draw the water, which is available throughout the Masjid al-Haram via water fountains and dispensing containers near the Tawaf area.\n\nHydrogeologically, the well is in the \"Wadi Ibrahim\" (Valley of Abraham). The upper half of the well is in the sandy alluvium of the valley, lined with stone masonry except for the top metre (3 ft) which has a concrete \"collar\". The lower half is in the bedrock. Between the alluvium and the bedrock is a section of permeable weathered rock, lined with stone, and it is this section that provides the main water entry into the well. Water in the well comes from absorbed rainfall in the Wadi Ibrahim, as well as run-off from the local hills. Since the area has become more and more settled, water from absorbed rainfall on the Wadi Ibrahim has decreased.\n\nThe Saudi Geological Survey has a \"Zamzam Studies and Research Centre\" which analyses the technical properties of the well in detail. Water levels were monitored by hydrograph, which in more recent times has changed to a digital monitoring system that tracks the water level, electric conductivity, pH, Eh, and temperature. All of this information is made continuously available via the Internet. Other wells throughout the valley have also been established, some with digital recorders, to monitor the response of the local aquifer system.\n\nZamzam water is colourless and odorless, but has a distinct taste, with a pH of 7.5–7.7, indicating that it is alkaline to some extent.\n\nThe British Food Standards Agency has in the past issued warnings about water claiming to be from the Zamzam Well containing dangerous levels of arsenic; such sales have also been reported in the United Arab Emirates (UAE), where it is illegal to sell Zamzam water. The Saudi government has prohibited the commercial export of Zamzam water from the kingdom. In May 2011, a BBC London investigation found that water taken from taps connected to the Zamzam Well contained high levels of nitrate, potentially harmful bacteria, and arsenic at levels three times the legal limit in the UK, the same levels found in illegal water purchased in the UK. Arsenic is a carcinogen, raising concerns that any who regularly consume commercial Zamzam water in large quantities may be exposed to higher risks of cancer.\n\nLater in that month the Council of British Hajjis stated that drinking Zamzam water was safe and disagreed with the BBC report. They also went on to point out that the Government of Saudi Arabia does not allow the export of Zamzam water for resale. Also stating that it was unknown if the water being sold in the UK was genuine, people should not buy it and report the sellers to the Trading Standards.\n\nThe BBC's findings have drawn mixed reactions from the Muslim community. Environmental health officer Dr Yunes Ramadan Teinaz told the British broadcaster about commercially marketed Zamzam water that, \"People see this water as a holy water. They find it difficult to accept that it is contaminated, but the authorities in Saudi Arabia or in the U.K. must take action.\" The Saudi authorities have stated that water from the well was tested by the Group Laboratories of CARSO-LSEHL in Lyon, licensed by the French Ministry of Health for the testing of drinking water. According to reports of these results, the level of arsenic in Zamzam water taken at its source is much lower than the maximum amount permitted by the World Health Organization. The Saudi authorities have thus said that the water is fit for human consumption. Zuhair Nawab, president of the Saudi Geological Survey (SGS), has claimed that the Zamzam Well is tested on a daily basis, in a process involving the taking of three samples from the well. These are said to be examined in the King Abdullah Zamzam Water Distribution Center in Mecca, which is equipped with advanced facilities. An article written in \"Skeptic\" magazine in 2017 states that the methodologies of testing used by Saudi authorities are incomplete, inadequate and biased.\n\n\n"}
{"id": "91556", "url": "https://en.wikipedia.org/wiki?curid=91556", "title": "Ziusudra", "text": "Ziusudra\n\nZiusudra ( \"Ziudsuřa(k)\" \"life of long days\"; ) or Zin-Suddu ( ) of Shuruppak (c. 2900 BC) is listed in the WB-62 Sumerian king list recension as the last king of Sumer prior to the great flood. He is subsequently recorded as the hero of the Sumerian creation myth, and is also known as the Hellenized 'Xisuthros' from the later writings of Berossus.\n\nZiusudra is one of several mythic characters that are protagonists of near-eastern Flood myths, including Atrahasis, Utnapishtim and the biblical Noah - although each story has distinctive elements, many key story elements are common to two, three, or four versions.\n\nIn the WB-62 Sumerian king list recension, Ziusudra, or Zin-Suddu of Shuruppak is listed as the last king of Sumer before a great flood. He is recorded as having reigned as both king and \"gudug\" priest for 10 \"sars\", or periods of 3,600 years, although this was probably a copy error for 10 years. In this version, Ziusudra inherited rulership from his father Šuruppak (written SU.KUR.LAM) who ruled for 10 \"sars\".\n\nThe line following Ziusudra in WB-62 reads: \"Then the flood swept over\". The next line reads: \"After the flood swept over, kingship descended from heaven; the kingship was in Kish\". The city of Kish flourished in the Early Dynastic period soon after an archaeologically attested river flood in Shuruppak (modern Tell Fara, Iraq) and various other Sumerian cities. This flood has been radiocarbon dated to ca. 2900 BC. Polychrome pottery from the Jemdet Nasr period (ca. 30th century BC) was discovered immediately below the Shuruppak flood stratum, and the Jemdet Nasr period immediately preceded the Early Dynastic I period.\n\nThe significance of Ziusudra's name appearing on the WB-62 king list is that it links the flood mentioned in the three surviving Babylonian deluge epics of Ziusudra (\"Eridu Genesis\"), Utnapishtim (\"Epic of Gilgamesh\"), and Atrahasis (\"Epic of Atra-Hasis\") to river flood sediments in Shuruppak, Uruk, Kish et al. that have been radiocarbon dated to ca. 2900 BC. This has led some scholars to conclude that the flood hero was king of Shuruppak at the end of the Jemdet Nasr period (ca. 3100–2900) which ended with the river flood of 2900 BC.\n\nZiusudra being a king from Shuruppak is supported by the Gilgamesh XI tablet making reference to Utnapishtim (Akkadian translation of the Sumerian name Ziusudra) with the epithet \"man of Shuruppak\" at line 23.\n\nThe tale of Ziusudra is known from a single fragmentary tablet written in Sumerian, datable by its script to the 17th century BC (Old Babylonian Empire), and published in 1914 by Arno Poebel. The first part deals with the creation of man and the animals and the founding of the first cities Eridu, Bad-tibira, Larak, Sippar, and Shuruppak. After a missing section in the tablet, we learn that the gods have decided to send a flood to destroy mankind. The god Enki (lord of the underworld sea of fresh water and Sumerian equivalent of Babylonian god Ea) warns Ziusudra, the ruler of Shuruppak, to build a large boat; the passage describing the directions for the boat is also lost. When the tablet resumes, it is describing the flood. A terrible storm raged for seven days, \"the huge boat had been tossed about on the great waters,\" then Utu (Sun) appears and Ziusudra opens a window, prostrates himself, and sacrifices an ox and a sheep. After another break, the text resumes, the flood is apparently over, and Ziusudra is prostrating himself before An (Sky) and Enlil (Lordbreath), who give him \"breath eternal\" and take him to dwell in Dilmun. The remainder of the poem is lost.\n\nThe Epic of Ziusudra adds an element at lines 258–261 not found in other versions, that after the river flood \"king Ziusudra ... they caused to dwell in the land of the country of Dilmun, the place where the sun rises\". In this version of the story, Ziusudra's boat floats down the Euphrates river into the Persian Gulf (rather than up onto a mountain, or up-stream to Kish). The Sumerian word \"KUR\" in line 140 of the Gilgamesh flood myth was interpreted to mean \"mountain\" in Akkadian, although in Sumerian, KUR did not mean \"mountain\" but rather \"land\", especially a foreign country.\n\nA Sumerian document known as the \"Instructions of Shuruppak\" dated by Kramer to about 2600 BC, refers in a later version to Ziusudra. Kramer stated \"Ziusudra had become a venerable figure in literary tradition by the middle of the third millennium B.C.\".\n\n\"Xisuthros\" (Ξισουθρος) is a Hellenization of the Sumerian Ziusudra, known from the writings of Berossus, a priest of Bel in Babylon, on whom Alexander Polyhistor relied heavily for information on Mesopotamia. Among the interesting features of this version of the flood myth, are the identification, through \"interpretatio graeca\", of the Sumerian god Enki with the Greek god Cronus, the father of Zeus; and the assertion that the reed boat constructed by Xisuthros survived, at least until Berossus' day, in the \"Corcyrean Mountains\" of Armenia. Xisuthros was listed as a king, the son of one Ardates, and to have reigned 18 \"sari\". One saros (\"shar\" in Akkadian) stands for 3600 and hence 18 \"sari\" was translated as 64,800 years. R. M. Best argued this was a mistranslation; the archaic U sign meaning year was confused with the \"sar\" sign which both have a 4-sided diamond shape and that Xisuthros actually reigned 18 years.\n\nZiusudra is also mentioned in other ancient literature, including \"The Death of Gilgamesh\" and \"The Poem of Early Rulers\", and a late version of \"The Instructions of Shuruppak\".\n\nAtrahasis (recorded in a 18th C. BC Akkadian myth) and Utnapishtim (recorded in the Epic of Gilgamesh, dating to the Neo-Sumerian 21st C. BC), as well as the biblical Noah are similar heroes of deluge myths of the ancient Near East.\n\nWith specific reference to Atrahasis - depending on source Atrahasis and Ziusudra are listed as son or grandson of the king Ubara-Tutu, and though the genealogies differ, brings the possibility to conflate the two.\n\nAlthough each version of the flood myth has distinctive story elements, there are numerous story elements that are common to two, three, or four versions. The earliest version of the flood myth is preserved fragmentarily in the \"Eridu Genesis\", written in Sumerian cuneiform and dating to the 17th century BC, during the 1st Dynasty of Babylon when the language of writing and administration was still Sumerian. Strong parallels are notable with other Near Eastern flood legends, such as the biblical account of Noah.\n\n\n\n"}
{"id": "52245941", "url": "https://en.wikipedia.org/wiki?curid=52245941", "title": "Zoka Forest", "text": "Zoka Forest\n\nZoka Forest is a natural tropical rain forest in the Northern Region of Uganda. The forest is a component of the larger East Moyo Wildlife Reserve.\n\nThe forest is in the southern part of Adjumani District, approximately south of Adjumani, the district capital. The coordinates of the forest are 03°01'03.0\"N, 31°39'21.0\"E (Latitude:3.017500; Longitude:31.655830).\n\nSituated in the Itirikwa and Ukusijoni sub-counties in the southern part of Adjumani District, the Zoka Central Forest Reserve measures about . It is the only natural tropical rain forest and the only natural forest resource in Adjumani District.\n\nIn August 2016, Prime Minister Ruhakana Rugunda launched an investigation into the plundering of the Zika Forest. The four-person team is led by Mary Karooro Okurut, the Minister for General Duties in the Office of the Prime Minister. Other team members include Lands, Housing and Urban Development Minister Betty Amongi, Minister of Water and Environment Sam Cheptoris, and Minister of State for Northern Uganda Grace Kwiyucwiny.\n\nThe National Forestry Authority (NFA) has in the past accused members of the Uganda People's Defence Force (UPDF) of illegally felling trees in the forest reserve and using UPDF trucks to transport the logs out of the forest. In October 2016, the resident commissioner of the Ajumani District told a press conference that she had photographs of UPDF trucks doing this. The trucks were followed to the Pabbo Army Detach in Amuru District, which is under the command of the 4th Division of the UPDF, headquartered in Gulu. The UPDF has mounted an investigation led by the 4th Division Commander, Brigadier Kayanja Muhanga.\n\nOn 4 October 2016, State Minister of Lands Persis Namuganza called a press conference to announce plans to de-gazette part of the Zoka Forest to allow the Madhvani Group to grow sugarcane in the area and relieve the contract farmers in the Busoga sub-region. The contract farmers in Busoga are starving because they neglected to grow food for their own sustenance and instead planted sugarcane.\n\nThe plan evoked immediate outrage from environmental groups, the NFA, politicians, and concerned citizens. Deputy Prime Minister Moses Ali, a member of parliament from the Adjumani District, called an impromptu meeting in which Minister of Lands, Housing and Urban Development Betty Amongi disowned the plan announced by Namuganza, her junior minister.\n\n"}
