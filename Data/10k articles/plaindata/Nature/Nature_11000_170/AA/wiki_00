{"id": "44248520", "url": "https://en.wikipedia.org/wiki?curid=44248520", "title": "2013 RH74", "text": "2013 RH74\n\n"}
{"id": "18679180", "url": "https://en.wikipedia.org/wiki?curid=18679180", "title": "Acoustic seabed classification", "text": "Acoustic seabed classification\n\nAcoustic seabed classification is the partitioning of a seabed acoustic image into discrete physical entities or classes. This is a particularly active area of development in the field of seabed mapping, marine geophysics, underwater acoustics and benthic habitat mapping. Seabed classification is one route to characterizing the seabed and its habitats. Seabed characterization makes the link between the classified regions and the seabed physical, geological, chemical or biological properties. Acoustic seabed classification is possible using a wide range of acoustic imaging systems including multibeam echosounders, sidescan sonar, single-beam echosounders, interferometric systems and sub-bottom profilers. Seabed classification based on acoustic properties can be divided into two main categories; surficial seabed classification and sub-surface seabed classification. Sub-surface imaging technologies use lower frequency sound to provide higher penetration, whereas surficial imaging technologies provide higher resolution imagery by utilizing higher frequencies (especially in shallow water).\n\nSurficial seabed classification is concerned primarily with distinguishing marine benthic habitat characteristics (e.g. hard, soft, rough, smooth, mud, sand, clay, cobble) of the surveyed area. Multibeam echosounders, sidescan sonar systems and acoustic ground discrimination systems (AGDS) are the most commonly used technologies. The use of optical sensors has been restricted to depths less than 40 m due to absorption of electromagnetic radiation by water. Despite this limitation, processing tools have been developed to classify data acquired using airborne bathymetric LiDAR systems. Nevertheless, acoustics remain the preferred method of imaging the seafloor because data can be acquired over a much larger area (than in-situ sampling) from almost any depth.\n\nMultibeam systems acquire both bathymetry (depth) and backscatter (intensity) data. Multibeam backscatter was previously considered to be a by-product of a multibeam survey, with bathymetry being the primary information. Recent advances in multibeam backscatter processing and analysis methods have increased the range of applications for which multibeam systems can be used. New methods of analyzing backscatter data, have increased its potential for seabed characterization. Backscatter data resolution has also increased significantly with the introduction of snippet data. Snippet data is raw backscatter time-series data for each beam footprint and each ping (Lockhart et. al., 2007). These advances have allowed some multibeam backscatter data to achieve a quality comparable to that of sidescan sonar imagery.\n\nDifferent classification approaches and algorithms can yield different results. These approaches include image-based seabed classification methods such as texture analysis, artificial neural networks (ANN); and other methods, such as angular response characterization (Hughes-Clarke et al., 1997). Image processing methods traditionally used in satellite remote sensing are often adapted to quantitatively analyze multibeam backscatter intensity data. After image segmentation and classification, acoustic imagery can be used to discriminate between areas with different morphological properties. No classification method produces a map that is 100% accurate and some attempt must always be made to assess the accuracy of classification results (e.g. confusion matrix).\n\nClassification maps are subject to ground-verification in order to identify the compositions and bottom type that characterize each class. The functionality of Geographic Information Systems (GIS) can be used to integrate data from different sources, including ground truth data. Such data may come from in-situ sediment grab sampling, the use of a dredge, trawl net, visual imagery or surveys using Remotely Operated Vehicles (ROVs). The seabed classification map can be combined with other information about the area, such as fish distribution and abundance or vegetation characteristics, to establish habitat groups based on associations. This process allows classification maps derived from multibeam data to help characterize the seabed and more effectively manage its use.\n\nSub-surface seabed classification is commonly referred to as sub-bottom profiling and is generally used for geological assessment of the sub-surface characteristics. Sub-bottom profiling can return information from tens to hundreds of meters below the seafloor, and is often used to complement reflection seismology. From sub-surface classifications, scientists and engineers can characterize rock and sediment types, as well as pore fluids. This information is used for many applications, such as slope failure analysis and hydrocarbon exploration.\n\n\nResources - seabed surface:\n\n"}
{"id": "1180789", "url": "https://en.wikipedia.org/wiki?curid=1180789", "title": "Bear Butte", "text": "Bear Butte\n\nBear Butte is a geological laccolith feature located near Sturgis, South Dakota, United States, that was established as a State Park in 1961. An important landmark and religious site for the Plains Indians tribes long before Europeans reached South Dakota, Bear Butte is called Mathó Pahá, or Bear Mountain, by the Lakota, or Sioux. To the Cheyenne, it is known as Noahȧ-vose (\"giving hill\") or Náhkȯhe-vose (\"bear hill\"), and is the place where Ma'heo'o (God) imparted to Sweet Medicine, a Cheyenne prophet, the knowledge from which the Cheyenne derive their religious, political, social, and economic customs.\n\nThe mountain is sacred to many indigenous peoples, who make pilgrimages to leave prayer cloths and tobacco bundles tied to the branches of the trees along the mountain's flanks. Other offerings are often left at the top of the mountain. The site is associated with various religious ceremonies throughout the year. The mountain is a place of prayer, meditation, and peace.\n\nThe park includes a campsite west of South Dakota Highway 79 where horseback riding, fishing, and boating are permitted. On the summit side of Highway 79, a moderately sized herd of buffalo roams the base of the mountain. An education center and a summit trail are available. Official park policy advises visitors to Bear Butte to respect worshipers and to leave religious offerings undisturbed. Park fees are waived for those undertaking religious activities.\n\nIn 2007, Gov. Mike Rounds of South Dakota announced a proposal to use state, federal and private money to buy a perpetual easement in order to prevent commercial and residential development of some land on the western side of Bear Butte. This would cost more than $1 million, but would prevent development of nuisance businesses (such as potentially lucrative biker bars) on ranch land near the mountain on the northern edge of the Black Hills.\n\nBear Butte is not strictly a butte (created primarily by erosion of sedimentary strata), but a laccolith: an intrusive body of igneous rock, uplifting the earlier sedimentary layers, which have since largely eroded away. This is the result of the forcible entry (or intrusion) of magma into cooler crustal rock in the Black Hills area during the Eocene Epoch. In this, Bear Butte shares a similar geological history with other formations in the region, including the Black Hills, Devils Tower, the Missouri Buttes, and some parts of the Rocky Mountains. It is possible that when the intrusion was emplaced, some magma may have breached the surface, forming a volcano; however, it would have eroded away long ago.\n\nThe peak rises above the surrounding plain and is above sea level.\n\nHuman artifacts have been found on or near Bear Butte that date back 10,000 years, indicating a long and continuous interest in the mountain. The Cheyenne and Lakota people have maintained a spiritual interest in Bear Butte from their earliest recorded history.\n\nNotable visitors like Red Cloud, Crazy Horse, and Sitting Bull made pilgrimages to the site. In 1857, a council of many Indian nations gathered at Bear Butte to discuss the growing presence of white settlers in the Black Hills.\n\nViolating a treaty of 1868, George Armstrong Custer led an expedition to the Black Hills region in 1874, and according to custom he camped near Bear Butte. Custer verified the rumors of gold in the Black Hills, and Bear Butte then served as an easily identifiable landmark for the rush of invading prospectors and settlers into the region. Indian reaction to the illegal movements of whites into the area was intense and hostile. Ultimately the government reneged on its treaty obligations regarding the Black Hills and instead embarked on a program to confine all northern Plains tribes to reservations.\n\nEzra Bovee homesteaded on the southern slopes of the mountain, and by the time of World War II, he and his family were the legal owners of the site. In the spring of 1945, the Northern Cheyenne received permission from Bovee to hold a ceremony at Bear Butte to pray for the end of World War II. The Cheyenne found that the Bovee family welcomed their interest in the mountain, and over the years the Bovees continued to encourage native religious ceremonies.\n\nBy the mid-1950s Ezra Bovee was attempting to stir up interest in making Bear Butte a national park. After his death, his family continued the effort. When federal interest in the project waned, the state government in Pierre took action, and Bear Butte became a state park in 1961 and was registered as a National Historic Landmark in 1981.\n\nFrank Fools Crow, the Lakota ceremonial chief (d. 1989), made pilgrimages to Bear Butte throughout his lifetime. Fools Crow taught racial harmony not just between whites and Indians, but among all the peoples of the world. He believed the Lakota should never sell the Black Hills. A bust and plaque in front of the education center at Bear Butte State Park honor Fools Crow's efforts.\n\nFrank Fools Crow was the plaintiff in one of the most prominent attempts by Native Americans to gain access to sacred lands under the American Indian Religious Freedom Act of 1978. The case, \"Fools Crow v. Gullett\", related to the introduction in 1982 of limits on when and for how long Lakota and Cheyenne religious ceremonies could take place on the Bluff. The Indian Americans argued that both the American Indian Religious Freedom Act and the First Amendment protected their right to unlimited access to the Bluff. They also wanted the Bluff to remain untouched as it was sacred. The plaintiffs lost their case on both the District and Appellate level and were denied a hearing by the Supreme Court.\n\nIn 2011, the National Trust for Historic Preservation included Bear Butte on its list of the 11 Most Endangered Places.\n\n\nOehlerking, Jerry. \"The Dick Williams Story: If Bear Butte Would Speak,\" South Dakota Conservation Digest, March/April 1977, pp. 22–25.\n\n"}
{"id": "8349656", "url": "https://en.wikipedia.org/wiki?curid=8349656", "title": "Boston Society of Natural History", "text": "Boston Society of Natural History\n\nThe Boston Society of Natural History (1830–1948) in Boston, Massachusetts, was an organization dedicated to the study and promotion of natural history. It published a scholarly journal and established a museum. In its first few decades, the society occupied several successive locations in Boston's Financial District, including Pearl Street, Tremont Street and Mason Street. In 1864 it moved into a newly constructed museum building at 234 Berkeley Street in the Back Bay, designed by architect William Gibbons Preston. In 1951 the society evolved into the Museum of Science, and relocated to its current site on the Charles River.\n\nFounders of the society in 1830 included Amos Binney Jr.; Edward Brooks; Walter Channing; Henry Codman; George B. Emerson; Joshua B. Flint; Benjamin D. Greene; Simon E. Greene; William Grigg; George Hayward; D. Humphreys Storer; and John Ware. Several had previously been involved with the Linnaean Society of New England. By 1838, the society held \"regular meetings on the 2nd and 4th Wednesday of each month.\" \"In its collection are about 700 specimens in mineralogy and geology, besides the rich collection of Dr. C.T. Jackson, and the state collection; botany, 5,000; mammalia, 30 entire skeletons and 30 crania; birds, 200 species; reptiles, 130; insects, about 15,000; crustacea, 130; radiata, 190. Library, 600 volumes and pamphlets. The room ... gratuitously opened to the public every Wednesday from 12 to 2 o'clock.\"\n\nAmong the many scholars and curators affiliated with the society: Alexander Emanuel Agassiz; T.T. Bouve; Thomas Mayo Brewer; George Emerson; A.A. Gould; F.W.P. Greenwood; Charles Thomas Jackson; Charles Sedgwick Minot; Albert Ordway; Samuel Hubbard Scudder; Charles J. Sprague; Alpheus Hyatt, and Jeffries Wyman.\n\n\"After World War II, under the leadership of Bradford Washburn, the society sold the Berkeley Street building, changed its name to the Boston Museum of Science. ... The cornerstone for the new Museum was laid at Science Park [in 1949] and a temporary building was erected to house the Museum's collections and staff. In 1951, the first wing of the new Museum officially opened.\"\n\n\n\n"}
{"id": "47308192", "url": "https://en.wikipedia.org/wiki?curid=47308192", "title": "CTAIDI", "text": "CTAIDI\n\nThe Customer Total Average Interruption Duration Index (CTAIDI) is a reliability indicator associated with electric power distribution. CTAIDI is the average total duration of interruption \"for customers who had at least one interruption during the period of analysis\", and is calculated as:\n\nformula_1\n\nwhere formula_2 is the number of customers and formula_3 is the annual outage time for location formula_4, and formula_5 is the number of customers at location formula_4 that were interrupted. In other words,\n\nformula_7\n\nCTAIDI is measured in units of time, such as minutes or hours. It is similar to CAIDI, but CAIDI divides the total duration of interruptions by the number of interruptions whereas CTAIDI divides by the number of interrupted customers. When CTAIDI is much greater than CAIDI, the service outages are more concentrated among certain customers.\n\nCTAIDI also has the same numerator as SAIDI, but SAIDI divides the total duration of interruptions by the total number of customers served. The fraction of distinct customers interrupted illustrates the relationship between several reliability indicators:\n\nformula_8\n\n"}
{"id": "55998207", "url": "https://en.wikipedia.org/wiki?curid=55998207", "title": "Circothecidae", "text": "Circothecidae\n\nCircothecidae are a family of Cambrian problematica, sometimes attributed to the Hyolitha, though some authors suggest (on the basis of no specified evidence) that they're definitely not.\n"}
{"id": "37710828", "url": "https://en.wikipedia.org/wiki?curid=37710828", "title": "David G. Haskell", "text": "David G. Haskell\n\nDavid George Haskell is a British-born American biologist, author, and professor of biology at , in Sewanee, Tennessee. In addition to scientific papers, he has written essays, poems, op-eds, and the book \"The Forest Unseen\" (Viking Press, Penguin Random House 2012) and \"The Songs of Trees\" (Viking Press, Penguin Random House 2017).\n\n\"The Forest Unseen\" was winner of the 2013 National Academies Communication Award for Best Book, finalist for the 2013 Pulitzer Prize in General Nonfiction, runner-up for the 2013 PEN/E. O. Wilson Literary Science Writing Award, winner of the 2012 National Outdoor Book Award for Natural History Literature, and the 2013 Reed Environmental Writing Award. \"The Forest Unseen\" has been translated into ten languages and was winner of the 2016 Dapeng Nature Book Award in China.\n\nHaskell's second book, \"The Songs of Trees\", was published in April 2017 by Viking. It won the 2018 John Burroughs Medal for Distinguished Natural History Writing. Public Radio International's Science Friday named \"The Songs of Trees\" of the Best Science Books of 2017, Maria Popova included the book in Brain Pickings Favorite Science Books of 2017, and Forbes.com named the book one of 10 Best Environment, Climate Science and Conservation Books of 2017.\n\nHaskell received his B.A. in zoology from the University of Oxford and his Ph.D. in evolutionary biology from Cornell University. In 2009 he was named the Carnegie-CASE Professor of the Year in Tennessee. He was awarded a Guggenheim Fellowship by the John Simon Guggenheim Memorial Foundation in 2014.\n\n\n"}
{"id": "626072", "url": "https://en.wikipedia.org/wiki?curid=626072", "title": "Dead zone (ecology)", "text": "Dead zone (ecology)\n\nDead zones are hypoxic (low-oxygen) areas in the world's oceans and large lakes, caused by \"excessive nutrient pollution from human activities coupled with other factors that deplete the oxygen required to support most marine life in bottom and near-bottom water. (NOAA)\". Historically, many of these sites were naturally occurring. However, in the 1970s, oceanographers began noting increased instances and expanses of dead zones. These occur near inhabited coastlines, where aquatic life is most concentrated. (The vast middle portions of the oceans, which naturally have little life, are not considered \"dead zones\".)\nIn March 2004, when the recently established UN Environment Programme published its first Global Environment Outlook Year Book (\"GEO Year Book 2003\"), it reported 146 dead zones in the world's oceans where marine life could not be supported due to depleted oxygen levels. Some of these were as small as a square kilometre (0.4 mi²), but the largest dead zone covered 70,000 square kilometres (27,000 mi²). A 2008 study counted 405 dead zones worldwide.\n\nAquatic and marine dead zones can be caused by an increase in nutrients (particularly nitrogen and phosphorus) in the water, known as eutrophication. These chemicals are the fundamental building blocks of single-celled, plant-like organisms that live in the water column, and whose growth is limited in part by the availability of these materials. Eutrophication can lead to rapid increases in the density of certain types of these phytoplankton, a phenomenon known as an algal bloom.\n\nLimnologist Dr. David Schindler, whose research at the Experimental Lakes Area led to the banning of harmful phosphates in detergents, warned about algal blooms and dead zones, \"The fish-killing blooms that devastated the Great Lakes in the 1960s and 1970s haven't gone away; they've moved west into an arid world in which people, industry, and agriculture are increasingly taxing the quality of what little freshwater there is to be had here...This isn't just a prairie problem. Global expansion of dead zones caused by algal blooms is rising rapidly.\"\n\nThe major groups of algae are Cyanobacteria, green algae, Dinoflagellates, Coccolithophores and Diatom algae. Increase in input of nitrogen and phosphorus generally causes Cyanobacteria to bloom. Cyanobacteria are not good food for zooplankton and fish and hence accumulate in water, die, and then decompose. The bacterial degradation of their biomass consumes the oxygen in the water, thereby creating the state of hypoxia. Other algae are consumed and hence do not accumulate to the same extent as Cyanobacteria. Dead zones can be caused by natural and by anthropogenic factors. Natural causes include coastal upwelling and changes in wind and water circulation patterns. Use of chemical fertilizers is considered the major human-related cause of dead zones around the world. Runoff from sewage, urban land use, and fertilizers can also contribute to eutrophication.\n\nNotable dead zones in the United States include the northern Gulf of Mexico region, surrounding the outfall of the Mississippi River, the coastal regions of the Pacific Northwest, and the Elizabeth River in Virginia Beach, all of which have been shown to be recurring events over the last several years.\n\nAdditionally, natural oceanographic phenomena can cause deoxygenation of parts of the water column. For example, enclosed bodies of water, such as fjords or the Black Sea, have shallow sills at their entrances, causing water to be stagnant there for a long time. The eastern tropical Pacific Ocean and northern Indian Ocean have lowered oxygen concentrations which are thought to be in regions where there is minimal circulation to replace the oxygen that is consumed. These areas are also known as oxygen minimum zones (OMZ). In many cases, OMZs are permanent or semipermanent areas.\n\nRemains of organisms found within sediment layers near the mouth of the Mississippi River indicate four hypoxic events before the advent of synthetic fertilizer. In these sediment layers, anoxia-tolerant species are the most prevalent remains found. The periods indicated by the sediment record correspond to historic records of high river flow recorded by instruments at Vicksburg, Mississippi.\n\nChanges in ocean circulation triggered by ongoing climate change could also add or magnify other causes of oxygen reductions in the ocean.\n\nIn August 2017, a report suggested that the US meat industry and agroeconomic system are predominantly responsible for the largest-ever dead zone in the Gulf of Mexico. Soil runoff and leached nitrate, exacerbated by agricultural land management and tillage practices as well as manure and synthetic fertilizer usage, contaminated water from the Heartland to the Gulf of Mexico. A large portion of the crops grown in this region are used as major feed components in the production of meat animals for agribusiness companies, like Tyson and Smithfield Foods.\n\nLow oxygen levels recorded along the Gulf Coast of North America have led to reproductive problems in fish involving decreased size of reproductive organs, low egg counts and lack of spawning.\n\nIn a study of the Gulf killifish by the Southeastern Louisiana University done in three bays along the Gulf Coast, fish living in bays where the oxygen levels in the water dropped to 1 to 2 parts per million (ppm) for three or more hours per day were found to have smaller reproductive organs. The male gonads were 34% to 50% as large as males of similar size in bays where the oxygen levels were normal (6 to 8 ppm). Females were found to have ovaries that were half as large as those in normal oxygen levels. The number of eggs in females living in hypoxic waters were only one-seventh the number of eggs in fish living in normal oxygen levels.\n\nFish raised in laboratory-created hypoxic conditions showed extremely low sex hormone concentrations and increased elevation of activity in two genes triggered by the hypoxia-inductile factor (HIF) protein. Under hypoxic conditions, HIF pairs with another protein, ARNT. The two then bind to DNA in cells, activating genes in those plant cells.\n\nUnder normal oxygen conditions, ARNT combines with estrogen to activate genes. Hypoxic cells \"in vitro\" did not react to estrogen placed in the tube. HIF appears to render ARNT unavailable to interact with estrogen, providing a mechanism by which hypoxic conditions alter reproduction in fish.\n\nIt might be expected that fish would flee the potential suffocation, but they are often quickly rendered unconscious and doomed. Slow moving bottom-dwelling creatures like clams, lobsters and oysters are unable to escape. All colonial animals are extinguished. The normal re-mineralization and recycling that occurs among benthic life-forms is stifled.\n\nDespite most other life forms being killed by the lack of oxygen, jellyfish can thrive and are sometimes present in dead zones in vast numbers. These jellyfish blooms produce mucus and waste, leading to major changes in food webs in the ocean. The organic carbon is metabolised by bacteria which return it to the atmosphere in the form of carbon dioxide in what has been termed a \"jelly carbon shunt\".\n\nIt has been shown that future changes in oxygen could affect most marine ecosystems and have socio-economic ramifications due to human dependency on marine goods and services.\n\nIn the 1970s, marine dead zones were first noted in settled areas where intensive economic use stimulated scientific scrutiny: in the U.S. East Coast's Chesapeake Bay, in Scandinavia's strait called the Kattegat, which is the mouth of the Baltic Sea and in other important Baltic Sea fishing grounds, in the Black Sea, and in the northern Adriatic.\n\nOther marine dead zones have appeared in coastal waters of South America, China, Japan, and New Zealand. A 2008 study counted 405 dead zones worldwide.\n\nResearchers from Baltic Nest Institute published in one of PNAS issues reports that the dead zones in the Baltic Sea have grown from approximately 5,000 km2 to more than 60,000 km2 in recent years.\n\nSome of the causes behind the elevated increase of dead zones can be attributed to the use of fertilizers, large animal farms, the burning of fossil fuels, and effluents from municipal wastewater treatment plants.\n\nThe Elizabeth River estuary is important for Norfolk, Virginia, Chesapeake, Virginia, Virginia Beach, Virginia and Portsmouth, Virginia. It has been polluted by nitrogen and phosphorus, but also toxic deposits from the shipbuilding industry, the military, the world's largest coal export facility, refineries, loading docks, container-repair facilities and others, so fish had been \"offlimits since the 1920s\". In 1993, a group formed to clean it up, adopting the mummichog as a mascot, and has removed thousands of tons of contaminated sediment. In 2006, a 35-acre biological dead zone called Money Point was dredged out, and this let fish return, and the wetland recover\n\nA dead zone exists in the central part of Lake Erie from east of Point Pelee to Long Point and stretches to shores in Canada and the United States. The zone has been noticed since the 1950s to 1960s, but efforts since the 1970s have been made by Canada and the US to reduce runoff pollution into the lake as means to reverse the dead zone growth. Overall the lake's oxygen level is poor with only a small area to the east of Long Point that has better levels. The biggest impact of the poor oxygen levels is to lacustrine life and fisheries industry.\n\nA dead zone exists in the Lower St. Lawrence River area from east the Saguenay River to east of Baie Comeau, greatest at depths over and noticed since the 1930s. The main concerns for Canadian scientists is the impact of fish found in the area.\n\nOff the coast of Cape Perpetua, Oregon, there is also a dead zone with a 2006 reported size of 300 square miles (780 km²). This dead zone only exists during the summer, perhaps due to wind patterns. The Oregon coast has also seen hypoxic water transporting itself from the continental shelf to the coastal embayments. This has seemed to cause intensity in several areas of Oregon's climate such as upwelled water containing oxygen concentration and upwelled winds.\n\nThe area of temporary hypoxic bottom water that occurs most summers off the coast of Louisiana in the Gulf of Mexico is the largest recurring hypoxic zone in the United States. The Mississippi River, which is the drainage area for 41% of the continental United States, dumps high-nutrient runoff such as nitrogen and phosphorus into the Gulf of Mexico. According to a 2009 fact sheet created by NOAA, \"seventy percent of nutrient loads that cause hypoxia are a result of this vast drainage basin\". which includes the heart of U.S. agribusiness, the Midwest. The discharge of treated sewage from urban areas (pop. c 12 million in 2009) combined with agricultural runoff deliver c. 1.7 million tons of phosphorus and nitrogen into the Gulf of Mexico every year. Even though Iowa occupies less than 5% of the Mississippi River drainage basin, average annual nitrate discharge from surface water in Iowa is about 204,000 to 222,000 metric tonnes, or 25% of all the nitrate which the Mississippi River delivers to the Gulf of Mexico. Export from the Raccoon River Watershed is among the highest in the United States with annual yields at 26.1 kg/ha/year which ranked as the highest loss of nitrate out of 42 Mississippi subwatersheds evaluated for a Gulf of Mexico hypoxia report. In 2012, Iowa introduced the Iowa Nutrient Reduction Strategy which, \"The Iowa Nutrient Reduction Strategy is a science and technology-based framework to assess and reduce nutrients to Iowa waters and the Gulf of Mexico. It is designed to direct efforts to reduce nutrients in surface water from both point and nonpoint sources in a scientific, reasonable and cost effective manner.\" The strategy continues to evolve, using voluntary methods to reduce Iowa's negative contributions through outreach, research, and implementation of nutrient holding practices.\n\nThe area of hypoxic bottom water that occurs for several weeks each summer in the Gulf of Mexico has been mapped most years from 1985 through 2017. The size varies annually from a record high in 2017 when it encompassed more than 22,730 sq kilometers (8,776 square miles) to a record low in 1988 of 39 sq kilometers (15 square miles). The 2015 dead zone measured 16,760 square kilometers (6,474 square miles).\nNancy Rabalais of the Louisiana Universities Marine Consortium in Cocodrie, Louisiana predicted the dead zone or hypoxic zone in 2012 will cover an area of 17,353 sq kilometers (6,700 square miles) which is larger than Connecticut; however, when the measurements were completed, the area of hypoxic bottom water in 2012 only totaled 7,480 sq kilometers. The models using the nitrogen flux from the Mississippi River to predict the \"dead zone\" areas have been criticized for being systematically high from 2006 to 2014, having predicted record areas in 2007, 2008, 2009, 2011, and 2013 that were never realized.\n\nIn late summer 1988 the dead zone disappeared as the great drought caused the flow of Mississippi to fall to its lowest level since 1933. During times of heavy flooding in the Mississippi River Basin, as in 1993, \"\"the \"dead zone\"\ndramatically increased in size, approximately larger than the previous year\".\n\nSome assert that the dead zone threatens lucrative commercial and recreational fisheries in the Gulf of Mexico. \"In 2009, the dockside value of commercial fisheries in the Gulf was $629 million. Nearly three million recreational fishers further contributed about $10 billion to the Gulf economy, taking 22 million fishing trips.\" Scientists are not in universal agreement that nutrient loading has a negative impact on fisheries. Grimes makes a case that nutrient loading enhances the fisheries in the Gulf of Mexico. Courtney et al. hypothesize, that nutrient loading may have contributed to the increases in red snapper in the northern and western Gulf of Mexico.\n\nShrimp trawlers first reported a 'dead zone' in the Gulf of Mexico in 1950, but it was not until 1970 when the size of the hypoxic zone had increased that scientists began to investigate.\n\nAfter 1950, the conversion of forests and wetlands for agricultural and urban developments accelerated. \"Missouri River Basin has had hundreds of thousands of acres of forests and wetlands (66,000,000 acres) replaced with agriculture activity [. . .] In the Lower Mississippi one-third of the valley's forests were converted to agriculture between 1950 and 1976.\" \n\nIn July 2007, a dead zone was discovered off the coast of Texas where the Brazos River empties into the Gulf.\n\nThe Energy Independence and Security Act of 2007 calls for the production of of renewable fuels by 2022, including of corn-based ethanol, a tripling of current production that would require a similar increase in corn production. Unfortunately, the plan poses a new problem; the increase in demand for corn production results in a proportional increase in nitrogen runoff. Although nitrogen, which makes up 78% of the Earth's atmosphere, is an inert gas, it has more reactive forms, two of which (nitrate and ammonia) are used to make fertilizer.\n\nAccording to , a professor of crop physiology at the University of Illinois at Urbana-Champaign, corn requires more nitrogen-based fertilizer because it produces a higher grain per unit area than other crops and, unlike other crops, corn is completely dependent on available nitrogen in soil. The results, reported 18 March 2008 in Proceedings of the National Academy of Sciences, showed that scaling up corn production to meet the goal would increase nitrogen loading in the Dead Zone by 10–18%. This would boost nitrogen levels to twice the level recommended by the Mississippi Basin/Gulf of Mexico Water Nutrient Task Force (Mississippi River Watershed Conservation Programs), a coalition of federal, state, and tribal agencies that have monitored the dead zone since 1997. The task force says a 30% reduction of nitrogen runoff is needed if the dead zone is to shrink.\n\nDead zones are reversible, though the extinction of organisms that are lost due to its appearance is not. The Black Sea dead zone, previously the largest in the world, largely disappeared between 1991 and 2001 after fertilizers became too costly to use following the collapse of the Soviet Union and the demise of centrally planned economies in Eastern and Central Europe. Fishing has again become a major economic activity in the region.\n\nWhile the Black Sea \"cleanup\" was largely unintentional and involved a drop in hard-to-control fertilizer usage, the U.N. has advocated other cleanups by reducing large industrial emissions. From 1985 to 2000, the North Sea dead zone had nitrogen reduced by 37% when policy efforts by countries on the Rhine River reduced sewage and industrial emissions of nitrogen into the water. Other cleanups have taken place along the Hudson River and San Francisco Bay.\n\nThe chemical aluminium sulfate can be used to reduce phosphates in water.\n\n\n\n"}
{"id": "30483507", "url": "https://en.wikipedia.org/wiki?curid=30483507", "title": "Dilys Breese", "text": "Dilys Breese\n\nDilys Breese (born 2 June 1932, Abergavenny, Monmouthshire; died 22 August 2007) was a natural history television producer for the BBC and an ornithologist with the British Trust for Ornithology, who commemorate her contribution by awarding the Dilys Breese Medal, funded by her bequest to them.\n\nBreese was brought up in Wales, she was educated at Oswestry Girls' High School, then graduated from St Andrews in 1954, with an MA in English Literature and Language.\n\nAfter graduation, she applied for a position as a trainee studio manager with BBC radio. While working on shows like Woman's Hour she developed an interest in natural history, and by 1970 was producing the majority of BBC Bristol's natural history output, with presenter Derek Jones. With Jones, she created the successful radio series The Living World and Wildlife.\n\nIn 1970, Breese joined the BBC Natural History Unit, where she produced television shows including The World About Us, Wildlife on One and The Natural World.\n\nShe left the BBC in 1991 and set up her own company, Kestrel Productions, making several short programmes until deteriorating health prevented her from working.\n\nBreese became a council member of the British Trust for Ornithology in 1973 and was its Honorary Secretary from 1998–2001. She chaired the working group developing 'Garden BirdWatch', which has since become the largest year-round citizen science project in the world. In 1983, she was the first recipient of the BTO's Golden Jubilee Medal for outstanding service to the Trust.\n\n\n"}
{"id": "1305653", "url": "https://en.wikipedia.org/wiki?curid=1305653", "title": "Duga radar", "text": "Duga radar\n\nDuga () was a Soviet over-the-horizon (OTH) radar system used as part of the Soviet missile defense early-warning radar network. The system operated from July 1976 to December 1989. Two operational Duga radars were deployed, one near Chernobyl and Chernihiv in the Ukrainian SSR (present-day Ukraine), the other in eastern Siberia.\n\nThe Duga systems were extremely powerful, over 10 MW in some cases, and broadcast in the shortwave radio bands. They appeared without warning, sounding like a sharp, repetitive tapping noise at 10 Hz, which led to it being nicknamed by shortwave listeners the Russian Woodpecker. The random frequency hops disrupted legitimate broadcasts, amateur radio operations, oceanic commercial aviation communications, utility transmissions, and resulted in thousands of complaints by many countries worldwide. The signal became such a nuisance that some receivers such as amateur radios and televisions actually began including 'Woodpecker Blankers' in their circuit designs in an effort to filter out the interference.\n\nThe unclaimed signal was a source for much speculation, giving rise to theories such as Soviet mind control and weather control experiments. However, because of its distinctive transmission pattern, many experts and amateur radio hobbyists quickly realized it to be an over-the-horizon radar system. NATO military intelligence had already given it the reporting name \"STEEL WORK\" or \"STEEL YARD\". While the amateur radio community was well aware of the system, this theory was not publicly confirmed until after the fall of the Soviet Union.\n\nThe Soviets had been working on early warning radar for their anti-ballistic missile systems through the 1960s, but most of these had been line-of-sight systems that were useful for raid analysis and interception only. None of these systems had the capability to provide early warning of a launch, within seconds or minutes of a launch, which would give the defences time to study the attack and plan a response. At the time, the Soviet early-warning satellite network was not well developed, and there were questions about their ability to operate in a hostile environment including anti-satellite efforts. An over-the-horizon radar sited in the USSR would not have any of these problems, and work on such a system for this associated role started in the late 1960s.\n\nThe first experimental system, Duga, was built outside Mykolaiv in Ukraine, successfully detecting rocket launches from Baikonur Cosmodrome at 2,500 kilometers. This was followed by the prototype Duga, built on the same site, which was able to track launches from the far east and submarines in the Pacific Ocean as the missiles flew towards Novaya Zemlya. Both of these radar systems were aimed east and were fairly low power, but with the concept proven, work began on an operational system. The new Duga-1 systems used a transmitter and receiver separated by about 60 km.\n\nAt some point in 1976, a new and powerful radio signal was detected simultaneously worldwide, and quickly dubbed 'the Woodpecker' by amateur radio operators. Transmission power on some Woodpecker transmitters was estimated to be as high as 10 MW equivalent isotropically radiated power. Even prior to 1976, a similar 'woodpecker' interference is remembered by radio amateurs occurring in the high frequencies. As early as 1963, or before, radio amateurs were calling this \"the Russian Woodpecker\". Little is apparently known about the power levels or Russian designation but is probably a forerunner of the Duga radar systems. It was also speculated at that time, at least among radio amateurs, that this was an over-the-horizon radar.\n\nTriangulation by both amateur radio hobbyists and NATO quickly revealed the signals came from a location in present day Ukraine, at the time called Ukrainian Soviet Socialist Republic (part of USSR). Confusion due to small differences in the reports being made from various sources led to the site being alternately located near Kiev, Minsk, Chernobyl, Gomel or Chernihiv. All of these reports were describing the same deployment, with the transmitter only a few kilometers southwest of Chernobyl (south of Minsk, northwest of Kyiv) and the receiver about 50 km northeast of Chernobyl (just west of Chernihiv, south of Gomel). At one time there was speculation that several transmitters were in use.\n\nThe radar system was given the code 5Н32-West by the Soviets, and was set up in two closed towns, Liubech-1 held the two transmitters and Chernobyl-2 the receivers. Unknown to civilian observers at the time, NATO was aware of the new installation. A second installation was built near Komsomolsk-on-Amur, in Bolshya Kartel and Lian, but did not become active for some time.\n\nThe NATO Reporting Name for the Duga-1 is often quoted as STEEL YARD. Many online and several print references use this name. However some sources also use the term STEEL WORK (or STEEL WORKS). As any \"official\" sources using NATO Reporting Names are likely to be classified, deconflicting this will be difficult. The earliest found open source mention of a NATO Reporting Name for this system, a reference publication in print while the system was still active, unambiguously uses the term STEEL WORK.\n\nEven from the earliest reports it was suspected that the signals were tests of an over-the-horizon radar, and this remained the most popular hypothesis during the Cold War. Several other theories were floated as well, including everything from jamming western broadcasts to submarine communications. The broadcast jamming theory was discarded early on when a monitoring survey showed that Radio Moscow and other pro-Soviet stations were just as badly affected by woodpecker interference as Western stations.\n\nAs more information about the signal became available, its purpose as a radar signal became increasingly obvious. In particular, its signal contained a clearly recognizable structure in each pulse, which was eventually identified as a 31-bit pseudo-random binary sequence, with a bit-width of 100 μs resulting in a 3.1 ms pulse. This sequence is usable for a 100 μs chirped pulse amplification system, giving a resolution of 15 km (10 mi) (the distance light travels in 50 μs). When a second Woodpecker appeared, this one located in eastern Russia but also pointed toward the US and covering blank spots in the first system's pattern, this conclusion became inescapable.\n\nIn 1988, the U.S. Federal Communications Commission (FCC) conducted a study on the Woodpecker signal. Data analysis showed an inter-pulse period of about 90 ms, a frequency range of 7 to 19 MHz, a bandwidth of 0.02 to 0.8 MHz, and typical transmission time of 7 minutes.\n\nTo combat this interference, amateur radio operators attempted to jam the signal by transmitting synchronized unmodulated continuous wave signals at the same pulse rate as the offending signal. They formed a club called The Russian Woodpecker Hunting Club. Core group members would frame the \"Official Practice Target\" in their radio shacks.\n\nStarting in the late 1980s, even as the FCC was publishing studies, the signals became less frequent, and in 1989, they disappeared altogether. Although the reasons for the eventual shutdown of the Duga systems have not been made public, the changing strategic balance with the end of the Cold War in the late 1980s likely had a major part to play. Another factor was the success of the US-KS early-warning satellites, which began entering service in the early 1980s, and by this time had grown into a complete network. The satellite system provides immediate, direct and highly secure warnings, whereas any radar-based system is subject to jamming, and the effectiveness of OTH systems is also subject to atmospheric conditions.\n\nAccording to some reports, the Komsomolsk-na-Amure installation in the Russian Far East was taken off combat alert duty in November 1989, and some of its equipment was subsequently scrapped. The original Duga-1 site lies within the 30 kilometer Zone of Alienation around the Chernobyl power plant. It appears to have been permanently deactivated, since their continued maintenance did not figure in the negotiations between Russia and Ukraine over the active Dnepr early warning radar systems at Mukachevo and Sevastopol. The antenna still stands, however, and has been used by amateurs as a transmission tower (using their own antennas) and has been extensively photographed.\n\nThe original Duga was the first experimental system. It was built outside the Black Sea port of Mykolaiv in the southern Ukraine, and successfully detected rocket launches from Baikonur Cosmodrome about 2,500 kilometers away. Duga is able to track launches from the Far East and from submarines in the Pacific Ocean, as the missiles fly towards Novaya Zemlya in the Arctic Ocean. This huge radar complex was restored in 2002 after a fire which seriously damaged it. The transmitter is located at and the receiver at .\n\nThe original Duga was supplanted by a pair of installations: western, Duga-1, and eastern, Duga-2. Duga-1 was built in northern Ukraine, between Liubech and Chernobyl-2, with the receiver at located a few kilometers west-north-west of Chernobyl; the transmitter is located at about 50 km northeast of Chernobyl (just west of Chernihiv, south of Gomel). The site is open for pre-arranged visits, for which a permit must be obtained in advance; it has been open since approximately 28 October 2013. Operators who provide tours of Chernobyl and the surrounding areas are able to obtain the relevant paperwork. \n\nDuga-2, the eastern system, is located near Komsomolsk-on-Amur in Khabarovsk Krai, with the receiver at , some 30 km southeast of the city, and the transmitter at , around 43 km north of the city.\n\nThe Ukrainian-developed computer game \"S.T.A.L.K.E.R.\" has a plot focused on the Chernobyl Nuclear Power Plant and the nuclear accident there. The game heavily features actual locations in the area, including the Duga-1 array. The array itself appears in \"\" in the fictional city of Limansk-13. While the 'Brain Scorcher' from \"\" was inspired by theories that Duga-1 was used for mind control, it does not take the form of the real array.\n\nMarkiyan Kamysh's novel about illegal trips to the Duga, \"A Stroll to the Zone\", was praised by reviewers as the most interesting literature debut of 2015 in Ukraine. The novel has been translated into French (with the title \"La Zone\"), and was published by French publishing house Arthaud (Groupe Flammarion).\n\nIn \"\", the map \"Grid\" is placed in Pripyat near the DUGA-1 array.\n\nIn the movie \"Divergent\", the fence around Chicago is derived from photographs of the Duga-1 array.\n\nThe 'Russian woodpecker' appears in Justin Scott's novel \"The Shipkiller\".\n\nThe Duga at Chernobyl was the focus of the 2015 documentary film, \"The Russian Woodpecker\", by Chad Gracia. The film includes interviews with the commander of the Duga, Vladimir Musiets, as well as the Vice-Commander, the Head of the Data Center, and others involved in building and operating the radar. The documentary, which won numerous awards, also includes drone video footage of the array and handheld video footage of the surroundings as well as a climb to the top by the cinematographer, Artem Ryzhykov. The film also proposes a conspiracy theory that the Chernobyl disaster was engineered to cover up failures in the radar's design. \n\nA Duga radar is featured in the 2017 game \"Player Unknown's Battlegrounds\" in a map which portrays a fictional Russian Military base.\n\n\n\n"}
{"id": "37071737", "url": "https://en.wikipedia.org/wiki?curid=37071737", "title": "Eko Atlantic", "text": "Eko Atlantic\n\nEko Atlantic, officially Nigeria International Commerce city, also known as Eko Atlantic City, or the initials E.A.C. and E.A., is a planned city of Lagos State, Nigeria, being constructed on land reclaimed from the Atlantic Ocean. Upon completion, the new peninsula, which is still under development is anticipating at least 250,000 residents and a daily flow of 150,000 commuters. The development will also have a positive environmental impact, as it will help in stopping the erosion of Lagos State's coastline.\n\nThe city adjoins Victoria Island district of Lagos city and the Phase 1 area of Lekki, to the north, while the entire Western, eastern and southern borders is a coastline. Eko Atlantic is expected to rise as the next generation of property on the African continent; having a total of 10 districts, spread across a land area of approximately , the city will satisfy needs for financial, commercial, residential and tourist accommodations.\n\nEko Atlantic development is being carried out as a Public–private partnership, with private companies and investors providing the funding, whilst Lagos State Government is a strategic partner, with the support of the Federal Government. The Contractors are China Communications Construction Group, a company that works in the field of marine dredging and landfill operation. Consultants are Royal Haskoning (traffic and transport expertise) and \"ar+h Architects\". \"South Energyx Nigeria Ltd.\" a subsidiary of the Chagoury group was specifically created to undertake the development. Testing of the sea defence system took place at the DHI Institute in Copenhagen, Denmark, where models were successfully tested for one-in-a-hundred-year ocean surges, and one-in-120-year, one-in-150-year and one-in-1,000-year storms.\n\nEko Atlantic will satisfy needs for financial, commercial, residential and tourist accommodations, with infrastructure in line with modern and environmental standards. These standards will offer the city’s residents water, waste management, security and transportation systems. The city will also have an independent source of energy generated specifically for the city.\n\nThe Eko Atlantic City project received global attention in 2009, as the Lagos State Government and its private sector partners on the Project, South Energyx, received the Clinton Global Initiative Commitment Certificate.\n\nEko Atlantic is master-planned to contain ten districts which are as follows:\n\n\nAs of May 2009 while the project was still in its dredging phase, about have been sand-filled and placed in the reclamation area, while about 35,000 tonnes of rock have been delivered to the site. In certain parts of Bar Beach the land being reclaimed can already be seen. Dredgers are working around the clock to fill the site with sand.\n\nOn 21 February 2013, a dedication ceremony was held at the reclaimed land of Eko Atlantic, with Goodluck Jonathan, Bill Clinton, Babatunde Fashola, Bola Tinubu, Aminu Tambuwal, and Ibikunle Amosun attending.\n\nIn March 2014, David Frame, managing director of South Energyx Nigeria Ltd., the firm responsible for the development, confirmed that \"The first residential tower will open in 2016\".\n\nThe Eko Atlantic project has been criticised by local residents living nearby, saying that ongoing construction works have caused coastal erosion and ocean surges; as ocean water surges through living areas, flooding access roads and taking down electricity poles and forcing residents to relocate. The Lagos State Government is also being criticised for failing to involve the people in the project.\n\nIn August 2012 the Atlantic Ocean surged and overflowed its banks, sweeping 16 people into the Atlantic Ocean, killing several people and flooding Kuramo Beach, Victoria Island and other areas. According to an environmental expert, \"the ocean surge occurred as a result of the failure of the contractors handling the sandfilling activities of the proposed Atlantic Ocean City, to put in place measure that would reduce the effect of the surge on the environment\". The Lagos State chapter of the People's Democratic Party issued an official statement, blaming the ACN now APC led state government's sand filling for the ocean surge. The party called for a stop to the Eko Atlantic project and immediate compensation to the bereaved families.\n\n\n"}
{"id": "2164694", "url": "https://en.wikipedia.org/wiki?curid=2164694", "title": "Elaeocarpus ganitrus", "text": "Elaeocarpus ganitrus\n\nElaeocarpus ganitrus, is a large evergreen broad-leaved tree whose seed is traditionally used for prayer beads in Hinduism and Buddhism. The seeds are known as rudraksha, or rudraksh, Sanskrit: \"\" (\"Rudra's Tear Drops\"). Rudraksha may be produced by several species of \"Elaeocarpus\"; however, \"E. ganitrus\" is the principal species used in the making of organic jewellery or \"mala\".\n\nRudraksha is a Sanskrit compound consisting of the name Rudra (\"Shiva\") and ' (\"Tear Drops\"). The specific epithet \"ganitrus\" is derived from \"ganitri\", the name for this species in Sundanese and Malay.\n\n\"Elaeocarpus ganitrus\" grows in the area from the Gangetic plain in the foothills of the Himalayas to South-East Asia, Nepal, Indonesia, New Guinea to Australia, Guam, and Hawaii. Rudraksha seeds are covered by an outer husk of blue colour when fully ripe, and for this reason are also known as blueberry beads. The blue colour is not derived from pigment but is structural. It is an evergreen tree that grows quickly. The rudraksha tree starts bearing fruit in three to four years. As the tree matures, the roots buttress rising up narrowly near the trunk and radiating out along the surface of the ground.\n\nChemical constituents present in \"E. ganitrus\" are elaeocarpidine, isoelaeocarpine, epiisoelaeocarpiline, rudrakine, flavonoids, quercetin, phytosterols, fat, alkaloids, carbohydrates, ethanol, proteins, tannins, gallic acid and ellagic acid.\n"}
{"id": "24262898", "url": "https://en.wikipedia.org/wiki?curid=24262898", "title": "Energy in Australia", "text": "Energy in Australia\n\nEnergy in Australia is the production in Australia of energy and electricity, for consumption or export. Energy policy of Australia describes the politics of Australia as it relates to energy.\n\nAustralia is a net energy exporter, and was the fourth-highest coal producer in the world in 2009.\n\nHistorically–and until recent times–energy in Australia was sourced largely from coal and natural gas, however due to the increasing effects of global warming and human-induced climate change on the global environment, there has been a greater shift towards renewable energy such as solar power and wind power both in Australia and abroad. This in turn has led to a decrease in the demand of coal worldwide.\n\nIn 2009, Australia had the highest per capita CO2 emissions in the world. At that time, Maplecroft's CO2 Energy Emissions Index (CEEI) showed that Australia releases 20.58 tons of CO2 per person per year, more than any other country. Since that time, however, emissions have been reduced. From 1990 to 2017, emissions per capita fell by one-third, with most of that drop occurring in the more recent years. Additionally, the emissions intensity of the economy fell by 58.4 percent during the same time period. These are the lowest values in 27 years.\n\nThe energy sector in Australia increased its carbon dioxide emissions by 8.2% from 2004 to 2010 on average. \n\nAccording to the International Energy Agency (IEA), global coal production increased 23% from 2005 to 2010 and 4.7% from 2009 to 2010. In Australia, coal production increased 12.9% between 2005 and 2010 and 5.3% between 2009 and 2010.\n\nIn 2009, Australia was the fourth-highest coal producer in the world, producing 335 megatonnes (Mt) of anthracite (black coal) and 64 Mt of lignite (brown coal). Australia was the biggest anthracite exporter, with 31% of global exports (262 Mt out of 836 Mt total). Lignite is not exported. 78% of its 2009 anthracite production was exported (262 Mt out of 335 Mt total). In this respect, Australia is an exception to most anthracite exporters. Australia's global anthracite export share was 14% of all production (836 Mt out of 5,990 Mt total).\n\nIn 2015, Australia was the biggest net exporter of coal, with 33% of global exports (392 Mt out of 1,193 Mt total). It was still the fourth-highest anthracite producer with 6.6% of global production (509 Mt out of 7,709 Mt total). 77% of production was exported (392 Mt out of 509 Mt total).\n\nNewcastle, New South Wales, is the world’s largest coal-export port. The Hunter Valley region in New South Wales is the chief coal region. Most coal mining in Australia is open cut.\n\nAustralia's oil production peaked in 2000, after gradually increasing since 1980. Net oil imports rose from 7% of total consumption in 2000 to 39% in 2006. Decreasing domestic oil production is the result of the decline of oil-producing basins and few new fields going online.\n\nAustralia's natural-gas reserves are an estimated 3,921 billion cubic meters (bcm), of which 20% are considered commercially proven (783 bcm). The gas basins with the largest recoverable reserves are the Carnarvon and Browse basins in Western Australia; the Bonaparte basin in the Northern Territory; the Gippsland and Otway basins in Victoria and the Cooper-Eromanga basin in South Australia and Queensland. In 2014–2015 Australia produced 66 bcm of natural gas, of which approximately 80% was produced in Western Australia and Queensland regions. Australia also produces LNG; LNG exports in 2004 were 7.9 Mt (10.7 bcm), 6% of world LNG trade. Australia also has large deposits of coal seam methane (CSM), most of which are located in the anthracite deposits of Queensland and New South Wales.\n\nOn 19 August 2009, Chinese petroleum company PetroChina signed a A$50 billion deal with American multinational petroleum company ExxonMobil to purchase liquefied natural gas from the Gorgon field in Western Australia, the largest contract signed to date between China and Australia. It ensures China a steady supply of LPG fuel for 20 years, forming China's largest supply of relatively clean energy. The agreement was reached despite relations between Australia and China being at their lowest point in years after the Rio Tinto espionage case and the granting of an Australian visa to Rebiya Kadeer.\n\nAustralia's oil shale resources are estimated at about 58 billion barrels, or 4,531 million tonnes of shale oil. The deposits are located in the eastern and southern states, with the greatest feasibility in the eastern Queensland deposits. Between 1862 and 1952, Australia mined four million tonnes of oil shale. The mining stopped when government support ceased. Since the 1970s, oil companies have been exploring possible reserves. From 2000 to 2004, the Stuart Oil Shale Project near Gladstone, Queensland produced over 1.5 million barrels of oil. The facility, in operable condition, is on care and maintenance and its operator (Queensland Energy Resources) is conducting research and design studies for the next phase of its oil-shale operations. A campaign by environmentalists opposed to the exploitation of oil-shale reserves may also have been a factor in its closure.\n\nSince 2005, wind power and rooftop solar have led to an increasing share of renewable energy in total electricity generation.\nDue to its large size and the location of its population, Australia lacks a single grid. \nAs of 2011, electricity producers in Australia were not building gas-fired power stations, while the four major banks were unwilling to make loans for coal-fired power stations, according to EnergyAustralia (formerly TRUenergy). In 2014, an oversupply of generation was expected to persist until 2024. However, a report published in 2017 by the Australian Energy Market Operator projected that energy supply in 2018 and 2019 is expected to meet demands, with a risk of supply falling short at peak demand times.\n\nFrom 2003 to 2013 real electric prices for households increased by an average of 72%. Much of this increase in price has been attributed to over-investment in increasing distribution networks and capacity, and environmental policy impacts. Further price increases are predicted to be moderate over the next few years (2017 on) due to changes in the regulation of transmission and distribution networks as well as increased competition in electricity wholesale markets as supply and demand merge.\n\nRenewable energy has potential in Australia, and the Climate Change Authority is reviewing the 20-percent Renewable Energy Target (RET). The production of 50 megawatts of wind power (power for nearly 21,000 homes annually) creates about 50 construction jobs and five staff positions. In recent years, wind and solar power have been the fastest growing source of energy in Australia.\n\nLower energy use could save A$25 billion, or A$840 per electricity customer, according to EnergyAustralia.\n\nAustralian total emissions in 2007 were 396 million tonnes of CO. That year, the country was among the top polluter nations of the world per capita. Australian per-capita emissions of carbon dioxide in 2007 were 18.8 tons of CO, compared to the EU average of 7.9 tons. The change in emissions from 1990 to 2007 was +52.5 percent, compared to the EU's -3.3 percent. The per-capita carbon footprint in Australia was rated 12th in the world by \"PNAS\" in 2011.\n\nDue to climate change, Australia is expected to experience harsher extreme weather events, mainly bush-fires and floods during summer. Rising sea levels are of particular concern for Australia, because most of the population lives in the coast (around 85%).\n\nWhen analysing employment data, the Australian Bureau of Statistics classifies the electricity and gas supply industry as part of the Electricity, Gas, Water and Waste Services Division. That division is the smallest industry in Australia in terms of employment.\n\nIn November 2017, the number of people employed in electricity supply, which includes electricity generation, transmission and distribution, was 64,200 (47,700 males, 16,600 females). The number of people employed in gas supply was 11,200 (9,000 males, 2,200 females). The total number of persons employed in electricity and gas supply industries was 75,400. This represents about 0.67 per cent of all employed persons in Australia. \n\nIn 2016, the major occupations in this Division were truck drivers (9,900), electricians (7,700), electrical distribution trades workers (5,400), and electrical engineers (4,400).\n\nIn 2015–16, annual direct full-time equivalent employment in renewable energy in Australia was estimated at 11,150. Employment in renewables peaked in 2011–12, probably due to the employment of construction workers to build renewable energy facilities. However, it decreased by 36 per cent in 2014–15, and by a further 16 per cent in 2015–16. The decline is attributed to a decrease in the number of roof-top solar photovoltaic systems being installed on the roofs of homes. Once construction of renewable energy facilities is completed, and only ongoing maintenance is required, employment falls quite significantly.\n\nFor most Australian states and territories the major contributor to employment in renewable energy is solar power. Employment in roof-top solar photovoltaic systems, including solar hot water systems, comprised half of all employment in renewable energy in 2015–16. Employment in large scale solar and wind power is driven primarily by installation activity, rather than ongoing operation and maintenance..\n\nIn Western Australia, 93 per cent of all jobs in renewable energy are in solar power. The proportion of employment in biomass is significantly greater in Queensland (42 per cent), where the sugar industry makes great use of sugar cane to generate electricity for sugar milling and to return to the grid. Most jobs in Tasmania’s renewable energy industry are in hydropower (87 per cent).\n\nJobs in the renewable energy industry are forecast to grow substantially by 2030, driven by growth in electricity demand and new renewable energy capacity. Conversely, jobs associated with coal-fired power stations are forecast to decline as those plants age and close. Such job losses would disproportionately affect some regional areas, such as the Latrobe Valley in Victoria, Newcastle and the Hunter Valley in New South Wales, Gladstone and Rockhampton in Queensland, and Collie in Western Australia. However, it is expected that the number of jobs created in renewable energy will far exceed the number of jobs lost in coal-based generation.\n\nIn June 2017 Alan Finkel released \"The Independent Review into the Future Security of the National Electricity Market\" (commonly referred to as the Finkel Report), which proposed an approach to increasing energy security and reliability through four outcomes. These would be: increased security, future reliability, rewarding consumers, and lower emissions. The report ultimately recommended a Clean Energy Target (CET) to provide incentives for growth in renewable energies.\n\nThe reaction to the report by scientific experts in the field leaned more towards positive. Positive reactions to the Report were due to the national strategy plan that provides a CET for Australia, creating customer incentives, and takes politics out of energy policy to help meet the Paris Agreement. Additionally, the Finkel Report was commended for recognizing the current technologies available and including market forces in its solutions by the Australian Academy of Technology Engineering.\n\nOn 17 October 2017, the Australian Government rejected Finkel's CET proposal, in favour of what it called the National Energy Guarantee (NEG), to reduce power prices and prevent blackouts. The strategy calls on electricity retailers to meet separate reliability and emissions requirements, rather than Dr Finkel’s CET recommendation. Under the plan, retailers will have to provide a minimum amount of baseload power from coal, gas or hydro, while also providing a specified level of low emissions energy. NEG has been criticised as turning away from renewable energy.\n\n"}
{"id": "48059648", "url": "https://en.wikipedia.org/wiki?curid=48059648", "title": "Energy in Mongolia", "text": "Energy in Mongolia\n\nMongolia had a total primary energy supply (TPES) of 3.94 Mtoe in 2012. Electricity consumption was 4.49 TWh. \nMongolia is a big producer of coal, which is mostly exported. Domestic consumption of coal accounts for about two thirds (66%) of Mongolia's primary energy and is the almost sole source of electricity, accounting for almost 95% of the domestic electricity production.\n\n"}
{"id": "52249821", "url": "https://en.wikipedia.org/wiki?curid=52249821", "title": "Floating mat", "text": "Floating mat\n\nA floating mat () is a layer of mosses and other, especially stoloniferous, plants that grows out from the shore across the surface of a lake or pond. This type of habitat is protected and is designated in the European Habitats Directive as \"LRT No. 7140 Transition and Floating Mat Bogs\".\n\nFloating mats are not always capable of bearing weight. There is a risk of drowning when walking on them.\n\nThe formation of floating mats is a process of sedimentation in water bodies. In bog ponds, floating mats of peat moss form as water levels fall and nutrients accumulate. In eutrophic waters, the formation of floating mats is caused by underwater peat that floats to the surface and is colonised by plants. The vegetative mats are held together by their root systems. Reeds or rushes growing by the lakeshore can eventually cut these mats off, which results in a floating island. If the floating mats are stable and large enough, even trees can grow on them, as at the Kleiner Arbersee. Beneath the floating mats, peat is formed, which slowly sinks downwards and gradually fills the water body.\n\nIn nutrient-poor to moderately nutrient-rich, acidic waterbodies, floating mats form out of peat mosses, (feathery bogmoss \"Sphagnum cuspidatum\", species of the complex \"Sphagnum recurvum\" s.l.) or brown mosses (\"Scorpidium scorpioides\"). Furthermore, floating mats are colonised by characteristic species of the small sedges such as the bog sedge (\"Carex limosa\"), (\"Carex rostrata\"), beak sedge (\"Rhynchospora\" ssp.), Rannoch-rush (\"Scheuchzeria palustris\") and marsh cinquefoil (\"Potentilla palustris\"). The edges of nutrient-rich waterbodies are colonised by reeds (\"Phragmites australis\"), bulrushes (\"Typha\" ssp.), hop sedge (\"Carex pseudocyperus\") and cowbane (\"Circuta virosa\"). Floating mats are habitats for shelly amoeba such as \"Amphitrema\" sp. and lake fly larvae.\n\n\n\n"}
{"id": "5905120", "url": "https://en.wikipedia.org/wiki?curid=5905120", "title": "Gadfly (mythology)", "text": "Gadfly (mythology)\n\nThe gadfly, a type of fly plaguing cattle, typically ones belonging to either the family Tabanidae (horse-flies) or the family Oestridae (bot flies), appears in Greek mythology as a tormenter to Io, the heifer maiden. Zeus lusts after Io and eventually turns her into a white heifer to hide her from his jealous wife, Hera. Hera is not fooled, and demands Io as a gift from Zeus. She then assigns Argus, the 100-eyed monster, the job of guarding Io. Hermes (ordered by Zeus) kills Argus and frees Io. When Hera finds out, she sends a gadfly to torment and sting Io, forcing her to wander farther and farther away from home.\n\nThe gadfly also plays a role in the myth of how Bellerophon loses Pegasus and the gods' favor. Bellerophon attempts to ride Pegasus to the top of Mt. Olympus, arrogantly believing himself worthy of entering the realm of the gods. Zeus is enraged by the human's audacity and sends a gadfly to sting Pegasus. The winged horse is startled and he rears backward. Bellerophon loses his grip and falls back to Earth. Athena spares his life by causing him to land on soft ground, but he becomes blind and wanders the earth alone until he dies, hated by both men and gods.\n"}
{"id": "511023", "url": "https://en.wikipedia.org/wiki?curid=511023", "title": "HOME STAR", "text": "HOME STAR\n\nHOME STAR, (also spelled \"HOMESTAR\"), informally known as Cash for Caulkers, is a United States government program proposed in November 2009 to encourage economic growth by offering incentives to homeowners and retailers for improving the energy efficiency of existing homes.\n\nIn late 2009 there was a broad perception that the United States economy was beginning to recover from the Late-2000s recession. There was a broad perception that government spending authorized by the American Recovery and Reinvestment Act of 2009 had contributed to the recovery, and some desire for the government to do more to encourage job growth and a faster recovery.\n\nIn mid-November former president Bill Clinton, and John Doerr of Barack Obama's President's Economic Recovery Advisory Board, proposed different versions of an economic stimulus program by which the government would offer tax incentives to encourage people to improve the energy efficiency of their homes. Doerr, in public speeches, called the proposal \"cash for caulkers\". Separately U.S. Representative Peter Welch proposed a system of energy rebates to Rahm Emanuel, Obama’s Chief of Staff. Obama, in turn, proposed the idea as part of a larger new stimulus program, at a speech at the Brookings Institution on December 8, 2009.\n\nThe stated goals of the proposed program are to reduce pollution, particularly greenhouse gases, by reducing household energy use, to save consumers money in the long term through lower power bills, and to stimulate American businesses through the money spent on appliances, materials, and installation. Improving the energy efficiency of \"fixed infrastructure\", which accounts for approximately 40% of all energy use in the United States, is considered the \"low hanging fruit\" of energy conservation - a step that achieves results relatively inexpensively and does not require any new technologies or changes to production or consumption methods.\n\nThe name \"Homestar\" is a reference to the popular energy star electronic device efficiency rating system, and the nickname \"Cash for Caulkers\" is a play on the earlier cash for clunkers automobile trade-in incentive.\n\nAs of December 2009, no proposed legislation had been released, and there were few specific details of how the program would be administered, which federal agencies would be involved, or how the tax incentives would be paid (or to whom). The program is expected to involve preliminary energy audits by private contractor energy experts, who then recommend a series of steps for each homeowner to upgrade their home's energy efficiency. As proposed the plan was for the government to pay 50% of the cost of each home improvement project through a rebate, tax credit, or funds paid to manufacturers and retailers, up to a maximum of $12,000 paid for each home. Alternatively, there was speculation that the federal government might give funds to local governments to run their own programs. There was no limitation on eligibility based on tax bracket or income.\n\nItems under consideration for the program included weatherization of home by installing additional insulation, new doors, and windows, and replacing old appliances with more energy-efficient new ones. Expensive items such as washing machines, dishwashers, refrigerators, air conditioners, and heaters, would be covered.\n\nThe program was expected to cost approximately $10 billion over the course of one year, paid for out of unspent Troubled Asset Relief Program funds, and would reduce energy consumption of homes that took full advantage of the program by up to 20%. To become effective it would have to be part of a bill passed by the United States Congress.\n\n"}
{"id": "44210467", "url": "https://en.wikipedia.org/wiki?curid=44210467", "title": "Hitler Stalingrad Speech", "text": "Hitler Stalingrad Speech\n\nThe Hitler Stalingrad Speech was an address made by Adolf Hitler to senior members of the Nazi Party on November 8, 1942. The speech took place at the Löwenbräukeller in Stiglmaierplatz in Munich during the height of the Battle of Stalingrad. For three-quarters of his oratory, Hitler speaks in a normal tone of voice, at one point making a joke, and only raising his voice at the end of his narrative. The speech is, along with the Mannerheim recording, one of very few recordings in which Hitler is heard speaking completely normally.\n\nThe first segment of Hitlers speech berates Soviet military ability, and Hitler cites several examples of Soviet military leaders exaggerating casualty figures.\n\nAfter a brief applause, Hitler resumes his speech and explains his reasoning for attacking Stalingrad. In his narrative, Hitler becomes almost comical, and suggests that the city of Stalingrad could have had another name (from Stalin) and this wasn't why he ordered his Army to attack. The comment drew a round of laughter from the assembled audience.\n\nIn the final segment of the speech, Hitler provides statistics for the strategic importance of Stalingrad and then proclaims that Germany now holds the city.\n\nThe Hitler Stalingrad Speech is portrayed in the film \"Stalingrad\" where a group of embattled Wehrmacht soldiers, entrenched from positions within the city of Stalingrad itself, listen to Hitler while they are in turn surrounded by Soviet forces. This speech is also featured in an episode of the 1988 miniseries \"War and Remembrance,\" when Hitler was addressing party faithful. It occurred on the same day as the Allied invasion of North Africa.\n\n\n"}
{"id": "56785809", "url": "https://en.wikipedia.org/wiki?curid=56785809", "title": "IGR J17329-2731", "text": "IGR J17329-2731\n\nIGR J17329-2731 as described by European Space Agency astronomers is a single faint transient X-ray source (ATel #10644) observed with Swift/XRT on 16 August 2017 from 2:26 to 2:45 UTC with an effective exposure of time of 1 ks. It was detected within the positional uncertainty provided by INTEGRAL IBIS imagery. It was described as the birth of a symbiotic X-ray binary, a \"first\" in the lifecycle of an interacting binary star, or a zombie neutron star brought back to life by its neighboring red giant. When first described in 2017, it was seen as an X-ray flare \"from an unknown source\" in the direction from the galactic (Milky Way) center.\n\n"}
{"id": "5863006", "url": "https://en.wikipedia.org/wiki?curid=5863006", "title": "Iron–nickel alloy", "text": "Iron–nickel alloy\n\nAn iron–nickel alloy or nickel–iron alloy, abbreviated FeNi or NiFe, is a group of alloys consisting primarily of the elements nickel (Ni) and iron (Fe). It is the main constituent of the \"iron\" planetary cores and iron meteorites. The acronym NiFe refers to various chemical reactions that involve an iron–nickel catalyst or component, or in geology, to the general composition of planetary cores (including Earth's).\n\nSome iron–nickel alloys are called nickel steel and usually contain additional elements, depending on the purpose.\n\nIron and nickel are notable for being the final elements produced by stellar nucleosynthesis, and the heaviest elements formed with a supernova or similarly cataclysmic event. Iron and nickel are the most abundant metals in metallic meteorites and in the dense metal cores of planets such as Earth.\n\nNickel–iron alloys occur naturally on Earth's surface as telluric or meteoric iron.\n\nThe affinity of nickel atoms (atomic number 28) for iron (atomic number 26) results in natural occurring alloys and a large number of commercial alloys, and provides a complex electron environment for catalyzing chemical reactions.\n\nIn steel metallurgy, nickel is alloyed with iron to produce maraging steel and some low-alloy steels. Other technological uses include Invar and Mu-metal.\n\nThe following table is an overview of different iron–nickel alloys. Naturally occurring alloys are a type of mineral and called native elements or native metals. Some of the entries have more than one crystal structure (e.g. meteoric iron is a mixture of two crystal structures).\n\n"}
{"id": "876994", "url": "https://en.wikipedia.org/wiki?curid=876994", "title": "Island of the Blue Dolphins", "text": "Island of the Blue Dolphins\n\nIsland of the Blue Dolphins is a 1960 children's novel by American writer Scott O'Dell, which tells the story of a 12-year-old girl stranded alone for years on an island off the California coast. It is based on the true story of Juana Maria, a Nicoleño Native American left alone for 18 years on San Nicolas Island during the 19th century.\n\n\"Island of the Blue Dolphins\" won the Newbery Medal in 1961. It was adapted into a film of the same name in 1964. O'Dell later wrote a sequel, \"Zia\", published in 1976.\n\nThe 50th Anniversary edition of \"Island of the Blue Dolphins\" includes a new introduction by Newbery Medalist Lois Lowry and also includes extracts from Father Gonzales Rubio in the Santa Barbara Mission's Book of Burials. \"Island of the Blue Dolphins: The Complete Reader's Edition\", a critical edition edited by Sara L. Schwebel, was published in October 2016 by the University of California Press. It includes two chapters deleted from the book before publication.\n\nThe novel is based on the true story of \"The Lone Woman of San Nicolas Island,\" a Nicoleño Native Californian left alone for 18 years on San Nicolas Island, one of the Channel Islands off the California coast, before being discovered and taken to the mainland in 1853 by sea otter hunter George Nidever and his crew. She is on record under the Christian name Juana Maria, assigned to her by the Santa Barbara Mission where she eventually was brought. No one alive spoke her language. According to Nidever, the Lone Woman lived in a structure supported by whale ribs and stashed useful objects around the island. In 2009, University of Oregon archaeologist Jon Erlandson found two old redwood boxes eroding from an island sea cliff, with a whale bone placed on top of them. With colleagues René Vellanoweth, Lisa Barnett-Thomas, and Troy Davis, Erlandson salvaged the boxes and other artifacts before they were destroyed by erosion. Vellanoweth and Barnett-Thomas later excavated the interior of the boxes in a San Nicolas Island laboratory and documented nearly 200 artifacts of Nicoleño, Euro-American, and Native Alaskan manufacture. The boxes appear to have been cached intentionally some time between 1815 and 1853, quite possibly by Juana Maria herself. It was also believed the Lone Woman lived in a cave on the island. In 2012, Naval archaeologist Steve Schwartz believed he discovered the buried location of that cave and began an investigation, working with archaeologist René Vellanoweth and his students from California State University, Los Angeles. Commanders at the Navy base on the island about 65 miles southwest of Point Mugu ordered Schwartz to halt the dig in 2015.\n\nThe main character is a Native American girl named Won-a-pa-lei, secret name is Karana. She has a brother named Ramo, whose curiosity usually leads to trouble, and a sister named Ulape. Her people live in a village called Ghalas-at and the tribe survives by means of gathering roots and fishing.\n\nOne day, a ship of Russian fur hunters and Aleut people led by Captain Orlov arrive and persuade the natives to let them hunt sea otter in exchange for other goods. However, the Russians attempt to swindle the islanders and leave without paying. When they are confronted by Karana's father Chief Chowig, a battle breaks out. The tribe is reduced with Karana's father and many other men in the tribe dying in battle against the well-armed Russians who escape largely unscathed.\n\nLater, the \"replacement chief\" Chief Kimki leaves the island on a canoe for new land in the East. Eventually, he sends a \"giant canoe\" to bring his people to the mainland even though he himself does not return. The white men who are missionaries come to Karana's village and tell them to pack their goods and go to the ship. Karana's brother Ramo misses the ship to retrieve his fishing spear. Although Karana urges the captain to wait for Ramo to return, the ship must leave before a storm approaches. Despite restraint, Karana jumps off the ship and swims to shore and the ship departs without them.\n\nThe siblings live alone on the island, hoping the ship will return. However, Ramo is brutally killed by a pack of feral dogs. Alone on the island, Karana takes on traditionally male tasks, such as hunting, making spears, and building canoes to survive. She vows to avenge her brother's death and kills several of the dogs, but has a change of heart when she encounters the leader of the pack. She tames him and names him Rontu (meaning \"Fox Eyes\" in her language).\n\nOver time, Karana makes a life for herself. She builds a home made of whale bones and stocks a cave with provisions in case the Aleuts come back, so she can hide from them. As she explores her island, Karana discovers ancient artifacts and a large squid (which she calls a devilfish). As time passes, she decides to hunt the devilfish. She also tames some birds and an otter while feeling a close kinship to the animals (the only inhabitants of the island beside herself).\n\nOne summer, the Aleuts return and Karana takes refuge in the cave. She observes the Aleuts closely and realizes that a girl named Tutok takes care of the domestic duties including getting water from the pool near Karana's cave. Fearful of being discovered, Karana goes out only at night, yet the curious girl stalks Karana and the two meet. Karana and Tutok meet several days in a row. However, when she lets Rontu out with her, Tutok calls him hers. Karana and Tutok exchange gifts, and she realizes how lonely she has been. Karana wishes that Tutok would not leave, yet the next day when Karana makes food for her she does not come. Karana goes searching and sees the ship departing. Sadly, she returns to her house and starts rebuilding.\nMore time passes and Rontu dies. Karana soon finds a young dog that looks like Rontu and takes him in naming him Rontu-Aru (meaning \"Son of Rontu\"). One day, Karana sees the sails of a ship. It docks at the shore, but then leaves. Two years later in the spring, the boat comes back, so Karana dresses in her finest attire and goes to the shore to meet the boat. Her rescuers see that her dress made of cormorant feathers is not appropriate for the mainland and they have a dress made for her. She does not like the dress, but Karana realizes that it is part of her new life. The ship takes Karana and Rontu-Aru to the mission in Santa Barbara, California. There, she finds out that the ship that had taken her people away had later sunk before it could return from the mainland for her.\n\nA film adaptation of \"Island of the Blue Dolphins\" was released on July 3, 1964. It was directed by James B. Clark and starred Celia Kaye as Karana. Jane Klove and Ted Sherdeman adapted the script from O'Dell's novel, and the film was produced by Robert B. Radnitz and Universal Pictures. The film was made on a slight budget but did receive a wide release three months after its New York premiere. The New York Times's Howard Thompson gave the film a rather condescending review upon its release, saying it was strictly a children's film. However, Kaye won a Golden Globe Award for New Star of the Year for her performance. The film earned an estimated $2 million in rentals in North America.\n\n"}
{"id": "17454819", "url": "https://en.wikipedia.org/wiki?curid=17454819", "title": "John Stewart Collis", "text": "John Stewart Collis\n\nJohn Stewart Collis (1900–1984) was an Irish biographer, rural author, and pioneer of the ecology movement. He is known for his book \"The Worm Forgives the Plough\" based on his wartime experience working in the Land Army in the Second World War.\n\nThe son of an Irish solicitor, Collis was born at Kilmore on the borders of County Dublin and County Wicklow, Ireland. He was educated at Bray preparatory school, Rugby School and Balliol College, Oxford. At the Oxford Union he learnt the art of public speaking, hearing politicians and authors including H. H. Asquith, G. K. Chesterton, Lloyd George and W. B. Yeats in the debating chamber. In the 1920s he became a close friend of the Guernsey-born G.B. Edwards, who lodged at his flat in Guildford Street. Both men became protégés of John Middleton Murry and contributed to \"The Adelphi\" magazine but later drifted apart. Collis, however, wrote an enthusiastic review of Edwards's \"The Book of Ebenezer Le Page\" in the \"Spectator\" when this novel was posthumously published in 1981.\n\nCollis's first book, a biography of George Bernard Shaw, was published in 1925, followed by biographies of Havelock Ellis, August Strindberg, Leo Tolstoy, the Carlyles and Christopher Columbus.\n\nCollis is remembered largely for \"While Following the Plough\" (1946) and \"Down to Earth\" (1947: as one volume, \"The Worm Forgives the Plough\", 1973).\n\n\"While Following the Plough\" was inspired by the years he chose to spend working as a farm labourer in the Land Army (which mainly consisted of women, known as \"land girls\") in Sussex and Dorset during the Second World War. He worked at J. G. Maynard's farm at Stonegate in Sussex in 1940 for a year, and then moved to Tarrant Hinton in Dorset for the rest of the war. The manuscript of the book was rejected by twelve publishers. At last, Jonathan Cape accepted it, but requested much revision. Collis waited for a while and returned the manuscript largely unchanged, thanking the publisher for their suggestions; Jonathan Cape answered saying they were \"delighted with the improvements\".\n\n\"Down to Earth\" was inspired by the year he spent working for Rolf Gardiner after the war, thinning a fourteen-acre wood on his own, using only an axe and other hand tools.\n\nThe novelist Margaret Drabble states that Collis wrote with imagination and authenticity about rural life, and that Collis's autobiography \"Bound upon a Course\" brought him belated recognition as a pioneer in the ecology movement.\n\nThe travel writer Robert Macfarlane praises his country writing as follows:\n\nThe biographer Michael Holroyd comments that \"While Following the Plough\" and \"Down to Earth\" were \"acclaimed on their appearance\", and are now considered classics.\n\nThe writer and paediatrician Robert Collis was his twin and Maurice Collis, writer and biographer, was his elder brother.\n\nIn 1986 Richard Ingrams wrote \"John Stewart Collis: A Memoir\" (Chatto & Windus). A 2009 edition of \"The Worm Forgives the Plough\", with an introduction by Robert Macfarlane, was issued by Vintage Classics.\n\n\n"}
{"id": "26913976", "url": "https://en.wikipedia.org/wiki?curid=26913976", "title": "Korravai", "text": "Korravai\n\nKorravai () or Korravi was the goddess of war and victory in ancient Tamil pantheon. She was considered the mother of Murugan, the Hindu god of war, now patron god of Tamil Nadu. The earliest references to Korravai are found in the ancient Tamil grammar Tolkappiyam, considered to be the earliest work of the ancient Sangam literature. Korravai is identified with goddess Durga. In early iconography, Korravai is presented as fierce and bloodthirsty.\n\nhttps://commons.wikimedia.org/wiki/File:Durga_Korravai.jpg\n\nHarvest and war was an important aspect in life of the ancient Tamils and they worshiped Korravai for the success on field and battlefield. It was a custom among ancient Tamil warriors and chieftains to offer the heads of slain enemies to goddess Korravai. The ancient Tamil script mentions that the devotees in a frenzy offer their own head to the goddess. Avipalli was mentioned in all the works except \"Veera Soliyam\". It was a self-sacrifice of a warrior to the goddess of war for the victory of his commander. During the Brahmanization of Tamil country, Korravai was adopted in Hindu pantheon and assigned to the goddess Durga, Kali and Parameswari. The epic Silappadikaram, clearly show that Korravai was completely absorbed in brahmanical tradition by merging with Durga, which makes here to sister of Vishnu and consort of Shiva\n\nIn Tamil Nadu, the blackbuck (Kalaimaan) is considered to be the vehicle of the Hindu goddess Korravai \n\nTo illustrate Korravai's place in the metaphysical world of the earliest sources, Kersenboom-Story provides a \"tentative\" fivefold classification of the disposition of the major spiritual powers.\n\nAccording to the early Tamil literature, the divine manifests itself in various shapes, shades and degrees of intensity. In most cases it is thought of as a power that is highly ambivalent: possibly benevolent, but usually dangerous and even malevolant. The most striking aspect of man's relation to these different manifestations is his attempt to control them by means of some type of 'dramatic performance'. True evil is too powerful to be dealt with by humans and has to be subdued by the god Murugaṇ. ... Tentatively, we classify the manifestations of the divine as follows:\n\n"}
{"id": "11530827", "url": "https://en.wikipedia.org/wiki?curid=11530827", "title": "Lake Saka Nature Reserve", "text": "Lake Saka Nature Reserve\n\nThe Lake Saka Nature Reserve () is a nature reserve at Sivriler village of Demirköy district in Kırklareli Province of Turkey close to İğneada on the Black Sea coast. It gets its name from the Lake Saka (), which is in the boundaries of the İğneada Floodplain Forests National Park. It is from Demirköy, and from İğneada.\n\nIt covers an area of . The sits is on a floodplain of a river that flows into the Black Sea, and seasonally floods thus rendering it a unique and one of the very few remaining floodplain forests in all of Europe.\n\nIn addition to the main flora of alder (\"Alnus\"), elm (\"Ulmus\") and ash (\"Fraxinus\"), trees such as oak (\"Quercus\"), hornbeam (\"Carpinus\"), beech (\"Fagaceae\"), black poplar (\"Populus nigra\"), white poplar (\"Populus alba\"), willow (\"Salix\"), linden (\"Tilia\") and walnut (\"Juglans\") are found in the nature reserve.\n\nThe fauna of the protected area consists of the mammals: deer (\"Cervidae\"), roe deer (\"Capreolus capreolus\"), fox (\"Canidae\"), gray wolf ((\"Canis lupus\"), hare (\"Lepus\"), wild boar (\"Sus scrofa\"), wildcat (\"Felis silvestris\") and the birds swan (\"Cygnus\"), mallard (\"Anas platyrhynchos\"), greylag goose (\"Anser anser\"), woodcock (\"Scolopax\") and common wood pigeon (\"Columba palumbus\"). The nature reserve is habitat for the reptiles such as viper (Viperidae), slow-worm (\"Anguis fragilis\") and water snake. The lake is home to the fish species of carp (\"Cyprinidae\"), red seabream, bass (\"Perciformes\") as well as to crustaceans like crayfish. \n\nAbout (in Turkish)\n"}
{"id": "6958105", "url": "https://en.wikipedia.org/wiki?curid=6958105", "title": "Limpopo Belt", "text": "Limpopo Belt\n\nThe Limpopo Belt is located in South Africa and Zimbabwe, runs E-NE, and joins the Kaapvaal craton to the south with the Zimbabwe craton to the north. The belt is of high-grade metamorphic rocks that have undergone a long cycle of metamorphism and deformation that ended 2.0 billion years ago, after the stabilisation of the adjacent massifs. The belt comprises 3 components: the Central Zone, the North Marginal Zone and the South Marginal Zone.\n\nThe 250 km wide Limpopo belt of southern Africa is an east-northeast trending zone of granulite facies tectonites separating the granitoid-greenstone terranes of the Kaapvaal and Zimbabwe cratons. Large scale ductile shear zones are an integral part of Limpopo belt architecture. They define the boundaries between the belt and the adjacent cratons and separate internal zones within the belt. The shear zones forming the external (northern, southern and western) margins of the belt are interpreted as uplift structures of the overthickened crust.\n\nThe crustal evolution of the Limpopo Central Zone can be summarized into three main periods: 3.2-2.9 Ga, ~2.6 Ga, and ~2.0 Ga. The two first periods are mainly characterized by magmatic activity leading to the formation of Archaean Tonalite-Trondhjemite-Granodiorite (TTG) such as the Sand River Gneisses or the Bulai Granite intrusion. The Early Proterozoic event took place under high-grade metamorphic conditions during which partial melting formed large amount of granitic melt.\n\nThe Limpopo Central Zone shows relics of late Archean high grade metamorphism. In the Northern (NMZ) and Southern Marginal Zones (SMZ) that adjoin the Zimbabwe and Kaapvaal cratons, respectively, the last high grade metamorphic episodes were late Archean. The relics in the Central Zone are characterised by an anticlockwise p-T-evolution, at ca. 2.55 Ga. In the NMZ repeated crustal remelting and intrusion of charnoenderbitic magmas continued to 2.58 Ga, producing counterclockwise p-T paths. In contrast the SMZ consists of medium to high pressure granulites, which underwent a clockwise p-T-evolution at ca. 2.69 Ga, followed by decompression and isobaric cooling. In the Northern Kaapvaal Craton (Renosterkoppies Greenstone Belt, Pietersburg area) tectonism took place under amphibolite facies conditions at ca. 2.75 Ga and can therefore not be related to any events in the Limpopo belt. Thus the different tectonic units have different late Archean tectonometamorphic histories. Trace element geochemistry as well as Pb + Nd isotope characteristics of the SMZ are similar to those of the Kaapvaal Craton, with low Th and U concentrations around 2 and 0.7 ppm. Low U concentrations in the SMZ are not a consequence of high grade metamorphism. The NMZ, with high Th, U concentrations (10.8 and 2.5 ppm) and radiogenic Pb, resembles the adjoining Zimbabwe craton more. The differences in late Archean tectonic styles between NMZ and SMZ+KC is a possible consequence of the differences in Th and U content of these provinces. \n\n\"The Mahalapye granite in the extreme western part of the Central Zone is a post-tectonic intrusion that crystallized at 2.023 Ga. This suggests that mineral ages of ca 2.0 Ga from the eastern part of the Central Zone date metamorphism during reworking of Archaean age shear zones rather than a collision between the Kaapvaal and Zimbabwe cratons as has been earlier suggested.\n\n"}
{"id": "16398789", "url": "https://en.wikipedia.org/wiki?curid=16398789", "title": "List of Dungeons &amp; Dragons 3rd edition monsters", "text": "List of Dungeons &amp; Dragons 3rd edition monsters\n\n\"Dungeons & Dragons\" 3rd Edition (see editions of \"Dungeons & Dragons\") was released in 2000. The first book containing monsters to be published was the \"Monster Manual\", released along with the other two \"core\" rulebooks. Wizards of the Coast officially discontinued the 3rd Edition line upon the release of a revision, known as version 3.5, in 2003, with the \"Monster Manual\" reprinted for the revised edition.\n"}
{"id": "33934947", "url": "https://en.wikipedia.org/wiki?curid=33934947", "title": "List of European windstorms", "text": "List of European windstorms\n\nThe following is a list of notable European windstorms.\n\n\n"}
{"id": "40620188", "url": "https://en.wikipedia.org/wiki?curid=40620188", "title": "List of ecoregions in Honduras", "text": "List of ecoregions in Honduras\n\nThis is a list of ecoregions in Honduras as defined by the World Wildlife Fund and the Freshwater Ecoregions of the World database.\n\n\n\n\n\n\n\n"}
{"id": "34897019", "url": "https://en.wikipedia.org/wiki?curid=34897019", "title": "List of lakes in Jyväskylä", "text": "List of lakes in Jyväskylä\n\nThere are 328 lakes in Jyväskylä, Finland. All of the lakes are listed here. Coordinates are given if there are many lakes with a same name.\n\nAhostenlampi, Ahvenlampi (coordinates) 14.239.1.004, Ahvenlampi 14.239.1.003, Ahvenus, Ala-Aitolampi, Ala-Kitulampi, Alainen Ruokepuolinen, Alainen Vehkajärvi, Alalampi, Alvajärvi-Korttajärvi, Ankeriasjärvi, Askeleentakanen, Asmalampi, Auvanen\n\nHaapalampi, Hangasjärvi 14.282.1.001, Hangasjärvi 14.282.1.011, Hanhijärvi, Hanhilampi, Haukilampi 14.317.1.015, Haukilampi 14.285.1.015, Haukilampi 14.295.1.011, Haukkalampi, Heinonen, Heinälampi, Hettee, Hietajärvi, Hiidenjärvi, Hirvijärvi, Hoikanjärvi, Hoikka, Honkalampi, Huhtalampi, Huhtilampi, Humalalampi, Hupelinlampi, Huujärvi, Höyhenisjärvi\n\nIso Hanslampi, Iso Heinäjärvi, Iso Housujärvi, Iso Joutenlampi, Iso-Kairahta, Iso-Kaukuu, Iso Kalliolampi, Iso Koirajärvi, Iso Koukkujärvi, Iso-Kovalainen, Iso-Kuukkanen, Iso Lampsinjärvi, Iso-Musta, Iso-Peräinen, Iso Peräjärvi, Iso-Soukka, Iso-Vasarainen, Iso Vihtajärvi, Iso Vääräpää, Isojärvi\n\nJoutee, Juoneenjärvi, Juurikkaanjärvi\n\nKaakkolampi (14.294.1.008), Kaakkolampi (14.295.1.017), Kaakkolampi (14.298.1.003), Kaakkolampi (14.312.1.006), Kaakonlampi, Kaijanlampi, Kaitajärvi (14.221.1.182), Kaitajärvi (14.221.1.200), Kaitajärvi (14.239.1.007), Kaitajärvi (14.293.1.007), Kaitalampi, Kalaton, Kaleton, Kalliojärvi (14.221.1.166), Kalliojärvi (14.284.1.012), Kalliojärvi (14.286.1.013), Kalliolampi (14.237.1.005), Kalliolampi (14.239.1.006), Kamppijärvi, Kangasjärvi, Kangaslampi (14.294.1.002), Kangaslampi (14.312.1.004), Karhujärvi, Karisjärvi (14.221.1.172), Karisjärvi (14.221.1.195), Kaukkaanlampi, Kauralampi, Kennäälänlampi, Kepolampi, Keski-Kännää, Keskinen, Keskinen Kattilajärvi, Keskinen Naulajärvi, Keskinen Vehkajärvi, Killervä, Kinnaslampi, Kivelän Viemärilampi, Kivijärvi (14.231.1.029), Kivijärvi (14.281.1.004), Kivilampi, Koiralampi, Kolmisoppinen, Kolujärvi, Korpijärvi, Korttajärvi, Koskenlampi, Kotajärvet Isompi, Kotajärvet Pienempi, Kotalampi (14.232.1.004), Kotalampi (14.283.1.009), Kotanen, Koukkujärvi, Koulun Viemärilampi, Kovaslammi, Koveroinen, Kuivalampi, Kukkarojärvi, Kumpulampi, Kumpulampi-Likolampi, Kurppa, Kuusjärvi, Köhniönjärvi, Köntyslampi\n\nLahnajärvi, Lahnalampi, Lapinjärvi, Lautakkojärvi, Lauttalampi (14.231.1.002), Lauttalampi (14.285.1.026), Lehesjärvi-Vähäjärvi, Lehmilampi, Leppävesi, Lepäslampi, Liinalampi (14.296.1.015), Liinalampi (14.311.1.014), Likolampi, Luhtajärvi, Lukkonen, Lummejärvi, Luonetjärvi, Luotojärvi, Löytänelampi\n\nMaahinjärvi, Maatianjärvi, Mainiotlammit, Majajärvi (14.293.1.005), Majajärvi (14.296.1.003), Marjojärvi, Masonjärvi, Metsä-Viemärilampi, Moksinjärvi, Moksinmyllylampi, Mustajärvi (14.231.1.018), Mustajärvi (14.285.1.020), Mustalammi, Mustalampi (14.232.1.006) (0,024 km²), Mustalampi (14.285.1.012), Mustalampi (14.285.1.025), Mustalampi (14.291.1.005), Mustalampi (14.295.1.018), Mutalampi, Muurikaisjärvi, Myllyjärvi (14.221.1.187), Myllyjärvi (14.231.1.003), Myllyjärvi (14.232.1.001), Myllyjärvi (14.284.1.013), Myllyjärvi (14.287.1.006), Myllyjärvi (14.293.1.001), Myllyjärvi (14.294.1.009), Myllylampi (14.231.1.005), Myllylampi (14.295.1.014), Mysiönlampi, Mäkijärvi, Mäyrälampi, Mökkilänjärvi, Mörkölampi,\n\nNameless (14.231.1.042), Nameless (14.231.1.043), Nameless (14.274.1.017), Nameless(14.285.1.029), Nameless (14.295.1.013), Naskuttajanlampi, Neulajärvi, Neulalampi, Niemijärvi, Niinijärvi (i), Niinijärvi (p), Nokkoslampi, Nälkäjärvi\n\nOnkilampi, Orajärvi,\n\nPahalampi, Pahkalampi, Painaanjärvi, Pajulampi, Palokkajärvi, Palvajärvi, Paskolampi, Patajärvi, Pennijärvi, Perälampi, Pieni Hanslampi, Pieni Hirvijärvi, Pieni Humalajärvi, Pieni Joutenlampi, Pieni Koirajärvi, Pieni-Kairahta, Pieni-Kaukuu, Pieni-Kovalainen, Pieni-Mustalampi, Pieni-Vasarainen, Pieni-Virkanen, Pikku-Lampsi, Pikku-Soukka, Pilkkurilammi, Pirttijärvi (14.221.1.211), Pirttijärvi (14.272.1.004), Pirttijärvi (14.283.1.004), Pirttijärvi (14.286.1.016), Pirttijärvi (14.287.1.008), Pirttilampi, Pitkäjärvi (14.285.1.011), Pitkäjärvi, Pitkäjärvi (14.317.1.002), Pohjanlampi, Punajärvi, Pyssyjärvi, Pyydysjärvi, Päijänne,\n\nRajajärvi, Raspio, Rimminjärvi, Rimminlampi, Ruokepuolinen, Ruokepuoliset (i), Ruokepuoliset (l),\n\nSaanijärvi, Saarijärvi (14.285.1.010), Saarijärvi (14.286.1.003), Saarijärvi (14.295.1.003), Salmijärvi (14.221.1.193), Salmijärvi (14.287.1.014), Saukkojärvi (e), Saukkojärvi (p), Savilammi, Savilampi, Sipilänjärvi, Sirkkalampi, Soidenlammi, Soidenlampi, Soimalampi, Sompanen, Sonnamanlampi, Suojärvi, Suolijärvi, Surkee, Syväjärvi, Syvälampi, Särkijärvi (14.221.1.205), Särkijärvi (14.283.1.012),\nSärkilampi (14.286.1.009), Särkilampi (14.295.1.031), Säynätjärvi, Sääksjärvi (Jyväskylä)\nSääksjärvi\n\nTalsanlampi, Tekolampi, Terva-alanen, Tervalampi, Tiehinen, Tuhkuri, Tuohelanjärvi, Tuohikotanen, Tuomiojärvi, Tyyppäälänjärvi Tyyppälänjärvi, Tyystlampi\n\nUittimenjärvi\n\nVaarunjärvi, Valkeajärvi (14.221.1.179), Valkeajärvi (14.221.1.201), Valkeajärvi (14.274.1.008), Valkealampi, Valkeislampi, Valkolampi (14.286.1.024), Valkolampi (14.295.1.049), Varrenvesi,\nVarsajärvi, Vasarainen, Vasikkalampi, Vatsajärvi, Vedenpuhdistamo (14.295.2.001), Vedenpuhdistamo (14.295.2.002), Veijonjärvi, Veijonjärvi, Velakallionlampi, Vesankajärvi, Viheriäislammi, Vihtajärvi, Viitalampi, Viljasjärvi, Vinkara, Virkanen, Virolainen, Vuohijärvi, Vuorenalanen, Vuorilampi (14.231.1.038), Vuorilampi (14.291.1.004), Vähä Heinäjärvi,\nVähä Tuomiojärvi, Vähä-Vesanka, Vähäjärvi (14.221.1.207), Vähäjärvi (14.287.1.005), Vääräjärvi (14.295.1.033), Vääräjärvi (14.312.1.003), Väärälampi\n\nYkshaukinen (14.286.1.006), Ykshaukinen (14.294.1.001), Ylä-Aitolampi, Ylä-Kitulampi, Ylä-Kännää, Ylä-Sallaajärvi, Ylä-Tuomiojärvi, Yläinen Kotajärvi, Yläinen Vehkajärvi, Yläinen Vihtajärvi, Ylälampi, Yölampi\n\nÖrö\n"}
{"id": "46938521", "url": "https://en.wikipedia.org/wiki?curid=46938521", "title": "List of mountain peaks of Wyoming", "text": "List of mountain peaks of Wyoming\n\nThis article comprises three sortable tables of major mountain peaks of the U.S. State of Wyoming.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nOf the highest major summits of Wyoming, five peaks exceed elevation, 16 peaks exceed , and 38 peaks exceed elevation.\n\nOf the most prominent summits of Wyoming, Cloud Peak and Gannett Peak both exceed of topographic prominence. Those two peaks and Grand Teton are ultra-prominent summits with more than of topographic prominence. Nine peaks exceed of topographic prominence.\n\nOf the most isolated major summits of Wyoming, Gannett Peak exceeds of topographic isolation, Cloud Peak exceeds , and four peaks exceed of topographic isolation.\n\n\n"}
{"id": "46313242", "url": "https://en.wikipedia.org/wiki?curid=46313242", "title": "Materials oscilloscope", "text": "Materials oscilloscope\n\nThe Materials Oscilloscope is termed for a time-resolved synchrotron High-energy X-ray technique to study rapid phase composition and microstructural related changes in a polycrystalline sample. Such device has been developed for in-situ studies of specimens undergoing physical thermo-mechanical simulation.\nTwo-dimensional diffraction images of a fine synchrotron beam interacting with the specimen are recorded in time frames, such that reflections stemming from individual crystallites of the polycrystalline material can be distinguished. Data treatment is undertaken in a way that diffraction rings are straightened and presented line by line streaked in time. The traces, so-called timelines in azimuthal-angle/time plots resemble to traces of an oscilloscope, giving insight on the processes happening in the material, while undergoing plastic deformation, or heating, or both, These timelines allow to distinguish grain growth or refinement, subgrain formation, slip deformation systems, crystallographic twinning, dynamic recovery, dynamic recrystallization, simultaneously in multiple phases.\n\nThe development has been undertaken from a project on Modern Diffraction Methods for the Investigation of Thermo-Mechanical Processes, and started with cold deformation of a copper specimen at the ESRF in 2007, followed by hot deformation of zirconium alloy at APS in 2008. Soon afterwards, a series of other materials has been tested and experience with the timeline traces gained. While ESRF and APS played the major role in experimental facilities,the Japanese high-energy synchrotron in the round, SPring-8 followed in 2013 by performing feasibility studies of this kind. Meanwhile, the new PETRA-III synchrotron at DESY built a dedicated beamline for this purpose, opening the Materials Oscilloscope investigations to a larger public. The name Materials Oscilloscope has been introduced in 2013 and used onward upon conferences such as MRS and TMS.\n\nBesides setups in multi-purpose facilities, the first dedicated end-station as been built at the PETRA-III storage ring, where this technique is routinely applied.\n"}
{"id": "17254387", "url": "https://en.wikipedia.org/wiki?curid=17254387", "title": "Motor fuel taxes in Canada", "text": "Motor fuel taxes in Canada\n\nIn Canada, motor vehicles are primarily powered by gasoline or diesel fuel. Other energy sources include ethanol, biodiesel, propane, compressed natural gas (CNG), electric batteries charged from an external source, and hydrogen. Canada, like most countries, has excise taxes and other taxes on gasoline, diesel, and other liquid and gas motor fuels (collectively called fuel taxes), and also taxes electricity at various administrative levels. Most provinces and territories in Canada also have taxes on these motor fuels, and some metropolitan areas such as Montreal, Greater Vancouver, and Victoria impose additional taxes.\n\nAdditionally, Canada's federal (national) government collects value-added tax (GST) across the country, and some provincial governments also collect a provincial sales tax (PST), which may be combined with the GST into a single harmonized sales tax (HST). HST, GST, or GST + PST where applicable, are calculated on the retail price \"including\" the excise taxes.\n\nAcross Canada, motor fuel taxes can vary greatly between locales. On average, about one-third of the total price of gasoline at the pump is tax. Total minimum taxes (taxes before GST/HST/PST is added at the retail level, but including GST/HST/PST on the excise taxes themselves) vary from 17.0¢/litre (64.4¢/US gallon) in the Yukon to 41.01¢/L ($1.552/US gallon) in Greater Vancouver.\n\nNotes: \n\nThe Government of Canada collects about $5 billion per year in excise taxes on gasoline, diesel, and aviation fuel as well as approximately $1.6 billion per year from GST revenues on gasoline and diesel (net of input tax credits). The Canada Revenue Agency, a part of the government, collects these taxes.\n\nCollectively, the provincial governments collect approximately $8 billion per year from excise taxes on gasoline and diesel.\n\nThe federal taxes go into general coffers and help to fund a range of programs: $2 billion of the approximately $5 billion collected from federal excise taxes goes into the now permanent annual Gas Tax Fund for municipal infrastructure. Provincial tax revenues usually go to fund road repair and construction, and additionally in some provinces a portion of revenues (for example, 2 cents/litre in Ontario) is also distributed directly to municipalities.\n\n\n"}
{"id": "35997634", "url": "https://en.wikipedia.org/wiki?curid=35997634", "title": "National Centre for Atmospheric Science", "text": "National Centre for Atmospheric Science\n\nThe National Centre for Atmospheric Science (NCAS) is a world leader in atmospheric science. NCAS is one of the Natural Environment Research Council's (NERC) six research centres, and was formed in 2002 to provide the UK with national capability in atmospheric science research and technology. \n\nNCAS research programmes include:\n\n\nNCAS provides UK researchers with scientific facilities and services that enable excellent atmospheric science at a national scale. These include a world-leading aircraft, ground-based instrumentation, and access to computer models and facilities for storing and accessing scientific data. NCAS is not based in one location, its staff, facilities and services are distributed across many UK universities and related institutions. \n\nThe head office is based in Leeds: National Centre For Atmospheric Science Fairbairn House 71-75 Clarendon Road Leeds LS2 9PH Tel: +44 (0) 113 34 36408\n\n"}
{"id": "558685", "url": "https://en.wikipedia.org/wiki?curid=558685", "title": "Natural environment", "text": "Natural environment\n\nThe natural environment encompasses all living and non-living things occurring naturally, meaning in this case not artificial. The term is most often applied to the Earth or some parts of Earth. This environment encompasses the interaction of all living species, climate, weather and natural resources that affect human survival and economic activity.\n\nThe concept of the \"natural environment\" can be distinguished as components:\n\nIn contrast to the natural environment is the built environment. In such areas where man has fundamentally transformed landscapes such as urban settings and agricultural land conversion, the natural environment is greatly modified into a simplified human environment. Even acts which seem less extreme, such as building a mud hut or a photovoltaic system in the desert, the modified environment becomes an artificial one. Though many animals build things to provide a better environment for themselves, they are not human, hence beaver dams, and the works of Mound-building termites, are thought of as natural.\n\nPeople seldom find \"absolutely natural\" environments on Earth, and naturalness usually varies in a continuum, from 100% natural in one extreme to 0% natural in the other. More precisely, we can consider the different aspects or components of an environment, and see that their degree of naturalness is not uniform. If, for instance, in an agricultural field, the mineralogic composition and the structure of its soil are similar to those of an undisturbed forest soil, but the structure is quite different.\n\n\"Natural environment\" is often used as a synonym for habitat. For instance, when we say that the natural environment of giraffes is the savanna.\n\nEarth science generally recognizes 4 spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere as correspondent to rocks, water, air, and life respectively. Some scientists include, as part of the spheres of the Earth, the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere, as well as the pedosphere (corresponding to soil) as an active and intermixed sphere. Earth science (also known as geoscience, the geosciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth. There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a qualitative and quantitative understanding of the principal areas or \"spheres\" of Earth.\n\nThe Earth's crust, or lithosphere, is the outermost solid surface of the planet and is chemically and mechanically different from underlying mantle. It has been generated greatly by igneous processes in which magma cools and solidifies to form solid rock. Beneath the lithosphere lies the mantle which is heated by the decay of radioactive elements. The mantle though solid is in a state of rheic convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. Volcanoes result primarily from the melting of subducted crust material or of rising mantle at mid-ocean ridges and mantle plumes.\n\nMost water is found in one or another natural kind of body of water.\n\nAn ocean is a major body of saline water, and a component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 362 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 ft) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. The deep seabeds are more than half the Earth's surface, and are among the least-modified natural environments. The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean.\n\nA river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. A few rivers simply flow into the ground and dry up completely before reaching another body of water. \nThe water in a river is usually in a channel, made up of a stream bed between banks. In larger rivers there is also a wider floodplain shaped by waters over-topping the channel. Flood plains may be very wide in relation to the size of the river channel. Rivers are a part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of water stored in glaciers and snowpacks.\n\nSmall rivers may also be termed by several other names, including stream, creek and brook. Their current is confined within a bed and stream banks. Streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general is known as \"surface hydrology.\" \n\nA lake (from Latin \"lacus\") is a terrain feature, a body of water that is localized to the bottom of basin. A body of water is considered a lake when it is inland, is not part of an ocean, and is larger and deeper than a pond.\nNatural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.\n\nA pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams by their current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.\n\nHumans impact the water in different ways such as modifying rivers (through dams and stream channelization), urbanization, and deforestation. These impact lake levels, groundwater conditions, water pollution, thermal pollution, and marine pollution. Humans modify rivers by using direct channel manipulation. They are building dams and reservoirs and manipulating the direction of the rivers and water path. Dams are good for humans, some communities need the reservoirs to survive. However, reservoirs and dams may negatively impact the environment and wildlife. Dams stops fish migration and the moving of organisms down stream. Urbanization effects the environment because of deforestation and changing lake levels, groundwater conditions, etc. Deforestation and urbanization go hand in hand. Deforestation may cause flooding, declining stream flow, and changes in riverside vegetation. The changing vegetation occurs because when trees cannot get adequate water they start to deteriorate, leading to a decreased food supply for the wildlife in an area.\n\nThe atmosphere of the Earth serves as a key factor in sustaining the planetary ecosystem. The thin layer of gases that envelops the Earth is held in place by the planet's gravity. Dry air consists of 78% nitrogen, 21% oxygen, 1% argon and other inert gases, such as carbon dioxide. The remaining gases are often referred to as trace gases, among which are the greenhouse gases such as water vapor, carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Air also contains a variable amount of water vapor and suspensions of water droplets and ice crystals seen as clouds. Many natural substances may be present in tiny amounts in an unfiltered air sample, including dust, pollen and spores, sea spray, volcanic ash, and meteoroids. Various industrial pollutants also may be present, such as chlorine (elementary or in compounds), fluorine compounds, elemental mercury, and sulphur compounds such as sulphur dioxide [SO].\n\nThe ozone layer of the Earth's atmosphere plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.\n\nEarth's atmosphere can be divided into five main layers. These layers are mainly determined by whether temperature increases or decreases with altitude. From highest to lowest, these layers are:\n\nWithin the five principal layers determined by temperature are several layers determined by other properties.\n\nThe potential dangers of global warming are being increasingly studied by a wide global consortium of scientists. These scientists are increasingly concerned about the potential long-term effects of global warming on our natural environment and on the planet. Of particular concern is how climate change and global warming caused by anthropogenic, or human-made releases of greenhouse gases, most notably carbon dioxide, can act interactively, and have adverse effects upon the planet, its natural environment and humans' existence. It is clear the planet is warming, and warming rapidly.–This warming is also responsible for the extinction of natural habitats, which in turn leads to a reduction in wildlife population.The most recent report from the Intergovernmental Panel on Climate Change (the group of the leading climate scientists in the world) concluded that the earth will warm anywhere from 2.7 to almost 11 degrees Fahrenheit (1.5 to 6 degrees Celsius) between 1990 and 2100.\nEfforts have been increasingly focused on the mitigation of greenhouse gases that are causing climatic changes, on developing adaptative strategies to global warming, to assist humans, other animal, and plant species, ecosystems, regions and nations in adjusting to the effects of global warming. Some examples of recent collaboration to address climate change and global warming include:\n\nA significantly profound challenge is to identify the natural environmental dynamics in contrast to environmental changes not within natural variances. A common solution is to adapt a static view neglecting natural variances to exist. Methodologically, this view could be defended when looking at processes which change slowly and short time series, while the problem arrives when fast processes turns essential in the object of the study.\n\nClimate looks at the statistics of temperature, humidity, atmospheric pressure, wind, rainfall, atmospheric particle count and other meteorological elements in a given region over long periods of time. Weather, on the other hand, is the present condition of these same elements over periods up to two weeks.\n\nClimates can be classified according to the average and typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme is the one originally developed by Wladimir Köppen. The Thornthwaite system, in use since 1948, uses evapotranspiration as well as temperature and precipitation information to study animal species diversity and the potential impacts of climate changes.\n\nWeather is a set of all the phenomena occurring in a given atmospheric area at a given time. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers, generally, to day-to-day temperature and precipitation activity, whereas climate is the term for the average atmospheric conditions over longer periods of time. When used without qualification, \"weather\" is understood to be the weather of Earth.\n\nWeather occurs due to density (temperature and moisture) differences between one place and another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On the Earth's surface, temperatures usually range ±40 °C (100 °F to −40 °F) annually. Over thousands of years, changes in the Earth's orbit have affected the amount and distribution of solar energy received by the Earth and influence long-term climate\n\nSurface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The atmosphere is a chaotic system, and small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that civilized human activity such as agriculture and industry has inadvertently modified weather patterns.\n\nEvidence suggests that life on Earth has existed for about 3.7 billion years. All known life forms share fundamental molecular mechanisms, and based on these observations, theories on the origin of life attempt to find a mechanism explaining the formation of a primordial single cell organism from which all life originates. There are many different hypotheses regarding the path that might have been taken from simple organic molecules via pre-cellular life to protocells and metabolism.\n\nAlthough there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms. In biology, the science of living organisms, \"life\" is the condition which distinguishes active organisms from inorganic matter, including the capacity for growth, functional activity and the continual change preceding death.\n\nA diverse variety of living organisms (life forms) can be found in the biosphere on Earth, and properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information. Living organisms undergo metabolism, maintain homeostasis, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations. More complex living organisms can communicate through various means.\n\nAn ecosystem (also called as environment) is a natural unit consisting of all plants, animals and micro-organisms (biotic factors) in an area functioning together with all of the non-living physical (abiotic) factors of the environment.\n\nCentral to the ecosystem concept is the idea that living organisms are continually engaged in a highly interrelated set of relationships with every other element constituting the environment in which they exist. Eugene Odum, one of the founders of the science of ecology, stated: \"Any unit that includes all of the organisms (i.e.: the \"community\") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.\"\n\nA greater number or variety of species or biological diversity of an ecosystem may contribute to greater resilience of an ecosystem, because there are more species present at a location to respond to change and thus \"absorb\" or reduce its effects. This reduces the effect before the ecosystem's structure is fundamentally changed to a different state. This is not universally the case and there is no proven relationship between the species diversity of an ecosystem and its ability to provide goods and services on a sustainable level.\n\nThe term ecosystem can also pertain to human-made environments, such as human ecosystems and human-influenced ecosystems, and can describe any situation where there is relationship between living organisms and their environment. Fewer areas on the surface of the earth today exist free from human contact, although some genuine wilderness areas continue to exist without any forms of human intervention.\n\nBiomes are terminologically similar to the concept of ecosystems, and are climatically and geographically defined areas of ecologically similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, often referred to \"as\" ecosystems. Biomes are defined on the basis of factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.\n\nGlobal biogeochemical cycles are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.\n\nWilderness is generally defined as a natural environment on Earth that has not been significantly modified by human activity. The WILD Foundation goes into more detail, defining wilderness as: \"The most intact, undisturbed wild natural areas left on our planet - those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure.\" Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.\n\nThe word, \"wilderness\", derives from the notion of wildness; in other words that which is not controllable by humans. The word's etymology is from the Old English \"wildeornes\", which in turn derives from \"wildeor\" meaning \"wild beast\" (wild + deor = beast, deer). From this point of view, it is the wildness of a place that makes it a wilderness. The mere presence or activity of people does not disqualify an area from being \"wilderness.\" Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered \"wild.\" This way of looking at wilderness includes areas within which natural processes operate without very noticeable human interference.\n\nWildlife includes all non-domesticated plants, animals and other organisms. Domesticating wild plant and animal species for human benefit has occurred many times all over the planet, and has a major impact on the environment, both positive and negative. Wildlife can be found in all ecosystems. Deserts, rain forests, plains, and other areas—including the most developed urban sites—all have distinct forms of wildlife. While the term in popular culture usually refers to animals that are untouched by civilized human factors, most scientists agree that wildlife around the world is (now) impacted by human activities.\n\nIt is the common understanding of \"natural environment\" that underlies environmentalism — a broad political, social, and philosophical movement that advocates various actions and policies in the interest of protecting what nature remains in the natural environment, or restoring or expanding the role of nature in this environment. While true wilderness is increasingly rare, \"wild\" nature (e.g., unmanaged forests, uncultivated grasslands, wildlife, wildflowers) can be found in many locations previously inhabited by humans.\n\nGoals for the benefit of people and natural systems, commonly expressed by environmental scientists and environmentalists include:\n\n\nIn some cultures the term environment is meaningless because there is no separation between people and what they view as the natural world, or their surroundings. Specifically in the United States, many native cultures do not recognize the \"environment\", or see themselves as environmentalists.\n\n"}
{"id": "44017873", "url": "https://en.wikipedia.org/wiki?curid=44017873", "title": "Negative energy", "text": "Negative energy\n\nNegative energy is a concept used in physics to explain the nature of certain fields, including the gravitational field and various quantum field effects.\n\nIn more speculative theories, negative energy is involved in wormholes which may allow for time travel and warp drives for faster-than-light space travel.\n\nThe strength of the gravitational attraction between two objects represents the amount of gravitational energy in the field which attracts them towards each other. When they are infinitely far apart, the gravitational attraction and hence energy approach zero. As two such massive objects move towards each other, the motion accelerates under gravity causing an increase in the positive kinetic energy of the system. At the same time, the gravitational attraction - and hence energy - also increase in magnitude, but the law of energy conservation requires that the net energy of the system not change. This issue can only be resolved if the change in gravitational energy is negative, thus cancelling out the positive change in kinetic energy. Since the gravitational energy is getting stronger, this decrease can only mean that it is negative.\n\nA universe in which positive energy dominates will eventually collapse in a \"big crunch\", while an \"open\" universe in which negative energy dominates will either expand indefinitely or eventually disintegrate in a \"big rip\". In the zero-energy universe model (\"flat\" or \"Euclidean\"), the total amount of energy in the universe is exactly zero: its amount of positive energy in the form of matter is exactly cancelled out by its negative energy in the form of gravity.\n\nNegative energies and negative energy density are consistent with quantum field theory.\n\nIn quantum theory, the uncertainty principle allows the vacuum of space to be filled with virtual particle-antiparticle pairs which appear spontaneously and exist for only a short time before, typically, annihilating themselves again. Some of these virtual particles can have negative energy. Their behaviour plays a role in several important phenomena, as described below.\n\nIn the Casimir effect, two flat plates placed very close together restrict the wavelengths of quanta which can exist between them. This in turn restricts the types and hence number and density of virtual particle pairs which can form in the intervening vacuum and can result in a negative energy density. This causes an attractive force between the plates, which has been measured.\n\nVirtual particles with negative energy can exist for a short period. This phenomenon is a part of the mechanism involved in Hawking radiation by which black holes evaporate.\n\nIt is possible to arrange multiple beams of laser light such that destructive quantum interference suppresses the vacuum fluctuations. Such a squeezed vacuum state involves negative energy. The repetitive waveform of light leads to alternating regions of positive and negative energy.\n\nAccording to the theory of the Dirac sea, developed by Paul Dirac in 1930, the vacuum of space is full of negative energy. This theory was developed to explain the anomaly of negative-energy quantum states predicted by the Dirac equation.\n\nThe Dirac sea theory correctly predicted the existence of antimatter two years prior to the discovery of the positron in 1932 by Carl Anderson. However, the Dirac sea theory treats antimatter as a hole where there is the absence of a particle rather than as a real particle. Quantum field theory (QFT), developed in the 1930s, deals with antimatter in a way that treats antimatter as made of real particles rather than the absence of particles, and treats a vacuum as being empty of particles rather than full of negative-energy particles like in the Dirac sea theory.\n\nQuantum field theory has displaced the Dirac sea theory as a more popular explanation of these aspects of physics. Both the Dirac sea theory and quantum field theory are equivalent by means of a Bogoliubov transformation, so the Dirac sea can be viewed as an alternative formulation of quantum field theory, and is thus consistent with it.\n\nNegative energy appears in the speculative theory of wormholes, where it is needed to keep the wormhole open. A wormhole directly connects two locations which may be separated arbitrarily far apart in both space and time, and in principle allows near-instantaneous travel between them.\n\nA theoretical principle for a faster-than-light (FTL) warp drive for spaceships has been suggested, involving negative energy. The Alcubierre drive comprises a solution to Einstein's equations of general relativity, in which a bubble of spacetime is moved rapidly by expanding space behind it and shrinking space in front of it.\n\n\n"}
{"id": "49781600", "url": "https://en.wikipedia.org/wiki?curid=49781600", "title": "NhaD family", "text": "NhaD family\n\nThe NhaD family (TC# 2.A.62) belongs to the Ion Transporter (IT) Superfamily. A representative list of proteins belonging to the NhaD family can be found in the Transporter Classification Database.\n\nThe NhaD Na/H antiporter has been characterized from two \"Vibrio\" species: \"V. parahaemolyticus\" and \"V. cholerae\" and in the haloalkaliphile, \"Alkalimonas amylolytica\". These proteins and their homologues are 400-500 aas long and exhibit 10-13 TMSs. They catalyze Na/H and Li/H antiport. They exhibit activity at basic pH (8-10) with no activity at pH 7.5. The \"Amylolytica\" antiporter has low Na affinity and has optimal activity at 600 mM Na. Homologues are found in proteobacteria of all groups, \"Flavobacteria\" and \"Chlamydia.\" Distant homologues of the IT superfamily are ubiquitous.\n\nThe generalized reaction catalyzed by NhaD is:nH (in) + mNa (out) ⇌ nH (out) + mNa (in).\n\n\n"}
{"id": "10536491", "url": "https://en.wikipedia.org/wiki?curid=10536491", "title": "Nuisance wildlife management", "text": "Nuisance wildlife management\n\nNuisance wildlife management is the term given to the process of selective removal of problem individuals or populations of specific species of wildlife. Other terms for the field, include wildlife damage management, wildlife control, and animal damage control to name a few. Some species of wildlife may become habituated to man's presence, causing property damage or risking transfer of disease to humans or pets (zoonosis). Many wildlife species coexist with humans very successfully, such as commensal rodents which have become more or less dependent on humans.\n\nTypically, species that are most likely to be considered a nuisance by humans have the following characteristics. First, they are adaptable to fragmented habitat. Animals such as Canada geese (\"Branta canadensis\") love ponds with low sloping banks leading to lush green grass. Humans love this sort of landscaping too, so it is not surprising that Canada geese have thrived (not to mention the decline in hunting).\n\nSecond, these animals are not tied to eating a specific type of food. For example, lynx do not thrive in human impacted environments because they rely so heavily on snowshoe hares. In contrast, raccoons have been very successful in urban landscapes because they can live in attics, chimneys, and even sewers, and can sustain themselves with food gained from trashcans and discarded litter. \n\nThird, successful animals must not pose an obvious significant risk to human health and safety. Animals perceived as grave threats will incur the extreme ire of humans and be under constant threat of humans seeking to eliminate them.\n\nFinally, successful animals in humanized landscapes are often perceived as \"cute\", at least until they become so numerous that their preferential status becomes diminished. Many wildlife species have the potential of becoming a \"nuisance\" species, and whether or not a species is regarded as a pest can be directly correlated with the degree to which that animal can be tolerated by humans. For many people, tree squirrels feeding in their yards or gardens are not a problem; a neighbor may feel that these same squirrels nesting in the attic of their house are a nuisance and a fire hazard, due to their habit of gnawing on electrical cables.\n\nCommon wildlife pests include armadillos, skunks, boars, foxes, squirrels, snakes, rats, groundhogs, beavers, opossums, raccoons, bats, moles, deer, mice, coyotes, bears, ravens, seagulls, woodpeckers and pigeons. Some of these species are protected by state or federal regulations, such as bears, ravens, bats, deer, woodpeckers, and coyotes, and a permit may be required to control some species.\n\nWildlife are usually only pests in certain situations, such as when their numbers become \"excessive\" in a particular area. Human-induced changes in the environment will often result in increased numbers of a species. For example, piles of scrap building material make excellent sites where rodents can nest. Food left out for household pets is often equally attractive to some wildlife species. In these situations, the wildlife have suitable food and habitat and may become a nuisance.\n\nThe primary objective of any control program should be to reduce damage in a practical, humane and environmentally acceptable manner. Wildlife managers and wildlife control operators (WCOs) use control methods based on the habits and biology of the animals causing damage. By using methods matched to the nuisance species, control efforts will be more effective and will serve to maximize safety to the environment, humans and other animals.\n\nA key to controlling wildlife damage is prompt and accurate determination of which animal is causing the damage. Even someone with no training or experience can sometimes identify the pest by thoroughly examining the damaged area. Because feeding indications of many wildlife species are similar, other signs – such as droppings, tracks, burrows, nests or food caches – are usually needed to make a positive species identification.\n\nAfter the wildlife pest is identified, control methods can be chosen appropriate to the animal species involved. Improper control methods may harm but not kill the animal, causing it to become leery of those and other methods in the future. For example, using traps and poison baits improperly or in the wrong situation may teach the animal that the control method is harmful. This may make the animal difficult to control later, even with the correct method.\n\n\nThe most commonly used methods for controlling nuisance wildlife around homes and gardens include exclusion, habitat modification, repellents, toxic baits, glue boards, traps and frightening. Wildlife control involves human risks both from possible injury to person and property, but also from zoonotic disease.\n\nPhysically excluding an offending animal from the area being damaged or disturbed is often the best and most permanent way to control the problem. Depending upon size of the area to be protected, this control method can range from inexpensive to prohibitively costly.\n\nFor example, damage by birds or rabbits to ornamental shrubs or garden plants can be reduced inexpensively by placing bird netting over the plants to keep the pests away. On the other hand, fencing out deer from a lawn or garden can be more costly. Materials needed for exclusion will depend upon the species causing the problem. Large mammals can be excluded with woven wire fences, poly-tape fences, and electric fences; but many communities forbid the use of electric fencing in their jurisdictions. Small mammals and some birds can be excluded with netting, tarp, hardware cloth or any other suitable material; nets come in different weave sizes suitable for different animals to be excluded.\n\nHowever, exclusion can interfere with the natural movement of wildlife, particularly when exclusion covers large areas of land.\n\nModifying an animal’s habitat often provides lasting and cost-effective relief from damage caused by nuisance wildlife. Habitat modification is effective because it limits access to one or more of the requirements for life – food, water or shelter. However, habitat modification, while limiting nuisance wildlife, may also limit desirable species such as songbirds as well.\n\nRodent- or bat-proofing buildings by sealing cracks and holes prevents these animals from gaining access to suitable habitats where they are not welcome. Storing seed and pet food in tightly closed containers, controlling weeds and garden debris around homes and buildings, and storing firewood and building supplies on racks or pallets above ground level are also practices that can limit or remove the animals’ sources of food, water or shelter.\n\nUsing a repellent that changes the behavior of an animal may lead to a reduction or elimination of damage. Several available repellents, such as objectionable-tasting coatings or odor repellents, may deter wildlife from feeding on plants. Other repellents such as sticky, tacky substances placed on or near windows, trees or buildings may deter many birds and small mammals. Unfortunately, most wildlife soon discover that repellents are not actually harmful, and the animals may quickly become accustomed to the smell, taste or feel of these deterrents.\n\nChemical repellents applied outdoors will have to be reapplied due to rain or heavy dew, or applied often to new plant growth to be effective. Failure to carefully follow the directions included with repellents can drastically diminish the effectiveness of the product. Some repellents contain toxic chemicals, such as paradichlorobenzene, and are ineffective unless used at hazardous concentrations. Other more natural repellents contain chili pepper or capsaicin extracted from hot peppers.\n\nHowever, even under the best of conditions, repellents frequently fail to live up to user expectations. The reason for this is twofold. First, many repellents simply don't work. For example, peer-reviewed publications have consistently shown that ultrasonic devices do not drive unwanted animals away. Second, even when the repellent has been shown to work, animals in dire need of food will \"hold their nose\" and eat anyway because the alternative is essentially death by starvation. Repellents are most successful (referring to products actually demonstrated by peer-reviewed research to be effective) when animals have access to alternative food sources in a different location.\n\nGlue traps and boards can be either a lethal or non-lethal method of control. Glue boards can be used to trap small mammals and snakes. Applying vegetable oil will dissolve the glue, allowing for release, but caution must be taken to avoid scratches and bites from the trapped animal.\n\nUsing traps can be very effective in reducing actual population numbers of certain species. However, many species cannot be trapped without a permit. In most cases, homeowners may trap an offending animal within 100 yards of their residence without a permit, however relocation is often illegal.\n\nTraditional live traps such as cage or box traps are easily purchased at most garden centers or hardware stores. These traps allow for safe release of the trapped animal. The release of the animal to another area may be prohibited by state law, or may be regulated by the local Department of Fish and Game. Leghold traps may allow for either release or euthanasia of the trapped animal. Traps such as body-gripping traps, scissor and harpoon traps, as well as rat/mouse snap traps, are nearly always lethal. Knowledge of animal behavior, trapping techniques, and baits is essential for a successful trapping program.(Bornheimer, Shane P. \"PreferredWildlifeservices.com\" July 2013)\n\nFrightening devices such as bells, whistles, horns, clappers, sonic emitters, audio tapes and other sound devices may be quite successful in the short term for repelling an animal from an area. Other objects such as effigies, lights, reflectors and windmills rely on visual stimulation to scare a problem animal away. Often nuisance animals become accustomed to these tactics, and will return later if exposed to these devices daily.\n\nIn 2013, Dr. John Swaddle and Dr. Mark Hinders at the College of William and Mary created a new method of deterring birds and other animals using benign sounds projected by conventional and directional (parametric) speakers. The initial objectives of the technology were to displace problematic birds from airfields to reduce bird strike risks, minimize agricultural losses due to pest bird foraging, displace nuisance birds that cause extensive repair and chronic clean-up costs, and reduce bird mortality from flying into man-made structures. The sounds, referred to as a “Sonic Net,” do not have to be loud and are a combination of wave forms - collectively called \"colored\" noise - forming non-constructive and constructive interference with how birds and other animals such as deer talk to each other. Technically, the Sonic Nets technology is not a bird or wildlife scarer, but discourages birds and animals from going into or spending time in the target area. The impact to the animals is similar to talking in a crowded room, and since they cannot understand each other they go somewhere else. Early tests at an aviary and initial field trials at a landfill and airfield indicate that the technology is effective and that birds do not habituate to the sound. The provisional and full patents were filed in 2013 and 2014 respectively, and further research and commercialization of the technology are ongoing.\n\nBefore initiating any wildlife control activities, a person must become familiar with applicable federal, state, and local laws. One way to learn these rules is to contact the state's wildlife agency, which is usually responsible for selling hunting and fishing licenses. In general, property owners are permitted to prevent wildlife damage through exclusion and habitat modification, though they may be prohibited from disturbing an occupied nest or den, or directly harming an animal.\n\nMany regulations exist in the United States concerning animal trapping including trap check intervals, usually requiring all traps be checked at least once during a 24-hour period. Some governments permit relocation of wildlife, however humane considerations must be taken into account before relocating wildlife, including population and habitat.\n\nThere are many ethical considerations in nuisance wildlife management. Some species of wildlife cannot be ethically relocated due to overabundance of competing species, or lack of availability of proper food and habitat. Control during the spring months does run the risk of killing the young by starvation. Proper euthanasia of animals when necessary is also a controversial and sensitive consideration to be taken prior to engaging in nuisance wildlife management, and requires training and certification in some areas of the United States. \n\n\n\n"}
{"id": "83726", "url": "https://en.wikipedia.org/wiki?curid=83726", "title": "Ocyrhoe", "text": "Ocyrhoe\n\nOcyrhoe (; Ancient Greek: Ὠκυρόη) or Ocyrrhoe (Ὠκυρρόη) refers to at least five characters in Greek mythology.\n\n\n\nA character in The Mongoliad is named Ocyrhoe.\n\n \n"}
{"id": "3548362", "url": "https://en.wikipedia.org/wiki?curid=3548362", "title": "Pele's tears", "text": "Pele's tears\n\nPele’s tears are small pieces of solidified lava drops formed when airborne particles of molten material fuse into tearlike drops of volcanic glass. Pele’s tears are jet black in color and are often found on one end of a strand of Pele's hair. Pele's tears is primarily a scientific term used by volcanologists. \n\nPele's tears, like Pele's hair, are named after Pele, the Hawaiian fire goddess of volcanoes.\n\nThe formation of these tears is a complex process depending on a number of different factors as a tiny droplet of lava is being erupted from a lava fountain. Whilst it is travelling through the air two things are happening: It is cooling down very rapidly (a process known as quenching) and it is being deformed. The deformation of a droplet depends on the speed at which it is erupted from the volcano, its surface tension, the viscosity (thickness) of the magma and the resistance it experiences as it travels through the air.\n\nPele's tears are also found entangled within fine strands of volcanic glass known as Pele’s hair and it was considered that they formed together under similar conditions. Shimozura (1994) investigated this further and found that the velocity of the erupting lava was the main factor in determining whether Pele’s tears or Pele's hair were formed. If the velocity of the erupting magma is high then Pele’s hair is formed and if the velocity is low the formation of Pele’s tears is favoured.\n\nUnder the microscope very tiny Pele’s tears (less than 1 μm) can be found within the cavities of Pele’s hair. This would suggest they formed prior to being trapped within the strand and supports a different mechanism for formation. It has been considered that they became trapped during transport in the eruptive plume.\n\nPele’s tears are interesting to volcanologists because trapped within the glass droplet are bubbles of gas and particles called vesicles. When these are analysed they can provide a great deal of information about the mechanisms of an eruption. For instance, the shape of a vesicle can provide an indication of the velocity of the eruption. When vesicles form within the lava they are spherical in shape. If the eruption is turbulent the vesicles will deform and become elongated in shape. If the velocity is low they will retain their spherical shape as in the case of Pele’s tears.\n\nThe composition of a magma chamber includes lava, crystals and gases known as volatiles. At high pressures within the magma chamber, gases are dissolved in the melt and are soluble. As the magma rises, the pressure decreases and the gases come out of solution. Some of these gases escape and some become trapped in the melt. When the gases trapped within the vesicles of Pele’s tears are analysed they can provide a great deal of information about the chemical composition of the magma chamber. This information can be used to determine the explosive nature of an eruption and help to understand the complex processes occurring within volcanos. This is important in the management of natural hazards.\n\n\n"}
{"id": "21648353", "url": "https://en.wikipedia.org/wiki?curid=21648353", "title": "Ringkøbing Fjord", "text": "Ringkøbing Fjord\n\nRingkøbing Fjord, despite its name, is in fact a shallow lagoon on the westcoast of Jutland. \n\nSkjern River terminates in Ringkøbing Fjord with a large river delta system to the east, and the lagoon is shielded from the North Sea by a long isthmus named Holmsland Dunes to the west. On the Hvide Sande Canal in the middle of the isthmus, there is a floodgate that provides access to the sea to the west. The fjord is about 30 kilometers long and 2 to 3 meters deep. In the southwestern corner is the Tipperne peninsula, where there is a bird reservation. The town of Ringkøbing lies to the north-east. Ringkøbing Fjord was once affected by oxygen depletion, but today both plant and fish life abounds.\n\nRingkøbing Fjord was originally a bay, around which two sandbars have gradually built up, with a sandbank that has shifted repeatedly over time as a result of shifting water currents. In the mid-17th century, the bank was near Sønder Havrvig, but it gradually moved south as sand was deposited on the shoal from the north. By the late 18th century, it was close to the town of Nymindegab. On several occasions, the surrounding dunes collapsed from the effects of the water, causing the old outflow to fill with sand and creating problems for local fisherman. In 1891, a channel was therefore dug south of Nymindegab but, in 1910, it was replaced by a channel in Hvide Sande. A storm surge in 1911 created an opening of 230 meters and extensive flooding around the fjord. In 1915, the passage at the southern end of the fjord was therefore reopended while the one near Hvide Sande was closed. It was, however, later reopened with the establishment of a lock in 1931.\n\n\n"}
{"id": "3984833", "url": "https://en.wikipedia.org/wiki?curid=3984833", "title": "Ripple (electrical)", "text": "Ripple (electrical)\n\nRipple (specifically ripple voltage) in electronics is the residual periodic variation of the DC voltage within a power supply which has been derived from an alternating current (AC) source. This ripple is due to incomplete suppression of the alternating waveform after rectification. Ripple voltage originates as the output of a rectifier or from generation and commutation of DC power.\n\nRipple (specifically ripple current or surge current) may also refer to the pulsed current consumption of non-linear devices like capacitor-input rectifiers.\n\nAs well as these time-varying phenomena, there is a frequency domain ripple that arises in some classes of filter and other signal processing networks. In this case the periodic variation is a variation in the insertion loss of the network against increasing frequency. The variation may not be strictly linearly periodic. In this meaning also, ripple is usually to be considered an incidental effect, its existence being a compromise between the amount of ripple and other design parameters.\n\nRipple is wasted power, and has many undesirable effects in a DC circuit: it heats components, causes noise and distortion, and may cause digital circuits to operate improperly. Ripple may be reduced by an electronic filter, and eliminated by a voltage regulator.\n\nRipple voltage is an alternating (AC) voltage which is a constituent part of a composite voltage waveform with a constant DC component (offset) which may be positive or negative, but for analysis is usually considered to be an absolute value. The ripple component is often small relative to the DC component, but in absolute terms, ripple (as in the case of HVDC transmission systems) may be thousands of volts. Ripple itself is a composite (non-sinusoidal) waveform consisting of harmonics of some fundamental frequency which is usually the AC line frequency of 50/60Hz, but in the case of switched-mode power supplies, the fundamental frequency can be tens of kilohertz to megahertz. The characteristics and components of ripple depend on its source: there is single-phase half- and full-wave rectification, and three-phase half- and full-wave rectification. Rectification can be controlled (uses Silicon Controlled Rectifiers (SCRs) or uncontrolled (uses diodes). There is in addition, active rectification which uses transistors.\n\nVarious properties of ripple voltage may be important depending on application: the equation of the ripple for Fourier analysis to determine the constituent harmonics; the peak (usually peak-to-peak) value of the voltage; the root mean square (RMS) value of the voltage which is a component of power transmitted; the ripple factor \"γ\", the ratio of RMS value to DC voltage output; the conversion ratio (also called the rectification ratio or \"efficiency\") \"η\", the ratio of DC output power to AC input power; and form-factor, the ratio of the RMS value of the output voltage to the average value of the output voltage. Analogous ratios for output ripple current may also be computed.\n\nAn electronic filter with high impedance at the ripple frequency may be used to reduce ripple voltage and increase or decrease DC output; such a filter is often called a smoothing filter.\n\nThe initial step in AC to DC conversion is to send the AC current through a rectifier. The ripple voltage output is very large in this situation; the peak-to-peak ripple voltage is equal to the peak AC voltage minus the forward voltage of the rectifier diodes. In the case of a SS silicon diode, the forward voltage is 0.7V; for vacuum tube rectifiers, forward voltage usually ranges between 25 and 67V (5R4). The output voltage is a sine wave with the negative half-cycles inverted. The equation is:\n\nThe Fourier expansion of the function is:\n\nSeveral relevant properties are apparent on inspection of the Fourier series:\n\nThe output voltages are:\n\nwhere\n\nThe ripple factor is:\n\nThe form factor is:\n\nThe peak factor is: \nThe conversion ratio is:\n\nThe transformer utilization factor is:\n\nReducing ripple is only one of several principal considerations in power supply filter design. The filtering of ripple voltage is analogous to filtering other kinds of signals. However, in AC/DC power conversion as well as DC power generation, high voltages and currents or both may be output as ripple. Therefore, large discrete components like high ripple-current rated electrolytic capacitors, large iron-core chokes and wire-wound power resistors are best suited to reduce ripple to manageable proportions before passing the current to an IC component like a voltage regulator, or on to the load. The kind of filtering required depends on the amplitude of the various harmonics of the ripple and the demands of the load. For example, a moving coil (MC) input circuit of a phono preamplifier may require that ripple be reduced to no more than a few hundred nanovolts (10V). In contrast, a battery charger, being a wholly resistive circuit, does not require any ripple filtering. Since the desired output is direct current (essentially 0Hz), ripple filters are usually configured as low pass filters characterized by shunt capacitors and series chokes. Series resistors may replace chokes for reducing the output DC voltage, and shunt resistors may be used for voltage regulation.\n\nMost power supplies are now switched mode designs. The filtering requirements for such power supplies are much easier to meet owing to the high frequency of the ripple waveform. The ripple frequency in switch-mode power supplies is not related to the line frequency, but is instead a multiple of the frequency of the chopper circuit, which is usually in the range of 50 kHz - 1Mhz.\nA capacitor input filter (in which the first component is a shunt capacitor) and choke input filter (which has a series choke as the first component) can both reduce ripple, but have opposing effects on voltage and current, and the choice between them depends on the characteristics of the load. Capacitor input filters have poor voltage regulation, so are preferred for use in circuits with stable loads and low currents (because low currents reduce ripple here). Choke input filters are preferred for circuits with variable loads and high currents (since a choke outputs a stable voltage and higher current means less ripple in this case).\n\nThe number of reactive components in a filter is called its \"order\". Each reactive component reduces signal strength by 6db/octave above (or below for a high-pass filter) the corner frequency of the filter, so that a 2nd order low pass filter for example, reduces signal strength by 12db/octave above the corner frequency. Resistive components (including resistors and parasitic elements like DCR of chokes and ESR of capacitors) also reduce signal strength, but their effect is \"linear\", and does not vary with frequency.\n\nA common arrangement is to allow the rectifier to work into a large smoothing capacitor which acts as a reservoir. After a peak in output voltage the capacitor (C) supplies the current to the load (R) and continues to do so until the capacitor voltage has fallen to the value of the now rising next half-cycle of rectified voltage. At that point the rectifier conducts again and delivers current to the reservoir until peak voltage is again reached.\n\nIf the RC time constant is large in comparison to the period of the AC waveform, then a reasonably accurate approximation can be made by assuming that the capacitor voltage falls linearly. A further useful assumption can be made if the ripple is small compared to the DC voltage. In this case the phase angle through which the rectifier conducts will be small and it can be assumed that the capacitor is discharging all the way from one peak to the next with little loss of accuracy.\nWith the above assumptions the peak-to-peak ripple voltage can be calculated as:\n\nThe definition of capacitance formula_23and current formula_24are\n\nformula_25,\n\nwhere formula_26is the amount of charge. The current and time formula_27is taken from start of capacitor discharge until the minimum voltage on a full wave rectified signal as shown on the figure to the right. The time formula_28would then be equal to half the period of the full wave input.\n\nCombining the three equations above to determine formula_30 gives,\n\nThus, for a full-wave rectifier:\n\nwhere\n\nFor the rms value of the ripple voltage, the calculation is more involved as the shape of the ripple waveform has a bearing on the result. Assuming a sawtooth waveform is a similar assumption to the ones above. The RMS value of a sawtooth wave is formula_36 where formula_37 is peak voltage. With the further approximation that formula_37 is formula_39, it yields the result:\n\nwhere\n\nAnother approach to reducing ripple is to use a series choke. A choke has a filtering action and consequently produces a smoother waveform with fewer high-order harmonics. Against this, the DC output is close to the average input voltage as opposed to the voltage with the reservoir capacitor which is close to the peak input voltage. Starting with the Fourier term for the second harmonic, and ignoring higher order harmonics,\n\nthe ripple factor is given by:\n\nwhere\n\nThere is a minimum inductance which is relative to the resistance of the load required in order for a series choke to continuously conduct current. If the inductance falls below that value, current will be intermittent and output DC voltage will rise from the average input voltage to the peak input voltage - in effect, the inductor will behave like a capacitor. That minimum inductance, called the \"critical inductance\" is formula_55 where R is the load resistance and f the line frequency. This gives values of L = R/1131 (often stated as R/1130) for 60Hz mains rectification, and L = R/942 for 50Hz mains rectification. Additionally, interrupting current to an inductor will cause its magnetic flux to collapse exponentially; as current falls, a voltage spike composed of very high harmonics results which can damage other components of the power supply or circuit. This phenomenon is called flyback voltage.\n\nThe complex impedance of a series choke is effectively part of the load impedance, so that lightly loaded circuits have increased ripple (just the opposite of a capacitor input filter). For that reason, a choke input filter is almost always part of an LC filter section, whose ripple reduction is independent of load current. The ripple factor is:\n\nwhere \n\nIn high voltage/low current circuits, a resistor may replace the series choke in an LC filter section (creating an RC filter section). This has the effect of reducing the DC output as well as ripple. The ripple factor is\n\nwhere \n\nSimilarly because of the independence of LC filter sections with respect to load, a reservoir capacitor is also commonly followed by one resulting in a low-pass Π-filter. A Π-filter results in a much lower ripple factor than a capacitor or choke input filter alone. It may be followed by additional LC or RC filter sections to further reduce ripple to a level tolerable by the load. However, use of chokes is deprecated in contemporary designs for economic reasons.\n\nA more common solution where good ripple rejection is required is to use a reservoir capacitor to reduce the ripple to something manageable and then pass the current through a voltage regulator circuit. The regulator circuit, as well as providing a stable output voltage, will incidentally filter out nearly all of the ripple as long as the minimum level of the ripple waveform does not go below the voltage being regulated to. Switched-mode power supplies usually include a voltage regulator as part of the circuit.\n\nVoltage regulation is based on a different principle than filtering: it relies on the peak inverse voltage of a diode or series of diodes to set a maximum output voltage; it may also use one or more voltage amplification devices like transistors to boost voltage during sags. Because of the non-linear characteristics of these devices, the output of a regulator is free of ripple. A simple voltage regulator may be made with a series resistor to drop voltage followed by a shunt zener diode whose Peak Inverse Voltage (PIV) sets the maximum output voltage; if voltage rises, the diode shunts away current to maintain regulation.\n\nRipple is undesirable in many electronic applications for a variety of reasons:\n\nRipple current is a periodic non-sinusoidal waveform derived from an AC power source characterized by high amplitude narrow bandwidth pulses.\nThe pulses coincide with peak or near peak amplitude of an accompanying sinusoidal voltage waveform.\n\nRipple current results in increased dissipation in parasitic resistive portions of circuits like ESR of capacitors, DCR of transformers and inductors, internal resistance of storage batteries. The dissipation is proportional to the current squared times resistance ( IR ). The RMS value of ripple current can be many times the RMS of the load current.\n\nRipple in the context of the frequency domain refers to the periodic variation in insertion loss with frequency of a filter or some other two-port network. Not all filters exhibit ripple, some have monotonically increasing insertion loss with frequency such as the Butterworth filter. Common classes of filter which exhibit ripple are the Chebyshev filter, inverse Chebyshev filter and the Elliptical filter. The ripple is not usually strictly linearly periodic as can be seen from the example plot. Other examples of networks exhibiting ripple are impedance matching networks that have been designed using Chebyshev polynomials. The ripple of these networks, unlike regular filters, will never reach 0 dB at minimum loss if designed for optimum transmission across the passband as a whole.\n\nThe amount of ripple can be traded for other parameters in the filter design. For instance, the rate of roll-off from the passband to the stopband can be increased at the expense of increasing the ripple without increasing the order of the filter (that is, the number of components has stayed the same). On the other hand, the ripple can be reduced by increasing the order of the filter while at the same time maintaining the same rate of roll-off.\n\n\n"}
{"id": "5714228", "url": "https://en.wikipedia.org/wiki?curid=5714228", "title": "Rusalka", "text": "Rusalka\n\nIn Slavic folklore, the rusalka (plural \"rusalki\") (; ) is a female entity, often malicious toward humankind and frequently associated with water. Folklorists have proposed a variety of origins for the entity, including that they may originally stem from Slavic paganism, where they may have been seen as benevolent spirits. Rusalki appear in a variety of media in modern popular culture, particularly in Slavic language-speaking countries, where they frequently resemble the concept of the mermaid.\n\nAccording to Vladimir Propp, the original \"rusalka\" was an appellation used by pagan Slavic peoples, who linked them with fertility and did not consider rusalki evil before the 19th century. They came out of the water in the spring to transfer life-giving moisture to the fields and thus helped nurture the crops.\n\nIn 19th century versions, a rusalka is an unquiet, dangerous being who is no longer alive, associated with the unclean spirit. According to Dmitry Zelenin, young women, who either committed suicide by drowning due to an unhappy marriage (they might have been jilted by their lovers or abused and harassed by their much older husbands) or who were violently drowned against their will (especially after becoming pregnant with unwanted children), must live out their designated time on earth as rusalki. However, the initial Slavic lore suggests that not all rusalki occurrences were linked with death from water.\n\nIt is accounted by most stories that the soul of a young woman who had died in or near a river or a lake would come back to haunt that waterway. This undead rusalka is not invariably malevolent, and would be allowed to die in peace if her death is avenged. Her main purpose is, however, to lure young men, seduced by either her looks or her voice, into the depths of said waterways where she would entangle their feet with her long red hair and submerge them. Her body would instantly become very slippery and not allow the victim to cling on to her body in order to reach the surface. She would then wait until the victim had drowned, or, on some occasions, tickle them to death, as she laughed. It is also believed, by a few accounts, that rusalki can change their appearance to match the tastes of men they are about to seduce, although a rusalka is generally considered to represent universal beauty, therefore is highly feared yet respected in Slavic culture.\n\nWhile lore often says that the rusalki could not completely stand out of water, some fiction works tell of rusalki that could climb trees and sing songs, sit on docks with only submerged feet and comb their hair, or even join other rusalki in circle dances in the field. A particular feature of such stories revolves around the fact that this behaviour would be limited to only certain periods of the year, usually the summer (see Rusalka Week section).\n\nSpecifics pertaining to rusalki differed among regions. Although in most tales they lived without men, in Ukraine they were often linked with water (in Belarus they were linked with the forest and field). Where land was fertile, the maidens appeared naked and beautiful. In harsher areas of Russia, they appeared as \"large breasted amazons\".\n\nThe \"rusalki\" were believed to be at their most dangerous during the Rusalka Week (\"Rusalnaya nedelja\") in early June. At this time, they were supposed to have left their watery depths in order to swing on branches of birch and willow trees by night. Swimming during this week was strictly forbidden, lest mermaids would drag a swimmer down to the river floor. A common feature of the celebration of Rusalnaya was the ritual banishment or burial of the rusalki at the end of the week, which remained as entertainment in Russia, Belarus, and Ukraine until the 1930s.\n\nRegarding representations of the rusalka in modern popular culture, folklorist Natalie Kononenko says, \"the currently dominant presents her as something like a mermaid, though she is pictured as having legs rather than a fish tail ... The current view of the \"rusalka\" as a seductive or seduced woman was probably influenced by written literature. In the past, her image was more complex and she more closely resembled a nature spirit, found not only near water but in fields, forests, and mountains, rather like the vila ...\".\n\n\n\n\n"}
{"id": "32708", "url": "https://en.wikipedia.org/wiki?curid=32708", "title": "Sarasvati River", "text": "Sarasvati River\n\nSarasvati River (Sanskrit: , \"IAST: sárasvatī nadī́\") was one of the Rigvedic rivers mentioned in the Rig Veda and later Vedic and post-Vedic texts. The Sarasvati River played an important role in Hinduism since Vedic Sanskrit. The first part of the Rig Veda is believed to have originated when the Vedic people lived on its banks, during the 2nd millennium BCE.\n\nThe goddess Sarasvati was originally a personification of this river, but later developed an independent identity. The Sarasvati is also considered by Hindus to exist in a metaphysical form, in which it formed a confluence with the sacred rivers Ganges and Yamuna, at the Triveni Sangam. According to Michael Witzel, superimposed on the Vedic Sarasvati river is the heavenly river Milky Way, which is seen as \"a road to immortality and heavenly after-life.\"\n\nRigvedic and later Vedic texts have been used to propose identification with present-day rivers, or ancient riverbeds. The Nadistuti hymn in the Rigveda (10.75) mentions the Sarasvati between the Yamuna in the east and the Sutlej in the west. Later Vedic texts like the Tandya and Jaiminiya Brahmanas, as well as the Mahabharata, mention that the Sarasvati dried up in a desert.\n\nSince the late 19th-century, scholars have conjectured that the Vedic Saraswati river is the Ghaggar-Hakra River system, which flows through northwestern India and eastern Pakistan. Satellite images have pointed to the more significant river once following the course of the present day Ghaggar River. Scholars have observed that major Indus Valley Civilization sites at Kalibangan (Rajasthan), Banawali and Rakhigarhi (Haryana), Dholavira and Lothal (Gujarat) also lay along this course.\n\nHowever, identification of the Vedic Sarasvati with the Ghaggar-Hakra system is problematic, since the Ghaggar-Hakra is not only mentioned separately in the \"Rig Veda\" river, but is described as having dried-up by the time of the composition of the Vedas and Hindu epics. In the words of Annette Wilke, the Ghaggar-Hakra had been reduced to a \"small, sorry trickle in the desert\", by the time that the Vedic people migrated into north-west India. Recent geophysical research suggests that the Ghaggar-Hakra system was a system of monsoon-fed rivers and that the Indus Valley Civilisation may have declined as a result of climate change. That is, the monsoons that fed the rivers diminished at around the time civilisation diminished some 4,000 years ago.\n\n\"Sarasvati\" may also be identified with the Helmand or Haraxvati river in southern Afghanistan., the name of which may have been reused in its Sanskrit form as the name of the Ghaggar-Hakra river, after the Vedic tribes moved to the Punjab. \"Sarasvati\" of the Rig Veda may also refer to two distinct rivers, with the family books referring to the Helmand River, and the more recent 10th mandala referring to the Ghaggar-Hakra.\n\nThe identification with the Ghaggar-Hakra system took on new significance in the early 21st century, with some suggesting an earlier dating of the Rig Veda; renaming the Indus Valley Civilisation as the \"Sarasvati culture\", the \"Sarasvati Civilization\", the \"Indus-Sarasvati Civilization\" or the \"Sindhu-Sarasvati Civilization,\" suggesting that the Indus Valley and Vedic cultures can be equated; and rejecting the Indo-Aryan migrations theory, which postulates a migration at 1500 BCE.\n\n' is the feminine of an adjective ' (which occurs in the Rigveda as the name of the keeper of the celestial waters), derived from Proto-Indo-Iranian *' (and earlier, PIE '), meaning ‘marshy, full of pools’, or ‘she with many lakes’. The other term \"\" is the Sanskrit grammatical feminine possessor suffix.\n\nSanskrit ' means ‘pool, pond or lake’; the feminine ' means ‘stagnant pool, swamp’. Like its cognates Welsh \"hêl, heledd\" ‘river meadow’ and Greek (\"hélos\") ‘swamp’, the Rigvedic term refers mostly to stagnant waters, and Mayrhofer considers unlikely a connection with the root *\"\" ‘run, flow’.\n\n' may be a cognate of Avestan \"Haraxatī\", perhaps originally referring to Arədvī Sūrā Anāhitā (modern \"Ardwisur Anahid\"), the Zoroastrian mythological world river, which would point to a common Indo-Iranian myth of a cosmic or mystical ' river. In the younger Avesta, \"Haraxatī\" is Arachosia, a region described to be rich in rivers, and its Old Persian cognate \"Harauvati\", which gave its name to the present-day Hārūt River in Afghanistan, may have referred to the entire Helmand drainage basin (the center of Arachosia).\n\nHowever, the Avestan x generally cognates with Sanskrit \"ksha\". The usual cognate to \"sva/sa\" syllable of Sanskrit is \"ngha/ŋh\" syllable of Avestan, as generally found in cognate-pairs like Vivasvan-Vivanghat and Rasa-Rangha.\n\nThe Saraswati river was revered and considered important for Hindus because it is said that it was on this river's banks, along with its tributary Drishadwati, in the Vedic state of Brahmavarta, that Vedic Sanskrit had its genesis, and important Vedic scriptures like initial part of Rigveda and several Upanishads were supposed to have been composed by Vedic seers. In the Manusmriti, Brahmavarta is portrayed as the \"pure\" centre of Vedic culture. Bridget and Raymond Allchin in \"The Rise of Civilization in India and Pakistan\" took the view that \"The earliest Aryan homeland in India-Pakistan (Aryavarta or Brahmavarta) was in the Punjab and in the valleys of the Sarasvati and Drishadvati rivers in the time of the Rigveda.\"\n\nThe Sarasvati River is mentioned in all but the fourth book of the Rigveda. The most important hymns related to Sarasvati are RV 6.61, RV 7.95 and RV 7.96. Macdonell and Keith provided a comprehensive survey of Vedic references to the Sarasvati River in their \"Vedic Index\".\n\n-Ambitame Naditame Devitame Sarasvati, Aparaastasya Iva Smaasi Yashastim Amba Naskruteem-\n\nThe Sarasvati is mentioned some fifty times in the hymns of the Rig Veda. It is mentioned in thirteen hymns of the late books (1 and 10) of the Rigveda. Only two of these references are unambiguously to the river: 10.64.9, calling for the aid of three \"great rivers\", Sindhu, Sarasvati and Sarayu; and 10.75.5, the geographical list of the Nadistuti sukta. The others invoke Sarasvati as a goddess without direct connection to a specific river.\n\nIn 10.30.12, her origin as a river goddess may explain her invocation as a protective deity in a hymn to the celestial waters. In 10.135.5, as Indra drinks Soma he is described as refreshed by Sarasvati. The invocations in 10.17 address Sarasvati as a goddess of the forefathers as well as of the present generation. In 1.13, 1.89, 10.85, 10.66 and 10.141, she is listed with other gods and goddesses, not with rivers. In 10.65, she is invoked together with \"holy thoughts\" (') and \"munificence\" ('), consistent with her role as a goddess of both knowledge and fertility.\n\nThough Sarasvati initially emerged as a river goddess in the Vedic scriptures, in later Hinduism of the Puranas, she was rarely associated with the river. Instead she emerged as an independent goddess of knowledge, learning, wisdom, music and the arts. The evolution of the river goddess into the goddess of knowledge started with later Brahmanas, which identified her as \"Vāgdevī\", the goddess of speech, perhaps due to the centrality of speech in the Vedic cult and the development of the cult on the banks of the river. It is also possible that two independently postulated goddesses were fused into one in later Vedic times. Aurobindo has proposed, on the other hand, that \"the symbolism of the Veda betrays itself to the greatest clearness in the figure of the goddess Sarasvati...She is, plainly and clearly, the goddess of the World, the goddess of a divine inspiration...\".\n\nIn post-Rigvedic literature, the disappearance of the Sarasvati is mentioned. Also the origin of the Sarasvati is identified as Plaksa Prasravana (Peepal tree or Ashwattha tree as known in India and Nepal).\n\nIn a supplementary chapter of the Vajasaneyi-Samhita of the Yajurveda (34.11), Sarasvati is mentioned in a context apparently meaning the Sindhu: \"Five rivers flowing on their way speed onward to Sarasvati, but then become Sarasvati a fivefold river in the land.\" According to the medieval commentator Uvata, the five tributaries of the Sarasvati were the Punjab rivers Drishadvati, Satudri (Sutlej), Chandrabhaga (Chenab), Vipasa (Beas) and the Iravati (Ravi).\n\nThe first reference to the disappearance of the lower course of the Sarasvati is from the Brahmanas, texts that are composed in Vedic Sanskrit, but dating to a later date than the Veda Samhitas. The Jaiminiya Brahmana (2.297) speaks of the 'diving under (upamajjana) of the Sarasvati', and the Tandya Brahmana (or Pancavimsa Br.) calls this the 'disappearance' (vinasana). The same text (25.10.11-16) records that the Sarasvati is 'so to say meandering' (kubjimati) as it could not sustain heaven which it had propped up.\n\nThe Plaksa Prasravana (place of appearance/source of the river) may refer to a spring in the Siwalik mountains. The distance between the source and the Vinasana (place of disappearance of the river) is said to be 44 Ashwin(between several hundred and 1600 miles) (Tandya Br. 25.10.16; cf. Av. 6.131.3; Pancavimsa Br.).\n\nIn the Latyayana Srautasutra (10.15-19) the Sarasvati seems to be a perennial river up to the Vinasana, which is west of its confluence with the Drshadvati (Chautang). The Drshadvati is described as a seasonal stream (10.17), meaning it was not from Himalayas. Bhargava has identified Drashadwati river as present day Sahibi river originating from Jaipur hills in Rajasthan. The Asvalayana Srautasutra and Sankhayana Srautasutra contain verses that are similar to the Latyayana Srautasutra.\n\nAccording to the Mahabharata, the Sarasvati dried up to a desert (at a place named Vinasana or Adarsana) and joins the sea \"impetuously\". The desert made when Saraswati dried up was the Thar desert. MB.3.81.115 locates the state of Kurupradesh or Kuru Kingdom to the south of the Sarasvati and north of the Drishadvati. The dried-up, seasonal Ghaggar River in Rajasthan and Haryana reflects the same geographical view described in the Mahabharata.\n\nAccording to Hindu scriptures, a journey was made during the Mahabharata by Balrama along the banks of the Saraswati from Dwarka to Mathura. There were ancient kingdoms too (the era of the Mahajanapads) that lay in parts of north Rajasthan and that were named on the Saraswati River.\n\nSeveral Puranas describe the Sarasvati River, and also record that the river separated into a number of lakes (\"saras\").\n\nIn the Skanda Purana, the Sarasvati originates from the water pot of Brahma and flows from Plaksa on the Himalayas. It then turns west at Kedara and also flows underground. Five distributaries of the Sarasvati are mentioned. The text regards Sarasvati as a form of Brahma's consort Brahmi. According to the Vamana Purana 32.1-4, the Sarasvati rose from the Plaksa tree (Pipal tree).\n\nThe \"Padma Purana\" proclaims:\n\nDiana Eck notes that the power and significance of the Sarasvati for present-day India is in the persistent symbolic presence at the confluence of rivers all over India. Although \"materially missing\", she is the third river, which emerges to join in the meeting of rivers, thereby making the waters triple holy.\n\nAfter the Vedic Sarasvati dried, new myths about the rivers arose. Sarasvati is described to flow in the underworld and rise to the surface at some places. For centuries, the Sarasvati river existed in a \"subtle or mythic\" form, since it corresponds with none of the major rivers of present-day South Asia. The confluence (\"sangam\") or joining together of the Ganges and Yamuna rivers at Triveni Sangam, Allahabad, is believed to also converge with the unseen Sarasvati river, which is believed to flow underground. This despite Allahabad being a considerable distance from the possible historic routes of an actual Sarasvati river.\n\nAt the Kumbh Mela, a mass bathing festival is held at Triveni Sangam, literally \"confluence of the three rivers\", every 12 years. The belief of Sarasvati joining at the confluence of the Ganges and Yamuna originates from the Puranic scriptures and denotes the \"powerful legacy\" the Vedic river left after her disappearance. The belief is interpreted as \"symbolic\". The three rivers Sarasvati, Yamuna, Ganga are considered consorts of the Hindu Trinity (Trimurti) Brahma, Vishnu (as Krishna) and Shiva respectively.\n\nIn lesser known configuration, Sarasvati is said to form the \"Triveni\" confluence with rivers Hiranya and Kapila at Somnath. There are several other \"Triveni\"s in India where two physical rivers are joined by the \"unseen\" Sarasvati, which adds to the sanctity of the confluence.\n\nAccording to Michael Witzel, superimposed on the Vedic Sarasvati river is the heavenly river Milky Way, which is seen as \"a road to immortality and heavenly after-life.\" The description of the Sarasvati as the river of heavens, is interpreted to suggest its mythical nature.\n\nRomila Thapar notes that \"once the river had been mythologized through invoking the memory of the earlier river, its name - Sarasvati - could be applied to many rivers, which is what happened in various parts of the [Indian] subcontinent.\"\n\nSeveral present-day rivers are also named Sarasvati, after the Vedic Sarasvati:\n\nAlready since the 19th century, attempts have been made to identify the mythical Sarasvati of the Vedas with physical rivers. Many think that the Vedic Sarasvati river once flowed east of the Indus (Sindhu) river. Scientists, geologists as well as scholars have identified the Sarasvati with many present-day or now defunct rivers.\n\nTwo theories are popular in the attempts to identify the Sarasvati. Several scholars have identified the river with the present-day Ghaggar-Hakra River or dried up part of it, which is located in Northwestern India and Pakistan. A second popular theory associates the river with the Helmand river or an ancient river in the present Helmand Valley in Afghanistan. Others consider Sarasvati a mythical river.\n\nThe identification with the Ghaggar-Hakra system took on new significance in the early 21st century, suggesting an earlier dating of the Rig Veda, and renaming the Indus Valley Civilisation as the \"Sarasvati culture\", the \"Sarasvati Civilization\", the \"Indus-Sarasvati Civilization\" or the \"Sindhu-Sarasvati Civilization,\" suggesting that the Indus Valley and Vedic cultures can be equated.\n\nThe Rig Veda contains several hymns which give an indication of the flow of the geography of the river, and an identification with the Ghaggra-Hakra:\n\nYet, the Rig Veda also contains clues for an identification with the Helmand river in Afghanistan: \n\nThe Rig Veda was written during the latter part of the late Harappan period, and according to Shaffer, the reason for the predominance of the Sarasvati in the Rigveda is the late Harappan (1900-1300 BCE) population shift eastwards to Haryana.\n\nThe present Ghaggar-Hakra River is a seasonal river in India and Pakistan that flows only during the monsoon season, but satellite images in possession of the ISRO and ONGC have confirmed that the major course of a river ran through the present-day Ghaggar River. Late in the 2nd millennium BCE the Ghaggar-Hakra fluvial system dried up, which affected the Harappan civilisation. Painted Grey Ware sites (ca. 1000 BCE) have been found in the bed and not on the banks of the Ghaggar-Hakra river, suggesting that the river had dried up before this period.\n\nA number of archaeologists and geologists have identified the Sarasvati river with the present-day Ghaggar-Hakra River, or the dried up part of it. According to R.U.S. Prasad, \"we [...] find a considerable body of opinions [sic] among the scholars, archaeologists and geologists, who hold that the Sarasvati originated in the Shivalik hills [...] and descended through Adi Badri, situated in the foothills of the Shivaliks, to the plains [...] and finally debouched herself into the Arabian sea at the Rann of Kutch.\"\n\nIn the 19th and early 20th century a number of scholars, archaeologists and geologists have identified the Vedic Sarasvati River with the Ghaggar-Hakra River, such as Christian Lassen (1800-1876), Max Müller (1823-1900), Marc Aurel Stein (1862-1943), C.F. Oldham and Jane Macintosh. Similarly, recent archaeologists and geologists, such as Philip and Virdi (2006), K.S. Valdiya (2013) have identified the Sarasvati with Ghaggar. Danino notes that \"the 1500 km-long bed of the Sarasvati\" was \"rediscovered\" in the 19th century. According to Danino, \"most Indologists\" were convinced in the 19th century that \"the bed of the Ghaggar-Hakra was the relic of the Sarasvati.\"\n\nArchaeologists Gregory Possehl and Jane McIntosh refer to the Ghaggar-Hakra river as \"Sarasvati\" throughout their respective 2002 and 2008 books on the Indus Civilisation, and Gregory Possehl stated:\nAccording to Valdiya, \"it is plausible to conclude that once upon a time the Ghagghar was known as \"Sarsutī\",\" which is \"a corruption of \"Sarasvati\",\" because \"at Sirsā on the bank of the Ghagghar stands a fortress called \"Sarsutī\". Now in derelict condition, this fortress of antiquity celebrates and honours the river \"Sarsutī\".\"\n\nSome paleo-environmental scientists have proposed that the Hakkra was fed by Himalayan sources, which made it a mighty river, but dried-up between 2500 BCE and 1900 BCE, due to tectonic disturbances which caused a tilt in topography of Northwest India, resulting in the migration of rivers. According to this theory, the Sutlej moved westward and became a tributary of the Indus River, while the Yamuna moved eastward and became a tributary of the Ganges, supposedly in the early 2nd millennium BCE, while reaching its current bed by 1st millennium BCE. The Drishadvati bed retained only a small seasonal flow. The water loss due to these movements caused the Ghaggar-Hakra river to dry up in the Thar Desert.\n\nRomila Thapar terms the identification \"controversial\" and dismisses it, noticing that the descriptions of Sarasvati flowing through the \"high mountains\" does not tally with Ghaggar's course and suggests that Sarasvati is Haraxvati of Afghanistan. Wilke suggests that the identification is problematic since the Ghaggar-Hakra river was already dried up at the time of the composition of the Vedas, let alone the migration of the Vedic people into northern India.\n\nGiosan et al., in their study \"Fluvial landscapes of the Harappan civilisation\", make clear that the Ghaggar-Hakra fluvial system was not a large glacier-fed Himalayan river, but a monsoonal-fed river. They concluded that the Indus Valley Civilisation died out because the monsoons, which fed the rivers that supported the civilisation, diminished. With the rivers drying out as a result, the civilisation diminished some 4000 years ago. This in particular effected the Ghaggar-Hakra system, which became ephemeral and was largely abandoned. The Indus Valley Civilisation had the option to migrate east toward the more humid regions of the Indo-Gangetic Plain, where the decentralized late Harappan phase took place.\n\nClift et al. (2012), using dating of zircon sand grains, have shown that subsurface river channels near the Indus Valley Civilisation sites in Cholistan immediately below the presumed Ghaggar-Hakra channel show sediment affinity not with the Ghagger-Hakra, but instead with the Beas River in the western sites and the Sutlej and the Yamuna in the eastern ones, further weakening the hypothesis that the Ghaggar-Hakra was once a large river, but suggesting that the Yamuna itself, or a channel of the Yamuna, along with a channel of the Sutlej may have flowed west some time between 47,000 BCE and 10,000 BCE. The drainage from the Yamuna may have been lost from the Ghaggar-Hakra well before the beginnings of Indus civilization.\n\nAjit Singh et al. (2017) show that the paleochannel of the Ghaggar-Hakra is a former course of the Sutlej, which diverted to its present course between 15,000 and 8,000 years ago, well before the development of the Harappan Civilisation. Ajit Singh et al. conclude that the urban populations settled not along a perennial river, but a monsoon-fed seasonal river that was not subject to devastating floods.\n\nRajesh Kocchar further notes that, even if the Sutlej and the Yamuna had drained into the Ghaggar during Rig Vedic, it still would not fit the Rig Vedic descriptions because \"the snow-fed Satluj and Yamuna would strengthen lower Ghaggar. Upper Ghaggar would still be as puny as it is today.\"\n\nAn alternative suggestion for the identity of the early Rigvedic Sarasvati River is the Helmand River and its tributary Arghandab in the Arachosia region in Afghanistan, separated from the watershed of the Indus by the Sanglakh Range. The Helmand historically besides Avestan \"Haetumant\" bore the name \"Haraxvaiti\", which is the Avestan form cognate to Sanskrit \"Sarasvati\". The Avesta extols the Helmand in similar terms to those used in the Rigveda with respect to the Sarasvati: \"the bountiful, glorious Haetumant swelling its white waves rolling down its copious flood\". However unlike the Rigvedic Sarasvati, Helmand river never attained the status of a deity despite the praises in the Avesta.\n\nThe identification of the \"Sarasvati\" river with the \"Helmand\" river was first proposed by E. Thomas in 1886, followed by Alfred Hillebrandt a couple of years thereafter. However, in the same year, Geologist R.D. Oldham, refuted this Afghan Sarasvatī thesis. Indologist A.B. Keith (1879-1944) also didn't subscribe to this theory and stated that there is no conclusive evidence to identify the Sarasvati with the Helmand river. \n\nAccording to Konrad Klaus, the geographic situation of the Sarasvati and the Helmand rivers are similar. Both flow into a terminal lakes: the Helmand into a swamp in the Iranian plateau (the extended wetland and lake system of Hamun-i-Helmand). This matches the Rigvedic description of the Sarasvati flowing to the \"samudra\", which according to him at that time meant 'confluence', 'lake', 'heavenly lake, ocean'; the current meaning of 'terrestrial ocean' was not even felt in the Pali Canon.\n\nRajesh Kocchar, after a detailed analysis of the Vedic texts and geological environments of the rivers, concludes that there are two Sarasvati rivers mentioned in the Rigveda. The early Rigvedic Sarasvati, which he calls \"Naditama Sarasvati\", is described in suktas 2.41, 7.36 etc. of the family books of the Rigveda, and drains into a samudra. The description of the \"Naditama Sarasvati\" in the Rigveda matches the physical features of the Helmand River in Afghanistan, more precisely its tributary the Harut River, whose older name was \"Haraxatī\" in Avestan. The later Rigvedic Sarasvati, which he calls \"Vinasana Sarasvati\", is described in the Rigvedic Nadistuti sukta (10.75), which was composed centuries later, after an eastward migration of the bearers of the Rigvedic culture to the western Gangetic plain some 600 km to the east. The Sarasvati by this time had become a mythical \"disappeared\" river, and the name was transferred to the Ghaggar which disappeared in the desert. The later Rigvedic Sarasvati is only in the post-Rig Vedic Brahmanas said to disappear in the sands. According to Kocchar the Ganga and Yamuna were small streams in the vicinity of the Harut River. When the Vedic people moved east into Punjab, they named the new rivers they encountered after the old rivers they knew from Helmand, and the \"Vinasana Sarasvati\" may correspond with the Ghaggar-Hakra river.\n\nRomila Thapar has also suggested that the Sarasvati river is the ancient Haraxvati river of Afghanistan, noticing that the descriptions of the Sarasvati flowing through the \"high mountains\" cannot be reconciled with the actual course of the present-day Ghaggar-Hakkar river.\n\nThe Vedic and Puranic statements about the drying-up and diving-under of the Sarasvati have been used as a reference point for the dating of the Harappan civilisation and the Vedic culture. Some see these texts as evidence for an earlier dating of the Rig Veda, identifying the Sarasvati with the Ghaggar-Hakra River, rejecting the Indo-Aryan migrations theory, which postulates a migration at 1500 BCE.\n\nMichel Danino places the composition of the Vedas in the third millennium BCE, a millennium earlier than the conventional dates. Danino notes that accepting the Rig Veda accounts as factual descriptions, and dating the drying up late in the third millennium, are incompatible. According to Danino, this suggests that the Vedic people were present in northern India in the third millennium BCE, a conclusion which is controversial amongst professional archaeologists. Danino states that there is an absence of \"any intrusive material culture in the Northwest during the second millennium BCE,\" a biological continuity in the skeletal remains, and a cultural continuity. Danino then states that if the \"testimony of the Sarasvati is added to this,\"\nDanino acknowledges that this asks for \"studying its tentacular ramifications into linguistics, archaeoastronomy, anthropology and genetics, besides a few other fields\".\n\nAnnette Wilke notes that the \"historical river\" Sarasvati was a \"topographically tangible mythogeme\", which was already reduced to a \"small, sorry tickle in the desert\", by the time of composition of the Hindu epics. These post-Vedic texts regularly talk about drying up of the river, and start associating the goddess Sarasvati with language, rather than the river.\n\nMichael Witzel also notes that the Rig Veda indicates that the Sarswati \"had already lost its main source of water supply and must have ended in a terminal lake (samudra).\"\n\nThe Indus Valley Civilisation (Harrapan Civilisation), which is named after the Indus, was largely located on the banks of and in the proximity of the Ghaggar-Hakra fluvial system.\n\nThe Indus Valley Civilisation is sometimes called the \"Sarasvati culture\", the \"Sarasvati Civilization\", the \"Indus-Sarasvati Civilization\" or the \"Sindhu-Sarasvati Civilization\", as it is theorized that the civilisation flourished on banks of the Sarasvati river, along with the Indus. Danino notes that the dating of the Vedas to the third millennium BCE coincides with the mature phase of the Indus Valley civilisation, and that it is \"tempting\" to equate the Indus Valley and Vedic cultures.\n\nIn 2015, Reuters reported that \"members of the Rashtriya Swayamsevak Sangh believe that proof of the physical existence of the Vedic river would bolster their concept of a golden age of Hindu India, before invasions by Muslims and Christians.\" The Bharatiya Janata Party Government had therefore ordered archaeologists to search for the river.\n\nAccording to the government of Indian state of Haryana, research and satellite imagery of the region has confirmed to have found the lost river when water was detected during digging of the dry river bed at Yamunanagar. The government constituted Saraswati Heritage Development Board (SHDB) had conducted a trial run on July 30, 2016 filling the river bed with 100 cusecs of water which was pumped into a dug-up channel from tubewells at Uncha Chandna village in Yamunanagar. The water is expected to fill the channel until Kurukshetra, a distance of 40 kilometres. Once confirmed that there is no obstructions in the flow of the water, the government proposes to flow in another 100 cusecs after a fortnight. There also are plans to build three dams on the river route to keep it flowing perennially.\n\nAshoke Mukherjee (2001), is critical of the attempts to identify the Rigvedic Sarasvati. Mukherjee notes that many historians and archaeologists, both Indian and foreign, concluded that the word \"Sarasvati\" (literally \"being full of water\") is not a noun, a specific \"thing\". However, Mukherjee believes that \"Sarasvati\" is initially used by the Rig Vedic people as an adjective to the Indus as a large river and later evolved into a \"noun\". Mukherjee concludes that the Vedic poets had not seen the palaeo-Sarasvati, and that what they described in the Vedic verses refers to something else. He also suggests that in the post-Vedic and Puranic tradition the \"disappearance\" of Sarasvati, which to refers to \"[going] under [the] ground in the sands\", was created as a complementary myth to explain the visible non-existence of the river.\n\n\n\n"}
{"id": "1184557", "url": "https://en.wikipedia.org/wiki?curid=1184557", "title": "Satellite imagery", "text": "Satellite imagery\n\nSatellite imagery (also Earth observation imagery or spaceborne photography) are images of Earth or other planets collected by imaging satellites operated by governments and businesses around the world. Satellite imaging companies sell images by licensing them to governments and businesses such as Apple Maps and Google Maps.\n\nThe first images from space were taken on sub-orbital flights. The U.S-launched V-2 flight on October 24, 1946 took one image every 1.5 seconds. With an apogee of 65 miles (105 km), these photos were from five times higher than the previous record, the 13.7 miles (22 km) by the Explorer II balloon mission in 1935. The first satellite (orbital) photographs of Earth were made on August 14, 1959 by the U.S. Explorer 6. The first satellite photographs of the Moon might have been made on October 6, 1959 by the Soviet satellite Luna 3, on a mission to photograph the far side of the Moon. The Blue Marble photograph was taken from space in 1972, and has become very popular in the media and among the public. Also in 1972 the United States started the Landsat program, the largest program for acquisition of imagery of Earth from space. Landsat Data Continuity Mission, the most recent Landsat satellite, was launched on 11 February 2013. In 1977, the first real time satellite imagery was acquired by the United States's KH-11 satellite system.\nAll satellite images produced by NASA are published by NASA Earth Observatory and are freely available to the public. Several other countries have satellite imaging programs, and a collaborative European effort launched the ERS and Envisat satellites carrying various sensors. There are also private companies that provide commercial satellite imagery. In the early 21st century satellite imagery became widely available when affordable, easy to use software with access to satellite imagery databases was offered by several companies and organizations.\n\nSatellite images have many applications in meteorology, oceanography, fishing, agriculture, biodiversity conservation, forestry, landscape, geology, cartography, regional planning, education, intelligence and warfare. Images can be in visible colors and in other spectra. There are also elevation maps, usually made by radar images. Interpretation and analysis of satellite imagery is conducted using specialized remote sensing software.\n\nThere are four types of resolution when discussing satellite imagery in remote sensing: spatial, spectral, temporal, and radiometric. Campbell (2002) defines these as follows:\n\nThe resolution of satellite images varies depending on the instrument used and the altitude of the satellite's orbit. For example, the Landsat archive offers repeated imagery at 30 meter resolution for the planet, but most of it has not been processed from the raw data. Landsat 7 has an average return period of 16 days. For many smaller areas, images with resolution as high as 41 cm can be available.\n\nSatellite imagery is sometimes supplemented with aerial photography, which has higher resolution, but is more expensive per square meter. Satellite imagery can be combined with vector or raster data in a GIS provided that the imagery has been spatially rectified so that it will properly align with other data sets.\n\nGeoEye's GeoEye-1 satellite was launched on September 6, 2008. The GeoEye-1 satellite has the high resolution imaging system and is able to collect images with a ground resolution of 0.41 meters (16 inches) in the panchromatic or black and white mode. It collects multispectral or color imagery at 1.65-meter resolution or about 64 inches.\n\nDigitalGlobe's WorldView-2 satellite provides high resolution commercial satellite imagery with 0.46 m spatial resolution (panchromatic only). The 0.46 meters resolution of WorldView-2's panchromatic images allows the satellite to distinguish between objects on the ground that are at least 46 cm apart. Similarly DigitalGlobe's QuickBird satellite provides 0.6 meter resolution (at NADIR) panchromatic images.\n\nDigitalGlobe's WorldView-3 satellite provides high resolution commercial satellite imagery with 0.31 m spatial resolution. WVIII also carries a short wave infrared sensor and an atmospheric sensor\n\nThe 3 SPOT satellites in orbit (Spot 2, 4 and 5) provide images with a large choice of resolutions – from 2.5 m to 1 km. Spot Image also distributes multiresolution data from other optical satellites, in particular from Formosat-2 (Taiwan) and Kompsat-2 (South Korea) and from radar satellites (TerraSar-X, ERS, Envisat, Radarsat). Spot Image is also the exclusive distributor of data from the high resolution Pleiades satellites with a resolution of 0.50 meter or about 20 inches. The launches occurred in 2011 and 2012, respectively. The company also offers infrastructures for receiving and processing, as well as added value options.\n\nThe Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) is an imaging instrument onboard Terra, the flagship satellite of NASA's Earth Observing System (EOS) launched in December 1999. ASTER is a cooperative effort between NASA, Japan's Ministry of Economy, Trade and Industry (METI), and Japan Space Systems (J-spacesystems). ASTER data is used to create detailed maps of land surface temperature, reflectance, and elevation. The coordinated system of EOS satellites, including Terra, is a major component of NASA's Science Mission Directorate and the Earth Science Division. The goal of NASA Earth Science is to develop a scientific understanding of the Earth as an integrated system, its response to change, and to better predict variability and trends in climate, weather, and natural hazards.\n\n\nBlackBridge, previously known as RapidEye, operates a constellation of five satellites, launched in August 2008, the RapidEye constellation contains identical multispectral sensors which are equally calibrated. Therefore, an image from one satellite will be equivalent to an image from any of the other four, allowing for a large amount of imagery to be collected (4 million km² per day), and daily revisit to an area. Each travel on the same orbital plane at 630 km, and deliver images in 5 meter pixel size. RapidEye satellite imagery is especially suited for agricultural, environmental, cartographic and disaster management applications. The company not only offers their imagery, but consults their customers to create services and solutions based on analysis of this imagery .\n\nEarth Resource Observation Satellites, better known as “EROS” satellites, are lightweight, low earth orbiting, high-resolution satellites designed for fast maneuvering between imaging targets. In the commercial high-resolution satellite market, EROS is the smallest very high resolution satellite; it is very agile and thus enables very high performances. The satellites are deployed in a circular sun-synchronous near polar orbit at an altitude of 510 km (+/- 40 km).\nEROS satellites imagery applications are primarily for intelligence, homeland security and national development purposes but also employed in a wide range of civilian applications, including: mapping, border control, infrastructure planning, agricultural monitoring, environmental monitoring, disaster response, training and simulations, etc.\n\nEROS A – a high resolution satellite with 1.9-1.2m resolution panchromatic was launched on December 5, 2000.\n\nEROS B - the second generation of Very High Resolution satellites with 70 cm resolution panchromatic, was launched on April 25, 2006.\n\nThe Meteosat-2 geostationary weather satellite began operationally to supply imagery data on 16 August 1981. Eumetsat has operated the Meteosats since 1987.\n\nBecause the total area of the land on Earth is so large and because resolution is relatively high, satellite databases are huge and image processing (creating useful images from the raw data) is time-consuming. Depending on the sensor used, weather conditions can affect image quality: for example, it is difficult to obtain images for areas of frequent cloud cover such as mountain-tops. For such reasons, publicly available satellite image datasets are typically processed for visual or scientific commercial use by third parties.\n\nCommercial satellite companies do not place their imagery into the public domain and do not sell their imagery; instead, one must be licensed to use their imagery. Thus, the ability to legally make derivative products from commercial satellite imagery is minimized.\n\nPrivacy concerns have been brought up by some who wish not to have their property shown from above. Google Maps responds to such concerns in their FAQ with the following statement: \"We understand your privacy concerns... The images that Google Maps displays are no different from what can be seen by anyone who flies over or drives by a specific geographic location.\"\n\n\n"}
{"id": "1955818", "url": "https://en.wikipedia.org/wiki?curid=1955818", "title": "Season (society)", "text": "Season (society)\n\nThe social season, or season, refers to the traditional annual period when it is customary for members of a social elite of society to hold balls, dinner parties and charity events. Until World War I, it was also the appropriate time to be resident in the city rather than in the country in order to attend such events.\n\nIn modern times in the United Kingdom, \"the Season\" is known to encompass various prestigious events that take place during the spring and summer. According to Sloaney Season, it starts with Cheltenham Festival (March), and includes Grand National (April), Badminton Horse Trials (May), Chelsea Flower Show (May), Epsom Derby, Royal Ascot, Henley Royal Regatta (July), and others.\n\nThe London social season evolved in the 17th and 18th centuries, and in its traditional form it peaked in the 19th century. In this era the British elite was dominated by landowning aristocratic and gentry families who generally regarded their country house as their main home, but spent several months of the year in the capital to socialise and to engage in politics. The most exclusive events were held at the town mansions of leading members of the aristocracy. Exclusive public venues such as Almack's played a secondary role. The Season coincided with the sitting of parliament and began some time after Christmas and ran until midsummer, roughly late June.\n\nThe social season played a role in the political life of the country: the members of the two Houses of Parliament were almost all participants in the season. But the Season also provided an opportunity for the children of marriageable age of the nobility and gentry to be launched into society. Debutantes were formally introduced into society by presentation to the monarch at royal court until it was abolished by Queen Elizabeth II in 1958. \n\nThe traditional Season went into decline after the First World War, when many aristocratic families gave up their London mansions. From this time on an increasing number of society events took place at public venues, making it harder to maintain social exclusivity.\n\nMany events that take place far from central London came to be regarded as part of the social season, including Royal Ascot and the Henley Royal Regatta. The events that now constitute the London social season are increasingly hosted or sponsored by large companies (i.e. \"corporate hospitality\"). Western dress codes still apply to certain events in the season, especially where the Queen maintains an official role.\n\nAccording to the peerage guide Debrett's, the traditional social season runs from April to August. The Sloaney runs a detailed guide to the British Social Season, known as Sloaney Season . \n\n\n\n\n\n\nAlthough several of these events are not actually held in London, such as the Hurlingham Polo Association at Guards Polo Club, the organisers of most events attempt to avoid date clashes, so it is generally possible to visit all of them in the same year.\n\nThe traditional end of the London Season is the Glorious Twelfth of August, which marks the beginning of the shooting season. Society would retire to the country to shoot birds during the autumn and hunt foxes during the winter before coming back to London again with the spring.\n\nMany events of the season have traditional expectations with regard to Western dress codes.\n\nLondon is the capital of shops and of speculation, the government is made there. The aristocracy inscribes itself there only during sixty days, it there takes its orders, it inspects the government kitchen, it passes in review its daughters to marry, and equipages to sell, it says good-day and goes away promptly ; - it is so little amusing that it supports itself only for the few days called the season.\n\n"}
{"id": "9034773", "url": "https://en.wikipedia.org/wiki?curid=9034773", "title": "Si 363", "text": "Si 363\n\nSi 363 is a bifunctional organosilane chemical used in the reinforcement of rubber articles, especially tires. SI363 is the trade name of a silane bonding agent in the trialkoxymercaptoalkyl-silane class and of formula SH(CH)HSi(OCHHCHH)(O(CHHCHHO)H(CHH)HCHH)H.\n\nWhen applied to tires, Si 363 reduces rolling resistance, thus leading to increased fuel economy. Both alkoxsilicon and sulfur entities are present within its molecular structure.\n"}
{"id": "850994", "url": "https://en.wikipedia.org/wiki?curid=850994", "title": "Stream gradient", "text": "Stream gradient\n\nStream gradient is the grade measured by the ratio of drop in elevation of a stream per unit horizontal distance, usually expressed as meters per kilometer or feet per mile.\n\nA high gradient indicates a steep slope and rapid flow of water (i.e. more ability to erode); where as a low gradient indicates a more nearly level stream bed and sluggishly moving water, that may be able to carry only small amounts of very fine sediment. High gradient streams tend to have steep, narrow V-shaped valleys, and are referred to as young streams. Low gradient streams have wider and less rugged valleys, with a tendency for the stream to meander. Many rivers involve, to some extent, a flattening of the river gradient as approach the terminus at sea level.\n\nA stream that flows upon a uniformly erodible substrate will tend to have a steep gradient near its source, and a low gradient nearing zero as it reaches its base level. Of course, a uniform substrate would be rare in nature; hard layers of rock along the way may establish a temporary base level, followed by a high gradient, or even a waterfall, as softer materials are encountered below the hard layer.\n\nHuman dams, glaciation, changes in sea level, and many other factors can also change the \"normal\" or \"natural\" gradient pattern.\n\nOn topographic maps, stream gradient can be easily approximated if the scale of the map and the contour intervals are known. Contour lines form a V-shape on the map, pointing upstream. By counting the number of lines that cross a certain segment of a stream, multiplying this by the contour interval, and dividing that quantity by the length of the stream segment, one obtains an approximation to the stream gradient.\n\nBecause stream gradient is customarily given in feet per 1000 feet, one should then measure the amount a stream segment rises and the length of the stream segment in feet, then multiply feet per foot gradient by 1000. For example, if one measures a scale mile along the stream length, and counts three contour lines crossed on a map with ten-foot contours, the gradient is approximately 5.7 feet per 1000 feet, a fairly steep gradient.\n\n"}
{"id": "1013793", "url": "https://en.wikipedia.org/wiki?curid=1013793", "title": "Tube socket", "text": "Tube socket\n\nTube sockets are electrical sockets into which vacuum tubes (also known as valves) can be plugged, holding them in place and providing terminals, which can be soldered into the circuit, for each of the pins. Sockets are designed to allow tubes to be inserted in only one orientation. They were used in all tube electronic equipment to allow tubes, which failed relatively frequently, to be quickly tested and replaced. In the day of tube radio and television it was common in the US for local drug stores to have vacuum tube testers, and sell replacement tubes. Many nixie tubes were also designed to use sockets.\n\nThroughout the tube era, as technology developed, sometimes differently in different parts of the world, many tube bases and sockets came into use. Sockets are not universal; different tubes may fit mechanically into the same socket, though they may not work properly and possibly become damaged.\n\nTube sockets were typically mounted in holes on a sheet metal chassis and wires or other components were hand soldered to lugs on the underside of the socket. In the 1950s, printed circuit boards were introduced and tube sockets were developed whose contacts could be soldered directly to the printed wiring tracks. Looking at the bottom of a socket, or, equivalently, a tube from its bottom, the pins were numbered clockwise, starting at an index notch or gap, a convention that has persisted into the integrated circuit era.\n\nIn the 1930s, tubes often had the connection to the control grid brought out through a metal top cap on the top of the tube. This was connected by using a clip with an attached wire lead. An example would be the 6A7 pentagrid converter. Later, some tubes, particularly those used as radio frequency (RF) power amplifiers or horizontal deflection amplifiers in TV sets, such as the 6DQ6, had the plate or anode lead protrude through the envelope. In both cases this allowed the tube's output circuitry to be isolated from the input (grid) circuit more effectively. In the case of the tubes with the plate brought out to a cap, this also allowed the plate to run at higher voltages (over 26,000 volts in the case of rectifiers for color television, such as the 3A3, as well as high-voltage regulator tubes.) A few unusual tubes had caps for both grid and plate; the caps were symmetrically placed, with divergent axes.\n\nThe earliest tubes, like the \"Deforest Spherical Audion\" from c. 1911, used the typical light bulb Edison socket for the heater, and flying leads for the other elements. Other tubes directly used flying leads for all of their contacts, like the \"Cunningham AudioTron\" from 1915, or the \"Deforest Oscillion\". Type C6A xenon thyratrons, used in servos for the U.S. Navy Stable Element Mark 6, had a mogul screw base and L-shaped stiff wires at the top for grid and anode connections. Mating connectors were machined pairs of brass blocks with clamping screws, attached to flying leads (free hanging).\nWhen tubes became more widespread, and new electrodes were added, more connections were required. Specially designed bases were created to account for this need. However, as the world was suffering from World War I, and the new electronics technology was just emerging, designs were far from being standardized. Usually, each company had their own tubes and sockets, which were not interchangeable with tubes from other companies. By the early 1920s, this situation was finally changing, and several standard bases were created. They consisted of a base (ceramic, metal, bakelite, etc.) with a number of prongs ranging from three to seven, with either a non-regular distribution or with one or two of the prongs of bigger diameter than the other, so that the tube could only be inserted in a certain position. Sometimes they relied on a bayonet on the side of the base. Examples of these are the very common USA bases UX4, UV4, UY5 and UX6, and the European B5, B6, B7, B8, C7, G8A, etc. Tubes in the USA typically had from four to seven pins in a circular array, with adjacent pairs of larger pins for heater connections.\n\nBefore AC line/mains-powered radios were developed, some four-pin tubes (in particular, the very common UX-201A ('01A)) had a bayonet pin on the side of a cylindrical base. The socket used that pin for retaining the tube; insertion finished with a slight clockwise turn. Leaf springs, essentially all in the same plane, pressed upward on the bottoms of the pins, also keeping the bayonet pin engaged.\n\nThe first hot-cathode CRT, the Western Electric 224-B, had a standard four-pin bayonet base, and the bayonet pin was a live connection. (Five effective pins: It was an electrostatic-deflection gas-focused type, with a diode gun and single-ended deflection. The anode and the other two plates were common.)\n\nAn early exception to these types of bases is the Peanut 215, which instead of using prongs had a tiny bayonet base with four drop-like contacts. Another exception is the European Side Contact series commonly known as P, which instead of using a prong, relied on side contacts at 90 degrees from the tube axis with four to twelve contacts.\n\nIn 1935, RCA introduced a new type of tube base for their new metal envelope tubes, which they called an \"octal base\". As the name implies it had eight pins—more than were usually used previously. Octal bases, as defined in IEC 60067, diagram IEC 67-I-5a, have a 45-degree angle between pins, which form a diameter circle around a diameter keyed post (sometimes called a \"spigot\") in the center. Octal sockets were designed to accept octal tubes, the rib in the keyed post fitting an indexing slot in the socket so the tube could only be inserted in one orientation.\n\nWhen used on metal tubes, pin 1 was always reserved for a connection to the metal shell, which was usually grounded for shielding purposes. The octal base soon caught on for glass tubes, where the large central post could also house and protect the \"evacuation tip\" of the glass tube. The eight available pins allowed more complex tubes than before, such as dual triodes, to be constructed. The glass envelope of an octal base tube was cemented into a bakelite or plastic base with a hollow post in the center, surrounded by eight metal pins. The wire leads from the tube were soldered into the pins, and the evacuation tip was protected inside the post.\n\nMatching plugs were also manufactured that let tube sockets be used as eight-pin electrical connectors; penurious experimenters would sometimes salvage the base from a discarded tube for this purpose. Octal sockets were used to mount other components, particularly electrolytic capacitor assemblies and electrical relays; octal-mount relays are still common.\n\nMost octal tubes following the widespread European designation system have penultimate digit \"3\" as in ECC34 (full details in the Mullard–Philips tube designation article). There is a different, totally obsolete, pre-world-war-II German octal type.\n\nOctal and miniature tubes are still in use in tube-type audio hi-fi and guitar amplifiers. Relays were historically manufactured in a vacuum tube form, and industrial grade relays continue to use the octal base for their pinout.\n\nA variant of the octal base, the B8G loctal base or \"lock-in\" base (sometimes spelled \"loktal\" — trademarked by Sylvania), was developed by Sylvania for ruggedized applications such as automobile radios. Along with B8B (a British designation out of date by 1958), these eight-pin locking bases are almost identical and the names usually taken as interchangeable (although there are some minor differences in specifications, such as spigot material and spigot taper, etc.). The pin geometry was the same as for octal, but the pins were thinner (although they will fit into a standard octal socket, they wobble and do not make good contact), the base shell was made of aluminium, and the center hole had an electrical contact that also mechanically locked (hence \"loctal\") the tube in place. Loctal tubes were only used widely by a few equipment manufacturers, most notably Philco, which used the tubes in many table radios. Loctal tubes have a small indexing mark on the side of the base skirt; they do not release easily from their sockets unless pushed from that side. Because the pins are actually the Fernico or Cunife lead-out wires from the tube, they are prone to intermittent connections caused by the build-up of electrolytic corrosion products due to the pin being of a different metallic composition to the socket contact.\n\nThe loctal tube's structure was supported directly by the connecting pins passing through the glass \"button\" base. Octal tube structures were supported on a glass \"pinch\", formed by heating the bottom of the envelope to fusing temperature, then squeezing the pinch closed. Sealing the pinch embedded the connecting wires in the pinch's glass and gave a vacuum-tight seal. The connecting wires then passed through the hollow base pins, where they were soldered to make permanent connections.\n\nLoctal tubes had shorter connecting lengths between the socket pins and the internal elements than did their octal counterparts. This allowed them to operate at higher frequencies than octal tubes. The advent of miniature \"all-glass\" seven- and nine-pin tubes overtook both octals and loctals, so the loctal's higher-frequency potential was never fully exploited.\n\nLoctal tube type numbers in the USA typically begin with \"7\" (for 6.3-volt types) or \"14\" for 12.6-volt types. This was fudged by specifying the heater voltage as \"nominally\" 7 or 14 volts so that the tube nomenclature fitted. Battery types (mostly 1.4-volt) are coded \"1Lxn\", where \"x\" is a letter and \"n\" a number, such as \"1LA4\". Russian loctals \"end\" in L, e.g. 6J1L. European designations are ambiguous; all B8G loctals have numbers either in the range:\n\n\nEfforts to introduce small tubes into the marketplace date from the 1920s, when experimenters and hobbyists made radios with so-called peanut tubes like the Peanut 215 mentioned above. Because of the primitive manufacturing techniques of the time, these tubes were too unreliable for commercial use.\n\nRCA announced in \"Electronics\" magazine, new miniature tubes, which proved reliable. The first ones, such as the 6J6 ECC91 VHF dual triode, were introduced in 1939. The bases commonly referred to as \"miniature\" are the 7-pin B7G type, and the slightly later 9-pin B9A (Noval). The pins are arranged evenly in a circle of eight or ten evenly spaced positions, with one pin omitted; this allows the tube to be inserted in only one orientation. Keying by omitting a pin is also used in 8 (subminiature), 10, and 12-pin (Compactron) tubes (a variant 10-pin form, \"Noval+1\", is basically a 9-pin socket with an added center contact).\n\nAs with loctal tubes, the pins of miniature tube are stiff wires protruding through the bottom of the glass envelope which plug directly into the socket. However, unlike all their predecessors, miniature tubes are not fitted with separate bases; the base is an integral part of the glass envelope. The pinched-off air evacuation nub is at the top of the tube, giving it its distinctive appearance. More than one functional section can be included in a single envelope; a dual triode configuration is particularly common. Seven- and nine-pin tubes were common, though miniature tubes with more pins, such as the Compactron series, were later introduced, and could fit up to three amplifying elements. Some miniature tube sockets had a skirt that mated with a cylindrical metal electrostatic shield that surrounded the tube, fitted with a spring to hold the tube in place if the equipment was subject to vibration. Sometimes the shield was also fitted with thermal contacts to transfer heat from the glass envelope to the shield and act as a heat sink, which was considered to improve tube life in higher power applications.\n\nElectrolytic effects from the differing metal alloys used for the miniature tube pins (usually Cunife or Fernico) and the tube base could cause intermittent contact due to local corrosion, especially in relatively low current tubes, such as were used in battery-operated radio sets. Malfunctioning equipment with miniature tubes can sometimes be brought back to life by removing and reinserting the tubes, disturbing the insulating layer of corrosion.\n\nMiniature tubes were widely manufactured for military use during World War II, and also used in consumer equipment. The Sonora Radio and Television Corporation produced the first radio using these miniature tubes, the \"Candid\", in April 1940. In June 1940 RCA released its battery-operated Model BP-10, the first superheterodyne receiver small enough to fit in a handbag or coat pocket. This model had the following tube lineup: 1R5 — pentagrid converter; 1T4 — I.F. amplifier; 1S5 — Detector/AVC/AF Amplifier; 1S4 — Audio Output. The BP-10 proved so popular that Zenith, Motorola, Emerson, and other radio manufacturers produced similar pocket radios based on RCA's miniature tubes. Several of these pocket radios were introduced in 1941 and sold until the suspension of radio production in April 1942 for the duration of World War II.\n\nAfter the war miniature tubes continued to be manufactured for civilian use, regardless of any technical advantage, as they were cheaper than octals and loctals.\n\nThe B7G (or \"small-button\" or \"heptal\") seven-pin miniature tubes are smaller than Noval, with seven pins arranged at 45-degree spacing in a 9.53 mm (3/8th inch) diameter arc, the \"missing\" pin position being used to position the tube in its socket (unlike octal, loctal and rimlock sockets). Examples include the 6AQ5/EL90 and 6BE6/EK90. European tubes of this type have numbers 90-99, 100-109, 190-199, 900-999. A few in the 100-109 series have unusual, non-B7G bases, \"e.g.\", Wehrmacht base.\n\nThe nine-pin miniature Noval B9A base, sometimes called button 9-pin, B9-1, offered a useful reduction in physical size compared to previous common types, such as octal (especially important in TV receivers where space was limited), while also providing a sufficient number of connections (unlike B7G) to allow effectively unrestricted access to all the electrodes, even of relatively complex tubes such as double triodes and triode-hexodes. It could also provide multiple connections to an electrode of a simpler device where useful, as in the four connections to the grid of a conventional grounded-grid UHF triode, \"e.g.\", 6AM4, to minimise the deleterious effects of lead inductance on the high-frequency performance.\n\nThis base type was used by many of the United States and most of the European tubes, \"e.g.\", 12AX7-ECC83, EF86 and EL84, produced commercially towards the end of the era before transistors largely displaced their use.\n\nThe IEC 67-I-12a specification calls for a 36-degree angle between the nine pins of 1.016 mm thickness, in an arc of diameter 11.89 mm.\n\nEuropean tubes of this type have numbers 80-89, 180-189, 280-289, 800-899, 8000-8999.\n\nThe Duodecar B12C base (IEC 67-I-17a) has 12 pins in a 19.1 mm diameter circle and dates from 1961. It was also called the Compactron T-9 construction/E12-70 base It is generally similar in form to a Noval socket, but larger. In the center is a clearance hole for a tube evacuation pip, which is typically on the bottom of a Compactron tube. (It should not be confused with the similar-sounding but differently sized Duodecal B12A base.)\n\nThe Rimlock (B8A) base is an 8-pin design with a pin circle diameter close to Noval, and uses a nub on the side of the envelope to engage with a guide and retaining spring in the socket wall. This provides pin registration (since the pins are equi-spaced) and also a fair degree of retention. Early tubes with this base type typically had a metal skirt around the lower ~15mm of the envelope to match the socket wall, and this offered a degree of built-in screening, but these were fairly soon replaced by \"skirtless\" versions, which had a characteristic widening in the glass to compensate physically for the absence of the skirt. In the European naming scheme, rimlock tubes are numbered in the ranges 40-49, 110-119 (with exceptions), and 400-499, \"e.g.\", EF40. Although virtually unknown elsewhere, this was a very common base type in European radios of the late 1940s through the 1950s, but was eventually displaced by the ubiquitous B7G and Noval (B9A) base types.\n\nBy 1935 new tube technologies were required for the development of radar and telecommunications. UHF requirements severely limited the existing tubes, so radical ideas were implemented which affected how these tubes connected to the host system. Two new bases appeared, the acorn tube and the lighthouse tube, both solving the same problems but with different approaches. Thompson, G.M. Rose, Saltzberg and Burnside from RCA created the acorn tube by using far smaller electrodes, with radial short connections. A different approach was taken by the designers of the lighthouse tube, such as the octal-base 2C43, which relied on using concentric cylindrical metal contacts in connections that minimized inductance, thus allowing a much higher frequency.\n\nNuvistors were very small, reducing stray capacitances and lead inductances. The base and socket were so compact that they were widely used in UHF TV tuners. They could also be used in small-signal applications at lower frequencies, as in the Ampex MR-70, a costly studio tape recorder whose entire electronics section was based on nuvistors.\n\nThere are many other socket types, of which a few are:\n\nA remarkably wide variety of tube and similar sockets is listed and described, with some informal application notes, at a commercial site, Pacific T.V., including nuvistor, 8-pin subminiature, vidicon, reflex klystron, nine-pin octal-like, 10-pin miniature (two types), 11-pin sub-magnal, diheptal 14-pin, and many display tubes such as Nixies and vacuum fluorescent types (and even more). As well, each socket has a link to a clear, high-quality picture. \n\nSome subminiature tubes with flexible wire leads all exiting in the same plane were connected by subminiature inline sockets. \n\nSome low-power reflex klystrons such as the 2K25 and 2K45 had small-diameter rigid coaxial outputs parallel to octal base pins. To accommodate the coax, one contact was replaced by a clearance hole.\n\nVacuum tubes for high-power applications often required custom socket designs. A jumbo four-prong socket was used for various industrial tubes. A specialized 7-pin socket (Septar or B7A), with all pins in a circle with one pin wider than the others, was used for transmitting tubes. Subminiature tubes with long wire leads, introduced in the 1950s, were often soldered directly to printed circuit boards. Sockets were made for early transistors, but quickly fell out of favor as transistor reliability became established. This also happened with early integrated circuits; IC sockets later became used only for devices that may need to be upgraded.\n\n"}
{"id": "5187243", "url": "https://en.wikipedia.org/wiki?curid=5187243", "title": "Yellow hypergiant", "text": "Yellow hypergiant\n\nA yellow hypergiant is a massive star with an extended atmosphere, a spectral class from A to K, and an initial mass of about 20–60 solar masses but having lost as much as half that mass. They are amongst the most visually luminous stars, with absolute magnitude (M) around −9, but also one of the rarest with just 15 known in the Milky Way and six of those in just a single cluster. They are sometimes referred to as cool hypergiants in comparison to O- and B-type stars, and sometimes as warm hypergiants in comparison to red supergiants.\n\nThe term \"hypergiant\"\" was used as early as 1929, but not for the stars currently known as hypergiants. Hypergiants are defined by their '0' luminosity class, and are higher in luminosity than the brightest supergiants of class Ia, although they were not referred to as hypergiants until the late 1970s. Another criterion for hypergiants was also suggested in 1979 for some other highly luminous mass-losing hot stars, but was not applied to cooler stars. In 1991, Rho Cassiopeiae was the first to be described as a yellow hypergiant, likely becoming grouped as a new class of luminous stars during discussions at the \"Solar physics and astrophysics at interferometric resolution\" workshop in 1992.\n\nDefinitions of the term hypergiant remains vague, and although luminosity class 0 is for hypergiants, they are more commonly designated by the alternative luminosity classes Ia-0 and Ia. Their great stellar luminosities are determined from various spectral features, which are sensitive to surface gravity, such as Hβ line widths in hot stars or a strong Balmer discontinuity in cooler stars. Lower surface gravity often indicates larger stars, and hence, higher luminosities. In cooler stars, the width of observed oxygen lines, such as O I at 777.4 nm., can be used to calibrate directly against stellar luminosity.\n\nOne astrophysical method used to definitively identify yellow hypergiants is the so-called \"Keenan-Smolinski\" criterion. Here all absorption lines should be strongly broadened, beyond those expected with bright supergiant stars, and also show strong evidence of significant mass loss. Furthermore, at least one broadened Hα component should also be present. They may also display very complex Hα profiles, typically having strong emission lines combined with absorption lines.\n\nThe terminology of yellow hypergiants is further complicated by referring to them as either cool hypergiants or warm hypergiants, depending on the context. Cool hypergiants refers to all sufficiently luminous and unstable stars cooler than blue hypergiants and LBVs, including both yellow and red hypergiants. The term warm hypergiants has been used for highly luminous class A and F stars in M31 and M33 that are not LBVs, as well as more generally for yellow hypergiants.\n\nYellow hypergiants occupy a region of the Hertzsprung–Russell diagram above the instability strip, a region where relatively few stars are found and where those stars are generally unstable. The spectral and temperature ranges are approximately A0-K2 and 4,000-8,000K respectively. The area is bounded on the high-temperature side by the \"Yellow Evolutionary Void\" where stars of this luminosity become extremely unstable and experience severe mass loss. The “Yellow Evolutionary Void” separates yellow hypergiants from luminous blue variables although yellow hypergiants at their hottest and luminous blue variables at their coolest can have approximately the same temperature near 8,000 K. At the lower temperature bound, yellow hypergiants and red supergiants are not clearly separated; RW Cephei (4,500 K, ) is an example of a star that shares characteristics of both yellow hypergiants and red supergiants.\n\nYellow hypergiants have a fairly narrow range of luminosities above (e.g. V382 Carinae at ) and below the Humphrey-Davidson limit at around . With their output peaking in the middle of the visual range, these are the most visually bright stars known with absolute magnitudes around -9 or -9.5 .\n\nThey are large and somewhat unstable, with very low surface gravities. Where yellow supergiants have surface gravities (log g) below about 2, the yellow hypergiants have log g around zero. In addition they pulsate irregularly, producing small variations in temperature and brightness. This produces very high mass loss rates, and nebulosity is common around the stars. Occasional larger outbursts can temporarily obscure the stars.\n\nYellow hypergiants form from massive stars after they have evolved away from the main sequence. Most observed yellow hypergiants have been through a red supergiant phase and are evolving back towards higher temperatures, but a few are seen in the brief first transition from main sequence to red supergiant. Supergiants with an initial mass less than will explode as a supernova while still red supergiants, while stars more massive than about will never cool beyond blue supergiant temperatures. The exact mass ranges depend on metallicity and rotation. Yellow supergiants cooling for the first time may be massive stars of up to or more, but post-red supergiant stars will have lost around half their initial mass.\n\nChemically, most yellow hypergiants show strong surface enhancement of nitrogen and also of sodium and some other heavy elements. Carbon and oxygen are depleted, while helium is enhanced, as expected for a post-main-sequence star.\n\nYellow hypergiants have clearly evolved off the main sequence and so have depleted the hydrogen in their cores. The majority of yellow hypergiants are postulated to be post-red supergiants evolving blueward, while more stable and less luminous yellow supergiants are likely to be evolving to red supergiants for the first time. There is strong chemical and surface gravity evidence that the brightest of the yellow supergiants, HD 33579, is currently expanding from a blue supergiant to a red supergiant.\n\nThese stars are doubly rare because they are very massive, initially hot class O-type main-sequence stars more than 15 times as massive as the Sun, but also because they spend only a few thousand years in the unstable yellow void phase of their lives. In fact, it is difficult to explain even the small number of observed yellow hypergiants, relative to red supergiants of comparable luminosity, from simple models of stellar evolution. The most luminous red supergiants may execute multiple \"blue loops\", shedding much of their atmosphere, but without actually ever reaching the blue supergiant stage, each one taking only a few decades at most. Conversely, some apparent yellow hypergiants may be hotter stars, such as the \"missing\" LBVs, masked within a cool pseudo-photosphere.\n\nRecent discoveries of blue supergiant supernova progenitors have also raised the question of whether stars could explode directly from the yellow hypergiant stage. A handful of possible yellow supergiant supernova progenitors have been discovered, but they all appear to be of relatively low mass and luminosity, not hypergiants. SN 2013cu is a type IIb supernova whose progenitor has been directly and clearly observed. It was an evolved star around 8,000K showing extreme mass loss of helium and nitrogen enriched material. Although the luminosity is not known, only a yellow hypergiant or luminous blue variable in outburst would have these properties.\n\nModern models suggest that stars with a certain range of masses and rotation rates may explode as supernovae without ever becoming blue supergiants again, but many will eventually pass right through the yellow void and become low-mass low-luminosity luminous blue variables and possibly Wolf–Rayet stars after that. Specifically, more massive stars and those with higher mass loss rates due to rotation or high metallicity will evolve beyond the yellow hypergiant stage to hotter temperatures before reaching core collapse.\n\nAccording to the current physical models of stars, a yellow hypergiant should possess a convective core surrounded by a radiative zone, as opposed to a sun-sized star, which consists of a radiative core surrounded by a convective zone. Because of their extreme luminosity and internal structure, yellow hypergiants suffer high rates of mass loss and are generally surrounded by envelopes of expelled material. A photogenic example of the nebulae that can result is IRAS 17163-3907, known as the Fried Egg, which has expelled several solar masses of material in just a few hundred years.\n\nThe yellow hypergiant is an expected phase of evolution as the most luminous red supergiants evolve bluewards, but they may also represent a different sort of star. LBVs during eruption have such dense winds that they form a pseudo-photosphere which appears as a larger cooler star despite the underlying blue supergiant being largely unchanged. These are observed to have a very narrow range of temperatures around 8,000K. At the bistability jump which occurs around 21,000K blue supergiant winds become several times denser and could be result in an even cooler pseudo-photosphere. No LBVs are observed just below the luminosity where the bistability jump crosses the S Doradus instability strip (not to be confused with the Cepheid instability strip), but it is theorised that they do exist and appear as yellow hypergiants because of their pseudo-photospheres.\n\n\nIn Westerlund 1:\n\nIn other galaxies:\n\n"}
{"id": "31040618", "url": "https://en.wikipedia.org/wiki?curid=31040618", "title": "Zeeman energy", "text": "Zeeman energy\n\nZeeman energy, or the external field energy, is the potential energy of a magnetised body in an external magnetic field. It is named after the Dutch physicist Pieter Zeeman, primarily known by the Zeeman effect. In SI units, it is given by\n\nwhere H is the external field, M the local magnetisation, and the integral is done over the volume of the body. This is the statistical average (over a \nunit volume macroscopic sample) of a corresponding microscopic Hamiltonial (energy) for each individual magnetic moment m, which is however experiencing a \"local\" induction B:\n\n"}
