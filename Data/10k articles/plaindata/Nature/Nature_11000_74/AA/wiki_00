{"id": "38227740", "url": "https://en.wikipedia.org/wiki?curid=38227740", "title": "1889–90 flu pandemic", "text": "1889–90 flu pandemic\n\nThe 1889–1890 flu pandemic (October 1889 – December 1890, with recurrences March – June 1891, November 1891 – June 1892, winter 1893–1894 and early 1895) was a deadly influenza pandemic that killed about 1 million people worldwide. The outbreak was dubbed \"Asiatic flu\" or \"Russian flu\" (not to be confused with the 1977–1978 epidemic caused by Influenza A/USSR/90/77 H1N1, which was also called Russian flu). For some time the virus strain responsible was conjectured (but not proven) to be Influenza A virus subtype H2N2. More recently, the strain was asserted to be Influenza A virus subtype H3N8.\n\nModern transport infrastructure assisted the spread of the 1889 influenza. The 19 largest European countries, including the Russian Empire, had 202,887 km of railroads and transatlantic travel by boat took less than six days (not significantly different than current travel time by air, given the time scale of the global spread of a pandemic).\n\nThe pandemic was first recorded in Saint Petersburg, Russia in December 1889. In four months it had spread throughout the Northern Hemisphere. Deaths peaked in Saint Petersburg on December 1, 1889, and in the United States during the week of January 12, 1890. The median time between the first reported case and peak mortality was five weeks.\n\nResearchers have tried for many years to identify the subtypes of Influenza A responsible for the 1889–1890, 1898–1900 and 1918 epidemics. Initially, this work was primarily based on \"seroarcheology\"—the detection of antibodies to influenza infection in the sera of elderly people—and it was thought that the 1889–1890 pandemic was caused by Influenza A subtype H2, the 1898–1900 epidemic by subtype H3, and the 1918 pandemic by subtype H1. With the confirmation of H1N1 as the cause of the 1918 flu pandemic following identification of H1N1 antibodies in exhumed corpses, reanalysis of seroarcheological data has indicated that Influenza A subtype H3 (possibly the H3N8 subtype), is the most likely cause for the 1889–1890 pandemic.\n\n\n"}
{"id": "16977319", "url": "https://en.wikipedia.org/wiki?curid=16977319", "title": "Absolute molar mass", "text": "Absolute molar mass\n\nAbsolute molar mass is a process used to determine the characteristics of molecules.\n\nThe first absolute measurements of molecular weights (i.e. made without reference to standards) were based on fundamental physical characteristics and their relation to the molar mass. The most useful of these were membrane osmometry and sedimentation.\n\nAnother absolute instrumental approach was also possible with the development of light scattering theory by Einstein, Raman, Debye, Zimm, and others. The problem with measurements made using membrane osmometry and sedimentation was that they only characterized the bulk properties of the polymer sample. Moreover, the measurements were excessively time consuming and prone to operator error. In order to gain information about a polydisperse mixture of molar masses, a method for separating the different sizes was developed. This was achieved by the advent of size exclusion chromatography (SEC). SEC is based on the fact that the pores in the packing material of chromatography columns could be made small enough for molecules to become temporarily lodged in their interstitial spaces. As the sample makes its way through a column the smaller molecules spend more time traveling in these void spaces than the larger ones, which have fewer places to \"wander\". The result is that a sample is separated according to its hydrodynamic volume formula_1. As a consequence, the big molecules come out first, and then the small ones follow in the elutent. By choosing a suitable column packing material it is possible to define the resolution of the system. Columns can also be combined in series to increase resolution or the range of sizes studied.\n\nThe next step is to convert the time at which the samples eluted into a measurement of molar mass. This is possible because if the molar mass of a standard were known, the time at which this standard eluted should be equal to a specific molar mass. Using multiple standards, a calibration curve of time versus molar mass can be developed. This is significant for polymer analysis because a single polymer could be shown to have many different components, and the complexity and distribution of which would also affect the physical properties. However this technique has shortcomings. For example, unknown samples are always measured in relation to known standards, and these standards may or may not have similarities to the sample of interest. The measurements made by SEC are then mathematically converted into data similar to that found by the existing techniques.\n\nThe problem was that the system was calibrated according to the Vh characteristics of polymer standards that are not directly related to the molar mass. If the relationship between the molar mass and Vh of the standard is not the same as that of the unknown sample, then the calibration is invalid. Thus, to be accurate, the calibration must use the same polymer, of the same conformation, in the same eluent and have the same interaction with the solvent as the hydration layer changes Vh.\n\nBenoit \"et al.\" showed that taking into account the hydrodynamic volume would solve the problem. In his publication, Benoit showed that all synthetic polymers elutes on the same curve when the log of the intrinsic viscosity multiplied by the molar mass was plotted against the elution volume. This is the basis of universal calibration which requires a viscometer to measure the intrinsic viscosity of the polymers. Universal calibration was shown to work for branched polymers, copolymers as well as starburst polymers.\n\nFor good chromatography, there must be no interaction with the column other than that produced by size. As the demands on polymer properties increased, the necessity of getting absolute information on the molar mass and size also increased. This was especially important in pharmaceutical applications where slight changes in molar mass (e.g. aggregation) or shape may result in different biological activity. These changes can actually have a harmful effect instead of a beneficial one.\n\nTo obtain molar mass, light scattering instruments need to measure the intensity of light scattered at zero angle. This is impractical as the laser source would outshine the light scattering intensity at zero angle. The 2 alternatives are to measure very close to zero angle or to measure at many angle and extrapolate using a model (Rayleigh, Rayleigh-Gans-Debye, Berry, Mie, etc.) to zero degree angle.\n\nTraditional light scattering instruments worked by taking readings from multiple angles, each being measured in series. A low angle light scattering system was developed in the early 1970s that allowed a single measurement to be used to calculate the molar mass. Although measurements at low angles are better for fundamental physical reasons (molecules tend to scatter more light in lower angle directions than in higher angles), low angle scattering events caused by dust and contamination of the mobile phase easily overwhelm the scattering from the molecules of interest. When the low-angle laser light scattering (LALLS) became popular in the 1970s and mid-1980s, good quality disposable filters were not readily available and hence multi-angle measurements gained favour.\n\nMulti-angle light scattering was invented in the mid-1980s and instruments like that were able to make measurements at the different angles simultaneously but it was not until the later 1980s (10-12) that the connection of multi-angle laser light scattering (MALS) detectors to SEC systems was a practical proposition enabling both molar mass and size to be determined from each slice of the polymer fraction.\n\nLight scattering measurements can be applied to synthetic polymers, proteins, pharmaceuticals and particles such as liposomes, micelles, and encapsulated proteins. Measurements can be made in one of two modes which are un-fractionated (batch mode) or in continuous flow mode (with SEC, HPLC or any other flow fractionation method). Batch mode experiments can be performed either by injecting a sample into a flow cell with a syringe or with the use of discrete vials. These measurements are most often used to measure timed events like antibody-antigen reactions or protein assembly. Batch mode measurements can also be used to determine the second virial coefficient (A2), a value that gives a measure of the likelihood of crystallization or aggregation in a given solvent. Continuous flow experiments can be used to study material eluting from virtually any source. More conventionally, the detectors are coupled to a variety of different chromatographic separation systems. The ability to determine the mass and size of the materials eluting then combines the advantage of the separation system with an absolute measurement of the mass and size of the species eluting.\n\nThe addition of an SLS detector coupled downstream to a chromatographic system allows the utility of SEC or similar separation combined with the advantage of an absolute detection method. The light scattering data is purely dependent on the light scattering signal times the concentration; the elution time is irrelevant and the separation can be changed for different samples without recalibration. In addition, a non-size separation method such as HPLC or IC can also be used.\nAs the light scattering detector is mass dependent, it becomes more sensitive as the molar mass increases. Thus it is an excellent tool for detecting aggregation. The higher the aggregation number, the more sensitive the detector becomes.\n\nLALS measurements are measuring at a very low angle where the scattering vector is almost zero. LALS does not need any model to fit the angular dependence and hence is giving more reliable molecular weights measurements for large molecules. LALS alone does not give any indication of the root mean square radius.\n\nMALS measurements work by calculating the amount of light scattered at each angle detected. The calculation is based on the intensity of light measured and the quantum efficiency of each detector. Then a model is used to approximate the intensity of light scattered at zero angle. The zero angle light scattered is then related to the molar mass.\n\nAs previously noted, the MALS detector can also provide information about the size of the molecule. This information is the Root Mean Square radius of the molecule (RMS or Rg). This is different from the Rh mentioned above who is taking the hydration layer into account. The purely mathematical root mean square radius is defined as the radii making up the molecule multiplied by the mass at that radius.\n\n"}
{"id": "36036985", "url": "https://en.wikipedia.org/wiki?curid=36036985", "title": "Amakinite", "text": "Amakinite\n\nAmakinite is a semi transparent yellow-green hydroxide mineral belonging to the brucite group that was discovered in 1962. Its chemical formula is written as (Fe,Mg)(OH). It usually occurs in the form of splotchy, anhedral crystals forming within a group or structure in other minerals or rocks, such as kimberlite. Its composition is as follows:\n\nAmakinite is slightly magnetic and was named for the Amakin Expedition, which prospected the diamond deposits of Yakutia.\n\n"}
{"id": "44205299", "url": "https://en.wikipedia.org/wiki?curid=44205299", "title": "Baudin Conservation Park", "text": "Baudin Conservation Park\n\nBaudin Conservation Park is a protected area located on the north coast of Dudley Peninsula on Kangaroo Island in South Australia about south east of Penneshaw. It was proclaimed under the \"National Parks and Wildlife Act 1972\" in 2002. The conservation park is classified as an IUCN Category III protected area.\n\n"}
{"id": "3518750", "url": "https://en.wikipedia.org/wiki?curid=3518750", "title": "Big Bay State Park", "text": "Big Bay State Park\n\nBig Bay State Park is a state park of Wisconsin, United States, on Madeline Island, the largest of 22 Apostle Islands in Lake Superior. The park has picturesque sandstone bluffs and caves and a sand beach. It encloses unique habitat types including lakeside dunes, sphagnum bogs, and old-growth forest. Bald eagles return annually to the park to nest and rear offspring.\n\nThe park, established in 1963, has picnic areas with tables, grills, water and toilets; a campground with drinking water, showers and toilets; an indoor camp for nonprofit groups; an outdoor group camp; and more than of trails, including nature trails. The park is open year-round, though winter visitation is mostly limited to hunters, snowshoers, and cross-country skiers.\n\nAll vehicles are required to purchase an admission pass, though pedestrians and bicyclists may enter free. To reach the park, visitors must take a 20-minute ferry ride from Bayfield, then travel approximately east on Highway H.\n\n"}
{"id": "7807", "url": "https://en.wikipedia.org/wiki?curid=7807", "title": "Cavitation", "text": "Cavitation\n\nCavitation is the formation of vapour cavities in a liquid, small liquid-free zones (\"bubbles\" or \"voids\"), that are the consequence of forces acting upon the liquid. It usually occurs when a liquid is subjected to rapid changes of pressure that cause the formation of cavities in the liquid where the pressure is relatively low. When subjected to higher pressure, the voids implode and can generate an intense shock wave.\n\nCavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n\nInertial cavitation is the process where a void or bubble in a liquid rapidly collapses, producing a shock wave. Inertial cavitation occurs in nature in the strikes of mantis shrimps and pistol shrimps, as well as in the vascular tissues of plants. In man-made objects, it can occur in control valves, pumps, propellers and impellers.\n\nNon-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.\n\nSince the shock waves formed by collapse of the voids are strong enough to cause significant damage to moving parts, cavitation is usually an undesirable phenomenon. It is very often specifically avoided in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.\n\nInertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined \"cavitation inception\" and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.\n\nOther ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a perfect vacuum, but has a relatively low gas pressure. Such a low-pressure bubble in a liquid begins to collapse due to the higher pressure of the surrounding medium. As the bubble collapses, the pressure and temperature of the vapor within increases. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.\n\nInertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n\nThe physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local temperature of the liquid reaches the saturation temperature, and further heat is supplied to allow the liquid to sufficiently phase change into a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.\n\nIn order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold.\n\nThe vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n\nNon-inertial cavitation is the process in which small bubbles in a liquid are forced to oscillate in the presence of an acoustic field, when the intensity of the acoustic field is insufficient to cause total bubble collapse. This form of cavitation causes significantly less erosion than inertial cavitation, and is often used for the cleaning of delicate materials, such as silicon wafers.\n\nHydrodynamic cavitation describes the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n\nHydrodynamic cavitation can be produced by passing a liquid through a constricted channel at a specific flow velocity or by mechanical rotation of an object through a liquid. In the case of the constricted channel and based on the specific (or unique) geometry of the system, the combination of pressure and kinetic energy can create the hydrodynamic cavitation cavern downstream of the local constriction generating high energy cavitation bubbles.\n\nThe process of bubble generation, and the subsequent growth and collapse of the cavitation bubbles, results in very high energy densities and in very high local temperatures and local pressures at the surface of the bubbles for a very short time. The overall liquid medium environment, therefore, remains at ambient conditions. When uncontrolled, cavitation is damaging; by controlling the flow of the cavitation, however, the power can be harnessed and non-destructive. Controlled cavitation can be used to enhance chemical reactions or propagate certain unexpected reactions because free radicals are generated in the process due to disassociation of vapors trapped in the cavitating bubbles.\n\nOrifices and venturi are reported to be widely used for generating cavitation. A venturi has an inherent advantage over an orifice because of its smooth converging and diverging sections, such that it can generate a higher flow velocity at the throat for a given pressure drop across it. On the other hand, an orifice has an advantage that it can accommodate a greater number of holes (larger perimeter of holes) in a given cross sectional area of the pipe.\n\nThe cavitation phenomenon can be controlled to enhance the performance of high-speed marine vessels and projectiles, as well as in material processing technologies, in medicine, etc. Controlling the cavitating flows in liquids can be achieved only by advancing the mathematical foundation of the cavitation processes. These processes are manifested in different ways, the most common ones and promising for control being bubble cavitation and supercavitation. The first exact classical solution should perhaps be credited to the well- known solution by H. Helmholtz in 1868. The earliest distinguished studies of academic type on the theory of a cavitating flow with free boundaries and supercavitation were published in the book \"Jets, wakes and cavities\" followed by \"Theory of jets of ideal fluid\". Widely used in these books was the well-developed theory of conformal mappings of functions of a complex variable, allowing one to derive a large number of exact solutions of plane problems. Another venue combining the existing exact solutions with approximated and heuristic models was explored in the work \"Hydrodynamics of Flows with Free Boundaries\" that refined the applied calculation techniques based on the principle of cavity expansion independence, theory of pulsations and stability of elongated axisymmetric cavities, etc. and in \"Dimensionality and similarity methods in the problems of the hydromechanics of vessels\".\n\nA natural continuation of these studies was recently presented in \"The Hydrodynamics of Cavitating Flows\" – an encyclopedic work encompassing all the best advances in this domain for the last three decades, and blending the classical methods of mathematical research with the modern capabilities of computer technologies. These include elaboration of nonlinear numerical methods of solving 3D cavitation problems, refinement of the known plane linear theories, development of asymptotic theories of axisymmetric and nearly axisymmetric flows, etc. As compared to the classical approaches, the new trend is characterized by expansion of the theory into the 3D flows. It also reflects a certain correlation with current works of an applied character on the hydrodynamics of supercavitating bodies.\n\nHydrodynamic cavitation can also improve some industrial processes. For instance, cavitated corn slurry shows higher yields in ethanol production compared to uncavitated corn slurry in dry milling facilities.\n\nThis is also used in the mineralization of bio-refractory compounds which otherwise would need extremely high temperature and pressure conditions since free radicals are generated in the process due to the dissociation of vapors trapped in the cavitating bubbles, which results in either the intensification of the chemical reaction or may even result in the propagation of certain reactions not possible under otherwise ambient conditions.\n\nIn industry, cavitation is often used to homogenize, or mix and break down, suspended particles in a colloidal liquid compound such as paint mixtures or milk. Many industrial mixing machines are based upon this design principle. It is usually achieved through impeller design or by forcing the mixture through an annular opening that has a narrow entrance orifice with a much larger exit orifice. In the latter case, the drastic decrease in pressure as the liquid accelerates into a larger volume induces cavitation. This method can be controlled with hydraulic devices that control inlet orifice size, allowing for dynamic adjustment during the process, or modification for different substances. The surface of this type of mixing valve, against which surface the cavitation bubbles are driven causing their implosion, undergoes tremendous mechanical and thermal localized stress; they are therefore often constructed of super-hard or tough materials such as stainless steel, Stellite, or even polycrystalline diamond (PCD).\n\nCavitating water purification devices have also been designed, in which the extreme conditions of cavitation can break down pollutants and organic molecules. Spectral analysis of light emitted in sonochemical reactions reveal chemical and plasma-based mechanisms of energy transfer. The light emitted from cavitation bubbles is termed sonoluminescence.\n\nUse of this technology has been tried successfully in alkali refining of vegetable oils.\n\nHydrophobic chemicals are attracted underwater by cavitation as the pressure difference between the bubbles and the liquid water forces them to join together. This effect may assist in protein folding.\n\nCavitation plays an important role for the destruction of kidney stones in shock wave lithotripsy. Currently, tests are being conducted as to whether cavitation can be used to transfer large molecules into biological cells (sonoporation). Nitrogen cavitation is a method used in research to lyse cell membranes while leaving organelles intact.\n\nCavitation plays a key role in non-thermal, non-invasive fractionation of tissue for treatment of a variety of diseases and can be used to open the blood-brain barrier to increase uptake of neurological drugs in the brain.\n\nCavitation also plays a role in HIFU, a thermal non-invasive treatment methodology for cancer. \n\nUltrasound sometimes is used to increase bone formation, for instance in post-surgical applications.\nUltrasound treatments or exposure can create cavitation that potentially may \"result in a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness, and fatigue.\".\n\nIt has been suggested that the sound of \"cracking\" knuckles derives from the collapse of cavitation in the synovial fluid within the joint. Movements that cause cracking expand the joint space, thus reducing pressure to the point of cavitation. It remains controversial whether this is associated with clinically significant joint injury such as osteoarthritis. Some physicians say that osteoarthritis is caused by cracking knuckles regularly, as this causes wear and tear and may cause the bone to weaken. The implication being that, it is not the \"bubbles popping,\" but rather, the bones rubbing together, that causes osteoarthritis.\n\nIn industrial cleaning applications, cavitation has sufficient power to overcome the particle-to-substrate adhesion forces, loosening contaminants. The threshold pressure required to initiate cavitation is a strong function of the pulse width and the power input. This method works by generating controlled acoustic cavitation in the cleaning fluid, picking up and carrying contaminant particles away so that they do not reattach to the material being cleaned.\n\nCavitation has been applied to egg pasteurization. A hole-filled rotor produces cavitation bubbles, heating the liquid from within. Equipment surfaces stay cooler than the passing liquid, so eggs don't harden as they did on the hot surfaces of older equipment. The intensity of cavitation can be adjusted, making it possible to tune the process for minimum protein damage.\n\nCavitation is, in many cases, an undesirable occurrence. In devices such as propellers and pumps, cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. Cavitation has also become a concern in the renewable energy sector as it may occur on the blade surface of tidal stream turbines.\n\nWhen the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.\n\nAlthough the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller's or pump's lifetime.\n\nAfter a surface is initially affected by cavitation, it tends to erode at an accelerating pace. The cavitation pits increase the turbulence of the fluid flow and create crevices that act as nucleation sites for additional cavitation bubbles. The pits also increase the components' surface area and leave behind residual stresses. This makes the surface more prone to stress corrosion.\n\nMajor places where cavitation occurs are in pumps, on propellers, or at restrictions in a flowing liquid.\n\nAs an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure around it can become. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n\nCavitation in pumps may occur in two different forms:\n\nSuction cavitation occurs when the pump suction is under a low-pressure/high-vacuum condition where the liquid turns into a vapor at the eye of the pump impeller. This vapor is carried over to the discharge side of the pump, where it no longer sees vacuum and is compressed back into a liquid by the discharge pressure. This imploding action occurs violently and attacks the face of the impeller. An impeller that has been operating under a suction cavitation condition can have large chunks of material removed from its face or very small bits of material removed, causing the impeller to look spongelike. Both cases will cause premature failure of the pump, often due to bearing failure. Suction cavitation is often identified by a sound like gravel or marbles in the pump casing.\n\nCommon causes of suction cavitation can include clogged filters, pipe blockage on the suction side, poor piping design, pump running too far right on the pump curve, or conditions not meeting NPSH (net positive suction head) requirements.\n\nIn automotive applications, a clogged filter in a hydraulic system (power steering, power brakes) can cause suction cavitation making a noise that rises and falls in synch with engine RPM. It is fairly often a high pitched whine, like set of nylon gears not quite meshing correctly.\n\nDischarge cavitation occurs when the pump discharge pressure is extremely high, normally occurring in a pump that is running at less than 10% of its best efficiency point. The high discharge pressure causes the majority of the fluid to circulate inside the pump instead of being allowed to flow out the discharge. As the liquid flows around the impeller, it must pass through the small clearance between the impeller and the pump housing at extremely high flow velocity. This flow velocity causes a vacuum to develop at the housing wall (similar to what occurs in a venturi), which turns the liquid into a vapor. A pump that has been operating under these conditions shows premature wear of the impeller vane tips and the pump housing. In addition, due to the high pressure conditions, premature failure of the pump's mechanical seal and bearings can be expected. Under extreme conditions, this can break the impeller shaft.\n\nDischarge cavitation in joint fluid is thought to cause the popping sound produced by bone joint cracking, for example by deliberately cracking one's knuckles.\n\nSince all pumps require well-developed inlet flow to meet their potential, a pump may not perform or be as reliable as expected due to a faulty suction piping layout such as a close-coupled elbow on the inlet flange. When poorly developed flow enters the pump impeller, it strikes the vanes and is unable to follow the impeller passage. The liquid then separates from the vanes causing mechanical problems due to cavitation, vibration and performance problems due to turbulence and poor filling of the impeller. This results in premature seal, bearing and impeller failure, high maintenance costs, high power consumption, and less-than-specified head and/or flow.\n\nTo have a well-developed flow pattern, pump manufacturer's manuals recommend about (10 diameters?) of straight pipe run upstream of the pump inlet flange. Unfortunately, piping designers and plant personnel must contend with space and equipment layout constraints and usually cannot comply with this recommendation. Instead, it is common to use an elbow close-coupled to the pump suction which creates a poorly developed flow pattern at the pump suction.\n\nWith a double-suction pump tied to a close-coupled elbow, flow distribution to the impeller is poor and causes reliability and performance shortfalls. The elbow divides the flow unevenly with more channeled to the outside of the elbow. Consequently, one side of the double-suction impeller receives more flow at a higher flow velocity and pressure while the starved side receives a highly turbulent and potentially damaging flow. This degrades overall pump performance (delivered head, flow and power consumption) and causes axial imbalance which shortens seal, bearing and impeller life.\nTo overcome cavitation:\nIncrease suction pressure if possible.\nDecrease liquid temperature if possible.\nThrottle back on the discharge valve to decrease flow-rate.\nVent gases off the pump casing.\n\nCavitation can occur in control valves. If the actual pressure drop across the valve as defined by the upstream and downstream pressures in the system is greater than the sizing calculations allow, pressure drop flashing or cavitation may occur. The change from a liquid state to a vapor state results from the increase in flow velocity at or just downstream of the greatest flow restriction which is normally the valve port. To maintain a steady flow of liquid through a valve the flow velocity must be greatest at the vena contracta or the point where the cross sectional area is the smallest. This increase in flow velocity is accompanied by a substantial decrease in the fluid pressure which is partially recovered downstream as the area increases and flow velocity decreases. This pressure recovery is never completely to the level of the upstream pressure. If the pressure at the vena contracta drops below the vapor pressure of the fluid bubbles will form in the flow stream. If the pressure recovers after the valve to a pressure that is once again above the vapor pressure, then the vapor bubbles will collapse and cavitation will occur.\n\nWhen water flows over a dam spillway, the irregularities on the spillway surface will cause small areas of flow separation in a high-speed flow, and, in these regions, the pressure will be lowered. If the flow velocities are high enough the pressure may fall to below the local vapor pressure of the water and vapor bubbles will form. When these are carried downstream into a high pressure region the bubbles collapse giving rise to high pressures and possible cavitation damage.\n\nExperimental investigations show that the damage on concrete chute and tunnel spillways can start at clear water flow velocities of between 12 and 15 m/s, and, up to flow velocities of 20 m/s, it may be possible to protect the surface by streamlining the boundaries, improving the surface finishes or using resistant materials.\n\nWhen some air is present in the water the resulting mixture is compressible and this damps the high pressure caused\nby the bubble collapses. If the flow velocities near the spillway invert are sufficiently high, aerators (or aeration devices) must be introduced to prevent cavitation. Although these have been installed for some years, the mechanisms of air entrainment at the aerators and the slow movement of the air away from the spillway surface are still challenging.\n\nThe spillway aeration device design is based upon a small deflection of the spillway bed (or sidewall) such as a ramp and offset to deflect the high flow velocity flow away from the spillway surface. In the cavity formed below the nappe, a local subpressure beneath the nappe is produced by which air is sucked into the flow. The complete design includes the deflection device (ramp, offset) and the air supply system.\n\nSome larger diesel engines suffer from cavitation due to high compression and undersized cylinder walls. Vibrations of the cylinder wall induce alternating low and high pressure in the coolant against the cylinder wall. The result is pitting of the cylinder wall, which will eventually let cooling fluid leak into the cylinder and combustion gases to leak into the coolant.\n\nIt is possible to prevent this from happening with the use of chemical additives in the cooling fluid that form a protective layer on the cylinder wall. This layer will be exposed to the same cavitation, but rebuilds itself. Additionally a regulated overpressure in the cooling system (regulated and maintained by the coolant filler cap spring pressure) prevents the forming of cavitation.\n\nFrom about the 1980s, new designs of smaller gasoline engines also displayed cavitation phenomena. One answer to the need for smaller and lighter engines was a smaller coolant volume and a correspondingly higher coolant flow velocity. This gave rise to rapid changes in flow velocity and therefore rapid changes of static pressure in areas of high heat transfer. Where resulting vapor bubbles collapsed against a surface, they had the effect of first disrupting protective oxide layers (of cast aluminium materials) and then repeatedly damaging the newly formed surface, preventing the action of some types of corrosion inhibitor (such as silicate based inhibitors). A final problem was the effect that increased material temperature had on the relative electrochemical reactivity of the base metal and its alloying constituents. The result was deep pits that could form and penetrate the engine head in a matter of hours when the engine was running at high load and high speed. These effects could largely be avoided by the use of organic corrosion inhibitors or (preferably) by designing the engine head in such a way as to avoid certain cavitation inducing conditions.\n\nSome hypotheses relating to diamond formation posit a possible role for cavitation—namely cavitiation in the kimberlite pipes providing the extreme pressure needed to change pure carbon into the rare allotrope that is diamond.\n\nThe loudest three sounds ever recorded, during the 1883 eruption of Krakatoa, are now understood as the bursts of three huge cavitation bubbles, each larger than the last, formed in the volcano's throat. Rising magma, filled with dissolved gasses and under immense pressure, encountered a different magma that compressed easily, allowing bubbles to grow and combine. \n\nThere exist macroscopic white lamellae inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n\nCavitation occurs in the xylem of vascular plants when the tension of water within the xylem exceeds atmospheric pressure. The sap vaporizes locally so that either the vessel elements or tracheids are filled with water vapor. Plants are able to repair cavitated xylem in a number of ways. For plants less than 50 cm tall, root pressure can be sufficient to redissolve the vapor. Larger plants direct solutes into the xylem via \"ray cells\", or in tracheids, via osmosis through bordered pits. Solutes attract water, the pressure rises and vapor can redissolve. In some trees, the sound of the cavitation is audible, particularly in summer, when the rate of evapotranspiration is highest. Some deciduous trees have to shed leaves in the autumn partly because cavitation increases as temperatures decrease.\n\nJust as cavitation bubbles form on a fast-spinning boat propeller, they may also form on the tails and fins of aquatic animals. This primarily occurs near the surface of the ocean, where the ambient water pressure is low.\n\nCavitation may limit the maximum swimming speed of powerful swimming animals like dolphins and tuna. Dolphins may have to restrict their speed because collapsing cavitation bubbles on their tail are painful. Tuna have bony fins without nerve endings and do not feel pain from cavitation. They are slowed down when cavitation bubbles create a vapor film around their fins. Lesions have been found on tuna that are consistent with cavitation damage.\n\nSome sea animals have found ways to use cavitation to their advantage when hunting prey. The pistol shrimp snaps a specialized claw to create cavitation, which can kill small fish. The mantis shrimp (of the \"smasher\" variety) uses cavitation as well in order to stun, smash open, or kill the shellfish that it feasts upon.\n\nThresher sharks use 'tail slaps' to debilitate their small fish prey and cavitation bubbles have been seen rising from the apex of the tail arc.\n\nIn the last half-decade, coastal erosion in the form of inertial cavitation has been generally accepted. Bubbles in an incoming wave are forced into cracks in the cliff being eroded. Varying pressure decompresses some vapor pockets which subsequently implode. The resulting pressure peaks can blast apart fractions of the rock.\n\n\n\n"}
{"id": "32397466", "url": "https://en.wikipedia.org/wiki?curid=32397466", "title": "Cirrus floccus", "text": "Cirrus floccus\n\nCirrus floccus is a type of cirrus cloud. The name \"cirrus floccus\" is derived from Latin, meaning \"a lock of wool\". Cirrus floccus occurs as small tufts of cloud, usually with a ragged base. The cloud can have virga falling from it, but the precipitation does not reach the ground. The individual tufts are usually isolated from each other. At formation, the cirrus floccus clouds are bright white and can be mistaken for altocumulus clouds; however, after a few minutes, the brightness begins to fade, indicating they are made up of pure ice, and are therefore at a higher level.\n\n\n"}
{"id": "763555", "url": "https://en.wikipedia.org/wiki?curid=763555", "title": "Cogeneration", "text": "Cogeneration\n\nCogeneration or combined heat and power (CHP) is the use of a heat engine or power station to generate electricity and useful heat at the same time. Trigeneration or combined cooling, heat and power (CCHP) refers to the simultaneous generation of electricity and useful heating and cooling from the combustion of a fuel or a solar heat collector. The terms \"cogeneration\" and \"trigeneration\" can be also applied to the power systems generating simultaneously electricity, heat, and industrial chemicals – e.g., syngas or pure hydrogen (article: combined cycles, chapter: natural gas integrated power & syngas (hydrogen) generation cycle).\n\nCogeneration is a more efficient use of fuel because otherwise wasted heat from electricity generation is put to some productive use. Combined heat and power (CHP) plants recover otherwise wasted thermal energy for heating. This is also called combined heat and power district heating. Small CHP plants are an example of decentralized energy. By-product heat at moderate temperatures (100–180 °C, 212–356 °F) can also be used in absorption refrigerators for cooling.\n\nThe supply of high-temperature heat first drives a gas or steam turbine-powered generator. The resulting low-temperature waste heat is then used for water or space heating. At smaller scales (typically below 1 MW) a gas engine or diesel engine may be used. Trigeneration differs from cogeneration in that the waste heat is used for both heating and cooling, typically in an absorption refrigerator. Combined cooling, heat and power systems can attain higher overall efficiencies than cogeneration or traditional power plants. In the United States, the application of trigeneration in buildings is called building cooling, heating and power. Heating and cooling output may operate concurrently or alternately depending on need and system construction.\n\nCogeneration was practiced in some of the earliest installations of electrical generation. Before central stations distributed power, industries generating their own power used exhaust steam for process heating. Large office and apartment buildings, hotels and stores commonly generated their own power and used waste steam for building heat. Due to the high cost of early purchased power, these CHP operations continued for many years after utility electricity became available.\n\nMany process industries, such as chemical plants, oil refineries and pulp and paper mills, require large amounts of process heat for such operations as chemical reactors, distillation columns, steam driers and other uses. This heat, which is usually used in the form of steam, can be generated at the typically low pressures used in heating, or can be generated at much higher pressure and passed through a turbine first to generate electricity. In the turbine the steam pressure and temperature is lowered as the internal energy of the steam is converted to work. The lower pressure steam leaving the turbine can then be used for process heat.\n\nSteam turbines at thermal power stations are normally designed to be fed high pressure steam, which exits the turbine at a condenser operating a few degrees above ambient temperature and at a few millimeters of mercury absolute pressure. (This is called a \"condensing\" turbine.) For all practical purposes this steam has negligible useful energy before it is condensed. Steam turbines for cogeneration are designed either for \"extraction\" of some steam at lower pressures after it has passed through a number of turbine stages, with the un-extracted steam going on through the turbine to a condenser. In this case, the extracted steam causes a mechanical power loss in the downstream stages of the turbine. Or they are designed, with or without extraction, for final exhaust at \"back pressure\" (non-condensing). The extracted or exhaust steam is used for process heating. Steam at ordinary process heating conditions still has a considerable amount of enthalpy that could be used for power generation, so cogeneration has an opportunity cost.\n\nA typical power generation turbine in a paper mill may have extraction pressures of 160 psig (1.103 MPa) and 60 psig (0.41 MPa). A typical back pressure may be 60 psig (0.41 MPa). In practice these pressures are custom designed for each facility. Conversely, simply generating process steam for industrial purposes instead of high enough pressure to generate power at the top end also has an opportunity cost (See: Steam supply and exhaust conditions). The capital and operating cost of high pressure boilers, turbines and generators are substantial. This equipment is normally operated continuously, which usually limits self-generated power to large-scale operations.\n\nA combined cycle (in which several thermodynamic cycles produce electricity), may also be used to extract heat using a heating system as condenser of the power plant's bottoming cycle. For example, the RU-25 MHD generator in Moscow heated a boiler for a conventional steam powerplant, whose condensate was then used for space heat. A more modern system might use a gas turbine powered by natural gas, whose exhaust powers a steam plant, whose condensate provides heat. Cogeneration plants based on a combined cycle power unit can have thermal efficiencies above 80%.\n\nThe viability of CHP (sometimes termed utilisation factor), especially in smaller CHP installations, depends on a good baseload of operation, both in terms of an on-site (or near site) electrical demand and heat demand. In practice, an exact match between the heat and electricity needs rarely exists. A CHP plant can either meet the need for heat (\"heat driven operation\") or be run as a power plant with some use of its waste heat, the latter being less advantageous in terms of its utilisation factor and thus its overall efficiency. The viability can be greatly increased where opportunities for trigeneration exist. In such cases, the heat from the CHP plant is also used as a primary energy source to deliver cooling by means of an absorption chiller.\n\nCHP is most efficient when heat can be used on-site or very close to it. Overall efficiency is reduced when the heat must be transported over longer distances. This requires heavily insulated pipes, which are expensive and inefficient; whereas electricity can be transmitted along a comparatively simple wire, and over much longer distances for the same energy loss.\n\nA car engine becomes a CHP plant in winter when the reject heat is useful for warming the interior of the vehicle. The example illustrates the point that deployment of CHP depends on heat uses in the vicinity of the heat engine.\n\nThermally enhanced oil recovery (TEOR) plants often produce a substantial amount of excess electricity. After generating electricity, these plants pump leftover steam into heavy oil wells so that the oil will flow more easily, increasing production. TEOR cogeneration plants in Kern County, California produce so much electricity that it cannot all be used locally and is transmitted to Los Angeles.\n\nCHP is one of the most cost-efficient methods of reducing carbon emissions from heating systems in cold climates and is recognized to be the most energy efficient method of transforming energy from fossil fuels or biomass into electric power. \nCogeneration plants are commonly found in district heating systems of cities, central heating systems of larger buildings (e.g. hospitals, hotels, prisons) and are commonly used in the industry in thermal production processes for process water, cooling, steam production or CO fertilization.\n\nTopping cycle plants primarily produce electricity from a steam turbine. Partly expanded steam is then condensed in a heating condensor at a temperature level that is suitable e.g. district heating or water desalination.\n\nBottoming cycle plants produce high temperature heat for industrial processes, then a waste heat recovery boiler feeds an electrical plant. Bottoming cycle plants are only used in industrial processes that require very high temperatures such as furnaces for glass and metal manufacturing, so they are less common.\n\nLarge cogeneration systems provide heating water and power for an industrial site or an entire town. Common CHP plant types are:\n\nSmaller cogeneration units may use a reciprocating engine or Stirling engine. The heat is removed from the exhaust and radiator. The systems are popular in small sizes because small gas and diesel engines are less expensive than small gas- or oil-fired steam-electric plants.\n\nSome cogeneration plants are fired by biomass, or industrial and municipal solid waste (see incineration). Some CHP plants utilize waste gas as the fuel for electricity and heat generation. Waste gases can be gas from animal waste, landfill gas, gas from coal mines, sewage gas, and combustible industrial waste gas.\n\nSome cogeneration plants combine gas and solar photovoltaic generation to further improve technical and environmental performance. Such hybrid systems can be scaled down to the building level and even individual homes.\n\nMicro combined heat and power or 'Micro cogeneration\" is a so-called distributed energy resource (DER). The installation is usually less than 5 kW in a house or small business. Instead of burning fuel to merely heat space or water, some of the energy is converted to electricity in addition to heat. This electricity can be used within the home or business or, if permitted by the grid management, sold back into the electric power grid.\n\nDelta-ee consultants stated in 2013 that with 64% of global sales the fuel cell micro-combined heat and power passed the conventional systems in sales in 2012. 20.000 units were sold in Japan in 2012 overall within the Ene Farm project. With a Lifetime of around 60,000 hours. For PEM fuel cell units, which shut down at night, this equates to an estimated lifetime of between ten and fifteen years. For a price of $22,600 before installation. For 2013 a state subsidy for 50,000 units is in place.\n\nMicroCHP installations use five different technologies: microturbines, internal combustion engines, stirling engines, closed cycle steam engines and fuel cells. One author indicated in 2008 that MicroCHP based on Stirling engines is the most cost effective of the so-called microgeneration technologies in abating carbon emissions; A 2013 UK report from Ecuity Consulting stated that MCHP is the most cost-effective method of utilising gas to generate energy at the domestic level. however, advances in reciprocation engine technology are adding efficiency to CHP plant, particularly in the biogas field. As both MiniCHP and CHP have been shown to reduce emissions they could play a large role in the field of CO reduction from buildings, where more than 14% of emissions can be saved using CHP in buildings. The University of Cambridge reported a cost effective steam engine MicroCHP prototype in 2017 which has the potential to be commercially competitive in the following decades.\n\nA plant producing electricity, heat and cold is called a trigeneration or polygeneration plant. Cogeneration systems linked to absorption chillers use waste heat for refrigeration.\n\nIn the United States, Consolidated Edison distributes 66 billion kilograms of 350 °F (180 °C) steam each year through its seven cogeneration plants to 100,000 buildings in Manhattan—the biggest steam district in the United States. The peak delivery is 10 million pounds per hour (or approximately 2.5 GW).\n\nCogeneration is still common in pulp and paper mills, refineries and chemical plants. In this \"industrial cogeneration/CHP\", the heat is typically recovered at higher temperatures (above 100 deg C) and used for process steam or drying duties. This is more valuable and flexible than low-grade waste heat, but there is a slight loss of power generation. The increased focus on sustainability has made industrial CHP more attractive, as it substantially reduces carbon footprint compared to generating steam or burning fuel on-site and importing electric power from the grid.\n\nIndustrial cogeneration plants normally operate at much lower boiler pressures than utilities. Among the reasons are: 1) Cogeneration plants face possible contamination of returned condensate. Because boiler feed water from cogeneration plants has much lower return rates than 100% condensing power plants, industries usually have to treat proportionately more boiler make up water. Boiler feed water must be completely oxygen free and de-mineralized, and the higher the pressure the more critical the level of purity of the feed water. 2) Utilities are typically larger scale power than industry, which helps offset the higher capital costs of high pressure. 3) Utilities are less likely to have sharp load swings than industrial operations, which deal with shutting down or starting up units that may represent a significant percent of either steam or power demand.\n\nA heat recovery steam generator (HRSG) is a steam boiler that uses hot exhaust gases from the gas turbines or reciprocating engines in a CHP plant to heat up water and generate steam. The steam, in turn, drives a steam turbine or is used in industrial processes that require heat.\n\nHRSGs used in the CHP industry are distinguished from conventional steam generators by the following main features:\n\nA heat pump may be compared with a CHP unit as follows. If, to supply thermal energy, the exhaust steam from the turbo-generator must be taken at a higher temperature than the system would produce most electricity at, the lost electrical generation is \"as if\" a heat pump were used to provide the same heat by taking electrical power from the generator running at lower output temperature and higher efficiency. Typically for every unit of electrical power lost, then about 6 units of heat are made available at about 90 °C. Thus CHP has an effective Coefficient of Performance (COP) compared to a heat pump of 6. However, for a remotely operated heat pump, losses in the electrical distribution network would need to be considered, of the order of 6%. Because the losses are proportional to the square of the current, during peak periods losses are much higher than this and it is likely that widespread (i.e. citywide application of heat pumps) would cause overloading of the distribution and transmission grids unless they were substantially reinforced.\n\nIt is also possible to run a heat driven operation combined with a heat pump, where the excess electricity (as heat demand is the defining factor on utilization) is used to drive a heat pump. As heat demand increases, more electricity is generated to drive the heat pump, with the waste heat also heating the heating fluid.\n\nMost industrial countries generate the majority of their electrical power needs in large centralized facilities with capacity for large electrical power output. These plants benefit from economy of scale, but may need to transmit electricity across long distances causing transmission losses. Cogeneration or trigeneration production is subject to limitations in the local demand, and thus may sometimes need to reduce e.g. heat or cooling production to match the demand. An example of cogeneration with trigeneration applications in a major city is the New York City steam system.\n\nEvery heat engine is subject to the theoretical efficiency limits of the Carnot cycle or subset Rankine cycle in the case of steam turbine power plants or Brayton cycle in gas turbine with steam turbine plants. Most of the efficiency loss with steam power generation is associated with the latent heat of vaporization of steam that is not recovered when a turbine exhausts its low temperature and pressure steam to a condenser. (Typical steam to condenser would be at a few millimeters absolute pressure and on the order of 5 °C/11 °F hotter than the cooling water temperature, depending on the condenser capacity.) In cogeneration this steam exits the turbine at a higher temperature where it may be used for process heat, building heat or cooling with an absorption chiller. The majority of this heat is from the latent heat of vaporization when the steam condenses.\n\nThermal efficiency in a cogeneration system is defined as:\n\nWhere:\n\nHeat output may be used also for cooling (for example in Summer), thanks to an absorption chiller.\nIf cooling is achieved in the same time, Thermal efficiency in a trigeneration system is defined as:\n\nWhere:\n\nTypical cogeneration models have losses as in any system. The energy distribution below is represented as a percent of total input energy:\n\nConventional central coal- or nuclear-powered power stations convert about 33-45% of their input heat to electricity. Brayton cycle power plants operate at up to 60% efficiency. In the case of conventional power plants approximately 10-15% of this heat is lost up the stack of the boiler, most of the remaining heat emerges from the turbines as low-grade waste heat with no significant local uses so it is usually rejected to the environment, typically to cooling water passing through a condenser. Because turbine exhaust is normally just above ambient temperature, some potential power generation is sacrificed in rejecting higher temperature steam from the turbine for cogeneration purposes.\n\nFor cogeneration to be practical power generation and end use of heat must be in relatively close proximity (<2 KM typically).\nEven though the efficiency of a small distributed electrical generator may be lower than a large central power plant, the use of its waste heat for local heating and cooling can result in an overall use of the primary fuel supply as great as 80%. This provides substantial financial and environmental benefits.\n\nTypically, for a gas-fired plant the fully installed cost per kW electrical is around £400/kW ($577 USD), which is comparable with large central power stations.\n\nThe EU has actively incorporated cogeneration into its energy policy via the CHP Directive. In September 2008 at a hearing of the European Parliament’s Urban Lodgment Intergroup, Energy Commissioner Andris Piebalgs is quoted as saying, “security of supply really starts with energy efficiency.” Energy efficiency and cogeneration are recognized in the opening paragraphs of the European Union’s Cogeneration Directive 2004/08/EC. This directive intends to support cogeneration and establish a method for calculating cogeneration abilities per country. The development of cogeneration has been very uneven over the years and has been dominated throughout the last decades by national circumstances.\n\nThe European Union generates 11% of its electricity using cogeneration. However, there is large difference between Member States with variations of the energy savings between 2% and 60%. Europe has the three countries with the world’s most intensive cogeneration economies: Denmark, the Netherlands and Finland. Of the 28.46 TWh of electrical power generated by conventional thermal power plants in Finland in 2012, 81.80% was cogeneration.\n\nOther European countries are also making great efforts to increase efficiency. Germany reported that at present, over 50% of the country’s total electricity demand could be provided through cogeneration. So far, Germany has set the target to double its electricity cogeneration from 12.5% of the country’s electricity to 25% of the country’s electricity by 2020 and has passed supporting legislation accordingly. The UK is also actively supporting combined heat and power. In light of UK’s goal to achieve a 60% reduction in carbon dioxide emissions by 2050, the government has set the target to source at least 15% of its government electricity use from CHP by 2010. Other UK measures to encourage CHP growth are financial incentives, grant support, a greater regulatory framework, and government leadership and partnership.\n\nAccording to the IEA 2008 modeling of cogeneration expansion for the G8 countries, the expansion of cogeneration in France, Germany, Italy and the UK alone would effectively double the existing primary fuel savings by 2030. This would increase Europe’s savings from today’s 155.69 Twh to 465 Twh in 2030. It would also result in a 16% to 29% increase in each country’s total cogenerated electricity by 2030.\n\nGovernments are being assisted in their CHP endeavors by organizations like COGEN Europe who serve as an information hub for the most recent updates within Europe’s energy policy. COGEN is Europe’s umbrella organization representing the interests of the cogeneration industry.\n\nThe European public–private partnership Fuel Cells and Hydrogen Joint Undertaking Seventh Framework Programme project ene.field deploys in 2017 up 1,000 residential fuel cell Combined Heat and Power (micro-CHP) installations in 12 states. Per 2012 the first 2 installations have taken place.\n\nIn the United Kingdom, the \"Combined Heat and Power Quality Assurance\" scheme regulates the combined production of heat and power. It was introduced in 1996. It defines, through calculation of inputs and outputs, \"Good Quality CHP\" in terms of the achievement of primary energy savings against conventional separate generation of heat and electricity. Compliance with Combined Heat and Power Quality Assurance is required for cogeneration installations to be eligible for government subsidies and tax incentives.\n\nPerhaps the first modern use of energy recycling was done by Thomas Edison. His 1882 Pearl Street Station, the world’s first commercial power plant, was a combined heat and power plant, producing both electricity and thermal energy while using waste heat to warm neighboring buildings. Recycling allowed Edison’s plant to achieve approximately 50 percent efficiency.\n\nBy the early 1900s, regulations emerged to promote rural electrification through the construction of centralized plants managed by regional utilities. These regulations not only promoted electrification throughout the countryside, but they also discouraged decentralized power generation, such as cogeneration.\n\nBy 1978, Congress recognized that efficiency at central power plants had stagnated and sought to encourage improved efficiency with the Public Utility Regulatory Policies Act (PURPA), which encouraged utilities to buy power from other energy producers.\n\nCogeneration plants proliferated, soon producing about 8% of all energy in the United States. However, the bill left implementation and enforcement up to individual states, resulting in little or nothing being done in many parts of the country.\n\nThe United States Department of Energy has an aggressive goal of having CHP constitute\n20% of generation capacity by the year 2030. Eight Clean Energy Application Centers have been established across the nation whose mission is to develop the required technology application knowledge and educational infrastructure necessary to lead \"clean energy\" (combined heat and power, waste heat recovery and district energy) technologies as viable energy options and reduce any perceived risks associated with their implementation. The focus of the Application Centers is to provide an outreach and technology deployment program for end users, policy makers, utilities, and industry stakeholders.\n\nHigh electric rates in New England and the Middle Atlantic make these areas of the United States the most beneficial for cogeneration.\n\nAny of the following conventional power plants may be converted to a combined cooling, heat and power system:\n\n"}
{"id": "29171044", "url": "https://en.wikipedia.org/wiki?curid=29171044", "title": "Collapsus", "text": "Collapsus\n\nCollapsus is a project that combines animation, interactive fiction, and documentary film. This story follows how the impending energy crisis affects ten young people, while international powers battle with political dissension and a fearful population during transition from Fossil fuel to alternative fuels. Set in the near future, Collapsus was initialized to raise awareness of the global issue of peak oil.\n\nThe project combines Video blogging, interactive maps, fictional newscasts, live action footage, and animation to immerse the player in the narrative, an example of Transmedia storytelling. The project requires the player to access and assess additional information and make decisions about the world's energy production at both a national and global scale.\n\nCollapsus was developed by Submarine Channel, with the Dutch public broadcaster VPRO, who produced the associated Energy Transition documentary the project is based on. Collapsus is directed by Tommy Pallotta, who produced A Scanner Darkly (film) and Waking Life.\n\nCollapsus blends live action and the rotoscoping animation technique, co-developed by Pallotta, and used in Waking Life and A Scanner Darkly (film).\n"}
{"id": "8850331", "url": "https://en.wikipedia.org/wiki?curid=8850331", "title": "Committee of Independent Georgia", "text": "Committee of Independent Georgia\n\nThe Committee of Independent Georgia (), also known as the Georgian Committee, was a political organization formed in 1914 by Georgian émigrés and students in Germany during World War I. It aimed at ending Imperial Russian rule in Georgia and reasserting the country’s independence under German protection. The Committee was chaired by Petre Surguladse; other members included Prince Georges Matchabelli, Mikheil Tsereteli (a notable scholar who had abandoned Kropotkinite Anarchism in favor of Georgian nationalism), Leo and Giorgi Kereselidze, and the Muslim Georgian Osman Bey (Meliton) Kartsivadze. The Committee also established branches in Austria-Hungary and the Ottoman Empire. \n\nDuring the Caucasus Campaign of 1916–7, the Committee was headquartered in Samsun, and later in Kerasunt. From there, the organization attempted to establish contacts with dissidents in Georgia. The leading independent political party, the Social Democrats (Mensheviks), maintained neutrality and did not engage in antiwar activity. Their expressed Russian orientation convinced the Tsarist authorities to allow them to operate freely in the Caucasus. Thus the Mensheviks were reluctant to collaborate with the Committee, which advocated a break with Russia and an independent Georgian state. Mikheil Tsereteli himself made a secret trip to Georgia (landing from a German submarine \"SM UB-42\"). He met with the Menshevik leader Noe Zhordania at Kutaisi, and urged him to consider a pro-German orientation. But at that time, Zhordania considered any confrontation with the Tsarist administration political suicide and Tsereteli’s mission ended unsuccessfully. Yet the Committee and its supporters, although not very numerous, represented quite a serious problem for the Russian government, especially after the Committee established a volunteer unit, the Georgian Legion, as part of the German Caucasus Expedition. Although the movement continued to be backed by Germany, its relations with the Ottoman government became extremely strained. As a result, the Georgian Legion was officially disbanded in April 1917. The members of both the Committee and the Legion returned to Georgia after the Russian Revolution of 1917 and joined the independence movement that concluded with the proclamation of the Democratic Republic of Georgia on May 26, 1918.\n\n"}
{"id": "56304633", "url": "https://en.wikipedia.org/wiki?curid=56304633", "title": "Conservation and restoration of wooden artifacts", "text": "Conservation and restoration of wooden artifacts\n\nThe conservation and restoration of wooden artifacts refers to the preservation of art and artifacts made of wood. Conservation and restoration in regards to cultural heritage is completed by a conservator-restorer. \n\nSee also Conservation and restoration of wooden furniture\n\nWood is a vascular material that comes from the trunk, roots, or stems of over 3,000 varieties of plants.. It is a cellular tissue and therefore can be understood by looking into the biological structure. .\n\nWood is porous and its growth is directional. This is due to the cellular structure of the material. Cellular structure determines factors such as grain, texture, and color. . Identification of wood types is undertaken by a conservation scientist.\nWood grain is created by the variations in number and arrangement of cells. It specifically refers to the longitudinal alignment of cells, or the direction in which the fibers are going. The six types of grain are: straight, irregular, diagonal, spiral, interlocked, and wavy. The directional of the grain directly corresponds with the strength of the wood. .\n\nTexture is dependent on the dimensions of the vessels that make up the wood. Large vessels result in course wood, while small vessels result in a fine texture. .\n\nColor is determined by the infiltrates in the cell walls of the wood. Infiltrates can be affected by light, air, and heat which cause chemical reactions within the cell wall. These chemical reactions are what give the wood its color or what lead to a change in the wood’s color. \n\nJoinery refers to any way that two separate components are put together. Joinery in regards to wood can take the form of any of the following:\n\nSee Woodworking joints\n\nThe success of joinery depends on the join chosen, the grain direction of joined parts, the amount of stresses imposed, wood movement in regards to moisture, and the surface quality of joins. If any one of these factors is compromised then the join is not as strong as it could be.\n\nWooden objects are often coated a surface protection or ornamentation. The following are common examples of surface treatments for wooden artifacts:\n\nHumidity influences the deterioration of wooden artifacts through due to too much moisture in the air, too little moisture in the air, or rapid fluctuations in moisture. Because of the composition of wood, careful control of humidity is a factor in preventive conservation. If the relative humidity is too low the wood will release some of its moisture into the air and dry out. Conversely, if the relative humidity is too high, the wood absorbs water from the air and expands. Rapid fluctuations in humidity can lead to warping, loosening of joints, and splitting. High levels of moisture in the air also create an environment that encourages biological deterioration.\n\nWeathering related to the damage of objects caused by the exposure to wind, rain, sunshine, snow, or any other natural occurrence. These natural phenomenon erode away at the surface of the wood, causing damage to surfaces and structure.\n\nChemical deterioration is caused by any agent that creates a chemical reaction within the cellular structure of the wood. The five common chemical agents that damage wood are:\n\nBiological deterioration in regards to wooden artifacts is caused by mainly by Insects and Fungi eating away at the surface of the material, which can lead to further problems if left untreated.\nInsects damage wooden artifacts by feeding on the organic material, leaving waste behind, or tunneling into the wood. Insect damage can be controlled by understanding their life cycle and needs. Because they rely on environment to regulate their body temperatures, insects will slow down activity the colder the temperature is and will not breed or develop if temperatures are under 50 degrees Fahrenheit, or 10 degrees Celsius. Aside from requiring high temperatures, insects also thrive on environments with high levels of relative humidity. Wooden artifacts can provide just the right environment for insects to feed, tunnel, breed, and reproduce, leading to a variety of damages to the wood, including, boring holes, waste material, chew marks, and exit holes.\n\nFungi cause damage to wooden artifacts by eating away at the wood and causing it to rot. Growth occurs on wooden artifacts when the environment is damp due to high levels of humidity and poor ventilation.\n\n\nPreventive conservation is a form of Collections care that acts to prevent damage to artifacts. This type of conservation can be undertaken by any person who is trained as a Collection manager, registrar, art handler, or conservator.\n\nOne of the most basic forms of preventive conservation is through environmental control, which includes the regulation of temperature, humidity, and light. Through environmental control, most forms of deterioration to wooden artifacts can be prevented relatively easily.\n\nTemperature affects wooden objects through its correlation with humidity. Generally lower temperatures reduce the amount of moisture in the air, while higher temperatures increase the amount of moisture in the air. As such, recommended temperature for storage and display of wooden artifacts is 70 degrees Fahrenheit during the winter months and 70-75 degrees Fahrenheit during the summer months.\n\nRecommended relative humidity for storage and display of wooden artifacts during the winter months is 35%-45% and 55%-65% during the summer months. Any percentage above 70 can lead to fungi or insect infestations. Careful control of relative humidity can reduce the risk of damage caused by loss or absorption of water. Equipment that can help in the regulation of humidity is air conditioning, a humidifier or dehumidifier depending on needs, or by implementing RH buffers. \n\nLight can cause cumulative damage to wooden artifacts, leading to discoloration of the surface. Long exposure to light can result in darkening of light woods and bleaching of dark woods. Infra red radiation and heat from light can also damage any finishes to the object, causing cracking of paint or varnish, brittleness, or softening of the surface.\n\nTo reduce the damage inflicted by light, it is recommended that the three following steps be taken:\n\nWith wooden artifacts exposed to open air, it is important to remove particulates from the object’s surface on a regular basis. The buildup of particulates can result in biological deterioration, encourage the growth of micro-organisms, and impair the qualities of an object's surface. Regular cleaning can reduce the long-term damage caused by the build-up of particulates. The most basic and effective method for regular cleaning of wooden artifacts is to dust off the surface with a dry cotton cloth. \n\nWooden artifacts should be kept out of direct sunlight and in a space where they will not get bumped or jostled around.\n\nTreatment for ridding wooden artifacts of insects usually involves a combination of the following techniques:\n\nSurface cleaning refers to the removal of particulates from the surface of an object. To clean the surface of wooden artifacts, conservators generally use a soft brush or a vacuum cleaner with a soft brush. If wet surface cleaning is necessary, a dilute detergent can be applied with the use of a cotton swab or soft cotton cloth.\n\nIf a wooden artifact has sustained water damage, then the object must be dried slowly so as not to cause splitting of the wood as it dries. Similar to relative humidity, a rapid fluctuation in moisture from water damage can cause further damage to wooden objects. Slow controlled drying can be achieved by lower the relative humidity and creating a tent for the artifact so that it does not lose moisture too quickly.\n\nIf a wooden object has been damaged by insects or fungi, one treatment method is to consolidate the damaged fibers using a liquid resin or solvent to strengthen the material. This method can lead to visual changes in the object and is not always reversible.\n\nConsolidation can also refer to the treatment of painted wood surfaces. Because of the complicated nature of wood as a material, consolidation of paint on a wooden artifact must be carefully undertaken. As wood expands and contracts with fluctuations in temperature and humidity, damages can occur to paint on the wood's surface.\n\nStructural repair to wooden artifacts, as with the conservation of any artifact, should be as unobtrusive as possible. One method for mending separated pieces of wooden artifacts is the use of hot or liquid hide glue. To reverse warping of wooden artifacts, conservators often treat artifacts using pressure. Treatment methods can be broken down into three categories: Direct Woodworking, Indirect Woodworking, and Moisture-Related Methods. \n"}
{"id": "37749393", "url": "https://en.wikipedia.org/wiki?curid=37749393", "title": "Copper in heat exchangers", "text": "Copper in heat exchangers\n\nHeat exchangers are devices that transfer heat in order to achieve desired heating or cooling. An important design aspect of heat exchanger technology is the selection of appropriate materials to conduct and transfer heat fast and efficiently.\n\nCopper has many desirable properties for thermally efficient and durable heat exchangers. First and foremost, copper is an excellent conductor of heat. This means that copper's high thermal conductivity allows heat to pass through it quickly. Other desirable properties of copper in heat exchangers include its corrosion resistance, biofouling resistance, maximum allowable stress and internal pressure, creep rupture strength, fatigue strength, hardness, thermal expansion, specific heat, antimicrobial properties, tensile strength, yield strength, high melting point, alloyability, ease of fabrication, and ease of joining.\n\nThe combination of these properties enable copper to be specified for heat exchangers in industrial facilities, HVAC systems, vehicular coolers and radiators, and as heat sinks to cool computers, disk drives, televisions, computer monitors, and other electronic equipment. Copper is also incorporated into the bottoms of high-quality cookware because the metal conducts heat quickly and distributes it evenly.\n\nNon-copper heat exchangers are also available. Some alternative materials include aluminium, carbon steel, stainless steel, nickel alloys, and titanium.\n\nThis article focuses on beneficial properties and common applications of copper in heat exchangers. New copper heat exchanger technologies for specific applications are also introduced.\n\nHeat exchangers using copper and its alloys have evolved along with heat transfer technologies over the past several hundred years. Copper condenser tubes were first used in 1769 for steam engines. Initially, the tubes were made of unalloyed copper. By 1870, Muntz metal, a 60% Cu-40% Zn brass alloy, was used for condensers in seawater cooling. Admiralty metal, a 70% Cu-30% Zn yellow brass alloy with 1% tin added to improve corrosion resistance, was introduced in 1890 for seawater service. By the 1920s, a 70% Cu-30% Ni alloy was developed for naval condensers. Soon afterwards, a 2% manganese and 2% iron copper alloy was introduced for better erosion resistance. A 90% Cu-10% Ni alloy first became available in the 1950s, initially for seawater piping. This alloy is now the most widely used copper-nickel alloy in marine heat exchangers.\n\nToday, steam, evaporator, and condenser coils are made from copper and copper alloys. These heat exchangers are used in air conditioning and refrigeration systems, industrial and central heating and cooling systems, radiators, hot water tanks, and under-floor heating systems.\n\nCopper-based heat exchangers can be manufactured with copper tube/aluminium fin, cupro-nickel, or all-copper constructions. Various coatings can be applied to enhance corrosion resistance of the tubes and fins.\n\nThermal conductivity (k, also denoted as λ or κ) is a measure of a material's ability to conduct heat. Heat transfer across materials of high thermal conductivity occurs at a higher rate than across materials of low thermal conductivity. In the International System of Units (SI), thermal conductivity is measured in watts per meter Kelvin (W/(m•K)). In the Imperial System of Measurement (British Imperial, or Imperial units), thermal conductivity is measured in Btu/(hr•ft⋅F).\n\nCopper has a thermal conductivity of 231 Btu/(hr-ft-F). This is higher than all other metals except silver, a precious metal. Copper has a 60% better thermal conductivity rating than aluminium and a 3,000% better rating than stainless steel.\n\nFurther information about the thermal conductivity of selected metals is available.\n\nCorrosion resistance is essential in heat transfer applications where fluids are involved, such as in hot water tanks, radiators, etc. The only affordable material that has similar corrosion resistance to copper is stainless steel. However, the thermal conductivity of stainless steel is 1/30th that of copper. Aluminium tubes are not suitable for potable or untreated water applications because it corrodes at pH<7.0 and releases hydrogen gas.\n\nProtective films can be applied to the inner surface of copper alloy tubes to increase corrosion resistance. For certain applications, the film is composed of iron. In power plant condensers, duplex tubes consisting of an inner titanium layer with outer copper-nickel alloys are employed. This enables the use of copper’s beneficial mechanical and chemical properties (e.g., stress corrosion cracking, ammonia attack) along with titanium’s excellent corrosion resistance. A duplex tube with inner aluminium brass or copper-nickel and outer stainless or mild steel can be used for cooling in the oil refining and petrochemical industries.\n\nCopper and copper-nickel alloys have a high natural resistance to biofouling relative to alternative materials. Other metals used in heat exchangers, such as steel, titanium and aluminium, foul readily. Protection against biofouling, particularly in marine structures, can be accomplished over long periods of time with copper metals.\n\nCopper-nickel alloys have been proven over many years in sea water pipework and other marine applications. These alloys resist biofouling in open seas where they do not allow microbial slime to build up and support macrofouling.\n\nResearchers attribute copper’s resistance to biofouling, even in temperate waters, to two possible mechanisms: 1) a retarding sequence of colonization through slow release of copper ions during the corrosion process, thereby inhibiting the attachment of microbial layers to marine surfaces; and/or, 2) separating layers that contain corrosive products and the larvae of macro-encrusting organisms. The latter mechanism deters the settlement of pelagic larval stages on the metal surface, rather than killing the organisms.\n\nDue to copper’s strong antimicrobial properties, copper fins can inhibit bacterial, fungal and viral growths that commonly build up in air conditioning systems. Hence, the surfaces of copper-based heat exchangers are cleaner for longer periods of time than heat exchangers made from other metals. This benefit offers a greatly expanded heat exchanger service life and contributes to improved air quality.\nHeat exchangers fabricated separately from antimicrobial copper and aluminium in a full-scale HVAC system have been evaluated for their ability to limit microbial growth under conditions of normal flow rates using single-pass outside air. Commonly used aluminium components developed stable biofilms of bacteria and fungi within four weeks of operation. During the same time period, antimicrobial copper was able to limit bacterial loads associated with the copper heat exchanger fins by 99.99% and fungal loads by 99.74%.\n\nCopper fin air conditioners have been deployed on buses in Shanghai to rapidly and completely kill bacteria, viruses and fungi that were previously thriving on non-copper fins and permitted to circulate around the systems. The decision to replace aluminium with copper followed antimicrobial tests by the Shanghai Municipal Center for Disease Control and Prevention (SCDC) from 2010 to 2012. The study found that microbial levels on copper fin surfaces were significantly lower than on aluminium, thereby helping to protect the health of bus passengers.\n\nFurther information about the benefits of antimicrobial copper in HVAC systems is available.\n\nInternally grooved copper tube of smaller diameters is more thermally efficient, materially efficient, and easier to bend and flare and otherwise work with. It is generally easier to make inner grooved tubes out of copper, a very soft metal.\n\nCopper alloys are extensively used as heat exchanger tubing in fossil and nuclear steam generating electric power plants, chemical and petrochemical plants, marine services, and desalination plants.\n\nThe largest use of copper alloy heat exchanger tubing on a per unit basis is in utility power plants. These plants contain surface condensers, heaters, and coolers, all of which contain copper tubing. The main surface condenser that accepts turbine-steam discharges uses the most amount of copper.\n\nCopper nickel is the group of alloys that are commonly specified in heat exchanger or condenser tubes in evaporators of desalination plants, process industry plants, air cooling zones of thermal power plants, high-pressure feed water heaters, and sea water piping in ships. The composition of the alloys can vary from 90% Cu–10% Ni to 70% Cu–30% Ni.\n\nCondenser and heat exchanger tubing of arsenical admiralty brass (Cu-Zn-Sn-As) once dominated the industrial facility market. Aluminium brass later rose in popularity because of its enhanced corrosion resistance. Today, aluminium-brass, 90%Cu-10%Ni, and other copper alloys are widely used in tubular heat exchangers and piping systems in seawater, brackish water and fresh water. Aluminium-brass, 90% Cu-10% Ni and 70% Cu-30% Ni alloys show good corrosion resistance in hot de-aerated seawater and in brines in multi-stage flash desalination plants.\n\nFixed tube liquid-cooled heat exchangers especially suitable for marine and harsh applications can be assembled with brass shells, copper tubes, brass baffles, and forged brass integral end hubs.\n\nCopper alloy tubes can be supplied either with a bright metallic surface (CuNiO) or with a thin, firmly attached oxide layer (aluminium brass). These finish types allow for the formation of a protective layer. The protective oxide surface is best achieved when the system is operated for several weeks with clean, oxygen containing cooling water. While the protective layer forms, supportive measures can be carried out to enhance the process, such as the addition of iron sulfate or intermittent tube cleaning. The protective film that forms on Cu-Ni alloys in aerated seawater becomes mature in about three months at 60 °F and becomes increasingly protective with time. The film is resistant to polluted waters, irregular velocities, and other harsh conditions. Further details are available.\n\nThe biofouling resistance of Cu-Ni alloys enables heat exchange units to operate for several months between mechanical cleanings. Cleanings are nevertheless needed to restore original heat transfer capabilities. Chlorine injection can extend the mechanical cleaning intervals to a year or more without detrimental effects on the Cu-Ni alloys.\n\nFurther information about copper alloy heat exchangers for industrial facilities is available.\n\nSolar water heaters can be a cost-effective way to generate hot water for homes in many regions of the world. Copper heat exchangers are important in solar thermal heating and cooling systems because of copper’s high thermal conductivity, resistance to atmospheric and water corrosion, sealing and joining by soldering, and mechanical strength. Copper is used both in receivers and in primary circuits (pipes and heat exchangers for water tanks) of solar thermal water systems.\n\nVarious types of solar collectors for residential applications are available with either direct circulation (i.e., heats water and brings it directly to the home for use) or indirect circulation (i.e., pumps a heat transfer fluid through a heat exchanger, which then heats water that flows into the home) systems. In an evacuated tube solar hot water heater with an indirect circulation system, the evacuated tubes contain a glass outer tube and metal absorber tube attached to a fin. Solar thermal energy is absorbed within the evacuated tubes and is converted into usable concentrated heat. Evacuated glass tubes have a double layer. Inside the glass tube is the copper heat pipe. It is a sealed hollow copper tube that contains a small amount of thermal transfer fluid (water or glycol mixture) which under low pressure boils at a very low temperature. The copper heat pipe transfers thermal energy from within the solar tube into a copper header. As the solution circulates through the copper header, the temperature rises.\n\nOther components in solar thermal water systems that contain copper include solar heat exchanger tanks and solar pumping stations, along with pumps and controllers.\n\nAir conditioning and heating in buildings and motor vehicles are two of the largest applications for heat exchangers. While copper tube is used in most air conditioning and refrigeration systems, typical air conditioning units currently use aluminium fins. These systems can harbor bacteria and mold and develop odors and fouling that can make them function poorly. Stringent new requirements including demands for increased operating efficiencies and the reduction or elimination of harmful emissions are enhancing copper's role in modern HVAC systems.\n\nCopper’s antimicrobial properties can enhance the performance of HVAC systems and associated indoor air quality. After extensive testing, copper became a registered material in the U.S. for protecting heating and air conditioning equipment surfaces against bacteria, mold, and mildew. Furthermore, testing funded by the U.S. Department of Defense is demonstrating that all-copper air conditioners suppress the growth of bacteria, mold and mildew that cause odors and reduce system energy efficiency. Units made with aluminium have not been demonstrating this benefit.\n\nCopper can cause a galvanic reaction in the presence of other alloys, leading to corrosion.\n\nWater heating is the second largest energy use in the home. Gas-water heat exchangers that transfer heat from gaseous fuels to water between 3 and 300 kilowatts thermal (kWth) have widespread residential and commercial use in water heating and heating boiler appliance applications.\n\nDemand is increasing for energy-efficient compact water heating systems. Tankless gas water heaters produce hot water when needed. Copper heat exchangers are the preferred material in these units because of their high thermal conductivity and ease of fabrication. To protect these units in acidic environments, durable coatings or other surface treatments are available. Acid-resistant coatings are capable of withstanding temperatures of 1000 °C.\n\nAir-source heat pumps have been used for residential and commercial heating and cooling for many years. These units rely on air-to-air heat exchange through evaporator units similar to those used for air conditioners. Finned water to air heat exchangers are most commonly used for forced air heating and cooling systems, such as with indoor and outdoor wood furnaces, boilers, and stoves. They can also be suitable for liquid cooling applications. Copper is specified in supply and return manifolds and in tube coils.\n\nGeothermal heat pump technology, variously known as \"ground source,\" \"earth-coupled,\" or \"direct exchange,\" relies on circulating a refrigerant through buried copper tubing for heat exchange. These units, which are considerably more efficient than their air-source counterparts, rely on the constancy of ground temperatures below the frost zone for heat transfer.\nThe most efficient ground source heat pumps use ACR, Type L or special-size copper tubing buried into the ground to transfer heat to or from the conditioned space. Flexible copper tube (typically 1/4-inch to 5/8-inch) can be buried in deep vertical holes, horizontally in a relatively shallow grid pattern, in a vertical fence-like arrangement in medium-depth trenches, or as custom configurations. Further information is available.\n\nCopper and aluminium are used as heat sinks and heat pipes in electronic cooling applications. A heat sink is a passive component that cools semiconductor and optoelectronic devices by dissipating heat into the surrounding air. Heat sinks have temperatures higher than their surrounding environments so that heat can be transferred into the air by convection, radiation, and conduction.\n\nAluminium is the most prominently used heat sink material because of its lower cost. Copper heat sinks are a necessity when higher levels of thermal conductivity are needed. An alternative to all-copper or all-aluminium heat sinks is the joining of aluminium fins to a copper base.\n\nCopper heat sinks are die-cast and bound together in plates. They spread heat quickly from the heat source to copper or aluminium fins and into the surrounding air.\n\nHeat pipes are used to move heat away from central processing units (CPUs) and graphics processing units (GPUs) and towards heat sinks, where thermal energy is dissipated into the environment. Copper and aluminium heat pipes are used extensively in modern computer systems where increased power requirements and associated heat emissions result in greater demands on cooling systems.\n\nA heat pipe typically consists of a sealed pipe or tube at both the hot and cold ends. Heat pipes utilize evaporative cooling to transfer thermal energy from one point to another by the evaporation and condensation of a working fluid or coolant. They are fundamentally better at heat conduction over larger distances than heat sinks because their effective thermal conductivity is several orders of magnitude greater than that of the equivalent solid conductor.\n\nWhen it is desirable to maintain junction temperatures below 125-150 °C, copper/water heat pipes are typically used. Copper/methanol heat pipes are used if the application requires heat pipe operations below 0 °C.\n\nCuproBraze is a copper-alloy heat exchanger technology developed for applications that need to withstand harsh conditions. The technology is particularly amenable for higher temperature and pressure environments required in cleaner diesel engines that are being mandated by global environmental regulations.\n\nApplications for CuproBraze include charge air coolers, radiators, oil coolers, climate control systems, and heat transfer cores. CuproBraze is particularly suited for charge air coolers and radiators in capital intensive industries where machinery must operate for long periods of time under harsh conditions without premature failures. For these reasons, CuproBraze is particularly suited for the off-road vehicle, truck, bus, industrial engine, generator, locomotive, and military equipment markets. The technology is also amenable for light trucks, SUVs and passenger cars.\n\nCuproBraze is an alternative to soldered copper/brass plate fin, soldered copper brass serpentine fin, and brazed aluminium serpentine fin. The technology enables brazed copper serpentine fins to be used in copper-brass heat exchanger designs. These are less expensive to manufacture than soldered serpentine fin designs. They are also stronger, lighter, more durable, and have tougher joints.\n\nThe benefits of smaller-diameter internally grooved copper tube for heat transfer are well documented.\nSmaller diameter coils have better rates of heat transfer than conventional sized coils and they can withstand higher pressures required by the new generation of environmentally friendlier refrigerants. Smaller diameter coils also have lower material costs because they require less refrigerant, fin, and coil materials; and they enable the design of smaller and lighter high-efficiency air conditioners and refrigerators because the evaporators and condensers coils are smaller and lighter. MicroGroove uses a grooved inner surface of the tube to increase the surface to volume ratio and increase turbulence to mix the refrigerant and homogenize temperatures across the tube.\n"}
{"id": "58988962", "url": "https://en.wikipedia.org/wiki?curid=58988962", "title": "Cosmopolitan localism", "text": "Cosmopolitan localism\n\nCosmopolitan localism is a social innovation approach to community development that seeks to link local and global communities through resilient infrastructures that bring production and consumption closer together, building on distributed systems.\n\nCosmopolitan localism fosters a global network of mutually supportive communities (neighbourhoods, villages, towns, cities and regions) who share and exchange knowledge, ideas, skills, technology, culture and (where socially and ecologically sustainable) resources. The approach seeks to foster a creative, reciprocal relationship between the local and the global. Cosmopolitan localism aims to address problems that emerge from globalization—namely, the subsuming of local cultures and economies into a homogenised and unsustainable global system—while simultaneously avoiding the pitfalls of localization, such as parochialism and isolationism.\n\nItalian social innovation scholar Ezio Manzini describes cosmopolitan localism as having the potential to generate a new sense of place. With cosmopolitan localism, places are not considered isolated entities, but nodes in short-distance and long-distance networks. The short-distance networks generate and regenerate the local socio-economic fabric and the long-distance networks connect a particular community to the rest of the world.\n\nCosmopolitan localism is topic of focus for transition designers who explore design-led societal transition toward more sustainable futures.\n"}
{"id": "25303707", "url": "https://en.wikipedia.org/wiki?curid=25303707", "title": "Double diversion", "text": "Double diversion\n\nThe double diversion is two-part theory about environmental harm that was developed by William Freudenburg and colleagues beginning in the 1990s, and focusing on \"disproportionality\" and \"distraction.\" The concept of disproportionality involves the observation that, rather than being a reflection of overall levels of economic activity, the majority of environmental destruction is actually due to a relatively small number of economic actors, which enjoy privileged access to natural resources, “diverting” those resources for the private benefit of the few. Freudenburg's original work on this concept was carried out in conjunction with his colleague from the University of Wisconsin-Madison, Peter Nowak. The reference to the \"double\" diversion reflects the argument that this first diversion is made possible in large part by the second—the diversion of attention, or distraction, often ironically relying on the widespread but empirically inaccurate belief that environmental harm is economically beneficial to the population as a whole.\n\nThe double diversion differs from previous theories about responsibility for environmental harm, which tend to focus on overall or average levels of environmental harm. The most notable example is Garret Hardin’s 1968 article \"The Tragedy of the Commons\", which focuses on the conflict between individual self-interest and what benefits the society as a whole. Hardin illustrates this concept through an example of herders sharing an open stretch of farmland, which results in each individual developing an incentive to increase his individual number of cows. Hardin argued that as a result, all of the individual herders ignore long-term benefits that result from environmental conservation, resulting in “individual rationality leading to a collective tragedy”.\n\nThis tendency to focus on overall impacts continued into the 1970s and 1980s with the development of the I = PAT equation by Barry Commoner, Paul R. Ehrlich and John Holdren. The formula states that human impact on the environmental equals the product of population, affluence, and technology, without considering that all individuals do not have equal environmental impacts. This is similarly true in the case of William Catton’s 1980 book \"Overshoot: the Ecological Basis of Revolutionary Change\", in which Catton emphasizes the argument that all of us have lifestyles that are based upon a time when our carrying capacity exceeded human load. He argues that because we have been unwilling to modify our lifestyles as carrying capacities have been exceeded, subsequent generations are destined to inherit a world of increasing conflict for finite resources Also integral to the individual-emphasis from the writing of this period is the notion of the conflict between economic protection and economic growth. The typical feeling of this period is articulated through O’Connor (1988, 1991), who sees capitalism as relying on prosperity for the legitimization of economic expansion. What occurs as a result, according to O’Connor, is an ever-increasing exploitation of both the environment and workers.\n\nAccording to the double diversion hypothesis, the emphasis on overall or average levels of individual impacts, which many 20th century authors stressed, ultimately missed the importance of the disproportionate impacts that economic outliers can have on the environment. According to Lisa Berry, “For societies where a small fraction of the population consumes many more resources or produces much more pollution than other members, the I = PAT model fails to communicate the vast differences in resource consumption that exist within groups.” traces this emphasis to the fact that most scientists are trained to focus on statistical means. However, according to the double diversion hypothesis, rather than being discarded, the extreme cases need to be the focus of the debate.\n\nWhen this privileged access is threatened, according to Freudenburg, organized producers use distraction, through privileged accounts” largely unchallenged stories and beliefs that divert attention away from their patterns of research use. These privileged accounts ultimately get repeated so often that they ultimately become “embedded” within our language and thought. If the opposition is still refuses to stay silent and challenges them on their inequitable effects on the environment they engage in a disappearing act, where they do not “respond” why their company has the right enjoy profits of a public good, but instead point to the benefits that the populace supposedly receives. As a result, when laws and policies are created, organized interests can expect to receive tangible rewards from the political system, while the remainder of the citizenry can expect to receive mainly symbolic rewards in the form of rhetoric.”\n\nThe double diversion is a sufficiently new theory for it to not have been tested extensively, but several notable studies suggest that it deserves greater attention. Although no notable counter-examples have yet been provided, their existence is entirely possible.\n\nA case study of disproportionality in agriculture is provided by Peter Nowak and colleagues, who researched the agricultural practices of several thousand farmers in Wisconsin. They discuss how the trend of subsistence farms developing into livestock ranches, where the reuse of manure is discouraged, has increased disproportionality of negative environmental practices. Nowak \"et al.\" found that a small number of farmers using non-normative practices had a significantly greater responsibility for environmental damage. This was true regarding the amount of fertilizer used, the degree of erosion from the production of croplands, and the amount of phosphorus that accumulated in watersheds. The majority of the soil test results are in the high or excessively high range for phosphorus values, with a clear sub-set of outliers that have values up to 900% above the sample mean. However, because environmental regulations were crafted for the “average” rather than the exceptional farms, the policies did little to curb the runoff from the few farms that created significantly more than “their share” of the phosphorus problem. Nowak et al. concludes that “we are suggesting that indicators of environmental degradation in this situation […] failed to decrease during the 1976–1994 period because of the increasing important of disproportionate contributions.” \nDisproportionality in agriculture has also been observed in water use, with farmers and organized farm interests in dry western states of the U.S. commonly using 80 percent of those state’s water, while only having an economic relevance of 1–3 percent of the total economy. This disproportionate effect signifies that twenty thousand times as many jobs would be created if this same amount of water was used in industry. Nevertheless, the top four percent of all farmers received 55 percent of all federal crop subsidies, with this trend of this disproportionality increasing into the future. Reluctance to challenge this status quo can be explained through diversion via the social multiplier effect: even if a given individual does not work in a given industry, that person’s attitudes may be affected by whether his or her friends and relatives do. Therefore, it makes sense that rural resident would be especially reluctant to criticize the industries or facilities they see as providing jobs for their friends and neighbors.\n\nDisproportionate effects have also been observed within the industrial setting in cases of both between and within-group comparisons. These inequalities can be measured using the Gini coefficient, (which ranges from 0 to 1, with a higher number signifying greater inequality.) uses this coefficient to establish both between and in-group comparisons of industry toxicity, finding that chemical companies had a level 0.865 in comparison to other industries. However, in-group disproportionality between individual chemical companies was even more substantial, with the 62 chemical company facilities he analyzed producing a Gini coefficient of 0.975. These results are consistent with the Kolmogorov–Smirnov test, which finds that expectations for proportionate relationships between economic activity and environmental harm can consistently be rejected at very high levels of significance.\n\nDisproportionality has also been observed by A. Kurniawan and A. Schmidt-Ott, who developed a new sensing method in order to measure emissions of individual cars. As in previous studies by Beaton \"et al.\" (1995) and Bishop & Stedman, (1996) they found that contrary to popular belief, a small group of “super polluters” is actually responsible for a disproportionate amount of total emissions. In this particular study, Kurniawan \"et al.\" found that 5 percent of the total vehicles accounted for 43 percent of emissions. From these findings, they conclude that “properly maintained vehicles of any age are relatively small contributors to exhaust pollution and that most of the pollution is cause by a small percentage of badly maintained cars.” Given these results, they suggest that unlike the current policies of most industrialized countries, the best solution is a targeted repair program that focuses on disproportionate polluters. The benefits of such a policy is demonstrated through are demonstrated by Netherlands where it has been implemented, and the contribution of super-polluters has been greatly diminished. Nonetheless, the failure of other countries to follow their lead has meant that “about half of the present particulate pollution by passenger cars would be avoidable if adequate monitoring would enable enforcement of existing laws”\n\n"}
{"id": "45318437", "url": "https://en.wikipedia.org/wiki?curid=45318437", "title": "Ecosystem decay", "text": "Ecosystem decay\n\nEcosystem decay is a term coined by Thomas Lovejoy to define the process of which species become extinct locally based on habitat fragmentation. This process is what led to the extinction of several species, including the Irish Elk. Ecosystem decay can be mainly attributed to population isolation, leading to inbreeding, leading to a decrease in the population of local species. Another factor is the absence of competition, preventing the mechanisms of natural selection to benefit the population. This leads to a lack of a skill set for the animal to adjust and adapt to a new environment. \n\nAlthough similar to forest fragmentation and island biogeography, ecosystem decay is what results in the event of forest fragmentation. \n\nEcosystem decay is a natural phenomenon that has several resulting features.\n\nThe process through which ecosystem decay occurs can be long and complicated or short and hasty. Overall, it still follows some basic guidelines. First, a piece of habitat is surrounded and thus isolated by farmland or cities.\nSecondly, pollination of the plants immediately ceases and the number of species thins out. Thirdly, through generations of inbreeding and thus higher birth mortality than birth survival rate and infertile dirt, the forest fragment will slowly decline to nothing.\n\nEcosystem decay is commonly caused by the harvesting of rain forest in appliance to certain laws or illegally for profit by humans. Certain countries such as Brazil prohibit the harvesting of Brazil nut trees and groves of this species causing forest fragmentation and thus causing ecosystem decay to occur. Cities, roads, farms and any other substantial barrier impeding and animals habitat can be a direct or an indirect cause. Naturally, fires and rising sea levels on low land can also cause habitat fragmentation and thus ecosystem decay. Although this process is much more lengthy, many species such as the Irish Elk and several species of ancient Australian Marsupials have been indirectly killed this way with contributions by Climate Change, Glaciation and Forest Fires.\n\nEleonore Setz was studying a patch of equatorial rainforest named reserve #1202 containing \"Pithecia pithecia (\"white-faced sakis), to study the effects of ecosystem decay. The 9.2 hectare (less than 25 acre) area had been isolated for five years when David Quammen noted results on the fragmentation of their habitat which resulted in them being stranded. The population of \"P. Pithecia\" was slowly declining at the time of the study and the population had declined to six.\n\n"}
{"id": "41456058", "url": "https://en.wikipedia.org/wiki?curid=41456058", "title": "Energy in Macau", "text": "Energy in Macau\n\nEnergy in Macau is related to all of the type of energy and its related infrastructure used in Macau, China. Energy-related affairs is administered under the Secretariat for Transport and Public Works of the Government of Macau.\n\nMacau import its natural gas supply from Hengqin Island in Zhuhai, Guangdong. The pipeline has a capacity of 520 million m of natural gas per year. The sole gas importer in Macau is Sinosky Energy (Holdings) Co. Ltd. ().\n\nPetroleum and other petrochemical products in Macau is handled by Nam Kwong Petroleum & Chemical Co., Ltd. (NKOIL; ) from its trading, storage, transportation, wholesale and retail, including the installation and maintenance of gas and fuel supply system.\n\nWater supply in Macau is handled by Macao Water.\n\n"}
{"id": "9939543", "url": "https://en.wikipedia.org/wiki?curid=9939543", "title": "Extended Groth Strip", "text": "Extended Groth Strip\n\nThe Extended Groth Strip is an image of a small region between the constellations of Ursa Major and Boötes, based on the results of a series of observations by the Hubble Space Telescope. It covers an area 70 arcminutes across and 10 arcminutes wide, which correlates to a patch of sky roughly the width of a finger stretched at arms length. The image was assembled from over 500 separate exposures taken with the Space Telescope's Advanced Camera for Surveys at 63 different pointings, spread out over the course of one year from June 2004 to March 2005. The complete image at the highest resolution in JPEG format is nearly 250 megabytes.\n\nThe Extended Groth Strip is named for Princeton University physicist Edward Groth. The project is jointly led by Sandra Faber, professor of physics and astronomy at the University of California at Santa Cruz, and Marc Davis, professor of astronomy at the University of California at Berkeley.\n\nThere are at least 50,000 galaxies in its view, giving new clues about the universe's youth, from its \"preteen\" years to young adulthood. The snowstorm of galaxies in the Hubble panorama does not appear evenly spread out. Some galaxies seem to be grouped together. Others are scattered through space. This uneven distribution of galaxies traces the concentration of dark matter, an invisible web-like structure stretching throughout space. Galaxies form in areas rich in dark matter.\n\nThe Extended Groth Strip is being studied in depth as part of the All-Wavelength Extended Groth Strip International Survey.\n\n\n"}
{"id": "25222240", "url": "https://en.wikipedia.org/wiki?curid=25222240", "title": "GEMIX", "text": "GEMIX\n\nThe GEMIX is a report commissioned by the Belgian Minister of Climate and Energy, Paul Magnette. The full title in French is \"Quel mix énergétique idéal pour la Belgique aux horizons 2020 et 2030?\". It was completed on 30 September 2009 and published on 9 October 2009. The report received a lot of publicity, due mainly to its recommendations on nuclear power.\n\nIt was written by Dominique Woitrin, Marie Pierre Fauconnier, Danielle Devogelaer, Jacques Percebois, Luigi De Paoli, Jacques De Ruyck and Wolfgang Eichhammer. Luc Dufresne was president of the workgroup.\n\n\n"}
{"id": "31346116", "url": "https://en.wikipedia.org/wiki?curid=31346116", "title": "Geographical cluster", "text": "Geographical cluster\n\nA geographical cluster is a localised anomaly, usually an excess of something given the distribution or variation of something else. Often it is considered as an incidence rate that is unusual in that there is more of some variable than might be expected. Examples would include: a local excess disease rate, a crime hot spot, areas of high unemployment, accident blackspots, unusually high positive residuals from a model, high concentrations of flora or fauna, physical features or events like earthquake epicenters etc... \n\nIdentifying these extreme regions may be useful in that there could be implicit geographical associations with other variables that can be identified and would be of interest. Pattern detection via the identification of such geographical clusters is a very simple and generic form of geographical analysis that has many applications in many different contexts. The emphasis is on localized clustering or patterning because this may well contain the most useful information. \n\nA geographical cluster is different from a high concentration as it is generally second order, involving the factoring in of the distribution of something else. \n\nIdentifying geographical clusters can be an important stage in a geographical analysis. Mapping the locations of unusual concentrations may help identify causes of these. Some techniques include the Geographical Analysis Machine and Besag and Newell's cluster detection method.\n"}
{"id": "4905794", "url": "https://en.wikipedia.org/wiki?curid=4905794", "title": "Geomagnetic latitude", "text": "Geomagnetic latitude\n\nGeomagnetic latitude is a parameter analogous to geographic latitude, except that, instead of being defined relative to the geographic poles, it is defined by the axis of the geomagnetic dipole, which can be accurately extracted from the International Geomagnetic Reference Model. \n\n"}
{"id": "9847653", "url": "https://en.wikipedia.org/wiki?curid=9847653", "title": "Global atmospheric electrical circuit", "text": "Global atmospheric electrical circuit\n\nThe global atmospheric electrical circuit is the course of continuous movement of atmospheric electricity between the ionosphere and the Earth. Through solar radiation, thunderstorms, and the fair-weather condition, the atmosphere is subject to a continual and substantial electrical current.\n\nPrincipally, thunderstorms throughout the world carry \"negative\" charges to the earth, which is then discharged gradually through the air in fair weather.\n\nThis atmospheric circuit is central to the study of atmospheric physics and meteorology. It is used in the prediction of thunderstorms, and was central to the understanding of electricity. In the past it has been suggested as a source of available energy, or communications platform.\n\nThe global electrical circuit is also applied to the study human health and air pollution, due of the interaction of negative ions and aerosols. The effect of global warming, and temperature-sensitivity of the Earth's electrical circuit is unknown.\n\nIn the 18th century, scientists began understanding the link between lightning and electricity. In addition to the iconic kite experiments of Benjamin Franklin and Thomas-François Dalibard, some early studies of electric charges in a \"cloudless atmosphere\" were done by John Canton, Giambatista Beccaria, and John Read.\n\nIn 1752, Louis-Guillaume Le Monnier observed electrification in fair weather. Various others performed measurements throughout the late 18th century, often finding consistent diurnal variations. During the 19th century, several long series of observations were made. Measurements near cities were heavily influenced by smoke pollution. In the early 20th century, balloon ascents provided information about the electric field in the upper atmosphere. Important work was done by the research vessel Carnegie, which produced standardised measurements around the world's oceans (where the air is relatively clean).\n\nC. T. R. Wilson was the first to present a theory of a global circuit in 1920.\n\nLightning strikes the earth 40,000 times per day, and can be thought to charge the earth like a battery. Thunderstorms generate an electrical potential difference between the earth's surface and the ionosphere, mainly by means of lightning. Because of this, the ionosphere is positively charged relative to the earth.\nConsequently, there is always a small current transporting charged particles between the ionosphere and the surface.\n\nThis current is carried by a small number of ions present in the atmosphere (generated mainly by cosmic rays in the upper atmosphere, and by radioactivity near the surface).\nDifferent locations, and meteorological conditions on the earth can have different electrical conductivity. Fair weather condition describes the state of atmospheric electricity where air carries this electrical current between the earth and the ionosphere.\n\nThe voltages involved in the Earth's circuit are significant. At sea level, the typical potential gradient in fair weather is 120 V/m. Nonetheless, since the conductivity of air is limited, the associated currents are also limited. A typical value is 1800 A over the entire planet.\nWhen it is not rainy or stormy, the amount of electricity within the atmosphere is typically between 1000 and 1800 amps. In fair weather conditions, there are about 3.5 microamps per square kilometer (9 microamps per square mile). This can produce a 200+ volt difference between the head and feet of a regular person.\n\nLocal turbulence, winds, and other fluctuations also cause small variations in the fair weather electric field, causing the fair-weather condition to be partially regional.\n\nThe Earth's electrical current varies according to a daily pattern called the Carnegie curve, believed to be caused by the regular daily variations in atmospheric electrification associated with the earth's weather regions. The pattern also shows seasonal variation, linked to the earth's solstices and equinoxes. It was named after the Carnegie Institution for Science.\n\n\n\n"}
{"id": "3025944", "url": "https://en.wikipedia.org/wiki?curid=3025944", "title": "Hettangian", "text": "Hettangian\n\nThe Hettangian is the earliest age and lowest stage of the Jurassic period of the geologic timescale. It spans the time between 201.3 ± 0.2 Ma and 199.3 ± 0.3 Ma (million years ago). The Hettangian follows the Rhaetian (part of the Triassic period) and is followed by the Sinemurian.\n\nIn European stratigraphy the Hettangian is a part of the time span in which the Lias was deposited. An example is the British Blue Lias, which has an upper Rhaetian to Sinemurian age. Another example is the lower Lias from the Northern Limestone Alps where well-preserved but very rare ammonites, including Alsatites, have been found.\n\nThe Hettangian was introduced in the literature by Swiss palaeontologist, Eugène Renevier, in 1864. The stage takes its name from Hettange-Grande, a town in north-eastern France, just south of the border with Luxembourg on the main road from Luxembourg City to Metz.\n\nThe base of the Hettangian stage (which is also the base of the Lower Jurassic series and the entire Jurassic system) is defined as the place in the stratigraphic column where fossils of the ammonite genus \"Psiloceras\" first appear. A global reference profile (a GSSP) for the base was defined 2010 at the Kuhjoch in the Karwendel in western Austria. The top of the Hettangian stage (the base of the Sinemurian) is at the first appearances of ammonite genera \"Vermiceras\" and \"Metophioceras\".\n\nThe Hettangian contains three ammonite biozones in the Tethys domain:\n\nAt the end of the Triassic period, the ammonites died out almost entirely. During the Hettangian, however, the \"Neoammonites\" developed relatively quickly, so that even in the middle Hettangian a large number of genera and species existed.\n\n\n\n"}
{"id": "357190", "url": "https://en.wikipedia.org/wiki?curid=357190", "title": "Kondo effect", "text": "Kondo effect\n\nIn physics, the Kondo effect describes the scattering of conduction electrons in a metal due to magnetic impurities, resulting in a characteristic change in electrical resistivity with temperature.\nThe effect was first described by Jun Kondo, who applied third-order perturbation theory to the problem to account for s-d electron scattering. Kondo's model predicted that the scattering rate of conduction electrons of the magnetic impurity should diverge as the temperature approaches 0 K. Extended to a lattice of \"magnetic impurities\", the Kondo effect likely explains the formation of \"heavy fermions\" and \"Kondo insulators\" in intermetallic compounds, especially those involving rare earth elements like cerium, praseodymium, and ytterbium, and actinide elements like uranium. The Kondo effect has also been observed in quantum dot systems.\n\nThe temperature dependence of the resistivity including the Kondo effect is written as:\n\nformula_1\n\nwhere ρ is the residual resistance, aT shows the contribution from the Fermi liquid properties, and the term bT is from the lattice vibrations; a, b and c are constants. Jun Kondo derived the third term of the logarithmic dependence.\n\nKondo's model was derived using perturbation theory, but later methods used non-perturbative techniques to refine his result. These improvements produced a finite resistivity but retained the feature of a resistance minimum at a non-zero temperature. One defines the Kondo temperature as the energy scale limiting the validity of the Kondo results. The Anderson impurity model and accompanying Wilsonian renormalization theory were an important contribution to understanding the underlying physics of the problem. Based on the Schrieffer-Wolff transformation, it was shown that Kondo model lies in the strong coupling regime of the Anderson impurity model. The Schrieffer-Wolff transformation projects out the high energy charge excitations in Anderson impurity model, obtaining the Kondo model as an effective Hamiltonian.\n\nThe Kondo effect can be considered as an example of asymptotic freedom, i.e. a situation where the coupling becomes non-perturbatively strong at low temperatures and low energies. In the Kondo problem, the coupling refers to the interaction between the localized magnetic impurities and the itinerant electrons.\n\nExtended to a lattice of \"magnetic impurities\", the Kondo effect likely explains the formation of \"heavy fermions\" and \"Kondo insulators\" in intermetallic compounds, especially those involving rare earth elements like cerium, praseodymium, and ytterbium, and actinide elements like uranium. In heavy fermion materials, the nonperturbative growth of the interaction leads to quasi-electrons with masses up to thousands of times the free electron mass, i.e., the electrons are dramatically slowed by the interactions. In a number of instances they actually are superconductors. More recently, it is believed that a manifestation of the Kondo effect is necessary for understanding the unusual metallic delta-phase of plutonium.\n\nMore recently the Kondo effect has been observed in quantum dot systems. In such systems, a quantum dot with at least one unpaired electron behaves as a magnetic impurity, and when the dot is coupled to a metallic conduction band, the conduction electrons can scatter off the dot. This is completely analogous to the more traditional case of a magnetic impurity in a metal.\n\nIn 2012, Beri and Cooper proposed a topological Kondo effect could be found with Majorana fermions, while it has been shown that quantum simulations with ultracold atoms may also demonstrate the effect.\n\nIn 2017, teams from the Vienna University of Technology and Rice University conducted experiments into the development of new materials and theoretical work experimenting with structures made from the metals cerium, bismuth and palladium in specific combinations, respectively. The results of these experiments were published in December 2017, and contributed to theoretical work being undertaken by Dr. Hsin-Hua Lai and his team at Rice University, who realized the potential to create an entirely new material. They \"stumbled upon\" a model, and found that \"the mass had gone from like 1,000 times the mass of an electron to zero\". This is a characteristic of a Weyl semimetal. The team dubbed this new quantum material Weyl-Kondo semimetal.\n\n"}
{"id": "26358494", "url": "https://en.wikipedia.org/wiki?curid=26358494", "title": "Kourbania", "text": "Kourbania\n\nKourbania ( (sing.), (pl.); via Turkish \"Kurban\"; from the Arabic \"qurban\" \"sacrificial victim\"; compare Hebrew \"korban\") refers to a practice of Christianized animal sacrifices in some parts of Greece. It usually involves the slaughter of lambs as \"kourbania\" offerings to certain saints. \n\nIn antiquity the sacrifice was offered for health or following an accident or illness, as a votive offering promised to the Lord by the community, or by the relatives of the victim. Writing in 1979, Stella Georgoudi stated that the custom survived in \"some villages of modern Greece\" and was \"slowly deteriorating and dying out\". \n\nA similar custom from Bulgaria known as Kurban is celebrated on St. George's day.\n\nThe practice involves the blood sacrifice (θυσία, \"thusia\") of a domestic animal to either a saint, taken as the tutelary of the village in question, or dedicated to the Holy Trinity or the Virgin. The animal is slaughtered outside the village church, during or after the Divine Liturgy, or on the eve of the feast day. The animal is sometimes led into the church before the icon of the saint, or even locked in the church during the night preceding the sacrifice. Most of the \"kourbania\" are spread between April and October. \n\nThe descriptions (for both the Byzantine and Turkish periods) of this θυσία, or \"kurban\" (in Turkish), are numerous indeed, and are an example of one popular element which the Turks adopted from Byzantium. The most detailed description is given by the sixteenth-century Turkish slave Bartholomaeus Gourgieuiz:\n\nIn the late nineteenth century, Greek Christians of the village of Zele (Sylata) in Cappadocia sacrificed animals to Saint Charalambos especially in time of illness. Though the Greeks frequently referred to these sacrifices by the Turkish term \"Kurban\", the sacrificial practices went back to Byzantine and pagan times as is evident from several factors. They frequently referred to these sacrifices by the ancient Greek terms θυσία and θάλι. The question of Christian borrowing from the Muslim \"Kurban\" sacrifice is probably restricted to the philological aspect, for the pagan sacrifice seems to have remained very lively and widespread in Byzantine times.\n\nOne of the most spectacular examples of its existence in Byzantine Anatolia was the sacrifice of the fawn to St. Athenogenes at Pedachthoe/Heracleopolis on July 17 (July 16). On that day the young animal and its mother passed before the altar of the monastery church of St. Athenogenes while the Gospels were being read. The fawn was sacrificed, cooked, and eaten by the congregation and thus the faithful celebrated the glory of the martyred saint. The pagan usage of animal sacrifice survived also in the Byzantine practice of slaughtering and roasting animals after the celebration of ecclesiastical festivals.\n\nIn the village of Mistegna on Lesbos, the \"kourbania\" is to the Akindinoi saints on one of the Sundays following Easter. Also on Lesbos, the bull sacrifice to Saint Charalampus is set on a Sunday in May, on Mount Taurus outside the village of Saint Paraskevi. \n\nIn the village of Mega Monastiri in northeastern Thrace, the community used to buy the most robust calves and raise them specifically for the \"kourbania\". These animals designated for sacrifice were never used for farm labour. In some instances, the animal was bathed and decorated with flowers or ribbons, its horns decorated with strips of gold foil and led to sacrifice through all the streets in a joyous procession. \n\nThe village priest then performed a number of rites to complete the consecration of the victim before the killing, but unlike the practice in antiquity, the act of killing the animal is no special office and can be performed by anyone. The sacrifice is followed by a festival. The food for the festival is prepared under the supervision of the churchwarden, and is blessed by the priest before the meal begins. In Mega Monastiri, these meals were the scene of gatherings of lineages or clans, each with its own stone table in the churchyard, the place of honour on the eastern end of the table reserved for the clan eldest. \n\nThe prayers said by the priest over the victim have a long tradition of attestation, dating from at least the 8th century, establishing the animal sacrifice as long-standing within Christian tradition, over at least a millennium. \n\nThe sixteenth canon of the asked the emperor to put an end to this practice; while the commentary of Balsamon indicates that it was widespread in the twelfth century, and it has survived to the present day. The liturgical sacrifice in the Armenian church, known as \"madagh,\" is also a survival from antiquity.\n\nIn the late 18th century, a monk Nicodemus denounced the \"kourbania\" as a \"barbaric custom\" and \"vestige of ancient pagan error\", without success, as he was himself accused of heresy by the village priests. \n\nAlso in the 18th century, bishop Theophiles of Campania attacked the custom as an imitation of the \"vain Hellenes\". Greek ethnographers in the 19th century did not hesitate to identify the \"kourbani\" as a survival of pre-Christian Greek antiquity. \n\nGeorgoudi (1979) prefers a comparison with the Hebrew sacrifices korban of the Old Testament, citing early medieval canons and conciliaries which denounce customs such as cooking meat in the sanctuary as Jewish and Armenian Christian, not Greek, practice.\n\n\n\n"}
{"id": "3665572", "url": "https://en.wikipedia.org/wiki?curid=3665572", "title": "List of Atlantic hurricane records", "text": "List of Atlantic hurricane records\n\nSince the reliable record keeping of tropical cyclone data within the North Atlantic Ocean began in 1851, there have been 1,574 systems of at least tropical storm intensity and 912 of at least hurricane intensity. Though a majority of these tropical depressions have fallen within climatological averages, prevailing atmospheric conditions occasionally lead to anomalous tropical systems which at times reach extremes in statistical record-keeping including in duration and intensity. The scope of this list is limited to tropical cyclone records solely within the Atlantic Ocean and is subdivided by their reason for notability.\n\nClimatologically speaking, approximately 97 percent of tropical cyclones that form in the North Atlantic develop between the dates of June 1 and November 30 – dates which delimit the modern-day Atlantic hurricane season. Though the beginning of the annual hurricane season has historically remained the same, the official end of the hurricane season has shifted from its initial date of October 31. Regardless, on average once every few years a tropical cyclone develops outside the limits of the season; as of June 2018 there have been 89 tropical cyclones in the off-season, with the most recent being Tropical Storm Alberto in 2018. The first tropical cyclone of the 1938 Atlantic hurricane season, which formed on January 3, became the earliest forming tropical storm and hurricane after reanalysis concluded on the storm in December 2012. Hurricane Able in 1951 was initially thought to be the earliest forming major hurricane – a tropical cyclone with winds exceeding  – however following post-storm analysis it was determined that Able only reached Category 1 strength which made Hurricane Alma of 1966 the new record holder; as it became a major hurricane on June 8. Though it developed within the bounds of the Atlantic hurricane season, Hurricane Audrey in 1957 was the earliest developing Category 4 hurricane on record after it reached the intensity on June 27. However, reanalysis from 1956 to 1960 by NOAA downgraded Audrey to a Category 3, making Hurricane Dennis of 2005 the earliest Category 4 on record on July 8, 2005. The earliest-forming Category 5 hurricane, Emily, reached the highest intensity on the Saffir–Simpson hurricane wind scale on July 17, 2005.\n\nThough the official end of the Atlantic hurricane season occurs on November 30, the dates of October 31 and November 15 have also historically marked the official end date for the hurricane season. December, the only month of the year after the hurricane season, has featured the cyclogenesis of fourteen tropical cyclones. Tropical Storm Zeta in 2005 was the latest tropical cyclone to attain tropical storm intensity as it did so on December 30. However, the second Hurricane Alice in 1954 was the latest forming tropical cyclone to attain hurricane intensity. Both Alice and Zeta were the only two storms to exist in two calendar years – the former from 1954 to 1955 and the latter from 2005 to 2006. No storms have been recorded to exceed Category 1 hurricane intensity in December. In 1999, Hurricane Lenny reached Category 4 intensity on November 17 as it took an unprecedented west to east track across the Caribbean; its intensity made it the latest developing Category 4 hurricane, though this was well within the bounds of the hurricane season. Hurricane Hattie (October 27-November 1, 1961) was initially thought to have been the latest forming Category 5 hurricane ever documented, though reanalysis indicated that a devastating hurricane in 1932 reached such an intensity at a later date. Consequently, this made the hurricane the latest developing tropical cyclone to reach all four Saffir–Simpson hurricane wind scale classifications past Category 1 intensity.\n\n\nGenerally speaking, the intensity of a tropical cyclone is determined by either the storm's maximum sustained winds or lowest barometric pressure. The following table lists the most intense Atlantic hurricanes in terms of their lowest barometric pressure. In terms of wind speed, Allen from 1980 was the strongest Atlantic tropical cyclone on record, with maximum sustained winds of . For many years, it was thought that Hurricane Camille also attained this intensity, but this conclusion was changed in 2014. The original measurements of Camille are suspect since wind speed instrumentation used at the time would likely be damaged by winds of such intensity. Nonetheless, their central pressures are low enough to rank them among the strongest recorded Atlantic hurricanes.\n\nOwing to their intensity, the strongest Atlantic hurricanes have all attained Category 5 classification. Hurricane Opal, the strongest Category 4 hurricane recorded, intensified to reach a minimum pressure of 916 mbar (hPa; 27.05 inHg), a pressure typical of Category 5 hurricanes. Nonetheless, the pressure remains too high to list Opal as one of the ten strongest Atlantic tropical cyclones. Presently, Hurricane Wilma is the strongest Atlantic hurricane ever recorded, after reaching an intensity of 882 mbar (hPa; 26.05 inHg) in October 2005; this also made Wilma the strongest tropical cyclone worldwide outside of the West Pacific, where seven tropical cyclones have been recorded to intensify to lower pressures. However, this was later superseded by Hurricane Patricia in 2015 in the east Pacific, which had a pressure reading of 872 mbar. Preceding Wilma is Hurricane Gilbert, which had also held the record for most intense Atlantic hurricane for 17 years. The 1935 Labor Day hurricane, with a pressure of 892 mbar (hPa; 26.34 inHg), is the third strongest Atlantic hurricane and the strongest documented tropical cyclone prior to 1950. Since the measurements taken during Wilma and Gilbert were documented using dropsonde, this pressure remains the lowest measured over land.\n\nHurricane Rita is the fourth strongest Atlantic hurricane in terms of barometric pressure and one of three tropical cyclones from 2005 on the list, with the others being Wilma and Katrina at first and seventh, respectively. However, with a barometric pressure of 895 mbar (hPa; 26.43 inHg), Rita is the strongest tropical cyclone ever recorded in the Gulf of Mexico. In between Rita and Katrina is Hurricane Allen. Allen's pressure was measured at 899 mbar. Hurricane Camille is the sixth strongest hurricane on record. Camille is the only storm to have been moved down the list due to post-storm analysis. Camille was originally recognized as the fifth strongest hurricane on record, but was dropped to the seventh strongest in 2014, with an estimated pressure at 905 mbars, tying it with Hurricanes Mitch, and Dean. Camille then was recategorized with a new pressure of 900 mbars. Currently, Mitch and Dean share intensities for the eighth strongest Atlantic hurricane at 905 mbar (hPa; 26.73 inHg). Hurricane Maria is in tenth place for most intense Atlantic tropical cyclone, with a pressure as low as 908 mbar (hPa; 26.81 inHg). In addition, the most intense Atlantic hurricane outside of the Caribbean Sea and Gulf of Mexico is Hurricane Irma of 2017, with a pressure of 914 mbar (hPa; 27.0 inHg).\n\nMany of the strongest recorded tropical cyclones weakened prior to their eventual landfall or demise. However, six of the storms remained intense enough at landfall to be considered some of the strongest landfalling hurricanes – six of the ten hurricanes on the list constitute six of the most intense Atlantic landfalls in recorded history. The 1935 Labor Day hurricane made landfall at peak intensity, the most intense Atlantic hurricane landfall. Hurricane Camille made landfall in Waveland, Mississippi with a pressure of 900 mbar (hPa; 26.58 inHg), making it the second most intense Atlantic hurricane landfall. Though it weakened slightly before its eventual landfall on the Yucatán Peninsula, Hurricane Gilbert maintained a pressure of 900 mbar (hPa; 26.58 inHg) at landfall, making its landfall the second strongest, tied with Camille. Similarly, Hurricane Dean made landfall on the peninsula, though it did so at peak intensity and with a higher barometric pressure; its landfall marked the fourth strongest in Atlantic hurricane history. Hurricane Maria made landfall in Puerto Rico shortly after its peak intensity, with a pressure of 920 mbar (hPa; 27.17 inHg). In addition, despite being well past its peak intensity, Hurricane Katrina made landfall in Buras-Triumph, Louisiana with a pressure of 920 mbar (hPa; 27.17 inHg), thus making it the ninth-strongest landfall, tied with Maria.\n\nIntensity is measured solely by central pressure.\n\n\nFrom 1981 to 2010, there were on average 12.1 storms in the Atlantic Basin. Each hurricane season may be impacted by an El Niño or La Niña which contributes to the amount of storms in any given year, and a hurricane with a peak intensity of Category 3 or higher on the Saffir-Simpson Hurricane Scale is classified as major. Most Atlantic hurricane seasons between 1851, and 1930 included 7 or fewer recorded tropical storms or hurricanes, as well as many seasons between 1930 and 1965. The usage of satellite data was not available until the mid-1960s, which makes early storm counts less reliable. \n\nThe most active Atlantic hurricane season in recorded history took place in 2005, when a total of 28 storms were counted. The storm count includes 15 hurricanes, of which 7 strengthened to major hurricane status. \n\nThis bar chart shows the number of named storms and hurricanes per year from 1851-2017. Data is incomplete prior to the advent of satellite tracking in the mid-1960s.\n\n\nBelow are the largest hurricanes (by gale diameter) ever observed in the Atlantic basin.\n\n\n\n"}
{"id": "22434048", "url": "https://en.wikipedia.org/wiki?curid=22434048", "title": "List of Goodenia species", "text": "List of Goodenia species\n\nThis is a list of Goodenia species\n\n"}
{"id": "47493344", "url": "https://en.wikipedia.org/wiki?curid=47493344", "title": "List of Pleurotus species", "text": "List of Pleurotus species\n\nPleurotus is a genus of fungi. , Index Fungorum lists 202 species in the genus.\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "45696711", "url": "https://en.wikipedia.org/wiki?curid=45696711", "title": "List of botanical gardens in Lithuania", "text": "List of botanical gardens in Lithuania\n\nThis list of botanical gardens in Lithuania is intended to include all significant botanical gardens and arboretums in Lithuania.\n\n\n\n"}
{"id": "28161", "url": "https://en.wikipedia.org/wiki?curid=28161", "title": "List of brightest stars", "text": "List of brightest stars\n\nThis is a list of the brightest stars down to magnitude +2.50, as determined by their \"maximum\", \"total\", or \"combined\" visual magnitudes as viewed from Earth. Although several of the brightest stars are also known close binary or multiple star systems, they appear to the naked eye as single stars. The given list below combines/adds the magnitudes of bright individual components. Most of the proper names in this list are those approved by the Working Group on Star Names. Popular star names here that have not been approved by the IAU appear with a short note.\n\nApparent visual magnitudes of the brightest star can also be compared to non-stellar objects in our Solar System. Here the maximum visible magnitudes above the brightest star, Sirius (−1.46), are as follows. Excluding the Sun, the brightest objects are the Moon (−12.7), Venus (−4.89), Jupiter (−2.94), Mars (−2.91), Mercury (−2.45), and Saturn (−0.49).\n\nAny exact order of the visual brightness of stars is not perfectly defined for four reasons:\n\n\nThe source of magnitudes cited in this list is the linked Wikipedia articles—this basic list is a catalog of what Wikipedia itself documents. References can be found in the individual articles.\n\n\n"}
{"id": "9948858", "url": "https://en.wikipedia.org/wiki?curid=9948858", "title": "List of environmental ministries", "text": "List of environmental ministries\n\nAn environmental ministry is a national or subnational government agency politically responsible for the environment and/or natural resources. Various other names are commonly used to identify such agencies, such as Ministry of the Environment, Department of the Environment, Department for the Environment, Department of Environmental Protection, Department of Natural Resources, and so forth. Such agencies typically address environmental concerns such as the maintenance of environmental quality, nature preserves, the sustained use of natural resources, and prevention of pollution or contamination of the natural environment. Following is a list of environmental ministries by country:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "43710862", "url": "https://en.wikipedia.org/wiki?curid=43710862", "title": "List of forest research institutes in India", "text": "List of forest research institutes in India\n\nThis is a List of forest research institutes in India.\n\nInstitutes under India's Ministry of Environment, Forest and Climate Change\n\n\nInstitutes under the Indian Council of Forestry Research and Education\n\n\nOther research institutes under the Ministry of Environment and Forestry\n\n\n\n\n\n\n"}
{"id": "3513442", "url": "https://en.wikipedia.org/wiki?curid=3513442", "title": "List of national parks of Pakistan", "text": "List of national parks of Pakistan\n\nPakistan has 29 protected areas known as national parks (). As of 2012, 22 of these are under supervision of respective provincial governments and remaining are in private care. Only some of these are under the conservation scope of IUCN. Protection and conservation of the environment of Pakistan was included in the concurrent constitution of 1973. As a result, \"Environment Protection Ordinance\" was enacted in 1983, which was mainly regulated by the Environment and Urban Affairs Division. Later, a new system of 'Modern Protected Areas' legislation began at the provincial level which assigned the protected areas with designations such as national parks, wildlife sanctuaries and game reserves. Further recommendations of the national parks of the Indomalaya ecozone were highlighted in the IUCN review of 1986. Nevertheless, the development of national parks was mainly carried out by National Conservation Strategy of 1992. Due to more awareness about their importance in conservation of biodiversity, 10 national parks have been established during the time period from 1993 to 2005.\n\nAccording to the 'Modern Protected Areas' legislation, a national park is a protected area set aside by the government for the protection and conservation of its outstanding scenery and wildlife in a natural state. It is accessible to public for research, education and recreation. In order to promote public use, construction of roads and rest houses is permitted. Use of firearms, polluting water, cleaning of land for cultivation, destruction of wildlife is banned in these areas. The oldest national park is Lal Suhanra in Bahawalpur District, established in 1972. It is also the only biosphere reserve of Pakistan. Lal Suhanra is the only national park established before the independence of the nation in August 1947. The main purpose of this area was to protect the wildlife of Cholistan Desert. Central Karakoram in Gilgit Baltistan is currently the largest national park in the country, spanning over a total approximate area of . The smallest national park is the Ayub, covering a total approximate area of .\n\n"}
{"id": "2578059", "url": "https://en.wikipedia.org/wiki?curid=2578059", "title": "List of the vascular plants of Britain and Ireland 4", "text": "List of the vascular plants of Britain and Ireland 4\n\nGo to:\n\nStatus key: \"*\" indicates an introduced species and \"e\" indicates an extinct species.\n\nGo to:\n"}
{"id": "21418164", "url": "https://en.wikipedia.org/wiki?curid=21418164", "title": "Longest rivers of the United Kingdom", "text": "Longest rivers of the United Kingdom\n\nThis is a list of the longest rivers of the United Kingdom.\n\nThere seems to be little consensus in published sources as to the lengths of rivers, nor much agreement as to what constitutes a river. Thus the River Ure and River Ouse can be counted as one river system or as two rivers. If it is counted as one, the River Aire/ River Ouse/Humber system would come fourth in the list, with a combined length of ; and the River Trent/Humber system would top the list with their combined length of . Also, the Thames tributary, the River Churn, sourced at Seven Springs, adds to the length of the Thames (from its traditional source at Thames Head). The Churn/Thames' length at is therefore greater than the Severn's length of . Thus, the combined Churn/Thames river would top the list. Sue Owen et al., in their book on rivers, generally restrict the length to the parts that bear the same name. Thus the River Nene is quoted at , but would be around more if the variously named sources were included. Many of the above lengths are considerably different from Sue Owen's list, some longer and some shorter.\n\nWhere a river ends in an estuary the conventional British approach has been to treat the river as ending at the end of the administrative zone. Thus the Severn ends at the mouth of the Bristol Avon and the Thames at the Yantlet Line. The currently accepted end of the Severn Estuary is about further, and the PLA's authority stretches now to Margate, further. Other countries have different conventions, making comparisons of limited value.\n\n"}
{"id": "16390520", "url": "https://en.wikipedia.org/wiki?curid=16390520", "title": "Low-power electronics", "text": "Low-power electronics\n\nLow-power electronics are electronics, such as notebook processors, that have been designed to use less electric power.\n\nThe earliest attempts to reduce the amount of power required by an electronic device were related to the development of the wristwatch. Electronic watches require electricity as a power source, and some mechanical movements and hybrid electronic-mechanical movements also require electricity. Usually the electricity is provided by a replaceable battery. The first use of electrical power in watches was as a substitute for the mainspring, to remove the need for winding. The first electrically powered watch, the Hamilton Electric 500, was released in 1957 by the Hamilton Watch Company of Lancaster, Pennsylvania.\n\nWatch batteries (strictly speaking cells, as a battery is composed of multiple cells) are specially designed for their purpose. They are very small and provide tiny amounts of power continuously for very long periods (several years or more). In some cases, replacing the battery requires a trip to a watch-repair shop or watch dealer. Rechargeable batteries are used in some solar-powered watches.\n\nThe first digital \"electronic\" watch, a Pulsar LED prototype in 1970. Digital LED watches were very expensive and out of reach to the common consumer until 1975, when Texas Instruments started to mass-produce LED watches inside a plastic case.\n\nMost watches with LED displays required that the user press a button to see the time displayed for a few seconds, because LEDs used so much power that they could not be kept operating continuously. Watches with LED displays were popular for a few years, but soon the LED displays were superseded by liquid crystal displays (LCDs), which used less battery power and were much more convenient in use, with the display always visible and no need to push a button before seeing the time. Only in darkness you had to press a button to light the display with a tiny light bulb, later illuminating LEDs.\n\nAs of 2013, processors specifically designed for wristwatches are the lowest-power processors manufactured today—often 4-bit, 32 kHz processors.\n\nWhen personal computers were first developed, power consumption was not an issue. Soon after though, development of portable computers started, and with it, the requirement to run a computer off a battery pack, setting off the search for a compromise between computing power and power consumption. Originally most processors ran both the core and I/O circuits at 5 volts, as in the Intel 8088 used by the first Compaq Portable. It was later reduced to 3.5, 3.3 and 2.5 volts to lower power consumption. For example, the Pentium P5 core voltage decreased from 5V in 1993, to 2.5V in 1997.\n\nWith lower voltage comes lower overall power consumption. By consuming less power, the system will be less expensive to run, but more importantly for portable or mobile systems, it will run much longer on existing battery technology. The emphasis on battery operation has driven many of the advances in lowering processor voltage because this has a significant effect on battery life. The second major benefit is that with less voltage and therefore less power consumption, there will be less heat produced. Processors that run cooler can be packed into systems more tightly and will last longer. The third major benefit is that a processor running cooler on less power can be made to run faster. Lowering the voltage has been one of the key factors in allowing the clock rate of processors to go higher and higher.\n\nThe density and speed of integrated-circuit computing elements have increased exponentially for several decades, following a trend described by Moore's Law. While it is generally accepted that this exponential improvement trend will end, it is unclear exactly how dense and fast integrated circuits will get by the time this point is reached. Working devices have been demonstrated which were fabricated with a MOSFET transistor channel length of 6.3 nanometres using conventional semiconductor materials, and devices have been built that used carbon nanotubes as MOSFET gates, giving a channel length of approximately one nanometre. The density and computing power of integrated circuits are limited primarily by power-dissipation concerns.\n\nThe overall power consumption of a new personal computer has been increasing at about 22% growth per year.\nThis increase in consumption comes even though the energy consumed by a single CMOS logic gate to change state has fallen exponentially with the Moore's law shrinking of process feature size.\nAn integrated-circuit chip contains many capacitive loads, formed both intentionally (as with gate-to-channel capacitance) and unintentionally (between conductors which are near each other but not electrically connected). Changing the state of the circuit causes a change in the voltage across these parasitic capacitances, which involves a change in the amount of stored energy. As the capacitive loads are charged and discharged through resistive devices, an amount of energy comparable to that stored in the capacitor is dissipated as heat:\n\nThe effect of heat dissipation on state change is to limit the amount of computation that may be performed within a given power budget. While device shrinkage can reduce some parasitic capacitances, the number of devices on an integrated-circuit chip has increased more than enough to compensate for reduced capacitance in each individual device. Some circuits – dynamic logic, for example – require a minimum clock rate in order to function properly, wasting \"dynamic power\" even when they do not perform useful computations. Other circuits – most prominently, the RCA 1802, but also several later chips such as the WDC 65C02, the Intel 80C85, the Freescale 68HC11 and some other CMOS chips – use \"fully static logic\" that has no minimum clock rate, but can \"stop the clock\" and hold their state indefinitely. When the clock is stopped, such circuits use no dynamic power but they still have a small, static power consumption caused by leakage current.\n\nAs circuit dimensions shrink, subthreshold leakage current becomes more prominent. This leakage current results in power consumption, even when no switching is taking place (static power consumption). In modern chips, this current generally accounts for half the power consumed by the IC.\n\nLoss from subthreshold leakage can be reduced by raising the threshold voltage and lowering the supply voltage. Both these changes slowdown the circuit significantly. To address this issue, some modern low-power circuits use dual supply voltages to improve speed on critical paths of the circuit and lower power-consumption on non-critical paths. Some circuits even use different transistors (with different threshold voltages) in different parts of the circuit, in an attempt to further reduce power consumption without significant performance loss.\n\nAnother method used to reduce power consumption is power gating: the use of sleep transistors to disable entire blocks when not in use. Systems which are dormant for long periods of time and \"wake up\" to perform a periodic activity are often in an isolated location monitoring an activity. These systems are generally battery- or solar-powered and hence, reducing power consumption is a key design issue for these systems. By shutting down a functional but leaky block until it is used, leakage current can be reduced significantly. For some embedded systems that only function for short periods at a time, this can dramatically reduce power consumption.\n\nTwo other approaches also exist to lower the power overhead of state changes. One is to reduce the operating voltage of the circuit, as in a dual-voltage CPU, or to reduce the voltage change involved in a state change (making a state change only, changing node voltage by a fraction of the supply voltage—low voltage differential signaling, for example). This approach is limited by thermal noise within the circuit. There is a characteristic voltage (proportional to the device temperature and to the Boltzmann constant), which the state switching voltage must exceed in order for the circuit to be resistant to noise. This is typically on the order of 50–100 mV, for devices rated to 100 degrees Celsius external temperature (about 4 \"kT\", where \"T\" is the device's internal temperature in kelvins and \"k\" is the Boltzmann constant).\n\nThe second approach is to attempt to provide charge to the capacitive loads through paths that are not primarily resistive. This is the principle behind adiabatic circuits. The charge is supplied either from a variable-voltage inductive power supply, or by other elements in a reversible-logic circuit. In both cases, the charge transfer must be primarily regulated by the non-resistive load. As a practical rule of thumb, this means the change rate of a signal must be slower than that dictated by the RC time constant of the circuit being driven. In other words, the price of reduced power consumption per unit computation is a reduced absolute speed of computation. In practice although adiabatic circuits have been built, they have been difficult to use to reduce computation power substantially in practical circuits.\n\nFinally, there are several techniques for reducing the number of state changes associated with a given computation. For clocked- logic circuits, clock gating technique is used, to avoid changing the state of functional blocks that are not required for a given operation. As a more-extreme alternative, the asynchronous logic approach implements circuits in such a way that a specific externally supplied clock is not required. While both of these techniques are used to different extents in integrated circuit design, the limit of practical applicability for each appears to have been reached.\n\nThere are a variety of techniques for reducing the amount of battery power required for a desired wireless communication goodput.\nSome wireless mesh networks use \"smart\" low power broadcasting techniques that reduce the battery power required to transmit.\n\nThis can be achieved by using power aware protocols and joint power control systems.\n\nThe weight and cost of power supply and cooling systems generally depends on the maximum possible power that could be used at some instant.\nThere are two ways to prevent a system from being permanently damaged by excessive heat.\nMost desktop computers design power and cooling systems around the worst-case CPU power dissipation at the maximum frequency, maximum workload, and worst-case environment.\nTo reduce weight and cost, many laptop computers systems choose to use a much lighter, lower-cost cooling system designed around a much lower Thermal Design Power, that is somewhat above expected maximum frequency, typical workload, and typical environment.\nTypically such systems reduce (throttle) the clock rate when the CPU die temperature gets too hot, reducing the power dissipated to a level that the cooling system can handle.\n\n\n\n"}
{"id": "32194976", "url": "https://en.wikipedia.org/wiki?curid=32194976", "title": "M85 fuel", "text": "M85 fuel\n\nM85 is a fuel blending 85% methanol and 15% petrol. M85 is a similar blend to the E85 biofuel, however M85 fuel cannot run in vehicles intended to use E85 fuel. While similar, M85 isn't as well known as its ethanol counterpart. Methanol is considered a viable fuel alternative as it contains high levels of hydrogen.\n"}
{"id": "20573621", "url": "https://en.wikipedia.org/wiki?curid=20573621", "title": "Mars ocean hypothesis", "text": "Mars ocean hypothesis\n\nThe Mars ocean hypothesis states that nearly a third of the surface of Mars was covered by an ocean of liquid water early in the planet’s geologic history. This primordial ocean, dubbed Paleo-Ocean and Oceanus Borealis, would have filled the basin Vastitas Borealis in the northern hemisphere, a region which lies 4–5 km (2.5–3 miles) below the mean planetary elevation, at a time period of approximately 4.1–3.8 billion years ago. Evidence for this ocean includes geographic features resembling ancient shorelines, and the chemical properties of the Martian soil and atmosphere. Early Mars would have required a denser atmosphere and warmer climate to allow liquid water to remain at the surface.\n\nFeatures shown by the Viking orbiters in 1976, revealed two possible ancient shorelines near the pole, Arabia and Deuteronilus, each thousands of kilometers long. Several physical features in the present geography of Mars suggest the past existence of a primordial ocean. Networks of gullies that merge into larger channels imply erosion by a liquid agent, and resemble ancient riverbeds on Earth. Enormous channels, 25 km wide and several hundred meters deep, appear to direct flow from underground aquifers in the Southern uplands into the Northern lowlands. Much of the northern hemisphere of Mars is located at a significantly lower elevation than the rest of the planet (the Martian dichotomy), and is unusually flat.\n\nThese observations led a number of researchers to look for remnants of more ancient coastlines and further raised the possibility that such an ocean once existed. In 1987, published the hypothesis of a primordial Mars ocean he dubbed Paleo-Ocean. The ocean hypothesis is important because the existence of large bodies of liquid water in the past would have had a significant impact on ancient Martian climate, habitability potential and implications for the search for evidence of past life on Mars.\n\nBeginning in 1998, scientists Michael Malin and Kenneth Edgett set out to investigate with higher-resolution cameras on board the Mars Global Surveyor with a resolution five to ten times better than those of the Viking spacecraft, in places that would test shorelines proposed by others in the scientific literature. Their analyses were inconclusive at best, and reported that the shoreline varies in elevation by several kilometers, rising and falling from one peak to the next for thousands of kilometers. These trends cast doubt on whether the features truly mark a long-gone sea coast and, have been taken as an argument against the Martian shoreline (and ocean) hypothesis.\n\nThe Mars Orbiter Laser Altimeter (MOLA), which accurately determined in 1999 the altitude of all parts of Mars, found that the watershed for an ocean on Mars would cover three-quarters of the planet. The unique distribution of crater types below 2400 m elevation in the Vastitas Borealis was studied in 2005. The researchers suggest that erosion involved significant amounts of sublimation, and an ancient ocean at that location would have encompassed a volume of 6 x 10 km.\n\nIn 2007, Taylor Perron and Michael Manga proposed a geophysical model that, after adjustment for true polar wander caused by mass redistributions from volcanism, the Martian paleo-shorelines first proposed in 1987 by John E. Brandenburg, meet this criterion. The model indicates that these undulating Martian shorelines can be explained by the movement of Mars's spin axis. Because spinning objects bulge at their equator, the polar wander could have caused the shoreline elevation to shift in a similar way as observed. Their model does not attempt to explain what caused Mars's rotation axis to move relative to the crust.\n\nResearch published in 2009 shows a much higher density of stream channels than formerly believed. Regions on Mars with the most valleys are comparable to what is found on the Earth. In the research, the team developed a computer program to identify valleys by searching for U-shaped structures in topographical data. The large amount of valley networks strongly supports rain on the planet in the past. The global pattern of the Martian valleys could be explained with a big northern ocean. A large ocean in the northern hemisphere would explain why there is a southern limit to valley networks; the southernmost regions of Mars, farthest from the water reservoir, would get little rainfall and would develop no valleys. In a similar fashion the lack of rainfall would explain why Martian valleys become shallower from north to south.\n\nA 2010 study of deltas on Mars revealed that seventeen of them are found at the altitude of a proposed shoreline for a Martian ocean. This is what would be expected if the deltas were all next to a large body of water. Research presented at a Planetary Conference in Texas suggested that the Hypanis Valles fan complex is a delta with multiple channels and lobes, which formed at the margin of a large, standing body of water. That body of water was a northern ocean. This delta is at the dichotomy boundary between the northern lowlands and southern highlands near Chryse Planitia.\n\nResearch published in 2012 using data from MARSIS, a radar on board the Mars Express orbiter, supports the hypothesis of an extinct large, northern ocean. The instrument revealed a dielectric constant of the surface that is similar to those of low-density sedimentary deposits, massive deposits of ground-ice, or a combination of the two. The measurements were not like those of a lava-rich surface.\n\nIn March 2015, scientists stated that evidence exists for an ancient volume of water that could comprise an ocean, likely in the planet's northern hemisphere and about the size of Earth's Arctic Ocean. This finding was derived from the ratio of water and deuterium in the modern Martian atmosphere compared to the ratio found on Earth and derived from telescopic observations. Eight times as much deuterium was inferred at the polar deposits of Mars than exists on Earth (VSMOW), suggesting that ancient Mars had significantly higher levels of water. The representative atmospheric value obtained from the maps (7 VSMOW) is not affected by climatological effects as those measured by localized rovers, although the telescopic measurements are within range to the enrichment measured by the \"Curiosity\" rover in Gale Crater of 5–7 VSMOW. Even back in 2001, a study of the ratio of molecular hydrogen to deuterium in the upper atmosphere of Mars by the NASA Far Ultraviolet Spectroscopic Explorer spacecraft suggested an abundant water supply on primordial Mars.\nFurther evidence that Mars once had a thicker atmosphere which would make an ocean more probable came from the MAVEN spacecraft that has been making measurements from Mars orbit. Bruce Jakosky, lead author of a paper published in Science, stated that \"We've determined that most of the gas ever present in the Mars atmosphere has been lost to space.\" This research was based upon two different isotopes of argon gas.\n\nFor how long this body of water was in the liquid form is still unknown, considering the high greenhouse efficiency required to bring water to the liquid phase in Mars at a heliocentric distance of 1.4–1.7 AU. It is now thought that the canyons filled with water, and at the end of the Noachian Period the Martian ocean disappeared, and the surface froze for approximately 450 million years. Then, about 3.2 billion years ago, lava beneath the canyons heated the soil, melted the icy materials, and produced vast systems of subterranean rivers extending hundreds of kilometers. This water erupted onto the now-dry surface in giant floods.\n\nNew evidence for a vast northern ocean was published in May 2016. A large team of scientists described how some of the surface in Ismenius Lacus quadrangle was altered by two tsunamis. The tsunamis were caused by asteroids striking the ocean. Both were thought to have been strong enough to create 30 km diameter craters. The first tsunami picked up and carried boulders the size of cars or small houses. The backwash from the wave formed channels by rearranging the boulders. The second came in when the ocean was 300 m lower. The second carried a great deal of ice which was dropped in valleys. Calculations show that the average height of the waves would have been 50 m, but the heights would vary from 10 m to 120 m. Numerical simulations show that in this particular part of the ocean two impact craters of the size of 30 km in diameter would form every 30 million years. The implication here is that a great northern ocean may have existed for millions of years. One argument against an ocean has been the lack of shoreline features. These features may have been washed away by these tsunami events. The parts of Mars studied in this research are Chryse Planitia and northwestern Arabia Terra. These tsunamis affected some surfaces in the Ismenius Lacus quadrangle and in the Mare Acidalium quadrangle. The impact that created the crater Lomonosov has been identified as a likely source of tsunami waves.\n\nResearch reported in 2017 found that the amount of water needed to develop valley networks, outflow channels, and delta deposits of Mars was larger than the volume of a Martian ocean. The estimated volume of an ocean on Mars ranges from 3 meters to about 2 kilometers GEL (Global equivalent layer). This implies that a large amount of water was available on Mars.\n\nIn 2018, a team of scientists proposed that Martian oceans appeared very early, before or along with the growth of Tharsis. Because of this the depth of the oceans would be only half as deep as had been thought. The full weight of Tharsis would have created deep basins, but if the ocean occurred before the mass of Tharsis had formed deep basins, much less water would be needed. Also, the shorelines would not be regular since Tharsis would still be growing and consequently changing the depth of the oceans's basin. As Tharsis volcanoes erupted they added huge amounts of gases into the atmosphere that created a global warming, thereby allowing liquid water to exist.\n\nThe existence of liquid water on the surface of Mars requires both a warmer and thicker atmosphere. Atmospheric pressure on the present-day Martian surface only exceeds that of the triple point of water (6.11 hPa) in the lowest elevations; at higher elevations water can exist only as a solid or a vapor (assuming pure water). Annual mean temperatures at the surface are currently less than 210 K, significantly less than what is needed to sustain liquid water. However, early in its history Mars may have had conditions more conducive to retaining liquid water at the surface.\n\nEarly Mars had a carbon dioxide atmosphere similar in thickness to present-day Earth (1000 hPa). Despite a weak early Sun, the greenhouse effect from a thick carbon dioxide atmosphere, if bolstered with small amounts of methane or insulating effects of carbon-dioxide-ice clouds, would have been sufficient to warm the mean surface temperature to a value above the freezing point of water. The atmosphere has since been reduced by sequestration in the ground in the form of carbonates through weathering, as well as loss to space through sputtering (an interaction with the solar wind due to the lack of a strong Martian magnetosphere). A study of dust storms with the Mars Reconnaissance Orbiter suggested that 10 percent of the water loss from Mars may have been caused by dust storms. It was observed that dust storms can carry water vapor to very high altitudes. Ultraviolet light from the Sun can then break the water apart in a process called photodissociation. The hydrogen from the water molecule then escapes into space.\n\nThe obliquity (axial tilt) of Mars varies considerably on geologic timescales, and has a strong impact on planetary climate conditions.\n\nConsideration of chemistry can yield additional insight into the properties of Oceanus Borealis. With a Martian atmosphere of predominantly carbon dioxide, one might expect to find extensive evidence of carbonate minerals on the surface as remnants from oceanic sedimentation. An abundance of carbonates has yet to be detected by the Mars space missions. However, if the early oceans were acidic, carbonates would not have been able to form. The positive correlation of phosphorus, sulfur, and chlorine in the soil at two landing sites suggest mixing in a large acidic reservoir. Hematite deposits detected by TES have also been argued as evidence of past liquid water.\n\nGiven the proposal of a vast primordial ocean on Mars, the fate of the water requires explanation. As the Martian climate cooled, the surface of the ocean would have frozen. One hypothesis states that part of the ocean remains in a frozen state buried beneath a thin layer of rock, debris, and dust on the flat northern plain Vastitas Borealis. The water could have also been absorbed into the subsurface cryosphere or been lost to the atmosphere (by sublimation) and eventually to space through atmospheric sputtering.\n\nThe existence of a primordial Martian ocean remains controversial among scientists. The Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE) has discovered large boulders on the site of the ancient seabed, which should contain only fine sediment. However, the boulders could have been dropped by icebergs, a process common on Earth. The interpretations of some features as ancient shorelines has been challenged.\n\nAlternate theories for the creation of surface gullies and channels include wind erosion, liquid carbon dioxide, and liquid methane.\n\nConfirmation or refutation of the Mars ocean hypothesis awaits additional observational evidence from future Mars missions.\n\n"}
{"id": "4154507", "url": "https://en.wikipedia.org/wiki?curid=4154507", "title": "Polymer brush", "text": "Polymer brush\n\nA polymer brush is the name given to a surface coating consisting of polymers tethered to a surface. The brush may be either in a solvated state, where the tethered polymer layer consists of polymer and solvent, or in a melt state, where the tethered chains completely fill up the space available. These polymer layers can be tethered to flat substrates such as silicon wafers, or highly curved substrates such as nanoparticles. Also, polymers can be tethered in high density to another single polymer chain, although this arrangement is normally named a bottle brush. Additionally, there is a separate class of polyelectrolyte brushes, when the polymer chains themselves carry an electrostatic charge.\n\nThe brushes are often characterized by the high density of grafted chains. The limited space then leads to a strong extension of the chains. Brushes can be used to stabilize colloids, reduce friction between surfaces, and to provide lubrication in artificial joints.\n\nPolymer brushes have been modeled with Monte Carlo methods, Brownian dynamics simulations, and molecular theories. \n\nPolymer molecules within a brush are stretched away from the attachment surface as a result of the fact that they repel each other (steric repulsion or osmotic pressure). More precisely, they are more elongated near the attachment point and unstretched at the free end, as depicted on the drawing.\n\nMore precisely, within the approximation derived by Milner, Witten, Cates, the average density of all monomers in a given chain is always the same up to a prefactor:\n\nformula_1\n\nformula_2\n\nwhere formula_3 is the altitude of the end monomer and formula_4 the number of monomers per chain.\n\nThe averaged density profile formula_5 of the end monomers of all attached chains, convoluted with the above density profile for one chain, determines the density profile of the brush as a whole:\n\nformula_6\n\nA dry brush has a uniform monomer density up to some altitude formula_7. One can show that the corresponding end monomer density profile is given by:\n\nformula_8\n\nwhere formula_9 is the monomer size.\n\nThe above monomer density profile formula_10 for one single chain minimizes the total elastic energy of the brush,\n\nformula_11\n\nregardless of the end monomer density profile formula_5, as shown in.\n\nAs a consequence, the structure of any brush can be derived from the brush density profile formula_13. Indeed, the free end distribution is simply a convolution of the density profile with the free end distribution of a dry brush:\n\n<math>\\epsilon(\\rho)=\\int_\\rho^\\infty -\\frac\n"}
{"id": "1190663", "url": "https://en.wikipedia.org/wiki?curid=1190663", "title": "Reuven Ramaty High Energy Solar Spectroscopic Imager", "text": "Reuven Ramaty High Energy Solar Spectroscopic Imager\n\nReuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI, originally High Energy Solar Spectroscopic Imager or HESSI) is a NASA solar flare observatory. It is the sixth mission in the Small Explorer program, selected in October 1997 and launched on 5 February 2002. Its primary mission is to explore the physics of particle acceleration and energy release in solar flares.\n\nHESSI was renamed to RHESSI on 29 March 2002 in honor of Reuven Ramaty, a pioneer in the area of high energy solar physics. RHESSI is the first space mission named after a NASA scientist. RHESSI was built by Spectrum Astro for Goddard Space Flight Center and is operated by the Space Sciences Laboratory in Berkeley, California. The principal investigator from 2002 to 2012 was Robert Lin, who was succeeded by Säm Krucker.\n\nRHESSI is designed to image solar flares in energetic photons from soft X-rays (~3 keV) to gamma rays (up to ~20 MeV) and to provide high resolution spectroscopy up to gamma-ray energies of ~20 MeV. Furthermore, it has the capability to perform spatially resolved spectroscopy with high spectral resolution.\n\nResearchers believe that much of the energy released during a flare is used to accelerate, to very high energies, electrons (emitting primarily X-rays) and protons and other ions (emitting primarily gamma rays). The new approach of the RHESSI mission is to combine, for the first time, high-resolution imaging in hard X-rays and gamma rays with high-resolution spectroscopy, so that a detailed energy spectrum can be obtained at each point of the image.\n\nThis new approach will enable researchers to find out where these particles are accelerated and to what energies. Such information will advance understanding of the fundamental high-energy processes at the core of the solar flare problem.\n\nThe primary scientific objective of RHESSI is to understand the following processes that take place in the magnetized plasmas of the solar atmosphere during a flare:\n\nThese high-energy processes play a major role at sites throughout the universe ranging from magnetospheres to active galaxies. Consequently, the importance of understanding these processes transcends the field of solar physics; it is one of the major goals of space physics and astrophysics.\n\nThe high energy processes of interest include the following:\n\nThese processes involve:\n\nIt is impossible to duplicate these conditions in laboratories on the Earth.\n\nThe acceleration of electrons is revealed by hard X-ray and gamma-ray bremsstrahlung while the acceleration of protons and ions is revealed by gamma-ray lines and continuum. The proximity of the Sun means, not only that these high-energy emissions are orders of magnitude more intense than from any other cosmic source, but also that they can be better resolved, both spatially and temporally.\n\nSince X-rays are not easily reflected or refracted, imaging in X-rays is difficult. One solution to this problem is to selectively block the X-rays. If the X-rays are blocked in a way that depends on the direction of the incoming photons, then it may be possible to reconstruct an image. The imaging capability of RHESSI is based on a Fourier-transform technique using a set of 9 Rotational Modulation Collimators (RMCs) as opposed to mirrors and lenses. Each RMC consist of two sets of widely spaced, fine-scale linear grids. As the spacecraft rotates, these grids block and unblock any X-rays which may be coming from the Sun modulating the photon signal in time. The modulation can be measured with a detector having no spatial resolution placed behind the RMC since the spatial information is now stored in the time domain. The modulation pattern over half a rotation for a single RMC provides the amplitude and phase of many spatial Fourier components over a full range of angular orientations but for a small range of spatial source dimensions. Multiple RMCs, each with different slit widths, provide coverage over a full range of flare source sizes. Images are then reconstructed from the set of measured Fourier components in exact mathematical analogy to multi-baseline radio interferometry.\n\nRHESSI provides spatial resolution of 2 arcseconds at X-ray energies from ~4 keV to ~100 keV, 7 arcseconds to ~400 keV, and 36 arcseconds for gamma-ray lines and continuum emission above 1 MeV.\n\nRHESSI can also see gamma rays coming from off-solar directions. The more energetic gamma rays pass through the spacecraft structure, and impact the detectors from any angle. This mode is used to observe gamma-ray bursts (GRBs). The incoming gamma rays are not modulated by the grids, so positional and imaging information is not recorded. However, a crude position can still be derived by the fact that the detectors have front and rear pickups. Also, the detectors near the burst shield the ones away from the burst. Comparing signal strengths around the nine crystals, and front-to-back, then gives a coarse, two-dimensional position in space.\n\nWhen combined with high-resolution time stamps of the detector hits, the RHESSI solution can be cross-referenced on the ground with other spacecraft in the IPN (Interplanetary Network) to provide a fine solution. The large area and high sensitivities of the Ge crystal assembly make RHESSI a formidable IPN component. Even when other spacecraft can provide burst locations, few can provide as high-quality spectra of the burst (in both time and energy) as RHESSI.\n\nRarely, however, a GRB occurs near the Sun, in the collimated field of view. The grids then provide full information, and RHESSI can provide a fine GRB location even without IPN correlation.\n\nThe entire spacecraft rotates to provide the necessary signal modulation. The four, fixed solar panels are designed to provide enough gyroscopic moment to stabilize rotation about the solar vector. This largely eliminates the need for attitude control.\n\nThe instrument detectors are nine high-purity germanium crystals. Each is cooled to cryogenic temperatures by a mechanical cryocooler. Germanium provides not only detections by the photoelectric effect, but inherent spectroscopy through the charge deposition of the incoming ray. The crystals are housed in a cryostat, and mounted with low-conductivity straps.\n\nA tubular telescope structure forms the bulk of the spacecraft. Its purpose is to hold the collimators above the Ge crystals at known, fixed positions.\n\nRHESSI observations have changed our perspective on solar flares, particularly on high-energy processes in flares. RHESSI observations has led to numerous publications in scientific journals and presentations at conferences. , RHESSI is mentioned in 970 publications, books, and presentations (as listed on NASA ADS). Between February 2006 to 2008, 200 publications have been published about RHESSI observations.\n\n\n"}
{"id": "966588", "url": "https://en.wikipedia.org/wiki?curid=966588", "title": "Scientific Assessment of Ozone Depletion", "text": "Scientific Assessment of Ozone Depletion\n\nThe Scientific Assessment of Ozone Depletion is a sequence of reports sponsored by WMO/UNEP. The most recent is the 2014 report.\nThe reports were set up to inform the Montreal Protocol and amendments about ozone depletion.\n\nThe Montreal and Vienna conventions were installed long before a scientific consensus was established. Until the 1980s, EU, NASA, NAS, UNEP, WMO and the British government had all issued further different scientific reports with dissenting conclusions. Sir Robert (Bob) Watson, Director of the Science Division of at National Aeronautics and Space Administration (NASA) played a crucial role in achieving unified reporting. The IPCC started from scratch with a more unified approach.\n\n\n\n\n\n\n(The bracketed 1988, 1985 and 1981 papers are precursor reports relevant to the Montreal Protocol but not directly part of this series).\n"}
{"id": "33870440", "url": "https://en.wikipedia.org/wiki?curid=33870440", "title": "Slough (hydrology)", "text": "Slough (hydrology)\n\nA slough ( or ) is a wetland, usually a swamp or shallow lake, often a backwater to a larger body of water. Water tends to be stagnant or may flow slowly on a seasonal basis.\n\nIn North America, \"slough\" may refer to a side-channel from or feeding a river, or an inlet or natural channel only sporadically filled with water. An example of this is Finn Slough on the Fraser River, whose lower reaches have dozens of notable sloughs. Some sloughs, like Elkhorn Slough, used to be mouths of rivers, but have become stagnant because tectonic activity cut off the river's source.\n\nIn the Sacramento River, Steamboat Slough was an alternate branch of the river, a preferred shortcut route for steamboats passing between Sacramento and San Francisco. Georgiana Slough was a steamboat route through the Sacramento–San Joaquin River Delta, from the Sacramento River to the San Joaquin River and Stockton.\n\nA slough, also called a tidal channel, is a channel in a wetland. Typically, it is either stagnant or slow flowing on a seasonal basis.\n\nVegetation patterns in a slough are largely determined by depth, distribution, and duration of in the environment. Moreover, these same variables also influence the distribution, abundance, reproduction, and seasonal movements of aquatic and terrestrial life within the sloughs. Sloughs support a wide variety of plant life that is adapted to rapidly changing physical conditions such as salinity, oxygen levels and depth.\n\nIn general, sloughs are microhabitats high in species diversity. Open water sloughs are characterized by submerged and floating vegetation which includes periphyton mats dominated by sawgrass typically. The topographical and vegetation heterogeneity of ridge and slough landscape influences the productivity and diversity of birds and fish adapted to that wetland.\n\nFish that typically inhabit sloughs include tidewater goby, California killifish, mosquitofish, and topsmelt. Food habits of fish within sloughs consist of preying upon invertebrates; mostly epifaunal crustacean followed by epifaunal and infaunal worms and mollusks. Fish can feed on zooplankton and plant material. Research on prey species for fish in sloughs found that in a study done on Elkhorn Slough in California the mean prey richness for fish was greatest near the ocean and lowest inshore. This allows for a higher availability of food to enhance the function of inshore habitats and emphasizes the importance of invertebrate prey populations and how they influence plant production.\n\nBirds also inhabit sloughs. Sloughs are hotspots for bird watching. For example, the Elkhorn Slough in the western United States is one of the premier bird watching sites in the western United States. Over 340 species are seen visiting including several rare and endangered species. Species of birds seen in sloughs such as the Elkhorn slough include Acorn Woodpecker, Brown Pelican, Caspian Tern, Great BlueHeron, Great Egret, Great Horned Owl, Snowy Plover, and White-tailed Kite.\n\nSloughs are largely influenced by human development such as urban and agricultural expansion, industrial and agricultural practices, water management practices, and humans influence on species composition. Uses of identifying these aspects of human involvement can help to better predict restoration efforts to be made in managing sloughs. Examples of attributes that are affected by human stress upon the environment include periphyton, marsh plant communities, tree islands, alligators, wading birds, and marsh fishes, invertebrates, and herpetofauna.\n\nA slough can form when a meander gets cut off from the main river channel creating an oxbow lake that accumulates with fine overbank sediment and organic material such as peat. This creates a wetland or swamp environment. One end of the oxbow configuration then continues to receive flow from the main channel, creating a slough.\n\nSloughs are typically associated with the ridge formations found in their presence. Such a landscape consists of mosaic linear ridges, typically of some sort of grass such as sawgrass ridges in the Florida Everglades, that are separated by deeper water sloughs.\n\nEdges of sloughs are layers of sediment deposited by a river over time. The development of this landscape is thought to occur by the preferential formation of peat in bedrock depressions. Multiple of these deposits mounted on top of the surrounding bedrock can become elongated alongside the slough and create flow diversions within the system. Different rates of this peat accumulation could be triggered by variations in microtopography that alter plant production and vegetation type. Water flow might be the key to preventing an accumulation of organic sediment in sloughs due to the fact that accumulation leads to lowering water depths and instead allows for the growth of vegetation.\n\nOverall little quantitative data on the degradation of slough landscape exists. Slough and ridge landscape has been greatly degraded in terms of both topographic and vegetation changes over time. Topographical changes create an increase in the relief between ridge crests and slough bottoms. Vegetation changes consist of an increase in the amount of dense grass and decrease in the area of open water, creating a blurring of the directional ridge and slough pattern.\n\nHistorical everglade and slough landscape has been greatly affected and degraded by human activity. Open water sloughs support important ecological functions that have been seen to be sensitive to hydrologic and water quality problems stemmed from human activities.\n\nSloughs are ecologically important as they are a part of an endangered environment; wetlands. They act as a buffer from land to sea and act as an active part of the estuary system where freshwater flows from creeks and runoff from the land mix with salty ocean water transported by tides. Restoration is a big effort in California wetlands to restore slough and ridge landscapes. Examples of restoration projects on slough landscapes include The Elkhorn Slough Tidal Wetland Project, Dutch Slough Tidal Restoration Project, and the McDaniel Slough wetland enhancement project to name a few.\n"}
{"id": "23611170", "url": "https://en.wikipedia.org/wiki?curid=23611170", "title": "Soil loss tolerance", "text": "Soil loss tolerance\n\nSoil loss tolerance for a specific soil, also known as the T value, is the maximum average annual soil loss expressed as tons per acre per year that will permit current production levels to be maintained economically and indefinitely. T values range from 1 to 5 tons per acre per year. According to the United States Department of Agriculture's National Resource Conservation Service, in 2007 in the US, 99 million acres (28% of all cropland) were eroding above soil loss tolerance (T) rates. This was compared to 169 million acres (40% of cropland) in 1982.\n\nThe idea of soil loss tolerance was initially devised by the SCS (known presently as the NRCS). It was based on the minimum soil loss rate required to reduce organic content and harm crop productivity. In its early stages of development, soil loss tolerance rates were inconsistent because they were obtained based on rough estimates. From 1961 to 1962, several groups of soil in the United States were designated with T rates ranging from 2 to 6 tons per acre per year. The rate was subsequently adjusted to 1 to 5 tons per acre per year. The value was adapted for use in conservation management beginning in the mid-1960s.\n"}
{"id": "5056281", "url": "https://en.wikipedia.org/wiki?curid=5056281", "title": "Tau Capricorni", "text": "Tau Capricorni\n\nThe Bayer designation Tau Capricorni (τ Cap, τ Capricorni) is shared by two star systems, in the constellation Capricornus:\nThey are separated by 0.50° on the sky.\n"}
{"id": "3220999", "url": "https://en.wikipedia.org/wiki?curid=3220999", "title": "The Mole People (film)", "text": "The Mole People (film)\n\nThe Mole People is a 1956 American black-and-white science fiction film distributed by Universal International, which was produced by William Alland, directed by Virgil W. Vogel, and stars John Agar, Hugh Beaumont, and Cynthia Patrick. The story is written by László Görög. The film was released on December 1, 1956, on a double feature with their jungle adventure film \"Curucu, Beast of the Amazon\". It has also been featured on an episode on \"Mystery Science Theater 3000\".\n\nA narration by Dr. Frank Baxter, an English professor at the University of Southern California, explains the premise of the movie and its basis in reality. He briefly discusses the hollow earth theories of John Symmes and Cyrus Teed among others, and says that the movie is a fictionalized representation of these unorthodox theories.\n\nArchaeologists Dr. Roger Bentley and Dr. Jud Bellamin find a race of Sumerian albinos living deep under the Earth. They keep mutant humanoid mole men as their slaves to harvest mushrooms, which serve as their primary food source because mushrooms can grow without sunlight (although the principles of thermodynamics would in reality prevent a fungus-based diet or other diet without input from photosynthesis from being sustainable on a trans-generational basis). The Sumerian albinos' ancestors relocated into the subterranean after cataclysmic floods in ancient Mesopotamia. They believe the men are messengers of Ishtar, their goddess. Whenever their population increases, they sacrifice old people to the Eye of Ishtar. These people have lived underground for so long that they are weakened by bright light which the archaeologists brought in the form of a flashlight. However, there is one girl named Adad who has natural Caucasian skin who is disdained by the others since she has the \"mark of darkness.\"\n\nWhen one of the archaeologists is killed by a mole person, Elinu, the High Priest, realizes they are not gods. He orders their capture and takes the flashlight to control the Mole People, not knowing it is depleted. The archaeologists are then sent to the Eye of Ishtar just as the Mole People rebel. Adad goes to the Eye only to realize it is really natural light coming from the surface and that the men had survived. They then climb to the surface. Unfortunately, Adad dies after reaching the surface, when an earthquake causes a column to fall over and crush her.\n\n\nThe film currently has a 4.8/10 rating on IMDb, 1.5/5 rating on AllMovie, and an audience score of 27% on the film review aggregator website Rotten Tomatoes, with an average score of 2.6/5, based on 542 reviews.\n\nUniversal first released \"The Mole People\" on VHS on June 30, 1993. Then, in 2006, the film was then released on DVD in a boxed set called \"The Classic Sci-Fi Ultimate Collection\", which features 4 other films (\"Tarantula\", \"The Incredible Shrinking Man\", \"The Monolith Monsters\", and \"Monster on the Campus\"). Universal then re-released this film in 2015 as a stand-alone DVD as part of its \"Universal Vault Series\". There is also a Region 2 DVD release of this film.\n\nThis movie was featured on the television program \"Mystery Science Theater 3000\". The characters respond to the abrupt and unsatisfying ending by bitterly declaring \"And no one trusted a John Agar movie again.\"; the ending was changed from a typical happily-ever-after scenario because members of the studio felt that Bently's romance with Adad would promote interracial relationships.\n\nA segment of this movie was used for the 1968 movie \"The Wild World of Batwoman\", as creatures created by one of the movie's villains. This use was itself parodied by Mystery Science Theater 3000, with Crow T. Robot and Tom Servo mocking the classic slogan for Reese's Peanut Butter cups followed by Mike Nelson imitating the movie's villain, proclaiming \"That's enough of THAT film.\"\n\nThe fictionalized Mesopotamian history presented by the movie is based largely on Panbabylonism, as both Sumerian and Judaic stories describe the same events of the movie. Dr. Bentley states that the Biblical flood is an established archaeological fact, and the stranding of the Sumerians atop the mountain is a reference to the tale of Noah's Ark.\n\nSimilarly to the protagonists of the movie Ishtar descends to the underworld. There is a Panbabylonic connection between Ishtar’s descent and the Old Testament story of Joseph. The descent to the underworld is a common story of world mythologies, as is the flood myth.\n\nThe movie erroneously associates Ishtar and the Sumerians. Ishtar was the Babylonian counterpart of the Sumerian goddess Inanna. The imagery associated with Ishtar in the movie is entirely fictional: Ishtar’s symbol was an eight-pointed star representing Venus rather than the uneven chevron in the movie. Viewers might also notice that all of the gods depicted on the temple walls are Egyptian, not Sumerian.\n\nAdad is an Akkadian (male) storm-god, counterpart to the Sumerian Ishkur.\n\n\n"}
{"id": "11148869", "url": "https://en.wikipedia.org/wiki?curid=11148869", "title": "Three-mile limit", "text": "Three-mile limit\n\nThe three-mile limit refers to a traditional and now largely obsolete conception of the international law of the seas which defined a country's territorial waters, for the purposes of trade regulation and exclusivity, as extending as far as the reach of cannons fired from land. (Improvements in cannons eventually allowed them to be able to fire a shell more than three miles, but Earth's curvature made this moot. From a height of a few meters above sea level (say, atop the wall of a coastal fort), the horizon is only about away. Thus there was no need to be able to shoot farther, since more distant targets would not be visible.)\n\nIn \"Mare clausum\" (1635) John Selden endeavoured to prove that the sea was in practice virtually as capable of appropriation as terrestrial territory. As conflicting claims grew out of the controversy, maritime states came to modulate their demands and base their maritime claims on the principle that it extended seawards from land. A workable formula was found by Cornelius Bynkershoek in his \"De dominio maris\" (1702), restricting maritime dominion to the actual distance within which cannon range could effectively protect it. Most maritime nations adopted this principle, which developed into a limit of . (It has also been suggested that the three-mile limit derived, at least in some cases, from the general application of the league (a common unit of measurement at sea) rather than from the range of cannon.)\n\nSince the mid-20th century, numerous nations have claimed territorial waters well beyond the traditional three-mile limit. Commonly these maritime territories extend from a coastline, and this was eventually established as the international norm by the 1982 United Nations Convention on the Law of the Sea. As a result, the three-mile limit has become largely obsolete. , only Gibraltar, Jordan, Palau, and Singapore retain it.\n\n"}
{"id": "6855474", "url": "https://en.wikipedia.org/wiki?curid=6855474", "title": "Triveni Sangam", "text": "Triveni Sangam\n\nIn Hindu tradition Triveni Sangam is the \"confluence\" of three rivers. \"Sangam\" is the Sanskrit word for confluence. The point of confluence is a sacred place for Hindus. A bath here is said to flush away all of one's sins and free one from the cycle of rebirth.\n\nOne such Triveni Sangam, in Allahabad, has the confluence of two rivers — the Ganges and the Yamuna. The two rivers maintain their visible identity and can be identified by their different colours. The water of the Ganges is clear, while that of the Yamuna is greenish in colour.\n\nA place of religious importance and the site for historic Kumbh Mela held every 12 years, over the years it has also been the site of immersion of ashes of several national leaders, including Mahatma Gandhi in 1948.\n\nThe auspiciousness of the confluence of two rivers referred to in one of the latest sections of the Rigveda, which says,\"Those who bathe at the place where the two rivers flow together, rise up to heaven\".\n\nBhagamandala is a pilgrimage place in Kodagu district of Karnataka. It is situated on the river Kaveri in its upstream stretches. At this place, the Kaveri is joined by two tributaries, the Kannike and the mythical Sujyoti river. It is considered sacred as a river confluence (\"kudala\" or \"triveni sangama\", in Kannada and Sanskrit respectively).\n\nThe Triveni Sangam in Erode is a confluence of 3 rivers, the Cauvery, Bhavani and Amudha. Of these three, the river Amudha is invisible and is said to flow underground and join the other two rivers from below. It is also called as Kooduthurai, where the famous Sangameswarar Temple is located.\n\nTirumakudalu Narasipura, commonly known as T. Narasipura, is a panchayat town in Mysore district in the Indian state of Karnataka. The first name refers to the land at the confluence, (trimakuta in Sanskrit) at the confluence of the Kaveri, Kabini and Spatika Sarovara (a mythical lake or spring, also named Gupta Gamini). This is the place in South India where local Kumbhamela is held every three years.\n\nKaliyar (Kali river), Thodupuzhayar (Thodupuzha river) and Kothayar (Kothamangalam river) merge and becomes Moovattupuzha river in Kerala and hence this place is called Moovattupuzha.\n\nMunnar city is where Mudhirapuzha, Nallathanni and Kundaly rivers merge, the name Munnar literally means \"three rivers\" in Malayalam and Tamil.\n\nKandakurthi is a village in Renjal mandal of Nizamabad district in the Indian state of Telangana. The river Godavari merges with the rivers Manjira and Haridra.\n"}
{"id": "23250279", "url": "https://en.wikipedia.org/wiki?curid=23250279", "title": "Water spirit", "text": "Water spirit\n\nA water spirit is a kind of supernatural being found in the myth and folklore of many cultures:\n\nSome of the water spirits in traditional African religion include:\n\nIn Celtic mythology:\n\nIn Germanic mythology:\n\nIn Greek mythology:\n\nIn Japanese folklore:\n\nIn Aztec belief:\n\nIn the mythology of Oceania:\n\nIn Roman mythology:\n\nIn Slavic mythology:\n\n"}
{"id": "36936958", "url": "https://en.wikipedia.org/wiki?curid=36936958", "title": "West African Aquatic Mammals Memorandum of Understanding", "text": "West African Aquatic Mammals Memorandum of Understanding\n\nThe Memorandum of Understanding (MoU) Concerning the Conservation of the Manatee and Small Cetaceans of Western Africa and Macaronesia is a Multilateral Environmental Memorandum of Understanding and entered into effect on 3 October 2008 under the auspices of the Convention on Migratory Species of Wild Animals (CMS), also known as the Bonn Convention. The MoU covers 29 range States (Angola, Benin, Burkina Faso, Cameroon, Cape Verde, Chad, Congo, Côte d'Ivoire, Democratic Republic of Congo, Equatorial Guinea, Gabon, Gambia, Ghana, Guinea, Guinea-Bissau, Liberia, Mali, Mauritania, Morocco, Namibia, Niger, Nigeria, Portugal (Madeira and Azores), São Tomé and Príncipe, Senegal, Sierra Leone, South Africa, Spain (Canary Islands) and Togo). As of August 2012, 17 range States have signed the MoU, as well as a number of cooperating organizations.\n\nIn May 2000 a workshop on “Conservation and Management of small cetaceans of the coast of Africa” was held in Conakry, Guinea. It was during this workshop that the idea of developing an action plan for the conservation of West African small cetaceans and manatees was launched. The meeting was attended by representatives of seven range States (Benin, Equatorial Guinea, Guinea, Ivory Coast, Senegal, Gambia and Togo) alongside international experts. The Conference of the Parties to the CMS Convention adopted two resolutions (Resolution 7.7 and 8.5) and one recommendation (Recommendation 7.3) at its Seventh and Eight meeting in respectively 2002 and 2005, to support the development of a CMS instrument on these species as well as Action Plans.\n\nA first negotiation meeting, called WATCH I (Western African Talks on Cetaceans and their Habitats), held in Adeje, Tenerife, Spain in October 2007 further elaborated on a MoU. One year later, 2–3 October 2008, WATCH II meeting was held in Lomé, Togo. Here, the final negotiation and signing of the MoU took place, including the adoption of two Action Plans, which are annexes to the MoU. The MoU was signed by 15 range States and four organizations and came into effect immediately. Later on, two other range States as well as two organizations also signed the MoU, bringing the total to 23.\n\nSignatories to the West African Aquatic Mammals MoU:\n\nIn addition, the following organizations have signed the MoU:\n\nDirect and incidental catch, coastal development, pollution, over-fishing and habitat degradation are the most immediate threats for West African marine mammal population, causing a rapid decline. The MoU aims to protect these species at a national, regional and global level. Efforts to protect marine mammals and raise awareness of their conservation needs include convening of meetings, undertaking of studies and filed activities, adoption of legal instruments, as well as the development of international agreements.\n\nThe MoU protects the West African manatee \"(Trichechus senegalensis)\" and all populations of small cetaceans including the endemic Atlantic humpback dolphin \"(Sousa teuszi)\". About thirty species are thought to occur in the region of West Africa and Macaronesia.\n\nConcerned by the conservation status of the manatees and small cetacean populations frequently visiting the coastal and inland waters of the region’s Western African range States, the range States decide to work closely together to achieve and maintain a favorable conservation status for the species and their habitats. To this end they will, individually or collectively:\n\nThe MoU took effect following the signature by the seventh range State (3 October 2008) and will remain in effect indefinitely subject to the right of any Signatory to terminate its participation by providing one year’s written notice to all other Signatories.\n\nMeetings of Signatories are organized regularly to review the conservation status of manatees and small cetaceans and the implementation of the MoU and Action Plans. National reports by individual Signatories and a report prepared by the secretariat are also submitted. As of August 2012, the first Meeting of Signatories is yet to take place.\n\nThe CMS Secretariat – located in Bonn, Germany – acts as the secretariat to the MoU. The secretariat organizes the regular meetings and prepares overview report compiled on the basis of information at its disposal.\n\nThe MoU is accompanied by two Action Plans, one for manatees and one for small cetaceans.\n\nThe Acton Plan for the African manatee, the least intensively studied of the Sirenians, aims to improve legislation for manatee protection, improve applied research, reduce pressures and promote a wide appreciation of the animal and their ecological and cultural value through communication and education.\n\nThe Action Plan for small cetaceans contains eight thematic sections: National, Regional and International Collaboration and Cooperation; Legislation and Policy; Ecosystem/Habitat Protection; Threat Reduction; Research and Monitoring; Capacity Building; Education and Awareness; Tourism Based on Small Cetaceans.\n\nPossibilities are being explored for the development of sub-regional implementation plans, probably through one or more workshops and potentially in collaboration with the University of Ghana. Meanwhile, Guinea has developed Action Plans based on those in the MoU for use in its own context at national level. A database tool modeled on that for the Pacific Islands Cetaceans MoU is being developed by WDCS. Furthermore, an exploratory survey of cetaceans and their status in Cameroon was carried out in 2011, with support from the CMS Small Grants Programme. Finally, two projects are being explored, one for developing a GEF project for implementation of the MoU and one for the establishment of a Technical Advisory Group for the MoU.\n\n"}
{"id": "751403", "url": "https://en.wikipedia.org/wiki?curid=751403", "title": "Western European broadleaf forests", "text": "Western European broadleaf forests\n\nThe palaearctic Western European broadleaf forests is an ecoregion, in the Temperate broadleaf and mixed forests Biome, that covers a large area in Western Europe including: Switzerland, Austria, France, Germany and Czech Republic.\n\nThe total Western European broadleaf forests area is around . In particular the ecoregion is found in the Massif Central, Central German Uplands, Jura Mountains, Bavarian Plateau, and Bohemian Massif. It is essentially composed of lowland and alti-montane beech and mixed beech forests. It also includes small part of sub-Mediterranean regions.\n\nThis area has been inhabited for thousands of years. It hosts large cities (Lyon, Nancy, Munich), some forests, but most of the countryside is agricultural land, cultivated with cereals (corn, wheat, oats). This ecoregion hosts a good variety of animal species, birds in particular, but most large mammals are in decline.\n"}
{"id": "412593", "url": "https://en.wikipedia.org/wiki?curid=412593", "title": "Willem Schouten", "text": "Willem Schouten\n\nWillem Cornelisz Schouten ( – 1625) was a Dutch navigator for the Dutch East India Company. He was the first to sail the Cape Horn route to the Pacific Ocean.\n\nWillem Cornelisz Schouten was born in c. 1567 in Hoorn, Holland, Seventeen Provinces.\n\nIn 1615 Willem Cornelisz Schouten and his younger brother Jan Schouten sailed from Texel in the Netherlands, in an expedition led by Jacob Le Maire and sponsored by Isaac Le Maire and his \"Australische Compagnie\" in equal shares with Schouten. The expedition consisted of two ships: \"Eendracht\" and \"Hoorn\". A main purpose of the voyage was to search for \"Terra Australis\". A further objective was to explore a western route to the Pacific Ocean to evade the trade restrictions of the Dutch East India Company (VOC) in the Spice Islands. In 1616 Schouten rounded Cape Horn, which he named after the recently destroyed ship \"Hoorn\", and the Dutch city of Hoorn, after which the lost ship was named, the town in which Schouten himself was born. Schouten named the strait itself \"Le Maire Strait\". Jan Schouten died on 9 March 1616 after the expedition left Juan Fernández. He crossed the Pacific along a southern role, discovering a number of atolls in the Tuamotu Islands, including Pukapuka, Manihi, Rangiroa and Takapoto, followed by Tafahi, Niuafoʻou and Niuatoputapu in the Tonga Islands, and Alofi and Futuna in the Wallis and Futuna Islands. He then followed the north coasts of New Ireland and New Guinea and visited adjacent islands, including what became known as the Schouten Islands before reaching Ternate in September 1616.\n\nAlthough he had opened an unknown route (south of Cape Horn) for the Dutch, the VOC claimed infringement of its monopoly of trade to the Spice Islands. Schouten was arrested (and later released) and his ship confiscated in Java. On his return he would sail again for the VOC, and on one of these trips he died off the coast of Madagascar in 1625.\n\nAbel Tasman later used Shouten's charts during his exploration of the north coast of New Guinea.\n\nSchouten described his travels in the \"Journal,\" published in a Dutch edition at Amsterdam in 1618 and soon translated into several other languages.\n\nAmong historians, including A. L. Rowse, there is no consensus about the authorship of this \"Journal\". Schouten has got the credit for it, and thus the voyage has come down to us under his name. The Dutch, French, German and Latin texts all have nine engraved maps and plates which are not present in the English version, \"THE RELATION\".\n\n\n"}
