{"id": "28292088", "url": "https://en.wikipedia.org/wiki?curid=28292088", "title": "10th edition of Systema Naturae", "text": "10th edition of Systema Naturae\n\nThe 10th edition of \"Systema Naturae\" is a book written by Swedish naturalist Carolus Linnaeus and published in two volumes in 1758 and 1759, which marks the starting point of zoological nomenclature. In it, Linnaeus introduced binomial nomenclature for animals, something he had already done for plants in his 1753 publication of \"Species Plantarum\".\n\nBefore 1758, most biological catalogues had used polynomial names for the taxa included, including earlier editions of \"Systema Naturae\". The first work to consistently apply binomial nomenclature across the animal kingdom was the 10th edition of \"Systema Naturae\". The International Commission on Zoological Nomenclature therefore chose 1 January 1758 as the \"starting point\" for zoological nomenclature, and asserted that the 10th edition of \"Systema Naturae\" was to be treated as if published on that date. Names published before that date are unavailable, even if they would otherwise satisfy the rules. The only work which takes priority over the 10th edition is Carl Alexander Clerck's ' or ', which was published in 1757, but is also to be treated as if published on January 1, 1758.\n\nDuring Linnaeus' lifetime, \"Systema Naturae\" was under continuous revision. Progress was incorporated into new and ever-expanding editions; for example, in his 1st edition (1735), whales and manatees were originally classified as species of fish (as was thought to be the case then), but in the 10th edition they were moved into the mammal class.\n\nThe Animal Kingdom (as described by Linnaeus): \"Animals enjoy sensation by means of a living organization, animated by a medullary substance; perception by nerves; and motion by the exertion of the will. They have members for the different purposes of life; organs for their different senses; and faculties (or powers) for the application of their different perceptions. They all originate from an egg. Their external and internal structure; their comparative anatomy, habits, instincts, and various relations to each other, are detailed in authors who prosessedly treat on their subjects.\" \n\nThe list has been broken down into the original six classes Linnaeus described for animals; Mammalia, Aves, Amphibia, Pisces, Insecta, & Vermes. These classes were ultimately created by studying the internal anatomy, as seen in his key:\n\nBy current standards Pisces and Vermes are informal groupings, Insecta also contained arachnids and crustaceans, and one order of Amphibia comprised sharks, lampreys, and sturgeons.\n\nLinnaeus described mammals as: \"Animals that suckle their young by means of lactiferous teats. In external and internal structure they resemble man: most of them are quadrupeds; and with man, their natural enemy, inhabit the surface of the Earth. The largest, though fewest in number, inhabit the ocean.\" \n\nLinnaeus divided the mammals based upon the number, situation, and structure of their teeth, into the following orders and genera:\n\n\nLinnaeus described birds as: \"A beautiful and cheerful portion of created nature consisting of animals having a body covered with feathers and down; protracted and naked jaws (the beak), two wings formed for flight, and two feet. They are areal, vocal, swift and light, and destitute of external ears, lips, teeth, scrotum, womb, bladder, epiglottis, corpus callosum and its arch, and diaphragm.\"\n\nLinnaeus divided the birds based upon the characters of the bill and feet, into the following 6 orders and 63 genera:\n\n\nLinnaeus described his \"Amphibia\" (comprising reptiles and amphibians) as: \"Animals that are distinguished by a body cold and generally naked; stern and expressive countenance; harsh voice; mostly lurid color; filthy odor; a few are furnished with a horrid poison; all have cartilaginous bones, slow circulation, exquisite sight and hearing, large pulmonary vessels, lobate liver, oblong thick stomach, and cystic, hepatic, and pancreatic ducts: they are deficient in diaphragm, do not transpire (sweat), can live a long time without food, are tenacious of life, and have the power of reproducing parts which have been destroyed or lost; some undergo a metamorphosis; some cast (shed) their skin; some appear to live promiscuously on land or in the water, and some are torpid during the winter.\" \n\nLinnaeus divided the amphibians based upon the limb structures and the way they breathed, into the following orders and genera:\n\n\nLinnaeus described fish as: \"Always inhabiting the waters; are swift in their motion and voracious in their appetites. They breathe by means of gills, which are generally united by a bony arch; swim by means of radiate fins, and are mostly covered over with cartilaginous scales. Besides they parts they have in common with other animals, they are furnished with a nictitant membrane, and most of them with a swim-bladder, by the contraction or dilatation of which, they can raise or sink themselves in their element at pleasure. \n\nLinnaeus divided the fishes based upon the position of the ventral and pectoral fins, into the following orders and genera:\n\n\n \nLinnaeus described his \"Insecta\" (comprising all arthropods, including insects, crustaceans, arachnids and others) as: \"A very numerous and various class consisting of small animals, breathing through lateral spiracles, armed on all sides with a bony skin, or covered with hair; furnished with many feet, and moveable antennae (or horns), which project from the head, and are the probable instruments of sensation.\" \n\nLinnaeus divided the insects based upon the form of the wings, into the following orders and genera:\n\n\nLinnaeus described his \"Vermes\" as: \"Animals of slow motion, soft substance, able to increase their bulk and restore parts which have been destroyed, extremely tenacious of life, and the inhabitants of moist places. Many of them are without a distinct head, and most of them without feet. They are principally distinguished by their tentacles (or feelers). By the Ancients they were not improperly called imperfect animals, as being destitute of ears, nose, head, eyes and legs; and are therefore totally distinct from Insects.\" \n\nLinnaeus divided the \"Vermes\" based upon the structure of the body, into the following orders and genera:\n\n\nThe second volume, published in 1759, detailed the kingdom Plantae, in which Linnaeus included true plants, as well as fungi, algae and lichens. In addition to repeating the species he had previously listed in his \"Species Plantarum\" (1753), and those published in the intervening period, Linnaeus described several hundred new plant species. The species from \"Species Plantarum\" were numbered sequentially, while the new species were labelled with letters. Many were sent to Linnaeus by his correspondents overseas, including Johannes Burman and David de Gorter in South Africa, Patrick Browne, Philip Miller and John Ellis in America, Jean-François Séguier, Carlo Allioni and Casimir Christoph Schmidel in the Alps, Gorter and Johann Ernst Hebenstreit in the Orient, and François Boissier de Sauvages de Lacroix, Gerard and Barnadet Gabriel across Europe.\n\nNew plant species described in the 10th edition of \"\" include:\n"}
{"id": "17145462", "url": "https://en.wikipedia.org/wiki?curid=17145462", "title": "6N24P", "text": "6N24P\n\nThe 6N24P (Russian: 6Н24П) is a miniature Russian-made medium gain dual triode vacuum tube, intended for service as a cascode amplifier at HF through VHF frequencies. It is a direct equivalent of ECC89 and 6FC7 vacuum tubes.\n\nUf = 6.3V, If = 300 mA uM = 33 Ia = 15 mA S = 12.5 mA/V Pa = 1.8 W\n\n\n"}
{"id": "14980432", "url": "https://en.wikipedia.org/wiki?curid=14980432", "title": "Atlantic coastal desert", "text": "Atlantic coastal desert\n\nThe Atlantic coastal desert is the westernmost ecoregion in the Sahara Desert of North Africa. It occupies a narrow strip along the Atlantic coast, where the more frequent fog and haze generated offshore by the cool Canary Current provides sufficient moisture to sustain a variety of lichens, succulents, and shrubs.\n\nIt covers in Western Sahara and Mauritania. It is bounded on the west by the Atlantic Ocean, on the east by the North Saharan steppe and woodlands, on the north by the Mediterranean Acacia-Argania dry woodlands, and on the south by the Sahelian Acacia savanna. \n\nThe cool ocean current gives an even higher atmospheric stability in the desert, by cooling air at the base. This increase in atmospheric stability serves to further reduce the amount of rainfall. Therefore, the climate is extremely dry with some 30 mm of annual precipitation in Dakhla, Western Sahara and 40 mm of annual precipitation in Nouadhibou, Mauritania and several years may pass without any rainfall at all. The climate is also very sunny year-round with around 3,200 hours of annual bright sunshine, though far less sunny than in other parts of the desert of North Africa due to fog and haze. Air in the Atlantic coastal desert is rather moist and the relative humidity is generally over 60 % while farther in the interior, it quickly lowers to 30 % or less. Temperatures are also much more moderated in this coastal desert and are relatively warm to truly hot in all seasons. Averages daily temperature is 20 °C (68 °F) in Dakhla. Maximum average high temperatures are 27 °C (80.6 °F) while minimum average low temperatures are 13 °C (55.4 °F) in Dakhla.\n\nThis bioregion is fairly rich in endemic plants but has no endemic fauna. The flora consists of a variety of lichens, succulents and drought-resistant shrubs. The Mediterranean monk seal (\"Monachus monachus\") is found here and the broad bays are important over-wintering grounds for large numbers of Palearctic wading birds. Greater flamingoes (\"Phoenicopterus roseus\") and many other waterbirds congregate on the wetlands during their migrations. Mammals found here include the Dorcas gazelle (\"Gazella dorcas\"), golden jackal (\"Canis aureus\"), fennec fox (\"Fennecus zerda\"), Rüppell's fox (\"Vulpes rueppelli\"), sand cat (\"Felis margarita\"), honey badger (\"Mellivora capensis\") and striped hyena (\"Hyaena hyaena\"). \n"}
{"id": "11278410", "url": "https://en.wikipedia.org/wiki?curid=11278410", "title": "Booster (electric power)", "text": "Booster (electric power)\n\nA booster was a motor-generator (MG) set used for voltage regulation in direct current (DC) electrical power circuits. The development of alternating current and solid-state devices has rendered it obsolete. Boosters were made in various configurations to suit different applications.\n\nIn the days of direct current mains, voltage drop along the line was a problem so line boosters were used to correct it. Suppose that the mains voltage was 110 V. Houses near the power station would receive 110 volts but those remote from the power station might receive only 100 V so a line booster would be inserted at an appropriate point to \"boost\" the voltage. It consisted of a motor, connected in parallel with the mains, driving a generator, in series with the mains. The motor ran at the depleted mains voltage of 100 V and the generator added another 10 V to restore the voltage to 110 V. This was an inefficient system and was made obsolete by the development of alternating current mains, which allowed for high-voltage distribution and voltage regulation by transformers.\n\nAgain in the days of direct current mains, power stations often had large lead-acid batteries for load balancing. These supplemented the steam-powered generators during peak periods and were re-charged off-peak. Sometimes one cell in the battery would become \"sick\" (faulty, reduced capacity) and a \"milking booster\" would be used to give it an additional charge and restore it to health. The milking booster was so-called because it \"milked\" the healthy cells in the battery to give an extra charge to the faulty one. The motor side of the booster was connected across the whole battery but the generator side was connected only across the faulty cell. During discharge periods the booster supplemented the output of the faulty cell.\n\nBefore solid-state technology became available, reversible boosters were sometimes used for speed control in DC electric locomotives. To avoid confusion, it should be explained that it is the electrical output of the booster that is reversible, not the direction of rotation.\n\nThe motor of the MG set was connected in parallel with the supply, usually at 600 volts, and was mechanically coupled, via a shaft with a heavy flywheel, to the generator. The generator was connected in series with the supply and the traction motors, and its output could be varied between +600 volts, through zero, to -600 volts by adjusting switches and resistors in the field circuit. This allowed the generator voltage to either oppose, or supplement, the line voltage. The net output voltage could therefore be varied smoothly between zero and 1,200 volts as follows:\n\n\nTo match the 1,200 volt output, the locomotive would have three 400 volt traction motors connected in series. Later locomotives had two 600 volt motors in series.\n\nWhen the locomotive was working at full power, half the energy came through the MG set and the other half came directly from the supply. This meant that the power rating of the MG set needed to be only half the rating of the traction motors. Thus there was a saving in weight and cost compared to the Ward Leonard system, in which the MG set had to be equal in power rating to the traction motors.\n\nIf the power supply to the locomotive was interrupted (e.g. because of a gap in the third rail at a junction) the flywheel would power the MG set for a short period to bridge the gap. During this period, the motor of the MG set would temporarily run as a generator. It was this system that was used in the design of British Rail classes 70, 71 and 74 (Class 73 does not utilise booster equipment).\n\nSome types of London Underground stock (e.g. London Underground O Stock) were fitted with Metadynes. These were four-brush electrical machines which differed from the reversible boosters described above.\n\nWhen cathode ray tubes were the standard for television receivers, after many years of service the tube would lose brightness, due to low electron emission in the electron gun assembly of each tube. A small \"booster\" transformer could be added to a set experiencing such symptoms; it would raise the voltage applied to the filament slightly, which would increase emission and restore brightness. Sometimes this step would extend the life of the expensive CRT by years, making it more economical than a replacement. \n\n"}
{"id": "23914186", "url": "https://en.wikipedia.org/wiki?curid=23914186", "title": "Boudewijn Seapark", "text": "Boudewijn Seapark\n\nBoudewijn Seapark is a marine mammal park and theme park located in Sint-Michiels, Bruges, Belgium.\n"}
{"id": "19324513", "url": "https://en.wikipedia.org/wiki?curid=19324513", "title": "Comendite", "text": "Comendite\n\nComendite is a hard, peralkaline igneous rock, a type of light blue grey rhyolite. Phenocrysts are sodic sanidine with minor albite and bipyramidal quartz. Comendite occurs in the mountains Tibrogargan, Coonowrin, Tunbubudla, Coochin, Saddleback, Tibberoowuccum and Ngungun in the Glass House Mountains, South East Queensland, Australia. The blue colour is caused by very small crystals of riebeckite or arfvedsonite. Comendite also occurs in Sardinia, Corsica, Ascension Island, Ethiopia, Somalia and other areas of East Africa.\nThe 1903 eruption of Changbaishan volcano in north-east China erupted comendite pumice.\n\nComendite derives its name from the area of Le Commende on San Pietro Island in Italy, where the rock type is found.\n"}
{"id": "4027140", "url": "https://en.wikipedia.org/wiki?curid=4027140", "title": "Court of the Lions", "text": "Court of the Lions\n\nThe Court of the Lions (; ) is the main courtyard of the Nasrid dynasty Palace of the Lions, in the heart of the Alhambra, the Moorish citadel formed by a complex of palaces, gardens and forts in Granada, Spain. It was commissioned by the Nasrid sultan Muhammed V of the Emirate of Granada in Al-Andalus. Its construction started in the second period of his reign, between 1362 and 1391 AD. The site is now part of the UNESCO World Heritage List and minted in Spain's 2011 limited edition of 2 € Commemorative Coins.\n\nThe Palace of the Lions, as well as the rest of the other new rooms built under Muhammad V, like the Mexuar or \"Cuarto Dorado\", represented the beginning of a new style, an exuberant mixture of Moorish and Christian influences that has been called Nasrid style. During the period that Muhammad V was ousted as sultan of Granada by his stepbrother, Abu-l Walid Ismail, he discovered in exile a host of new aesthetic influences that were not in the language of his predecessors, not even in his own first contributions to the enrichment of the Nasrid palaces of the Alhambra. In Fes he saw the Almoravid mosque of Qarawiyyin, built by Andalusian architects. The splendor of the decorations, specially the profuse use of the muqarnas that had once decorated the palaces and mosques of Al-Ándalus, stunned the ex-sultan, as did the ruins of the Roman city of Volubilis, where he could directly examine the classical orders, Roman ornamentation and, above all, the disposition of the Roman impluvium; the Roman ruins at Volubilis were particularly well preserved since they had been abandoned for a period of time in the Middle Ages and later re-used as a necropolis. \n\nMuhammad became an ally of his personal friend, the Christian king Pedro I of Castile, who helped him to regain the throne and defeat the usurpers. Meanwhile, he was also astonished with the construction of the palace of Pedro I, the Alcázar of Seville, built in Mudéjar style by architects from Toledo, Seville and Granada. The influence of this Mudéjar style of King Pedro in the future Palace of the Lions was going to be decisive, especially the structure and disposition of the Qubba rooms along two axis of the 'Patio de las Doncellas' (\"Courtyard of the Maidens\").\n\nThe Courtyard of the Lions is an oblong courtyard, 35 m in length and 20 m in width, surrounded by a low gallery supported on 124 white marble columns. A pavilion projects into the courtyard at each extremity, with filigree walls and light domed roof, elaborately ornamented. The square is paved with coloured tiles, and the colonnade with white marble; while the walls are covered 1.5 m up from the ground with blue and yellow tiles, with a border above and below enamelled blue and gold. The columns supporting the roof and gallery are irregularly placed, with a view to artistic effect; and the general form of the piers, arches and pillars is most graceful. They are adorned by varieties of foliage, etc.; above each arch there is a large square of arabesques; and over the pillars is another square of filigree work. In the center of the courtyard is the celebrated Fountain of Lions, a magnificent alabaster basin supported by the figures of twelve lions in white marble. At present, the fountain is under restoration in an effort to preserve its integrity.\n\nThe structure of the courtyard, has, as it has been said, a direct influence of the Sevillian Patio de las Doncellas, but its meaning and origins trace their roots to early Islamic gardening, the courtyard divided in four parts, each one of them symbolizing one of the four parts of the world. Each part is irrigated by a water channel that symbolize the four rivers of Paradise. This courtyard is, therefore, an architectural materialization of Paradise, where the gardens, the water, and the columns form a conceptual and physical unity. The slender column forest have been said to represent the palm trees of an oasis in the desert, deeply related with Paradise in the Nasrid imagination. In the poem of Ibn Zamrak on the basin of the fountain, a further meaning is stated clearly: \"The fountain is the Sultan, which smothers with his graces all his subjects and lands, as the water wets the gardens.\"\n\nNowadays the flower garden has been substituted by a dry garden of pebbles, in order not to affect the foundation of the palace with the watering. In Nasrid times, the floor of the quartered planting beds was slightly lower than the general level, and the visual effect was like a tapestry of flowers, as the top of the plants were cut to the same level of the courtyard, and these were carefully chosen to cover a host of color nuances.\n\nSome research suggests that the 11th century lions of the Lion Fountain came from the house of the Jewish vizier Yusuf ibn Nagrela (d. 1066). It is not known if they were made before his death, and at the time, he was accused of wanting to build a bigger palace than the king's. They are large for sculptures of animals in Islamic art, but as in other sites of al-Andalus such as the earlier Medina Azahara near Cordoba, there are multiple animals. The Pisa Griffin is even larger.\n\nThere is an almost exact description of the original fountain, written by the poet Ibn Gabirol in the 11th century. They represent the 12 tribes of Israel, two of them have a triangle on the forehead, indicating the two extant tribes \"Judá\" and \"Leví\". The Lions were removed in 2007 for restoration on the premises while the fountain was restored \"in situ\". The lions were put back in place in July 2012 after reconstruction of the traditional water flow system of the Court of the Lions.\n\nThe poet and minister Ibn Zamrak wrote a poem to describe the beauty of the courtyard. It is carved around the rim of the basin:\n\n\n"}
{"id": "41777477", "url": "https://en.wikipedia.org/wiki?curid=41777477", "title": "Director of National Intelligence Review Group on Intelligence and Communications Technologies", "text": "Director of National Intelligence Review Group on Intelligence and Communications Technologies\n\nThe Director of National Intelligence Review Group on Intelligence and Communications Technologies was a review group formed by the Director of National Intelligence of the United States in light of the global surveillance disclosures of 2013. In December 2013, the five-member group produced a public report. \n\nOn August 12, 2013, President Barack Obama issued a Presidential Memorandum instructing the Director of National Intelligence, James Clapper, to form a \"Review Group on Intelligence and Communications Technologies\". Obama instructed that \"The Review Group will assess whether, in light of advancements in communications technologies, the United States employs its technical collection capabilities in a manner that optimally protects our national security and advances our foreign policy while appropriately accounting for other policy considerations, such as the risk of unauthorized disclosure and our need to maintain the public trust.\"\n\nThe memorandum called for an interim report within 60 days of establishment and a final report by December 15, 2013.\n\nThe group included former counter-terrorism czar Richard A. Clarke, former Acting Central Intelligence Agency director Michael Morell, University of Chicago Law professor Geoffrey Stone, former administrator of the White House Office of Information and Regulatory Affairs Cass Sunstein and former Chief Counselor for Privacy in the Office of Management and Budget Peter Swire.\n\nThe 300-page report, entitled \"Liberty and Security in a Changing World\", was released on December 12, 2013. It contained over 40 recommendations.\n\nThe Electronic Frontier Foundation released a statement criticizing the report, saying \"we’re disappointed that the recommendations suggest a path to continue untargeted spying. Mass surveillance is still heinous, even if private company servers are holding the data instead of government data centers.\"\n\n"}
{"id": "23653324", "url": "https://en.wikipedia.org/wiki?curid=23653324", "title": "Earth System Science Partnership", "text": "Earth System Science Partnership\n\nThe Earth System Science Partnership (ESSP) is a partnership under the auspices of the International Council for Science (ICSU) for the integrated study of the Earth system, the ways that it is changing, and the implications for global and regional sustainability. It includes Diversitas, IGBP, WCRP and IHDP.\n\nIn the present era, global environmental changes are both accelerating and changing Earth's systems into a state with no analogue in previous history. The Earth System is the unified set of physical, chemical, biological and social components, processes and interactions that together determine the state and dynamics of planet Earth, including its biota and its human occupants.\n\nEarth system science is the study of the Earth system, with an emphasis on observing, understanding and predicting global environmental changes involving interactions between land, atmosphere, water, ice, biosphere, societies, technologies and economies. \n\n\n"}
{"id": "51651490", "url": "https://en.wikipedia.org/wiki?curid=51651490", "title": "Electra (Oceanid)", "text": "Electra (Oceanid)\n\nIn Greek mythology, Electra (; , \"Ēlektra\" \"amber\") was the Oceanid daughter of Oceanus and Tethys. According to Hesiod, she was the wife of Thaumas, and by him, the mother of Iris, the messenger of the gods, and the Harpies.\n\nThe names of Electra's Harpy daughters vary. Hesiod and Apollodorus name them: Aello and Ocypete. Virgil, names Celaeno as one of the Harpies. However while Hyginus, \"Fabulae\" Preface has the Harpies, Celaeno, Ocypete, and Podarce, as daughters of Thaumas and Electra, at \"Fabuale\" 14.18, the Harpies are said to be named Aellopous, Celaeno, and Ocypete, and are the daughters of Thaumas and Ozomene.\n\nThe late 4th-early 5th century poet Nonnus gives Electra and Thaumas two children, Iris, and the river Hydaspes.\n\n"}
{"id": "12340904", "url": "https://en.wikipedia.org/wiki?curid=12340904", "title": "Energy in Uganda", "text": "Energy in Uganda\n\nBurning of renewable resources provides approximately 90 percent of the energy in Uganda, though the government is attempting to become energy self-sufficient. While much of the hydroelectric potential of the country is untapped, the government decision to expedite the creation of domestic petroleum capacity coupled with the discovery of large petroleum reserves holds the promise of a significant change in Uganda's status as an energy-importing country.\n\nIn the 1980s, charcoal and fuel wood met more than 95 percent of Uganda's energy needs. In 2005 and 2006, low water levels of Lake Victoria, the main source of the country's electricity generation potential, led to a generation shortage and an energy crisis. As a result, the country experienced frequent and prolonged blackouts. As of June 2016, according to the Uganda Bureau of Statistics, about twenty percent of Ugandans had access to electricity. As of February 2015 and according to the Uganda Electricity Regulatory Authority, Uganda's installed electricity capacity was 810 megawatts, with peak demand of 509.4 megawatts so that \"the incidence of load shedding due to shortage in supply is now close to zero.\" , according to Irene Muloni, the Uganda Minister of Energy, the country's generation capacity had increased to 950 megawatts. Uganda expects to have a generating capacity of at least 1,900 megawatts by the end of 2019, as forecast by the Uganda Ministry of Energy and Mineral Development. In March 2018, the World Bank Group estimated that about 26 percent of Uganda's population (8 percent in rural areas) had access to grid-electricity at that time.\n\nPoor maintenance during the politically unstable 1980s resulted in a drop in production at the Owen Falls Dam (now Nalubaale Power Station), at the mouth of the White Nile, from 635.5 million kilowatt-hours in 1986 to 609.9 million kilowatt-hours in 1987, with six of ten generators broken by the end of 1988. The 200 megawatt Kiira Hydroelectric Power Station, built adjacent to the Nalubaale Power Station, raised total production capacity to 380 megawatts.\n\nBetween 2007 and 2012, the 250 megawatt Bujagali Hydroelectric Power Station was constructed as a public-private project, at a cost of approximately US$862 million. The consortium that owns the station includes the Aga Khan Fund for Economic Development, Sithe Global Power LLC (a subsidiary of the Blackstone Group), and the government of Uganda. Bujagali Energy Limited is a special-vehicle company created to run the power station on behalf of the shareholders.\n\nIn October 2013, construction of the 183 megawatt Isimba Power Station began, approximately downstream of Bujagali, at a budgeted cost of approximately US$590 million, as a public enterprise with funding from the Export-Import Bank of China. Commissioning is planned during the second half of 2018, although power generation may begin as early as 2016.\n\nAlso in 2013, work on the 600 megawatt Karuma Power Station commenced at a budgeted cost of about US$2 billion, including US$250 million to build the high-voltage transmission lines to evacuate the generated power. Completion is planned for late 2018.\n\n, about six operational mini-hydropower plants are connected to the national electricity grid, supplying about 65 megawatts. These include Nyagak I (3.5 megawatts), Kabalega (9 megawatts), Kanungu (6.6 megawatts), Bugoye (13 megawatts), Mubuku I (5 megawatts), Mubuku III (10 megawatts), and Mpanga (18 megawatts).\n\nTwo heavy fuel oil thermal power stations exist in the country.\n\nNamanve Power Station is a 50 megawatt plant owned by Jacobsen Electricity Company (Uganda) Limited, a wholly owned subsidiary of Jacobsen Elektro, an independent Norwegian power production company. The plant cost US$92 million (€66 million) to build in 2008.\n\nTororo Power Station is an 89 megawatt heavy fuel-oil powered plant owned by Electro-Maxx Limited, a Ugandan company and a subsidiary of the Simba Group of Companies, owned by Ugandan industrialist Patrick Bitature. This plant is licensed to sell up to 50 megawatts to the national electricity grid.\n\nNamanve and Tororo are used as stand-by power sources to avoid load-shedding when hydropower generation fails to meet demand.\n\nFive sugar manufacturers in Uganda have total cogeneration capacity of about 110 megawatts, of which about 50 percent is available for sale to the national grid. The cogeneration power plants and their generation capacities include Kakira Power Station (52 megawatts), Kinyara Power Station (40 megawatts), Lugazi Power Station (14 megawatts), Kaliro Power Station (12 megawatts) and Mayuge Thermal Power Station (1.6 megawatts).\n\nUganda is highly vulnerable to oil price shocks as it imports almost all of its of oil (2013 figure). The oil comes through the Kenyan port of Mombasa.\n\nThe governments of Kenya, Uganda, and Rwanda are jointly developing the Kenya–Uganda–Rwanda Petroleum Products Pipeline to carry refined petroleum products from Mombasa through Nairobi to Eldoret, all in Kenya. From Eldoret, the pipeline will continue through Malaba to Kampala in Uganda, continuing on to Kigali in Rwanda. The feasibility study for the Eldoret to Kampala pipeline extension was awarded to an international firm in 1997. The study was completed in 1998 and the report submitted the following year. The separate feasibility study for the Kampala to Kigali extension was awarded to the East African Community in September 2011. The governments of Kenya, Uganda, and Rwanda accepted the findings of the studies. The construction contract was initially awarded, in 2007, to Tamoil, a company owned by the Government of Libya. That contract was voided in 2012 after the company failed to implement the project. , fourteen companies had submitted bids to construct the pipeline extension from Kenya to Rwanda. Construction is expected to begin in 2014, with a 32-month construction timeframe. Commissioning is expected in 2016.\n\nIn 2006, Uganda confirmed the existence of commercially viable petroleum reserves in the Western Rift Valley around Lake Albert. In June 2006, Hardman Resources of Australia discovered oil sands at Waranga 1, Waranga 2, and Mputa. President Yoweri Museveni announced that he expected production of to by 2009.\n\nIn July 2007, Heritage Oil, one of several companies prospecting around Lake Albert, raised its estimate for the Kingfisher well (block 3A) in Hoima District, Bunyoro sub-region, stating that they thought it was bigger than of crude. Heritage's partner, London-based Tullow Oil, which had bought Hardman Resources, was more guarded, but stated their confidence that the Albertine Basin as a whole contained over one billion barrels. The Kingfisher-1 well flowed of 30-32 API oil.\n\nThis news came on the heels of Tullow's 11 July 2007 report that the Nzizi 2 appraisal well confirmed the presence of per day of natural gas. Heritage in a report to its partners talked of Ugandan reserves of worth $7 billion as the \"most exciting new play in sub-Saharan Africa in the past decade.\" However, development will require a pipeline to the coast, which will need $80 oil to justify. Relations between Uganda and the neighboring Democratic Republic of the Congo (DRC) have been tense since the discovery of oil, as both countries seek to clarify the border delineation on the lake in their favor, in particular the ownership of small Rukwanzi Island. Ugandan foreign minister Sam Kutesa made an emergency visit to Kinshasa in an attempt to smooth tensions.\n\n\"The Economist\" magazine, noting that the DRC has assigned exploration blocks on its side of the border, proposed that the situation should sort itself amicably: Uganda needs a stable and secure border in order to attract foreign investment developing the oil reserves, while the cost of transporting the oil to the DRC's sole port at Matadi is so prohibitive that the Congolese government is nearly obliged to seek pipeline access through Uganda.\n\nAfter an initial period of disagreement between the Government of Uganda and the petroleum exploration companies, the two sides agreed in April 2013 to simultaneously build a crude oil pipeline to the Kenyan coast (Uganda–Kenya Crude Oil Pipeline) and an oil refinery in Uganda (Uganda Oil Refinery).\n\nIn February 2015, the Ugandan government selected the consortium led by Russia's RT Global Resources as the winning bidder, to construct the refinery. The government was expected to begin in-depth negotiations with the winning bidder for a binding agreement to construct the refinery. The negotiations were expected to last about 60 days. If the parties failed to agree on terms, the government planned to negotiate with the losing bidder, the consortium led by SK Energy of South Korea, to construct the refinery. When those talks broke down in July 2016, Uganda began talks with the reserve bidder, the consortium led by SK Engineering & Construction of South Korea.\n\nIn August 2017, negotiations with the consortium led by SK Engineering & Construction also broke down. Negotiations were then started with a new consortium led by Guangzhou Dongsong Energy Group, a Chinese company. Those talks collapsed in June 2017 when CPECC, the main contractor in the consortium, pulled out of the talks.\n\nIn August 2017, Albertine Graben Refinery Consortium, a new consortium led by General Electric (GE) of the United States and J&K Minerals Africa agreed to build the US$4 billion refinery. GE is to own 50 percent and J&K Minerals Africa to own 10 percent, while the government of Uganda and other investors take up the remaining 40 percent. Total SA, has pledged to take up a 10 percent stake in the refinery.\n\nIn December 2017, Irene Muloni, Uganda's Energy Minister announced that the country planned to join the Organization of the Petroleum Exporting Countries (OPEC), by 2020, when first oil is expected.\n\nTo diversify the national energy pool, the Electricity Regulatory Authority in December 2014 licensed two solar power stations, each with capacity to generate 10 megawatts. The stations, Tororo Solar Power Station and Soroti Solar Power Station, were expected to come online no later than December 2015. In December 2016, Soroti Solar Power Station was completed and connected to the national grid. Tororo Solar Power Station was also brought online in October 2017.\n\n\n"}
{"id": "1417710", "url": "https://en.wikipedia.org/wiki?curid=1417710", "title": "Energy policy", "text": "Energy policy\n\nEnergy policy is the manner in which a given entity (often governmental) has decided to address issues of energy development including energy production, distribution and consumption. The attributes of energy policy may include legislation, international treaties, incentives to investment, guidelines for energy conservation, taxation and other public policy techniques. Energy is a core component of modern economies. A functioning economy requires not only labor and capital but also energy, for manufacturing processes, transportation, communication, agriculture, and more. \n\nConcerning the term of energy policy, the importance of implementation of an eco-energy-oriented policy at a global level to address the issues of global warming and climate changes should be accentuated.\n\nAlthough research is ongoing, the \"human dimensions\" of energy use are of increasing interest to business, utilities, and policymakers. Using the social sciences to gain insights into energy consumer behavior can empower policymakers to make better decisions about broad-based climate and energy options. This could facilitate more efficient energy use, renewable energy commercialization, and carbon emission reductions. Access to energy is also critical for basic social needs, such as lighting, heating, cooking, and health care. As a result, the price of energy has a direct effect on jobs, economic productivity and business competitiveness, and the cost of goods and services.\n\nA national energy policy comprises a set of measures involving that country's laws, treaties and agency directives. The energy policy of a sovereign nation may include one or more of the following measures:\n\n\nFrequently the dominant issue of energy policy is the risk of supply-demand mismatch (see: energy crisis). Current energy policies also address environmental issues (see: climate change), particularly challenging because of the need to reconcile global objectives and international rules with domestic needs and laws. Some governments state explicit energy policy, but, declared or not, each government practices some type of energy policy. Economic and energy modelling can be used by governmental or inter-governmental bodies as an advisory and analysis tool (see: economic model, POLES).\n\nThere are a number of elements that are naturally contained in a national energy policy, regardless of which of the above measures was used to arrive at the resultant policy. The chief elements intrinsic to an energy policy are:\n\n\nEven within a state it is proper to talk about energy policies in plural. Influential entities, such as municipal or regional governments and energy industries, will each exercise policy. Policy measures available to these entities are lesser in sovereignty, but may be equally important to national measures. In fact, there are certain activities vital to energy policy which realistically cannot be administered at the national level, such as monitoring energy conservation practices in the process of building construction, which is normally controlled by state-regional and municipal building codes (although can appear basic federal legislation).\n\nBrazil is the 10th largest energy consumer in the world and the largest in South America. At the same time, it is an important oil and gas producer in the region and the world's second largest ethanol fuel producer. The governmental agencies responsible for energy policy are the Ministry of Mines and Energy (MME), the National Council for Energy Policy (CNPE, in the Portuguese-language acronym), the National Agency of Petroleum, Natural Gas and Biofuels (ANP) and the National Agency of Electricity (ANEEL). State-owned companies Petrobras and Eletrobrás are the major players in Brazil's energy sector.\n\nCurrently, the major issues in U.S. energy policy revolve around the rapidly growing production of domestic and other North American energy resources. The U.S. drive toward energy independence and less reliance on oil and coal is fraught with partisan conflict because these issues revolve around how best to balance both competing values, such as environmental protection and economic growth, and the demands of rival organized interests, such as those of the fossil fuel industry and of the newer renewable energy businesses. \n\nAlthough the European Union has legislated, set targets, and negotiated internationally in the area of energy policy for many years, and evolved out of the European Coal and Steel Community, the concept of introducing a mandatory common European Union energy policy was only approved at the meeting of the European Council on October 27, 2005 in London. Following this the first policy proposals, \"Energy for a Changing World\", were published by the European Commission, on January 10, 2007. The most well known energy policy objectives in the EU are 20/20/20 objectives, binding for all EU Member States. The EU is planning to increase the share of renewable energy in its final energy use to 20%, reduce greenhouse gases by 20% and increase energy efficiency by 20%.\n\nIn September 2010, the German government adopted a set of ambitious goals to transform their national energy system and to reduce national greenhouse gas emissions by 80 to 95% by 2050 (relative to 1990).\nThis transformation become known as the \"Energiewende\". Subsequently, the government decided to the phase-out the nation's fleet of nuclear reactors, to be complete by 2022.\nAs of 2014, the country is making steady progress on this transition.\n\nThe energy policy of the United Kingdom has achieved success in reducing energy intensity (but still really high), reducing energy poverty, and maintaining energy supply reliability to date. The United Kingdom has an ambitious goal to reduce carbon dioxide emissions for future years, but it is unclear whether the programs in place are sufficient to achieve this objective (the way to be so efficient as France is still hard). Regarding energy self sufficiency, the United Kingdom policy does not address this issue, other than to concede historic energy self sufficiency is currently ceasing to exist (due to the decline of the North Sea oil production). With regard to transport, the United Kingdom historically has a good policy record encouraging public transport links with cities, despite encountering problems with high speed trains, which have the potential to reduce dramatically domestic and short-haul European flights. The policy does not, however, significantly encourage hybrid vehicle use or ethanol fuel use, options which represent viable short term means to moderate rising transport fuel consumption. Regarding renewable energy, the United Kingdom has goals for wind and tidal energy. The White Paper on Energy, 2007, set the target that 20% of the UK's energy must come from renewable sources by 2020.\n\nThe Soviet Union was the largest energy provider in the world until the late 1980s. Russia, one of the world's energy superpowers, is rich in natural energy resources, the world’s leading net energy exporter, and a major supplier to the European Union. The main document defining the energy policy of Russia is the Energy Strategy, which initially set out policy for the period up to 2020, later was reviewed, amended and prolonged up to 2030. While Russia has also signed and ratified the Kyoto Protocol. Numerous scholars note that Russia uses its energy exports as a foreign policy instrument towards other countries. \n\nThe energy policy of India is characterized by trades between four major drivers:\n\nIn recent years, these challenges have led to a major set of continuing reforms, restructuring and a focus on energy conservation.\n\nThe energy policy of Thailand is characterized by 1) increasing energy consumption efficiency, 2) increasing domestic energy production, 3) increasing the private sector's role in the energy sector, 4) increasing the role of market mechanisms in setting energy prices. These policies have been consistent since the 1990s, despite various changes in governments. The pace and form of industry liberalization and privatization has been highly controversial.\n\nThe first National Energy Policy (NEP) of Bangladesh was formulated in 1996 by the Ministry of Power, Energy and Mineral resources to ensure proper exploration, production, distribution and rational use of energy resources to meet the growing energy demands of different zones, consuming sectors and consumers groups on a sustainable basis.[1] With rapid change of global as well as domestic situation, the policy was updated in 2004. The updated policy included additional objectives namely to ensure environmentally sound sustainable energy development programmes causing minimum damage to environment, to encourage public and private sector participation in the development and management of energy sector and to bring the entire country under electrification by the year 2020.[2]\n\nAustralia's energy policy features a combination of coal power stations and hydro electricity plants. The Australian government has decided not to build nuclear power plants, although it is one of the world's largest producers of uranium.\n\n\n\n"}
{"id": "69902", "url": "https://en.wikipedia.org/wiki?curid=69902", "title": "Extreme weather", "text": "Extreme weather\n\nExtreme weather includes unexpected, unusual, unpredictable, severe or unseasonal weather; weather at the extremes of the historical distribution—the range that has been seen in the past. Often, extreme events are based on a location’s recorded weather history and defined as lying in the most unusual ten percent. In recent years some extreme weather events have been attributed to human-induced global warming, with studies indicating an increasing threat from extreme weather in the future.\n\nAccording to IPCC (2011) estimates of annual losses have ranged since 1980 from a few billion to above US$200 billion (in 2010 dollars), with the highest value for 2005 (the year of Hurricane Katrina). The global weather-related disaster losses because many impacts, such as loss of human lives, cultural heritage, and ecosystem services, are difficult to value and monetize, and thus they are poorly reflected in estimates of losses.\n\nHeat waves are periods of abnormally high temperatures and heat index. Definitions of a heatwave vary because of the variation of temperatures in different geographic locations. Excessive heat is often accompanied by high levels of humidity, but can also be catastrophically dry.\n\nBecause heat waves are not visible as other forms of severe weather are, like hurricanes, tornadoes, and thunderstorms, they are one of the less known forms of extreme weather. Severe heat weather can damage populations and crops due to potential dehydration or hyperthermia, heat cramps, heat expansion and heat stroke. Dried soils are more susceptible to erosion, decreasing lands available for agriculture. Outbreaks of wildfires can increase in frequency as dry vegetation has increased likeliness of igniting. The evaporation of bodies of water can be devastating to marine populations, decreasing the size of the habitats available as well as the amount of nutrition present within the waters. Livestock and other animal populations may decline as well.\n\nDuring excessive heat plants shut their leaf pores (stomata), a protective mechanism to conserve water but also curtails plants' absorption capabilities. This leaves more pollution and ozone in the air, which leads to a higher mortality in the population. It has been estimated that extra pollution during the hot summer 2006 in the UK, cost 460 lives. The European heat waves from summer 2003 are estimated to have caused 30,000 excess deaths, due to heat stress and air pollution. Over 200 U.S cities have registered new record high temperatures. The worst heatwave in the USA occurred in 1936 and killed more than 5000 people directly. The worst heat wave in Australia occurred in 1938-39 and killed 438. The second worst was in 1896.\n\nPower outages can also occur within areas experiencing heat waves due to the increased demand for electricity (i.e. air conditioning use). The urban heat island effect can increase temperatures, particularly overnight.\n\nA cold wave is a weather phenomenon that is distinguished by a cooling of the air. Specifically, as used by the U.S. National Weather Service, a cold wave is a rapid fall in temperature within a 24-hour period requiring substantially increased protection to agriculture, industry, commerce, and social activities. The precise criterion for a cold wave is determined by the rate at which the temperature falls, and the minimum to which it falls. This minimum temperature is dependent on the geographical region and time of year. Cold waves generally are capable of occurring any geological location and are formed by large cool air masses that accumulate over certain regions, caused by movements of air streams.\n\nA cold wave can cause death and injury to livestock and wildlife. Exposure to cold mandates greater caloric intake for all animals, including humans, and if a cold wave is accompanied by heavy and persistent snow, grazing animals may be unable to reach necessary food and water, and die of hypothermia or starvation. Cold waves often necessitate the purchase of fodder for livestock at considerable cost to farmers. Human populations can be inflicted with frostbites when exposed for extended periods of time to cold and may result in the loss of limbs or damage to internal organs.\n\nExtreme winter cold often causes poorly insulated water pipes to freeze. Even some poorly protected indoor plumbing may rupture as frozen water expands within them, causing property damage. Fires, paradoxically, become more hazardous during extreme cold. Water mains may break and water supplies may become unreliable, making firefighting more difficult.\n\nCold waves that bring unexpected freezes and frosts during the growing season in mid-latitude zones can kill plants during the early and most vulnerable stages of growth. This results in crop failure as plants are killed before they can be harvested economically. Such cold waves have caused famines. Cold waves can also cause soil particles to harden and freeze, making it harder for plants and vegetation to grow within these areas. One extreme was the so-called Year Without a Summer of 1816, one of several years during the 1810s in which numerous crops failed during freakish summer cold snaps after volcanic eruptions reduced incoming sunlight.\n\nIn general climate models show that with climate change, the planet will experience more extreme weather. In particular temperature record highs outpace record lows and some types of extreme weather such as extreme heat, intense precipitation, and drought have become more frequent and severe in recent decades. Some studies assert a connection between rapidly warming arctic temperatures and thus a vanishing cryosphere to extreme weather in mid-latitudes.\n\nIn the PNAS, Steven C. Sherwood and Matthew Huber state that humans and other mammals cannot tolerate a wet-bulb temperature of over 35 °C for extended periods, and that this \"would begin to occur with global-mean warming of about 7 °C ... With 11–12 °C warming, such regions would spread to encompass the majority of the human population as currently distributed. Eventual warmings of 12 °C are possible from fossil fuel burning.\"\n\nThere has been long ongoing debate about a possible increase of tropical cyclones as an effect of global warming. However, the 2012 IPCC special report on extreme events SREX states that \"there is low confidence in any observed long-term (i.e., 40 years or more) increases in tropical cyclone activity (i.e., intensity, frequency, duration), after accounting for past changes in observing capabilities.\" \nIncreases in population densities increase the number of people affected and damage caused by an event of given severity. The World Meteorological Organization and the U.S. Environmental Protection Agency have in the past linked increasing extreme weather events to global warming, as have Hoyos \"et al.\" (2006), writing that the increasing number of category 4 and 5 hurricanes is directly linked to increasing temperatures. Similarly, Kerry Emanuel in \"Nature\" writes that hurricane power dissipation is highly correlated with temperature, reflecting global warming.\n\nHurricane modeling has produced similar results, finding that hurricanes, simulated under warmer, high CO conditions, are more intense than under present-day conditions. Thomas Knutson and Robert E. Tuleya of the NOAA stated in 2004 that warming induced by greenhouse gas may lead to increasing occurrence of highly destructive category-5 storms. Vecchi and Soden find that wind shear, the increase of which acts to inhibit tropical cyclones, also changes in model-projections of global warming. There are projected increases of wind shear in the tropical Atlantic and East Pacific associated with the deceleration of the Walker circulation, as well as decreases of wind shear in the western and central Pacific. The study does not make claims about the net effect on Atlantic and East Pacific hurricanes of the warming and moistening atmospheres, and the model-projected increases in Atlantic wind shear.\n\n\n\n"}
{"id": "9414169", "url": "https://en.wikipedia.org/wiki?curid=9414169", "title": "Geomagnetically induced current", "text": "Geomagnetically induced current\n\nGeomagnetically induced currents (GIC), affecting the normal operation of long electrical conductor systems, are a manifestation at ground level of space weather. During space weather events, electric currents in the magnetosphere and ionosphere experience large variations, which manifest also in the Earth's magnetic field. These variations induce currents (GIC) in conductors operated on the surface of Earth. Electric transmission grids and buried pipelines are common examples of such conductor systems. GIC can cause problems, such as increased corrosion of pipeline steel and damaged high-voltage power transformers. GIC are one possible consequence of geomagnetic storms, which may also affect geophysical exploration surveys and oil and gas drilling operations.\n\nThe Earth's magnetic field varies over a wide range of timescales. The longer-term variations, typically occurring over decades to millennia, are predominantly the result of dynamo action in the Earth's core. Geomagnetic variations on timescales of seconds to years also occur, due to dynamic processes in the ionosphere, magnetosphere and heliosphere. These changes are ultimately tied to variations associated with the solar activity (or sunspot) cycle and are manifestations of space weather.\n\nThe fact that the geomagnetic field does respond to solar conditions can be useful, for example, in investigating Earth structure using magnetotellurics, but it also creates a hazard. This geomagnetic hazard is primarily a risk to technology under the Earth's protective atmospheric blanket.\n\nA time-varying magnetic field external to the Earth induces telluric currents—electric currents in the conducting ground. These currents create a secondary (internal) magnetic field. As a consequence of Faraday's law of induction, an electric field at the surface of the Earth is induced associated with time variations of the magnetic field. The surface electric field causes electrical currents, known as geomagnetically induced currents (GIC), to flow in any conducting structure, for example, a power or pipeline grid grounded in the Earth. This electric field, measured in V/km, acts as a voltage source across networks.\n\nExamples of conducting networks are electrical power transmission grids, oil and gas pipelines, non-fiber optic undersea communication cables, non-fiber optic telephone and telegraph networks and railways. GIC are often described as being quasi direct current (DC), although the variation frequency of GIC is governed by the time variation of the electric field. For GIC to be a hazard to technology, the current has to be of a magnitude and occurrence frequency that makes the equipment susceptible to either immediate or cumulative damage. The size of the GIC in any network is governed by the electrical properties and the topology of the network. The largest magnetospheric-ionospheric current variations, resulting in the largest external magnetic field variations, occur during geomagnetic storms and it is then that the largest GIC occur. Significant variation periods are typically from seconds to about an hour, so the induction process involves the upper mantle and lithosphere. Since the largest magnetic field variations are observed at higher magnetic latitudes, GIC have been regularly measured in Canadian, Finnish and Scandinavian power grids and pipelines since the 1970s. GIC of tens to hundreds of amperes have been recorded. GIC have also been recorded at mid-latitudes during major storms. There may even be a risk to low latitude areas, especially during a storm commencing suddenly because of the high, short-period rate of change of the field that occurs on the day side of the Earth.\n\nGIC were first observed on the emerging electric telegraph network in 1847–8 during Solar cycle 9. Technological change and the growth of conducting networks have made the significance of GIC greater in modern society. The technical considerations for undersea cables, telephone and telegraph networks and railways are similar. Fewer problems have been reported in the open literature, about these systems. This suggests that the hazard is less today, or that there are reliable methods of equipment protection.\n\nModern electric power transmission systems consist of generating plants inter-connected by electrical circuits that operate at fixed transmission voltages controlled at substations. The grid voltages employed are largely dependent on the path length between these substations and 200-700 kV system voltages are common. There is a trend towards higher voltages and lower line resistances to reduce transmission losses over longer and longer path lengths. Low line resistances produce a situation favourable to the flow of GIC. Power transformers have a magnetic circuit that is disrupted by the quasi-DC GIC: the field produced by the GIC offsets the operating point of the magnetic circuit and the transformer may go into half-cycle saturation. This produces harmonics to the AC waveform, localised heating and leads to high reactive power demands, inefficient power transmission and possible mis-operation of protective measures. Balancing the network in such situations requires significant additional reactive power capacity. The magnitude of GIC that will cause significant problems to transformers varies with transformer type. Modern industry practice is to specify GIC tolerance levels on new transformers.\n\nOn 13 March 1989, a severe geomagnetic storm caused the collapse of the Hydro-Québec power grid in a matter of seconds as equipment protective relays tripped in a cascading sequence of events. Six million people were left without power for nine hours, with significant economic loss. Since 1989, power companies in North America, the United Kingdom, Northern Europe, and elsewhere have invested in evaluating the GIC risk and in developing mitigation strategies.\n\nGIC risk can, to some extent, be reduced by capacitor blocking systems, maintenance schedule changes, additional on-demand generating capacity, and ultimately, load shedding. These options are expensive and sometimes impractical. The continued growth of high voltage power networks results in higher risk. This is partly due to the increase in the interconnectedness at higher voltages, connections in terms of power transmission to grids in the auroral zone, and grids operating closer to capacity than in the past.\n\nTo understand the flow of GIC in power grids and to advise on GIC risk, analysis of the quasi-DC properties of the grid is necessary. This must be coupled with a geophysical model of the Earth that provides the driving surface electric field, determined by combining time-varying ionospheric source fields and a conductivity model of the Earth. Such analyses have been performed for North America, the UK and in Northern Europe. The complexity of power grids, the source ionospheric current systems and the 3D ground conductivity make an accurate analysis difficult. By being able to analyze major storms and their consequences we can build a picture of the weak spots in a transmission system and run hypothetical event scenarios.\n\nGrid management is also aided by space weather forecasts of major geomagnetic storms. This allows for mitigation strategies to be implemented. Solar observations provide a one- to three-day warning of an Earthbound coronal mass ejection (CME), depending on CME speed. Following this, detection of the solar wind shock that precedes the CME in the solar wind, by spacecraft at the Lagrangian point, gives a definite 20 to 60 minutes warning of a geomagnetic storm (again depending on local solar wind speed). It takes approximately two to three days after a CME launches from the Sun for a geomagnetic storm to reach Earth and to affect the Earth‘s geomagnetic field.\n\nMajor pipeline networks exist at all latitudes and many systems are on a continental scale. Pipeline networks are constructed from steel to contain high-pressure liquid or gas and have corrosion resistant coatings. Weathering and other damage to the pipeline coating can result in the steel being exposed to moist air or to the ground, causing localised corrosion problems. Cathodic protection is used to minimise corrosion by maintaining the steel at a negative potential with respect to the ground. The operating potential is determined from the electro-chemical properties of the soil and Earth in the vicinity of the pipeline. The GIC hazard to pipelines is that GIC cause swings in the pipe-to-soil potential, increasing the rate of corrosion during major geomagnetic storms (Gummow, 2002). GIC risk is not a risk of catastrophic failure, but a reduced service life of the pipeline.\n\nPipeline networks are modeled in a similar manner to power grids, for example through distributed source transmission line models that provide the pipe-to-soil potential at any point along the pipe (Boteler, 1997; Pulkkinen et al., 2001). These models need to consider complicated pipeline topologies, including bends and branches, as well as electrical insulators (or flanges) that electrically isolate different sections. From a detailed knowledge of the pipeline response to GIC, pipeline engineers can understand the behaviour of the cathodic protection system even during a geomagnetic storm, when pipeline surveying and maintenance may be suspended.\n\n\n\n\nPower grid related links\n"}
{"id": "41348328", "url": "https://en.wikipedia.org/wiki?curid=41348328", "title": "Girish Sant", "text": "Girish Sant\n\nGirish Sant was a noted energy analyst held in high esteem as an energy policy commentator from India. He co-founded the non-governmental organisation Prayas in Pune, India. His analytical inputs helped shape India's energy policy over the decades of the 1990s and 2000s. He was considered an effective team builder and mentored several energy researchers and activists.\n\nGirish spent his childhood in Thane, and joined IIT Mumbai in 1982 for BTech in Chemical engineering. After completing BTech in 1986, he also completed Masters in Energy Systems in 1988 from IIT.\n\nGirish's years in IIT Mumbai brought out his leadership, team building and mountaineering skills. He was an accomplished mountaineer and rock climber, and made important rock climbing ascents with fellow mountaineers including the first ever climb of the Konkan Kada. He was an active member of the IIT Mountaineering Club and also the Institute Mountaineering Secretary during 1985–86.\n\nDuring his stay at IIT, particularly during his Masters study, Girish started thinking about full-time work in a field of direct social relevance along with friends – Ajit Gaunekar and Aniruddha Ketkar. He started interacting with Subodh Wagle, then research fellow at Center for Technology Alternatives for Rural Areas, with whom he explored appropriate technologies, rural society and related developmental paradigms.\n\nBy 1988, when Girish completed Masters in energy systems, he developed a clear idea that he wanted to work for the betterment of society and not for personal prosperity, using his professional skills on issues related to energy. He relocated to Pune and initially worked as a lecturer in an engineering college, undertook sporadic energy audit and industrial consultancy projects and then worked at Systems Research Institute. This was a period of exploration along with other like-minded friends – Shripad Dharmadhikary, Sanjeevani and Vinay Kulkarni – that brought him closer to people's movements, particularly the NBA.\n\nDuring this period he came across the Development Focused End Use Oriented (DEFENDUS) approach to power sector planning developed by Prof. Amulya Kumar N. Reddy. Subsequent interactions with Prof. Reddy shaped his thinking and work in the energy sector in the early period. By this time, Girish started working with Shantanu Dixit who continued to be his colleague for the rest of his life. Analytical motivation from DEFENDUS, along with the support of friends and a scholarship from Dr. Ashok Gadgil, led to his first major work on development of a least cost plan for Maharashtra. Development of the least cost plan, its dissemination to various quarters and subsequent responses from power sector actors and activists contributed to his understanding of the energy sector and the broader political economy and institutional dynamics of the sector in India.\n\nThis was also the time of reforms for the power sector in India, which witnessed entry of projects such as Dabhol Power Company built by Enron. Girish, Subodh and Shantanu were able to see the long term implications of such projects and reforms for the Indian power sector and economy at large. Realising the need to de-mystify such complex projects and to highlight their implications for people of the state and the country, they worked relentlessly to unravel the complex power purchase agreement of Enron and communicate the devastating impact of the project to activists and the wider community. These early experiences shaped his vision for the power sector as well as his strategic and substantive approach to work in the energy sector.\n\nIn 1994, his work in the energy sector evolved into the formation of Prayas, Initiatives in Health, Energy, Learning and Parenthood along with Sanjeevani and Vinay Kulkarni.\n\nGirish believed in and ensured teamwork and democratic working of the group. Under his leadership, the Energy Group within Prayas (PEG), which started with three people, expanded to a team of over 15 researchers from a variety of backgrounds. Girish had the ability to connect with a wide range of professionals, which attracted senior researchers as well as young engineers to join Prayas.\n\nGirish was particular about encouraging intellectual and substantive growth of colleagues, and supported new initiatives in the form of Resources and Livelihoods group of Prayas as well as academic interests of young researchers. Girish paid meticulous attention to the internal processes within Prayas and ensured that proper procedures were followed. Many peers and friends of Girish consider his institution building abilities as important a contribution and achievement as his substantive work in the energy sector. Girish assisted the likes of Sucheta Dalal, then a columnist in the Times of India, in understanding the controversial Dabhol power project and the Enron India scam that they unearthed. \n\nGirish emphasised the need to be agile and to undertake strategic interventions in the sector. High quality and in-depth analysis, comprehensive approach, and prioritising interests of disadvantaged sections became the hallmark of his work and he successfully cultivated these principles across PEG. He believed that improving governance in infrastructure sectors like energy has the dual advantage of improving lives of the poor as well as saving public money that can then be spent on other services such as education. He successfully motivated and actively supported many young researchers to take up the task of policy advocacy in the energy sector based on public interest analysis.\n\nIn spite of several accomplishments and achieving an important stature in the energy sector in India, Girish remained humble and self-effacing, as is reflected in many of the tributes on his memorial webpage and in the Smriti Grantha (or Collection of Memoirs). He was mild mannered and soft-spoken, even when trying to convince someone holding a contrary opinion. This quality endeared him to many in the sector resulting in increased impact.\n\nGirish was known in the energy sector for his use of high quality analysis to expose inadequacies of conventional planning and projects that result out of such a process. Under his guidance, PEG undertook techno-economic analysis of three large hydro-electric projects, Sardar Sarovar and Maheshwar in India and Bujagali in Uganda. The group analysed Sardar Sarovar and Maheshwar projects and highlighted inefficiencies therein, proposing several techno-economically feasible and socially desirable alternatives. Analysis of Bujagali Hydroelectric Power Station brought out inflated capital costs and one-sided nature of the power purchase agreement and led to renegotiation of the contract.\n\nThe wave of independent power producers in the 1990s was followed by State Electricity Board (SEB) reforms supported by the World Bank and Asian Development Bank, starting from Orissa in 1996. Under these reforms unbundling the SEB into generation, transmission and distribution companies, setting up a regulatory commission and gradually privatising the distribution was presented as the solution to all problems in power sector. PEG was the first to prepare a public interest critique of the Orissa model of reforms in 1998 and the role of Multi-lateral Development Banks, arguing that democratising governance is the key to addressing the power sector crisis, rather than focusing only on infusing capital or changing ownership.\n\nPEG realised that the fight against unjust, inefficient projects needs to be started at the macro-level planning stage itself. Experience of disseminating least cost plan and struggle against Enron project, highlighted the influence of political economy on decision making and underscored the need for improving governance through enhanced transparency, accountability and participation (TAP) in the energy sector decision making. These insights have since guided the group's work in the energy sector and led to the group undertaking early interventions in improving the newly emerging independent regulatory commissions. Girish provided strategic guidance on the initial idea of bringing together a transnational network of civil society groups, called the Electricity Governance Initiative, that would work together to advance the principles of transparent, inclusive and accountable governance of electricity.\n\nGirish was keen that analysis is followed by actual interventions aimed at pro-people changes. Accordingly, PEG actively engaged with several state as well as central regulatory commissions, with the aim of making regulatory process more transparent, accountable, participatory and helped serve the public interest more effectively. Subsequent to enactment of Electricity Act 2003, PEG was actively involved in giving inputs to national policies such as National Electricity Policy, Tariff Policy and Competitive Bidding Guidelines. This analysis of the Indian power sector and its role in the regulatory process was acknowledged by many in the sector.\n\nSince 2006, Girish focused more on macro issues of resource availability, utilisation, and growing importance of global climate debate on India's energy policy. In 2009, he co-authored a report, 'An Overview of India's Energy Trends', highlighting important differences in energy production and consumption trends of India, US, European Union and China. Based on this work, he was invited to make presentations at high level meetings at COP15 at Copenhagen and at The Center for Clean Air Policy, Washington, D.C. He was India's representative at a UN workshop on non-Annex 1 NAMAs.\n\nSubsequently, he was one of India's representatives in the BASIC Expert Group (an informal energy expert group formed by BASIC governments) that worked towards developing greater understanding of energy use in BASIC countries and for evolving common approach to climate negotiation. All these efforts and analysis contributed to strengthening India's position in the global discourse on climate change and energy, and also helped shape the domestic policy discourse.\n\nOn the domestic front, while welcoming the investment in renewable energy (RE), Girish suggested measures to improve effectiveness and equity in RE expansion. He was instrumental in making a case for setting up a National Wind Energy Mission, which is scheduled to begin in 2014.\n\nSearch for innovative solutions to vexed problems was another characteristic of Girish. This search led to a unique and novel concept for improving efficiency of commonly used domestic appliances. Though Girish and several other researchers had pointed out that energy efficiency of commonly used domestic appliances is very poor and using most efficient appliances instead of these inefficient appliances will lead to savings of thousands of MWs, a workable large scale solution to achieve this transformation was elusive. Girish, along with colleagues at Prayas, developed a concept called 'Super-Efficient Equipment Program – SEEP' under which nominal incentives are provided to appliance manufacturers to bring super-efficient equipment into the market. He successfully convinced Government of India and Planning Commission officials of the benefits of implementing such a program. Under this program, which will be launched in 2014 as part of the 12th Five Year Plan, it is expected that over five million 'super-efficient' fans, which consume half the electricity of normal fans, will be sold in the market. This approach is also being adopted at the global level under the auspices of the Clean Energy Ministerial.\n\nGirish was also part of several official committees, such as Planning Commission's working groups for 11th and 12th five-year plans, Planning Commission's Steering Committee on Energy, the Supreme Court appointed Committee on Solid Waste Disposal, and Planning Commission's Expert Group on Low Carbon Strategies for Inclusive Growth.\n\nGirish died on 2 February 2012 in New Delhi due to cardiac arrest.\n\nA committee consisting of people from within and outside Prayas was formed to manage activities organised in Girish's memory and \"to further his work of independent analysis and advocacy to promote public interest issues in the energy sector\". These activities include an annual memorial lecture organised in Pune and a fellowship for young researchers pursuing public interest research and advocacy.\n\n\n"}
{"id": "4478230", "url": "https://en.wikipedia.org/wiki?curid=4478230", "title": "Global strategic petroleum reserves", "text": "Global strategic petroleum reserves\n\nGlobal strategic petroleum reserves (GSPR) refer to crude oil inventories (or stockpiles) held by the government of a particular country, as well as private industry, to safeguard the economy and help maintain national security during an energy crisis.\n\nAccording to the United States Energy Information Administration, approximately of oil are held in strategic reserves, of which 1.4 billion is government-controlled. The remainder is held by private industry. In 2004 the U.S. Strategic Petroleum Reserve had the largest strategic reserve, with much of the remainder held by the other 27 members of the International Energy Agency. Some non-IEA countries have started work on their own strategic petroleum reserves. China has the largest of these new reserves.\n\nGlobal oil consumption is in the region of per day. The 4.1 bbl reserve is equivalent to 41 days of production. If there ever is a dramatic fall in global output, as envisaged by some peak oil analysts, the strategic petroleum reserves might be used to cover the shortfall. Covering a 50% shortfall would deplete the reserves in 82 days, although export leaders the Middle East and Russian exports represent only 22% and 6% respectively of global production.\n\nAccording to a March 2001 agreement, all 28 members of the International Energy Agency must have a strategic petroleum reserve equal to 90 days of the previous year's net oil imports for their respective countries.\nOnly net-exporter members of the IEA are exempt from this requirement. The exempt countries are Canada, Denmark, Norway, and the United Kingdom. However, the UK and Denmark recently drew up plans to create their own strategic reserves in order to meet their legal obligations as European Union member states. This agreement was reviewed and ratified by Steven Brown in 2008.\n\nTo allow oil-exporting countries increased flexibility in their production quotas, there has been a progressive movement towards forward commercial storage agreements. These agreements allow petroleum to be stored within an oil-importing country. However, the reserves are technically under the control of the oil-exporting country. Such agreements enable oil-importing countries to access these commercial reserves in a timely and cost effective way.\n\nSeveral countries have agreements to share their stockpiles with other countries in the event of an emergency.\n\nIn 2007, Japan announced a plan to share its strategic reserves with other countries in the region. Negotiations are under way between Japan and New Zealand for an oil-sharing deal whereby Japan sells part of its strategic reserves to New Zealand in the event of an emergency. New Zealand would be required to pay the market price for the oil, plus negotiated option fees for the amount of oil previously held for them by Japan.\n\nSouth Korea and Japan have agreed to share their oil reserves in the event of an emergency.\n\nAccording to the 1975 Second Sinai withdrawal document signed by the United States and Israel, in an emergency the U.S. is obligated to make oil available for sale to Israel for a period of up to five years.\n\nFrance, Germany and Italy have an oil-sharing agreement in place that allows them to buy oil from each other in the event of an emergency.\nIn 1968, the six members of the European Economic Community – Belgium, France, Germany, Italy, Luxembourg and the Netherlands – agreed to maintain a minimum level of crude oil stocks and oil products corresponding to 65 days' worth of domestic consumption. In 1972, this obligation was raised to 90 days.\n\nKenya is setting up a Strategic Fuel Reserve, similar to that of cereals. The stocks would be procured by the National Oil Corporation of Kenya and stored by the Kenya Pipeline Company Limited.\n\nMalawi is considering creating a 22-day reserve of fuel, which is an expansion from the current five-day reserve. The government is planning to build storage facilities in the provinces of Chipoka and Mchinji as well as Kamuzu International Airport.\n\nSouth Africa has an SPR managed by PetroSA. The main facility is the Saldanha Bay oil storage facility, which is a major transit point for oil shipping. Saldanha Bay's six in-ground concrete storage tanks give the facility a storage capacity of .\n\nIn 2007, China announced the expansion of its crude reserves into a two-part system. China's reserves would consist of a government-controlled strategic reserve complemented by mandated commercial reserves. The government-controlled reserves are being completed in three stages. Phase one consisted of a reserve, mostly completed by the end of 2008. The second phase of the government-controlled reserves with an additional was to be completed by 2011. Recently, Zhang Guobao, head of the National Energy Administration, stated that there will be a third phase that will expand reserves by with the goal of increasing China's SPR to 90 days of supply by 2020.\n\nThe planned state reserves of together with the planned enterprise reserves of will provide around 90 days of consumption or a total of .\n\nIn 2003, India started development on a strategic crude oil reserve sized at , enough to provide two weeks of consumption. Petroleum stocks have been transferred from the Indian Oil Corporation (IndianOil) to the Oil Industry Development Board (OIDB). The OIDB then created the Indian Strategic Petroleum Reserves Ltd (ISPRL) to serve as the controlling government agency for the strategic reserve.\n\nThe facilities are located at:\n\nOn 21 December 2011, a senior oil ministry official announced that India was planning to augment its crude reserve capacity to 132 million barrels by 2020.\n\nAs of 2010, Japan has an SPR composed of the following three types of stockpiles:\nThe state-controlled reserves and the privately held stockpiles total about . The Japanese SPR is run by the Japan Oil, Gas and Metals National Corporation.\n\nIn South Korea, refineries, specified distributors, and importers, are obliged to hold from 40 days to 60 days of their daily import, sale, or refined production, based on the previous 12 months. At the end of 2010, South Korea possessed a total storage capacity of 286 million barrels (45.5 million cubic meters), composed of 146 mb of South Korea National Oil Corporation's facilities used for government stocks and international joint oil stockpiling, and 140 mb used for industry operation and mandatory industry stocks. South Korea's oil stocks in terms of days of net imports have consistently been above 160 days since January 2009, hitting the country’s historical record of 240 days (124 days of government stocks and 117 days of industry stocks) in March 2014.\n\nThe Philippines had plans for a National Petroleum Strategic Reserve by 2010 with an approximate size of .\n\nSingapore does not have any oil reserves.\n\nTaiwan has an SPR with a 1999-reported size of . Taiwan's refiners (Kaohsiung ; Ta-Lin ; Tao-Yuan ; Mailiao 150,000 bbl/d) are also required to store at least 30 days of petroleum stocks. As of 2005, these mandated commercial reserves total of strategic petroleum stocks.\n\nThailand increased the size of its SPR from 60 days to 70 days of consumption in 2006.\n\nPakistan has announced plans for a 20-day emergency reserve.\n\nIn the European Union, according to Council Directive 68/414/EEC of 20 December 1968, all 28 member states are required to have a strategic petroleum reserve within the territory of the E.U. equal to at least 90 days of average domestic consumption.\n\nThe Czech Republic has a four-tank SPR facility in Nelahozeves run by the company CR Mero. The Czech SPR is equal to 100 days of consumption or .\n\nDenmark has a reserve equal to 81 days of consumption (about 1.4 million tonnes). Not counting reserves held by the military defence.\n\nFinland has an SPR with an approximate size of .\n\nFrance has an SPR with an approximate size of . As of 2000, jet fuel stocks for at least 55 days of consumption were required, with half of those stocks controlled by the \"Société Anonyme de Gestion des Stocks de Sécurité\" (SAGESS) and the other half controlled by producers.\n\nGermany created the Federal Oil Reserve in 1970, located in the Etzel salt caverns near Wilhelmshaven in northern Germany, with an initial size of . The current German Federal Oil Reserve and the Erdölbevorratungsverband (EBV) (the German stockholding company) mandates that refiners must keep 90 days of stock on hand, giving Germany an approximate reserve size of as of 1997. The German SPR is the largest in Europe.\n\nHungary has an SPR equal to approximately 90 days of consumption or .\n\nIreland has approximately 31 days of oil stocks in Ireland and another nine days of oil stocks held in other EU members states. Additionally, it has stock tickets (contracts with a third party whereby the government has the option of purchasing oil in the event of an emergency) and stocks held by large industry or large consumers. In total, Ireland has approximately 100 days' worth of oil at its disposal.\n\nThe Netherlands maintains a stockpile equal to 90 days of net oil imports. In 2013, this was about four million tonnes of oil.\n\nPoland has an SPR equal to approximately 70 days of consumption. Another facility holding 20 days of consumption was completed in 2008. Poland also requires oil companies to maintain reserves sufficient to provide 73 days of consumption.\n\nPortugal has an SPR with an approximate size of .\n\nSlovakia has an SPR with an approximate size of .\n\nSpain has an SPR with an approximate size of .\n\nSweden has an SPR with an approximate size of .\n\nThe United Kingdom recently drew up plans to create its own strategic fuel reserves utilizing Steven Brown as an agreement agent.\n\nAs of 2011, Russia is accumulating strategic reserves of refined oil products to be held by Rosneftegaz, a state-owned company. The reserves will be held at commercial refineries, Transneft facilities and state reserve facilities. The current planned size is .\n\nSwitzerland has SPRs consisting of gas, diesel, jet fuel and heating oil for 4.5 months of consumption. The reserves were created in the 1940s.\n\nIn April 2006, the Fars News Agency reported that Iran was planning to create an SPR. The National Iranian Oil Company (NIOC) began construction of 15 crude oil storage tanks with a capacity of . In August 2008, Iran announced plans to expand the SPR with a new facility on Kharg Island with four tanks holding each. Iran's SPR facilities are:\n\nKuwait has a joint stockpile located in South Korea. The deal gives South Korea first rights to purchase the oil. As of 2006, the size of the stockpile is .\n\nAs of 1975, Israel is believed to have a strategic oil reserve equal to 270 days of consumption.\n\nJordan has strategic oil reserves equal to 60 days of consumption or .\n\nThe United States has the world's largest reported Strategic Petroleum Reserve with a total capacity of 727 million barrels. If completely filled, the U.S. SPR could theoretically replace about 60 days of oil imports. The U.S. is estimated to import approximately of crude oil. According to the U.S. Department of Energy, the facilities' maximum flow rate is limited to approximately when filled to maximum capacity, declining as the reserve is emptied. The reserves are kept in salt caverns located at:\n\nThe U.S. also has the Northeast Home Heating Oil Reserve to supply northeast home owners with heating oil if there is a shortage.\n\nAs of 2008, New Zealand has a strategic reserve with a size of 170,000 tons or . Much of this reserve is based upon ticketed option contracts with Australia, Japan, the United Kingdom and the Netherlands, which allow for guaranteed purchases of petroleum in the event of an emergency.\n\nAs of 2008, Australia holds three weeks of petroleum, instead of the allotted 90 days that was agreed upon, according to the study 'Liquid Fuel Security' authored by Air Vice-Marshal John Blackburn, AO (retired).\n\n\nFor more on APEC strategic reserves:\nFor more info on the IEA reserves:\n"}
{"id": "34197579", "url": "https://en.wikipedia.org/wiki?curid=34197579", "title": "Gregory Rift", "text": "Gregory Rift\n\nThe Gregory Rift is the eastern branch of the East African Rift fracture system. The rift is being caused by the separation of the Somali plate from the Nubian plate, driven by a thermal plume. Although the term is sometimes used in the narrow sense of the Kenyan Rift, the larger definition of the Gregory Rift is the set of faults and grabens extending southward from the Gulf of Aden through Ethiopia and Kenya into Northern Tanzania, passing over the local uplifts of the Ethiopian and Kenyan domes.\nAncient fossils of early hominins, the ancestors of humans, have been found in the southern part of the Gregory Rift.\n\nThe Gregory Rift is named in honour of the British geologist John Walter Gregory who explored the geology of the rift in 1892-93 and 1919.\n\nThe Gregory Rift lies within the Mozambique belt, often considered to be the remains of an orogenic system similar to the Himalayas. This belt runs from Ethiopia through Kenya, Tanzania and Mozambique.\nThe rift is widest at the northern end in the Afar region, narrowing to a few kilometers in northern Tanzania, then splaying out in the North Tanzania Divergence.\nThe Gregory Rift has shoulders rising over above sea level, above the inner part of the graben.\nThe Tanzanian portion includes Mount Kilimanjaro, the highest mountain in Africa, and the huge caldera of Ngorongoro.\nThis portion also contains Ol Doinyo Lengai, the world's only active carbonatite volcano.\n\nLakes in the rift other than Lake Turkana are mostly small and shallow, some with fresh water but many being saline. The thickness of lake sediments is mostly unknown. In Lake Turkana they seem to be at most thick, in the Baringo - Bogoria half-graben from to thick and in the Afar depression up to thick.\n\nThe first geologist to explore the region was Joseph Thomson, a member of an expedition in 1879–1880 sponsored by the Royal Geographical Society of Britain. From his observations he deduced the existence of a great fault.\n\nThomson returned in 1883, traveling through the rift valley in Kenya from Mount Longonot to Lake Baringo. Describing the valley around this lake he said: \"Imagine if you can a trough or depression 3300 feet above sea level, and twenty miles broad, the mountains rising with very great abruptness on both sides to a height of 9000 feet\". John Walter Gregory visited central Kenya in 1893 and again in 1919. His 1896 book \"The Great Rift Valley\" is considered a classic. Gregory was the first to use the term \"rift valley\", which he defined as \"a linear valley with parallel and almost vertical sides, which has fallen owing to a series of parallel faults\".\n\nIn 1913 the German geologist Hans Reck made the first study of the strata in the Olduvai Gorge to the west of the Crater Highlands. He brought a large collection of mammalian fossils back to Berlin. In 1928 Louis Leakey, the anthropologist, visited Berlin, where he saw that some of Reck's materials were artifacts. Leakey began exploring Olduvai in the 1930s and collecting material that has led to the site being recognized as an important center of early hominin occupation.\n\nVolcanism and rifting started in Kenya in the northern region of Turkana between 40 and 35 million years ago and then spread north and south. To the south volcanism and rifting happened together, first in other parts of northern Kenya around 30 million years ago, then around 15 million years ago in the central part of the Kenyan Rift, 12 million years ago in southern Kenya and 8 million years ago in northern Tanzania.\nWhen rifting reached the Tanzanian Craton, the rift split into the eastern Gregory Rift and the western Albertine Rift, which are separated by the wide East African Plateau. Large shield volcanoes near the margins of the craton and in the adjacent Mozambique belt issued large volumes of basaltic to trachytic magmatism between five and one million years ago, with faulting around 1.2 million years ago.\n\nVolcanic activity started in the central Ethiopian plateau around 30 million years ago, long before rifting began.\nThe first period of activity deposited flood basalts and rhyolites from to thick.\nUplift of the Ethiopian plateau began around this time or soon after.\nBetween 30 million and 10 million years ago synrift shield volcanoes deposited from to of additional material over the Ethiopian flood basalts.\nRifting in Ethiopia began about 18 million years ago in the southwest and 11 million years ago in northern parts of the Main Ethiopian Rift as the opening of the Gregory rift caused the Afar Triple Junction to form.\nVolcanism from the Middle Pleistocene onward formed a chain of volcanoes along the floor of the rift throughout its length, dividing it into separate valleys.\n\nThere are some indications that the lithosphere may have thinned below the Gregory rift, although based on basalt geochemistry the lithosphere is at least thick below the south of Kenya.\nThe Gregory rift is oriented NS, and in the past the minimum horizontal tectonic stress direction was EW, the direction of extension. The alignment of rows of small vents, cones, domes and collapse pits in the Suswa, Silali and Kinangop Plateau regions support this theory.\nHowever, data from oil and gas exploration wells in Kenya, vents in volcanic shields to the east of the rift at Huri Hills, Mount Marsabit and Nyambeni Hills and recent small cones at Suswa and east of the Silali caldera all indicate that the minimum horizontal stress direction has changed to NW-SE within the last half million years.\n\n"}
{"id": "12991758", "url": "https://en.wikipedia.org/wiki?curid=12991758", "title": "High Rock (Ontario)", "text": "High Rock (Ontario)\n\nHigh Rock is a hill in Nipissing District of Northeastern Ontario, Canada, located southwest of the village of Temagami. The highest point on High Rock Island, it is one of many scenic viewpoints on Lake Temagami.\n\nHigh Rock, along with Devil Mountain and Maple Mountain, is considered sacred to the Temagami First Nation.\n"}
{"id": "44981414", "url": "https://en.wikipedia.org/wiki?curid=44981414", "title": "Ice Energy", "text": "Ice Energy\n\nIce Energy is an energy storage company serving utility companies in California. Its main product is the Ice Bear system, developed for small to mid-sized commercial buildings. The Ice Bear freezes water at night when electricity is cheaper and uses that ice for space cooling during the day.\n\nIce Energy was founded in 2003. In August 2014, Ice Energy revealed a version of the Ice Bear for single-family homes called the Ice Cub. In November, the company won sixteen contracts with Southern California Edison. The contracts totaled 25.6 megawatts.\n\n"}
{"id": "24697624", "url": "https://en.wikipedia.org/wiki?curid=24697624", "title": "Jani Aaltonen", "text": "Jani Aaltonen\n\nJani Aaltonen (born 22 January 1990) is a Finnish footballer.\n"}
{"id": "3301025", "url": "https://en.wikipedia.org/wiki?curid=3301025", "title": "List of Lepidoptera that feed on clovers", "text": "List of Lepidoptera that feed on clovers\n\nClovers, \"Trifolium\" species, are used as food plants by the caterpillars of a number of Lepidoptera (butterflies and moths), including:\n\nSpecies which feed exclusively on \"Trifolium\"\n\n\nSpecies which feed on \"Trifolium\" and other plants\n\n\n"}
{"id": "7105739", "url": "https://en.wikipedia.org/wiki?curid=7105739", "title": "List of Salticidae species (T–V)", "text": "List of Salticidae species (T–V)\n\nList of Salticidae species T–V includes all described species with a scientific name starting from T to V of the spider family Salticidae as of December 18, 2016.\n\n\"Tabuina\" \n\n\"Tacuna\" \n\n\"Taivala\" \n\n\"Talavera\" \n\n\"Tamigalesus\" \n\n\"Tanybelus\" \n\n\"Tanzania\" \n\n\"Tara\" \n\n\"Taraxella\" \n\n\"Tarkas\" \n\n\"Tarne\" \n\n\"Tarodes\" \n\n\"Tasa\" \n\n\"Tatari\" \n\n\"Tauala\" \n\n\"Telamonia\" \n\n\"Terralonus\" \n\n\"Thammaca\" \n\n\"Theriella\" \n\n\"Thianella\" \n\nSee \"Thiania\" § Species.\n\n\"Thiodina\" \n\n\"Thiratoscirtus\" \n\n\"Thorelliola\" \n\n\"Thrandina\" \n\nSee \"Thyene\" § Species.\n\n\"Thyenillus\" \n\nSee \"Thyenula\" § Species.\n\n\"Tisaniba\" \n\n\"Titanattus\" \n\n\"Toloella\" \n\n\"Tomobella\" \n\n\"Tomocyrba\" \n\n\"Tomomingi\" \n\n\"Toticoryx\" \n\n\"Toxeus\" \n\n\"Triggella\" \n\n\"Trite\" \n\n\"Truncattus\" \n\n\"Trydarssus\" \n\n\"Tullgrenella\" \n\n\"Tulpius\" \n\n\"Tusitala\" \n\n\"Tutelina\" \n\n\"Tuvaphantes\" \n\n\"Tylogonus\" \n\n\"Udalmella\" \n\n\"Udvardya\" \n\n\"Ugandinella\" \n\n\"Uluella\" \n\n\"Ureta\" \n\n\"Uroballus\" \n\n\"Urogelides\" \n\n\"Uxuma\" \n\n\"Vailimia\" \n\n\"Variratina\" \n\n\"Vatovia\" \n\n\"Veissella\" \n\n\"Viciria\" \n\n\"Vinnius\" \n\n\"Viribestus\" \n\n\"Viroqua\" \n\n"}
{"id": "6185870", "url": "https://en.wikipedia.org/wiki?curid=6185870", "title": "List of Sites of Special Scientific Interest in Western Isles North", "text": "List of Sites of Special Scientific Interest in Western Isles North\n\nThe following is a list of Sites of Special Scientific Interest in the Western Isles North Area of Search. For Western Isles South see List of SSSIs in Western Isles South. For SSSIs elsewhere in Scotland, see List of SSSIs by Area of Search.\n\n"}
{"id": "32630150", "url": "https://en.wikipedia.org/wiki?curid=32630150", "title": "List of ancient oceans", "text": "List of ancient oceans\n\nThis is a list of former oceans that disappeared due to tectonic movements and other geographical and climatic changes. In alphabetic order:\n\n\n"}
{"id": "8571568", "url": "https://en.wikipedia.org/wiki?curid=8571568", "title": "List of stars in Vulpecula", "text": "List of stars in Vulpecula\n\nThis is the list of notable stars in the constellation Vulpecula, sorted by decreasing brightness.\n\n\n"}
{"id": "428089", "url": "https://en.wikipedia.org/wiki?curid=428089", "title": "Lists of galaxies", "text": "Lists of galaxies\n\nThis is a list of lists of galaxies.\n\n\n\n"}
{"id": "8230144", "url": "https://en.wikipedia.org/wiki?curid=8230144", "title": "Mammal Species of the World", "text": "Mammal Species of the World\n\nMammal Species of the World: A Taxonomic and Geographic Reference is a standard reference work in mammalogy giving descriptions and bibliographic data for the known species of mammals. It is now in its third edition, published in late 2005, which was edited by Don E. Wilson and DeeAnn M. Reeder.\n\nAn online version is hosted by Bucknell University, from which the names of the species can be downloaded as a custom dictionary. A partial online version is available at Google Books (see \"External links\" below).\n\nThe Checklist Committee is charged with compiling and updating MSW. In its Annual Report for 2015, the Committee noted that it is under contract with Johns Hopkins Press for the 4th edition of MSW, which will be edited by DeeAnn M. Reeder and Kristofer M. Helgen. The database has been made editable for the authors, leading to more frequent website updates. The publication was due in 2017.\n\n"}
{"id": "1675601", "url": "https://en.wikipedia.org/wiki?curid=1675601", "title": "Manifold vacuum", "text": "Manifold vacuum\n\nManifold vacuum, or engine vacuum in an internal combustion engine is the difference in air pressure between the engine's intake manifold and Earth's atmosphere.\n\nManifold vacuum is an effect of a piston's movement on the induction stroke and the choked flow through a throttle in the intake manifold of an engine. It is a measure of the amount of restriction of airflow through the engine, and hence of the unused power capacity in the engine. In some engines, the manifold vacuum is also used as an auxiliary power source to drive engine accessories and for the crankcase ventilation system.\n\nManifold vacuum should not be confused with venturi vacuum, which is an effect exploited in carburetors to establish a pressure difference roughly proportional to mass airflow and to maintain a somewhat constant air/fuel ratio. It is also used in light airplanes to provide airflow for pneumatic gyroscopic instruments.\n\nThe rate of airflow through an internal combustion engine is an important factor determining the amount of power the engine generates. Most gasoline engines are controlled by limiting that flow with a throttle that restricts intake airflow, while a diesel engine is controlled by the amount of fuel supplied to the cylinder, and so has no \"throttle\" as such. Manifold vacuum is present in all naturally aspirated engines that use throttles (including carbureted and fuel injected gasoline engines using the Otto cycle or the two-stroke cycle; diesel engines do not have throttle plates).\n\nThe mass flow through the engine is the product of the rotation rate of the engine, the displacement of the engine, and the density of the intake stream in the intake manifold. In most applications the rotation rate is set by the application (engine speed in a vehicle or machinery speed in other applications). The displacement is dependent on the engine geometry, which is generally not adjustable while the engine is in use (although a handful of models do have this feature, see variable displacement). Restricting the input flow reduces the density (and hence pressure) in the intake manifold, reducing the amount of power produced. It is also a major source of engine drag (see engine braking), as the engine must pump material from the low-pressure intake manifold into the exhaust manifold (at ambient atmospheric pressure).\n\nWhen the throttle is opened (in a car, the accelerator pedal is depressed), ambient air is free to fill the intake manifold, increasing the pressure (filling the vacuum). A carburetor or fuel injection system adds fuel to the airflow in the correct proportion, providing energy to the engine. When the throttle is opened all the way, the engine's air induction system is exposed to full atmospheric pressure, and maximum airflow through the engine is achieved. In a naturally aspirated engine, output power is limited by the ambient barometric pressure. Superchargers and turbochargers boost manifold pressure above atmospheric pressure.\n\nModern engines use a manifold absolute pressure (abbreviated as \"MAP\") sensor to measure air pressure in the intake manifold. Manifold absolute pressure is one of a multitude of parameters used by the engine control unit (ECU) to optimize engine operation. It is important to differentiate between absolute and gauge pressure when dealing with certain applications, particularly those that experience changes in elevation during normal operation. \n\nMotivated by government regulations mandating reduction of fuel consumption (in the USA) or reduction of carbon dioxide emissions (in Europe), passenger cars and light trucks have been fitted with a variety of technologies (downsized engines; lockup, multi-ratio and overdrive transmissions; variable valve timing, forced induction, diesel engines, et al.) which render manifold vacuum inadequate or unavailable. Electric vacuum pumps are now commonly used for powering pneumatic accessories.\n\nManifold vacuum is caused by a different phenomenon than venturi vacuum, which is present inside carburetors. Venturi vacuum is caused by the venturi effect which, for fixed ambient conditions (air density and temperature), depends on the total mass flow through the carburetor. In engines that use carburetors, the venturi vacuum is approximately proportional to the total mass flow through the engine (and hence the total power output). As ambient pressure (altitude, weather) or temperature change, the carburetor may need to be adjusted to maintain this relationship.\n\nManifold pressure may also be \"ported\". Porting is selecting a location for the pressure tap within the throttle plate's range of motion. Depending on throttle position, a ported pressure tap may be either upstream or downstream of the throttle. As the throttle position changes, a \"ported\" pressure tap is selectively connected to either manifold pressure or ambient pressure. Antique (pre-OBD II) engines often used ported manifold pressure taps for ignition distributors and emission-control components.\n\nMost automobiles use four-stroke Otto cycle engines with multiple cylinders attached to a single inlet manifold. During the induction stroke, the piston descends in the cylinder and the intake valve is open. As the piston descends it effectively increases the volume in the cylinder above it, setting up low pressure. Atmospheric pressure pushes air through the manifold and carburetor or fuel injection system, where it is mixed with fuel. Because multiple cylinders operate at different times in the engine cycle, there is almost constant pressure difference through the inlet manifold from carburetor to engine.\n\nTo control the amount of fuel/air mix entering the engine, a simple butterfly valve (throttle plate) is generally fitted at the start of the intake manifold (just below the carburetor in carbureted engines). The butterfly valve is simply a circular disc fitted on a spindle, fitting inside the pipe work. It is connected to the accelerator pedal of the car, and is set to be fully open when the pedal is fully depressed and fully closed when the pedal is released. The butterfly valve often contains a small \"idle cutout\", a hole that allows small amounts of fuel/air mixture into the engine even when the valve is fully closed, or the carburetor has a separate air bypass with its own idle jet.\n\nIf the engine is operating under light or no load and low or closed throttle, there is high manifold vacuum. As the throttle is opened, the engine speed increases rapidly. The engine speed is limited only by the amount of fuel/air mixture that is available in the manifold. Under full throttle and light load, other effects (such as valve float, turbulence in the cylinders, or ignition timing) limit engine speed so that the manifold pressure can increase—but in practice, parasitic drag on the internal walls of the manifold, plus the restrictive nature of the venturi at the heart of the carburetor, means that a low pressure will always be set up as the engine's internal volume exceeds the amount of the air the manifold is capable of delivering.\n\nIf the engine is operating under heavy load at wide throttle openings (such as accelerating from a stop or pulling the car up a hill) then engine speed is limited by the load and minimal vacuum will be created. Engine speed is low but the butterfly valve is fully open. Since the pistons are descending more slowly than under no load, the pressure differences are less marked and parasitic drag in the induction system is negligible. The engine pulls air into the cylinders at the full ambient pressure.\n\nMore vacuum is created in some situations. On deceleration or when descending a hill, the throttle will be closed and a low gear selected to control speed. The engine will be rotating fast because the road wheels and transmission are moving quickly, but the butterfly valve will be fully closed. The flow of air through the engine is strongly restricted by the throttle, producing a strong vacuum on the engine side of the butterfly valve which will tend to limit the speed of the engine. This phenomenon, known as engine braking, is used to prevent acceleration or even to slow down with minimal or no brake usage (as when descending a long or steep hill). This vacuum braking should not be confused with compression braking (aka a \"Jake brake\"), or with exhaust braking, which are often used on large diesel trucks. Such devices are necessary for engine braking with a diesel as they lack a throttle to restrict the air flow enough to create sufficient vacuum to brake a vehicle.\n\nThis low (or negative) pressure can be put to uses. A pressure gauge measuring the manifold pressure can be fitted to give the driver an indication of how hard the engine is working and it can be used to achieve maximum momentary fuel efficiency by adjusting driving habits: minimizing manifold vacuum increases momentary efficiency. A weak manifold vacuum under closed-throttle conditions shows that the butterfly valve or internal components of the engine (valves or piston rings) are worn, preventing good pumping action by the engine and reducing overall efficiency.\n\nVacuum is often used to drive auxiliary systems on the vehicle. Vacuum-assist brake servos, for example, use atmospheric pressure pressing against the engine manifold vacuum to increase pressure on the brakes. Since braking is nearly always accompanied by the closing of the throttle and associated high manifold vacuum, this system is simple and almost foolproof. Vacuum tanks were installed on trailers to control their integrated braking systems.\n\nPrior to the introduction of Federal Motor Vehicle Safety Standards in the USA by the National Traffic and Motor Vehicle Safety Act of 1966, it was common to use manifold vacuum to drive windscreen wipers with a pneumatic motor. This system was cheap & simple but resulted in the comical yet unsafe effect of wipers which operate at full speed while the engine idles, operate around half speed while cruising, and stop altogether when the driver depresses the pedal fully. Vehicle HVAC systems also used manifold vacuum to drive actuators controlling airflow and temperature.\n\nAnother obsolete accessory is the \"Autovac\" fuel lifter which uses vacuum to raise fuel from the main tank to a small auxiliary tank, from which it flows by gravity to the carburetor. This eliminated the fuel pump which, in early cars, was an unreliable item.\n\nMany diesel engines do not have butterfly valve throttles. The manifold is connected directly to the air intake and the only suction created is that caused by the descending piston with no venturi to increase it, and the engine power is controlled by varying the amount of fuel that is injected into the cylinder by a fuel injection system. This assists in making diesels much more efficient than petrol engines.\n\nIf vacuum is required (vehicles that can be fitted with both petrol and diesel engines often have systems requiring it), a butterfly valve connected to the throttle can be fitted to the manifold. This reduces efficiency and is still not as effective as it is not connected to a venturi. Since low-pressure is only created on the overrun (such as when descending hills with a closed throttle), not over a wide range of situations as in a petrol engine, a vacuum tank is fitted.\n\nMost diesel engines now have a separate vacuum pump (\"exhauster\") fitted to provide vacuum at all times, at all engine speeds.\n\nMany new BMW petrol engines do not use a throttle in normal running, but instead use \"Valvetronic\" variable-lift intake valves to control the amount of air entering the engine. Like a diesel engine, manifold vacuum is practically non-existent in these engines and a different source must be utilised to power the brake servo.\n\n"}
{"id": "51784", "url": "https://en.wikipedia.org/wiki?curid=51784", "title": "Map projection", "text": "Map projection\n\nA map projection is a systematic transformation of the latitudes and longitudes of locations from the surface of a sphere or an ellipsoid into locations on a plane. Maps cannot be created without map projections. All map projections necessarily distort the surface in some fashion. Depending on the purpose of the map, some distortions are acceptable and others are not; therefore, different map projections exist in order to preserve some properties of the sphere-like body at the expense of other properties. There is no limit to the number of possible map projections.\n\nMore generally, the surfaces of planetary bodies can be mapped even if they are too irregular to be modeled well with a sphere or ellipsoid; see below. Even more generally, projections are a subject of several pure mathematical fields, including differential geometry, projective geometry, and manifolds. However, \"map projection\" refers specifically to a cartographic projection.\n\nMaps can be more useful than globes in many situations: they are more compact and easier to store; they readily accommodate an enormous range of scales; they are viewed easily on computer displays; they can facilitate measuring properties of the region being mapped; they can show larger portions of the Earth's surface at once; and they are cheaper to produce and transport. These useful traits of maps motivate the development of map projections.\n\nHowever, Carl Friedrich Gauss's Theorema Egregium proved that a sphere's surface cannot be represented on a plane without distortion. The same applies to other reference surfaces used as models for the Earth, such as oblate spheroids, ellipsoids and geoids. Since any map projection is a representation of one of those surfaces on a plane, all map projections distort. Every distinct map projection distorts in a distinct way. The study of map projections is the characterization of these distortions.\n\n\"Projection\" is not limited to perspective projections, such as those resulting from casting a shadow on a screen, or the rectilinear image produced by a pinhole camera on a flat film plate. Rather, any mathematical function transforming coordinates from the curved surface to the plane is a projection. Few projections in actual use are perspective.\n\nFor simplicity, most of this article assumes that the surface to be mapped is that of a sphere. In reality, the Earth and other large celestial bodies are generally better modeled as oblate spheroids, whereas small objects such as asteroids often have irregular shapes. These other surfaces can be mapped as well. Therefore, more generally, a map projection is any method of \"flattening\" a continuous curved surface onto a plane.\n\nMany properties can be measured on the Earth's surface independent of its geography. Some of these properties are:\n\nMap projections can be constructed to preserve at least one of these properties, though only in a limited way for most. Each projection preserves, compromises, or approximates basic metric properties in different ways. The purpose of the map determines which projection should form the base for the map. Because many purposes exist for maps, a diversity of projections have been created to suit those purposes.\n\nAnother consideration in the configuration of a projection is its compatibility with data sets to be used on the map. Data sets are geographic information; their collection depends on the chosen datum (model) of the Earth. Different datums assign slightly different coordinates to the same location, so in large scale maps, such as those from national mapping systems, it is important to match the datum to the projection. The slight differences in coordinate assignation between different datums is not a concern for world maps or other vast territories, where such differences get shrunk to imperceptibility.\n\nThe classical way of showing the distortion inherent in a projection is to use Tissot's indicatrix. For a given point, using the scale factor \"h\" along the meridian, the scale factor \"k\" along the parallel, and the angle \"θ′\" between them, Nicolas Tissot described how to construct an ellipse that characterizes the amount and orientation of the components of distortion. By spacing the ellipses regularly along the meridians and parallels, the network of indicatrices shows how distortion varies across the map.\n\nThe creation of a map projection involves two steps:\n\nSome of the simplest map projections are literal projections, as obtained by placing\na light source at some definite point relative to the globe and projecting its features onto a specified surface. This is not the case for most projections, which are defined only in terms of mathematical formulae that have no direct geometric interpretation. However, picturing the light source-globe model can be helpful in understanding the basic concept of a map projection\n\nA surface that can be unfolded or unrolled into a plane or sheet without stretching, tearing or shrinking is called a \"developable surface\". The cylinder, cone and the plane are all developable surfaces. The sphere and ellipsoid do not have developable surfaces, so any projection of them onto a plane will have to distort the image. (To compare, one cannot flatten an orange peel without tearing and warping it.)\n\nOne way of describing a projection is first to project from the Earth's surface to a developable surface such as a cylinder or cone, and then to unroll the surface into a plane. While the first step inevitably distorts some properties of the globe, the developable surface can then be unfolded without further distortion.\n\nOnce a choice is made between projecting onto a cylinder, cone, or plane, the aspect of the shape must be specified. The aspect describes how the developable surface is placed relative to the globe: it may be \"normal\" (such that the surface's axis of symmetry coincides with the Earth's axis), \"transverse\" (at right angles to the Earth's axis) or \"oblique\" (any angle in between).\n\nThe developable surface may also be either \"tangent\" or \"secant\" to the sphere or ellipsoid. Tangent means the surface touches but does not slice through the globe; secant means the surface does slice through the globe. Moving the developable surface away from contact with the globe never preserves or optimizes metric properties, so that possibility is not discussed further here.\n\nTangent and secant lines (\"standard lines\") are represented undistorted. If these lines are a parallel of latitude, as in conical projections, it is called a \"standard parallel\". The \"central meridian\" is the meridian to which the globe is rotated before projecting. The central meridian (usually written \"λ\") and a parallel of origin (usually written \"φ\") are often used to define the origin of the map projection.\n\nA globe is the only way to represent the earth with constant scale throughout the entire map in all directions. A map cannot achieve that property for any area, no matter how small. It can, however, achieve constant scale along specific lines.\n\nSome possible properties are:\n\nProjection construction is also affected by how the shape of the Earth or planetary body is approximated. In the following section on projection categories, the earth is taken as a sphere in order to simplify the discussion. However, the Earth's actual shape is closer to an oblate ellipsoid. Whether spherical or ellipsoidal, the principles discussed hold without loss of generality.\n\nSelecting a model for a shape of the Earth involves choosing between the advantages and disadvantages of a sphere versus an ellipsoid. Spherical models are useful for small-scale maps such as world atlases and globes, since the error at that scale is not usually noticeable or important enough to justify using the more complicated ellipsoid. The ellipsoidal model is commonly used to construct topographic maps and for other large- and medium-scale maps that need to accurately depict the land surface. Auxiliary latitudes are often employed in projecting the ellipsoid.\n\nA third model is the geoid, a more complex and accurate representation of Earth's shape coincident with what mean sea level would be if there were no winds, tides, or land. Compared to the best fitting ellipsoid, a geoidal model would change the characterization of important properties such as distance, conformality and equivalence. Therefore, in geoidal projections that preserve such properties, the mapped graticule would deviate from a mapped ellipsoid's graticule. Normally the geoid is not used as an Earth model for projections, however, because Earth's shape is very regular, with the undulation of the geoid amounting to less than 100 m from the ellipsoidal model out of the 6.3 million m Earth radius. For irregular planetary bodies such as asteroids, however, sometimes models analogous to the geoid are used to project maps from.\n\nA fundamental projection classification is based on the type of projection surface onto which the globe is conceptually projected. The projections are described in terms of placing a gigantic surface in contact with the earth, followed by an implied scaling operation. These surfaces are cylindrical (e.g. Mercator), conic (e.g. Albers), and plane (e.g. stereographic). Many mathematical projections, however, do not neatly fit into any of these three conceptual projection methods. Hence other peer categories have been described in the literature, such as pseudoconic, pseudocylindrical, pseudoazimuthal, retroazimuthal, and polyconic.\n\nAnother way to classify projections is according to properties of the model they preserve. Some of the more common categories are:\nBecause the sphere is not a developable surface, it is impossible to construct a map projection that is both equal-area and conformal.\n\nThe three developable surfaces (plane, cylinder, cone) provide useful models for understanding, describing, and developing map projections. However, these models are limited in two fundamental ways. For one thing, most world projections in use do not fall into any of those categories. For another thing, even most projections that do fall into those categories are not naturally attainable through physical projection. As L.P. Lee notes,\n\nLee's objection refers to the way the terms \"cylindrical\", \"conic\", and \"planar\" (azimuthal) have been abstracted in the field of map projections. If maps were projected as in light shining through a globe onto a developable surface, then the spacing of parallels would follow a very limited set of possibilities. Such a cylindrical projection (for example) is one which:\n\nWhere the light source emanates along the line described in this last constraint is what yields the differences between the various \"natural\" cylindrical projections. But the term \"cylindrical\" as used in the field of map projections relaxes the last constraint entirely. Instead the parallels can be placed according to any algorithm the designer has decided suits the needs of the map. The famous Mercator projection is one in which the placement of parallels does not arise by \"projection\"; instead parallels are placed how they need to be in order to satisfy the property that a course of constant bearing is always plotted as a straight line.\n\nA \"normal cylindrical projection\" is any projection in which meridians are mapped to equally spaced vertical lines and circles of latitude (parallels) are mapped to horizontal lines.\n\nThe mapping of meridians to vertical lines can be visualized by imagining a cylinder whose axis coincides with the Earth's axis of rotation. This cylinder is wrapped around the Earth, projected onto, and then unrolled.\n\nBy the geometry of their construction, cylindrical projections stretch distances east-west. The amount of stretch is the same at any chosen latitude on all cylindrical projections, and is given by the secant of the latitude as a multiple of the equator's scale. The various cylindrical projections are distinguished from each other solely by their north-south stretching (where latitude is given by φ):\n\nIn the first case (Mercator), the east-west scale always equals the north-south scale. In the second case (central cylindrical), the north-south scale exceeds the east-west scale everywhere away from the equator. Each remaining case has a pair of secant lines—a pair of identical latitudes of opposite sign (or else the equator) at which the east-west scale matches the north-south-scale.\n\nNormal cylindrical projections map the whole Earth as a finite rectangle, except in the first two cases, where the rectangle stretches infinitely tall while retaining constant width.\n\nPseudocylindrical projections represent the \"central\" meridian as a straight line segment. Other meridians are longer than the central meridian and bow outward, away from the central meridian. Pseudocylindrical projections map parallels as straight lines. Along parallels, each point from the surface is mapped at a distance from the central meridian that is proportional to its difference in longitude from the central meridian. Therefore, meridians are equally spaced along a given parallel. On a pseudocylindrical map, any point further from the equator than some other point has a higher latitude than the other point, preserving north-south relationships. This trait is useful when illustrating phenomena that depend on latitude, such as climate. Examples of pseudocylindrical projections include:\nThe HEALPix projection combines an equal-area cylindrical projection in equatorial regions with the Collignon projection in polar areas.\n\nThe term \"conic projection\" is used to refer to any projection in which meridians are mapped to equally spaced lines radiating out from the apex and circles of latitude (parallels) are mapped to circular arcs centered on the apex.\nWhen making a conic map, the map maker arbitrarily picks two standard parallels. Those standard parallels may be visualized as secant lines where the cone intersects the globe—or, if the map maker chooses the same parallel twice, as the tangent line where the cone is tangent to the globe. The resulting conic map has low distortion in scale, shape, and area near those standard parallels. Distances along the parallels to the north of both standard parallels or to the south of both standard parallels are stretched; distances along parallels between the standard parallels are compressed. When a single standard parallel is used, distances along all other parallels are stretched.\n\nConic projections that are commonly used are:\n\n\nAzimuthal projections have the property that directions from a central point are preserved and therefore great circles through the central point are represented by straight lines on the map. These projections also have radial symmetry in the scales and hence in the distortions: map distances from the central point are computed by a function \"r\"(\"d\") of the true distance \"d\", independent of the angle; correspondingly, circles with the central point as center are mapped into circles which have as center the central point on the map.\n\nThe mapping of radial lines can be visualized by imagining a plane tangent to the Earth, with the central point as tangent point.\n\nThe radial scale is \"r′\"(\"d\") and the transverse scale \"r\"(\"d\")/(\"R\" sin ) where \"R\" is the radius of the Earth.\n\nSome azimuthal projections are true perspective projections; that is, they can be constructed mechanically, projecting the surface of the Earth by extending lines from a point of perspective (along an infinite line through the tangent point and the tangent point's antipode) onto the plane:\n\nOther azimuthal projections are not true perspective projections:\n\nConformal, or orthomorphic, map projections preserve angles locally, implying that they map infinitesimal circles of constant size anywhere on the Earth to infinitesimal circles of varying sizes on the map. In contrast, mappings that are not conformal distort most such small circles into ellipses of distortion. An important consequence of conformality\nis that relative angles at each point of the map are correct, and the local scale (although varying throughout the map) in every direction around any one point is constant. These are some conformal projections:\n\nEqual-area maps preserve area measure, generally distorting shapes in order to do that. Equal-area maps are also called \"equivalent\" or \"authalic\". These are some projections that preserve area:\nThese are some projections that preserve distance from some standard point or line:\n\nGreat circles are displayed as straight lines:\n\nDirection to a fixed location B (the bearing at the starting location A of the shortest route) corresponds to the direction on the map from A to B:\n\nCompromise projections give up the idea of perfectly preserving metric properties, seeking instead to strike a balance between distortions, or to simply make things \"look right\". Most of these types of projections distort shape in the polar regions more than at the equator. These are some compromise projections:\n\nThe mathematics of projection do not permit any particular map projection to be \"best\" for everything. Something will always be distorted. Thus, many projections exist to serve the many uses of maps and their vast range of scales. \n\nModern national mapping systems typically employ a transverse Mercator or close variant for large-scale maps in order to preserve conformality and low variation in scale over small areas. For smaller-scale maps, such as those spanning continents or the entire world, many projections are in common use according to their fitness for the purpose, such as Winkel tripel, Robinson and Mollweide. Reference maps of the world often appear on compromise projections. Due to distortions inherent in any map of the world, the choice of projection becomes largely one of aesthetics.\n\nThematic maps normally require an equal area projection so that phenomena per unit area are shown in correct proportion.\nHowever, representing area ratios correctly necessarily distorts shapes more than many maps that are not equal-area. \n\nThe Mercator projection, developed for navigational purposes, has often been used in world maps where other projections would have been more appropriate. This problem has long been recognized even outside professional circles. For example, a 1943 \"New York Times\" editorial states:\n\nA controversy in the 1980s over the Peters map motivated the American Cartographic Association (now Cartography and Geographic Information Society) to produce a series of booklets (including \"Which Map Is Best\") designed to educate the public about map projections and distortion in maps. In 1989 and 1990, after some internal debate, seven North American geographic organizations adopted a resolution recommending against using any rectangular projection (including Mercator and Gall–Peters) for reference maps of the world.\n\n\nNotes\n\n"}
{"id": "789632", "url": "https://en.wikipedia.org/wiki?curid=789632", "title": "Marc Sleen", "text": "Marc Sleen\n\nMarcel Honoree Nestor, \"ridder\" Neels (30 December 1922 – 6 November 2016), known as Marc Sleen, was a Belgian cartoonist. He was mostly known for his comic \"The Adventures of Nero and Co.\", but also created gag comics like \"Piet Fluwijn en Bolleke\", \"De Lustige Kapoentjes\", \"Doris Dobbel\", \"Oktaaf Keunink\" and \"De Ronde van Frankrijk\".\n\nSleen was one of the most celebrated comics artists in his home country. His work is admired for its absurd and sometimes satirical comedy, as well for the fact that he worked completely singlehandedly without any assistance for 45 years on end, a feat that landed him a spot in \"The Guinness Book of Records\" in 1992. (This feat has been surpassed since by Jim Russell's \"The Potts\", which ran for 62 years.) He was one of the few comics artists in Belgium who had a museum dedicated to his work.\n\nMarc Sleen was born as Marcel Neels in Gentbrugge, near Ghent. He studied drawing in Ghent. During the Second World War he was imprisoned by Nazi soldiers in Fort Breendonk because his brother worked for the resistance. He was tortured and put in the death cell, but saved by the fact that after D-Day the officers moved all the prisoners to a different prison, where he could escape. In 1944 he started to work as a political caricaturist in the Flemish newspaper \"De Standaard\". He also contributed illustrations and short comics for the newspaper and the youth supplement, and made illustrations and his first comics for the magazine \"Ons Volk\".\n\nIn October 1947, Marc Sleen started a new series, \"The adventures of detective Van Zwam\" in the newspaper \"De Nieuwe Gids\". In the first adventure Detective Van Zwam encounters a fool who thinks he is emperor Nero. After he regains his senses, they continue calling him Nero and slowly he became the star of the series. The name changes accordingly to \"The adventures of detective Van Zwam and Nero\" and after nine stories to \"The adventures of Nero and co\".\n\nThe series appeared for 55 years with a rhythm of two strips every day. This was typical for the Flemish comic tradition, as with \"Suske en Wiske\" (\"Spike and Suzy\"). \"Nero\" became well known for its ironic humour and references to current affairs. For instance, in the album, \"Het Vredesoffensief Van Nero\" (\"Nero's Peace Offensive\") (1951), Nero visits Joseph Stalin to make him drink an elixir that will make him a pacifist.\n\nBesides \"Nero\" Sleen drew many other comic strip series, many of them gag-a-day comics, for magazines like \"'t Kapoentje\" and \"Ons Volkske\". Among the most well known were \"Piet Fluwijn en Bolleke\" (1947–1965), \"Doris Dobbel\" (1950–1965), \"Oktaaf Keunink\" (1952–1965) and \"De Lustige Kapoentjes\". Sleen also drew a daily cartoon during the Tour de France from 1947 until 1965, called \"De Ronde van Frankrijk\".\nBetween 1950 and 1965 Sleen published \"Nero\" in \"Het Volk\", after which he moved to \"De Standaard\". This caused a huge copyright controversy, as several newspapers fought over the rights over his syndicated comics. Thousands of readers switched from \"Het Volk\" to \"De Standaard\", just to follow his adventures in the newspaper. After that switch, he dropped all other series and devoted himself solely to \"Nero\".\n\nFrom 1992 to 2002, he was aided by Dirk Stallaert, a young Flemish comic artist, and at first the intention was to let Stallaert continue the series after Marc Sleen retired. But in the end, Stallaert didn't feel ready to continue it alone, and at the end of 2002, at the age of 80, Marc Sleen ended his career as a comics artist.\n\nSleen designed album covers for records by Flemish actor, comedian and singer Jef Burm. Burm was a former school mate of his.\n\nMarc Sleen was also known as a traveller and animal friend. He made 35 safaris to Africa between 1961 and 1991, making more than 20 documentaries for the Vlaamse Radio- en Televisieomroep, mostly for the TV show \"Allemaal Beestjes\". A few books and records about his safaris appeared as well. Many of his comics featured animals and countries he has visited.\n\nMarc Sleen is commonly considered one of the big names of the Flemish comics, together with Willy Vandersteen and Jef Nys.\n\nIn 2005 he was selected as one of the 111 nominees for the title \"The Greatest Belgian\" (De Grootste Belg) in the Flemish edition. He ended in 48th place.\n\nOn June 19, 2009, a museum dedicated to his life and career was opened in Brussels: the Marc Sleen Museum. Both Marc Sleen as well as King Albert II of Belgium were present. The king was a fan of \"Nero\" since his youth and both he and Baudouin of Belgium learned Dutch by reading \"Nero\".\n\nSleen died at the age of 93 on the evening of November 6, 2016. He was buried in the Campo Santo in Ghent.\n\n\nStatues of his creations have been erected in Turnhout (1991), Hoeilaart (1994) and Middelkerke (1997). An exclusive museum opposite the Belgian Centre for Comic Strip Art is devoted to his work .\n\nHis comics were drawn rapidly in a \"flexible and loose\" style. They can be divided in one-page or one-strip gag series like \"Piet Fluwijn\" and \"De Lustige Kapoentjes\", and humorous adventure comics of book length (generally between 32 and 64 pages) like \"Stropke en Flopke\" and \"Nero\".\n\n\n"}
{"id": "26078087", "url": "https://en.wikipedia.org/wiki?curid=26078087", "title": "Marine Science Co-ordination Committee", "text": "Marine Science Co-ordination Committee\n\nThe Marine Science Co-ordination Committee (MSCC) is a UK government committee composed of representatives from public-funded bodies who have a remit to undertake marine scientific research. There are also three non-executive members.\nThe role of the MSCC is to implement the pan-UK Marine Science Strategy, which covers the period 2010-2025.\nThe secretariat for the MSCC is hosted by the Department of Environment, Food and Rural Affairs (Defra) and the Natural Environment Research Council's National Oceanography Centre.\n\nThe committee usually meets twice per year and was formed in 2008 in response to the recommendations of the House of Commons Select Committee Report 'Investigating the Oceans',and replaces the Inter-Agency Committee on Marine Science and Technology (IACMST). \n\nThe Marine Science Co-ordination Committee's work is overseen by a Ministerial Marine Science Group, representing UK and Devolved Governments.\n\nThe MSCC also has oversight of the Marine Environmental Data and Information Network (MEDIN) and the UK Underwater Sound Forum.\n\n"}
{"id": "1176971", "url": "https://en.wikipedia.org/wiki?curid=1176971", "title": "Micrometeorite", "text": "Micrometeorite\n\nA micrometeorite is essentially a micrometeoroid that has survived entry through Earth's atmosphere. The size of such a particle ranges from 50 µm to 2 mm. Usually found on Earth's surface, micrometeorites differ from meteorites in that they are smaller in size, more abundant, and different in composition. They are a subset of cosmic dust, which also includes the smaller interplanetary dust particles (IDPs).\n\nMicrometeorites enter Earth's atmosphere at high velocities (at least 11 km/s) and undergo heating through atmospheric friction and compression. Micrometeorites individually weigh between 10 and 10 g and collectively comprise most of the extraterrestrial material that has come to the present-day Earth.\n\nFred Lawrence Whipple first coined the term \"micro-meteorite\" to describe dust-sized objects that fall to the Earth. Sometimes meteoroids and micrometeoroids entering the Earth's atmosphere are visible as meteors or \"shooting stars\", whether or not they reach the ground and survive as meteorites and micrometorites.\n\nMicrometeorite (MM) textures vary as their original structural and mineral compositions are modified by the degree of heating that they experience entering the atmosphere—a function of their initial speed and angle of entry. They range from unmelted particles that retain their original mineralogy (Fig. 1 a, b), to partially melted particles (Fig. 1 c, d) to round melted cosmic spherules (Fig. 1 e, f, g, h, Fig. 2) some of which have lost a large portion of their mass through vaporization (Fig. 1 i). Classification is based on composition and degree of heating.\n\nThe extraterrestrial origins of micrometeorites are determined by microanalyses that show that:\n\nAn estimated 30,000 ± 20,000 tonnes per year (t/yr) of cosmic dust enters the upper atmosphere each year of which less than 10% (2700 ± 1400 t/yr) is estimated to reach the surface as particles. Therefore, the mass of micrometeorites deposited is roughly 50 times higher than that estimated for meteorites, which represent approximately 50 t/yr, and the huge number of particles entering the atmosphere each year (~10 > 10 µm) suggests that large MM collections contain particles from all dust producing objects in the Solar System including asteroids, comets, and fragments from our Moon and Mars. Large MM collections provide information on the size, composition, atmospheric heating effects and types of materials accreting on Earth while detailed studies of individual MMs give insights into their origin, the nature of the carbon, amino acids and pre-solar grains they contain.\n\nMicrometeorites have been collected from deep-sea sediments, sedimentary rocks and polar sediments; they are currently collected primarily from polar snow and ice. Because of their low concentrations on the Earth's surface, MMs are sought in environments that concentrate these materials relative to terrestrial particles.\n\nMelted micrometeorites (cosmic spherules) were first collected from deep-sea sediments during the 1873 to 1876 expedition of HMS \"Challenger\". In 1891, Murray and Renard found \"two groups [of micrometeorites]: first, black magnetic spherules, with or without a metallic nucleus; second, brown-coloured spherules resembling chondr(ul)es, with a crystalline structure\". In 1883, they suggested that these spherules were extraterrestrial because they were found far from terrestrial particle sources, they did not resemble magnetic spheres produced in furnaces of the time, and their nickel-iron (Fe-Ni) metal cores did not resemble metallic iron found in volcanic rocks. The spherules were most abundant in slowly accumulating sediments, particularly red clays deposited below the carbonate compensation depth, a finding that supported a meteoritic origin. In addition to those spheres with Fe-Ni metal cores, some spherules larger than 300 µm contain a core of elements from the platinum group.\n\nSince the first collection of HMS \"Challenger\", cosmic spherules have been recovered from ocean sediments using cores, box cores, clamshell grabbers, and magnetic sleds. Among these a magnetic sled, called the \"Cosmic Muck Rake\", retrieved thousands of cosmic spherules from the top 10 cm of red clays on the Pacific Ocean floor.\n\nTerrestrial sediments also contain micrometeorites. These have been found in samples that:\nThe oldest MMs are totally altered iron spherules found in 140- to 180-million-year-old hardgrounds.\n\nAmateur collectors may find micrometeorites in areas where dirt and dust from a large area has been concentrated, such as from a roof downspout.\n\nMicrometeorites found in polar sediments are much less weathered than those found in other terrestrial environments, as evidenced by little etching of interstitial glass, and the presence of large numbers of glass spherules and unmelted micrometeorites, particle types that are rare or absent in deep-sea samples. The MMs found in polar regions have been collected from Greenland snow, Greenland cryoconite, Antarctic blue ice Antarctic aeolian (wind-driven) debris, ice cores, the bottom of the South Pole water well, Antarctic sediment traps and present day Antarctic snow.\n\nModern classification of meteorites and micrometeorites is complex; the 2007 review paper of Krot et al. summarizes modern meteorite taxonomy. Linking individual micrometeorites to meteorite classification groups requires a comparison of their elemental, isotopic and textural characteristics.\n\nWhereas most meteorites likely originate from asteroids, the contrasting makeup of micrometeorites suggests that most originate from comets.\n\nFewer than 1% of MMs are achondritic and are similar to HED meteorites, which are thought to be from the asteroid, 4 Vesta. Most MMs are compositionally similar to carbonaceous chondrites, whereas approximately 3% of meteorites are of this type. The dominance of carbonaceous chondrite-like MMs and their low abundance in meteorite collections suggests that most MMs derive from sources different than those for most meteorites. Since most meteorites probably derive from asteroids, an alternative source for MMs might be comets. The idea that MMs might originate from comets originated in 1950.\n\nUntil recently the greater-than-25-km/s entry velocities of micrometeoroids, measured for particles from comet streams, cast doubts against their survival as MMs. However, recent dynamical simulations suggest that 85% of cosmic dust could be cometary. Furthermore, analyses of particles returned from the comet, Wild 2, by the \"Stardust\" spacecraft show that these particles have compositions that are consistent with many micrometeorites. Nonetheless, some parent bodies of micrometeorites appear to be asteroids with chondrule-bearing carbonaceous chondrites.\n\nThe influx of micrometeoroids also contributes to the composition of regolith (planetary/lunar soil) on other bodies in the Solar System. Mars has an estimated annual micrometeoroid influx of between 2,700 and 59,000 t/yr. This contributes to about 1m of micrometeoritic content to the depth of the Martian regolith every billion years. Measurements from the Viking program indicate that the Martian regolith is composed of 60% basaltic rock and 40% rock of meteoritic origin. The lower-density Martian atmosphere allows much larger particles than on Earth to survive the passage through to the surface, largely unaltered until impact. While on Earth particles that survive entry typically have undergone significant transformation, a significant fraction of particles entering the Martian atmosphere throughout the 60 to 1200-μm diameter range probably survive unmelted.\n\n\n"}
{"id": "18612528", "url": "https://en.wikipedia.org/wiki?curid=18612528", "title": "Ministry of Water and Power", "text": "Ministry of Water and Power\n\nThe Ministry of Water and Power (, abbreviated as MoPW) was a federal ministry in Pakistan. \n\nThe ministry was dissolved in August 2017. The water division was merged with newly created Ministry of Water Resources and the power division was moved under Ministry of Energy.\n\nNTDC is a limited company established in 1998. The main function of company is to purchase electric power from generation companies and then sells to distribution companies.\n\nThe Federal Flood Commission (FFC) is an agency within the Ministry that was created in 1977 in response to severe flooding by the Indus River. The FFC has been charged to execute flood control projects and protect lives and property from the impact of floods. By 2010 the FFC had received Rs 87.8 billion since its inception, and its own documents demonstrate that numerous projects were initiated, paid for, and seemingly completed. However, after the devastating 2010 Pakistan floods the agency has been severely criticized as apparently very little actual work had been done on the ground and it was accused of ineffectiveness and corruption. \n\nThe Private Power and Infrastructure Board (PPIB) was created in 1994 to promote private sector participation in the power sector of Pakistan. PPIB facilitates investors in establishing private power projects and related infrastructure, executes Implementation Agreement (IA) with Project Sponsors and issues sovereign guarantees on behalf of government.\n\n\n\n \n"}
{"id": "8643", "url": "https://en.wikipedia.org/wiki?curid=8643", "title": "Molecular diffusion", "text": "Molecular diffusion\n\nMolecular diffusion, often simply called diffusion, is the thermal motion of all (liquid or gas) particles at temperatures above absolute zero. The rate of this movement is a function of temperature, viscosity of the fluid and the size (mass) of the particles. Diffusion explains the net flux of molecules from a region of higher concentration to one of lower concentration. Once the concentrations are equal the molecules continue to move, but since there is no concentration gradient the process of molecular diffusion has ceased and is instead governed by the process of self-diffusion, originating from the random motion of the molecules. The result of diffusion is a gradual mixing of material such that the distribution of molecules is uniform. Since the molecules are still in motion, but an equilibrium has been established, the end result of molecular diffusion is called a \"dynamic equilibrium\". In a phase with uniform temperature, absent external net forces acting on the particles, the diffusion process will eventually result in complete mixing.\n\nConsider two systems; S and S at the same temperature and capable of exchanging particles. If there is a change in the potential energy of a system; for example μ>μ (μ is Chemical potential) an energy flow will occur from S to S, because nature always prefers low energy and maximum entropy.\n\nMolecular diffusion is typically described mathematically using Fick's laws of diffusion.\n\nDiffusion is of fundamental importance in many disciplines of physics, chemistry, and biology. Some example applications of diffusion:\n\nDiffusion is part of the transport phenomena. Of mass transport mechanisms, molecular diffusion is known as a slower one.\n\nIn cell biology, diffusion is a main form of transport for necessary materials such as amino acids within cells. Diffusion of solvents, such as water, through a semipermeable membrane is classified as osmosis.\n\nMetabolism and respiration rely in part upon diffusion in addition to bulk or active processes. For example, in the alveoli of mammalian lungs, due to differences in partial pressures across the alveolar-capillary membrane, oxygen diffuses into the blood and carbon dioxide diffuses out. Lungs contain a large surface area to facilitate this gas exchange process.\n\nFundamentally, two types of diffusion are distinguished:\n\nThe diffusion coefficients for these two types of diffusion are generally different because the diffusion coefficient for chemical diffusion is binary and it includes the effects due to the correlation of the movement of the different diffusing species.\n\nBecause chemical diffusion is a net transport process, the system in which it takes place is not an equilibrium system (i.e. it is not at rest yet). Many results in classical thermodynamics are not easily applied to non-equilibrium systems. However, there sometimes occur so-called quasi-steady states, where the diffusion process does not change in time, where classical results may locally apply. As the name suggests, this process is a not a true equilibrium since the system is still evolving.\n\nNon-equilibrium fluid systems can be successfully modeled with Landau-Lifshitz fluctuating hydrodynamics. In this theoretical framework, diffusion is due to fluctuations whose dimensions range from the molecular scale to the macroscopic scale.\n\nChemical diffusion increases the entropy of a system, i.e. diffusion is a spontaneous and irreversible process. Particles can spread out by diffusion, but will not spontaneously re-order themselves (absent changes to the system, assuming no creation of new chemical bonds, and absent external forces acting on the particle).\n\n\"Collective diffusion\" is the diffusion of a large number of particles, most often within a solvent.\n\nContrary to brownian motion, which is the diffusion of a single particle, interactions between particles may have to be considered, unless the particles form an ideal mix with their solvent (ideal mix conditions correspond to the case where the interactions between the solvent and particles are identical to the interactions between particles and the interactions between solvent molecules; in this case, the particles do not interact when inside the solvent).\n\nIn case of an ideal mix, the particle diffusion equation holds true and the diffusion coefficient \"D\" the speed of diffusion in the particle diffusion equation is independent of particle concentration. In other cases, resulting interactions between particles within the solvent will account for the following effects:\n\nTransport of material in stagnant fluid or across streamlines of a fluid in a laminar flow occurs by molecular diffusion. Two adjacent compartments separated by a partition, containing pure gases A or B may be envisaged. Random movement of all molecules occurs so that after a period molecules are found remote from their original positions. If the partition is removed, some molecules of A move towards the region occupied by B, their number depends on the number of molecules at the point considered. Concurrently, molecules of B diffuse toward regimens formerly occupied by pure A.\nFinally, complete mixing occurs. Before this point in time, a gradual variation in the concentration of A occurs along an axis, designated x, which joins the original compartments. This variation, expressed mathematically as -dC/dx, where C is the concentration of A. The negative sign arises because the concentration of A decreases as the distance x increases. Similarly, the variation in the concentration of gas B is -dC/dx. The rate of diffusion of A, N, depend on concentration gradient and the average velocity with which the molecules of A moves in the x direction. This relationship is expressed by Fick's Law\n\nwhere D is the Diffusivity of A through B, proportional to the average (squared?) molecular velocity and, therefore dependent on the temperature and pressure of gases. The rate of Diffusion N,is usually expressed as the number of moles diffusing across unit area in unit time. As with the basic equation of heat transfer, this indicates that the rate of force is directly proportional to the driving force, which is the concentration gradient.\nThis basic equation applies to a number of situations. Restricting discussion exclusively to steady state conditions, in which neither dC/dx or dC/dx change with time, equimolecular counterdiffusion is considered first.\n\nIf no bulk flow occurs in an element of length dx, the rates of diffusion of two ideal gases (of similar molar volume) A and B must be equal and opposite, that is formula_2.\nThe partial pressure of A changes by dP over the distance dx. Similarly, the partial pressure of B changes dP. As there is no difference in total pressure across the element (no bulk flow), we have\n\nFor an ideal gas the partial pressure is related to the molar concentration by the relation\n\nwhere n is the number of moles of gas \"A\" in a volume \"V\". As the molar concentration \"C\" is equal to \"n/ V\" therefore\nConsequently, for gas A,\n\nwhere D is the diffusivity of A in B. Similarly,\n\nConsidering that dP/dx=-dP/dx, it therefore proves that D=D=D. If the partial pressure of A at x is P and x is P, integration of above equation,\n\nA similar equation may be derived for the counterdiffusion of gas B.\n\n"}
{"id": "16822693", "url": "https://en.wikipedia.org/wiki?curid=16822693", "title": "Mosses of Western Australia", "text": "Mosses of Western Australia\n\nWestern Australia has relatively few species of moss; the most recent census found just 192 taxa. This represents just 10% of Australia's total moss flora, even though Western Australia accounts for about one third of the Australia by area. This relatively low diversity has been attributed to the lack of rainforest in the state.\n\nBy far the majority of the state's moss species occur in the Southwest Botanical Province, with over 80% of all species, genera and families occurring there. This includes four species that are apparently endemic to the province.\n\nAbout 70% of Western Australia's moss taxa occur also in South Australia, and a similar proportion occur also in New South Wales. Only about 50% occur also in Queensland. About half are restricted to Australia, New Zealand and South Africa, and a further 10% occur also only in South America.\n\nThis is a list of mosses of Western Australia, with classification updated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "45603322", "url": "https://en.wikipedia.org/wiki?curid=45603322", "title": "Murray's Hypocycloidal Engine", "text": "Murray's Hypocycloidal Engine\n\nMurray's Hypocycloidal Engine, now in Thinktank, Birmingham Science Museum, England, was made around 1805 and is the world's third-oldest working steam engine and the oldest working engine with a hypocycloidal gear.\n\nDesigned by Matthew Murray, and made by Fenton, Murray and Wood of Holbeck, Leeds, it is one of only two of the type to survive; the other is located at The Henry Ford, Michigan, United States.\n\nThe single-cylinder engine was used by John Bradley & Co of Stourbridge from 1805 until 1931, and by N. Hingley & Sons Ltd of Netherton from 1931 until 1961, when it was acquired by Birmingham City Council for their science museum.\n\nMurray patented the hypocycloidal arrangement in 1802.\n\n"}
{"id": "40992663", "url": "https://en.wikipedia.org/wiki?curid=40992663", "title": "Nicholas Hammond (ornithologist)", "text": "Nicholas Hammond (ornithologist)\n\nNicholas 'Nick' Hammond is an English ornithologist and author, and a former director of the Wildlife Trust for Bedfordshire, Cambridgeshire and Northamptonshire. He was editor of the Royal Society for the Protection of Birds' \"Birds\" magazine, and later its director of communications. He has written a number of books on, and speaks about, wildlife art.\n\nHammond undertook three stints as editor of the Royal Society for the Protection of Birds (RSPB) magazine \"Birds\", from volume 1 number 7 – vol. 5 no. 5 (November/December 1966—September/October 1974); vol. 6 no. 6 – vol. 6 no. 9 (Spring-Winter 1977) and vol. 10 no. 3 – vol. 11 no. 1 (Autumn 1984-Spring 1986). He also served as the charity's director of communications.\n\n"}
{"id": "14751689", "url": "https://en.wikipedia.org/wiki?curid=14751689", "title": "Ocean Observatories Initiative", "text": "Ocean Observatories Initiative\n\nThe Ocean Observatories Initiative (OOI) is a National Science Foundation (NSF) Division of Ocean Sciences program that focuses the science, technology, education and outreach of an emerging network of science driven ocean observing systems. It is a networked infrastructure of science-driven sensor systems to measure the physical, chemical, geological and biological variables in the ocean and seafloor as well as the overlying atmosphere, providing an integrated system collecting data on coastal, regional and global scales. \nOOI is funded by the National Science Foundation (NSF).\n\nOOI's goal is to deliver data and data products for a 25-year-plus time period within a scalable architecture that can meet emerging technical advances in ocean science. These data are freely accessible online through the OOI cyberinfrastructure.\nOOI builds on the heritage of the ship-based expeditions of the last century and the more recent emphasis to increase ocean observation and in situ monitoring.\n\nIn 2003 the Pew Oceans Commission recommended changes designed to improve society’s use and stewardship of, and impact on, the coastal and global ocean.\n\nIn 2007, the National Science and Technology Council’s Joint Subcommittee on Ocean Science and Technology developed an Ocean Research Priorities Strategy, which provides a research investment framework to advance understanding of ocean processes and interactions that facilitate responsible use of the ocean environment. The ORPS identified three cross-cutting elements, one of which is ocean observing for research and management.\n\nIn late 2007 the OOI project underwent its Preliminary Design Review and in 2008 completed its Final Network Design Review resulting in the Final Network Design. In May 2009, the National Science Board authorized the Director of NSF to award funds for the construction and initial operation of the OOI. In September 2009, NSF and the Consortium for Ocean Leadership signed the Cooperative Agreement that initiated the construction phase of the OOI.\n\nAfter nearly 10 years and expenses of US$386 million, in June 2016, NSF announced that most OOI data were flowing in real time from more than 900 sensors at the 7 sites. The annual budget is approximately $55 million.\n\nThe OOI Program is managed and coordinated by the OOI Project Office at the Consortium for Ocean Leadership, in Washington, D.C., and is responsible for construction and initial operation. Five Implementing Organizations are responsible for construction and development. \n\nThe sites and platforms of the OOI components target the following key scientific processes:\n\nQuantify the air-sea exchange of energy and mass, especially during high winds (greater than 20 meters-per-second), to provide estimates of energy and gas exchange between the surface and deep ocean. Such measurements should improve the predictive capability of storm forecasting and climate change models.\n\nExamine how climate variability affects ocean circulation, weather patterns, the ocean’s biochemical environment and marine ecosystems.\n\nUnderstand the role of turbulent mixing in the transfer of materials within the ocean and in the exchange of energy and gases between the ocean and atmosphere.\n\nQuantify the processes governing the coastal ocean.\n\nExamine the degree to which active tectonic plate boundaries influence the ocean from physical, chemical and biological perspectives. Lithospheric movements and interactions at plate boundaries at or beneath the seafloor are responsible for short-term events such as earthquakes, tsunamis and volcanic eruptions. These regions are also host to the densest hydrothermal and biological activity in the ocean basins.\n\nThe oceanic crust contains the largest aquifer on Earth. Thermal circulation and reactivity of seawater-derived fluids can modify the composition of oceanic plates, lead to the formation of hydrothermal vents that support unique micro- and macro-biological communities and concentrate methane to form massive methane gas and methane hydrate reservoirs.\n\nThe four main components that comprise the OOI are: Coastal Global Scale Nodes (CGSN); Regional Scale Nodes (RSN); Cyberinfrastructure (CI) and Education and Public Engagement (EPE). Implementing organizations are responsible for these components.\n\nThe CGSN is made up of two coastal arrays and four global arrays.\n\nCoastal arrays provide sustained, adaptable access to complex coastal systems. Coastal arrays extend from the continental shelf to the continental slope, allowing scientists to examine coastal processes including upwelling, hypoxia, shelf break fronts, and the role of filaments and eddies in cross-shelf exchange. Technologies that gather data in the coastal region include moored buoys with fixed sensors, moored vertical profilers, seafloor cables, gliders and autonomous underwater vehicles.\n\nThe coastal observatory includes a long-term Endurance Array in the Eastern Pacific and a re-locatable Pioneer Array in the Western Atlantic. Woods Hole Oceanographic Institution installed and operates the Pioneer Array. Oregon State University installed and operates the Endurance Array.\n\nThe Pioneer Array is a network of platforms and sensors that operate on the continental shelf and slope south of New England. A moored array is centered at the shelf break in the mid-Atlantic Bight south of Cape Cod, Massachusetts. Autonomous underwater vehicles (AUVs) sample the frontal region in the vicinity of the moored array and gliders resolve mesoscale features on the outer shelf and the slope sea between the shelf break front and the Gulf Stream.\n\nThe Pioneer Array provides a three-dimensional view of key biophysical interactions at the shelf break using its flexible, multiplatform array that combines moored and mobile components with high spatial and temporal resolution. In its initial location south of Cape Cod, the Pioneer is embedded within an established regional observing system. The Pioneer Array is planned to move from place to place over approximately five-year intervals to characterize processes in different coastal ocean environments.\n\nThe Endurance Array, located on the continental shelf and slope off Oregon and Washington, provides a long-term network of moorings, benthic nodes, cabled and uncabled sensors and gliders. The array consists of two lines of moorings, one off Newport, Oregon (the Oregon Line) and the other off Grays Harbor, Washington (the Washington Line). Gliders sample between the mooring lines. The array focuses on observing the influence of the Columbia River on the coastal ecosystem. It also samples a prototypical upwelling regime on a narrow continental shelf where anoxia events are common. Some Endurance Array Oregon Line infrastructure connects to the RSN cabled network to provide enhanced power and communications for observing water column and seafloor processes.\n\nLocations of the global arrays were selected by a team of scientists (~300 people) based on regions that are under-sampled and subject to extreme conditions (e.g., high winds and sea states) that are challenging for continuous or even frequent ship-based measurements. The planned global study sites include instrumented moorings and gliders in four locations: Argentine Basin; Irminger Sea; Southern Ocean; and Station Papa. The global arrays are developed and operated by Woods Hole and Scripps.\n\nObservations from these high latitude areas are critical to understanding ocean circulation and climate change processes. The global arrays each include four moorings (except Station Papa, which has three) composed of fixed and moving sensors that measure air-sea fluxes of heat, moisture, and momentum—as well as physical, biological, and chemical properties of the water column. Each array also includes gliders to sample within the array’s footprint.\n\nRegional Scale Nodes are cabled arrays of ocean observing sensors in the Northeast Pacific Ocean. They are connected by approximately 900 kilometers (560 miles) of electro-optical cable. The design provides high power (10 kV, 8 kW) and bandwidth (10 GbE) to sensor arrays on the seafloor and throughout the water column using moorings with instrumented wire-following profilers, 200 m instrumented platforms and winched profilers. The RSN was installed and operated by the University of Washington.\n\nThe two primary study sites are Hydrate Ridge, an area of massive sub-seafloor gas-hydrate deposits and fluxes of methane from the seafloor into the ocean, and Axial Seamount, the most magmatically robust volcano on the Juan de Fuca Ridge spreading center that erupted in April 2011.\n\nThe RSN complements the NEPTUNE cabled observatory that Ocean Networks Canada operates on the northern Juan de Fuca plate. Together these observatories enable long-term, plate-scale seafloor and ocean investigations in the Northeast Pacific.\n\nThe Cyberinfrastructure component links marine infrastructure to scientists and users. It manages and integrates data from the different OOI sensors. It provides a common operating infrastructure, the Integrated Observatory Network (ION), connecting and coordinating the operations of the marine components (global, regional, and coastal scale arrays). It also provides resource management, observatory mission command and control, product production, data management and distribution (including strong data provenance) and centrally available collaboration tools.\n\nION connects and coordinates operations of the OOI marine components with oceanographic research communities. The University of California, San Diego initially designed the cyber infrastructure. The project later relocated to Rutgers University.\n\nEducation and Public Engagement includes educational data visualization tools to engage students in active scientific inquiry. Collectively, these tools provide easy access for the development and use of scientific data visualization, helping translate science themes into educational materials, deliver the capability to build and edit online lesson and lab units, enable virtual collaboration and sharing of oceanographic data and learning materials and facilitate broader data access. Services were developed and integrated into a single interface planned for release in fall 2014—accessible through a web application programming interface (API). This API also provides a framework that other computer scientists and programmers can use to develop new educational products for the web and other mobile applications.\n\nSpecifically, the EPE planned five online services including: \n\n"}
{"id": "21691250", "url": "https://en.wikipedia.org/wiki?curid=21691250", "title": "Okinawa Trough", "text": "Okinawa Trough\n\nThe (also called , literally China-Ryukyu Border Trough ) is a seabed feature of the East China Sea. It is an active, initial back-arc rifting basin which has formed behind the Ryukyu arc-trench system in the West Pacific. It developed where the Philippine Sea Plate is subducting under the Eurasia Plate.\n\nIt is a back-arc basin formed by extension within the continental lithosphere behind the far deeper Ryukyu Trench-arc system. It has a large section more than deep and a maximum depth of .\n\nThe Okinawa Trough still in an early stage of evolving from arc type to back-arc activity.\n\nThe existence of the Okinawa Trough complicates descriptive issues in the East China Sea. According to Professor Ji Guoxing of the Asia-Pacific Department at Shanghai Institute for International Studies,\n\nOn August 15, 2013, China’s mission did a presentation to the Commission on the Limits of the Continental Shelf (CLCS) established under the United Nations Convention on the Law of the Sea (UNCLOS). The presentation was on the proposal that demarcates the limits of the outer continental shelf beyond 200 nm in part of the East China Sea. China states that China’s continental shelf in the East China Sea extends to China-Ryukyu Border Trough naturally, which has been over 200 nautical miles away from the mainland baseline of Chinese territorial waters. According to UNCLOS, any country claiming continental shelves beyond 200 nm shall provide relevant scientific evidence to CLCS. To collect solid data, China deployed 14 scientific survey ships, covering an area of 250,000 square kilometers.\n\n"}
{"id": "305399", "url": "https://en.wikipedia.org/wiki?curid=305399", "title": "Porphyry (geology)", "text": "Porphyry (geology)\n\nPorphyry is a textural term for an igneous rock consisting of large-grained crystals such as feldspar or quartz dispersed in a fine-grained silicate rich, generally aphanitic matrix or groundmass. The larger crystals are called phenocrysts. In its non-geologic, traditional use, the term \"porphyry\" refers to the purple-red form of this stone, valued for its appearance.\n\nThe term \"porphyry\" is from Ancient Greek (πορφύρα \"porphúra\") and means \"purple\". Purple was the color of royalty, and the \"imperial porphyry\" was a deep purple igneous rock with large crystals of plagioclase. Some authors claimed the rock was the hardest known in antiquity. \"Imperial\" grade porphyry was thus prized for monuments and building projects in Imperial Rome and later. Porphyry typically has hardness 7 on the Mohs scale of mineral hardness, corresponding to steel and quartz. \n\nSubsequently, the name was given to any igneous rocks with large crystals. The adjective \"porphyritic\" now refers to a certain texture of igneous rock regardless of its chemical and mineralogical composition. Its chief characteristic is a large difference in size between the tiny matrix crystals and the much larger phenocrysts. Porphyries may be aphanites or phanerites, that is, the groundmass may have invisibly small crystals as in basalt, or crystals easily distinguishable with the eye, as in granite. Most types of igneous rocks display some degree of porphyritic texture.\n\nPorphyry deposits are formed when a column of rising magma is cooled in two stages. In the first stage, the magma is cooled slowly deep in the crust, creating the large crystal grains with a diameter of 2  mm or more. In the second and final stage, the magma is cooled rapidly at relatively shallow depth or as it erupts from a volcano, creating small grains that are usually invisible to the unaided eye.\n\nThe term porphyry is also used for a mineral deposit called a \"copper porphyry\". The different stages of cooling that create porphyritic textures in intrusive and hypabyssal porphyritic rocks also lead to a separation of dissolved metals into distinct zones. \n\nThis process, which occurs primarily when fluids are driven off the cooling magma, is one of the main reasons for the existence in the world of rich, localized metal ore deposits such as those of gold, copper, molybdenum, lead, tin, zinc, rhenium and tungsten. This enrichment occurs in the porphyry itself, or in other related igneous rocks or surrounding country rocks, especially carbonate rock (in a process similar to skarns). Collectively, these type of deposits are known as \"porphyry copper deposits\".\n\nRhomb porphyry is a volcanic rock with gray-white large porphyritic rhomb- shaped phenocrysts embedded in a very fine-grained red-brown matrix. The composition of rhomb porphyry places it in the trachyte–latite classification of the QAPF diagram.\n\nRhomb porphyry lavas are only known from three rift areas: the East African Rift (including Mount Kilimanjaro), Mount Erebus near the Ross Sea in Antarctica, and the Oslo graben in Norway. It is intrusive.\n\nPliny's Natural History affirmed that the \"Imperial Porphyry\" had been discovered at an isolated site in Egypt in AD 18, by a Roman legionary named Caius Cominius Leugas. Ancient Egyptians used other decorative porphyritic stones of a very close composition and appearance, but apparently remained unaware of the presence of the Roman grade although it was located in their own country. \n\nThis particular Imperial grade of porphyry came from a single quarry in the Eastern Desert of Egypt, from 600 million-year-old andesite of the Arabian-Nubian Shield. The road from the quarry westward to Qena (Roman Maximianopolis) on the Nile, which Ptolemy put on his second-century map, was first described by Strabo, and it is to this day known as the \"Via Porphyrites\", the Porphyry Road, its track marked by the hydreumata, or watering wells that made it viable in this utterly dry landscape. \n\nPorphyry was extensively used in Byzantine imperial monuments, for example in Hagia Sophia and in the \"Porphyra\", the official delivery room for use of pregnant Empresses in the Great Palace of Constantinople.\n\nAfter the fourth century the quarry was lost to sight for many centuries. The scientific members of the French Expedition under Napoleon sought it in vain, and it was only when the Eastern Desert was reopened for study under Muhammad Ali that the site was rediscovered by James Burton and John Gardiner Wilkinson in 1823.\n\nAs early as 1850 BC on Crete in Minoan Knossos there were large column bases made of porphyry. All the porphyry columns in Rome, the red porphyry togas on busts of emperors, the porphyry panels in the revetment of the Pantheon, as well as the altars and vases and fountain basins reused in the Renaissance and dispersed as far as Kiev, all came from the one quarry at \"Mons Porpyritis\" (\"Porphyry Mountain\", the Arabic \"Jabal Abu Dukhan\"), which seems to have been worked intermittently between 29 and 335 AD. Porphyry was also used for the blocks of the Column of Constantine in Istanbul.\n\nIn countries where many cars have studded winter tires such as Sweden, Finland and Norway, it is common that highways are paved with asphalt made of porphyry aggregate to make the wearing course withstand the extreme wear from the spiked tires. \n\n\n"}
{"id": "53663058", "url": "https://en.wikipedia.org/wiki?curid=53663058", "title": "Q-slope", "text": "Q-slope\n\nThe Q-slope method for rock slope engineering and rock mass classification is developed by Barton and Bar. It expresses the quality of the rock mass for slope stability using the Q-slope value, from which long-term stable, reinforcement-free slope angles can be derived.\n\nThe Q-slope value can be determined with:\n\nformula_1\n\nQ-slope utilizes similar parameters to the Q-system which has been used for over 40 years in the design of ground support for tunnels and underground excavations. The first four parameters, RQD (rock quality designation), J (joint set number), J (joint roughness number) and J (joint alteration number) are the same as in the Q-system. However, the frictional resistance pair J and J can apply, when needed, to individual sides of a potentially unstable wedges. Simply applied orientation factors (), like (J/J)x0.7 for set J and (J/J)x0.9 for set J, provide estimates of overall whole-wedge frictional resistance reduction, if appropriate. The Q-system term J is replaced with J, and takes into account a wider range of environmental conditions appropriate to rock slopes, which are exposed to the environment indefinitely. The conditions include the extremes of erosive intense rainfall, ice wedging, as may seasonally occur at opposite ends of the rock-type and regional spectrum. There are also slope-relevant SRF (strength reduction factor) categories.\n\nMultiplication of these terms results in the Q-slope value, which can range between 0.001 (exceptionally poor) to 1000 (exceptionally good) for different rock masses.\n\nA simple formula for the steepest slope angle (β), in degrees, not requiring reinforcement or support is given by:\n\nformula_2\n\nQ-slope is intended for use in reinforcement-free site access road cuts, roads or railway cuttings, or individual benches in open cast mines. It is based on over 200 case studies in slopes ranging from 35 to 90 degrees in fresh hard rock slopes as well as weak, weathered and saprolitic rock slopes. \n\nRock slope design techniques have been derived using Q-slope and geophysical survey data, primarily based on V (P-wave velocity).\n\nQ-slope is not intended as a substitute for conventional and more detailed slope stability analyses, where these are warranted.\n\n"}
{"id": "25833426", "url": "https://en.wikipedia.org/wiki?curid=25833426", "title": "Redwood Summer", "text": "Redwood Summer\n\nOrganized in 1990, Redwood Summer was a movement of environmental activism aimed at protecting old-growth redwood (\"Sequoia sempervirens\") trees from logging by northern California timber companies. The first official protest associated with Redwood Summer took place in June 1990 at the Louisiana Pacific export dock in Samoa California. Beginning that same month logging companies organized \"Right to Work Rallies\" in support of the timber extraction industry. Redwood Summer is a part of the larger Timber Wars of the 1990s. \"Timber Wars\" is also the title of a book by Judi Bari documenting the protests over the decade.\n\nA 1990 California Voter Initiative, Proposition 130 (\"Forests Forever\") was placed on the November 6, 1990 ballot. The Redwood Summer organizers sought to disrupt logging until forest lands gained extra protection under Proposition 130. \n\nOn May 24, 1990, Judi Bari and Darryl Cherney were driving through Oakland, California when a pipe bomb exploded directly under Bari's driver side seat. They were on a tour to recruit college students for Redwood Summer. A film advocating the theory that the FBI placed the bomb, titled \"Who Bombed Judi Bari?\" (not to be confused with the 1991 Stephen Talbot documentary of the same name) was released in 2012.\n\nAlthough the history of Earth First! had continually been controversial, Bari attempted to make Redwood Summer an act of nonviolent civil disobedience, rather than a flat-out confrontation. Protesters were asked to keep the demonstration free of items that may possible compromise this goal, such as alcohol, drugs, and weapons. Regardless of this intention Bari, and Redwood Summer, faced limited support from other environmental organizations. The Sierra Club refused to participate in the movement reportedly citing insurance and liability concerns. Gail Lucas, who represented The Sierra Club in the matter, denounced Redwood Summer saying it could “generate strong antagonism”. Bari did not feel that Lucas “represent[ed] the people who wrote the Forests Forever initiative, organized the Redwood Summer protests, or filed the grassroots lawsuits.” The Environmental protection Information Center (EPIC) of Garberville, CA did not criticize the Redwood Summer movement or Earth First!, but needed to maintain “clean hands” should they be involved in a litigation with Maxxam, parent corporation of Pacific Lumber Company. In general, Redwood Summer lived up to its commitment of nonviolence, causing inconvenience to timber workers and slowing logging in demonstration areas. Despite its stated cooperative intentions the demonstrations tended to be confrontational, fueling animosity from timber workers, and sparked numerous counter protests. Proposition 130 was defeated after an opposition campaign that highlighted Earth First! and Redwood Summer. Over the next decade EPIC filled numerous lawsuits against timber companies in the area.\n"}
{"id": "18350922", "url": "https://en.wikipedia.org/wiki?curid=18350922", "title": "Riometer", "text": "Riometer\n\nA riometer (commonly \"r\"elative \"i\"onospheric \"o\"pacity meter, although originally: Relative Ionospheric Opacity Meter for Extra-Terrestrial Emissions of Radio noise) is an instrument used to quantify the amount of electromagnetic-wave ionospheric absorption in the atmosphere. As the name implies, a riometer measures the \"opacity\" of the ionosphere to radio noise emanating from cosmic origin. In the absence of any ionospheric absorption, this radio noise, averaged over a sufficiently long period of time, forms a \"quiet-day curve.\" Increased ionization in the ionosphere will cause absorption of radio signals (both terrestrial and extraterrestrial), and a departure from the quiet-day curve. The difference between the quiet-day curve and the riometer signal is an indicator of the amount of absorption, and is measured in decibels. Riometers are generally passive radio antenna operating in the VHF radio frequency range (~30-40 MHz). Electromagnetic radiation of that frequency is typically Galactic synchrotron radiation and is absorbed in the Earth's D region of the ionosphere.\n\nThe riometer was developed in the mid 1950s by scientists at the University of Alaska who were researching the radio propagation effects of aurorae. At times aurorae resulted in complete failure of long distance radio communication to planes in the Arctic - a matter of considerable concern to the US Air Force at a time of tension with the Soviet Union. Riometers are still used today for ionospheric research and are typically located in polar and sub-polar areas.\n\nInitially, riometers were single, wide-beam detectors and measured the cosmic noise absorption (CNA). Multi-beam riometers have also been developed, which have multiple narrow beams, typically formed by a Butler matrix on a phased antenna array. Each beam forms its own riometer and has its own quiet-day curve determination. These individual beams form pixels on the sky allowing simple images of cosmic noise absorption to be formed. More recently, interferometry has been used to provide all-sky, spatially-continuous imaging of CNA. It is also possible to use riometers to observe multiple frequencies (typically in the range 25-40 MHz). An inverse problem technique can be applied to the measurements to ascertain not just the absorption, but a model of the electron content as a function of sight distance.\n\n"}
{"id": "6092968", "url": "https://en.wikipedia.org/wiki?curid=6092968", "title": "SEG-Y", "text": "SEG-Y\n\nThe SEG-Y (sometimes SEG Y) file format is one of several standards developed by the Society of Exploration Geophysicists (SEG) for storing geophysical data. It is an open standard, and is controlled by the SEG Technical Standards Committee, a non-profit organization.\n\nThe format was originally developed in 1973 to store single-line seismic reflection digital data on magnetic tapes. The specification was published in 1975.\n\nThe format and its name evolved from the SEG \"Ex\" or Exchange Tape Format. However, since its release, there have been significant advancements in geophysical data acquisition, such as 3-dimensional seismic techniques and high speed, high capacity recording.\n\nThe most recent revision of the SEG-Y format was published in 2017, named the \"rev 2.0\" specification. It still features certain legacies of the original format (referred as \"rev 0\"), such as an optional SEG-Y tape label, the main 3200 byte textual EBCDIC character encoded tape header and a 400 byte binary header.\n\nThis image shows the byte stream structure of a SEG-Y file, with rev 1 Extended Textual File Header records.\n\nMany SEG-Y programs do not totally follow the specification. For example, ODEC uses the opposite byte order and adds 320 bytes to the tail of each trace.\n\n\n"}
{"id": "987039", "url": "https://en.wikipedia.org/wiki?curid=987039", "title": "Siberian Traps", "text": "Siberian Traps\n\nThe Siberian Traps (, ) is a large region of volcanic rock, known as a large igneous province, in Siberia, Russia. The massive eruptive event that formed the Traps is one of the largest-known volcanic events that has occurred in the last years.\n\nThe eruptions continued for roughly two million years and spanned the P–T boundary, or the Permian–Triassic boundary, which occurred between .\n\nLarge volumes of basaltic lava covered a large expanse of Siberia in a flood basalt event. Today, the area is covered by about seven million km of basaltic rock, with a volume of around 4 million km.\n\nThe source of the Siberian Traps basaltic rock has been attributed to a mantle plume, which rose until it impacted against the bottom of the Earth's crust, producing volcanic eruptions through the Siberian Craton. It has been suggested that, as the Earth's lithospheric plates moved over the mantle plume (the Iceland plume), the plume produced the Siberian Traps in the Permian and Triassic periods, later going on to produce volcanic activity on the floor of the Arctic Ocean in the Jurassic and Cretaceous, and then generating volcanic activity in Iceland. Other plate tectonic causes have also been suggested. Another possible cause may be the impact that formed the Wilkes Land crater in Antarctica, which is estimated to have occurred around the same time and been nearly antipodal to the traps.\n\nThe main source of rock in this formation is basalt, but both mafic and felsic rocks are present, so this formation is officially called a Flood Basalt Province. The inclusion of mafic and felsic rock indicates multiple other eruptions that occurred and coincided with the one-million-year-long eruption that created the majority of the basaltic layers. The traps are divided into sections based on their chemical, stratigraphical, and petrographical composition.\n\nOne of the major questions is whether the Siberian Traps were directly responsible for the Permian–Triassic mass extinction event that occurred 250 million years ago, or if they were themselves caused by some other, larger event, such as an asteroid impact. A recent hypothesis put forward is that the volcanism triggered the growth of Methanosarcina, a microbe that then spewed enormous amounts of methane into Earth's atmosphere, ultimately altering the Earth's carbon cycle based on observations such as a significant increase of inorganic carbon reservoirs in marine environments.\n\nThis extinction event, also called the Great Dying, affected all life on Earth, and is estimated to have killed about 95% of all species living at the time. Some of the disastrous events that impacted the Earth continued to repeat themselves on Earth 5 to 6 million years after the initial extinction occurred. Over time a small portion of the life that survived the extinction was able to repopulate and expand starting with low trophic levels (local communities) until the higher trophic levels (large habitats) were able to be re-established. Calculations of sea water temperature from δO measurements indicate that at the peak of the extinction, the Earth underwent lethally hot global warming, in which equatorial ocean temperatures exceeded . It took roughly 8 to 9 million years for any diverse ecosystem to be re-established; however, new classes of animals were established after the extinction that did not exist beforehand.\n\nPalaeontological evidence further indicates that the global distribution of tetrapods vanished, with very rare exceptions in the region of Pangaea that is today Utah, between latitudes bounded by approximately 40°S to 30°N. The tetrapod gap of equatorial Pangaea coincides with an end-Permian to Middle Triassic global \"coal gap\" that indicates the loss of peat swamps. Peat formation, a product of high plant productivity, was reestablished only in the Anisian stage of the Triassic, and even then only in high southern latitudes, although gymnosperm forests appeared earlier (in the Early Spathian), but again only in northern and southern higher latitudes. In equatorial Pangaea, the establishment of conifer-dominated forests was not until the end of the Spathian, and the first coals at these latitudes did not appear until the Carnian, around 15 million years after their end-Permian disappearance. These signals suggest equatorial temperatures exceeded their thermal tolerance for many marine vertebrates at least during two thermal maxima, whereas terrestrial equatorial temperatures were sufficiently severe to suppress plant and animal abundance during most of the Early Triassic.\n\nThe volcanism that occurred in the Siberian Traps resulted in copious amounts of magma being ejected from the Earth's crust—leaving permanent traces of rock from the same time period of the mass extinction that is able to be examined today. More specifically, zircon is found in some of the volcanic rocks. To further the accuracy of the age of the zircon, several varying aged pieces of zircon were organized into a timeline based on when they crystallized. The CA-TIMS technique, a chemical abrasion age-dating technique that eliminates variability in accuracy due to lead depletion in zircon over time, was then used to accurately determine the age of the zircons found in the Siberian Traps. Eliminating the variability due to lead, the CA-TIMS age-dating technique allowed uranium within the zircon to be the centre focus in linking the volcanism in the Siberian Traps that resulted in high amounts of magmatic material with the Permian–Triassic mass extinction.\n\nTo further the connection between the Permian–Triassic extinction event, other disastrous events occurred around the same time period, such as sea level changes, meteor impacts and volcanism. Specifically focusing on volcanism, rock samples from the Siberian Traps and other southern regions were obtained and compared. Basalts and gabbro samples from several southern regions close to and from the Siberian Traps were dated based on argon isotope 40 and argon isotope 39 age-dating methods. Feldspar and biotite was specifically used to focus on the samples age and duration of the presence magma from the volcanic event in the Siberian Traps. The majority of the basalt and gabbro samples dated to 250 million years ago, covered a surface area of five million square kilometres on the Siberian Traps and occurred within a short period of time with rapid rock solidification/cooling. Studies confirmed that samples of gabbro and basalt from the same time period of the Permian–Triassic event from the other southern regions also matched the age of samples within the Siberian Traps. This confirms the assumption of the linkage between the age of volcanic rocks within the Siberian Traps, along with rock samples from other southern regions to the Permian–Triassic mass extinction event.\n\nThe giant Norilsk-Talnakh nickel–copper–palladium deposit formed within the magma conduits in the most complete part of the Siberian Traps. It has been linked to the Permian–Triassic extinction event, which occurred approximately 251.4 million years ago, based on large amounts of nickel and other elements found in rock beds that were laid down after the extinction occurred. The method used to correlate the extinction event with the surplus amount of nickel located in the Siberian Traps, is by comparing the timeline of the magmatism within the traps and the timeline of the extinction itself. Before the linkage between magmatism and the extinction event was discovered, it was hypothesized that the mass extinction and volcanism occurred at the same time due to the linkages in rock composition.\n\n\n"}
{"id": "1225465", "url": "https://en.wikipedia.org/wiki?curid=1225465", "title": "Sinus Medii", "text": "Sinus Medii\n\nSinus Medii (\"Central Bay\") is a small lunar mare. It takes its name from its location at the intersection of the Moon's equator and prime meridian; as seen from the Earth, this feature is located in the central part of the Moon's near side, and it is the point closest to the Earth. From this spot the Earth would always appear directly overhead, although the planet's position would vary slightly due to libration.\n\nDuring the Apollo program, Sinus Medii was desingated ALS3. Flight operations planners were concerned about having the optimum lighting conditions at the landing site, hence alternative landing sites moved progressively westward, following the terminator. A delay of two days for weather or equipment reasons would have sent Apollo 11 to Sinus Medii instead of ALS2, Mare Tranquillitatis; another two-day delay would have resulted in ALS5, a site in Oceanus Procellarum, being targeted.\n\nThe selenographic coordinates of Sinus Medii are , and its diameter is 335 km. It joins Mare Insularum in the west with Mare Vaporum to the north.\n\nThe eastern part of this area is notable for a series of rille systems. In the far northeast is the Rima Hyginus, which is bisected by the crater Hyginus. At the far eastern end is the 220-km long Rima Ariadaeus rille which continues eastward to the edge of the Mare Tranquillitatis. At longitudes 4-6° E is the Rimae Triesnecker rille system, named after the crater Triesnecker just to the west.\n\nThe northern edge of the Sinus Medii is formed by a highland region, with the impact craters Murchison and Pallas along the border. Near the northern border on this mare is the cup-shaped Chladni.\n\nAnother highland region lies to the south and southeastern edge of the Sinus Medii. Several flooded craters lie along this border, with Flammarion near the western edge, then Oppolzer, Réaumur, and Seeliger further east. The Rima Flammarion and Rima Oppolzer rilles lie along the edge of the mare near their corresponding craters. Also along the southeast border and bisecting the prime meridian is the crater Rhaeticus.\n\nIn the western half of the interior are the small craters Bruce and Blagg. Near the western end of the mare are the flooded craters Schröter and Sömmering.\n\nThe English astronomer William Gilbert was the first to give a name to this mare, calling it Insula Medilunaria (\"Middlemoon Island\"). The idea for its present name originates with Michael Van Langren, who labelled it Sinus Medius in his 1645 map. Johannes Hevelius called the feature Mare Adriaticum (\"The Adriatic Sea\") in his 1647 map. Giovanni Riccioli called it Sinus Aestuum (\"Bay of Hot Days\") in his 1651 map.\n\nThe Surveyor 6 mission landed to the west-southwest of Bruce crater inside Sinus Medii in November 1967. Prior to that in July 1967, the Surveyor 4 had crashed nearby.\n\n"}
{"id": "10328999", "url": "https://en.wikipedia.org/wiki?curid=10328999", "title": "Stabilization Fund of the Russian Federation", "text": "Stabilization Fund of the Russian Federation\n\nThe Stabilization fund of the Russian Federation () was\nestablished based on a resolution of the Government of Russia on 1 January 2004, as a part of the federal budget to balance the federal budget at the time of when oil price falls below a cut-off price, currently set at US$27 per barrel. In February 2008 the Stabilization Fund was split into a Reserve Fund, which is invested abroad in low-yield securities and used when oil and gas incomes fall, and the National Welfare Fund, which invests in riskier, higher return vehicles, as well as federal budget expenditures. The Reserve Fund was given $125 billion and the National Welfare Fund was given $32 billion. By the end of 2016 the two funds consisted respectively of $38.2 and 72.2 billion. \n\nThe Fund was created to create a reserve of liquidity with the additional benefit of reducing inflationary pressure and insulating the economy of Russia from volatility of raw material export earnings, which was among the reasons of the 1998 Russian financial crisis. To prevent high inflation rates the fund is invested into abroad only.\n\nAccording to amendments to Russia’s budget code inspired by President Vladimir Putin’s budget address of March 2007 and passed in April 2007, in February 2008 the Stabilization Fund was supposed to be split into a Reserve Fund, to be invested abroad in low-yield securities and used when oil and gas incomes fall, and a Future Generations’ Fund (later renamed the National Wealth Fund), which will invest in riskier, higher return vehicles, as well as federal budget expenditures. Unlike the Stabilization Fund, the new funds will also accumulate revenues from oil products and natural gas.\n\nOn 21 May 2007, President Vladimir Putin urged the government to pump surplus oil revenue into domestic stocks by buying Russian blue chips such as Gazprom and Rosneft, which had fallen since the beginning of the year, instead of foreign securities, which previously had been explicitly forbidden due to fear of inflation.\n\nThe Fund accumulates revenues from the export duty for oil and the tax on\noil mining operations when the price for Urals oil exceeds the set cut-off price.\n\nThe capital of the Fund may be used to cover the federal budget deficit and\nfor other purposes, if its balance exceeds 500 billion rubles, spending amounts are subject to the federal budget law for the corresponding fiscal year.\n\nAs the capital of the Fund had exceeded the level of 500 billion rubles in\n2005, part of its surplus was used for early foreign debt repayments as well as to\ncover Russian Pension Fund's deficit. The details of these transactions in 2005 are\nas follows:\n\nThe Fund is managed by the Russian Ministry of Finance, in accordance with the procedure defined by the Government of the Russian Federation. Some functions of asset management may be delegated to the Central Bank of the Russian Federation (\"the Bank of Russia\"), based on its agreement with the Government.\n\nConcerning the Fund's objectives, its capital is to be invested in foreign sovereign debt securities. Securities' eligibility criteria are subject to the Government's approval.\n\nThe Ministry of Finance is empowered by the Government to establish the\nFund's currency composition and its strategic asset allocation in line with the\ninvestment policy for the Fund's management.\n\nThe Ministry of Finance may use one or both of the following schemes\ndefined by the Government to invest the Fund's capital.\n\nThe Fund assets are currently invested solely under second scheme\n(allocation to the Federal Treasury's accounts with the Bank of Russia).\n\nThe Government determined that eligible debt securities for the Fund investment are to correspond to the following requirements.\n\nDebt securities on the date of purchase will have a minimum remaining maturity of 0,25 years and are not to exceed three years.\n\nThe Fund assets are currently invested in the following currency composition:\n\nCurrency composition and the maturity restrictions are applicable to all Fund's assets and are subject to revisions by the Ministry of Finance.\n\nThe Ministry of Finance publishes a monthly report in mass media on the\nFund's accumulation, spending and balance and reports quartelry and annually to the Government\non accumulation, investment and spending of the Fund's capital.\nThe Government will report quarterly and annually on the Fund's accumulation,\nspending and investment of capital to both chambers of the\nRussian Parliament (State Duma and Council of Federation).\n\n\n"}
{"id": "86359", "url": "https://en.wikipedia.org/wiki?curid=86359", "title": "Swamp", "text": "Swamp\n\nA swamp is a wetland that is forested. Many swamps occur along large rivers where they are critically dependent upon natural water level fluctuations. Other swamps occur on the shores of large lakes. Some swamps have hammocks, or dry-land protrusions, covered by aquatic vegetation, or vegetation that tolerates periodic inundation or soil saturation. The two main types of swamp are \"true\" or swamp forests and \"transitional\" or shrub swamps. In the boreal regions of Canada, the word swamp is colloquially used for what is more correctly termed a bog, fen, or muskeg. The water of a swamp may be fresh water, brackish water or seawater. Some of the world's largest swamps are found along major rivers such as the Amazon, the Mississippi, and the Congo.\n\nA marsh is a wetland composed mainly of grasses and reeds found near the fringes of lakes and streams, serving as a transitional area between land and aquatic ecosystems. A swamp is a wetland composed of trees and shrubs found along large rivers and lake shores.\n\nSwamps are characterized by slow-moving to stagnant waters. Many adjoin rivers or lakes. Swamps are features of areas with very low topographic relief.\n\nHistorically, humans have drained swamps to provide additional land for agriculture and to reduce the threat of diseases borne by swamp insects and similar animals. Many swamps have also undergone intensive logging, requiring the construction of drainage ditches and canals. These ditches and canals contributed to drainage and, along the coast, allowed salt water to intrude, converting swamps to marsh or even to open water. Large areas of swamp were therefore lost or degraded. Louisiana provides a classic example of wetland loss from these combined factors. Europe has probably lost nearly half its wetlands. New Zealand lost 90 percent of its wetlands over a period of 150 years. Ecologists recognise that swamps provide valuable ecological services including flood control, fish production, water purification, carbon storage, and wildlife habitat. In many parts of the world authorities protect swamps. In parts of Europe and North America, swamp restoration projects are becoming widespread. Often the simplest steps to restoring swamps involve plugging drainage ditches and removing levees.\n\nSwamps and other wetlands have traditionally held a very low property value compared to fields, prairies, or woodlands. They have a reputation for being unproductive land that cannot easily be utilized for human activities, other than perhaps hunting and trapping. Farmers, for example, typically drained swamps next to their fields so as to gain more land usable for planting crops.\n\nMany societies now realize that swamps are critically important to providing fresh water and oxygen to all life, and that they are often breeding grounds for a wide variety of species. Indeed, floodplain swamps are extremely important in fish production. Government environmental agencies (such as the United States Environmental Protection Agency) are taking steps to protect and preserve swamps and other wetlands. In Europe, major effort is being invested in the restoration of swamp forests along rivers. Conservationists work to preserve swamps such as those in northwest Indiana in the United States Midwest that were preserved as part of the Indiana Dunes. The problem of invasive species has also been put into greater light such as in places like the Everglades.\n\nSwamps can be found on all continents except Antarctica.\n\nThe largest swamp in the world is the Amazon River floodplain, which is particularly significant for its large number of fish and tree species.\n\nThe Sudd and the Okavango Delta are Africa's best known marshland areas. The Bangweulu Floodplains make up Africa's largest swamp.\n\nThe Tigris-Euphrates river system is a large swamp and river system in southern Iraq, traditionally inhabited in part by the Marsh Arabs.\n\nIn Asia, tropical peat swamps are located in mainland East Asia and Southeast Asia. In Southeast Asia, peatlands are mainly found in low altitude coastal and sub-coastal areas and extend inland for distance more than along river valleys and across watersheds. They are mostly to be found on the coasts of East Sumatra, Kalimantan (Central, East, South and West Kalimantan provinces), West Papua, Papua New Guinea, Brunei, Peninsular Malaya, Sabah, Sarawak, Southeast Thailand, and the Philippines (Riley \"et al.\",1996). Indonesia has the largest area of tropical peatland. Of the total tropical peat swamp, about are located in Indonesia (Page, 2001; Wahyunto, 2006).\n\nThe Vasyugan Swamp is a large swamp in the western Siberia area of the Russian Federation. This is one of the largest swamps in the world, covering an area larger than Switzerland.\n\nThe Atchafalaya Swamp at the lower end of the Mississippi River is the largest swamp in the United States. It is an important example of southern cypress swamp but it has been greatly altered by logging, drainage and levee construction. Other famous swamps in the United States are the forested portions of the Everglades, Okefenokee Swamp, Barley Barber Swamp, Great Cypress Swamp and the Great Dismal Swamp. The Okefenokee is located in extreme southeastern Georgia and extends slightly into northeastern Florida. The Great Cypress Swamp is mostly in Delaware but extends into Maryland on the Delmarva Peninsula. Point Lookout State Park on the southern tip of Maryland contains a large amount of swamps and marshes. The Great Dismal Swamp lies in extreme southeastern Virginia and extreme northeastern North Carolina. Both are National Wildlife Refuges. Another swamp area, Reelfoot Lake of extreme western Tennessee and Kentucky, was created by the 1811–12 New Madrid earthquakes. Caddo Lake, the Great Dismal and Reelfoot are swamps that are centered at large lakes. Swamps are often called \"bayous\" in the southeastern United States, especially in the Gulf Coast region.\n\nThe world's largest wetlands include significant areas of swamp, such as in the Amazon and Congo River basins. Further north, however, the largest wetlands are bogs.\n\n\n\n\n\n"}
{"id": "22056917", "url": "https://en.wikipedia.org/wiki?curid=22056917", "title": "UK Energy Research Centre", "text": "UK Energy Research Centre\n\nThe UK Energy Research Centre (UKERC) is the focal point for UK research on sustainable energy, and is central to the Research Councils' Energy Programme. The centre has its headquarters at the campus of Imperial College, London, providing support to 70 researchers based in 11 universities and research institutions across the UK: UCL; Strathclyde; Leeds; Imperial College London; Exeter; Sussex; the University of East Anglia; the Plymouth Marine Laboratory; Cardiff; Oxford; Aberdeen.\n\nThe organisation takes an independent, whole-systems approach, drawing on technical and non-technical disciplines including engineering, economics and the physical, environmental and social sciences. The Centre helps to co-ordinate the overall UK energy research effort and acts as a bridge between the UK energy research community, the international research community, business and policymakers.\n\nUKERC was established in April 2004, following a recommendation from the 2002 Energy Review initiated by Sir David King, the UK Government's Chief Scientific Advisor. The Centre was set up to address key controversies in the energy field through comprehensive assessments of the current state of knowledge. The first phase of the Centre ran from 2004 - 2009.\n\nIn March 2009, £18.5 million was allocated to support the second phase of work at the UK Energy Research Centre for 2009 – 2014. Under the second phase of funding, UKERC focused on five themes: Energy Demand, Energy Supply, Energy Systems, Energy and Environment, and Technology and Policy Assessment.\n\nIn May 2014, the UK Energy Research Centre was awarded funding for a third phase of work, which would run from 2014 to 2019.\n\nUKERC's third phase combines a research programme focused on six core themes: future energy system pathways; resources and vectors; energy systems at multiple scales; energy, economy and societal preferences; decision making; technology, policy and assessment - with an HQ function aimed at engaging with the wider UK energy research community, policy makers and energy industry. \n\nThe Centre acts as the bridge between the UK energy research community and the wider world; fulfilling the role of a national energy research laboratory by both informing and representing the energy research community and acting as a catalyst for UK-wide collaboration in order to address the key energy challenges ahead.\n\nUKERC's interdisciplinary research studentships have enabled whole-systems interdisciplinary research across scientific, engineering and socio-economic boundaries.\n\nFull details of all UKERC's activities and research output can be found at UKERC's website.\n\nUKERC's Research Atlas is an information resource for current and past UK energy research and development activity. The online database has information on energy-related research capabilities in the UK and a series of energy roadmaps showing research problems to be overcome before new technologies can be made commercially viable.\nThe National Energy Research Network (NERN) is open to all energy researchers and other sectors, including business, and provides updates on news, jobs, events, opportunities and developments across the energy field in the form of a weekly newsletter.\n\nUKERC runs an annual Summer School, which runs for a week and combines seminars from internationally leading speakers with project work and professionally facilitated developmental activities.\n\n\n"}
{"id": "9553481", "url": "https://en.wikipedia.org/wiki?curid=9553481", "title": "Uras (mythology)", "text": "Uras (mythology)\n\nUraš or Urash, in Sumerian mythology, is a goddess of earth, and one of the consorts of the sky god Anu. She is the mother of the goddess Ninsun and a grandmother of the hero Gilgamesh.\n\nHowever, \"Uras\" may only have been another name for Antum, Anu's wife. The name \"Uras\" even became applied to Anu himself, and acquired the meaning \"heaven\". Ninurta also was apparently called \"Uras\" in later times.\n\n\n"}
