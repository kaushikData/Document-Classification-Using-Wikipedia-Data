{"id": "24779405", "url": "https://en.wikipedia.org/wiki?curid=24779405", "title": "2004 Russia–Belarus energy dispute", "text": "2004 Russia–Belarus energy dispute\n\nThe 2004 Russia–Belarus energy dispute was a commercial and diplomatic dispute between Russia and Belarus that escalated in January 2004. Close relations between the two countries and willingness for political integration had made it possible for Belarus to purchase gas from Russia at heavily discounted prices. In the late 1990s, Russian foreign policy shifted away from geopolitics and became more pragmatic and economical, especially after the inauguration of President Vladimir Putin. As a result, Gazprom moved to ensure the reliability of gas transits to Europe by attempting to establish control over the Belarusian transit network. Belarus initially agreed to sell 50% of the network, but after disagreements over price, Belarus severed the contract. Gazprom announced price rises, and after Belarus refused, Gazprom ceased to import gas to Belarus on 1 January 2004. Belarus compensated by siphoning from gas meant for transit to Europe, which on 18 February resulted in Gazprom completely shutting off the supply to Belarus. Other companies supplied Belarus on short-term contracts until June 2004, when a new contract with Gazprom was finally signed.\n\nBelarus is an important transit route of Russian gas to Europe, with around 20–25% of Gazprom's European exports passing through Belarusian territory. Two major pipelines run through the country: Northern Lights and Yamal-Europe. The former is used to transit Russian gas to Europe as well as for Belarusian domestic use; the latter transits gas solely for export to Europe. The Belarusian economy is heavily gas dependent—gas accounted for 59.9% of the country's energy balance in 2003. In addition, most of the electricity in the country is generated from gas. In 2003, Belarusian gas consumption was . Domestic gas production amounted to only . The rest was imported from Russia, chiefly from Gazprom. For political reasons, Belarus was able to purchase gas from Russia for Russian domestic prices, which were only a quarter of the international market price. In January, 2003, Belarus paid $34.37 per 1,000 cubic meters for its imports.\n\nAfter the dissolution of the Soviet Union in 1991, Russia and Belarus enjoyed relatively good relations. Both countries strove for political integration, Russia mostly for geopolitical and Belarus chiefly for economic reasons. Russia also saw political integration as a means for eventually gaining full control over the Belarusian transit routes—thus ensuring the reliability of transit. The Belarusian leadership chose to build close relations with Russia, with the ultimate aim of formal unification. Gas price agreements between the two countries were settled politically, with the commercial side being given less attention.\n\nThe relations between the two countries began to change as a result of the 1998 Russian financial crisis. Russia no longer saw itself strong enough to sustain its aspirations of superpower status. Consequently, Russia began to attach more significance to geo-economics rather than geo-politics in its relations with CIS neighbours. This development accelerated during Vladimir Putin's presidency, when Russian foreign policy became more pragmatic and economised.\n\nBecause of domestic payment collection problems, Belarus accumulated debts for its gas imports. When disagreements over the political integration increased, Gazprom realized that the Belarusian debts would in future undermine the reliability of Belarusian transit routes. Consequently, Gazprom sought to establish a joint venture to own and operate the Belarusian transit network, to ensure uninterrupted transit of gas to Europe.\n\nIn an intergovernmental agreement signed in April 2002, Belarus promised to sell 50% of Beltransgaz, the company owning the Belarusian transit network, to Gazprom. The agreement also stipulated that gas prices to Belarus would be the same as Russian domestic prices for the next five years. The contract did not specify the value of Beltransgaz. Belarus estimated it as $5–6 billion, while Gazprom proposed a price of $500–600 million. The Belarusian President Alexander Lukashenko later suggested $2.5 billion as a lower limit, but this was rejected by Gazprom. As the political agreements that had given Belarus the right to purchase gas at Russian domestic prices were now broken, Gazprom, backed by the Russian government, now moved to abolish the price discounts.\n\nGazprom stated that if an agreement was not signed until 2004, it would increase gas prices from $30/m to $50 per 1,000 cubic meters. Belarus refused, and on 1 January 2004, Gazprom stopped shipping gas via the Northern lights pipeline. Belarus was able to compensate by purchasing gas from non-Gazprom exporters such as Itera and TransNafta on short-term contracts. This continued until 18 February, when the companies refused to sign further short-term supply contracts. Since Belarus was dependent on gas for most of its heat and electricity production, the situation in the country during cold winter started to become critical. After deliveries stopped, Belarus started to siphon gas meant for transit to Europe from the Yamal-Europe pipeline, without Gazprom's approval. As a result, at 18:00 Moscow time on 18 February, Gazprom completely cut off supplies to the Belarusian network. Germany experienced only minor shortfalls in deliveries because of extensive storages of gas and due to most of imports coming through Ukraine; however, Poland reported more severe disruption. Supplies to Kaliningrad Oblast were also affected. Belarus managed to sign a new short-term contract with TransNafta at the price of $46.68 per 1,000 cubic meters, which resulted in Gazprom resuming supplies before midnight of 19 February. Similar contracts supplied Belarus until June, when Belarus finally agreed a new contract with Gazprom for delivering gas for the rest of 2004 with the price of $46.68 per 1,000 cubic meters.\n\nIn mid-2004, political relations between Belarus and Russia started to improve, and a new agreement between Belarus and Gazprom was signed. The two sides now agreed to appoint an outside consultancy firm to define an appropriate value for the sale of Beltransgaz.\n\nAlthough the 2004 dispute further strengthened the perception that Belarus and its economy were heavily dependent on Russian gas and Gazprom, it also became clear that Belarus also possessed some important cards. In 2007, after a later dispute, Gazprom agreed to pay $2.5 billion for Beltransgaz—several times more than it was prepared to pay in 2004. The 2004 dispute also raised concerns about reliability of Gazprom's supplies to Europe, and highlighted the fact that Gazprom had not solved the issue of reliable transit.\n"}
{"id": "52432122", "url": "https://en.wikipedia.org/wiki?curid=52432122", "title": "Allam power cycle", "text": "Allam power cycle\n\nThe Allam power cycle or Allam cycle is a process for converting fossil fuels into mechanical power, while capturing the generated carbon dioxide and water. The main inventor behind the process is English engineer Rodney John Allam.\n\nA conventional combined cycle power station burns natural gas or gasified coal with air to drive a gas turbine and then uses the hot exhaust gas to heat water and drive a steam turbine. The working fluid for the gas turbine is carbon dioxide and water vapor from the combustion, along with nitrogen from the air (partially converted into nitrogen oxide). With this system, it is possible to achieve an LHV efficiency of around 55 to 59 percent. The gases driving the gas turbine will also constitute the principal emissions from the station, unless captured by an external system, at expense of a lower total efficiency. In addition, the station requires a cooling tower or access to large amounts of water for cooling.\n\nIn contrast, the Allam cycle uses a single turbine, driven by a working fluid consisting of only water and carbon dioxide, the mass of the latter being around 97 percent of the total. This is achieved by using pure oxygen instead of air to burn the fuel. Similarly to combined-cycle plants, the intended fuel is natural gas or gasified coal. The exhaust gas is cooled in a heat exchanger, and the steam is condensed and separated from the flow, becoming a potential source of fresh water. The carbon dioxide is compressed mechanically, and a small amount, matching the amount continuously added through combustion, is captured at high pressure, ready for pipeline transmission. The rest of the carbon dioxide is reheated in the heat exchanger and recycled into the combustion unit, where it continues to form the vast majority of the working fluid.\n\nEven though only one turbine is used, the Allam cycle can potentially achieve efficiencies up to about 59 percent (LHV) for natural gas and 51–52 percent (LHV) for gasified coal. This is due to the use of carbon dioxide as the main constituent of the working fluid. In its supercritical state, which it reaches in the combustion unit, carbon dioxide is very efficient for driving a turbine. Also, energy losses from phase transitions of water are avoided, which allows Allam cycle plants to recover more energy in their heat exchangers than combined cycle plants can do. At this efficiency, carbon dioxide capture is already a part of the Allam cycle, whereas if desired in conventional plants, the capture has to be added as an external function that may decrease the overall efficiency by around 10 percent.\n\nUnlike the combined cycle, the Allam cycle requires an air separation unit, to supply the cycle with oxygen. On the other hand, there is no need for large-scale external cooling, there is only one turbine, and small components can be used, as the system operates at high pressures and thus high power densities. Complete Allam cycle power stations can therefore be smaller than combined cycle stations, even before adding the considerable size of carbon dioxide capture equipment. This potentially makes the capital costs for the Allam cycle lower than for the combined cycle at equivalent capacities. Other advantages include the complete absence of nitrogen oxide in the waste, and for stations located in dry environments, the integrated generation of water.\n\nA 25 MW/50 MW demonstration plant for electricity generation using the Allam cycle is being built in La Porte, Texas. The project is a collaboration between, among others, Exelon, Toshiba, McDermott, and \"8 Rivers Capital\"'s portfolio company, NET Power. Construction started in March 2016 and was completed in 2017. As of June 2018 it is undergoing testing prior to commencement of service.\n\nThe carbon dioxide supplied by Allam cycle plants could potentially be sold for use in enhanced oil recovery.\n\n"}
{"id": "8641237", "url": "https://en.wikipedia.org/wiki?curid=8641237", "title": "Baptismal vows", "text": "Baptismal vows\n\nBaptismal vows are the renunciations required of an adult candidate for baptism just before the sacrament is conferred. In the case of an infant baptism they are given by the godparents.\n\nAccording to the Roman Ritual, three questions are addressed to the person to be baptized: \"Dost thou renounce Satan? and all his works? and all his pomps?\" To each of these interrogation the person, or the sponsor in his name, replies: \"I do renounce\".\n\nThe practice of renewing the baptismal promises is more or less widespread and often happens in association with other sacraments of First Holy Communion and Confirmation.\n"}
{"id": "34524359", "url": "https://en.wikipedia.org/wiki?curid=34524359", "title": "Birmingham Natural History Society", "text": "Birmingham Natural History Society\n\nBirmingham Natural History Society is a learned society for the study of the natural history of Birmingham, England, and in the surrounding midlands region, and beyond. It was founded in 1858, and is a registered charity. The Society has had various names, e.g. in the 1870s it was called the Birmingham Natural History and Microscopical Society, and from 1894 to 1963 the Birmingham Natural History and Philosophical Society.\n\nThe Society was founded in 1858. For a considerable part of its early life, it was called the Birmingham Natural History and Microscopical Society. In 1877, the Society played the lead in forming the Union of Midland Natural History Societies, which published the journal \"The Midland Naturalist\". Some of the societies in the union later merged with the Birmingham Natural History Society. The Birmingham Philosophical Society, founded in 1876, merged in 1894, when the title of the Society was changed to the Birmingham Natural History and Philosophical Society. The Midland Malacological Society and the Birmingham Entomological Society were amalgamated in 1906 and 1908 respectively. The society's activities were greatly reduced by world war one but activity had picked up again by 1920. In 1924 the society carried out an extensive survey of Hartlebury Common.\n\nThe Society rooms suffered bomb damage on October 25, 1940 and as a result the society suspended its activities for the remainder of hostilities. The society began to meet again in 1945 and restarted publication of its proceedings in 1950.\n\nThe current title of the Society was first used in its \"Proceedings\" in 1964.\n\nImportant early members of the society included the botanist James Eustace Bagnall (1830-1918), who produced the first Flora of Sutton Park (now a national nature reserve) and later the first \"Flora of Warwickshire\".\n\nThe society was responsible for the designation of Edgbaston Pool as a Site of Special Scientific Interest, and until 2012 was formally involved in its management.\n\nThe society operates a library for its members, and holds regular meetings (at the Friends Meeting House in Selly Oak, ), field trips and training sessions. It publishes a journal, \"Proceedings of the Birmingham Natural History Society\" (ISSN 0144-1477). First published in 1870, the journal currently appears in four parts to a volume; parts have appeared a different intervals over time, e.g. each part of Volume 15 covered two calendar years whereas each part of Volume 29 covered three.\n\nAs of 2012, the hon. secretary is Dr Peter Jarvis.\n\n\n"}
{"id": "18952367", "url": "https://en.wikipedia.org/wiki?curid=18952367", "title": "Chilean Matorral", "text": "Chilean Matorral\n\nThe Chilean Matorral (NT1201) is a terrestrial ecoregion of central Chile, located on the west coast of South America. It is in the Mediterranean forests, woodlands, and scrub biome, part of the Neotropic ecozone.\n\nMatorral is typically characterized by a temperate Mediterranean climate, with rainy winters and dry summers. It is one of the world's five Mediterranean climate regions, which are all located in the middle latitudes on the west coast of continents. The Mediterranean Basin, the California chaparral and woodlands ecoregion of California and Baja California, the Cape Province of South Africa, and southwestern corner of Australia are the other Mediterranean climate regions.\n\nThe Matorral occupies central Chile between 32° and 37° south latitude. The Pacific Ocean lies to the west, and the Chilean Coastal Range lies parallel to the coast. The Chilean Central Valley lies between the Coastal range and the Andes Mountains, which bound the Matorral ecoregion on the east. To the north is the extremely dry Atacama desert, which separates the Matorral from the tropical forests of northern South America. A semi-desert region known as \"El Norte Chico\", (the \"little north\") lies between 28° and 32° south latitude, and is the transition zone between the Atacama desert and the Matorral. To the south lies the cooler and wetter Valdivian temperate rain forests ecoregion, which includes most of South America's temperate rain forests.\n\nThe Chilean Matorral ecoregion is home to several plant communities.\n\nThe ecoregion has many endemic plant species, with affinities to the South American tropics, the Antarctic flora, and the Andes. About 95% of the plant species are endemic to Chile, including \"Gomortega keule\", \"Pitavia punctata\", \"Nothofagus alessandrii\", and the Chilean Wine Palm, \"Jubaea chilensis\".\n\nThe Matorral contains the majority of Chile's population and largest cities. The Central valley is Chile's main agricultural region, and the region is also subject to extensive grazing, logging, and urbanization. Of Chile's ecoregions, the Matorral is the least protected by national parks and preserves.\n\n"}
{"id": "53045879", "url": "https://en.wikipedia.org/wiki?curid=53045879", "title": "Cholchol Formation", "text": "Cholchol Formation\n\nCholchol Formation () is a geological formation composed of sediments that were deposited during the Miocene in the Temuco Basin of south–central Chile. The sediments were deposited in a marine environment. \n"}
{"id": "2606596", "url": "https://en.wikipedia.org/wiki?curid=2606596", "title": "Christianization of Poland", "text": "Christianization of Poland\n\nThe Christianization of Poland (Polish: \"chrystianizacja Polski\") refers to the introduction and subsequent spread of Christianity in Poland. The impetus to the process was the Baptism of Poland (), the personal baptism of Mieszko I, the first ruler of the future Polish state, and much of his court. The ceremony took place on the Holy Saturday of 14 April 966, although the exact location is still disputed by historians, with the cities of Poznań and Gniezno being the most likely sites. Mieszko's wife, Dobrawa of Bohemia, is often credited as a major influence on Mieszko's decision to accept Christianity.\n\nWhile the spread of Christianity in Poland took centuries to finish, the process was ultimately successful, as within several decades Poland joined the rank of established European states recognised by the papacy and the Holy Roman Empire. According to historians, the baptism of Poland marks the beginning of Polish statehood. Nevertheless, the Christianization was a long and arduous process, as most of the Polish population remained pagan until the pagan reaction during the 1030s.\n\nBefore the adoption of Christianity in modern- day Poland, there were a number of different pagan tribes. Svetovid was among the most widespread pagan gods worshiped in Poland. Christianity arrived around the late 9th century, most likely around the time when the Vistulan tribe encountered the Christian rite in dealings with their neighbors, the Great Moravia (Bohemian) state.\n\nThe Moravian cultural influence played a significant role in the spread of Christianity onto the Polish lands and the subsequent adoption of that religion. In the opinion of Davies, the Christianization of Poland through the Czech–Polish alliance represented a conscious choice on the part of Polish rulers to ally themselves with the Czech state rather than the German one. In a similar fashion, some of the later political struggles involved the Polish Church refusing to subordinate itself to the German hierarchy and instead being directly subordinate to the Vatican.\n\nThe \"Baptism of Poland\" refers to the ceremony when the first ruler of the Polish state, Mieszko I and much of his court, converted to the Christian religion. Mieszko's wife Dobrawa of Bohemia, a zealous Christian, played a significant role in promoting Christianity in Poland, and might have had significant influence on converting Mieszko himself.\n\nThe exact place of Mieszko's baptism is disputed; historians, however, argue that Gniezno or Poznań are the most likely sites. Some historians have suggested less likely alternative locations, such as Ostrów Lednicki, or even in German Regensburg. The date of Mieszko's baptism was on the Holy Saturday of 14 April 966.\n\nThe ceremony was preceded by a week of oral catechism and several days of fasting. The actual ceremony involved pouring water over the segregated groups of men and women, although it is possible that their heads were immersed instead, and anointed with the chrism.\n\nThe baptismal mission which began in the two major cities of Gniezno and Poznań with the baptism of Mieszko and his court spread throughout the country. During the 10th and 11th centuries various ecclesiastical organs were established in Poland. This included the building of churches and the appointment of clergy. The first Bishop of Poland, Jordan, was appointed by Pope John XIII in 968. Mieszko's son Bolesław I Chrobry supported Christianization missions to neighboring lands, notably the mission of future Saint Adalbert of Prague to Old Prussians, and established the Archbishopric of Gniezno in the year 1000.\n\nAlthough at first the Christian religion was \"unpopular and alien\", Mieszko's baptism was highly influential but needed to be enforced by the state, and ran into some popular opposition, including an uprising in the 1030s (particularly intense in the years of 1035–1037). Nonetheless, by that time Poland had won recognition as a proper European state, both from the papacy and from the Holy Roman Empire.\n\nOut of various provinces of today's Poland, Christianity's spread was slowest in Pomerania, where it gained a significant following only around the 12th century. Initially, the clergy came from the Western Christian European countries; native Polish clergy took three or four generations to emerge, and were supported by the monasteries and friars that grew increasingly common in the 12th century. By the 13th century Roman Catholicism had become the dominant religion throughout Poland.\n\nIn adopting Christianity as the state religion, Mieszko sought to achieve several personal goals. He saw Poland's baptism as a way of strengthening his hold on power, as well as using it as a unifying force for the Polish people. It replaced several smaller cults with a single, central one, clearly associated with the royal court. It would also improve the position and respectability of the Polish state on the international, European scene. The Church also helped to strengthen the monarch's authority, and brought to Poland much experience with regard to state administration. Thus, the Church organisation supported the state, and in return, bishops received important government titles (in the later era, they were members of the Senate of Poland).\n\nThe preparations for the millennial celebrations begun with the Great Novena of 1957, which marked a nine years period of fast and prayer. In 1966, the People's Republic of Poland witnessed large festivities on the 1,000-year anniversary of those events, with the Church celebrating the 1,000 years of Christianity in Poland, while the Communist government celebrated the secular 1,000 years of the Polish State, culminated in twice denying Pope Paul VI permission to visit Poland that year. The Communist state's desire to separate religion from the state made the festivities a culture clash between the state and the Church. While the Church was focusing on the religious, ecclesiastical aspects of the baptism, with slogans (in Latin) like \"Sacrum Poloniae Millenium\" (Poland's Sacred Millennium) the Communist Party was framing the celebrations as a secular, political anniversary of the creation of the Polish state, with slogans (in Polish) like \"Tysiąclecie Państwa Polskiego\" (A Thousand Years of the Polish State). As Norman Davies noted, both the Church and the Party had \"rival, and mutually exclusive, interpretations of [Poland's baptism] significance.\"\n\nOn 30 July 1966, the U.S. Bureau of Engraving and Printing issued 128,475,000 commemorative stamps honoring the millennium anniversary of the adoption of Christianity in Poland.\n\n"}
{"id": "20857581", "url": "https://en.wikipedia.org/wiki?curid=20857581", "title": "Coal-water slurry fuel", "text": "Coal-water slurry fuel\n\nCoal-water slurry fuel is a combustible mixture of fine coal particles suspended in water. It can be used to power boilers, gas turbines, diesel engines and heating and power stations.\n\nA coal-water slurry fuel is defined by a number of factors including its viscosity, particle size, rate of sedimentation, ignition temperature (), combustion temperature (), ash content and calorific value (3700–4700 kcal/kg).\n\nWhen coal-water slurry fuel combusts, over ninety-nine percent of its carbon content is consumed. Coal-water slurry fuel is fire-proof and explosion-proof. Ash content of less than ten percent is desirable for boilers. For diesel engines, there is no limit.\n\nThe production of coal-water slurry fuel involves the crushing of coal or coal sludge to particles between 10 and 65 micrometers diameter (standard crushers can be used); wet milling and homogenisation (with additives as required). The resulting product (coal slurry) is then prepared for intermediate storage or transport. \n\nLarge particle coal-water slurry fuel can be used to produce steam in boilers. Smaller (under 80 micrometer) particle coal-water slurry fuel can be used in diesel engines with or without co-fuels. For example, low speed marine or modular power plant diesels can operate on pure coal-water slurry fuel whereas medium speed diesels such as locomotive engines may need diesel as a co-fuel which will act as an ignition source. Very small (5 to 10 micrometer) particle coal-water slurry fuel has been trialled in combined cycle gas turbine power plants. Smaller sized particles are more versatile in use but are more difficult to produce.\n\nConverting coal into a liquid form may simplify the delivery and dispensing of the fuel. It may be a cost-efficient alternative to oil and natural gas. Separating non-carbonaceous material before making the slurry may reduce the production of ash to two percent.\n\nIn the late 1950s, the Soviet Union looked for new methods of using coal sludge for power generation. Coal-water slurry fuel was made in a ball mill which pulverised the coal or coal sludge. This was done near a coal mine in Belovo, Siberia. The coal-water slurry fuel was transported through a pipeline to Novosibirskaya TEC-5, Novosibirsk, a distance of . The pipeline had three intermediate pumping stations. The Belovo Novosibirsk project used coal-water slurry fuel in steam boilers at a rate of 1340 tonnes per hour. There was a further project at Belovo Novosibirsk between 1989 and 1993 and development of coal-water slurry fuel technology for district heating stations and power stations. In 2004, a coal-water slurry fuel production plant was opened at Enskiy village, Murmansk.\n\nIn 2012, the Austrian company, Effective Energy Technology GmbH (EET) built facilities for production and combustion of coal-water slurry fuel in Europe. Examples of the technology in practice are \"hydroshock\" type wet-milling devices and standard water boilers equipped with swirl pre-chamber and coal-water slurry nozzle., developed by group of companies EET.\n\nVerkhovna Rada requested China Development Bank credit for the implementation of coal-water slurry fuel technology. \"Ukteplokom\", a state owned entity, commenced operation.\n\n\n"}
{"id": "2108104", "url": "https://en.wikipedia.org/wiki?curid=2108104", "title": "Cooperative Institute for Climate Science", "text": "Cooperative Institute for Climate Science\n\nThe Cooperative Institute for Climate Science (CICS) fosters research collaborations between the National Oceanic and Atmospheric Administration (NOAA)/Office of Oceanic and Atmospheric Research (OAR) Geophysical Fluid Dynamics Laboratory (GFDL) and the Princeton University. It is one of 16 NOAA Cooperative Institutes (CIs).\n\nThe CICS research themes are:\n"}
{"id": "58511110", "url": "https://en.wikipedia.org/wiki?curid=58511110", "title": "Doreen Wallace", "text": "Doreen Wallace\n\nDoreen Wallace (Doreen Eileen Agnew Wallace, married name Rash, 1897 – 22 October 1989) was an English novelist, grammar school teacher and social campaigner. In more than 40 novels she has been seen to explore examples of \"comic and tragic cross-purposes between different classes, sexes and generations\".\n\nBorn in Low Lorton, Cumberland (now in the Cumbrian borough of Allerdale) in 1897, Doreen Wallace was the only child of R. B. Agnew Wallace and his wife Mary Elizabeth, \"née\" Peebles. She was educated at Malvern Girls' College and then took an English honours degree course at Somerville College, Oxford. Her father introduced her to poetry, but also drank heavily and sometimes fought with his wife.\n\nAfter teaching for a while at Diss Grammar School in Norfolk, she was married in 1922 to Rowland H. Rash, a long-established farmer and landowner in Wortham, Suffolk. While they raised three children, she taught part-time for the Workers' Educational Association and began a writing career that lasted into the 1980s.\n\nIn 1978, two years after writing her last novel, Doreen Wallace returned to live in Diss for the remaining eleven years of her life, leaving her son Rowland Murray Wallace Rash at Wortham. Of her two daughters, Stella became an artist and Laura farmed in Arizona for many years before returning to Diss.\n\nDoreen Wallace died of bronchopneumonia on 22 October 1989. A biography of her appeared in 2000.\n\nWallace's social campaigning came to the fore in the agricultural depression of the 1930s. Her moves against the imposition of tithes led to the stock of two farms being impounded in 1935, a siege at Wortham Manor, confrontation with local Blackshirts, and bankruptcy in 1939. The tithe issue led to disagreement and estrangement from Dorothy Sayers, who had been a close friend.\n\nWhile recent graduates, Wallace and Eleanore Geach contributed a joint \"Ballade of Ladies Who Died for Love\" to the volume \"Oxford Poetry 1918\", questioning why a woman would \"Seek death to end your misery?/Why did ye not forget your pain/In new loves and new ecstasy?\" She also collaborated with Geach on a volume of verses, \"–Esques\", in the same year.\n\nHowever, novels, agriculture and gardening were to be Wallace's mainstays as a writer. Her first novel, \"A Little Learning\" (1931) has a girl with an Oxford degree escaping from her rough farming family into a dull, loveless marriage. Her 45 novels cover in comic and tragic ways conflicts between different classes, sexes and generations. \"How Little We Know\" (1949) features a retired colonel and his wife failing to notice the danger to their proletarian maid from a genteel lodger. In \"Daughters\" (1955), the middle-class girls are overcome by tedium. \"Woman with a Mirror\" (1963) tells of the downfall of a flirt in a little Lake District town. \"Barnham Rectory\", an earlier novel (1934), was declared a \"choice\" by the Book Society. Her other novels include \"Even Such Is Time\" (1934), \"Going to Sea\" (1936), \"The Time of Wild Roses\" (1938), \"Green Acres\" (1941), \"The Noble Savage\" (1945), \"Willow Farm\" (1948), \"Sons of Gentlemen\" (1953), \"Daughters\" (1955), \"Woman with a Mirror\" (1963), \"The Mill Pond\" (1966), \"An Earthly Paradise\" (1971) and \"Landscape with Figures\" (1976).\n\nWallace has been described as belonging to the Somerville School of novelists, along with Vera Brittain, Winifred Holtby, Muriel Jaeger, Margaret Kennedy and others.\n\nWallace's non-fiction mainly deals with the countryside – \"The Tithe War\" (a vivid account of her activism, 1934), \"East Anglia\" (with R. P. Bagnall-Oakeley, 1939), \"How to Grow Food\" (1940), a gardening diary entitled \"In a Green Shade\" (1950), and \"Outlook for Farming\". She also wrote for \"The Times\" and the \"Eastern Daily Press\" in this period.\n\nWallace's poem \"Ninety-First Birthday Ode\", dated 18 June 1988, includes arguments for the right to die and permitting euthanasia.\n\nWallace also gained a local reputation as a water-colour painter.\n"}
{"id": "22560835", "url": "https://en.wikipedia.org/wiki?curid=22560835", "title": "Downhole heat exchanger", "text": "Downhole heat exchanger\n\nA downhole heat exchanger, (DHE) also called a borehole heat exchanger, (BHE) is a heat exchanger installed inside a borehole. It is used to capture or dissipate heat to or from the ground. DHT's are used for geothermal heating, sometimes with the help of a geothermal heat pump.\n\nThe heat exchanger usually consists of one or two u-tubes through which the carrier fluid, usually water, circulates. The space around the u-tubes is filled with groundwater or backfilled with thermally conductive grout.\n\nAnother design uses a single open pipe to flow water downward. The water then returns through the annular gap between the pipe and the casing. This design provides better thermal contact than u-tubes, but risks contamination by groundwater. Since this involves practically no downhole equipment, these systems usually only go by the name of borehole heat exchangers (BHT).\n\nIf no casing is installed and groundwater is permitted to charge the system, this arrangement is no longer a BHT, but rather a standing column well.\n"}
{"id": "1565410", "url": "https://en.wikipedia.org/wiki?curid=1565410", "title": "Elephanta (wind)", "text": "Elephanta (wind)\n\nThe Elephanta is a strong southerly or southeasterly wind which blows on the Malabar coast of India during the months of September and October and marks the end of the southwest monsoon.\n\n"}
{"id": "6300866", "url": "https://en.wikipedia.org/wiki?curid=6300866", "title": "Fast Auroral Snapshot Explorer", "text": "Fast Auroral Snapshot Explorer\n\nThe Fast Auroral Snapshot Explorer (FAST) is a NASA plasma physics satellite, and is the second spacecraft in the Small Explorer program. It was launched on August 21, 1996, from Vandenberg Air Force Base aboard a Pegasus XL rocket. The spacecraft was designed and built by NASA's Goddard Space Flight Center. Flight operations were handled by Goddard for the first three years, and thereafter were transferred to the University of California, Berkeley's Space Sciences Laboratory.\n\nFAST was designed to observe and measure the plasma physics of the auroral phenomena which occur around both of Earth's poles. While its Electric Field Experiment failed around 2002, all other instruments continued to operate normally until science operations were ended on May 1, 2009. Various engineering tests were conducted afterward.\n\n\n\n"}
{"id": "8331834", "url": "https://en.wikipedia.org/wiki?curid=8331834", "title": "Fengyun 2-05", "text": "Fengyun 2-05\n\nFengyun 2-05, also known as Fengyun 2D is a Chinese weather satellite which was launched in 2006. It was the fourth satellite to be launched of the Fengyun 2 series, and the second operational spacecraft. It is part of the Fengyun programme.\n\nA Long March 3A carrier rocket was used to launch Fengyun 2-05, flying from Launch Area 2 at the Xichang Satellite Launch Centre. The launch took place at 00:53 UTC (08:53 CST) on 8 December 2006, with the carrier rocket placing the satellite into a geosynchronous transfer orbit. An FG-36 apogee motor was then used to raise the satellite into geosynchronous orbit. By 7 February 2007, it was in an orbit with a perigee of , and apogee of , and 2.4 degrees inclination. It is positioned at a longitude of 86.5 degrees east.\n"}
{"id": "22163671", "url": "https://en.wikipedia.org/wiki?curid=22163671", "title": "Genoese map", "text": "Genoese map\n\nThe Genoese map is a 1457 world map. The map relied extensively on the account of the traveler to Asia Niccolo da Conti, rather than the usual source of Marco Polo. The author is not known, but is a more modern development than the Fra Mauro world map, with fairly good proportions given to each continents. The map also depicts a three-masted European ship in the Indian Ocean, something which had not occurred yet at the time.\n\nA Genoese flag in the upper northwest corner of the map establishes this map’s origin, along with the coat of arms of the Spinolas, a prominent Genoese mercantile family. Niccolò de'Conti was from a noble mercantile family; at an early age he decided to follow in the family tradition by establishing a lucrative trading operation in the East.\n\nThe Genoese map’s sea monsters reflect the cartographer’s interest in exotic wonders, which is everywhere in evidence on the map, and typical of the scientific outlook of the early modern period, which was driven by curiosity and took a great interest in marvels. The demon-like monster in particular is evidence of the cartographer’s research in recent travel literature to find sea monsters for his map.\n\nThis map was done in rich color and was not made particularly used for anything but for display.They say this map was sent to the Portuguese court in 1474 and then to Columbus and this is the map that he used to travel the India sea to the Atlantic but was never proven. The map is now the property of the Italian government and is to be found in the Biblioteca Nazionale Centrale of Florence however the map was not taken care of due to the way it’s been handled throughout the years.\n\nThe ships that could carry a thousand men—six hundred seamen and four hundred soldiers. The larger vessels also carried small boats, which were used, as Marco Polo states, “to lay out the anchors, catch fish, bring supplies aboard, and the like. When the ship is under sail, she carries these boats slung to her side.” Many of the vessels had as many as four decks, and even the smaller ones, fifty or sixty cabins. Vegetables, we are told, were sometimes grown on board.\n\nThe oval form is not unknown among medieval maps. Hugh of Saint Victor had described the world as being the shape of Noah’s Ark, and Ranulf Higden world maps were oval. A standard way of describing the earth was to compare it to an egg. The main purpose of the analogy seems to have been to describe the various spheres surrounding the earth (egg white, shell), but the idea of an egg shape could have been derived from these works. Another possibility is that the oval form represents the \"mandorla\", or nimbus, which surrounded Christ in many medieval works of art.\n\n\n"}
{"id": "41358817", "url": "https://en.wikipedia.org/wiki?curid=41358817", "title": "German Horton Hunt Emory", "text": "German Horton Hunt Emory\n\nGerman H.H. Emory (1882–1918) was a prominent American lawyer and soldier from Baltimore. He was awarded the Distinguished Service Cross \"for extraordinary heroism in action while serving with the 3rd Battalion, 320th Infantry Regiment, 80th Division, American Expeditionary Forces, near Sommerance, France November 1, 1918.\" He was killed in action in the Meuse-Argonne Offensive, in the waning days of World War I. He is commemorated with a memorial painting and two memorial sculptures in Maryland courthouses.\n\nGerman H.H. Emory was born on September 27, 1882, in Murner's Branch, Allegany County, Maryland. His father, William Hopper Emory (1849–1908), was an insurance broker. His mother was Eleanor Louisa Hunt (1851–1927). He grew up at 1403 West Lanvale Street in Baltimore, which is now the site of Harlem Park Middle School.\nThe Emory Family has deep roots on the Eastern Shore of Maryland. Arthur Emory, Sr. (c. 1640-1699) arrived in Maryland in the 1660s. In 1667 he is reputed to have received from Lord Baltimore several land grants on the Wye, Choptank, and Chester rivers on the Eastern Shore of the Chesapeake Bay.\nThe Poplar Grove Plantation, near Centreville, Maryland, is the ancestral home of the Emorys. It lies on Spaniard Neck, a peninsula on the south side of the Chester River in Queen Anne's County, just above the mouth of the Corsica River. Today the C.V. Starr Center for the Study of the American Experience is overseeing the Poplar Grove Project, an ongoing exploration of the Emory family's papers, in conjunction with the Maryland State Archives. To date, over 28,406 documents have been recovered and scanned.\nThe Emory family produced two celebrated military men in the 19th century: Col. Thomas Emory, (1782-1842), and one of his sons, General William H. Emory.\n\nGerman was named after his maternal grandfather, German Horton Hunt (1828–1907), a leading Baltimore manufacturer of the 19th century, whose Poole and Hunt Engineering and Machine Company produced machinery and castings for a worldwide market from 1853 to 1889.\nEmory was educated in Baltimore, at The Hill School, near Pottstown, Pennsylvania, and at St. Luke's School in Philadelphia. He then entered the University of Maryland School of Law, the second oldest law school in the country. While there he joined the Eta Chapter of Phi Sigma Kappa, a social fraternity, and became the 51st member to sign the Eta Chapter Roll. Graduating from the law school earlier than others, he was forced to wait until his twenty-first birthday to be admitted to the Maryland Bar in 1903.\n\nEmory married Lucy Imogen Stump (1883–1978) on June 15, 1907, in Baltimore. Lucy was also from Baltimore.\n\nThe Stump Family had been in Maryland since Colonial times, and were active in the Susquehanna River area, Western Maryland, and Baltimore in milling, mining, and banking. They also birthed a Confederate Brigadier General, James J. Archer.\n\nOne of Lucy's three sisters, Anna Abell Stump, married William Lee Rawls, \"one of the most prominent attorneys in Baltimore\", and gave birth to five sons, including philosopher John Rawls.\n\nGerman and Lucy had three boys. Their oldest son, German H. H. Emory, Jr. (1910–1980), became the president of the Riegel Textile Company in New York City German and Lucy's second son, Richard Woollen Emory (1913–2003), followed his father into the law and practiced for many years in Maryland at Venable, Baetjer and Howard in Baltimore. The major's third son, Morris Soper Emory (1915–2000), was named after Judge Soper, and worked in the insurance business and lived in Philadelphia most of his life.\n\nLucy Emory remained a widow for 60 years until her death in 1978.\n\nEmory began the practice of law with the firm of Slingluff & Slingluff from 1903 to 1906, moving to private practice from 1906 to 1907. In 1908, Emory became a principal in the law firm of Johnson, Emory, Olmstead and Cator. During this period, in 1910, he also served as Assistant City Solicitor for Baltimore City, serving under Edgar Allan Poe, the second cousin of the famous American writer. In 1911, Emory formed a partnership with Morris A. Soper, who would soon become the Chief Judge of the Supreme Bench of Baltimore City. When Judge Soper ascended to the bench in 1913, Emory made the final change of his legal career, becoming a member of the firm of Frank, Emory & Beeuwkes.\nOn January 8, 1919, The Supreme Bench of Baltimore held memorial proceedings for the six Baltimore lawyers killed in the First World War. Asked to speak about German Emory was Albert Cabell Ritchie, then the Attorney General of Maryland, and soon to become the longest-serving governor in Maryland's history. Mr. Ritchie reflected in part, \"Emory's knowledge of the law was broad. His advice was sound. His trial practice, at nisi prius and in the appellate courts, was very large. He was a born lawyer, broadened and developed by constant study and application and by tireless industry.\"\nIn his speech about German Emory at another memorial service held in 1933 for the Six Members of the Baltimore Bar who were killed in the First World War, organized by the German H.H.Emory Post of the American Legion and held in the Baltimore City Courtroom, Chief Judge Samuel K. Dennis, of the Supreme Bench of Baltimore recollected, \"Emory was an especially gifted lawyer. Ready, suave, cool, democratic and magnetic, he captured courts and juries, made converts to his cause. He loved his profession and was signally successful in it. He was considered one of the best trial lawyers in the city.\nBusiness also engaged Emory in 1914 as vice-president of Poole Engineering and Machine Company, his grandfather's old enterprise. During this period the Poole Company made, among other products, coastal defense artillery mountings and base rings for battleship turret guns in its Hampden/Woodberry facilities. They also established a rural ammunition loading works outside the city in Texas, Maryland.\nEmory was offered the Democratic nomination for Judge of the Supreme Court in 1917. He had entertained for several years an ambition to go on the bench, but he declined to become a candidate, stating to the lawyers who called on him to offer political support that he expected to enter the US Army. It was at that time that he had applied for acceptance to the first Officer's Camp at Fort Myer, Virginia, in order to participate in the training that would take him to a position of leadership in the US Army.\n\nEmory was a strong advocate of America's entrance into the war. In April of 1917, Emory was present at the Capital when President Wilson went to Congress to ask for a declaration of war. Emory contended that it was his duty to fight for the principles that he upheld, and he was one of the first Marylanders to volunteer.\nCaptain Thomas H. Westlake, of Cleveland, said, \"Emory had something unusual in him because of the fact that he had volunteered at the age of almost 40, with a wife and three children dependant upon him.\" Emory's explanation was, \"My country was my idol! To it I sacrificed every selfish, every endearing sentiment.\" \n\nWhile at Fort Myer, Virginia Emory acted as a correspondent for the Baltimore Sun. His series of five articles, entitled \"Reports from a Rookie\" and carrying his byline, appeared from time to time in the Sun, and explained many details of camp life and the rigors of military training of the day. In his first such report, published on May 20, 1917, Emory wrote, \"One coming to camp is struck, instantly, by the type of men Uncle Sam has taken under his care for the next three months, and to whom he will entrust the training of the first army of 500,000. There are quite a number of men between the ages of 30 and 40, but the great majority are from 20 to 30. They are all fine, upstanding fellows, and most of them have had college educations. Many of the men are married and many were doing well in civil life before coming here. They fully appreciated what it would mean for a man to leave his family, friends and business for an indefinite period, and it makes a fellow feel proud of being an American when he glances about and thinks for a moment of the sacrifices voluntarily assumed by the 2,500 men here and the 38,000 men at the other training camps.\" \n\nOn May 26, in his second report, Emory wrote, \"The men now know that this country will immediately begin to play a man's part in the war; that our present assistance to the Allies will be not merely money and munitions, but troops, standing shoulder to shoulder with the armies of France and England.\" \n\nMajor Emory was one of the few men who came out of the first officers' training camp with the rank of Captain. He was subsequently ordered to Camp Lee, Virginia, where he spent nearly a year. On March 7, 1918 Captain Emory was promoted to Major and placed in command of the Third Battalion, 320th Infantry Regiment, attached to the 80th Division of the American Expeditionary Forces. Two months later, on May 17, 1918, the 80th began to leave Camp Lee, heading for Newport News, Virginia and embarkation. When troops were being rushed to France, the 23,000 soldiers of the 80th Division were sent \"over there,\" arriving in St. Nazaire, Bordeaux and Brest on June 8, 1918.\n\nThe assembly point for the division was Calais, from which it departed early in June for training with the British Army in Artois and Picardy, France, and subsequently participated in the initial phases of the Somme Offensive. From there, 80th Division troops were abruptly moved to St. Mihiel for the first American Army offensive, serving under General John J. \"Black Jack\" Pershing. General Pershing praised the actions of the 80th Division for having gained all of their battle objectives. General Cronkhite, in command of the 80th, recognized his men with the statement, \"The 80th Only Moves Forward\", the motto that has marked the Division since 1918.\n\nCommencing September 14, the division moved into the Argonne region and began its preparations for the offensive in that region during the period September 26 through November 11. One day before Major Emory's birthday, in conjunction with other American divisions, the 80th attacked at Bethincourt, advancing through the hell that was trench warfare a distance of nine kilometers in two days. Major Emory received the American Expeditionary Forces Citation for Gallantry in Action, for rallying the men of the 320th Regiment under terrific machine gun fire in action in the Bois des Ogons, France on September 26, 1918.\n\nOn September 29 the 80th Division was relieved and assembled in the vicinity of Cuisy, where on October 4 it again attacked, and over difficult ground attained a distance of four kilometers in nine days. On October 12 the division was again relieved and proceeded by march and bus to the Thiaucourt area where it was re-equipped.\n\nAt Thiacourt, the U.S. Army troops began using Springfield and Browning Automatic Rifles. Prior to that, British Enfields and French Cha Chat automatics had been used. With barely three weeks training with the newer rifles, the 80th troops were thrust into the third and final phase of the Meuse-Argonne Offensive. The 80th Division was the only one that saw action during each phase of that offensive.\n\nOn October 29, Emory's Regiment entered the line at St. Georges-St. Juvin and began preparation for what would be its last assault on the German lines.\n\nAt 11:30 p.m. on the evening of the 31st, Major Emory moved out with the battalion to take up their positions for the \"jumping off\" point along the St. Juvin-St. Georges road. At 1:45 a.m. on November 1, Emory signaled back to Regimental Headquarters, \"The Omnibus is Full\" - the signal that the Battalion was in position. At 3:30 a.m. the Allied artillery barrage began, answered almost immediately by an enemy counter-barrage, reported later as the heaviest in all of the war. A slight cessation of the barrage, at 4:30 a.m., was accompanied by word that the Germans were advancing. At 5:30 a.m. the ground assault began.\n\nIn the Argonne Forest outside Romagne, France, at 8:15 in the morning on November 1, 1918, just ten days before the Armistice, and just five days before the 320th Regiment would be relieved, Major German H. H. Emory died of machine gun wounds sustained while directing his battalion in their role as the assaulting unit in an attack that ultimately ended the war.\n\nFor his valor on the field of battle, Major Emory was honored by General of the Armies John J. \"Black Jack\" Pershing with the Distinguished Service Cross. This is the second highest military decoration awarded to a member of the United States Army for extreme gallantry and risk of life in actual combat with an armed enemy force. The citation reads, \"The President of the United States of America, authorized by Act of Congress, July 9, 1918, takes pride in presenting the Distinguished Service Cross (Posthumously) to German H. H. Emory, Major, U.S. Army, for extraordinary heroism in action while serving with the 3rd Battalion, 320th Infantry Regiment, 80th Division, American Expeditionary Forces, near Sommerance, November 1, 1918. On the morning of November 1st, 1918, the 3rd Battalion, 320th Infantry had advanced under heavy enemy artillery and machine gun fire to the north slope of the Ravine Aux Pierres, north of the St. Juvin - St. Georges Road. The crest of the slope was being swept with a murderous machine gun fire and the advance of the battalion was momentarily checked. Without care for his personal safety and inspired only by the thought that his battalion must go forward, Major Emory, though exposed to direct machine gun fire and in plain view of the enemy, calmly moved back and forth along his whole front encouraging his troops and personally directing the attack. By his magnificent example of coolness and bravery, he so encouraged and inspired the men of his command that they held this very exposed position and finally succeeded in overcoming enemy resistance.\" \n\nOriginally Major Emory was buried in a little courtyard in the demolished town of St. Juvin, a small wooden cross with his name, rank, regiment and date of death marking his resting place. At the time of his death he was just 36 years old. Later his body was moved and re-interred at the Meuse-Argonne American Cemetery and Memorial (also known as the Argonne American Cemetery), along with 14,245 other American comrades from the American Expeditionary Forces (AEF). Most lost their lives during the Meuse-Argonne Offensive, which was the largest battle in American military history, involving 1.2 million American troops and was the principal engagement of the United States in WW 1. It lasted from September 26 to the Armistice with Germany on November 11, 1918.\n\nMajor Emory's oil portrait hangs in Courtroom 226 of the Clarence M. Mitchell Jr. Courthouse in Baltimore, MD. It was commissioned by his fellow attorneys in the Baltimore Bar Association and executed by the distinguished Baltimore portrait artist Thomas Cromwell Corner (1865–1938), who was schooled both in the United States and Paris, and was a founding member of the Baltimore Museum of Art. Emory's posthumous oil portrait is one of the largest in the Mitchell Courthouse, measuring 50\" by 76\". It depicts him wearing his Distinguished Service Cross. He is the only lawyer in the courthouse's extensive art collection to be pictured in a military uniform. Thomas C. Corner is the most prolific painter in the courthouse collection, with 20 portraits to his credit. Ironically, twenty years earlier, Corner had painted an oil half-length portrait of the major's grandfather, German H. Hunt.\nAlong with five other Baltimore attorneys who died in World War 1, Emory is also honored with a bronze American eagle sculpture, rendered by the two Mitchell Courthouse architects J.B.Noel Wyatt (1847–1926) and William Nolting (1866–1940). It is mounted on a marble pedestal and located in the Criminal Court Lobby of the Mitchell Courthouse. Of the six Baltimore attorneys who died in World War 1 and honored by this sculpture, Major Emory was the most senior in rank. Another attorney, Lieutenant Merrill Rosenfeld, 115th Infantry, also received the Distinguished Service Cross.\n\nThe Maryland State Bar Association dedicated another bronze memorial tablet, over 6 feet in height, to these six Baltimore attorneys, plus one other Maryland lawyer, who died in the war, in the main entrance hall at the Maryland Court of Appeals in Annapolis on January 11, 1921. It was designed and executed by the Baltimore sculptor Ephraim Keyser and \"represents the lawyer, turning in answer to his country's call, from his life work in his chosen profession, to serve its need on the field of battle.\" The pedestal altar reads in part, \"In grateful memory of the Lawyers of Maryland who lost their lives in the Great War while serving in the armed forces of their country. 1917–1918, \"O fortunata mors quae naturae debita pro patria est potissimum reddita.\" (O fortunate the death, the debt to nature owed by all, Paid by them in advance for their country's cause. Horace)\n\nIn addition, his home church in Baltimore, Saint Michael and All Angels Protestant Episcopal Church, honored Emory with the placing of a bronze memorial tablet, the gift of the Fidelity Lodge of Masons, of which Emory was a member. Present at the dedication service on October 31, 1920 was Major General Adelbert Cronkhite, who was in command of the 80th Division in France, when Emory was killed. At the service Emory's priest, Bishop Philip Cook, related that Emory was moved primarily by the fact that he thought America was in mortal danger.\n\n"}
{"id": "26122170", "url": "https://en.wikipedia.org/wiki?curid=26122170", "title": "Gonâve Microplate", "text": "Gonâve Microplate\n\nThe Gonâve Microplate forms part of the boundary between the North American Plate and the Caribbean Plate. It is bounded to the west by the Cayman spreading center, to the north by the Septentrional-Oriente fault zone and to the south by the Walton fault zone and the Enriquillo-Plantain Garden fault zone. The existence of this microplate was first proposed in 1991. This has been confirmed by GPS measurements, which show that the overall displacement between the two main plates is split almost equally between the transform fault zones that bound the Gonâve microplate. The microplate is expected to eventually become accreted to the North American Plate.\n\nThe Gonâve Microplate is an approximately 1,100 km long strip, consisting mainly of oceanic crust of the Cayman Trough but including island arc material at its eastern end on the western part of Hispaniola. Further east a separate \"Hispaniola microplate\" has been identified. At its western end, the Gonâve Microplate is bounded by the mid-Cayman spreading centre. To the north it is bounded by the Septentrional-Oriente fault zone and to the south by a more complex strike-slip fault system that includes the Walton fault and the Enriquillo-Plantain Garden fault zone. As the northern and southern boundaries approach the eastern edge of the Caribbean Plate they become less distinct and the eastern boundary is not as well defined.\n\nThe presence of a separate Gonâve Microplate was first suggested by the analysis of sidescan sonar results from the Cayman Trough. This study found evidence for continuous transform type faults along the southern flank of the trough, to both sides of the spreading centre. GPS data supports the existence of the microplate by showing that the relative motion between the North American and Caribbean plates is split almost equally between the two bounding transform fault systems. Comparison of these rates with observations of magnetic stripes within the Cayman Trough suggests that displacement is increasingly being transferred from the northern fault system to the southern one. This observation is consistent with the eventual accretion of the Gonâve Microplate to the North American Plate.\n\nThe Gonâve Microplate began to form in the Early Eocene after the northern part of the leading edge of the Caribbean Plate (present day Cuba) collided with the Bahamas platform. This part of the plate was unable to move further to the east and a transform fault system developed to the south, effectively cutting off this northern area and accreting it to the North American Plate. A large left-stepping offset formed along this zone just east of the Yucatán peninsula creating a pull-apart basin, which continued to extend until the onset of seafloor spreading, creating the Cayman spreading centre. Further movement on this fault system created the Cayman Trough, although at that time the future microplate was still firmly attached to the Caribbean Plate. During the Late Miocene, the part of the Caribbean Plate formed by Hispaniola began to collide with the Bahamas platform and a new strike-slip fault system developed through Jamaica and southern Hispaniola, the Enriquillo-Plantain Garden fault zone, isolating part of the Cayman Trough and the central part of Hispaniola to form the Gonâve microplate. It has been suggested that the Gonave microplate will also become accreted to the North American Plate, as all the plate boundary displacement transfers onto the southern fault system.\n"}
{"id": "2235270", "url": "https://en.wikipedia.org/wiki?curid=2235270", "title": "International Ultraviolet Explorer", "text": "International Ultraviolet Explorer\n\nThe International Ultraviolet Explorer (IUE) was an astronomical observatory satellite primarily designed to take ultraviolet spectra. The satellite was a collaborative project between NASA, the UK Science Research Council and the European Space Agency (ESA). The mission was first proposed in early 1964, by a group of scientists in the United Kingdom, and was launched on January 26, 1978 aboard a NASA Delta rocket. The mission lifetime was initially set for 3 years, but in the end it lasted almost 18 years, with the satellite being shut down in 1996. The switch-off occurred for financial reasons, while the telescope was still functioning at near original efficiency.\n\nIt was the first space observatory to be operated in real time by astronomers who visited the groundstations in the United States and Europe. Astronomers made over 104,000 observations using the IUE, of objects ranging from solar system bodies to distant quasars. Among the significant scientific results from IUE data were the first large scale studies of stellar winds, accurate measurements of the way interstellar dust absorbs light, and measurements of the supernova SN1987A which showed that it defied stellar evolution theories as they then stood. When the mission ended, it was considered the most successful astronomical satellite ever.\n\nThe human eye can perceive light with wavelengths between roughly 350 (violet) and 700 (red) nanometres. Ultraviolet light has wavelengths between roughly 10 nm and 350 nm. UV light can be harmful to human beings, and is strongly absorbed by the ozone layer. This makes it impossible to observe UV emission from astronomical objects from the ground. Many types of object emit copious quantities of UV radiation, though: the hottest and most massive stars in the universe can have surface temperatures high enough that the vast majority of their light is emitted in the UV. Active Galactic Nuclei, accretion disks, and supernovae all emit UV radiation strongly, and many chemical elements have strong absorption lines in the UV, so that UV absorption by the interstellar medium provides a powerful tool for studying its composition.\n\nUV astronomy was impossible before the Space Age, and some of the first space telescopes were UV telescopes designed to observe this previously inaccessible region of the electromagnetic spectrum. One particular success was the second Orbiting Astronomical Observatory, which had a number of 20 cm UV telescopes on board. It was launched in 1968, and took the first UV observations of 1200 objects, mostly stars. The success of OAO-2 motivated astronomers to consider larger missions.\n\nThe orbiting ultraviolet satellite which ultimately became the IUE mission was first proposed in 1964 by British astronomer Robert Wilson. The European Space Research Organisation was planning a \"Large Astronomical Satellite\", and had sought proposals from the astronomical community for its aims and design. Wilson headed a British team which proposed an ultraviolet spectrograph, and their design was recommended for acceptance in 1966.\n\nHowever, management problems and cost overruns led to the cancellation of the LAS program in 1968. Wilson's team scaled down their plans and submitted a more modest proposal to ESRO, but this was not selected as the Cosmic Ray satellite was given precedence. Rather than give up on the idea of an orbiting UV telescope, they instead sent their plans to NASA administrator Leo Goldberg, and in 1973 the plans were approved. The proposed telescope was renamed the \"International Ultraviolet Explorer\".\n\nThe telescope was designed from the start to be operated in real time, rather than by remote control. This required that it would be launched into a geosynchronous orbit – that is, one with a period equal to one sidereal day of 23h 56m. A satellite in such an orbit remains visible from a given point on the Earth's surface for many hours at a time, and can thus transmit to a single ground station for a long period of time. Most space observatories in Earth orbit, such as the Hubble Space Telescope, are in a low orbit in which they spend most of their time operating autonomously because only a small fraction of the Earth's surface can see them at a given time. Hubble, for example, orbits the Earth at an altitude of approximately 600 km, while a geosynchronous orbit has an average altitude of 36,000 km.\n\nAs well as allowing continuous communication with ground stations, a geosynchronous orbit also allows a larger portion of the sky to be viewed continuously. Because the distance from Earth is greater, the Earth occupies a much smaller portion of the sky as seen from the satellite than it does from low Earth orbit.\n\nA launch into a geosynchronous orbit requires much more energy for a given weight of payload than a launch into low Earth orbit. This meant that the telescope had to be relatively small, with a 45 cm primary mirror, and a total weight of 312 kg. Hubble, in comparison, weighs 11.1 tonnes and has a 2.4 m mirror. The largest ground-based telescope, the Gran Telescopio Canarias, has a primary mirror 10.4 m across. A smaller mirror means less light-gathering power, and less spatial resolution, compared to a larger mirror.\n\nThe stated aims of the telescope at the start of the mission were:\n\n\nThe telescope was constructed as a joint project between NASA, ESRO (which became ESA in 1975) and the UK Science and Engineering Research Council. SERC provided the Vidicon cameras for the spectrographs as well as software for the scientific instruments. ESA provided the solar arrays to power the spacecraft as well as a ground observing facility in Villafranca del Castillo, Spain. NASA contributed the telescope, spectrograph, and spacecraft as well as launching facilities and a second ground observatory in Greenbelt, Maryland at the Goddard Space Flight Center.\n\nAccording to the agreement setting up the project the observing time would be divided between the contributing agencies with 2/3 to NASA, 1/6 to ESA and 1/6 to the UK's Science Research Council.\n\nThe telescope mirror was a reflector of the Ritchey-Chretien type, which has hyperbolic primary and secondary mirrors. The primary was 45 cm across. The telescope was designed to give high quality images over a 16 arcminute field of view (about half the apparent diameter of the Sun or Moon). The primary mirror was made of beryllium, and the secondary of fused silica – materials chosen for their light weight, moderate cost, and optical quality.\n\nThe instrumentation on board consisted of the Fine Error Sensors (FES), which were used for pointing and guiding the telescope, a high resolution and a low resolution spectrograph, and four detectors.\n\nThere were two Fine Error Sensors (FES), and their first purpose was to image the field of view of the telescope in visible light. They could detect stars down to 14th magnitude, about 1500 times fainter than can be seen with the naked eye from Earth. The image was transmitted to the ground station, where the observer would verify that the telescope was pointing at the correct field, and then acquire the exact object to be observed. If the object to be observed was fainter than 14th magnitude, the observer would point the telescope at a star that could be seen, and then apply \"blind\" offsets, determined from the coordinates of the objects. The accuracy of the pointing was generally better than 2 arcseconds for blind offsets \n\nThe FES acquisition images were the telescope's only imaging capability; for UV observations, it only recorded spectra. For this, it was equipped with two spectrographs. They were called the Short Wavelength Spectrograph and the Long Wavelength Spectrograph, and covered wavelength ranges of 115 to 200 nanometres and 185 to 330 nm respectively. Each spectrograph had both high and low resolution modes, with spectral resolutions of 0.02 and 0.6 nm respectively.\n\nThe spectrographs could be used with either of two apertures. The larger aperture was a slot with a field of view roughly 10 × 20 arcsec; the smaller aperture was a circle about 3 arcsec in diameter. The quality of the telescope optics was such that point sources appeared about 3 arcsec across, so use of the smaller aperture required very accurate pointing, and it did not necessarily capture all of the light from the object. The larger aperture was therefore most commonly used, and the smaller aperture only used when the larger field of view would have contained unwanted emission from other objects.\n\nThere were two cameras for each spectrograph, one designated the primary and the second being redundant in case of failure of the first. The cameras were named LWP, LWR, SWP and SWR where P stands for prime, R for redundant and LW/SW for long/short wavelength. The cameras were television cameras, sensitive only to visible light, and light gathered by the telescope and spectrographs first fell on a UV-to-visible converter. This was a caesium-tellurium cathode, which was inert when exposed to visible light, but which gave off electrons when struck by UV photons due to the photoelectric effect. The electrons were then detected by the TV cameras. The signal could be integrated for up to many hours, before being transmitted to Earth at the end of the exposure.\n\nThe IUE was launched from Cape Canaveral, Florida on a Delta rocket, on 26 January 1978. It was launched into a transfer orbit, from which its on-board rockets fired it into its planned geosynchronous orbit. The orbit was inclined by 28.6° to the Earth's equator, and had an orbital eccentricity of 0.24, meaning that the satellite's distance from Earth varied between 25,669 km and 45,887 km. The ground track was initially centred at a longitude of approximately 70 degrees W.\n\nThe first 60 days of the mission were designated as the commissioning period. This was divided into three main stage. Firstly, as soon as its instruments were switched on, the IUE observed a small number of high priority objects, to ensure that some data had been taken in the event of an early failure. The first spectrum, of the star Eta Ursae Majoris, was taken for calibration purposes three days after launch. The first science observations targeted objects including the Moon, the planets from Mars to Uranus, hot stars including Eta Carinae, cool giant stars including Epsilon Eridani, the black hole candidate Cygnus X-1, and galaxies including M81 and M87.\n\nThen, the spacecraft systems were tested and optimised. The telescope was focussed, and the prime and redundant cameras in both channels were tested. It was found that the SWR camera did not work properly, and so the SWP camera was used throughout the mission. Initially, this camera suffered from significant electronic noise, but this was traced to a sensor used to align the telescope after launch. Once this sensor was switched off, the camera performed as expected. The cameras were then adjusted for best performance, and the slewing and guiding performance of the telescope evaluated and optimised \n\nFinally, image quality and spectral resolution were studied and characterised, and the performance of the telescope, spectrographs and cameras were calibrated using observations of well-known stars.\n\nAfter these three phases were completed, the \"routine phase\" of operations began on 3 April 1978. Optimisation, evaluation and calibration operations were far from complete, but the telescope was understood well enough for routine science observations to begin.\n\nUse of the telescope was divided between NASA, ESA and SERC in approximate proportion to their relative contributions to the satellite construction: two thirds of the time was available to NASA, and one sixth each to ESA and SERC. Telescope time was obtained by submitting proposals, which were reviewed annually. Each of the three agencies considered applications separately for its allocated observing time. Astronomers of any nationality could apply for telescope time, choosing whichever agency they preferred to apply to.\n\nIf an astronomer was awarded time, then when their observations were scheduled, they would travel to the ground stations which operated the satellite, so that they could see and evaluate their data as it was taken. This mode of operation was very different from most space facilities, for which data is taken with no real time input from the astronomer concerned, and instead resembled the use of ground-based telescopes.\n\nFor most of its lifetime, the telescope was operated in three eight-hour shifts each day, two from the US ground station at the Goddard Space Flight Center in Maryland, and one from the ESA ground station at Villanueva de la Cañada near Madrid. Because of its elliptical orbit, the spacecraft spent part of each day in the Van Allen belts, during which time science observations suffered from higher background noise. This time occurred during the second US shift each day, and was generally used for calibration observations and spacecraft 'housekeeping', as well as for science observations that could be done with short exposure times.\n\nThe twice-daily transatlantic handovers required telephone contact between Spain and the US to coordinate the switch. Observations were not coordinated between the stations, so that the astronomers taking over after the handover would not know where the telescope would be pointing when their shift started. This sometimes meant that observing shifts started with a lengthy pointing manoeuvre, but allowed maximum flexibility in scheduling of observing blocks.\n\nData was transmitted to Earth in real time at the end of each science observation. The camera read-out formed an image of 768×768 pixels, and the analogue-to-digital converter resulted in a dynamic range of 8 bits. The data was then transmitted to Earth via one of six transmitters on the spacecraft; four were S-band transmitters, placed at points around the spacecraft such that no matter what its attitude, one could transmit to the ground, and two were VHF transmitters, which could sustain a lower bandwidth, but consumed less power, and also transmitted in all directions. The VHF transmitters were used when the spacecraft was in the Earth's shadow and thus reliant on battery power instead of solar power.\n\nIn normal operations, observers could hold the telescope in position and wait approximately 20 minutes for the data to be transmitted, if they wanted the option of repeating the observation, or they could slew to the next target and then start the data transmission to Earth while observing the next target.\n\nThe data transmitted were used for \"quick look\" purposes only, and full calibration was carried out by IUE staff later. Astronomers were then sent their data on magnetic tape by post, about a week after processing. From the date of the observation, the observers had a six-month proprietary period during which only they had access to the data. After six months, it became public.\n\nThe IUE allowed astronomers their first view of the ultraviolet light from many celestial objects, and was used to study objects ranging from Solar System planets to distant quasars. During its lifetime, hundreds of astronomers observed with IUE, and during its first decade of operations, over 1500 peer reviewed scientific articles based on IUE data were published. Nine symposia of the International Astronomical Union were devoted to discussions of IUE results.\n\nAll the planets in the Solar System except Mercury were observed; the telescope could not point at any part of the sky within 45° of the Sun, and Mercury's greatest angular distance from the Sun is only about 28°. IUE observations of Venus showed that the amount of sulfur monoxide and sulfur dioxide in its atmosphere declined by a large amount during the 1980s. The reason for this decline is not yet fully understood, but one hypothesis is that a large volcanic eruption had injected sulfur compounds into the atmosphere, and that they were declining following the end of the eruption.\n\nHalley's Comet reached perihelion in 1986, and was observed intensively with the IUE, as well as with a large number of other ground-based and satellite missions. UV spectra were used to estimate the rate at which the comet lost dust and gas, and the IUE observations allowed astronomers to estimate that a total of 3×10 tons of water evaporated from the comet during its passage through the inner Solar System.\n\nSome of the most significant results from IUE came in the studies of hot stars. A star that is hotter than about 10,000 K emits most of its radiation in the UV, and thus if it can only be studied in visible light, a large amount of information is being lost. The vast majority of all stars are cooler than the Sun, but the fraction that is hotter includes massive, highly luminous stars which shed enormous quantities of matter into interstellar space, and also white dwarf stars, which are the end stage of stellar evolution for the vast majority of all stars and which have temperatures as high as 100,000 K when they first form.\n\nThe IUE discovered many instances of white dwarf companions to main sequence stars. An example of this kind of system is Sirius, and at visible wavelengths the main sequence star is far brighter than the white dwarf. However, in the UV, the white dwarf can be as bright or brighter, as its higher temperature means it emits most of its radiation at these shorter wavelengths. In these systems, the white dwarf was originally the heavier star, but has shed most of its mass during the later stages of its evolution. Binary stars provide the only direct way to measure the mass of stars, from observations of their orbital motions. Thus, observations of binary stars where the two components are at such different stages of stellar evolution can be used to determine the relationship between the mass of stars and how they evolve.\n\nStars with masses of around ten times that of the Sun or higher have powerful stellar winds. The Sun loses about 10 solar masses per year in its solar wind, which travels at up to around 750 km/s, but the massive stars can lose as much as a billion times more material each year in winds travelling at several thousand kilometres per second. These stars exist for a few million years, and during this time the stellar wind carries away a significant fraction of their mass, and plays a crucial role in determining whether they explode as supernovae or not. This stellar mass loss was first discovered using rocket-borne telescopes in the 1960s, but the IUE allowed astronomers to observe a very large number of stars, allowing the first proper studies of how stellar mass loss is related to mass and luminosity.\n\nIn 1987, a star in the Large Magellanic Cloud exploded as a supernova. Designated SN 1987A, this event was of enormous importance to astronomy, as it was the closest known supernova to Earth, and the first visible to the naked eye, since Kepler's star in 1604 – before the invention of the telescope. The opportunity to study a supernova so much more closely than had ever been possible before triggered intense observing campaigns at all major astronomical facilities, and the first IUE observations were made about 14 hours after the discovery of the supernova.\n\nIUE data were used to determine that the progenitor star had been a blue supergiant, where theory had strongly expected a red supergiant. Hubble Space Telescope images revealed a nebula surrounding the progenitor star which consisted of mass lost by the star long before it exploded; IUE studies of this material showed that it was rich in nitrogen, which is formed in the CNO cycle – a chain of nuclear reactions which produces most of the energy emitted by stars much more massive than the Sun. Astronomers inferred that the star had been a red supergiant, and had shed a large amount of matter into space, before evolving into a blue supergiant and exploding.\n\nThe IUE was used extensively to investigate the interstellar medium. The ISM is normally observed by looking at background sources such as hot stars or quasars; interstellar material absorbs some of the light from the background source and so its composition and velocity can be studied. One of IUE's early discoveries was that the Milky Way is surrounded by a vast halo of hot gas, known as a galactic corona. The hot gas, heated by cosmic rays and supernovae, extends several thousand light years above and below the plane of the Milky Way.\n\nIUE data was also crucial in determining how the light from distant sources is affected by dust along the line of sight. Almost all astronomical observations are affected by this interstellar extinction, and correcting for it is the first step in most analyses of astronomical spectra and images. IUE data was used to show that within the galaxy, interstellar extinction can be well described by a few simple equations. The relative variation of extinction with wavelength shows little variation with direction; only the absolute amount of absorption changes. Interstellar absorption in other galaxies can similarly be described by fairly simple 'laws'.\n\nThe IUE vastly increased astronomers' understanding of active galactic nuclei (AGN). Before its launch, 3C 273, the first known quasar, was the only AGN that had ever been observed at UV wavelengths. With IUE, UV spectra of AGN became widely available.\n\nOne particular target was NGC 4151, the brightest Seyfert galaxy. Starting soon after IUE's launch, a group of European astronomers pooled their observing time to repeatedly observe the galaxy, to measure variations over time of its UV emission. They found that the UV variation was much greater than that seen at optical and infrared wavelengths. IUE observations were used to study the black hole at the centre of the galaxy, with its mass being estimated at between 50 and 100 million times that of the Sun. The UV emission varied on timescales of a few days, implying that the region of emission was only a few light days across.\n\nQuasar observations were used to probe intergalactic space. Clouds of hydrogen gas in between the Earth and a given quasar will absorb some of its emission at the wavelength of Lyman alpha. Because the clouds and the quasar are all at different distances from Earth, and moving at different velocities due to the expansion of the universe, the quasar spectrum has a \"forest\" of absorption features at wavelengths shorter than its own Lyman alpha emission. Before IUE, observations of this so-called Lyman-alpha forest were limited to very distant quasars, for which the redshift caused by the expansion of the universe brought it into optical wavelengths. IUE allowed nearer quasars to be studied, and astronomers used this data to determine that there are fewer hydrogen clouds in the nearby universe than there are in the distant universe. The implication is that over time, these clouds have formed into galaxies.\n\nThe IUE was designed to have a minimum lifetime of three years, and carried consumable sufficient for a five-year mission. However, it lasted far longer than its design called for. Occasional hardware failures caused difficulties, but innovative techniques were devised to overcome them. For example, the spacecraft was equipped with six gyros to stabilise the spacecraft. Successive failures of these in 1979, 1982, 1983, 1985 and 1996 ultimately left the spacecraft with a single functional gyro. Telescope control was maintained with two gyros by using the telescope's Sun sensor to determine the spacecraft's attitude, and stabilisation in three axes proved possible even after the fifth failure, by using the Sun sensor, the Fine Error Sensors and the single remaining gyro. Most other parts of the telescope systems remained fully functional throughout the mission.\n\nIn 1995, budget concerns at NASA almost led to the termination of the mission, but instead the operations responsibilities were redivided, with ESA taking control for 16 hours a day, and GSFC for the remaining 8 only. The ESA 16 hours was used for science operations, while the GSFC 8 hours was used only for maintenance. In February 1996, further budget cuts led ESA to decide that it would no longer maintain the satellite. Operations ceased in September of that year, and on 30 September all the remaining hydrazine was discharged, the batteries were drained and switched off, and at 1844 UT, the radio transmitter was shut down and all contact with the spacecraft was lost.\n\nIt continues to orbit the Earth in its geosynchronous orbit, and will continue to do so more or less indefinitely as it is far above the upper reaches of the Earth's atmosphere. Anomalies in the Earth's gravity due to its non-spherical shape meant that the telescope tended to drift west from its original location at approximately 70°W longitude towards approximately 110°W. During the mission, this drift was corrected by occasional rocket firings, but since the end of the mission the satellite has drifted uncontrolled to the west of its former location.\n\nThe IUE archive is one of the most heavily used astronomical archives. Data were archived from the start of the mission, and access to the archive was free to anyone who wished to use it. However, in the early years of the mission, long before the advent of the World Wide Web and fast global data transmission links, access to the archive required a visit in person to one of two Regional Data Analysis Facilities (RDAFs), one at the University of Colorado and the other at GSFC.\n\nIn 1987 it became possible to access the archive electronically, by dialling into a computer at Goddard. The archive, then totalling 23 Gb of data, was connected to the computer on a mass storage device. A single user at a time could dial in, and would be able to retrieve an observation in 10–30 seconds.\n\nAs the mission entered its second decade, plans were made for its final archive. Throughout the mission, calibration techniques were improved, and the final software for data reduction yielded significant improvements over earlier calibrations. Eventually, the entire set of available raw data was recalibrated using the final version of the data reduction software, creating a uniform high quality archive. Today, the archive is hosted at the Space Telescope Science Institute and is available via the World Wide Web.\n\nThe IUE mission, by virtue of its very long duration and the fact that for most of its lifetime it provided astronomers' only access to UV light, had a major impact on astronomy. By the end of its mission it was considered by far the most successful and productive space observatory mission. For many years after the end of the mission, its archive was the most heavily used dataset in astronomy, and IUE data has been used in over 250 PhD projects worldwide. Almost 4,000 peer-reviewed papers have now been published based on IUE data, including some of the most cited astronomy papers of all time. The most cited paper based on IUE data is one analysing the nature of interstellar reddening, which has subsequently been cited over 5,500 times. For comparison, the Hubble Space Telescope has now been in orbit for 21 years (as of 2011) and Hubble data has been used in almost 10,000 peer-reviewed publications.\n\n"}
{"id": "22834616", "url": "https://en.wikipedia.org/wiki?curid=22834616", "title": "Island Arc (journal)", "text": "Island Arc (journal)\n\nIsland Arc (print: , online: ) is a peer-reviewed quarterly scientific journal that was established in 1992, covering \"Earth Sciences of Convergent Plate Margins and Related Topics\". It is published by Wiley-Blackwell on behalf of the Geological Society of Japan, in association with the Japan Association for Quaternary Research, Japan Association of Mineralogical Sciences, Palaeontological Society of Japan and the Society of Resource Geology.\n\nThe journal is abstracted and indexed in EBSCO databases, Aquatic Sciences and Fisheries Abstracts, Cambridge Scientific Abstracts, Chemical Abstracts Service, Current Contents/Physical, Chemical & Earth Sciences, InfoTrac, ProQuest, Science Citation Index, Scopus, and The Zoological Record. According to the Journal Citation Reports, the journal's 2009 impact factor is 1.182, ranking it 82nd out of 153 in the category \"Geosciences, Multidisciplinary\".\n\nIsland arc, the geographical and geological feature that the journal takes its name from.\n"}
{"id": "3482270", "url": "https://en.wikipedia.org/wiki?curid=3482270", "title": "Jesper Olsen (runner)", "text": "Jesper Olsen (runner)\n\nJesper Olsen, or Jesper Kenn Olsen, is an multiple national record holder ultra distance runner from Denmark, and was the second person verified to have run around the world (16,000 miles: 2004-2005), as well as the first verified to have run around the world in a north-south rather than east-west direction (25,000 miles: 2008-2010, 2011-2012, due to 6-month illness and injury).\n\nOlsen has a master's degree in political science from Copenhagen University, Denmark. He also has a law degree.\n\nOlsen has been a marathon runner since the age of 15. He has achieved various milestones, including the European elite on 100 km and 24-hours; the national recordholder on 100 km, 24-hours and 6-days (6:58, 224 km, 549 km), the national elite on marathon (2:27), as well as the Cliff Young Australian 6-day race in November 2004, running 756 kilometres, and the South African 6-day race in April 2008 with a total of 685 kilometres.\n\nFollowing positive reception of his proposal of a world run in 2001, Olsen also founded the World Run project as the organization to undertake the attempt and its support.\n\nThe concept of Olsen's world run originated as a suggestion made in 2001 by Olsen to David Blaikie, who published it with an invitation for comments on his website \"ultramarathonworld.com\". Olsen suggested that, without taking sides in then-current controversies in the ultra-running world, a professionally organized world run would be a \"constructive\" and \"truly sportsman[like]\" response to widespread ultrarunner community skepticism and discussion concerning Robert Garside's world run, which had been in progress since 1997 but was viewed with great skepticism by Blaikie and many ultra-runners and had not yet been authenticated by Guinness at the time. In his letter, Olsen stated that while he was \"fairly new to the 'real' ultra-running\" world, he did hold the Danish national record for the 100 km run (6:58:31) and for the 24-hour run (223 km), had been running marathons since around 1986 (15 years), and having finished a degree, was able to commit the time required if the proposal gained the necessary support from others.\n\nOlsen's run around the world took 22 months. It started on 1 January 2004 and finished on 23 October 2005. His route consisted of: London-Copenhagen-Moscow-Vladivostok-(air)-Niigata-Tokyo-(air)-Sydney-Perth-(air)-Los Angeles-Vancouver-New York-(air)-Shannon-Dublin-(air)-Liverpool-London. Olsen averaged a day, slightly more than a marathon. It totalled just over 16,000 miles (26,000 km), exceeding the distance of the first verified walk around the world (Dave Kunst, 1970-1974, 14,452 miles (23,123 km)) but around (or slightly under) half the distance of the first verified run around the world, when Garside's run was eventually verified by Guinness in 2007 (Robert Garside, 1997-2003, estimated 30,000 - 40,000 miles (48,000 - 64,000 km)) During most of the run, Olsen pushed a baby carriage, in which he kept food, beverages, a tent, and other equipment. While running through Russia and half of the U.S., he was aided by a support car transporting these supplies. From London to central Siberia he was accompanied by Alexander Korotkov of Russia, who planned to run around the world with Olsen but gave up in central Siberia.\nIn October 2006, Jorden Rundt i Løb (\"World Run\" in English) was published.\n\nOlsen and Sarah Barnett ran the North-South route starting on 1 July 2008. The North-South run aimed to complete a distance of with GPS tracking and live coverage, thus making it the world's longest fully GPS-documented run. The run went from top to bottom of the globe and back, running across four continents and a huge range of temperatures and terrain. It can be seen as a run in a circle around the world in southern, later northern direction with the poles excluded. It started at North Cape, Norway (1 July 2008) passing Helsinki, Finland (4 August), Copenhagen, Denmark (25 August), Budapest, Hungary (25 September), and Istanbul, Turkey (5 November). On December 1, 2008, near Silifke, Turkey, Barnett had to give up after , and Olsen continued alone. He passed Cairo, Egypt (1 January 2009) and Addis Abeba, Ethiopia (16 April). Cape Town in South Africa was reached by 15 March 2010, thereby completing the first half of the run and the first documented run through Africa, a distance of .\n\nOlsen spent more than six months recovering in Denmark due to dysenteria, malaria, and two operations to eliminate deep infections in his right arm. He then continued his run on 1 January 2011 from Punta Arenas for the last half of the run, through South America and North America to Newfoundland . On 28 July 2012, Olsen announced on his website the completion of World Run 2 in Cape Spear, Newfoundland.\n\nIn June 2014, Olsen's book \"The Runner’s Guide to the Planet\" was released on iTunes. Olsen also has two other books in Danish published on iTunes in January and April 2014.\n\n\n\n"}
{"id": "44130688", "url": "https://en.wikipedia.org/wiki?curid=44130688", "title": "Johannes de Cuba", "text": "Johannes de Cuba\n\nJohann von Wonnecke Caub or Johannes de Cuba (1430–1503), is the attributed author of an early printed book on natural history, first published in 1485, then revised and expanded in 1491.\n\n \nThe book first appeared in German under the title of \"Gart der Gesundheit\" (1485), printed in the workshop of Peter Schöffer, then was translated into Latin and expanded under the title of \"Hortus sanitatis\" (1491), edited by Jacob von Meydenbach. Contrary to some historians, it is probably not a translation of \"Pseudo-Apuleius Herbarius\" (1484), but an original work of a much larger scope.\n\nIt was translated into French in 1500 under the title of \"Jardin de santé : herbes, arbres et choses qui de iceuly coqueurent et conviennet a lusage de medecine\".\n\n\"Hortus sanitatis\" is divided into several treatises:\n\n\nThis work has an obvious medical orientation, including the sections on animals and certain minerals. The illustrations are sketchy although quite lifelike. These and the text will be repeatedly reused in other works, even if the scientific quality of the work is very poor. The author uncritically reproduces many legends, such as the tree of life and coiled snakes; \"De animalibus vitam in terris ducentium\", in addition to references regarding genuine fauna, replaces a variant of the mythical centaur with an onocentaure, a man with a donkey's head.\n\nLittle is known of the life of Cuba. He was probably a doctor in Frankfurt.\n"}
{"id": "46214623", "url": "https://en.wikipedia.org/wiki?curid=46214623", "title": "Lake Beloye (Ryazan Oblast)", "text": "Lake Beloye (Ryazan Oblast)\n\nLake Beloye ( — literally \"White lake\") is a lake in Klepikovsky District, Ryazan Oblast, Russia. It is notable for its relatively great depth for the region - - created as a deep gouge in the landscape in the most recent glaciation. The lake is located in the Meshchyera Lowlands, a glacial alluvial plain of swampy lowlands, gravelly moraines and limestone bedrock. It was once connected by an artificial canal to Lake Velikoye (\"Grand Lake\", a much larger lake with a surface area of ). The canal was filled in 2009. Lake Beloye is surrounded by pine forests, with reeds and sedge predominating on the shore. It is a popular lake for recreation and fishing, being home to pike, carp and other fish.\n\nLake Beloye was classified in 1974 as a \"Nature Monument of Regional Importance\", for its \"scientific, cultural, educational and health\" value (World Database of Protected Places ID #206212). The site records the presence several species listed as vulnerable in the Red Book of Russia, including the Viviparous lizard, the reptile living the farthest north.\n\n\n"}
{"id": "33998064", "url": "https://en.wikipedia.org/wiki?curid=33998064", "title": "List of Canadian plants by genus L", "text": "List of Canadian plants by genus L\n\nBelow is a list of Canadian plants by genus. Due to the vastness of Canada's biodiversity, this page is divided.\n\nThis is a (partial) list of the plant species considered native to Canada. Many of the plants seen in Canada are introduced, either intentionally or accidentally. For these plants, see List of introduced species to Canada.\n\nA | B | C | D | E | F | G | H | I J K | L | M | N | O | P Q | R | S | T | U V W | X Y Z\n\n\n\"See:\" Flora of Canada#References\n"}
{"id": "5879905", "url": "https://en.wikipedia.org/wiki?curid=5879905", "title": "List of Diguetidae species", "text": "List of Diguetidae species\n\nThis page lists all described species of the spider family Diguetidae as of Dec. 25, 2016.\n\n\"Diguetia\" \n\n\"Segestrioides\" \n\n"}
{"id": "31257336", "url": "https://en.wikipedia.org/wiki?curid=31257336", "title": "List of Hibbertia species", "text": "List of Hibbertia species\n\nThis is a list of Hibbertia species. All species are endemic to Australia, except where otherwise shown.\n\n"}
{"id": "31187664", "url": "https://en.wikipedia.org/wiki?curid=31187664", "title": "List of Japanese nuclear incidents", "text": "List of Japanese nuclear incidents\n\nThis is a list of Japanese atomic, nuclear and radiological accidents, incidents and disasters.\n\n"}
{"id": "5860331", "url": "https://en.wikipedia.org/wiki?curid=5860331", "title": "List of Sites of Special Scientific Interest in Oxfordshire", "text": "List of Sites of Special Scientific Interest in Oxfordshire\n\nOxfordshire in South East England has an area of 2,605 square kilometres and a population of 648,700. \nIn England, the body responsible for designating Sites of Special Scientific Interest (SSSIs) is Natural England, which is responsible for protecting England's natural environment. Notification as an SSSI gives legal protection to the best sites for wildlife and geology. As of 2011, there are 110 SSSIs in Oxfordshire, 77 of which have been designated for biological interest, 27 for geological interest, and 6 for both biological and geological interest.\n\nFor other counties, see List of SSSIs by Area of Search.\n\nThe data in the table is taken from Natural England's website in the form of citation sheets for each SSSI.\n"}
{"id": "6336667", "url": "https://en.wikipedia.org/wiki?curid=6336667", "title": "List of earthquakes in Argentina", "text": "List of earthquakes in Argentina\n\nThis is a list of earthquakes in Argentina.\n\n\n"}
{"id": "51115791", "url": "https://en.wikipedia.org/wiki?curid=51115791", "title": "List of invasive species in California", "text": "List of invasive species in California\n\nInvasive species in California, the introduced species of fauna−animals and flora−plants that are established and have naturalized within California.\n\nNative plants and animals can become threatened endangered species from the spread of invasive species in natural habitats and/or developed areas (e.g. agriculture, transport, settlement).\n\nSome of the invasive animal species include:\n\n\n\n\n\n\nSome of the invasive plant species include:\n\n\n"}
{"id": "18863701", "url": "https://en.wikipedia.org/wiki?curid=18863701", "title": "List of national parks of Niger", "text": "List of national parks of Niger\n\nNiger is home to a number of national parks and protected areas, including two UNESCO-MAB Biosphere Reserves. The protected areas of Niger normally have a designation and status determined by the Government of Niger. Further, fourteen sites also have international designations, applied by UNESCO and the Ramsar Convention on wetlands protection. Protected lands in Niger are managed by a number of authorities, and the areas of authority and structure have changed a number of times since independence. Some of the first reserves, parks, and protected areas were designated under French Colonial rule and much of the legal regime is based on these colonial laws. Niger is also party to a number of international agreements and participates in international ecological, conservation, and resource management programs with its neighbors, region, and worldwide.\n\nProtected lands in Niger fall under both national and international regulation, and are managed by elements of the Nigerien government, as well as regional bodies and international designation oversight bodies. Initial classification of lands as legally protected for conservation of flora, fauna, landscape, and resource protection was done under French Colonial rule beginning in 1936. Much of the legal framework of land management and protection is based on these original regulations. As of the late 1990s, most land management was the area of the Nigerien Natural Resource Management Unit (\"Cellule de Gestion des Resources Naturelles\") of the Inter-ministerial Sub-committee for Rural Development (\"Sous-Comité Interministériel chargé de la politique de Développement Rural au Niger\"), which includes ministries focused on environmental issues, industrial resource extraction, economic growth, and farming. Nigerien designated protected areas were administered by the Direction of Fauna, Fisheries and Aquaculture (\"Direction de la Faune, de la Pêche et de la Pisciculture - DFPP\") of the Ministry of Hydrology and the Environment (\"Ministère de l'Hydraulique et de l'Environnement - MHE\"). Actual protection is the responsibility of the DFPP's Fauna and Apiculture Management Service (\"Service d'Aménagement de la Faune et de l'Apiculture - SAFA\") which in 1987 had only 40 staff actually managing or guarding sites.\n\nNote: most sites have at least two overlapping designations.\n\nVarious Nigerien government designations, administered by the \"Direction de l'Environnement\" office of the Ministry of Hydrology and Environment\n\n\nAdditionally, several sites have international designations as protected areas. As signatories of the below conventions, the Government of Niger places restrictions on use of these lands.\n\n\nOther international conventions ratified by the Nigerien government include the Convention on Biological Diversity, the Convention on Migratory Species, CITES, the Convention to Combat Desertification, the Convention on Climate Change, the African - Eurasian Waterfowl Agreement, the African Convention on the Conservation of Nature and Natural Resources, the Convention on Game Hunting and the Convention on Plant Protection.\n\nA number of Nigerien sites are recognised by conservation programmes as conservation areas of special importance, even where there is no formal government convention. Notable among these are 15 BirdLife International designated Important Bird Areas (IBA). Many of these are additional designations given to existing Nigerien government, IUCN, or Ramsar designated sites.\n\n\n\n\n"}
{"id": "54540915", "url": "https://en.wikipedia.org/wiki?curid=54540915", "title": "List of red dwarfs", "text": "List of red dwarfs\n\nThis is a list of exceptional red dwarf stars.\n\nList of red dwarfs that are otherwise notable, for characteristics not otherwise listed.\n\nThis is a list of red dwarfs that currently hold records.\n\nThis is a list of red dwarfs with names that are not systematically designated.\nThis is a list of the first discovered red dwarf stars.\n\nThis is a list of titleholders of being the closest red dwarf to Earth, and its succession over time.\n\nThis is a list of titleholders of being the most distant red dwarf to Earth, and its succession over time.\n\nThis is a list of titleholders of being the highest proper motion red dwarf relative to the Sun, and its succession over time.\n\nThis is a list of titleholders of being the highest peculiar velocity red dwarf relative to the local referrant, and its succession over time.\n\nThese are stars that are closest to the boundary to becoming brown dwarfs (aka. failed stars) but still remain fully fusing stars.\n\nThis is a list of titleholders of being the red dwarf with the least mass, and its succession over time.\n\nThese are stars are the ones with the bulkiest masses that remain fully conductive, and unable to ever fuse helium, and will not form planetary nebulae, thus never entering red giant star phase.\n\nThis is a list of titleholders of being the red dwarf with the most mass, and its succession over time.\n\nThis is a list of titleholders of being the red dwarf with the smallest volume, and its succession over time.\n\nThis is a list of titleholders of being the red dwarf with the largest volume, and its succession over time.\n\nThis is a list of the least intrinsically energetic, the least luminous (absolute magnitude) red dwarf stars\n\nThis is a list of the least luminous (absolute magnitude) red dwarfs\n\nThis is a list of the most intrinsically energetic, the most luminous (absolute magnitude) red dwarf stars\n\nThis is a list of the most luminous (absolute magnitude) red dwarfs\n\nThis is a list of the least bright (apparent magnitude) red dwarf stars, as they appear from Earth\n\nThis is a list of titleholders of being the dimmest (apparent magnitude) red dwarf as seen from Earth, and its succession over time.\n\nThis is a list of the most bright (apparent magnitude) red dwarf stars, as they appear from Earth\n\nNo red dwarf stars are visible to the naked eye.\n\nThis is a list of titleholders of being the brightest (apparent magnitude) red dwarf as seen from Earth, and its succession over time.\n\nThis is a list of the youngest red dwarf stars, as they appear when they are observed.\n\nThis is a list of titleholders of being the youngest red dwarf as seen from Earth, and its succession over time.\n\nThis is a list of the oldest red dwarf stars, as they appear when they are observed.\n\nThis is a list of titleholders of being the oldest red dwarf as seen from Earth, and its succession over time.\n\n"}
{"id": "38216839", "url": "https://en.wikipedia.org/wiki?curid=38216839", "title": "List of supranational environmental agencies", "text": "List of supranational environmental agencies\n\nA variety of supranational environmental agencies, commissions, programs and secretariats exist across the world today. Some are global in nature, others regional; they may be multi- or bilateral in character. Some are responsible for broad areas of environmental policy, regulation and implementation; others for very specific issue areas. This article lists notable supranational environmental agencies, by region.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "58882118", "url": "https://en.wikipedia.org/wiki?curid=58882118", "title": "List of threatened ecological communities of Australia", "text": "List of threatened ecological communities of Australia\n\nThis list of threatened ecological communities of Australia is derived from the Australian federal government's assessment of submissions regarding ecological communities, assemblages of flora and fauna with identified interactions in particular habitats, with determinations on their conservation status and level of protection under the \"Environment Protection and Biodiversity Conservation Act 1999\". Their status as threatened ecological communities (TEC) is noted as meeting the criteria as critically endangered, endangered, or vulnerable, and where a revision has resulted in delisting or ineligibility.\n"}
{"id": "7788910", "url": "https://en.wikipedia.org/wiki?curid=7788910", "title": "List of tornadoes and tornado outbreaks in Asia", "text": "List of tornadoes and tornado outbreaks in Asia\n\n\"Parent article:\" List of tornadoes and tornado outbreaks\n\nThese are some notable tornadoes, tornado outbreaks, and tornado outbreak sequences that have occurred in Asia, including the Arabian Peninsula.\n\n\n\n"}
{"id": "2357167", "url": "https://en.wikipedia.org/wiki?curid=2357167", "title": "Machapuchare", "text": "Machapuchare\n\nMachapuchare, Machhapuchchhre or Machhapuchhre (), is a mountain in the Annapurna Himalayas of north central Nepal. It is revered by the local population as particularly sacred to the god Shiva, and hence is off limits to climbing.\n\nMachapuchare is at the end of a long spur ridge, coming south out of the main backbone of the Annapurna Himalayas, which forms the eastern boundary of the Annapurna Sanctuary. The Sanctuary is a favorite trekking destination, and the site of the base camps for the South Face of Annapurna and for numerous smaller objectives. The peak is about north of Pokhara, the main town of the region.\n\nDue to its southern position in the range, and the particularly low terrain that lies south of the Annapurna Himalayas, Machapuchare commands tremendous vertical relief in a short horizontal distance. This, combined with its steep, pointed profile, make it a particularly striking peak, despite a lower elevation than some of its neighbors. Its double summit resembles the tail of a fish, hence the name meaning \"fish's tail\" in Nepalese. It is also nicknamed the \"Matterhorn of Nepal\".\n\nMachapuchare has never been climbed to its summit. The only attempt was in 1957 by a British team led by Lieutenant Colonel Jimmy Roberts. Climbers Wilfrid Noyce and A. D. M. Cox climbed to within of the summit via the north ridge, to an approximate altitude of . They did not complete the ascent, as they had promised not to set foot on the actual summit. Since then, the mountain has been declared sacred, and is now closed to climbers.\n\n\n"}
{"id": "21281954", "url": "https://en.wikipedia.org/wiki?curid=21281954", "title": "Marine snow", "text": "Marine snow\n\nIn the deep ocean, marine snow is a continuous shower of mostly organic detritus falling from the upper layers of the water column. It is a significant means of exporting energy from the light-rich photic zone to the aphotic zone below. The term was first coined by the explorer William Beebe as he observed it from his bathysphere. As the origin of marine snow lies in activities within the productive photic zone, the prevalence of marine snow changes with seasonal fluctuations in photosynthetic activity and ocean currents. Marine snow can be an important food source for organisms living in the aphotic zone, particularly for organisms which live very deep in the water column.\n\nMarine snow is made up of a variety of mostly organic matter, including dead or dying animals and phytoplankton, protists, fecal matter, sand, and other inorganic dust. Most trapped particles are more vulnerable to grazers than they would be as free floating individuals and can be classified as \"olive green\" or \"gray body\" cells, which are plant parts and degrading plant material. A majority of marine snow composition is actually made up of aggregates of smaller particles held together by a sugary mucus, transparent extracellular polysaccharides (TEPs). These are natural polymers exuded as waste products mostly by phytoplankton and bacteria. Mucus secreted by zooplankton (mostly salps, appendicularians, and pteropods) also contribute to the constituents of marine snow aggregates. These aggregates grow over time and may reach several centimeters in diameter, traveling for weeks before reaching the ocean floor.\n\nMarine snow often forms during algal blooms. As phytoplankton accumulate, they aggregate or get captured in other aggregates, both of which accelerate the sinking rate. Aggregation and sinking is actually thought to be a large component of sources for algae loss from surface water. Most organic components of marine snow are consumed by microbes, zooplankton and other filter-feeding animals within the first 1,000 metres of their journey. In this way marine snow may be considered the foundation of deep-sea mesopelagic and benthic ecosystems: As sunlight cannot reach them, deep-sea organisms rely heavily on marine snow as an energy source. The small percentage of material not consumed in shallower waters becomes incorporated into the muddy \"ooze\" blanketing the ocean floor, where it is further decomposed through biological activity.\n\nMarine snow aggregates exhibit characteristics that fit Goldman's \"aggregate spinning wheel hypothesis\". This hypothesis states that phytoplankton, microorganisms and bacteria live attached to aggregate surfaces and are involved in rapid nutrient recycling. Phytoplankton have been shown to be able to take up nutrients from small local concentrations of organic material (e.g. fecal matter from an individual zooplankton cell, regenerated nutrients from organic decomposition by bacteria). As the aggregates slowly sink to the bottom of the ocean, the many microorganisms residing on them are constantly respiring and contribute greatly to the microbial loop.\n\nAggregates begin as the colloidal fraction, which typically contains particles sized between 1 nm and several micrometers. The colloidal fraction of the ocean contains a large amount of organic matter unavailable to grazers. This fraction has a much higher total mass than either phytoplankton or bacteria but is not readily available due to size characteristics of the particles in relation to potential consumers. The colloidal fraction must aggregate in order to be more bioavailable. Aggregation theory outlines main mechanisms by which marine aggregates may form and are as follows:\n\nAggregation theory represents a two state system. At low cell concentrations aggregation is relatively unimportant and somewhat more unlikely. However, at higher cell concentrations it becomes increasingly important. A model has been proposed to characterize the formation of marine aggregates and the loss due to sinking:\nwhere\n\nThus, aggregation of marine particles is more prevalent when cell and particle concentration is higher (e.g. algal blooms)\n\nExport production is the amount of organic matter produced in the ocean by primary production that is not recycled (remineralised) before it sinks into the aphotic zone. Because of the role of export production in the ocean's biological pump, it is typically measured in units of carbon (e.g. mg C m d).\n\nThe fraction of primary production that is exported to the aphotic zone is generally higher when primary production occurs in short (seasonal) bursts, than when it occurs more evenly spread out across a year.\n\nBecause of the relatively long residence time of the ocean's thermohaline circulation, carbon transported as marine snow into the deep ocean by the biological pump can remain out of contact with the atmosphere for more than 1000 years. That is, when the marine snow is finally decomposed to inorganic nutrients and dissolved carbon dioxide, these are effectively isolated from the surface ocean for relatively long time-scales related to ocean circulation. Consequently, enhancing the quantity of marine snow that reaches the deep ocean is the basis of several geoengineering schemes to enhance carbon sequestration by the ocean. Ocean nourishment and iron fertilisation seek to boost the production of organic material in the surface ocean, with a concomitant rise in marine snow reaching the deep ocean. These efforts have not yet produced a sustainable fertilization that effectively transports carbon out of the system.\n\nIncreases in ocean temperatures, a projected indicator of climate change, may result in a decrease in the production of marine snow due to the enhanced stratification of the water column. Increasing stratification decreases the availability of phytoplankton nutrients such as nitrate, phosphate and silicic acid, and could lead to a decrease in primary production and, thus, marine snow.\n\nMarine snow has also begun to garner interest from microbiologists, owing to the microbial communities associated with it. Recent research indicates transported bacteria may exchange genes with previously thought to be isolated populations of bacteria inhabiting the breadth of the ocean floor. In such an immense area there may be as yet undiscovered species tolerant of high pressures and extreme cold, perhaps finding use in bioengineering and pharmacy.\n\n"}
{"id": "422481", "url": "https://en.wikipedia.org/wiki?curid=422481", "title": "Mass–energy equivalence", "text": "Mass–energy equivalence\n\nIn physics, mass–energy equivalence states that anything having mass has an equivalent amount of energy and vice versa, with these fundamental quantities directly relating to one another by Albert Einstein's famous formula:\n\nformula_1\n\nThis formula states that the equivalent energy () can be calculated as the mass () multiplied by the speed of light ( = about m/s) squared. Similarly, anything having energy exhibits a corresponding mass given by its energy divided by the speed of light squared . Because the speed of light is a very large number in everyday units, the formula implies that even an everyday object at rest with a modest amount of mass has a very large amount of energy intrinsically. Chemical, nuclear, and other energy transformations may cause a system to lose some of its energy content (and thus some corresponding mass), releasing it as the radiant energy of light or as thermal energy for example.\n\nMass–energy equivalence arose originally from special relativity as a paradox described by Henri Poincaré. Einstein proposed it on 21 November 1905, in the paper \"Does the inertia of a body depend upon its energy-content?\", one of his \"Annus Mirabilis (Miraculous Year) papers\". Einstein was the first to propose that the equivalence of mass and energy is a general principle and a consequence of the symmetries of space and time.\n\nA consequence of the mass–energy equivalence is that if a body is stationary, it still has some internal or intrinsic energy, called its rest energy, corresponding to its rest mass. When the body is in motion, its total energy is greater than its rest energy, and equivalently its \"total mass\" (also called relativistic mass in this context) is greater than its rest mass. This rest mass is also called the intrinsic or invariant mass because it remains the same regardless of this motion, even for the extreme speeds or gravity considered in special and general relativity.\n\nThe mass–energy formula also serves to convert to (and vice versa), no matter what system of measurement units is used.\n\nThe formula was initially written in many different notations, and its interpretation and justification was further developed in several steps.\nIn \"Does the inertia of a body depend upon its energy content?\" (1905), Einstein used to mean the speed of light in a vacuum and to mean the energy lost by a body in the form of radiation. Consequently, the equation was not originally written as a formula but as a sentence in German saying that \"if a body gives off the energy \"\" in the form of radiation, its mass diminishes by .\" A remark placed above it informed that the equation was approximated by neglecting \"magnitudes of fourth and higher orders\" of a series expansion.\n\nIn May 1907, Einstein explained that the expression for energy of a moving mass point assumes the simplest form, when its expression for the state of rest is chosen to be (where is the mass), which is in agreement with the \"principle of the equivalence of mass and energy\". In addition, Einstein used the formula , with being the energy of a system of mass points, to describe the energy and mass increase of that system when the velocity of the differently moving mass points is increased.\n\nIn June 1907, Max Planck rewrote Einstein's mass–energy relationship as , where is the pressure and the volume to express the relation between mass, its \"latent energy\", and thermodynamic energy within the body. Subsequently, in October 1907, this was rewritten as and given a quantum interpretation by Johannes Stark, who assumed its validity and correctness (\"Gültigkeit\").\n\nIn December 1907, Einstein expressed the equivalence in the form and concluded: \"A mass ' is equivalent, as regards inertia, to a quantity of energy '. [...] It appears far more natural to consider every inertial mass as a store of energy.\"\n\nIn 1909, Gilbert N. Lewis and Richard C. Tolman used two variations of the formula: and , with being the relativistic energy (the energy of an object when the object is moving), is the rest energy (the energy when not moving), is the relativistic mass (the rest mass and the extra mass gained when moving), and is the rest mass (the mass when not moving). The same relations in different notation were used by Hendrik Lorentz in 1913 (published 1914), though he placed the energy on the left-hand side: and , with being the total energy (rest energy plus kinetic energy) of a moving material point, its rest energy, the relativistic mass, and the invariant (or rest) mass.\n\nIn 1911, Max von Laue gave a more comprehensive proof of from the stress–energy tensor, which was later (1918) generalized by Felix Klein.\n\nEinstein returned to the topic once again after World War II and this time he wrote in the title of his article intended as an explanation for a general reader by analogy.\n\nMass and energy can be seen as two names (and two measurement units) for the same underlying, conserved physical quantity. Thus, the laws of conservation of energy and conservation of (total) mass are equivalent and both hold true. Einstein elaborated in a 1946 essay that \"the principle of the conservation of mass [...] proved inadequate in the face of the special theory of relativity. It was therefore merged with the energy conservation principle—just as, about 60 years before, the principle of the conservation of mechanical energy had been combined with the principle of the conservation of heat [thermal energy]. We might say that the principle of the conservation of energy, having previously swallowed up that of the conservation of heat, now proceeded to swallow that of the conservation of mass—and holds the field alone.\"\n\nIf the conservation of mass law is interpreted as conservation of \"rest\" mass, it does not hold true in special relativity. The \"rest\" energy (equivalently, rest mass) of a particle can be converted, not \"to energy\" (it already \"is\" energy (mass)), but rather to \"other\" forms of energy (mass) that require motion, such as kinetic energy, thermal energy, or radiant energy. Similarly, kinetic or radiant energy can be converted to other kinds of particles that have rest energy (rest mass). In the transformation process, neither the total amount of mass nor the total amount of energy changes, since both properties are connected via a simple constant. This view requires that if either energy or (total) mass disappears from a system, it is always found that both have simply moved to another place, where they are both measurable as an increase of both energy and mass that corresponds to the loss in the first system.\n\nWhen an object is pushed in the direction of motion, it gains momentum and energy, but when the object is already traveling near the speed of light, it cannot move much faster, no matter how much energy it absorbs. Its momentum and energy continue to increase without bounds, whereas its speed approaches (but never reaches) a constant value—the speed of light. This implies that in relativity the momentum of an object cannot be a constant times the velocity, nor can the kinetic energy be a constant times the square of the velocity.\n\nA property called the relativistic mass is defined as the ratio of the momentum of an object to its velocity. Relativistic mass depends on the motion of the object, so that different observers in relative motion see different values for it. If the object is moving slowly, the relativistic mass is nearly equal to the rest mass and both are nearly equal to the usual Newtonian mass. If the object is moving quickly, the relativistic mass is greater than the rest mass by an amount equal to the mass associated with the kinetic energy of the object. As the object approaches the speed of light, the relativistic mass grows infinitely, because the kinetic energy grows infinitely and this energy is associated with mass.\n\nThe relativistic mass is always equal to the total energy (rest energy plus kinetic energy) divided by . Because the relativistic mass is exactly proportional to the energy, relativistic mass and relativistic energy are nearly synonyms; the only difference between them is the units. If length and time are measured in natural units, the speed of light is equal to 1, and even this difference disappears. Then mass and energy have the same units and are always equal, so it is redundant to speak about relativistic mass, because it is just another name for the energy. This is why physicists usually reserve the useful short word \"mass\" to mean rest mass, or invariant mass, and not relativistic mass.\n\nThe relativistic mass of a moving object is larger than the relativistic mass of an object that is not moving, because a moving object has extra kinetic energy. The \"rest mass\" of an object is defined as the mass of an object when it is at rest, so that the rest mass is always the same, independent of the motion of the observer: it is the same in all inertial frames.\n\nFor things and systems made up of many parts, like an atomic nucleus, planet, or star, the relativistic mass is the sum of the relativistic masses (or energies) of the parts, because energies are additive in isolated systems. This is not true in open systems, however, if energy is subtracted. For example, if a system is \"bound\" by attractive forces, and the energy gained due to the forces of attraction in excess of the work done is removed from the system, then mass is lost with this removed energy. For example, the mass of an atomic nucleus is less than the total mass of the protons and neutrons that make it up, but this is only true after this energy from binding has been removed in the form of a gamma ray (which in this system, carries away the mass of the energy of binding). This mass decrease is also equivalent to the energy required to break up the nucleus into individual protons and neutrons (in this case, work and mass would need to be supplied). Similarly, the mass of the solar system is slightly less than the sum of the individual masses of the sun and planets.\n\nFor a system of particles going off in different directions, the invariant mass of the system is the analog of the rest mass, and is the same for all observers, even those in relative motion. It is defined as the total energy (divided by ) in the center of mass frame (where by definition, the system total momentum is zero). A simple example of an object with moving parts but zero total momentum is a container of gas. In this case, the mass of the container is given by its total energy (including the kinetic energy of the gas molecules), since the system total energy and invariant mass are the same in any reference frame where the momentum is zero, and such a reference frame is also the only frame in which the object can be weighed. In a similar way, the theory of special relativity posits that the thermal energy in all objects (including solids) contributes to their total masses and weights, even though this energy is present as the kinetic and potential energies of the atoms in the object, and it (in a similar way to the gas) is not seen in the rest masses of the atoms that make up the object.\n\nIn a similar manner, even photons (light quanta), if trapped in a container space (as a photon gas or thermal radiation), would contribute a mass associated with their energy to the container. Such an extra mass, in theory, could be weighed in the same way as any other type of rest mass. This is true in special relativity theory, even though individually photons have no rest mass. The property that trapped energy \"in any form\" adds weighable mass to systems that have no net momentum is one of the characteristic and notable consequences of relativity. It has no counterpart in classical Newtonian physics, in which radiation, light, heat, and kinetic energy never exhibit weighable mass under any circumstances.\n\nJust as the relativistic mass of an isolated system is conserved through time, so also is its invariant mass.This property allows the conservation of all types of mass in systems, and also conservation of all types of mass in reactions where matter is destroyed (annihilated), leaving behind the energy that was associated with it (which is now in non-material form, rather than material form). Matter may appear and disappear in various reactions, but mass and energy are both unchanged in this process.\n\nAs is noted above, two different definitions of mass have been used in special relativity, and also two different definitions of energy. The simple equation formula_2 is not generally applicable to all these types of mass and energy, except in the special case that the total additive momentum is zero for the system under consideration. In such a case, which is always guaranteed when observing the system from either its center of mass frame or its center of momentum frame, formula_2 is always true for any type of mass and energy that are chosen. Thus, for example, in the center of mass frame, the total energy of an object or system is equal to its rest mass times formula_4, a useful equality. This is the relationship used for the container of gas in the previous example. It is \"not\" true in other reference frames where the center of mass is in motion. In these systems or for such an object, its total energy depends on both its rest (or invariant) mass, and its (total) momentum.\n\nIn inertial reference frames other than the rest frame or center of mass frame, the equation formula_2 remains true if the energy is the relativistic energy \"and\" the mass is the relativistic mass. It is also correct if the energy is the rest or invariant energy (also the minimum energy), \"and\" the mass is the rest mass, or the invariant mass. However, connection of the total or relativistic energy (formula_6) with the rest or invariant mass (formula_7) requires consideration of the system's total momentum, in systems and reference frames where the total momentum (of magnitude ) has a non-zero value. The formula then required to connect the two different kinds of mass and energy, is the extended version of Einstein's equation, called the relativistic energy–momentum relation:\n\nor\n\nHere the formula_10 term represents the square of the Euclidean norm (total vector length) of the various momentum vectors in the system, which reduces to the square of the simple momentum magnitude, if only a single particle is considered. This equation reduces to formula_2 when the momentum term is zero. For photons where formula_12, the equation reduces to formula_13.\n\nMass–energy equivalence states that any object has a certain energy, even when it is stationary. In Newtonian mechanics, a motionless body has no kinetic energy, and it may or may not have other amounts of internal stored energy, like chemical energy or thermal energy, in addition to any potential energy it may have from its position in a field of force. In Newtonian mechanics, all of these energies are much smaller than the mass of the object times the speed of light squared.\n\nIn relativity, all the energy that moves with an object (that is, all the energy present in the object's rest frame) contributes to the total mass of the body, which measures how much it resists acceleration. Each bit of potential and kinetic energy makes a proportional contribution to the mass. As noted above, even if a box of ideal mirrors \"contains\" light, then the individually massless photons still contribute to the total mass of the box, by the amount of their energy divided by .\n\nIn relativity, removing energy is removing mass, and for an observer in the center of mass frame, the formula indicates how much mass is lost when energy is removed. In a nuclear reaction, the mass of the atoms that come out is less than the mass of the atoms that go in, and the difference in mass shows up as heat and light with the same relativistic mass as the difference (and also the same invariant mass in the center of mass frame of the system). In this case, the in the formula is the energy released and removed, and the mass is how much the mass decreases. In the same way, when any sort of energy is added to an isolated system, the increase in the mass is equal to the added energy divided by . For example, when water is heated it gains about of mass for every joule of heat added to the water.\n\nAn object moves with different speed in different frames, depending on the motion of the observer, so the kinetic energy in both Newtonian mechanics and relativity is \"frame dependent\". This means that the amount of relativistic energy, and therefore the amount of relativistic mass, that an object is measured to have depends on the observer. The \"rest mass\" is defined as the mass that an object has when it is not moving (or when an inertial frame is chosen such that it is not moving). The term also applies to the invariant mass of systems when the system as a whole is not \"moving\" (has no net momentum). The rest and invariant masses are the smallest possible value of the mass of the object or system. They also are conserved quantities, so long as the system is isolated. Because of the way they are calculated, the effects of moving observers are subtracted, so these quantities do not change with the motion of the observer.\n\nThe rest mass is almost never additive: the rest mass of an object is not the sum of the rest masses of its parts. The rest mass of an object is the total energy of all the parts, including kinetic energy, as measured by an observer that sees the center of the mass of the object to be standing still. The rest mass adds up only if the parts are standing still and do not attract or repel, so that they do not have any extra kinetic or potential energy. The other possibility is that they have a positive kinetic energy and a negative potential energy that exactly cancels.\n\nWhenever any type of energy is removed from a system, the mass associated with the energy is also removed, and the system therefore loses mass. This mass defect in the system may be simply calculated as , and this was the form of the equation historically first presented by Einstein in 1905. However, use of this formula in such circumstances has led to the false idea that mass has been \"converted\" to energy. This may be particularly the case when the energy (and mass) removed from the system is associated with the \"binding energy\" of the system. In such cases, the binding energy is observed as a \"mass defect\" or deficit in the new system.\n\nThe fact that the released energy is not easily weighed in many such cases, may cause its mass to be neglected as though it no longer existed. This circumstance has encouraged the false idea of conversion of \"mass\" to energy, rather than the correct idea that the binding energy of such systems is relatively large, and exhibits a measurable mass, which is removed when the binding energy is removed. .\n\nThe difference between the rest mass of a bound system and of the unbound parts is the binding energy of the system, if this energy has been removed after binding. For example, a water molecule weighs a little less than two free hydrogen atoms and an oxygen atom. The minuscule mass difference is the energy needed to split the molecule into three individual atoms (divided by ), which was given off as heat when the molecule formed (this heat had mass). Likewise, a stick of dynamite in theory weighs a little bit more than the fragments after the explosion, but this is true only so long as the fragments are cooled and the heat removed. In this case the mass difference is the energy/heat that is released when the dynamite explodes, and when this heat escapes, the mass associated with it escapes, only to be deposited in the surroundings, which absorb the heat (so that total mass is conserved).\n\nSuch a change in mass may only happen when the system is open, and the energy and mass escapes. Thus, if a stick of dynamite is blown up in a hermetically sealed chamber, the mass of the chamber and fragments, the heat, sound, and light would still be equal to the original mass of the chamber and dynamite. If sitting on a scale, the weight and mass would not change. This would in theory also happen even with a nuclear bomb, if it could be kept in an ideal box of infinite strength, which did not rupture or pass radiation. Thus, a 21.5 kiloton () nuclear bomb produces about one gram of heat and electromagnetic radiation, but the mass of this energy would not be detectable in an exploded bomb in an ideal box sitting on a scale; instead, the contents of the box would be heated to millions of degrees without changing total mass and weight. If then, however, a transparent window (passing only electromagnetic radiation) were opened in such an ideal box after the explosion, and a beam of X-rays and other lower-energy light allowed to escape the box, it would eventually be found to weigh one gram less than it had before the explosion. This weight loss and mass loss would happen as the box was cooled by this process, to room temperature. However, any surrounding mass that absorbed the X-rays (and other \"heat\") would \"gain\" this gram of mass from the resulting heating, so the mass \"loss\" would represent merely its relocation. Thus, no mass (or, in the case of a nuclear bomb, no matter) would be \"converted\" to energy in such a process. Mass and energy, as always, would both be separately conserved.\n\nMassless particles have zero rest mass. Their relativistic mass is simply their relativistic energy, divided by , or . The energy for photons is , where is Planck's constant and is the photon frequency. This frequency and thus the relativistic energy are frame-dependent.\n\nIf an observer runs away from a photon in the direction the photon travels from a source, and it catches up with the observer—when the photon catches up, the observer sees it as having less energy than it had at the source. The faster the observer is traveling with regard to the source when the photon catches up, the less energy the photon has. As an observer approaches the speed of light with regard to the source, the photon looks redder and redder, by relativistic Doppler effect (the Doppler shift is the relativistic formula), and the energy of a very long-wavelength photon approaches zero. This is because the photon is \"massless\"—the rest mass of a photon is zero.\n\nTwo photons moving in different directions cannot both be made to have arbitrarily small total energy by changing frames, or by moving toward or away from them. The reason is that in a two-photon system, the energy of one photon is decreased by chasing after it, but the energy of the other increases with the same shift in observer motion. Two photons not moving in the same direction comprise an inertial frame where the combined energy is smallest, but not zero. This is called the center of mass frame or the center of momentum frame; these terms are almost synonyms (the center of mass frame is the special case of a center of momentum frame where the center of mass is put at the origin). The most that chasing a pair of photons can accomplish to decrease their energy is to put the observer in a frame where the photons have equal energy and are moving directly away from each other. In this frame, the observer is now moving in the same direction and speed as the center of mass of the two photons. The total momentum of the photons is now zero, since their momenta are equal and opposite. In this frame the two photons, as a system, have a mass equal to their total energy divided by . This mass is called the invariant mass of the pair of photons together. It is the smallest mass and energy the system may be seen to have, by any observer. It is only the invariant mass of a two-photon system that can be used to make a single particle with the same rest mass.\n\nIf the photons are formed by the collision of a particle and an antiparticle, the invariant mass is the same as the total energy of the particle and antiparticle (their rest energy plus the kinetic energy), in the center of mass frame, where they automatically move in equal and opposite directions (since they have equal momentum in this frame). If the photons are formed by the disintegration of a \"single\" particle with a well-defined rest mass, like the neutral pion, the invariant mass of the photons is equal to rest mass of the pion. In this case, the center of mass frame for the pion is just the frame where the pion is at rest, and the center of mass does not change after it disintegrates into two photons. After the two photons are formed, their center of mass is still moving the same way the pion did, and their total energy in this frame adds up to the mass energy of the pion. Thus, by calculating the invariant mass of pairs of photons in a particle detector, pairs can be identified that were probably produced by pion disintegration.\n\nA similar calculation illustrates that the invariant mass of systems is conserved, even when massive particles (particles with rest mass) within the system are converted to massless particles (such as photons). In such cases, the photons contribute invariant mass to the system, even though they individually have no invariant mass or rest mass. Thus, an electron and positron (each of which has rest mass) may undergo annihilation with each other to produce two photons, each of which is massless (has no rest mass). However, in such circumstances, no system mass is lost. Instead, the system of both photons moving away from each other has an invariant mass, which acts like a rest mass for any system in which the photons are trapped, or that can be weighed. Thus, not only the quantity of relativistic mass, but also the quantity of invariant mass does not change in transformations between \"matter\" (electrons and positrons) and energy (photons).\n\nIn physics, there are two distinct concepts of mass: the gravitational mass and the inertial mass. The gravitational mass is the quantity that determines the strength of the gravitational field generated by an object, as well as the gravitational force acting on the object when it is immersed in a gravitational field produced by other bodies. The inertial mass, on the other hand, quantifies how much an object accelerates if a given force is applied to it. The mass–energy equivalence in special relativity refers to the inertial mass. However, already in the context of Newton gravity, the Weak Equivalence Principle is postulated: the gravitational and the inertial mass of every object are the same. Thus, the mass–energy equivalence, combined with the Weak Equivalence Principle, results in the prediction that all forms of energy contribute to the gravitational field generated by an object. This observation is one of the pillars of the general theory of relativity.\n\nThe above prediction, that all forms of energy interact gravitationally, has been subject to experimental tests. The first observation testing this prediction was made in 1919. During a solar eclipse, Arthur Eddington observed that the light from stars passing close to the Sun was bent. The effect is due to the gravitational attraction of light by the Sun. The observation confirmed that the energy carried by light indeed is equivalent to a gravitational mass. Another seminal experiment, the Pound–Rebka experiment, was performed in 1960. In this test a beam of light was emitted from the top of a tower and detected at the bottom. The frequency of the light detected was higher than the light emitted. This result confirms that the energy of photons increases when they fall in the gravitational field of the Earth. The energy, and therefore the gravitational mass, of photons is proportional to their frequency as stated by the Planck's relation.\n\nMax Planck pointed out that the mass–energy equivalence formula implied that bound systems would have a mass less than the sum of their constituents, once the binding energy had been allowed to escape. However, Planck was thinking about chemical reactions, where the binding energy is too small to measure. Einstein suggested that radioactive materials such as radium would provide a test of the theory, but even though a large amount of energy is released per atom in radium, due to the half-life of the substance (1602 years), only a small fraction of radium atoms decay over an experimentally measurable period of time.\n\nOnce the nucleus was discovered, experimenters realized that the very high binding energies of the atomic nuclei should allow calculation of their binding energies, simply from mass differences. But it was not until the discovery of the neutron in 1932, and the measurement of the neutron mass, that this calculation could actually be performed (see nuclear binding energy for example calculation). A little while later, the Cockcroft–Walton accelerator produced the first transmutation reaction (), verifying Einstein's formula to an accuracy of ±0.5%.\nIn 2005, Rainville et al. published a direct test of the energy-equivalence of mass lost in the binding energy of a neutron to atoms of particular isotopes of silicon and sulfur, by comparing the mass lost to the energy of the emitted gamma ray associated with the neutron capture. The binding mass-loss agreed with the gamma ray energy to a precision of ±0.00004%, the most accurate test of to date.\n\nThe mass–energy equivalence formula was used in the understanding of nuclear fission reactions, and implies the great amount of energy that can be released by a nuclear fission chain reaction, used in both nuclear weapons and nuclear power. By measuring the mass of different atomic nuclei and subtracting from that number the total mass of the protons and neutrons as they would weigh separately, one gets the exact binding energy available in an atomic nucleus. This is used to calculate the energy released in any nuclear reaction, as the difference in the total mass of the nuclei that enter and exit the reaction.\n\nEinstein used the CGS system of units (centimeters, grams, seconds, dynes, and ergs), but the formula is independent of the system of units. In natural units, the numerical value of the speed of light is set to equal 1, and the formula expresses an equality of numerical values: . In the SI system (expressing the ratio in joules per kilogram using the value of in meters per second):\n\nSo the energy equivalent of one kilogram of mass is\n\nor the energy released by combustion of the following:\n\nAny time energy is generated, the process can be evaluated from an perspective. For instance, the \"Gadget\"-style bomb used in the Trinity test and the bombing of Nagasaki had an explosive yield equivalent to 21 kt of TNT. About 1 kg of the approximately 6.15 kg of plutonium in each of these bombs fissioned into lighter elements totaling almost exactly one gram less, after cooling. The electromagnetic radiation and kinetic energy (thermal and blast energy) released in this explosion carried the missing one gram of mass. This occurs because nuclear binding energy is released whenever elements with more than 62 nucleons fission.\n\nAnother example is hydroelectric generation. The electrical energy produced by Grand Coulee Dam's turbines every 3.7 hours represents one gram of mass. This mass passes to electrical devices (such as lights in cities) powered by the generators, where it appears as a gram of heat and light. Turbine designers look at their equations in terms of pressure, torque, and RPM. However, Einstein's equations show that all energy has mass, and thus the electrical energy produced by a dam's generators, and the resulting heat and light, all retain their mass—which is equivalent to the energy. The potential energy—and equivalent mass—represented by the waters of the Columbia River as it descends to the Pacific Ocean would be converted to heat due to viscous friction and the turbulence of white water rapids and waterfalls were it not for the dam and its generators. This heat would remain as mass on site at the water, were it not for the equipment that converted some of this potential and kinetic energy into electrical energy, which can move from place to place (taking mass with it).\n\nWhenever energy is added to a system, the system gains mass, as shown when the equation is rearranged:\n\nNote that no net mass or energy is really created or lost in any of these examples and scenarios. Mass/energy simply moves from one place to another. These are some examples of the \"transfer\" of energy and mass in accordance with the \"principle of mass–energy conservation\".\n\nAlthough mass cannot be converted to energy, in some reactions matter particles (which contain a form of rest energy) can be destroyed and the energy released can be converted to other types of energy that are more usable and obvious as forms of energy—such as light and energy of motion (heat, etc.). However, the total amount of energy and mass does not change in such a transformation. Even when particles are not destroyed, a certain fraction of the ill-defined \"matter\" in ordinary objects can be destroyed, and its associated energy liberated and made available as the more dramatic energies of light and heat, even though no identifiable real particles are destroyed, and even though (again) the total energy is unchanged (as also the total mass). Such conversions between types of energy (resting to active energy) happen in nuclear weapons, in which the protons and neutrons in atomic nuclei lose a small fraction of their average mass, but this mass loss is not due to the destruction of any protons or neutrons (or even, in general, lighter particles like electrons). Also the mass is not destroyed, but simply removed from the system in the form of heat and light from the reaction.\n\nIn nuclear reactions, typically only a small fraction of the total mass–energy of the bomb converts into the mass–energy of heat, light, radiation, and motion—which are \"active\" forms that can be used. When an atom fissions, it loses only about 0.1% of its mass (which escapes from the system and does not disappear), and additionally, in a bomb or reactor not all the atoms can fission. In a modern fission-based atomic bomb, the efficiency is only about 40%, so only 40% of the fissionable atoms actually fission, and only about 0.03% of the fissile core mass appears as energy in the end. In nuclear fusion, more of the mass is released as usable energy, roughly 0.3%. But in a fusion bomb, the bomb mass is partly casing and non-reacting components, so that in practicality, again (coincidentally) no more than about 0.03% of the total mass of the entire weapon is released as usable energy (which, again, retains the \"missing\" mass). See nuclear weapon yield for practical details of this ratio in modern nuclear weapons.\n\nIn theory, it should be possible to destroy matter and convert all of the rest-energy associated with matter into heat and light (which would of course have the same mass), but none of the theoretically known methods are practical. One way to convert all the energy within matter into usable energy is to annihilate matter with antimatter. But antimatter is rare in our universe, and must be made first. Due to inefficient mechanisms of production, making antimatter always requires far more usable energy than would be released when it was annihilated.\n\nSince most of the mass of ordinary objects resides in protons and neutrons, converting all the energy of ordinary matter into more useful energy requires that the protons and neutrons be converted to lighter particles, or particles with no rest-mass at all. In the Standard Model of particle physics, the number of protons plus neutrons is nearly exactly conserved. Still, Gerard 't Hooft showed that there is a process that converts protons and neutrons to antielectrons and neutrinos. This is the weak SU(2) instanton proposed by Belavin Polyakov Schwarz and Tyupkin. This process, can in principle destroy matter and convert all the energy of matter into neutrinos and usable energy, but it is normally extraordinarily slow. Later it became clear that this process happens at a fast rate at very high temperatures, since then, instanton-like configurations are copiously produced from thermal fluctuations. The temperature required is so high that it would only have been reached shortly after the big bang.\n\nMany extensions of the standard model contain magnetic monopoles, and in some models of grand unification, these monopoles catalyze proton decay, a process known as the Callan-Rubakov effect. This process would be an efficient mass–energy conversion at ordinary temperatures, but it requires making monopoles and anti-monopoles first. The energy required to produce monopoles is believed to be enormous, but magnetic charge is conserved, so that the lightest monopole is stable. All these properties are deduced in theoretical models—magnetic monopoles have never been observed, nor have they been produced in any experiment so far.\n\nA third known method of total matter–energy \"conversion\" (which again in practice only means conversion of one type of energy into a different type of energy), is using gravity, specifically black holes. Stephen Hawking theorized that black holes radiate thermally with no regard to how they are formed. So, it is theoretically possible to throw matter into a black hole and use the emitted heat to generate power. According to the theory of Hawking radiation, however, the black hole used radiates at a higher rate the smaller it is, producing usable powers at only small black hole masses, where usable may for example be something greater than the local background radiation. It is also worth noting that the ambient irradiated power would change with the mass of the black hole, increasing as the mass of the black hole decreases, or decreasing as the mass increases, at a rate where power is proportional to the inverse square of the mass. In a \"practical\" scenario, mass and energy could be dumped into the black hole to regulate this growth, or keep its size, and thus power output, near constant. This could result from the fact that mass and energy are lost from the hole with its thermal radiation.\n\nIn developing special relativity, Einstein found that the kinetic energy of a moving body is\n\nwith the velocity, the rest mass, and the Lorentz factor.\n\nHe included the second term on the right to make sure that for small velocities the energy would be the same as in classical mechanics, thus satisfying the correspondence principle:\n\nWithout this second term, there would be an additional contribution in the energy when the particle is not moving.\n\nEinstein found that the total momentum of a moving particle is:\n\nIt is this quantity that is conserved in collisions. The ratio of the momentum to the velocity is the relativistic mass, .\n\nAnd the relativistic mass and the relativistic kinetic energy are related by the formula:\n\nEinstein wanted to omit the unnatural second term on the right-hand side, whose only purpose is to make the energy at rest zero, and to declare that the particle has a total energy, which obeys:\n\nwhich is a sum of the rest energy and the kinetic energy. This total energy is mathematically more elegant, and fits better with the momentum in relativity. But to come to this conclusion, Einstein needed to think carefully about collisions. This expression for the energy implied that matter at rest has a huge amount of energy, and it is not clear whether this energy is physically real, or just a mathematical artifact with no physical meaning.\n\nIn a collision process where all the rest-masses are the same at the beginning as at the end, either expression for the energy is conserved. The two expressions only differ by a constant that is the same at the beginning and at the end of the collision. Still, by analyzing the situation where particles are thrown off a heavy central particle, it is easy to see that the inertia of the central particle is reduced by the total energy emitted. This allowed Einstein to conclude that the inertia of a heavy particle is increased or diminished according to the energy it absorbs or emits.\n\nAfter Einstein first made his proposal, it became clear that the word mass can have two different meanings. Some denote the \"relativistic mass\" with an explicit index:\n\nThis mass is the ratio of momentum to velocity, and it is also the relativistic energy divided by (it is not Lorentz-invariant, in contrast to formula_7). The equation holds for moving objects. When the velocity is small, the relativistic mass and the rest mass are almost exactly the same.\n\nAlso Einstein (following Hendrik Lorentz and Max Abraham) used velocity- and direction-dependent mass concepts () in his 1905 electrodynamics paper and in another paper in 1906.\nHowever, in his first paper on (1905), he treated as what would now be called the \"rest mass\". Some claim that (in later years) he did not like the idea of \"relativistic mass\".  When modern physicists say \"mass\", they are usually talking about rest mass, since if they meant \"relativistic mass\", they would just say \"energy\".\n\nConsiderable debate has ensued over the use of the concept \"relativistic mass\" and the connection of \"mass\" in relativity to \"mass\" in Newtonian dynamics. For example, one view is that only rest mass is a viable concept and is a property of the particle; while relativistic mass is a conglomeration of particle properties and properties of spacetime. A perspective that avoids this debate, due to Kjell Vøyenli, is that the Newtonian concept of mass as a particle property and the relativistic concept of mass have to be viewed as embedded in their own theories and as having no precise connection.\n\nWe can rewrite the expression as a Taylor series:\n\nFor speeds much smaller than the speed of light, higher-order terms in this expression get smaller and smaller because is small. For low speeds we can ignore all but the first two terms:\n\nThe total energy is a sum of the rest energy and the Newtonian kinetic energy.\n\nThe classical energy equation ignores both the part, and the high-speed corrections. This is appropriate, because all the high-order corrections are small. Since only \"changes\" in energy affect the behavior of objects, whether we include the part makes no difference, since it is constant. For the same reason, it is possible to subtract the rest energy from the total energy in relativity. By considering the emission of energy in different frames, Einstein could show that the rest energy has a real physical meaning.\n\nThe higher-order terms are extra corrections to Newtonian mechanics, and become important at higher speeds. The Newtonian equation is only a low-speed approximation, but an extraordinarily good one. All of the calculations used in putting astronauts on the moon, for example, could have been done using Newton's equations without any of the higher-order corrections.\nThe total mass energy equivalence should also include the rotational and vibrational kinetic energies as well as the linear kinetic energy at low speeds.\n\nWhile Einstein was the first to have correctly deduced the mass–energy equivalence formula, he was not the first to have related energy with mass. But nearly all previous authors thought that the energy that contributes to mass comes only from electromagnetic fields.\n\nIn 1717 Isaac Newton speculated that light particles and matter particles were interconvertible in \"Query 30\" of the \"Opticks\", where he asks:\n\nIn 1734 the Swedish scientist and theologian Emanuel Swedenborg in his \"Principia\" theorized that all matter is ultimately composed of dimensionless points of \"pure and total motion.\" He described this motion as being without force, direction or speed, but having the potential for force, direction and speed everywhere within it.\n\nThere were many attempts in the 19th and the beginning of the 20th century—like those of J. J. Thomson (1881), Oliver Heaviside (1888), and George Frederick Charles Searle (1897), Wilhelm Wien (1900), Max Abraham (1902), Hendrik Antoon Lorentz (1904) — to understand how the mass of a charged object depends on the electrostatic field. This concept was called electromagnetic mass, and was considered as being dependent on velocity and direction as well. Lorentz (1904) gave the following expressions for longitudinal and transverse electromagnetic mass:\n\nwhere\n\nAnother way of deriving some sort of electromagnetic mass was based on the concept of radiation pressure. In 1900, Henri Poincaré associated electromagnetic radiation energy with a \"fictitious fluid\" having momentum and mass \n\nBy that, Poincaré tried to save the center of mass theorem in Lorentz's theory, though his treatment led to radiation paradoxes.\n\nFriedrich Hasenöhrl showed in 1904, that electromagnetic cavity radiation contributes the \"apparent mass\"\n\nto the cavity's mass. He argued that this implies mass dependence on temperature as well.\n\nAlbert Einstein did not formulate exactly the formula in his 1905 \"Annus Mirabilis\" paper \"Does the Inertia of an object Depend Upon Its Energy Content?\"; rather, the paper states that if a body gives off the energy in the form of radiation, its mass diminishes by . (Here, \"radiation\" means electromagnetic radiation, or light, and mass means the ordinary Newtonian mass of a slow-moving object.) This formulation relates only a change in mass to a change in energy without requiring the absolute relationship.\n\nObjects with zero mass presumably have zero energy, so the extension that all mass is proportional to energy is obvious from this result. In 1905, even the hypothesis that changes in energy are accompanied by changes in mass was untested. Not until the discovery of the first type of antimatter (the positron in 1932) was it found that all of the mass of pairs of resting particles could be converted to radiation.\n\nAlready in his relativity paper \"On the electrodynamics of moving bodies\", Einstein derived the correct expression for the kinetic energy of particles:\n\nNow the question remained open as to which formulation applies to bodies at rest. This was tackled by Einstein in his paper \"Does the inertia of a body depend upon its energy content?\", where he used a body emitting two light pulses in opposite directions, having energies of before and after the emission as seen in its rest frame. As seen from a moving frame, this becomes and . Einstein obtained:\n\nthen he argued that can only differ from the kinetic energy by an additive constant, which gives\n\nNeglecting effects higher than third order in after a Taylor series expansion of the right side of this gives:\n\nEinstein concluded that the emission reduces the body's mass by , and that the mass of a body is a measure of its energy content.\n\nThe correctness of Einstein's 1905 derivation of was criticized by Max Planck (1907), who argued that it is only valid to first approximation. Another criticism was formulated by Herbert Ives (1952) and Max Jammer (1961), asserting that Einstein's derivation is based on begging the question.\nOn the other hand, John Stachel and Roberto Torretti (1982) argued that Ives' criticism was wrong, and that Einstein's derivation was correct.\nHans Ohanian (2008) agreed with Stachel/Torretti's criticism of Ives, though he argued that Einstein's derivation was wrong for other reasons. For a recent review, see Hecht (2011).\n\nAn alternative version of Einstein's thought experiment was proposed by Fritz Rohrlich (1990), who based his reasoning on the Doppler effect.\nLike Einstein, he considered a body at rest with mass . If the body is examined in a frame moving with nonrelativistic velocity , it is no longer at rest and in the moving frame it has momentum . Then he supposed the body emits two pulses of light to the left and to the right, each carrying an equal amount of energy . In its rest frame, the object remains at rest after the emission since the two beams are equal in strength and carry opposite momentum.\n\nHowever, if the same process is considered in a frame that moves with velocity to the left, the pulse moving to the left is redshifted, while the pulse moving to the right is blue shifted. The blue light carries more momentum than the red light, so that the momentum of the light in the moving frame is not balanced: the light is carrying some net momentum to the right.\n\nThe object has not changed its velocity before or after the emission. Yet in this frame it has lost some right-momentum to the light. The only way it could have lost momentum is by losing mass. This also solves Poincaré's radiation paradox, discussed above.\n\nThe velocity is small, so the right-moving light is blueshifted by an amount equal to the nonrelativistic Doppler shift factor . The momentum of the light is its energy divided by , and it is increased by a factor of . So the right-moving light is carrying an extra momentum given by:\n\nThe left-moving light carries a little less momentum, by the same amount . So the total right-momentum in the light is twice . This is the right-momentum that the object lost.\n\nThe momentum of the object in the moving frame after the emission is reduced to this amount:\n\nSo the change in the object's mass is equal to the total energy lost divided by . Since any emission of energy can be carried out by a two step process, where first the energy is emitted as light and then the light is converted to some other form of energy, any emission of energy is accompanied by a loss of mass. Similarly, by considering absorption, a gain in energy is accompanied by a gain in mass.\n\nLike Poincaré, Einstein concluded in 1906 that the inertia of electromagnetic energy is a necessary condition for the center-of-mass theorem to hold. On this occasion, Einstein referred to Poincaré's 1900 paper and wrote:\n\nIn Einstein's more physical, as opposed to formal or mathematical, point of view, there was no need for fictitious masses. He could avoid the \"perpetuum mobile\" problem because, on the basis of the mass–energy equivalence, he could show that the transport of inertia that accompanies the emission and absorption of radiation solves the problem. Poincaré's rejection of the principle of action–reaction can be avoided through Einstein's , because mass conservation appears as a special case of the energy conservation law.\n\nDuring the nineteenth century there were several speculative attempts to show that mass and energy were proportional in various ether theories. In 1873 Nikolay Umov pointed out a relation between mass and energy for ether in the form of , where . The writings of Samuel Tolver Preston, and a 1903 paper by Olinto De Pretto, presented a mass–energy relation. Bartocci (1999) observed that there were only three degrees of separation linking De Pretto to Einstein, concluding that Einstein was probably aware of De Pretto's work.\n\nPreston and De Pretto, following Le Sage, imagined that the universe was filled with an ether of tiny particles that always move at speed . Each of these particles has a kinetic energy of up to a small numerical factor. The nonrelativistic kinetic energy formula did not always include the traditional factor of , since Leibniz introduced kinetic energy without it, and the is largely conventional in prerelativistic physics. By assuming that every particle has a mass that is the sum of the masses of the ether particles, the authors concluded that all matter contains an amount of kinetic energy either given by or depending on the convention. A particle ether was usually considered unacceptably speculative science at the time, and since these authors did not formulate relativity, their reasoning is completely different from that of Einstein, who used relativity to change frames.\n\nIndependently, Gustave Le Bon in 1905 speculated that atoms could release large amounts of latent energy, reasoning from an all-encompassing qualitative philosophy of physics.\n\nIt was quickly noted after the discovery of radioactivity in 1897, that the total energy due to radioactive processes is about one \"million times\" greater than that involved in any known molecular change. However, it raised the question where this energy is coming from. After eliminating the idea of absorption and emission of some sort of Lesagian ether particles, the existence of a huge amount of latent energy, stored within matter, was proposed by Ernest Rutherford and Frederick Soddy in 1903. Rutherford also suggested that this internal energy is stored within normal matter as well. He went on to speculate in 1904:\n\nEinstein's equation is in no way an explanation of the large energies released in radioactive decay (this comes from the powerful nuclear forces involved; forces that were still unknown in 1905). In any case, the enormous energy released from radioactive decay (which had been measured by Rutherford) was much more easily measured than the (still small) change in the gross mass of materials as a result. Einstein's equation, by theory, can give these energies by measuring mass differences before and after reactions, but in practice, these mass differences in 1905 were still too small to be measured in bulk. Prior to this, the ease of measuring radioactive decay energies with a calorimeter was thought possibly likely to allow measurement of changes in mass difference, as a check on Einstein's equation itself. Einstein mentions in his 1905 paper that mass–energy equivalence might perhaps be tested with radioactive decay, which releases enough energy (the quantitative amount known roughly by 1905) to possibly be \"weighed,\" when missing from the system (having been given off as heat). However, radioactivity seemed to proceed at its own unalterable (and quite slow, for radioactives known then) pace, and even when simple nuclear reactions became possible using proton bombardment, the idea that these great amounts of usable energy could be liberated at will with any practicality, proved difficult to substantiate. Rutherford was reported in 1933 to have declared that this energy could not be exploited efficiently: \"Anyone who expects a source of power from the transformation of the atom is talking moonshine.\"\n\nThis situation changed dramatically in 1932 with the discovery of the neutron and its mass, allowing mass differences for single nuclides and their reactions to be calculated directly, and compared with the sum of masses for the particles that made up their composition. In 1933, the energy released from the reaction of lithium-7 plus protons giving rise to 2 alpha particles (as noted above by Rutherford), allowed Einstein's equation to be tested to an error of ±0.5%. However, scientists still did not see such reactions as a practical source of power, due to the energy cost of accelerating reaction particles.\n\nAfter the very public demonstration of huge energies released from nuclear fission after the atomic bombings of Hiroshima and Nagasaki in 1945, the equation became directly linked in the public eye with the power and peril of nuclear weapons. The equation was featured as early as page 2 of the Smyth Report, the official 1945 release by the US government on the development of the atomic bomb, and by 1946 the equation was linked closely enough with Einstein's work that the cover of \"Time\" magazine prominently featured a picture of Einstein next to an image of a mushroom cloud emblazoned with the equation. Einstein himself had only a minor role in the Manhattan Project: he had cosigned a letter to the U.S. President in 1939 urging funding for research into atomic energy, warning that an atomic bomb was theoretically possible. The letter persuaded Roosevelt to devote a significant portion of the wartime budget to atomic research. Without a security clearance, Einstein's only scientific contribution was an analysis of an isotope separation method in theoretical terms. It was inconsequential, on account of Einstein not being given sufficient information (for security reasons) to fully work on the problem.\n\nWhile is useful for understanding the amount of energy potentially released in a fission reaction, it was not strictly necessary to develop the weapon, once the fission process was known, and its energy measured at 200 MeV (which was directly possible, using a quantitative Geiger counter, at that time). As the physicist and Manhattan Project participant Robert Serber put it: \"Somehow the popular notion took hold long ago that Einstein's theory of relativity, in particular his famous equation , plays some essential role in the theory of fission. Albert Einstein had a part in alerting the United States government to the possibility of building an atomic bomb, but his theory of relativity is not required in discussing fission. The theory of fission is what physicists call a non-relativistic theory, meaning that relativistic effects are too small to affect the dynamics of the fission process significantly.\" However the association between and nuclear energy has since stuck, and because of this association, and its simple expression of the ideas of Albert Einstein himself, it has become \"the world's most famous equation\".\n\nWhile Serber's view of the strict lack of need to use mass–energy equivalence in designing the atomic bomb is correct, it does not take into account the pivotal role this relationship played in making the fundamental leap to the initial hypothesis that large atoms were energetically \"allowed\" to split into approximately equal parts (before this energy was in fact measured). In late 1938, Lise Meitner and Otto Robert Frisch—while on a winter walk during which they solved the meaning of Hahn's experimental results and introduced the idea that would be called atomic fission—directly used Einstein's equation to help them understand the quantitative energetics of the reaction that overcame the \"surface tension-like\" forces that hold the nucleus together, and allowed the fission fragments to separate to a configuration from which their charges could force them into an energetic \"fission\". To do this, they used \"packing fraction\", or nuclear binding energy values for elements, which Meitner had memorized. These, together with use of allowed them to realize on the spot that the basic fission process was energetically possible:\n\n"}
{"id": "1478210", "url": "https://en.wikipedia.org/wiki?curid=1478210", "title": "NEODyS", "text": "NEODyS\n\nNEODyS (Near Earth Objects Dynamic Site) is an Italian and Spanish service that provides information on Near Earth Objects with a Web-based interface. It is based on a continually and (almost) automatically maintained database of near earth asteroid orbits. This site provides a number of services to the NEO community. The main service is an impact monitoring system (CLOMON2) of all near-Earth asteroids covering a period until the year 2100.\n\n\nNEODyS is based on a Postgresql database running on a Linux system.\n\nThe database of orbits is continually and automatically maintained with the most recent Minor Planet Center observations. The orbits are computed with the OrbFit software package provided by the OrbFit Consortium. All of the computational services provided by this site can also be done with this software package.\n\nNEODyS is continually expanding and improving. The following are the next tasks of the project:\n\nOn February 5, 2009 NEODyS went through a major update of the system. A new web-interface is available running the free software PHP. This allows a more flexible and easy to update system. In the same period the system switched to the new version of the Fortran 95 code OrbFit 4.0.\n\nThe NEODyS team is composed by Andrea Milani (Dep. of mathematics, University of Pisa, Italy), María Eugenia San Saturio Lapeña (E.T.S. de Ingenieros Industriales, University of Valladolid, Spain), Oscar Arratia (University of Valladolid), Fabrizio Bernardi (IASF-INAF, Rome, Italy),\nGiovanni F. Gronchi(University of Pisa, Italy), Giovanni B. Valsecchi (IASF-INAF, Rome, Italy) and Giacomo Tommei (University of Pisa, Italy).\n\n\n"}
{"id": "3982783", "url": "https://en.wikipedia.org/wiki?curid=3982783", "title": "Oceans Act of 2000", "text": "Oceans Act of 2000\n\nThe Oceans Act of 2000 established the United States Commission on Ocean Policy, a working group tasked with the development of what would be known as the \"National Oceans Report.\"\n\nThe objective of the report is to promote the following:\n\n\nResponses from the executive branch to the commission's report are listed in a National Ocean Policy, sent to the legislative branch.\n\nThe act was passed by the United States Congress on July 25, 2000 and signed by the President a fortnight later.\n\n\nChair: supervises commission staff and regulates funding.\n\nMembers must be \"balanced by area of expertise and balanced geographically\".\n\nTo be eligible, members must be \"Representatives, knowledgeable in ocean and coastal activities, from state and local governments, ocean-related industries, academic and technical institutions, and public interest organizations involved with scientific, regulatory, economic, and environmental ocean and coastal activities.\" (http://www.oceancommission.gov/documents/oceanact.html)\n\nThe Commission's report is required to include the following, as relevant to U.S. ocean and coastal activities:\n\nThe Commission is to give equal consideration to environmental, technical feasibility, economic, and scientific factors. In addition, the recommendations may not be specific to the lands or waters within a single state.\n\n\nThe Commission is required to hold public meetings. The Commission must hold at least one meeting in each of 6 specified areas around the country. Meetings must be advertised in the U.S. Federal Register.\n\nThe bill has been referred to the following committees:\n\n\nS.Amdt. 3620 by U.S. Sen. Hollings [D-SC]\n\nThe Act provides for $8.5 million for the Commission.\n\nIn 1999, $3.5 million was appropriated for the same effort, but never used. Therefore, only $2.5 million would need to be accumulated to completely cover the cost of this act.\n\n\nA biennial report must be submitted by the U.S. President to Congress of all federal programs incorporated with coastal and ocean activities. This was set to begin in September 2001.\n\nThe report must include:\n\n"}
{"id": "981125", "url": "https://en.wikipedia.org/wiki?curid=981125", "title": "Operation Highjump", "text": "Operation Highjump\n\nOperation Highjump, officially titled The United States Navy Antarctic Developments Program, 1946–1947, was a United States Navy operation organized by Rear Admiral Richard E. Byrd, Jr., USN (Ret), Officer in Charge, Task Force 68, and led by Rear Admiral Richard H. Cruzen, USN, Commanding Officer, Task Force 68. Operation Highjump commenced 26 August 1946 and ended in late February 1947. Task Force 68 included 4,700 men, 13 ships, and 33 aircraft. Operation Highjump's primary mission was to establish the Antarctic research base Little America IV.\n\nHighjump’s objectives, according to the U.S. Navy report of the operation, were:\n\nThe Western Group of ships reached the Marquesas Islands on December 12, 1946, whereupon the \"Henderson\" and \"Cacapon\" set up weather monitoring stations. By December 24, the \"Currituck\" had begun launching aircraft on reconnaissance missions.\n\nThe Eastern Group of ships reached Peter I Island in late December 1946.\n\nOn January 1, 1947, Lieutenant Commander Thompson and Chief Petty Officer Dixon utilized \"Jack Browne\" masks and DESCO Oxygen rebreathers to log the first dive by Americans under the Antarctic. Paul Allman Siple, Ph.D. was the senior U.S. War Department representative on the expedition. Dr. Siple was the same Eagle Scout who accompanied Admiral Byrd on the previous Byrd Antarctic expeditions.\n\nThe Central Group of ships reached the Bay of Whales on January 15, 1947, where they began construction of Little America IV.\n\nNaval ships and personnel were withdrawn back to the United States in late February 1947, and the expedition was terminated due to the early approach of winter and worsening weather conditions.\n\nAdmiral Byrd discussed the lessons learned from the operation in an interview with Lee van Atta of International News Service held aboard the expedition's command ship the USS \"Mount Olympus\". The interview appeared in the Wednesday, March 5, 1947 edition of the Chilean newspaper \"El Mercurio\" and read in part as follows:\nAdmiral Richard E. Byrd warned today that the United States should adopt measures of protection against the possibility of an invasion of the country by hostile planes coming from the polar regions. The admiral explained that he was not trying to scare anyone, but the cruel reality is that in case of a new war, the United States could be attacked by planes flying over one or both poles. This statement was made as part of a recapitulation of his own polar experience, in an exclusive interview with International News Service. Talking about the recently completed expedition, Byrd said that the most important result of his observations and discoveries is the potential effect that they have in relation to the security of the United States. The fantastic speed with which the world is shrinking – recalled the admiral – is one of the most important lessons learned during his recent Antarctic exploration. I have to warn my compatriots that the time has ended when we were able to take refuge in our isolation and rely on the certainty that the distances, the oceans, and the poles were a guarantee of safety.\n\nAfter the operation ended, a follow-up Operation Windmill returned to the area in order to provide ground-truthing to the aerial photography of Highjump from 1947-1948. Finn Ronne also financed a private operation to the same territory until 1948.\n\nAs with other U.S. Antarctic expeditions, interested persons were allowed to send letters with enclosed envelopes to the base, where commemorative cachets were added to their enclosures, which were then returned to the senders. These souvenir philatelic covers are readily available at low cost. It is estimated that at least 150,000 such envelopes were produced, though their final number may be considerably higher.\n\nOn December 30, 1946, aviation radiomen Wendell K. Hendersin, Fredrick W. Williams, and Ensign Maxwell A. Lopez were killed when their Martin PBM Mariner \"George 1\" crashed during a blizzard. The surviving six crew members were rescued 13 days later, including aviation radioman James H. Robbins and co-pilot William Kearns. A plaque honoring the three killed crewmen was later erected at the McMurdo Station research base, and Mount Lopez on Thurston Island was named in honor of killed airman Maxwell A. Lopez.\n\nIn December 2004, an attempt was made to locate the remains of the plane. There are ongoing efforts to repatriate the bodies of the three men killed in the crash.\n\nOn January 21, 1947, Vance N. Woodall died during a \"ship unloading accident\". In a crew profile, deckman Edward Beardsley described his worst memory as \"when Seaman Vance Woodall died on the Ross Ice Shelf under a piece of roller equipment designed to 'pave' the ice to build an airstrip.\"\n\nRear Admiral Richard H. Cruzen, USN, Commanding\n\nCapt. George J. Dufek, USN, Commanding\n\nCapt. Charles A. Bond, USN, Commanding\n\nRear Admiral Richard H. Cruzen, USN, Commanding Officer\n\nRear Adm. Richard E. Byrd, Jr. USN, (Ret), Officer in Charge\n\nCapt. Clifford M. Campbell, USN, Commanding\nThe documentary about the expedition \"The Secret Land\" was filmed entirely by military photographers (both USN and US Army) and narrated by actors Robert Taylor, Robert Montgomery, and Van Heflin. It features Chief of Naval Operations Fleet Admiral Chester W. Nimitz in a scene where he is discussing Operation Highjump with admirals Byrd and Cruzen. The film has re-enacted scenes of critical events, such as shipboard damage control and Admiral Byrd throwing items out of an airplane to lighten it to avoid crashing into a mountain. It won the 1948 Academy Award for Best Documentary Feature.\n\n. Christopher Brand has a film in development of Operation Highjump entitled \"Highjump\".\n\nIn March 2018 Wargaming announced a new operation for their game World of Warships called \"Highjump\", a science fiction setting based on Operation Highjump.\n\n\n"}
{"id": "11738757", "url": "https://en.wikipedia.org/wiki?curid=11738757", "title": "Peak demand", "text": "Peak demand\n\nPeak demand is an historically high point in the sales record of a particular product. In terms of energy use, peak demand describes a period of simultaneous, strong consumer demand.\n\nThe peak demand of an installation or a system is simply the highest demand that has occurred over a specified time period (Gönen 2008). Peak demand is typically characterized as annual, daily or seasonal and has the unit of power.\nPeak demand, peak load or on-peak are terms used in energy demand management describing a period in which electrical power is expected to be provided for a sustained period at a significantly higher than average supply level. Peak demand fluctuations may occur on daily, monthly, seasonal and yearly cycles. For an electric utility company, the actual point of peak demand is a single half-hour or hourly period which represents the highest point of customer consumption of electricity. At this time there is a combination of office, domestic demand and at some times of the year, the fall of darkness.\n\nSome utilities will charge customers based on their individual peak demand. The highest demand during each month or even a single 15 to 30 minute period of highest use in the previous year may be used to calculate charges.\n\nElectricity network is built to deal with the highest possible peak demand otherwise blackout may happen. In Australia, demand tariff has three components:peak demand charge, energy charge and daily connection charge. For example for large customers (commercial,industrial or mixed of commercial/residential), the peak demand charge is based on the highest 30 minutes electricity consumption in a month; the energy charge is based on a month electricity consumption. This type of demand tariff is gradually introduced to residential households and will be rolled out by 2020 in Queensland Australia. How to manage electricity bills under demand tariff can be challenging. The key solutions involve improving building efficiency and managing the operational settings of large power appliances .\n\nIt depends on the demography, the economy, the weather, the climate, the season, the day of the week and other factors. For example, in industrialised regions of China or Germany, the peak demands mostly occur in day time, while solar photovoltaic system can help reduce it. However, in more service based economy such as Australia, the daily peak demands often occur in the late afternoon to early evening time (e.g. 4pm to 8pm). Residential and commercial electricity demand contributes a lot to this type of network peak demand . \n\nPeak demand is considered to be the opposite to off-peak hours when power demand is usually low. There are off-peak time-of-use rates. Sometimes, there are 3 time-of-use zones: peak, shoulder and offpeak. Shoulder is often the time between peak and offpeak in weekdays. Weekends are often just peak and offpeak in terms of managing electricity loads for the network. \n\nThe maximum demand dictates the size of generators, transmission lines, transformers and circuit breakers for utilities even if that amount lasts just one hour per year. Natural gas fueled power generators must all have adequately sized pipelines. Power generation which is able to be rapidly ramped up for peak demand often uses more expensive fuels, is less efficient and has higher marginal carbon emissions.\n\nPeak demand may exceed the maximum supply levels that the electrical power industry can generate, resulting in power outages and load shedding. This often occurs during heat waves when use of air conditioners and powered fans raises the rate of energy consumption significantly. During a shortage authorities may request the public to curtail their energy use and shift it to a non-peak period.\n\nPower stations providing power to electrical grids for peak demand are called peaking power plants or 'peakers'. Natural gas fueled power stations can be fired up rapidly and are therefore often utilized at peak demand times. Pumped storage type dams such as Carters Dam in the U.S. state of Georgia help to meet peak demand as well.\n\nThe chances that a wind farm will be unable to meet peak demand are greater than for a fossil-fueled power station, due to the ability to store liquid fuels for use during peak demand.\n"}
{"id": "5478961", "url": "https://en.wikipedia.org/wiki?curid=5478961", "title": "Peter Tertzakian", "text": "Peter Tertzakian\n\nPeter Tertzakian is an economist and author. He is the Executive Director of the Arc Energy Research Institute, as well as the Chief Energy Economist & Managing Director at ARC Financial Corporation, an energy-focused private equity firm. His two books, \"A Thousand Barrels a Second\" and \"The End of Energy Obesity\", examine the transformation of the global energy sector through economic, environmental and geopolitical pressures.\n\nRaised in Edmonton, Alberta, Peter Tertzakian is now a long-time resident of Calgary, Alberta. He lives there with his wife, Janet, and they have two children, Alexander and André. He enjoys photography. The asteroid 100077 Tertzakian has been named in his honour.\n\nPeter Tertzakian has an undergraduate degree in Geophysics from the University of Alberta, and a graduate degree in Econometrics from the University of Southampton, U.K. He also holds a Master of Science in Management of Technology from the MIT Sloan School of Management. Tertzakian is an Adjunct Professor with the Haskayne School of Business at the University of Calgary.\n\nPeter Tertzakian began his career as a geophysicist with the Chevron Corporation in 1982 where he spent eight years working in field operations, seismic data processing and geophysical software development. He moved from the oil & gas to the financial sector in 1990 and has since been analyzing technology and energy-related businesses.\n\nTertzakian joined ARC Financial Corporation in 2002 after 20 years in the energy and finance industry. He is a member of the Executive, Investment and Strategy committees. In addition to directing ARC Financial's economic research at the Arc Energy Research Institute, he oversees the publication of the ARC Energy Charts, a weekly journal of energy trends. He was previously a Board Chairman of the Honens International Piano Competition.\n\n\n"}
{"id": "3874557", "url": "https://en.wikipedia.org/wiki?curid=3874557", "title": "Pygarg", "text": "Pygarg\n\nThe pygarg () is an animal mentioned in the Bible in as one of the animals permitted for food. The Septuagint translates the Hebrew ' () as ' in Koiné Greek (\"white-rumped\", from ' \"buttocks\" and ' \"white\"), and the King James Version takes from there its term \"pygarg\".\n\nHenry Baker Tristram (1867) proposed that the pygarg was the Saharan antelope addax and described it as \"a large animal, over high at the shoulder, and, with its gently-twisted horns, feet long. Its colour is pure white, with the exception of a short black mane, and a tinge of tawny on the shoulders and back\".\n\nOutside the biblical use, the term was also applied to the Siberian roe deer in the 18th century, whose specific name is \"\" in scientific Latin.\n"}
{"id": "8518813", "url": "https://en.wikipedia.org/wiki?curid=8518813", "title": "Pyrene (mythology)", "text": "Pyrene (mythology)\n\nIn Greek mythology, Pyrene (Ancient Greek: Πυρήνη) may refer to:\n"}
{"id": "29476668", "url": "https://en.wikipedia.org/wiki?curid=29476668", "title": "Soil bioengineering", "text": "Soil bioengineering\n\nSoil and Water Bioengineering is a discipline of civil engineering. It pursues technological, ecological, economic as well as design goals and seeks to achieve these primarily by making use of living materials, i.e. seeds, plants, part of plants and plant communities, and employing them in near–natural constructions while exploiting the manifold abilities inherent in plants.\nSoil bioengineering may sometimes be a substitute for classical engineering works; however, in most cases it is a meaningful and necessary method of complementing the latter.\nIts application suggests itself in all fields of soil and hydraulic engineering, especially for slope and embankment stabilization and erosion control.\n\nSoil bioengineering is the use of living plant materials to provide some engineering function. Soil bioengineering is an effective tool for treatment of a variety of unstable and / or eroding sites. Soil bioengineering techniques have been used for many centuries. More recently Schiechtl (1980) has encouraged the use of soil bioengineering with a variety of European examples. Soil bioengineering is now widely practiced throughout the world for the treatment of erosion and unstable slopes.\n\nSoil Bioengineering methods can be applied wherever the plants which are used as living building materials are able to grow well and develop.\nThis is the case in tropical, subtropical and temperate zones whereas there are obvious limits in dry and cold regions, i.e. where arid, semi–arid and frost zones prevail. In exceptional cases, lack of water may be compensated for by watering or irrigation.\nIn Europe, dry conditions limiting application exist in the Mediterranean as well as in some inner alpine and eastern European snowy regions. However, limits are most frequently imposed in alpine and arctic regions. These can usually be clearly noticed by the limited growth of woody plants (forest, tree and shrub lines) and the upper limits of closed turf cover. The more impoverished a region is in species, the less suited it is for the application of bioengineering methods.\n\n\nApart from these, ecological functions are gaining in importance, particularly as these can be fulfilled to a very limited extent only by classical engineering constructions.\n\n\n\nBioengineering control works are not always necessarily cheaper in construction when compared to classical engineering structures. However, when taking into account their lifetime including their service and maintenance, they will normally turn out to be more economical. Their special advantages are:\n\nThe result of soil bioengineering protection works are living systems which develop further and maintain their balance by natural succession (i.e. by dynamic self–control, without artificial input of energy). If the right living but also non–living building materials and the appropriate types of construction are chosen, exceptionally high sustainability requiring little maintenance effort can be achieved. \n"}
{"id": "1846298", "url": "https://en.wikipedia.org/wiki?curid=1846298", "title": "Supramolecular assembly", "text": "Supramolecular assembly\n\nA supramolecular assembly or \"supermolecule\" is a well defined complex of molecules held together by noncovalent bonds. While a supramolecular assembly can be simply composed of two molecules (e.g., a DNA double helix or an inclusion compound), it is more often used to denote larger complexes of molecules that form sphere-, rod-, or sheet-like species. Micelles, liposomes and biological membranes are examples of supramolecular assemblies. The dimensions of supramolecular assemblies can range from nanometers to micrometers. Thus they allow access to nanoscale objects using a bottom-up approach in far fewer steps than a single molecule of similar dimensions.\n\nThe process by which a supramolecular assembly forms is called molecular self-assembly. Some try to distinguish self-assembly as the process by which individual molecules form the defined aggregate. Self-organization, then, is the process by which those aggregates create higher-order structures. This can become useful when talking about liquid crystals and block copolymers.\n\nAs studied in coordination chemistry, metal ions (usually transition metal ions) exist in solution bound to ligands, In many cases, the coordination sphere defines geometries conducive to reactions either between ligands or involving ligands and other external reagents.\n\nA well known metal-ion-templating was described by Charles Pederson in his synthesis of various crown ethers using metal cations as template. For example, 18-crown-6 strongly coordinates potassium ion thus can be prepared through the Williamson ether synthesis using potassium ion as the template metal.\n\nMetal ions are frequently used for assembly of large supramolecular structures. Metal organic frameworks (MOFs) are one example. MOFs are infinite structures where metal serve as nodes to connect organic ligands together. SCCs are discrete systems where selected metals and ligands undergo self-assembly to form finite supramolecular complexes, usually the size and structure of the complex formed can be determined by the angularity of chosen metal-ligand bonds.\n\nHydrogen bond-assisted supramolecular assembly is the process of assembling small organic molecules to form large supramolecular structures by non-covalent hydrogen bonding interactions. The directionality, reversibility, and strong bonding nature of hydrogen bond make it an attractive and useful approach in supramolecular assembly. Functional groups such as carboxylic acids, ureas, amines, and amides are commonly used to assemble higher order structures upon hydrogen bonding.\n\nHydrogen bond play an essential role in the assembly of secondary and tertiary structures of large biomolecules. DNA double helix is formed by hydrogen bonding between nucleobases: adenine and thymine forms two hydrogen bonds, while guanine and cytosine forms three hydrogen bonds (Figure \"Hydrogen bonds in (a) DNA duplex formation\"). Another prominent example of hydrogen bond-assisted assembly in nature is the formation of protein secondary structures. Both the α-helix and β-sheet are formed through hydrogen bonding between the amide hydrogen and the amide carbonyl oxygen (Figure \"Hydrogen bonds in (b) protein β-sheet structure\").\n\nIn supramolecular chemistry, hydrogen bonds have been broadly applied to crystal engineering, molecular recognition, and catalysis. Hydrogen bonds are among the mostly used synthons in bottom-up approach to engineering molecular interactions in crystals. Representative hydrogen bond patterns for supramolecular assembly is shown in Figure \"Representative hydrogen bond patterns in supramolecular assembly\". A 1: 1 mixture of cyanuric acid and melamine forms crystal with a highly dense hydrogen-bonding network. This supramolecular aggregates has been used as templates to engineering other crystal structures.\n\nSupramolecular assemblies have no specific applications but are the subject of many intriguing reactions. A supramolecular assembly of peptide amphiphiles in the form of nanofibers has been shown to promote the growth of neurons. An advantage to this supramolecular approach is that the nanofibers will degrade back into the individual peptide molecules that can be broken down by the body. By self-assembling of dendritic dipeptides, hollow cylinders can be produced. The cylindrical assemblies possess internal helical order and self-organize into columnar liquid crystalline lattices. When inserted into vesicular membranes, the porous cylindrical assemblies mediate transport of protons across the membrane. Self-assembly of dendrons generates arrays of nanowires. Electron donor-acceptor complexes comprise the core of the cylindrical supramolecular assemblies, which further self-organize into two-dimensional columnar liquid crystaline lattices. Each cylindrical supramolecular assembly functions as an individual wire. High charge carrier mobilities for holes and electrons were obtained.\n\n"}
{"id": "50747142", "url": "https://en.wikipedia.org/wiki?curid=50747142", "title": "Sustainability Network", "text": "Sustainability Network\n\nThe Sustainability Network () (REDE) is a Brazilian political party founded in 2013 by Marina Silva, Brazilian politician from Acre.\n\nThe party formed a strategic alliance with the Brazilian Socialist Party for the 2014 Brazilian general election, until its registration as an independent political party was approved in 2015.\n\nThe Sustainability Network has 19,090 members as of January 2017.\n\nFor the Brazilian general election of 2018 REDE has formed with the Green Party the coalition \"United to transform Brazil\", in support of Marina Silva.\n"}
{"id": "5470991", "url": "https://en.wikipedia.org/wiki?curid=5470991", "title": "Walkabout (novel)", "text": "Walkabout (novel)\n\nWalkabout is a novel written by James Vance Marshall, first published in 1959 as \"The Children\". It is about two children who get lost in the Australian Outback and are helped by an Aborigine on his walkabout. A film based on the book, with the same title came out in 1971, but deviated from the original plot.\n\nThe book opens with two American siblings, Peter and Mary, in a gully in the Australian outback. They are lost as a result of a plane crash. Peter says they should seek out their uncle, who lives in Adelaide; Mary agrees and they begin walking across the desert, but they don't know that it is across the other side.\n\nThe next day they keep walking and searching for food but their efforts are in vain. Suddenly, an Aborigine appears and startles them, mostly due to his nudity. Hoping to make him leave, Mary glares at him. This proves ineffective. Hoping to find out about the strangers, he inspects both of them but finds nothing of interest, so he leaves.\n\nPeter and Mary, shocked at losing their only hope for survival, follow him. Peter attempts to communicate with him through gestures of eating and drinking and the Aborigine comprehends their situation. He indicates that they should follow him, which they do. He arrives at a waterhole where the children drink their fill. Then, the Aborigine prepares food for the hungry children. After this, he begins to lead the children to the next waterhole.\n\nThe problems of the lack of communication shows clearly in this book. The Aborigine misreads Mary's look of disgust at his nakedness and thinks she has seen the spirit of death and is about to die soon and falls into a mental euthanasia.\n\nWhen the trio arrived at the next waterhole, the symptoms of the flu Peter has unwittingly passed on to him start to show in the Aborigine. He begins to worry and decides he must tell the children he needs a burial platform to keep bad spirits from his body and to keep the snakes from \"molesting his body\" after his death. Peter is gathering firewood so to avoid interrupting a man at work, the Aborigine seeks Mary who is bathing. The Aborigine doesn't see a bath as something private; he arrives at the pool and Mary is terrified; she begins to threaten the Aborigine with snarls and a rock. He is confused and becomes depressed, believing that he will not have his burial platform.\n\nMary goes to Peter and tells him to leave with her but Peter is concerned about the Aborigine so Mary is forced to stay. Peter tells her that the Aborigine is very sick; he realizes that the Aborigine could die while Mary refuses to believe that the flu could be fatal, not understanding the native boy's fear of the Spirit of Death he believes she saw in him. Soon, Mary goes to investigate. Finally, she acknowledges that he is actually dying and forgives him. She lays his head in her lap and he touches her hair. Mary realizes that they are not so different, despite his appearance and language. He dies later in the night. They bury him and leave for the food and water-filled valley Peter was told about by the Aborigine before he died.\n\nThey stop at a pool where they eat some yabbies, observe platypus and leave. In a valley rich in water, food, and wildlife, they survive for many days with the skills learned from the Aborigine. They also discover some wet clay which they use to draw pictures: Peter draws nature while Mary draws stylish women and her dream house. Eventually, the children see smoke and see Aboriginal swimmers. One of the swimmers, a man, sees the drawings. His son owns a \"warrigal\", or pet dog, which serves as a link between the boy and Peter. The father sees Mary's dream house and realizes Mary and Peter seek civilization. In a wide variety of gestures and drawings, he tells the children that there is a house like that across the hills and demonstrates how to reach there. The overjoyed children begin their trek back to civilization.\n\nIt is by far Vance's most popular work, due in large part to the success of the related film. Reviewers have praised \"Walkabout\" for its detailed and accurate descriptions of the Australian environment.\n\n"}
{"id": "68073", "url": "https://en.wikipedia.org/wiki?curid=68073", "title": "Welwitschia", "text": "Welwitschia\n\nWelwitschia is a monotypic gymnosperm genus, comprising solely the distinctive \"Welwitschia mirabilis\". The plant is commonly known simply as welwitschia in English, but the name tree tumbo is also used. It is called \"kharos\" or \"khurub\" in Nama, \"tweeblaarkanniedood\" in Afrikaans, \"nyanka\" in Damara, and \"onyanga\" in Herero. \"Welwitschia\" is the only living genus of the family Welwitschiaceae and order Welwitschiales, in the division Gnetophyta. Informal sources commonly refer to the plant as a \"living fossil\". \"Welwitschia mirabilis\" is endemic to the Namib desert within Namibia and Angola.\n\n\"Welwitschia\" is named after the Austrian botanist and doctor Friedrich Welwitsch, who was the first European to describe the plant, in 1859 in present-day Angola. Welwitsch was so overwhelmed by the plant that he, \"could do nothing but kneel down and gaze at it, half in fear lest a touch should prove it a figment of the imagination.\" Joseph Dalton Hooker of the Linnean Society of London, using Welwitsch's description and collected material along with material from the artist Thomas Baines who had independently recorded the plant in Namibia, described the species.\n\nWelwitsch proposed calling the genus \"Tumboa\" after what he believed to be the local name, \"tumbo\". Hooker asked Welwitsch for permission to name the genus \"Welwitschia\" instead. Welwitsch concurred and supplied some well-preserved material from which Hooker was able to make substantial progress in determining its botanical affinities. The taxonomy of \"Welwitschia\" subsequently changed intermittently with the development of new classification systems (see Flowering plants: History of classification), however, its current taxonomic status is essentially the same as Hooker's placement.\n\nMost botanists have treated \"Welwitschia\" as a distinct monotypic genus in a monotypic family or even order. Most recent systems place \"Welwitschia mirabilis\" in its own family Welwitschiaceae in the gymnosperm order Gnetales.\n\nAfter germination, the seedling produces two cotyledons which grow to in length, and have reticulate venation. Subsequently, two foliage leaves are produced at the edge of a woody bilobed crown. The permanent leaves are opposite (at right angles to the cotyledons), amphistomatic (producing stomata on both sides of the leaf), parallel-veined and ribbon-shaped. Shortly after the appearance of the foliage leaves, the apical meristem dies and meristematic activity is transferred to the periphery of the crown.\n\nThe two foliage leaves grow continuously from a basal meristem reaching lengths up to . The tips of the leaves split and fray into several well-separated strap-shaped sections by the distortions of the woody portions surrounding the apical slit, and also by wind and adventitious external injuries. The largest specimens may be no more than tall above ground, but the circumference of the leaves in contact with the sand may exceed .\n\n\"Welwitschia\" has an elongated shallow root system consisting of \"a tapering taproot with one or more non-tapering extensions, some pronounced lateral roots, and a network of delicate spongy roots\" and a woody fibrous unbranched main stem. The roots extend to a depth roughly equal to the span of the living leaves from tip to tip. The main stem consists of an unbranched woody crown roughly shaped like an inverted cone. The only branching in the shoot system occurs in the reproductive branches, which bear strobili.\n\nThe species is dioecious, with separate male and female plants. Fertilization is carried out by insects including flies and true bugs. The commonest of the true bugs attending \"Welwitschia\" is a member of the family Pyrrhocoridae, \"Probergrothius angolensis\", but a hypothesized role in pollination has so far not been demonstrated. Infrequently, wasps and bees also play a role as pollinators of \"Welwitschia\". At least some of the pollinators are attracted by \"nectar\" produced on both male and female strobili.\n\n\"Welwitschia\" has been classified as a CAM plant (crassulacean acid metabolism) after reconciliation of some initially contradictory and confusing data. There are however some very puzzling aspects to the matter; for example, the employment of the CAM metabolism is very slight, which was part of the reason that it took so long to establish its presence at all; it is not understood why this should be.\n\nThe age of individual plants is difficult to assess, but many plants may be over 1000 years old. Some individuals may be more than 2000 years old. Because \"Welwitschia\" only produces a single pair of foliage leaves, the plant was thought by some to be neotenic, consisting essentially of a \"giant seedling.\" However, research showed that its anatomy is not consistent with the giant seedling idea. Instead, the plant is more accurately thought of as achieving its unusual morphology as a result of having \"lost its head\" (apical meristem) at an early stage.\n\n\"Welwitschia mirabilis\" is endemic to the Kaokoveld Centre. The population is distributed southwards from the Bentiaba River in southern Angola, to the Kuiseb River in Namibia and up to inland of the coast. The area is extremely arid: the coast is recorded as having almost zero rainfall, while less than of rain falls annually below the escarpment in the wet season from February to April. Populations tend to occur in ephemeral water courses, indicating a dependence on ground water in addition to precipitation from fog.\n\n\"Welwitschia mirabilis\" grows readily from seed, which may be bought from specialty seed dealers. The seeds have been shown to display orthodox seed behavior, which in general means that they may be stored for long periods at suitably low humidity and temperature. \"Welwitschia\" seeds naturally develop suitably low water concentrations as they ripen.\n\nRemoval of the outer seed coverings enhances germination performance, which suggests that the seeds may display non-deep physiological dormancy. On planting the seed it is necessary to keep it moist, but not immersed in water, for the first two weeks of cultivation; it has been suggested that soaking the seeds in water before planting interferes with germination. Seeds collected from the wild often are heavily contaminated with spores of the fungus \"Aspergillus niger var. phoenicis\", which causes them to rot shortly after they germinate.\n\nThe fungal inoculum infects the growing cones of \"W. mirabilis\" early during their development, and a sharp increase in infection occurs when the pollination drops appear; through those drops the fungal spores may gain access to the interior of the developing seed. Seeds in the wild may therefore be obliterated through fungal action even before they are fully developed. Seeds from botanical gardens or other cultivated sources are much cleaner and less likely to rot. The fungicide tebuconazole may be useful in controlling limited \"A. niger\" seed infection.\n\nThe population of \"Welwitschia mirabilis\" in the wild is reasonably satisfactory at present. Plants in Angola are better protected than those in Namibia, because of the relatively high concentration of land mines in Angola, which keep collectors away.\n\nAlthough \"Welwitschia mirabilis\" is not at present immediately threatened, there being abundant populations over a large area, its status is far from secure; its recruitment and growth rates are low, and its range, though wide, covers only a single compact, ecologically limited and vulnerable area. The remarkable longevity of Welwitschia favours its survival of temporary periods adverse to reproduction, but it offers no protection against circumstances of direct threat, such as overgrazing and disease. Fungal infection of female cones severely reduces seed viability, reducing already inherently low recruitment. Other threats include injury from off-road vehicles, collection of wild plants and overgrazing by zebras, rhinos, and domestic animals.\n\nThe plant figures as a charge in the national coat of arms of Namibia. Claire Waight Keller included it to represent the country in Meghan Markle's wedding veil, which included the distinctive flora of each Commonwealth country.\n\n\n"}
{"id": "18923321", "url": "https://en.wikipedia.org/wiki?curid=18923321", "title": "Xylotomy", "text": "Xylotomy\n\nXylotomy is the preparation of small slivers of wood for examination under a microscope, often using a microtome.\n\nIt is useful for providing forensic evidence in some criminal cases where finding a fragment of wood on an individual and matching it to a weapon used in a crime would be helpful. During the trial of Bruno Richard Hauptmann, accused in the Lindbergh kidnapping during the 1930s, the xylotomist Arthur Koehler was able to provide crucial evidence by linking a piece of pine from a ladder used in a kidnapping to one particular factory whose machinery was defective, and from there to one particular lumberyard. Koehler searched for eighteen months for the yard, and presented his evidence dramatically in court by presenting the court with a chalk rubbing of the samples, which he made there and then, demonstrating that they were identical. He also matched other pieces of the ladder to a chisel used to create the joists, missing from the defendant's tool chest, and to a missing plank from the suspect's attic floor.\n\nIt may also be useful in forestry studies. Identifying the species of a piece of wood is not always easy, in which case a xylotheque may provide samples with which the xylotomist can compare his own. Xylotomy may be carried out by a forensic biologist. People who specialize in xylotomy are rare. The public have been known to confuse it with someone who plays the xylophone and they do not tend to take much interest in the subject.\n\n"}
