{"id": "21897806", "url": "https://en.wikipedia.org/wiki?curid=21897806", "title": ".eco", "text": ".eco\n\n.eco is a top-level domain (TLD). It was proposed in ICANN's new generic top-level domain (gTLD) program and became available to the general public on April 25, 2017.\n\nBig Room is the registry for the domain and names can be registered at go.eco. The company won the rights through a community application after assembling a coalition of more than 50 environmental organizations. The registry is advised by a council of these organizations.\n\n"}
{"id": "3999992", "url": "https://en.wikipedia.org/wiki?curid=3999992", "title": "Accidental release source terms", "text": "Accidental release source terms\n\nAccidental release source terms are the mathematical equations that quantify the flow rate at which accidental releases of liquid or gaseous pollutants into the ambient environment can occur at industrial facilities such as petroleum refineries, petrochemical plants, natural gas processing plants, oil and gas transportation pipelines, chemical plants, and many other industrial activities. Governmental regulations in many countries require that the probability of such accidental releases be analyzed and their quantitative impact upon the environment and human health be determined so that mitigating steps can be planned and implemented. \n\nThere are a number of mathematical calculation methods for determining the flow rate at which gaseous and liquid pollutants might be released from various types of accidents. Such calculational methods are referred to as \"source terms\", and this article on accidental release source terms explains some of the calculation methods used for determining the mass flow rate at which gaseous pollutants may be accidentally released.\n\nWhen gas stored under pressure in a closed vessel is discharged to the atmosphere through a hole or other opening, the gas velocity through that opening may be choked (i.e., it has attained a maximum) or it may be non-choked.\n\nChoked velocity, also referred to as sonic velocity, occurs when the ratio of the absolute source pressure to the absolute downstream pressure is equal to or greater than [(\"k\" + 1) / 2], where \"k\" is the specific heat ratio of the discharged gas (sometimes called the isentropic expansion factor and sometimes denoted as formula_1).\n\nFor many gases, \"k\" ranges from about 1.09 to about 1.41, and therefore [(\"k\" + 1) / 2] ranges from 1.7 to about 1.9, which means that choked velocity usually occurs when the absolute source vessel pressure is at least 1.7 to 1.9 times as high as the absolute downstream ambient atmospheric pressure.\n\nWhen the gas velocity is choked, the equation for the mass flow rate in SI metric units is:\n\nor this equivalent form:\n\nFor the above equations, it is important to note that although the gas velocity reaches a maximum and becomes choked, the mass flow rate is not choked. The mass flow rate can still be increased if the source pressure is increased.\n\nWhenever the ratio of the absolute source pressure to the absolute downstream ambient pressure is less than \n[ (\"k\" + 1) / 2], then the gas velocity is non-choked (i.e., sub-sonic) and the equation for mass flow rate is:\n\nor this equivalent form:\n\nThe above equations calculate the initial instantaneous mass flow rate for the pressure and temperature existing in the source vessel when a release first occurs. The initial instantaneous flow rate from a leak in a pressurized gas system or vessel is much higher than the average flow rate during the overall release period because the pressure and flow rate decrease with time as the system or vessel empties. Calculating the flow rate versus time since the initiation of the leak is much more complicated, but more accurate. Two equivalent methods for performing such calculations are presented and compared at www.air-dispersion.com/feature2.html.\n\nThe technical literature can be very confusing because many authors fail to explain whether they are using the universal gas law constant \"R\" which applies to any ideal gas or whether they are using the gas law constant \"R\" which only applies to a specific individual gas. The relationship between the two constants is \"R\" = \"R\"/\"M\".\n\nNotes:\n\nP.K. Ramskill's equation for the non-choked flow of an ideal gas is shown below as equation (1):\n\nThe gas density, formula_7, in Ramskill's equation is the ideal gas density at the downstream conditions of temperature and pressure and it is defined in equation (2) using the ideal gas law: \n\nSince the downstream temperature T is not known, the isentropic expansion equation below is used to determine T in terms of the known upstream temperature T: \n\nCombining equations (2) and (3) results in equation (4) which defines formula_7 in terms of the known upstream temperature T:\n\nUsing equation (4) with Ramskill's equation (1) to determine non-choked mass flow rates for ideal gases gives identical results to the results obtained using the non-choked flow equation presented in the previous section above.\n\nThree different methods of calculating the rate of evaporation from a non-boiling liquid pool are presented in this section. The results obtained by the three methods are somewhat different.\n\nThe following equations are for predicting the rate at which liquid evaporates from the surface of a pool of liquid which is at or near the ambient temperature. The equations were derived from field tests performed by the U.S. Air Force with pools of liquid hydrazine. \n\nIf \"T\" = 0°C or less, then \"T\" = 1.0\n\nIf \"T\" > 0°C, then \"T\" = 1.0 + 0.0043 \"T\"\n\nThe following equations are for predicting the rate at which liquid evaporates from the surface of a pool of liquid which is at or near the ambient temperature. The equations were developed by the United States Environmental Protection Agency using units which were a mixture of metric usage and United States usage. The non-metric units have been converted to metric units for this presentation.\n\nNB, the constant used here is 0.284 from the mixed unit formula/2.205lb/kg. The 82.05 become 1.0 = (ft/m)² × mmHg/kPa.\n\nThe U.S. EPA also defined the pool depth as 0.01m (i.e., 1cm) so that the surface area of the pool liquid could be calculated as:\n\nNotes:\n\nThe following equations are for predicting the rate at which liquid evaporates from the surface of a pool of liquid which is at or near the ambient temperature. The equations were developed by Warren Stiver and Dennis Mackay of the Chemical Engineering Department at the University of Toronto. \n\nThe following equation is for predicting the rate at which liquid evaporates from the surface of a pool of cold liquid (i.e., at a liquid temperature of about 0°C or less).\n\nLiquefied gases such as ammonia or chlorine are often stored in cylinders or vessels at ambient temperatures and pressures well above atmospheric pressure. When such a liquefied gas is released into the ambient atmosphere, the resultant reduction of pressure causes some of the liquefied gas to vaporize immediately. This is known as \"adiabatic flashing\" and the following equation, derived from a simple heat balance, is used to predict how much of the liquefied gas is vaporized. \n\nIf the enthalpy data required for the above equation is unavailable, then the following equation may be used.\n\n\n"}
{"id": "10958349", "url": "https://en.wikipedia.org/wiki?curid=10958349", "title": "Air-mass thunderstorm", "text": "Air-mass thunderstorm\n\nAn air-mass thunderstorm, also called an \"ordinary\", \"single cell\", or \"garden variety\" thunderstorm, is a thunderstorm that is generally weak and usually not severe. These storms form in environments where at least some amount of Convective Available Potential Energy (CAPE) is present, but very low levels of wind shear and helicity. The lifting source, which is a crucial factor in thunderstorm development, is usually the result of uneven heating of the surface, though they can be induced by weather fronts and other low-level boundaries associated with wind convergence. The energy needed for these storms to form comes in the form of insolation, or solar radiation. Air-mass thunderstorms do not move quickly, last no longer than an hour, and have the threats of lightning, as well as showery light, moderate, or heavy rainfall. Heavy rainfall can interfere with microwave transmissions within the atmosphere. \n\nLightning characteristics are related to characteristics of the parent thunderstorm, and could induce wildfires near thunderstorms with minimal rainfall. On unusual occasions there could be a weak downburst and small hail. They are common in temperate zones during a summer afternoon. Like all thunderstorms, the mean-layered wind field the storms form within determine motion. When the deep-layered wind flow is light, outflow boundary progression will determine storm movement. Since thunderstorms can be a hazard to aviation, pilots are advised to fly above any haze layers within regions of better visibility and to avoid flying under the anvil of these thunderstorms, which can be regions where hail falls from the parent thunderstorm. Vertical wind shear is also a hazard near the base of thunderstorms which have generated outflow boundaries.\n\nThe trigger for the lift of the initial cumulus cloud can be insolation heating the ground producing thermals, areas where two winds converge forcing air upwards, or where winds blow over terrain of increasing elevation. The moisture rapidly cools into liquid drops of water due to the cooler temperatures at high altitude, which appears as \"cumulus\" clouds. As the water vapor condenses into liquid, latent heat is released which warms the air, causing it to become less dense than the surrounding dry air. The air tends to rise in an \"updraft\" through the process of convection (hence the term convective precipitation). This creates a low-pressure zone beneath the forming thunderstorm, otherwise known as a cumulonimbus cloud. In a typical thunderstorm, approximately 5×10 kg of water vapor is lifted into the Earth's atmosphere. As they form in areas of minimal vertical wind shear, the thunderstorm's rainfall creates a moist and relatively cool outflow boundary with undercuts the storm's low level inflow, and quickly causes dissipation. Waterspouts, small hail, and strong wind gusts can occur in association with these thunderstorms.\n\nAlso known as single cell thunderstorms, these are the typical summer thunderstorms in many temperate locales. They also occur in the cool unstable air which often follows the passage of a cold front from the sea during winter. Within a cluster of thunderstorms, the term \"cell\" refers to each separate principal updraft. Thunderstorm cells occasionally form in isolation, as the occurrence of one thunderstorm can develop an outflow boundary which sets up new thunderstorm development. Such storms are rarely severe and are a result of local atmospheric instability; hence the term \"air mass thunderstorm\". When such storms have a brief period of severe weather associated with them, it is known as a pulse severe storm. Pulse severe storms are poorly organized due to the minimal vertical wind shear in the storm's environment and occur randomly in time and space, making them difficult to forecast. Between formation and dissipation, single cell thunderstorms normally last 20–30 minutes.\n\nThe two major ways thunderstorms move are via advection of the wind and propagation along outflow boundaries towards sources of greater heat and moisture. Many thunderstorms move with the mean wind speed through the Earth's troposphere, or the lowest of the Earth's atmosphere. Younger thunderstorms are steered by winds closer to the Earth's surface than more mature thunderstorms as they tend not to be as tall. If the gust front, or leading edge of the outflow boundary, moves ahead of the thunderstorm, the thunderstorm's motion will move in tandem with the gust front. This is more of a factor with thunderstorms with heavy precipitation (HP), such as air-mass thunderstorms. When thunderstorms merge, which is most likely when numerous thunderstorms exist in proximity to each other, the motion of the stronger thunderstorm normally dictates future motion of the merged cell. The stronger the mean wind, the less likely other processes will be involved in storm motion. On weather radar, storms are tracked by using a prominent feature and tracking it from scan to scan.\n\nConvective rain, or showery precipitation, occurs from cumulonimbus clouds. It falls as showers with rapidly changing intensity. Convective precipitation falls over a certain area for a relatively short time, as convective clouds such as thunderstorms have limited horizontal extent. Most precipitation in the tropics appears to be convective. Graupel and hail are good indicators of convective precipitation and thunderstorms. In mid-latitudes, convective precipitation is intermittent and often associated with baroclinic boundaries such as cold fronts, squall lines, and warm fronts. High rainfall rates are associated with thunderstorms with larger raindrops. Heavy rainfall leads to fading of microwave transmissions starting above the frequency of 10 gigahertz (GHz), but is more severe above frequencies of 15 GHz.\n\nRelationships between lightning frequency and the height of precipitation within thunderstorms have been found. Thunderstorms which show radar returns above in height are associated with storms which have more than ten lightning flashes per minute. There is also a correlation between the total lightning rate and the size of the thunderstorm, its updraft velocity, and amount of graupel over land. The same relationships fail over tropical oceans, however. Lightning from low precipitation (LP) thunderstorms is one of the leading causes of wildfires.\n\nIn areas where these thunderstorms form in isolation and horizontal visibility is good, pilots can evade these storms rather easily. In more moist atmospheres which become hazy, pilots navigate above the haze layer in order to get a better vantage point of these storms. Flying under the anvil of thunderstorms is not advised, as hail is more likely to fall in such areas outside the thunderstorm's main rain shaft. When an outflow boundary forms due to a shallow layer of rain-cooled air spreading out near ground level from the parent thunderstorm, both speed and directional wind shear can result at the leading edge of the three-dimensional boundary. The stronger the outflow boundary is, the stronger the resultant vertical wind shear will become.\n\n"}
{"id": "62692", "url": "https://en.wikipedia.org/wiki?curid=62692", "title": "Archean", "text": "Archean\n\nThe Archean Eon (, also spelled Archaean or Archæan) is one of the four geologic eons of Earth history, occurring (4 to 2.5 billion years ago). During the Archean, the Earth's crust had cooled enough to allow the formation of continents and life started to form.\n\nArchean (or Archaean) comes from the ancient Greek (\"\"), meaning \"beginning, origin\". Its earliest use is from 1872, when it meant \"of the earliest geological age.\" Before the Hadean Eon was recognized, the Archean spanned Earth's early history from its formation about 4,540 million years ago until 2,500 million years ago.\n\nInstead of being based on stratigraphy, the beginning and end of the Archean Eon are defined chronometrically. The eon's lower boundary or starting point of 4 Gya (4 billion years ago) is officially recognized by the International Commission on Stratigraphy.\n\nWhen the Archean began, the Earth's heat flow was nearly three times as high as it is today, and it was still twice the current level at the transition from the Archean to the Proterozoic (2,500 million years ago). The extra heat was the result of a mix of remnant heat from planetary accretion, from the formation of the Earth's core, and from the disintegration of radioactive elements.\n\nAlthough a few mineral grains are known to be Hadean, the oldest rock formations exposed on the surface of the Earth are Archean. Archean rocks are found in Greenland, Siberia, the Canadian Shield, Montana and Wyoming (exposed parts of the Wyoming Craton), the Baltic Shield, Scotland, India, Brazil, western Australia, and southern Africa. Granitic rocks predominate throughout the crystalline remnants of the surviving Archean crust. Examples include great melt sheets and voluminous plutonic masses of granite, diorite, layered intrusions, anorthosites and monzonites known as sanukitoids. Archean Eon rocks are often heavily metamorphized deep-water sediments, such as graywackes, mudstones, volcanic sediments, and banded iron formations. Volcanic activity was considerably higher than today, with numerous lava eruptions, including unusual types such as komatiite. Carbonate rocks are rare, indicating that the oceans were more acidic due to dissolved carbon dioxide than during the Proterozoic. Greenstone belts are typical Archean formations, consisting of alternating units of metamorphosed mafic igneous and sedimentary rocks. The metamorphosed igneous rocks were derived from volcanic island arcs, while the metamorphosed sediments represent deep-sea sediments eroded from the neighboring island arcs and deposited in a forearc basin. Greenstone belts, being both types of metamorphosed rock, represent sutures between the protocontinents.\nThe Earth's continents started to form in the Archean, although details about their formation are still being debated, due to lack of extensive geological evidence. One hypothesis is that rocks that are now in India, western Australia, and southern Africa formed a continent called Ur as of 3,100 Ma. A differing conflicting hypothesis is that rocks from western Australia and southern Africa were assembled in a continent called Vaalbara as far back as 3,600 Ma. Although the first continents formed during this eon, rock of this age makes up only 7% of the present world's cratons; even allowing for erosion and destruction of past formations, evidence suggests that only 5–40% of the present area of continents formed during the Archean.\n\nBy the end of the Archaean c. 2500 Ma, plate tectonic activity may have been similar to that of the modern Earth. There are well-preserved sedimentary basins, and evidence of volcanic arcs, intracontinental rifts, continent-continent collisions and widespread globe-spanning orogenic events suggesting the assembly and destruction of one and perhaps several supercontinents. Liquid water was prevalent, and deep oceanic basins are known to have existed attested by the presence of banded iron formations, chert beds, chemical sediments and pillow basalts.\n\nThe Archean atmosphere is thought to have nearly lacked free oxygen. Astronomers think that the Sun had about 70–75 percent of the present luminosity, yet temperatures on Earth appear to have been near modern levels after only 500 Ma of Earth's formation (the faint young Sun paradox). The presence of liquid water is evidenced by certain highly deformed gneisses produced by metamorphism of sedimentary protoliths. The moderate temperatures may reflect the presence of greater amounts of greenhouse gases than later in the Earth's history. Alternatively, Earth's albedo may have been lower at the time, due to less land area and cloud cover.\n\nThe processes that gave rise to life on Earth are not completely understood, but there is substantial evidence that life came into existence either near the end of the Hadean Eon or early in the Archean Eon. \n\nThe earliest evidence for life on Earth are graphite of biogenic origin found in 3.7-billion-year-old metasedimentary rocks discovered in Western Greenland.\n\nThe earliest identifiable fossils consist of stromatolites, which are microbial mats formed in shallow water by cyanobacteria. The earliest stromatolites are found in 3.48 billion-year-old sandstone discovered in Western Australia. Stromatolites are found throughout the Archean and become common late in the Archean. Cyanobacteria were instrumental in creating free oxygen in the atmosphere.\n\nFurther evidence for early life is found in 3.47-billon-year-old baryte, in the Warrawoona Group of Western Australia. This mineral shows sulfur fractionation of as much as 21.1%, which is evidence of sulfate-reducing bacteria that metabolize sulfur-32 more readily than sulfur-34.\n\nEvidence of life in the Late Hadean is more controversial. In 2015, biogenic carbon has been detected in zircons dated to 4.1 billion years ago, but this evidence is preliminary and needs validation. \n\nEarth was very hostile to life before 4.2–4.3 Ga and the conclusion is that before the Archean Eon, life as we know it would have been challenged by these environmental conditions. While life could have arisen before the Archean, the conditions necessary to sustain life could not have occurred until the Archean Eon.\n\nLife in the Archean was limited to simple single-celled organisms (lacking nuclei), called Prokaryota. In addition to the domain Bacteria, microfossils of the domain Archaea have also been identified. There are no known eukaryotic fossils from the earliest Archean, though they might have evolved during the Archean without leaving any. No fossil evidence has been discovered for ultramicroscopic intracellular replicators such as viruses.\n\nFossilized microbes from terrestrial microbial mats show that life was already established on land 3.22 billion years ago.\n\n"}
{"id": "19044806", "url": "https://en.wikipedia.org/wiki?curid=19044806", "title": "Astra 5°E", "text": "Astra 5°E\n\nAstra 5°E is the name for the Astra communications satellites co-located at the 5° east position in the Clarke Belt which are owned and operated by SES based in Betzdorf, Luxembourg. 5° east is one of the major TV satellite positions serving Europe (the others being at 19.2° east, 28.2° east, 13° east, and 23.5° east).\n\nThe Astra satellites at 5° east provide for services downlinking to the Nordic countries, Eastern Europe and sub-Saharan Africa in the 11.70 GHz-12.75 GHz range of the K band, and at present the Astra 4A and the SES-5 are regularly operational at this position.\n\nSatellites at 5°E were originally operated by Swedish Space Corporation (SSC), and then Nordic Satellite AB (NSAB, itself 50% owned by SSC) before SES took full control of the position and the satellites in 2010, renaming the Sirius 4 satellite to Astra 4A and later adding its own Astra 1E to the group followed by the SES-5.\n\n\n\nAstra 5°E is SES' position for direct-to-home (DTH) broadcasting to Sweden, Denmark, Norway and Finland, as well as Eastern European and Baltic countries including Belarus, Bulgaria, Estonia, Latvia, Lithuania, Romania, Russia, and Ukraine, and sub-Saharan Africa.\n\nOver 460 TV, radio and interactive channels (including 9 high definition TV stations serve over 21 million\ndirect-to-home, SMATV and cable homes via the 5°E position. Cable distribution to head-ends, contribution links, data services, and broadband capacity are also provided\n\nAstra 4A provides DTH coverage to multiple African markets with a dedicated K band beam from a single orbital position, which eliminates the need for dual-illumination from separate beams. This position also provides uplink and downlink within the African footprint, and inter-connectivity between Africa and Europe, so that DTH broadcasting out of Europe is available without the need for expensive fibre links.\n\nSES-5 provides two Ku-band beams for DTH services, one for the Nordic and Baltic countries and one serving Sub-Saharan Africa. It also has C-band capacity on a global coverage beam and a hemispheric coverage for Europe, Africa and the Middle East to deliver broadband, maritime communications, GSM backhaul, and VSAT applications. SES-5 also carries a hosted L-band payload for the European Commission’s European Geostationary Navigation Overlay Service (EGNOS).\n\nAs of the end of 2016, the Astra satellites at 5° east broadcast to 51.6 million households \n\nThe Astra 5°E orbital position was originally the Direct broadcast satellite orbital position allocated to Sweden with Swedish Satellite Corporation's Tele-X (launched 1989) the first TV satellite at this position. In 1994, Tele-X was joined by Sirius 1, bought by NSAB from BSkyB in 1993 after Sky Television plc's merger with British Satellite Broadcasting, and moved to 5°E from its original orbital position at 30°West as Marcopolo 1.\n\nSirius 1 was later joined at 5° east by Sirius 2 (1997) and then Sirius 3 (1998), with the most recent addition, Sirius 4, launched in November 2007.\n\nTele-X was retired to a graveyard orbit in 1998. Sirius 1 was moved to 13° west and renamed Sirius W in 2000 and retired in 2003. In 2008 Sirius 2 was moved to 31.5°E and renamed Astra 5A but in January 2009, the spacecraft suffered a failure and was withdrawn from service some four years ahead of its expected end of life. Sirius 3 is in inclined orbit at 51.2° east.\n\nIn 2000, SES bought the 50% shareholding in NSAB owned by Teracom and Tele Danmark and in 2003 increased that holding to 75%, renaming the company SES Sirius AB. In 2008 Astra acquired further shares to take its shareholding in SES Sirius to 90% and in March 2010 took full control of the company. In June 2010, the affiliate company was renamed SES Astra and the Sirius 4 satellite renamed Astra 4A. SES Astra is now a non-autonomous part of SES.\n\nAstra 4A was originally the designation given in 2007 to just part of the Sirius 4 satellite (six transponders of the FSS Africa beam) owned and operated by SES Sirius. From June 2010, the Astra 4A designation has applied to the entire satellite.\n\nIn September 2010, Astra 1E was moved to 5° east to provide further backup for Astra 4A until the launch of SES-5 (Astra 4B). Astra 1E was originally launched to the primary Astra position of 19.2°E but, prior to its move to 5°E, since October 2007 it had been used at 23.5°E to provide additional capacity before the launch of Astra 3B to that position in May 2010.\n\nOn July 9, 2012 SES-5 was successfully launched from Baikonur in Kazakhstan and on September 17, 2012 it started commercial operations at 5°E. SES-5 was originally named Sirius 5, but renamed to Astra 4B in 2010 and then to SES-5 in 2011.\n\nIn July 2015 Astra 2D arrived, inactive, at the Astra 5°E position, moved from Astra 28.2°E where it had served all its active life (2001-2013).\n\n\n"}
{"id": "52507691", "url": "https://en.wikipedia.org/wiki?curid=52507691", "title": "Autogamy", "text": "Autogamy\n\nAutogamy, or self-fertilization, refers to the fusion of two gametes that come from one individual. Autogamy is predominantly observed in the form of self-pollination, a reproductive mechanism employed by many flowering plants. However, species of protists have also been observed using autogamy as a means of reproduction. Flowering plants engage in autogamy regularly, while the protists that engage in autogamy only do so in stressful environments.\n\n\"Paramecium aurelia\" is the most commonly studied protozoan for autogamy. Similar to other unicellular organisms, \"Paramecium aurelia\" typically reproduce asexually via binary fission or sexually via cross-fertilization. However, studies have shown that when put under nutritional stress, \"Paramecium aurelia\" will undergo meiosis and subsequent fusion of gametic-like nuclei. This process, defined as hemixis, a chromosomal rearrangement process, takes place in a number of steps. First, the two micronuclei of Paramecium aurelia enlarge and divide two times to form eight nuclei. Some of these daughter nuclei will continue to divide to create potential future gametic nuclei. Of these potential gametic nuclei, one will divide two more times. Of the four daughter nuclei arising from this step, two of them become anlagen, or cells that will form part of the new organism. The other two daughter nuclei become the gametic micronuclei that will undergo autogamous self-fertilization. These nuclear divisions are observed mainly when the Paramecium aurelia is put under nutritional stress. Research shows that \"Paramecium aurelia\" undergo autogamy synchronously with other individuals of the same species.\n\nIn \"Paramecium tetraurelia\", vitality declines over the course of successive asexual cell divisions by binary fission. Clonal aging is associated with a dramatic increase in DNA damage. When paramecia that have experienced clonal aging undergo meiosis, either during conjugation or automixis, the old macronucleus disintegrates and a new macronucleus is formed by replication of the micronuclear DNA that had just experienced meiosis followed by syngamy. These paramecia are rejuvenated in the sense of having a restored clonal lifespan. Thus it appears that clonal aging is due in large part to the progressive accumulation of DNA damage, and that rejuvenation is due to repair of DNA damage during meiosis that occurs in the micronucleus during conjugation or automixis and reestablishment of the macronucleus by replication of the newly repaired micronuclear DNA.\n\nSimilar to Paramecium aurelia, the parasitic ciliate \"Tetrahymena rostrata\" has also been shown to engage in meiosis, autogamy and development of new macronuclei when placed under nutritional stress. Due to the degeneration and remodeling of genetic information that occurs in autogamy, genetic variability arises and possibly increases an offspring’s chances of survival in stressful environments.\n\n\"Allogromia laticollaris\" is perhaps the best-studied foraminiferan amoeboid for autogamy. \"Allogromia laticollaris\" can alternate between sexual reproduction via cross-fertilization and asexual reproduction via binary fission. The details of the life cycle of Allogromia laticollaris are unknown, but similar to \"Paramecium aurelia\", \"Allogromia laticollaris\" is also shown to sometimes defer to autogamous behavior when placed in nutritional stress. As seen in Paramecium, there is some nuclear dimorphism observed in \"Allogromia laticollaris.\" There are often observations of macronuclei and chromosomal fragments coexisting in \"Allogromia laticollaris.\" This is indicative of nuclear and chromosomal degeneration, a process similar to the subdivisions observed in \"Paramecium aurelia.\" Multiple generations of haploid \"Allogromia laticollaris\" individuals can exist before autogamy actually takes place. The autogamous behavior in Allogromia laticollaris has the added consequence of giving rise to daughter cells that are substantially smaller than those rising from binary fission. It is hypothesized that this is a survival mechanism employed when the cell is in stressful environments, and thus not able to allocate all resources to creating offspring. If a cell was under nutritional stress and not able to function regularly, there would be a strong possibility of its offspring’s fitness being sub-par.\n\nSelf-pollination is an example of autogamy that occurs in flowering plants. Self-pollination occurs when the sperm in the pollen from the stamen of a plant goes to the carpels of that same plant and fertilizes the egg cell present. Self-pollination can either be done completely autogamously or geitonogamously. In the former, the egg and sperm cells that united came from the same flower. In the latter, the sperm and egg cells can come from a different flower on the same plant. While the latter method does blur the lines between autogamous self-fertilization and normal sexual reproduction, it is still considered autogamous self-fertilization.\n\nSelf-pollination can lead to inbreeding depression due to expression of deleterious recessive mutations. Meiosis followed by self-pollination results in little genetic variation, raising the question of how meiosis in self-pollinating plants is adaptively maintained over an extended period in preference to a less complicated and less costly asexual ameiotic process for producing progeny. For instance, \"Arabidopsis thaliana\" is a predominantly self-pollinating plant that has an outcrossing rate in the wild estimated at less than 0.3%, and self-pollination appears to have evolved roughly a million years ago or more. An adaptive benefit of meiosis that may explain its long-term maintenance in self-pollinating plants is efficient recombinational repair of DNA damage.\n\n There are several advantages for the self-fertilization observed in flowering plants and protists. In flowering plants, it is important for some plants not to be dependent on pollinating agents that other plants rely on for fertilization. This is unusual, however, considering that many plant species have evolved to become incompatible with their own gametes. While these species would not be well served by having autogamous self-fertilization as a reproductive mechanism, other species, which do not have self-incompatibility, would benefit from autogamy. Protists have the advantage of diversifying their modes of reproduction. This is useful for a multitude of reasons. First, if there is an unfavorable change in the environment that puts the ability to deliver offspring at risk, then it is advantageous for an organism to have autogamy at its disposal. In other organisms, it is seen that genetic diversity arising from sexual reproduction is maintained by changes in the environment that favor certain genotypes over others.\nAside from extreme circumstances, it is possible that this form of reproduction gives rise to a genotype in the offspring that will increase fitness in the environment. This is due to the nature of the genetic degeneration and remodeling intrinsic to autogamy in unicellular organisms. Thus, autogamous behavior may become advantageous to have if an individual wanted to ensure offspring viability and survival. This advantage also applies to flowering plants. However, it is important to note that this change has not shown to produce a progeny with more fitness in unicellular organisms. It is possible that the nutrition deprived state of the parent cells before autogamy created a barrier for producing offspring that could thrive in those same stressful environments.\n\n In flowering plants, autogamy has the disadvantage of producing low genetic diversity in the species that use it as the predominant mode of reproduction. This leaves those species particularly susceptible to pathogens and viruses that can harm it. In addition, the foraminiferans that use autogamy have shown to produce substantially smaller progeny as a result. This indicates that since it is generally an emergency survival mechanism for unicellular species, the mechanism does not have the nutritional resources that would be provided by the organism if it were undergoing binary fission.\n\n The evolutionary shift from outcrossing to self-fertilization is one of the most frequent evolutionary transitions in plants. About 10-15% of flowering plants are predominantly self-fertilizing.\nSince autogamy in flowering plants and autogamy in unicellular species is fundamentally different, and plants and protists are not related, it is likely that both instances evolved separately. In flowering plants, it is believed that autogamy evolved one million years ago. However, due to the little overall genetic variation that arises in progeny, it is not fully understood how autogamy has been maintained in the tree of life.\n"}
{"id": "24742108", "url": "https://en.wikipedia.org/wiki?curid=24742108", "title": "Bajío dry forests", "text": "Bajío dry forests\n\nThe Bajío dry forests ecoregion, of the Tropical and subtropical dry broadleaf forests Biome, is located in western−central Mexico.\n\nThe Bajío dry forests lie in the southwestern portion of the Mexican Plateau. They are bounded on the southeast, south, and southwest by the Trans-Mexican Volcanic Belt pine-oak forests, which occupy the folded mountains and volcanoes of the Trans-Mexican Volcanic Belt that form the southern edge of the Mexican Plateau. The Sierra Madre Occidental pine-oak forests of the Sierra Madre Occidental bound the ecoregion on the northwest.\n\nThe numerous mountains of the plateau which rise above the dry forests are occupied by sky islands of pine-oak forest. To the north, the Bajío dry forests transition to the drier, more temperate Central Mexican Matorral. Most of the ecoregion lies within the basin of the Lerma River, and the dry forests extend around Lake Chapala at the eastern end of the region, and into the endorheic basins of Lake Cuitzeo and Lake Pátzcuaro in the south.\n\nNative Mammals include Mexican wolf \"(Canus lupus baileyi)\" and pocketed free-tailed bat \"(Nyctinomops femorosaccus).\" Birds include the black-throated magpie-jay \"(Calocitta colliei),\" thick-billed kingbird \"(Tyrannus crassirostris),\" whiskered screech owl \"(Otus trichopsus),\" orange-fronted parakeet \"(Aratinga caniculanis),\" dwarf vireo \"(Vireo nelsoni)\", and black-polled yellowthroat \"(Geothlypis speciosa).\"\n\nThe ecoregion is densely populated, and centuries of human use have reduced the dry forests to small pockets. Dry deciduous forest used to be the dominant vegetation, but thorn scrub and subtropical matorral are now more common, interspersed with agricultural and pasture lands. The cities of Guadalajara, Morelia, and Querétaro lie within the ecoregion.\n\n"}
{"id": "8618983", "url": "https://en.wikipedia.org/wiki?curid=8618983", "title": "Banjica forest", "text": "Banjica forest\n\nByford's Forest (} is a forest in Belgrade, the capital of Serbia. It is located in the Belgrade's municipality of Voždovac and until 2015 was known as Banjica Forest (}.\n\nByford's Forest is a meridionally elongated wooded area between the Boulevard of the Liberation on the east and the street of \"Pavla Jurišića Šturma\" on the west. South and southeast border is marked by the \"Crnotravska\" street. It begins less than away from Terazije, downtown Belgrade. It borders the neighborhoods of Diplomatska Kolonija and Dedinje on the west, Banjica on the southwest and south, Trošarina on the southeast and Voždovac on the east. The forest is some long, up to 300 wide and covers an area of .\n\nBefore World War I there was a wood in this area, but was cut down to make room for the vegetable gardens. During the Interbellum, military command built workshops for the repair of the vehicles within the complex of the military auto motorized units of the Royal Yugoslav Army (Serbian: Autokomanda), which gave name to the neighboring square and the neighborhood. Prior to the World War II, shooting range was located here. In an effort to create the forested belt around the Belgrade, the forest, initially named Banjica Forest, was planted from 1948 to 1950 (for example, Šumice Forest was created as a part of the same project).\n\nMost common tree species are pedunculate oak, red maple, silver maple and box elder, but there are many others as well. There are numerous wildflowers on the forest floor, including wood avens, violets, strawberries, garlic mustard, deadnettles etc.\n\nBird species are very diverse so because of them, Banjica forest is now a natural monument, protected by the state. 68 bird species live in the forest, 40 of which are resident birds, 16 are migratory birds and 12 are passing. The most common breeding birds are nightingale, blackcap, great tit, European magpie, hooded crow, common blackbird, woodpigeon and great spotted woodpecker. Other bird species in the forest include pheasant, common kestrel, Eurasian sparrowhawk, tawny owl and common chaffinch. During the winter, hawks, common buzzards and song thrush also settle in the forest.\n\nMammals include Eastern European hedgehog, moles, several species of shrews, various bats, the local brown subspecies of the red squirrel, wood mouse, yellow-necked mouse and least weasel. Altogether, there are 10 species of mammals in the forest, including hares which settled the forest in 2008.\n\nThe man-made facilities in the forest include the Lukoil gas station and the Best Western \nHotel \"M\". The south-western extension of the forest was turned into the large sports complex of \"Banjica\" decades ago.\n\nOn the occasion of the April 22, 2007, the Earth Day, the city government announced its plans for the Banjica forest in 2007, which will include the construction of the trim trail, artificial bed for the forest's stream and placing of bird houses through the forest.\n\nAs in the spring of 2011, the main forest path (going along the length of the forest) has a form of a functional trim trail, while the smaller paths are left in their natural state. Some old trees were cut and timber was cut in smaller pieces and left at the spot, attracting insects and insect-eating birds. Benches and wooden wastebaskets are placed periodically along the trim trail, while there are also several well-spaced spots with information tables and multiple benches under wooden roofs, intended for rest of several families. There are a wooden and a stone-based bridge over the widest parts of the stream, as well as several plank bridges over the narrowest parts. Bird houses are placed all over the forest.\n\nTimothy John Byford, British director, author and educator, who lived in Serbia since 1971, studied birds in the forest, and from 1986 to 1989, collected data on all 68 species. He began an initiative to protect the forest. He died in 2014, before the forest was placed under protection. The Institute for Nature Conservation of Serbia and the Belgrade City Secretariat for Environmental Protection, after a public hearing, declared the forest a natural monument on 15 September 2015. Also, by the same act, the name of the forest was changed to the Byford's Forest.\n"}
{"id": "13548180", "url": "https://en.wikipedia.org/wiki?curid=13548180", "title": "Central American Seaway", "text": "Central American Seaway\n\nThe Central American Seaway, also known as the Panamanic Inter-American and Proto-Caribbean Seaway, was a body of water that once separated North America from South America. It formed in the Mesozoic (200–154 Ma) during the breakup of the supercontinent Pangaea, and closed when the Isthmus of Panama was formed by volcanic activity in the late Pliocene (2.76–2.54 Ma).\n\nThe closure of the Central American Seaway had tremendous effects on oceanic circulation and the biogeography of the adjacent seas, isolating many species and triggering speciation and diversification of tropical and sub-tropical marine fauna. The inflow of nutrient-rich water of deep Pacific origin into the Caribbean was blocked, so local species had to adapt to an environment of lower productivity. It had an even larger impact on terrestrial life. The seaway had isolated South America for much of the Cenozoic, allowing the evolution of a wholly unique diverse mammalian fauna there; when it closed, a faunal exchange with North America ensued, leading to the extinction of many of the native South American forms.\n\nThe evidence for when the Central American landmass emerged and the closing of the Central American Seaway can be divided into three categories. The first is the direct geologic observation of crustal thickening and submarine deposits in Central America. The second is the Great American Interchange of vertebrates between North and South America which required a continuous land bridge across the two areas for the organisms to travel along with a climate that was very different than the climate today. Lastly is the development of differences in marine assemblages and their isotopic signatures in the Caribbean from those in the Pacific. The Central American Seaway was closed by the elevation of the Central American Isthmus which is proposed to have occurred three and a half to five million years ago. The closing of the Central American Seaway is also supported by the evolution of taxa on different sides of the Central American Isthmus along with the different histories of the oceans on either side of the isthmus.\n\nThe closing of the seaway allowed a major migration of land mammals between North and South America or The Great American Interchange. This allowed the entrance of species of mammals such as cats, horses, elephants and camels to migrate from North America to South America. There is much controversy about glacial and interglacial climates in South America. Research shows that vegetation in most of the Amazon basin has changed a very small amount since glacial times. It is also believed that the area was more of a savannah during glacial times, but it is believed that the area is quite the same. Predicted connections between a closed seaway and glaciation are causation for a very different North Atlantic Ocean circulation having an effect on the surrounding atmospheric temperatures. The emergence of the isthmus caused a reflection of the westward-flowing North Equatorial Current northward and enhanced the northward-flowing Gulf Stream.\n\n"}
{"id": "12350617", "url": "https://en.wikipedia.org/wiki?curid=12350617", "title": "Color–color diagram", "text": "Color–color diagram\n\nIn astronomy, color–color diagrams are a means of comparing the apparent magnitudes of stars at different wavelengths. Astronomers typically observe at narrow bands around certain wavelengths, and objects observed will have different brightnesses in each band. The difference in brightness between two bands is referred to as color. On color–color diagrams, the color defined by two wavelength bands is plotted on the horizontal axis, and then the color defined by another brightness difference (though usually there is one band involved in determining both colors) will be plotted on the vertical axis.\n\nAlthough stars are not perfect blackbodies, to first order the spectra of light emitted by stars conforms closely to a black-body radiation curve, also referred to sometimes as a thermal radiation curve. The overall shape of a black-body curve is uniquely determined by its temperature, and the wavelength of peak intensity is inversely proportional to temperature, a relation known as Wien's Displacement Law. Thus, observation of a stellar spectrum allows determination of its effective temperature. Obtaining complete spectra for stars through spectrometry is much more involved than simple photometry in a few bands. Thus by comparing the magnitude of the star in multiple different color indices, the effective temperature of the star can still be determined, as magnitude differences between each color will be unique for that temperature. As such, color-color diagrams can be used as a means of representing the stellar population, much like a Hertzsprung–Russell diagram, and stars of different spectral classes will inhabit different parts of the diagram. This feature leads to applications within various wavelength bands.\n\nIn the stellar locus, stars tend to align in a more or less straight feature. If stars were perfect black bodies, the stellar locus would be a pure straight line indeed. The divergences with the straight line are due to the absorptions and emission lines in the stellar spectra. These divergences can be more or less evident depending on the filters used: narrow filters with central wavelength located in regions without lines, will produce a response close to the black body one, and even filters centered at lines if they are broad enough, can give a reasonable blackbody-like behavior.\n\nTherefore, in most cases the straight feature of the stellar locus can be described by Ballesteros' formula deduced for pure blackbodies:\n\nwhere A, B, C and D are the magnitudes of the stars measured through filters with central frequencies formula_2, formula_3, formula_4 and formula_5 respectively, and k is a constant depending on the central wavelength and width of the filters, given by:\n\nNote that the slope of the straight line depends only on the effective wavelength, not in the filter width.\n\nAlthough this formula cannot be directly used to calibrate data, if one has data well calibrated for two given filters, it can be used to calibrate data in other filters. It can be used to measure the effective wavelength midpoint of an unknown filter too, by using\ntwo well known filters. This can be useful to recover information on the filters used\nfor the case of old data, when logs are not conserved and filter information has been lost.\n\nThe color-color diagram of stars can be used to directly calibrate or to test colors and magnitudes in optical and infrared imaging data. Such methods take advantage of the fundamental distribution of stellar colors in our galaxy across the vast majority of the sky, and the fact that observed stellar colors (unlike apparent magnitudes) are independent of the distance to the stars. Stellar locus regression (SLR) was a method developed to eliminate the need for standard star observations in photometric calibrations, except highly infrequently (once a year or less) to measure color terms. SLR has been used in a number of research initiatives. The NEWFIRM survey of the NOAO Deep Wide-Field Survey region used it to arrive at more accurate colors than would have otherwise been attainable by traditional calibration methods, and South Pole Telescope used SLR in the measurement of redshifts of galaxy clusters. The blue-tip method is closely related to SLR, but was used mainly to correct Galactic extinction predictions from IRAS data. Other surveys have used the stellar color-color diagram primarily as a calibration diagnostic tool, including The Oxford-Dartmouth Thirty Degree Survey and Sloan Digital Sky Survey (SDSS).\n\nAnalyzing data from large observational surveys, such as the SDSS or 2 Micron All Sky Survey (2MASS), can be challenging due to the huge number of data produced. For surveys such as these, color-color diagrams have been used to find outliers from the main sequence stellar population. Once these outliers are identified, they can then be studied in more detail. This method has been used to identify ultracool subdwarfs. Unresolved binary stars, which appear photometrically to be points, have been identified by studying color-color outliers in cases where one member is off the main sequence. The stages of the evolution of stars along the asymptotic giant branch from carbon star to planetary nebula appear on distinct regions of color–color diagrams. Quasars also appear as color-color outliers.\n\nColor–color diagrams are often used in infrared astronomy to study star forming regions. Stars form in clouds of dust. As the star continues to contract, a circumstellar disk of dust is formed, and this dust is heated by the star inside. The dust itself then begins to radiate as a blackbody, though one much cooler than the star. As a result, an excess of infrared radiation is observed for the star. Even without circumstellar dust, regions undergoing star formation exhibit high infrared luminosities compared to stars on the main sequence. Each of these effects is distinct from the reddening of starlight which occurs as a result of scattering off of dust in the interstellar medium.\n\nColor–color diagrams allow for these effects to be isolated. As the color–color relationships of main sequence stars are well known, a theoretical main sequence can be plotted for reference, as is done with the solid black line in the example to the right. Interstellar dust scattering is also well understood, allowing bands to be drawn on a color–color diagram defining the region in which stars reddened by interstellar dust are expected to be observed, indicated on the color–color diagram by dashed lines. The typical axes for infrared color–color diagrams have (H–K) on the horizontal axis and (J–H) on the vertical axis (see infrared astronomy for information on band color designations). On a diagram with these axes, stars which fall to the right of the main sequence and the reddening bands drawn are significantly brighter in the K band than main sequence stars, including main sequence stars which have experienced reddening due to interstellar dust. Of the J, H, and K bands, K is the longest wavelength, so objects which are anomalously bright in the K band are said to exhibit infrared excess. These objects are likely protostellar in nature, with the excess radiation at long wavelengths caused by suppression by the reflection nebula in which the protostars are embedded. Color–color diagrams can be used then as a means of studying stellar formation, as the state of a star in its formation can be roughly determined by looking at its position on the diagram.\n\n\n"}
{"id": "39214128", "url": "https://en.wikipedia.org/wiki?curid=39214128", "title": "Continental drip", "text": "Continental drip\n\nContinental drip is the observation that southward-pointing landforms are more numerous and prominent than northward-pointing landforms. For example, Africa, South America, India etc. all taper off to a point towards the south. The name is a play on \"continental drift\".\n\nThe observation was made by Ormonde de Kay in a 1973 tongue-in-cheek paper. John C. Holden expanded and illustrated his own version of the idea in 1976.\n\nThe planet simulator (software toy) \"SimEarth\" by Maxis includes continental drip in its Terran (Earth) simulations.\n"}
{"id": "18687743", "url": "https://en.wikipedia.org/wiki?curid=18687743", "title": "End-Botomian mass extinction", "text": "End-Botomian mass extinction\n\nThe Botomian stage of the Early Cambrian epoch lasted from \"ca\" 524 to \"ca\" 517 million years ago. At the end of the Botomian stage there was a mass extinction that wiped out a high percentage of the organisms for which fossils were found. Organisms which produced small shelly fossils were almost exterminated.\n\n"}
{"id": "3307613", "url": "https://en.wikipedia.org/wiki?curid=3307613", "title": "Flood forecasting", "text": "Flood forecasting\n\nFlood forecasting is the use of forecasted precipitation and streamflow data in rainfall-runoff and streamflow routing models to forecast flow rates and water levels for periods ranging from a few hours to days ahead, depending on the size of the watershed or river basin. Flood forecasting can also make use of forecasts of precipitation in an attempt to extend the lead-time available. \n\nFlood forecasting is an important component of flood warning, where the distinction between the two is that the outcome of flood forecasting is a set of forecast time-profiles of channel flows or river levels at various locations, while \"flood warning\" is the task of making use of these forecasts to tell decisions on warnings of floods.\n\nReal-time flood forecasting at regional area can be done within seconds by using the technology of artificial neural network. Effective real-time flood forecasting models could be useful for early warning and disaster prevention.\n\n\n"}
{"id": "11517267", "url": "https://en.wikipedia.org/wiki?curid=11517267", "title": "Geotope", "text": "Geotope\n\nGeotope is the geological component of the abiotic matrix present in an ecotope. Example geotopes might be: an exposed outcrop of rocks, an erratic boulder, a grotto or ravine, a cave, an old stone wall marking a property boundary, and so forth.\n\nIt is a loanword from German \"()\" in the study of ecology and might be the model for many other similar words coined by analogy. As the prototype, it has enjoyed wider currency than many of the other words modelled on it, including physiotope, with which it is used synonymously. But the geotope is properly the rocks and not the whole lay of the land (which would be the physiotope).\n\n\n"}
{"id": "1558869", "url": "https://en.wikipedia.org/wiki?curid=1558869", "title": "Global temperature record", "text": "Global temperature record\n\nThe global temperature record shows the fluctuations of the temperature of the atmosphere and the oceans through various spans of time. The most detailed information exists since 1850, when methodical thermometer-based records began. There are numerous estimates of temperatures since the end of the Pleistocene glaciation, particularly during the current Holocene epoch. Older time periods are studied by paleoclimatology.\n\nWeather balloon radiosonde measurements of atmospheric temperature at various altitudes begin to show an approximation of global coverage in the 1950s. Since December 1978, microwave sounding units on satellites have produced data which can be used to infer temperatures in the troposphere.\n\nSeveral groups have analyzed the satellite data to calculate temperature trends in the troposphere. Both the University of Alabama in Huntsville (UAH) and the private, NASA funded, corporation Remote Sensing Systems RSS (RSS) find an upward trend.\n\nFor the lower troposphere (TLT), UAH find a global average trend since 1978 of +0.140 °C/decade, to January 2011.\nRSS finds +0.148 °C/decade, to January 2011.\n\nIn 2004 Fu et al. found trends of +0.19 °C/decade when applied to the RSS dataset. Vinnikov and Grody found +0.20 °C/decade up between 1978 and 2005, since which the dataset has not been updated.\n\nDetailed information exists since 1850, when methodical thermometer-based records began.\n\nProxy measurements can be used to reconstruct the temperature record before the historical period. Quantities such as tree ring widths, coral growth, isotope variations in ice cores, ocean and lake sediments, cave deposits, fossils, ice cores, borehole temperatures, and glacier length records are correlated with climatic fluctuations. From these, proxy temperature reconstructions of the last 2000 years have been performed for the northern hemisphere, and over shorter time scales for the southern hemisphere and tropics.\n\nGeographic coverage by these proxies is necessarily sparse, and various proxies are more sensitive to faster fluctuations. For example, tree rings, ice cores, and corals generally show variation on an annual time scale, but borehole reconstructions rely on rates of thermal diffusion, and small scale fluctuations are washed out. Even the best proxy records contain far fewer observations than the worst periods of the observational record, and the spatial and temporal resolution of the resulting reconstructions is correspondingly coarse. Connecting the measured proxies to the variable of interest, such as temperature or rainfall, is highly non-trivial. Data sets from multiple complementary proxies covering overlapping time periods and areas are reconciled to produce the final reconstructions.\n\nProxy reconstructions extending back 2,000 years have been performed, but reconstructions for the last 1,000 years are supported by more and higher quality independent data sets. These reconstructions indicate:\n\nAs well as natural, numerical proxies (tree-ring widths, for example) there exist records from the human historical period that can be used to infer climate variations, including: reports of frost fairs on the Thames; records of good and bad harvests; dates of spring blossom or lambing; extraordinary falls of rain and snow; and unusual floods or droughts. Such records can be used to infer historical temperatures, but generally in a more qualitative manner than natural proxies.\n\nRecent evidence suggests that a sudden and short-lived climatic shift between 2200 and 2100 BCE occurred in the region between Tibet and Iceland, with some evidence suggesting a global change. The result was a cooling and reduction in precipitation. This is believed to be a primary cause of the collapse of the Old Kingdom of Egypt.\n\nMany estimates of past temperatures have been made over Earth's history. The field of paleoclimatology includes ancient temperature records. As the present article is oriented toward recent temperatures, there is a focus here on events since the retreat of the Pleistocene glaciers. The 10,000 years of the Holocene epoch covers most of this period, since the end of the Northern Hemisphere's Younger Dryas millennium-long cooling. The Holocene Climatic Optimum was generally warmer than the 20th century, but numerous regional variations have been noted since the start of the Younger Dryas.\n\nEven longer term records exist for few sites: the recent Antarctic EPICA core reaches 800 kyr; many others reach more than 100,000 years. The EPICA core covers eight glacial/interglacial cycles. The NGRIP core from Greenland stretchs back more than 100 kyr, with 5 kyr in the Eemian interglacial. Whilst the large-scale signals from the cores are clear, there are problems interpreting the detail, and connecting the isotopic variation to the temperature signal.\n\nOn longer time scales, sediment cores show that the cycles of glacials and interglacials are part of a deepening phase within a prolonged ice age that began with the glaciation of Antarctica approximately 40 million years ago. This deepening phase, and the accompanying cycles, largely began approximately 3 million years ago with the growth of continental ice sheets in the Northern Hemisphere. Gradual changes in Earth's climate of this kind have been frequent during the Earth's 4500 million year existence and most often are attributed to changes in the configuration of continents and ocean sea ways.\n\n"}
{"id": "41494415", "url": "https://en.wikipedia.org/wiki?curid=41494415", "title": "Gribskov", "text": "Gribskov\n\nGribskov (Grib Forest) is Denmark's fourth largest forest, comprising c. 5,600 ha of woodland situated in northern Zealand, west and south of Lake Esrum. The forest is owned and administered by the State of Denmark, and a part of the Kongernes Nordsjælland National Park. In July 2015, it was one of three forests included in a UNESCO World Heritage Site, the Par force hunting landscape in North Zealand.\n\nGribskov is usually divided into four sections: The northwest surrounding the small village of Maarum, the northeast on the banks of Lake Esrum, the southwest around the small lake of Gribsø and finally the southeast, enclosing the village of Nødebo on the southern banks of Lake Esrum.\n\nOnly a thin strip of Hillerød town in the south separates Gribskov from many larger woodlands such as Store Dyrehave at 1,100 ha, Tokkekøb Hegn at 631 ha and several smaller woods.\n\nThe Danish name Gribskov translates literally as \"Grib forest\" in English. The first part, 'grib', is the imperative form of the verb for 'catch' or 'grab', but the actual meaning and etymology of the word go a bit deeper. 'Grib' refers to the Old Danish word for something 'without any specific owner', so 'Gribskov' actually means a woodland of common ownership.\n\nGribskov and Lake Esrum are designated as EU habitat directive and Natura 2000 areas, as part of an even larger preserve. On top of that, Gribskov is designated as an Important Bird Area (IBA). Around 20% or c. 1,200 ha of the forest has been reserved as 'forest to be untouched', in an effort to preserve some of the few spots of semi-natural woodland (SNW) in Denmark and stimulate the growth of new.\n\nThe birdlife in Gribskov is varied and of international importance. The forest is home to the largest populations of common goldeneye, green sandpiper and red-backed shrike in Denmark and near Nødebo at Lake Esrum, a noisy colony of great cormorants has found a home. Cormorants can be a problematic bird to administer locally, but they are protected in Denmark and on list III in the Berne convention.\n\nThe forest grows in a hilly terrain (by Danish standards), with lower lying areas in the east and west. The low-lying areas are dominated by beech and oak, but with several forest types mixed in, such as wood pastures or old coppice woodland with alder and ash. There are also numerous small ponds, bogs, swamps and springs, some enshrouded by myths, superstition or old folk tales.\n\nGribskov is more than 10,000 years old, dating from the end of the last ice age, but the forest bears the marks of an intensive plantation industry that accelerated from the late 1700s and peaked in the 1800s. Former wetlands were drained and many new tree species were introduced, especially European spruce. These practises have now stopped in Gribskov. Artificial ditches are being filled to allow a more natural waterflow and the spruce plantations are cut down, to be naturally and quickly replaced by alder, birch and willow in coming years. It is expected that Gribskov will comprise more semi-natural woodland of deciduous trees in the future.\n\nThe forest of Gribskov offers a rare opportunity to observe free roaming deer of all the four species living in Denmark; namely the roe deer, sika deer, red deer and fallow deer, with roe and fallow deer being most common in Gribskov. Roe deer have lived here for as long as the forest itself, while fallow deer were introduced at some point during the middle ages. The fallow deer population in Gribskov is the largest free roaming fallow deer population in Denmark, at 600-800 animals.\n\nThere has been a long tradition of surface water draining by ditch-digging and natural waterflow regulation in Gribskov for various reasons, but these practises have now ceased and work is in progress to re-establish a more natural waterflow and improved conditions for wetland areas. These measures have already enhanced the biological diversity and has had a direct positive influence on the living conditions for birds in the forest. \n\nThere are several interesting bodies of water in Gribskov, seen both from a scientific and a folkloristic viewpoint. \"Store Gribsø\" (Large Grib-lake), or simply \"Gribsø\", is only a 10 ha lake, but is nevertheless the largest forest-enclosed lake in Gribskov. It is a so-called dystrophic lake and it is impossible to see the bottom in its dark waters, even though it is only 11 m deep. The lake has no outflows and it can be ice cold just beneath the surface, so care should be taken when bathing. Tradition says the lake is bottomless and was created when God angrily punished a nunnery that once was here. The nuns showed more interest in the monks at Esrum Abbey than in God, so he opened up the ground and the chasm swallowed up all the nuns and the entire monastery. The monastery continued to sink and sink and that was how the lake was created. It is said that one can still hear the monastery's bells ringing down in the lake on quiet evenings.\n\nThere are many small ponds, streams and lakes throughout Gribskov, but the larger ones—\"Store Gribsø\", \"Solbjerg Engsø\" and \"Strødam Engsø\"—all are situated in the southwestern parts. The latter two are the largest and attract a rich birdlife, but they are both on the edge of the forest.\n\nThe most prominent landmark is perhaps \"Svenskegrøften\" (lit.: The Swedish Ditch) initiated in 1576. It is a 2–3 km long artificial canal, winding its way through the forest from the lake of Store Gribsø and south towards the settlement of Gadevang in the southeastern section. As the name implies, Swedish prisoners of war were used for this large project, ordered by King Frederik II. The ditch is just one part of a larger network of ditches dug since the middle ages, to supply the Frederiksborg Palace with running water, to exploit the water resource for watermills in earlier times and to drain the wetlands so the land could be used for plantations. There are an estimated 526 km of artificial ditches in Gribskov.\n\nThere are several relics of the past in Gribskov. One example is the megalithic passage grave just outside Kagerup, a village south of Maarum in the northwest of the forest. It was raised at some point in the neolithic Stone Age, about 5-6,000 years ago and is referred to as \"Jættestuen\", simply meaning The Passage Grave in English. Not far from the megalithic tomb are two round dolmens, one of which is heavily deteriorated. Another megalithic passage grave is situated in the southwestern part of the woods. This tomb is known as \"Mor Gribs Hule\" (lit.: Mother Grib's Lair) and of similar age and origin as Jættestuen. Tradition says that the notorious sorceress Mother Grib lived in the grave chamber. She used to whistle at wayfarers, leading them astray to be robbed and killed by her sons This story gives an entirely new meaning to the name of Gribskov.\n\nJust northwest of Gribskov, the small woodland of Valby Hegn holds a total of seven long barrows from the neolithic.\n\nGribskov is cut through by the Gribskov Line, an old railway line laid out in 1878, then offering the urban population a first-time opportunity to visit the forests.\n\nNear the village of Nødebo at Lake Esrum in the southeastern part of the forest is \"Skovskolen\" (lit.: The Forest School), a large school situated in the old foresters lodge, \"Skovfryd\" (lit.: Forest-joy), from 1829-30. Here forest engineers, landscape engineers and nature guides are educated.\n\nIn the northeastern corner is the old Esrum Abbey.\n\nGribskov has a long tradition for forestry of all kinds.\n\nIn 1736, the German forester Johann Georg von Langen participated in restoring the Danish woodlands of the time by introducing European larch. Some of the first larch trees were planted in Gribskov in 1776 and they still can be seen there today in the northwestern parts, just east of Mårum. Known as \"Tinghuslærkene\" (lit.: The Tinghus-larches), one of the trees, now marked with a yellow ring and standing 36 m tall, was picked in 1935 by the Danish forestry geneticist Carl Syrach-Larsen for hybrid experimentation. From the marked tree in Gribskov, he developed a very successful hybrid with Japanese larch, able to withstand the devastating fungal larch canker disease also known as \"Lachnellula willkommii\". The hybrids also had a faster and healthier growth. The hybrid is known as \"Larix × marschlinsii\" or \"L. × eurolepis\" (discouraged name) and can also occur spontaneously, wherever European and Japanese larch grow together.\n\nNowadays parts of Gribskov are used for seed production of species such as European spruce.\n\nThe history of hunting in Gribskov also reaches far back in time. The most visible signs are perhaps the extensive path structures laid out in different parts of the forests, especially near Nødebo, in the years 1680–90 by King Christian V. These are long straight lines, usually designed in star-patterns, merging and radiating from strategical points. They were constructed and used for running up and tiring the game (usually deer) in so-called parforce hunting, by horse and packs of hunting dogs. The same kind of layout and design also can be seen in the nearby woodlands of Store Dyrehave and Jægersborg Dyrehave, just south of Gribskov. They all are former royal game reserves. Gribskov still is used for hunting today—in particular deer—and some areas are not to be disturbed, but parforce hunting is illegal and has been since the year 1777.\n\n\n"}
{"id": "19775248", "url": "https://en.wikipedia.org/wiki?curid=19775248", "title": "Henry Hoyle Howorth", "text": "Henry Hoyle Howorth\n\nSir Henry Hoyle Howorth (1 July 1842 – 15 July 1923) was a British Conservative politician, barrister and amateur historian and geologist.\n\nHe was born in Lisbon, Portugal, the son of Henry Howorth, a merchant in that city. He was educated at Rossall School before studying law. He was called to the bar by the Inner Temple in 1867, and practised on the Northern Circuit. He was also the maternal great uncle of anthropologist Sir Edmund Ronald Leach.\n\nHe was a Unionist in politics, and was elected as Conservative member of parliament for Salford South in 1886. He was re-elected in 1892 and 1895 before retiring from the Commons at the 1900 general election.\n\nApart from the law and politics, Howorth was deeply interested in archaeology, history, numismatics and ethnography. He was a prolific writer, contributing articles to a number of journals.\n\nIn 1892 he was appointed a Knight Commander of the Indian Empire in recognition of his works on the history and ethnography of Asia. In 1893 he was made a Fellow of the Royal Society, against considerable opposition as he lacked any formal scientific education. He subsequently became Honorary Librarian of Chetham's College and a Trustee of the British Museum. He was also a Member of the Chetham Society, serving as a Member of Council from 1877 until 1900.\n\nHoworth was a controversialist, frequently airing his opinions on the letters page of \"The Times\", sometimes under the pseudonym \"A Manchester Conservative\". He married Katherine Brierley in 1869 and they had three sons, one of whom was Sir Rupert Howorth. His wife predeceased him in 1921. Sir Henry Howorth died in July 1923 aged 81, and was buried in Putney Vale Cemetery.\n\nHoworth rejected the uniformitarianism of James Hutton and Charles Lyell. He attacked the ice age theory in his book \"The Mammoth and the Flood\" (1887). He defended a form of neo-diluvialism, that catastrophic floods had devastated large areas of the earth.\n\nHe did not believe in a global flood and considered the biblical deluge just one of many flood myths to support his theory. He used geological evidence to support his theory in \"The Glacial Nightmare and the Flood\" (1893). In 1905, he wrote another book \"Ice or Water\" that attempted to refute the glacial theory in detail. Professional geologists were not convinced by his theory.\n\nThe books he authored summarise his areas of interest:\n\n"}
{"id": "11470059", "url": "https://en.wikipedia.org/wiki?curid=11470059", "title": "History of energy", "text": "History of energy\n\nThe word energy derives from Greek \"ἐνέργεια\" (\"energeia\"), which appears for the first time in the work Nicomachean Ethics of 4th century BCE.\nThe concept of energy emerged from the idea of \"vis viva\" (living force), which Leibniz defined as the product of the mass of an object and its velocity squared; he believed that total \"vis viva\" was conserved. To account for slowing due to friction, Leibniz claimed that heat consisted of the random motion of the constituent parts of matter — a view shared by Isaac Newton, although it would be more than a century until this was generally accepted.\n\nÉmilie marquise du Châtelet in her book Institutions de Physique (\"Lessons in Physics\"), published in 1740, incorporated the idea of Leibniz with practical observations of Gravesande to show that the \"quantity of motion\" of a moving object is proportional to its mass and its velocity squared (not the velocity itself as Newton taught—what was later called momentum).\n\nIn 1802 lectures to the Royal Society, Thomas Young was the first to use the term \"energy\" in its modern sense, instead of \"vis viva\". In the 1807 publication of those lectures, he wrote,\n\nGustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy.\"\n\nIt was argued for some years whether energy was a substance (the caloric) or merely a physical quantity.\n\nThe development of steam engines required engineers to develop concepts and formulas that would allow them to describe the mechanical and thermal efficiencies of their systems. Engineers such as Sadi Carnot, physicists such as James Prescott Joule, mathematicians such as Émile Clapeyron and Hermann von Helmholtz, and amateurs such as Julius Robert von Mayer all contributed to the notion that the ability to perform certain tasks, called work, was somehow related to the amount of energy in the system. In the 1850s, Glasgow professor of natural philosophy William Thomson and his ally in the engineering science William Rankine began to replace the older language of mechanics with terms such as \"actual energy\", \"kinetic energy\", and \"potential energy\". William Thomson (Lord Kelvin) amalgamated all of these laws into the laws of thermodynamics, which aided in the rapid development of explanations of chemical processes using the concept of energy by Rudolf Clausius, Josiah Willard Gibbs and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius, and to the introduction of laws of radiant energy by Jožef Stefan.\nRankine, coined the term \"potential energy\". In 1881, William Thomson stated before an audience that:\n\nOver the following thirty years or so this newly developing science went by various names, such as the dynamical theory of heat or energetics, but after the 1920s generally came to be known as thermodynamics, the science of energy transformations.\n\nStemming from the 1850s development of the first two laws of thermodynamics, the science of energy have since branched off into a number of various fields, such as biological thermodynamics and thermoeconomics, to name a couple; as well as related terms such as entropy, a measure of the loss of useful energy, or power, an energy flow per unit time, etc. In the past two centuries, the use of the word energy in various \"non-scientific\" vocations, e.g. social studies, spirituality and psychology has proliferated the popular literature.\n\nIn 1918 it was proved that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time. That is, energy is conserved because the laws of physics do not distinguish between different moments of time (see Noether's theorem).\n\nDuring a 1961 lecture for undergraduate students at the California Institute of Technology, Richard Feynman, a celebrated physics teacher and Nobel Laureate, said this about the concept of energy:\n\n\n\n"}
{"id": "847357", "url": "https://en.wikipedia.org/wiki?curid=847357", "title": "Holy water", "text": "Holy water\n\nHoly water is water that has been blessed by a member of the clergy or a religious figure. The use for cleansing prior to a baptism and spiritual cleansing is common in several religions, from Christianity to Sikhism. The use of holy water as a sacramental for protection against evil is common among Anglicans, Roman Catholics, and Eastern Christians.\n\nIn Catholicism, Anglicanism, Eastern Orthodoxy, Oriental Orthodoxy and some other churches, holy water is water that has been sanctified by a priest for the purpose of baptism, the blessing of persons, places, and objects, or as a means of repelling evil.\n\nThe use of holy water used by various sects of Christianity is a practice only attested to in Catholic documents. The Apostolic Constitutions, which goes back to about the year 400, attribute the precept of using holy water to the Apostle Matthew. It is plausible that in earliest Christian times water was used for expiatory and purificatory purposes in a way analogous to its employment in Jewish Law. Yet, in many cases, the water used for the Sacrament of Baptism was flowing water, sea or river water, and it could not receive the same blessing as that contained in the baptisteries in the view of the Roman Catholic church. However, Eastern Orthodox do perform the same blessing, whether in a baptistry or an outdoor body of water.\n\nSprinkling with holy water is used as a sacramental that recalls baptism. Holy water is kept in the holy water font, which is typically located at the entrance to the church (or sometimes in a separate room or building called a baptistery). Smaller vessels, called stoups, are usually placed at the entrances of the church, to enable people to sprinkle themselves with it on entering. In recent years, with the concerns over influenza, new holy water machines that work like an automatic soap dispenser have become popular.\n\nIn the Middle Ages the power of holy water was considered so great that in some places fonts had locked covers to prevent the theft of holy water for unauthorized magic practices. The Constitutions of Archbishop Edmund Rich (1236) prescribe that \"Fonts are to be kept under lock and key, because of witchcraft (\"sortilegia\"). Similarly the chrism and sacred oil are kept locked up.\"\n\nIn Catholicism, holy water, as well as water used during the washing of the priest's hands at mass, is not allowed to be disposed of in regular plumbing. Roman Catholic churches will usually have a special basin (a \"Sacrarium\") that leads directly into the ground for the purpose of proper disposal. A hinged lid is kept over the holy water basin to distinguish it from a regular sink basin, which is often just beside it. Items that contained holy water are separated, drained of the holy water, and then washed in a regular manner in the adjacent sink.\n\nHoly water fonts have been identified as a potential source of bacterial and viral infection. In the late 19th century, bacteriologists found staphylococci, streptococci, coli bacilli, Loeffler's bacillus, and other bacteria in samples of holy water taken from a church in Sassari, Italy. In a study performed in 1995, 13 samples were taken when a burn patient acquired a bacterial infection after exposure to holy water. The samples in that study were shown to have a \"wide range of bacterial species\", some of which could cause infection in humans. During the swine flu epidemic of 2009, Bishop John Steinbock of Fresno, California recommended that \"holy water should not be in the fonts\" due to fear of spreading infections. Also in response to the swine flu, an automatic, motion-detecting holy water dispenser was invented and installed in an Italian church in 2009.\n\nAs a reminder of baptism, Catholic Christians dip their fingers in the holy water and make the sign of the cross when entering the church. The liturgy may begin on Sundays with the Rite of Blessing and Sprinkling Holy Water, in which holy water is sprinkled upon the congregation; this is called \"aspersion\", from the Latin, \"aspergere\", to sprinkle. This ceremony dates back to the ninth century. An \"aspergill\" or \"aspergillum\" is a brush or branch used to sprinkle the water. An \"aspersorium\" is the vessel which holds the holy water and into which the aspergillum is dipped, though elaborate Ottonian examples are known as \"situlae\". Blessed salt may be added to the water \"where it is customary.\"\n\nThis use of holy water and making a sign of the cross when entering a church reflects a renewal of baptism, a cleansing of venial sin, as well as providing protection against evil. It is sometimes accompanied by the following prayer:\n\nAlthough not actually holy water since it has not been blessed by a priest, some Catholics believe that water from specific shrines, such as Lourdes, can bring healing.\n\nThe traditional Latin formula for blessing the water is as follows:\n\nExorcizo te, creatura aquæ, in nomine Dei Patris omnipotentis, et in nomine Jesu Christi, Filii ejus Domini nostri, et in virtute Spiritus Sancti: ut fias aqua exorcizata ad effugandam omnem potestatem inimici, et ipsum inimicum eradicare et explantare valeas cum angelis suis apostaticis, per virtutem ejusdem Domini nostri Jesu Christ: qui venturus est judicare vivos et mortuos et sæculum per ignem. Deus, qui ad salutem humani generis maxima quæque sacramenta in aquarum substantia condidisti: adesto propitius invocationibus nostris, et elemento huic, multimodis purificationibus præparato, virtutem tuæ benedictionis infunde; ut creatura tua, mysteriis tuis serviens, ad abigendos dæmones morbosque pellendos divinæ gratiæ sumat effectum; ut quidquid in domibus vel in locis fidelium hæc unda resperserit careat omni immunditia, liberetur a noxa. Non illic resideat spiritus pestilens, non aura corrumpens: discedant omnes insidiæ latentis inimici; et si quid est quod aut incolumitati habitantium invidet aut quieti, aspersione hujus aquæ effugiat: ut salubritas, per invocationem sancti tui nominis expetita, ab omnibus sit impugnationibus defensa. Per Dominum, amen.\n\nEnglish translation:\n\nI exorcise thee, creature of water, in the name of God the Father almighty, in the name of Jesus Christ, his Son, our Lord, and in the power of the Holy Spirit, that you may put to flight all the power of the enemy, and that enemy and to root out and, along with his fallen angels through the power of our Lord Jesus Christ, who shall come to judge the living and the dead and the world by fire. O God, who for the salvation of the human race has built Thy greatest mysteries in the substance, in your kindness hear our prayers, and with the element to this, for many kinds of purifications of his well-prepared, the power of Thy blessing, Serve it; the creation of Thy mysteries, serving as an agent of divine grace; is sprinkled with this water in their houses or in the buildings of the faithful, that whatever might be free from all uncleanness, he is freed from every harm. It is not no pestilent spirit, no taint of corruption; let all the wiles of the lurking enemy; or to provide for the safety and peace of the inhabitants of that which is, and if there be any, by the sprinkling of this water, so that health, through the invocation of Thy holy name, made secure against all attacks. Through the end.\n\nCatholic saints have written about the power of holy water as a force that repels evil. Saint Teresa of Avila, a Doctor of the Church who reported visions of Jesus and Mary, was a strong believer in the power of holy water and wrote that she used it with success to repel evil and temptations. She wrote:\nIn \"Holy Water and Its Significance for Catholics\" Henry Theiler states that in addition to being a strong force in repelling evil, holy water has the twofold benefit of providing grace for both body and soul.\n\nThe new \"Rituale Romanum\" excludes the exorcism prayer on the water. Exorcized salt used to be added to the holy Water as well. Priests can now use the older form if they wish according to \"Summorum Pontificum\", an apostolic letter by Pope Benedict XVI.\n\nAmong the Eastern Orthodox and the Byzantine Rite Catholics holy water is used frequently in rites of blessing and exorcism, and the water for baptism is always sanctified with a special blessing.\n\nThere are two rites for blessing holy water: the \"Great Blessing of Waters\" which is held on the Feast of Theophany and at baptisms, and the \"Lesser Blessing of Waters\" which is conducted according to need and local custom during the rest of the year, certain feast days calling for the Lesser Blessing of Waters as part of their liturgical observance. Both forms are based upon the Rite of Baptism. After the blessing of holy water the faithful are sprinkled with it and each drinks some of it.\n\nHoly water is drunk by the faithful after it is blessed and it is a common custom for the pious to drink holy water every morning. In the monasteries of Mount Athos holy water is always drunk in conjunction with consuming antidoron. Eastern Orthodox do not typically bless themselves with holy water upon entering a church as Western Catholics do, but a quantity of holy water is often kept in a font placed in the narthex (entrance) of the church, available for anyone who would like to partake of it or to take some of it home.\n\nAfter the annual Great Blessing of Waters at Theophany (also known as Epiphany), the priest goes to the homes of the faithful within his parish and, in predominantly Orthodox lands, to the buildings throughout town, and blesses them with holy water.\n\nWhen blessing objects such as the palms on Palm Sunday, Paschal eggs and other foods for Easter, candles, or liturgical instruments and sacred vessels (icons and crosses are not blessed, however, as they are considered intrinsically holy and redeemed). The blessing is completed by a triple sprinkling with holy water using the words, \"This (\"name of item\") is blessed by the sprinkling of this holy water, in the name of the Father, and of the Son, and of the Holy Spirit.\"\n\nThroughout the centuries, there have been many springs of water that have been believed by members of the Orthodox Church to be miraculous. Some still flow, such as the one at Pochaev Lavra in Ukraine, and the Life-Giving Spring of the Theotokos in Constantinople (commemorated on Bright Friday).\n\n\"Holy water\" is not a term used in Church of England rites, although font water is sanctified in the Church of England baptism rite. In contrast, the Episcopal Church (United States) does expressly mention the optional use of holy water in some recent liturgies of blessing. More generally, the use of water within High Church Anglicanism or Anglo-Catholicism adheres closely to Roman Catholic practice. In many such Anglican churches baptismal water is used for the asperges. Stoups with sanctified water are sometimes found near the doors of High Church Anglican churches for the faithful to use in making the sign of the cross upon entering the church.\n\nThe use of holy water in some synods of Lutheranism is for the baptism of infants and new members of the church. The water is believed to be blessed by God, as it is used in a sacrament. The water is applied to the forehead of the laity being baptised and the minister performs the sign of the cross. Lutherans tend to have holy water fonts at the entrance of the church.\n\nOther synods do not use holy water, however they do typically treat the water used for baptisms in a more respectful manner.\n\nIn the Methodist tradition, Holy Baptism is often administered by sprinkling or pouring holy water over the candidate. The official Baptismal Liturgy, as well as the liturgy for Reaffirmation of Baptism commonly done through asperges, has a prayer for the blessing of this water:\n\nIn Ancient Greek religion, holy water called \"chernips\" () was created when a torch from a religious shrine was extinguished in it. In Greek religion, purifying people and locations with water was part of the process of distinguishing the sacred from the profane.\n\nThe Book of Numbers mentions using water in a test for the purity of a wife accused of marital infidelity. Known as the Ordeal of the Bitter Water, the accused woman (sotah) would first be asked to confess to her crime before the court. If she refused to confess, she would then undergo the ritual. She would be stripped naked and then drink a mixture of water and dust. If she was guilty, she would be supposedly cursed to miscarry any pregnancy, though the text is unclear on the exact punishment. If she was innocent, there would be no effect.\n\nSikhs use the Punjabi term \"amrita\" (ਅੰਮ੍ਰਿਤ) for the holy water used in the baptism ceremony known as \"Amrit Sanskar\" or \"Amrit Chhakhna\".\n\nIn Hinduism, water represents God in a spiritual sense which is the central theme in Mantra Pushpam from Taithreeya Aranyakam of Yajur Veda. Bathing in holy water is, thus, a key element in Hinduism, and the Ganges is considered the holiest Hindu river.\n\nThe idea of \"blessed water\" is used in virtually all Buddhist traditions. In the Theravada tradition, water is put into a new pot and kept near a \"Paritrana\" ceremony, a blessing for protection. This \"lustral water\" can be created in a ceremony in which the burning and extinction of a candle above the water represents the elements of earth, fire, and air. This water is later given to the people to be kept in their home. Not only water but also oil and strings are blessed in this ceremony. Most Mahayana Buddhists typically recite sutras or various mantras (typically that of the bodhisattva Avalokitesvara for example) numerous times over the water, which is then either consumed or is used to bless homes afterwards. In Vajrayana Buddhism, a \"Bumpa\", a ritual object, is one of the \"Ashtamangala\", used for storing sacred water sometimes, symbolizing wisdom and long life.\n\nThe Sunni Muslim variety of holy water is the Zamzam water that comes from a spring by The Kaaba.\n\nThe drinking of \"healing water\" (\"āb-i shifā\") is a practice in various denominations of Shia Islam. In the tradition of the Twelver Shi’a, many dissolve the dust of sacred locations such as Karbala (\"khāk-i shifa\") and Najaf and drink the water (\"āb-i shifā\") as a cure for illness, both spiritual and physical. The Ismaili tradition involves the practice of drinking water blessed by the Imam of the time. This water is taken in the name of the Imam and has a deep spiritual significance. This is evident from the names used to designate the water, including light (\"nūr\") and ambrosia (\"amṛt, amī, amīras, amījal\"). This practice is recorded from the 13th and 14th centuries and continues to the present day. The ceremony is known as \"ghat-pat\" in South Asia.\n\nIn Wicca and other ceremonial magic traditions, a bowl of salt is blessed and a small amount is stirred into a bowl of water that has been ritually purified. In some traditions of Wicca, this mixture of water and salt symbolizes the brine of the sea, which is regarded as the womb of the Goddess, and the source of all life on Earth. The mixture is consecrated and used in many religious ceremonies and magical rituals.\n\nHoly water has also been believed to ward off or act as a weapon against mythical evil creatures, such as vampires. In eastern Europe, one might sprinkle holy water onto the corpse of a suspected vampire in order to destroy it or render it inert. Thereafter, the concept proliferated into fiction about such creatures.\n\n\n\n"}
{"id": "23136449", "url": "https://en.wikipedia.org/wiki?curid=23136449", "title": "Indirect land use change impacts of biofuels", "text": "Indirect land use change impacts of biofuels\n\nThe indirect land use change impacts of biofuels, also known as ILUC, relates to the unintended consequence of releasing more carbon emissions due to land-use changes around the world induced by the expansion of croplands for ethanol or biodiesel production in response to the increased global demand for biofuels.\n\nAs farmers worldwide respond to higher crop prices in order to maintain the global food supply-and-demand balance, pristine lands are cleared to replace the food crops that were diverted elsewhere to biofuels' production. Because natural lands, such as rainforests and grasslands, store carbon in their soil and biomass as plants grow each year, clearance of wilderness for new farms translates to a net increase in greenhouse gas emissions. Due to this change in the carbon stock of the soil and the biomass, indirect land use change has consequences in the greenhouse gas (GHG) balance of a biofuel.\n\nOther authors have also argued that indirect land use changes produce other significant social and environmental impacts, affecting biodiversity, water quality, food prices and supply, land tenure, worker migration, and community and cultural stability.\n\nThe estimates of carbon intensity for a given biofuel depend on the assumptions regarding several variables. As of 2008, multiple full life cycle studies had found that corn ethanol, cellulosic ethanol and Brazilian sugarcane ethanol produce lower greenhouse gas emissions than gasoline. None of these studies, however, considered the effects of indirect land-use changes, and though land use impacts were acknowledged, estimation was considered too complex and difficult to model. A controversial paper published in February 2008 in Sciencexpress by a team led by Searchinger from Princeton University concluded that such effects offset the (positive) direct effects of both corn and cellulosic ethanol and that Brazilian sugarcane performed better, but still resulted in a small carbon debt. \n\nAfter the Searchinger team paper, estimation of carbon emissions from ILUC, together with the food vs. fuel debate, became one of the most contentious issues relating to biofuels, debated in the popular media, scientific journals, op-eds and public letters from the scientific community, and the ethanol industry, both American and Brazilian. This controversy intensified in April 2009 when the California Air Resources Board (CARB) set rules that included ILUC impacts to establish the California Low-Carbon Fuel Standard that entered into force in 2011.\n\nIn May 2009 U.S. Environmental Protection Agency (EPA) released a notice of proposed rulemaking for implementation of the 2007 modification of the Renewable Fuel Standard (RFS). EPA's proposed regulations also included ILUC, causing additional controversy among ethanol producers. EPA's February 3, 2010 final rule incorporated ILUC based on modelling that was significantly improved over the initial estimates.\n\nThe UK Renewable Transport Fuel Obligation program requires the Renewable Fuels Agency (RFA) to report potential indirect impacts of biofuel production, including indirect land use change or changes to food and other commodity prices. A July 2008 RFA study, known as the Gallager Review, found several risks and uncertainties, and that the \"quantification of GHG emissions from indirect land-use change requires subjective assumptions and contains considerable uncertainty\", and required further examination to properly incorporate indirect effects into calculation methodologies. A similarly cautious approach was followed by the European Union. In December 2008 the European Parliament adopted more stringent sustainability criteria for biofuels and directed the European Commission to develop a methodology to factor in GHG emissions from indirect land use change.\n\nBefore 2008, several full life cycle (\"Well to Wheels\" or WTW) studies had found that corn ethanol reduced transport-related greenhouse gas emissions. In 2007 a University of California, Berkeley team led by Farrel evaluated six previous studies, concluding that corn ethanol reduced GHG emissions by only 13 percent. However, 20 to 30 percent reduction for corn ethanol, and 85 to 85 percent for cellulosic ethanol, both figures estimated by Wang from Argonne National Laboratory, are more commonly cited. Wang reviewed 22 studies conducted between 1979 and 2005, and ran simulations with Argonne's GREET model. These studies accounted for direct land use changes. Several studies of Brazilian sugarcane ethanol showed that sugarcane as feedstock reduces GHG by 86 to 90 percent given no significant land use change. Estimates of carbon intensity depend on crop productivity, agricultural practices, power sources for ethanol distilleries and the energy efficiency of the distillery. None of these studies considered ILUC, due to estimation difficulties. Preliminary estimates by Delucchi from the University of California, Davis, suggested that carbon released by new lands converted to agricultural use was a large percentage of life-cycle emissions.\n\nIn 2008 Timothy Searchinger, a lawyer from Environmental Defense Fund, concluded that ILUC affects the life cycle assessment and that instead of saving, both corn and cellulosic ethanol increased carbon emissions as compared to gasoline by 93 and 50 percent respectively. Ethanol from Brazilian sugarcane performed better, recovering initial carbon emissions in 4 years, while U.S. corn ethanol required 167 years and cellulosic ethanol required a 52 years payback period. The study limited the analysis a 30-year period, assuming that land conversion emits 25 percent of the carbon stored in soils and all carbon in plants cleared for cultivation. Brazil, China, and India were considered among the overseas locations where land use change would occur as a result of diverting U.S. corn cropland, and it was assumed that new cropland in each of these regions correspond to different types of forest, savanna or grassland based on the historical proportion of each converted to cultivation in these countries during the 1990s.\n\nFargione and his team published a separate paper in the same issue of Sciencexpress claiming that clearing lands to produce biofuel feedstock created a carbon deficit. This deficit applies to both direct and indirect land use changes. The study examined six conversion scenarios: Brazilian Amazon to soybean biodiesel, Brazilian Cerrado to soybean biodiesel, Brazilian Cerrado to sugarcane ethanol, Indonesian or Malaysian lowland tropical rainforest to palm biodiesel, Indonesian or Malaysian peatland tropical rainforest to palm biodiesel, and U.S. Central grassland to corn ethanol. The carbon debt was defined as the amount of released during the first 50 years of this process of land conversion. For the two most common ethanol feedstocks, the study found that sugarcane ethanol produced on natural cerrado lands would take about 17 years to repay its carbon debt, while corn ethanol produced on U.S. central grasslands would result in a repayment time of about 93 years. The worst-case scenario is converting Indonesian or Malaysian tropical peatland rainforest to palm biodiesel production, which would require about 420 years to repay.\n\nThe Searchinger and Fargione studies created controversy in both the popular media and in scientific journals. Robert Zubrin observed that Searchinger's \"indirect analysis\" approach is pseudo-scientific and can be used to \"prove anything\".\n\nWang and Haq from Argonne National Laboratory claiming: the assumptions were outdated; they ignored the potential of increased efficiency; and no evidence showed that \"U.S. corn ethanol production has so far caused indirect land use in other countries.\" They concluded that Searchinger demonstrated that ILUC \"is much more difficult to model than direct land use changes\". In his response Searchinger rebutted each technical objection and asserted that \"... any calculation that ignores these emissions, however challenging it is to predict them with certainty, is too incomplete to provide a basis for policy decisions.\"\n\nAnother criticism, by Kline and Dale from Oak Ridge National Laboratory, held that Searchinger et al. and Fargione et al. \"... do not provide adequate support for their claim that bioufuels cause high emissions due to land-use change\", as their conclusions depends on a misleading assumption because more comprehensive field research found that these land use changes \"... are driven by interactions among cultural, technological, biophysical, economic, and demographic forces within a spatial and temporal context rather than by a single crop market\". Fargione et al. responded in part that although many factors contributed to land clearing, this \"observation does not diminish the fact that biofuels also contribute to land clearing if they are produced on existing cropland or on newly cleared lands\". Searching disagreed with all of Kline and Dale arguments.\n\nThe U.S. biofuel industry also reacted, claiming that the \"Searchinger study is clearly a 'worst case scenario' analysis ...\" and that this study \"relies on a long series of highly subjective assumptions ...\" Searchinger rebutted each claim, concluding that NFA's criticisms were invalid. He noted that even if some of his assumptions are high estimates, the study also made many conservative assumptions.\n\nIn February 2010, Lapola estimated that planned expansion of Brazilian sugarcane and soybean biofuel plantations through 2020 would replace rangeland, with small direct land-use impact on carbon emissions. However, the expansion of the rangeland frontier into Amazonian forests, driven by cattle ranching, would indirectly offset the savings. \"Sugarcane ethanol and soybean biodiesel each contribute to nearly half of the projected indirect deforestation of 121,970 km by 2020, creating a carbon debt that would take about 250 years to be repaid...\"\n\nThe research also found that oil palm would cause the least land-use changes and associated carbon debt. The analysis also modeled livestock density increases and found that \"a higher increase of 0.13 head per hectare in the average livestock density throughout the country could avoid the indirect land-use changes caused by biofuels (even with soybean as the biodiesel feedstock), while still fulfilling all food and bioenergy demands.\" The authors conclude that intensification of cattle ranching and concentration on oil palm are required to achieve effective carbon savings, recommending closer collaboration between the biofuel and cattle-ranching sectors.\n\nThe main Brazilian ethanol industry organization (UNICA) commented that such studies missed the continuing intensification of cattle production already underway.\n\nA study by Arima \"et al.\" published in May 2011 used spatial regression modeling to provide the first statistical assessment of ILUC for the Brazilian Amazon due to soy production. Previously, the indirect impacts of soy crops were only anecdotal or analyzed through demand models at a global scale, while the study took a regional approach. The analysis showed a strong signal linking the expansion of soybean fields in settled agricultural areas at the southern and eastern rims of the Amazon basin to pasture encroachments for cattle production on the forest frontier. The results demonstrate the need to include ILUC in measuring the carbon footprint of soy crops, whether produced for biofuels or other end-uses.\n\nThe Arima study is based on 761 municipalities located in the Legal Amazon of Brazil, and found that between 2003 and 2008, soybean areas expanded by 39,100 km² in the basin's agricultural areas, mainly in Mato Grosso. The model showed that a 10% (3,910 km²) reduction of soy in old pasture areas would have led to a reduction in deforestation of up to 40% (26,039 km²) in heavily forested municipalities of the Brazilian Amazon. The analysis showed that the displacement of cattle production due to agricultural expansion drives land use change in municipalities located hundreds of kilometers away, and that the Amazonian ILUC is not only measurable but its impact is significant.\n\nOn April 23, 2009, California Air Resources Board (CARB) approved the specific rules and carbon intensity reference values for the California Low-Carbon Fuel Standard (LCFS) that take effect January 1, 2011. CARB's rulemaking included ILUC. For some biofuels, CARB identified land use changes as a significant source of additional GHG emissions. It established one standard for gasoline and alternative fuels, and a second for diesel fuel and its replacements.\n\nThe public consultation process before the ruling, and the ruling itself were controversial, yielding 229 comments. ILUC was one of the most contentious issues. On June 24, 2008, 27 scientists and researchers submitted a letter saying, \"As researchers and scientists in the field of biomass to biofuel conversion, we are convinced that there simply is not enough hard empirical data to base any sound policy regulation in regards to the indirect impacts of renewable biofuels production. The field is relative new, especially when compared to the vast knowledge base present in fossil fuel production, and the limited analyses are driven by assumptions that sometimes lack robust empirical validation.\" The New Fuels Alliance, representing more than two-dozen biofuel companies, researchers and investors, questioned the Board intention to include indirect land use change effects into account, wrote \"While it is likely true that zero is not the right number for the indirect effects of any product in the real world, enforcing indirect effects in a piecemeal way could have very serious consequences for the LCFS... The argument that zero is not the right number does not justify enforcing a different wrong number, or penalizing one fuel for one category of indirect effects while giving another fuel pathway a free pass.\"\n\nOn the other side, more than 170 scientists and economists urged that CARB, \"include indirect land use change in the lifecycle analyses of heat-trapping emissions from biofuels and other transportation fuels. This policy will encourage development of sustainable, low-carbon fuels that avoid conflict with food and minimize harmful environmental impacts... There are uncertainties inherent in estimating the magnitude of indirect land use emissions from biofuels, but assigning a value of zero is clearly not supported by the science.\"\n\nIndustry representatives complained that the final rule overstated the environmental effects of corn ethanol, and also criticized the inclusion of ILUC as an unfair penalty to domestic corn ethanol because deforestation in the developing world was being tied to U.S. ethanol production. The 2011 limit for LCFS means that Mid-west corn ethanol failed, unless current carbon intensity was reduced. Oil industry representatives complained that the standard left oil refiners with few options, such as Brazilian sugarcane ethanol, with its accompanying tariff. CARB officials and environmentalists counter that time and economic incentives will allow produces to adapt.\n\nUNICA welcomed the ruling, while urging CARB to better reflect Brazilian practices, lowering their estimates of Brazilian emissions.\n\nThe only Board member who voted against the ruling explained that he had a \"hard time accepting the fact that we're going to ignore the comments of 125 scientists\", referring to the letter submitted by a group of scientists questioning the ILUC penalty. \"They said the model was not good enough ... to use at this time as a component part of such an historic new standard.\" CARB advanced the expected date for an expert working group to report on ILUC with refined estimates from January 2012 to January 2011.\n\nOn December 2009 the Renewable Fuels Association (RFA) and Growth Energy, two U.S. ethanol lobbying groups, filed a lawsuit challenging LCFS' constitutionality. The two organizations argued that LCFS violated both the Supremacy Clause and the Commerce Clause, jeopardizing the nationwide ethanol market.\n\nThe Energy Independence and Security Act of 2007 (EISA) established new renewable fuel categories and eligibility requirements, setting mandatory lifecycle emissions limits. EISA explicitly mandated EPA to include \"direct emissions and significant indirect emissions such as significant emissions from land use changes.\"\n\nEISA required a 20% reduction in lifecycle GHG emissions for any fuel produced at facilities that commenced construction after December 19, 2007 to be classified as a \"renewable fuel\"; a 50% reduction for fuels to be classified as \"biomass-based diesel\" or \"advanced biofuel\", and a 60% reduction to be classified as \"cellulosic biofuel\". EISA provided limited flexibility to adjust these thresholds downward by up to 10 percent, and EPA proposed this adjustment for the advanced biofuels category. Existing plants were grandfathered in.\n\nOn May 5, 2009, EPA released a notice of proposed rulemaking for implementation of the 2007 modification of the Renewable Fuel Standard, known as RFS2. The draft of the regulations was released for public comment during a 60-day period, a public hearing was held on 9 June 2009, and also a workshop was conducted on 10–11 June 2009.\n\nEPA's draft analysis stated that ILUC can produce significant near-term GHG emissions due to land conversion, but that biofuels can pay these back over subsequent years. EPA highlighted two scenarios, varying the time horizon and the discount rate for valuing emissions. The first assumed a 30-year time period uses a 0 percent discount rate (valuing emissions equally regardless of timing). The second scenario used a 100-year time period and a 2% discount rate.\n\nOn the same day that EPA published its notice of proposed rulemaking, President Obama signed a Presidential Directive seeking to advance biofuels research and commercialization. The Directive established the Biofuels Interagency Working Group, to develop policy ideas for increasing investment in next-generation fuels and for reducing their environmental footprint.\n\nThe inclusion of ILUC in the proposed ruling provoked complaints from ethanol and biodiesel producers. Several environmental organizations welcomed the inclusion of ILUC but criticized the consideration of a 100-year payback scenario, arguing that it underestimated land conversion effects. American corn growers, biodiesel producers, ethanol producers and Brazilian sugarcane ethanol producers complained about EPA's methodology, while the oil industry requested an implementation delay.\n\nOn June 26, 2009, the House of Representatives approved the American Clean Energy and Security Act 219 to 212, mandating EPA to exclude ILUC for a 5-year period, vis a vis RFS2. During this period, more research is to be conducted to develop more reliable models and methodologies for estimating ILUC, and Congress will review this issue before allowing EPA to rule on this matter. The bill failed in the U.S. Senate.\n\nOn February 3, 2010, EPA issued its final RFS2 rule for 2010 and beyond. The rule incorporated direct and significant indirect emissions including ILUC. EPA incorporated comments and data from new studies. Using a 30-year time horizon and a 0% discount rate, EPA concluded that multiple biofuels would meet this standard.\n\nEPA's analysis accepted both ethanol produced from corn starch and biobutanol from corn starch as \"renewable fuels\". Ethanol produced from sugarcane became an \"advanced fuel\". Both diesel produced from algal oils and biodiesel from soy oil and diesel from waste oils, fats, and greases fell in the \"biomass-based diesel\" category. Cellulosic ethanol and cellulosic diesel met the \"cellulosic biofuel\" standard.\n\nThe table summarizes the mean GHG emissions estimated by EPA modelling and the range of variations considering that the main source of uncertainty in the life cycle analysis is the GHG emissions related to international land use change.\n\nUNICA welcomed the ruling, in particular, for the more precise lifecycle emissions estimate and hoped that classification the advanced biofuel designation would help eliminate the tariff.\n\nThe U.S. Renewable Fuels Association (RFA) also welcomed the ruling, as ethanol producers \"require stable federal policy that provides them the market assurances they need to commercialize new technologies\", restating their ILUC objection.\n\nRFA also complained that corn-based ethanol scored only a 21% reduction, noting that without ILUC, corn ethanol achieves a 52% GHG reduction. RFA also objected that Brazilian sugarcane ethanol \"benefited disproportionally\" because EPA's revisions lowered the initially equal ILUC estimates by half for corn and 93% for sugarcane.\n\nSeveral Midwestern lawmakers commented that they continued to oppose EPA's consideration of the \"dicey science\" of indirect land use that \"punishes domestic fuels\". House Agriculture Chairman Collin Peterson said, \"... to think that we can credibly measure the impact of international indirect land use is completely unrealistic, and I will continue to push for legislation that prevents unreliable methods and unfair standards from burdening the biofuels industry.\"\n\nEPA Administrator Lisa P. Jackson commented that the agency \"did not back down from considering land use in its final rules, but the agency took new information into account that led to a more favorable calculation for ethanol\". She cited new science and better data on crop yield and productivity, more information on co-products that could be produced from advanced biofuels and expanded land-use data for 160 countries, instead of the 40 considered in the proposed rule.\n\nAs of 2010, European Union and United Kingdom regulators had recognized the need to take ILUC into account, but had not determined the most appropriate methodology.\n\nThe UK Renewable Transport Fuel Obligation (RTFO) program requires fuel suppliers to report direct impacts, and asked the Renewable Fuels Agency (RFA) to report potential indirect impacts, including ILUC and commodity price changes. The RFA's July 2008 \"Gallager Review\", mentioned several risks regarding biofuels and required feedstock production to avoid agricultural land that would otherwise be used for food production, despite concluding that \"quantification of GHG emissions from indirect land-use change requires subjective assumptions and contains considerable uncertainty\". Some environmental groups argued that emissions from ILUC were not being taken into account and could be creating more emissions.\nOn December 17, 2008, the European Parliament approved the Renewable Energy Sources Directive (COM(2008)19) and amendments to the Fuel Quality Directive (Directive 2009/30), which included sustainability criteria for biofuels and mandated consideration of ILUC. The Directive established a 10% biofuel target. A separate Fuel Quality Directive set the EU's Low Carbon Fuel Standard, requiring a 6% reduction in GHG intensity of EU transport fuels by 2020. The legislation ordered the European Commission to develop a methodology to factor in GHG emissions from ILUC by December 31, 2010, based on the best available scientific evidence.\n\nIn the meantime, the European Parliament defined lands that were ineligible for producing biofuel feedstocks for the purpose of the Directives. This category included wetlands and continuously forested areas with canopy cover of more than 30 percent or cover between 10 and 30 percent given evidence that its existing carbon stock was low enough to justify conversion.\n\nThe Commission subsequently published terms of reference for three ILUC modeling exercises: one using a General Equilibrium model; one using a Partial Equilibrium model and one comparing other global modeling exercises. It also consulted on a limited range of high-level options for addressing ILUC to which 17 countries and 59 organizations responded. The United Nations Special Rapporteur on the Right to Food and several environmental organizations complained that the 2008 safeguards were inadequate. UNICA called for regulators to establish an empirical and \"globally accepted methodology\" to consider ILUC, with the participation of researchers and scientists from biofuel crop-producing countries.\n\nIn 2010 some NGOs accused the European Commission of lacking transparency given its reluctance to release documents relating to the ILUC work. In March 2010 the Partial and General Equilibrium Modelling results were made available, with the disclaimer that the EC had not adopted the views contained in the materials. These indicate that a 1.25% increase in EU biofuel consumption would require around of land globally.\n\nThe scenarios for varied from 5.6-8.6% of road transport fuels. The study found that ILUC effects offset part of the emission benefits, and that above the 5.6% threshold, ILUC emissions increase rapidly increase. For the expected scenario of 5.6% by 2020, the study estimated that biodiesel production increases would be mostly domestic, while bioethanol production would take place mainly in Brazil, regardless of EU duties. The analysis concluded that eliminating trade barriers would further reduce emissions, because the EU would import more from Brazil. Under this scenario, \"\"direct emission savings from biofuels are estimated at 18 Mt , additional emissions from ILUC at 5.3 Mt (mostly in Brazil), resulting in a global net balance of nearly 13 Mt savings in a 20 years horizon.\" The study also found that ILUC emissions were much greater for biodiesel from vegetable oil and estimated that in 2020 even at the 5.6% level were over half the greenhouse gas emissions from diesel.\n\nAs part of the announcement, the Commission stated that it would publish a report on ILUC by the end of 2010.\n\nOn June 10, 2010, the EC announced its decision to set up certification schemes for biofuels, including imports as part of the Renewable Energy Directive. The Commission encouraged E.U. nations, industry and NGOs to set up voluntary certification schemes. EC figures for 2007 showed that 26% of biodiesel and 31% of bioethanol used in the E.U. was imported, mainly from Brazil and the United States.\n\nUNICA welcomed the EU efforts to \"engage independent experts in its assessments\" but requested that improvements because \"... the report currently contains a certain number of inaccuracies, so once these are corrected, we anticipate even higher benefits resulting from the use of Brazilian sugarcane ethanol.\" UNICA highlighted the fact that the report assumed land expansion that \"does not take into consideration the agro-ecological zoning for sugarcane in Brazil, which prevents cane from expanding into any type of native vegetation.\"\n\nCritics said the 10% figure was reduced to 5.6% of transport fuels partly by exaggerating the contribution of electric vehicles (EV) in 2020, as the study assumed EVs would represent 20% of new car sales, two and six times the car industry's own estimate. They also claimed the study \"exaggerates to around 45 percent the contribution of bioethanol—the greenest of all biofuels—and consequently downplays the worst impacts of biodiesel.\"\n\nEnvironmental groups found that the measures \"are too weak to halt a dramatic increase in deforestation\". According to Greenpeace, \"indirect land-use change impacts of biofuel production still are not properly addressed\", which for them was the most dangerous problem of biofuels\n\nIndustry representatives welcomed the certification system and some dismissed concerns regarding the lack of land use criteria. UNICA and other industry groups wanted the gaps in the rules filled to provide a clear operating framework.\n\nThe negotiations between the European Parliament and the Council of European Ministers continue. A deal is not foreseen before 2014\n\n"}
{"id": "18856716", "url": "https://en.wikipedia.org/wiki?curid=18856716", "title": "Journal of Green Building", "text": "Journal of Green Building\n\nThe Journal of Green Building is a quarterly peer-reviewed academic journal covering research on green buildings, applications, techniques, and processes. It was established in 2006 and the editor-in-chief is Steffen Lehmann (University of South Australia). The journal is abstracted and indexed in the Arts & Humanities Citation Index and Current Contents/Arts & Humanities.\n\n"}
{"id": "42481182", "url": "https://en.wikipedia.org/wiki?curid=42481182", "title": "Kızılırmak Delta", "text": "Kızılırmak Delta\n\nThe Kızılırmak Delta is the delta of the Kızılırmak River. It is the biggest wetland in the Black Sea Region and is listed under the Ramsar Convention. as internationally important. All of the first and second delta plains and most of the third delta plain is dominated by agriculture. It is important for biodiversity of plants and birds.. In 2016 it was inscribed in the Tentative list of World Heritage Sites in Turkey.\n\nIn 2010 Liman Lake was found to be eutrophic possibly due to agricultural pollution via drainage channels.\n\nWhite stork breed here.\n\nSmall numbers of sturgeon have been observed in the estuary and may still attempt to migrate upstream.\n\n"}
{"id": "32601753", "url": "https://en.wikipedia.org/wiki?curid=32601753", "title": "List of Indonesian endemic animals", "text": "List of Indonesian endemic animals\n\nThis page contains the list of Indonesian animals.\n\nOragotan\n\n"}
{"id": "165263", "url": "https://en.wikipedia.org/wiki?curid=165263", "title": "List of U.S. National Forests", "text": "List of U.S. National Forests\n\nThe United States has 154 protected areas known as National Forests covering 188,336,179 acres (762,169 km/294,275 sq. mi). The National Forests are managed by the U.S. Forest Service, an agency of the U.S. Department of Agriculture. The first National Forest was established as the Yellowstone Park Timber and Land Reserve on March 30, 1891, then in the Department of the Interior. In 1897, the Organic Act provided purposes for which forest reserves could be established, including to protect the forest, secure water supplies, and supply timber. With the Forest Reserve Act of 1891, the President of the United States was given the power to set aside forest reserves in the public domain. With the Transfer Act of 1905, forest reserves became part of the U.S. Department of Agriculture in the newly created U.S. Forest Service.\n\nBy 1907 President Theodore Roosevelt more than doubled the forest reserve acreage, and Congress responded by limiting the President's ability to proclaim new reserves. The National Forest System underwent a major reorganization in 1908, and in 1911 Congress authorized new additions to the system under the authority of the Weeks Act. The management goals provided by the Organic Act were expanded upon by the Multiple-Use Sustained-Yield Act of 1960 to include \"outdoor recreation, range, timber, watershed, and wildlife and fish purposes\" as well as for the establishment of wilderness areas.\n\nAs of September 30, 2014, the Forest Service manages a total , of which are National Forests. The additional land areas include 20 National Grasslands, 59 purchase units, 19 research and experimental areas, five land utilization projects and 37 other areas. The National Forest System has an extensive and complicated history of reorganization, so while there are currently 154 named National Forests, many of these are managed together as either a single forest or separate forests.\n\nThere is at least one National Forest in all but ten states:\nConnecticut, Delaware, Hawaii, Iowa, Kansas, Maryland, Massachusetts, North Dakota, New Jersey, and Rhode Island (although Kansas and North Dakota have national grasslands). In addition, Puerto Rico contains El Yunque National Forest. Alaska has the most national forest land with 21.9 million acres (8.9 million ha), followed by California (20.8 million acres, 8.4 million ha) and Idaho (20.4 million acres, 8.3 million ha). Idaho also has the greatest percent of its land in national forests with 38.2%, followed by Oregon with 24.7% and Colorado with 20.9%. On maps, national forests in the west generally show the true extent of their area, but those in the east often only show purchase districts, within which usually only a minority of the land is owned by the Forest Service.\n\n\n\n"}
{"id": "8613679", "url": "https://en.wikipedia.org/wiki?curid=8613679", "title": "List of extinct cetaceans", "text": "List of extinct cetaceans\n\nThe list of extinct cetaceans features the extinct genera and species of the order Cetacea. The cetaceans (whales, dolphins and porpoises) are descendants of land-living mammals, the even-toed ungulates. The earliest cetaceans were still hoofed mammals. These early cetaceans became gradually better adapted for swimming than for walking on land, finally evolving into fully marine cetaceans.\n\nThis list currently includes only fossil genera and species. However, the Atlantic population of gray whales (\"Eschrichtius robustus\") became extinct in the 18th century, and the baiji (or Chinese river dolphin, \"Lipotes vexillifer\") was declared \"functionally extinct\" after an expedition in late 2006 failed to find any in the Yangtze River.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification follows Steeman, 2007.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "881156", "url": "https://en.wikipedia.org/wiki?curid=881156", "title": "List of giant sequoia groves", "text": "List of giant sequoia groves\n\nThe following is a list of giant sequoia groves. All naturally occurring groves of giant sequoias are located in moist, unglaciated ridges and valleys of the west slope of the Sierra Nevada range in California, United States. They occur between 13702000 meters (4500–6500 ft) elevation in the northern half of the range, and 1700–2250 m (5500–7500 ft) in the south.\n\nGroves in the northern half of the range (north of the Kings River) are widely scattered and mostly small, while those south of the Kings River are more numerous. The total area of all the groves combined is approximately 14,416 ha (35,607 acres). The groves are listed from north to south in the list below.\n\nThis list is based on five different sources, with slightly varying views on what constitutes a discrete grove; the differing interpretations are noted \"in italics\". The lists of groves were compiled by Rundel (1972; recognizing 75 groves), Flint (1987; recognizing 65 groves), Willard (1994; recognizing 65 groves), the \"Giant Sequoia National Monument Visitor's Guide\" (2003) and the Draft Giant Sequoia National Monument Plan 2010. Currently, the U.S. National Park Service cites Rundel's total of 75 groves in its visitor publications. The updated lists from Willard and Flint are now known to be more accurate, therefore some of Rundel's 75 groves have been removed from this list. Below compiles a list of 68 sequoia groves.\n\nListed North to South\nThe 16 groves in the Kings River watershed are in Kings Canyon National Park, the northern section of Giant Sequoia National Monument, or Sequoia National Forest, in southernmost Fresno County and Tulare County (listed alphabetically):\n\n\nThe 23 groves in the Kaweah River watershed are all in Sequoia National Park or in private ownership, except the northernmost in Sequoia National Forest & Kings Canyon National Park (listed North to South):\nThe 21 groves in the Tule River and Kern River watersheds are mostly in Giant Sequoia National Monument, with some areas in Sequoia National Park, Mountain Home Demonstration State Forest-California State Forest and Tule Indian Reservation; all in southern Tulare County. (listed alphabetically)\n\n\n\n"}
{"id": "38676739", "url": "https://en.wikipedia.org/wiki?curid=38676739", "title": "List of highest mountains of Switzerland", "text": "List of highest mountains of Switzerland\n\nThis is a list of the highest mountains of Switzerland. This list only includes summits above with a topographic prominence of at least 30 metres. For a list of mountains (major summits only) including lower summits, see List of mountains of Switzerland. \n\nThe Union Internationale des Associations d'Alpinisme defines a summit in the Alps as independent, if the connecting ridge between it and a higher summit drops at least 30 m (a prominence/drop of 30 m, with the lowest point referred to as the \"key col\"). There are over 250 such summits exceeding 3,600 m in Switzerland, located in the High Alps, in the cantons of Valais, Bern, Graubünden, Uri and Glarus. All mountain heights and prominences on the list are from the largest-scale maps available.\n"}
{"id": "9017674", "url": "https://en.wikipedia.org/wiki?curid=9017674", "title": "List of sweetgum diseases", "text": "List of sweetgum diseases\n\nThe following is a list of diseases of sweetgum (\"Liquidambar\" spp.).\n\n"}
{"id": "15522138", "url": "https://en.wikipedia.org/wiki?curid=15522138", "title": "Marine counterparts of land creatures", "text": "Marine counterparts of land creatures\n\nThe idea that there are specific marine counterparts to land creatures, inherited from the writers on natural history in Antiquity, was firmly believed in Islam and in Medieval Europe, and is exemplified by the creatures represented in the medieval animal encyclopedias called bestiaries and in the parallels drawn in the moralising attributes attached to each. \"The creation was a mathematical diagram drawn in parallel lines,\" T. H. White said a propos the bestiary he translated. \"Things did not only have a moral they often had physical counterparts in other strata. There was a horse in the land and a sea-horse in the sea. For that matter there was probably a Pegasus in heaven\". The idea of perfect analogies in the fauna of land and sea was considered part of the perfect symmetry of the Creator's plan, offered as the \"book of nature\" to mankind, for which a text could be found in \"Job\":\nBut ask the animals, and they will teach you, or the birds of the air, and they will tell you; or speak to the earth, and it will teach you, or let the fish of the sea inform you. Which of all these does not know that the hand of the Lord has done this? In his hand is the life of every creature and the breath of all mankind.\n\nThe idea appears in the Jewish Tannaic sources as well, as brought down in Babylonian Talmud, Chulin 127a. Rashi (Psalms 49:2) traces this to a biblical source – the land is referred to as \"Chaled\", from the weasel (chulda), because the weasel is the only animal on dry land that does not have its counterpart in the sea.\nAll of Creation was considered to reflect the Creator, and Man could learn about the Creator through studying the Creation, an assumption that underlies the \"watchmaker analogy\" offered as a proof of God's existence.\n\nThe correspondence between the realms of earth and sea, extending to its denizens, offers examples of the taste for allegory engendered by Christian and Islamic methods of exegesis, which also encouraged the doctrine of signatures, a \"key\" to the meaning and use of herbs.\n\nThe source text that was most influential in compiling the bestiaries of the 12th and 13th centuries was the \"Physiologus\", one of the most widely read and copied secular texts of the Middle Ages. Written in Greek in Alexandria the 2nd century CE and accumulating further \"exemplary\" beasts in the next three centuries and more, \"Physiologus\" was transmitted in the West in Latin, and eventually translated into many vernacular languages: many manuscripts in various languages survive.\nAelian, \"On the Characteristics of Animals\" (A. F. Scholfield, in Loeb Classical Library, 1958).\n\nChristian writers, trained in anagogical thinking and expecting to find spiritual instruction inherent in the processes of Nature, disregarded the caveat in Pliny's Natural History, where the idea is presented as a \"vulgar opinion\": \n\nHence it is that the vulgar notion may very possibly be true, that whatever is produced in any other department of Nature, is to be found in the sea as well; while, at the same time, many other productions are there to be found which nowhere else exist. That there are to be found in the sea the forms, not only of terrestrial animals, but of inanimate objects even, is easily to be understood by all who will take the trouble to examine the grape-fish, the sword-fish, the sawfish, and the cucumber-fish, which last so strongly resembles the real cucumber both in colour and in smell.\nPliny points out that many more things are found in the sea than on the land, and also mentions the correspondences that may be discovered between many non-living objects of the land and living creatures in the sea. \n\nSaint Augustine of Hippo reasons based on analogy, that since there is a serpent in the grass, there must be an eel in the sea; because there is a Leviathan in the sea, there must be a Behemoth on the land. (\"City of God\"? xi.15?)\nThe reaction to such anagogical thinking set in with the unfolding of critical scientific thought in the 17th century. Sir Thomas Browne devoted a chapter of his \"Pseudodoxia Epidemica\" to dispelling such a belief: Chapter XXIV: \"That all Animals in the land are in their kinde in the Sea.\" During the Enlightenment the ancient conception was given an innovative and rationalized cast by Benoît de Maillet in describing the transformations and metamorphoses undergone by creatures of the sea to render them fit for life on land, a proto-evolutionist concept, though it was based on superficial morphological similarities:\nThere are in the Sea, Fish of almost all the Figures of Land-Animals, and even of Birds. She includes Plants, Flowers, and some Fruits; the Nettle, the Rose, the Pink, the Melon and the Grape, are to be found there.<br>\n<br>\nAs for the Quadrupeds, we not only find in the Sea, Species of the same Figure and Inclinations, and in the Waves living on the same Aliments by which they are nourished on Land, we have also Examples of those Species living equally in the Air and in the Water. Have not the Sea-Apes precisely the same figure with those of the Land?\n\nThough in \"Moby-Dick\" Ishmael, with a nod to Sir Thomas Browne's wording, denies the claim that land animals find their counterparts in the sea,For though some old naturalists have maintained that all creatures of the land are of their kind in the sea; and though taking a broad general view of the thing, this may very well be; yet coming to specialties, where, for example, does the ocean furnish any fish that in disposition answers to the sagacious kindness of the dog? The accursed shark alone can in any generic respect be said to bear comparative analogy to him.\nin discussing dolphins trained to aid scuba divers, a 1967 \"Popular Mechanics\" article could still casually state: \"It's hoped that the marine counterparts of some land animals can be trained to become useful members of the Man-in-the-Sea program.\"\n"}
{"id": "24082423", "url": "https://en.wikipedia.org/wiki?curid=24082423", "title": "Mass–luminosity relation", "text": "Mass–luminosity relation\n\nIn astrophysics, the mass–luminosity relation is an equation giving the relationship between a star's mass and its luminosity, first noted by Jakob Karl Ernst Halm. The relationship is represented by the equation:\nwhere \"L\" and \"M\" are the luminosity and mass of the Sun and 1 < \"a\" < 6. The value \"a\" = 3.5 is commonly used for main-sequence stars. This equation and the usual value of \"a\" = 3.5 only applies to main-sequence stars with masses 2\"M\" < \"M\" < 55\"M\" and does not apply to red giants or white dwarfs. As a star approaches the Eddington Luminosity then \"a\" = 1.\n\nIn summary, the relations for stars with different ranges of mass are, to a good approximation, as the following:\n\nFor stars with masses less than 0.43\"M\", convection is the sole energy transport process, so the relation changes significantly. For stars with masses \"M\" > 55\"M\" the relationship flattens out and becomes \"L\" ∝ \"M\" but in fact those stars don't last because they are unstable and quickly lose matter by intense solar winds. It can be shown this change is due to an increase in radiation pressure in massive stars. These equations are determined empirically by determining the mass of stars in binary systems to which the distance is known via standard parallax measurements or other techniques. After enough stars are plotted, stars will form a line on a logarithmic plot and slope of the line gives the proper value of \"a\".\n\nAnother form, valid for K-type main-sequence stars, that avoids the discontinuity in the exponent has been given by Cuntz & Wang; it reads:\nwith \n(\"M\" in \"M\"). This relation is based on data by Mann and collaborators, who used moderate-resolution spectra of nearby late-K and M dwarfs with known parallaxes and interferometrically determined radii to refine their effective temperatures and luminosities. Those stars have also been used as a calibration sample for Kepler candidate objects. Besides avoiding the discontinuity in the exponent at \"M\" = 0.43\"M\", the relation also recovers \"a\" = 4.0 for \"M\" ≃ 0.85\"M\".\n\nThe mass/luminosity relation is important because it can be used to find the distance to binary systems which are too far for normal parallax measurements, using a technique called \"dynamical parallax\". In this technique, the masses of the two stars in a binary system are estimated, usually as being the mass of the Sun. Then, using Kepler's laws of celestial mechanics, the distance between the stars is calculated. Once this distance is found, the distance away can be found via the arc subtended in the sky, giving a preliminary distance measurement. From this measurement and the apparent magnitudes of both stars, the luminosities can be found, and by using the mass–luminosity relationship, the masses of each star. These masses are used to re-calculate the separation distance, and the process is repeated. The process is iterated many times, and accuracies as high as 5% can be achieved. The mass/luminosity relationship can also be used to determine the lifetime of stars by noting that lifetime is approximately proportional to M/L although one finds that more massive stars have shorter lifetimes than what the M/L relationship predicts. A more sophisticated calculation factors in a star's loss of mass over time.\n\nDeriving a theoretically exact mass/luminosity relation requires finding the energy generation equation and building a thermodynamic model of the inside of a star. However, the basic relation \"L\" ∝ \"M\" can be derived using some basic physics and simplifying assumptions. The first such derivation was performed by astrophysicist Arthur Eddington in 1924. The derivation showed that stars can be approximately modelled as ideal gases, which was a new, somewhat radical idea at the time. What follows is a somewhat more modern approach based on the same principles.\n\nAn important factor controlling the luminosity of a star (energy emitted per unit time) is the rate of energy dissipation through its bulk. Where there is no heat convection, this dissipation happens mainly by photons diffusing. By integrating Fick's first law over the surface of some radius \"r\" in the radiation zone (where there is neglible convection), we get the total outgoing energy flux which is equal to the luminosity by conservation of energy:\nWhere \"D\" is the photons diffusion coefficient, and \"u\" is the energy density.\n\nNote that this assumes that the star is not fully convective, and that all heat creating processes (nucleosynthesis) happen in the core, below the radiation zone. These two assumptions are not correct in red giants, which do not obey the usual mass-luminosity relation. Stars of low mass are also fully convective, hence do not obey the law.\n\nApproximating the star by a black body, the energy density is related to the temperature by Stefan-Boltzmann law by:\nwhere\nis the Stefan-Boltzmann constant, \"c\" is the speed of light, \"k\" is Boltzmann constant and formula_11 is the reduced Planck constant.\n\nAs in the elementary theory of diffusion coefficient in gases, the diffusion coefficient \"D\" approximately satisfies:\nwhere λ is the photon mean free path.\n\nSince matter is fully ionized in the star core (as well as where the temperature is of the same order of magnitude as inside the core), photons collide mainly with electrons, and so λ satisfies \nHere formula_14 is the electron density and:\nis the cross section for electron-photon scattering, equal to Thomson cross-section. α is the fine-structure constant and \"m\" the electron mass.\n\nThe average stellar electron density is related to the star mass \"M\" and radius \"R\"\n\nFinally, by the virial theorem, the total kinetic energy is equal to half the gravitational potential energy \"E\", so if the average nuclei mass is \"m\", then the average kinetic energy per nucleus satisfies:\nwhere the temperature \"T\" is averaged over the star and \"C\" is a factor of order one related to the stellar structure and can be estimated from the star approximate polytropic index.\nNote that this does not hold for large enough stars, where the radiation pressure is larger than the gas pressure in the radiation zone, hence the relation between temperature, mass and raidus is different, as elaborated below.\n\nWrapping up everything, we also take \"r\" to be equal to \"R\" up tp a factor, and \"n\" at \"r\" is replaced by its stellar average up to a facotr. The combined factor is approximately 1/15 for the sun, and we get:\n\nThe added factor is actually dependent on \"M\", therefore the law has an approximate formula_21 dependence.\n\nOne may distinguish between the cases of small and large stellar masses by deriving the above results using radiation pressure. In this case, it is easier to use the optical opacity formula_22 and to consider the internal temperature T directly; more precisely, one can consider the average temperature in the radiation zone.\n\nThe consideration begins by noting the relation between the radiation pressure P and luminosity. The gradient of radiation pressure is equal to the momentum transfer absorbed from the radiation, giving:\n\nformula_23\n\nwhere c is the velocity of light. Here, formula_24; the photon mean free path.\n\nThe radiation pressure is related to the temperature by formula_25, therefore\n\nformula_26\n\nfrom which it follows directly that\n\nformula_27.\n\nIn the radiation zone gravity is balanced by the pressure on the gas coming from both itself (approximated by ideal gas pressure) and from the radiation. For a small enough stellar mass the latter is negligible and one arrives at\n\nformula_28\n\nas before. More precisely, since integration was done from 0 to R so formula_29 on the left side, but the surface temperature T can be neglected with respect to the internal temperature T.\n\nFrom this it follows directly that\n\nformula_30\n\nFor a large enough stellar mass, the radiation pressure is larger than the gas pressure in the radiation zone. Plugging in the radiation pressure, instead of the ideal gas pressure used above, yields\n\nformula_31\n\nhence\n\nformula_32\n\nTo the first approximation, stars are black body radiators with a surface area of 4\"πR\". Thus, from the Stefan–Boltzmann law, the luminosity is related to the surface temperature \"T\", and thrught it to the color of the star, by\nwhere \"σ\" is Stefan-Boltzmann constant, 5.67 × 10W m K.\n\nThe luminosity is equal to the total energy produced by the star per unit time. Since this energy is produced by nucleosynthesis, usually in the star core (this is not true for red giants), the core temperature is related to the luminosity by the nucleosynthesis rate per unit volume:\nHere, ε is the total energy emitted in the chain reaction or reaction cycle. formula_35 is the Gamow peak energy, dependent on \"E\", the Gamow factor. Additionally, \"S\"(\"E\")/E is the reaction cross section, \"n\" is number density, formula_36 is the reduced mass for the particle collision, and \"A\",\"B\" are the two species participating in the limiting reaction (e.g. both stand for a proton in the proton-proton chain reaction, or \"A\" a proton and \"B\" an nucleus for the CNO cycle).\n\nSince the radius \"R\" is itself a function of the temperature and the mass, one may solve this equation to get the core temperature.\n"}
{"id": "2521128", "url": "https://en.wikipedia.org/wiki?curid=2521128", "title": "MegaFlyover", "text": "MegaFlyover\n\nThe MegaFlyover project was a seven-month aerial survey from June 2004 to January 2005 by explorer/ecologist J. Michael Fay and pilot Peter Ragg sponsored by the National Geographic Society and others. They criss-crossed Africa from South Africa to Morocco in a modified Cessna 182, logging 60,000 miles (about 100,000 kilometers) and taking more than 100,000 high-resolution digital GPS-marked images shot from low altitude (every 20 seconds).\n\nIt was partly inspired by Mike's earlier MegaTransect expedition in 1999.\n\nFive hundred of these images have been added to Google Earth's database.\n\n"}
{"id": "298837", "url": "https://en.wikipedia.org/wiki?curid=298837", "title": "Meteorological disasters", "text": "Meteorological disasters\n\nMeteorological disasters are caused by extreme weather, e.g. rain, drought, snow, extreme heat or cold, ice, or wind. Violent, sudden and destructive to the environment related to, produced by, or affecting the earth's atmosphere, especially the weather-forming processes.\n\nExamples of weather disasters include blizzard, cyclones, droughts, hailstorms, heat waves, hurricanes, floods (caused by rain), and tornadoes.\n\n\n"}
{"id": "53471367", "url": "https://en.wikipedia.org/wiki?curid=53471367", "title": "Montreux Record", "text": "Montreux Record\n\nThe Montreux Record is a register of wetland sites on the List of Ramsar wetlands of international importance where changes in ecological character have occurred, are occurring, or are likely to occur as a result of technological developments, pollution or other human interference. It is a voluntary mechanism to highlight specific wetlands of international importance that are facing immediate challenges. It is maintained as part of the List of Ramsar wetlands of international importance.\n\nAt present, 48 sites are listed in Montreux Record . 32 sites are removed from this record.\n\nARGENTINA \n\nLaguna de Llancanelo, designated 08/11/95, Mendoza, 65,000 ha, Montreux Record 02/07/01\n\nAUSTRIA \n\nDonau-March-Auen, designated 16/12/82, Nieder Österreich, 38,500 ha, Montreux Record 04/07/90\n\nBELGIUM \n\nDe Ijzerbroeken te Diksmuide en Lo-Renige, designated 04/03/86, Vlaamse Gewest, 2,360 ha, Montreux Record 04/07/90, removed from Record 17/01/94, replaced on Record 17/05/99\nSchorren van de Beneden Schelde, designated 04/03/86, Vlaamse Gewest, 420 ha, Montreux Record 04/07/90\n\nBULGARIA\n\nDurankulak Lake, designated 28/11/84, Varna, 350 ha, Montreux Record 16/06/93\nSrebarna, designated 24/09/75, Silistra, 600 ha, Montreux Record 16/06/93\n\nCHILE\n\nCarlos Anwandter Sanctuary, designated 27/07/81, Región X, 4,877 ha, Montreux Record 06/10/06\n\nCOSTA RICA\n\nPalo Verde, designated 27/12/91, Guanacaste, 19,800 ha, Montreux Record 16/06/93\n\nCROATIA\n\nKopacki Rit, designated 03/02/93, 17,770 ha, Montreux Record 16/06/93\n\nCZECH REPUBLIC \n\nLitovelské Pomoraví, designated 02/11/93, Olomouc, Sumperk, 5,122 ha, Montreux Record 26/02/97\nMokrady dolního Podyjí (floodplain of lower Dyje River), designated 02/11/93, Breclav, 11,500 ha, Montreux Record 06/06/05\nPoodri, designated 02/11/93, Ostrava, N. Jicín, 5,450 ha, Montreux Record 06/06/05\nTrebonská rybníky (Trebon fishponds), designated 02/07/90, J. Hradec, Tábor, C. Budejovice, 10,165 ha, Montreux Record 18/09/94\n\nDEMOCRATIC REPUBLIC OF CONGO \n\nParc national des Mangroves, designated 18/01/96, Bas-Zaïre, 66,000 ha , Montreux Record 11/04/00\n\nDENMARK\n\nRingkøbing Fjord, designated 02/09/77, Ringkøbing, 27,520 ha, Montreux Record 04/07/90\n\nEGYPT\n\nLake Bardawil, designated 09/09/88, 59,500 ha, Montreux Record 04/07/90\nLake Burullus, designated 09/09/88, Kafr El Sheikh, 46,200 ha, Montreux Record 04/07/90\n\nGERMANY \n\nWattenmeer, Ostfriesisches Wattenmeer & Dollart, designated 26/02/76, Niedersachsen, 121,620 ha, Montreux Record 04/07/90\n\nGREECE \n\nAmvrakikos gulf, designated 21/08/75, Aitoloakarnania, Preveza, Arta, 23,649 ha, Montreux Record 04/07/90\nAxios, Loudias, Aliakmon delta, designated 21/08/75, Thessaloniki, Imanthia, Piera, 11,808 ha, Montreux Record 04/07/90\nKotychi lagoons, designated 21/08/75, Ileia, 6,302 ha, Montreux Record 04/07/90\nLake Vistonis, Porto Lagos, Lake Ismaris & adjoining lagoons, designated 21/08/75, Rodopi, Xanthi, 24,396 ha, Montreux Record 04/07/90\nLakes Volvi & Koronia, designated 21/08/75, Thessaloniki, 16,388 ha, Montreux Record 04/07/90\nMessolonghi lagoons, designated 21/08/75, Aitoloakarnania, 33,687 ha, Montreux Record 04/07/90\nNestos delta & adjoining lagoons, designated 21/08/75, Xanthi, 21,930 ha, Montreux Record 04/07/90\n\nGUATEMALA\n\nLaguna del Tigre, designated 26/06/90, El Petén, 48,372 ha, Montreux Record 16/06/93\n\nINDIA \n\nKeoladeo National Park, designated 01/10/81, Rajasthan, 2,873 ha, Montreux Record 04/07/90\nLoktak Lake, designated 23/03/90, Manipur, 26,600 ha, Montreux Record 16/06/93\n\nIRAN \n\nAnzali Mordab (Talab) complex, designated 23/06/75, Gilan, 15,000 ha, Montreux Record 31/12/93\nHamun-e-Puzak, south end, designated 23/06/75, Sistan & Baluchestan, 10,000 ha, Montreux Record 04/07/90\nHamun-e-Saberi & Hamun-e-Helmand, designated 23/06/75, Sistan & Baluchestan, 50,000 ha, Montreux Record 04/07/90\nNeyriz Lakes & Kamjan Marshes, designated 23/06/75, Fars, 108,000 ha, Montreux Record 04/07/90\nShadegan Marshes & mudflats of Khor-al Amaya & Khor Musa, designated 23/06/75, Khuzestan, 400,000 ha, Montreux Record 16/06/93\nShurgol, Yadegarlu & Dorgeh Sangi Lakes, designated 23/06/75, Azarbayjan-e Gharbi, 2,500 ha, Montreux Record 04/07/90\n\nIRAQ \n\nHawizeh Marsh Ramsar site, designated 17/10/07, Basra, Amara, 137,700 ha, Montreux Record 28/04/10\n\nJORDAN \n\nAzraq Oasis, designated 10/01/77, 7,372 ha, Montreux Record 04/07/90\n\nNICARAGUA\n\nSistema de Humedales de la Bahía de Bluefields, designated 08/11/01, Atlántico Sur, 86,501 ha, Montreux Record 15/01/07\n\nSENEGAL\n\nBassin du Ndiael, designated 11/07/77, Saint-Louis, 10,000 ha, Montreux Record 04/07/90\n\nSOUTH AFRICA \n\nBlesbokspruit, designated 02/10/86, Gauteng, 1,858 ha, Montreux Record 06/05/96\nOrange River Mouth, designated 28/06/91, Northern Cape, 2,000 ha, Montreux Record 26/09/95\n\nSPAIN\n\nDoñana, designated 04/05/82, Andalucía, 50,720 ha, Montreux Record 04/07/90\nLas Tablas de Daimiel, designated 04/05/82, Castilla-La Mancha, 1,928 ha, Montreux Record 04/07/90\n\nTUNISIA \n\nIchkeul, designated 24/11/80, Bizerte, 12,600 ha, Montreux Record 04/07/90\n\nUGANDA \n\nLake George, designated 04/03/88, Toro Province, 15,000 ha, Montreux Record 04/07/90\n\nUNITED KINGDOM \n\nThe Dee Estuary, designated 17/07/85, England, Wales, 13,085 ha, Montreux Record 04/07/90\nOuse Washes, designated 05/01/76, England, 2,469 ha, Montreux Record 31/10/00\n\nUNITED STATES OF AMERICA \n\nEverglades, designated 04/06/87, Florida, 566,143 ha, Montreux Record 16/06/93\n\nURUGUAY\n\nBañados del Este y Franja Costera, designated 22/05/84, Rocha, Treinta y Tres, 435,000 ha, Montreux Record 04/07/90\n\nformer USSR \n\nIn addition, three Ramsar Sites were placed on the Montreux Record by the USSR and have subsequently been redesignated or acknowledged as Ramsar Sites by successor states to the Soviet Union (Azerbaijan, Kazakhstan, and Kyrgyz Republic). The Secretariat is pursuing discussions with those three Contracting Parties about their wishes concerning the present status of those sites vis-à-vis the Montreux Record, and they are not presently considered to be part of the above list.\n\nIsyk-Kul State Reserve with the Lake Isyk-Kul, designated 12/11/02 Kyrgyz republic, Montreux Record 04/07/90\nKirov Bays, designated 11/10/76, Azerbaijan, 132,500 ha, Montreux Record 04/07/90\nLakes of the lower Turgay & Irgiz, designated 11/10/76, Kazakhstan, 348,000 ha, Montreux Record 16/06/93\n\nList of Ramsar sites removed from the Montreux Record\n\n1. Algeria, Lac Oubeïra, designated 04/11/83, 2,200 ha, Montreux Record 04/07/90, removed 18/11/97\n\n2. Algeria, Lac Tonga, designated 04/11/83, 2,700 ha, Montreux Record 16/06/93, removed 07/09/09\n\n3. Algeria, Oasis de Ouled Saïd, designated 02/02/01, 25,400 ha, Montreux Record 14/06/01, removed 07/09/09\n\n4. Belgium, De Ijzerbroeken te Diksmuide en Lo-Renige, designated 04/03/86, 2,360 ha, Montreux Record 04/07/90, removed from Record 17/01/94, replaced on Record 17/05/99\n\n5.Bolivia, Laguna Colorada, designated 27/06/90, 5,240 ha, Montreux record 16/06/93, removed 07/08/96\n\n6. Czech Republic, Novozámecky a Brehynsky rybník (Novozámeckv/Brehynsky fishponds), designated 02/07/90, 923 ha, Montreux Record 18/09/94, removed 26/09/02\n\n7. Germany, Unterer Niederrhein, designated 28/10/83, 25,000 ha, Montreux Record 16/06/93, removed 08/01/99\n\n8. Greece, Artificial Lake Kerkini, designated 21/08/75, 10,996 ha, Montreux Record 07/04/90, removed 18/05/99\n\n9. Greece, Evros delta, designated 21/08/75, 9,267 ha, Montreux Record 07/04/90, removed 18/05/99\n\n10. Greece, Lake Mikri Prespa, designated 21/08/75, 5,078 ha, Montreux Record 07/04/90, removed 18/05/99\n\n11. Iceland, Myvatn-Laxá (part), designated 02/12/77, 20,000 ha, Montreux Record 04/07/90, removed 16/06/93\n\n12. Iceland, Thjörsárver, designated 20/03/90, 37,500 ha, Montreux Record 04/07/90, removed 16/06/93\n\n13. India, Chilika Lake, designated 01/10/81, 116,500 ha, Montreux Record 16/06/93, removed 11/11/02\n\n14. Iran. Alagol, Ulmagol & Ajigol Lakes, designated 23/06/75, 1,400 ha, Montreux Record 16/06/93, removed 17/07/09 (PDF)\n\n15. Italy, Laguna di Orbetello, designated 14/12/76, 887 ha, Montreux Record 31/12/93, removed 21/03/00\n\n16. Italy, Palude della Diaccia Botrona, designated 22/05/91, 2,500 ha, Montreux Record 31/12/93, removed 21/03/00\n\n17. Italy, Stagno di Molentargius, designated 14/12/76, 1,401 ha, Montreux Record 04/07/90, removed 02/07/08 (PDF)\n\n18. Italy, Torre Guaceto, designated 21/07/81, 940 ha, Montreux Record 31/12/93, removed 21/03/00\n\n19. Italy, Stagno di Cagliari, designated 14/12/76, 3,466 ha, Montreux Record 04/07/90, removed 25/11/08 (PDF)\n\n20. Mauritania, Parc National du Diawling, designated 23/08/94, 15,600 ha, Montreux Record 28/02/02, removed 08/09/09\n\n21. Mexico, Ría Lagartos, designated 04/07/86, 47,840 ha, Montreux Record 04/07/90, removed 07/08/96\n\n22. Netherlands, Groote Peel, designated 23/05/80, 900 ha, Montreux Record 04/07/90, removed 16/06/93\n\n23. Poland, Jezioro Siedmiu Wysp, designated 03/01/84, Suwalki, Olsztyn, 999 ha, Montreux Record 04/07/90, removed 05/11/07 (PDF)\n\n24. Poland, Warta River Mouth (formerly Slonsk Reserve), designated 03/01/84, Gorzów, 4,235 ha, Montreux Record 16/06/93, removed 05/11/07 (PDF)\n\n25. Senegal, Djoudj, designated 11/07/77, 16,000 ha, Montreux Record 16/06/93, removed 08/09/09\n\n26. South Africa, St. Lucia System, designated 02/10/86, 155,500 ha, Montreux Record 04/07/90, removed 11/03/96\n\n27. Trinidad and Tobago, Nariva Swamp, designated 21/12/92, 6,234 ha, Montreux Record 16/06/93, removed 07/01/02\n\n28. Ukraine, Karkinitski & Dzharylgatska Bays, designated [23/11/95], 87,000 ha, Montreux Record 04/07/90, removed 29/08/03\n\n29. Ukraine, Tendrivska Bay, designated [23/11/95], 38,000 ha, Montreux Record 16/06/93, removed 29/08/03\n\n30. Ukraine, Yagorlytska Bay, designated [23/11/95], 34,000 ha, Montreux Record 16/06/93, removed 29/08/03\n\n31. United Kingdom, Bridgend Flats, Islay, designated 14/07/88, 331 ha, Montreux Record 04/07/90, removed 09/11/91\n\n32. Venezuela, Cuare, designated 23/11/88, 9,968 ha, Montreux Record 16/06/93, removed 07/08/96\n"}
{"id": "16271681", "url": "https://en.wikipedia.org/wiki?curid=16271681", "title": "Mount Tlaloc", "text": "Mount Tlaloc\n\nMount Tlaloc (Spanish: Monte Tláloc, sometimes wrongly listed as \"Cerro el Mirador\"; Nahuatl: \"Tlālōcatepētl\") is a mountain and archaeological site in central Mexico. It is located in the State of Mexico, in the municipalities of Ixtapaluca and Texcoco, close to the state border with Puebla. Formerly an active volcano, it has an official altitude of above sea level, thus being the 9th tallest mountain of Mexico.\n\nThe mountain was considered by the Nahuan peoples, foremost among them the Aztecs, to be specially sacred to the raingod Tlaloc. In fact, the mountain was believed to be one of his primary earthly dwelling places, called Tlalocan. Attribution of this and other mountains to the sacred presence of rain deities predates the Aztec era by centuries, even millennia. At the summit there are still remains of a shrine where high ceremonies would have been carried out. The rites of Tlaloc were otherwise performed at his temples, most famously that occupying one half of the Templo Mayor at the heart of the temple precinct of nearby Mexico-Tenochtitlan. The inherent analogy of temple pyramids to sacred mountains allows for the very likely possibility that the central temple of the Aztec capital, as such, was at least partly a symbolic representation of the actual mount Tlaloc, and that the summit shrine of the temple was itself an analogue to that atop the mountain.\n\nTogether with mount Telapón () and some other, lower peaks, mount Tlaloc forms the \"Sierra de Río Frío\", the northernmost tip of the Sierra Nevada. The mountain is easily accessible from Federal Highway 150 at the town of Río Frío de Juárez. The long, but easy and non-technical hike provides an elevation gain of over 1200 m. More strenuous routes depart from San Pablo Ixayoc and from other towns outside Texcoco.\n\nMount Tlaloc features an enclosed precinct on its summit which could be reached through by taking a pathway up the mountain and entering the western side of the enclosure. The structure of the precinct consisted largely of pumice and tufa, which were locally found and were easily molded due to their soft physicality.The precinct houses 5 rock formations, one at the center and 4 at the corners of the \"tetzacualo\", or courtyard. The center rock is thought to be analogous to the Codex Borgia which depicts Tlaloc standing in the center of his four rain forms (which are represented by the four directions). These 5 rock formations are representative of the Tlaloque, which are spirits that used vessels of water to distribute rain across the land. In addition, one side of the courtyard featured a construct that housed a statue of Tlaloc among other idols that represented nearby sacred regions. This precinct served a major role in allowing a setting at which supplication could be made to the Tlaloque for sustenance of crops and the people of Mesoamerica.\n\nThe Aztecs were known for their rituals that were undertaken in order to secure rain for the land. Extravagant ceremonies, plentiful offerings, and the lives of young Mesoamericans were all offered to Tlaloc in order to please him several times each year with an emphasis at the beginning of the rain season. One annual feast called Huey Tozoztli occurred atop Mount Tlaloc and coincided with the date of highest annual temperature, which usually occurred in April, right before the start of the rainy season. The rulers and elites of Tenochtitlan and nearby states, such as Xochimilco, Tlaxcala, and Tlacopan, were also cited to have joined the feast. Another ceremony that took place atop Mount Tlaloc was Atlachualo which was celebrated from mid-February to early March. This ceremony involved the sacrifice of children who were dressed \"like gods\" and taken to the mountain top and had their hearts removed for ceremonial purposes. The children were encouraged to cry because their tears symbolized abundant rains and if they did not cry on their own on the way to the precinct, their fingernails were cited to have been removed to incite tears. The main objective of these offerings were to please Tlaloc and the Tlaloque in order to ensure rainfall for the rainy season to come, hence why both of the above ceremonies occurred a few months before the rainy season of summer.\n\nDue to the irregularities of the damage done to the walls of the enclosure, researchers have claimed that the damage was likely as a result of human contact, rather than natural phenomena. The site was originally covered with pieces of green stone, shards of pottery, and obsidian blades. The archaeologists claim that the site maybe have been used up until the end of the Christian era. The modern shrine that appears is currently at the summit was likely built around the 1970s as an aerial view of the ritual site in 1956 does not show the current statue. \n\n\n"}
{"id": "38803848", "url": "https://en.wikipedia.org/wiki?curid=38803848", "title": "Open energy system models", "text": "Open energy system models\n\nOpen energy system models are energy system models that are open source. Similarly open energy system data employs open data methods to produce and distribute datasets primarily for use by open energy system models.\n\nEnergy system models are used to explore future energy systems and are often applied to questions involving energy and climate policy. The models themselves vary widely in terms of their type, design, programming, application, scope, level of detail, sophistication, and shortcomings. The open energy modeling projects listed here fall exclusively within the bottom-up paradigm, in which a model is a relatively literal representation of the underlying system. For many models, some form of mathematical optimization is used to inform the solution process.\n\nSeveral drivers favor the development of open models and open data. There is an increasing interest in making public policy energy models more transparent to improve their acceptance by policymakers and the public. There is also a desire to leverage the benefits that open data and open software development can bring, including reduced duplication of effort, better sharing of ideas and information, improved quality, and wider engagement and adoption. Model development is therefore usually a team effort and constituted as either an academic project, a commercial venture, or a genuinely inclusive community initiative.\n\nThis article does not cover projects which simply make their source code or spreadsheets available for public download, but which omit a recognized free and open-source software license. The absence of a license agreement creates a state of legal uncertainty whereby potential users cannot know which limitations the owner may want to enforce in the future. The projects listed here are deemed suitable for inclusion through having pending or published academic literature or by being reported in secondary sources.\n\nAn open energy system modeling project typically comprises a codebase, datasets, and software documentation and perhaps scientific publications. The project repository may be hosted on an institutional server or on a public code-hosting site, such as GitHub. Some projects release only their codebase, while others ship some or all of their datasets as well. Projects may also offer email lists, chat rooms, and web forums to aid collaboration.\n\nThe majority of projects are based within university research groups, either singingly or as academic collaborations.\n\nA 2017 paper lists the benefits of open data and models and discusses the reasons that many projects nonetheless remain closed. The paper makes a number of recommendations for projects wishing to transition to a more open approach. The authors also conclude that, in terms of openness, energy research has lagged behind other fields, most notably physics, biotechnology, and medicine.\n\nOpen energy system modeling came of age in the 2010s. Just two projects were cited in a 2011 paper on the topic: OSeMOSYS and TEMOA. Balmorel was also active at that time, having been made public in 2001. , this article lists 25 such undertakings (with a further six waiting to be ).\n\nThe use of open energy system models and open energy data represents one attempt to improve the transparency, comprehensibility, and reproducibility of energy system models, particularly those used to aid public policy development.\n\nA 2010 paper concerning energy efficiency modeling argues that \"an open peer review process can greatly support model verification and validation, which are essential for model development\". To further honor the process of peer review, researchers argue, in a 2012 paper, that it is essential to place both the source code and datasets under publicly accessible version control so that third-parties can run, verify, and scrutinize specific models. A 2016 paper contends that model-based energy scenario studies, seeking to influence decision-makers in government and industry, must become more comprehensible and more transparent. To these ends, the paper provides a checklist of transparency criteria that should be completed by modelers. The authors however state that they \"consider open source approaches to be an extreme case of transparency that does not automatically facilitate the comprehensibility of studies for policy advice.\"\n\nA one-page opinion piece from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for peer review.\n\nState-sponsored open source projects in any domain are a relatively new phenomena.\n\n, the European Commission now supports several open source energy system modeling projects to aid the transition to a low-carbon energy system for Europe. The Dispa-SET project (below) is modeling the European electricity system and hosts its codebase on GitHub. The MEDEAS project, which will design and implement a new open source energy-economy model for Europe, held its kick-off meeting in February 2016. , the project had yet to publish any source code. The established OSeMOSYS project (below) is developing a multi-sector energy model for Europe with Commission funding to support stakeholder outreach. The flagship model however remains closed source.\n\nThe United States NEMS national model is available but nonetheless difficult to use. NEMS does not classify as an open source project in the accepted sense.\n\nOpen electricity sector models are confined to just the electricity sector. These models invariably have a temporal resolution of one hour or less. Some models concentrate on the engineering characteristics of the system, including a good representation of high-voltage transmission networks and AC power flow. Others models depict electricity spot markets and are known as dispatch models. While other models embed autonomous agents to capture, for instance, bidding decisions using techniques from bounded rationality. The ability to handle variable renewable energy, transmission systems, and grid storage are becoming important considerations.\n\nDIETER stands for Dispatch and Investment Evaluation Tool with Endogenous Renewables. DIETER is a dispatch and investment model. It was first used to study the role of power storage and other flexibility options in a future greenfield setting with high shares of renewable generation. DIETER is being developed at the German Institute for Economic Research (DIW), Berlin, Germany. The codebase and datasets for Germany can be downloaded from the project website. The basic model is fully described in a DIW working paper and a journal article. DIETER is written in GAMS and was developed using the CPLEX commercial solver.\n\nDIETER is framed as a pure linear (no integer variables) cost minimization problem. In the initial formulation, the decision variables include the investment in and dispatch of generation, storage, and DSM capacities in the German wholesale and balancing electricity markets. Later model extensions include vehicle-to-grid interactions and prosumage of solar electricity.\n\nThe first study using DIETER examines the power storage requirements for renewables uptake ranging from 60% to 100%. Under the baseline scenario of 80% (the lower bound German government target for 2050), grid storage requirements remain moderate and other options on both the supply side and demand side offer flexibility at low cost. Nonetheless storage plays an important role in the provision of reserves. Storage becomes more pronounced under higher shares of renewables, but strongly depends on the costs and availability of other flexibility options, particularly biomass availability.\n\nUnder development at the European Commission's Joint Research Centre (JRC), Petten, the Netherlands, is a unit commitment and dispatch model intended primarily for Europe. It is written in Python (with Pyomo) and GAMS and uses Python for data processing. A valid GAMS license is required. The model is formulated as a mixed integer problem and JRC uses the proprietary CPLEX sover although open source libraries may also be deployed. Technical descriptions are available for versions2.0 and2.1. is hosted on GitHub, together with a trial dataset, and third-party contributions are encouraged. The codebase has been tested on Windows, macOS, and Linux. Online documentation is available.\n\nThe SET in the project name refers to the European Strategic Energy Technology Plan (SET-Plan), which seeks to make Europe a leader in energy technologies that can fulfill future (2020 and 2050) energy and climate targets. Energy system modeling, in various forms, is central to this European Commission initiative.\n\nThe model power system is managed by a single operator with full knowledge of the economic and technical characteristics of the generation units, the loads at each node, and the heavily simplified transmission network. Demand is deemed fully inelastic. The system is subject to intra-period and inter-period unit commitment constraints (the latter covering nuclear and thermal generation for the most part) and operated under economic dispatch. Hourly data is used and the simulation horizon is normally one year. But to ensure the model remains tractable, two day rolling horizon optimization is employed. The model advances in steps of one day, optimizing the next 48hours ahead but retaining results for just the first 24hours.\n\nTwo related publications describe the role and representation of flexibility measures within power systems facing ever greater shares of variable renewable energy (VRE). These flexibility measures comprise: dispatchable generation (with constraints on efficiency, ramp rate, part load, and up and down times), conventional storage (predominantly pumped-storage hydro), cross-border interconnectors, demand side management, renewables curtailment, last resort load shedding, and nascent power-to-X solutions (with X being gas, heat, or mobility). The modeler can set a target for renewables and place caps on and other pollutants. Planned extensions to the software include support for simplified AC power flow (transmission is currently treated as a transportation problem), new constraints (like cooling water supply), stochastic scenarios, and the inclusion of markets for ancillaryservices.\n\nEMLab-Generation is an agent-based model covering two interconnected electricity markets – be they two adjoining countries or two groups of countries. The software is being developed at the Energy Modelling Lab, Delft University of Technology, Delft, the Netherlands. A factsheet is available. And software documentation is available. EMLab-Generation is written in Java.\n\nEMLab-Generation simulates the actions of power companies investing in generation capacity and uses this to explore the long-term effects of various energy and climate protection policies. These policies may target renewable generation, emissions, security of supply, and/or energy affordability. The power companies are the main agents: they bid into power markets and they invest based on the net present value (NPV) of prospective power plant projects. They can adopt a variety of technologies, using scenarios from the 2011 IEA World Energy Outlook. The agent-based methodology enables different sets of assumptions to be tested, such as the heterogeneity of actors, the consequences of imperfect expectations, and the behavior of investors outside of ideal conditions.\n\nEMLab-Generation offers a new way of modeling the effects of public policy on electricity markets. It can provide insights into actor and system behaviors over time – including such things as investment cycles, abatement cycles, delayed responses, and the effects of uncertainty and risk on investment decisions.\n\nA 2014 study using EMLab-Generation investigates the effects of introducing floor and ceiling prices for under the EU ETS. And in particular, their influence on the dynamic investment pathway of two interlinked electricity markets (loosely Great Britain and Central Western Europe). The study finds a common, moderate auction reserve price results in a more continuous decarbonisation pathway and reduces price volatility. Adding a ceiling price can shield consumers from extreme price shocks. Such price restrictions should not lead to an overshoot of emissions targets in the long-run.\n\nEMMA is the European Electricity Market Model. It is a techno-economic model covering the integrated Northwestern European power system. EMMA is being developed by the energy economics consultancy Neon Neue Energieökonomik, Berlin, Germany. The source code and datasets can be downloaded from the project website. A manual is available. EMMA is written in GAMS and uses the CPLEX commercial solver.\n\nEMMA models electricity dispatch and investment, minimizing the total cost with respect to investment, generation, and trades between market areas. In economic terms, EMMA classifies as a partial equilibrium model of the wholesale electricity market with a focus on the supply-side. EMMA identifies short-term or long-term optima (or equilibria) and estimates the corresponding capacity mix, hourly prices, dispatch, and cross-border trading. Technically, EMMA is a pure linear program (no integer variables) with about two million variables. , the model covers Belgium, France, Germany, the Netherlands, and Poland and supports conventional generation, renewable generation, and cogeneration.\n\nEMMA has been used to study the economic effects of the increasing penetration of variable renewable energy (VRE), specifically solar power and wind power, in the Northwestern European power system. A 2013 study finds that increasing VRE shares will depress prices and, as a consequence, the competitive large-scale deployment of renewable generation will be more difficult to accomplish than many anticipate. A 2015 study estimates the welfare-optimal market share for wind and solar power. For wind, this is 20%, three-fold more than at present.\n\nAn independent 2015 study reviews the EMMA model and comments on the high assumed specific costs for renewable investments.\n\nGENESYS stands for Genetic Optimisation of a European Energy Supply System. The software is being developed jointly by the Institute of Power Systems and Power Economics (IAEW) and the Institute for Power Electronics and Electrical Drives (ISEA), both of RWTH Aachen University, Aachen, Germany. The project maintains a website where potential users can request access to the codebase and the dataset for the 2050 base scenario only. Detailed descriptions of the software are available. GENESYS is written in C++ and uses Boost libraries, the MySQL relational database, the Qt4 application framework, and optionally the CPLEX solver.\n\nThe GENESYS simulation tool is designed to optimize a future EUMENA (Europe, Middle East, and North Africa) power system and assumes a high share of renewable generation. It is able to find an economically optimal distribution of generator, storage, and transmission capacities within a 21region EUMENA. It allows for the optimization of this energy system in combination with an evolutionary method. The optimization is based on a covariance matrix adaptation evolution strategy (CMA-ES), while the operation is simulated as a hierarchical set-up of system elements which balance the load between the various regions at minimum cost using the network simplex algorithm. GENESYS ships with a set of input time series and a set of parameters for the year 2050, which the user can modify.\n\nA future EUMENA energy supply system with a high share of renewable energy sources (RES) will need a strongly interconnected energy transport grid and significant energy storage capacities. GENESYS was used to dimension the storage and transmission between the 21different regions. Under the assumption of 100% self-supply, about of RES in total and a storage capacity of about are needed, corresponding to 6% of the annual energy demand, and a HVDC transmission grid of . The combined cost estimate for generation, storage, and transmission, excluding distribution, is 6.87¢/kWh.\n\nA 2016 study looked at the relationship between storage and transmission capacity under high shares of renewable energy sources (RES) in an EUMENA power system. It found that, up to a certain extent, transmission capacity and storage capacity can substitute for each other. For a transition to a fully renewable energy system by 2050, major structural changes are required. The results indicate the optimal allocation of photovoltaics and wind power, the resulting demand for storage capacities of different technologies (battery, pumped hydro, and hydrogen storage) and the capacity of the transmission grid.\n\nNEMO, the National Electricity Market Optimiser, is a chronological dispatch model for testing and optimizing different portfolios of conventional and renewable electricity generation technologies. It applies solely to the Australian National Electricity Market (NEM), which, despite its name, is limited to east and south Australia. NEMO has been in development at the Centre for Energy and Environmental Markets (CEEM), University of New South Wales (UNSW), Sydney, Australia since 2011. The project maintains a small website and runs an email list. NEMO is written in Python. NEMO itself is described in two publications. The data sources are also noted. Optimizations are carried out using a single-objective evaluation function, with penalties. The solution space of generator capacities is searched using the CMA-ES (covariance matrix adaptation evolution strategy) algorithm. The timestep is arbitrary but one hour is normally employed.\n\nNEMO has been used to explore generation options for the year 2030 under a variety of renewable energy (RE) and abated fossil fuel technology scenarios. A 2012 study investigates the feasibility of a fully renewable system using concentrated solar power (CSP) with thermal storage, windfarms, photovoltaics, existing hydroelectricity, and biofuelled gas turbines. A number of potential systems, which also meet NEM reliability criteria, are identified. The principal challenge is servicing peak demand on winter evenings following overcast days and periods of low wind. A 2014 study investigates three scenarios using coal-fired thermal generation with carbon capture and storage (CCS) and gas-fired gas turbines with and without capture. These scenarios are compared to the 2012 analysis using fully renewable generation. The study finds that \"only under a few, and seemingly unlikely, combinations of costs can any of the fossil fuel scenarios compete economically with 100% renewable electricity in a carbon constrained world\". A 2016 study evaluates the incremental costs of increasing renewable energy shares under a range of greenhouse gas caps and carbon prices. The study finds that incremental costs increase linearly from zero to 80% RE and then escalate moderately. The study concludes that this cost escalation is not a sufficient reason to avoid renewables targets of 100%.\n\nOnSSET is the OpeN Source Spatial Electrification Toolkit. OnSSET is being developed by the Energy Systems Analysis Group (dESA), KTH Royal Institute of Technology, Stockholm, Sweden. The software is used to examine areas not served by grid-based electricity and identify the technology options and investment requirements that will provide least-cost access to electricity services. OnSSET is designed to support the United Nations' SDG7: the provision of affordable, reliable, sustainable, and modern energy for all. The Python implementation of the toolkit is known as PyOnSSET and was released on 26November 2016. PyOnSSET does not ship with data, but suitable datasets are available from energydata.info. The project maintains a website and hosts a forum on Reddit.\n\nOnSSET can estimate, analyze, and visualize the most cost-effective electrification access options, be they conventional grid, mini-grid, or stand-alone. The toolkit supports a range of conventional and renewable energy technologies, including photovoltaics, wind turbines, and small hydro generation. , bioenergy and hybrid technologies, such as wind-diesel, are being added.\n\nOnSSET utilizes energy and geographic information, the latter may include settlement size and location, existing and planned transmission and generation infrastructure, economic activity, renewable energy resources, roading networks, and nighttime lighting needs. The GIS information can be supported using the proprietary ArcGIS package or an open source equivalent such as GRASS or QGIS.\n\nOnSSET has been used for case studies in Afghanistan, Bolivia, Ethiopia, Nigeria, and Tanzania. OnSSET has also been applied in India, Kenya, and Zimbabwe. In addition, continental studies have been carried out for Sub-Saharan Africa and Latin America. , there are plans to apply OnSSET in developing Asia, to increase the resolution of the analysis, and to extend support for various productive uses of electricity.\n\nOnSSET results have contributed to the IEA \"World Energy Outlook\" reports for 2014 and 2015 and the World Bank Global Tracking Framework report in 2015.\n\npandapower is a power system analysis and optimization program being jointly developed by the Energy Management and Power System Operation research group, University of Kassel and the Department for Distribution System Operation, Fraunhofer Institute for Energy Economics and Energy System Technology (IEE), both of Kassel, Germany. The codebase is hosted on GitHub and is also available as a package. The project maintains a website, an emailing list, and online documentation. pandapower is written in Python. It uses the pandas library for data manipulation and analysis and the PYPOWER library to solve for power flow. Unlike some open source power system tools, pandapower does not depend on proprietary platforms like MATLAB.\n\npandapower supports the automated analysis and optimization of distribution and transmission networks. This allows a large of number of scenarios to be explored, based on different future grid configurations and technologies. pandapower offers a collection of power system elements, including: lines, 2-winding transformers, 3-winding transformers, and ward-equivalents. It also contains a switch model that allows the modeling of ideal bus-bus switches as well as bus-line/bus-trafo switches. The software supports topological searching. The network itself can be plotted, with or without geographical information, using the matplotlib and plotly libraries.\n\nA 2016 publication evaluates the usefulness of the software by undertaking several case studies with major distribution system operators (DSO). These studies examine the integration of increasing levels of photovoltaics into existing distribution grids. The study concludes that being able to test a large number of detailed scenarios is essential for robust grid planning. Notwithstanding, issues of data availability and problem dimensionality will continue to present challenges.\n\nA 2018 paper describes the package and its design and provides an example case study. The article explains how users work with an element-based model (EBM) which is converted internally to a bus-branch model (BBM) for computation. The package supports power system simulation, optimal power flow calculations (cost information is required), state estimation (should the system characterization lacks fidelity), and graph-based network analysis. The case study shows how a few tens of lines of scripting can interface with pandapower to advance the design of a system subject to diverse operating requirements. The associated code is hosted on GitHub as jupyter notebooks.\n\n, BNetzA, the German network regulator, is using pandapower for automated grid analysis. Energy research institutes in Germany are also following the development of pandapower.\n\nThe PowerMatcher software implements a smart grid coordination mechanism which balances distributed energy resources (DER) and flexible loads through autonomous bidding. The project is managed by the Flexiblepower Alliance Network (FAN) in Amsterdam, the Netherlands. The project maintains a website and the source code is hosted on GitHub. , existing datasets are not available. PowerMatcher is written in Java.\n\nEach device in the smart grid system – whether a washing machine, a wind generator, or an industrial turbine – expresses its willingness to consume or produce electricity in the form of a bid. These bids are then collected and used to determine an equilibrium price. The PowerMatcher software thereby allows high shares of renewable energy to be integrated into existing electricity systems and should also avoid any local overloading in possibly aging distribution networks.\n\nrenpass is an acronym for Renewable Energy Pathways Simulation System. renpass is a simulation electricity model with high regional and temporal resolution, designed to capture existing systems and future systems with up to 100% renewable generation. The software is being developed by the Centre for Sustainable Energy Systems (CSES or ZNES), University of Flensburg, Germany. The project runs a website, from where the codebase can be download. renpass is written in R and links to a MySQL database. A PDF manual is available. renpass is also described in a PhD thesis. , renpass is being extended as renpassG!S, based on oemof.\n\nrenpass is an electricity dispatch model which minimizes system costs for each time step (optimization) within the limits of a given infrastructure (simulation). Time steps are optionally 15 minutes or one hour. The method assumes perfect foresight. renpass supports the electricity systems found in Austria, Belgium, the Czech Republic, Denmark, Estonia, France, Finland, Germany, Latvia, Lithuania, Luxembourg, the Netherlands, Norway, Poland, Sweden, and Switzerland.\n\nThe optimization problem for each time step is to minimize the electricity supply cost using the existing power plant fleet for all regions. After this regional dispatch, the exchange between the regions is carried out and is restricted by the grid capacity. This latter problem is solved with a heuristic procedure rather than calculated deterministically. The input is the merit order, the marginal power plant, the excess energy (renewable energy that could be curtailed), and the excess demand (the demand that cannot be supplied) for each region. The exchange algorithm seeks the least cost for all regions, thus the target function is to minimize the total costs of all regions, given the existing grid infrastructure, storage, and generating capacities. The total cost is defined as the residual load multiplied by the price in each region, summed over all regions.\n\nA 2012 study uses renpass to examine the feasibility of a 100% renewable electricity system for the Baltic Sea region (Denmark, Estonia, Finland, Germany, Latvia, Lithuania, Poland, and Sweden) in the year 2050. The base scenario presumes conservative renewable potentials and grid enhancements, a 20% drop in demand, a moderate uptake of storage options, and the deployment of biomass for flexible generation. The study finds that a 100% renewable electricity system is possible, albeit with occasional imports from abutting countries, and that biomass plays a key role in system stability. The costs for this transition are estimated at 50€/MWh. A 2014 study uses renpass to model Germany and its neighbors. A 2014 thesis uses renpass to examine the benefits of both a new cable between Germany and Norway and new pumped storage capacity in Norway, given 100% renewable electricity systems in both countries. Another 2014 study uses renpass to examine the German \"Energiewende\", the transition to a sustainable energy system for Germany. The study also argues that the public trust needed to underpin such a transition can only be built through the use of transparent open source energy models.\n\nSciGRID, short for Scientific Grid, is an open source model of the German and European electricity transmission networks. The research project is managed by Next Energy (officially the EWE Research Centre for Energy Technology) located at the University of Oldenburg, Oldenburg, Germany. The project maintains a website and an email newsletter. SciGRID is written in Python and uses a PostgreSQL database. The first release (v0.1) was made on 15June 2015.\n\nSciGRID aims to rectify the lack of open research data on the structure of electricity transmission networks within Europe. This lack of data frustrates attempts to build, characterise, and compare high resolution energy system models. SciGRID utilizes transmission network data available from the OpenStreetMap project, available under the Open Database License (ODbL), to automatically author transmission connections. SciGRID will not use data from closed sources. SciGRID can also mathematically decompose a given network into a simpler representation for use in energy models.\n\nA related project is GridKit, released under an MIT license. GridKit is being developed to investigate the possibility of a 'heuristic' analysis to augment the route-based analysis used in SciGRID. Data is available for network models of the European and North-American high-voltage electricity grids.\n\nSIREN stands for SEN Integrated Renewable Energy Network Toolkit. The project is run by Sustainable Energy Now, an NGO based in Perth, Australia. The project maintains a website. SIREN runs on Windows and the source code is hosted on SourceForge. The software is written in Python and uses the SAM model (System Advisor Model) from the US National Renewable Energy Laboratory to perform energy calculations. SIREN uses hourly datasets to model a given geographic region. Users can use the software to explore the location and scale of renewable energy sources to meet a specified electricity demand. SIREN utilizes a number of open or publicly available data sources: maps can be created from OpenStreetMap tiles and weather datasets can be created using NASA MERRA-2 satellite data.\n\nA 2016 study using SIREN to analyze Western Australia's South-West Interconnected System (SWIS) finds that it can transition to 85% renewable energy (RE) for the same cost as new coal and gas. In addition, 11.1million tonnes of eq emissions would be avoided. The modeling assumes a carbon price of AUD$30/t. Further scenarios examine the goal of 100% renewable generation.\n\nSWITCH is a loose acronym for solar, wind, conventional and hydroelectric generation, and transmission. SWITCH is an optimal planning model for power systems with large shares of renewable energy. SWITCH is being developed by the Department of Electrical Engineering, University of Hawai'i, Mānoa, Hawaii, USA. The project runs a small website and hosts its codebase and datasets on GitHub. SWITCH is written in Pyomo, an optimization components library programmed in Python. It can use either the open source GLPK solver or the commercial CPLEX and Gurobi solvers.\n\nSWITCH is a power system model, focused on renewables integration. It can identify which generator and transmission projects to build in order to satisfy electricity demand at the lowest cost over a several year period while also reducing emissions. SWITCH utilizes multi-stage stochastic linear optimization with the objective of minimizing the present value of the cost of power plants, transmission capacity, fuel usage, and an arbitrary per-tonne charge (to represent either a carbon tax or a certificate price), over the course of a multi-year investment period. It has two major sets of decision variables. First, at the start of each investment period, SWITCH selects how much generation capacity to build in each of several geographic load zones, how much power transfer capability to add between these zones, and whether to operate existing generation capacity during the investment period or to temporarily mothball it to avoid fixed operation and maintenance costs. Second, for a set of sample days within each investment period, SWITCH makes hourly decisions about how much power to generate from each dispatchable power plant, store at each pumped hydro facility, or transfer along each transmission interconnector. The system must also ensure enough generation and transmission capacity to provide a planning reserve margin of 15% above the load forecasts. For each sampled hour, SWITCH uses electricity demand and renewable power production based on actual measurements, so that the weather-driven correlations between these elements remain intact.\n\nFollowing the optimization phase, SWITCH is used in a second phase to test the proposed investment plan against a more complete set of weather conditions and to add backstop generation capacity so that the planning reserve margin is always met. Finally, in a third phase, the costs are calculated by freezing the investment plan and operating the proposed power system over a full set of weather conditions.\n\nA 2012 paper uses California from 2012 to 2027 as a case study for SWITCH. The study finds that there is no ceiling on the amount of wind and solar power that could be used and that these resources could potentially reduce emissions by 90% or more (relative to 1990 levels) without reducing reliability or severely raising costs. Furthermore, policies that encourage electricity customers to shift demand to times when renewable power is most abundant (for example, though the well-timed charging of electric vehicles) could achieve radical emission reductions at moderate cost.\n\nURBS, Latin for city, is a linear programming model for exploring capacity expansion and unit commitment problems and is particularly suited to distributed energy systems (DES). It is being developed by the Institute for Renewable and Sustainable Energy Systems, Technical University of Munich, Germany. The codebase is hosted on GitHub. URBS is written in Python and uses the Pyomo optimization packages.\n\nURBS classes as an energy modeling framework and attempts to minimize the total discounted cost of the system. A particular model selects from a set of technologies to meet a predetermined electricity demand. It uses a time resolution of one hour and the spatial resolution is model-defined. The decision variables are the capacities for the production, storage, and transport of electricity and the time scheduling for their operation.\n\nThe software has been used to explore cost-optimal extensions to the European transmission grid using projected wind and solar capacities for 2020. A 2012 study, using high spatial and technological resolutions, found variable renewable energy (VRE) additions cause lower revenues for conventional power plants and that grid extensions redistribute and alleviate this effect. The software has also been used to explore energy systems spanning Europe, the Middle East, and North Africa (EUMENA) and Indonesia, Malaysia, and Singapore.\n\nOpen energy system models capture some or all of the energy commodities found in an energy system. All models include the electricity sector. Some models add the heat sector, which can be important for countries with significant district heating. Other models add gas networks. With the advent of emobility, other models still include aspects of the transport sector. Indeed, coupling these various sectors using power-to-X technologies is an emerging area of research.\n\nBalmorel is a market-based energy system model from Denmark. Development was originally financed by the Danish Energy Research Program in 2001. The codebase was made public in March 2001. The Balmorel project maintains an extensive website, from where the codebase and datasets can be download as a zip file. Users are encouraged to register. Documentation is available from the same site. Balmorel is written in GAMS.\n\nThe original aim of the Balmorel project was to construct a partial equilibrium model of the electricity and CHP sectors in the Baltic Sea region, for the purposes of policy analysis. These ambitions and limitations have long since been superseded and Balmorel is no longer tied to its original geography and policy questions. Balmorel classes as a dispatch and investment model and uses a time resolution of one hour. It models electricity and heat supply and demand, and supports the intertemporal storage of both. Balmorel is structured as a pure linear program (no integer variables).\n\n, Balmorel has been the subject of some 22publications. A 2008 study uses Balmorel to explore the Nordic energy system in 2050. The focus is on renewable energy supply and the deployment of hydrogen as the main transport fuel. Given certain assumptions about the future price of oil and carbon and the uptake of hydrogen, the model shows that it is economically optimal to cover, using renewable energy, more than 95% of the primary energy consumption for electricity and district heat and 65% of the transport. A 2010 study uses Balmorel to examine the integration of plug-in hybrid vehicles (PHEV) into a system comprising one quarter wind power and three quarters thermal generation. The study shows that PHEVs can reduce the emissions from the power system if actively integrated, whereas a hands-off approach – letting people charge their cars at will – is likely to result in an increase in emissions. A 2013 study uses Balmorel to examine cost-optimized wind power investments in the Nordic-Germany region. The study investigates the best placement of wind farms, taking into account wind conditions, distance to load, and the generation and transmission infrastructure already in place.\n\nCalliope is an energy system modeling framework, with a focus on flexibility, high spatial and temporal resolution, and the ability to execute different runs using the same base-case dataset. The project is being developed at the Department of Environmental Systems Science, ETH Zurich, Zürich, Switzerland. The project maintains a website, hosts the codebase at GitHub, operates an issues tracker, and runs two email lists. Calliope is written in Python and uses the Pyomo library. It can link to the open source GLPK solver and the commercial CPLEX and Gurobi solvers. PDF documentation is available.\n\nA Calliope model consists of a collection of structured text files, in YAML and CSV formats, that define the technologies, locations, and resource potentials. Calliope takes these files, constructs a pure linear optimization (no integer variables) problem, solves it, and reports the results in the form of pandas data structures for analysis. The framework contains five abstract base technologies – supply, demand, conversion, storage, transmission – from which new concrete technologies can be derived. The design of Calliope enforces the clear separation of framework (code) and model (data).\n\nA 2015 study uses Calliope to compare the future roles of nuclear power and CSP in South Africa. It finds CSP could be competitive with nuclear by 2030 for baseload and more competitive when producing above baseload. CSP also offers less investment risk, less environmental risk, and other co-benefits. A second 2015 study compares a large number of cost-optimal future power systems for Great Britain. Three generation technologies are tested: renewables, nuclear power, and fossil fuels with and without carbon capture and storage (CCS). The scenarios are assessed on financial cost, emissions reductions, and energy security. Up to 60% of variable renewable capacity is possible with little increase in cost, while higher shares require large-scale storage, imports, and/or dispatchable renewables such as tidal range.\n\nDESSTinEE stands for Demand for Energy Services, Supply and Transmission in EuropE. DESSTinEE is a model of the European energy system in 2050 with a focus on the electricity system. DESSTinEE is being developed primarily at the Imperial College Business School, Imperial College London (ICL), London, United Kingdom. The software can be downloaded from the project website. DESSTinEE is written in Excel/VBA and comprises a set of standalone spreadsheets. A flier is available.\n\nDESSTinEE is designed to investigate assumptions about the technical requirements for energy transport – particularly electricity – and the scale of the economic challenge to develop the necessary infrastructure. Forty countries are considered in and around Europe and ten forms of primary and secondary energy are supported. The model uses a predictive simulation technique, rather than solving for either partial or general equilibrium. The model projects annual energy demands for each country to 2050, synthesizes hourly profiles for electricity demand in 2010 and 2050, and simulates the least-cost generation and transmission of electricity around the region.\n\nA 2016 study using DESSTinEE (and a second model eLOAD) examines the evolution of electricity load curves in Germany and Britain from the present until 2050. In 2050, peak loads and ramp rates rise 20–60% and system utilization falls 15–20%, in part due to the substantial uptake of heat pumps and electric vehicles. These are significant changes.\n\nThe Energy Transition Model (ETM) is an interactive web-based model using a holistic description of a country's energy system. It is being developed by Quintel Intelligence, Amsterdam, the Netherlands. The project maintains a project website, an interactive website, and a GitHub repository. ETM is written in Ruby (on Rails) and displays in a web browser. ETM consists of several software components as described in the documentation.\n\nETM is fully interactive. After selecting a region (France, Germany, the Netherlands, Poland, Spain, United Kingdom, EU-27, or Brazil) and a year (2020, 2030, 2040, or 2050), the user can set 300 sliders (or enter numerical values) to explore the following:\n\n\nETM is based on an energy graph (digraph) where nodes (vertices) can convert from one type of energy to another, possibly with losses. The connections (directed edges) are the energy flows and are characterized by volume (in megajoules) and carrier type (such as coal, electricity, usable-heat, and so forth). Given a demand and other choices, ETM calculates the primary energy use, the total cost, and the resulting emissions. The model is demand driven, meaning that the digraph is traversed from \"useful demand\" (such as space heating, hot water usage, and car-kilometers) to \"primary demand\" (the extraction of gas, the import of coal, and so forth).\n\nEnergyPATHWAYS is a bottom-up energy sector model used to explore the near-term implications of long-term deep decarbonization. The lead developer is energy and climate protection consultancy, Evolved Energy Research, San Francisco, USA. The code is hosted on GitHub. EnergyPATHWAYS is written in Python and links to the open source Cbc solver. Alternatively, the GLPK, CPLEX, or Gurobi solvers can be employed. EnergyPATHWAYS utilizes the PostgreSQL object-relational database management system (ORDBMS) to manage its data.\n\nEnergyPATHWAYS is a comprehensive accounting framework used to construct economy-wide energy infrastructure scenarios. While portions of the model do use linear programming techniques, for instance, for electricity dispatch, the EnergyPATHWAYS model is not fundamentally an optimization model and embeds few decision dynamics. EnergyPATHWAYS offers detailed energy, cost, and emissions accounting for the energy flows from primary supply to final demand. The energy system representation is flexible, allowing for differing levels of detail and the nesting of cities, states, and countries. The model uses hourly least-cost electricity dispatch and supports power-to-gas, short-duration energy storage, long-duration energy storage, and demand response. Scenarios typically run to 2050.\n\nA predecessor of the EnergyPATHWAYS software, named simply PATHWAYS, has been used to construct policy models. The California PATHWAYS model was used to inform Californian state climate targets for 2030. And the US PATHWAYS model contributed to the UN Deep Decarbonization Pathways Project (DDPP) assessments for the United States. , the DDPP plans to employ EnergyPATHWAYS for future analysis.\n\nETEM stands for Energy Technology Environment Model. The ETEM model offers a similar structure to OSeMOSYS but is aimed at urban planning. The software is being developed by the ORDECSYS company, Chêne-Bougeries, Switzerland, supported with European Union and national research grants. The project has two websites. The software can be downloaded from first of these websites (but , this looks out of date). A manual is available with the software. ETEM is written in MathProg. Presentations describing ETEM are available.\n\nETEM is a bottom-up model that identifies the optimal energy and technology options for a regional or city. The model finds an energy policy with minimal cost, while investing in new equipment (new technologies), developing production capacity (installed technologies), and/or proposing the feasible import/export of primary energy. ETEM typically casts forward 50years, in two or five year steps, with time slices of four seasons using typically individual days or finer. The spatial resolution can be highly detailed. Electricity and heat are both supported, as are district heating networks, household energy systems, and grid storage, including the use of plug-in hybrid electric vehicles (PHEV). ETEM-SG, a development, supports demand response, an option which would be enabled by the development of smart grids.\n\nThe ETEM model has been applied to Luxembourg, the Geneva and Basel-Bern-Zurich cantons in Switzerland, and the Grenoble metropolitan and Midi-Pyrénées region in France. A 2005 study uses ETEM to study climate protection in the Swiss housing sector. The ETEM model was coupled with the GEMINI-E3 world computable general equilibrium model (CGEM) to complete the analysis. A 2012 study examines the design of smart grids. As distribution systems become more intelligent, so must the models needed to analysis them. ETEM is used to assess the potential of smart grid technologies using a case study, roughly calibrated on the Geneva canton, under three scenarios. These scenarios apply different constraints on emissions and electricity imports. A stochastic approach is used to deal with the uncertainty in future electricity prices and the uptake of electric vehicles.\n\nficus is a mixed integer optimization model for local energy systems. It is being developed at the Institute for Energy Economy and Application Technology, Technical University of Munich, Munich, Germany. The project maintains a website. The project is hosted on GitHub. ficus is written in Python and uses the Pyomo library. The user can choose between the open source GLPK solver or the commercial CPLEX and Gurobi solvers.\n\nBased on URBS, ficus was originally developed for optimizing the energy systems of factories and has now been extended to include local energy systems. ficus supports multiple energy commodities – goods that can be imported or exported, generated, stored, or consumed – including electricity and heat. It supports multiple-input and multiple-output energy conversion technologies with load-dependent efficiencies. The objective of the model is to supply the given demand at minimal cost. ficus uses exogenous cost time series for imported commodities as well as peak demand charges with a configurable timebase for each commodity in use.\n\noemof stands for Open Energy Modelling Framework. The project is managed by the Reiner Lemoine Institute, Berlin, Germany and the Center for Sustainable Energy Systems (CSES or ZNES) at the University of Flensburg and the Flensburg University of Applied Sciences, both Flensburg, Germany. The project runs two websites and a GitHub repository. oemof is written in Python and uses Pyomo and COIN-OR components for optimization. Energy systems can be represented using spreadsheets (CSV) which should simplify data preparation. was released on 1December 2016.\n\noemof classes as an energy modeling framework. It consists of a linear or mixed integer optimization problem formulation library (solph), an input data generation library (feedin-data), and other auxiliary libraries. The solph library is used to represent multi-regional and multi-sectoral (electricity, heat, gas, mobility) systems and can optimize for different targets, such as financial cost or emissions. Furthermore, it is possible to switch between dispatch and investment modes. In terms of scope, oemof can capture the European power system or alternatively it can describe a complex local power and heat sector scheme.\n\nOSeMOSYS stands for Open Source Energy Modelling System. OSeMOSYS is intended for national and regional policy development and uses an intertemperal optimization framework. The model posits a single socially motivated operator/investor with perfect foresight. The OSeMOSYS project is a community endeavor, supported by the Energy Systems Analysis Group (dESA), KTH Royal Institute of Technology, Stockholm, Sweden. The project maintains a website providing background. The project also offers several active internet forums on Reddit. OSeMOSYS was originally written in MathProg, a high-level mathematical programming language. It was subsequently reimplemented in GAMS and Python and all three codebases are now maintained. The project also provides a test model called UTOPIA. A manual is available.\n\nOSeMOSYS provides a framework for the analysis of energy systems over the medium (10–15 years) and long term (50–100 years). OSeMOSYS uses pure linear optimization, with the option of mixed integer programming for the treatment of, for instance, discrete power plant capacity expansions. It covers most energy sectors, including heat, electricity, and transport. OSeMOSYS is driven by exogenously defined energy services demands. These are then met through a set of technologies which draw on a set of resources, both characterized by their potentials and costs. These resources are not limited to energy commodities and may include, for example, water and land-use. This enables OSeMOSYS to be applied in domains other than energy, such as water systems. Technical constraints, economic restrictions, and/or environmental targets may also be imposed to reflect policy considerations. OSeMOSYS is available in extended and compact MathProg formulations, either of which should give identical results. In its extended version, OSeMOSYS comprises a little more than 400 lines of code.\n\nA key paper describing OSeMOSYS is available. A 2011 study uses OSeMOSYS to investigate the role of household investment decisions. A 2012 study extends OSeMOSYS to capture the salient features of a smart grid. The paper explains how to model variability in generation, flexible demand, and grid storage and how these impact on the stability of the grid. OSeMOSYS has been applied to village systems. A 2015 paper compares the merits of stand-alone, mini-grid, and grid electrification for rural areas in Timor-Leste under differing levels of access. In a 2016 study, OSeMOSYS is modified to take into account realistic consumer behavior. Another 2016 study uses OSeMOSYS to build a local multi-regional energy system model of the Lombardy region in Italy. One of the aims of the exercise was to encourage citizens to participate in the energy planning process. Preliminary results indicate that this was successful and that open modeling is needed to properly include both the technological dynamics and the non-technological issues. A 2017 paper covering Alberta, Canada factors in the risk of overrunning specified emissions targets because of technological uncertainty. Among other results, the paper finds that solar and wind technologies are built out seven and five years earlier respectively when emissions risks are included. Another 2017 paper analyses the electricity system in Cyprus and finds that, after European Union environmental regulations are applied post-2020, a switch from oil-fired to natural gas generation is indicated.\n\nOSeMOSYS has been used to construct wide-area electricity models for Africa, comprising 45countries and South America, comprising 13countries. It has also been used to support United Nations' regional climate, land, energy, and water strategies (CLEWS) for the Sava river basin, central Europe, the Syr Darya river basin, eastern Europe, and Mauritius. Models have previously been built for the Baltic States, Bolivia, Nicaragua, and Sweden.\n\nIn 2016, work started on a browser-based interface to OSeMOSYS, known as the Model Management Infrastructure (MoManI). Lead by the UN Department of Economic and Social Affairs (DESA), MoManI is being trialled in selected countries. The interface can be used to construct models, visualize results, and develop better scenarios. Atlantis is the name of a fictional country case-study for training purposes.\n\nThe OSeMBE reference model covering western and central Europe was announced on 27 April 2018. The model uses the MathProg implemention of OSeMOSYS but requires a small patch first. The model, funded as part of Horizon 2020 and falling under work package WP7 of the REEEM project, will be used to help stakeholders engage with a range of sustainable energy futures for Europe. The REEEM project runs from early-2016 till mid-2020.\n\nOSeMOSYS is used for university teaching. To that end, a 2017 paper describes the basic UTOPIA model, with an explanation on how to generate Pareto frontiers for a given system.\n\nPyPSA stands for Python for Power System Analysis. PyPSA is a free software toolbox for simulating and optimizing electric power systems and allied sectors. It supports conventional generation, variable wind and solar generation, electricity storage, coupling to the natural gas, hydrogen, heat, and transport sectors, and hybrid alternating and direct current networks. Moreover, PyPSA is designed to scale well. The project is managed by the Institute for Automation and Applied Informatics (IAI), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany, although the project itself exists independently under its own name and accounts. The project maintains a website and runs an email list. PyPSA itself is written in Python and uses the Pyomo library. The source code is hosted on GitHub and is also released periodically as a PyPI package.\n\nThe basic functionality of PyPSA is described in a 2018 paper. PyPSA sits between traditional steady-state power flow analysis software and full multi-period energy system models. It can be invoked using either non-linear power flow equations for system simulation or linearized approximations to enable the joint optimization of operations and investment across multiple periods. Generator ramping and multi-period up and down-times can be specified, DSM is supported, but demand remains price inelastic.\n\nA 2018 study examines potential synergies between sector coupling and transmission reinforcement in a future European energy system constrained to reduce carbon emissions by 95%. The PyPSA-Eur-Sec-30 model captures the demand-side management potential of battery electric vehicles (BEV) as well as the role that power-to-gas, long-term thermal energy storage, and related technologies can play. Results indicate that BEVs can smooth the daily variations in solar power while the remaining technologies smooth the synoptic and seasonal variations in both demand and renewable supply. Substantial buildout of the electricity grid is required for a least-cost configuration. More generally, such a system is both feasible and affordable. The underlying datasets are available from Zenodo.\n\n, PyPSA is used by more than a dozen research institutes and companies worldwide. Some research groups have independently extended the software, for instance to model integer transmission expansion.\n\nTEMOA stands for Tools for Energy Model Optimization and Analysis. The software is being developed by the Department of Civil, Construction, and Environmental Engineering, North Carolina State University, Raleigh, North Carolina, USA. The project runs a website and a forum. The source code is hosted on GitHub. The model is programmed in Pyomo, an optimization components library written in Python. TEMOA can be used with any solver that Pyomo supports, including the open source GLPK solver. TEMOA uses version control to publicly archive source code and datasets and thereby enable third-parties to verify all published modeling work.\n\nTEMOA classes as a modeling framework and is used to conduct analysis using a bottom-up, technology rich energy system model. The model objective is to minimize the system-wide cost of energy supply by deploying and utilizing energy technologies and commodities over time to meet a set of exogenously specified end-use demands. TEMOA is \"strongly influenced by the well-documented MARKAL/TIMES model generators\".\n\nStatistics for the 25 open energy modeling projects listed are as follows:\n\nThe GAMS language requires a proprietary environment and its significant cost effectively limits participation to those who can access an institutional copy.\n\nA number of technical component models are now also open source. While these component models do not constitute systems models aimed at public policy development (the focus of this page), they nonetheless warrant a mention. Component models can be linked or otherwise adapted into these broader initiatives.\n\nA number of electricity auction models have been written in GAMS, AMPL, MathProg, and other languages. These include:\n\n\n\n\nMany projects rely on a pure linear or mixed integer solver to perform classical optimization, constraint satisfaction, or some mix of the two. While there are several open source solver projects, the most commonly deployed solver is GLPK. GLPK has been adopted by Calliope, ETEM, ficus, OSeMOSYS, SWITCH, and TEMOA. Another alternative is the Clp solver. Proprietary solvers outperform open source solvers by a considerable margin (perhaps ten-fold), so choosing an open solver will limit performance in terms of both speed and memory consumption.\n\nGeneral\n\n\nSoftware\n\n\n\n"}
{"id": "38083742", "url": "https://en.wikipedia.org/wiki?curid=38083742", "title": "Pit water", "text": "Pit water\n\nPit water, mine water or mining water is water that collects in a mine and which has to be brought to the surface by water management methods in order to enable the mine to continue working.\n\nAlthough all water that enters pit workings originates from atmospheric precipitation, the miner distinguishes between surface water and groundwater. Surface water enters the pit through openings in the mine at the surface of the ground, such as tunnel portals or shaft entrances. During heavy rain, water seeps into the earth and forms ground water when it meets layers of impervious rock. Pit water is mainly interstitial water and groundwater that seeps into the mine workings.\n\n\n\n"}
{"id": "3551532", "url": "https://en.wikipedia.org/wiki?curid=3551532", "title": "Premelting", "text": "Premelting\n\nPremelting (also surface melting) refers to a quasi-liquid film than can occur on the surface of a solid even below melting point (formula_1). The thickness of the film is temperature (formula_2) dependent. This effect is common for all crystalline materials. \nPremelting shows its effects in frost heave, the growth of snowflakes and, taking grain boundary interfaces into account, maybe even in the movement of glaciers.\n\nConsidering a solid-vapour interface, complete and incomplete premelting can be distinguished. During a temperature rise from below to above formula_3, in the case of complete premelting, the solid melts homogeneously from the outside to the inside; in the case of incomplete premelting, the liquid film stays very thin during the beginning of the melting process, but droplets start to form on the interface. In either case, the solid always melts from the outside inwards, never from the inside.\n\nThe first to mention premelting might have been Michael Faraday in 1842 for ice surfaces. He compared the effect which holds a snowball together to that which makes buildings from moistured sand stable. Another interesting thing he mentioned is that two blocks of ice can freeze together. Later Tammann and Stranski suggested that all surfaces might, due to the reduction of surface energy, start melting at their surfaces. Frenkel strengthened this by noting that, in contrast to liquids, no overheating can be found for solids. After extensive studies on many materials, it can be concluded that it is a common attribute of the solid state that the melting process begins at the surface.\n\nThere are several ways to approach the topic of premelting the most figurative way might be thermodynamically. A more detailed or abstract view on what physics is important for premelting is given by the Lifshitz and the Landau theories.\nOne always starts with looking at a crystalline solid phase (fig. 1: (1) solid) and another phase. This second phase (fig. 1: (2)) can either be vapour, liquid or solid. Further it can consist of the same chemical material or another. In the case of the second phase being a solid of the same chemical material one speaks of grain boundaries. This case is very important when looking at polycrystalline materials.\n\nIn the following thermodynamical equilibrium is assumed, as well as for simplicity (2) should be a vaporous phase.\n\nThe first (1) and the second (2) phase are always divided by some form of interface, what results in an interfacial energy formula_4. One can now ask whether this energy can be lowered by inserting a third phase (l) in between (1) and (2). Written in interfacial energies this would mean:\n\nIf this is the case then it is more efficient for the system to form a separating phase (3). The only possibility for the system to form such a layer is to take material of the solid and \"melt\" it to a quasi-liquid. In further notation there will be no distinction between quasi-liquid and liquid but one should always keep in mind that there is a difference. This difference to a real liquid becomes clear when looking at a very thin layer (l). As, due to the long range forces of the molecules of the solid material the liquid very near the solid still \"feels\" the order of crystalline solid and hence itself is in a state providing a not liquid like amount of order. As considering a very thin layer at the moment it is clear that the whole separating layer (l) is too well ordered for a liquid. Further comments on ordering can be found in the paragraph on Landau theory.\n\nNow, looking closer at the thermodynamics of the newly introduced phase (l), its Gibbs energy can be written as:\n\nWere formula_2 is the temperature, formula_6 the pressure, formula_7 the thickness of (l) corresponding to the number or particles formula_8 in this case. formula_9 and formula_10 are the atomic density and the chemical potential in (l) and formula_11. Note that one has to consider that the interfacial energies can just be added to the Gibbs energy in this case. As noted before formula_7 corresponds formula_8 so the derivation to formula_7 results in:\n\nWhere formula_15. Hence formula_16 and formula_17 differ and formula_18 can be defined. Assuming that a Taylor expansion around the melting point formula_19 is possible and using the Clausius–Clapeyron equation one can get the following results:\nformula_22\n\nformula_24\n\nWhere formula_25 is in the order of molecular dimensions formula_26 the specific melting heat and formula_27\n\nThese formulas also show that the more the temperature increases, the more increases the thickness of the premelt as this is energetically advantageous. This is the explanation why no overheating exists for this type of phase transition.\n\nWith the help of the Lifshitz Theory on Casimir, respectively van der Waals, interactions of macroscopic bodies premelting can be viewed from an electrodynamical perspective.\nA good example for determining the difference between complete and incomplete premelting is ice. From VUV frequencies on the polarizability of ice is greater than that of water, at lower frequencies this is vice versa. Assuming there is already a film of thickness d on the solid it is easy for any components for electromagnetic waves to travel through the film in the direction perpendicular to the solid surface as long d is small. Hence as long as the film is thin compared to the frequency interaction from the solid to the whole film is possible. But when d gets large against typical VUV frequencies the electronic structure of the film will be too slow to promote the high frequencies to the other end of the liquid phase. Thus this end of the liquid phase feels only a retarded van der Waals interaction from the solid phase. Hence the attraction between the liquid molecules themselves will outweigh and they will start forming droplets instead to thicken the film further. So the speed of light limits complete premelting.\nThis makes it a question of solid and surface free energies whether complete premelting occurs. Complete surface melting will occur when formula_28 is monotonically decreasing. If formula_28 instead shows a global minimum at finite d than the premelting will be incomplete.\nThis implies: When the long range interactions in the system are attractive than there will be incomplete premelting — assuming the film thickness is larger than any repulsive interactions. Is the film thickness small compared to the range of the repulsive interactions present and the repulsive interactions are stronger than the attractive ones than complete premelting can occur.\nFor van der Waals interactions Lifshitz theory can now calculate which type of premelting should occur for a special system. In fact small differences in systems can affect the type of premelting. For example, ice in an atmosphere of water vapour shows incomplete premelting, whereas the premelting of ice in air is complete.\n\nFor solid–solid interfaces it cannot be predicted in general whether the premelting is complete or incomplete when only considering van der Waals interactions. Here other types of interactions become very important. This also accounts for grain boundaries.\n\nMost insight in the problem probably emerges when approaching the effect form Landau Theory. Which is a little bit problematic as the melting of a bulk in general has to be considered as a first order phase transition, meaning the order parameter formula_30 jumps at formula_31. The derivation of Lipowski (basic geometry shown in fig.2) leads to the following results when formula_32:\nformula_33\n\nWhere formula_34 is the order parameter at the border between (2) and (l), formula_35 the so-called extrapolation length and formula_36 a constant that enters the model and has to be determined using experiment and other models. Hence one can see that the order parameter in the liquid film can undergo a continuous phase transition for large enough extrapolation length. A further result is that formula_37 what corresponds to the result of the thermodynamical model in the case of short range interactions. \nLandau Theory does not consider fluctuations like capillary waves, this could change the results qualitatively.\n\nThere are several techniques to prove the existence of a liquid layer on a well-ordered surface. Basically it is all about showing that there is a phase on top of the solid which has hardly any order (quasi-liquid, see fig. order parameter). One possibility was done by Frenken and van der Veen using proton scattering on a lead (Pb) single crystal (110) surface. First the surface was atomically cleaned in [UHV], because one obviously has to have a very well ordered surface for such experiments. Than they did proton shadowing and blocking measurements. An ideal shadowing and blocking measurements results in an energy spectrum of the scattered protons that shows only a peak for the first surface layer and nothing else. Due to the non ideality of the experiment the spectrum also shows effects of the underlying layers. That means the spectrum is not one well defined peak but has a tail to lower energies due to protons scattered on deeper layers which results in losing energies because of stopping.\nThis is different for a liquid film on the surface: This film does hardly (to the meaning of hardly see Landau theory) have any order. So the effects of shadowing and blocking vanish what means all the liquid film contributes the same amount of scattered electrons to the signal. Therefore, the peak does not only have a tail, but also becomes broadened.\nDuring their measurements Frenken and van der Veen raised the temperature to the melting point and hence could show that with increasing temperature a disordered film formed on the surface in equilibrium with a still well ordered Pb crystal.\nUp to here, an ideal surface was considered, but going beyond the idealized case there are several effects which influence premelting:\n\n\nThe friction coefficient for ice, without a liquid film on the surface, is measured to be formula_39. A comparable friction coefficient is that of rubber or bitumen (roughly 0.8), which would be very difficult to ice skate on. The friction coefficients needs to be around or below 0.005 for ice skating to be possible. The reason ice skating is possible is because there is a thin film of water present between the blade of the ice skate and the ice. The origin of this water film has been a long-standing debate.\nThere are three proposed mechanisms that could account for a film of liquid water on the ice surface:\nWhile contributions from all three of these factors are usually in effect when ice skating, the scientific community has long debated over which is the dominating mechanism. For several decades it was common to explain the low friction of the skates on ice by pressure melting, but there are several recent arguments that contradict this hypothesis. The strongest argument against pressure melting is that ice skating is still possible below temperatures under -20 °C (253K). At this temperature, a great deal of pressure (>100MPa) is required to induce melting. Just below -23 °C (250K), increasing the pressure can only form a different solid structure of ice (Ice III) since the isotherm no longer passes through the liquid phase on the phase diagram. While impurities in the ice will suppress the melting temperature, many materials scientists agree that pressure melting is not the dominate mechanism.\nThe thickness of the water film due to premelting is also limited at low temperatures. While the water film can reach thicknesses on the order of formula_40, at temperatures around -10 °C the thickness is on the order of nm.\nAlthough, De Koning et al. found in their measurements that the adding of impurities to the ice can lower the friction coefficient up to 15%. The friction coefficient increases with skating speed, which could yield different results depending on the skating technique and speeds.\nWhile the pressure melting hypothesis may have been put to rest, the debate between premelting and friction as the dominate mechanism still rages on.\n\n\n\n"}
{"id": "52871169", "url": "https://en.wikipedia.org/wiki?curid=52871169", "title": "Protalus rampart", "text": "Protalus rampart\n\nA protalus rampart (or pronival rampart) is a depositional landform of periglacial origin. It forms as rock debris falls onto a steep snow slope from a cliff above and slides down the snow surface to come to a rest at the foot of the slope. Over a long period of time, sufficient material can accumulate in this way to produce a distinct bank of stony material which, long after the snowbed has melted away, remains as a rampart (a bank or mound similar to a manmade rampart). The debris may also accumulate through avalanching or landslide. Protalus ramparts may be distinguished from glacial moraines by their lack of rock fragments with glacial abrasion or striations. The morphology of the site may also suggest it being unfavourable for the development of a glacier, but suitable for this mechanism.\n\nProtalus ramparts are recorded in the Cairngorms and northwest Highlands of Scotland. An especially large example on the north side of Baosbheinn measures 1 km in length and reaches a height of 55 m. It is thought to result from a major rock slope failure, however evidence leads some to class it as a protalus rock glacier. Several features amongst the mountains of the Brecon Beacons National Park have been interpreted by some authors as protalus ramparts but as moraines by others. The depositional ridge of Fan Fechan beneath Fan Hir, formerly thought to be a rampart due to its linear nature paralleling the line of the cliff above is now considered to be a moraine.\n"}
{"id": "55026959", "url": "https://en.wikipedia.org/wiki?curid=55026959", "title": "Region of the 10 Thousanders", "text": "Region of the 10 Thousanders\n\nThe Region of the 10 Thousanders () is a region in the Swabian Jura in the German state of Baden-Württemberg, the name of which alludes to the high mountain peaks in the area.\n\nAlmost all the highest mountains of the Swabian Jura (each over ), including their highest summit (), are located in this relatively small region which only covers 20 km² around Deilingen, Wehingen and Gosheim in the southwestern Jura.\n\nThe name \"Region of the 10 Thousanders\" goes back to an action group that was initiated by various restaurants and municipalities of the Heuberg to promote regional tourism.\n\nThe 10 \"thousanders\" are – sorted by height in metres (m) above sea level (NHN):\n\nThe following form fairly unified high mountain chains which makes them difficult to identify for those not acquainted with the area:\nThe two other Jura peaks that are over , the high and very striking Plettenberg () near Dotternhausen and the high Schafberg () near Hausen am Tann lies roughly 8 km north-northeast and are not counted within the \"10 Thousanders\".\n\n"}
{"id": "19343913", "url": "https://en.wikipedia.org/wiki?curid=19343913", "title": "Renewable and Sustainable Energy Reviews", "text": "Renewable and Sustainable Energy Reviews\n\nRenewable and Sustainable Energy Reviews is a peer-reviewed scientific journal covering research on sustainable energy. It is published in 12 issues per year by Elsevier and the editor-in-chief is L. Kazmerski (National Renewable Energy Laboratory). According to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 6.798.\n\nThe Journal considers articles based on the themes of Energy resources, Applications, Utilization, Environment, Techno-socio-economic aspects, systems, and sustainability.\n\nIn 2018, the journal accidentally retracted an article from the journal because of \"human error\". The article has since been reinstated in the journal.\n\n"}
{"id": "86821", "url": "https://en.wikipedia.org/wiki?curid=86821", "title": "Robor", "text": "Robor\n\nIn Gallo-Roman religion, Robor or Roboris was a god invoked alongside the \"genius loci\" on a single inscription found in Angoulême.\n\n\n"}
{"id": "20130172", "url": "https://en.wikipedia.org/wiki?curid=20130172", "title": "Seasonality", "text": "Seasonality\n\nIn time series data, seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly. Seasonality may be caused by various factors, such as weather, vacation, and holidays and consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series.\n\nSeasonal fluctuations in a time series can be contrasted with cyclical patterns. The latter occur when the data exhibits rises and falls that are not of a fixed period. Such non-seasonal fluctuations are usually due to economic conditions and are often related to the \"business cycle\"; their period usually extends beyond a single year, and the fluctuations are usually of at least two years.\n\nOrganisations facing seasonal variations, such as ice-cream vendors, are often interested in knowing their performance relative to the normal seasonal variation. Seasonal variations in the labour market can be attributed to the entrance of school leavers into the job market as they aim to contribute to the workforce upon the completion of their schooling. These regular changes are of less interest to those who study employment data than the variations that occur due to the underlying state of the economy; their focus is on how unemployment in the workforce has changed, despite the impact of the regular seasonal variations.\nIt is necessary for organisations to identify and measure seasonal variations within their market to help them plan for the future. This can prepare them for the temporary increases or decreases in labour requirements and inventory as demand for their product or service fluctuates over certain periods. This may require training, periodic maintenance, and so forth that can be organized in advance. Apart from these considerations, the organisations need to know if variation they have experienced has been more or less than the expected amount, beyond what the usual seasonal variations account for.\n\nThere are several main reasons for studying seasonal variation:\n\nThe following graphical techniques can be used to detect seasonality:\n\nA really good way to find periodicity, including seasonality, in any regular series of data is to remove any overall trend first and then to inspect time periodicity.\n\nThe run sequence plot is a recommended first step for analyzing any time series. Although seasonality can sometimes be indicated by this plot, seasonality is shown more clearly by the seasonal subseries plot or the box plot. The seasonal subseries plot does an excellent job of showing both the seasonal differences (between group patterns) and also the within-group patterns. The box plot shows the seasonal difference (between group patterns) quite well, but it does not show within group patterns. However, for large data sets, the box plot is usually easier to read than the seasonal subseries plot.\n\nThe seasonal plot, seasonal subseries plot, and the box plot all assume that the seasonal periods are known. In most cases, the analyst will in fact, know this. For example, for monthly data, the period is 12 since there are 12 months in a year. However, if the period is not known, the autocorrelation plot can help. If there is significant seasonality, the autocorrelation plot should show spikes at lags equal to the period. For example, for monthly data, if there is a seasonality effect, we would expect to see significant peaks at lag 12, 24, 36, and so on (although the intensity may decrease the further out we go).\n\nAn autocorrelation plot (ACF) can be used to identify seasonality, as it calculates the difference (residual amount) between a Y value and a lagged value of Y. The result gives some points where the two values are close together ( no seasonality ), but other points where there is a large discrepancy. These points indicate a level of seasonality in the data.\n\nSemiregular cyclic variations might be dealt with by spectral density estimation.\n\nSeasonal variation is measured in terms of an index, called a seasonal index. It is an average that can be used to compare an actual observation relative to what it would be if there were no seasonal variation. An index value is attached to each period of the time series within a year. This implies that if monthly data are considered there are 12 separate seasonal indices, one for each month. The following methods use seasonal indices to measure seasonal variations of a time-series data.\n\nThe measurement of seasonal variation by using the ratio-to-moving-average method provides an index to measure the degree of the seasonal variation in a time series. The index is based on a mean of 100, with the degree of seasonality measured by variations away from the base. For example, if we observe the hotel rentals in a winter resort, we find that the winter quarter index is 124. The value 124 indicates that 124 percent of the average quarterly rental occur in winter. If the hotel management records 1436 rentals for the whole of last year, then the average quarterly rental would be 359= (1436/4). As the winter-quarter index is 124, we estimate the number of winter rentals as follows:\n\n359*(124/100)=445;\n\nHere, 359 is the average quarterly rental. 124 is the winter-quarter index. 445 the seasonalized winter-quarter rental.\n\nThis method is also called the percentage moving average method. In this method, the original data values in the time-series are expressed as percentages of moving averages. The steps and the tabulations are given below.\n\n1. Find the centered 12 monthly (or 4 quarterly) moving averages of the original data values in the time-series.\n\n2. Express each original data value of the time-series as a percentage of the corresponding centered moving average values obtained in step(1).In other words, in a multiplicative time-series model, we get(Original data values)/(Trend values) *100 = (T*C*S*I)/(T*C)*100 = (S*I) *100.\nThis implies that the ratio–to-moving average represents the seasonal and irregular components.\n\n3. Arrange these percentages according to months or quarter of given years. Find the averages over all months or quarters of the given years.\n\n4. If the sum of these indices is not 1200(or 400 for quarterly figures), multiply then by a correction factor = 1200/ (sum of monthly indices). Otherwise, the 12 monthly averages will be considered as seasonal indices.\n\nLet us calculate the seasonal index by the ratio-to-moving-average method from the following data:\n\nNow calculations for 4 quarterly moving averages and ratio-to-moving-averages are shown in the below table.\n\nNow the total of seasonal averages is 398.85. Therefore, the corresponding correction factor would be 400/398.85 = 1.00288. Each seasonal average is multiplied by the correction factor 1.00288 to get the adjusted seasonal indices as shown in the above table.\n\n1. In an additive time-series model, the seasonal component is estimated as: S = Y − ( T + C + I )\n\nWhere S is for Seasonal values\n\nY is for actual data values of the time-series\n\nT is for trend values\n\nC is for cyclical values\n\nI is for irregular values.\n\n2. In a multiplicative time-series model, the seasonal component is expressed in terms of ratio and percentage as\n\nformula_1;\n\nHowever, in practice the detrending of time-series is done to arrive at formula_2.\n\nThis is done by dividing both sides of formula_3 by trend values T so that formula_4.\n\n3. The deseasonalized time-series data will have only trend (T) cyclical(C) and irregular (I) components and is expressed as:\n\nA completely regular cyclic variation in a time series might be dealt with in time series analysis by using a sinusoidal model with one or more sinusoids whose period-lengths may be known or unknown depending on the context. A less completely regular cyclic variation might be dealt with by using a special form of an ARIMA model which can be structured so as to treat cyclic variations semi-explicitly. Such models represent cyclostationary processes.\n\nAnother method of modelling periodic seasonality is the use of pairs of Fourier terms. Similar to using the sinusoidal model, Fourier terms added into regression models utilize sine and cosine terms in order to simulate seasonality. However, the seasonality of such a regression would be represented as the sum of sine or cosine terms, instead of a single sine or cosine term in a sinusoidal model. Every periodic function can be approximated with the inclusion of Fourier terms.\n\nThe difference between a sinusoidal model and a regression with Fourier terms can be simplified as below:\n\nSinusoidal Model: \n\nRegression With Fourier Terms: \n\nSeasonal adjustment is any method for removing the seasonal component of a time series. The resulting seasonally adjusted data are used, for example, when analyzing or reporting non-seasonal trends over durations rather longer than the seasonal period. An appropriate method for seasonal adjustment is chosen on the basis of a particular view taken of the decomposition of time series into components designated with names such as \"trend\", \"cyclic\", \"seasonal\" and \"irregular\", including how these interact with each other. For example, such components might act additively or multiplicatively. Thus, if a seasonal component acts additively, the adjustment method has two stages:\n\nIf it is a multiplicative model,the magnitude of the seasonal fluctuations will vary with the level, which is more likely to occur with economic series. When taking seasonality into account, the seasonally adjusted multiplicative decomposition can be written as formula_9; whereby the original time series is divided by the estimated seasonal component.\n\nThe multiplicative model can be transformed into an additive model by taking the log of the time series;\n\nSA Multiplicative decomposition: formula_10\n\nTaking log of the time series of the multiplicative model: formula_11\n\nOne particular implementation of seasonal adjustment is provided by X-12-ARIMA.\n\nIn regression analysis such as ordinary least squares, with a seasonally varying dependent variable being influenced by one or more independent variables, the seasonality can be accounted for and measured by including \"n\"-1 dummy variables, one for each of the seasons except for an arbitrarily chosen reference season, where \"n\" is the number of seasons (e.g., 4 in the case of meteorological seasons, 12 in the case of months, etc.). Each dummy variable is set to 1 if the data point is drawn from the dummy's specified season and 0 otherwise. Then the predicted value of the dependent variable for the reference season is computed from the rest of the regression, while for any other season it is computed using the rest of the regression and by inserting the value 1 for the dummy variable for that season.\n\nA seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week.\nA cycle occurs when the data exhibit rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the \"business cycle\".\nIt is important to distinguish cyclic patterns and seasonal patterns. Seasonal patterns have a fixed and known length, while cyclic patterns have variable and unknown length. The average length of a cycle is usually longer than that of seasonality, and the magnitude of cyclic variation is usually more variable than that of seasonal variation.\n\n\n\n"}
{"id": "2793808", "url": "https://en.wikipedia.org/wiki?curid=2793808", "title": "Secretariat of Energy (Mexico)", "text": "Secretariat of Energy (Mexico)\n\nThe Secretariat of Energy (Spanish: \"Secretaría de Energia\") is the government department in charge of production and regulation of energy in Mexico, this secretary is a member of the Executive Cabinet.\n\nThe current Secretariat of Energy is Pedro Joaquín Coldwell.\n\nOn December 7, 1946, the \"Secretaría de Bienes Nacionales e Inspección Administrativa\" (Secretariat of National Assets and Administrative Inspection) was created with Alfonso Caso Andrade serving as secretary. In 1958, the name was changed to \"Secretaría de Patrimonio Nacional\" (Secretariat of National Assets, SEPANAL). In the late 1970s and 1980s it changed names two more times, being known as the \"Secretaría de Patrimonio y Fomento Industrial\" (Secretariat of Assets and Industrial Development) during the presidency of José López Portillo and as the \"Secretaría de Energía, Minas e Industria Paraestatal\" (Secretariat of Energy, Mining and Semi-Public Industries) until obtaining its current name in 1994.\n\n\n"}
{"id": "20055670", "url": "https://en.wikipedia.org/wiki?curid=20055670", "title": "Shockley–Queisser limit", "text": "Shockley–Queisser limit\n\nIn physics, the Shockley–Queisser limit, also known as the detailed balance limit, Shockley Queisser Efficiency Limit or SQ Limit, refers to the maximum theoretical efficiency of a solar cell using a single p-n junction to collect power from the cell. It was first calculated by William Shockley and Hans-Joachim Queisser at Shockley Semiconductor in 1961, giving a maximum efficiency of 30% at 1.1 eV. However, this calculation used a simplified model of the solar spectrum, and more recent calculations give a maximum efficiency of 33.7% at 1.34 eV, but the value is still referred to as the Shockley-Queisser limit in their honor. The limit is one of the most fundamental to solar energy production with photovoltaic cells, and is considered to be one of the most important contributions in the field.\n\nThe limit is that the maximum solar conversion efficiency is around 33.7% for a single p-n junction photovoltaic cell, assuming typical sunlight conditions (unconcentrated, AM 1.5 solar spectrum), and subject to other caveats and assumptions discussed below. This maximum occurs at a band gap of 1.34 eV. That is, of all the power contained in sunlight (about 1000 W/m²) falling on an ideal solar cell, only 33.7% of that could ever be turned into electricity (337 W/m²). The most popular solar cell material, silicon, has a less favorable band gap of 1.1 eV, resulting in a maximum efficiency of about 32%. Modern commercial mono-crystalline solar cells produce about 24% conversion efficiency, the losses due largely to practical concerns like reflection off the front of the cell and light blockage from the thin wires on the cell surface.\n\nThe Shockley–Queisser limit only applies to conventional solar cells with a single p-n junction; tandem solar cells with multiple layers can (and do) outperform this limit, and so can solar thermal and certain other solar energy systems. In the extreme limit, for a tandem solar cell with an infinite number of layers, the corresponding limit is 86.8% using concentrated sunlight. (See Solar cell efficiency.)\n\nIn a traditional solid-state semiconductor such as silicon, a solar cell is made from two doped crystals, one an n-type semiconductor, which has extra free electrons, and the other a p-type semiconductor, which is lacking free electrons, referred to as \"holes.\" When initially placed in contact with each other, some of the electrons in the n-type portion will flow into the p-type to \"fill in\" the missing electrons. Eventually enough will flow across the boundary to equalize the Fermi levels of the two materials. The result is a region at the interface, the p-n junction, where charge carriers are depleted on each side of the interface. In silicon, this transfer of electrons produces a potential barrier of about 0.6 V to 0.7 V.\n\nWhen the material is placed in the sun, photons from the sunlight can be absorbed in the p-type side of the semiconductor, causing electrons in the valence band to be promoted in energy to the conduction band. This process is known as photoexcitation. As the name implies, electrons in the conduction band are free to move about the semiconductor. When a load is placed across the cell as a whole, these electrons will flow from the p-type side into the n-type side, lose energy while moving through the external circuit, and then go back into the p-type material where they can re-combine with the valence-band holes they left behind. In this way, sunlight creates an electric current.\n\nThe Shockley–Queisser limit is calculated by examining the amount of electrical energy that is extracted per photon of incoming sunlight. There are several considerations:\n\nAny material, that is not at absolute zero (0 Kelvin), emits electromagnetic radiation through the black-body radiation effect. In a cell at room temperature, this represents approximately 7% of all the energy falling on the cell.\n\nAny energy lost in a cell is turned into heat, so any inefficiency in the cell increases the cell temperature when it is placed in sunlight. As the temperature of the cell increases, the outgoing radiation and heat loss through conduction and convection also increase, until an equilibrium is reached. In practice, this equilibrium is normally reached at temperatures as high as 360 Kelvin, and consequently, cells normally operate at lower efficiencies than their room-temperature rating. Module datasheets normally list this temperature dependency as (NOCT - Nominal Operating Cell Temperature).\n\nFor a \"blackbody\" at normal temperatures, a very small part of this radiation (the number per unit time and per unit area given by , \"c\" for \"cell\") is photons having energy greater than the band gap (wavelength less than about 1.1 microns for silicon), and part of these photons (Shockley and Queisser use the factor \"t\") are generated by recombination of electrons and holes, which decreases the amount of current that could be generated otherwise. This is a very small effect, but Shockley and Queisser assume that the total rate of recombination (see below) when the voltage across the cell is zero (short circuit or no light) is proportional to the blackbody radiation . This rate of recombination plays a negative role in the efficiency. Shockley and Queisser calculate to be 1700 photons per second per square centimetre for silicon at 300K.\n\nAbsorption of a photon creates an electron-hole pair, which could potentially contribute to the current. However, the reverse process must also be possible, according to the principle of detailed balance: an electron and a hole can meet and recombine, emitting a photon. This process reduces the efficiency of the cell. Other recombination processes may also exist (see \"Other considerations\" below), but this one is absolutely required.\n\nIn the Shockley–Queisser model, the recombination rate depends on the voltage across the cell but is the same whether or not there is light falling on the cell. A factor \"f\" gives the ratio of recombination that produces radiation to total recombination, so the rate of recombination per unit area when \"V\" = 0 is \"2tQ\"/\"f\" and thus depends on \"Q\", the flux of blackbody photons above the band-gap energy. The factor of 2 was included on the assumption that radiation emitted by the cell goes in both directions. (This is actually debatable if a reflective surface is used on the shady side.) When the voltage is non-zero, the concentrations of charge carriers (electrons and holes) change (see Shockley diode equation), and according to the authors the rate of recombination changes by a factor of exp(\"V\"/\"V\"), where \"V\" is the voltage equivalent of the temperature of the cell, or \"thermal voltage\", namely\n\n(\"q\" being the charge of an electron). Thus the rate of recombination, in this model, is proportional to exp(\"V\"/\"V\") times the blackbody radiation above the band-gap energy:\n\n(This is actually an approximation to the more accurate expression\n\nwhich is correct so long as the cell is thick enough to act as a black body. The difference in maximum theoretical efficiency however is negligibly small, except for tiny bandgaps below 200meV.)\n\nThe rate of \"generation\" of electron-hole pairs \"not\" due to incoming sunlight stays the same, so recombination minus spontaneous generation is\n\nformula_4\n\nwhere formula_5\n\nThe rate of generation of electron-hole pairs due to sunlight is\n\nwhere formula_7 is the number of photons above the band-gap energy falling on the cell per unit area, and \"t\" is the fraction of these that generate an electron-hole pair. This rate of generation is called \"I\" because it is the \"short circuit\" current (per unit area). When there is a load, then \"V\" will not be zero and we have a current equal to the rate of generation of pairs due to the sunlight minus the difference between recombination and spontaneous generation:\n\nThe open-circuit voltage is therefore given (assuming \"f\" does not depend on voltage) by\n\nThe product of the short-circuit current \"I\" and the open-circuit voltage \"V\" Shockley and Queisser call the \"nominal power\". It is not actually possible to get this amount of power out of the cell, but we can get close (see \"Impedance matching\" below).\n\nThe ratio of the open-circuit voltage to the band-gap voltage Shockley and Queisser call \"V\". Under open-circuit conditions, we have\n\nAsymptotically, this gives\n\nor\n\nwhere is the voltage equivalent of the temperature of the sun. As the ratio goes to zero, the open-circuit voltage goes to the band-gap voltage, and as it goes to one, the open-circuit voltage goes to zero. This is why the efficiency falls if the cell heats up. In fact this expression represents the thermodynamic upper limit of the amount of work that can be obtained from a heat source at the temperature of the sun and a heat sink at the temperature of the cell.\n\nSince the act of moving an electron from the valence band to the conduction band requires energy, only photons with more than that amount of energy will produce an electron-hole pair. In silicon the conduction band is about 1.1 eV away from the valence band, this corresponds to infrared light with a wavelength of about 1.1 microns. In other words, photons of red, yellow and blue light and some near-infrared will contribute to power production, whereas radio waves, microwaves, and most infrared photons will not. This places an immediate limit on the amount of energy that can be extracted from the sun. Of the 1,000 W/m² in AM1.5 sunlight, about 19% of that has less than 1.1 eV of energy, and will not produce power in a silicon cell.\n\nAnother important contributor to losses is that any energy above and beyond the bandgap energy is lost. While blue light has roughly twice the energy of red light, that energy is not captured by devices with a single p-n junction. The electron is ejected with higher energy when struck by a blue photon, but it loses this extra energy as it travels toward the p-n junction (the energy is converted into heat). This accounts for about 33% of the incident sunlight, meaning that, for silicon, from spectrum losses alone there is a theoretical conversion efficiency limit of about 48%, ignoring all other factors.\n\nThere is a trade-off in the selection of a bandgap. If the band gap is large, not as many photons create pairs, whereas if the band gap is small, the electron-hole pairs do not contain as much energy.\n\nShockley and Queisser call the efficiency factor associated with spectrum losses , for \"ultimate efficiency function\". Shockley and Queisser calculated that the best band gap for sunlight happens to be 1.1 eV, the value for silicon, and gives a of 44%. They used blackbody radiation of 6000K for sunlight, and found that the optimum band gap would then have an energy of 2.2 \"kT\". (At that value, 22% of the blackbody radiation energy would be below the band gap.) Using a more accurate spectrum may give a slightly different optimum. A blackbody at 6000 K puts out 7348 W per square centimetre, so a value for of 44% and a value of photons per joule (corresponding to a band gap of 1.09 V, the value used by Shockley and Queisser) gives equal to photons per second per square centimetre.\n\nIf the resistance of the load is too high, the current will be very low, while if the load resistance is too low, the voltage drop across it will be very low. There is an optimal load resistance that will draw the most power from the solar cell at a given illumination level. Shockley and Queisser call the ratio of power extracted to the impedance matching factor, . (It is also call the fill factor.) The optimum depends on the shape of the versus curve. For very low illumination, the curve is more or less a diagonal line, and will be 1/4. But for high illumination, approaches 1. Shockley and Queisser give a graph showing as a function of the ratio of the open-circuit voltage to the thermal voltage . According to the authors, this ratio is well approximated by , where is the combination of factors , in which is the solid angle of the sun divided by π. The maximum value of without light concentration (with reflectors for example) is just , or , according to the authors. Using the above mentioned values of and , this gives a ratio of open-circuit voltage to thermal voltage of 32.4 ( equal to 77% of the band gap). The authors derive the equation\n\nwhich can be solved to find , the ratio of optimal voltage to thermal voltage. For a of 32.4, we find equal to 29.0. One can then use the formula\n\nto find the impedance matching factor. For a of 32.4, this comes to 86.5%.\n\nConsidering the spectrum losses alone, a solar cell has a peak theoretical efficiency of 48% (or 44% according to Shockley and Queisser – their \"ultimate efficiency factor\"). Thus the spectrum losses represent the vast majority of lost power. Including the effects of recombination and the \"I\" versus \"V\" curve, the efficiency is described by the following equation:\n\nwith\n\nwhere , , and are respectively the ultimate efficiency factor, the ratio of open-circuit voltage to band-gap voltage, and the impedance matching factor (all discussed above). Letting be 1, and using the values mentioned above of 44%, 77%, and 86.5% for the three factors gives about 29% overall efficiency. Shockley and Queisser say 30% in their abstract, but do not give a detailed calculation. A more recent reference gives, for a single-junction cell, a theoretical peak performance of about 33.7%, or about 337 W/m² in AM1.5.\n\nWhen the amount of sunlight is increased using reflectors or lenses, the factor (and therefore ) will be higher. This raises both and . Shockley and Queisser include a graph showing the overall efficiency as a function of band gap for various values of . For a value of 1, the graph shows a maximum efficiency of just over 40%, getting close to the ultimate efficiency (by their calculation) of 44%.\n\nShockley and Queisser's work considered the most basic physics only; there are a number of other factors that further reduce the theoretical power.\n\nWhen an electron is ejected through photoexcitation, the atom it was formerly bound to is left with a net positive charge. Under normal conditions, the atom will pull off an electron from a surrounding atom in order to neutralize itself. That atom will then attempt to remove an electron from another atom, and so forth, producing an ionization chain reaction that moves through the cell. Since these can be viewed as the motion of a positive charge, it is useful to refer to them as \"holes\", a sort of virtual positive electron.\n\nLike electrons, holes move around the material, and will be attracted towards a source of electrons. Normally these are provided through an electrode on the back surface of the cell. Meanwhile, the conduction-band electrons are moving forward towards the electrodes on the front surface. For a variety of reasons, holes in silicon move much more slowly than electrons. This means that during the finite time while the electron is moving forward towards the p-n junction, it may meet a slowly moving hole left behind by a previous photoexcitation. When this occurs, the electron recombines at that atom, and the energy is lost (normally through the emission of a photon of that energy, but there are a variety of possible processes).\n\nRecombination places an upper limit on the \"rate\" of production; past a certain rate there are so many holes in motion that new electrons will never make it to the p-n junction. In silicon this reduces the theoretical performance under normal operating conditions by another 10% over and above the thermal losses noted above. Materials with higher electron (or hole) mobility can improve on silicon's performance; gallium arsenide (GaAs) cells gain about 5% in real-world examples due to this effect alone. In brighter light, when it is concentrated by mirrors or lenses for example, this effect is magnified. Normal silicon cells quickly saturate, while GaAs continue to improve at concentrations as high as 1500 times.\n\nRecombination between electrons and holes is detrimental in a solar cell, so designers try to minimize it. However, radiative recombination—when an electron and hole recombine to create a photon that exits the cell into the air—is inevitable, because it is the time-reversed process of light absorption. Therefore, the Shockley–Queisser calculation takes radiative recombination into account; but it assumes (optimistically) that there is no other source of recombination. More realistic limits, which are lower than the Shockley–Queisser limit, can be calculated by taking into account other causes of recombination. These include recombination at defects and grain boundaries.\n\nIn crystalline silicon, even if there are no crystalline defects, there is still Auger recombination, which occurs much more often than radiative recombination. By taking this into account, the theoretical efficiency of crystalline silicon solar cells was calculated to be 29.4%.\n\nIt is important to note that the analysis of Shockley and Queisser was based on the following assumptions:\n\nNone of these assumptions is necessarily true, and a number of different approaches have been used to significantly surpass the basic limit.\n\nThe most widely explored path to higher efficiency solar cells has been multijunction photovoltaic cells, also known as \"tandem cells\". These cells use multiple p-n junctions, each one tuned to a particular frequency of the spectrum. This reduces the problem discussed above, that a material with a single given bandgap cannot absorb sunlight below the bandgap, and cannot take full advantage of sunlight far above the bandgap. In the most common design, a high-bandgap solar cell sits on top, absorbing high-energy, low-wavelength light, and transmitting the rest. Beneath it is a lower-bandgap solar cell which absorbs some of the lower-energy, longer-wavelength light. There may be yet another cell beneath that one, with as many as four layers in total.\n\nThe calculation of the fundamental efficiency limits of these multijunction cells works in a fashion similar to those for single-junction cells, with the caveat that some of the light will be converted to other frequencies and re-emitted within the structure. Using methods similar to the original Shockley–Queisser analysis with these considerations in mind produces similar results; a two-layer cell can reach 42% efficiency, three-layer cells 49%, and a theoretical infinity-layer cell 68% in non-concentrated sunlight.\n\nThe majority of tandem cells that have been produced to date use three layers, tuned to blue (on top), yellow (middle) and red (bottom). These cells require the use of semiconductors that can be tuned to specific frequencies, which has led to most of them being made of gallium arsenide (GaAs) compounds, often germanium for red, GaAs for yellow, and GaInP for blue. They are very expensive to produce, using techniques similar to microprocessor construction but with \"chip\" sizes on the scale of several centimeters. In cases where outright performance is the only consideration, these cells have become common; they are widely used in satellite applications for instance, where the power-to-weight ratio overwhelms practically every other consideration. They also can be used in concentrated photovoltaic applications (see below), where a relatively small solar cell can serve a large area.\n\nTandem cells are not restricted to high-performance applications; they are also used to make moderate-efficiency photovoltaics out of cheap but low-efficiency materials. One example is amorphous silicon solar cells, where triple-junction tandem cells are commercially available from Uni-Solar and other companies.\n\nSunlight can be concentrated with lenses or mirrors to much higher intensity. The sunlight intensity is a parameter in the Shockley–Queisser calculation, and with more concentration, the theoretical efficiency limit increases somewhat. If, however, the intense light heats up the cell, which often occurs in practice, the theoretical efficiency limit may go down all things considered.\n\nIn practice, the choice of whether or not to use light concentration is based primarily on other factors besides the small change in solar cell efficiency. These factors include the relative cost per area of solar cells versus focusing optics like lenses or mirrors, the cost of sunlight-tracking systems, the proportion of light successfully focused onto the solar cell, and so on.\n\nA wide variety of optical systems can be used to concentrate sunlight, including ordinary lenses and curved mirrors, fresnel lenses, arrays of small flat mirrors, and luminescent solar concentrators. Another proposal suggests spreading out an array of microscopic solar cells on a surface, and focusing light onto them via microlens arrays, while yet another proposal suggests designing a semiconductor nanowire array in such a way that light is concentrated in the nanowires.\n\nThere has been some work on producing mid-energy states within single crystal structures. These cells would combine some of the advantages of the multi-junction cell with the simplicity of existing silicon designs. A detailed limit calculation for these cells with infinite bands suggests a maximum efficiency of 77.2% To date, no commercial cell using this technique has been produced.\n\nAs discussed above, photons with energy below the bandgap are wasted in ordinary single-junction solar cells. One way to reduce this waste is to use photon upconversion, i.e. incorporating into the module a molecule or material that can absorb two or more below-bandgap photons and then emit one above-bandgap photon. Another possibility is to use two-photon absorption, but this can only work at extremely high light concentration.\n\nThermal upconversion is based on the absorption of photons with low energies in the upconverter, which heats up and re-emits photons with higher energies. The upconversion efficiency can be improved by controlling the optical density of states of the absorber and also by tuning the angularly-selective emission characteristics. For example, a planar thermal upconverting platform can have a front surface that absorbs low-energy photons incident within a narrow angular range, and a back surface that efficiently emits only high-energy photons. A hybrid thermophotovoltaic platform exploiting thermal upconversion was theoretically predicted to demonstrate maximum conversion efficiency of 73% under illumination by non-concentrated sunlight. A detailed analysis of non-ideal hybrid platforms that allows for up to 15% of absorption/re-emission losses yielded limiting efficiency value of 45% for Si PV cells.\n\nOne of the main loss mechanisms is due to the loss of excess carrier energy above the bandgap. It should be no surprise that there has been a considerable amount of research into ways to capture the energy of the carriers before they can lose it in the crystal structure. One system under investigation for this is quantum dots.\n\nA related concept is to use semiconductors that generate more than one excited electron per absorbed photon, instead of a single electron at the band edge. Quantum dots have been extensively investigated for this effect, and they have been shown to work for solar-relevant wavelengths in prototype solar cells.\n\nAnother, more straightforward way to utilise multiple exciton generation is a process called singlet fission (or singlet exciton fission) by which a singlet exciton is converted into two triplet excitons of lower energy. This allows for higher theoretical efficiencies when coupled to a low bandgap semiconductor and quantum efficiencies exceeding 100% have been reported.\n\nAlso in materials where the (excited) electrons interact strongly with the remaining electrons such as Mott insulators multiple excitons can be generated.\n\nAnother possibility for increased efficiency is to convert the frequency of light down towards the bandgap energy with a fluorescent material. In particular, to exceed the Shockley–Queisser limit, it is necessary for the fluorescent material to convert a single high-energy photon into several lower-energy ones (quantum efficiency > 1). For example, one photon with more than double the bandgap energy can become two photons above the bandgap energy. In practice, however, this conversion process tends to be relatively inefficient. If a very efficient system were found, such a material could be painted on the front surface of an otherwise standard cell, boosting its efficiency for little cost. In contrast, considerable progress has been made in the exploration of fluorescent downshifting, which converts high-energy light (e. g., UV light) to low-energy light (e. g., red light) with a quantum efficiency smaller than 1. The cell may be more sensitive to these lower-energy photons. Dyes, rare-earth phosphors and quantum dots are actively investigated for fluorescent downshifting. For example, silicon quantum dots enabled downshifting has led to the efficiency enhancement of the state-of-the-art silicon solar cells.\n\nThermophotovoltaic cells are similar to phosphorescent systems, but use a plate to act as the downconvertor. Solar energy falling on the plate, typically black-painted metal, is re-emitted as lower-energy IR, which can then be captured in an IR cell. This relies on a practical IR cell being available, but the theoretical conversion efficiency can be calculated. For a converter with a bandgap of 0.92 eV, efficiency is limited to 54% with a single-junction cell, and 85% for concentrated light shining on ideal components with no optical losses and only radiative recombination.\n\n\n"}
{"id": "4459356", "url": "https://en.wikipedia.org/wiki?curid=4459356", "title": "Solvated electron", "text": "Solvated electron\n\nA solvated electron is a free electron in (solvated in) a solution, and is the smallest possible anion. Solvated electrons occur widely, although it is difficult to observe them directly since their lifetimes are so short. The deep color of solutions of alkali metals in ammonia arises from the presence of solvated electrons: blue when dilute and copper-colored when more concentrated (> 3 molar). Classically, discussions of solvated electrons focus on their solutions in ammonia, which are stable for days, but solvated electrons also occur in water and other solvents -- in fact, in any solvent that mediates outer-sphere electron transfer. The real hydration energy of the solvated electron can be estimated by using the hydration energy of proton in water combined with kinetic data from pulse radiolysis experiments. The solvated electron forms an acid-base pair with atomic hydrogen.\n\nThe solvated electron is responsible for a great deal of radiation chemistry.\n\nAlkali metals dissolve in liquid ammonia giving deep blue solutions which are conducting in nature. The blue colour of the solution is due to ammoniated electrons which absorb energy in the visible region of light. Alkali metals also dissolve in hexamethylphosphoramide, forming blue solutions.\n\nFocusing on ammonia solutions, all of the alkali metals, as well as Ca, Sr, Ba, Eu, and Yb (also Mg using an electrolytic process), dissolve to give the characteristic blue solutions. Other amines, such as methylamine and ethylamine, are also suitable solvents.\n\nA lithium ammonia solution at −60 °C is saturated at about 15 mol% metal (MPM). When the concentration is increased in this range electrical conductivity increases from 10 to 10 ohmcm (larger than liquid mercury). At around 8 MPM, a \"transition to the metallic state\" (TMS) takes place (also called a \"metal to nonmetal transition\" (MNMT)). At 4 MPM a liquid-liquid phase separation takes place: the less dense gold-color phase becomes immiscible from a more dense blue phase. Above 8 MPM the solution is bronze/gold-colored. In the same concentration range the overall density decreases by 30%.\n\nDilute solutions are paramagnetic and at around 0.5 MPM all electrons are paired up and the solution becomes diamagnetic. Several models exist to describe the spin-paired species: as an ion trimer; as an ion-triple—a cluster of two single-electron solvated-electron species in association with a cation; or as a cluster of two solvated electrons and two solvated cations.\n\nSolvated electrons produced by dissolution of reducing metals in ammonia and amines are the anions of salts called electrides. Such salts can be isolated by the addition of macrocyclic ligands such as crown ether and cryptands. These ligands bind strongly the cations and prevent their re-reduction by the electron.\n\nIts standard electrode potential value is -2.77 V. Equivalent conductivity 177 Mho cm is similar to that of hydroxide ion. This value of equivalent conductivity corresponds to a diffusivity of 4,75*10 cms.\n\nSome thermodynamic properties of the solvated electron have been investigated by Jortner and Noyes (1966)\n\nAlkaline aqueous solutions above pH = 9.6 regenerate the hydrated electron through the reaction of hydrated atomic hydrogen with hydroxide ion giving water beside hydrated electrons.\n\nBelow pH = 9.6 the hydrated electron reacts with hydronium ion giving atomic hydrogen, which in turn can react with the hydrated electron giving hydroxide ion and usual molecular hydrogen H.\n\nThe properties of solvated electron can be investigated using the rotating ring-disk electrode.\n\nThe solvated electron reacts with oxygen to form a superoxide radical (O). With nitrous oxide, solvated electrons react to form hydroxyl radicals (HO). The solvated electrons can be scavenged from both aqueous and organic systems with nitrobenzene or sulfur hexafluoride.\n\nA common use of sodium dissolved in liquid ammonia is the Birch reduction. Other reactions where sodium is used as a reducing agent also are assumed to involve solvated electrons, e.g. the use of sodium in ethanol as in the Bouveault–Blanc reduction.\n\nThe diffusivity of the solvated electron in liquid ammonia can be determined using potential-step chronoamperometry.\n\nThe first recorded observation of the color of metal-electride solutions is generally attributed to Sir Humphry Davy. In 1807–1809, he examined the addition of grains of potassium to gaseous ammonia (liquefaction of ammonia was invented in 1823). James Ballantyne Hannay and J. Hogarth repeated the experiments with sodium in 1879–1880. W. Weyl in 1844 and C.A. Seely in 1871 were the first to use liquid ammonia. Hamilton Cady in 1897 was the first to relate the ionizing properties of ammonia to that of water. Charles A. Kraus measured the electrical conductance of metal ammonia solutions and in 1907 was the first to attribute it the electrons liberated from the metal. In 1918, G. E. Gibson and W. L. Argo introduced the solvated electron concept. They noted based on absorption spectra that different metals and different solvents (methylamine, ethylamine) produce the same blue color, attributed to a common species, the solvated electron. In the 1970s, solid salts containing electrons as the anion were characterized.\n\n"}
{"id": "45311471", "url": "https://en.wikipedia.org/wiki?curid=45311471", "title": "Southern Anatolian montane conifer and deciduous forests", "text": "Southern Anatolian montane conifer and deciduous forests\n\nThe Southern Anatolian montane conifer and deciduous forests ecoregion, in the Mediterranean forests, woodlands, and scrub biome, is of the eastern Mediterranean Basin.\n\nThe ecoregion covers an area of , and covers portions of Israel, Jordan, Lebanon, Syria, the Palestinian territories and Turkey.\n"}
{"id": "7040456", "url": "https://en.wikipedia.org/wiki?curid=7040456", "title": "The Birds of Australia (Gould)", "text": "The Birds of Australia (Gould)\n\nThe Birds of Australia was a book written by John Gould and published in seven volumes between 1840 and 1848. It was the first comprehensive survey of the birds of Australia and included descriptions of 681 species, 328 of which were new to science and were first described by Gould.\n\nGould and his wife Elizabeth travelled to Australia from England in 1838 to prepare the book. They spent a little under two years collecting specimens for the book. John travelled widely and made extensive collections of Australian birds and other fauna. Elizabeth, who had illustrated several of his earlier works, made hundreds of drawings from specimens for publication in \"The Birds of Australia\"\n\nThe plates of the book were produced by lithography, Elizabeth produced 84 plates before she died in 1841, Edward Lear produced one, Waterhouse Hawkins contributed one and the remaining 595 plates were produced by H. C. Richter from Elizabeth's drawings and were published under his name.\n\n250 sets of the seven-volume work were printed. Complete sets of original volumes recently sold at auction for more than A$350,000.\n\nIn 1865 Gould published a revised and updated version of the text of \"The Birds of Australia\" in the two-volume \"Handbook to the Birds of Australia\".\n\n"}
{"id": "40015230", "url": "https://en.wikipedia.org/wiki?curid=40015230", "title": "United States energy law", "text": "United States energy law\n\nUnited States energy law is a function of the federal government, states, and local governments. At the federal level, it is regulated extensively through the United States Department of Energy.\nEvery state, the Federal government, and the District of Columbia collect some motor vehicle excise taxes. Specifically, these are excise taxes on gasoline, diesel fuel, and gasohol. While many western states rely a great deal on severance taxes on oil, gas, and mineral production for revenue, most states get a relatively small amount of their revenue from such sources.\n\nThe practice of energy law has been the domain of law firms working on behalf of utility companies, rather than legal scholars or other legal actors (such as private lawyers and paralegals), especially in Texas, but this is changing. Some officials from energy agencies may take jobs in the utilities or other companies they regulate, such as the former FERC chairman did in 2008.\n\nThe American Bar Association (ABA) has a \"Section of Environment, Energy, and Resources\", which is a \"forum for lawyers working in areas related to environmental law, natural resources law, and energy law.\" The Section houses several substantive committees on environmental and energy law that release current information on topics of interest to practitioners and news of committee activities. The ABA recognized 'environmental and energy law' as one of the practice areas where legal work may be found in 2009.\n\nUnder the common law, persons who owned real property owned \"from the depths to the heavens\". Therefore, real estate traditionally has included all rights to water, oil, gas, and other minerals underground. The United States Supreme Court has held that as far as air rights, \"this doctrine has no place in the modern world,\" but it remains as a source of law to this day, or \"fundamental to property rights in land.\"\n\nAn easement or license to drill for oil, gas, or minerals generally runs with the land, and thus is an appurtenant easement. However, a utility easement generally runs with the owner of the easement, rather than running with the land, and as such, is an example of an easement in gross.\n\nThe West digest system, used in WestLaw, has allocated several topics in energy law:\n\nThere are many library research resources available about American oil and gas law.\n\nPrevious to the 1920s, the role of the Federal government in energy was restricted to the disposition of oil, gas, and coal on Federal lands. The Mineral Leasing Act of 1920 et seq. is the major Federal law that authorizes and governs leasing of public lands for developing deposits of hydrocarbons and other minerals. Previous to the act, these materials were subject to mining claims under the General Mining Act of 1872. In \"BP America Production Co. v. Burton\", 549 U.S. 84 (2006), the Supreme Court held that a statute of limitations does not apply to government actions for contract claims for an agency to recover royalties on such leases.\n\nConcerned that a predicted immanent shortage of petroleum would not leave enough fuel for naval vessels, President William Howard Taft established the first Naval Petroleum Reserve by withdrawing Federal land over the Elk Hills Oil Field in California from being claimed and drilled by private companies. A number of additional Naval Petroleum Reserves and Naval Oil Shale Reserves were established by Taft and his successor, Woodrow Wilson.\n\nUntil the 1920s, \"the federal government did not play an active role in the energy industries,\" sometimes explained as due to \"the widespread belief in the unlimited supply of energy.\" The first US law was the Federal Power Act of 1920 (later amended in 1935 and 1986). The Manhattan Project of the 1940s \"initiated the era of nuclear regulation.\" In 1946, the Atomic Energy Act was passed.\n\nThe Price-Anderson Nuclear Industries Indemnity Act was first enacted in 1957, and has been renewed periodically, which governs a no-fault insurance regime for nuclear accidents. The statute was upheld as constitutional in \"Duke Power Co. v. Carolina Environmental Study Group\", 438 U.S. 59 (1978).\n\nThe Department of Energy and its constituent Federal Energy Regulatory Commission (FERC) were created in 1977, through the Department of Energy Organization Act. The stated purposes of these \"federal energy laws and regulations is to provide affordable energy by sustaining competitive markets, while protecting the economic, environmental, and security interests of the United States.\" The U.S. Nuclear Regulatory Commission (NRC) regulates the use of nuclear power and its uses as a defense weaponry.\n\nOther statutes are the Public Utility Regulatory Policies Act, the Energy Security Act, the Price-Anderson Nuclear Industries Indemnity Act, and the Energy Policy Act of 1992 Most of these laws are codified at U.S. Code, Title 16, Chapter 12 – Federal regulation and development of power. The Commodity Futures Modernization Act of 2000 also affects energy trading companies. The Oil Pollution Act of 1973 and Oil Pollution Act of 1990 affect the transportation of oil on the high seas.\n\nAs of January 1, 2008, the Federal excise tax is 18.3 cents per gallon on gasoline, 24.3 cents per gallon on diesel, and 13 cents per gallon on gasohol.\n\nThe most recent major law is the Energy Policy Act of 2005, which was an attempt to combat growing energy problems, changed the energy policy of the United States by providing tax incentives and loan guarantees for energy production of various types.\n\nTwo Federal laws passed in 2007 were the Energy Independence and Security Act of 2007, and the Food and Energy Security Act of 2007.\n\nThe Biomass Research and Development Board was expected to release a report in late 2008 about biomass as fuel.\n\nIn August 2008, it was revealed that oil speculators had increased the volatility of the price of oil; Congressman John Dingell criticized the Commodity Futures Trading Commission for failing to scrutinize oil futures traders, in particular the Swiss company Vitol. On June 22, 2008, Obama proposed the repeal of the Enron loophole as a means to curb speculation on skyrocketing oil prices.\n\nIn October 2008, as the Democratic Party approached victory in the 2008 elections, they remained divided on energy policy, thus a consensus was not expected in energy law. President Barack Obama's first Secretary of Energy, Steven Chu, had no prior expertise in law.\n\nThe Department of Energy (DOE) will, by administrative measures, reduce the Hanford nuclear reservation (originally 586 square miles) to 10 square miles. Much of the remaining area will go to the Hanford Reach National Monument.\n\nIn October 2009, Secretary Chu announced a new program, Arpa-e, which will fund grants authorized under the Energy Independence and Security Act of 2007.\n\nThe \"Energy Credit for Qualified Fuel Cell Property and Qualified Microturbine Property\" was created in 2008, but it appears to have expired as of 2013.\n\nIndividual taxpayers may claim several energy credits to reduce their Federal Income taxes, if they file the Long Form 1040 along with Form 5695 attached. These include the \"Resident energy efficient property tax credit\", and starting in the 2012 tax year, a \"nonbusiness energy property credit\". Some of these provisions were extended by ARRA (see below) and by later bills.\n\nAs part of the $787 Billion stimulus package or \"ARRA\" (technically the American Recovery and Reinvestment Act of 2009),\n\nUS law now allows rebates for energy efficient products and for weatherization. Energy law and policy are significantly affected by this new law.\n\nThe states affect energy in numerous ways, including taxes, land use controls, regulation of energy utilities, and energy subsidies. States may establish environmental standards stricter than those set by the federal government. Regulation of oil and gas production, particularly on non-federal land, is largely left up to the states.\n\nAlaska has vast energy resources:\n\nLikewise, Alaska receives a large proportion of its state revenues from its severance tax on oil: a full 68% of all revenue, much more than any state (only Wyoming coming close). Its dependence on petroleum revenues and federal subsidies allows it to have the lowest individual tax burden in the United States.\n\nThe state created the Alaska Permanent Fund from this \"golden egg\", which is owned and managed by the state, and \"created by a constitutional amendment\":\n\nThe constitutional provisions are found at Alaska Constitution Article IX, Section 15. Statutes regulate how the Fund is to be invested, as well as how the income is to be disbursed. Regulations state additional details regarding control of the Fund.\n\nThe Federal government runs the Alaska Natural Gas Transportation Projects, which are new pipelines, and its \"Federal Coordinator\" (director) is nominated with advice and consent of the Senate by the President of the United States. Drue Pearce was the first director; she was nominated by George W. Bush and served from December 13, 2006 through January 3, 2010. Larry Persilly is the current coordinator; he was nominated by Barack Obama on December 9, 2009, and was confirmed by the United States Senate on March 10, 2010.\n\nThe most populous state in the United States, California has gone through a series of energy crises, and has reacted with several laws concerning energy. The California Energy Code, or Title 24 of the California Code of Regulations, also titled \"The Energy Efficiency Standards for Residential and Nonresidential Buildings\", were established in 1978 in response to a legislative mandate to reduce California's energy consumption. The standards are updated periodically to allow consideration and possible incorporation of new energy efficiency technologies and methods, such as the Programmable Communicating Thermostat.\n\nCalifornia assesses an excise tax with the same basic rate of 18 cents per gallon on gasoline, diesel fuel, and gasohol. The state collects a relatively small 6.6 percent of its revenue from extraction and related taxes.\n\nAs a major energy producer, New Mexico has government offices related to energy, including the Energy Conservation and Management Division, which is part of the state's Energy, Minerals and Natural Resources Department. All of the major laws impacting energy are available from the Division's website. These include links to all of the state's statutes and related government websites, Federal and State regulations, and Executive orders.\n\nUranium mining in New Mexico had been significant from about 1950 until 1998. Several oil and gas companies developed uranium ore mines in New Mexico during that period. As of 2007, at least one company was evaluating development by in-situ leaching; there are potentially large deposits of coffinite and uranium oxide ores available in New Mexico.\n\nNew Mexico has enacted a number of new laws related to energy, including to create a New Mexico Renewable Energy Transmission Authority and to increase its renewable portfolio standards. According to one law firm's summary of President Obama's Economic Recovery Package, the state stands to gain much from the new administration, because \"New Mexico leaders and laboratories are at the forefront of energy policy.\" For example, former University of New Mexico Law School professor Suedeen G. Kelly was a member of the Federal Energy Regulatory Commission.\n\nThe state collects an effective rate of 18.875 cent per gallon tax on gasoline and gasohol, and 22.875 cents per gallon on diesel. Like many western states, it collects significant revenue from extraction taxes—20.9 percent of its overall sources.\n\nThe City of Albuquerque passed an ordinance to regulate \"efficiency standards for heating and cooling equipment,\" which was struck down by the U.S. District Court as violating the Commerce Clause of the U.S. Constitution.\n\nThe town board of Taos passed a \"strict new new building code\" in 2009 that mandated energy savings:\n\nThe town debated the proposed Ordinance 08-16, High Performance Building Ordinance, starting in October 2008, postponed it for legal review, debated it in February 2009, and passed it in March 2009.\n\nThe New Mexico Gas Company offers an Energy Star Home Rebate.\n\nIn October 2009, Governor Bill Richardson announced 21 grants for energy projects that are being funded by $8 Million in ARRA funds.\n\nNew York has an Energy Law. Under New York law, \"energy\" and \"energy resources\" are defined as:\n\nThe chief regulator is the \"Commissioner\" or \"president\" of the New York State Energy Research and Development Authority (also called NYSERDA). The board of directors of NYSERDA includes—as a matter of law—several utility insiders, as well as \"ex officio\" commissioners. Vincent DeIorio, a lawyer, is chairman of the board, and Francis J. Murray Jr. is President and CEO. NYSERDA was created as a public benefit corporation under NY law. New York's other regulatory bodies and entities include the New York State Public Service Commission, the New York State Department of Health, the New York Power Authority, and the Long Island Power Authority.\n\nIn addition to Energy Law, the state has a variety of laws regulating and taxing energy, and its courts have issued significant case law concerning energy taxes. Two trial court cases in 2012 allowed local zoning law to pre-empt state law by effectively banning hydrofracking, but this is being appealed.\n\nUnder New York law, both the New York Attorney General or a district attorney may prosecute alleged polluters who make oil spills. The state has enacted a number of recent laws to control carbon emissions.\n\nThe state collects an effective rate of 24.4 cent per gallon tax on gasoline and gasohol, and 22.65 cents per gallon on diesel. New York collects one of the smallest amounts of revenue from extraction taxes of any state—only 5.8 percent of its overall sources.\n\nThe Keystone state has been a major energy producer for over one hundred years. The state passed a Renewable energy law in Pennsylvania.\n\nOil, gas, and other energy resources are regulated by the powerful Texas Railroad Commission. It is the oldest regulatory agency in Texas, having been created in 1891. It \"oversee[s] the Texas oil and gas industry, gas utilities, pipeline safety, safety in the liquefied petroleum gas industry, and the surface mining of coal.\"\n\nAccording to \"Forbes\", the University of Houston has an \"exceptional\" energy policy and law program.\n\nThe state of Vermont, like other states, has a comprehensive statutory scheme governing energy generation and transmission issues, colloquially referred to as \"Section 248.\" The reference is to 30 V.S.A. Sec. 248, which is administered by Vermont's Public Service Board, a quasi-judicial board with three members. Section 248 is not to be confused with Vermont's comprehensive law governing land development and subdivision – Act 250. The state has an independent energy distributor, the Vermont Electric Cooperative.\n\nWyoming is the top coal producer of the 50 states in the United States, has significant oil and gas reserves, and its government and laws reflect an interest in energy production, especially fossil fuels. The Wyoming Oil and Gas Conservation Commission regulates many aspects of oil, coal, and gas development in this resource-rich state. There is an annual state Gas Fair. The University of Wyoming is well known for its research on energy development. The University sponsored a symposium on coal gasification in 2007.\n\nWyoming assesses an excise tax with the same rate of 14 cents per gallon on gasoline, diesel fuel, and gasohol. The state collects the largest percentage—46 percent of its revenue—from extraction and related taxes, the second highest of the states, surpassed only by Alaska.\n\n\"Governing\" has noted that starting in 2000, many observers have viewed the state's \"overreliance on minerals taxes\" to be \"fiscally unhealthy\", but it was rescued by the oil, gas, and coal boom; there remains a political wariness about imposing an income tax, yet in 2012 the state imposed a tax on wind turbines.\n\nFlorida and South Carolina have instituted utility fees to finance planned nuclear reactors.\n\nIndiana passed in 2009 a law \"that allows the state's finance authority to negotiate long-term contracts to buy and sell synthetic natural gas from a planned southern Indiana coal-gasification plant.\"\n\nMassachusetts Governor Deval Patrick successfully pushed for \"clean energy initiatives\" in the 2008 legislative session, calling it \"one of the most productive in a long, long time.\"\n\nNew Hampshire passed in 2008 an energy law, signed by Governor John Lynch, which \"provides guidelines for residential wind energy systems ... such as height, noise, setbacks and aesthetics and outlines a process for input from neighbors.\" This was found necessary because a University of New Hampshire student, Laura Carpenter, found that \"most communities had no ordinances or zoning rules that specifically address small residential wind turbines.\"\n\nOhio requires utilities to meet regulatory goals for conservation.\n\n\n"}
{"id": "247993", "url": "https://en.wikipedia.org/wiki?curid=247993", "title": "Wadden Sea National Parks", "text": "Wadden Sea National Parks\n\nThe Wadden Sea National Parks in Denmark, Germany and the Netherlands are located along the German Bight of the North Sea. In Germany and Denmark they also mark the area of the UNESCO World Heritage Site of the Wadden Sea. Divided from each other by administrative borders, they form a single ecological entity. The purpose of the national parks is the protection of the Wadden Sea ecoregion.\n\n\n\n\n"}
