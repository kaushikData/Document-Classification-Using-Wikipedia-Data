{"id": "56242574", "url": "https://en.wikipedia.org/wiki?curid=56242574", "title": "2017 YZ1", "text": "2017 YZ1\n\n is a member of the Apollo asteroids, which cross the orbit of Earth. Apollo's are the largest group of near-Earth objects with nearly 10 thousand known objects.\n\nIt orbits the Sun at a distance of 0.88–1.77 AU once every 18 months (559 days; semi-major axis of 1.33 AU). Its orbit has an eccentricity of 0.33 and an inclination of 21° with respect to the ecliptic.\n\nThe 9 January 2018 solution with a 15-day observation arc was listed at Torino scale 1 with a 1:21,000 chance of impacting Earth on 30 June 2047. By 9 January 2018, the geocentric 30 June 2047 uncertainty region had shrunk to ±50 million km. With a longer 20 day observation arc, it dropped to Torino scale 0 and had a 1:670,000 chance of impacting Earth on 30 June 2047. On 18 January 2018 it was removed from the Sentry Risk Table. With a 28-day observation arc, the nominal solution suggests it will be about from Earth on 30 June 2047. The 3-sigma uncertainty in the 2047 close approach distance is about ±13 million km.\n\nThis minor planet has neither been numbered nor named.\n\n\n"}
{"id": "8046543", "url": "https://en.wikipedia.org/wiki?curid=8046543", "title": "Akdalaite", "text": "Akdalaite\n\nAkdalaite is a very rare mineral found in Kazakhstan and has the formula 5AlO·HO. It was formerly believed to be 4AlO·HO. It is therefore the same as tohdite an artificially produced phase. Studies on the crystal structure and spectra indicate that this is an aluminium oxide hydroxide.\n\n"}
{"id": "5827326", "url": "https://en.wikipedia.org/wiki?curid=5827326", "title": "Alfred de Grazia", "text": "Alfred de Grazia\n\nAlfred de Grazia (December 29, 1919 – July 13, 2014), born in Chicago, Illinois, was a political scientist and author. He developed techniques of computer-based social network analysis in the 1950s, developed new ideas about personal digital archives in the 1970s, and defended the catastrophism thesis of Immanuel Velikovsky.\n\nHis father, Joseph Alfred de Grazia, was born in Licodia, province of Catania, in Sicily and was politically active in a troubled period in the history of the island. He emigrated to the United States at the age of twenty, after having hit the mayor of Licodia with his clarinet during a political scuffle. He became a bandmaster, music teacher, in and out of the WPA and a musical union leader in Chicago. In 1916, he married Chicago-born Katherine Lupo Cardinale whose parents had emigrated from Sicily. Her brother was the boxer Charles Kid Lucca, Canadian champion welter-weight champion from 1910-1914. They had three more sons, Sebastian de Grazia, winner of the Pulitzer Prize, Edward de Grazia, a prominent first amendment lawyer and co-founder of Cardozo School of Law at Yeshiva University, and Victor de Grazia who was Deputy-Governor of the State of Illinois from 1973 to 1977 .\n\nDe Grazia attended the University of Chicago, receiving an A.B. there in 1939, attended law school at Columbia University from 1940–1941, and in 1948 earned a PhD in political science from the University of Chicago. His thesis was published in 1951 as \"Public and Republic: Political Representation in America\". When reviewed by \"The New York Times\" it was called \"A thoroughgoing examination of the meaning of representation, the fundamental element in any definition of republic.\" and August Heckscher in the \"New York Herald Tribune\" said it was \"A sober scholarly volume, authoritative in its field.\" Charles E. Merriam, founder of the behavioristic approach in political science, wrote: \"All scholars in the field of political science and particularly those in the area of representation are under lasting obligation to the writer of this volume for a learned and helpful treatment of one of the major problems of our times. The book will enrich the literature on this very important subject.\"\n\nIn World War II, de Grazia served in the United States Army, rising from private to captain. He specialized in mechanized warfare, intelligence and psychological warfare. He received training in this then new field in Washington D.C. and the newly established Camp Ritchie in Maryland. He served with the 3rd, 5th and 7th Armies and as a liaison officer with the British 8th Army. He took part in six campaigns, from North Africa to Italy (Battle of Monte Cassino) to France and Germany.\n\nDe Grazia co-authored a report on psychological warfare for the Supreme Headquarters of the Allied Expeditionary Force. By the end of the war, he was Commanding Officer of the Psychological Warfare Propaganda Team attached to the headquarters of the 7th Army. With his fiancée and later wife, wife Jill deGrazia (née Bertha Oppenheim), he carried on an extensive wartime correspondence of over 2,000 lengthy letters, published on the web under the title \"Letters of Love and War\". Scott Turow cites the letters as being among the sources for his 2005 novel \"Ordinary Heroes\"\n\nDe Grazia wrote manuals of psychological warfare for the CIA for the Korean War and organized and investigated psychological operations for the Department of Defence during the Vietnam War. His reports on psychological operations, now largely declassified, include \"Target Analysis and Media in Propaganda to Audiences Abroad\" (1952), \"Elites Analysis\" (1955), as well as \"Psychological Operations in Vietnam\" (1968). On October 31, 2014, he posthumously made a Distinguished Member of the Regiment of Psychological Operations of the Special Operations Command at Fort Bragg, North Carolina.\n\nFor his service in World War II, de Grazia earned the Bronze Star and the EAME Campaign Medal, as well as the Croix de Guerre from France. On December 31, 2013, he was awarded the highest French distinction, being made a Chevalier of the Legion of Honor by decree of President François Hollande.\nHe is also a recipient (posthumously) of the MG Robert A. McClure Medal for Exemplary Service in Psychological Operations\n\nDe Grazia was assistant professor of political science at the University of Minnesota, Minneapolis from 1948 to 1950, before becoming associate professor of political science at Brown University. In 1952, he was appointed to be Director of the Committee for Research in the Social Sciences at Stanford University, supported by a Ford Foundation grant. He wrote the textbook \"The Elements of Political Science\" in two volumes: \"Political Behavior\" and \"Political Organization\" (1952). One reviewer of it wrote: \"Mr. De Grazia has undertaken to dissect the whole body of political science... He achieves his purpose with unfailing clarity, and his readers will learn from him the range, the goals, and the techniques of the study of politics...\"\n\nIn 1955, he was turned down for academic tenure at Stanford after doing a study of \"the origins and present restrictions on the political activities of workers\" for a foundation. From 1959, he was professor of government and social theory at New York University.\n\nIn 1957 de Grazia founded \"PROD: Political Research: Organization and Design\", which was described as \"probably...the authentic spokesman for the newest currents among the \"avant-garde\" of political behavior\". It was later renamed \"The American Behavioral Scientist\", an academic journal devoted to the Chicago school of behaviorist sociology. In 1965, he began the \"Universal Reference System,\" the first computerized reference system in the social sciences.\n\nDe Grazia was a staunch supporter of the power of Congress against the encroachments of the Presidency, which he called the \"Executive Force\" According to Raymond Tatalovich and Steven Schier:\n\nThe American Enterprise Institute published several of his books on the subject, including \"Congress and the Presidency: their Role in Modern Times\", a debate with Arthur M. Schlesinger Jr., who defended the case for a strong Presidency.\n\nDe Grazia became interested in Immanuel Velikovsky's catastrophist theories. Following considerable criticism of Velikovsky's claims by the scientific community, de Grazia dedicated the entire September 1963 issue of \"American Behavioral Scientist\" to the issue. He also self-published two books on it, \"The Velikovsky Affair: The Warfare of Science and Scientism\" and \"Cosmic Heretics: A Personal History of Attempts to Establish and Resist Theories of Quantavolution and Catastrophe in the Natural and Human Sciences\".\n\nMichael Polanyi stated:\n\nIn a review of the second book, Henry Bauer suggests that de Grazia's efforts may be responsible for Velikovsky's continuing notability.\n\nIn both books de Grazia subscribes to the thesis that, in the words of Henry Bauer, \"the affair revealed something seriously rotten in the state of science\". The review however suggests that the rejection came about…\n\nThe review further suggests that \"de Grazia does not understand how the content of science is generated\" and that his \"understanding of science as a social activity is ambiguous.\"\n\nIn the second book, de Grazia upholds Velikovsky's most general claim, that geologically recent (in the last 15,000 years) extraterrestrially-caused catastrophes occurred, and had a significant impact on the Earth and its inhabitants. De Grazia terms this belief \"Quantavolution\".\n\nIn the early 1970s, de Grazia founded the \"University of the New World\" in Haute-Nendaz Switzerland, as an unstructured alternative to American universities. He invited Beat author William S. Burroughs to teach at it. In his biography of Burroughs, Ted Morgan described the students that it attracted as \"drifters and dropouts on the international hippie circuit\"; he suggested that this resulted in a culture clash with the \"prim Swiss\", and that the university lacked adequate facilities or a sound business model.\n\nIn 2002, de Grazia was appointed visiting professor in the Department of Mathematics, Statistics, Computing and Applications of the University of Bergamo in Italy. He had previously been a visiting lecturer at the University of Rome, the University of Bombay, the University of Istanbul, and the University of Gothenburg in Sweden.\n\nAlfred de Grazia was married to Jill Oppenheim (d. 1996) from 1942 to 1971, to Nina Mavridis from 1972 to 1973, and from 1982 to his death to Anne-Marie (Ami) Hueber - de Grazia, a French writer \n\nHe had seven children with Jill Oppenheim. One of them, Carl, a musician, died in 2000. One of his daughters, Victoria de Grazia, a Professor of Contemporary History at Columbia University, is a member of the American Academy.\n\n\n\nNotes\nFurther reading\n\n"}
{"id": "43549657", "url": "https://en.wikipedia.org/wiki?curid=43549657", "title": "Aquaman (film)", "text": "Aquaman (film)\n\nAquaman is a 2018 American superhero film based on the DC Comics character of the same name, distributed by Warner Bros. Pictures. It is the sixth installment in the DC Extended Universe (DCEU). Directed by James Wan, with a screenplay by David Leslie Johnson-McGoldrick and Will Beall, from a story by Geoff Johns, Wan and Beall, it stars Jason Momoa as the title character, with Amber Heard, Willem Dafoe, Patrick Wilson, Dolph Lundgren, Yahya Abdul-Mateen II, and Nicole Kidman in supporting roles. It is the third live-action theatrical film featuring the title character, following \"\" (2016) and \"Justice League\" (2017) and the first full-length feature film centered around the character. In \"Aquaman\", Arthur Curry, the heir to the underwater kingdom of Atlantis, must step forward to lead his people against his half-brother, Orm, who seeks to unite the seven underwater kingdoms against the surface world.\n\nDevelopment of an Aquaman film began in 2004, with several plans falling through over the years. In August 2014, Beall and Kurt Johnstad were hired to write two competing scripts and the film was officially announced in October 2014. Wan signed on as director in April 2015 and in July 2016 it was announced the film would move forward with Beall's screenplay, although Wan, Johnstad, Johns and Johnson-McGoldrick all performed various rewrites. The main cast was confirmed throughout 2016 and early 2017. Principal photography began in Australia on May 2, 2017. Most of the film was shot at Village Roadshow Studios in Gold Coast, Queensland, with production also held in Canada, Italy and Morocco, and wrapped on October 21, 2017.\n\n\"Aquaman\" had its world premiere in London on November 26, 2018, and is scheduled to be released in the United States by Warner Bros. Pictures in Real D 3D, Dolby Cinema, IMAX and IMAX 3D on December 21, 2018.\n\nArthur Curry, the reluctant ruler of Atlantis and King of the Seven Seas, finds himself caught between a surface world constantly ravaging the sea and Atlanteans looking to lash out in revolt, but committed to protecting the entire globe.\n\n\nAdditionally, Ludi Lin portrays Murk, the Captain of the Men-of-War, the frontline army of Atlantis, while Temuera Morrison portrays Thomas Curry, a lighthouse keeper who is Arthur Curry's father. Randall Park portrays Dr. Stephen Shin, a marine biologist who was friends with Thomas Curry, and Michael Beach portrays Jesse Kane, a member of a group of boatjackers and David Kane's father.\n\nDjimon Hounsou, Natalia Safran, and Sophia Forrest provide the motion-capture of the Fisherman King, the Fisherman Queen, and the Fisherman Princess respectively, whom Orm creates an alliance with as part of his plan to unite the seven kingdoms of the sea. Graham McTavish has been cast in an undisclosed role. Leigh Whannell, Wan's long time collaborator, appears in the film as a plane pilot. Julie Andrews provides the voice of Karathen, a mythical sea creature that allies with Aquaman.\n\nIn 2004, FilmJerk.com reported that Sunrise Entertainment's Alan and Peter Riche planned to bring Aquaman to the big screen for Warner Bros. with Robert Ben Garant writing the screenplay. However, the film fell through. In July 2009, it was reported that \"Aquaman\" was in development at Leonardo DiCaprio's Appian Way. Warner chairman and CEO Barry Meyer said that the \"Aquaman\" film was in development. A source from Warner Bros. told The Wrap that they were discussing the possibilities, with the mention of more \"Man of Steel\" movies as well as a Superman/Batman film, a Wonder Woman film, and an Aquaman film. Geoff Johns told \"Variety\" that Aquaman is a priority character for the company. It was announced on August 12, 2014, that Warner Bros. had hired screenwriters Will Beall and Kurt Johnstad to pen two separate scripts for an upcoming Aquaman film. The film was being developed on dual tracks, meaning that two scripts were written, one by Beall and one by Johnstad, but only the better version would move forward.\n\nOn April 10, 2015, \"The Hollywood Reporter\" reported that James Wan was the frontrunner to direct the film. In June 2015, Wan was confirmed to direct the film and overlook the screenplay by Johnstad. On November 12, 2015, David Leslie Johnson-McGoldrick was hired to write the script, however it was unclear whether he would be writing a separate script or working with Wan. It was then revealed that previous script plans had been scrapped and that both Wan and Johns planned to move forward with a new script written by screenwriter Will Beall. Later, Johnson-McGoldrick was brought back to the project to work on a rewrite of Beall's script.\n\nOn March 2016, it was announced that the events of \"Aquaman\" will be set after \"Justice League\". Wan confirmed later on Twitter that cinematographer Don Burgess, who previously worked with Wan on \"The Conjuring 2\", will serve as cinematographer for \"Aquaman\". Pre-production began in Australia in late November 2016.\n\nOn October 20, 2014, in an interview with ComicBook.com, Jason Momoa revealed that the \"Justice League\" film would be coming first and that's what they were preparing for, and he didn't know if the solo \"Aquaman\" film would come before or after \"Justice League\". He thought it might be the origin, where Aquaman came from. Warner Bros. announced \"Aquaman\" as a part of the DC Extended Universe, with Jason Momoa starring. In December 2014, it was revealed that Momoa had signed a four-picture deal with the studio and DC, and he wanted Zack Snyder to direct the solo \"Aquaman\" film. On January 13, 2016, \"The Hollywood Reporter\" announced that Amber Heard entered negotiations to play the female lead role of Mera, Aquaman's love interest; her casting was confirmed two months later. In April 2016, Willem Dafoe was cast in an undisclosed role, later revealed to be Nuidis Vulko. On December 12, 2016 it was confirmed that Patrick Wilson will play the villainous Ocean Master, the half-brother of Aquaman. On January 31, 2017, Yahya Abdul-Mateen II was added to the cast as the villain, Black Manta. That same day, the press reported that Nicole Kidman had entered talks to play Queen Atlanna. Two months later, Kidman confirmed her participation in the film. By February 2017, New Zealand actor Temuera Morrison entered talks to play Thomas Curry, Aquaman's human father. On April 12, Dolph Lundgren was cast to play Nereus, king of Xebel. Ludi Lin was cast in the film on May 15, 2017. Almost two weeks later, Michael Beach, who voiced Devil Ray, a character loosely based on Black Manta in \"Justice League Unlimited\", was cast as Black Manta's father. In October 2017, Graham McTavish revealed that he has a role in the film. In April 2018, Randall Park was cast as Dr. Stephen Shin. In July, Djimon Hounsou, Natalia Safran and Sophia Forrest were cast as the Fisherman King, Fisherman Queen and the Fisherman Princess. In November 2018, it was revealed that Julie Andrews has a voice role in the film.\n\nPrincipal photography began in Australia on May 2, 2017, under the working title \"Ahab\". A majority of the film was shot at Village Roadshow Studios in Gold Coast, Queensland with production also held in Newfoundland, Canada, as well as Sicily and Morocco. Between May and August 2017, production also took place on location around a number of places of the Australian Gold Coast including Main Beach, Coomera, Southport and Amity Point in North Stradbroke Island, Queensland, as well as Hastings Point in New South Wales. On filming underwater sequences, Wan stated that \"the underwater world is super complicated\" and \"it's not an easy shoot\". On August 11, 2017, filming began on the Arthur Curry Lighthouse set at Hastings Point and ended later that month. In September, during an interview with Kiss Radio, actor Ludi Lin described Wan's vision for the film as 'Star Wars underwater'. That same month, filming took place in Newfoundland and Labrador. Willem Dafoe finished up his part by late September. On October 13, James Wan announced that Patrick Wilson wrapped on the film. Filming on location took place in the deserts of Morocco by mid-October, which included the cities of Merzouga and Erfoud. Principal photography wrapped on October 21, 2017.\n\nJames Wan's five-time collaborator Kirk M. Morri served as the editor for \"Aquaman\". Two-time Academy Award winner Charles Gibson (\"Babe\" and \"\"), and Kelvin McIlwain (\"The Fast and the Furious\" franchise) served as overall visual effects supervisors. Rodeo FX, Scanline VFX, Moving Picture Company (MPC), Industrial Light & Magic (ILM), Weta Digital, Method Studios, and Digital Domain provided the visual effects for the film. On November 3, 2018, Wan announced that post-production on the film was completed.\n\nOn March 7, 2018, Rupert Gregson-Williams was announced as the composer for \"Aquaman\". Gregson-Williams previously wrote the score for \"Wonder Woman\", the fourth film in the DC Extended Universe. The soundtrack will be released by WaterTower Music on December 14, 2018. The album will feature an original song by American musician Skylar Grey entitled \"Everything I Need\", written by Grey and Elliott Taylor.\n\nIn March 2017, a first look at \"Aquaman\" was shown prior to shooting during the CinemaCon convention in Las Vegas, Nevada with Momoa introducing a video of director James Wan showing off a concept art sizzle reel for the movie. Later, on July 22, the film's first footage made its debut at San Diego Comic-Con (SDCC) 2017 with a teaser presented by Momoa during the Warner Bros.'s panel at Hall H; director James Wan presented the footage stating that \"in a lot of ways, this is an origin story\", referring to the film. In April 2018, another teaser with new rough footage was shown off by Wan and Momoa at CinemaCon, joined by Amber Heard, Patrick Wilson and Yahya Abdul-Mateen II on stage. In an interview with \"Entertainment Weekly\" during the event, Wan teased the conflict between Arthur Curry and his half-brother and main antagonist in the film, Orm / Ocean Master, stating that \"it's almost a very classic Shakespearean story about brother from another world vs. brother from another world. And it really is a classic story of sibling rivalry\". On June 11, 2018, the film's first trailer was previewed at the European exhibitors' conference CineEurope in Barcelona, Spain. A first look at Black Manta, Ocean Master, Queen Atlanna and Nuidis Vulko were revealed by \"Entertainment Weekly\" on June 14, 2018. The next day, director Wan announced that the first trailer would premiere at San Diego Comic-Con (SDCC) 2018, during the Warner Bros.'s panel.\n\nOn July 16, 2018, an official teaser poster was released. On July 21, 2018, the first trailer was released at the SDCC 2018, being considered the best received trailer during the international convention; it was attached to theatrical showings of \"Teen Titans Go! To the Movies\", \"\", \"The Meg\", \"The Predator\", and \"Venom\". The cast also appeared as guests at the late-night talk show Conan with Conan O'Brien during SDCC, on Sunday, July 22. On October 5, 2018, a 5-minute Extended Video was released by Warner Bros. It received positive reactions by audiences, with praise directed towards the special effects, action, cinematography, and faithfulness to the comic book. The first official TV spot for the film was released by the studio on October 16, 2018, followed by a second one on November 1, 2018. The same month, character posters were released for Aquaman, Mera, Black Manta, Ocean Master, King Nereus, Queen Atlanna and Nuidis Vulko. \n\nOn November 7, 2018, the studio announced the schedule for the worldwide promotion tour, taking place during the months of November and December, with fan events, screenings and premieres in major cities around the globe, including Beijing, London, New York City, Manila, Los Angeles, Miami, Gold Coast, Sydney, and Hawaii. Additionally, it was announced that the film will be screened on December 7, 2018, during Brazil Comic Con (CCXP) in São Paulo. The following week, an official behind the scenes featurette was released, which included footage not seen before in the mainstream trailers. Two days later, the film's two main posters were released, with Aquaman and Mera showing off their comic-accurate suits. On November 19, 2018, the final trailer for the film was released, alongside the announcement of the ticket sales beginning. The same day, a 30-minute footage was shown in China during the first stop of the film promotion tour, generating rave reactions among the attendees.\n\n\"Aquaman\" had its world premiere at the Empire, Leicester Square in London on November 26, 2018. It is scheduled to be released in the United States by Warner Bros. Pictures in RealD 3D, Dolby Cinema, IMAX and IMAX 3D on December 21, 2018. It was previously set for July 27, 2018, and then moved to October 5, 2018, before settling on its December release date. Internationally, the film will be released in China on December 7, 2018, in the United Kingdom on December 12, 2018, and in Brazil and Russia on December 13, 2018. On November 19, 2018, Atom Tickets was announced that Amazon Prime members in the United States would have early access to tickets for a December 15 screening of the film at select Regal, National Amusements, ArcLight Cinemas, and AMC theaters. \n\nIn the United States and Canada, \"Aquaman\" will be released alongside \"Bumblebee\", \"Second Act,\" and \"Welcome to Marwen\", and is projected to gross around $65 million in its opening weekend, and $100 million over its first five days. However, inside analysts told \"TheWrap\" that the film could reach $100 million in its opening weekend alone, which would make it the first film to do so since \"\" in June. \n\nThe day after announcing the early Amazon screenings, the film's first 24 hour pre-sale totals became the highest in the history of Atom Tickets, beating out \"\", as well as outpacing \"\", another film Amazon Prime offered early to subscribers the previous December. On Fandango, it also set the record for number of tickets sold, beating out \"Venom\".\n\n"}
{"id": "6801845", "url": "https://en.wikipedia.org/wiki?curid=6801845", "title": "Baraba steppe", "text": "Baraba steppe\n\nThe Baraba steppe, also known as Barabinsk steppe, Barabinskaya steppe , is a grassland steppe and wooded flat plain situated in western Siberia.\n\nThe steppe has an area of 117,000 km² and stretches between the Irtysh and the Ob Rivers. Barabinsk is the largest city on the steppe. The Baraba steppe also contains an important Russian agricultural district.\n\n"}
{"id": "43646640", "url": "https://en.wikipedia.org/wiki?curid=43646640", "title": "Biodiversity of Westchester County, New York", "text": "Biodiversity of Westchester County, New York\n\nWestchester County is located in southern New York, sharing its southern boundary with New York City and its northern border with Putnam County. It is bordered on the west side by the Hudson River and on the east side by the Long Island Sound and Fairfield County, Connecticut. The county has a total area of 500 square miles (1,300 km2), of which 430 square miles (1,100 km2) is land and 69 square miles (180 km2) (14%) is water. It is an area rich in biodiversity with many parks, preserves, and the Edith G. Read Wildlife Sanctuary, a 179-acre sanctuary along Long Island Sound migratory flyway. In winter months, the 85-acre lake hosts more than 5,000 ducks. Recognized by Audubon New York. Literary environmental writer Alex Shoumatoff hailed Westchester County as the \"most richly diversified deciduous forest in the world\" in a 1978 \"The New Yorker\" profile, at the time estimating 4,200 species.\n\nThere are 1,168 species of vascular plants in Westchester County, according to the Parks Department. \n\nEndangered plants:\n\nThreatened plants:\n\nSpecial concern plants:\n\nInvasive plants:\n\nThere are 311 species of birds in Westchester County, as documented by the Parks Department. The local Audubon Society chapter records 368 bird species.\n\nEndangered birds:\n\nThreatened birds:\n\nSpecial Concern birds:\n\nThere are 33 species of mammals in Westchester County.\n\nSpecial concern mammals:\n\nThere are 19 species of reptiles in Westchester County.\n\nEndangered reptiles:\n\nThreatened reptiles:\n\nSpecial concern reptiles:\n"}
{"id": "14064345", "url": "https://en.wikipedia.org/wiki?curid=14064345", "title": "Cannington Manor Provincial Park", "text": "Cannington Manor Provincial Park\n\nCannington Manor Provincial Park is a historic park which was established in 1882 by Captain Edward Michell Pierce (died June 20, 1888) as an aristocratic English colony. Cannington Manor is located west and north of Saskatchewan Highway 603. \n5 townships The Colony is 16 kilometres southeast of Moose Mountain Provincial Park, and south of Moosomin. \n\nCaptain Pierce established an agricultural college and attracted \"remittance men\" as students for £100 a year. The intention of the college was to instruct these bachelor sons of wealthy families to farm and homestead in the last best west. The brothers Ernest, Billy and Bertie Beckton constructed \"Didsbury\", ranch house within Cannington Manor colony. \n\nThe cultural and recreational life emulated English upper class society. Thoroughbred racing, polo matches, theatrical plays, fox hunting, billiards, soccer and tennis were all enjoyed by the colony students and settlers. This was a contrast to the neighbouring homesteaders who were barely eking out a living proving their land and making improvements to earn land title grants from the Dominion Government. \n\nSoon a dairy, a school / town hall, blacksmith, Moose Mountain trading company store, Harold Fripp flour mill, C.E. Phipps Land Titles Office, carpenter shop, Mitre hotel were built to support a burgeoning community which soon reached 200 residents.\n\nCaptain Pierce, the founder of the community died on June 20, 1888. C.E. Phipps established the land titles office here in 1889 and moved to Oxbow in 1891. Spencer Page was the colony's first teacher in 1889 before turning to politics in 1894. Louis Kent the second teacher in Cannington Manor started his duties in 1894 and passed on in 1896. Robert Bird the Moose Mountain trading company store keeper moved to B.C. in 1898. Ernest Maltby postmaster and his wife Mary arrived in 1892 moved in 1901 to B.C. \n\nIn 1901 - 1902 the CPR regional branch line was constructed south of the village rather than through the village. During a time of travel by horseback this distance was detrimental to the growth of the community. The passing of the founder, a few years earlier, a drought and low grain prices soon made it unfeasible for settlers to continue the lifestyle at Cannington Manor.\n\n\n"}
{"id": "56213092", "url": "https://en.wikipedia.org/wiki?curid=56213092", "title": "Christkindlmarkt at Marienplatz", "text": "Christkindlmarkt at Marienplatz\n\nThe Christkindlmarkt at Marienplatz is a Christmas market at Marienplatz in Munich.\n\nThe largest and oldest Christmas market in Munich was first documented as a Nikolaimarkt (Nikolausmarkt) in 1310, making it one of the oldest Christmas markets in the German-speaking world. In 1806, the Nikolaimarkt was renamed the Christmarkt. Since 1972, the market has found its place of business on the Marienplatz after several changes of location. Over the last few years, the exhibition area has been extended considerably (e.g. Rindermarkt). The organizer is the cultural office of the city administration. \n\nWith around 140 market stands, it now has around three million visitors every year from all over the world. In addition to the extensive musical-cultural accompanying program, the almost 30-meter-high Christmas tree in front of the town hall, with around 2,500 lights, is donated every year to the citizens of Munich by a different town from Austria or Italy. As a reward for the gift, the municipality can present itself in the inner courtyard of the town hall and operate a Glühwein stand. The high interest of non-municipal communities for this in this self-presentation position, forced the organizers to create a multi-faceted waiting list. The market is usually open daily from the Friday before the first Advent Sunday, until Christmas Eve.\n\nTo the west of the Marienplatz, in the direction of Stachus, is the so-called Kripperlmarkt. More than ten stands around the Richard Strauss fountain in the Neuhauserstraße offer only Nativity scenes, figures, and other accessories. There are also some market stands offering food and drinks.\nThe Kripperlmarkt in the Munich pedestrian zone is open daily and also closes on 24 December (Christmas Eve) at 2 pm.\n\n"}
{"id": "1242991", "url": "https://en.wikipedia.org/wiki?curid=1242991", "title": "Clay–water interaction", "text": "Clay–water interaction\n\nClay-water interaction is an all-inclusive term to describe various progressive interactions between clay minerals and water. In the dry state, clay packets exist in face-to-face stacks like a deck of playing cards, but clay packets begin to change when exposed to water. Five descriptive terms describe the progressive interactions that can occur in a clay-water system, such as a water mud.\n\n(1) Hydration occurs as clay packets absorb water and swell.\n\n(2) Dispersion (or disaggregation) causes clay platelets to break apart and disperse into the water due to loss of attractive forces as water forces the platelets farther apart.\n\n(3) Flocculation begins when mechanical shearing stops and platelets previously dispersed come together due to the attractive force of surface charges on the platelets.\n\n(4) Deflocculation, the opposite effect, occurs by addition of chemical deflocculant to flocculated mud; the positive edge charges are covered and attraction forces are greatly reduced.\n\n(5) Aggregation, a result of ionic or thermal conditions, alters the hydrational layer around clay platelets, removes the deflocculant from positive edge charges and allows platelets to assume a face-to-face structure.\n"}
{"id": "44160275", "url": "https://en.wikipedia.org/wiki?curid=44160275", "title": "Clinton Conservation Park", "text": "Clinton Conservation Park\n\nClinton Conservation Park is a protected area located on the coastline at the north end of Gulf St Vincent in South Australia. The conservation park consists of two parts with the first extending from Clinton on Yorke Peninsula on the west side of Gulf St Vincent to Port Wakefield on the east side of the gulf and the second extending south of Port Wakefield to Sandy Point. The conservation park which was originally proclaimed in 1970, was re-proclaimed under the \"National Parks and Wildlife Act 1972\". The area contained within the conservation park including the ‘head of gulf wetland and Wakefield River estuary’ is considered to be ‘important as a fish nursery and a significant site for migratory wading birds’. The conservation park is classified as an IUCN Category III protected area.\n\n\n"}
{"id": "421129", "url": "https://en.wikipedia.org/wiki?curid=421129", "title": "Cob (material)", "text": "Cob (material)\n\nCob, cobb or clom (in Wales) is a natural building material made from subsoil, water, fibrous organic material (typically straw), and sometimes lime. The contents of subsoil naturally vary, and if it does not contain the right mixture it can be modified with sand or clay. Cob is fireproof, resistant to seismic activity, and inexpensive. It can be used to create artistic, sculptural forms, and its use has been revived in recent years by the natural building and sustainability movements.\n\nIn technical building and engineering documents such as the Uniform Building Code, cob may be referred to as an \"unburned clay masonry\" when used in a structural context. It might also be referred to as an \"aggregate\" in non-structural contexts, such as a \"clay and sand aggregate\" or more simply an \"organic aggregate,\" such as where the cob is an insulating filler between post and beam construction.\n\n\"Cob\" is an English term attested to around the year 1600 for an ancient building material that has been used for building since prehistoric times. The etymology of \"cob\" and \"cobbing\" is unclear, but in several senses means to \"beat\" or \"strike\", which is how cob material is applied to a wall.\n\nSome of the oldest man-made structures in Afghanistan are composed of rammed earth and cob. Cobwork (\"tabya\") was used in the Maghreb and al-Andalus in the 11th and 12th centuries, and was described in detail by Ibn Khaldun in the 14th century.\n\nCob material is known by many names including \"adobe\", \"lump clay\", \"puddled clay\", \"chalk mud\", \"wichert\", \"clay daubins\", \"swish\" (Asante Twi), \"torchis\" (French), \"bauge\" (French), \"bousille\" (French mud with moss), and \"cat and clay\".\n\nCob structures can be found in a variety of climates across the globe. European examples include:\n\n\nMany old cob buildings can be found in Africa, the Middle East, and many parts of the southwestern United States. A number of cob cottages survive from mid-19th-century New Zealand.\n\nTraditionally, English cob was made by mixing the clay-based subsoil with sand, straw and water using oxen to trample it. English soils contain varying amounts of chalk, and cob made with significant amounts of chalk are called \"chalk cob\" or \"wychert\". The earthen mixture was then ladled onto a stone foundation in courses and trodden onto the wall by workers in a process known as \"cobbing\". The construction would progress according to the time required for the prior course to dry. After drying, the walls would be trimmed and the next course built, with lintels for later openings such as doors and windows being placed as the wall takes shape.\n\nThe walls of a cob house are generally about thick, and windows were correspondingly deep-set, giving the homes a characteristic internal appearance. The thick walls provided excellent thermal mass which was easy to keep warm in winter and cool in summer. Walls with a high thermal mass value act as a thermal buffer inside the home.\nThe material has a long life-span even in rainy and/or humid climates, provided a tall foundation and large roof overhang are present.\n\nCob is fireproof, while \"fire cob\" (cob without straw or fiber) is a refractory material (the same material, essentially, as unfired common red brick), and historically, has been used to make chimneys, fireplaces, forges and crucibles. Without fiber, however, cob loses most of its tensile strength.\n\nWhen Kevin McCabe constructed a two-story, four bedroom cob house in England, UK in 1994, it was reputedly the first cob residence built in the country in 70 years. His techniques remained very traditional; the only innovations he made were using a tractor to mix the cob and adding sand or shillet, a gravel of crushed shale, to reduce shrinkage.\n\nIn 2000-01, a modern, four bedroom cob house in Worcestershire, England, UK, designed by Associated Architects, was sold for £999,000. Cobtun House was erected in 2001 and won the Royal Institute of British Architects' Sustainable Building of the Year award in 2005. The total construction cost was £300,000, but the metre-thick outer cob wall cost only £20,000.\n\nIn the Pacific Northwest of the United States there has been a resurgence of cob construction, both as an alternative building practice and one desired for its form, function, and cost effectiveness. Pat Hennebery, Tracy Calvert, Elke Cole, and the Cobworks workshops erected more than ten cob houses in the Southern Gulf Islands of British Columbia, Canada.\n\nIn 2010, Sota Construction Services in Pittsburgh, Pennsylvania, United States completed construction on its new 7,500 square foot corporate headquarters, which featured exterior cob walls along with other energy saving features like radiant heat flooring, a rooftop solar panel array, and daylighting. The cob walls, in conjunction with the other sustainable features, enabled the edifice to earn a LEED Platinum rating in 2012, and it also received one of the highest scores by percentage of total points earned in any LEED category.\n\nIn 2007, Ann and Gord Baird began constructing a two-storey cob house in Victoria, British Columbia, Canada for an estimated $210,000 CDN. The home of 2,150 square feet includes heated floors, solar panels, and a southern exposure to enable passive solar heating.\n\nWelsh architect Ianto Evans and researcher Linda Smiley refined the construction technique known as \"Oregon Cob\" in the 1980s and 1990s. Oregon Cob integrates the variation of wall layup technique which uses loaves of mud mixed with sand and straw with a rounded architectural stylism. They are experimenting with a mixture of cob and straw bale denominated \"balecob\".\n\n\n\n"}
{"id": "42823898", "url": "https://en.wikipedia.org/wiki?curid=42823898", "title": "Das Kapital", "text": "Das Kapital\n\nDas Kapital, also known as Capital. Critique of Political Economy (, ; 1867–1883) by Karl Marx is a foundational theoretical text in materialist philosophy, economics and politics. Marx aimed to reveal the economic patterns underpinning the capitalist mode of production, in contrast to classical political economists such as Adam Smith, Jean-Baptiste Say, David Ricardo and John Stuart Mill. Marx did not live to publish the planned second and third parts, but they were both completed from his notes and published after his death by his colleague Friedrich Engels. \"Das Kapital\" is the most cited book in the social sciences published before 1950.\n\nIn \"Das Kapital\" (1867), Marx proposes that the motivating force of capitalism is in the exploitation of labor, whose unpaid work is the ultimate source of surplus value. The owner of the means of production is able to claim the right to this surplus value because he or she is legally protected by the ruling regime through property rights and the legally established distribution of shares which are by law only to be distributed to company owners and their board members. The historical section shows how these rights were acquired in the first place chiefly through plunder and conquest and the activity of the merchant and \"middle-man\". In producing capital (produced goods), the workers continually reproduce the economic conditions by which they labour. \"Capital\" proposes an explanation of the \"laws of motion\" of the capitalist economic system, from its origins to its future, by describing the dynamics of the accumulation of capital, the growth of wage labour, the transformation of the workplace, the concentration of capital, commercial competition, the banking system, the decline of the profit rate, land-rents, \"et cetera\".\n\nThe critique of the political economy of capitalism proposes that:\n\nAfter two decades of economic study and preparatory work (especially regarding the theory of surplus value), the first volume appeared in 1867 as \"The Production Process of Capital\". After Marx's death in 1883, from manuscripts and the first volume Engels introduced Volume II: \"The Circulation Process of Capital\" in 1885; and Volume III: \"The Overall Process of Capitalist Production\" in 1894. These three volumes are collectively known as \"Das Kapital\".\n\n\"Capital, Volume I\" (1867) is a critical analysis of political economy, meant to reveal the contradictions of the capitalist mode of production, how it was the precursor of the socialist mode of production and of the class struggle rooted in the capitalist social relations of production. The first of three volumes of \"Das Kapital. Kritik der politischen Ökonomie\" (\"Capital. Critique of Political Economy\") was published on 14 September 1867, dedicated to Wilhelm Wolff and was the sole volume published in Marx’s lifetime.\n\n\"Capital, Volume II\", subtitled \"The Process of Circulation of Capital\", was prepared by Engels from notes left by Marx and published in 1885. It is divided into three parts: \"The Metamorphoses of Capital and Their Circuits\", \"The Turnover of Capital\" and \"The Reproduction and Circulation of the Aggregate Social Capital\". In \"Volume II\", the main ideas behind the marketplace are to be found: how value and surplus-value are realized. Its dramatis personae are not so much the worker and the industrialist (as in \"Volume I\"), but rather the money owner (and money lender), the wholesale merchant, the trader and the entrepreneur or functioning capitalist. Moreover, workers appear in \"Volume II\", essentially as buyers of consumer goods and therefore as sellers of the commodity labour power, rather than producers of value and surplus-value—though this latter quality, established in \"Volume I\", remains the solid foundation on which the whole of the unfolding analysis is based. Reading \"Volume II\" is of monumental significance to understanding the theoretical construction of Marx's whole argument. Marx himself quite precisely clarified this place in a letter sent to Engels on 30 April 1868: \"In Book 1... we content ourselves with the assumption that if in the self-expansion process £100 becomes £110, the latter will find already in existence in the market the elements into which it will change once more. But now we investigate the conditions under which these elements are found at hand, namely the social intertwining of the different capitals, of the component parts of capital and of revenue (= s)\". This intertwining, conceived as a movement of commodities and of money, enabled Marx to work out at least the essential elements, if not the definitive form, of a coherent theory of the trade cycle, based upon the inevitability of periodic disequilibrium between supply and demand under the capitalist mode of production (Mandel, 1978, \"Introdution to Volume II of Capital\"). \"Volume II\" of \"Capital\" has indeed been not only a sealed book, but also a forgotten one. To a large extent, it remains so to this very day. Part 3 is the point of departure for a topic given its Marxist treatment later in detail by Rosa Luxemburg, among others.\n\n\"Capital, Volume III\", subtitled \"The Process of Capitalist Production as a Whole\", was prepared by Friedrich Engels from notes left by Karl Marx and published in 1894. It is in seven parts:\nThe work is best known today for Part 3, which in summary says that as the organic fixed capital requirements of production rise as a result of advancements in production generally, the rate of profit tends to fall. This result, which orthodox Marxists believe is a principal contradictory characteristic leading to an inevitable collapse of the capitalist order, was held by Marx and Engels to—as a result of various contradictions in the capitalist mode of production—result in crises whose resolution necessitates the emergence of an entirely new mode of production as the culmination of the same historical dialectic that led to the emergence of capitalism from prior forms.\n\nThe purpose of \"Das Kapital\" (1867) was a scientific foundation for the politics of the modern labour movement. The analyses were meant \"to bring a science, by criticism, to the point where it can be dialectically represented\" and so \"reveal the law of motion of modern society\" to describe how the capitalist mode of production was the precursor of the socialist mode of production. The argument is a critique of the classical economics of Adam Smith, David Ricardo, John Stuart Mill and Benjamin Franklin, drawing on the dialectical method that G. W. F. Hegel developed in \"Science of Logic\" and \"The Phenomenology of Spirit\"; other intellectual influences on \"Capital\" were the French socialists Charles Fourier, Comte de Saint-Simon, Jean Charles Léonard de Sismondi and Pierre-Joseph Proudhon; and the Greek philosophers, especially Aristotle.\n\nAt university, Marx wrote a dissertation comparing the philosophy of nature in the works of the philosophers Democritus (circa 460–370 BC) and Epicurus (341–270 BC). The logical architecture of \"Das Kapital\" is derived in part from the \"Politics\" and the \"Nicomachean Ethics\" by Aristotle, including the fundamental distinction between use value and exchange value, the \"syllogisms\" (C-M-C' and M-C-M') for simple commodity circulation and the circulation of value as capital. Moreover, the description of machinery, under capitalist relations of production, as \"self-acting automata\" derives from Aristotle’s speculations about inanimate instruments capable of obeying commands as the condition for the abolition of slavery. In the nineteenth century, Marx’s research of the available politico-economic literature required twelve years, usually in the British Library, London.\n\nAt the time of his death (1883), Marx had prepared the manuscript for \"Das Kapital, Volume IV\", a critical history of theories of surplus value of his time, the nineteenth century. The philosopher Karl Kautsky (1854–1938) published a partial edition of Marx's surplus-value critique and later published a full, three-volume edition as \"Theorien über den Mehrwert\" (\"Theories of Surplus Value\", 1905–1910). The first volume was published in English as \"A History of Economic Theories\" (1952).\n\n\"Capital, Volume I\" (1867) was published in Marx’s lifetime, but he died in 1883 before completing the manuscripts for \"Capital, Volume II\" (1885) and \"Capital, Volume III\" (1894), which friend and collaborator Friedrich Engels edited and published as the work of Marx. The first translated publication of \"Das Kapital\" was in Imperial Russia in March 1872. It was the first foreign publication and the English edition appeared in 1887. Despite Tsarist censorship proscribing \"the harmful doctrines of socialism and communism\", the Russian censors considered \"Capital\" as a \"strictly scientific work\" of political economy the content of which did not apply to monarchic Russia, where \"capitalist exploitation\" had never occurred and was officially dismissed, given \"that very few people in Russia will read it, and even fewer will understand it\". Nonetheless, Marx acknowledged that Russia was the country where \"Capital\" \"was read and valued more than anywhere\". The Russian edition was the fastest selling. 3,000 copies were sold in one year while the German edition took five years to sell 1,000, thus the Russian translation sold fifteen times faster than the German original.\n\nIn the wake of the global economic collapse of 2008–2009, \"Das Kapital\" was reportedly in high demand in Germany. In 2012, Red Quill Books released \"Capital: In Manga!\", a comic book version of Volume I which is an expanded English translation of the wildly successful 2008 Japanese pocket version \"Das Kapital\" Manga de Dokuha.\n\nThe foreign editions of \"Capital. Critique of Political Economy\" (1867) by Karl Marx include a Russian translation by the revolutionary Mikhail Bakunin (1814–1876). Eventually Marx's work was translated into all major languages. The English translation by Samuel Moore and Marx's son-in-law Edward Aveling of book 1, overseen by Engels, was published in 1887 as \"Capital: A Critical Analysis of Capitalist Production\" by Swan Sonnenschein, Lowrey, & Co. and reissued in the 1970s by Progress Publishers in Moscow while a more recent English translation was made by Ben Fowkes and David Fernbach (the Penguin edition). The definitive critical edition of Marx's works, \"MEGA II\" (\"Marx-Engels Gesamtausgabe\"), includes \"Das Kapital\" in German (and French, for the first volume) and shows all the versions and alterations made to the text, plus a very extensive apparatus of footnotes and (cross-)references.\n\nIn 2017, the historian Gareth Stedman Jones wrote in the Books and Arts section of the scientific journal \"Nature\":\nPositive reception also cited the soundness of the methodology used in producing the book, which is called immanent critique. This approach, which starts from simple category and gradually unfolds into complex categories, employed \"internal\" criticism that finds contradiction within and between categories while discovering aspects of reality that the categories cannot explain. This meant that Marx had to build his arguments on historical narratives and empirical evidence rather than the arbitrary application of his ideas in his evaluation of capitalism.\n\nOn the other hand, \"Das Kapitalism\" was also criticized for a number of weaknesses. For instance, there are theorists who stressed that this text was unable to reconcile the capitalist exploitation with prices dependent upon subjective wants in exchange relations. There are also those who argued that Marx's so-called \"increasing misery\" doctrine, which is presumed to mean that the proletariat is absolutely immiserated. \n\n\"Capital, Volume I\" (1867); published in Marx’s lifetime:\n\n\"Capital, Volume II\" (1885); manuscript not completed by Marx before his death in 1883; subsequently edited and published, by friend and collaborator Friedrich Engels, as the work of Marx:\n\n\"Capital, Volume III\" (1894); manuscript not completed by Marx before his death in 1883; subsequently edited and published, by friend and collaborator Friedrich Engels, as the work of Marx:\n\n\"Capital, Volume IV\" (1905–1910); critical history of theories of surplus value; manuscript written by Marx; partial edition edited and published, after Marx's death, by Karl Kautsky, as \"Theories of Surplus Value\"; other editions published later:\n\n\n"}
{"id": "55524455", "url": "https://en.wikipedia.org/wiki?curid=55524455", "title": "Digital image correlation for electronics", "text": "Digital image correlation for electronics\n\nDigital image correlation analyses have applications in material property characterization, displacement measurement, and strain mapping. As such, DIC is becoming an increasingly popular tool when evaluating the thermo-mechanical behavior of electronic components and systems.\n\nThe most common application of DIC in the electronics industry is the measurement of coefficient of thermal expansion (CTE). Because it is a non-contact, full-field surface technique, DIC is ideal for measuring the effective CTE of printed circuit boards (PCB) and individual surfaces of electronic components. It is especially useful for characterizing the properties of complex integrated circuits, as the combined thermal expansion effects of the substrate, molding compound, and die make effective CTE difficult to estimate at the substrate surface with other experimental methods. DIC techniques can be used to calculate average in-plane strain as a function of temperature over an area of interest during a thermal profile. Linear curve-fitting and slope calculation can then be used to estimate an effective CTE for the observed area. Because the driving factor in solder fatigue is most often the CTE mismatch between a component and the PCB it is soldered to, accurate CTE measurements are vital for calculating printed circuit board assembly (PCBA) reliability metrics.\n\nDIC is also useful for characterizing the thermal properties of polymers. Polymers are often used in electronic assemblies as potting compounds, conformal coatings, adhesives, molding compounds, dielectrics, and underfills. Because the stiffness of such materials can vary widely, accurately determining their thermal characteristics with contact techniques that transfer load to the specimen, such as dynamic mechanical analysis (DMA) and thermomechanical analysis (TMA), is difficult to do with consistency. Accurate CTE measurements are important for these materials because, depending on the specific use case, expansion and contraction of these materials can drastically affect solder joint reliability. For example, if a stiff conformal coating or other polymeric encapsulation is allowed to flow under a QFN, its expansion and contraction during thermal cycling can add tensile stress to the solder joints and expedite fatigue failures.\n\nDIC techniques will also allow the detection of glass transition temperature (T). At a glass transition temperature, the strain vs. temperature plot will exhibit a change in slope.\n\nDetermining the T is very important for polymeric materials that could have glass transition temperatures within the operating temperature range of the electronics assemblies and components on which they are used. For example, some potting materials can see the Elastic Modulus of the material change by a factor of 100 or more over the glass transition region. Such changes can have drastic effects on an electronic assembly's reliability if they are not planned for in the design process.\n\nWhen 3D DIC techniques are employed, out-of-plane motion can be tracked in addition to in-plane motion. Out-of-plane warpage is especially of interest at the component level of electronics packaging for solder joint reliability quantification. Excessive warpage during reflow can contribute to defective solder joints by lifting the edges of the component away from the board and creating head-in-pillow defects in ball grid arrays (BGA). Warpage can also shorten the fatigue life of adequate joints by adding tensile stresses to edge joints during thermal cycling.\n\nWhen a PCBA is over-constrained, thermo-mechanical stress brought about during thermal expansion can cause board strains that could negatively affect individual component and overall assembly reliability. The full-field monitoring capabilities of an image correlation technique allow for the measurement of strain magnitude and location on the surface of a specimen during a displacement-causing event, such as PCBA during a thermal profile. These \"strain maps\" allow for the comparison of strain levels over full areas of interest. Many traditional discrete methods, like extensometers and strain gauges, only allow for localized measurements of strain, inhibiting their ability to efficiently measure strain across larger areas of interest. DIC techniques have also been used to generate strain maps from purely mechanical events, such as drop impact tests, on electronic assemblies.\n\n"}
{"id": "1138590", "url": "https://en.wikipedia.org/wiki?curid=1138590", "title": "Electricity meter", "text": "Electricity meter\n\nAn electricity meter, electric meter, electrical meter, or energy meter is a device that measures the amount of electric energy consumed by a residence, a business, or an electrically powered device.\n\nElectric utilities use electric meters installed at customers' premises for billing purposes. They are typically calibrated in billing units, the most common one being the kilowatt hour (\"kWh\"). They are usually read once each billing period.\n\nWhen energy savings during certain periods are desired, some meters may measure demand, the maximum use of power in some interval. \"Time of day\" metering allows electric rates to be changed during a day, to record usage during peak high-cost periods and off-peak, lower-cost, periods. Also, in some areas meters have relays for demand response load shedding during peak load periods.\n\nAs commercial use of electric energy spread in the 1880s, it became increasingly important that an electric energy meter, similar to the then existing gas meters, was required to properly bill customers, instead of billing for a fixed number of lamps per month.\n\nDC meters measured charge in ampere-hours. Since the voltage of the supply should remain substantially constant, the reading of the meter was proportional to actual energy consumed. For example, if a meter recorded that 100 ampere-hours had been consumed on a 200-volt supply, then 20 kilowatt-hours of energy had been supplied.\n\nMany experimental types of meter were developed. Thomas Edison at first worked on a direct current (DC) electromechanical meter with a direct reading register, but instead developed an electrochemical metering system, which used an electrolytic cell to totalise current consumption. At periodic intervals the plates were removed and weighed, and the customer billed. The electrochemical meter was labour-intensive to read and not well received by customers.\n\nAn early type of electrochemical meter used in the United Kingdom was the 'Reason' meter. This consisted of a vertically mounted glass structure with a mercury reservoir at the top of the meter. As current was drawn from the supply, electrochemical action transferred the mercury to the bottom of the column. Like all other DC meters, it recorded ampere-hours. Once the mercury pool was exhausted, the meter became an open circuit. It was therefore necessary for the consumer to pay for a further supply of electricity, whereupon, the supplier's agent would unlock the meter from its mounting and invert it restoring the mercury to the reservoir and the supply.\n\nIn 1885 Ferranti offered a mercury motor meter with a register similar to gas meters; this had the advantage that the consumer could easily read the meter and verify consumption. The first accurate, recording electricity consumption meter was a DC meter by Dr Hermann Aron, who patented it in 1883. Hugo Hirst of the British General Electric Company introduced it commercially into Great Britain from 1888. Aron's meter recorded the total charge used over time, and showed it on a series of clock dials.\n\nThe first specimen of the AC kilowatt-hour meter produced on the basis of Hungarian Ottó Bláthy's patent and named after him was presented by the Ganz Works at the Frankfurt Fair in the autumn of 1889, and the first induction kilowatt-hour meter was already marketed by the factory at the end of the same year. These were the first alternating-current watt-hour meters, known by the name of Bláthy-meters. The AC kilowatt hour meters used at present operate on the same principle as Bláthy's original invention. Also around 1889, Elihu Thomson of the American General Electric company developed a recording watt meter (watt-hour meter) based on an ironless commutator motor. This meter overcame the disadvantages of the electrochemical type and could operate on either alternating or direct current.\n\nIn 1894 Oliver Shallenberger of the Westinghouse Electric Corporation applied the induction principle previously used only in AC ampere-hour meters to produce a watt-hour meter of the modern electromechanical form, using an induction disk whose rotational speed was made proportional to the power in the circuit. The Bláthy meter was similar to Shallenberger and Thomson meter in that they are two-phase motor meter. Although the induction meter would only work on alternating current, it eliminated the delicate and troublesome commutator of the Thomson design. Shallenberger fell ill and was unable to refine his initial large and heavy design, although he did also develop a polyphase version.\n\nThe most common unit of measurement on the electricity meter is the kilowatt hour [\"kWh\"], which is equal to the amount of energy used by a load of one kilowatt over a period of one hour, or 3,600,000 joules. Some electricity companies use the SI megajoule instead.\n\nDemand is normally measured in watts, but averaged over a period, most often a quarter- or half-hour.\n\nReactive power is measured in \"thousands of volt-ampere reactive-hours\", (kvarh). By convention, a \"lagging\" or inductive load, such as a motor, will have positive reactive power. A \"leading\", or capacitive load, will have negative reactive power.\n\nVolt-amperes measures all power passed through a distribution network, including reactive and actual. This is equal to the product of root-mean-square volts and amperes.\n\nDistortion of the electric current by loads is measured in several ways. Power factor is the ratio of resistive (or real) power to volt-amperes. A capacitive load has a leading power factor, and an inductive load has a lagging power factor. A purely resistive load (such as a filament lamp, heater or kettle) exhibits a power factor of 1. Current harmonics are a measure of distortion of the wave form. For example, electronic loads such as computer power supplies draw their current at the voltage peak to fill their internal storage elements. This can lead to a significant voltage drop near the supply voltage peak which shows as a flattening of the voltage waveform. This flattening causes odd harmonics which are not permissible if they exceed specific limits, as they are not only wasteful, but may interfere with the operation of other equipment. Harmonic emissions are mandated by law in EU and other countries to fall within specified limits.\n\nIn addition to metering based on the amount of energy used, other types of metering are available. Meters which measured the amount of charge (coulombs) used, known as ampere-hour meters, were used in the early days of electrification. These were dependent upon the supply voltage remaining constant for accurate measurement of energy usage, which was not a likely circumstance with most supplies. The most common application was in relation to special-purpose meters to monitor charge / discharge status of large batteries. Some meters measured only the length of time for which charge flowed, with no measurement of the magnitude of voltage or current being made. These are only suited for constant-load applications and are rarely used today.\n\nElectricity meters operate by continuously measuring the instantaneous voltage (volts) and current (amperes) to give energy used (in joules, kilowatt-hours etc.). Meters for smaller services (such as small residential customers) can be connected directly in-line between source and customer. For larger loads, more than about 200 ampere of load, current transformers are used, so that the meter can be located other than in line with the service conductors. The meters fall into two basic categories, electromechanical and electronic.\n\nThe most common type of electricity meter is the electromechanical watt-hour meter.\n\nOn a single-phase AC supply, the electromechanical induction meter operates through electromagnetic induction by counting the revolutions of a non-magnetic, but electrically conductive, metal disc which is made to rotate at a speed proportional to the power passing through the meter. The number of revolutions is thus proportional to the energy usage. The voltage coil consumes a small and relatively constant amount of power, typically around 2 watts which is not registered on the meter. The current coil similarly consumes a small amount of power in proportion to the square of the current flowing through it, typically up to a couple of watts at full load, which is registered on the meter.\n\nThe disc is acted upon by two sets of induction coils, which form, in effect, a two phase linear induction motor. One coil is connected in such a way that it produces a magnetic flux in proportion to the voltage and the other produces a magnetic flux in proportion to the current. The field of the voltage coil is delayed by 90 degrees, due to the coil's inductive nature, and calibrated using a lag coil. This produces eddy currents in the disc and the effect is such that a force is exerted on the disc in proportion to the product of the instantaneous current, voltage and phase angle (power factor) between them. A permanent magnet acts as an eddy current brake, exerting an opposing force proportional to the speed of rotation of the disc. The equilibrium between these two opposing forces results in the disc rotating at a speed proportional to the power or rate of energy usage. The disc drives a register mechanism which counts revolutions, much like the odometer in a car, in order to render a measurement of the total energy used.\n\nDifferent phase configurations use additional voltage and current coils.\n\nThe disc is supported by a spindle which has a worm gear which drives the register. The register is a series of dials which record the amount of energy used. The dials may be of the \"cyclometer\" type, an odometer-like display that is easy to read where for each dial a single digit is shown through a window in the face of the meter, or of the pointer type where a pointer indicates each digit. With the dial pointer type, adjacent pointers generally rotate in opposite directions due to the gearing mechanism.\n\nThe amount of energy represented by one revolution of the disc is denoted by the symbol Kh which is given in units of watt-hours per revolution. The value 7.2 is commonly seen. Using the value of Kh one can determine their power consumption at any given time by timing the disc with a stopwatch.\n\n<math>P = \n"}
{"id": "33024295", "url": "https://en.wikipedia.org/wiki?curid=33024295", "title": "Energy in the Middle East", "text": "Energy in the Middle East\n\nEnergy in the Middle East describes energy and electricity production, consumption and import in the Middle East. Energy policy of the Middle East will describe the politics of the Middle East related to energy more in detail.\n\nEnergy export from the Middle East in 2010 was 12,228 TWh. The major exporters were Saudi Arabia 37.2%, Qatar 14.3% and Iran 12.9%.\n\nIn 2009 the largest share of oil production was in the Middle East (24 million barrels daily, or 31 per cent of global production. According to Transparency International based on BP data regionally the largest share of proved oil reserves is in the Middle East (754 billion barrels, constituting 51 per cent of global reserves including oil sands and 57 per cent excluding them). According to BP of the world oil reserves were in Saudi Arabia 18%, Iran 9%, Iraq 8%, Kuwait 7% and UAE 7%.\n\nIn June 2015, Jim Hollis, CEO of NEOS, during “Oil and Gas: Governance and Integration” forum, stated that Lebanon’s potential offshore natural gas reserves are estimated at 25 trillion cubic feet, according to initial estimates carried out in the country’s exclusive economic zone.\n\nMiddle Eastern countries possess about 41 per cent of natural gas reserves. According to BP in 2009 of the proved gas reserves were in Iran 16% and Qatar 14%.\n\nMajor energy companies in the Middle East include Saudi Aramco, Qatar Petroleum, Kuwait Petroleum Corporation KPC and National Iranian Oil Company NIOC.\n\nSeveral Middle Eastern countries are among the world's top carbon dioxide emitters per capita: World dirty top countries were in 2009 (tonnes/capita): 1) Gibraltar 152, 2) Virgin Islands U,S 114, 3) Qatar 80, 4) Netherlands Antilles 51, 5) Bahrain 43. 6) United Arab Emirates 40, 7) Trinidad and Tobago 39, Singapore 34 and Kuwait 32. All emissions from building and cement production are local but some people may argue that some United Arab Emirates produced fuels and/or goods are consumed abroad. One of the biggest sources of emissions in the Middle East is air conditioning, a virtual necessity in the region's often torrid climate. Per capita use of air conditioning in the Middle East is currently far lower than in the United States, but is expected to increase.\n"}
{"id": "3251231", "url": "https://en.wikipedia.org/wiki?curid=3251231", "title": "Flame maple", "text": "Flame maple\n\nFlame maple (tiger maple), also known as \"flamed maple\", \"curly maple\", \"ripple maple\", \"fiddleback\" or \"tiger stripe\", is a feature of maple in which the growth of the wood fibers is distorted in an undulating chatoyant pattern, producing wavy lines known as \"flames\". This effect is often mistakenly said to be part of the grain of the wood; it is more accurately called \"figure\", as the distortion is perpendicular to the grain direction. Prized for its beautiful appearance, it is used frequently in the manufacturing of musical instruments, such as violins and bassoons, and fine furniture. Another well-known use of the material is its use in guitars.\n\nDuring the westward expansion of early settlers and explorers into the lands west of the Appalachian Mountains, curly maple was often used for making the stocks used on Kentucky rifles.\n\nAs previously mentioned, the structure of flame maple consists of distorted patterns found in the wood of specific maple trees. Although this pattern makes its appearance strikingly different from regular maple, the generally behaves in a very similar way. When woodworking, one must consider the type of wood they are working with. Hard woods like oak require heavy-duty equipment to shape, while softwoods such as Pine can be cut and shaved with much less force and lighter tools. Maple woods fall under the hardwood category, though they are generally lighter and easier to work than stronger hardwoods like oak and mahogany. \n\nAs with most other woods, flame maple does have the potential to crack and splinter. To counter this, carpenters working with flame maple must use sharper, quicker tools over their heavier counterparts. Saw blades for example prove more effective against the flame maple when given a fine row of teeth instead of coarser, large teeth one may find on a blade used for oak. Flame maple, (and maple in general) is known for being stiff due to its tight grain pattern and solid structure, so applying too much stress to one side of the maple stock can result in fragmentation, and catastrophic failure. To prevent this, carpenters open use pre-shaped jigs to hold the maple in place. \n\nPopularization \n\nFlame maple is especially popular in guitars, and has been for decades. Generally, the process includes cutting the maple to shape, forming it if necessary through mold-warping and applying a clear coat or lacquer to protect the finish, and ensure the wood holds its shape. Flame maple is especially popular, due to its inclusion on the Gibson Les Paul, particularly the one used by Slash, the iconic lead guitarist of Guns N Roses. As his fame increased so did the fame and value of this guitar style, prompting many guitar companies to provide similar flame-maple finishes thereafter. \n\nControversy \n\nAmong guitar communities, one debate always present is that of tonality. This is the debate on how much or how little the instrument's material affects its sound. While this is a massive debate in itself, flame maple is a hardwood which is generally regarded to produce a bright, shimmering sound, due to its rigidity and reflection against sound waves. This effect is generally noticeable in acoustic flame-maple guitars, but arguably insignificant in electric guitars. Additionally, the effects of clear-coating or applying nitro to a flame maple finish are also up for debate. \n\nAccording to the \"Beauty Of The Burst\" by Yasuhiko Watanabe, the figures seen on the sunburst Les Paul are categorized into 8 types: 6 types of flame maple (\"Curly\", \"Ribbon curly\", \"Flame\", \"Tiger stripe\", \"Fiddleback\", \"Pin stripe\"), and 2 other types (\"Blister\" and \"Bird's eye\"). Note that usually the last two types are not considered as the flame maple variations, along with the quilt maple.\n\n"}
{"id": "11335844", "url": "https://en.wikipedia.org/wiki?curid=11335844", "title": "Greenhouse gas emissions by the United States", "text": "Greenhouse gas emissions by the United States\n\nAccording to the U.S. Environmental Protection Agency (EPA), the United States produced 6,587 million metric tonnes of carbon-dioxide equivalent greenhouse gas (GHG) emissions in 2015. Compared to 2014 levels, U.S. greenhouse gas emissions decreased in 2015. Compared to levels in 1990, emissions have increased by about 4 percent. From year to year, emissions can rise and fall due to changes in the economy, the price of fuel, and other factors. The EPA has attributed recent decreases to a reduction in emissions from fossil fuel combustion, which was a result of multiple factors including substitution from coal to natural gas consumption in the electric power sector; warmer winter conditions that reduced demand for heating fuel in the residential and commercial sectors; and a slight decrease in electricity demand.\n\nWhile the Bush administration opted against Kyoto-type policies to reduce emissions, the Obama administration and various state, local, and regional governments have attempted to adopt some Kyoto Protocol goals on a local basis. For example, the Regional Greenhouse Gas Initiative (RGGI) founded in January 2007 is a state-level emissions capping and trading program by nine northeastern U.S. states. In December 2009 President Obama set a target for reducing U.S. greenhouse gas (GHG) emissions in the range of 17% below 2005 levels by 2020.\n\nThe U.S. State Department offered a nation-level perspective in the Fourth US Climate Action Report (USCAR) to the United Nations Framework Convention on Climate Change, including measures to address climate change. The report showed that the country was on track to achieve President Bush's goal of reducing greenhouse gas emissions per unit of gross domestic product) by 18% from 2002 to 2012. Over that same period, actual GHG emissions were projected to increase by 11%. The report estimated that in 2006, U.S. GHG emissions decreased 1.5% from 2005 to 7,075.6 million tonnes of carbon dioxide equivalent. This was an increase of 15.1% from the 1990 levels of 6,146.7 million tonnes (or 0.9% annual increase), and an increase of 1.4% from the 2000 levels of 6,978.4 million tonnes. By 2012 GHG emissions were projected to increase to more than 7,709 million tonnes of carbon dioxide equivalent, which would be 26% above 1990 levels.\n\nAccording to a 2016 study U.S. methane emissions were underestimated by the EPA since at least a decade, on the order of 30 to 50 percent.\nU.S. carbon dioxide emissions from energy use rose by 1.6% in 2007, according to preliminary estimates by the United States Department of Energy's Energy Information Administration (EIA). Electricity generation increased by 2.5%, and carbon dioxide emissions from the power sector increased even more, at 3%, indicating that U.S. utilities shifted towards energy sources that emitted more carbon. That shift was partially caused by a 40 billion kilowatt-hour decrease in hydropower production causing a greater reliance on fossil fuels like natural gas and coal. Carbon dioxide emissions from power plants fueled with natural gas increased by 10.5%, while coal-burning power plants increased their emissions by 1.8%. Paper is the fourth largest contributor to greenhouse gas emissions in the United States and one of the largest consumers of industrial water among all Organisation for Economic Co-operation and Development (OECD) countries.\n\nIn 2007 the National Oceanic and Atmospheric Administration stated that the \"U.S. and global annual temperatures are now approximately 1.0°F warmer than at the start of the 20th century, and the rate of warming has accelerated over the past 30 years, increasing globally since the mid-1970s at a rate approximately three times faster than the century-scale trend. The past nine years have all been among the 25 warmest years on record for the contiguous U.S., a streak which is unprecedented in the historical record.\"\n\nA study completed in 2015 provides a breakdown of how much emissions are created by certain parts of the United States economy. The largest group being electricity production accounting for 37% of carbon emissions, followed by personal ground travel at 22%, natural gas not used towards power at 11%, movement of goods by boat, truck, or air at 9%, oil refining at 6%, other uses of petroleum at 6%, jet fuel, specifically for air travel, at 4%, and the last 5% spread among other miscellaneous uses. A large portion of carbon emissions created by the United states is from personal use. The University of Michigan created a detailed list of steps that can be taken to lower personal carbon emissions.\n\nSince January 1, 2010 the USEPA's Mandatory Reporting of Greenhouse Gases rule, requires thousands of companies in the US to monitor their greenhouse gas emissions and begin reporting them in 2011.\nA detailed inventory of fossil fuel emissions is provided by the Project Vulcan.\n\nThe White House announced on 25 November 2009 that President Barack Obama is offering a U.S. target for reducing greenhouse gas emissions in the range of 17% below 2005 levels by 2020. The proposed target agrees with the limit set by climate legislation that has passed the U.S. House of Representatives, but the U.S. Senate is currently considering a bill that cuts GHG emissions to 20% below 2005 levels by 2020. The White House noted that the final U.S. emissions target will ultimately fall in line with the climate legislation, once that legislation passes both houses of Congress and is approved by the President. In light of the president's goal for an 83% reduction in GHG emissions by 2050, the pending legislation also includes a reduction in GHG emissions to 30% below 2005 levels by 2025 and to 42% below 2005 levels by 2030. The Waste Prevention Rule which was enacted by the Obama Administration in November 2016 to regulate limits on the amount of methane released while drilling for oil and natural gas on federal land was repealed on September 18, 2018.\n\nThe U.S. strategy integrates actions to address climate change including actions to mitigate greenhouse gas emissions into a broader agenda that promotes energy security, pollution reduction, and sustainable economic development.\n\nAs of 2010, buildings in the United States consume roughly 48% of the country's electricity and contribute a similar percentage of GHG. Engineers Matthias Hollwich and Marc Kushner won the IBM Engineering Innovation award in 2008 with their innovative urban \"biogrid\" concept called: \"MEtreePOLIS–Atlanta in 2018–\"the city of the future.\" By prompting scientists, engineers, and architects in creating new designs and technologies, there is a hope that positive futures will be attainable without causing environmental damages. \n\n\n\nAs of 2011, 71% of petroleum consumed in the USA was used for transportation.\n\nPrograms to reduce greenhouse gas emissions from the transportation sector:\n\n\n\n\n\n\n\nCalifornia: \n\n\n\n\n\n\n\n\n\n\nThe British climate envoy in the 2007 meeting of the world's top 16 polluters, John Ashton, said the United States seemed isolated on the issue of fighting climate change: \"The argument that we can do this through voluntary approaches is now pretty much discredited internationally\".\n\n"}
{"id": "9642744", "url": "https://en.wikipedia.org/wiki?curid=9642744", "title": "Gweagal", "text": "Gweagal\n\nThe Gweagal (also spelt Gwiyagal) were a clan of the Eora tribe of Indigenous Australians, who are traditional custodians of the southern geographic areas of Sydney, New South Wales, Australia.\n\nThe Gweagal lived on the southern shores of Botany Bay (Kurnell Peninsula). The tribe territory, although not clearly defined, spanned the areas between the Cooks and Georges River, south to the Port Hacking estuary and westwards towards Liverpool. They were the northernmost tribe of the Dharawal nation.\n\nEach Gweagal tribe consisted of approximately 20 to 50 people who lived in their own territory amongst social and economic units having strong ties to land and sacred sites. They had no written language and each tribe had its own dialect, they also knew how to light fires long before the arrival of Europeans. They were often seen by early settlers to be naked but with minimal clothing that consisted of a woven hair sash in which they used to carry tools and weapons and sometimes the optional possum-skin coat for the winter season. They wore resin in their hair that gave it a mop-like appearance and used native animal hide to make fur coats and ceremonial attire. Tool makers chose to grind axes close to pools or streams, as the water was used as a lubricant for grinding and sharpening. The stone that was used was mainly igneous or metamorphic rock, and only one of the ends was ground to a blade. Axe grinding grooves used in for this purpose can be found near a stream between River Road and Salt Pan Creek at Revesby Heights. In 1961 a notice was erected describing the site.\n\nThe Gweagal Aborigines were the guardians of the sacred white clay pits in their territory. Members of the tribe walked hundreds of miles to collect the clay, it was considered sacred amongst the indigenous locals and had many uses. They used it to line the base of their canoes so they could light fires, and also as a white body paint, (as witnessed by Captain James Cook). Colour was added to the clay using berries, which produced a brightly coloured paint that was used in ceremonies. It was also eaten as a medicine, an antacid. Geebungs and other local berries were mixed in the clay and it was eaten as a dietary supplement with zinc.\nNaturally forming and human-modified caves or hanging rock shelters were regularly utilised by the first Australians during walkabout - seaonally guided maintenance of land and the vast natural gardens routinely tended by the Aboriginal people.\nIn the Royal National Park some of the caves were used as burial sites. In tribal lands and Dreamtime places this cultural practice continues. Charles Sturt documented large stone dwellings consisting of four rooms or more, along the banks of the Murray river and the shores of lake Victoria. These buildings were re-purposed by invaders, colonists and settlers in the early years of colonisation, the foundations of many such buildings can be seen in archaeological sites around Australia.\n\nA rock cave collapse at Port Hacking prior to the landing of James Cook claimed many lives of the Gweagal. This cave was later dynamited, showing many skeletons.\n\nCaves and shelters are located in various places along the George's River, which over the years have eroded into the sandstone cliffs. There is a large cave located in Peakhurst with its ceiling blackened from smoke. There are caves located around Evatt Park, Lugarno with oyster shells ground into the cave floor. A cave has also been discovered near a Baptist church in Lugarno, and another near Margaret Crescent, Lugarno (now destroyed by development), it was found to contain ochre and a spear head on the floor of the cave when it was excavated. Another cave exists on Mickey's Point, Padstow, which was named after a local Aborigine.\n\nAustralia’s First Nation people decorated their caves and homes with carvings, sculpture, beads, paintings, drawings and etchings using white, red and other colored earth, clay or charcoal. Symbols such as “water well” with a red ochre hand directed newcomers to wells and water storage. Footprints on a line signalled that there were stairs or steps in the area.\nThe dwellings boasted thermal mass unlike modern day housing, which kept an even temperature year-round. Rugs, furs and woven mats provided further warmth and comfort. Fire was used to cook, produce materials and keep their shelters warm.\n\nThe territory of the Gweagal had much to offer. The Georges River provided fish and oysters. Various small creeks, most of which are now covered drains, provided fresh water. Men and women fished in canoes or from the shore using barbed spears and fishing lines with hooks that were crafted from crescent-shaped pieces of shell. Waterfowl could be caught in the swamplands near Towra Point and the variety of soils supported a variety of edible and medicinal plants. Birds and their eggs, possums, wallabies and goannas were also a part of their staple diet. The abundant food source meant that these natives were less nomadic than those of Outback Australia.\n\nMiddens have been found all the way along tidal sections of the Georges River where shells, fish bones, and other waste products have been thrown into heaps. This, as well as environmental modifications such as dams, building foundations, large earthen excavations and wells, gives evidence of where Aborigines established villages for long periods, and are found where oysters, fresh water, and strategic views come together. Middens have been found in Oatley, and Oatley Point was known as a feasting ground. In Lugarno a midden is still existent and may be found in Lime Kiln Bay.\n\nThe Gweagal Aborigines made first contact with James Cook and other Europeans, occupying the area which is now 'Captain Cooks Landing Place Reserve’, Kurnell, on the shores of Botany Bay. It was in this place that the crew of Captain Cook’s first voyage tried to make contact with the indigenous people of Australia.\n\nFor eight days between late April and early May 1770, Cook’s ship the 'Endeavour' was anchored in the bay. Sir Joseph Banks stated that some of the Aborigines withdrew into the bushes as the Endeavour came near. Several warriors remained on the rocks, \"threatening and menacing with their pikes and swords\". When Cook and his crewmen tried to land, two of the tribesmen stood on the rocks, warning them off with spears and sticks. After about 15 minutes there was an exchange of musket fire and spears. One shot wounded a local man in the leg, no harm came upon Cook's crew.\n\nThe sailors then proceeded to walk onto the beach and up to an encampment. Both Cook and Banks tried, with great difficulty, to make contact with the local people but without success due to the Aborigines avoiding contact after the first encounter, they simply went about their daily affairs seeming to ignore the strangers; they fished from canoes, cooked shellfish on the shore, walked along the beach, but at the same time, watched Cook’s crew with caution. There were a few cases where, Gweagal men tried to approach members of Cook’s expedition, before shying away. In all of their responses, the locals sought to deal with them in a way that would allow them “to affirm their rights to their land and their resources and defuse any potential conflict or hostility.\"\n\nIn 1770, after returning to England from their voyage in the South Pacific, Cook and Banks brought with them a large collection of flora and fauna, along with cultural artifacts from their most recent venture. The find included a collection of roughly fifty Australian Aboriginal spears that were owned by the Gweagal people.\n\nBanks was convinced the spears were abandoned (on the shores of Kurnell) and \"thought it no improper measure to take with them all the lances which they could find, somewhere between 40 or 50\".\n\nFour of those spears - the only material reminders of the first meeting between Aborigines and Englishmen on the east coast still exist: two bone-tipped three-pronged spears, one bone-tipped four-pronged spear and a shaft with a single hardwood head. Cook gave the spears to his patron, John Montagu, First Lord of the Admiralty and Fourth Earl of Sandwich, who then gave them, to his \"alma mater\" Trinity College. Archaeologists quote them as being priceless, as the spears are among the few remaining artefacts that can be traced back to Cook's first voyage. Although the spears remain in the ownership of Trinity, they are now on display at the Museum of Archaeology and Anthropology of Cambridge University in England.\n"}
{"id": "1992718", "url": "https://en.wikipedia.org/wiki?curid=1992718", "title": "Göksu", "text": "Göksu\n\nThe Göksu (Turkish for \"blue water\" also called \"Geuk Su\", \"Goksu Nehri\"; medieval Latin: \"Saleph\", Ancient Greek: Καλύκαδνος \"Calycadnus\") is a river on the Taşeli plateau (Turkey). Both its sources arise in the Taurus Mountains—the northern in the Geyik Mountains and the southern in the Haydar Mountains. Their confluence is south of Mut.\n\nThe river is 260 km long and empties into the Mediterranean Sea 16 km southeast of Silifke (in Mersin province). The delta of the Göksu, including Akgöl Lake and Paradeniz Lagoon, is one of the most important breeding areas in the Near East; over 300 bird species have been observed. Among others, flamingos, herons, bee-eaters, kingfishers, gulls, nightingales and warblers breed here. The endangered Loggerhead Sea Turtle (Caretta caretta) lays eggs here.\n\nDue to demand for summer vacation apartments by the locals, and since necessary precautions are not taken and public attention is minimal in this part of Turkey, the ecosystem around Akgöl Lake and Paradeniz Lagoon is in heavy danger.\n\nIn 1190, while on the Third Crusade, Emperor Frederick Barbarossa drowned in the river, then known by its Latin name \"Saleph\" and located within the Armenian Kingdom of Cilicia. The crusaders had reached Asia Minor in March, where the emperor continued his campaign to the Holy Land. Having plundered the city of Konya and defeated the forces of the Sultanate of Rum at the Battle of Iconium, his forces arrived on the banks of the river on June 10. \n\nSeveral contradictory statements reflect the circumstances of his sudden death, which have not been conclusively established. According to some sources, the emperor was lost in the current when he tried to cross the water near Silifke; other chronicles report he wished to cool down from the heat of the day and suffered a heart attack while taking a bath. The mortal remains were preserved according to the \"Mos Teutonicus\" process and transferred to Tarsus, Antioch and Tyre by his followers.\n\nOnce without a leader, his crusader's army dispersed; the remnants later joined the Siege of Acre. A monument in Barbarossa's honor was erected on the road from Silifke to Mut.\n\n"}
{"id": "33129476", "url": "https://en.wikipedia.org/wiki?curid=33129476", "title": "Hot water storage tank", "text": "Hot water storage tank\n\nA hot water storage tank (also called a hot water tank, thermal storage tank, hot water thermal storage unit, heat storage tank and hot water cylinder) is a water tank used for storing hot water for space heating or domestic use.\n\nWater is a convenient heat storage medium because it has a high specific heat capacity. This means, compared to other substances, it can store more heat per unit of weight. Water is non-toxic and low cost.\n\nAn efficiently insulated tank can retain stored heat for days, reducing fuel costs. Hot water tanks may have a built-in gas or oil burner system, electric immersion heaters. Some types use an external heat exchanger such as a central heating system, or heated water from another energy source. The most typical, in the domestic context, is a fossil-fuel burner, electric immersion elements, or a district heating scheme.\n\nWater heaters for washing, bathing, or laundry have thermostat controls to regulate the temperature, in the range of , and are connected to the domestic cold water supply.\n\nWhere the local water supply has a high content of dissolved minerals such as limestone, heating the water causes the minerals to precipitate in the tank (scaling). A tank may develop leaks due to corrosion after only a few years, a problem exacerbated by dissolved oxygen in the water which accelerates corrosion of both tank and fittings.\n\nTypically hot water storage tanks are wrapped in heat insulation to reduce energy consumption, speed up the heating process, and maintain the desired operating temperature. Thicker thermal insulation reduces standby heat loss. Water heaters are available with various insulation ratings but it is possible to add layers of extra insulation on the outside of a water heater to reduce heat loss. In extreme conditions, the heater itself might be wholly enclosed in a specially constructed insulated space.\n\nThe most commonly available type of water heater insulation is fiberglass, fixed in place with tape or straps or the outer jacket of the water heater. Insulation must not block air flow or combustion gas outflow, where a burner is used. \n\nIn extremely humid locations, adding insulation to an already well-insulated tank may cause condensation leading to rust, mold, or other operational problems so some air flow must be maintained, usually by convection caused by waste heat, but in particularly humid conditions such ventilation may be fan-assisted.\n\nMost modern water heaters have applied polyurethane foam (PUF) insulation. Where access to the inner tank is a priority (in cases of particularly aggressive minerals or oxygen levels in the local water supply) the PUF can be applied in encapsulated form, allowing the removal of insulation layer for regular integrity checks and if required, repairs to the water tank.\n\nIn a solar water heating system, a solar hot water storage tank stores heat from solar thermal collectors. The tank has a built-in heat-exchanger to heat domestic cold water. In relatively mild climates, such as the Mediterranean, the (heavily insulated but metal-wrapped) storage tanks are often roof-mounted. All such tanks share the same problems as artificially-heated tanks including limestone deposit and corrosion, and suffer similar reductions in overall efficiency unless scrupulously maintained.\n\nWater heater tanks may be made of vitreous enamel-lined carbon steel, stainless steel, or copper.\n\nWhile copper and stainless steel domestic hot water tanks are more commonplace in Europe, carbon steel tanks are more common in the United States, where typically the periodic check is neglected, the tank develops a leak whereupon the entire appliance is replaced. Even when neglected, carbon steel tanks tend to last for a few years more than their manufacturer's warranty, which is typically 3 to 12 years in the US.\n\nVitreous-lined tanks are much lower in initial cost, and often include one or more sacrificial anode rods designed to protect the tank from perforation caused by corrosion made necessary since chlorinated water is very corrosive to carbon steel. As it is very nearly impossible to apply any protective coating perfectly (without microscopic cracks or pinhole defects in the protective layer) manufacturers may recommend a periodic check of any sacrificial anode, replacing it when necessary.\n\nSome manufacturers offer an extended warranty kit that includes a replacement anode rod. Because conventional hot water storage tanks can be expected to leak every 5 to 15 years, high-quality installations will include, and most US building/plumbing codes now require, a shallow metal or plastic pan to collect the seepage when it occurs.\n\nThis method stores heat in a tank by using external heat-exchangers (coils) that can be directly tapped or used to power other (external) heat-exchangers.\n\nThe chief benefit is that by avoiding drawing-off domestic hot water directly, the tank is not continually fed with cold water, which in 'hard' water areas reduces the deposit of limescale to whatever is dissolved in the original charge of water plus relatively trivial amounts added to replace losses due to seepage.\n\nAn added benefit is reduced oxygen levels in such a closed system, which allows for some relaxation in the requirements for materials used in the hot water storage tank and the closed water circuits, external heat exchangers, and associated pipework.\n\nWhile an external heat exchanger system used for domestic hot water will have mineral deposits, descaling agents extend the life of such a system. \n\nAnother method to store heat in a hot water storage tank has many names: Stratified hot water storage tank with closed water circuit, stratified thermal storage, thermocline tank and water stratified tank storage but in all cases the significant difference is that pains are taken to maintain the vertical stratification of the water column, in other words to keep the hot water at the top of the tank while the water at the bottom is at a distinctly lower temperature.\n\nThis is desirable in places with a wide climatic range where summer cooling is as important as heating in winter, and entails one or more of the following measures: \n\nWhen a stratified hot water storage tank has closed water circuits, the water temperatures can be up to 90 to 95 °C at the top and 20 to 40 °C at the bottom. Calm, undisturbed water is a relatively poor heat conductor when compared to glass, bricks and soil.\n\n(Illustrated by a still lake, where the surface water can be comfortably warm for swimming but deeper layers be so cold as to represent a danger to swimmers, the same effect as gives rise to notices in London's city docks warning 'Danger Cold Deep Water).\n\nAccordingly, an arbitrary volume of hot water can be stored, as long as the stratification is kept intact. In this case there must not be vertical metal plates or tubes as they would conduct heat through the water layers, defeating the purpose of stratification. When effectively employed this technique can maintain water as high as 95 °C (i.e. just below boiling) yielding a higher energy density, and this energy can be stored a long time provided the hot water remains undiluted.\n\nDepending on the purpose of the installations, water exchanges tapping different levels allow water temperatures appropriate to the required use to be selected.\n\nIn many solar heating systems the energy parameters can be read as a function of time, from the 'dwell' time necessary to transform daylight into heat, at its peak the maximum hot water temperature near the top of the tank.\n\nWhen flow starts from the uppermost outlet, cold water enters the tank at the bottom. This drop in temperature causes the thermostat to switch on the electric heating element at the bottom of the tank. When the water at the top of the tank is drawn off the hot water at the top is displaced by relatively cooler water, the top thermostat turns the top element on. When the flow stops, the elements stay on until their settings are met.\n\nWhile it is common to have the top and bottom thermostats set differently in order to save energy, the fact that hot water rises means the thermostat controlling the upper element should feed the hottest supply, while the lower element the warmest.\n\nIf the thermostats in such a system are reversed - warm feed from the top, hot from the center - it may not only affect the energy efficiency of the system, feeding scalding water to a domestic hot water outlet may be dangerous, or if directed to warm-feed washers damage them beyond repair.\n\nHot water can cause painful, dangerous scalding injuries, especially in children and the elderly. Water at the outlet should not exceed 49 degrees Celsius. Some jurisdictions set a limit of 49 degrees on tank setpoint temperature. On the other hand, water stored below 60 degrees Celsius can permit the growth of bacteria, such as those that cause Legionnaire's disease, which is a particular danger to those with compromised immune systems. One technical solution would be use of mixing valves at outlets used for sinks, baths or showers, that would automatically mix cold water to maintain a maximum below 49 C. A proposal to add this to the building code of Canada was unsuccessful. \n\n"}
{"id": "28642548", "url": "https://en.wikipedia.org/wiki?curid=28642548", "title": "IGNITE SystemsGo", "text": "IGNITE SystemsGo\n\nIGNITE is a program designed to promote the study of engineering and the development of work force skills. It encourages students to pursue careers in science, technology, engineering, and mechanics.\n\nSystemsGo is the program and curriculum provided by IGNITE to be used in classrooms across Texas. It is based in Fredericksburg, Texas. It allows students to learn more about the past, present, and future of rocket technology, as well as using information learned from independent study to complete hands-on projects.\n\nEvery year for 3 days, usually in April, teams from high schools all over the US gather in Fredericksburg, Texas to launch their semester project rockets. The IGNITE program offers high school students three different levels of projects, ranging from a mid-sized rocket traveling to a one mile apogee to rockets weighing several hundred pounds with the potential to reach near-space altitudes.\n\nThe increasingly popular meets attract many residents of the area, including children. Events other than watching the main rockets launch include crafts, science projects, food vendors, and visiting with representatives from various universities who set up a booth at the event. All activities are free. There is, however, a small charge for children wishing to purchase miniature rocket kits, which are actually launched in a separate area. The National Oceanic and Atmospheric Administration also attends the meeting and assembles an outpost to help monitor the weather in order to ensure safe conditions for flight.\n\n\"Tsiolkovsky Level\" : The first stage of the IGNITE Program, named after the Soviet rocket scientist Konstantin Tsiolkovsky, requires students to propel a research package to an altitude of 5,280 feet (one mile) and to recover it after launch. This project is completed over the Spring semester with launches held in Fredericksburg, Texas.\n\n\"Oberth Level\" : The intermediate stage, named after the German physicist Hermann Oberth, gives students the goal of designing and launching a rocket able to reach transonic velocity while maintaining a maximum altitude of less than 13,000 feet. This project is completed over the Spring semester with launches held in Fredericksburg, Texas.\n\n\"Goddard Level\" : The most advanced level of the IGNITE Program, named after the American engineer and rocket scientist Robert H. Goddard, challenges students to create a rocket from scratch that can reach an altitude of 100,000 feet, which is considered to fall within the near space region of Earth’s atmosphere. Rockets for this level are often several hundred pounds and can reach velocities of Mach 3 or even Mach 4. Launches for this level are hosted by the US Air Force at the White Sands Missile Range, New Mexico. Schools which offer the Goddard Level of the IGNITE Program are as follows:\n\n\nThe IGNITE program has been praised by important figures such as Ex-President George W. Bush, NASA Administrator Daniel Goldin, and Texas Governor Rick Perry, along with various other congressman and senators.\n"}
{"id": "12836631", "url": "https://en.wikipedia.org/wiki?curid=12836631", "title": "Java Evolutionary Computation Toolkit", "text": "Java Evolutionary Computation Toolkit\n\nECJ is a freeware evolutionary computation research system written in Java. It is a framework that supports a variety of evolutionary computation techniques, such as genetic algorithms, genetic programming, evolution strategies, coevolution, particle swarm optimization, and differential evolution. The framework models iterative evolutionary processes using a series of pipelines arranged to connect one or more subpopulations of individuals with selection, breeding (such as crossover, and mutation operators that produce new individuals. The framework is open source and is distributed under the Academic Free License. ECJ was created by Sean Luke, a computer science professor at George Mason University, and is maintained by Sean Luke and a variety of contributors.\n\nFeatures (listed from ECJ's project page):\n\nGeneral Features:\n\n\nEC Features:\n\n\nGP Tree Representations:\n\n\nVector (GA/ES) Representations:\n\n\nOther Representations:\n\n\n\n"}
{"id": "9019587", "url": "https://en.wikipedia.org/wiki?curid=9019587", "title": "List of hazelnut diseases", "text": "List of hazelnut diseases\n\nThis article is a list of diseases of hazelnut (\"Corylus avellana\" & \"Corylus\" spp.).\n\n"}
{"id": "22899237", "url": "https://en.wikipedia.org/wiki?curid=22899237", "title": "List of mountain passes of South Africa", "text": "List of mountain passes of South Africa\n\n\n"}
{"id": "21543727", "url": "https://en.wikipedia.org/wiki?curid=21543727", "title": "List of prions", "text": "List of prions\n\nThis is a list of the genera, species, and sub-species belonging to the prions, which belong to the \"Procellariiformes\".\n\n\n\n"}
{"id": "2262453", "url": "https://en.wikipedia.org/wiki?curid=2262453", "title": "List of rattlesnake species and subspecies", "text": "List of rattlesnake species and subspecies\n\nThis list of rattlesnake species and subspecies includes the genera \"Crotalus\" and \"Sistrurus\". It follows the taxonomy currently provided by ITIS, which is based on the continuing work of Dr. Roy McDiarmid.\n\n\n\n"}
{"id": "38227676", "url": "https://en.wikipedia.org/wiki?curid=38227676", "title": "List of waterbodies in Saxony-Anhalt", "text": "List of waterbodies in Saxony-Anhalt\n\nWaterbody; Length in km; in Saxony-Anhalt flowing through; (confluence of ...); Remarks\n\nElbe; 1.091 km; rises in the Giant Mountains of the Czech Republic at a height of ca. 1,386 m, flows through or touches the Czech Republic, Saxony, Saxony-Anhalt, Brandenburg, Lower Saxony, Mecklenburg-Western Pomerania, Hamburg, Schleswig-Holstein\n\nElbe tributaries and water bodies with their confluence in Saxony-Anhalt\n\nSautal (left)\n\nElbe tributaries and waterbodies with their confluence outside Saxony-Anhalt\n\n(via Oker, Aller to the Weser)\n\n\nWaterbody; Waterbody system; Area in ha; location of the nearest significant settlement; Remarks\n\n\"The standing waterbodies are sorted by type. Their boundaries are however, somewhat fluid.\"\n\n\n\n\n\n\n\n\n\n"}
{"id": "15210932", "url": "https://en.wikipedia.org/wiki?curid=15210932", "title": "Lists of star names", "text": "Lists of star names\n\nIn astronomy, star names, in contrast to star designations, are proper names of stars that have emerged from usage in pre-modern astronomical traditions. Lists of these names appear in the following articles:\n"}
{"id": "39421551", "url": "https://en.wikipedia.org/wiki?curid=39421551", "title": "Mammals of Africa", "text": "Mammals of Africa\n\nMammals of Africa is a book series of six volumes from Bloomsbury Publishing. Published in 2013 and edited by Jonathan Kingdon, David Happold, Thomas Butynski, Michael Hoffmann, Meredith Happold and Jan Kalina, it describes every species of African land mammal which comprise 1,160 species and 16 orders.\n\n\n"}
{"id": "46223972", "url": "https://en.wikipedia.org/wiki?curid=46223972", "title": "Mandrake", "text": "Mandrake\n\nA mandrake is the root of a plant, historically derived either from plants of the genus \"Mandragora\" found in the Mediterranean region, or from other species, such as \"Bryonia alba\", the English mandrake, which have similar properties. The plants from which the root is obtained are also called \"mandrakes\". Mediterranean mandrakes are perennial herbaceous plants with ovate leaves arranged in a rosette, a thick upright root, often branched, and bell-shaped flowers followed by yellow or orange berries. They have been placed in different species by different authors. They are highly variable perennial herbaceous plants with long thick roots (often branched) and almost no stem. The leaves are borne in a basal rosette, and are variable in size and shape, with a maximum length of . They are usually either elliptical in shape or wider towards the end (obovate), with varying degrees of hairiness.\n\nBecause mandrakes contain deliriant hallucinogenic tropane alkaloids and the shape of their roots often resembles human figures, they have been associated with a variety of superstitious practices throughout history. They have long been used in magic rituals, today also in contemporary pagan traditions such as Wicca and Odinism.\n\nThe English name of the plant derived from Latin \"mandragora\" through French \"main-de-gloire\".\n\nAll species of \"Mandragora\" contain highly biologically active alkaloids, tropane alkaloids in particular. The alkaloids make the plant, in particular the root and leaves, poisonous, via anticholinergic, hallucinogenic, and hypnotic effects. Anticholinergic properties can lead to asphyxiation. Accidental poisoning is not uncommon. Ingesting mandrake root is likely to have other adverse effects such as vomiting and diarrhea. The alkaloid concentration varies between plant samples. Clinical reports of the effects of consumption of Mediterranean mandrake include severe symptoms similar to those of atropine poisoning, including blurred vision, dilation of the pupils (mydriasis), dryness of the mouth, difficulty in urinating, dizziness, headache, vomiting, blushing and a rapid heart rate (tachycardia). Hyperactivity and hallucinations also occurred in the majority of patients.\n\nThe root is hallucinogenic and narcotic. In sufficient quantities, it induces a state of unconsciousness and was used as an anaesthetic for surgery in ancient times. In the past, juice from the finely grated root was applied externally to relieve rheumatic pains. It was also used internally to treat melancholy, convulsions, and mania. When taken internally in large doses, however, it is said to excite delirium and madness.\n\nIn the past, mandrake was often made into amulets which were believed to bring good fortune, cure sterility, etc. In one superstition, people who pull up this root will be condemned to hell, and the mandrake root would scream as it was pulled from the ground, killing anyone who heard it. Therefore, in the past, people have tied the roots to the bodies of animals and then used these animals to pull the roots from the soil.\n\nTwo references to \"(dûdâ'îm\")—literally meaning \"love plant\"—occur in the Jewish scriptures. The Septuagint translates as (\"mandragóras\"), and the Vulgate follows the Septuagint. A number of later translations into different languages follow Septuagint (and Vulgate) and use mandrake as the plant as the proper meaning in both the Book of Genesis 30:14–16 and Song of Songs 7:12-13. Others follow the example of the Luther Bible and provide a more literal translation.\n\nIn Genesis 30:14, Reuben, the eldest son of Jacob and Leah, finds mandrake in a field. Rachel, Jacob's infertile second wife and Leah's sister, is desirous of the and barters with Leah for them. The trade offered by Rachel is for Leah to spend that night in Jacob's bed in exchange for Leah's . Leah gives away the plant to her barren sister, but soon after this (Genesis 30:14–22), Leah, who had previously had four sons but had been infertile for a long while, became pregnant once more and in time gave birth to two more sons, Issachar and Zebulun, and a daughter, Dinah. Only years after this episode of her asking for the mandrakes did Rachel manage to become pregnant.\n\nSir Thomas Browne, in \"Pseudodoxia Epidemica\", ch. VII, suggested the \"dudai'im\" of Genesis 30:14 is the opium poppy, because the word \"duda'im\" may be a reference to a woman's breasts.\n\nThe final verses of Chapter 7 of Song of Songs (Song of Songs 7:12–13), are:\n\nAccording to the legend, when the root is dug up, it screams and kills all who hear it. Literature includes complex directions for harvesting a mandrake root in relative safety.\nFor example, Josephus (\"circa\" 37–100 AD) of Jerusalem gives the following directions for pulling it up:\n\nExcerpt from Chapter XVI, \"Witchcraft and Spells\", of \"Transcendental Magic: Its Doctrine and Ritual\" by nineteenth-century occultist and ceremonial magician Eliphas Levi.\n\nThe following is taken from Jean-Baptiste Pitois' \"The History and Practice of Magic\":\n\n"}
{"id": "40143821", "url": "https://en.wikipedia.org/wiki?curid=40143821", "title": "Mesozoic–Cenozoic radiation", "text": "Mesozoic–Cenozoic radiation\n\nThe Mesozoic–Cenozoic Radiation is the third major extended increase in biodiversity in the Phanerozoic, after the Cambrian Explosion and the Great Ordovician Biodiversification Event. The Mesozoic–Cenozoic Radiation began in the mid-Mesozoic and extends through the Cenozoic, with no indication of having yet ended. It differs from the earlier radiations in that the diversity appears largely in the variety of species and other taxa below the level of order. \"The spectacular radiation of the angiosperms, mammals and certain reptile groups (such as the snakes) on land is matched by that of the planktonic foraminifera, neogastropods, heteroconch bivalves, cheilostome bryozoans, decapod crustaceans and teleost fish in shallow seas.\" The marine diversification is largely that represented by the modern evolutionary fauna, and there are also the terrestrial diversifications of the birds and among the insects.\n"}
{"id": "57630", "url": "https://en.wikipedia.org/wiki?curid=57630", "title": "Monsoon", "text": "Monsoon\n\nMonsoon () is traditionally defined as a seasonal reversing wind accompanied by corresponding changes in precipitation, but is now used to describe seasonal changes in atmospheric circulation and precipitation associated with the asymmetric heating of land and sea. Usually, the term monsoon is used to refer to the rainy phase of a seasonally changing pattern, although technically there is also a dry phase. The term is sometimes incorrectly used for locally heavy but short-term rains, although these rains meet the dictionary definition of monsoon.\n\nThe major monsoon systems of the world consist of the West African and Asia-Australian monsoons. The inclusion of the North and South American monsoons with incomplete wind reversal has been debated.\n\nThe term was first used in English in British India and neighbouring countries to refer to the big seasonal winds blowing from the Bay of Bengal and Arabian Sea in the southwest bringing heavy rainfall to the area.\n\nThe English \"monsoon\" came from Portuguese \"monção\", ultimately from Arabic \"mawsim\" (موسم \"season\"), \"perhaps partly via early modern Dutch \"monson\".\"\n\nStrengthening of the Asian monsoon has been linked to the uplift of the Tibetan Plateau after the collision of the Indian sub-continent and Asia around 50 million years ago. Because of studies of records from the Arabian Sea and that of the wind-blown dust in the Loess Plateau of China, many geologists believe the monsoon first became strong around 8 million years ago. More recently, studies of plant fossils in China and new long-duration sediment records from the South China Sea led to a timing of the monsoon beginning 15–20 million years ago and linked to early Tibetan uplift. Testing of this hypothesis awaits deep ocean sampling by the Integrated Ocean Drilling Program. The monsoon has varied significantly in strength since this time, largely linked to global climate change, especially the cycle of the Pleistocene ice ages. A study of marine plankton suggested that the Indian Monsoon strengthened around 5 million years ago. Then, during ice periods, the sea level fell and the Indonesian Seaway closed. When this happened, cold waters in the Pacific were impeded from flowing into the Indian Ocean. It is believed that the resulting increase in sea surface temperatures in the Indian Ocean increased the intensity of monsoons.\n\nFive episodes during the Quaternary at 2.22 Ma (PL-1), 1.83 Ma (PL-2), 0.68 Ma (PL-3), 0.45 Ma (PL-4) and 0.04 Ma (PL-5) were identified which showed a weakening of Leeuwin Current (LC). The weakening of the LC would have an effect on the sea surface temperature (SST) field in the Indian Ocean, as the Indonesian through flow generally warms the Indian Ocean. Thus these five intervals could probably be those of considerable lowering of SST in the Indian Ocean and would have influenced Indian monsoon intensity. During the weak LC, there is the possibility of reduced intensity of the Indian winter monsoon and strong summer monsoon, because of change in the Indian Ocean dipole due to reduction in net heat input to the Indian Ocean through the Indonesian through flow. Thus a better understanding of the possible links between El Niño, Western Pacific Warm Pool, Indonesian Throughflow, wind pattern off western Australia, and ice volume expansion and contraction can be obtained by studying the behaviour of the LC during Quaternary at close stratigraphic intervals.\n\nThe impact of monsoon on the local weather is different from place to place. In some places there is just a likelihood of having a little more or less rain. In other places, quasi semi-deserts are turned into vivid green grasslands where all sorts of plants and crops can flourish.\n\nThe Indian Monsoon turns large parts of India from a kind of semi-desert into green lands. See photos only taken 3 months apart in the Western Ghats. In places like this it is crucial for farmers to have the right timing for putting the seeds on the fields, as it is essential to use all the rain that is available for growing crops.\n\nMonsoons are large-scale sea breezes which occur when the temperature on land is significantly warmer or cooler than the temperature of the ocean. These temperature imbalances happen because oceans and land absorb heat in different ways. Over oceans, the air temperature remains relatively stable for two reasons: water has a relatively high heat capacity (3.9 to 4.2 J g K), and because both conduction and convection will equilibrate a hot or cold surface with deeper water (up to 50 metres). In contrast, dirt, sand, and rocks have lower heat capacities (0.19 to 0.35 J g K), and they can only transmit heat into the earth by conduction and not by convection. Therefore, bodies of water stay at a more even temperature, while land temperature are more variable.\n\nDuring warmer months sunlight heats the surfaces of both land and oceans, but land temperatures rise more quickly. As the land's surface becomes warmer, the air above it expands and an area of low pressure develops. Meanwhile, the ocean remains at a lower temperature than the land, and the air above it retains a higher pressure. This difference in pressure causes sea breezes to blow from the ocean to the land, bringing moist air inland. This moist air rises to a higher altitude over land and then it flows back toward the ocean (thus completing the cycle). However, when the air rises, and while it is still over the land, the air cools. This decreases the air's ability to hold water, and this causes precipitation over the land. This is why summer monsoons cause so much rain over land.\n\nIn the colder months, the cycle is reversed. Then the land cools faster than the oceans and the air over the land has higher pressure than air over the ocean. This causes the air over the land to flow to the ocean. When humid air rises over the ocean, it cools, and this causes precipitation over the oceans. (The cool air then flows towards the land to complete the cycle.)\n\nMost summer monsoons have a dominant westerly component and a strong tendency to ascend and produce copious amounts of rain (because of the condensation of water vapor in the rising air). The intensity and duration, however, are not uniform from year to year. Winter monsoons, by contrast, have a dominant easterly component and a strong tendency to diverge, subside and cause drought.\n\nSimilar rainfall is caused when moist ocean air is lifted upwards by mountains, surface heating, convergence at the surface, divergence aloft, or from storm-produced outflows at the surface. However the lifting occurs, the air cools due to expansion in lower pressure, and this produces condensation.\n\nThe monsoon of western Sub-Saharan Africa is the result of the seasonal shifts of the Intertropical Convergence Zone and the great seasonal temperature and humidity differences between the Sahara and the equatorial Atlantic Ocean. It migrates northward from the equatorial Atlantic in February, reaches western Africa on or near June 22, then moves back to the south by October. The dry, northeasterly trade winds, and their more extreme form, the harmattan, are interrupted by the northern shift in the ITCZ and resultant southerly, rain-bearing winds during the summer. The semiarid Sahel and Sudan depend upon this pattern for most of their precipitation.\n\nThe North American monsoon (NAM) occurs from late June or early July into September, originating over Mexico and spreading into the southwest United States by mid-July. It affects Mexico along the Sierra Madre Occidental as well as Arizona, New Mexico, Nevada, Utah, Colorado, West Texas and California. It pushes as far west as the Peninsular Ranges and Transverse Ranges of Southern California, but rarely reaches the coastal strip (a wall of desert thunderstorms only a half-hour's drive away is a common summer sight from the sunny skies along the coast during the monsoon). The North American monsoon is known to many as the \"Summer\", \"Southwest\", \"Mexican\" or \"Arizona\" monsoon. It is also sometimes called the \"Desert monsoon\" as a large part of the affected area are the Mojave and Sonoran deserts. However, it is debatable whether the North and South American weather patterns with incomplete wind reversal should be counted as true monsoons.\n\nThe Asian monsoons may be classified into a few sub-systems, such as the Indian Subcontinental Monsoon which affects the Indian subcontinent and surrounding regions including Nepal, and the East Asian Monsoon which affects southern China, Taiwan, Korea and parts of Japan.\n\nThe southwestern summer monsoons occur from July through September. The Thar Desert and adjoining areas of the northern and central Indian subcontinent heat up considerably during the hot summers. This causes a low pressure area over the northern and central Indian subcontinent. To fill this void, the moisture-laden winds from the Indian Ocean rush into the subcontinent. These winds, rich in moisture, are drawn towards the Himalayas. The Himalayas act like a high wall, blocking the winds from passing into Central Asia, and forcing them to rise. As the clouds rise their temperature drops and precipitation occurs. Some areas of the subcontinent receive up to of rain annually.\n\nThe southwest monsoon is generally expected to begin around the beginning of June and fade away by the end of September. The moisture-laden winds on reaching the southernmost point of the Indian Peninsula, due to its topography, become divided into two parts: the \"Arabian Sea Branch\" and the \"Bay of Bengal Branch\".\n\nThe \"Arabian Sea Branch\" of the Southwest Monsoon first hits the Western Ghats of the coastal state of Kerala, India, thus making this area the first state in India to receive rain from the Southwest Monsoon. This branch of the monsoon moves northwards along the Western Ghats (Konkan and Goa) with precipitation on coastal areas, west of the Western Ghats. The eastern areas of the Western Ghats do not receive much rain from this monsoon as the wind does not cross the Western Ghats.\n\nThe \"Bay of Bengal Branch\" of Southwest Monsoon flows over the Bay of Bengal heading towards North-East India and Bengal, picking up more moisture from the Bay of Bengal. The winds arrive at the Eastern Himalayas with large amounts of rain. Mawsynram, situated on the southern slopes of the Khasi Hills in Meghalaya, India, is one of the wettest places on Earth. After the arrival at the Eastern Himalayas, the winds turns towards the west, travelling over the Indo-Gangetic Plain at a rate of roughly 1–2 weeks per state, pouring rain all along its way. June 1 is regarded as the date of onset of the monsoon in India, as indicated by the arrival of the monsoon in the southernmost state of Kerala.\n\nThe monsoon accounts for nearly 80% of the rainfall in India. Indian agriculture (which accounts for 25% of the GDP and employs 70% of the population) is heavily dependent on the rains, for growing crops especially like cotton, rice, oilseeds and coarse grains. A delay of a few days in the arrival of the monsoon can badly affect the economy, as evidenced in the numerous droughts in India in the 1990s.\n\nThe monsoon is widely welcomed and appreciated by city-dwellers as well, for it provides relief from the climax of summer heat in June. However, the roads take a battering every year. Often houses and streets are waterlogged and slums are flooded despite drainage systems. A lack of city infrastructure coupled with changing climate patterns causes severe economic loss including damage to property and loss of lives, as evidenced in the 2005 flooding in Mumbai that brought the city to a standstill. Bangladesh and certain regions of India like Assam and West Bengal, also frequently experience heavy floods during this season. Recently, areas in India that used to receive scanty rainfall throughout the year, like the Thar Desert, have surprisingly ended up receiving floods due to the prolonged monsoon season.\n\nThe influence of the Southwest Monsoon is felt as far north as in China's Xinjiang. It is estimated that about 70% of all precipitation in the central part of the Tian Shan Mountains falls during the three summer months, when the region is under the monsoon influence; about 70% of that is directly of \"cyclonic\" (i.e., monsoon-driven) origin (as opposed to \"local convection\").\n\nAround September, with the sun fast retreating south, the northern land mass of the Indian subcontinent begins to cool off rapidly. With this air pressure begins to build over northern India, the Indian Ocean and its surrounding atmosphere still holds its heat. This causes cold wind to sweep down from the Himalayas and Indo-Gangetic Plain towards the vast spans of the Indian Ocean south of the Deccan peninsula. This is known as the Northeast Monsoon or Retreating Monsoon.\n\nWhile travelling towards the Indian Ocean, the dry cold wind picks up some moisture from the Bay of Bengal and pours it over peninsular India and parts of Sri Lanka. Cities like Chennai, which get less rain from the Southwest Monsoon, receive rain from this Monsoon. About 50% to 60% of the rain received by the state of Tamil Nadu is from the Northeast Monsoon. In Southern Asia, the northeastern monsoons take place from October to December when the surface high-pressure system is strongest. The jet stream in this region splits into the southern subtropical jet and the polar jet. The subtropical flow directs northeasterly winds to blow across southern Asia, creating dry air streams which produce clear skies over India. Meanwhile, a low pressure system known as a monsoon trough develops over South-East Asia and Australasia and winds are directed toward Australia.\n\nThe East Asian monsoon affects large parts of Indo-China, Philippines, China, Taiwan, Korea and Japan. It is characterised by a warm, rainy summer monsoon and a cold, dry winter monsoon. The rain occurs in a concentrated belt that stretches east-west except in East China where it is tilted east-northeast over Korea and Japan. The seasonal rain is known as \"Meiyu\" in China, \"Jangma\" in Korea, and \"Bai-u\" in Japan, with the latter two resembling frontal rain.\n\nThe onset of the summer monsoon is marked by a period of premonsoonal rain over South China and Taiwan in early May. From May through August, the summer monsoon shifts through a series of dry and rainy phases as the rain belt moves northward, beginning over Indochina and the South China Sea (May), to the Yangtze River Basin and Japan (June) and finally to North China and Korea (July). When the monsoon ends in August, the rain belt moves back to South China.\n\nAlso known as the Indo-Australian Monsoon. The rainy season occurs from September to February and it is a major source of energy for the Hadley circulation during boreal winter. The \"Maritime Continent Monsoon\" and the \"Australian Monsoon\" may be considered to be the same system, the Indo-Australian Monsoon.\n\nIt is associated with the development of the Siberian High and the movement of the heating maxima from the Northern Hemisphere to the Southern Hemisphere. North-easterly winds flow down Southeast Asia, are turned north-westerly/westerly by Borneo topography towards Australia. This forms a cyclonic circulation vortex over Borneo, which together with descending cold surges of winter air from higher latitudes, cause significant weather phenomena in the region. Examples are the formation of a rare low-latitude tropical storm in 2001, Tropical Storm Vamei, and the devastating flood of Jakarta in 2007.\n\nThe onset of the monsoon over the Maritime Continent tends to follow the heating maxima down Vietnam and the Malay Peninsula (September), to Sumatra, Borneo and the Philippines (October), to Java, Sulawesi (November), Irian Jaya and Northern Australia (December, January). However, the monsoon is not a simple response to heating but a more complex interaction of topography, wind and sea, as demonstrated by its abrupt rather than gradual withdrawal from the region. The Australian monsoon (the \"Wet\") occurs in the southern summer when the monsoon trough develops over Northern Australia. Over three-quarters of annual rainfall in Northern Australia falls during this time.\n\nThe European Monsoon (more commonly known as the return of the westerlies) is the result of a resurgence of westerly winds from the Atlantic, where they become loaded with wind and rain. These westerly winds are a common phenomenon during the European winter, but they ease as spring approaches in late March and through April and May. The winds pick up again in June, which is why this phenomenon is also referred to as \"the return of the westerlies\".\n\nThe rain usually arrives in two waves, at the beginning of June and again in mid- to late June. The European monsoon is not a monsoon in the traditional sense in that it doesn't meet all the requirements to be classified as such. Instead the return of the westerlies is more regarded as a conveyor belt that delivers a series of low pressure centres to Western Europe where they create unsettled weather. These storms generally feature significantly lower than average temperatures, fierce rain or hail, thunder and strong winds.\n\nThe return of the westerlies affects Europe's Northern Atlantic coastline, more precisely Ireland, Great Britain, the Benelux countries, Western Germany, Northern France and parts of Scandinavia.\n\n\n\n"}
{"id": "2936643", "url": "https://en.wikipedia.org/wiki?curid=2936643", "title": "Moon dog", "text": "Moon dog\n\nA moon dog, moondog, or mock moon, (scientific name paraselene, plural \"paraselenae\", meaning \"beside the moon\") is a relatively rare bright circular spot on a lunar halo caused by the refraction of moonlight by hexagonal-plate-shaped ice crystals in cirrus or cirrostratus clouds.\nMoon dogs appear as part of the 22° halo, roughly 10 Moon diameters outside the Moon. They are exactly analogous to sun dogs, but are rarer because the Moon must be bright, about quarter moon or more, for the moon dogs to be observed. Moon dogs show little color to the unaided human eye because their light is not bright enough to activate the cone cells.\n\n"}
{"id": "3073044", "url": "https://en.wikipedia.org/wiki?curid=3073044", "title": "Nesaea", "text": "Nesaea\n\nNesaea was one of the Nereids who gathered round Thetis in her sympathetic grief for Achilles' loss of Patroclus. This name is used to describe a genus of plants in the family Lythraceae.\n"}
{"id": "18515482", "url": "https://en.wikipedia.org/wiki?curid=18515482", "title": "Optical heterodyne detection", "text": "Optical heterodyne detection\n\nOptical heterodyne detection is a method of extracting information encoded as modulation of the phase, frequency or both of electromagnetic radiation in the wavelength band of visible or infrared light. The light signal is compared with standard or reference light from a \"local oscillator\" (LO) that would have a fixed offset in frequency and phase from the signal if the latter carried null information. \"Heterodyne\" signifies more than one frequency, in contrast to the single frequency employed in homodyne detection.\nThe comparison of the two light signals is typically accomplished by combining them in a photodiode detector, which has a response that is linear in energy, and hence quadratic in amplitude of electromagnetic field. Typically, the two light frequencies are similar enough that their difference or beat frequency produced by the detector is in the radio or microwave band that can be conveniently processed by electronic means.\n\nThis technique became widely applicable to topographical and velocity-sensitive imaging with the invention in the 1990s of synthetic array heterodyne detection. The light reflected from a target scene is focussed on a relatively inexpensive photodetector consisting of a single large physical pixel, while a different LO frequency is also tightly focussed on each virtual pixel of this detector, resulting in an electrical signal from the detector carrying a mixture of beat frequencies that can be electronically isolated and distributed spatially to present an image of the scene.\n\nOptical heterodyne detection began to be studied at least as early as 1962, within two years of the construction of the first laser.\n\nIt is instructive to contrast the practical aspects of optical band detection to Radio Frequency (RF) band heterodyne detection.\n\nUnlike RF band detection, optical frequencies oscillate too rapidly to directly measure and process the electric field electronically. Instead optical photons are (usually) detected by absorbing the photon's energy, thus only revealing the magnitude, and not by following the electric field phase. Hence the primary purpose of heterodyne mixing is to down shift the signal from the optical band to an electronically tractable frequency range.\n\nIn RF band detection, typically, the electromagnetic field drives oscillatory motion of electrons in an antenna; the captured EMF is subsequently electronically mixed with a local oscillator (LO) by any convenient non-linear circuit element with a quadratic term (most commonly a rectifier). In optical detection, the desired non-linearity is inherent in the photon absorption process itself. Conventional light detectors—so called \"Square-law detectors\"—respond to the photon energy to free bound electrons, and since the energy flux scales as the square of the electric field, so does the rate at which electrons are freed. A difference frequency only appears in the detector output current when both the LO and signal illuminate the detector at the same time, causing the square of their combined fields to have a cross term or \"difference\" frequency modulating the average rate at which free electrons are generated.\n\nAnother point of contrast is the expected bandwidth of the signal and local oscillator. Typically, an RF local oscillator is a pure frequency; pragmatically, \"purity\" means that a local oscillator's frequency bandwidth is much much less than the difference frequency. With optical signals, even with a laser, it is not simple to produce a reference frequency sufficiently pure to have either an instantaneous bandwidth or long term temporal stability that is less than a typical megahertz or kilohertz scale difference frequency. For this reason, the same source is often used to produce the LO and the signal so that their difference frequency can be kept constant even if the center frequency wanders.\n\nAs a result, the mathematics of squaring the sum of two pure tones, normally invoked to explain RF heterodyne detection, is an oversimplified model of optical heterodyne detection. Nevertheless, the intuitive pure-frequency heterodyne concept still holds perfectly for the wideband case \"provided that the signal and LO are mutually coherent\". Indeed, one can obtain narrow-band interference from coherent broadband sources: this is the basis for white light interferometry and optical coherence tomography. Mutual coherence permits the rainbow in Newton's rings, and supernumerary rainbows.\n\nConsequently, optical heterodyne detection is usually performed as interferometry where the LO and signal share a common origin, rather than, as in radio, a transmitter sending to a remote receiver. The remote receiver geometry is uncommon because generating a local oscillator signal that is coherent with a signal of independent origin is technologically difficult at optical frequencies. However, lasers of sufficiently narrow linewidth to allow the signal and LO to originate from different lasers do exist.\n\nAfter optical heterodyne became an established technique, consideration was given to the conceptual basis for operation at such low signal light levels that \"only a few, or even fractions of, photons enter the receiver in a characteristic time interval\". It was concluded that even when photons of different energies are absorbed at a countable rate by a detector at different (random) times, the detector can still produce a difference frequency. Hence light seems to have wave-like properties not only as it propagates through space, but also when it interacts with matter. Progress with photon counting was such that by 2008 it was proposed that, even with larger signal strengths available, it could be advantageous to employ local oscillator power low enough to allow detection of the beat signal by photon counting. This was understood to have a main advantage of imaging with available and rapidly developing large-format multi-pixel counting photodetectors.\n\nPhoton counting was applied with frequency-modulated continuous wave (FMCW) lasers. Numerical algorithms were developed to optimize the statistical performance of the analysis of the data from photon counting.\n\nThe amplitude of the down-mixed difference frequency can be larger than the amplitude of the original signal itself. The difference frequency signal is proportional to the product of the \"amplitudes\" of the LO and signal electric fields. Thus the larger the LO amplitude, the larger the difference-frequency amplitude. Hence there is gain in the photon conversion process itself.\nThe first two terms are proportional to the average (DC) energy flux absorbed (or, equivalently, the average current in the case of photon counting). The third term is time varying and creates the sum and difference frequencies. In the optical regime the sum frequency will be too high to pass through the subsequent electronics. In many applications the signal is weaker than the LO, thus it can be seen that gain occurs because the energy flux in the difference frequency formula_2 is greater than the DC energy flux of the signal by itself formula_3.\n\nBy itself, the signal beam's energy flux, formula_3, is DC and thus erases the phase associated with its optical frequency; Heterodyne detection allows this phase to be detected. If the optical phase of the signal beam shifts by an angle phi, then the phase of the electronic difference frequency shifts by exactly the same angle phi. More properly, to discuss an optical phase shift one needs to have a common time base reference. Typically the signal beam is derived from the same laser as the LO but shifted by some modulator in frequency. In other cases, the frequency shift may arise from reflection from a moving object. As long as the modulation source maintains a constant offset phase between the LO and signal source, any added optical phase shifts over time arising from external modification of the return signal are added to the phase of the difference frequency and thus are measurable.\n\nAs noted above, the difference frequency linewidth can be much smaller than the optical linewidth of the signal and LO signal, provided the two are mutually coherent. Thus small shifts in optical signal center-frequency can be measured: For example, Doppler lidar systems can discriminate wind velocities with a resolution better than 1 meter per second, which is less than a part in a billion Doppler shift in the optical frequency. Likewise small coherent phase shifts can be measured even for nominally incoherent broadband light, allowing optical coherence tomography to image micrometer-sized features. Because of this, an electronic filter can define an effective optical frequency bandpass that is narrower than any realizable wavelength filter operating on the light itself, and thereby enable background light rejection and hence the detection of weak signals.\n\nAs with any small signal amplification, it is most desirable to get gain as close as possible to the initial point of the signal interception: moving the gain ahead of any signal processing reduces the additive contributions of effects like resistor Johnson-Nyquist noise, or electrical noises in active circuits. In optical heterodyne detection, the mixing-gain happens directly in the physics of the initial photon absorption event, making this ideal. Additionally, to a first approximation, absorption is perfectly quadratic, in contrast to RF detection by a diode non-linearity.\n\nOne of the virtues of heterodyne detection is that the difference frequency is generally far removed spectrally from the potential noises radiated during the process of generating either the signal or the LO signal, thus the spectral region near the difference frequency may be relatively quiet. Hence, narrow electronic filtering near the difference frequency is highly effective at removing the remaining, generally broadband, noise sources.\n\nThe primary remaining source of noise is photon shot noise from the nominally constant DC level, which is typically dominated by the Local Oscillator (LO). Since the Shot noise scales as the \"amplitude\" of the LO electric field level, and the heterodyne gain also scales the same way, the ratio of the shot noise to the mixed signal is constant no matter how large the LO.\n\nThus in practice one increases the LO level, until the gain on the signal raises it above all other additive noise sources, leaving only the shot noise. In this limit, the signal to noise ratio is affected by the shot noise of the \"signal\" only (i.e. there is no noise contribution from the powerful LO because it divided out of the ratio). At that point there is no change in the signal to noise as the gain is raised further. (Of course, this is a highly idealized description; practical limits on the LO intensity matter in real detectors and an impure LO might carry some noise at the difference frequency)\n\nArray detection of light, i.e. detecting light in a large number of independent detector pixels, is common in digital camera image sensors. However, it tends to be quite difficult in heterodyne detection, since the signal of interest is oscillating (also called AC by analogy to circuits), often at millions of cycles per second or more. At the typical frame rates for image sensors, which are much slower, each pixel would integrate the total light received over many oscillation cycles, and this time-integration would destroy the signal of interest. Thus a heterodyne array must usually have parallel direct connections from every sensor pixel to separate electrical amplifiers, filters, and processing systems. This makes large, general purpose, heterodyne imaging systems prohibitively expensive. For example, simply attaching 1 million leads to a megapixel coherent array is a daunting challenge.\n\nTo solve this problem, synthetic array heterodyne detection (SAHD) was developed. In SAHD, large imaging arrays can be multiplexed into virtual pixels on a single element detector with single readout lead, single electrical filter, and single recording system. The time domain conjugate of this approach is Fourier transform heterodyne detection, which also has the multiplex advantage and also allows a single element detector to act like an imaging array. SAHD has been implemented as Rainbow heterodyne detection in which instead of a single frequency LO, many narrowly spaced frequencies are spread out across the detector element surface like a rainbow. The physical position where each photon arrived is encoded in the resulting difference frequency itself, making a virtual 1D array on a single element detector. If the frequency comb is evenly spaced then, conveniently, the Fourier transform of the output waveform is the image itself. Arrays in 2D can be created as well, and since the arrays are virtual, the number of pixels, their size, and their individual gains can be adapted dynamically. The multiplex disadvantage is that the shot noise from all the pixels combine since they are not physically separated.\n\nAs discussed, the LO and signal must be temporally coherent. They also need to be spatially coherent across the face of the detector or they will destructively interfere. In many usage scenarios the signal is reflected from optically rough surfaces or passes through optically turbulent media leading to wavefronts that are spatially incoherent. In laser scattering this is known as speckle.\n\nIn RF detection the antenna is rarely larger than the wavelength so all excited electrons move coherently within the antenna, whereas in optics the detector is usually much larger than the wavelength and thus can intercept a distorted phase front, resulting in destructive interference by out-of-phase photo-generated electrons within the detector.\n\nWhile destructive interference dramatically reduces the signal level, the summed amplitude of a spatially incoherent mixture does not approach zero but rather the mean amplitude of a single speckle. However, since the standard deviation of the coherent sum of the speckles is exactly equal to the mean speckle intensity, optical heterodyne detection of scrambled phase fronts can never measure the absolute light level with an error bar less than the size of the signal itself. \"This upper bound signal-to-noise ratio of unity is only for absolute magnitude measurement\": it can have signal-to-noise ratio better than unity for phase, frequency or time-varying relative-amplitude measurements in a stationary speckle field.\n\nIn RF detection, \"diversity reception\" is often used when the primary antenna is coincidentally located at an interference null point: by having more than one antenna one can adaptively switch to whichever antenna has the strongest signal or even incoherently add all of the antenna signals. Simply adding the antennae coherently can produce destructive interference just as happens in the optical realm.\n\nThe analogous diversity reception for optical heterodyne has been demonstrated with arrays of photon-counting detectors. For incoherent addition of the multiple element detectors in a random speckle field, the ratio of the mean to the standard deviation will scale as the square root of the number of independently measured speckles. This improved signal-to-noise ratio makes absolute amplitude measurements feasible in heterodyne detection.\n\nHowever, as noted above, scaling physical arrays to large element counts is challenging for heterodyne detection due to the oscillating or even multi-frequency nature of the output signal. Instead, a single-element optical detector can also act like diversity receiver via synthetic array heterodyne detection or Fourier transform heterodyne detection. With a virtual array one can then either adaptively select just one of the LO frequencies, track a slowly moving bright speckle, or add them all in post-processing by the electronics.\n\nOne can incoherently add the magnitudes of a time series of \"N\" independent pulses to obtain a improvement in the signal to noise on the amplitude, but at the expense of losing the phase information. Instead coherent addition (adding the complex magnitude and phase) of multiple pulse waveforms would improve the signal to noise by a factor of \"N\", not its square root, and preserve the phase information. The practical limitation is adjacent pulses from typical lasers have a minute frequency drift that translates to a large random phase shift in any long distance return signal, and thus just like the case for spatially scrambled-phase pixels, destructively interfere when added coherently. However, coherent addition of multiple pulses is possible with advanced laser systems that narrow the frequency drift far below the difference frequency (intermediate frequency). This technique has been demonstrated in multi-pulse coherent Doppler LIDAR.\n\n\n"}
{"id": "11624022", "url": "https://en.wikipedia.org/wiki?curid=11624022", "title": "Pacific War series", "text": "Pacific War series\n\nThe Pacific War is a series of alternate history novels written by Newt Gingrich and William R. Forstchen with Albert S. Hanser. The series deals with the Pacific War between the United States of America and the Empire of Japan. The point of divergence is the decision of Admiral Isoroku Yamamoto, commander-in-chief of the Japanese Combined Fleet, to take personal command of the 1st Air Fleet for the attack on Pearl Harbor, rather than delegate it to Adm. Chūichi Nagumo.\n\nThe first novel, \"Pearl Harbor: A Novel of December 8th\", covers the background up through the attack on the United States Navy base at Pearl Harbor, Hawaii. (\"December 8th\" is the date in Japan, on the west side of the International Date Line; the local time was December 7.)\n\nThe novel begins in Japan in 1934 where Lieutenant Commander James Watson of the US Navy and his equally-ranked friend Cecil Stanford of the British Royal Navy are guests of the Etajima Naval Academy, witnessing the harsh, aggressive training of recruits. They meet friendly young Lieutenant Mitsuo Fuchida of the Imperial Japanese Navy and the three discuss the growing military strength of Japan and the increasing political tensions across the Pacific.\n\nIn 1936, Stanford makes his report of his impressions of Japanese culture and their military & political ambitions to Winston Churchill. Back in Japan, Fuchida and Commander Genda Minoru formulate the new naval doctrines in which air-power will supersede the battleship as the prime weapon of the Imperial Fleet.\n\nIn December 1937, Watson is on board the US Navy gunboat USS \"Panay\" on the Yangtze River near Shanghai when it is strafed and sunk by Japanese aircraft despite the vessel clearly displaying the American flag. A few days later, Stanford, now a journalist, has the misfortune to witness, and narrowly escape (with the assistance of German businessman John Rabe), the brutal atrocities committed on the Chinese civilian population of Nanking by the Imperial Japanese Army. Fuchida and Stanford meet in China but the latter’s anger & disgust over recent events causes a falling-out between the two friends.\n\nIn September 1940, Genda, now a naval attaché in London, watches the Battle of Britain being fought over the English capital and he later gives his critical appraisal of the Campaign to USAAC Colonel Carl Spaatz.\n\nJanuary 1941. Watson, having lost one of his hands when the \"Panay\" sank and now living in Hawaii as a civilian, is recalled to duty as an Intelligence officer in the US Navy. He and his new friend and colleague Captain Tom Collingwood are assigned the task of deciphering Japanese coded communications. It proves to be a frustrating and laborious task.\n\nFebruary 1941. Admiral Yamamoto is advised by Genda on the crucial importance of naval airpower in the coming war with the West that seems inevitable.\n\nJune–September 1941. Fuchida strives to develop new techniques in attacking vessels in shallow-water harbors whilst Japanese Prime-Minister Konoye, with little authority over his country’s armed forces, reluctantly submits plans for military expansion to the Emperor for approval. Stanford is in Hanoi when he hears the news of the Japanese decision to occupy French Indochina, signalling that the Japanese are prepared to openly provoke the West. The US begins an embargo on oil supplies to Japan and Emperor Hirohito secretly approves plans for military action.\n\nOctober–November 1941. Genda manages to convince Yamamoto to personally command the naval task force assigned to mount a surprise attack on the US naval base at Pearl Harbor on Oahu in the Hawaiian Islands. Genda and Fuchida both feel that the original choice of commander, Admiral Nagumo, a ‘Battleship Admiral’ of the old school, lacks both conviction and a proper understanding of the new carrier tactics. Stanford is reassigned to Singapore by the now Prime-Minister Winston Churchill to report on the growing threat in the East. The Japanese carrier fleet sets sail from Japan on November 26 (Tokyo time), bound for Hawaii. On November 28, the US carrier , Admiral William Halsey commanding, departs Pearl Harbor, bound for Wake Island with a cargo of fighter aircraft.\n\nDecember 7, 1941 (December 8, Tokyo Time). The first and second waves of Japanese aircraft launched from the six fleet carriers of Yamamoto’s Imperial Task Force located north of Hawaii attack Pearl Harbor and US Naval & Army airfields on Oahu. Watson witnesses at close hand the destruction of Battleship Row on Ford Island including the terrible losses of the battleships \"Arizona\" and \"Oklahoma\". Fuchida, personally leading the attack, notes, as the second wave departs, that many of the harbor facilities including the main dry dock and the oil storage tanks along with all of the US submarines are still intact. Upon return to the flagship \"Akagi\", Fuchida advises Yamamoto to launch a third strike, as does Genda. The Admiral agrees but decides to send only half the available bombers, retaining the rest as a reserve in case the so far un-located US carriers should appear.\n\nShortly before 3pm local time, the third wave arrives over Pearl Harbor. This time, the defences are on alert, putting up a dense anti-aircraft barrage which inflicts severe losses, the strike force losing a third of its aircraft. But the attackers cause heavy damage nonetheless, destroying No 1 dry-dock, the oil tank farms and the headquarters of the Pacific Fleet, killing the Commander-in-Chief Admiral Kimmel. In addition, a number of ships left intact after the earlier attacks are sunk or damaged, including all of the submarines still moored in the harbor. Fuchida, his aircraft badly damaged, barely makes it back to his carrier. \nThe novel ends with Admiral Yamamoto grimly resolved to remain in Hawaiian waters until the battle is brought to a decisive conclusion and Admiral Halsey on board the USS \"Enterprise\", less than a day’s sail from Pearl Hearbor, mounting a search for the enemy fleet that he has vowed vengeance upon.\n\nThe novel begins where the previous book left off. 1800hrs on December 7, 1941, the Japanese carrier fleet is still 150 miles north of Oahu. The three air strikes that Yamamoto has despatched to Oahu has cost him over 80 aircraft, leaving the fleet with just under 300 planes still airworthy. Yamamoto is grimly pleased with the results thus far but he is troubled by reports that the Japanese Foreign Ministry failed in their mission to deliver a formal declaration of war to the US prior to the air attacks, thus allowing the Americans to brand them as ‘sneak attacks’, igniting their anger and righteous indignation.\n\nYamamoto knows that the US Navy possesses at least three aircraft-carriers in the Pacific theatre. He despatches two of his battleships, the \"Hiei\" and the \"Kirishima\", to mount a nocturnal bombardment of Pearl Harbor, hoping to provoke a counter-strike by the US carriers, thus exposing the location of the latter. Chief-of-Staff Rear Admiral Kusaka expresses his grave reservations about the risks of using the battleships as bait.\n\nMidnight, December 8 (local time). James Watson, his arm injured by shrapnel during the earlier attacks, has made his way home to his half-Japanese wife Margaret and his mother-in-law ‘Nan’ who is a Nisei, a first-generation Japanese immigrant to Hawaii of whom there are many thousands living on the islands. Watson has to quickly evacuate his family when the sudden enemy bombardment of Oahu begins, causing severe damage to both the harbor and Honolulu and provoking confused return fire by US coastal batteries, ‘friendly fire’ adding to the casualty toll. South-west of Oahu, Admiral Halsey, furious at this latest attack, immediately orders the \"Enterprise\" to move in closer in order to launch a counter-strike.\n\n0100hrs. The US Navy manages to deliver its first return blow when a small force of warships led by Rear-Admiral Draemel on board the destroyer USS \"Ward\", engages the Japanese battleships off the coast of Oahu. The gallant American force suffers heavy losses, including the \"Ward\", taking Draemel with her along with the cruiser USS \"Minneapolis\" but they manage to score a torpedo hit on the \"Hiei\", crippling the large battleship.\n\n0200hrs. Having lost a full squadron of dive-bombers the previous day at Pearl Harbor, Halsey has only 56 aircraft remaining on the \"Enterprise\" with which to engage the entire Japanese fleet. He commits half of his available bombers to a full-out search for the enemy carriers. Meanwhile, Yamamoto has divided his carriers, despatching the \"Hiryu\" and \"Soryu\", commanded by Admiral Ozawa, closer to Oahu to cover the crippled \"Hiei\" and guard against attacks from the south-east whilst the rest of the carriers are north-west of the islands, covering the west flank. The Japanese launch scout-planes to find the US carriers. On Oahu, Watson, Captain Collingwood and assistant Dianne St Clair desperately try to re-organise communications to co-ordinate the groups of US warships that are scattered throughout the Pacific and now converging on Hawaii.\n\n0630hrs. As President Roosevelt delivers his famous Day of Infamy speech in Washington, US aircraft from the \"Enterprise\" attack the \"Hiei\", further crippling her. One of Yamamoto’s scout planes locates Halsey’s task-force.\n\n0730hrs. Aircraft from the IJN carriers \"Soryu\" and \"Hiryu\" attack Halsey’s force, sinking the cruiser USS \"Salt Lake City\" and badly damaging the \"Enterprise\", leaving her still able to launch but not recover her planes. The \"Enterprise\" launches what aircraft she has left-a mere two dozen-in a counter-strike against the Japanese flat-tops.\n\n0945hrs. The \"Enterprise\"s attack is successful, scoring two hits on the carrier \"Soryu\", leaving her temporarily out of action, but only seven planes survive to head to Oahu, including F4F pilot Lieutenant Dellacroce and dive-bomber pilot Lieutenant Dan Struble.\n1100hrs. The \"Enterprise\" is attacked again by a second wave from Ozawa’s force, the Japanese pilots believing it to be a second US carrier as Halsey’s crew had extinguished the fires from the previous strike. Struck by bombs and torpedoes, the US carrier is desperately injured, perhaps mortally. In Washington, President Roosevelt confers with Admiral Stark and General Marshall, deciding that available resources need to be concentrated on defending Hawaii and that it is virtually impossible to relieve the embattled US forces defending the Philippines which have been invaded by the Imperial army.\n\n1400hrs. US Task Force 12, comprising the carrier USS \"Lexington\" and her escorts, led by Admiral Newton, now enters the fray, having returned from Midway atoll where she had ferried aircraft prior to the Japanese attack.\n\n1630hrs. A tiny force of sixteen aircraft, consisting of the surviving \"Enterprise\" planes and the last handful of flyable US Army aircraft at Hickam airfield on Oahu, makes a gallant attack on Yamamoto’s main carrier force. Meeting heavy resistance, the motley flight is all but wiped out but Lt Dan Struble, dying and his aircraft afire, crash-dives into Yamamoto’s flagship carrier, the \"Akagi\". Lt Dave Dellacroce’s F4F is one of only four planes to escape.\n\n1730hrs. The atmosphere on Oahu is fearful, tense and angry with rumours and mis-information circulating everywhere. The National Guard are enforcing martial law. Groggy with pain and fatigue, Watson is driven home by Dianne who has just learnt her boyfriend, an Army pilot, was killed the previous day. There is a tense episode when Dianne meets Watson’s Japanese wife & mother-in-law.\n\n1750hrs. The crippled \"Hiei\", now 30 miles south-west of Oahu, is torpedoed and sunk by the submarine USS \"Gudgeon\".\n\n1915hrs. The severely damaged \"Enterprise\", thanks to a superhuman effort on the part of her crew, has managed to stay afloat and is commencing a slow, dangerous crawl to the safety of the West Coast of the US mainland.\n\n0550hrs. December 9 (local time). A Japanese scout plane sights the location of Task Force 12. Shortly afterwards, a US submarine signals the location of Yamamoto’s main force. Both sides launch air-strikes, the groups passing within sight of each other on the way to their respective targets. The \"Lexington\" is sunk, as is the already damaged \"Akagi\". Newton and Yamamoto abandon their respective flagships. Yamamoto orders his fleet to withdraw.\n\n1000hrs. An angry, drunken lynch-mob threaten to execute a young Japanese boy in the street where Watson's family are staying. Margaret is attacked by a would-be rapist but Dianne, armed with a pistol, keeps the mob at bay until National Guardsmen arrive to restore order.\n\nEvening, December 10. Yamamoto receives a summons to appear before the Emperor, most likely to account for the losses his fleet has suffered. Although his fleet has inflicted severe damage on the Americans, the decisive victory that he yearned for has not occurred. Both sides resign themselves to a long, bitter conflict.\n\n\nAlthough the sinkings of the , USS \"Arizona\" and USS \"Oklahoma\" occurred in factual history as described in these alternate history novels, other warships that are damaged or sunk in the Pacific War series had, of course, different careers in real-life.\n\n\n\n"}
{"id": "16985133", "url": "https://en.wikipedia.org/wiki?curid=16985133", "title": "Periphery countries", "text": "Periphery countries\n\nIn world systems theory, the periphery countries (sometimes referred to as just the periphery) are those that are less developed than the semi-periphery and core countries. These countries usually receive a disproportionately small share of global wealth. They have weak state institutions and are dependent on – according to some, exploited by – more developed countries. These countries are usually behind because of obstacles such as lack of technology, unstable government, and poor education and health systems. In some instances, the exploitation of periphery countries' agriculture, cheap labor, and natural resources aid core countries in remaining dominant. This is best described by dependency theory, which is one theory on how globalization can affect the world and the countries in it. It is, however, possible for periphery countries to rise out of their status and move into semi-periphery or core status. This can be done by doing things such as industrializing, stabilizing the government and political climate, etc.\n\nPeriphery countries are those that exist on the outer edges of global trade. There could be many reasons for a country to be considered peripheral, such as a dysfunctional or inefficient government. For example, some nations customs and ports are so inefficient that even though they are geographically closer it is cheaper to ship goods from longer distances. Other reasons such as wars, non-central location, insufficient infrastructure (rail lines, roads and communications) will keep a country in the periphery of global trade. Generally the populations tend to be poor and destitute so the core countries will exploit them for cheap labor and will even purposely interfere with their politics to keep things this way. Usually a peripheral country will specialize in one particular industry, leaving it vulnerable to economic instability and limiting international investment. Sometimes countries decide to isolate themselves, such as 14th century China.\n\nThere are a variety of reasons that periphery countries remain the way they are. One important factor that keeps countries in the periphery is the lack of development of technology. Another way periphery countries come to be is either the lack of a central government or the periphery country is under the control of another country. Periphery countries are known for exporting raw goods to core countries. What tends to happen is the maximum gain a periphery nation could earn is less than needed to maintain an equilibrium between costs and revenues. One thing periphery nations could do is to stop the increase of exports. At the beginning of the 19th century, Asia and Africa were considered periphery and their lack of development enabled the United States and Germany to remain successful core nations.\n\nAlthough periphery nations are exploited by core countries, there is a purpose to the unequal exchanges of goods. For instance, the core countries have an incentive to gain a profit and this enables the world market to further grow. At times, there is a change in the balance of trade between the periphery and core countries. This occurs when the prices of exports from periphery countries decrease at a faster rate than the exports from core nations. In this case, the governments of the periphery nation are affected in several ways. For instance, there is an increase in unemployment as well as a decrease in state income. This type of interaction is unique because the core country involved is somewhat weaker than normal. An example of this occurring is the growth of the industrial capabilities of Italy and Russia towards the end of the 19th century. This has also occurred in other periphery nations such as Brazil, Mexico, and South Africa.\n\nThe world system at this time was much different from the world system of today. Several areas were beginning to develop into trading powers but none were able to gain total control. For this reason, a core and periphery developed in each region as opposed to a global scale. Cities began to become the \"core\" with the more agricultural countryside becoming a sort of \"periphery\". The most underdeveloped region that was still involved in trade at the time was Europe. It had the weakest core and periphery areas.\n\nTwo examples of periphery countries in the late 15th century and early 16th century are Poland and Latin America. At this time, Poland was mainly exporting wheat to other areas of Europe and Poland wanted cheap labor. As a result, landlords enslaved rural workers on their estate lands. Also, Latin America experienced an enslavement of their natives and imports of slaves from Africa. Forced mining labor was placed on the slaves, which enabled Latin America to export cheap goods to Europe. Both Poland and Latin America were similar during this time period because the aristocrats of these areas became more wealthy due to their interactions with the world economy. These areas of the world were also different from during medieval times in Europe. They are different because during the late 15th century and early 16th century, Poland and Latin America were producing goods and exporting them rather than simply consuming their raw goods.\n\nThe relationship that the periphery countries have with the core countries is one that is defined by the exploitation of the periphery countries by the core countries. As many countries began to industrialize they looked for cheap goods and products. These industrialized \"core\" countries would then look to the less developed \"periphery\" countries for cheap goods. In most cases it is much easier and inexpensive to get these goods from other countries. Core countries realized this and began to use these cheap resources.\n\nFor the core countries to remain at the core, it is necessary that the periphery countries keep producing these cheap goods and remain mostly agricultural. The core countries are able to get goods very cheaply from the periphery and then are able to manufacture products and sell them at a relatively high price. The periphery countries are unable to make any gains because of this relationship and it is therefore impossible for them to ever industrialize. It is argued that if these countries are never able industrialize, they will continue to remain on the periphery.\n\nThe current relationship between core countries and periphery countries was mostly defined in the era of imperialism that occurred in the late 19th through the early 20th centuries. It was at this time that the countries with the strongest economies and militaries began to exploit those countries with weaker states. A result of this exploitation was the tendency of underdeveloped states or colonies to move more towards the production of one type of export that would then come to dominate their land, territory and lifestyle economy. Some examples of the time include Brazil's coffee production and Cuba's cigar production.\n\nPeriphery countries are continuously exploited by countries due to the exportation of surpluses of raw goods to the more technologically industrialized core countries for manufacturing and distribution. Recently some of the manufacturing has been moved to periphery countries but it is still controlled and owned by the core countries. There are, however, ways in which periphery countries can rise from their poor status and become semi-periphery countries or even core countries. It is crucial for the core countries to keep exploiting the natural resources of the periphery countries and to keep the governments semi-stable or else it could cause economic unrest for the core countries as a whole.\n\nThere are several ways in which periphery countries are able to escape their poor status. Some of these ways are stabilizing their governments, becoming more industrialized and using natural resources to benefit themselves rather than core countries, and creating a better education system. Developing a banking system that can compete on a global scale is also another way in which periphery countries can help better themselves in the global market.\n\nOne main way in which a periphery country can rise to semi-periphery or core status is through the stabilization of its government. A country with a dictatorship type government is much easier to exploit and corrupt than one with a well organized, elected government and core countries use this to their advantage. Political unrest is usually a cause for military action from the core countries in order to protect their interests and keep a cooperative dictator or government in power. Once the citizens of these countries become exploited enough, they can stage a coup in order to overthrow their government and put someone who they feel will help the country into power. If this is done successfully and the new leader is stays true to his/her word, the country can take the next necessary step in rising from periphery status and that is to start to industrialize.\n\nSome Neo-Marxists believe that it would actually be best for periphery countries to cut all economic and political ties with the core countries. This would, in their opinion, allow the periphery countries to develop and industrialize at their own pace instead of being exploited and driven by core countries demands. Doing this would allow these countries to spend their money on industrializing and bettering themselves, rather than importing goods from core countries. It also would allow these countries to become more independent from the core countries, causing them to move to semi-peripheral status.\n\nMost periphery countries rely almost entirely on agriculture and other natural resources such as oil, coal, and diamonds in order to gain some sort of profit, but this also keeps them from growing economically. In order for them to grow they must industrialize in order to produce finished goods for exportation around the world, instead of allowing the core countries to profit from their natural resources. Industrializing and adapting newer technology is one of the major ways in which periphery countries can begin to raise their standard of living and help increase the wealth of their citizens. Becoming industrialized also will help to force trade to come to their cities, if they can produce goods at competitive prices, allowing them to reach out to the global market and take hold. Once a periphery country can industrialize, and use its own resources to its own benefit, it will begin to enter semi-periphery status.\n\nIn order for a periphery country to industrialize, it must first undergo proto-industrialization. In this stage, a market-based economy begins to form, normally in rural areas, using agricultural products. Proto-industrialization also helps to organize the rural market in these country and allows for them to become more capitalistic. Finally, once these countries develop this style of economy, they can begin to build factories and machines.\n\nOne of the final steps for a periphery country to rise to semi-periphery or core status is to educate its citizens. Raising the literacy rate allows ideas to spread more quickly through a country and also allows people to better communicate with themselves and the rest of the world. Also once universities are developed a country can begin to research new technology. Researching new technology can help a country to better compete in a global market by becoming more efficient or selling new technology and industrial techniques. If education and industry is allowed to become developed enough it is entirely possible for a periphery country to rise to core country status and become a leader in the global market. Another way in which periphery countries better their education system is by spending money to send university level students and staff abroad to places such as the U.S. and Europe to receive better education.\n\nOnce the people in these countries have become educated enough and they realize their place in the world economy, they can also demand help from the core countries. Although unlikely, due to the fact that the core countries rely on the exploitation of the periphery, there have been pushes for core countries to help better the periphery countries. Some of the ideas suggested are to help aid the periphery countries in developing by exploiting them less, help the periphery countries lose some of their debt and raise the prices on goods coming from these countries to allow them to be more profitable. These policies are obviously not beneficial to the core countries and is mostly why they have never been adapted successfully but this is another way in which the periphery could rise to a higher status.\n\nDuring the early 20th century the economy of the Russian Empire was a primarily agrarian country with isolated pockets of heavy industries. The Empire fell in 1917; the core of its industrial workers shrank from 3.6 million in 1917 to 1.5 million in 1920. After the end of the Russian Civil War the Soviet Union was industrialized under the rule of Joseph Stalin. Industrialization peaked in 1929-1932 in a rapid campaign described as \"a revolution from above\". Former personal private farms were collectivized in the early 1930s and gradually supplied with tractors and other machinery. Mechanization of farm labor, among other factors, contributed to freeing up workers for the newly built factories. In 1928-1932 alone at least ten million peasants migrated to the cities, causing \"an unprecedented demographic upheaval\". Industrialization allowed the country to trade in the global trade market. By the 1950s and 60s, only about 30 years after it began to industrialize, the Soviet Union was considered by most scholars a core country along with the United States.\n\nOnce a periphery country rises up to core countries status it will be more dependent on other periphery countries for natural resources. They may also start to exploit other periphery countries to continue to better themselves. One of the biggest impacts of this rise of status is the effects it has on the people of these countries. Health care is one of the first major improvements these countries will see, people will no longer die en masse from diseases such as malaria and will be better treated for non-communicable diseases.\nEducation is also another way in which the citizens will benefit. As a country becomes richer, it is able to build more schools and better fund the schools already built. This was seen in Russia after the October Revolution. A better educated public leads to a more efficient workforce, and can also lead the country to technological breakthroughs in industry and manufacturing. These countries will also experience much less severe famine now that they are able to trade successfully on a global scale.\n\nPeriphery countries as listed in the appendix of \"Trade Globalization since 1795: waves of integration in the world-system\" that appeared in the American Sociological Review (Dunn, Kawana, Brewer (2000)).\nAnd this is the periphery listing according to Babones (2005), who notes that this list is composed of countries that \"have been consistently classified into a single one of the three zones [core, semi-periphery or periphery] of the world economy over the entire 28-year study period\".\n\n"}
{"id": "10108509", "url": "https://en.wikipedia.org/wiki?curid=10108509", "title": "Philip Hollom", "text": "Philip Hollom\n\nPhilip Arthur Dominic Hollom (9 June 1912 – 20 June 2014) was a British ornithologist. He was born in Bickley, Kent, England, the second of five sons. His younger brother, Sir Jasper Hollom, was Deputy Governor of the Bank of England from 1970 to 1980, having been Chief Cashier of the Bank of England from 1962 to 1966. \n\nIn March 1951 he became a member of the editorial board of \"British Birds\" magazine under the senior editorship of Max Nicholson, whom he succeeded in 1960. Nicholson, who had remained on the editorial board, and Hollom stood down in 1972 and were replaced on the board by Ian Wallace and Malcolm Ogilvie.\n\nHollom was a Council member and Vice President of the Ornithological Society of the Middle East. He was the first chairman of the British Birds Rarities Committee and was awarded the British Trust for Ornithology's Tucker Medal in 1954 and the British Ornithologists' Union's Union Medal \"for his outstanding contribution to the BOU and to ornithology\" in 1984. \n\nHe lived in Hydestyle, Surrey, from the mid-1980s until his death. He turned 100 in June 2012 and died on 20 June 2014 at the age of 102. He had been a member of the BOU for 81 years. Hollom was survived by his daughter and two sons.\n\n\n\n"}
{"id": "4999235", "url": "https://en.wikipedia.org/wiki?curid=4999235", "title": "Radappertization", "text": "Radappertization\n\nRadappertization is a form of food irradiation which applies a dose of ionizing radiation sufficient to reduce the number and activity of viable microorganisms to such an extent that very few, if any, are detectable in the treated food by any recognized method (viruses being excepted). No microbial spoilage or toxicity should become detectable in a food so treated, regardless of the conditions under which it is stored, provided the packaging remains undamaged. The required dose is usually in the range of 25-45 kiloGrays.\n\nRadappertized foods packaged correctly will keep indefinitely.\n\nRadappertization is derived from the combination of \"rad\"iation and Appert, the name of the French scientist and engineer who invented sterilized food for the troops of Napoleon.\n\n"}
{"id": "3682781", "url": "https://en.wikipedia.org/wiki?curid=3682781", "title": "Seam sealant", "text": "Seam sealant\n\nSeam sealants are chemical coating compositions.\n\nSeam sealing was already performed manually successfully for preventing perforation corrosion in the 1980s. Today used in the OEM automotive industry primarily for the purpose of seals against air leaks and to waterproof sheetmetal overlaps that occur in the assembly of a vehicle. Such overlaps are typically decorative rather than structurally supportive. Accordingly, they are usually only spot welded and this process results in a closure that is not air or water tight.\n\nSeam sealants are sprayed or extruded over the joined edges of these overlaps, and they then either cure to a flexible waterproof \"seal\" by drying (dehydrating) in the case of water borne compositions, or thermoset irreversibly to a flexible adherent seam seal by going through an oven bake in the case of plasticized polyvinylchloride compositions. Most interior seam seals are not visible after the vehicle is finished, because they are covered by carpeting, interior roof headliner, or decorative trim panels. Exterior seam seals are always painted over and are referred to as \"coach joint seals.\"\n\nThe flat-stream application with many special solutions (special nozzles) was increasingly used as the most flexible procedure.\n\n"}
{"id": "38589062", "url": "https://en.wikipedia.org/wiki?curid=38589062", "title": "Society of Wetland Scientists", "text": "Society of Wetland Scientists\n\nThe Society of Wetland Scientists, Inc. (SWS) is an international, professional non-profit organization devoted to promoting understanding, conservation, protection, restoration, science-based management, and sustainability of wetlands. SWS has members in governmental agencies, non-governmental organizations, academia and private consulting. However, Society membership is open to anyone with an interest in wetlands. SWS has always been known for being a forum for scientists and managers to meet and work together. Based in Madison, Wisconsin, United States, it has 3000+ members, worldwide.\n\nSWS has sixteen regional chapters around the world that provide a local resource for networking, education and other wetland-related events: Alaska, Asia, Canada, Central, China, Europe, International, Mid-Atlantic, New England, North Central, Oceania, Pacific Northwest, Rocky Mountain, South Atlantic, South Central, Western. SWS has nine sections that work to enhance the SWS Annual Meeting by organizing symposia and workshops: Biogeochemistry, Education, Global Change Ecology, Peatlands, Public Policy and Regulation, Ramsar, Wildlife, Wetland Restoration, and Women in Wetlands. \n\nSWS has been managed by an association management company, AMPED Association Management, since 2010. SWS is associated with the SWS Professional Certification Program, which works \"to identify qualified individuals to assess and manage the Nation’s resources.\" The certification program is run by a separate office and collects separate membership dues.\n\nSWS was founded in March 1980 by Richard Macomber, a biologist with the United States Army Corps of Engineers Board of Rivers and Harbors. That same year, the first SWS Annual Meeting was held in Tampa, Florida, United States. The first president of SWS was James F. Parnell from the University of North Carolina Wilmington. The first issue of \"Wetlands\", the Society's premier, international journal, was published in 1981 as proceedings for the Annual Meeting. Since that time, \"Wetlands\" has evolved into a quarterly journal, communicating research to an expanding community of international and interdisciplinary wetland professionals. It is currently published by Springer on behalf of the SWS.\n\nSince 2015, the SWS Europe Chapter has been encouraging more robust protection for the Republic of Macedonia's Lake Ohrid, one of the most biodiverse inland waters on Earth, and the last remains of a previously extensive wetland along its shore, Studenchishte Marsh. Alongside supporting an initiative by local organizations EDEN and Ohrid SOS to establish both areas jointly as a Wetland of International Importance under the Ramsar Convention, SWS has also released a Declaration on the Protection of the Lake Ohrid Ecosystem, which outlines the importance of the location and proposes various measures for its protection, revitalization and sustainable development.\n"}
{"id": "4444029", "url": "https://en.wikipedia.org/wiki?curid=4444029", "title": "Surge (glacier)", "text": "Surge (glacier)\n\nGlacial surges are short-lived events where a glacier can advance substantially, moving at velocities up to 100 times faster than normal. Surging glaciers cluster around a few areas. High concentrations of surging glaciers occur in Svalbard, the Canadian Arctic islands, Alaska and Iceland. In some glaciers, surges can occur in fairly regular cycles, with 15 to 100 or more surge events per year. In other glaciers, surging remains unpredictable. In some glaciers, however, the period of stagnation and build-up between two surges typically lasts 10 to 200 years and is called the quiescent phase.\nDuring this period the velocities of the glacier are significantly lower, and the glaciers can retreat substantially.\n\nGlacier surges have been divided into two categories depending on the character of the surge event. Glaciers in Alaska exhibit surges with a sudden onset, extremely high (tens of meters/day)maximum flow rate and a sudden termination, often with a discharge of stored water. These are called Alaskan-type surges and it is suspected that these surges are hydrologically controlled.\n\nSurges in Svalbard typically exhibit different behavior. Svalbard surges are typically associated with slower onset with an acceleration phase, rising to a maximum velocity which is typically slower (up to four or five meters per day) than Alaskan surges, and a return to quiessence often taking years. Features observed during the active or surge phase include potholes, known as lacunas and medial moraines.\n\nIn the Norwegian Arctic, Svalbard is an archipelago containing hundreds of glaciers. Svalbard is more than 60% covered by glaciers and of these glaciers, hundreds have been observed to surge.\n\nGlacial surges in Karakoram occur in the presence of \"extreme uplift and denudation.\"\n\nIn 1980, there were several mini-surges of Variegated Glacier in Alaska. Mini surges typically show lag times of basal flow of 5–10 hours, which correlates to differences between the surging part of a glacier and the output of water and sediment. When the 1982 surge ended on July 5, there was a large flood event that day, and more flooding in the following days. What Humphrey found in his study is that behind the glacial surge zone, there are predominantly low basal water velocities, and high sliding rates before the rapid release of large quantities of water.\n\nThere have been many theories of why glacial surges occur.\n\nSurges may be caused by the supply of meltwater to the base of a glacier. Meltwater is important in reducing frictional forces to glacial ice flow. The distribution and pressure of water at the bed modulates the glacier's velocity and therefore mass balance. Meltwater may come from a number of sources, including supraglacial lakes, geothermal heating of the bed, conduction of heat into the glacier and latent heat transfers. There is a positive feedback between velocity and friction at the bed, high velocities will generate more frictional heat and create more meltwater. Crevassing is also enhanced by greater velocity flow which will provide further rapid transmission paths for meltwater flowing towards the bed. However, Humphrey found no precise correlation between ice-slow down and the release of water inside of a glacier.\n\nThe evolution of the drainage system under the glacier plays a key role in surge cycles.\n\nGlaciers that exhibit surges like those in Svalbard; with slower onset phase, and a longer termination phase may be thermally controlled rather than Hydrologically controlled. These surges tend to last for longer periods of time than Hydrologically controlled surges.\n\nIn other cases, the geology of the underlying country rock may dictate surge frequency. For example, poorly consolidated sedimentary rocks are more prone to failure under stress; a sub-glacial \"landslip\" may permit the glacier to slide. This explains why surging glaciers tend to cluster in certain areas.\n\nMeier and Post suggest that once mass accumulates to a critical point, basal melting begins to occur. This provides a buoyancy force, \"lifting\" the glacier from the bed and reducing the friction force.\n\n"}
{"id": "35645766", "url": "https://en.wikipedia.org/wiki?curid=35645766", "title": "Sutter's Mill meteorite", "text": "Sutter's Mill meteorite\n\nThe Sutter's Mill meteorite is a carbonaceous chondrite which entered the Earth's atmosphere and broke up at about 07:51 Pacific time on April 22, 2012, with fragments landing in the United States. The name comes from the Sutter's Mill, a California Gold Rush site, near which some pieces were recovered. This was the largest meteoroid impact over land since asteroid 2008 TC3. Meteor astronomer Peter Jenniskens assigned SM numbers to each meteorite, with the documented find location preserving information about where a given meteorite was located in the impacting meteoroid. As of May 2014, 79 fragments had been publicly documented with a find location. The largest (SM53) weighs , and the second largest (SM50) weighs .\n\nThe meteorite was found to contain some of the oldest material in the solar system. Two 10-micron diamond grains (xenoliths) were found in the meteorite recovered before the rain fell. In primitive meteorites like Sutter's Mill, some grains survived from what existed in the cloud of gas, dust and ice that formed the solar system.\n\nDuring the 2012 Lyrids meteor shower, a bolide and sonic boom rattled buildings in California and Nevada in daylight conditions in the early morning at 07:51 PDT on 22 April 2012. The bolide air burst was caused by a random meteoroid, not a member of the Lyrids shower. The bolide was so bright that witnesses were seeing spots afterward. The falling meteorites were detected by weather radar over an area centered on the Sutter's Mill site in Coloma, between Auburn, California, and Placerville, California.\n\nRobert Ward found a small CM chondrite fragment in the Henningsen Lotus Park just west of Coloma, CA on 24 April 2012. Later that day, Peter Jenniskens found a crushed 4 g meteorite in the parking lot of that same park and Brien Cook found a 5 g meteorite off Petersen Road in Lotus. These were the only meteorites found before rain hit the area on 25 April.\n\nOn 1 May 2012, the James W. Marshall Gold Discovery State Historic Park's Ranger Suzie Matin discovered two pieces of the meteorite (SM14 @ 11.5 grams) in her front yard. The park contains what is now known as Sutter's Mill (site). On 3 May 2012 scientists with the Ames Research Center and the SETI Institute utilized an airship to search the strewn field for the impact scars of kg-sized meteorites, but it is now understood that the high entry speed prevented the survival of meteorites as large as found in the Murchison meteorite fall. Ground-based searches resulted in the additional recovery of two pristinely collected meteorites (SM12 and SM67) for scientific study.\n\nThis is the third witnessed meteorite fall in California, following Red Canyon Lake on 11 August 2007 and San Juan Capistrano on 15 March 1973, while a few months after this event the Novato meteorite fell on 17 October 2012.\n\nA consortium of over 50 scientists investigated the circumstances of the impact and the properties of the meteorites. The event was recorded by two infrasound monitoring stations of the Comprehensive Nuclear-Test-Ban Treaty Organization’s International Monitoring System. The preliminary analysis are indicative of energy yield of approximately 4 kilotons of TNT equivalent. Hiroshima's \"Little Boy\" had a yield of about 16 kt. The air burst had approximate coordinates of .\n\nBased on the infrasound signal and the brightness of the fireball in photographs and two video records, the incoming meteoroid was estimated to have been 2–4 meters in diameter, between the size of a dish washer and a mini van. Before entry in Earth's atmosphere, the meteoroid probably had an absolute magnitude (H) of roughly 31. The meteoroid entered at a record speed of 28.6 ± 0.7 km/s, the fastest fireball on record from which meteorites were later recovered. It broke apart at an altitude of 48 km, the highest breakup event on record resulting in meteorites on the ground.\n\nBefore entry, the meteoroid moved on an eccentric orbit, stretching from just inside the orbit of Jupiter to the orbit of Mercury. The orbit had a shallow inclination and an orbital period suggesting that this meteoroid originated in the 3:1 mean motion resonance with Jupiter. The CM chondrite Maribo moved on a similar orbit, but rotated by 120 degrees in the direction of the line of apsides. The asteroid family that is the source region of CM chondrite-type meteorites is now thought to be located close to the 3:1 mean motion resonance, in low-inclined orbits, and may be the Eulalia asteroid family.\n\nThe meteorite type is similar to that of the 1969 Murchison meteorite in Australia. Unlike Muchison, Sutter's Mill shows clear brecciation: fragments of CM lithologies with different aqueous alteration and thermal processing histories are embedded in a fine grained CM matrix material. The Sutter's Mill meteorite originated from near the surface of its parent body. As a whole, Sutter's Mill is harder than Murchison.\n\nThe pre-rain collected meteorite SM2 was found to contain the mineral oldhamite (CaS), a mineral that reacts readily with water vapor. This xenolithic material may have come from enstatite chondrites impacting on the surface of the CM chondrite parent body in the past.\n\nThe Sutter's Mill meteorite is used to test sample collection and analysis procedures for NASA's OSIRIS-REx sample return mission.\n\n\n"}
{"id": "53009998", "url": "https://en.wikipedia.org/wiki?curid=53009998", "title": "Sweet chestnut of Ovacık village", "text": "Sweet chestnut of Ovacık village\n\nSweet chestnut of Ovacık village () is a very old chestnut tree in Izmir Province, western Turkey. It is a registered natural monument of the country.\n\nThe chestnut tree is located at Ovacık Highland close to Ovacık village in Bayındır district of Izmir Province. It is a sweet chestnut (\"Castanea sativa\"). The tree is high, has a circumference of at diameter. Its age is dated to be about 500 years old.\n\nThe tree was registered a natural monument on February 21, 1995. The protected area of the plant covers .\n"}
{"id": "32840848", "url": "https://en.wikipedia.org/wiki?curid=32840848", "title": "The Cloud (poem)", "text": "The Cloud (poem)\n\n\"The Cloud\" is a major 1820 poem written by Percy Bysshe Shelley. \"The Cloud\" was written during late 1819 or early 1820, and submitted for publication on 12 July 1820. The work was published in the 1820 collection \"Prometheus Unbound, A Lyrical Drama, in Four Acts, With Other Poems\" by Charles and James Ollier in London in August 1820. The work was proof-read by John Gisborne. There were multiple drafts of the poem. The poem consists of six stanzas in anapestic or antidactylus meter, a foot with two unaccented syllables followed by an accented syllable.\n\nThe cloud is a metaphor for the unending cycle of nature: \"I silently laugh at my own cenotaph/ ... I arise and unbuild it again.\" As with the wind and the leaves in \"Ode to the West Wind\", the skylark in \"To a Skylark\", and the plant in \"The Sensitive Plant\", Shelley endows the cloud with sentient traits that personify the forces of nature.\n\nIn \"The Cloud\", Shelley relies on the imagery of transformation or metamorphosis, a cycle of birth, death, and rebirth: \"I change, but I cannot die.\" Mutability or change is a fact of physical nature.\n\nLightning or electricity is the \"pilot\" or guide for the cloud. Lightning is attracted to the \"genii\" in the earth which results in lightning flashes. The genii symbolize the positive charge of the surface of the earth while the cloud possesses a negative charge.\n\nBritish scientist and poet Erasmus Darwin, the grandfather of Charles Darwin, had written about plant life and science in the poem collection \"The Botanic Garden\" (1791) and on \"spontaneous vitality\", that \"microscopic animals are said to remain dead for many days or weeks ... and quickly to recover life and motion\" when water and heat are added, in \"The Temple of Nature\" (1803). Percy Bysshe Shelley had cited Darwin in his Preface to the anonymously published novel \"Frankenstein; or, The Modern Prometheus\" (1818), explaining how the novel was written and its meaning. He argued that imparting life to a corpse \"as not of impossible occurrence\".\n\nThe cloud is a personification and a metaphor for the perpetual cycle of transformation and change in nature. All life and matter are interconnected and undergo unending change and metamorphosis.\n\nA review of the 1820 \"Prometheus Unbound\" collection in the September and October 1821 issues of \"The London Magazine\" noted the originality of \"The Cloud\": \"It is impossible to peruse them without admiring the peculiar property of the author's mind, which can doff in an instant the cumbersome garments of metaphysical speculations, and throw itself naked as it were into the arms of nature and humanity. The beautiful and singularly original poem of 'The Cloud' will evince proofs of our opinion, and show the extreme force and freshness with which the writer can impregnate his poetry.\"\n\nIn the October 1821 issue of \"Quarterly Review\", W.S. Walker argued that \"The Cloud\" is related to \"Prometheus Unbound\" in that they are both absurd and \"galimatias\".\n\nJohn Todhunter wrote in 1880 that \"The Cloud\" and \"To a Skylark\" were \"the two most popular of Shelley's lyrics\".\n\nIn 1889, Francis Thompson asserted that \"The Cloud\" was the \"most typically Shelleyan of all the poems\" because it contained \"the child's faculty of make-believe raised to the nth power\" and that \"He is still at play, save only that his play is such as manhood stops to watch, and his playthings are those which the gods give their children. The universe is his box of toys. He dabbles his fingers in the dayfall. He is gold-dusty with tumbling amidst the stars.\"\n\nOn 20 April 1919, a silent black and white movie was released in the US entitled \"The Cloud\" which was \"a visual poem featuring clouds and landscapes in accompaniment to the words of Shelley's poem 'The Cloud'.\" The film was directed by W.A. Van Scoy and produced by the Post Nature Pictures company.\n\n\n"}
{"id": "34154322", "url": "https://en.wikipedia.org/wiki?curid=34154322", "title": "Vector model of the atom", "text": "Vector model of the atom\n\nIn physics, in particular quantum mechanics, the vector model of the atom is a model of the atom in terms of angular momentum. It can be considered as the extension of the Rutherford-Bohr-Sommerfeld atom model to multi-electron atoms.\n\nThe model is a convenient representation of the angular momenta of the electrons in the atom. Angular momentum is always split into orbital L, spin S and total J:\nGiven that in quantum mechanics, angular momentum is quantized and there is an uncertainty relation for the components of each vector, the representation turns out to be quite simple (although the background mathematics is quite complex). Geometrically it is a discrete set of right-circular cones, without the circular base, in which the axes of all the cones are lined up onto a common axis, conventionally the z-axis for three-dimensional Cartesian coordinates. Following is the background to this construction.\n\nThe commutator implies that for each of L, S, and J, only one component of any angular momentum vector can be measured at any instant of time; at the same the other two are indeterminate. The commutator of any two angular momentum operators (corresponding to component directions) is non-zero. Following is a summary of the relevant mathematics in constructing the vector model.\n\nThe commutation relations are (using the Einstein summation convention):\n\nwhere \n\nThe magnitudes of L, S and J however \"can\" be measured at the same time, since the commutation of the square of an angular momentum operator (full resultant, not components) with any one component is zero, so simultaneous measurement of formula_3 with formula_4, formula_5 with formula_6 and formula_7 with formula_8 satisfy:\n\nThe magnitudes satisfy all of the following, in terms of operators and vector components:\n\nand quantum numbers:\n\nwhere \n\nwhich respectively take the values:\n\nThese mathematical facts suggest the continuum of all possible angular momenta for a corresponding specified quantum number:\n\n\nThe geometrical result is a cone of vectors, the vector starts at the apex of the cone and its tip reaches the circumference of the cone. It is convention to use the z-component for the measurable component of angular momentum, so the axis of the cone must be the z-axis, directed from the apex to the plane defined by the circular base of the cone, perpendicular to the plane. For different quantum numbers, the cones are different. So there are a \"discrete\" number of states the angular momenta can be, ruled by the above possible values for formula_14, \"s\", and \"j\". Using the previous set-up of the vector as part of a cone, each state must correspond to a cone. This is for increasing formula_14, \"s\", and \"j\", and decreasing formula_14, \"s\", and \"j\"> Negative quantum numbers correspond to cones reflected in the \"x\"-\"y\" plane. One of these states, for a quantum number equal to zero, clearly doesn't correspond to a cone, only a circle in the \"x\"-\"y\" plane.\n\nThe number of cones (including the degenerate planar circle) equals the multiplicity of states, formula_17.\n\nIt can be considered the extension of the Bohr model because Niels Bohr also proposed angular momentum was quantized according to:\nwhere \"m\" is an integer, produced correct results for the Hydrogen atom. Although the Bohr model doesn't apply to multi-electron atoms, it was the first successful quantization of angular momentum applied to the atom, preceding the vector model of the atom.\n\nFor one-electron atoms (i.e. hydrogen), there is only one set of cones for the orbiting electron. For multi-electron atoms, there are many states, due to the increasing number of electrons.\n\nThe angular momenta of all electrons in the atom add vectorially. Most atomic processes, both nuclear and chemical (electronic) – except in the absolutely stochastic process of radioactive decay - are determined by spin-pairing and coupling of angular momenta due to neighbouring nucleons and electrons. The term \"coupling\" in this context means the vector superposition of angular momenta, that is, magnitudes and directions are added. \n\nIn multi-electron atoms, the vector sum of two angular momenta is:\n\nfor the z-component, the projected values are:\n\nwhere\n\nand the magnitudes are:\n\nin which\n\nThis process may be repeated for a third electron, then the fourth etc. until the total angular momentum has been found.\n\nThe process of adding all angular momenta together is a laborious task, since the resultant momenta is not definite, the entire cones of precessing momenta about the z-axis must be incorporated into the calculation. This can be simplified by some developed approximations - such as the \"Russell-Saunders coupling\" scheme in L-S coupling, named after H. N. Russell and F. A. Saunders (1925).\n\n\n\n"}
{"id": "32571", "url": "https://en.wikipedia.org/wiki?curid=32571", "title": "Volcano", "text": "Volcano\n\nA volcano is a rupture in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.\n\nEarth's volcanoes occur because its crust is broken into 17 major, rigid tectonic plates that float on a hotter, softer layer in its mantle. Therefore, on Earth, volcanoes are generally found where tectonic plates are diverging or converging, and most are found underwater. For example, a mid-oceanic ridge, such as the Mid-Atlantic Ridge, has volcanoes caused by divergent tectonic plates whereas the Pacific Ring of Fire has volcanoes caused by convergent tectonic plates. Volcanoes can also form where there is stretching and thinning of the crust's plates, e.g., in the East African Rift and the Wells Gray-Clearwater volcanic field and Rio Grande Rift in North America. This type of volcanism falls under the umbrella of \"plate hypothesis\" volcanism. Volcanism away from plate boundaries has also been explained as mantle plumes. These so-called \"hotspots\", for example Hawaii, are postulated to arise from upwelling diapirs with magma from the core–mantle boundary, 3,000 km deep in the Earth. Volcanoes are usually not created where two tectonic plates slide past one another.\n\nErupting volcanoes can pose many hazards, not only in the immediate vicinity of the eruption. One such hazard is that volcanic ash can be a threat to aircraft, in particular those with jet engines where ash particles can be melted by the high operating temperature; the melted particles then adhere to the turbine blades and alter their shape, disrupting the operation of the turbine. Large eruptions can affect temperature as ash and droplets of sulfuric acid obscure the sun and cool the Earth's lower atmosphere (or troposphere); however, they also absorb heat radiated from the Earth, thereby warming the upper atmosphere (or stratosphere). Historically, volcanic winters have caused catastrophic famines.\n\nThe word \"volcano\" is derived from the name of Vulcano, a volcanic island in the Aeolian Islands of Italy whose name in turn comes from Vulcan, the god of fire in Roman mythology. The study of volcanoes is called volcanology, sometimes spelled \"vulcanology\".\n\nAt the mid-oceanic ridges, two tectonic plates diverge from one another as new oceanic crust is formed by the cooling and solidifying of hot molten rock. Because the crust is very thin at these ridges due to the pull of the tectonic plates, the release of pressure leads to adiabatic expansion (without transfer of heat or matter) and the partial melting of the mantle, causing volcanism and creating new oceanic crust. Most divergent plate boundaries are at the bottom of the oceans; therefore, most volcanic activity on the Earth is submarine, forming new seafloor. Black smokers (also known as deep sea vents) are evidence of this kind of volcanic activity. Where the mid-oceanic ridge is above sea-level, volcanic islands are formed; for example, Iceland.\n\nSubduction zones are places where two plates, usually an oceanic plate and a continental plate, collide. In this case, the oceanic plate subducts, or submerges, under the continental plate, forming a deep ocean trench just offshore. In a process called flux melting, water released from the subducting plate lowers the melting temperature of the overlying mantle wedge, thus creating magma. This magma tends to be extremely viscous because of its high silica content, so it often does not attain the surface but cools and solidifies at depth. When it does reach the surface, however, a volcano is formed. Typical examples are Mount Etna and the volcanoes in the Pacific Ring of Fire.\n\nHotspots are volcanic areas believed to be formed by mantle plumes, which are hypothesized to be columns of hot material rising from the core-mantle boundary in a fixed space that causes large-volume melting. Because tectonic plates move across them, each volcano becomes dormant and is eventually re-formed as the plate advances over the postulated plume. The Hawaiian Islands are said to have been formed in such a manner; so has the Snake River Plain, with the Yellowstone Caldera being the part of the North American plate above the hot spot. This theory, however, has been doubted.\n\nThe most common perception of a volcano is of a conical mountain, spewing lava and poisonous gases from a crater at its summit; however, this describes just one of the many types of volcano. The features of volcanoes are much more complicated and their structure and behavior depends on a number of factors. Some volcanoes have rugged peaks formed by lava domes rather than a summit crater while others have landscape features such as massive plateaus. Vents that issue volcanic material (including lava and ash) and gases (mainly steam and magmatic gases) can develop anywhere on the landform and may give rise to smaller cones such as Puu Ōō on a flank of Hawaii's Kīlauea.\nOther types of volcano include cryovolcanoes (or ice volcanoes), particularly on some moons of Jupiter, Saturn, and Neptune; and mud volcanoes, which are formations often not associated with known magmatic activity. Active mud volcanoes tend to involve temperatures much lower than those of igneous volcanoes except when the mud volcano is actually a vent of an igneous volcano.\n\nVolcanic fissure vents are flat, linear fractures through which lava emerges.\n\nShield volcanoes, so named for their broad, shield-like profiles, are formed by the eruption of low-viscosity lava that can flow a great distance from a vent. They generally do not explode catastrophically. Since low-viscosity magma is typically low in silica, shield volcanoes are more common in oceanic than continental settings. The Hawaiian volcanic chain is a series of shield cones, and they are common in Iceland, as well.\n\nLava domes are built by slow eruptions of highly viscous lava. They are sometimes formed within the crater of a previous volcanic eruption, as in the case of Mount Saint Helens, but can also form independently, as in the case of Lassen Peak. Like stratovolcanoes, they can produce violent, explosive eruptions, but their lava generally does not flow far from the originating vent.\n\nCryptodomes are formed when viscous lava is forced upward causing the surface to bulge. The 1980 eruption of Mount St. Helens was an example; lava beneath the surface of the mountain created an upward bulge which slid down the north side of the mountain.\n\nVolcanic cones or cinder cones result from eruptions of mostly small pieces of scoria and pyroclastics (both resemble cinders, hence the name of this volcano type) that build up around the vent. These can be relatively short-lived eruptions that produce a cone-shaped hill perhaps 30 to 400 meters high. Most cinder cones erupt only once. Cinder cones may form as flank vents on larger volcanoes, or occur on their own. Parícutin in Mexico and Sunset Crater in Arizona are examples of cinder cones. In New Mexico, Caja del Rio is a volcanic field of over 60 cinder cones.\n\nBased on satellite images it was suggested that cinder cones might occur on other terrestrial bodies in the Solar system too; on the surface of Mars and the Moon.\n\nStratovolcanoes or composite volcanoes are tall conical mountains composed of lava flows and other ejecta in alternate layers, the strata that gives rise to the name. Stratovolcanoes are also known as composite volcanoes because they are created from multiple structures during different kinds of eruptions. Strato/composite volcanoes are made of cinders, ash, and lava. Cinders and ash pile on top of each other, lava flows on top of the ash, where it cools and hardens, and then the process repeats. Classic examples include Mount Fuji in Japan, Mayon Volcano in the Philippines, and Mount Vesuvius and Stromboli in Italy.\n\nThroughout recorded history, ash produced by the explosive eruption of stratovolcanoes has posed the greatest volcanic hazard to civilizations. Not only do stratovolcanoes have greater pressure buildup from the underlying lava flow than shield volcanoes, but their fissure vents and monogenetic volcanic fields (volcanic cones) also have more powerful eruptions because they are often under extension. They are also steeper than shield volcanoes, with slopes of 30–35° compared to slopes of generally 5–10°, and their loose tephra are material for dangerous lahars. Large pieces of tephra are called volcanic bombs. Big bombs can measure more than 4 feet(1.2 meters) across and weigh several tons.\n\nA supervolcano usually has a large caldera and can produce devastation on an enormous, sometimes continental, scale. Such volcanoes are able to severely cool global temperatures for many years after the eruption due to the huge volumes of sulfur and ash released into the atmosphere. They are the most dangerous type of volcano. Examples include Yellowstone Caldera in Yellowstone National Park and Valles Caldera in New Mexico (both western United States); Lake Taupo in New Zealand; Lake Toba in Sumatra, Indonesia; and Ngorongoro Crater in Tanzania. Because of the enormous area they may cover, supervolcanoes are hard to identify centuries after an eruption. Similarly, large igneous provinces are also considered supervolcanoes because of the vast amount of basalt lava erupted (even though the lava flow is non-explosive).\n\nSubmarine volcanoes are common features of the ocean floor. In shallow water, active volcanoes disclose their presence by blasting steam and rocky debris high above the ocean's surface. In the ocean's deep, the tremendous weight of the water above prevents the explosive release of steam and gases; however, they can be detected by hydrophones and discoloration of water because of volcanic gases. Pillow lava is a common eruptive product of submarine volcanoes and is characterized by thick sequences of discontinuous pillow-shaped masses which form under water. Even large submarine eruptions may not disturb the ocean surface due to the rapid cooling effect and increased buoyancy of water (as compared to air) which often causes volcanic vents to form steep pillars on the ocean floor. Hydrothermal vents are common near these volcanoes, and some support peculiar ecosystems based on dissolved minerals. Over time, the formations created by submarine volcanoes may become so large that they break the ocean surface as new islands or floating pumice rafts.\n\nSubglacial volcanoes develop underneath icecaps. They are made up of flat lava which flows at the top of extensive pillow lavas and palagonite. When the icecap melts, the lava on top collapses, leaving a flat-topped mountain. These volcanoes are also called table mountains, tuyas, or (uncommonly) mobergs. Very good examples of this type of volcano can be seen in Iceland, however, there are also tuyas in British Columbia. The origin of the term comes from Tuya Butte, which is one of the several tuyas in the area of the Tuya River and Tuya Range in northern British Columbia. Tuya Butte was the first such landform analyzed and so its name has entered the geological literature for this kind of volcanic formation. The Tuya Mountains Provincial Park was recently established to protect this unusual landscape, which lies north of Tuya Lake and south of the Jennings River near the boundary with the Yukon Territory.\n\nMud volcanoes or mud domes are formations created by geo-excreted liquids and gases, although there are several processes which may cause such activity. The largest structures are 10 kilometers in diameter and reach 700 meters high.\n\nAnother way of classifying volcanoes is by the \"composition of material erupted\" (lava), since this affects the shape of the volcano. Lava can be broadly classified into four different compositions:\n\nTwo types of lava are named according to the surface texture: Aa (pronounced ) and pāhoehoe (), both Hawaiian words. Aa is characterized by a rough, clinkery surface and is the typical texture of viscous lava flows. However, even basaltic or mafic flows can be erupted as aa flows, particularly if the eruption rate is high and the slope is steep.\n\nPāhoehoe is characterized by its smooth and often ropey or wrinkly surface and is generally formed from more fluid lava flows. Usually, only mafic flows will erupt as pāhoehoe, since they often erupt at higher temperatures or have the proper chemical make-up to allow them to flow with greater fluidity.\n\nA popular way of classifying magmatic volcanoes is by their frequency of eruption, with those that erupt regularly called active, those that have erupted in historical times but are now quiet called dormant or inactive, and those that have not erupted in historical times called extinct. However, these popular classifications—extinct in particular—are practically meaningless to scientists. They use classifications which refer to a particular volcano's formative and eruptive processes and resulting shapes.\n\nThere is no consensus among volcanologists on how to define an \"active\" volcano. The lifespan of a volcano can vary from months to several million years, making such a distinction sometimes meaningless when compared to the lifespans of humans or even civilizations. For example, many of Earth's volcanoes have erupted dozens of times in the past few thousand years but are not currently showing signs of eruption. Given the long lifespan of such volcanoes, they are very active. By human lifespans, however, they are not.\n\nScientists usually consider a volcano to be \"erupting\" or \"likely to erupt\" if it is currently erupting, or showing signs of unrest such as unusual earthquake activity or significant new gas emissions. Most scientists consider a volcano \"active\" if it has erupted in the last 10,000 years (Holocene times) – the Smithsonian Global Volcanism Program uses this definition of \"active\". Most volcanoes are situated on the Pacific Ring of Fire. An estimated 500 million people live near active volcanoes.\n\n\"Historical time\" (or recorded history) is another timeframe for \"active\". The \"Catalogue of the Active Volcanoes of the World\", published by the International Association of Volcanology, uses this definition, by which there are more than 500 active volcanoes. However, the span of recorded history differs from region to region. In China and the Mediterranean, it reaches back nearly 3,000 years, but in the Pacific Northwest of the United States and Canada, it reaches back less than 300 years, and in Hawaii and New Zealand, only around 200 years.\n\nAs of 2013, the following are considered Earth's most active volcanoes:\n\n\n, the longest ongoing (but not necessarily continuous) volcanic eruptive phases are:\n\n\nOther very active volcanoes include:\n\nExtinct volcanoes are those that scientists consider unlikely to erupt again because the volcano no longer has a magma supply. Examples of extinct volcanoes are many volcanoes on the Hawaiian – Emperor seamount chain in the Pacific Ocean (although some volcanoes at the eastern end of the chain are active), Hohentwiel in Germany, Shiprock in New Mexico, and Zuidwal volcano in the Netherlands. Edinburgh Castle in Scotland is famously located atop an extinct volcano. Otherwise, whether a volcano is truly extinct is often difficult to determine. Since \"supervolcano\" calderas can have eruptive lifespans sometimes measured in millions of years, a caldera that has not produced an eruption in tens of thousands of years is likely to be considered dormant instead of extinct. Some volcanologists refer to extinct volcanoes as inactive, though the term is now more commonly used for dormant volcanoes once thought to be extinct.\n\nIt is difficult to distinguish an extinct volcano from a dormant (inactive) one. Dormant volcanoes are those that have not erupted for thousands of years, but are likely to erupt again in the future. Volcanoes are often considered to be extinct if there are no written records of its activity. Nevertheless, volcanoes may remain dormant for a long period of time. For example, Yellowstone has a repose/recharge period of around 700,000 years, and Toba of around 380,000 years. Vesuvius was described by Roman writers as having been covered with gardens and vineyards before its eruption of 79 CE, which destroyed the towns of Herculaneum and Pompeii. Before its catastrophic eruption of 1991, Pinatubo was an inconspicuous volcano, unknown to most people in the surrounding areas. Two other examples are the long-dormant Soufrière Hills volcano on the island of Montserrat, thought to be extinct before activity resumed in 1995, and Fourpeaked Mountain in Alaska, which, before its September 2006 eruption, had not erupted since before 8000 BCE and had long been thought to be extinct.\n\nThe three common popular classifications of volcanoes can be subjective and some volcanoes thought to have been extinct have erupted again. To help prevent people from falsely believing they are not at risk when living on or near a volcano, countries have adopted new classifications to describe the various levels and stages of volcanic activity. Some alert systems use different numbers or colors to designate the different stages. Other systems use colors and words. Some systems use a combination of both.\n\nThe United States Geological Survey (USGS) has adopted a common system nationwide for characterizing the level of unrest and eruptive activity at volcanoes. The new volcano alert-level system classifies volcanoes now as being in a normal, advisory, watch or warning stage. Additionally, colors are used to denote the amount of ash produced.\n\nThe Decade Volcanoes are 16 volcanoes identified by the International Association of Volcanology and Chemistry of the Earth's Interior (IAVCEI) as being worthy of particular study in light of their history of large, destructive eruptions and proximity to populated areas. They are named Decade Volcanoes because the project was initiated as part of the United Nations-sponsored International Decade for Natural Disaster Reduction (the 1990s). The 16 current Decade Volcanoes are\n\nThe Deep Earth Carbon Degassing Project, an initiative of the Deep Carbon Observatory, monitors nine volcanoes, two of which are Decade volcanoes. The focus of the Deep Earth Carbon Degassing Project is to use Multi-Component Gas Analyzer System instruments to measure CO/SO ratios in real-time and in high-resolution to allow detection of the pre-eruptive degassing of rising magmas, improving prediction of volcanic activity.\n\nThere are many different types of volcanic eruptions and associated activity: phreatic eruptions (steam-generated eruptions), explosive eruption of high-silica lava (e.g., rhyolite), effusive eruption of low-silica lava (e.g., basalt), pyroclastic flows, lahars (debris flow) and carbon dioxide emission. All of these activities can pose a hazard to humans. Earthquakes, hot springs, fumaroles, mud pots and geysers often accompany volcanic activity.\n\nThe concentrations of different volcanic gases can vary considerably from one volcano to the next. Water vapor is typically the most abundant volcanic gas, followed by carbon dioxide and sulfur dioxide. Other principal volcanic gases include hydrogen sulfide, hydrogen chloride, and hydrogen fluoride. A large number of minor and trace gases are also found in volcanic emissions, for example hydrogen, carbon monoxide, halocarbons, organic compounds, and volatile metal chlorides.\n\nLarge, explosive volcanic eruptions inject water vapor (HO), carbon dioxide (CO), sulfur dioxide (SO), hydrogen chloride (HCl), hydrogen fluoride (HF) and ash (pulverized rock and pumice) into the stratosphere to heights of 16–32 kilometres (10–20 mi) above the Earth's surface. The most significant impacts from these injections come from the conversion of sulfur dioxide to sulfuric acid (HSO), which condenses rapidly in the stratosphere to form fine sulfate aerosols. The SO emissions alone of two different eruptions are sufficient to compare their potential climatic impact. The aerosols increase the Earth's albedo—its reflection of radiation from the Sun back into space—and thus cool the Earth's lower atmosphere or troposphere; however, they also absorb heat radiated up from the Earth, thereby warming the stratosphere. Several eruptions during the past century have caused a decline in the average temperature at the Earth's surface of up to half a degree (Fahrenheit scale) for periods of one to three years; sulfur dioxide from the eruption of Huaynaputina probably caused the Russian famine of 1601–1603.\n\nA volcanic winter is thought to have taken place around 70,000 years ago after the supereruption of Lake Toba on Sumatra island in Indonesia. According to the Toba catastrophe theory to which some anthropologists and archeologists subscribe, it had global consequences, killing most humans then alive and creating a population bottleneck that affected the genetic inheritance of all humans today.\n\nIt has been suggested that volcanic activity caused or contributed to the End-Ordovician, Permian-Triassic, Late Devonian mass extinctions, and possibly others. The massive eruptive event which formed the Siberian Traps, one of the largest known volcanic events of the last 500 million years of Earth's geological history, continued for a million years and is considered to be the likely cause of the \"Great Dying\" about 250 million years ago, which is estimated to have killed 90% of species existing at the time.\n\nThe 1815 eruption of Mount Tambora created global climate anomalies that became known as the \"Year Without a Summer\" because of the effect on North American and European weather. Agricultural crops failed and livestock died in much of the Northern Hemisphere, resulting in one of the worst famines of the 19th century.\n\nThe freezing winter of 1740–41, which led to widespread famine in northern Europe, may also owe its origins to a volcanic eruption.\n\nSulfate aerosols promote complex chemical reactions on their surfaces that alter chlorine and nitrogen chemical species in the stratosphere. This effect, together with increased stratospheric chlorine levels from chlorofluorocarbon pollution, generates chlorine monoxide (ClO), which destroys ozone (O). As the aerosols grow and coagulate, they settle down into the upper troposphere where they serve as nuclei for cirrus clouds and further modify the Earth's radiation balance. Most of the hydrogen chloride (HCl) and hydrogen fluoride (HF) are dissolved in water droplets in the eruption cloud and quickly fall to the ground as acid rain. The injected ash also falls rapidly from the stratosphere; most of it is removed within several days to a few weeks. Finally, explosive volcanic eruptions release the greenhouse gas carbon dioxide and thus provide a deep source of carbon for biogeochemical cycles.\n\nGas emissions from volcanoes are a natural contributor to acid rain. Volcanic activity releases about 130 to 230 teragrams (145 million to 255 million short tons) of carbon dioxide each year. Volcanic eruptions may inject aerosols into the Earth's atmosphere. Large injections may cause visual effects such as unusually colorful sunsets and affect global climate mainly by cooling it. Volcanic eruptions also provide the benefit of adding nutrients to soil through the weathering process of volcanic rocks. These fertile soils assist the growth of plants and various crops. Volcanic eruptions can also create new islands, as the magma cools and solidifies upon contact with the water.\n\nAsh thrown into the air by eruptions can present a hazard to aircraft, especially jet aircraft where the particles can be melted by the high operating temperature; the melted particles then adhere to the turbine blades and alter their shape, disrupting the operation of the turbine. Dangerous encounters in 1982 after the eruption of Galunggung in Indonesia, and 1989 after the eruption of Mount Redoubt in Alaska raised awareness of this phenomenon. Nine Volcanic Ash Advisory Centers were established by the International Civil Aviation Organization to monitor ash clouds and advise pilots accordingly. The 2010 eruptions of Eyjafjallajökull caused major disruptions to air travel in Europe.\n\nThe Earth's Moon has no large volcanoes and no current volcanic activity, although recent evidence suggests it may still possess a partially molten core. However, the Moon does have many volcanic features such as maria (the darker patches seen on the moon), rilles and domes.\n\nThe planet Venus has a surface that is 90% basalt, indicating that volcanism played a major role in shaping its surface. The planet may have had a major global resurfacing event about 500 million years ago, from what scientists can tell from the density of impact craters on the surface. Lava flows are widespread and forms of volcanism not present on Earth occur as well. Changes in the planet's atmosphere and observations of lightning have been attributed to ongoing volcanic eruptions, although there is no confirmation of whether or not Venus is still volcanically active. However, radar sounding by the Magellan probe revealed evidence for comparatively recent volcanic activity at Venus's highest volcano Maat Mons, in the form of ash flows near the summit and on the northern flank.\nThere are several extinct volcanoes on Mars, four of which are vast shield volcanoes far bigger than any on Earth. They include Arsia Mons, Ascraeus Mons, Hecates Tholus, Olympus Mons, and Pavonis Mons. These volcanoes have been extinct for many millions of years, but the European \"Mars Express\" spacecraft has found evidence that volcanic activity may have occurred on Mars in the recent past as well.\n\nJupiter's moon Io is the most volcanically active object in the solar system because of tidal interaction with Jupiter. It is covered with volcanoes that erupt sulfur, sulfur dioxide and silicate rock, and as a result, Io is constantly being resurfaced. Its lavas are the hottest known anywhere in the solar system, with temperatures exceeding 1,800 K (1,500 °C). In February 2001, the largest recorded volcanic eruptions in the solar system occurred on Io. Europa, the smallest of Jupiter's Galilean moons, also appears to have an active volcanic system, except that its volcanic activity is entirely in the form of water, which freezes into ice on the frigid surface. This process is known as cryovolcanism, and is apparently most common on the moons of the outer planets of the solar system.\n\nIn 1989 the Voyager 2 spacecraft observed cryovolcanoes (ice volcanoes) on Triton, a moon of Neptune, and in 2005 the Cassini–Huygens probe photographed fountains of frozen particles erupting from Enceladus, a moon of Saturn. The ejecta may be composed of water, liquid nitrogen, ammonia, dust, or methane compounds. Cassini–Huygens also found evidence of a methane-spewing cryovolcano on the Saturnian moon Titan, which is believed to be a significant source of the methane found in its atmosphere. It is theorized that cryovolcanism may also be present on the Kuiper Belt Object Quaoar.\n\nA 2010 study of the exoplanet COROT-7b, which was detected by transit in 2009, suggested that tidal heating from the host star very close to the planet and neighboring planets could generate intense volcanic activity similar to that found on Io.\n\nMany ancient accounts ascribe volcanic eruptions to supernatural causes, such as the actions of gods or demigods. To the ancient Greeks, volcanoes' capricious power could only be explained as acts of the gods, while 16th/17th-century German astronomer Johannes Kepler believed they were ducts for the Earth's tears. One early idea counter to this was proposed by Jesuit Athanasius Kircher (1602–1680), who witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth with a central fire connected to numerous others caused by the burning of sulfur, bitumen and coal.\n\nVarious explanations were proposed for volcano behavior before the modern understanding of the Earth's mantle structure as a semisolid material was developed. For decades after awareness that compression and radioactive materials may be heat sources, their contributions were specifically discounted. Volcanic action was often attributed to chemical reactions and a thin layer of molten rock near the surface.\n\n\n"}
{"id": "26746182", "url": "https://en.wikipedia.org/wiki?curid=26746182", "title": "West African Power Pool", "text": "West African Power Pool\n\nThe West African Power Pool (WAPP) is a cooperation of the national electricity companies in Western Africa under the auspecies of the Economic Community of West African States (ECOWAS). The members of WAPP are working for establishing a reliable power grid for the region and a common market for electricity. It was founded in 2000.\n\n"}
{"id": "27506751", "url": "https://en.wikipedia.org/wiki?curid=27506751", "title": "West Rapti River", "text": "West Rapti River\n\nWest Rapti drains Rapti Zone in Mid-Western Region, Nepal, then Awadh and Purvanchal regions of Uttar Pradesh state, India before joining the Ghaghara -- a major left bank tributary of the Ganges known as the Karnali inside Nepal.\n\nThe West Rapti is notable for \"janajati\" ethnic groups – Kham Magar among its highland sources and then Tharu in Inner Terai Deukhuri Valley, for its irrigation and hydroelectric potential, and for recurrent floods that led to its nickname \"Gorakhpur's Sorrow\".\n\nThe Rapti rises south of a prominent E-W ridgeline midway between the western Dhaulagiri Himalaya and the Mahabharat Range. A summit on this ridgeline marks a triple divide. North of the triple divide the Karnali and Gandaki basins are adjacent; south of it the Rapti and similar but smaller \"Babai River\" (; ) separate the two larger basins. After crossing into India, the Babai and Rapti separately join the Karnali's continuation called \"Ghaghara\". The Ghaghara ultimately joins the Ganges.\n\nThe Rapti's headwaters descend south from rugged highlands populated by Kham Magar. The western tributary \"Mādī Kholā\" () rises in northwestern Rolpa and is joined by \"Lungrī Kholā\" () draining northeastern Rolpa. The Mardi then crosses into Pyuthan. It is joined by east-flowing Arun Kholā () at Devithān () where it enters a gorge through the Mahabharat Range.\n\n\"Jhimruk Kholā\" () -- east of the Mardi—mainly drains Pyuthan. Below the upper highlands, an alluvial valley opens where Bahun and Chhetri rice farmers irrigate paddy fields. At \"Cherneta\", Pyuthan the Jhimruk approaches within 1.5 km of the Mardi and a 12 megawatt hydroelectric plant exploits the Jhimruk being 200 meters higher.\n\nBelow Cherneta the Jhimruk loops east, becoming the border between Pyuthan and Arghakhanchi District. Its valley narrows and steepens as it enters the Mahabharat Range. Partway through it joins the Mardi and the combined flow is then named the Rapti. The main river emerges from its gorge into the lower Siwalik Hills and Dang District. At Bhalubang Bazaar Nepal's east-west Mahendra Highway bridges the river.\n\nBelow Bhalubang, Inner Terai \"Deukhuri Valley\" opens between the \"Dang\" and \"Dudhwa Ranges\", both subranges of the Siwaliks. Valley, following the WNW trend of the Siwalik hills for 100 km. Although the land is fertile, before DDT came into use in the 1950s Deukhuri was so malarial that only the Tharu people who had genetic resistance could be confident of surviving the warmer months.\n\nThe river crosses from Dang into Banke District. Approaching Nepalganj—largest town in Nepal's western Terai—the Dudhwa Hills fall away and the river turns SE, crossing into Uttar Pradesh, India and flowing through districts Shravasti, Siddharth Nagar, Basti, Sant Kabir Nagar and Gorakhpur, passing Gorakhpur city at about 135 air miles (215 km) from Nepal.\n\nJust west of the city it is joined by the smaller Rohini rising further east in Nepal's Nawalparasi and Rupandehi Districts, draining 794 km in Nepal then 1892 km in India. 60 km beyond Gorakhpur the Rapti joins the Ghaghara (Karnali) at Rajpur. About 120 km further on at Chhapra, the Ghaghara reaches the Ganges.\n\nAciravati, Achirvati or Airavati is the ancient name for a river has been identified with the modern Rapti, flowing through what is now Nepal and the northern portion of Uttar Pradesh. The Chinese pilgrim Xuanzang knew it as A-chi-lo. Jain texts mention it as Eravai.\n\nThe ancient city of Sravasti, once capital of Kosala Kingdom, stood on the western bank of the Achirvati. The river was a tributary of the Sarayu. It was one of the five great rivers that constituted the Ganges group of rivers and one of the sacred rivers of the Buddhist midland.\n\nThe Rapti's flow has great seasonal variation because the river lacks sources in high elevation glaciers and snowfields to buffer pre-monsoon drought. Average monthly flows at \"Jalkundi\" (27°58'N, 82°14'E) in Deukhuri Valley vary from 17.6 m/s in pre-monsoon April to 451 m/s at the peak of the monsoon in August. Maximum recorded flood was 7,390 m/s on 10 September 1981. 100-year flood flows are predicted at 10,100 m/s. Over in Uttar Pradesh are at risk of floods every year.\n\nFlood control projects under study include a dam at Jalkundi that would inundate of farmland in Deukhuri Valley. An alternative dam site is upstream at \"Naumure\" on the Pyuthan-Dang district border (27°53'N, 82°48'E). This would be an earthen dam 169 m high with 351 million cubic meters live storage capacity, storing excess monsoon flows for irrigation use during the following dry season and generating up to 207 megawatts. Impoundment would mainly be in gorges through the Mahabharat Range, inundating less farmland than the Jalkundi alternative. Plans are also underway for three irrigation sub-projects – Kapilvastu District involving interbasin water transfer to the southeast, Deukhuri Valley , and Banke District .\n"}
{"id": "6569214", "url": "https://en.wikipedia.org/wiki?curid=6569214", "title": "Winkler index", "text": "Winkler index\n\nThe Winkler Index, sometimes known as the Winkler Scale or Winkler Regions, is a technique for classifying the climate of wine growing regions based on heat summation or growing degree-days. In the system, geographical areas are divided into five climate regions based on temperature converted to growing degree-days, and is commonly known as Regions I–V (see below). The system was developed at the University of California, Davis by A. J. Winkler and Maynard Amerine.\n\nThe system is based on both the hypothesis and observations that grapevines do not grow if the temperature is below 50 °F (10 °C). Each day during the growing season (assumed under the system to be April 1 through October 31 in the Northern Hemisphere; October 1 through April 30 in the Southern Hemisphere) are assigned growing degree-days according to the amount that the day's average temperature exceeds this threshold; one degree day per degree Fahrenheit over 50 °F (or with SI units, degrees Celsius over 10 °C is used). All days during the growing season are then added up (all negative values are set to zero), with the sum of the growing degree-days used to determine the region's classification in the original Winkler index as follows:\nThe system was originally developed for and is used officially in California and was based on the general ripening capabilities and wine styles that can be achieved in the climate due to heat accumulation (growing degree-days). The general ripening capabilities include hybrid grape varieties through early season, mid-season, and late season ripening \"V. Vinifera\" and even table grapes in the warmest areas of Region V. The general wine styles include lighter, more subtle wines with lower alcohol and brighter fruit aromas and flavors (including Champagne and other sparkling wines) found in cooler climates (Regions Ia, Ib, II and lower III) to bolder, bigger wines often with higher alcohol and lush, darker fruit aromas and flavors that are found in warmer climates (Region III, IV and V). Region V was stated as also having a tendency to be more suitable to higher production wines, Sherry and other fortified wines.\n\nOne issue with the original work done by Amerine and Winkler was that it did not specify a lower class limit for Region I (originally 2500 or less) or an upper class limit for Region V (originally 4000 or greater). Subsequent research using high resolution spatial climate data identified these limits for California, Oregon, Washington and Idaho, along with Australia. The results provided a lower bound to Region I of 1500 F° units (850 C° units) and an upper bound to Region V of 4900 F° units (2700 C° units). Furthermore, additional research in other wine regions found that Region I was best divided into a Region Ia (very early ripening varieties, mostly hybrid grapes) and Region Ib (early ripening varieties, mostly \"V. Vinifera\").\n\nThe Winkler Index is also widely used in many other growing regions in the United States, such as Oregon and Washington, along with Canada, South America, Australia, New Zealand, South Africa, and Europe. However, it is less widely used in Europe where the Huglin index is favored. The Huglin index uses a similar formula but gives more weight to maximum temperatures and uses an adjustment for longer day lengths found at higher latitudes. It is also functionally similar to growing season average temperatures (simple average of temperatures across the seven month growing season).\n\nThe table below provides examples of the ripening and wine style concept used in the application of the Winkler Index for numerous wine regions globally. Region Ia are the coolest areas with known regions including Champagne, Central Otago, and Valais. Region Ia also includes numerous newer regions growing grapes and making wine including southern England, areas in northern Europe, Nova Scotia, and southern areas of Chile and Argentina. Region Ia areas ripen a range of hybrid grapes and some very early ripening \"V. Vinifera.\" Region Ib is slightly warmer, can ripen early varieties such as Chardonnay, Pinot noir, Sauvignon blanc or Riesling with characteristic locations within the Rhine and Mosel valleys, Burgundy and the Loire Valley, or the Willamette Valley in Oregon as good examples. Region II includes cooler locations within areas such as Bordeaux, Coonawarra, and Valle de Curicó in Chile. Warmer areas in these wine regions fall in a Winkler Region III as do much of the Northern Rhône, Rioja, Umbria, and the Margaret River. Region IV includes portions of the Napa Valley, Stellenbosch, Corsica, Tuscany, and Alentejo where the warmer climates allow for the ripening of later varieties such as Cabernet sauvignon, Sangiovese, and Syrah. The warmest areas are found in Region V and include areas in the central valley of California, inland Australia and wine producing regions in Morocco, Madeira, Apulia, and Jerez. \nThere are numerous issues and limitations associated with the use of growing degree-days. First, the Winkler index and its classification of climate regions by growing degree-days only describe one aspect of an area's climate—mean daily temperature. Many other important factors which contribute to a region's suitability for viticulture (and its terroir) are excluded; among them sun exposure, latitude, precipitation, soil conditions, and the risk of extreme weather which might damage grapevines (e.g., winter freezes, spring and fall frosts, hail, etc.). As originally developed the climates of California were defined for relatively large areas using only one or two climate stations. This macroscale approach will invariably not capture the microscale influences that are an important aspect of growing any crop. To address these issues research has been increasingly using spatial climate data to better depict within region and even within vineyard differences in climate and therefore ripening and wine style potential. To create spatially appropriate climate data numerous stations and/or sensors are used to collect data which can then be interpolated over the landscape due to known interactions with elevation, aspect, slope, and distance to the coast or other water bodies using Geographic Information Systems (GIS). Instead of depicting a region as all one Winkler region (Napa Valley AVA being a Region III for example), spatial data summaries show the Napa Valley having a full range of Winkler regions, 12% a Region II, 56% a Region III, and 30% a Region IV (whereas the table above shows one station in Napa, St. Helena as being a Region IV).\n\nOther significant differences exist depending on the time period of the data and formula used for calculating growing degree-days. First, to be comparable growing degree-day numbers from various sources need to come from the same time period. Due to both a variable climate and climate change, a comparison of a ten-year period from the 1970s and the 2000s would be inappropriate as the variation and trends over time would make them incomparable. Furthermore, a sufficient time period is suggested to allow the averaging to smooth out some of the variability. The standard time period in use is the climatological normal period of 30 years, however if 30 years of data is not available then at the minimum five years should be used. However a five-year period is not directly comparable to a 30-year period. How data are averaged (i.e., hourly, daily, or monthly) is also very important. While weather stations today can average data to an hour, a minutes or even seconds, historical data used to calculate growing degree-days has been done mostly on daily or monthly averages (the table above was done using monthly climatological normals). Shorter term averaging to minutes, or more commonly hourly, arguably better reflects the true thermal effects on the crops, but will result in growing degree-day values that are lower than both daily and monthly. Monthly averaged data can be very problematic as it can underestimate heat accumulation during the first and last months of the growing season. Therefore, it is paramount that one know the time period that the growing degree-day values are calculated from so as to be comparable.\n\nThe Winkler index uses the standard method of calculating growing degree-days in viticulture and is based on using a base temperature of 50 °F (10 °C) with no upper temperature cut-off. The first issue is that 50 °F (10 °C) is not likely the best base temperature even though it is the most commonly used value. Even the early research on this topicstressed that the base temperature threshold for accumulation for early versus late budding varieties is likely strongly cultivar specific. Various research worldwide has pointed to base temperatures ranging from 39 to 45 °F (4 to 7 °C), but there has been little confirmation of these thresholds across numerous wine regions and for a wider range of varieties. At the other end of the formula, the calculation for growing degree-days used in viticulture and wine production does not normally use an upper cut-off. Conceptually an upper cut-off would be applied if the plant system stopped being photosynthetically active at some point due to heat stress from high temperatures. While this may be proven for some crops, there is not a universal number for an upper threshold for grapes so the majority of the published data for comparison purposes in viticulture and wine production does not limit maximum temperatures. This issue is problematic because many weather stations today have integrated the corn growing degree-day method in their software. The corn growing degree-day method uses both a base temperature adjustment and an upper threshold, neither of which are common in viticulture and wine production use, and can confound any comparison with published data using the simple average method.\n\nFurthermore, more complex climate indices have been introduced to address perceived shortcomings in the Winkler index including the Huglin Index, the Biologically Effective Degree-Day Index, and the Multicriteria Climatic Classification system (Geoviticulture MCC). These indices attempt to account for day length and solar, frost, and drought variability that can be found in different locations. Each have been used in various research settings, but have some limitations to the general user in that some variables needed to calculate the indices are not readily available from all weather/climate stations and/or to the general public.\n\nOverall each of these issues needs to be carefully considered when comparing growing degree-day values from published data in magazines, books, scientific articles, and even from growers in the same region.\n\n\n"}
