{"id": "1362251", "url": "https://en.wikipedia.org/wiki?curid=1362251", "title": "Airshed", "text": "Airshed\n\nAn airshed is a part of the atmosphere that behaves in a coherent way with respect to the dispersion of emissions. It typically forms an analytical or management unit. Also: a geographic boundary for air-quality standards.\n\nAlternatively - an airshed is a geographical area where local topography and meteorology limit the dispersion of pollutants away from the area.\n\n"}
{"id": "12437208", "url": "https://en.wikipedia.org/wiki?curid=12437208", "title": "Alan Weisman", "text": "Alan Weisman\n\nAlan H. Weisman (born March 24, 1947) is an American author, professor, and journalist.\n\nWeisman was born on March 24, 1947, in Minneapolis, Minnesota. Up until 8th grade he loved science but his science teacher hated him, and this English teacher loved him. So he changed to become a journalist. He holds a bachelor's degree in literature and a master's degree in journalism from Northwestern University. From 2004 to 2013 he was Laureate Professor in Journalism and Latin American Studies at the University of Arizona, where he led an annual field program in international field journalism. He has also taught writing and journalism at Prescott College and Williams College and has been a Fulbright Scholar in Colombia.\n\nHe has written several books and won numerous international awards for his work in journalism and literature. Recent works include the critically acclaimed \"The World Without Us\", which describes a post-human scenario of the planet and \"Countdown: Our Last, Best Hope for a Future on Earth?\", which has been short listed for a Los Angeles Times science and technology book prize. \nAmong his other works are \"Gaviotas: A Village to Reinvent the World\" (1998), winner of the Social Inventions Award from the Global Ideas Bank, \"An Echo In My Blood\" (1999), \"La Frontera: The United States Border With Mexico\", and \"We, Immortals\" (1979). His reports from around the world have appeared in \"Harper's\", \"The New York Times Magazine\", the \"Los Angeles Times\" magazine, the \"Atlantic Monthly\", \"Orion\", \"Audubon\", \"Mother Jones\", \"Discover\", \"Condé Nast Traveler\", \"Resurgence\", and several anthologies, including \"The Best American Science Writing 2006\".\n\nWeisman is also a producer of radio documentaries for Homelands Productions, creating reports for National Public Radio and American Public Media.\n\nAmong his awards and citations are the Four Corners Award for Best Nonfiction Book, The Wenjin Prize from the National Library of China, a Los Angeles Press Club Award for Best Feature Story, a Best of the West Award in Journalism, a Robert F. Kennedy Journalism Citation, the Unity Media Award, the Brazilian government's Premio Nacional de Jornalismo Radiofonico, and major grants from The Ford Foundation, the Corporation for Public Broadcasting, the Rockefeller Foundation and the MacArthur Foundation. His book \"The World Without Us\" was a finalist for the National Book Critics' Circle Award, the J. Anthony Lukas Book Prize, the Orion Prize, and the Rachel Carson Environment Book Award. \"Countdown\" won the 2013 Paris Book Festival Prize for nonfiction, the 2014 Los Angeles Times Book Prize, the 2014 Nautilus Gold Book Award, the Population Institute’s 2014 Global Media Award for best book, and was a finalist for the Orion Prize and the Books for a Better Life Award.\n\nWeisman appeared as a guest on \"The Daily Show\" on August 21, 2007, and on \"Real Time with Bill Maher\" on March 14, 2014.\n\n\nThough many call him a misanthrope, Weisman asserts that he is not. He believes \"birds aren't the only thing that can sing on this planet\".\nWeisman lives in Massachusetts with his wife, Beckie Kravetz, who is a sculptor and mask maker.\n\n\n"}
{"id": "3196275", "url": "https://en.wikipedia.org/wiki?curid=3196275", "title": "Alfred Thomas Agate", "text": "Alfred Thomas Agate\n\nAlfred Thomas Agate (February 14, 1812 in Sparta, New York – January 5, 1846 in Washington, D.C.) was a noted American artist, painter and miniaturist.\n\nAgate lived in New York from 1831 to 1838. He studied with his brother, Frederick Styles Agate, a portrait and historical painter. He later went on to study with Thomas Seir Cummings. By the late 1830s, Agate was exhibiting his work at the National Academy of Design in New York, and established himself as a skilled painter in oils. He was elected into the National Academy of Design as an honorary member in 1840.\n\nAgate drew landscapes, portraits, and scientific illustrations. For much of his landscapes, Agate used a camera lucida, a device which projected the scene onto a piece of paper for purposes of tracing.\n\nAgate created many artworks during his service with the United States Exploring Expedition of 1838–1842 under Charles Wilkes. He was especially good at botanical illustrations, and was the designated portrait and botanical artist of the expedition.\n\nThe United States Exploring Expedition passed through the Ellice Islands and visited Funafuti, Nukufetau and Vaitupu in 1841. During the visit of the expedition to the Ellice Islands (now known as Tuvalu) Alfred Thomas Agate recorded the dress and tattoo patterns of men of Nukufetau.\n\nAgate created the first known picture of Mount Shasta. Agate contributed more than half (173 of 342) of the sketches and paintings reproduced as lithographs illustrating the five volumes of the expedition's reports. He sketched the Oregon Territory, including a look into a \"Chinook Lodge\", an \"Indian Burial Place\", an \"Indian Mode of Rocking Cradle\", and a picture of the wreck of one of the expedition's sailing ships at the mouth of the Columbia River.\n\nAgate lived in Washington, D.C. from 1842 onward, but his health suffered severely from the expedition and he died four years later of consumption.\n\nOn Agate's death in 1846, the drawings passed to his widow, Elizabeth Hill Kennedy Agate, who later married Dr. William J. C. Du Hamel of Washington, D.C. In 1926, one of her daughters from this marriage, Elizabeth A. Du Hamel, sold them to the Naval Historical Foundation. The Naval Historical Foundation donated Agate's artwork to the Navy Art Collection in 1998.\n\nIn 1841, Agate Passage near Bainbridge Island, Washington was named by Lt. Charles Wilkes in honor of Agate. Agate Island in Fiji was also named in honor of Agate. Botanist Asa Gray used Agate's drawings and the expedition's specimens for botanical reports, and named a violet, \"Agatea violaris\", after him.\n\n\n"}
{"id": "56589031", "url": "https://en.wikipedia.org/wiki?curid=56589031", "title": "Atmospheric Science Letters", "text": "Atmospheric Science Letters\n\nAtmospheric Science Letters is a monthly peer-reviewed open access scientific journal covering the atmospheric sciences. It was established in 2000 and is published by John Wiley & Sons on behalf of the Royal Meteorological Society, of which it is the official journal. The editor-in-chief is Ian N. James. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.504, ranking it 56th out of 85 journals in the category \"Meteorology & Atmospheric Sciences\".\n"}
{"id": "5441706", "url": "https://en.wikipedia.org/wiki?curid=5441706", "title": "Big Pocono State Park", "text": "Big Pocono State Park\n\nBig Pocono State Park is a Pennsylvania state park in Jackson and Pocono townships in Monroe County, Pennsylvania in northeastern Pennsylvania in the United States. The park is on Camelback Mountain and is maintained jointly by the Pennsylvania Department of Conservation and Natural Resources and Camelback Ski Corporation.\n\nFrom the summit of the mountain, one has a view of vast portions of eastern Pennsylvania as well as parts of New Jersey and New York. A paved drive, in length, around the summit provides visitors with a view in all directions. Visitors can see the Delaware Water Gap from this location.\n\nThe park, except for the Camelback Ski Area, closes for the winter seven days following the end of deer season in December and reopens as conditions permit in the spring.\n\nThe land on which Big Pocono State Park is situated was owned by Henry S. Cattell at the turn of the 20th century. Knowing of the fondness that others in the area shared with him for the view that the summit provided, Cattell constructed a stone cabin there in 1908. For many years the cabin was left unlocked so that anyone who wished to use it as a shelter, could.\n\nThe Pennsylvania Game Commission purchased the land in 1928, 12 years following the death of Cattell. In 1950, the state leased a portion of the land on the north slope for commercial development. The company was later renamed the Camelback Ski Corporation, and the facility is now a major ski resort.\n\nIn 1953, the Pennsylvania Department of Forests and Waters (now the Department of Conservation and Natural Resources) acquired of state game lands, which comprised the land that was purchased by the Game Commission, including the ski area lease, and the area immediately around the summit.\n\nBig Pocono State Park opened to the public in 1954, following the construction of facilities and the scenic drive around the summit. The Cattell Cabin served as a park office and a nature museum for many years.\n\nAlthough the museum no longer exists, the stone cabin still stands and is paramount only to the great views one can obtain by going to the different observation points provided by both DCNR and the Camelback Ski Corporation.\n\nBig Pocono State Park offers a hiking trail system that consists of of interconnecting trails. The trails are of varying terrain and grade, and park officials caution inexperienced hikers to inquire at the park office before attempting the steep and rocky North Trail.\n\nIn addition to the hiking trails, there is also a horseback riding trail (). The trailhead is near the entrance of the park, on the west end.\n\nThree picnic areas with a total of 50 tables are situated within the park. The picnic area adjacent to Cattell Cabin has tables in an open setting, and the east and south picnic areas are in a wooded setting.\n\nThe Camelback Ski Area, located on the north slope, boasts the largest skiing, snowboarding and snowtubing facility in the Pocono Mountains. The facility has 33 trails, 13 lifts, two terrain parks, detachable quads, a halfpipe, night skiing and 100% snowmaking. The ski area also operates the Cameltop restaurant, which is located near the summit and is open from Memorial Day to Columbus Day. In the summertime, a waterpark named Camelbeach is operated on the grounds of Camelback, and this can be seen from Big Pocono.\n\nHunting is permitted in designated areas of the park during hunting seasons.\n\nThe following state parks are within of Big Pocono State Park:\n"}
{"id": "44091768", "url": "https://en.wikipedia.org/wiki?curid=44091768", "title": "Bongsanglay Natural Park", "text": "Bongsanglay Natural Park\n\nBongsanglay Natural Park (also spelled \"Bongsalay\" and \"Bujong Sanglay\") is a protected area of mangrove forests and swamps on Ticao Island in the Bicol Region of the Philippines. It is located in the municipality of Batuan in the island province of Masbate covering an area of . The protected area was established on 29 December 1981 when the area \"from Panciscan Point in Bitos Bay up to Bano Sanlay\" in Batuan was declared a Mangrove Swamp Forest Reserve under Proclamation No. 2152 signed by President Ferdinand Marcos. In 2000, when President Joseph Estrada signed Proclamation No. 319, Bongsanglay was reclassified as a natural park pursuant to the National Integrated Protected Areas System (NIPAS) Act.\n\nBongsanglay Natural Park is the only remaining primary growth mangrove forest in the Bicol Region as all the other regional mangroves have been replanted. It is located in the barangay of Royroy near the southern tip of Ticao Island by the Biton Bay facing the Ticao Pass and Samar Sea marine corridor. A small fishing community can be found not far from the mangroves. There is also a 1,000-meter canopy walk that traverses the park for patrolling and for ecotourism purposes.\n\nThe park hosts 36 species of mangroves dominated by the \"Rhizophora spp.\" and which also include \"Avicennia\" species of which the oldest is more than 125 years old. It also supports 68 avifauna species, including mangrove heron, Pacific reef heron, little egret, wandering whistling duck, white-collared kingfisher, Pacific swallow, common emerald dove, zebra dove, amethyst brown dove, common cuckoo, river kingfisher, pied triller, grey wagtail, grey-tailed tattler and the endemic Philippine duck.\nIn addition, 3 species of amphibians and 11 species of reptiles have also been observed in the park.\n"}
{"id": "54564418", "url": "https://en.wikipedia.org/wiki?curid=54564418", "title": "Chandrasekhar's H-function", "text": "Chandrasekhar's H-function\n\nIn atmospheric radiation, Chandrasekhar's \"H\"-function appears as the solutions of problems involving scattering, introduced by the Indian American astrophysicist Subrahmanyan Chandrasekhar. The Chandrasekhar's \"H\"-function formula_1 defined in the interval formula_2, satisfies the following nonlinear integral equation\n\nwhere the characteristic function formula_4 is an even polynomial in formula_5 satisfying the following condition\n\nIf the equality is satisfied in the above condition, it is called \"conservative case\", otherwise \"non-conservative\". Albedo is given by formula_7. An alternate form which would be more useful in calculating the \"H\" function numerically by iteration was derived by Chandrasekhar as,\n\nIn conservative case, the above equation reduces to \n\nThe \"H\" function can be approximated up to an order formula_10 as\n\nwhere formula_12 are the zeros of Legendre polynomials formula_13 and formula_14 are the positive, non vanishing roots of the associated characteristic equation\n\nwhere formula_16 are the quadrature weights given by\n\nIn complex variable formula_18 the \"H\" equation is \n\nthen for formula_20, a unique solution is given by\n\nwhere the imaginary part of the function formula_22 can vanish iff formula_23 is real i.e., formula_24. Then we have\n\nThe above solution is unique and bounded in the interval formula_26 for conservative cases. In non-conservative cases, if the equation formula_27 admits the roots formula_28, then there is a further solution given by\n\nand\n\n\n"}
{"id": "7595969", "url": "https://en.wikipedia.org/wiki?curid=7595969", "title": "Cultural depictions of lions", "text": "Cultural depictions of lions\n\nCultural depictions of lions are known in European, African and Asian countries. The lion has been an important symbol to humans for tens of thousands of years. The earliest graphic representations feature lions as organized hunters with great strength, strategies, and skills. In later depictions of human cultural ceremonies, lions were often used symbolically and may have played significant roles in magic, as deities or close association with deities, and served as intermediaries and clan identities.\n\nThe earliest historical records in Egypt present an established religious pantheon that included a lioness as one of the most powerful cultural figures, protecting the people and especially, their rulers, as well as being assigned powerful roles in nature. As human groups moved from being isolated clans and tribes to cities, kingdoms, and countries, ancient symbols retained their importance as they assumed new roles and lions have remained as popular symbols through to modern times.\n\nDepictions of lions in other cultures resembled this and all changed into more supportive roles as human figures began to be portrayed as deities. Similar imagery persisted and was retained through cultural changes, sometimes unchanged. Adoptions of lion imagery as symbols into other cultures without direct contact with lions could be very imaginative, often lacking accurate anatomical details or creating unrealistic characteristics. The association of lions with virtues and character traits was adopted in cultures where and when the religious symbolism had ceased.\n\nThe earliest known cave paintings of lions were found in the Chauvet Cave and in Lascaux in France's Ardèche region and represent some of the earliest paleolithic cave art, dating to between 32,000 and 15,000 years ago. The zoomorphic Löwenmensch figurine from Hohlenstein-Stadel and the ivory carving of a lion's head from Vogelherd Cave in the Swabian Jura in southwestern Germany were carbon-dated 39,000 years old, dating from the Aurignacian culture.\n\nThe earliest tomb paintings in Ancient Egypt, at Nekhen, c. 3500 BC, classified as Naqada, possibly Gerzeh, culture include images of lions, including an image of a human (or deity) flanked by two lions in an upright posture. Among ancient Egyptians, from prehistoric times through well documented records, the war goddess Sekhmet, a lioness, later depicted as woman with a lioness head, was one of their major deities. She was a sun deity as well as a fierce warrior and protector. Usually she was assigned significant roles in the natural environment. The Egyptians held that this sacred lioness was responsible for the annual flooding of the Nile, the most significant contributing factor to the success of the culture. Sometimes with regional differences in names, a lioness deity was the patron and protector of the people, the king, and the land. As the country united, a blending of those deities was assigned to Sekhmet.\nSimilar regional lioness deities assumed minor roles in the pantheon or, when so significant in a region, continued local religious observance in their own right, such as Bast. Offspring of these deities found niches in the expanding pantheon as well.\n\nDuring the New Kingdom the Nubian gods Maahes (god of war and protection and the son of Bast) and Dedun (god of incense, hence luxury and wealth) were depicted as lions. Maahes was absorbed into the Egyptian pantheon, and had a temple at the city the invading Greeks called Leontopolis, \"City of Lions\", at the delta in Lower Egypt. His temple was attached to the major temple of his mother, Bast. Dedun was not absorbed into the Ancient Egyptian religion and remained a Nubian deity.\n\nBast, originally depicted as a lioness and the \"eye of Ra\" in the delta region, was the parallel deity to Sekhmet in the southern region. Her nature gradually changed after the unification of the country and Sekhmet prevailed throughout. At that time Bast changed into the goddess of personal protection with different responsibilities, and often was depicted as a very tame lioness or a cat. She is shown to the left atop an alabaster jar that contained precious oils and lotions. The name of the stone probably bears her named because materials sacred to her usually were stored in it..\n\nThe sphinx of Ancient Egypt shows the head and shoulders of a human and the body of a lioness. The statues represents Sekhmet, who was the protector of the pharaohs. Later pharaohs were depicted as sphinxes, being thought as the offspring of the deity.\n\nIn ancient Mesopotamia, the lion was regarded as a symbol of kingship.\nSculptures and reliefs of the Neo-Assyrian Empire dating to the 6th and 7th centuries BC were rediscovered and excavated in the mid 19th century. Several reliefs feature lions, including the Lion Hunt of Ashurbanipal.\nA well-known detail of this relief is \"The Dying Lioness\" depicting a half-paralyzed lioness pierced with arrows. Other Assyrian palace reliefs from this era depict dozens of lions being hunted, originally in an Assyrian royal palace in Nineveh, located in modern day Iraq. The Babylonian goddess Ishtar was represented driving a chariot drawn by seven lions.\nIshtar's Sumerian analogue Inanna was frequently depicted standing on the backs of two lionesses. Such symbolism was appropriated by Saddam Hussein's regime in Iraq for their Lion of Babylon tank, with the technology adapted from a Russian model.\n\nAncient depictions often described as \"panthers\" because of no mane, in fact, are lionesses and may be identified easily by the distinctive tip of their tails that artists familiar with their subject, correctly portrayed.\n\nLions have been widely used in sculpture to provide a sense of majesty and awe, especially on public buildings. Lions were bold creatures and many ancient cities would have an abundance of lion sculptures to show strength in numbers as well. This usage dates back to the origin of civilization. There are lions at the entrances of cities and sacred sites from Mesopotamian cultures; notable examples include the Lion Gate of ancient Mycenae in Greece that has two lionesses flanking a column that represents a deity, and the gates in the walls of the Hittite city of Bogazköy, Turkey.\nThe \"Lion of Menecrates\" is a funerary statue of a crouching lioness, found near the cenotaph of Menecrates. The lion is by a famous Corinthian sculptor of Archaic Greece, end of the seventh century BC, and is now in the Archaeological Museum of Corfu.\n\nLion in the Iranian mythology is a symbol of courage and monarchy. He is portrayed standing beside the kings in artifacts and sitting on the graves of knights. Imperial seals were also decorated with carved lions. The lion and sun motif is based largely on astronomical configurations, and the ancient zodiacal sign of the sun in the house of Leo. Lion and sun will become a symbol of royalty in Iranian flag and coins. Goddesses Anahita sometimes have portrayed standing on a lion. Lion is also title of the fourth grade of Mithraism.\n\nLions have been extensively used in ancient Persia as sculptures and on the walls of palaces, in fire temples, tombs, on dishes and jewellery; especially during the Achaemenid Empire. The gates were adorned with lions. Evidences are found in Persepolis, Susa, Hyrcania.\n\nSeveral discoveries of lion bones in Greece, the Ukraine and the Balkans have confirmed that lions lived there up to perhaps 1,000 BC, which was previously only a suspicion by some archaeologists. Thus the strong emphasis on lions in the earliest figurative Greek art, especially that of Mycenaean Greece from around 1600-1400 BC, reflected the world in which Greeks lived, rather than being based on stories from further east, as once thought.\n\nLionesses often flanked the Gorgon, a vestige of the earliest Greek protective deity that often was featured atop temples of later eras. The western pediment from the Artemis Temple of Corfu is a well preserved example.\nThe most notable lion of Ancient Greek mythology was the Nemean lion, killed barehanded by Heracles, who subsequently bore the pelt as an invulnerable magic cloak.\nThis lion is also said to be represented by the constellation of Leo, and also the sign of the Zodiac.\n\nLions are known in many cultures as the king of animals, which can be traced to the Babylonian Talmud, and to the classical book \"Physiologus\". In his fables, the famed Greek story teller Aesop used the lion's symbolism of power and strength in The Lion and the Mouse and Lion's Share.\nSince classical antiquity, a \"Gaetulian lion\" in literature is a lion of fierce reputation. Gaetulia, in ancient geography, was the land of the Gaetuli, a warlike tribe of ancient Libya that appears in Virgil's \"Aeneid\" (19 BC). The Gaetulia lion appears in \"Odes\" of Horace (23 BC), Pliny the Elder's \"Natural History\" (77 AD), Philostratus's \"Life of Apollonius of Tyana\" ( 215), Robert Louis Stevenson's \"Travels with a Donkey in the Cévennes\" (1879).\n\nIn Socrates' model of the psyche (as described by Plato), the bestial, selfish nature of humanity is described metaphorically as a lion, the \"leontomorphic principle\".\n\nSeveral Biblical accounts document the presence of lions, and cultural perception of them in ancient Israel.\nThe best known Biblical account featuring lions comes from the Book of Daniel (chapter 6), where Daniel is thrown into a den of lions and miraculously survives.\n\nA lesser known Biblical account features Samson who kills a lion with his bare hands, later sees bees nesting in its carcass, and poses a riddle based on this unusual incident to test the faithfulness of his fiancée (Judges 14).\n\nThe prophet Amos said (Amos, 3, 8): \"The lion hath roared, who will not fear? the Lord GOD hath spoken, who can but prophesy?\", i.e., when the gift of prophecy comes upon a person, he has no choice but to speak out.\n\nIn 1 Peter 5:8, the Devil is compared to a roaring lion \"seeking someone to devour.\"\n\nIn Christian tradition, Mark the Evangelist, the author of the second gospel is symbolized by a lion - a figure of courage and monarchy. It also represents Jesus' Resurrection (because lions were believed to sleep with open eyes, a comparison with Christ in the tomb), and Christ as king. Some Christian legends refer to Saint Mark as \"Saint Mark the Lionhearted\". Legends say that he was fed to the lions and the animals refused to attack or eat him. Instead the lions slept at his feet, while he petted them. When the Romans saw this, they released him, spooked by the sight.\n\nThe lion is the biblical emblem of the tribe of Judah and later the Kingdom of Judah. It is contained within Jacob's blessing to his fourth son in the penultimate chapter of the Book of Genesis, \"Judah is a lion's whelp; On prey, my son have you grown. He crouches, lies down like a lion, like the king of beasts—who dare rouse him?\" (Genesis 49:9). In the modern state of Israel, the lion remains the symbol of the capital city of Jerusalem, emblazoned on both the flag and coat of arms of the city.\n\nIn gnostic traditions, the Demiurge is depicted as a lion-faced figure (\"leontoeides\"). The gnostic concept of the Demiurge is usually that of a malevolent, petty creator of the physical realm, a false deity responsible for human misery and the gross matter than traps the spiritual essence of the soul, and thus an \"animal-like\" nature. As a lion-headed figure, the Demiurge is associated with devouring flames, destroying the souls of humans after they die, as well as with arrogance and callousness.\n\nA lion-faced figurine is usually associated with the Mithraic mysteries. Without any known parallel in classical, Egyptian, or middle-eastern art, what this figure is meant to represent currently is unknown. Some have interpreted it to be a representation of Ahriman, of the aforementioned gnostic Demiurge, or of some similar malevolent, tyrannical entity, but it has also been interpreted as some sort of time or season deity, or even a more positive symbol of enlightenment and spiritual transcendence.\n\nIn a key scene of \"Yvain, the Knight of the Lion\" (), a romance by Chrétien de Troyes, the hero is depicted as rescuing a lion from a serpent.\n\nSubsequently, the lion proves to be a loyal companion and a symbol of knightly virtue, and helps Yvain complete his altruistic ventures. In the happy end, the lion comes to dwell with Yvain and his wife Laudine at their castle.\n\nIn Middle Eastern culture, both Arab and Persian, the lion is regarded as the symbol of courage, bravery, royalty, and chivalry. The Middle Eastern depiction of lions are derived from earlier Mesopotamian Babylonian and Persian arts. Islamic art commonly manifests its aesthetic elements predominantly in Islamic calligraphy, floral, and geometric decorative patterns, since Islamic religious tradition discourages the depictions of humans and living creatures (animals) in its sculpture. Through Persian arts miniatures and paintings, however, the depictions of humans and animals survives. In Muslim Spain period, the lion court of Alhambra palace displays the lion statues as supporters and waterspout of fountain.\n\n\"Aslan\" or \"Arslan\" (Ottoman ارسلان \"arslān\" and اصلان \"aṣlān\") is the Turkish and Mongolian word for \"lion\". It was used as a title by a number of Seljuk and Ottoman rulers, including Alp Arslan and Ali Pasha, and is a Turkic name.\n\nThe lion symbolism and its cultural depictions can be found in Hindu and Buddhist art of India and Southeast Asia. The lion symbolism in India was based upon Asiatic lions that once spread in Indian subcontinent as far as the Middle East.\n\nNarasimha (\"man-lion\"), also spelt \"Narasingh\", \"Narasinga\", is described as an incarnation (Avatara) of Vishnu in the Puranic texts of Hinduism. It is worshiped as \"Lion God\" and considered sacred by all Hindus in India.\n\nLions are also found in Buddhist symbolism. Lion pillars erected during the reign of Emperor Ashoka show lions and the chakra emblem. The lions depicted in the Lion Capital of Ashoka inspired artists who designed the Emblem of India.\n\nSingh is an ancient Indian vedic name meaning \"lion\", dating more than 2,000 years ago to ancient India. It was originally only used by Rajputs, a Hindu Kshatriya or military caste in India. After the birth of the Khalsa brotherhood in 1699, the Sikhs also adopted the name \"Singh\" due to the wishes of Guru Gobind Singh. Along with millions of Hindu Rajputs and numerous other Hindu martial groups today, it is also used by more than 20 million Sikhs worldwide. The appellation of the name Singh was used by the Rajputs before being adopted by the Sikhs in 1699. Therefore, all \"Singh\"s in Indian history before 1699 are Hindu and mainly Rajputs. The lion also features as the carrier or the vehicle of Durga, the Hindu goddess of war, worshipped in and around the Bengal region.\n\nThe lion is symbolic for the Sinhalese, Sri Lanka's ethnic majority; the term derived from the Indo-Aryan \"Sinhala\", meaning the \"lion people\" or \"people with lion blood\", while a sword-wielding lion is the central figure on the modern national flag of Sri Lanka.\n\nThe entrance to Sigiriya, the Lion-Rock of Sri Lanka, was through the Lion Gate, the mouth of a stone lion. The paws of the lion is one of seven World Heritage Sites in Sri Lanka.\n\nLions were never native animals of Southeast Asia in recorded history. As the result, the depiction of lion in ancient Southeast Asian art, especially in ancient Java and Cambodia, is far from naturalistic style as depicted in Greek or Persian art counterparts, since the artist who carved the lion sculpture never saw the lion before, and all were based on perception and imagination. The cultural depictions and the reverence of lion as the noble and powerful beast in Southeast Asia was influenced by Indian culture.\n\nStatue of a pair of lions often founds in temples in Southeast Asia as the gate guardian. In Borobudur Buddhist monument Central Java, Indonesia andesite stone statues of lions guarding four main entrances of Borobudur. The thrones of Buddha and Boddhisattva found in Kalasan and Mendut buddhist temples of ancient Java depicted elephant, lion, and makara. The statue of a winged lion also is found in Penataran temple East Java, as well as in Balinese temples. The Balinese winged lion often served as the guardian statue or as the pedestal of wooden column.\n\nIn Cambodia statues of lions flanking the temple gate or access roads are commonly found in temples of Angkor. Bakong, a stepped pyramid Hindu temple from earlier period also displays lion statues as guardians of each stage on each of the cardinal points. Khmer lion guardian statues are commonly found in Angkor Wat, Bayon, Pre Rup and Srah Srang. Just like ancient Java, the depiction of lion in ancient Khmer art is not in naturalistic style, more like a symbolic mythical animal derived from Indian Hindu-Buddhist art. The royal emblem of Cambodia depicting a pair of guardian animals; \"gajasingha\" (hybrid of elephant and lion) and \"singha\" (lion). In Thailand, a pair of lion statues are often placed in front of temple gate as guardian. The style of Thai lion is similar to those of Cambodian, since Thailand derived many of its aesthetics and arts elements from Cambodian Khmer art.\n\nIn Myanmar, the statue of lion called Chinthe guarding the stupas, pagodas, and Buddhist temples in Bagan, while pair of lions are also featured in the country's coat-of-arms.\n\nThe island nation of Singapore (\"Singapura\") derives its name from the Malay words (lion) and (city), which in turn is from the Tamil-Sanskrit சிங்க \"singa\" and புர . According to the Malay Annals, this name was given by a fourteenth-century Sumatran Malay prince named Sang Nila Utama, who, on alighting the island after a thunderstorm, spotted an auspicious beast on shore that his chief minister identified as a lion (Asiatic lion). Recent studies of Singapore indicate that lions have never lived there, and the beast seen by Sang Nila Utama likely was a tiger.\n\nIn the modern era, the lion or Merlion became the icon of Singapore due to the island's name. The Merlion also figures heavily in the official symbols of the Philippines as it was once an overseas possession of Spain; it appears on the coat-of-arms of Manila, as well as the emblems of the President, Vice-President, and its navy.\n\nThe common motif of the \"majestic and powerful\" lion was introduced to China by Buddhist missionaries from India, somewhere in the first century AD. Lions themselves, however, are not native to China, yet appear in the art of China and the Chinese people believe that lions protect humans from evil spirits, hence the Chinese New Year lion dance to scare away demons and ghosts. Chinese guardian lions are frequently used in sculpture in traditional Chinese architecture. For instance, in the Forbidden City in Beijing, two lion statues are seen in almost every door entrance.\n\nLions feature prominently in the Tibetan culture with a pair of Snow Lions seen on the Tibetan flag. The Snow Lions are mythical creatures that are seen as protector entities. The Snow Lion symbolizes fearlessness, unconditional cheerfulness, east, and the Earth element. It is one of the Four Dignities. It ranges over the mountains, and is commonly pictured as being white with a turquoise mane.\n\nLions (獅子, \"shishi\") feature prominently in many kabuki plays and other forms of Japanese legend and traditional tales.\n\nVarious kings and political leaders in different cultures and times, famed for courage or fierceness, were entitled \"the lion\" - such as:\n\n\n\nThe lion is a common charge in heraldry, traditionally symbolizing courage. The following positions of heraldic lions are recognized:\nThe lion holds historical significance for English heraldry and symbolism. The three lions was a symbol for Richard the Lionheart, and later, for England. For many centuries the lion had been a feature of the Armorial of Plantagenet of the House of Plantagenet, and is still worn by both the England national football team and England and Wales cricket team.\n\nThe lion rampant continues to be used widely today; the Royal Banner of Scotland has given rise to its use as the emblem for the Scotland national football team and Rangers and Dundee United of the Scottish Premier League, as well as English Premier League club Aston Villa; and not only sport but businesses such as the French car company Peugeot, the international beer company Lion Nathan, and Caledonian MacBrayne ferries. Arising from heraldic use, the \"Red Lion\" is also a popular pub name, with over 600 pubs bearing the name. A rarer inn name is the \"White Lion\", derived from Edward IV or the Duke of Norfolk. Though the lion rampant appears on the Lyon coat of arms and flag, the French city's name has an unrelated derivation despite the similarity. \"Rampant\" lions are common charges in heraldry. For example, the arms of the Carter of Castle Martin family, Ireland (see Carter-Campbell of Possil) include a pair of rampant combatant lions.\n\nIn the Middle Ages, when lions became a major element in heraldry, few Europeans had any chance to see actual lions. The lions were for them nearly as much legendary animals as were dragons or gryffins, which also commonly appeared on coats of arms.\n\nNational currencies of three countries in Europe are named after the lion: the Bulgarian lev (, plural: лева, левове / leva, levove), and the Moldovan and Romanian leu (/leŭ/, plural: lei /lej/) all mean \"lion\".\n\nA lion appears on the South African 50-rand banknotes.\n\nNo less than 18 consecutive ships of the British Royal Navy bore the name HMS \"Lion\". Also, various other navies have used the name for their vessels, as did civil shipping companies.\n\n\n \nMetro-Goldwyn-Mayer studios have used a lion as their logo since 1924. Five different lions have played Leo the Lion, the lion seen at the start of every MGM film.\nThe lion's role as \"king of the beasts\" has been utilized in cartoons, from the Leonardo Lion of \"King Leonardo and His Short Subjects\" (1960-1963) series to the Disney animated feature film \"The Lion King\" (1994)\n\n\nThe lion is a popular mascot or symbol, for businesses, government entities, sports, and other uses; for example:\n\n\n\n\n"}
{"id": "2595216", "url": "https://en.wikipedia.org/wiki?curid=2595216", "title": "East Siberian taiga", "text": "East Siberian taiga\n\nThe East Siberian taiga ecoregion, in the Taiga and Boreal forests Biome, is a very large biogeographic region in eastern Russia.\n\nThis vast ecoregion is located in the heart of Siberia, stretching over 20° of latitude and 50° of longitude (52° to 72° N, and 80° to 130° E). The climate in the East Siberian taiga is subarctic (tropical deciduous and coniferous) and displays high continentality, with extremes ranging from to . Winters are long and very cold, but dry, with little snowfall due to the effects of the Siberian anticyclone. Summers are short, but can be quite warm for the northerly location.\n\nPrecipitation is low, ranging from to , decreasing from east to west. The topography of this ecoregion is varied, consisting of wide, flat plains and areas of karst topography. In contrast to the neighbouring West Siberian taiga, large bogs and wetlands are conspicuously absent. Some trees also shed their leaves annually, a characteristic of deciduous forests.\n\nVegetation consists mainly of vast, dense forests of Dahurian Larch (\"Larix gmelinii\"), with Siberian Larch (\"Larix sibirica\") and hybrid between the Dahurian and Siberian Larches (\"Larix x czekanowskii\") occurring as one moves to the west. Cranberry (\"Vaccinium oxycoccus\") and bilberry (\"Vaccinium myrtillus\") bushes dominate the understory.\n\nThroughout the ecoregion, smaller areas dominated by Siberian pine (\"Pinus sibirica\"), Scots pine (\"Pinus sylvestris\"), Siberian spruce (\"Picea obovata\") and Siberian fir (\"Abies sibirica\") can be found. Pine forests and deciduous forests composed of birch and poplar species become more common as one moves south, and at the headwaters of the Lena River and the Nizhnyaya Tunguska River, as well as in the Angara River basin, steppe and shrub-steppe communities can be found along with areas of forest-steppe.\n\nThis region contains the highest number of brown bears (\"Ursus arctos collaris\"), Eurasian wolf (\"Canis lupus\"), moose (\"Alces alces\") and wild reindeer (\"Rangifer tarandus\") in Russia. Further south mammals in the East Siberian taiga include Siberian musk deer (\"Moschus moschiferus\"), wapiti also known as Asian elk (\"Cervus canadensis\") and wild boar (\"Sus scrofa\"). Birds of this ecoregion include the golden eagle (\"Aquila chrysaetos\"), peregrine falcon (\"Falco peregrinus\"), osprey (\"Pandion haliaetus\"), black stork (\"Ciconia nigra\"), hooded crane (\"Grus monacha\"), carrion crow (\"Corvus corone\"), the Siberian blue and rufous-tailed robins (\"Luscinia cyane\" and \"L. sibilans\", respectively), the thrush nightingale (\"Luscinia luscinia\"), Pallas's rosefinch (\"Carpodacus roseus\"), Pacific swift (\"Apus pacificus\") and Baikal teal (\"Anas formosa\").\n\nAlthough little of this ecoregion is protected, its conservation status is listed as \"Relatively Stable/Intact\". Protected areas in this ecoregion include:\nAll are \"Zapovedniks\", (that is, strict ecological reserves). The main threats to this ecoregion's integrity are poaching and clear-cut logging in the southern and central portions of the region.\n\nAfter the collapse of the Soviet Union, came a new threat. There is little forest in China, and Chinese entrepreneurs (due to the lack of woods, and fuelled by strong economic growth), began to show interest in obtaining the woods from RF. High levels of corruption and some other reasons allowed them to achieve their goal. A new law adopted was Federal law 473-FZ, which protects the rights of foreign companies in RF, and Chinese lumberjacks began to destroy all the trees on the leased land (and far beyond too). There have been numerous attempts of deforestation in the protected floodplain of the rivers. Because of the large environmental damage, their activities caused protests by local residents and the World Wide Fund for Nature. The actions of the Chinese companies pose a threat to the native population of Siberia and the Far East (Evenks, Udege et al.), depriving them of their habitat and traditional ways of life. Total deforestation by Chinese companies creates an additional threat to rare and endangered species such as the Siberian tiger, Amur leopard, East Siberian brown bear et al..\n\n\n"}
{"id": "1323604", "url": "https://en.wikipedia.org/wiki?curid=1323604", "title": "Energy accounting", "text": "Energy accounting\n\nEnergy accounting is a system used to measure, analyze and report the energy consumption of different activities on a regular basis. This is done to improve energy efficiency, and to monitor the environment impact of energy consumption.\n\nEnergy accounting is a system used in energy management systems to measure and analyze energy consumption to improve energy efficiency within an organization. Organisations such as Intel corporation use these systems to track energy usage. \n\nVarious energy transformations are possible. An energy balance can be used to track energy through a system. This becomes a useful tool for determining resource use and environmental impacts. How much energy is needed at each point in a system is measured, as well as the form of that energy. An accounting system keeps track of energy in, energy out, and non-useful energy versus work done, and transformations within a system. Sometimes, non-useful work is what is often responsible for environmental problems.\n\nEnergy returned on energy invested (EROEI) is the ratio of energy delivered by an energy technology to the energy invested to set up the technology.\n\n"}
{"id": "29232461", "url": "https://en.wikipedia.org/wiki?curid=29232461", "title": "F.W. Clarke Medal", "text": "F.W. Clarke Medal\n\nThe F.W. Clarke Medal is an annual award presented by the Geochemical Society to an early-career scientist for a single outstanding contribution to geochemistry or cosmochemistry, published either as a single paper or a series of papers on a single topic. The award is named after Frank Wigglesworth Clarke, one of the founding fathers of geochemistry.\n\n"}
{"id": "3269260", "url": "https://en.wikipedia.org/wiki?curid=3269260", "title": "Functional ecology", "text": "Functional ecology\n\nFunctional ecology is a branch of ecology that focuses on the roles, or functions, that species play in the community or ecosystem in which they occur. In this approach, physiological, anatomical, and life history characteristics of the species are emphasized. The term \"function\" is used to emphasize certain physiological processes rather than discrete properties, describe an organism's role in a trophic system, or illustrate the effects of natural selective processes on an organism. This sub-discipline of ecology represents the crossroads between ecological patterns and the processes and mechanisms that underlie them. It focuses on traits represented in large number of species and can be measured in two ways. The first being screening, which involves measuring a trait across a number of species, and the second being empiricism, which provides quantitative relationships for the traits measured in screening. Functional ecology often emphasizes an integrative approach, using organism traits and activities to understand community dynamics and ecosystem processes, particularly in response to the rapid global changes occurring in earth’s environment.\n\nFunctional ecology sits at the nexus of several disparate disciplines and serves as the unifying principle between evolutionary ecology, evolutionary biology, genetics and genomics, and traditional ecological studies, and attempts to understand species' \"competitive abilities, patterns of species co-occurrence, community assembly, and the role of different traits on ecosystem functioning\".\n\nThe notion that ecosystems' functions can be affected by their constituent parts has its origins in the 19th century. Charles Darwin's \"On The Origin of Species\" is one of the first texts to directly comment on the effect of biodiversity on ecosystem health by noting a positive correlation between plant density and ecosystem productivity. In his influential 1927 work, \"Animal Ecology\", Charles Elton proposed classifying an ecosystem based on the how its members utilize resources. By the 1950s, Elton's model of ecosystems was widely accepted, where organisms that shared similarities in resource use occupied the same 'guild' within an ecosystem.\n\nBeginning in the 1970s, an increased interest in functional classification revolutionized functional ecology. 'Guilds' would be re-termed 'functional groups', and classification schemes began to focus more on interactions between species and trophic levels. Functional ecology became widely understood to be the study of ecological processes that concern the adaptations of organism within the ecosystem. In the 1990s, biodiversity became better understood as the diversity of species' ecological functions within an ecosystem, rather than simply a great number of different species present. Finally, in the 2000s researchers began using functional classification schemes to examine ecosystems' and organisms' responses to drastic change and disturbance, and the impact of function loss on the health of an ecosystem.\n\nFunctional diversity is widely considered to be “the value and the range of those species and organismal traits that influence ecosystem functioning” In this sense, the use of the term “function” may apply to individuals, populations, communities, trophic levels, or evolutionary process (i.e. considering the function of adaptations). Functional diversity was conceived as an alternative classification to schemes using genetic diversity or physiological diversity to measure the ecological importance of species in an environment, as well as a way to understand how biodiversity affects specific ecosystem functions, where in this context, 'biodiversity' refers to the diversity of ecosystem functions present in a given system. Understanding ecosystems via functional diversity is as powerful as it is broadly applicable and gives insight into observable patterns in ecosystems, such as species occurrence, species competitive abilities, and the influence of biological communities on ecosystem functioning.\n\nA key interest of modern research in Functional Ecology is the impact of functional diversity on ecosystem health. Unsurprisingly, biodiversity has a positive impact on the productivity of an ecosystem. Increased functional diversity increases both the capacity of the ecosystem to regulate the flux of energy and matter through the environment (Ecosystem Functions) as well as the ecosystem's ability to produce resources beneficial to humans such as air, water, and wood (Ecosystem Services). Ecosystem Functions are drastically reduced with decreases in the diversity of genes, species and functional groups present within an ecosystem. In fact, reductions in functional diversity broadly impact the survivability of organisms in an environment regardless of functional group, trophic level, or species, implying that the organization and interaction of communities in an ecosystem has a profound impact on its ability to function and self-sustain. Furthermore, diversity improves environmental stability. The greater an ecosystem's diversity, the more resilient it is to changes in species composition (e.g. extinction events or invasive species) and extraneous changes to environmental conditions (e.g. logging, farming, and pollution). Moreover, the benefits that diversity provides to an environment scale non-linearly with the amount of diversity. Unfortunately, this relationship also acts in the opposite direction. The \"loss\" of diversity non-linearly disrupts ecosystems (even stable ones); this negative impact is especially detrimental when the loss is across trophic levels. For, example, the loss of a single tertiary predator can have cascading effects on the food chain, resulting in reduction of plant biomass and genetic diversity. This in turn can alter the \"vegetation structure, fire frequency, and even disease epidemics in a range of ecosystems\". The effects of diversity on ecosystems are so powerful, that they can rival the impact of climate change and other global ecosystem stressors.\n\nAlternatively, in rare situations, diversity has been shown to retard ecological productivity. In experimentally concocted microscopic environments, a diverse culture of bacteria was unable to out-produce a homogeneous culture of an 'efficient' control strain. However, the statistical validity and setup of these experiments have been questioned, and require further investigation to carry substantial merit. In general, the current consensus that diversity is beneficial to ecosystem health has much more theoretical and empirical support and is more widely applicable.\n\nMost models of complex functional diversity are only effective in a small range of spacial scales. However, by defining the functional trait probability density as a \"function representing the distribution of probabilities of observing each possible trait value in a given ecological unit,\" the results of many models can be generalized to larger scales. At larger spatial scales, more environmental heterogeneity may increase opportunities for species to exploit more functional groups. Consistent with this conclusion, tests of theoretical models predict that the net effects of biodiversity on ecosystem functions grow stronger over time, over larger spacial scales, and with more heterogeneous natural resources. However, these results are expected to underestimate the actual relationshipm impling that large space and time scales coupled with diverse resources are more than necessary to sustain an ecosystem.\n\nA functional approach to understanding and dealing with environments provides numerous benefits to our understanding of biology and its applications in our lives. While the concept of functional ecology is still in its infancy, it has been widely applied throughout biological studies to better understand organisms, environments, and their interactions.\n\nThe notions of functional ecology have beneficial implications for species detection and classification. When detecting species, ecologically important traits, such as plant height, influence the probability of detection during field surveys. When holistically analyzing an environment, the systematic error of imperfect species detection can lead to incorrect trait-environment evolutionary conclusions as well as poor estimates of functional trait diversity and environmental role. For example, if small species of insects are less likely to be detected, researchers may conclude that they are much more scarce (and thus less impactful) in the environment than larger species of insects. This 'detection filtering' has major consequences on functional packaging and the defining functional groups in an ecosystem. Thankfully, correlations between environmental change and evolutionary adaptation is are much larger than the effects of imperfect species detection. Nevertheless, approaching ecosystems with theoretical maps of functional relationships between species and groups can reduce the likelihood of improper detection and improve the robustness of any biological conclusions drawn.\n\nA functional approach to defining traits can even help species classification. Trait focused schemes of taxonomy have long been used to classify species, but the number and type of 'trait' to consider is widely debated. Considering more traits in a classification scheme will separate species into more specific functional groups, but may lead to an overestimation of total functional diversity in the environment. However, considering too few traits runs the risk of classifying species as functionally redundant, when they are in fact vital to the health of the ecosystem. So, before one can classify organisms by traits, the definition of 'trait' must be settled. Rather than define traits as proxies for organism performance, as Darwin did, modern ecologists favor a more robust definition of traits often referred to as \"functional traits\". Under this paradigm, functional traits are defined as morpho-physiophenological traits which impact fitness indirectly via their effects on growth, reproduction and survival. Notice that is definition is not specific to species. Since larger biological organizations grow, reproduce and sustain just as individual organisms do, functional traits can be used to describe ecosystem processes and properties as well. To distinguish between functional traits at different scales, the classification scheme adopts the following nomenclature. Individual organisms have Ecophysiological traits and life-history traits; populations have demographic traits; communities have response traits; and ecosystems have effect traits. At each level, functional traits can directly and indirectly influence functional traits in the levels above or below them. For example, when averaged over an ecosystem, individual plants' heights can contribute to ecosystem productivity or efficiency.\n\nFunctional Ecology is closely intertwined with genomics. Understanding the functional niches that organisms occupy in a ecosystem can provide clues to genetic differences between members of a genus. On the other hand, discovering the traits/functions that genes encode for yields insight into the roles that organisms perform in their environment. This kind of genomic study is referred to as genomic ecology or ecogenomics. Genomic ecology can classify traits on cellular and physiological levels leading to a more refined classification system. In addition, once genetic markers for functional traits in individuals are identified, predictions about the functional diversity and composition of an ecosystem can be made from the genetic data of a few species in a process called \"reverse ecology\". Reverse ecology can contribute to better taxonomy of organisms as well. Rather than defining species by genetic proximity alone, organisms can be additionally classified by the functions they serve in the same ecology. This application of reverse ecology has proven especially useful in the classification of bacteria. Researchers were able to identify the correspondence between genetic variation and ecological niche function in the genus \"Agrobacterium\" and their greater biological implication on species distinction and diversity in the ecosystem. The researchers found that 196 genes specific to \"Agrobacterium fabrum\" coded for metabolic pathways specific to plants which allowed for the use of plant-specific compounds and sugars to avoid iron deficiency. This trait, unique to \"Agrobacterium fabrum,\" allowed it to avoid competition with closely related bacteria in \"Agrobacterium\" found within the same environment. Thus, understanding the genetics of \"Agrobacterium fabrum\" allowed researchers to infer that it evolved into the niche (i.e. ecological role) of a plant so that it could avoid competing with its close relatives. If this process can be shown to generalize, then the ecological functions of other organisms can be inferred simply from genetic information.\n\nHowever, reverse ecology and genomic ecology face several hurdles before they can be accepted as rigorous and mainstream approaches to taxonomy or ecology. One of the major challenges is that technologies for the sequencing and comparison of transcriptomic data do not exist, making the acquisition of transcriptomic data dependent on environmental conditions. Additionally, as studied environments increase in complexity, transcriptomic data becomes harder to collect. Furthermore, the functions that many discovered genes encode for are still unknown making it difficult if not impossible to infer ecological function from a genome. Testing hypotheses concerning what functions given genes encode for is difficult experimentally and is expensive and time-consuming.\n\nFunctional ecology also has broad applications to the science of and debate over de-extinction, the resurrection of extinct species. Function ecology can be applied to strategically assess the resurrection of extinct species to maximize its impact on an environment. To avoid reintroducing a species that is rendered functionally redundant by one of its ancestors, a functional analysis of global ecosystems can be performed to determine which ecosystems would benefit most from the added functional diversity of the reintroduced species. These considerations are important because, while many species currently being considered for de-extinction are terrestrial, they are also functionally redundant in their former ecosystems. However, many extinct marine species have been identified as functionally unique in their environments, even today, which makes a strong case for their reintroduction. In fact, while some functions have been recovered by evolution, as is the case with many extinct terrestrial species, some functional gaps have widened over time. Reintroducing extinct species has the potential to close these gaps, making richer, more balanced ecosystems.\n\nFurthermore, before a species goes extinct in the classical sense of the word, keeping a functional perspective in mind can avoid \"functional extinction\". Functional extinction is defined as \"the point at which a species fails to perform its historical functional role\". Endangered species such as species of tigers, tuna and sea otters usually qualify for this threshold. If functional ecology is considered, new species (not necessarily extinct) can be introduced into ecosystem where a species has become functionally extinct before any de-extinction action ever needs to be taken. This can be a key transformative process in ecological preservation and restoration because functional extinction can have cascading effects on the health of an ecosystem. For example, species that engineer ecosystems such as beavers are particularly unique functionally; their absence from an ecosystem could be devastating.\n\nWhile functional arguments for reintroduction of extinct species, may paint thoughtful reintroduction as a ecological boon, the ethical and practical debate over de-extinction has not left functional approaches unscathed. The main critique of functional arguments in favor of de-extinction are largely focused on contentions that ecological functions are often ambiguously defined and that it is unclear what functions must be present to define an ecosystem. These arguments suggest that reintroducing an extinct species could be drastically harm an ecosystem if conclusions about its function or the functions of the species it is intended to replace are incorrect. Additionally, even if an extinct species' function is well understood de-extinction could be equally harmful if the function served by the extinct species is no longer needed by the ecosystem.\n\nThe scientific journal \"Functional Ecology\" is published by the British Ecological Society since 1986\n\n"}
{"id": "9251112", "url": "https://en.wikipedia.org/wiki?curid=9251112", "title": "Giovanni (meteorology)", "text": "Giovanni (meteorology)\n\nGiovanni (meteorology) - Web interface that allows users to analyze NASA's gridded data from various satellite and surface observations.\n\nGiovanni provides researchers with the capability to examine data on atmospheric chemistry, atmospheric temperature, water vapor and clouds, atmospheric aerosols, precipitation, and ocean chlorophyll and surface temperature. The primary data consist of global gridded data sets with reduced spatial resolution. Basic analytical functions performed by Giovanni currently are carried out by the Grid Analysis and Display System (GrADS). \n\nThe GES-DISC Interactive Online Visualization ANd aNalysis Infrastructure (Giovanni) allows to explore satellite data using sophisticated analyses and visualizations.\nGiovanni allows access to data from multiple remote sites, supports multiple data formats including Hierarchical Data Format (HDF), HDF-EOS, network Common Data Form (netCDF), GRIdded Binary (GRIB), and binary, and multiple plot types including area, time, Hovmoller, and image animation. \n\n"}
{"id": "4414339", "url": "https://en.wikipedia.org/wiki?curid=4414339", "title": "Hanns Hörbiger", "text": "Hanns Hörbiger\n\nHanns Hörbiger (29 November 1860, in Atzgersdorf – 11 October 1931, in Mauer) was an Austrian engineer from Vienna with roots in Tyrol. He took part in the construction of the Budapest subway and in 1894 invented a new type of valve essential for compressors still in widespread use today.\n\nHe is also remembered today for his pseudoscientific Welteislehre (\"World Ice\") theory.\n\nHanns Hörbiger was born in Atzgersdorf, a suburb of Liesing, Vienna and studied engineering at the local Technical College.\n\nIn 1894 Hörbiger had an idea for a new design of blast furnace blowing engine: he replaced the old and easily damaged leather flap valves with a steel valve. Opening and closing automatically, and light and frictionless guided, the disk valve eliminated all the drawbacks of previous valve designs.\n\nHörbiger registered a patent for his invention, which smoothed the way for efficient steel production and greater productivity in mining. High-pressure chemistry and the global network of gas exchange – none of these would be possible without the Hörbiger Valve.\n\nIn 1900, Hanns Hörbiger and the engineer Friedrich Wilhelm Rogler founded an engineer’s office in Budapest, which was moved to Vienna in 1903. By 1925 it had developed into the Hörbiger & Co. company. Alfred Hörbiger, one of Hörbiger’s sons, joined the company in 1925 and assumed the management, while Hanns Hörbiger devoted himself to scientific study until his death in 1931.\n\nThe company developed rapidly under Alfred Hörbiger’s management: a production facility was taken into service in Vienna and an affiliated company was set up in Düsseldorf. Hörbiger expanded into England and concluded numerous licensing agreements with leading manufacturers of piston blowers, compressors and ships’ Diesel engines in Europe and North America.\n\nThe success was driven by originality and inventive genius. The disk valve became more sophisticated: Hörbiger developed highlift or high-pressure valves, compressor control systems and damper plates. By 1937, 98 percent of production was destined for export. The name Hörbiger had become a dependable trademark in valve and control technology for compressors. As of 2017, the engineering company founded by Hanns Hörbiger still exists as HOERBIGER and is a major supplier of compression technology.\n\nHörbiger is nowadays chiefly remembered for his \"Welteislehre\" (\"World Ice\") theory, which he first put forward in the 1913 book, \"Wirbelstürme, Wetterstürze, Hagelkatastrophen und Marskanal-Verdoppelungen\", written in collaboration with amateur astronomer Philipp Fauth. Hörbiger's theories were later popularized by H.S. Bellamy, and influenced Hans Robert Scultetus, head of the \"Pflegestätte für Wetterkunde\" (Meteorology Section) of the SS-Ahnenerbe, who believed that \"Welteislehre\" could be used to provide accurate long-range weather forecasts.\n\nTwo of Hörbiger's sons, Paul and Attila, were matineé idols in the interwar years, and Paul Hörbiger's granddaughter Mavie Hörbiger also went on to become a celebrated actress. His two other sons devoted themselves to promoting their father's theory.\n\nThe Deslandres crater on the moon was designated \"Hörbiger\" by Philipp Fauth on his private lunar chart; it subsequently received the official name of Deslandres, following a 1942 suggestion by E. M. Antoniadi in 1942, which was approved at the International Astronomical Union's 1948 General Assembly.\n"}
{"id": "68316", "url": "https://en.wikipedia.org/wiki?curid=68316", "title": "Heat pump", "text": "Heat pump\n\nA heat pump is a device that transfers heat energy from a source of heat to what is called a heat sink. Heat pumps move thermal energy in the opposite direction of spontaneous heat transfer, by absorbing heat from a cold space and releasing it to a warmer one. A heat pump uses a small amount of external power to accomplish the work of transferring energy from the heat source to the heat sink.\nThe most common design of a heat pump involves four main components – a condenser, an expansion valve, an evaporator and a compressor. The heat transfer medium circulated through these components is called refrigerant.\n\nWhile air conditioners and freezers are familiar examples of heat pumps, the term \"heat pump\" is more general and applies to many HVAC (heating, ventilating, and air conditioning) devices used for space heating or space cooling. When a heat pump is used for heating, it employs the same basic refrigeration-type cycle used by an air conditioner or a refrigerator, but in the opposite direction – releasing heat into the conditioned space rather than the surrounding environment. In this use, heat pumps generally draw heat from the cooler external air or from the ground.\n\nIn heating mode, heat pumps are three to four times more effective at heating than simple electrical resistance heaters using the same amount of electricity. However, the typical cost of installing a heat pump is also higher than that of a resistance heater.\n\nWhen discussing about and comparing energy efficiency of heat pumping applications some different factors are mainly used, COP (Coefficient of Performance), SCOP (Seasonal Coefficient of Performance) and SPF (Seasonal Performance Factor). The more efficient a heat pump is the less energy consuming it will be and the more cost-effective it can be to operate. There are several factors that will affect the efficiency of a heat pump such as climate, temperature, auxiliary equipment, technology, size and control system.\n\nHeat energy naturally transfers from warmer places to colder spaces. However, a heat pump can reverse this process, by absorbing heat from a cold space and releasing it to a warmer one. Heat is not conserved in this process and requires some amount of external energy, such as electricity. In heating, ventilation and air conditioning (HVAC) systems, the term \"heat pump\" usually refers to vapor-compression refrigeration devices optimized for high efficiency in both directions of thermal energy transfer. These heat pumps can be reversible, and work in either direction to provide heating or cooling to the internal space.\n\nHeat pumps are used to transfer heat because less high-grade energy is required than is released as heat. Most of the energy for heating comes from the external environment, only a fraction of which comes from electricity (or some other high-grade energy source required to run a compressor). In electrically-powered heat pumps, the heat transferred can be three or four times larger than the electrical power consumed, giving the system a coefficient of performance (COP) of 3 or 4, as opposed to a COP of 1 for a conventional electrical resistance heater, in which all heat is produced from input electrical energy.\n\nHeat pumps use a refrigerant as an intermediate fluid to absorb heat where it vaporizes, in the evaporator, and then to release heat where the refrigerant condenses, in the condenser. The refrigerant flows through insulated pipes between the evaporator and the condenser, allowing for efficient thermal energy transfer at relatively long distances.\n\nReversible heat pumps work in either direction to provide heating or cooling to the internal space. They employ a reversing valve to reverse the flow of refrigerant from the compressor through the condenser and evaporation coils.\n\nIn heating mode, the outdoor coil is an evaporator, while the indoor is a condenser. The refrigerant flowing from the evaporator (outdoor coil) carries the thermal energy from outside air (or soil, or better still, moving water) indoors. Vapor temperature is augmented within the pump by compressing it. The indoor coil then transfers thermal energy (including energy from the compression) to the indoor air, which is then moved around the inside of the building by an air handler.\n\nAlternatively, thermal energy is transferred to water, which is then used to heat the building via radiators or underfloor heating. The heated water may also be used for domestic hot water consumption. The refrigerant is then allowed to expand, and hence cool, and absorb heat from the outdoor temperature in the outside evaporator, and the cycle repeats. This is a standard refrigeration cycle, save that the \"cold\" side of the refrigerator (the evaporator coil) is positioned so it is outdoors where the environment is colder.\n\nIn cold weather, the outdoor unit of an air source heat pump needs to be intermittently defrosted. This will cause the auxiliary or emergency heating elements (located in the air-handler) to be activated. At the same time, the frost on the outdoor coil will quickly be melted due to the warm refrigerant. The condenser/evaporator fan will not run during defrost mode.\n\nIn cooling mode the cycle is similar, but the outdoor coil is now the condenser and the indoor coil (which reaches a lower temperature) is the evaporator. This is the familiar mode in which air conditioners operate.\n\nMilestones:\n\nMechanical heat pumps exploit the physical properties of a volatile evaporating and condensing fluid known as a refrigerant. The heat pump compresses the refrigerant to make it hotter on the side to be warmed, and releases the pressure at the side where heat is absorbed. \n\nThe working fluid, in its gaseous state, is pressurized and circulated through the system by a compressor. On the discharge side of the compressor, the now hot and highly pressurized vapor is cooled in a heat exchanger, called a condenser, until it condenses into a high pressure, moderate temperature liquid. The condensed refrigerant then passes through a pressure-lowering device also called a metering device. This may be an expansion valve, capillary tube, or possibly a work-extracting device such as a turbine. The low-pressure liquid refrigerant then enters another heat exchanger, the evaporator, in which the fluid absorbs heat and boils. The refrigerant then returns to the compressor and the cycle is repeated.\n\nIt is essential that the refrigerant reaches a sufficiently high temperature, when compressed, to release heat through the \"hot\" heat exchanger (the condenser). Similarly, the fluid must reach a sufficiently low temperature when allowed to expand, or else heat cannot flow from the ambient cold region into the fluid in the cold heat exchanger (the evaporator). In particular, the pressure difference must be great enough for the fluid to condense at the hot side and still evaporate in the lower pressure region at the cold side. The greater the temperature difference, the greater the required pressure difference, and consequently the more energy needed to compress the fluid. Thus, as with all heat pumps, the coefficient of performance (amount of thermal energy moved per unit of input work required) decreases with increasing temperature difference.\n\nInsulation is used to reduce the work and energy required to achieve a low enough temperature in the space to be cooled.\n\nHeat is typically transferred through engineered heating or cooling systems by using a flowing gas or liquid. Air is sometimes used, but quickly becomes impractical under many circumstances because it requires large ducts to transfer relatively small amounts of heat. In systems using refrigerant, this working fluid can also be used to transfer heat a considerable distance, though this can become impractical because of increased risk of expensive refrigerant leakage. When large amounts of heat are to be transferred, water is typically used, often supplemented with antifreeze, corrosion inhibitors, and other additives.\n\nA common source or sink for heat in smaller installations is the outside air, as used by an air-source heat pump. A fan is needed to improve heat exchange efficiency.\n\nLarger installations handling more heat, or in tight physical spaces, often use water-source heat pumps. The heat is sourced or rejected in water flow, which can carry much larger amounts of heat through a given pipe or duct cross-section than air flow can carry. The water may be heated at a remote location by boilers, solar energy, or other means. Alternatively when needed, the water may be cooled by using a cooling tower, or discharged into a large body of water, such as a lake, stream or an ocean.\n\nGeothermal heat pumps or ground-source heat pumps use shallow underground heat exchangers as a heat source or sink, and water as the heat transfer medium. This is possible because below ground level, the temperature is relatively constant across the seasons, and the earth can provide or absorb a large amount of heat. Ground source heat pumps work in the same way as air-source heat pumps, but exchange heat with the ground via water pumped through pipes in the ground. Ground source heat pumps are more simple and therefore more reliable than air source heat pumps as they do not need fan or defrosting systems and can be housed inside. Although a ground heat exchanger requires a higher initial capital cost, the annual running costs are lower, because well-designed ground source heat pump systems operate more efficiently because they start with a warmer source temperature than the air in winter.\n\nHeat pump installations may be installed alongside an auxiliary conventional heat source such as electrical resistance heaters, or oil or gas combustion. The auxiliary source is installed to meet peak heating loads, or to provide a back-up system.\n\nThere are millions of domestic installations using air source heat pumps. They are used in climates with moderate space heating and cooling needs (HVAC) and may also provide domestic hot water. The purchase costs are supported in various countries by consumer rebates.\n\nIn HVAC applications, a heat pump is typically a vapor-compression refrigeration device that includes a reversing valve and optimized heat exchangers so that the direction of \"heat flow\" (thermal energy movement) may be reversed. The reversing valve switches the direction of refrigerant through the cycle and therefore the heat pump may deliver either heating or cooling to a building. In cooler climates, the default setting of the reversing valve is heating.\n\nThe default setting in warmer climates is cooling. Because the two heat exchangers, the condenser and evaporator, must swap functions, they are optimized to perform adequately in both modes. Therefore, the SEER rating, which is the Seasonal Energy Efficiency Rating, of a reversible heat pump is typically slightly less than two separately optimized machines. For equipment to receive the Energy Star Rating, it must have a rating of at least 14.5 SEER.\n\nIn water heating applications, a heat pump may be used to heat or preheat water for swimming pools or heating potable water for use by homes and industry. Usually heat is extracted from outdoor air and transferred to an indoor water tank, another variety extracts heat from indoor air to assist in cooling the space.\n\nHeat pumps can also be used as heat supplier for district heating. Possible heat sources for such applications are sewage water, ambient water (like sea, lake and river water), industrial waste heat, geothermal energy, flue gas, waste heat from district cooling and heat from solar heat storage. In Europe, more than 1500 MW were installed since the 1980s, of which about 1000 MW were in use in Sweden in 2017. Large scale heat pumps for district heating combined with thermal energy storage offer high flexibility for the integration of variable renewable energy. Therefore they are regarded as a key technology for smart energy systems with high shares of renewable energy up to 100 % and advanced 4th generation district heating systems.\n\nOne example for such an usage is the Drammen Heat Pump commissioned in 2011. This heat pump uses heat from a fjord whose temperature is around 8 °C using 3 systems giving a combined capacity of 14 megawatts to town center residences and businesses. A city ordinance mandates this heating system for many new buildings.\n\nThere is a great potential to reduce the energy consumption and related greenhouse gas emissions in the industry by application of industrial heat pumps. An international collaboration project completed in 2015 collected totally 39 examples of R&D-projects and 115 case studies worldwide. The study shows that short payback periods are possible (less than 2 years), high reduction of CO2-emissionen can be achieved (in some cases more than 50 %).\n\nUntil the 1990s, the refrigerants were often chlorofluorocarbons such as R-12 (dichlorodifluoromethane), one in a class of several refrigerants using the brand name Freon, a trademark of DuPont. Its manufacture is now banned or severely restricted by the Montreal Protocol of August 1987 because of the damage that CFCs cause to the ozone layer if released into the atmosphere.\n\nOne widely adopted replacement refrigerant is the hydrofluorocarbon (HFC) known as R-134a (1,1,1,2-tetrafluoroethane). Heat pumps using R-134a replaced R-12 (dichlorodifluoromethane) and have similar thermodynamic properties but with insignificant ozone depletion potential and a somewhat lower global warming potential. Other substances such as liquid R-717 ammonia are widely used in large-scale systems, or occasionally the less corrosive but more flammable propane or butane, can also be used.\n\nSince 2001, carbon dioxide, R-744, has increasingly been used, utilizing the transcritical cycle, although it requires much higher working pressures. In residential and commercial applications, the hydrochlorofluorocarbon (HCFC) R-22 is still widely used, however, HFC R-410A does not deplete the ozone layer and is being used more frequently; however, it is a powerful greenhouse gas which contributes to climate change. Hydrogen, helium, nitrogen, or plain air is used in the Stirling cycle, providing the maximum number of options in environmentally friendly gases.\n\nMore recent refrigerators use R600A which is isobutane, and does not deplete the ozone and is less harmful to the environment. Dimethyl ether (DME) has also gained in popularity as a refrigerant.\n\nAs quite similar criteria shall be fulfilled by working fluids applied to heat pumps, refrigeration and ORC cycles, several working fluids are applied by all these technologies and can be sorted into the same thermodynamic classification category based on the shape of their saturation curve.\n\nA ground source heat pump has no need for an outdoor unit with moving mechanical components: no external noise is produced.\n\nAn air source heat pump requires an outdoor unit containing moving mechanical components including fans which produce noise. In 2013, the CEN started work on standards for protection from noise pollution caused by heat pump outdoor units. Although the CEN/TC 113 Business Plan outset was that \"consumers increasingly require a low acoustic power of these units as the users and their neighbours now reject noisy installations\", no standards for noise barriers or other means of noise protection had been developed by January 2016.\n\nIn the United States, the allowed nighttime noise level was defined in 1974 as \"an average 24-hr exposure limit of 55 A-weighted decibels (dBA) to protect the public from all adverse effects on health and welfare in residential areas (U.S. EPA 1974). This limit is a day–night 24-hr average noise level (LDN), with a 10-dBA penalty applied to nighttime levels between 2200 and 0700 hours to account for sleep disruption and no penalty applied to daytime levels. The 10-dB(A) penalty makes the permitted U.S. nighttime noise level equal to 45 dB(A), which is more than is accepted in some European countries but less than the noise produced by some heat pumps.\n\nAnother feature of ASHP external heat exchangers is their need to stop the fan from time to time for a period of several minutes in order to get rid of frost that accumulates in the outdoor unit in the heating mode. After that, the heat pump starts to work again. This part of the work cycle results in two sudden changes of the noise made by the fan. The acoustic effect of such disruption on neighbors is especially powerful in quiet environments where background nighttime noise may be as low as 0 to 10dBA. This is included in legislation in France. According to the French concept of noise nuisance, \"noise emergence\" is the difference between ambient noise including the disturbing noise, and ambient noise without the disturbing noise.\n\nWhen comparing the performance of heat pumps, it is best to avoid the word \"efficiency\", which has a very specific thermodynamic definition. The term coefficient of performance (COP) is used to describe the ratio of useful heat movement per work input. Most vapor-compression heat pumps use electrically powered motors for their work input.\n\nAccording to the US EPA, geothermal heat pumps can reduce energy consumption up to 44% compared with air-source heat pumps and up to 72% compared with electric resistance heating. The COP for heat pumps range from 3.2 to 4.5 for air source heat pumps to 4.2 to 5.2 for ground source heat pumps.\n\nWhen used for heating a building with an outside temperature of, for example, 10 °C, a typical air-source heat pump (ASHP) has a COP of 3 to 4, whereas an electrical resistance heater has a COP of 1.0. That is, one joule of electrical energy will cause a resistance heater to produce only one joule of useful heat, while under ideal conditions, one joule of electrical energy can cause a heat pump to move three or four joules of heat from a cooler place to a warmer place. Note that an air source heat pump is more efficient in hotter climates than cooler ones, so when the weather is much warmer the unit will perform with a higher COP (as it has a smaller temperature gap to bridge). When there is a wide temperature differential between the hot and cold reservoirs, the COP is lower (worse). In extreme cold weather the COP will go down to 1.0.\n\nOn the other hand, well designed ground-source heat pump (GSHP) systems benefit from the moderate temperature underground, as the ground acts naturally as a store of thermal energy. Their year-round COP is therefore normally in the range of 3.2 to 5.0.\n\nWhen there is a high temperature differential (e.g., when an air-source heat pump is used to heat a house with an outside temperature of, say, 0 °C (32 °F)), it takes more work to move the same amount of heat to indoors than on a milder day. Ultimately, due to Carnot efficiency limits, the heat pump's performance will decrease as the outdoor-to-indoor temperature difference increases (outside temperature gets colder), reaching a theoretical limit of 1.0 at −273 °C. In practice, a COP of 1.0 will typically be reached at an outdoor temperature around −18 °C (0 °F) for air source heat pumps.\n\nAlso, as the heat pump takes heat out of the air, some moisture in the outdoor air may condense and possibly freeze on the outdoor heat exchanger. The system must periodically melt this ice; this defrosting translates into an additional energy (electricity) expenditure. When it is extremely cold outside, it is simpler to heat using an alternative heat source (such as an electric resistance heater, oil furnace, or gas furnace) rather than to run an air-source heat pump. Also, avoiding the use of the heat pump during extremely cold weather translates into less wear on the machine's compressor.\n\nThe design of the evaporator and condenser heat exchangers is also very important to the overall efficiency of the heat pump. The heat exchange surface areas and the corresponding temperature differential (between the refrigerant and the air stream) directly affect the operating pressures and hence the work the compressor has to do in order to provide the same heating or cooling effect. Generally, the larger the heat exchanger, the lower the temperature differential and the more efficient the system becomes.\n\nHeat exchangers are expensive, requiring drilling for some heat-pump types or large spaces to be efficient, and the heat pump industry generally competes on price rather than efficiency. Heat pumps are already at a price disadvantage when it comes to initial investment (not long-term savings) compared to conventional heating solutions like boilers, so the drive towards more efficient heat pumps and air conditioners is often led by legislative measures on minimum efficiency standards. Electricity rates will also influence the attractiveness of heat pumps.\n\nIn cooling mode, a heat pump's operating performance is described in the US as its energy efficiency ratio (EER) or seasonal energy efficiency ratio (SEER), and both measures have units of BTU/(h·W) (1 BTU/(h·W) = 0.293 W/W). A larger EER number indicates better performance. The manufacturer's literature should provide both a COP to describe performance in heating mode, and an EER or SEER to describe performance in cooling mode. Actual performance varies, however, and depends on many factors such as installation details, temperature differences, site elevation, and maintenance.\n\nAs with any piece of equipment that depends on coils to transfer heat between air and a fluid, it is important for both the condenser and evaporator coils to be kept clean. If deposits of dust and other debris are allowed to accumulate on the coils, the efficiency of the unit (both in heating and cooling modes) will suffer.\n\nHeat pumps are more \"effective\" for heating than for cooling an interior space if the temperature differential is held equal. This is because the compressor's input energy is also converted to useful heat when in heating mode, and is discharged along with the transported heat via the condenser to the interior space. But for cooling, the condenser is normally outdoors, and the compressor's dissipated work (waste heat) must also be transported to outdoors using more input energy, rather than being put to a useful purpose. For the same reason, opening a food refrigerator or freezer has the net effect of heating up the room rather than cooling it, because its refrigeration cycle rejects heat to the indoor air. This heat includes the compressor's dissipated work as well as the heat removed from the inside of the appliance.\n\nThe COP for a heat pump in a heating or cooling application, with steady-state operation, is:\n\nwhere\n\nThe coefficient of performance (COP) increases as the temperature difference, or \"lift\", decreases between heat source and destination. The COP can be maximized at design time by choosing a heating system requiring only a low final water temperature (e.g. underfloor heating), and by choosing a heat source with a high average temperature (e.g. the ground). Domestic hot water (DHW) and conventional heating radiators require high water temperatures, reducing the COP that can be attained, and affecting the choice of heat pump technology.\n\nOne observation is that while current \"best practice\" heat pumps (ground source system, operating between 0 °C and 35 °C) have a typical COP around 4, no better than 5, the maximum achievable is 8.8 because of fundamental Carnot cycle limits. This means that in the coming decades, the energy efficiency of top-end heat pumps could roughly double. Cranking up efficiency requires the development of a better gas compressor, fitting HVAC machines with larger heat exchangers with slower gas flows, and solving internal lubrication problems resulting from slower gas flow.\n\nDepending on the working fluid, the expansion stage can be important also. Work done by the expanding fluid cools it and is available to replace some of the input power. (An evaporating liquid is cooled by free expansion through a small hole, but an ideal gas is not.)\n\nThe two main types of heat pumps are compression and absorption. Compression heat pumps operate on mechanical energy (typically driven by electricity), while absorption heat pumps may also run on heat as an energy source (from electricity or burnable fuels). An absorption heat pump may be fueled by natural gas or LP gas, for example. While the gas utilization efficiency in such a device, which is the ratio of the energy supplied to the energy consumed, may average only 1.5, that is better than a natural gas or LP gas furnace, which can only approach 1.\n\nBy definition, all heat sources for a heat pump must be colder in temperature than the space to be heated. Most commonly, heat pumps draw heat from the air (outside or inside air) or from the ground (groundwater or soil).\n\nThe heat drawn from ground-sourced systems is in most cases stored solar heat, and it should not be confused with direct geothermal heating, though the latter will contribute in some small measure to all heat in the ground. True geothermal heat, when used for heating, requires a circulation pump but no heat pump, since for this technology the ground temperature is higher than that of the space that is to be heated, so the technology relies only upon simple heat convection.\n\nOther heat sources for heat pumps include water; nearby streams and other natural water bodies have been used, and sometimes domestic waste water (via drain water heat recovery) which is often warmer than cold winter ambient temperatures (though still of lower temperature than the space to be heated).\n\nA number of sources have been used for the heat source for heating private and communal buildings.\n\n\nAir-air heat pumps, that extract heat from outside air and transfer this heat to inside air, are the most common type of heat pumps and the cheapest. These are similar to air conditioners operating in reverse. Air-water heat pumps are otherwise similar to air-air heat pumps, but they transfer the extracted heat into a water heating circuit, floor heating being the most efficient, and they can also transfer heat into a domestic hot water tank for use in showers and hot water taps of the building. However, ground-water heat pumps are more efficient than air-water heat pumps, and therefore they are often the better choice for providing heat for the floor heating and domestic hot water systems.\n\nAir source heat pumps are relatively easy and inexpensive to install and have therefore historically been the most widely used heat pump type. However, they suffer limitations due to their use of the outside air as a heat source. The higher temperature differential during periods of extreme cold leads to declining efficiency. In mild weather, COP may be around 4.0, while at temperatures below around 0 °C (32 °F) an air-source heat pump may still achieve a COP of 2.5. The average COP over seasonal variation is typically 2.5-2.8, with exceptional models able to exceed this in mild climates.\n\nThe heating output of low temperature optimized heat pumps (and hence their energy efficiency) still declines dramatically as the temperature drops, but the threshold at which the decline starts is lower than conventional pumps, as shown in the following table (temperatures are approximate and may vary by manufacturer and model):\n\n\nGround-source heat pumps, also called geothermal heat pumps, typically have higher efficiencies than air-source heat pumps. This is because they draw heat from the ground or groundwater which is at a relatively constant temperature all year round below a depth of about 30 feet (9 m). This means that the temperature differential is lower, leading to higher efficiency. Well maintained ground-source heat pumps typically have COPs of 4.0 at the beginning of the heating season, with lower seasonal COPs of around 3.0 as heat is drawn from the ground. The tradeoff for this improved performance is that a ground-source heat pump is more expensive to install, due to the need for the drilling of boreholes for vertical placement of heat exchanger piping or the digging of trenches for horizontal placement of the piping that carries the heat exchange fluid (water with a little antifreeze).\n\nWhen compared, groundwater heat pumps are generally more efficient than heat pumps using heat from the soil. Closed loop soil or ground heat exchangers tend to accumulate cold if the ground loop is undersized. This can be a significant problem if nearby ground water is stagnant or the soil lacks thermal conductivity, and the overall system has been designed to be just big enough to handle a \"typical worst case\" cold spell, or is simply undersized for the load. One way to fix cold accumulation in the ground heat exchanger loop is to use ground water to cool the floors of the building on hot days, thereby transferring heat from the dwelling into the ground loop. There are several other methods for replenishing a low temperature ground loop; one way is to make large solar collectors, for instance by putting plastic pipes just under the roof, or by putting coils of black polyethylene pipes under glass on the roof, or by piping the tarmac of the parking lot. A further solution is to ensure ground collector arrays are correctly sized, by ensuring soil thermal properties and thermal conductivity are correctly measured and integrated into the design.\n\n\n\nHybrid (or twin source) heat pumps: when outdoor air is above 4 to 8 Celsius, (40-50 Fahrenheit, depending on ground water temperature) they use air; when air is colder, they use the ground source. These twin source systems can also store summer heat, by running ground source water through the air exchanger or through the building heater-exchanger, even when the heat pump itself is not running. This has dual advantage: it functions as a low running cost for air cooling, and (if ground water is relatively stagnant) it cranks up the temperature of the ground source, which improves the energy efficiency of the heat pump system by roughly 4% for each degree in temperature rise of the ground source.\n\nThe air/water-brine/water heat pump is a hybrid heat pump, developed in Rostock, Germany, that uses only renewable energy sources. Unlike other hybrid systems, which usually combine both conventional and renewable energy sources, it combines air and geothermal heat in one compact device. The air/water-brine/water heat pump has two evaporators — an outside air evaporator and a brine evaporator — both connected to the heat pump cycle. This allows use of the most economical heating source for the current external conditions (for example, air temperature). The unit automatically selects the most efficient operating mode — air or geothermal heat, or both together. The process is controlled by a control unit, which processes the large amounts of data delivered by the complex heating system.\n\nThe control unit comprises two controllers, one for the air heat cycle and one for the geothermal circulation, in one device. All components communicate over a common bus to ensure they interact to enhance the efficiency of the hybrid heating system. The German Patent and Trade Mark Office in Munich granted the air/water-brine/water heat pump a patent in 2008, under the title “Heat pump and method for controlling the source inlet temperature to the heat pump”. This hybrid heat pump can be combined with a solar thermal system or with an ice-storage. It trades and is marketed under the name \"ThermSelect\". In the United Kingdom, \"ThermSelect\" won the 2013 Commercial Heating Product of the Year award of the HVR Awards for Excellence, organised by \"Heating and Ventilating Review\", an industry magazine.\n\nA solar-assisted heat pump is a machine that represents the integration of a heat pump and thermal solar panels in a single integrated system. Typically these two technologies are used separately (or only placing them in parallel) to produce hot water. In this system the solar thermal panel performs the function of the low temperature heat source and the heat produced is used to feed the heat pump's evaporator. The goal of this system is to get high COP and then produce energy in a more efficient and less expensive way.\n\nIn 1881, the German physicist Emil Warburg put a block of iron into a strong magnetic field and found that it increased very slightly in temperature. Some commercial ventures to implement this technology are underway, claiming to cut energy consumption by 40% compared to current domestic refrigerators. The process works as follows: Powdered gadolinium is moved into a magnetic field, heating the material by 2 to 5 °C (4 to 9 °F). The heat is removed by a circulating fluid. The material is then moved out of the magnetic field, reducing its temperature below its starting temperature.\n\nSolid state heat pumps using the thermoelectric effect have improved over time to the point where they are useful for certain refrigeration tasks. Thermoelectric (Peltier) heat pumps are generally only around 10-15% as efficient as the ideal refrigerator (Carnot cycle), compared with 40–60% achieved by conventional compression cycle systems (reverse Rankine systems using compression/expansion); however, this area of technology is currently the subject of active research in materials science.\nA reason why this is popular is because it has a \"long lifetime\" as there are no moving parts and it does not use potentially hazardous refrigerants.\n\nNear-solid-state heat pumps using thermoacoustics are commonly used in cryogenic laboratories.\n\n"}
{"id": "22584796", "url": "https://en.wikipedia.org/wiki?curid=22584796", "title": "Hotbed", "text": "Hotbed\n\nA hotbed is a biological term for an area of decaying organic matter that is warmer than its surroundings. The heat gradient is generated by the decomposition of organic substituents within the pile by microorganism metabolization.\n\nA hotbed covered with a small glass cover (also called a hotbox) is used as a small version of a hothouse (heated greenhouse). Oftentimes, this bed is made of manure from animals such as horses. These animals pass undigested plant cellulose in their droppings, creating a good environment for microorganisms to come and break down the cellulose and create a hotbed. \n\nSome egg-laying animals, such as the brush turkey, make or use hotbeds to incubate their eggs.\n\nBy extension, the term \"hotbed\" is used metaphorically to describe an environment that is ideal for the growth or development of something, especially of something undesirable.\n"}
{"id": "743012", "url": "https://en.wikipedia.org/wiki?curid=743012", "title": "Hydrotherapy", "text": "Hydrotherapy\n\nHydrotherapy, formerly called hydropathy and also called water cure, is a part of alternative medicine (particularly naturopathy), occupational therapy, and physiotherapy, that involves the use of water for pain relief and treatment. The term encompasses a broad range of approaches and therapeutic methods that take advantage of the physical properties of water, such as temperature and pressure, for therapeutic purposes, to stimulate blood circulation and treat the symptoms of certain diseases.\n\nVarious therapies used in the present-day hydrotherapy employ water jets, underwater massage and mineral baths (e.g. balneotherapy, Iodine-Grine therapy, Kneipp treatments, Scotch hose, Swiss shower, thalassotherapy) or whirlpool bath, hot Roman bath, hot tub, Jacuzzi, cold plunge and mineral bath.\nWater therapy may be restricted to use as aquatic therapy, a form of physical therapy, and as a cleansing agent. However, it is also used as a medium for delivery of heat and cold to the body, which has long been the basis for its application. Hydrotherapy involves a range of methods and techniques, many of which use water as a medium to facilitate thermoregulatory reactions for therapeutic benefit.\n\nHydrotherapy is used as an adjunct to therapy, including in nursing, where its use is now long established. It continues to be widely used for burn treatment, although shower-based hydrotherapy techniques have been increasingly used in preference to full-immersion methods, partly for the ease of cleaning the equipment and reducing infections due to contamination. When removal of tissue is necessary for the treatment of wounds, hydrotherapy which performs selective mechanical debridement can be used. Examples of this include directed wound irrigation and therapeutic irrigation with suction.\n\nThe appliances and arrangements by means of which heat and cold are brought to bear are (a) packings, hot and cold, general and local, sweating and cooling; (b) hot air and steam baths; (c) general baths, of hot water and cold; (d) sitz (sitting), spinal, head and foot baths; (e) bandages (or compresses), wet and dry; also (f) fomentations and poultices, hot and cold, , , rubbings and water potations, hot and cold.\n\nHydrotherapy which involves submerging all or part of the body in water can involve several types of equipment:\n\nWhirling water movement, provided by mechanical pumps, has been used in water tanks since at least the 1940s. Similar technologies have been marketed for recreational use under the terms \"hot tub\" or \"spa\".\n\nIn some cases baths with whirlpool water flow aren't used to manage wounds because a whirlpool will not selectively target the tissue to be removed and can damage all tissue. Whirlpools also create an unwanted risk of bacterial infection, can damage fragile body tissue, and in the case of treating arms and legs, bring risk of complications from edema.\n\nThe therapeutic use of water has been recorded in ancient Egyptian, Greek and Roman civilizations. Egyptian royalty bathed with essential oils and flowers, while Romans had communal public baths for their citizens. Hippocrates prescribed bathing in spring water for sickness. Other cultures noted for a long history of hydrotherapy include China and Japan, the latter being centred primarily around Japanese hot springs or “onsen”. Many such histories predate the Roman thermae.\n\nAn important note on the growth of hydropathy is that it started to become prominent as traditional medical practice was becoming more professional in terms of how doctors operated, alienating many patients, as they felt that the medical encounter was becoming less personalized, and the more scientific the medical language became, the less that they could easily understand. Hydropathy was a return to a treatment that was spiritual and natural, making it more palatable to those who felt uncomfortable with the direction that traditional medicine was taking.\n\nThe general idea behind hydropathy during the 1800s was to be able to induce something called a crisis. The thinking was that water invaded any cracks, wounds, or imperfections in the skin, which were filled with impure fluids. health was considered to be the natural state of the body, and filling these spaces with pure water, would flush the impurities out, which would rise to the surface of the skin, producing pus. The event of this pus emerging was called a crisis, and was achieved through a multitude of methods. These methods included techniques such as sweating, the plunging bath, the half bath, the head bath, the sitting bath, and the douche bath. All of these were ways to gently expose the patient to cold water in different ways.\n\nTwo English works on the medical uses of water were published in the 18th century that inaugurated the new fashion for hydrotherapy. One of these was by Sir John Floyer, a physician of Lichfield, who, struck by the remedial use of certain springs by the neighbouring peasantry, investigated the history of cold bathing and published a book on the subject in 1702. The book ran through six editions within a few years and the translation of this book into German was largely drawn upon by Dr J. S. Hahn of Silesia as the basis for his book called \"On the Healing Virtues of Cold Water, Inwardly and Outwardly Applied, as Proved by Experience\", published in 1738.\n\nThe other work was a 1797 publication by Dr James Currie of Liverpool on the use of hot and cold water in the treatment of fever and other illness, with a fourth edition published in 1805, not long before his death. It was also translated into German by Michaelis (1801) and Hegewisch (1807). It was highly popular and first placed the subject on a scientific basis. Hahn's writings had meanwhile created much enthusiasm among his countrymen, societies having been formed everywhere to promote the medicinal and dietetic use of water; and in 1804 Professor E.F.C. Oertel of Anspach republished them and quickened the popular movement by unqualified commendation of water drinking as a remedy for all diseases.\n\nVincent Preissnitz was the son of a peasant farmer who, as a young child, observed a wounded deer bathing a wound in a pond near his home. Over the course of several days, he would see this deer return and eventually the wound was healed. Later as a teenager, Preissnitz was attending to a horse cart, when the cart ran him over, breaking three of his ribs. A physician told him that they would never heal. Preissnitz decided to try his own hand at healing himself, and wrapped his wounds with damp bandages. By daily changing his bandages and drinking large quantities of water, after about a year, his broken ribs had been cured. Preissnitz quickly gained fame in his hometown and became the consulted physician.\n\nLater in life, Preissnitz became the head of a hydropathy clinic in Gräfenberg in 1826. He was extremely successful and by 1840, he had 1600 patients in his clinic including many fellow physicians, as well as important political figures such as nobles and prominent military officials. Treatment length at Preissnitz's clinic varied. Much of his theory was about inducing the above mentioned crisis, which could happen quickly, or could occur after three to four years. In accordance with the simplistic nature of hydropathy, a large part of the treatment was based on living a simple lifestyle. These lifestyle adjustments included dietary changes such as eating only very coarse food, such as jerky and bread, and of course drinking large quantities of water. Preissnitz's treatments also included a great deal of less strenuous exercise, mostly including walking. Ultimately, Preissnitz's clinic was extremely successful, and he gained fame across the western world. His practice even influenced the hydropathy that took root overseas in America.\n\nSebastian Kneipp was born in Germany and he considered his own role in hydropathy to be that of continuing Preissnitz's work. Kneipp's own practice of hydropathy was even gentler than the norm. He believed that typical hydropathic practices deployed were \"too violent or too frequent\" and he expressed concern that such techniques would cause emotional or physical trauma to the patient. Kneipp's practice was more all encompassing than Preissnitz's, and his practice involved not only curing the patients' physical woes, but emotional and mental as well. \n\nKneipp introduced four additional principles to the therapy: medicinal herbs, massages, balanced nutrition, and \"regulative therapy to seek inner balance\". Kneipp had a very simple view of an already simple practice. For him, hydropathy's primary goals were strengthening the constitution and removing poisons and toxins in the body. These basic interpretations of how hydropathy worked hinted at his complete lack of medical training. Kneipp did have, however, a very successful medical practice in spite of, perhaps even because of, his lack of medical training. As mentioned above, some patients were beginning to feel uncomfortable with traditional doctors because of the elitism of the medical profession. The new terms and techniques that doctors were using were difficult for the average person to understand. Having no formal training, all of his instructions and published works are described in easy to understand language and would have seemed very appealing to a patient who was displeased with the direction traditional medicine was taking.\n\nA significant factor in the popular revival of hydrotherapy was that it could be practised relatively cheaply at home. The growth of hydrotherapy (or 'hydropathy' to use the name of the time), was thus partly derived from two interacting spheres: \"the hydro and the home\".\n\nHydrotherapy as a formal medical tool dates from about 1829 when Vincenz Priessnitz (1799–1851), a farmer of Gräfenberg in Silesia, then part of the Austrian Empire, began his public career in the paternal homestead, extended so as to accommodate the increasing numbers attracted by the fame of his cures.\n\nAt Gräfenberg, to which the fame of Priessnitz drew people of every rank and many countries, medical men were conspicuous by their numbers, some being attracted by curiosity, others by the desire of knowledge, but the majority by the hope of cure for ailments which had as yet proved incurable. Many records of experiences at Gräfenberg were published, all more or less favorable to the claims of Priessnitz, and some enthusiastic in their estimate of his genius and penetration.\n\nCaptain R. T. Claridge was responsible for introducing and promoting hydropathy in Britain, first in London in 1842, then with lecture tours in Ireland and Scotland in 1843. His 10-week tour in Ireland included Limerick, Cork, Wexford, Dublin and Belfast, over June, July and August 1843, with two subsequent lectures in Glasgow.\n\nSome other Englishmen preceded Claridge to Graefenberg, although not many. One of these was Dr. James Wilson, who himself, along with Dr James Manby Gully, established and operated a water cure establishment at Malvern in 1842. In 1843, Wilson and Gully published a comparison of the efficacy of the water-cure with drug treatments, including accounts of some cases treated at Malvern, combined with a prospectus of their Water Cure Establishment. Then in 1846 Gully published \"The Water Cure in Chronic Disease\", further describing the treatments available at the clinic.\n\nThe fame of the water-cure establishment grew, and Gully and Wilson became well-known national figures. Two more clinics were opened at Malvern. Famous patients included Charles Darwin, Charles Dickens, Thomas Carlyle, Florence Nightingale, Lord Tennyson and Samuel Wilberforce. With his fame he also attracted criticism:\nSir Charles Hastings, a physician and founder of the British Medical Association, was a forthright critic of hydropathy, and Gully in particular.\nFrom the 1840s, hydropathics were established across Britain. Initially, many of these were small institutions, catering to at most dozens of patients. By the later nineteenth century the typical hydropathic establishment had evolved into a more substantial undertaking, with thousands of patients treated annually for weeks at a time in a large purpose-built building with lavish facilities – baths, recreation rooms and the like – under the supervision of fully trained and qualified medical practitioners and staff.\nIn Germany, France and America, and in Malvern, England, hydropathic establishments multiplied with great rapidity. Antagonism ran high between the old practice and the new. Unsparing condemnation was heaped by each on the other; and a legal prosecution, leading to a royal commission of inquiry, served but to make Priessnitz and his system stand higher in public estimation.\n\nIncreasing popularity soon diminished caution whether the new method would help minor ailments and be of benefit to the more seriously injured. Hydropathists occupied themselves mainly with studying chronic invalids well able to bear a rigorous regimen and the severities of unrestricted crisis. The need of a radical adaptation to the former class was first adequately recognized by John Smedley, a manufacturer of Derbyshire, who, impressed in his own person with the severities as well as the benefits of the cold water cure, practised among his workpeople a milder form of hydropathy, and began about 1852 a new era in its history, founding at Matlock a counterpart of the establishment at Gräfenberg.\n\nErnst Brand (1827–1897) of Berlin, Raljen and Theodor von Jürgensen of Kiel, and Karl Liebermeister of Basel, between 1860 and 1870, employed the cooling bath in abdominal typhus with striking results, and led to its introduction to England by Dr Wilson Fox. In the Franco-German War the cooling bath was largely employed, in conjunction frequently with quinine; and it was used in the treatment of hyperpyrexia.\n\nHydrotherapy, especially as promoted during the height of its Victorian revival, has often been associated with the use of cold water, as evidenced by many titles from that era. However, not all therapists limited their practice of hydrotherapy to cold water, even during the height of this popular revival.\n\nThe specific use of heat was however often associated with the Turkish bath. This was introduced by David Urquhart into England on his return from the East in the 1850s, and ardently adopted by Richard Barter. The Turkish bath became a public institution, and, with the morning tub and the general practice of water drinking, is the most noteworthy of the many contributions by hydropathy to public health.\n\nThe first U.S. hydropathic facilities were established by Joel Shew and R. T. Trall in the 1840s. Dr Charles Munde also established early hydrotherapy facilities in the 1850s. Trall also co-edited the \"Water Cure Journal\".\n\nBy 1850, it was said that \"there are probably more than one hundred\" facilities, along with numerous books and periodicals, including the New York \"Water Cure Journal\", which had \"attained an extent of circulation equalled by few monthlies in the world\". By 1855, there were attempts by some to weigh the evidence of treatments in vogue at that time.\n\nFollowing the introduction of hydrotherapy to the U.S., John Harvey Kellogg employed it at Battle Creek Sanitarium, which opened in 1866, where he strove to improve the scientific foundation for hydrotherapy. Other notable hydropathic centers of the era included the Cleveland Water Cure Establishment, founded in 1848, which operated successfully for two decades, before being sold to an organization which transformed it into an orphanage.\n\nAt its height, there were over 200 water-cure establishments in the United States, most located in the northeast. Few of these lasted into the postbellum years, although some survived into the 20th century including institutions in Scott (Cortland County), Elmira, Clifton Springs and Dansville. While none were located in Jefferson County, the Oswego Water Cure operated in the city of Oswego.\nIn November 1881, the \"British Medical Journal\" noted that hydropathy was a specific instance, or \"particular case\", of general principles of thermodynamics. That is, \"the application of heat and cold in general\", as it applies to physiology, mediated by hydropathy. In 1883, another writer stated \"Not, be it observed, that hydropathy is a water treatment after all, but that water is the medium for the application of heat and cold to the body\".\n\nHydrotherapy was used to treat people with mental illness in the 19th and 20th centuries and before World War II, various forms of hydrotherapy were used to treat alcoholism. The basic text of the Alcoholics Anonymous fellowship, \"Alcoholics Anonymous\", reports that A.A. co-founder Bill Wilson was treated by hydrotherapy for his alcoholism in the early 1930s.\n\nA subset of cryotherapy involves cold water immersion or ice baths, used by physical therapists, sports medicine facilities and rehab clinics. Proponents assert that it results in improved return of blood flow and byproducts of cellular breakdown to the lymphatic system and more efficient recycling.\n\nAlternating the temperatures, either in a shower or complementary tanks, combines the use of hot and cold in the same session. Proponents claim improvement in circulatory system and lymphatic drainage. Experimental evidence suggests that contrast hydrotherapy helps to reduce injury in the acute stages by stimulating blood flow and reducing swelling.\n\nThe growth of hydrotherapy, and various forms of hydropathic establishments, resulted in a form of tourism, both in the UK, and in Europe. At least one book listed English, Scottish, Irish and European establishments suitable for each specific malady, while another focused primarily on German spas and hydropathic establishments, but including other areas. While many bathing establishments were open all year round, doctors advised patients not to go before May, \"nor to remain after October. English visitors rather prefer cold weather, and they often arrive for the baths in May, and return again in September. Americans come during the whole season, but prefer summer. The most fashionable and crowded time is during July and August\". In Europe, interest in various forms of hydrotherapy and spa tourism continued unabated through the 19th century and into the 20th century, where \"in France, Italy and Germany, several million people spend time each year at a spa.\" In 1891, when Mark Twain toured Europe and discovered that a bath of spring water at Aix-les-Bains soothed his rheumatism, he described the experience as \"so enjoyable that if I hadn't had a disease I would have borrowed one just to have a pretext for going on\".\n\nThis was not the first time such forms of spa tourism had been popular in Europe and the U.K. Indeed,\n\nin Europe, the application of water in the treatment of fevers and other maladies had, since the seventeenth century, been consistently promoted by a number of medical writers. In the eighteenth century, taking to the waters became a fashionable pastime for the wealthy classes who decamped to resorts around Britain and Europe to cure the ills of over-consumption. In the main, treatment in the heyday of the British spa consisted of sense and sociability: promenading, bathing, and the repetitive quaffing of foul-tasting mineral waters.\nA hydropathic establishment is a place where people receive hydropathic treatment. They are commonly built in spa towns, where mineral-rich or hot water occurs naturally.\n\nSeveral hydropathic institutions wholly transferred their operations away from therapeutic purposes to become tourist hotels in the late 20th century whilst retaining the name 'Hydro'. There are several prominent examples in Scotland at Crieff, Peebles and Seamill amongst others.\n\nCanine hydrotherapy is a form of hydrotherapy directed at the treatment of chronic conditions, post-operative recovery, and pre-operative or general fitness in dogs.\n\n\na. While the second sense, of water as a form of torture is documented back to at least the 15th century, the first use of the term \"water cure\" as a torture is indirectly dated to around 1898, by U.S. soldiers in the Spanish–American War, after the term had been introduced to America in the mid-19th century in the therapeutic sense, which was in widespread use. Indeed, while the torture sense of \"water cure\" was by 1900–1902 established in the American army, with a conscious sense of irony, this sense was not in widespread use. \"Webster's\" 1913 dictionary cited only the therapeutic sense, \"water cure\" being synonymous with \"hydropathy\", the term by which hydrotherapy was known in the 19th century and early 20th century.\n\nThe late 19th century expropriation of the term \"water cure\", already in use in the therapeutic sense, to denote the polar opposite of therapy, namely torture, has the hallmark of arising in the sense of irony. This would be in keeping with some of the reactions to water cure therapy and its promotion, which included not only criticism, but also parody and satire.\n\n"}
{"id": "5895015", "url": "https://en.wikipedia.org/wiki?curid=5895015", "title": "Inertial fusion power plant", "text": "Inertial fusion power plant\n\nAn inertial fusion power plant is intended to produce electric power by use of inertial confinement fusion techniques on an industrial scale. This type of power plant is still in a research phase.\n\nTwo established options for possible medium-term implementation of fusion energy production are magnetic confinement, being used in the ITER international project, and laser-based inertial confinement, as used in the French Laser Mégajoule and in the American NIF. Inertial confinement fusion (ICF), including heavy-ion inertial fusion (HIF), has been proposed as a possible additional means of implementing a fusion power plant.\n\nThe operation of an IFE reactor is in some ways analogous to the operation of the four stroke cycle of a petrol engine:\n\nTo allow such an operation, an inertial fusion reactor is made of several subsets:\n\n\nSeveral projects of inertial fusion power plants have been proposed, including power production plans based on the following experimental devices, either in operation or under construction:\nOnly the US and French projects are based on z-pinch confinement; others are based on laser confinement techniques.\n\nLivermore's IFE (LIFE) project was cancelled in January 2014.\n\nAs of June 2006, Megajoule and NIF lasers were not yet in complete service. Inertial confinement and laser confinement fusion experiments had not gone beyond the first phase. Around 2010, NIF and Megajoule were planned for completion.\n\nIn the magnetic confinement field, the 2nd phase corresponds to the objectives of ITER, the 3rd to these of its follower DEMO, in 20 to 30 years, and the 4th to those of a possible PROTO, in 40 to 50 years. The various phases of such a project are the following:\n\n\n"}
{"id": "142915", "url": "https://en.wikipedia.org/wiki?curid=142915", "title": "Karlu Karlu / Devils Marbles Conservation Reserve", "text": "Karlu Karlu / Devils Marbles Conservation Reserve\n\nKarlu Karlu / Devils Marbles Conservation Reserve is a protected area located south of Tennant Creek, Northern Territory, Australia and north of Alice Springs. The nearest settlement is the small town of Wauchope located to the south.\n\nThe Devils Marbles are of great cultural and spiritual significance to the traditional Aboriginal owners (Ree) of the land, and the reserve protects one of the oldest religious sites in the world as well as the natural rock formations found there. \"Karlu Karlu\" is the local Aboriginal term for both the rock features and the surrounding area. The Aboriginal term translates as \"round boulders\" and refers to the large boulders found mainly in the western side of the reserve. The origin of the English name for the same boulders is the following quote:\n\nThe area was originally named \"Devils Marbles Reserve\" in October 1961. The name was changed to \"Devil's Marbles Conservation Reserve\" on 21 September 1979 under the \"Territory Parks and Wildlife Conservation Act\". In 1982, almost the entire reserve was registered as a sacred site by the Aboriginal Areas Protection Authority.\n\nOwnership of Karlu Karlu/Devils Marbles was officially passed from the Parks and Wildlife Service of the Northern Territory back to the Traditional Owners at a ceremony held on the reserve in October 2008. The reserve is now leased back to the Parks Service under a 99-year lease and the site is jointly managed by rangers and Traditional Owners. Visitor access has not been affected. On 6 July 2011, the name \"Karlu Karlu / Devils Marbles Conservation Reserve\" was assigned, corresponding with the joint management structure.\n\nThe \"Devils Marbles\" are large granitic boulders that form the exposed top layer of an extensive and mostly underground granite formation. The natural processes of weathering and erosion have created the various shapes of the boulders. Some of the boulders are naturally but precariously balanced atop one another or on larger rock formations, while others have been split cleanly down the middle by natural forces. The boulders are situated in a wide and shallow desert valley, and are found in scattered groups mainly in the western side of the reserve. A short access road leads directly into the boulder fields from the Stuart Highway.\n\nBeing one of the most widely recognised symbols of Australia’s outback, the Devils Marbles are also one of the most visited reserves in the Northern Territory. The Parks and Wildlife Service recorded 96,172 visitors in 2007. The visitation count increased to 137,500 by 2012. The peak season is in the cooler months from May to August. The reserve is one of the main tourist attractions in the Barkly region since it is easily seen and accessed just off the Stuart Highway, the major north/south road connecting Darwin and Alice Springs, and points further south.\n\nThe Devils Marbles are accessible all year round with a network of pathways, information boards and a basic camping area. Rangers offer a program of live events at the site as part of their \"Territory Parks Alive\" program between May and September each year.\n\nKarlu Karlu are culturally and spiritually significant objects of the local Aboriginal people. Most of the conservation reserve is a Registered Sacred Site, protected under the Northern Territory Aboriginal Sacred Sites Act. Although Karlu Karlu is within country originally belonging to the Alyawarre people, all of the other local Aboriginal groups, which include the Kaytetye, Warumungu and Warlpiri people, also have spiritual connections and responsibilities for the area.\n\nAccounts of local Aboriginal people believing the boulders to be eggs of the mythical Rainbow Serpent are incorrect. In reality, a number of traditional Dreaming stories (none of which are about serpents) have Karlu Karlu as their setting, hence its great importance as a sacred site. These stories are alive and well and are passed on from generation to generation of Traditional Owners. Only a handful of stories are considered suitable to tell to uninitiated visitors.\n\nOne of the main Dreaming stories for the area which can be told to the public relates to how Karlu Karlu was created. This tradition tells of \"Arrange\", the Devil Man, who came from a hill nearby and travelled through the area. Whilst walking along, Arrange made a hair-string belt which is a kind of traditional adornment worn only by initiated men. As he was twirling the hair to make strings, he dropped clusters of hair on the ground.\n\nThe clusters turned into the big red boulders at Karlu Karlu that have become so famous today. On his way back to his hill, Arrange spat on the ground. His spit turned into the granite boulders in the central part of the reserve. Arrange finally returned to his place of origin, a hill called \"Ayleparrarntenhe\" where he remains today.\n\nThe Devils Marbles constitute a degraded nubbin. The formation is made of granite and are part of the top layer of a formation which penetrates the ground from below, like little geological islands in the desert, surrounded by large amounts of sandstone. The granite was formed millions of years ago as a result of the hardening of magma within the Earth's crust. Thick layers of sandstone on top of the granite exerted extreme downward pressure on the granite. After some time, tectonic forces caused folding of the Earth's crust in the area, which lifted the granite and fractured the sandstone, allowing the granite to come closer to the surface. As the pressure diminished, the granite expanded causing cracks to form, and then the larger formations began to separate into big, square blocks.\n\nThe next phase of the formation of the Marbles started when the blocks were exposed to water. The surface of the blocks began to decay under the influence of the water and a layer of loose material surrounded the individual blocks. When the blocks came to the surface completely, the loose material was eroded away by water and wind.\n\nThe rounding of the granite blocks is a result of chemical and physical (also called mechanical) weathering. Chemical processes cause the surface of the blocks to expand and contract, resulting in thin layers of rock coming off the boulder in a process called exfoliation. This process rounds the granite block because the chemical processes have more effect on areas with edges. The rock begins to look like it is made of layers like an onion. Only the outer few centimetres are affected by chemical weathering in a process called spheroidal weathering.\n\nThe boulders are affected more deeply by the extreme temperature differences between day and night in the arid desert region where the reserve is located. During daylight hours the rocks expand slightly and after nightfall they contract slightly, repeating the process every 24 hours. These repeated cycles of expansion and contraction, called thermal stress weathering, create cracks which sometimes go so deep that the boulder is completely split in half.\n\nOne of the boulders was removed from the reserve in 1952 and taken to Alice Springs to form a memorial to John Flynn, the founder of the Royal Flying Doctor Service. The boulder was chosen as a symbol of his link to the outback, but it became the source of a lengthy controversy. The boulder had unwittingly been removed from a sacred site of the Aboriginal women of the area. Eventually, after more than 45 years of negotiations, a boulder swap was arranged and the sacred boulder was removed from the grave and returned to its original place on 4 September 1999. The grave is now marked with a similar but non-sacred boulder donated by the Arrernte people.\n\n\n"}
{"id": "8103336", "url": "https://en.wikipedia.org/wiki?curid=8103336", "title": "Light leak", "text": "Light leak\n\nA light leak is a hole or gap in the body of a camera, or other optical instrument, where light is able to \"leak\" into the normally light-tight chamber, exposing the film or sensor with extra light. This light is diffuse, although parts within the camera may cast shadows or reflect it in a particular way. For most purposes this is considered a problem. Within the lomography movement it is seen as a positive effect, giving photos character.\n\nOne frequent source of light leaks in 35 mm cameras is around the film door due to degrading foam. Replacing the foam is a simple matter. Medium format system cameras or large format cameras may have leaks between their various interchangeable parts or in old leather bellows. Electrical tape is often used to repair light leaks in these cases.\n\nA light leak, considered as a problem, is a kind of stray light. It is possible to have a \"virtual\" light leak in spectral regions, like portions of the IR spectrum at room temperature, where surfaces inside the system emit significant amounts of radiation.\n\nThey can be created and emulated in digital photography and videography, either during production or after. In the first instance, the photographer or videographer removes the camera lens while photographing or recording and overloads the image sensor. This is usually used to create leaks which can then be overlaid onto another image or video. Or they can be created entirely digitally, with common photo-editing software packages such as Adobe Photoshop, and overlaid into the image.\n\n"}
{"id": "36973916", "url": "https://en.wikipedia.org/wiki?curid=36973916", "title": "Lignum nephriticum", "text": "Lignum nephriticum\n\nLignum nephriticum (Latin for \"kidney wood\") is a traditional diuretic that was derived from the wood of two tree species, the narra (\"Pterocarpus indicus\") and the Mexican kidneywood (\"Eysenhardtia polystachya \"). The wood is capable of turning the color of water it comes in contact with into beautiful opalescent hues that change depending on light and angle, the earliest known record of the phenomenon of fluorescence. Due to this strange property, it became well known in Europe from the 16th to the early 18th-century Europe. Cups made from \"lignum nephriticum\" were given as gifts to royalty. Water drunk from such cups, as well as imported powders and extracts from \"lignum nephriticum\", were thought to have great medicinal properties.\n\nThe \"lignum nephriticum\" derived from Mexican kidneywood was known as the coatli, coatl, or cuatl (\"snake water\") or tlapalezpatli (\"blood-tincture medicine\") in the Nahuatl language. It was traditionally used by the Aztec people as a diuretic prior to European contact. Similarly, the \"lignum nephriticum\" cups made from narra wood were part of the native industry of the Philippines before the arrival of the Spanish. The cups were manufactured in southern Luzon, particularly in the Naga region. The name of which was derived from the abundance of the narra trees, which was known as \"naga\" in the Bikol language (literally \"serpent\" or \"dragon\").\n\nThe first known description of the medicine appears in the \"Historia general de las cosas de la Nueva España\" (1560–1564) by the Spanish Franciscan missionary and ethnographer Bernardino de Sahagún. In the most famous surviving manuscripts of the work, the Florentine Codex, Sahagún called it by its Nahuatl name, \"coatli\", given by Aztec healers. He described its unusual property of turning the color of water that comes in contact with it to bright blue: \"\"...patli, yoan aqujxtiloni, matlatic iniayo axixpatli\" [...it is a medicine, and makes the water of blue color, its juice is medicinal for the urine]\".\n\nThe Spanish physician and botanist Nicolás Monardes also independently described the medicine in his 1565 work \"Historia medicinal de las cosas que se traen de nuestras Indias Occidentales\". He is the origin of the name \"lignum nephriticum\", due to the use of the wood to treat liver and kidney ailments in New Spain. He described the wood as white in color. It was prepared by being sliced into very thin chips. The chips were then placed in clear spring water. After about half an hour, the water starts to turn a very pale blue, turning bluer as time passes. The water is then drunk as is or mixed with wine. He observed that despite the change in color, the wood itself imparts no taste to the water.\n\nIn 1570, Francisco Hernández de Toledo, the court physician of King Philip II of Spain, led what is considered the first scientific expedition to the Americas. When he returned to Spain in 1577, he gave testimony of the medicinal properties of \"lignum nephriticum\", as described by Monardes. However, he expressed uncertainty as to its origin, stating that while he was told the source plant was a shrub, he had personally also witnessed specimens that reached the size of very large trees.\nIn 1646, Athanasius Kircher, a German Jesuit scholar residing in Rome, published an account of his experiments on \"lignum nephriticum\" in his work \"Ars Magna Lucis et Umbræ\". He conducted his experiments on a cup given to him as a gift from Jesuit missionaries in Mexico. He commented that while previous authors only described the color of the water mixed with \"lignum nephriticum\" as blue, his own experiments actually showed that the wood turned the water into all kinds of colors, depending on the light. He later presented the cup to the Holy Roman Emperor, Ferdinand III.\n\nA second cup was described by in 1650 by the Swiss botanist Johann Bauhin in his great work \"Historia plantarum universalis\". He had received it under the name \"palum indianum\" from a colleague. Unlike the wood in Monardes' account, the wood the cup was made from was reddish in color. It was a handspan in diameter, about , and adorned with variegated lines. Shavings from the same wood were included with the cup. Bauhin observed that when water was poured into the cup with the wood shavings, the water shortly turned into \"a wonderful blue and yellow color, and when held up against the light beautifully resembled the varying color of the opal, giving forth reflections, as in that gem, of fiery yellow, bright red, glowing purple, and sea green most wonderful to behold.\" Bauhin believed that the wood was taken from a species of ash (\"Fraxinus\").\n\nIn 1664, Robert Boyle explained the phenomenon to be dependent on pH. In an unpublished paper in 1665, Sir Isaac Newton first mentioned the cups in the paper \"Of Colours\". He later also mentioned in 1672, in his theories of light and color. However, the botanical origin of wood used in \"lignum nephriticum\" was eventually lost in the late 18th century.\n\nIn 1915, the source of the wood was rediscovered by the American botanist William Edwin Safford. He deduced that \"lignum nephriticum\" actually came from two species of trees that became confused as one. He identified the original traditional remedy described by Monardes as Mexican kidneywood, a native of Mexico, while the cups which became famous in Europe were originally carved from narra wood by the inhabitants of southern Luzon from the Philippines. It was imported into Mexico through the Manila-Acapulco Galleon trade and from there, introduced to Europe. Both species are members of the legume family Fabaceae.\n\nIt is now known that Sahagún's and Monardes' accounts are the earliest known records of the phenomenon of fluorescence. The unusual property of the wood was caused by the compound matlaline, which is the oxidation product of one of the flavonoids found in the wood. The phenomenon is also exhibited by other members of the family Fabaceae.\n\n"}
{"id": "25318873", "url": "https://en.wikipedia.org/wiki?curid=25318873", "title": "List of Superfund sites in Louisiana", "text": "List of Superfund sites in Louisiana\n\nThis is a list of Superfund sites in Louisiana designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of May 3, 2010, there were nine Superfund sites on the National Priorities List in Louisiana. Three more sites have been proposed for entry on the list and eleven others have been cleaned up and removed from it.\n\n"}
{"id": "9132706", "url": "https://en.wikipedia.org/wiki?curid=9132706", "title": "List of foliage plant diseases (Araliaceae)", "text": "List of foliage plant diseases (Araliaceae)\n\nThis is a list of diseases of foliage plants belonging to the family Araliaceae that occur in the United States.\n\n"}
{"id": "26179944", "url": "https://en.wikipedia.org/wiki?curid=26179944", "title": "List of glaciers in Glacier National Park (U.S.)", "text": "List of glaciers in Glacier National Park (U.S.)\n\nThere are at least 35 named glaciers in Glacier National Park (U.S.). At the end of the Little Ice Age about 1850, the area containing the national park had 150 glaciers. There are 25 active glaciers remaining in the park today. Since the latest interglacial period began 10,000 years ago, there have been regular climate shifts causing periods of glacier growth or melt-back. The glaciers are currently being studied to see the effect of global warming It is estimated that if current warming trends continue, there will be no glaciers left in the park by 2030.\n\n\n\n\n"}
{"id": "52637930", "url": "https://en.wikipedia.org/wiki?curid=52637930", "title": "List of largest energy companies", "text": "List of largest energy companies\n\nAn energy company is a company which operates within the energy industry, which can be involved in the production and sale of energy, including fuel extraction, manufacturing, refining and distribution. The companies listed below are traded on public stock exchanges, as such state owned energy companies, such as Aramco are not included. However Saudi Arabia’s deputy crown prince, Mohammed bin Salman, revealed plans to float 5% of the shares of Aramco, creating a publicly traded company with a market capitalisation of around $2 trillion (almost three times that of Apple).\n\nThe energy industry can be sub-divided into further, more specific areas - such as: the petroleum industry (oil companies, petroleum refiners, fuel transport and end-user sales at gas stations), gas and coal industry, as well as renewable energy companies. Also included are energy industry service companies as well as those which fall under the Energy service company umbrella.\n\n\n"}
{"id": "58635971", "url": "https://en.wikipedia.org/wiki?curid=58635971", "title": "List of pipeline accidents in the United States in 2013", "text": "List of pipeline accidents in the United States in 2013\n\nThe following is a list of pipeline accidents in the United States in 2013. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n\n"}
{"id": "9017643", "url": "https://en.wikipedia.org/wiki?curid=9017643", "title": "List of sweet potato diseases", "text": "List of sweet potato diseases\n\nThis article is a list of diseases of the sweet potato, (\"Ipomoea batatas\").\n\n"}
{"id": "20505921", "url": "https://en.wikipedia.org/wiki?curid=20505921", "title": "List of the ultra-prominent summits of Alaska", "text": "List of the ultra-prominent summits of Alaska\n\nThe following sortable table comprises the 65 ultra-prominent summits of the U.S. State of Alaska. Each of these peaks has at least of topographic prominence.\n\nTopographic elevation is the vertical distance above the reference geoid, a mathematical model of the Earth's sea level as an equipotential gravitational surface. The topographic prominence of a summit is the elevation difference between that summit and the highest or key col to a higher summit. The topographic isolation of a summit is the minimum great-circle distance to a point of equal elevation.\n\nThis article defines a significant summit as a summit with at least of topographic prominence, and a major summit as a summit with at least of topographic prominence. An ultra-prominent summit is a summit with at least of topographic prominence. There are 126 ultra-prominent summits in the United States.\n\nIf an elevation or prominence is calculated as a range of values, the arithmetic mean is shown.\n\nOf the 126 ultra-prominent summits of the United States, the following 65 (or 52%) rise in Alaska. Four of these peaks lie on the international border with Yukon and two lie on the international border with British Columbia.\n\n\n"}
{"id": "18155816", "url": "https://en.wikipedia.org/wiki?curid=18155816", "title": "Lists of environmental publications", "text": "Lists of environmental publications\n\nThis is a list of all environmental publication lists.\n\n\n"}
{"id": "18960525", "url": "https://en.wikipedia.org/wiki?curid=18960525", "title": "Mark Beaumont", "text": "Mark Beaumont\n\nMark Ian Macleod Beaumont BEM (born 1 January 1983) is a record-breaking long-distance British cyclist, adventurer, broadcaster, documentary maker and author. He holds the record for cycling round the world, completing his route on 18 September 2017, having taken less than 79 days. On 18 February 2010 Beaumont completed a quest to cycle the Americas, cycling from Anchorage, Alaska, US to Ushuaia in Southern Argentina, for a BBC Television series.\n\nIn the summer of 2011 Beaumont joined a six-man team to row from Resolute Bay in the Nunavut Territory, Canada to the 1996 location of the North Magnetic Pole. Each of these expeditions was filmed for BBC One documentaries. On 1 February 2012 Beaumont and his team of rowers were rescued from the Atlantic Ocean when their rowing boat capsized during a crossing from Morocco to Barbados. On 21 May 2015 he rode from Cairo to Cape Town (10,000 km) and broke the world record for fastest solo ride for the length of Africa by finishing in 42 days and 8 hours.\nBeaumont was home-schooled until the age of 11 by his mother, Una. He was then educated at the High School of Dundee. At age 15, he completed a solo ride the length of Great Britain from John o'Groats to Land's End.\n\nHe has a degree in politics from the University of Glasgow. He was awarded Graduate of the Year 2009 from the University of Glasgow, and won the 2010 Glenfiddich Spirit of Scotland Award. In 2012 he was awarded the degree of Doctor of Laws honoris causa from the University of Dundee. Beaumont was made Rector of the University of Dundee in January 2016.\n\nIn the 2018 New Year Honours, Beaumont was awarded the British Empire Medal (BEM) \"for services to sport, broadcasting and charity.\" \n\nIn 2008, Beaumont successfully broke the world record for a circumnavigational bike tour of the World. To qualify for the Guinness world record Beaumont was required to travel an 18,000-mile route that passes through two approximately antipodal points. The route began and ended in Paris, France, riding through 20 countries across Europe, the Middle East, India, Asia, Australasia and North America. The bicycle used for the attempt was a Koga-Miyata with a Rohloff internal gearing hub. The bike was loaded with of equipment such as tools, cameras and other equipment to support him during the journey. The new record was set at 194 days and 17 hours beating the previous record of 276 days and 19 hours.\n\nBeaumont endured many hardships during his voyage. In Lafayette, Louisiana he was involved in a collision with a car and robbed later the same day, and elsewhere struggled with illnesses such as dysentery.\n\nAs a result of breaking the world record, Beaumont raised £18,000 for charity.\n\nBeaumont's video diaries of the journey formed the basis of a BAFTA-nominated documentary, \"The Man who Cycled the World\", which was broadcast by the BBC in August 2008.\n\nBeaumont's around-the-world cycling record was broken by Vin Cox on 1 August 2010. Beaumont regained the Guinness around-the-world cycling record in 2017 (see 'Around the World in 80 days' below).\n\nBeaumont cycled from Anchorage, Alaska, US to Ushuaia in Southern Argentina, arriving on 18 February 2010. In addition to cycling in 268 days, he climbed the highest peaks in North and South America: Denali and Aconcagua. Along with commenting online throughout the journey, he recorded the adventure for a BBC One documentary series, \"The Man Who Cycled The Americas\". The first in the series of three episodes was broadcast on Tuesday, 23 March 2010 on BBC One. Beaumont bypassed Colombia and the Darien Gap on his journey.\n\nDuring Summer 2017, Beaumont completed a second global circumnavigation, similar to his first 10 years prior, known as the \"Around the World in 80 Days Artemis Challenge\". \n\nThe intention was to complete the trip in 80 days, inspired by the Jules Verne story \"Around the World in Eighty Days\". A support team traveled with him on his journey, providing nutritional, mechanical and logistical support. His target was to cover approximately a day. He completed the journey one day ahead of schedule on 18 September, with a total time of 78 days, 14 hours, 40 minutes. This beat the previous around-the-world cycling record by 44 days.\n\nAs with Beaumonts original (2008) route, the trip started and finished in Paris, France, crossing Russia and Mongolia to Beijing, China (a more northerly route than in 2008), before echoing his original route across Australia and New Zealand. The route across North America was also a longer, more northerly route, before a final leg from Lisbon, Portugal back to Paris. \n\nDuring the summer of 2011, Beaumont joined a team of six in rowing through the Canadian Arctic, as the BBC cameraman as well as on the oars. Their aim was to reach a 1996 location of the North Magnetic Pole. It is only in the last few years that the sea ice has melted enough for such a route to be attempted.\n\nIn early 2012, Beaumont joined another team in an attempt to break the world record for rowing across the Atlantic Ocean. After 27 days and over 2,000 miles into the expedition, they capsized and had to be rescued.\n\nIn addition to being an accomplished cyclist, Beaumont is a skier and rower. In June 2009, he planned to participate in a 12-strong team attempt to row across the North Atlantic in a record-breaking 45 days. The team would have been led by Edinburgh stockbroker turned adventurer Leven Brown whose 14-man La Mondiale crew had beaten the record from the Canaries to Barbados. However, the boat he was going to use sank in the Atlantic Ocean.\n\nBeaumont was the first torchbearer for day 26 of the 2012 Summer Olympics torch relay.\n\nIn September 2012, Beaumont planned to lead the World Cycle Challenge – the first fully supported group circumnavigation of the globe, with the aim of cycling in 245 days.\n\nIn May 2015, Beaumont set a new record in his \"Africa Solo\" challenge, cycling from Cairo to Cape Town in 42 days, and beating the previous record by 17 days. The bike he rode for this and the subsequent North Coast 500 challenge was a Koga Solacio which is on display at Edinburgh Airport.\n\nIn November 2015, Beaumont set the record for completing the North Coast 500 (a 518.7 mile route around Scotland) by bicycle in 37 hours 56 minutes and 44 seconds. This effort was later beaten in 2016 by James McCallum, who completed the route in 31 hours.\n\n\n\n"}
{"id": "375826", "url": "https://en.wikipedia.org/wiki?curid=375826", "title": "Mohorovičić discontinuity", "text": "Mohorovičić discontinuity\n\nThe Mohorovičić discontinuity (), usually referred to as the Moho, is the boundary between the Earth's crust and the mantle.\n\nNamed after the pioneering Croatian seismologist Andrija Mohorovičić, the Moho separates both the oceanic crust and continental crust from underlying mantle.The Mohorovičić discontinuity was first identified in 1909 by Mohorovičić, when he observed that seismograms from shallow-focus earthquakes had two sets of P-waves and S-waves, one that followed a direct path near the Earth's surface and the other refracted by a high-velocity medium.\n\nThe Moho lies almost entirely within the lithosphere; only beneath mid-ocean ridges does it define the lithosphere–asthenosphere boundary. The Mohorovičić discontinuity is below the ocean floor, and beneath typical continental crusts, with an average of .\n\nImmediately above the Moho, the velocities of primary seismic waves (P-waves) are consistent with those through basalt (6.7–7.2 km/s), and below they are similar to those through peridotite or dunite (7.6–8.6 km/s). That suggests the Moho marks a change of composition, but the interface appears to be too even for any believable sorting mechanism within the Earth. Near-surface observations suggest such sorting produces an irregular surface.\n\nThe Moho is characterized by a transition zone of up to 500 m thick. Ancient Moho zones are exposed above-ground in numerous ophiolites around the world.\n\nDuring the late 1950s and early 1960s the executive committee of the U.S. National Science Foundation funded drilling a hole through the ocean floor to reach this boundary. However the operation, named Project Mohole, never received sufficient support and was mismanaged; the United States Congress canceled it in 1967. Soviet scientists at the Kola Institute pursued the goal simultaneously; after 15 years they reached a depth of , the world's deepest hole, before abandoning their attempt in 1989.\n\nReaching the discontinuity remains an important scientific objective. One proposal considers a rock-melting radionuclide-powered capsule with a heavy tungsten needle that can propel itself down to the Moho discontinuity and explore Earth's interior near it and in the upper mantle. The Japanese project Chikyu Hakken (\"Earth Discovery\") also aims to explore in this general area with the drilling ship, \"Chikyū\", built for the Integrated Ocean Drilling Program (IODP).\n\nPlans called for the drill-ship \"JOIDES Resolution\" to sail from Colombo in Sri Lanka in late 2015 and to head for the Atlantis Bank, a promising location in the southwestern Indian Ocean on the Southwest Indian Ridge, to attempt to drill an initial bore hole to a depth of approximately 1.5 kilometres.\nThe attempt did not even reach 1.3 km, but researchers hope to further their investigations at a later date.\n\n\n\n"}
{"id": "41097", "url": "https://en.wikipedia.org/wiki?curid=41097", "title": "Nuclear electromagnetic pulse", "text": "Nuclear electromagnetic pulse\n\nA nuclear electromagnetic pulse (commonly abbreviated as nuclear EMP, or NEMP) is a burst of electromagnetic radiation created by nuclear explosions. The resulting rapidly changing electric and magnetic fields may couple with electrical and electronic systems to produce damaging current and voltage surges. The specific characteristics of any particular nuclear EMP event vary according to a number of factors, the most important of which is the altitude of the detonation.\n\nThe term \"electromagnetic pulse\" generally excludes optical (infrared, visible, ultraviolet) and ionizing (such as X-ray and gamma radiation) ranges. In military terminology, a nuclear warhead detonated tens to hundreds of kilometers above the Earth's surface is known as a high-altitude electromagnetic pulse (HEMP) device. Effects of a HEMP device depend on factors including the altitude of the detonation, energy yield, gamma ray output, interactions with the Earth's magnetic field and electromagnetic shielding of targets.\n\nThe fact that an electromagnetic pulse is produced by a nuclear explosion was known in the earliest days of nuclear weapons testing. The magnitude of the EMP and the significance of its effects, however, were not immediately realized.\n\nDuring the first United States nuclear test on 16 July 1945, electronic equipment was shielded because Enrico Fermi expected the electromagnetic pulse. The official technical history for that first nuclear test states, \"All signal lines were completely shielded, in many cases doubly shielded. In spite of this many records were lost because of spurious pickup at the time of the explosion that paralyzed the recording equipment.\" During British nuclear testing in 1952–1953 instrumentation failures were attributed to \"radioflash\", which was their term for EMP.\n\nThe first openly reported observation of the unique aspects of high-altitude nuclear EMP occurred during the helium balloon lofted Yucca nuclear test of the Hardtack I series on 28 April 1958. In that test, the electric field measurements from the 1.7 kiloton weapon went off the scale of the test instruments and was estimated to be about 5 times the oscilloscope limits. The Yucca EMP was initially positive-going whereas low-altitude bursts were negative pulses. Also, the polarization of the Yucca EMP signal was horizontal, whereas low-altitude nuclear EMP was vertically polarized. In spite of these many differences, the unique EMP results were dismissed as a possible wave propagation anomaly.\n\nThe high-altitude nuclear tests of 1962, as discussed below, confirmed the unique results of the Yucca high-altitude test and increased the awareness of high-altitude nuclear EMP beyond the original group of defense scientists. The larger scientific community became aware of the significance of the EMP problem after a three-article series on nuclear EMP was published in 1981 by William J. Broad in \"Science\".\n\nIn July 1962, the US carried out the Starfish Prime test, exploding a 1.44 megaton bomb above the mid-Pacific Ocean. This demonstrated that the effects of a high-altitude nuclear explosion were much larger than had been previously calculated. Starfish Prime made those effects known to the public by causing electrical damage in Hawaii, about away from the detonation point, knocking out about 300 streetlights, setting off numerous burglar alarms and damaging a microwave link.\n\nStarfish Prime was the first success in the series of United States high-altitude nuclear tests in 1962 known as Operation Fishbowl. Subsequent tests gathered more data on the high-altitude EMP phenomenon.\n\nThe Bluegill Triple Prime and Kingfish high-altitude nuclear tests of October and November 1962 in Operation Fishbowl provided data that was clear enough to enable physicists to accurately identify the physical mechanisms behind the electromagnetic pulses.\n\nThe EMP damage of the Starfish Prime test was quickly repaired due, in part, to the fact that the EMP over Hawaii was relatively weak compared to what could be produced with a more intense pulse, and in part due to the relative ruggedness (compared to today) of Hawaii's electrical and electronic infrastructure in 1962.\n\nThe relatively small magnitude of the Starfish Prime EMP in Hawaii (about 5.6 kilovolts/metre) and the relatively small amount of damage (for example, only 1 to 3 percent of streetlights extinguished) led some scientists to believe, in the early days of EMP research, that the problem might not be significant. Later calculations showed that if the Starfish Prime warhead had been detonated over the northern continental United States, the magnitude of the EMP would have been much larger (22 to 30 kV/m) because of the greater strength of the Earth's magnetic field over the United States, as well as its different orientation at high latitudes. These calculations, combined with the accelerating reliance on EMP-sensitive microelectronics, heightened awareness that EMP could be a significant problem.\n\nIn 1962, the Soviet Union also performed three EMP-producing nuclear tests in space over Kazakhstan, the last in the \"Soviet Project K nuclear tests\". Although these weapons were much smaller (300 kiloton) than the Starfish Prime test, they were over a populated, large land mass and at a location where the Earth's magnetic field was greater; the damage caused by the resulting EMP was reportedly much greater than in Starfish Prime. The geomagnetic storm–like E3 pulse from Test 184 induced a current surge in a long underground power line that caused a fire in the power plant in the city of Karaganda.\n\nAfter the collapse of the Soviet Union, the level of this damage was communicated informally to US scientists. For a few years US and Russian scientists collaborated on the HEMP phenomenon. Funding was secured to enable Russian scientists to report on some of the Soviet EMP results in international scientific journals. As a result, formal documentation of some of the EMP damage in Kazakhstan exists but is still sparse in the open scientific literature.\n\nFor one of the K Project tests, Soviet scientists instrumented a section of telephone line in the area that they expected to be affected by the pulse. The monitored telephone line was divided into sub-lines of in length, separated by repeaters. Each sub-line was protected by fuses and by gas-filled overvoltage protectors. The EMP from the 22 October (K-3) nuclear test (also known as Test 184) blew all of the fuses and fired all of the overvoltage protectors in all of the sub-lines.\n\nPublished reports, including a 1998 IEEE article, have stated that there were significant problems with ceramic insulators on overhead electrical power lines during the tests. A 2010 technical report written for Oak Ridge National Laboratory stated that \"Power line insulators were damaged, resulting in a short circuit on the line and some lines detaching from the poles and falling to the ground.\"\n\nNuclear EMP is a complex multi-pulse, usually described in terms of three components, as defined by the International Electrotechnical Commission (IEC).\n\nThe three components of nuclear EMP, as defined by the IEC, are called \"E1\", \"E2\" and \"E3\".\n\nThe E1 pulse is the very fast component of nuclear EMP. E1 is a brief but intense electromagnetic field that induces high voltages in electrical conductors. E1 causes most of its damage by causing electrical breakdown voltages to be exceeded. E1 can destroy computers and communications equipment and it changes too quickly (nanoseconds) for ordinary surge protectors to provide effective protection from it. Fast-acting surge protectors (such as those using TVS diodes) will block the E1 pulse.\n\nE1 is produced when gamma radiation from the nuclear detonation ionizes (strips electrons from) atoms in the upper atmosphere. This is known as the Compton effect and the resulting current is called the \"Compton current\". The electrons travel in a generally downward direction at relativistic speeds (more than 90 percent of the speed of light). In the absence of a magnetic field, this would produce a large, radial pulse of electric current propagating outward from the burst location confined to the source region (the region over which the gamma photons are attenuated). The Earth's magnetic field exerts a force on the electron flow at a right angle to both the field and the particles' original vector, which deflects the electrons and leads to synchrotron radiation. Because the outward traveling gamma pulse is propagating at the speed of light, the synchrotron radiation of the Compton electrons adds coherently, leading to a radiated electromagnetic signal. This interaction produces a large, brief, pulse.\n\nSeveral physicists worked on the problem of identifying the mechanism of the HEMP E1 pulse. The mechanism was finally identified by Conrad Longmire of Los Alamos National Laboratory in 1963.\n\nLongmire gives numerical values for a typical case of E1 pulse produced by a second-generation nuclear weapon such as those of Operation Fishbowl. The typical gamma rays given off by the weapon have an energy of about 2 MeV (mega-electron volts). The gamma rays transfer about half of their energy to the ejected free electrons, giving an energy of about 1 MeV.\n\nIn a vacuum and absent a magnetic field, the electrons would travel with a current density of tens of amperes per square metre. Because of the downward tilt of the Earth's magnetic field at high latitudes, the area of peak field strength is a U-shaped region to the equatorial side of the detonation. As shown in the diagram, for nuclear detonations in the Northern Hemisphere, this U-shaped region is south of the detonation point. Near the equator, where the Earth's magnetic field is more nearly horizontal, the E1 field strength is more nearly symmetrical around the burst location.\n\nAt geomagnetic field strengths typical of the mid latitudes, these initial electrons spiral around the magnetic field lines with a typical radius of about 85 metres (about 280 feet). These initial electrons are stopped by collisions with air molecules at an average distance of about 170 metres (a little less than 580 feet). This means that most of the electrons are stopped by collisions with air molecules before completing a full spiral around the field lines.\n\nThis interaction of the negatively charged electrons with the magnetic field radiates a pulse of electromagnetic energy. The pulse typically rises to its peak value in some 5 nanoseconds. Its magnitude typically decays by half within 200 nanoseconds. (By the IEC definition, this E1 pulse ends 1000 nanoseconds after it begins.) This process occurs simultaneously on about 10 electrons. The simultaneous action of the electrons causes the resulting pulse from each electron to radiate coherently, adding to produce a single large amplitude, but narrow, radiated pulse.\n\nSecondary collisions cause subsequent electrons to lose energy before they reach ground level. The electrons generated by these subsequent collisions have so little energy that they do not contribute significantly to the E1 pulse.\n\nThese 2 MeV gamma rays typically produce an E1 pulse near ground level at moderately high latitudes that peaks at about 50,000 volts per metre. The ionization process in the mid-stratosphere causes this region to become an electrical conductor, a process that blocks the production of further electromagnetic signals and causes the field strength to saturate at about 50,000 volts per metre. The strength of the E1 pulse depends upon the number and intensity of the gamma rays and upon the rapidity of the gamma ray burst. Strength is also somewhat dependent upon altitude.\n\nThere are reports of \"super-EMP\" nuclear weapons that are able to exceed the 50,000 volt per metre limit by unspecified mechanisms. The reality and possible construction details of these weapons are classified and are, therefore, unconfirmed in the open scientific literature \n\nThe E2 component is generated by scattered gamma rays and inelastic gammas produced by neutrons. This E2 component is an \"intermediate time\" pulse that, by IEC definition, lasts from about 1 microsecond to 1 second after the explosion. E2 has many similarities to lightning, although lightning-induced E2 may be considerably larger than a nuclear E2. Because of the similarities and the widespread use of lightning protection technology, E2 is generally considered to be the easiest to protect against.\n\nAccording to the United States EMP Commission, the main problem with E2 is that it immediately follows E1, which may have damaged the devices that would normally protect against E2.\n\nThe EMP Commission Executive Report of 2004 states, \"In general, it would not be an issue for critical infrastructure systems since they have existing protective measures for defense against occasional lightning strikes. The most significant risk is synergistic, because the E2 component follows a small fraction of a second after the first component's insult, which has the ability to impair or destroy many protective and control features. The energy associated with the second component thus may be allowed to pass into and damage systems.\"\n\nThe E3 component is different from E1 and E2. E3 is a much slower pulse, lasting tens to hundreds of seconds. It is caused by the nuclear detonation's temporary distortion of the Earth's magnetic field. The E3 component has similarities to a geomagnetic storm caused by a solar flare. Like a geomagnetic storm, E3 can produce geomagnetically induced currents in long electrical conductors, damaging components such as power line transformers.\n\nBecause of the similarity between solar-induced geomagnetic storms and nuclear E3, it has become common to refer to solar-induced geomagnetic storms as \"Solar EMP\". \"Solar EMP,\" does not include E1 or E2 components.\n\nFactors that control weapon effectiveness include altitude, yield, construction details, target distance, intervening geographical features, and local strength of the Earth's magnetic field.\n\nAccording to an internet primer published by the Federation of American Scientists\n\nThus, for equipment to be affected, the weapon needs to be above the visual horizon.\n\nThe altitude indicated above is greater than that of the International Space Station and many low Earth orbit satellites. Large weapons could have a dramatic impact on satellite operations and communications such as occurred during Operation Fishbowl. The damaging effects on orbiting satellites are usually due to factors other than EMP. In the Starfish Prime nuclear test, most damage was to the satellites' solar panels while passing through radiation belts created by the explosion.\n\nFor detonations within the atmosphere, the situation is more complex. Within the range of gamma ray deposition, simple laws no longer hold as the air is ionised and there are other EMP effects, such as a radial electric field due to the separation of Compton electrons from air molecules, together with other complex phenomena. For a surface burst, absorption of gamma rays by air would limit the range of gamma ray deposition to approximately 10 miles, while for a burst in the lower-density air at high altitudes, the range of deposition would be far greater.\n\nTypical nuclear weapon yields used during Cold War planning for EMP attacks were in the range of 1 to 10 megatons This is roughly 50 to 500 times the size of the Hiroshima and Nagasaki bombs. Physicists have testified at United States Congressional hearings that weapons with yields of 10 kilotons or less can produce a large EMP.\n\nThe EMP at a fixed distance from an explosion increases at most as the square root of the yield (see the illustration to the right). This means that although a 10 kiloton weapon has only 0.7% of the energy release of the 1.44-megaton Starfish Prime test, the EMP will be at least 8% as powerful. Since the E1 component of nuclear EMP depends on the prompt gamma ray output, which was only 0.1% of yield in Starfish Prime but can be 0.5% of yield in low yield pure nuclear fission weapons, a 10 kiloton bomb can easily be 5 x 8% = 40% as powerful as the 1.44 megaton Starfish Prime at producing EMP.\n\nThe total prompt gamma ray energy in a fission explosion is 3.5% of the yield, but in a 10 kiloton detonation the triggering explosive around the bomb core absorbs about 85% of the prompt gamma rays, so the output is only about 0.5% of the yield. In the thermonuclear Starfish Prime the fission yield was less than 100% and the thicker outer casing absorbed about 95% of the prompt gamma rays from the pusher around the fusion stage. Thermonuclear weapons are also less efficient at producing EMP because the first stage can pre-ionize the air which becomes conductive and hence rapidly shorts out the Compton currents generated by the fusion stage. Hence, small pure fission weapons with thin cases are far more efficient at causing EMP than most megaton bombs.\n\nThis analysis, however, only applies to the fast E1 and E2 components of nuclear EMP. The geomagnetic storm-like E3 component of nuclear EMP is more closely proportional to the total energy yield of the weapon.\n\nIn nuclear EMP all of the components of the electromagnetic pulse are generated outside of the weapon.\n\nFor high-altitude nuclear explosions, much of the EMP is generated far from the detonation (where the gamma radiation from the explosion hits the upper atmosphere). This electric field from the EMP is remarkably uniform over the large area affected.\n\nAccording to the standard reference text on nuclear weapons effects published by the U.S. Department of Defense, \"The peak electric field (and its amplitude) at the Earth's surface from a high-altitude burst will depend upon the explosion yield, the height of the burst, the location of the observer, and the orientation with respect to the geomagnetic field. As a general rule, however, the field strength may be expected to be tens of kilovolts per metre over most of the area receiving the EMP radiation.\"\n\nThe text also states that, \"... over most of the area affected by the EMP the electric field strength on the ground would exceed 0.5\"E\". For yields of less than a few hundred kilotons, this would not necessarily be true because the field strength at the Earth's tangent could be substantially less than 0.5\"E\".\"\n\nIn other words, the electric field strength in the entire area that is affected by the EMP will be fairly uniform for weapons with a large gamma ray output. For smaller weapons, the electric field may fall at a faster rate as distance increases.\n\nAn energetic EMP can temporarily upset or permanently damage electronic equipment by generating high voltage and high current surges; semiconductor components are particularly at risk. The effects of damage can range from imperceptible to the eye, to devices literally blowing apart. Cables, even if short, can act as antennas to transmit pulse energy to equipment.\n\nOlder, vacuum tube (valve) based equipment is generally much less vulnerable to nuclear EMP than solid state equipment, which is much more susceptible to damage by large, brief voltage and current surges. Soviet Cold War-era military aircraft often had avionics based on vacuum tubes because solid-state capabilities were limited and vacuum-tube gear was believed to be more likely to survive.\n\nOther components in vacuum tube circuitry can be damaged by EMP. Vacuum tube equipment was damaged in the 1962 testing. The solid state PRC-77 VHF manpackable 2-way radio survived extensive EMP testing. The earlier PRC-25, nearly identical except for a vacuum tube final amplification stage, was tested in EMP simulators, but was not certified to remain fully functional.\n\nEquipment that is running at the time of an EMP is more vulnerable. Even a low-energy pulse has access to the power source, and all parts of the system are illuminated by the pulse. For example, a high-current arcing path may be created across the power supply, burning out some device along that path. Such effects are hard to predict, and require testing to assess potential vulnerabilities.\n\nMany nuclear detonations have taken place using aerial bombs. The B-29 aircraft that delivered the nuclear weapons at Hiroshima and Nagasaki did not lose power from electrical damage, because electrons (ejected from the air by gamma rays) are stopped quickly in normal air for bursts below roughly , so they are not significantly deflected by the Earth's magnetic field.\n\nIf the aircraft carrying the Hiroshima and Nagasaki bombs had been within the intense nuclear radiation zone when the bombs exploded over those cities, then they would have suffered effects from the charge separation (radial) EMP. But this only occurs within the severe blast radius for detonations below about 10 km altitude.\n\nDuring Operation Fishbowl, EMP disruptions were suffered aboard a KC-135 photographic aircraft flying from the detonations at burst altitudes. The vital electronics were less sophisticated than today's and the aircraft was able to land safely.\n\nAn EMP would probably not affect most cars, despite modern cars' heavy use of electronics, because cars' electronic circuits and cabling are likely too short to be affected. In addition, cars' metallic frames provide some protection. However, even a small percentage of cars breaking down due to an electronic malfunction would cause temporary traffic jams.\n\nAn EMP has a smaller effect the shorter the length of an electrical conductor; though other factors affect the vulnerability of electronics as well, so no cutoff length determines whether some piece of equipment will survive. However, small electronic devices, such as wristwatches and cell phones, would most likely withstand an EMP.\n\nThough voltages can accumulate in electrical conductors after an EMP, it will generally not flow out into human or animal bodies, and thus contact is safe.\n\nThe United States EMP Commission was created by the United States Congress in 2001. The commission is formally known as the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack.\n\nThe Commission brought together notable scientists and technologists to compile several reports. In 2008, the Commission released the \"Critical National Infrastructures Report\". This report describes the likely consequences of a nuclear EMP on civilian infrastructure. Although this report covered the United States, most of the information is applicable to other industrialized countries. The 2008 report was a followup to a more generalized report issued by the commission in 2004.\n\nIn written testimony delivered to the United States Senate in 2005, an EMP Commission staff member reported:\n\nThe United States EMP Commission determined that long-known protections are almost completely absent in the civilian infrastructure of the United States and that large parts of US military services were less-protected against EMP than during the Cold War. In public statements, the Commission recommended making electronic equipment and electrical components resistant to EMP – and maintaining spare parts inventories that would enable prompt repairs. The United States EMP Commission did not look at other nations.\n\nIn 2011 the Defense Science Board published a report about the ongoing efforts to defend critical military and civilian systems against EMP and other nuclear weapons effects.\n\nThe United States military services developed, and in some cases published, hypothetical EMP attack scenarios.\n\nIn 2016 the Los Alamos Laboratory started phase 0 of a multi-year study (through to phase 3) to investigate EMP's which prepared the strategy to be followed for the rest of the study.\n\nIn 2017 the US department of energy published the \"DOE Electromagnetic Pulse Resilience Action Plan\", Edwin Boston published a dissertation on the topic and the EMP Commission published \"Assessing the threat from electromagnetic pulse (EMP)\". The EMP commission was closed in summer 2017. They found that earlier reports had underestimated the effects of an EMP attack on the national infrastructure and highlighted issues with communications from the DoD due to the classified nature of the material and recommended that the DHS instead of going to the DOE for guidance and direction should directly cooperate with the more knowledgeable parts of the DOE. Several reports are in process of being released to the general public..\n\nThe problem of protecting civilian infrastructure from electromagnetic pulse has been intensively studied throughout the European Union, and in particular by the United Kingdom.\n\nAs of 2017, several power utility companies in the United States have been involved in a three-year research program on the impact of HEMP to the United States power grid led by an industry non-profit organization, Electric Power Research Institute (EPRI).\n\nEspecially since the 1980s, Nuclear EMP weapons have gained a significant presence in fiction and popular culture.\n\nThe popular media often depict EMP effects incorrectly, causing misunderstandings among the public and even professionals, and official efforts have been made in the United States to set the record straight. The United States Space Command commissioned science educator Bill Nye to produce a video called \"Hollywood vs. EMP\" so that inaccurate Hollywood fiction would not confuse those who must deal with real EMP events. The video is not available to the general public.\n\n\n\n"}
{"id": "36962483", "url": "https://en.wikipedia.org/wiki?curid=36962483", "title": "Ocean dredging", "text": "Ocean dredging\n\nOcean dredging was an oceanography technique introduced in the nineteenth century and developed by naturalist Edward Forbes. He would lay out the dredged material on the deck to examine, preserve and study it. The practice was chronicled in a remembrance of Forbes by William Jerdan in his 1866 book \"Men I Have Known\".\n"}
{"id": "11307108", "url": "https://en.wikipedia.org/wiki?curid=11307108", "title": "Odyssey Marine Exploration", "text": "Odyssey Marine Exploration\n\nOdyssey Marine Exploration, Inc. is an American company engaged in the salvage of deep-water shipwrecks. Odyssey salvaged the U.S. Civil War era shipwreck of the SS \"Republic\" in 2003 and recovered over 50,000 coins and 14,000 artifacts from the site nearly 1,700 feet deep. Odyssey has several shipwreck projects in various stages of development around the world, including the codenamed Black Swan Project.\n\nBetween 1998 and 2001, Odyssey Marine Exploration searched for HMS \"Sussex\" and stated that it had located the shipwreck off Gibraltar at a depth of 821 metres. The English ship sank in a storm in 1694 during the War of the Grand Alliance as it was transporting 10 tons of gold coins to buy the allegiance of the Duke of Savoy, Victor Amadeus II, against France.\n\nIn September 2002, Odyssey reached an agreement with the British government on a formula for sharing any potential spoils, under which Odyssey would get 80 percent of the proceeds up to $45 million, 50 percent from $45 million to $500 million and 40 percent above $500 million. The British government would get the rest.\nThe company was poised to start the excavation in the late summer of 2003, according to the approved project plan and engaged Gifford and Partners to assist with archaeological aspects, but the project was delayed when Odyssey discovered and began excavating the SS \"Republic\" which continued until early 2003. The \"Sussex\" agreement was criticized by some archaeological organizations and charities, including the Council for British Archaeology, Rescue, and the Institute of Field Archaeologists, denouncing it as a dangerous precedent for the \"ransacking\" of shipwrecks by private firms under the aegis of archaeological research. An early day motion was signed by 60 British MPs condemning the treasure hunting.\n\nIn December 2005 Odyssey began archaeological investigation of the site believed to be HMS \"Sussex\". The company adhered to the project plan submitted and accepted by the Sussex Archaeological Executive, a committee of archaeological consultants approved by the UK Government. As of 2009, Odyssey had completed Phase 1A (the non-disturbance survey) and a substantial portion of Phase 1B (trial excavation of the site believed to be HMS \"Sussex\") to the satisfaction of the UK Government. Odyssey was authorized by HMG to complete phase 1B of the project.\n\nBefore Odyssey could complete Phase 1B of the \"Sussex\" project plan, it was stopped by the Spanish authorities, in particular the Junta of Andalusia in January 2006. In early June 2006, Odyssey provided clarification to Spain's Ministry of Foreign Affairs through the offices of the embassy of the United Kingdom. Odyssey then awaited final comments on the plan before resuming operations on the shipwreck believed to be that of the \"Sussex\".\n\nIn March 2007, Andalusia gave its assent for the excavation to start with the condition that Spanish archaeologists take part in order to ascertain that the shipwreck to be excavated was indeed the \"Sussex\" and not a Spanish vessel. While waiting for Andalusia to appoint an archaeologist to participate in the \"Sussex\" expedition, the company began operations on the \"Black Swan\" salvage (see below) and since then, Spain has rescinded all cooperation with the company.\n\nIn May 2007, the company announced the salvage of 17 tons of mainly silver and some gold coins from a wreck in an undisclosed location \"in international waters\". The shipwreck was later proved to be that of the Spanish frigate \"Nuestra Señora de las Mercedes\", which blew up and sank in the Action of 5 October 1804.\n\nFollowing the discovery, Odyssey brought the coins and artifacts recovered into the jurisdiction of the United States Federal Court by filing an admiralty arrest pursuant to admiralty law. At that time, the Spanish government claimed that they believed the site was in Spanish territorial waters. The Spanish Government has since conceded that claim and sought to claim the discovered treasure based on their belief that the \"Nuestra Señora de las Mercedes\" should be afforded sovereign immunity.\n\nOn July 12, 2007, the Civil Guard seized the Odyssey Marine Exploration research vessel \"Ocean Alert\" off the European coast. The Spanish Civil Guard claims to be responsible for customs control and European Union borders in this region, under the EU Schengen Agreement. This, however, is disputed by the Government of Gibraltar and the UK Government who claim that the ship was detained in international waters near Gibraltar and that Spain therefore had no legal authority to board the vessel without the express consent of the flag state of the ship—in this case, Panama. The Gibraltar Government stated that although this was a matter for the Government of Panama, they are \"concerned that international shipping using Gibraltar port should be interfered with in this way in international waters.\"\n\nThe ship was ordered to sail to the Spanish port of Algeciras to undergo a search and inspection. Issues include the value and cultural significance of the shipwreck and the disputed status of Gibraltar between the UK and Spain. Panama is also involved because Odyssey's vessel is flagged there.\n\nA U.S. federal judge awarded the treasure to Spain in December 2009, on the ground that the ship remained the property of Spain, but Odyssey appealed the ruling. Finally, after a five-year legal battle, in February 2012 the U.S. Supreme Court ordered Odyssey Marine to relinquish the treasure to Spanish authorities. Spain's culture minister indicated the treasure would be divided among several national museums. In September 2013, a U.S. district judge further ruled that Odyssey had acted in \"bad faith\", should have recognized Spain's right, and should thus reimburse $1 million in Spain's attorneys fees. Odyssey then issued a statement recognizing that this case was unusual and that the court ruling has clarified the applicable law, which Odyssey is committed to respecting.\n\nIn 2009, Odyssey Marine Exploration's searches were the subject of the Discovery Channel program, \"Treasure Quest\". \n\nIn 2014, Odyssey Marine Exploration was selected to salvage gold from the 1857 shipwreck of the SS \"Central America\".\n\nOdyssey's stock price fell below $1 in 2014 and traded even lower in 2015.\n\nIn December 2015, Odyssey announced the sale of part of its assets to a company called Monaco Financials for $21 million. This capital allowed the company to reimburse a $11.7 million bank debt. Sold assets included the company's headquarters building in Florida, 50% of underwater mining business Neptune Minerals and a profit-sharing agreement on future shipwreck salvages. CEO Mark Gordon also stated that the company would refocus on underwater mining, while treasure hunting would remain \"part of the mix\".\n\n"}
{"id": "15430596", "url": "https://en.wikipedia.org/wiki?curid=15430596", "title": "Paanajärvi National Park", "text": "Paanajärvi National Park\n\nPaanajärvi National Park is a Russian national park in the Loukhsky District of northwestern Republic of Karelia, in northwestern Russia. \n\nThe park was established in 1992. It has received a PAN Parks certificate.\n\nPaanajärvi National Park is located in the Karelia Region of northern Europe, along the Finnish–Russian border. \n\nIt protects of pristine Scandinavian and Russian Taiga ecoregion forest habitats, lakes, and rivers.\n\nOulanka National Park is adjacent on the west along the border, within Finland, with contiguous protection of this Karelian Taiga habitat.\n\n\n"}
{"id": "38878700", "url": "https://en.wikipedia.org/wiki?curid=38878700", "title": "Revolutions of 1917–1923", "text": "Revolutions of 1917–1923\n\nThe Revolutions of 1917–1923 were a period of political unrest and revolts around the world inspired by the success of the Russian Revolution and the disorder created by the aftermath of World War I. The uprisings were mainly socialist or anti-colonial in nature and were mostly short-lived, failing to have a long-term impact. Out of all the revolutionary activity of the era, the revolutionary wave of 1917–1923 mainly refers to the unrest caused by World War I in Europe.\n\nIn war-torn Imperial Russia, the liberal February Revolution toppled the monarchy. It was unstable and the Bolsheviks seized power in the October Revolution. The ascendant communist party soon withdrew from the war with some territorial concessions by the Treaty of Brest-Litovsk. It then battled its political rivals in the Russian Civil War, including invading forces from the Allied Powers. In response to Lenin, the Bolshevik Party and the emerging Soviet Union, anti-communists from a broad assortment of ideological factions fought against them, particularly through the counter-revolutionary White movement and the peasant Green Army, the various nationalist movements in Ukraine after the Russian Revolution and other would-be new states like those in Soviet Transcaucasia and Soviet Central Asia, through the anarchist-inspired Third Russian Revolution and Tambov Rebellion.\n\nBy 1921, due to exhaustion, the collapse of transportation and markets, and threats of starvation, even dissident elements of the Red Army itself were in revolt against the communist state, as shown by the Kronstadt rebellion. However, the multiple anti-Bolshevik forces were uncoordinated and disorganized, and in every case operated on the periphery. The Red Army, operating at the center, defeated them one by one and regained control. The complete failure of Comintern-inspired revolutions was a sobering experience in Moscow, and the Bolsheviks moved from world revolution to the theme of socialism in one country, Russia. Lenin moved to open trade relations with Britain, Germany, and other major countries. Most dramatically, in 1921, Lenin introduced a sort of small-scale capitalism with his New Economic Policy (or NEP). In this process of revolution and counter-revolution the Union of Soviet Socialist Republics (USSR) was officially born in 1922.\n\nThe Leninist victories also inspired a surge by the world Communist movement: the larger German Revolution and its offspring, like the Bavarian Soviet Republic, as well as the neighboring Hungarian Revolution, and the Biennio Rosso in Italy in addition to various smaller uprisings, protests and strikes, all proved abortive.\n\nThe Bolsheviks sought to coordinate this new wave of revolution in the Soviet-led Communist International, while new communist parties separated from their former socialist organizations and the older, more moderate Second International. Despite ambitions for world revolution, the far-flung Comintern movement had more setbacks than successes through the next generation, and it was abolished in 1943. After the Second World War when the Red Army occupied most of Eastern Europe, Communists would come to power in the Baltic states, Poland, Hungary, Czechoslovakia, Romania, Bulgaria, and East Germany.\n\nIn Ireland, then part of the United Kingdom, the nationalist Easter Rising of 1916 anticipated the Irish War of Independence (1919–1921) within the same historical period as this first wave of communist revolution. The Irish republican movement of the time was predominantly nationalist and populist, and although it had left-wing positions and included socialists and communists, it was not Communist. The Irish and Soviet Russian Republics nevertheless found common ground in their opposition to British interests, and established a trading relationship.\n\nThe same was true of the Mexican Revolution (1910–1920), which had broken out in 1910 but had devolved into factional fighting among the rebels by 1915, as the more radical forces of Emiliano Zapata and Pancho Villa lost ground to the more conservative \"Sonoran oligarchy\" and its Constitutional Army. The Felicistas, the last major group of counterrevolutionaries, abandoned their armed campaign in 1920, and the internecine power struggles abated for a time after revolutionary General Álvaro Obregón had bribed or slain his former allies and rivals alike, but the following decade witnessed the assassination of Obregon and several others, abortive military coup attempts and a massive right-wing uprising, the Cristero War, due to religious persecution of Roman Catholics.\n\nThe Sette Giugno of 1919 was a revolt characterised by a series of riots and protests by the Maltese population, initially as a reaction to the rise in the cost of living in the aftermath of World War I, and the sacking of hundreds of workers from the dockyard. This coincided with popular demands for self-government, which resulted in a National Assembly being formed in Valletta at the same time of the riots. This dramatically boosted the uprising, as many people headed to Valletta to show their support for the Assembly. This led to the British forces firing into the crowd, killing four local men. The cost of living increased dramatically after the war. Imports were limited, and as food became scarce prices rose; this made the fortune of farmers and merchants with surpluses to trade.\n\nThe Egyptian Revolution of 1919 was a countrywide revolution against the British occupation of Egypt and Sudan. It was carried out by Egyptians and Sudanese from different walks of life in the wake of the British-ordered exile of revolutionary leader Saad Zaghloul, and other members of the Wafd Party in 1919. The revolution led to Britain's recognition of Egyptian independence in 1922, and the implementation of a new constitution in 1923. Britain, however, continued in control of what was renamed the Kingdom of Egypt. British guided the king and retained control of the Canal Zone, Sudan and Egypt's external and military affairs. King Fuad died in 1936 and Farouk inherited the throne at the age of sixteen. Alarmed by the Second Italo-Abyssinian War when Italy invaded Ethiopia, he signed the Anglo-Egyptian Treaty, requiring Britain to withdraw all troops from Egypt by 1949, except at the Suez Canal. During World War II, British troops used Egypt as a major base for its operations throughout the region. British troops were withdrawn to the Suez Canal area in 1947, but nationalist, anti-British feelings continued to grow after the war.\n\n\n\n\n\n\n\n"}
{"id": "58444317", "url": "https://en.wikipedia.org/wiki?curid=58444317", "title": "Río Frías Formation", "text": "Río Frías Formation\n\nRío Frías Formation () is a Middle Miocene geologic formation made up sedimentary rock located in Aysén Region, western Patagonia. The formation crops out along the upper couse of Cisnes River (). Marsupial fossils have been found in the formation. The Friasian period in the South American Land Mammal Ages is named after the formation.\n\nRío Frías Formation was discovered by Santiago Roth in the summer of 1897–98. Roth was a Swiss immigrant who had been sent to survey the area by Francisco Moreno. Moreno was director of La Plata Museum and was involved in the Cordillera of the Andes Boundary Case between Chile and Argentina, thus there was both a political and scientific motivation behind the exploration of Patagonia. Santiago Roth called the upper course of Río Cisnes for Río Frías being unaware that it was the same river. Further he thought this unexplored area to be in Argentina and not in Chile. This led the formation to acquire its name. Roth sent fossils he collected from the formation to Florentino Ameghino who was active at La Plata. It was with this fossils Ameghino established the Friasian period. Later research on Ar/Ar data revealed the base of the formation dating to 16.5 Ma, which means a slight overlap with the Santacrucian land mammal age (ending at 16.3 Ma). The formation was deposited in a fluvial environment, characterized by an intermontane valley flanked by Cretaceous basement rocks.\n\nThe following fossils have been found in the formation:\n\n"}
{"id": "5119092", "url": "https://en.wikipedia.org/wiki?curid=5119092", "title": "Soil quality", "text": "Soil quality\n\nSoil quality is a measure of the condition of soil relative to the requirements of one or more biotic species and or to any human need or purpose. According to the United States Department of Agriculture Natural Resources Conservation Service, \"Soil quality is the capacity of a specific kind of soil to function, within natural or managed ecosystem boundaries, to sustain plant and animal productivity, maintain or enhance water and air quality, and support human health and habitation. The European Commission's Joint Research Centre proposed a definition, stating that \"Soil quality is an account of the soil's ability to provide ecosystem and social services through its capacities to perform its functions under changing conditions.\" \n\nSoil quality reflects how well a soil performs the functions of maintaining biodiversity and productivity, partitioning water and solute flow, filtering and buffering, nutrient cycling, and providing support for plants and other structures. Soil management has a major impact on soil quality. \n\nSoil quality in agricultural terms is measured on a scale of soil value (\"Bodenwertzahl\") in Germany.\n\nSoil quality is related to soil function. Unlike water or air, for which established standards have been set, soil quality is difficult to define or quantify. Soil quality can be evaluated using the Soil Management Assessment Framework.\n"}
{"id": "38333055", "url": "https://en.wikipedia.org/wiki?curid=38333055", "title": "Special Flood Hazard Area", "text": "Special Flood Hazard Area\n\nA Special Flood Hazard Area (SFHA) is an area identified by the United States Federal Emergency Management Agency (FEMA) as an area with a special flood or mudflow, and/or flood related erosion hazard, as shown on a flood hazard boundary map or flood insurance rate map. Areas within the SFHA are designated on the flood insurance rate map as Zone A, AO, A1-A30, AE, A99, AH, AR, AR/A, AR/AE, AR/AH, AR/AO, AR/A1-A30, V1-V30 or V.\n\nLand areas that are at high risk for flooding are called special flood hazard areas (SFHAs), or floodplains. These areas are indicated on flood insurance rate maps (FIRMs).\n\nIn high-risk areas, there is at least a 1 in 4 chance of flooding during a 30-year mortgage.\n"}
{"id": "28997628", "url": "https://en.wikipedia.org/wiki?curid=28997628", "title": "Spotted bamboo", "text": "Spotted bamboo\n\nSpotted bamboo refers to several types of bamboo with stems that are mottled by dark spots, sometimes considered to be within the genus \"Phyllostachys\" and forms of \"Phyllostachys bambusoides\", also known as teardrop bamboo and as mottled bamboo. Phyllostachys bambusoides\" forma. \"lacrima-deae is widely encountered.\n\n\"Phyllostachys bambusoides\" forma. \"lacrima-deae\", is native to Hunan, Henan, Jianxi and Zhejiang, and especially the Jiuyi Mountains areas of China.\n\nThe stems of the spotted bamboos are esteemed and cost-effective for making the handles of Chinese brushes, used for calligraphy and painting.\n\nExamples of brushes from the eighth century CE (corresponding to the Tang Dynasty, in China) are preserved in the Shōsōin, in Japan; in fact, the prestige value of this type of bamboo was evidently so high at the time that among the Shōsōin treasures are preserved objects made out of some sort of imitation spotted bamboo.\n\nLegend has it that when Emperor Shun died suddenly during a trip to Cangwu, the tears of his two concubines, (the Xiang River goddesses Ehuang (娥皇) and Nüying (女英)) dropped onto surrounding bamboo and stained it forever. Mottled bamboo is known for being a decorative plant. The Chinese name of originates from the legend. \"Xiang\" (湘) refers to the Xiang River, where the story supposedly took place, \"fei\" (妃) means \"concubine\" and \"zhu\" (竹) means \"bamboo\", thus the \"bamboo of the concubines by the Xiang River\".\n\nThe term \"mottled bamboo\" is also used to describe online discussion board moderators in Mainland China, because when the word for moderator (版主) is entered using the keyboard in Pinyin, the word for mottled bamboo (斑竹) appears as the first option, as they sound very similar. Hence, mottled bamboo gradually became another way of calling a moderator.\n\n\n"}
{"id": "7133995", "url": "https://en.wikipedia.org/wiki?curid=7133995", "title": "Stefan flow", "text": "Stefan flow\n\nThe Stefan flow, occasionally called Stefan's flow, is a transport phenomenon concerning the movement of a chemical species by a flowing fluid (typically in the gas phase) that is induced to flow by the production or removal of the species at an interface. Any process that adds the species of interest to or removes it from the flowing fluid may cause the Stefan flow, but the most common processes include evaporation, condensation, chemical reaction, sublimation, ablation, adsorption, absorption, and desorption. It was named after the Austrian physicist, mathematician, and poet Josef Stefan for his early work on calculating evaporation rates.\n\nThe Stefan flow is distinct from diffusion as described by Fick's law, but diffusion almost always also occurs in multi-species systems that are experiencing the Stefan flow. In systems undergoing one of the species addition or removal processes mentioned previously, the addition or removal generates a mean flow in the flowing fluid as the fluid next to the interface is displaced by the production or removal of additional fluid by the processes occurring at the interface. The transport of the species by this mean flow is the Stefan flow. When concentration gradients of the species are also present, diffusion transports the species relative to the mean flow. The total transport rate of the species is then given by a summation of the Stefan flow and diffusive contributions.\n\nAn example of the Stefan flow occurs when a droplet of liquid evaporates in air. In this case, the vapor/air mixture surrounding the droplet is the flowing fluid, and liquid/vapor boundary of the droplet is the interface. As heat is absorbed by the droplet from the environment, some of the liquid evaporates into vapor at the surface of the droplet, and flows away from the droplet as it is displaced by additional vapor evaporating from the droplet. This process causes the flowing medium to move away from the droplet at some mean speed that is dependent on the evaporation rate and other factors such as droplet size and composition. In addition to this mean flow, a concentration gradient must exist in the neighborhood of the droplet (assuming an isolated droplet) since the flowing medium is mostly air far from the droplet and mostly vapor near the droplet. This gradient causes Fickian diffusion that transports the vapor away from the droplet and the air towards it, with respect to the mean flow. Thus, in the frame of the droplet, the flow of vapor away from the droplet is faster than for the pure Stefan flow, since diffusion is working in the same direction as the mean flow. However, the flow of air away from the droplet is slower than the pure Stefan flow, since diffusion is working to transport air back towards the droplet against the Stefan flow. Such flow from evaporating droplets is important in understanding the combustion of liquid fuels such as diesel in internal combustion engines, and in the design of such engines. The Stefan flow from evaporating droplets and subliming ice particles also plays prominently in meteorology as it influences the formation and dispersion of clouds and precipitation.\n\n"}
{"id": "26951907", "url": "https://en.wikipedia.org/wiki?curid=26951907", "title": "Sweet Grass Hills", "text": "Sweet Grass Hills\n\nThe Sweet Grass Hills (Blackfoot: \"kátoyissiksi\", Cheyenne: \"vé'ho'ôhtsévóse\", Salish: \"ččaɫalqn \", \"three peaks\") are a small group of low mountains rising more than 3,000 feet above the surrounding plains southwest of Whitlash, Montana in Liberty and Toole County, Montana. The tallest point in the hills is West Butte at . Quite prominent in the local area, they are clearly visible from US Highway 2 to the south and can sometimes be seen as far north as the Crowsnest Highway (Highway 3) near Medicine Hat in Alberta. Other named peaks in the small group are Gold Butte (6,512 feet), East Butte (with two peaks, the taller of which at 6,958 ft), and Mount Lebanon (5807 ft). \nThe Sweet Grass Hills are an example of the island ranges that dot the central third portion of the state of Montana. These island ranges, completely surrounded by the 'sea' of plains and not geographically (or often geologically) part of the Rocky Mountains to the west, are \"biological hotspots\", containing more species than the prairie below.\n\nThe hills were formed in the early Paleogene period, and geologically are known as stocks - intrusive igneous rock forced up from below and solidified. Because of uplift and erosion, the overlying rock formations have disappeared. The upper slopes of the hills have not been glaciated and would have stood above the ice sheet that covered the area during the last ice age. Since the Sweet Grass Hills are at a higher elevation than the surrounding prairie, temperatures are cooler and precipitation is higher. Forested areas, consisting mainly of Douglas fir, are therefore able to grow on the hills' slopes.\n\nThe Sweet Grass Hills are a sacred site of the Blackfoot aboriginal people who live on both sides of the 49th parallel north which forms the Canada–United States border. The Hills were the site of a battle between Native people and wolfers in 1872. The Sweetgrass Hills Treaty of 1887 was signed between the Blackfeet and the United States, ceding 17,500,000 acres of land to the US Government, and dividing the remaining 6,000,000 into three separate reservations – The Blackfeet, Fort Belknap and Fort Peck. This is often referred to as the Sweetgrass Hills Treaty/Agreement and was ratified by congress in 1888. In 1993, the Hills were listed as one of America's Most Endangered Places. Mineral claims were staked here by Manhattan Minerals in 1995.\n\n"}
{"id": "1667261", "url": "https://en.wikipedia.org/wiki?curid=1667261", "title": "Terrestrial gamma-ray flash", "text": "Terrestrial gamma-ray flash\n\nA terrestrial gamma-ray flash (TGF) is a burst of gamma rays produced in Earth's atmosphere. TGFs have been recorded to last 0.2 to 3.5 milliseconds, and have energies of up to 20 million electronvolts. It is speculated that TGFs are caused by intense electric fields produced above or inside thunderstorms. Scientists have also detected energetic positrons and electrons produced by terrestrial gamma-ray flashes.\n\nTerrestrial gamma-ray flashes were first discovered in 1994 by BATSE, or Burst and Transient Source Experiment, on the Compton Gamma-Ray Observatory, a NASA spacecraft. A subsequent study from Stanford University in 1996 linked a TGF to an individual lightning strike occurring within a few milliseconds of the TGF. BATSE detected only a small number of TGF events in nine years (76), due to its having been constructed to study gamma ray bursts from outer space, which last much longer.\n\nIn the early 2000s, the Ramaty High Energy Solar Spectroscopic Imager (RHESSI) satellite observed TGFs with much higher energies than those recorded by BATSE. The RHESSI data led scientists to estimate that approximately 50 TGFs occur each day, more than previously thought but still only representing a very small fraction of the total lightning on Earth (3-4 million lightning events per day on average). A few years later, scientists using NASA's Fermi Gamma-ray Space Telescope, which was designed to monitor gamma rays, estimated that about 500 TGFs occur daily worldwide, but most go undetected.\n\nThough the details of the mechanism are uncertain, there is a consensus forming about the physical requirements. It is presumed that TGF photons are emitted by electrons traveling at speeds very close to the speed of light that collide with the nuclei of atoms in the air and release their energy in the form of gamma rays (bremsstrahlung ). Large populations of energetic electrons can form by avalanche growth driven by electric fields, a phenomenon called relativistic runaway electron avalanche (RREA). The electric field is likely provided by lightning, as most TGFs have been shown to occur within a few milliseconds of a lightning event (Inan et al. 1996). Beyond this basic picture the details are uncertain. Recent research has shown that electron-electron (Bremsstrahlung) leads first to an enrichment of high-energy electrons and subsequently enlarges the number of high-energy photons.\n\nSome of standard theoretical frameworks have been borrowed from other lightning-associated discharges like sprites, blue jets, and elves, which were discovered in the years immediately preceding the first TGF observations. For instance, that field may be due to the separation of charges in a thundercloud (\"DC\" field) often associated with sprites, or due to the electromagnetic pulse (EMP) produced by a lightning discharge, often associated with elves. There is also some evidence that certain TGFs occur in the absence of lightning strikes, though in the vicinity of general lightning activity, which has evoked comparisons to blue jets.\nThe DC field model requires a very large thundercloud charge to create sufficient fields at high altitudes (e.g. 50–90 km, where sprites form). Unlike the case of sprites, these large charges do not seem to be associated with TGF-generating lightning. Thus the DC field model requires the TGF to occur lower down, at the top of the thundercloud (10–20 km) where a local field can be stronger. This hypothesis is supported by two independent observations. First, the spectrum of the gamma-rays seen by RHESSI matches very well to the prediction of relativistic runaway at 15–20 km. Second, TGFs are strongly concentrated around Earth's equator when compared to lightning. (They may also be concentrated over water compared to lightning in general.) Thundercloud tops are higher near the equator, and thus the gamma-rays from TGFs produced there have a better chance of escaping the atmosphere. The implication would then be that there are many lower-altitude TGFs not seen from space, particularly at higher latitudes.\nAn alternative hypothesis, the EMP model, relaxes the requirement on thundercloud charge but instead requires a large current pulse moving at very high speed. The required current pulse speed is very restrictive, and there is not yet any direct observational support for this model.\n\nAnother hypothetical mechanism is that TGFs are produced within the thundercloud itself, either in the strong electric fields near the lightning channel or in the static fields that exist over large volumes of the cloud. These mechanisms rely on extreme activity of the lightning channel to start the process (Carlson et al. 2010) or on strong feedback to allow even small-scale random events to trigger production.\n\nIt has been suggested that TGFs must also launch beams of highly relativistic electrons and positrons which escape the atmosphere, propagate along Earth's magnetic field and precipitate on the opposite hemisphere . A few cases of TGFs on RHESSI, BATSE, and Fermi-GBM have shown unusual patterns that can be explained by such electron/positron beams, but such events are very unusual.\n\nCalculations have shown that TGFs can liberate not only positrons, but also neutrons and protons. Neutrons have already been measured in electric discharges, whereas there is no experimental confirmation of discharge related protons (2016). Recent research has shown that the fluence of these neutrons lies between 10 and 10 per ms and per m depending on the detection altitude. The energy of most of these neutrons, even with initial energies of 20 MeV, decreases down to the keV range within 1 ms.\n\nTerrestrial gamma-ray flashes pose a challenge to current theories of lightning, especially with the discovery of the clear signatures of antimatter produced in lightning.\n\nIt has been discovered in the past 15 years that among the processes of lightning is some mechanism capable of generating gamma rays, which escape the atmosphere and are observed by orbiting spacecraft. Brought to light by NASA's Gerald Fishman in 1994 in an article in \"Science\", these so-called terrestrial gamma-ray flashes (TGFs) were observed by accident, while he was documenting instances of extraterrestrial gamma ray bursts observed by the Compton Gamma Ray Observatory (CGRO). TGFs are much shorter in duration, however, lasting only about 1 ms.\n\nProfessor Umran Inan of Stanford University linked a TGF to an individual lightning stroke occurring within 1.5 ms of the TGF event, proving for the first time that the TGF was of atmospheric origin and associated with lightning strikes.\n\nCGRO recorded only about 77 events in 10 years; however, more recently the Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) spacecraft, as reported by David Smith of UC Santa Cruz, has been observing TGFs at a much higher rate, indicating that these occur about 50 times per day globally (still a very small fraction of the total lightning on the planet). The energy levels recorded exceed 20 MeV.\n\nScientists from Duke University have also been studying the link between certain lightning events and the mysterious gamma ray emissions that emanate from the Earth's own atmosphere, in light of newer observations of TGFs made by RHESSI. Their study suggests that this gamma radiation fountains upward from starting points at surprisingly low altitudes in thunderclouds.\n\nSteven Cummer, from Duke University's Pratt School of Engineering, said, \"These are higher energy gamma rays than those coming from the Sun. And yet here they are coming from the kind of terrestrial thunderstorm that we see here all the time.\"\n\nEarly hypotheses of this pointed to lightning generating high electric fields and driving relativistic runaway electron avalanche at altitudes well above the cloud where the thin atmosphere allows gamma rays to easily escape into space, similar to the way sprites are generated. Subsequent evidence however, has suggested instead that TGFs may be produced by driving relativistic electron avalanches within or just above high thunderclouds. Though hindered by atmospheric absorption of the escaping gamma rays, these theories do not require the exceptionally intense lightning that high altitude theories of TGF generation rely on.\n\nThe role of TGFs and their relationship to lightning remains a subject of ongoing scientific study.\n\nIn 2009, the Fermi Gamma-ray Space Telescope in Earth orbit observed intense burst of gamma rays corresponding to positron annihilations coming out of a storm formation. Scientists wouldn't have been surprised to see a few positrons accompanying any intense gamma ray burst, but the lightning flash detected by Fermi appeared to have produced about 100 trillion positrons. This was reported by news media in January 2011, and had never been previously observed.\n\nThe Atmosphere-Space Interactions Monitor (ASIM), an experiment dedicated to study TGFs, was launched to the International Space Station on 2 April 2018 and is scheduled to be mounted on the Columbus External Payload Facility in late April 2018.\n\n\n"}
{"id": "23547703", "url": "https://en.wikipedia.org/wiki?curid=23547703", "title": "Tetrahedral hypothesis", "text": "Tetrahedral hypothesis\n\nThe tetrahedral hypothesis is an obsolete scientific theory attempting to explain the arrangement of the Earth's continents and oceans by referring to the geometry of a tetrahedron. Although it was a historically interesting theory in the late 19th and early 20th century, it was superseded by the concepts of continental drift and modern plate tectonics.\n\nThis idea, described as ‘\"ingenious\" by geologist Arthur Holmes, is now of historical interest only, being finally refuted by that same Holmes (see reference 7). It attempted to explain apparent anomalies in the distribution of land and water on the Earth's surface:\n\nTo understand its appeal, consider the \"regular solids\": the sphere and the 5-member set of Platonic Solids. The solid with the lowest number of sides is the tetrahedron (four equilateral triangles); progressing through the hexahedron or cube, the octahedron, the dodecahedron and the icosahedron (20 sides), the sphere can be considered to have an infinite number of sides. All six regular solids share many symmetries.\n\nNow, for each regular solid, we may relate its surface area and volume by the equation:\nwhere\" k\" is a characteristic of each solid, \"V\" its volume, and \"A\" its area. As we traverse the set in order of increasing number of faces, we find that k increases for each member; it is 0.0227 for a tetrahedron and 0.0940 for a sphere. Thus the tetrahedron is the regular solid with the largest surface area for a given volume, and makes a reasonable endpoint for a shrinking spherical Earth.\n\nThe theory was first proposed by William Lowthian Green in 1875.\nIt was still popular in 1917 when summarized as:\n\n\"The law of least action … demands that the somewhat rigid crustal portion of the earth keep in contact with the lessening interior with the least possible readjustment of its surface. … a shrinking sphere tends by the law of least action to collapse into a tetrahedron, or a tetra-hedroid, a sphere marked by four equal and equidistant triangular projections; and the earth with its three about equal and equidistant double continental masses triangular southward with three intervening depressed oceans triangular northward, its northern ocean and southern continent, with land everywhere antipodal to water, realizes the tetrahedroid status remarkably.“\nThis is suggesting that a cooling spherical Earth might have shrunk to form a tetrahedron, with its vertices and edges forming the continents, and four oceans (Pacific Ocean, Atlantic Ocean, Indian Ocean and Arctic Ocean) on its faces.\n\nBy 1915 German Alfred Wegener (1880–1930) had proposed in his continental drift theory that land masses moved great distances over the Earth's history. Wegener was also at first met with hostile reactions.\nBy the mid-1920s Holmes had developed theories on what could cause the drift.\nThe plate tectonics theory is now generally accepted to explain the dynamic nature of the Earth's surface; the tetrahedral shape plays no special role in modern theories.\nExplanations of details such as water to land ratios, the precise shape of continents and their sizes continue to be developed.\n"}
{"id": "11539327", "url": "https://en.wikipedia.org/wiki?curid=11539327", "title": "Throwim Way Leg", "text": "Throwim Way Leg\n\nThrowim Way Leg is a book written by Australian zoologist and climate change activist Tim Flannery. It documents Flannery's experiences conducting scientific research in the highlands of Papua New Guinea and Indonesian Western New Guinea. The book describes the flora and fauna of the island and the cultures of the various tribes. The title is an anglicised spelling of the New Guinean Pidgin \"Tromoi Lek,\" to go on a journey.\n\n"}
{"id": "19536787", "url": "https://en.wikipedia.org/wiki?curid=19536787", "title": "Tiangong-2", "text": "Tiangong-2\n\nTiangong-2 () is a Chinese space laboratory and part of the Project 921-2 space station program. Tiangong-2 was launched on 15 September 2016.\n\nTiangong-2 is neither designed nor planned to be a permanent orbital station; rather, it is intended as a testbed for key technologies that will be used in the Chinese large modular space station, which is planned for launch between 2019 and 2022.\n\nThe China Manned Space Engineering Office published a brief description of Tiangong-2 and its successor Tiangong-3 in 2008, indicating that at least two crewed spaceships would be launched to dock with Tiangong-2.\n\nTiangong-2 was originally expected to be launched by the China National Space Agency by 2015 to replace the prototype module Tiangong-1, which was launched in September 2011. In March 2011, Chinese officials stated that Tiangong-2 was scheduled to be launched by 2015. An uncrewed cargo spacecraft will dock with the station, allowing for resupply.\n\nIn September 2014, its launch was postponed to September 2016. Plans for visits in October 2016 by the crewed mission Shenzhou 11 and the uncrewed resupply craft Tianzhou were made public. The station was successfully launched from Jiuquan aboard a Long March 2F rocket on 15 September 2016. Shenzhou 11 successfully docked with Tiangong-2 on 19 October 2016.\n\nAboard the Shenzhou 11, launched from Jiuquan Satellite Launch Center in the Gobi desert, were Commander Jing Haipeng and Chen Dong who formed the inaugural crew for the space laboratory. It was China's first manned mission for more than three years.\n\nDuring the 30 days the two astronauts were aboard Tiangong-2, they conducted a number of scientific and technical experiments on the physiological effects of weightlessness, tests on human-machine collaboration on in-orbit maintenance technology and released an accompanying satellite successfully. Accompanying photography and near-distance fly-by observation were also carried out. They collected abundant data and made some achievements in programs of gamma-ray burst polarimeter, space cold atomic clock and preparation of new materials.\n\nShenzhou 11 separated from the orbiting Tiangong-2 space lab on November 17, reentry module landed successfully at the expected site in central Inner Mongolia Autonomous Region at about 13:59 Beijing Time.\n\nOn April 22, 2017, the cargo vessel Tianzhou-1 successfully docked with Tiangong-2 marking the first successful docking and refuelling with the orbiting space laboratory. It subsequently performed a second docking and refueling on June 15, 2017. On September 12, 2017, Tianzhou-1 performed the third and final docking and refuelling with Tiangong 2, with what is termed a fast docking which took 6.5 hours, rather than 2 days, to complete.\n\nIn June 2018, Tiangong 2 performed orbital maneuvers lowering the orbit to 292 × 297 kilometers, likely in preparation for deorbiting. It has since returned to its usual orbit.\n\nThe dimensions of Tiangong-2 are:\n\nTiānhé-1 is the core module of a planned modular space station. The core module and its other parts are to be launched between 2019 and 2022.\n\n"}
{"id": "46224462", "url": "https://en.wikipedia.org/wiki?curid=46224462", "title": "Velocity interferometer system for any reflector", "text": "Velocity interferometer system for any reflector\n\nVelocity interferometer system for any reflector (VISAR) is a time-resolved velocity measurement tool using laser interferometry used to measure the surface velocity of solids moving at high velocities. For solids experiencing high velocity impact or explosive conditions, VISAR plots the free-surface velocity against time to view the shock wave profile of a material (See Figure). VISAR is a useful tool in determining the pressure-density relationship of a material known as the Rankine-Hugoniot conditions or simply the \"Hugoniot\". \n\nIn recent years another time-resolved velocity measurement tool called laser Doppler velocimetry has achieved popularity in the shock physics community as an adjunct or replacement for VISAR. This device is essentially a Displacement interferometer of the normal Michelson variety. As such it requires extremely fast data acquisition devices (digital oscilloscopes with bandwidths of 10GHz or higher) and is limited in the range of velocities it can cover. As the surface moves, the reflected light interferes with itself and sinusoidal 'fringes' in light intensity are produced and recorded. A cycle of light intensity or fringe count indicates a displacement of the surface corresponding to one wavelength of the light. The rate at which these fringes occur is thus proportional to the velocity of the surface. To derive a velocity history the fringe (displacement) data must be differentiated with respect to time, usually by Fourier Analysis. This differentiation or FA step inevitably reduces the time resolution and accuracy of the velocity history.\n\nThe VISAR on the other hand is configured to 'optically differentiate' so that the light intensity variation due to interference varies sinusoidally with the velocity of the surface not the displacement. Also called a 'Delay-Leg Interferometer,' it is an extremely clever device and remains the best and most accurate method for recording the velocity history of fast moving surfaces.\n\nThe original VISARs were built at the National Laboratories and had free-space beams on optical tables with discrete optical components such as beam-splitting pellicles, mirrors, quarter wave delay plates, glass etalons, high voltage photo-multiplier tubes, Argon ion lasers and so on. They required a fair amount of expertise to maintain. \n\nModern versions such as the Mark IV-3000 from Martin, Froeschner & Associates (mfaoptics.com) implement the same optical arrangement entirely in single-mode optical fibre with all solid state telecomm components such as InGaAs photodiodes, Er doped fibre amplifiers (EDFAs) and extremely high purity (<2kHz linewidth) lasers. Velocity resolution down to 0.01 m/s has been demonstrated with time resolution <1ns.\n\n"}
{"id": "1484541", "url": "https://en.wikipedia.org/wiki?curid=1484541", "title": "Wavefront", "text": "Wavefront\n\nIn physics, a wavefront is the locus of points characterized by propagation of positions of identical phase: propagation of a point in 1D, a curve in 2D or a surface in 3D. For an electromagnetic wave, the wavefront is represented as a surface of identical phase, and can be modified with conventional optics. For instance, a lens can change the shape of optical wavefronts from planar to spherical as the lens introduces a spatial phase variation across the beam shape. \n\nOptical systems can be described with Maxwell's equations, and linear propagating waves such as sound or electron beams have similar wave equations. However, given the above simplifications, Huygens' principle provides a quick method to predict the propagation of a wavefront through, for example, free space. The construction is as follows: Let every point on the wavefront be considered a new point source. By calculating the total effect from every point source, the resulting field at new points can be computed. Computational algorithms are often based on this approach. Specific cases for simple wavefronts can be computed directly. For example, a spherical wavefront will remain spherical as the energy of the wave is carried away equally in all directions. Such directions of energy flow, which are always perpendicular to the wavefront, are called rays creating multiple wavefronts.\n\nThe simplest form of a wavefront is the plane wave, where the rays are parallel to one another. The light from this type of wave is referred to as collimated light. The plane wavefront is a good model for a surface-section of a very large spherical wavefront; for instance, sunlight strikes the earth with a spherical wavefront that has a radius of about 150 million kilometers (1 AU). For many purposes, such a wavefront can be considered planar over distances of the diameter of Earth.\n\nWavefront travel with the speed of light in all directions in an isotropic medium.\n\nMethods utilizing wavefront measurements or predictions can be considered an advanced approach to lens optics, where a single focal distance may not exist due to lens thickness or imperfections. Note also that for manufacturing reasons, a perfect lens has a spherical (or toroidal) surface shape though, theoretically, the ideal surface would be \"aspheric\". Shortcomings such as these in an optical system cause what are called optical aberrations. The best-known aberrations include spherical aberration and coma.\nHowever there may be more complex sources of aberrations such as in a large telescope due to spatial variations in the index of refraction of the atmosphere. The deviation of a wavefront in an optical system from a desired perfect planar wavefront is called the \"wavefront aberration\". Wavefront aberrations are usually described as either a sampled image or a collection of two-dimensional polynomial terms. Minimization of these aberrations is considered desirable for many applications in optical systems.\n\nA wavefront sensor is a device which measures the wavefront aberration in a coherent signal to describe the optical quality or lack thereof in an optical system. A very common method is to use a Shack-Hartmann lenslet array. There are many applications that include adaptive optics, optical metrology and even the measurement of the aberrations in the eye itself. In this approach, a weak laser source is directed into the eye and the reflection off the retina is sampled and processed.\n\nAlternative wavefront sensing techniques to the Shack-Hartmann system are emerging. Mathematical techniques like phase imaging or curvature sensing are also capable of providing wavefront estimations. These algorithms compute wavefront images from conventional brightfield images at different focal planes without the need for specialised wavefront optics. While Shack-Hartmann lenslet arrays are limited in lateral resolution to the size of the lenslet array, techniques such as these are only limited by the resolution of digital images used to compute the wavefront measurements.\n\nAnother application of software reconstruction of the phase is the control of telescopes through the use of adaptive optics. A common method is the Roddier test, also called wavefront curvature sensing. It yields good correction, but needs an already good system as a starting point.\n\n\n\n\n"}
{"id": "33118", "url": "https://en.wikipedia.org/wiki?curid=33118", "title": "Woodworking", "text": "Woodworking\n\nWoodworking is the activity or skill of making items from wood, and includes cabinet making (cabinetry and furniture), wood carving, joinery, carpentry, and woodturning.\n\nAlong with stone, clay and animal parts, wood was one of the first materials worked by early humans. Microwear analysis of the Mousterian stone tools used by the Neanderthals show that many were used to work wood. The development of civilization was closely tied to the development of increasingly greater degrees of skill in working these materials.\nAmong early finds of wooden tools are the worked sticks from Kalambo Falls, Clacton-on-Sea and Lehringen. The spears from Schöningen (Germany) provide some of the first examples of wooden hunting gear. Flint tools were used for carving. Since Neolithic times, carved wooden vessels are known, for example, from the Linear Pottery culture wells at Kückhofen and Eythra.\n\nExamples of Bronze Age wood-carving include tree trunks worked into coffins from northern Germany and Denmark and wooden folding-chairs. The site of Fellbach-Schmieden in Germany has provided fine examples of wooden animal statues from the Iron Age. Wooden idols from the La Tène period are known from a sanctuary at the source of the Seine in France.\n\nThere is significant evidence of advanced woodworking in ancient Egypt. Woodworking is depicted in many extant ancient Egyptian drawings, and a considerable amount of ancient Egyptian furniture (such as stools, chairs, tables, beds, chests) has been preserved. Tombs represent a large collection of these artefacts and the inner coffins found in the tombs were also made of wood. The metal used by the Egyptians for woodworking tools was originally copper and eventually, after 2000 BC bronze as ironworking was unknown until much later.\n\nCommonly used woodworking tools included axes, adzes, chisels, pull saws, and bow drills. Mortise and tenon joints are attested from the earliest Predynastic period. These joints were strengthened using pegs, dowels and leather or cord lashings. Animal glue came to be used only in the New Kingdom period. Ancient Egyptians invented the art of veneering and used varnishes for finishing, though the composition of these varnishes is unknown. Although different native acacias were used, as was the wood from the local sycamore and tamarisk trees, deforestation in the Nile valley resulted in the need for the importation of wood, notably cedar, but also Aleppo pine, boxwood and oak, starting from the Second Dynasty.\n\nWoodworking was essential to the Romans. It provided, sometimes the only, material for buildings, transportation, tools, and household items. Wood also provided pipes, dye, waterproofing materials, and energy for heat.Although most examples of Roman woodworking have been lost, the literary record preserved much of the contemporary knowledge. Vitruvius dedicates an entire chapter of his De architectura to timber, preserving many details. Pliny, while not a botanist, dedicated six books of his Natural History to trees and woody plants, providing a wealth of information on trees and their uses.\n\nThe progenitors of Chinese woodworking are considered to be Lu Ban (魯班) and his wife Lady Yun, from the Spring and Autumn period (771 to 476 BC). Lu Ban is said to have introduced the plane, chalk-line, and other tools to China. His teachings were supposedly left behind in the book \"Lu Ban Jing\" (魯班經, \"Manuscript of Lu Ban\"). Despite this, it is believed that the text was written some 1500 years after his death. This book is filled largely with descriptions of dimensions for use in building various items such as flower pots, tables, altars, etc., and also contains extensive instructions concerning Feng Shui. It mentions almost nothing of the intricate glue-less and nail-less joinery for which Chinese furniture was so famous.\n\nWith the advances in modern technology and the demands of industry, woodwork as a field has changed. The development of Computer Numeric Controlled (CNC) Machines, for example, has made us able to mass-produce and reproduce products faster, with less waste, and often more complex in design than ever before. CNC Routers can carve complicated and highly detailed shapes into flat stock, to create signs or art. Rechargeable power tools speed up creation of many projects and require much less body strength than in the past, for example when boring multiple holes. Skilled fine woodworking, however, remains a craft pursued by many. There remains demand for hand crafted work such as furniture and arts, however with rate and cost of production, the cost for consumers is much higher.\n\nHistorically, woodworkers relied upon the woods native to their region, until transportation and trade innovations made more exotic woods available to the craftsman. Woods are typically sorted into three basic types: hardwoods typified by tight grain and derived from broadleaf trees, softwoods from coniferous trees, and man-made materials such as plywood and MDF.\n\nHardwoods, botanically known as angiosperms, are deciduous and shed their leaves annually with temperature changes. Softwoods come from trees botanically known as gymnosperms, which are coniferous, cone-bearing, and stay green year round. Although a general pattern, softwoods are not necessarily always “softer” than hardwoods, and vice versa.\n\nSoftwood is most commonly found in the regions of the world with lower temperatures and is typically less durable, lighter in weight, and more vulnerable to pests and fungal attacks in comparison to hardwoods. They typically have a paler color and a more open grain than hardwoods, which contributes to the tendency of felled softwood to shrink and swell as it dries. Softwoods usually have a lower density, around 25-37lb/cu ft, which can compromise its strength. Density, however, does vary within both softwoods and hardwoods depending on the wood's geographical origin and growth rate. However, the lower density of softwoods also allows it to have a greater strength with lighter weight. In the United States, softwoods are typically cheaper and more readily available and accessible. Most softwoods are suitable for general construction, especially framing, trim, and finish work, and carcassing.\n\nHardwoods are separated into two categories, temperate and tropical hardwoods, depending on their origin. Temperate hardwoods are found in the regions between the tropics and poles, and are of particular interest to wood workers for their cost-effective aesthetic appeal and sustainable sources. Tropical hardwoods are found within the equatorial belt, including Africa, Asia, and South America. Hardwoods flaunt a higher density, around 65lb/cu ft as a result of slower growing rates and is more stable when drying. As a result of its high density, hardwoods are typically heavier than softwoods but can also be more brittle. While there are an abundant number of hardwood species, only 200 are common enough and pliable enough to be used for woodworking. Hardwoods have a wide variety of properties, making it easy to find a hardwood to suit nearly any purpose, but they are especially suitable for outdoor use due to their strength and resilience to rot and decay. The coloring of hardwoods ranges from light to very dark, making it especially versatile for aesthetic purposes. However, because hardwoods are more closely grained, they are typically harder to work than softwoods. They are also harder to acquire in the United States and, as a result, are more expensive.\n\nTypically furniture such as tables and chairs is made using solid stock from hardwoods due to its strength and resistance to warping. Additionally, they also have a greater variety of grain patterns and color and take a finish better which allows the woodworker to exercise a great deal of artistic liberty. Hardwoods can be cut more cleanly and leave less residue on sawblades and other woodworking tools. Cabinet/fixture makers employ the use of plywood and other man made panel products. Some furniture, such as the Windsor chair involve green woodworking, shaping with wood while it contains its natural moisture prior to drying.\n\nCedars are strong, aromatic softwoods that are capable of enduring outdoor elements, the most common of which is the Western Red Cedar. Western Red Cedar can sustain wet environments without succumbing to rot, and as a result is commonly used for outdoor projects such as patios, outdoor furniture, and building exteriors. This wood can be easily found at most home centers for a moderate price.\n\nFir, also known as Douglas Fir, is very inexpensive and common at local home centers. It has a characteristic straight, pronounced grain with a red-brown tint. However, its grain pattern is relatively plain and it does not stain well, so Fir is commonly used when the finished product will be painted. While commonly used for building, this softwood would also be suitable for furniture-making as well.\n\nThis hardwood is relatively easy to work with and takes stain well, but its white to light brown color with a straight grain is visually appealing on its own. However, ash is much more difficult to find than other common woods, and won’t be found at the local home center. Larger lumber yards should have it in stock.\n\nWhether yellow or white birch, these hardwoods are stable and easy to work with. Despite this, Birch is prone to blotching when stained, so painting birch products is probably best. Birch is easily found at many home centers and is a relatively inexpensive hardwood.\n\nPopular and easy to work with, cherry is in high demand for its reddish-brown color and ease of staining and finishing. Cherry likely won’t be at the local home center, but should be at a lumberyard for a somewhat expensive price. This hardwood is a very common material for furniture, and is resistant to normal wear-and-tear, but it is best for indoor pieces.\n\nA hardwood, mahogany has a trademark reddish-brown to deep-red tint and is known as “one of the great furniture woods.” However, mahogany is not typically grown in sustainable forests, and thus runs a steep price at local lumber yards.\n\nWith two varieties, red and white, oak is known to be easy to work with and relatively strong. However, furniture makers often opt for white oak over red oak for its attractive figure and moisture-resistance. Depending on the kind needed, oak can probably be found at a local home center or a lumberyard for a bit pricier than other hardwoods.\n\nWith strength, sturdiness, and durability, maple is a common material for furniture for the bedroom and even china cabinets. Maple is moisture-resistant and frequently displays stand-out swirls in the wood grain, an aesthetically pleasing differentiator from other hardwoods. While most commonly a lighter color, maple also can take stains and paint well.\n\nThere are many factors to consider when deciding what type of wood to use for a project. One of the most important is the workability of the wood: the way in which it responds when worked by hand or tools, the quality of the grain, and how it responds to adhesives and finishes. When the workability of wood is high, it offers a lower resistance when cutting and has a diminished blunting effect on tools. Highly workable wood is easier to manipulate into desired forms. If the wood grain is straight and even, it will be much easier to create strong and durable glued joints. Additionally, it will help protect the wood from splitting when nailed or screwed. Coarse grains require a lengthy process of filing and rubbing down the grain to produce a smooth result.\n\nAnother important factor to be considered is the durability of the wood, especially in regards to moisture. If the finished project will be exposed to moisture (e.g. outdoor projects) or high humidity or condensation (e.g. in kitchens or bathrooms), then the wood needs to be especially durable in order to prevent rot. Because of their oily qualities, many tropical hardwoods such as teak and mahogany are popular for such applications.\n\nAgba (G\"ossweilerodendron\" balsamiferum)\n\nAlder (\"Alnus\" glutinosa)\n\nBasswood (\"Tilia\" americana)\n\nObeah (\"Triplochiton\" scleroxylon)\n\nPine (\"Pinus)\"\n\nWestern Cedar (\"Thuja\" plicata\")\"\n\nTeak (\"Tectona\" grandis)\n\nIron (\"Milicia excelsa)\"\n\nJarrah (\"Eucalyptus\" marginata)\n\nChestnut (\"Castanea)\"\n\nOak (\"Quercus)\"\n\nCedar (\"Thuja)\"\n\nWhile many woods can be used for carving, there are some clear favorites, including Aspen, Basswood, Butternut, Black Walnut, and Oak. Because it has almost no grain and is notably soft, Basswood is particularly popular with beginner carvers. It is used in many lower-cost instruments like guitars and electric basses. Aspen is similarly soft, although slightly harder, and readily available and inexpensive. Butternut has a deeper hue than Basswood and Aspen and has a nice grain that is easy to carve, and thus friendly for beginners. It's also suitable for furniture. While more expensive that Basswood, Aspen, and Butternut, Black Walnut is a popular choice for its rich color and grain. Lastly, Oak is a strong, sturdy, and versatile wood for carving with a defined grain. It's also a popular wood for furniture making.\n\n\n\n\n"}
