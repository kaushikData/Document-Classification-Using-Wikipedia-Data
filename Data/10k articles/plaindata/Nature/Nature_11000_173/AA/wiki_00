{"id": "21617199", "url": "https://en.wikipedia.org/wiki?curid=21617199", "title": "Archeio-Marxism", "text": "Archeio-Marxism\n\nArcheio-Marxism () was a radical left political movement that was active in interwar Greece. The main motto of its supporters was \"first education and then action\" and they were against mass demonstrations and unionism. They were named after the \"Archive of Marxism\" () magazine, that was published in 1921. \n\nFrom 1930 to 1934, the organization was a member of Trotsky's International Left Opposition, renaming itself officially as the Bolshevist–Leninist Organization of Greece – Archeio-Marxists (Κομμουνιστική Οργάνωση Μπολσεβίκων Λενινιστών Ελλάδας - Αρχειομαρξιστών). With some 2,000 members, it constituted the ILO's largest member group, and its leader, Dimitris Giotopoulos (alias: \"Witte\") occupied an important position in its secretariat. \n\nFollowing Giotopoulos' quarrel with Trotsky in 1933-1934, the Archeio-Marxists left the ILO, and by the late 1930s, they became affiliated with the so-called \"London Bureau\", having formed the Communist Archio-Marxist Party of Greece. The organization survived the repression of the Metaxas Regime, but its members came under increasing attack from both right-wing groups and the KKE-led EAM-ELAS resistance front at the end of World War II and during the Greek Civil War.\n\n"}
{"id": "22945287", "url": "https://en.wikipedia.org/wiki?curid=22945287", "title": "Artificial life framework", "text": "Artificial life framework\n\nArtificial Life is a free and open sourced Java framework created to simulate Life. It is a multi-agents framework where each agent runs its own Thread.\n\nThe agents are split into two different categories: the services and the processes. The services deliver services to other agents and the processes execute specific tasks.\n\nThe agents are organized in a tree structure called Instance. Within an Instance, the services and the processes are grouped together and unlimited groups can be defined. Each node of the Instance tree can have a collection of views used to monitor the activity of the node or interact with the node. The definition of Java classes used within the Instance (i.e. processes, services or views) is defined in the Instance Model.\n\nIn order to avoid dead locking, Artificial Life implements a messaging system, a method invocation mechanism based on the messaging system and an event mechanism also based on the messaging system.\n\n"}
{"id": "50550012", "url": "https://en.wikipedia.org/wiki?curid=50550012", "title": "Biological rules", "text": "Biological rules\n\nA biological rule or biological law is a generalized law, principle, or rule of thumb formulated to describe patterns observed in living organisms. Biological rules and laws are often developed as succinct, broadly applicable ways to explain complex phenomena or salient observations about the ecology and biogeographical distributions of plant and animal species around the world, though they have been proposed for or extended to all types of organisms. Many of these regularities of ecology and biogeography are named after the biologists who first described them.\n\nFrom the birth of their science, biologists have sought to explain apparent regularities in observational data. In his biology, Aristotle inferred rules governing differences between live-bearing tetrapods (in modern terms, terrestrial placental mammals). Among his rules were that brood size decreases with adult body mass, while lifespan increases with gestation period and with body mass, and fecundity decreases with lifespan. Thus, for example, elephants have smaller and fewer broods than mice, but longer lifespan and gestation. Rules like these concisely organized the sum of knowledge obtained by early scientific measurements of the natural world, and could be used as models to predict future observations. Among the earliest biological rules in modern times are those of Karl Ernst von Baer (from 1828 onwards) on embryonic development, and of Constantin Wilhelm Lambert Gloger on animal pigmentation, in 1833.\nThere is some scepticism among biogeographers about the usefulness of general rules. For example, J.C. Briggs, in his 1987 book \"Biogeography and Plate Tectonics\", comments that while Willi Hennig's rules on cladistics \"have generally been helpful\", his progression rule is \"suspect\".\n\n\n"}
{"id": "34017505", "url": "https://en.wikipedia.org/wiki?curid=34017505", "title": "By the Sun and Stars", "text": "By the Sun and Stars\n\nBy the Sun and Stars is an editorial documentation of an epic journey around the globe by sailboat. It was written via a logbook by Capt. Wladek Wagner while he traversed the vast oceans of the world. Published in 1986 by Cody Publications, Inc. (Kissimmee, Florida).\n"}
{"id": "47623374", "url": "https://en.wikipedia.org/wiki?curid=47623374", "title": "Carnot method", "text": "Carnot method\n\nThe Carnot method is an allocation procedure for dividing up fuel input (primary energy, end energy) in joint production processes that generate two or more energy products in one process (e.g. cogeneration or trigeneration). It is also suited to allocate other streams such as CO-emissions or variable costs. The potential to provide physical work (exergy) is used as the distribution key. For heat this potential can be assessed the Carnot efficiency. Thus, the Carnot method is a form of an exergetic allocation method. It uses mean heat grid temperatures at the output of the process as a calculation basis. The Carnot method's advantage is that no external reference values are required to allocate the input to the different output streams; only endogenous process parameters are needed. Thus, the allocation results remain unbiased of assumptions or external reference values that are open for discussion. \n\nThe fuel share a which is needed to generate the combined product electrical energy W (work) and a for the thermal energy H (useful heat) respectively, can be calculated accordingly to the first and second laws of thermodynamics as follows:\n\na= (1 · η) / (η + η · η)\n\na= (η · η) / (η + η · η)\n\nNote: a + a = 1<br>\n<br> \nwith<br>\na: allocation factor for electrical energy, i.e. the share of the fuel input which is allocated to electricity production<br>\na: allocation factor for thermal energy, i.e. the share of the fuel input which is allocated to heat production<br>\n\nη = W/Q <br>\nη = H/Q <br>\nW: electrical work <br>\nH: useful heat <br>\nQ: Total heat, fuel or primary energy input<br><br>\nand <br>\nη: Carnot factor 1-T/T (Carnot factor for electrical energy is 1)<br>\nT: lower temperature, inferior (ambient) <br>\nT: upper temperature, superior (useful heat) <br>\n\nIn heating systems, a good approximation for the upper temperature is the average between forward and return flow on the distribution side of the heat exchager.<br>\nT = (T+T) / 2 <br>\nor - if more thermodynamic precision is needed - the logarithmic mean temperature\nis used <br>\nT = (T-T) / ln(T/T) <br>\nIf process steam is delivered which condenses and evaporates at the same temperature, T is the temperature of the saturated steam of a given pressure.\n\nThe fuel intensity or the fuel factor for electrical energy f resp. thermal energy f is the relation of specific input to output.\n\nf= a / η = 1 / (η + η · η)\n\nf= a / η = η / (η + η · η)\n\nTo obtain the primary energy factors of cogenerated heat and electricity, the energy prechain needs to be considered.\n\nf = f · f <br>\nf = f · f <br>\n<br>\nwith <br>\nf: primary energy factor of the used fuel<br>\n\nThe reciprocal value of the fuel factor (f-intensity) describes the effective efficiency of the assumed sub-process, which in case of CHP is only responsible for electrical or thermal energy generation. This equivalent efficiency corresponds to the effective efficiency of a \"virtual boiler\" or a \"virtual generator\" within the CHP plant.\nη = η / a = 1 / f <br>\nη = η / a = 1 / f <br>\n<br>\nwith <br> \nη: effective efficiency of electricity generation within the CHP process<br>\nη: effective efficiency of heat generation within the CHP process<br>\n\nNext to the efficiency factor which describes the quantity of usable end energies, the quality of energy transformation according to the entropy law is also important. With rising entropy, exergy declines. Exergy does not only consider energy but also energy quality. It can be considered a product of both. Therefore any energy transformation should also be assessed according to its exergetic efficiciency or loss ratios. The quality of the product \"thermal energy\" is fundamentally determined by the mean temperature level at which this heat is delivered. Hence, the exergetic efficiency η describes how much of the fuel's potential to generate physical work remains in the joint energy products. With cogeneration the result is the following relation: \n\nη = η + η · η\n<br>\n<br>\nThe allocation with the Carnot method always results in:<br> \nη = η = η<br>\n<br>\nwith<br>\nη = exergetic efficiency of the combined process<br>\nη = exergetic efficiency of the virtual electricity-only process<br>\nη = exergetic efficiency of the virtual heat-only process<br> \n\nThe main application area of this method is cogeneration, but it can also be applied to other processes generating a joint products, such as a chiller generating cold and producing waste heat which could be used for low temperature heat demand, or a refinery with different liquid fuels plus heat as an output.\n\nLet's assume a joint production with Input \"I\" and a first output \"O\" and a second output \"O\". \"f\" is a factor for rating the relevant product in the domain of primary energy, or fuel costs, or emissions, etc. \n\nevaluation of the input = evaluation of the output\n\nf · I = f · O + f · O \n\nThe factor for the input \"f\" and the quantities of \"I\", \"O\", and \"O\" are known. An equation with two unknowns \"f\" and \"f\" has to be solved, which is possible with a lot of adequate tuples. As second equation, the physical transformation of product \"O\" in \"O\" and vice versa is used. \n\nO = η · O\n\n\"η\" is the transformation factor from \"O\" into \"O\", the inverse \"1/η\"=\"η\" describes the backward transformation. A reversible transformation is assumed, in order not to favour any of the two directions. Because of the exhangeability of \"O\" and \"O\", the assessment of the two sides of the equation above with the two factors \"f\" and \"f\" should therefore result in an equivalent outcome. Output \"O\" evaluated with \"f\" shall be the same as the amount of \"O\" generated from \"O\" and evaluated with \"f\". \n\nf · (η · O) = f · O \n\nIf we put this into the first equation, we see the following steps:\n\nf · I = f · O + f · (η × O)\n\nf · I = f · (O + η · O) \n\nf = f · (O/I + η · O/I) \n\nf = f · (η + η · η) \n\nf = f / (η + η · η)\nor respectively\nf = η · f / (η + η · η) \n\nwith \"η\" = \"O/I\" and \"η\" = \"O/I\"\n\n"}
{"id": "5007747", "url": "https://en.wikipedia.org/wiki?curid=5007747", "title": "Centers for Space Oceanography", "text": "Centers for Space Oceanography\n\nThe Centers for Space Oceanography (CSO) are an operating division of the Argos Foundation, Inc. CSO was established in August 2004 by a Memorandum of Understanding between the Argos Foundation, the Florida Aerospace Finance Corporation, the Florida Space Research Institute, and North American CLS, Inc. While operating under the corporate umbrella of the Argos Foundation, an executive appointee from each of the founding organizations form the Steering Committee of CSO. The first Chairman of the Steering Committee was Stephen Lee Morgan; the first Executive Director of CSO was Lawrence M. Harvey. In December 2005, NACLS withdrew from the Steering Committee, and was replaced by Gladius, LLC.\n\nCSO exists to develop and operate \"data fusion\" capabilities involving the observation of the Earth's oceans from space, and tracking items (ranging from man-made assets such as ships, to marine mammals and other animals) from space, enabling researchers to examine and understand the relationship of such \"mobiles\" with respect to physical oceanographic elements (such as currents, waves, ocean temperature gradients, etc.). Working with a number of Industrial Team partners, CSO and its associated university and agency researchers develop capabilities, which are then made available to the general public at no charge or at cost. Regionally specific affiliated centers are established to focus attention and resources upon a given region on the Earth's surface. The first such regional center was the Caribbean Center for Ocean Management & Research, established in Florida in 2006.\n\nOn 31 January 2006, CSO conducted its First CSO Symposium at Kennedy Space Center, Florida, hosting over 30 attendees from Government, universities, U.S. Federal and Florida agencies, and Industrial Team executives. CSO continues as a division of the not-for-profit Argos Foundation, while the affiliated regional centers are established as separate legal entities controlled by CSO, but in some cases are for-profit organizations.\n\n"}
{"id": "857973", "url": "https://en.wikipedia.org/wiki?curid=857973", "title": "Contact electrification", "text": "Contact electrification\n\nContact electrification was an erroneous scientific theory from the Enlightenment that attempted to account for all the sources of electric charge known at the time. It has since been superseded by more modern notions. In the late 18th century, scientists developed sensitive instruments for detecting 'electrification', otherwise known as electrostatic charge imbalance. The phenomenon of electrification by contact, or \"contact tension\", was quickly discovered. When two objects were touched together, sometimes the objects became spontaneously charged. One object developed a net negative charge, while the other developed an equal and opposite positive charge. Then it was discovered that 'piles' of dissimilar metal disks separated by acid-soaked cloth, Voltaic piles, could also produce charge differences. Although it was later found that these effects were caused by different physical processes - triboelectricity, the Volta effect, differing work functions of metals, and others - at the time they were all thought to be caused by a common 'contact electrification' process.\n\nThe contact electrification phenomenon allowed the construction of so-called 'frictional' electrostatic generators such as Ramsden's or Winter's machines, but it also led directly to the development of useful devices such as batteries, fuel cells, electroplating, thermocouples. Contact between materials is responsible for such modern electrical technology as semiconductor junction devices including radio detector diodes, photocells, LEDs, and thermoelectric cells.\n\nThe theory held that static electricity was generated by means of contact between dissimilar materials, and was in close agreement with the principles of static electricity as then understood. It was eventually replaced by the current theory of electrochemistry, namely, that electricity is generated by the action of chemistry and the exchange of electrons between atoms making up the battery. An important fact leading to the rejection of the theory of contact tension was the observation that corrosion, that is, the chemical degradation of the battery, seemed unavoidable with its use, and that the more electricity was drawn from the battery, the faster the corrosion proceeded.\n\nThe Volta effect (described below) corresponds to a weak electric potential difference developed by the contact of different metals. Nowadays, this is often known as a contact potential difference. This effect was first discovered by Alessandro Volta, and can be measured using a capacitance electroscope comprising different metals. However, this effect does not, by itself, account for the action of electric batteries.\n\nA number of high voltage dry piles were invented between the early 19th century and the 1830s in an attempt to determine the answer to this question, and specifically to support Volta’s hypothesis of contact tension. The Oxford Electric Bell is one example. Francis Ronalds in 1814 was one of the first to realise that dry piles also worked through chemical reaction rather than metal to metal contact, even though corrosion was not visible due to the very small currents generated.\n\nIf two different insulators are touched together, such as when a piece of rubber is touched against a piece of glass, then the surface of the rubber will acquire an excess negative charge, and the glass will acquire an equal positive charge. If the surfaces are then pulled apart, a very high voltage is produced. This so-called \"tribo\" or \"rubbing\" effect is not well understood. It may be caused by electron-stealing via quantum tunneling, or by transfer of surface ions. Friction is not required, although in many situations it greatly increases the phenomenon. Certain phenomena related to frictionally generated electrostatic charges have been known since antiquity, though of course the modern theory of electricity was developed after the Scientific Revolution.\n\nIf a piece of metal is touched against an electrolytic material, the metal will spontaneously become charged, while the electrolyte will acquire an equal and opposite charge. Upon first contact, a chemical reaction called a 'half-cell reaction' occurs on the metal surface. As metal ions are transferred to or from the electrolyte, and as the metal and electrolyte become oppositely charged, the increasing voltage at the thin insulating layer between metal and electrolyte will oppose the motion of the flowing ions, causing the chemical reaction to come to a stop. If a second piece of a different type of metal is placed in the same electrolyte bath, it will charge up and rise to a different voltage. If the first metal piece is touched against the second, the voltage on the two metal pieces will be forced closer together, and the chemical reactions will run constantly. In this way the 'contact electrification' becomes continuous. At the same time, an electric current will appear, with the path forming a closed loop which leads from one metal part to the other, through the chemical reactions on the first metal surface, through the electrolyte, then back through the chemical reactions on the second metal surface. In this way, contact electrification leads to the invention of the Galvanic cell or battery. \"See also: Dry pile\"\n\nIf two metals having differing work functions are touched together, one steals electrons from the other, and the opposite net charges grow larger and larger; this is the Volta effect. The process is halted when the difference in electric potential (electrostatic potential) between the two metals reaches a particular value, namely the difference in work function values - usually less than one volt. At this point, the Fermi levels for the two metals are equal, and there is no voltage difference between them. [If there were a voltage difference between them, then a current would flow between them: so \"zero current\" implies \"zero voltage difference\".]\n\nIf a metal touches a semiconductor material, or if two different semiconductors are placed into contact, one becomes charged slightly positive and the other slightly negative. It is found that if this junction between semiconductors is connected to a power supply, and if the power supply is set to a voltage slightly higher than the natural voltage appearing because of contact electrification, then for one polarity of voltage there will be a current between the two semiconductor parts, but if the polarity is reversed, the current stops. Thus contact between materials lead to the invention of the semiconductor diode or rectifier and triggered the revolution in semiconductor electronics and physics.\n\nIn materials with a direct band gap, if bright light is aimed at one part of the contact area between the two semiconductors, the voltage at that spot will rise, and an electric current will appear. When considering light in the context of contact electrification, the light energy is changed directly into electrical energy, allowing creation of solar cells. Later it was found that the same process can be reversed, and if a current is forced backwards across the contact region between the semiconductors, sometimes light will be emitted, allowing creation of the light-emitting diode (LED).\n"}
{"id": "50710599", "url": "https://en.wikipedia.org/wiki?curid=50710599", "title": "Cooks Monument and Reserve", "text": "Cooks Monument and Reserve\n\nCooks Monument and Reserve is a heritage-listed memorial at Charlotte Street, Cooktown, Shire of Cook, Queensland, Australia. It was designed in the office of the Queensland Colonial Architect and built by Hobbs and Carter in 1887. It was added to the Queensland Heritage Register on 30 April 1997.\n\nThe Cook Monument at Cooktown was erected in 1887 by Hobbs and Carter of South Brisbane, to designs prepared in the office of the Queensland Colonial Architect, George St Paul Connolly.\n\nCooktown had been established in September 1873 as the Endeavour River port for the Palmer River goldfields. By the mid-1880s it was serving one of the most significant goldfields in Queensland. A railway was constructed from Cooktown to Laura between 1884 and 1888, further opening the port to development. By the late 1880s Cooktown had become the important centre not only of a thriving mining district (boosted by the 1887 discovery of tin along the Annan River), but also of pearling, beche-de-mer, and pastoral activity. After 1885, Cooktown was also the main port for Queensland trade with New Guinea.\n\nIn May 1886, the then Premier of Queensland, Sir Samuel Walker Griffith, travelled to Cooktown and other northern ports on a tour of inspection, partly associated with defence issues, more with political strategy. The Separation movement was gaining momentum in Townsville and the far north, and Griffith was keen to demonstrate his government's commitment to, and concern for, northern Queensland. The Cooktown Municipal Council at this time was seeking to commemorate James Cook's encampment at the Endeavour River in the winter of 1770, and it appears that Griffith, as political expediency, committed the Colonial Architect's office to designing the Cook monument at Cooktown.\n\nThe Cooktown Municipal Council had intended that the monument include a statue of James Cook, but when tenders were called in June 1886, then again in September 1886 and February 1887 with different materials specified, they were for the column only. In December 1887 Sir Samuel Griffith re-visited Cooktown, and when requested by the mayor to provide assistance in commissioning the statue, refused, stating that it was more important to commemorate the event than the person.\n\nThe contract had been let in February 1887 to Hobbs and Carter of South Brisbane, with a price of and completion time of 8 months. It appears that the memorial was not constructed on the site originally proposed, which necessitated an additional foundation, bringing the final cost to . The foundation stone was laid on 1 September 1887 by the Mayor of Cooktown, John Savage, and the work had been completed by January 1888. Sandstone used in the construction came from Murphys Creek in southeast Queensland, and it was reputedly the first stone structure erected in Cooktown.\n\nAround the monument, a small () reserve for municipal purposes was gazetted in June 1888. A brick-lined town well, believed to have been built by the Cooktown Municipal Council to service ships berthed at the nearby Cooktown wharves, was constructed in the reserve, south of the monument. This work probably was undertaken post-1888, as the Council filled in much of the Cook Monument Reserve in 1889. A small vertical boiler engine pumped water from the well to the wharves through a 3\" diameter pipe. It is understood that the well has also supplied adjacent Charlotte Street businesses from time to time.\n\nIn the 20th century, several other memorials were erected within the reserve:\n\nCook's Monument is situated in a small reserve located between Charlotte Street and the Endeavour River estuary, on a slight rise adjacent to the site where James Cook beached the barque Endeavour for repairs in mid-1770. The reserve is grassed, with palms and mature trees around the perimeter.\n\nThe monument comprises a tall, slender sandstone column, topped with a detailed capital, which rises from a square pedestal resting upon a granite base and plinth. The steps of the plinth lead to a square dado incorporating drinking fountains on each side. These fountains were once linked to a regular water supply [possibly an internal tank], and the water outlets emerge from the mouths of sculpted animal heads. The dado is set beneath a modest entablature and the pediment is flanked by sandstone urns at each corner.\n\nTo the north of the Cook Monument is the 1803 cannon sent in 1885 to Cooktown by the Queensland colonial government, for the use of the local volunteer defence force. It stands on a concrete platform. Adjacent to the gun is an interpretative sign.\n\nTo the south of the Cook Monument, but within the same reserve, are three more recent memorials and the early town well:\n\nCooks Monument and Reserve was listed on the Queensland Heritage Register on 30 April 1997 having satisfied the following criteria.\n\nThe place is important in demonstrating the evolution or pattern of Queensland's history.\n\nCook's Monument and Reserve is significant historically for its commemoration of Cook's encampment below Grassy Hill, at the mouth of the Endeavour River, in mid-1770 - the first official British sojourn on the east coast of Australia. The place is significant also as Queensland's earliest memorial to James Cook and the crew of the Endeavour, and illustrates Cooktown's early developed sense of historical importance.\n\nThe site contains an early brick-lined town well, which is important in illustrating the establishment of municipal services in early Cooktown.\n\nThe place is important in demonstrating the principal characteristics of a particular class of cultural places.\n\nThe monument is a fine example of its type and one of the finest sandstone public memorials in Queensland. It is also an excellent example of the work of the Queensland colonial architect's office under George St Paul Connolly.\n\nThe place is important because of its aesthetic significance.\n\nIts aesthetic setting within a grassy, treed reserve contributes significantly to the townscape of Cooktown, and from the Endeavour River estuary is a Cooktown landmark.\n\nThe place has a strong or special association with a particular community or cultural group for social, cultural or spiritual reasons.\n\nThe place has a special association for the people of Cooktown with their sense of historical identity. A number of other memorials have been placed in the reserve, illustrating this continued association.\n"}
{"id": "2865779", "url": "https://en.wikipedia.org/wiki?curid=2865779", "title": "Deneb el Okab", "text": "Deneb el Okab\n\nThe traditional star name Deneb el Okab refers to two stars in the Aquila constellation:\n\nThe name derives from the Arabic term ذنب العقاب \"ðanab al-uqāb\" meaning \"the tail of the eagle\".\n\n"}
{"id": "41326124", "url": "https://en.wikipedia.org/wiki?curid=41326124", "title": "Ecoult", "text": "Ecoult\n\nEcoult is an energy storage company based out of Australia.\n\nEcoult was originally formed in 2007 by the Australian Commonwealth Scientific and Industrial Research Organisation (CSIRO). CSIRO invented the UltraBattery, a hybrid ultracapacitor and lead-acid battery technology, and formed a subsidiary company, Ecoult, to commercialize the technology.\n\nIn May 2010, East Penn Manufacturing acquired Ecoult from CSIRO, along with the global license to manufacture the UltraBattery (outside Japan and Thailand where Furukawa Battery holds the head license).\n\nEcoult was named in the Cleantech Group's 2013 Global Cleantech 100 in 2013.\n\n"}
{"id": "23830502", "url": "https://en.wikipedia.org/wiki?curid=23830502", "title": "Erosion index", "text": "Erosion index\n\nThe erosion index (EI, also called the erodibility index) is created by dividing potential erosion (from all sources except gully erosion) by the T value, which is the rate of soil erosion above which long term productivity may be adversely affected. The erodibility index is used in conjunction with conservation compliance and the Conservation Reserve Program. For example, one of the eligibility requirements for the CRP is that land have an EI greater than 8.\n"}
{"id": "6818021", "url": "https://en.wikipedia.org/wiki?curid=6818021", "title": "Evacuation tip", "text": "Evacuation tip\n\nThe evacuation tip is a port on any glass envelope or vessel inside of which specific gasses or a vacuum must be held. They are very often seen on nixies or vacuum tubes. It used to evacuate gases from the tube, and in the case of nixies, add the proper mix of gases before being sealed. It provides a convenient port for the amateur glasshacker to introduce gases to alter the performance of the tube. It is also known as the \"tubulation\" or \"pip\".\n"}
{"id": "762047", "url": "https://en.wikipedia.org/wiki?curid=762047", "title": "Freshwater ecosystem", "text": "Freshwater ecosystem\n\nFreshwater ecosystems are a subset of Earth's aquatic ecosystems. They include lakes and ponds, rivers, streams, springs, and wetlands. They can be contrasted with marine ecosystems, which have a larger salt content. Freshwater habitats can be classified by different factors, including temperature, light penetration, nutrients, and vegetation.\n\nFreshwater ecosystems can be divided into lentic ecosystems (still water) and lotic ecosystems (flowing water).\n\nLimnology (and its branch freshwater biology) is a study about freshwater ecosystems. It is a part of hydrobiology.\n\nOriginal attempts to understand and monitor freshwater ecosystems were spurred on by threats to human health (ex. Cholera outbreaks due to sewage contamination). Early monitoring focused on chemical indicators, then bacteria, and finally algae, fungi and protozoa. A new type of monitoring involves quantifying differing groups of organisms (macroinvertebrates, macrophytes and fish) and measuring the stream conditions associated with them. \n\nFive broad threats to freshwater biodiversity include overexploitation, water pollution, flow modification, destruction or degradation of habitat, and invasion by exotic species. Recent extinction trends can be attributed largely to sedimentation, stream fragmentation, chemical and organic pollutants, dams, and invasive species. Common chemical stresses on freshwater ecosystem health include acidification, eutrophication and copper and pesticide contamination.\n\nOver 123 freshwater fauna species have gone extinct in North America since 1900. Of North American freshwater species, an estimated 48.5% of mussels, 22.8% of gastropods, 32.7% of crayfishes, 25.9% of amphibians, and 21.2% of fish are either endangered or threatened. Extinction rates of many species may increase severely into the next century because of invasive species, loss of keystone species, and species which are already functionally extinct (e.g., species which are not reproducing). Even using conservative estimates, freshwater fish extinction rates in North America are 877 times higher than background extinction rates (1 in 3,000,000 years). Projected extinction rates for freshwater animals are around five times greater than for land animals, and are comparable to the rates for rainforest communities. \n\nCurrent freshwater biomonitoring techniques focus primarily on community structure, but some programs measure functional indicators like biochemical (or biological) oxygen demand, sediment oxygen demand, and dissolved oxygen. Macroinvertebrate community structure is commonly monitored because of the diverse taxonomy, ease of collection, sensitivity to a range of stressors, and overall value to the ecosystem. Additionally, algal community structure (often using diatoms) is measured in biomonitoring programs. Algae are also taxonomically diverse, easily collected, sensitive to a range of stressors, and overall valuable to the ecosystem. Algae grow very quickly and communities may represent fast changes in environmental conditions. \n\nIn addition to community structure, responses to freshwater stressors are investigated by experimental studies that measure organism behavioural changes, altered rates of growth, reproduction or mortality. Experimental results on single species under controlled conditions may not always reflect natural conditions and multi-species communities. \n\nThe use of reference sites is common when defining the idealized \"health\" of a freshwater ecosystem. Reference sites can be selected spatially by choosing sites with minimal impacts from human disturbance and influence. However, reference conditions may also be established temporally by using preserved indicators such as diatom valves, macrophyte pollen, insect chitin and fish scales can be used to determine conditions prior to large scale human disturbance. These temporal reference conditions are often easier to reconstruct in standing water than moving water because stable sediments can better preserve biological indicator materials. \n\n\n"}
{"id": "2381969", "url": "https://en.wikipedia.org/wiki?curid=2381969", "title": "Fuath", "text": "Fuath\n\nA fuath (plural \"fuathan\"; ) or vough (phonetic transcription), literally meaning \"hate\" in Scottish Gaelic and Irish Gaelic, designates a class of malevolent Highland Gaelic mythological water spirits, inhabiting the sea, rivers, fresh water, or sea lochs.\n\nAs a generic term the \"fuath\" can include the beithir (behir), peallaidh and ùruisg.\nThe \"fuath\" in collected Gaelic folklore is substituted by the term \"kelpie\" in the English translation and commentary portions of John Francis Campbell's \"Popular Tales of the West Highlands\". More broadly, this name is even given to highland or nature spirits. \n\nTheir appearance ranges from covered in shaggy, yellow fur to just having a mane down its back, webbed toes, tails with spikes, and no nose. They are prone to wearing green, whether it be a dress, robe, or kirtle, as it is the colour of faeries.\n\nThey sometimes intermarry with human beings (typically the female), whose offspring will share a mane, tail, and/or webbed digits. Their banes include sunlight and cold steel, which will kill them instantly. They grow restless upon crossing a stream.\n\nAn alternative name for this class of monsters is Arrachd.\n\nSimilarity or equivalence to the bean nighe or Northern Ireland's \"uisges\" have been noted.\n\nIn \"The Brollachan\" (\"Popular Tales\" II, Tale #37), a fuath is represented as being the mother of the brollachan (or brollichan), as a creature with eyes and a mouth but no shape, and limited ability of speech. When Murray or \"Ally\" of the mill stoked the fire with fresh peat, it caused severe burns to the brollachan lying there, prompting a fuath who was the brollachan's mother to appear, demanding vengeance for her son. But the misshapen brollachan, when asked about the identity of the culprit, could only answer \"me\" and \"you,\" and the fuath abandoned. Campbell draws the parallel with the story of the Cyclops, who when attempting to name the man who blinded him could only say that it was \"No man\" who harmed him, because that was the pseudonym that Odysseus had given. This motif is also used in the Northumbrian tale of Ainsel or \"My Own Self\".\n\nCitations\n\nBibliography\n\n"}
{"id": "252572", "url": "https://en.wikipedia.org/wiki?curid=252572", "title": "Glory (optical phenomenon)", "text": "Glory (optical phenomenon)\n\nA glory is an optical phenomenon, resembling an iconic saint's halo around the shadow of the observer's head, caused by sunlight or (more rarely) moonlight interacting with the tiny water droplets that compose mist or clouds. The glory consists of one or more concentric, successively dimmer rings, each of which is red on the outside and bluish towards the centre. Due to its appearance, the phenomenon is sometimes mistaken for a circular rainbow, but the latter has a much larger diameter and is caused by different physical processes.\n\nGlories arise due to wave interference of light internally refracted within small droplets.\n\nDepending on circumstances (such as the uniformity of droplet size in the clouds), one or more of the glory's rings can be visible. The angular size of the inner and brightest ring is much smaller than that of a rainbow, about 5° to 20°, depending on the size of the droplets. In the right conditions, a glory and a rainbow can occur simultaneously.\n\nLike a rainbow, a glory is centered on the antisolar (or, in case of the Moon, antilunar) point, which coincides with the shadow of the observer's head. Since this point is by definition diametrically opposed to the Sun's (or Moon's) position in the sky, it always lies below the observer's horizon when the Sun (Moon) is up. In order to see a glory, therefore, the clouds or fog causing it must be located \"below\" the observer, in a straight line with the Sun/Moon and the observer's eye. Hence, the glory is commonly observed from a high viewpoint such as a mountain, tall building or from an aircraft. In the latter case, if the plane is flying sufficiently low for its shadow to be visible on the clouds, the glory always surrounds it. This is sometimes called \"The Glory of the Pilot\".\n\nWhen viewed from a mountain or tall building, glories are often seen in association with a Brocken spectre, also called Mountain Spectre, the apparently enormously magnified shadow of an observer, cast (when the Sun is low) on clouds below the mountain on which the viewer is standing. The name derives from the Brocken, the tallest peak of the Harz mountain range in Germany. Because the peak is above the cloud level and the area frequently misty, conditions conducive to casting a shadow on a cloud layer are common. Giant shadows that seemed to move by themselves due to movement of the cloud layer (this movement is another part of the definition of the Brocken Spectre), and that were surrounded by glories, may have contributed to the reputation the Harz mountains hold as a refuge for witches and evil spirits. In Goethe's Faust, the Brocken is called the \"Blocksberg\" and is the site of the Witches' Sabbath on Walpurgis Night.\n\nThe scientific explanation is still the subject of debates and research. In 1947, the Dutch astronomer Hendrik van de Hulst suggested that surface waves are involved. He speculated that the colored rings of the glory are caused by two-ray interference between \"short\" and \"long\" path surface waves—which are generated by light rays entering the droplets at diametrically opposite points (both rays suffer one internal reflection). A new theory by Brazilian physicist Herch Moysés Nussenzveig, however, suggests that the light energy beamed back by a glory originates mostly from classical wave tunneling (synonymous in the paper to the evanescent wave coupling), which is an interaction between an evanescent light wave traveling along the surface of the drop and the waves inside the drop.\n\nC. T. R. Wilson saw a glory while working as a temporary observer at the Ben Nevis weather station. Inspired by the impressive sight, he decided to build a device for creating clouds in the laboratory, so that he could make a synthetic, small-scale glory. His work led directly to the cloud chamber, a device for detecting ionizing radiation for which he and Arthur Compton received the Nobel Prize for Physics in 1927.\n\nIn China, the phenomenon is called Buddha's light (or halo). It is often observed on cloud-shrouded high mountains, such as Huangshan and Mount Emei. Records of the phenomenon at Mount Emei date back to A.D. 63. The colorful halo always surrounds the observer's own shadow, and thus was often taken to show the observer's personal enlightenment (associated with Buddha or divinity).\n\nStylized glories appear occasionally in Western heraldry. Two glories appear on the Great Seal of the United States: A glory breaking through clouds surrounding a cluster of 13 stars on the obverse, and a glory surrounding the Eye of Providence surmounting an unfinished pyramid on the reverse.\n\nLeo Frankowski made glories a key plot element in his \"Conrad Stargard\" saga, where the protagonist and title character is sent back in time to the 13th century where he has to establish himself and cope with various crises including planning for the eventual Mongol invasion of Eastern Europe in 1241. In the third book, \"The Radiant Warrior\", Stargard begins building a modern army and uses the reliable glories along one stretch of his boot camp to invoke religious faith-backed \"esprit de corps\" and feelings of elite invincibility in his newly forming cadre. The same phenomenon dupes the highly pious heir apparent of the Polish duchy into strongly supporting the new model army's pragmatic departures from the day's chivalristic practices.\n\nThis atmospheric effect also makes at least one appearance in Gothic fiction. In James Hogg's \"The Private Memoirs and Confessions of a Justified Sinner\", George Colwan walks to the top of Arthur's Seat on a foggy day, while his half-brother Robert Wringhim secretly follows him with murderous intent. George sees shimmering colored light in front of him. Then he sees the shadow of an enormous dark figure advancing toward him threateningly—the Brocken spectre created by the shadow of Robert sneaking up behind him. In other words, the \"good\" George is surrounded by a glory, while the \"evil\" Robert appears as a dark spectre.\n\nIn Masami Kurumada's \"Saint Seiya\" comic book, which is inspired by Greek mythology, warriors belonging to divine armies battle each other for possession of the Earth. The main characters, warriors known as Saints, belong to Athena's army, and one of the antagonistic armies they face belongs to the Olympic Gods, composed of warriors called the Angels. As the Saints wear protective armors called Cloths, which represent the 88 astronomical constellations, the Angels don similar attire, their armors being known as Glory, which Kurumada named after the optical phenomenon in reference to it being traditionally associated to angels in religious imagery, and the Glory armors represent angels in different poses and sizes.\n\n\n"}
{"id": "12374722", "url": "https://en.wikipedia.org/wiki?curid=12374722", "title": "Harry Frauca", "text": "Harry Frauca\n\nHarry Frauca (born 14 October 1928) was an Australian naturalist, writer and photographer. Of Catalan origin, he moved to Australia in the 1950s and became an Australian citizen. From 1960 he became a full-time writer and photographer on natural history. From 1970 he collected insects for the Australian National Insect Collection. The last years of his life were spent in Bundaberg, Queensland, with his wife Claudia dying in 1979.\n\nHe has been honoured in the name of the Harry Frauca Walkway, a 200 m walkway at Baldwin Swamp, Bundaberg, as well as the Harry Frauca Walking Track and Harry Frauca Information Panel at the Mount Walsh National Park, Biggenden.\n\nAs well as articles in \"Walkabout\" and elsewhere, books authored or coauthored by Frauca include:\n\n"}
{"id": "57146914", "url": "https://en.wikipedia.org/wiki?curid=57146914", "title": "Hydrogen Council", "text": "Hydrogen Council\n\nThe Hydrogen Council is a group of 39 companies within energy, transport and industry who are working together to promote hydrogen as a key component of the energy transition.\n\nThe Hydrogen Council announced their formation at the World Economic Forum in Davos on January 7, 2017. The aims of the initiative are 'to explain why hydrogen emerges among the key solutions for the energy transition, in the mobility as well as in the power, industrial and residential sectors' as expressed by the CEO of Air Liquide in one of two formal statements made at the Forum. $10.7 billion were announced as being made available to support the aims of the council over a 5-year period.\n\nSteering members include 3M, Air Liquide S.A., Alstom, Anglo American plc, Audi AG, BMW Group, Bosch, China Energy, Daimler AG, Engie S.A., GM, Great Wall Motor, Honda Motor Co. Ltd, Hyundai Motor Company, Iwatani Corporation, JXTG Nippon Oil & Energy Corporation, Kawasaki Heavy Industries Ltd., Plastic Omnium, Royal Dutch Shell, Statoil ASA, The Linde Group, Total S.A., Toyota Motor Corporation and Weichai.\n\nSupporting members include Ballard, Plug Power, Faber Industries, Faurecia, First Element Fuel (True Zero), Gore, Hexagon Composites, Hydrogenics, Marubeni, McPhy, Mitsubishi, Mitsui & Co, Nel Hydrogen, Royal Vopak and Toyota Tsusho.\n\nTwo reports have been released by the Council ('How hydrogen empowers the energy transition' January 2017, and 'Hydrogen scaling up: A sustainable pathway for the global energy transition' November 2017) outlining a path towards increased use of hydrogen as an energy carrier and feedstock within transport, industry applications, for residential and commercial building heat and for electricity generation. The reports were authored by the Study Task Force of the Hydrogen Council with analytical support from McKinsey & Company.\n\nScenarios outlined in the reports suggest that hydrogen technologies could contribute to meeting 18% of the world’s final energy demands, avoiding 6 Gt of CO2 emissions and creating a market with revenues of $2.5 trillion each year while providing 30 million jobs by mid-century.\n\nInvestments of $20–25 billion annually for a total of $280 billion up to 2030 are outlined within the reports.\n"}
{"id": "43441652", "url": "https://en.wikipedia.org/wiki?curid=43441652", "title": "Hydrogenics", "text": "Hydrogenics\n\nHydrogenics is a developer and manufacturer of hydrogen generation and fuel cell products based on water electrolysis and proton exchange membrane (PEM) technology. Hydrogenics is divided into two business units: OnSite Generation and Power Systems. Onsite Generation is headquartered in Oevel, Belgium and had 73 full-time employees as of December 2013. Power Systems is based in Mississauga, Ontario, Canada, with a satellite facility in Gladbeck, Germany. It had 62 full-time employees as of December 2013. Hydrogenics maintains operations in Belgium, Canada and Germany with satellite offices in the United States, Indonesia, Malaysia and Russia.\n\nDaryl Wilson is president and CEO of Hydrogenics. He earned his bachelor's degree in Chemical Engineering from the University of Toronto, and his MBA in Operations Management and Management Science from McMaster University in 1990. Prior to working for Hydrogenics, he held senior leadership positions at Royal Group Technologies, Zenon Environmental, Toyota and Dofasco.\n\nThe OnSite Generation business segment is based on water electrolysis technology, which involves the decomposition of water into oxygen (O2) and hydrogen gas (H2) by passing an electric current through a liquid electrolyte. The resultant hydrogen gas is then captured and used for industrial gas applications, hydrogen fueling applications, and is used to store renewable and surplus energy in the form of hydrogen gas. Hydrogenics' HySTAT electrolyzer products can be used both indoors and outdoors.\n\nThe Power Systems business segment is based on PEM fuel cell technology, which transforms chemical energy resulting from the electrochemical reaction of hydrogen and oxygen into electrical energy. (Edgar) Its HyPM products can handle electrical power outputs ranging from 1 kilowatt to 1 megawatt. The company also develops and delivers hydrogen generation products based on PEM water electrolysis.\n\nPower-to-Gas is an energy process and storage technology, which takes the excess power generated by wind turbines, solar power, or biomass power plants and converts carbon dioxide and water into methane using electrolysis, enabling it be stored. The excess electricity can then be held in existing reserves, including power and natural gas grids. This allows for seasonally adjusted storage of significant amounts of power and the provision of CO2-neutral fuels in the form of the resulting renewable energy source gas.\n\nIn 1988, Hydrogenics was founded under the name Traduction Militech Translation Inc. In 1995, it entered into the fuel cell technology development business and Traduction Militech Translation changed its name to Hydrogenics in 1990.\n\nIn 2002, Hydrogenics acquired EnKAT GmbH, which formed its Hydrogenics Europe division. It also acquired Greenlight Power Technologies, Inc., a competing fuel cell testing business, in 2003. A year later, in 2004, the company acquired Stuart Energy, a manufacturer of hydrogen-generation products based on alkaline electrolyte technology.\n\nIn 2007, Hydrogenics narrowed the focus of its fuel cell activities by exiting the fuel cell testing business and working more on forklift power and backup power markets. That same year, Heliocentris partnered with Hydrogenics and SMA Solar Technologie to incorporate Hydrogenics' fuel cell power modules into stationary backup power systems.\n\nIn September 2010, Hydrogenics formed an alliance with CommScope Inc., a Hickory, North Carolina-based multinational telecommunications company. Per the alliance, CommScope invested US$8.5 million in Hydrogenics as part of a joint product development program.\n\nHydrogenics signed a Memorandum of Understanding (MoU) with Iwatani Corporation, a Japanese industrial energy company, in April 2012. The companies began to collaborate on hydrogen solutions in the Japanese energy market, including utility-scale hydrogen energy storage, hydrogen generation and fuelling, fuel cell integration, and industrial hydrogen generation. Later that month Hydrogenics and Enbridge Inc. entered into a joint venture to develop utility-scale energy storage beginning in Ontario. Under the agreement, hydrogen produced during periods of excess renewable generation will be injected into Enbridge's existing natural gas pipeline network. In June 2013, Hydrogenics announced that its Power-to-Gas facility was operational with the first direct injection of hydrogen into a gas pipeline.\n\nHydrogenics entered into a joint venture with South Korea-based Kolon Water & Energy to provide power generation in that country in June 2014.\n\nIn June 2000, General Motors and Hydrogenics released their codeveloped HydroGen1, a vehicle powered by a first generation proton exchange membrane fuel cell system. The following year, in October, the two companies developed low-pollution technology to power cars and trucks.\n\nIn December 2002, Natural Resources Canada (NRCan) selected Hydrogenics to develop a next-generation hybrid fuel cells bus; Hydrogenics integrated its vehicle-to-grid technology into a 12.5 meter New Flyer Inverno 40i transit bus. Hydrogenics' FC Hybrid Tecnobus midibus was exhibited in Europe in 2005.\n\nIn January 2010, Hydrogenics began development of a next-generation power system to be used for surface mobility applications on the moon for the Canadian Space Agency. The system includes an electrolyzer that produces both hydrogen and oxygen using solar power, and a fuel cell system that can be used for mobility, auxiliary, and life support systems. Heliocentris and FAUN Umwelttechnick collaborated with Hydrogenics to develop a hybrid waste disposal vehicle for BSR (Berliner Stadtreinigung) in August of that year.\n\nIn July 2012, Hydrogenics joined a consortium with EU members to build the world’s largest steady state hydrogen storage facility in the Puglia region of Italy. The system is part of the R&D smart grid project \"INGRID.\"\n\nIn April 2013, Hydrogenics won a contract to supply a 1 megawatt hydrogen energy storage system to German utility E.ON in Hamburg. The system will use electrolyzers based on Hydrogenics' proton exchange membrane (PEM) technology for hydrogen production and use excess power generated from regional renewable energy sources, primarily wind energy. In November the first of E.ON's P2G facilities provided by Hydrogenics became operational. The Falkenhagen facility uses wind-powered electrolysis equipment to transform water to hydrogen, which is then mixed with natural gas.\n\nIn February 2014, Hydrogenics was awarded two projects with the United Kingdom government. Hydrogenics will provide its technology to build hydrogen fuel stations throughout the UK.\n\nHydrogenics was selected as a Preferred Respondent for a power-to-gas project in Ontario by the Independent Electricity System Operator. (IESO), a corporation responsible for operating the electricity market and directing the operation of the bulk electrical system in the province of Ontario, Canada, in July 2014.\n\n\n"}
{"id": "14895", "url": "https://en.wikipedia.org/wiki?curid=14895", "title": "Insulin", "text": "Insulin\n\nInsulin (from Latin \"insula\", island) is a peptide hormone produced by beta cells of the pancreatic islets; it is considered to be the main anabolic hormone of the body. It regulates the metabolism of carbohydrates, fats and protein by promoting the absorption of carbohydrates, especially glucose from the blood into liver, fat and skeletal muscle cells. In these tissues the absorbed glucose is converted into either glycogen via glycogenesis or fats (triglycerides) via lipogenesis, or, in the case of the liver, into both. Glucose production and secretion by the liver is strongly inhibited by high concentrations of insulin in the blood. Circulating insulin also affects the synthesis of proteins in a wide variety of tissues. It is therefore an anabolic hormone, promoting the conversion of small molecules in the blood into large molecules inside the cells. Low insulin levels in the blood have the opposite effect by promoting widespread catabolism, especially of reserve body fat.\nBeta cells are sensitive to glucose concentrations, also known as blood sugar levels. When the glucose level is high, the beta cells secrete insulin into the blood; when glucose levels are low, secretion of insulin is inhibited. Their neighboring alpha cells, by taking their cues from the beta cells, secrete glucagon into the blood in the opposite manner: increased secretion when blood glucose is low, and decreased secretion when glucose concentrations are high. Glucagon, through stimulating the liver to release glucose by glycogenolysis and gluconeogenesis, has the opposite effect of insulin. The secretion of insulin and glucagon into the blood in response to the blood glucose concentration is the primary mechanism of glucose homeostasis.\nIf beta cells are destroyed by an autoimmune reaction, insulin can no longer be synthesized or be secreted into the blood. This results in type 1 diabetes mellitus, which is characterized by abnormally high blood glucose concentrations, and generalized body wasting. In type 2 diabetes mellitus the destruction of beta cells is less pronounced than in type 1 diabetes, and is not due to an autoimmune process. Instead there is an accumulation of amyloid in the pancreatic islets, which likely disrupts their anatomy and physiology. The pathogenesis of type 2 diabetes is not well understood but patients exhibit a reduced population of islet beta-cells, reduced secretory function of islet beta-cells that survive, and peripheral tissue insulin resistance. Type 2 diabetes is characterized by high rates of glucagon secretion into the blood which are unaffected by, and unresponsive to the concentration of glucose in the blood. Insulin is still secreted into the blood in response to the blood glucose. As a result, the insulin levels, even when the blood sugar level is normal, are much higher than they are in healthy persons.\nThe human insulin protein is composed of 51 amino acids, and has a molecular mass of 5808 Da. It is a dimer of an A-chain and a B-chain, which are linked together by disulfide bonds. Insulin's structure varies slightly between species of animals. Insulin from animal sources differs somewhat in effectiveness (in carbohydrate metabolism effects) from human insulin because of these variations. Porcine insulin is especially close to the human version, and was widely used to treat type 1 diabetics before human insulin could be produced in large quantities by recombinant DNA technologies.\nThe crystal structure of insulin in the solid state was determined by Dorothy Hodgkin. It is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.\n\nInsulin may have originated more than a billion years ago. The molecular origins of insulin go at least as far back as the simplest unicellular eukaryotes. Apart from animals, insulin-like proteins are also known to exist in the Fungi and Protista kingdoms.\n\nInsulin is produced by beta cells of the pancreatic islets in most vertebrates and by the Brockmann body in some teleost fish. Cone snails \"Conus geographus\" and \"Conus tulipa\", venomous sea snails that hunt small fish, use modified forms of insulin in their venom cocktails. The insulin toxin, closer in structure to fishes' than to snails' native insulin, slows down the prey fishes by lowering their blood glucose levels.\n\nThe preproinsulin precursor of insulin is encoded by the \"INS\" gene.\n\nA variety of mutant alleles with changes in the coding region have been identified. A read-through gene, INS-IGF2, overlaps with this gene at the 5' region and with the IGF2 gene at the 3' region.\n\nIn the pancreatic β cells, glucose is the primary physiological stimulus for the regulation of insulin synthesis. Insulin is mainly regulated through the transcription factors PDX1, NeuroD1, and MafA.\n\nPDX1 (Pancreatic and duodenal homeobox protein 1) is in the nuclear periphery upon low blood glucose levels interacting with corepressors HDAC1 and 2 which is downregulating the insulin secretion. An increase in blood glucose levels causes phosphorylation of PDX1 and it translocates centrally and binds the A3 element within the insulin promoter. Upon translocation it interacts with coactivators HAT p300 and acetyltransferase set 7/9. PDX1 affects the histone modifications through acetylation and deacetylation as well as methylation. It is also said to suppress glucagon.\n\nNeuroD1, also known as β2, regulates insulin exocytosis in pancreatic β cells by directly inducing the expression of genes involved in exocytosis. It is localized in the cytosol, but in response to high glucose it becomes glycosylated by OGT and/or phosphorylated by ERK, which causes translocation to the nucleus. In the nucleus β2 heterodimerizes with E47, binds to the E1 element of the insulin promoter and recruits co-activator p300 which acetylates β2. It is able to interact with other transcription factors as well in activation of the insulin gene.\n\nMafA is degraded by proteasomes upon low blood glucose levels. Increased levels of glucose make an unknown protein glycosylated. This protein works as a transcription factor for MafA in an unknown manner and MafA is transported out of the cell. MafA is then translocated back into the nucleus where it binds the C1 element of the insulin promoter.\n\nThese transcription factors work synergistically and in a complex arrangement. Increased blood glucose can after a while destroy the binding capacities of these proteins, and therefore reduce the amount of insulin secreted, causing diabetes. The decreased binding activities can be mediated by glucose induced oxidative stress and antioxidants are said to prevent the decreased insulin secretion in glucotoxic pancreatic β cells. Stress signalling molecules and reactive oxygen species inhibits the insulin gene by interfering with the cofactors binding the transcription factors and the transcription factors it self.\n\nSeveral regulatory sequences in the promoter region of the human insulin gene bind to transcription factors. In general, the A-boxes bind to Pdx1 factors, E-boxes bind to NeuroD, C-boxes bind to MafA, and cAMP response elements to CREB. There are also silencers that inhibit transcription.\n\nWithin vertebrates, the amino acid sequence of insulin is strongly conserved. Bovine insulin differs from human in only three amino acid residues, and porcine insulin in one. Even insulin from some species of fish is similar enough to human to be clinically effective in humans. Insulin in some invertebrates is quite similar in sequence to human insulin, and has similar physiological effects. The strong homology seen in the insulin sequence of diverse species suggests that it has been conserved across much of animal evolutionary history. The C-peptide of proinsulin (discussed later), however, differs much more among species; it is also a hormone, but a secondary one.\nThe primary structure of bovine insulin was first determined by Frederick Sanger in 1951. After that, this polypeptide was synthesized independently by several groups. The 3-dimensional structure of insulin was determined by X-ray crystallography in Dorothy Hodgkin's laboratory in 1969 (PDB file 1ins).\n\nInsulin is produced and stored in the body as a hexamer (a unit of six insulin molecules), while the active form is the monomer. The hexamer is an inactive form with long-term stability, which serves as a way to keep the highly reactive insulin protected, yet readily available. The hexamer-monomer conversion is one of the central aspects of insulin formulations for injection. The hexamer is far more stable than the monomer, which is desirable for practical reasons; however, the monomer is a much faster-reacting drug because diffusion rate is inversely related to particle size. A fast-reacting drug means insulin injections do not have to precede mealtimes by hours, which in turn gives people with diabetes more flexibility in their daily schedules. Insulin can aggregate and form fibrillar interdigitated beta-sheets. This can cause injection amyloidosis, and prevents the storage of insulin for long periods.\n\nInsulin is produced in the pancreas and the Brockmann body (in some fish), and released when any of several stimuli are detected. These stimuli include ingested protein and glucose in the blood produced from digested food. Carbohydrates can be polymers of simple sugars or the simple sugars themselves. If the carbohydrates include glucose, then that glucose will be absorbed into the bloodstream and blood glucose level will begin to rise. In target cells, insulin initiates a signal transduction, which has the effect of increasing glucose uptake and storage. Finally, insulin is degraded, terminating the response.\n\nIn mammals, insulin is synthesized in the pancreas within the beta cells. One million to three million pancreatic islets form the endocrine part of the pancreas, which is primarily an exocrine gland. The endocrine portion accounts for only 2% of the total mass of the pancreas. Within the pancreatic islets, beta cells constitute 65–80% of all the cells.\n\nInsulin consists of two polypeptide chains, the A- and B- chains, linked together by disulfide bonds. It is however first synthesized as a single polypeptide called preproinsulin in beta cells. Preproinsulin contains a 24-residue signal peptide which directs the nascent polypeptide chain to the rough endoplasmic reticulum (RER). The signal peptide is cleaved as the polypeptide is translocated into lumen of the RER, forming proinsulin. In the RER the proinsulin folds into the correct conformation and 3 disulfide bonds are formed. About 5–10 min after its assembly in the endoplasmic reticulum, proinsulin is transported to the trans-Golgi network (TGN) where immature granules are formed. Transport to the TGN may take about 30 min.\n\nProinsulin undergoes maturation into active insulin through the action of cellular endopeptidases known as prohormone convertases (PC1 and PC2), as well as the exoprotease carboxypeptidase E. The endopeptidases cleave at 2 positions, releasing a fragment called the C-peptide, and leaving 2 peptide chains, the B- and A- chains, linked by 2 disulfide bonds. The cleavage sites are each located after a pair of basic residues (lysine-64 and arginine-65, and arginine-31 and −32). After cleavage of the C-peptide, these 2 pairs of basic residues are removed by the carboxypeptidase. The C-peptide is the central portion of proinsulin, and the primary sequence of proinsulin goes in the order \"B-C-A\" (the B and A chains were identified on the basis of mass and the C-peptide was discovered later).\n\nThe resulting mature insulin is packaged inside mature granules waiting for metabolic signals (such as leucine, arginine, glucose and mannose) and vagal nerve stimulation to be exocytosed from the cell into the circulation.\n\nThe endogenous production of insulin is regulated in several steps along the synthesis pathway:\n\nInsulin and its related proteins have been shown to be produced inside the brain, and reduced levels of these proteins are linked to Alzheimer's disease.\n\nInsulin release is stimulated also by beta-2 receptor stimulation and inhibited by alpha-1 receptor stimulation.  In addition, cortisol, glucagon and growth hormone antagonize the actions of insulin during times of stress.  Insulin also inhibits fatty acid release by hormone sensitive lipase in adipose tissue.\n\nBeta cells in the islets of Langerhans release insulin in two phases. The first-phase release is rapidly triggered in response to increased blood glucose levels, and lasts about 10 minutes. The second phase is a sustained, slow release of newly formed vesicles triggered independently of sugar, peaking in 2 to 3 hours. Reduced first-phase insulin release may be the earliest detectable beta cell defect predicting onset of type 2 diabetes. First-phase release and insulin sensitivity are independent predictors of diabetes.\n\nThe description of first phase release is as follows:\n\nThis is the primary mechanism for release of insulin. Other substances known to stimulate insulin release include the amino acids arginine and leucine, parasympathetic release of acetylcholine (acting via the phospholipase C pathway), sulfonylurea, cholecystokinin (CCK, also via phospholipase C), and the gastrointestinally derived incretins, such as glucagon-like peptide-1 (GLP-1) and glucose-dependent insulinotropic peptide (GIP).\n\nRelease of insulin is strongly inhibited by norepinephrine (noradrenaline), which leads to increased blood glucose levels during stress. It appears that release of catecholamines by the sympathetic nervous system has conflicting influences on insulin release by beta cells, because insulin release is inhibited by α-adrenergic receptors and stimulated by β-adrenergic receptors. The net effect of norepinephrine from sympathetic nerves and epinephrine from adrenal glands on insulin release is inhibition due to dominance of the α-adrenergic receptors.\n\nWhen the glucose level comes down to the usual physiologic value, insulin release from the β-cells slows or stops. If the blood glucose level drops lower than this, especially to dangerously low levels, release of hyperglycemic hormones (most prominently glucagon from islet of Langerhans alpha cells) forces release of glucose into the blood from the liver glycogen stores, supplemented by gluconeogenesis if the glycogen stores become depleted. By increasing blood glucose, the hyperglycemic hormones prevent or correct life-threatening hypoglycemia.\n\nEvidence of impaired first-phase insulin release can be seen in the glucose tolerance test, demonstrated by a substantially elevated blood glucose level at 30 minutes after the ingestion of a glucose load (75 or 100 g of glucose), followed by a slow drop over the next 100 minutes, to remain above 120 mg/100 ml after two hours after the start of the test. In a normal person the blood glucose level is corrected (and may even be slightly over-corrected) by the end of the test.\n\nEven during digestion, in general, one or two hours following a meal, insulin release from the pancreas is not continuous, but oscillates with a period of 3–6 minutes, changing from generating a blood insulin concentration more than about 800 p mol/l to less than 100 pmol/l. This is thought to avoid downregulation of insulin receptors in target cells, and to assist the liver in extracting insulin from the blood. This oscillation is important to consider when administering insulin-stimulating medication, since it is the oscillating blood concentration of insulin release, which should, ideally, be achieved, not a constant high concentration. This may be achieved by delivering insulin rhythmically to the portal vein or by islet cell transplantation to the liver.\n\nThe blood insulin level can be measured in international units, such as µIU/mL or in molar concentration, such as pmol/L, where 1 µIU/mL equals 6.945 pmol/L. A typical blood level between meals is 8–11 μIU/mL (57–79 pmol/L).\n\nThe effects of insulin are initiated by its binding to a receptor present in the cell membrane. The receptor molecule contains an α- and β subunits. Two molecules are joined to form what is known as a homodimer. Insulin binds to the α-subunits of the homodimer, which faces the extracellular side of the cells. The β subunits have tyrosine kinase enzyme activity which is triggered by the insulin binding. This activity provokes the autophosphorylation of the β subunits and subsequently the phosphorylation of proteins inside the cell known as insulin receptor substrates (IRS). The phosphorylation of the IRS activates a signal transduction cascade that leads to the activation of other kinases as well as transcription factors that mediate the intracellular effects of insulin.\n\nThe cascade that leads to the insertion of GLUT4 glucose transporters into the cell membranes of muscle and fat cells, and to the synthesis of glycogen in liver and muscle tissue, as well as the conversion of glucose into triglycerides in liver, adipose, and lactating mammary gland tissue, operates via the activation, by IRS-1, of phosphoinositol 3 kinase (PI3K). This enzyme converts a phospholipid in the cell membrane by the name of phosphatidylinositol 4,5-bisphosphate (PIP2), into phosphatidylinositol 3,4,5-triphosphate (PIP3), which, in turn, activates protein kinase B (PKB). Activated PKB facilitates the fusion of GLUT4 containing endosomes with the cell membrane, resulting in an increase in GLUT4 transporters in the plasma membrane. PKB also phosphorylates glycogen synthase kinase (GSK), thereby inactivating this enzyme. This means that its substrate, glycogen synthase (GS), cannot be phosphorylated, and remains dephosphorylated, and therefore active. The active enzyme, glycogen synthase (GS), catalyzes the rate limiting step in the synthesis of glycogen from glucose. Similar dephosphorylations affect the enzymes controlling the rate of glycolysis leading to the synthesis of fats via malonyl-CoA in the tissues that can generate triglycerides, and also the enzymes that control the rate of gluconeogenesis in the liver. The overall effect of these final enzyme dephosphorylations is that, in the tissues that can carry out these reactions, glycogen and fat synthesis from glucose are stimulated, and glucose production by the liver through glycogenolysis and gluconeogenesis are inhibited. The breakdown of triglycerides by adipose tissue into free fatty acids and glycerol is also inhibited.\n\nAfter the intracellular signal that resulted from the binding of insulin to its receptor has been produced, termination of signaling is then needed. As mentioned below in the section on degradation, endocytosis and degradation of the receptor bound to insulin is a main mechanism to end signaling. In addition, the signaling pathway is also terminated by dephosphorylation of the tyrosine residues in the various signaling pathways by tyrosine phosphatases. Serine/Threonine kinases are also known to reduce the activity of insulin.\n\nThe structure of the insulin–insulin receptor complex has been determined using the techniques of X-ray crystallography.\n\nThe actions of insulin on the global human metabolism level include:\n\nThe actions of insulin (indirect and direct) on cells include:\n\nInsulin also influences other body functions, such as vascular compliance and cognition. Once insulin enters the human brain, it enhances learning and memory and benefits verbal memory in particular. Enhancing brain insulin signaling by means of intranasal insulin administration also enhances the acute thermoregulatory and glucoregulatory response to food intake, suggesting that central nervous insulin contributes to the co-ordination of a wide variety of homeostatic or regulatory processes in the human body. Insulin also has stimulatory effects on gonadotropin-releasing hormone from the hypothalamus, thus favoring fertility.\n\nOnce an insulin molecule has docked onto the receptor and effected its action, it may be released back into the extracellular environment, or it may be degraded by the cell. The two primary sites for insulin clearance are the liver and the kidney. The liver clears most insulin during first-pass transit, whereas the kidney clears most of the insulin in systemic circulation. Degradation normally involves endocytosis of the insulin-receptor complex, followed by the action of insulin-degrading enzyme. An insulin molecule produced endogenously by the beta cells is estimated to be degraded within about one hour after its initial release into circulation (insulin half-life ~ 4–6 minutes).\n\nInsulin is a major regulator of endocannabinoid (EC) metabolism and insulin treatment has been shown to reduce intracellular ECs, the 2-arachidonylglycerol (2-AG) and anandamide (AEA), which correspond with insulin-sensitive expression changes in enzymes of EC metabolism. In insulin-resistant adipocytes, patterns of insulin-induced enzyme expression is disturbed in a manner consistent with elevated EC synthesis and reduced EC degradation. Findings suggest that insulin-resistant adipocytes fail to regulate EC metabolism and decrease intracellular EC levels in response to insulin stimulation, whereby obese insulin-resistant individuals exhibit increased concentrations of ECs. This dysregulation contributes to excessive visceral fat accumulation and reduced adiponectin release from abdominal adipose tissue, and further to the onset of several cardiometabolic risk factors that are associated with obesity and type 2 diabetes.\n\nHypoglycemia, also known as \"low blood sugar\", is when blood sugar decreases to below normal levels. This may result in a variety of symptoms including clumsiness, trouble talking, confusion, loss of consciousness, seizures or death. A feeling of hunger, sweating, shakiness and weakness may also be present. Symptoms typically come on quickly.\n\nThe most common cause of hypoglycemia is medications used to treat diabetes mellitus such as insulin and sulfonylureas. Risk is greater in diabetics who have eaten less than usual, exercised more than usual or have drunk alcohol. Other causes of hypoglycemia include kidney failure, certain tumors, such as insulinoma, liver disease, hypothyroidism, starvation, inborn error of metabolism, severe infections, reactive hypoglycemia and a number of drugs including alcohol. Low blood sugar may occur in otherwise healthy babies who have not eaten for a few hours.\n\nThere are several conditions in which insulin disturbance is pathologic:\n\nBiosynthetic human insulin (insulin human rDNA, INN) for clinical use is manufactured by recombinant DNA technology. Biosynthetic human insulin has increased purity when compared with extractive animal insulin, enhanced purity reducing antibody formation. Researchers have succeeded in introducing the gene for human insulin into plants as another method of producing insulin (\"biopharming\") in safflower. This technique is anticipated to reduce production costs.\n\nSeveral analogs of human insulin are available. These insulin analogs are closely related to the human insulin structure, and were developed for specific aspects of glycemic control in terms of fast action (prandial insulins) and long action (basal insulins). The first biosynthetic insulin analog was developed for clinical use at mealtime (prandial insulin), Humalog (insulin lispro), it is more rapidly absorbed after subcutaneous injection than regular insulin, with an effect 15 minutes after injection. Other rapid-acting analogues are NovoRapid and Apidra, with similar profiles. All are rapidly absorbed due to amino acid sequences that will reduce formation of dimers and hexamers (monomeric insulins are more rapidly absorbed). Fast acting insulins do not require the injection-to-meal interval previously recommended for human insulin and animal insulins. The other type is long acting insulin; the first of these was Lantus (insulin glargine). These have a steady effect for an extended period from 18 to 24 hours. Likewise, another protracted insulin analogue (Levemir) is based on a fatty acid acylation approach. A myristic acid molecule is attached to this analogue, which associates the insulin molecule to the abundant serum albumin, which in turn extends the effect and reduces the risk of hypoglycemia. Both protracted analogues need to be taken only once daily, and are used for type 1 diabetics as the basal insulin. A combination of a rapid acting and a protracted insulin is also available, making it more likely for patients to achieve an insulin profile that mimics that of the body´s own insulin release.\n\nInsulin is usually taken as subcutaneous injections by single-use syringes with needles, via an insulin pump, or by repeated-use insulin pens with disposable needles. Inhaled insulin is also available in the U.S. market now.\n\nSynthetic insulin can trigger adverse effects, so some people with diabetes rely on animal-source insulin.\n\nUnlike many medicines, insulin currently cannot be taken orally because, like nearly all other proteins introduced into the gastrointestinal tract, it is reduced to fragments, whereupon all activity is lost. There has been some research into ways to protect insulin from the digestive tract, so that it can be administered orally or sublingually.\n\nIn 1869, while studying the structure of the pancreas under a microscope, Paul Langerhans, a medical student in Berlin, identified some previously unnoticed tissue clumps scattered throughout the bulk of the pancreas. The function of the \"little heaps of cells\", later known as the \"islets of Langerhans\", initially remained unknown, but Édouard Laguesse later suggested they might produce secretions that play a regulatory role in digestion. Paul Langerhans' son, Archibald, also helped to understand this regulatory role. The term \"insulin\" originates from \"insula\", the Latin word for islet/island.\n\nIn 1889, the physician Oskar Minkowski, in collaboration with Joseph von Mering, removed the pancreas from a healthy dog to test its assumed role in digestion. On testing the urine, they found sugar, establishing for the first time a relationship between the pancreas and diabetes. In 1901, another major step was taken by the American physician and scientist Eugene Lindsay Opie, when he isolated the role of the pancreas to the islets of Langerhans: \"Diabetes mellitus when the result of a lesion of the pancreas is caused by destruction of the islands of Langerhans and occurs only when these bodies are in part or wholly destroyed\".\n\nOver the next two decades researchers made several attempts to isolate the islets' secretions. In 1906 George Ludwig Zuelzer achieved partial success in treating dogs with pancreatic extract, but he was unable to continue his work. Between 1911 and 1912, E.L. Scott at the University of Chicago tried aqueous pancreatic extracts and noted \"a slight diminution of glycosuria\", but was unable to convince his director of his work's value; it was shut down. Israel Kleiner demonstrated similar effects at Rockefeller University in 1915, but World War I interrupted his work and he did not return to it.\n\nIn 1916, Nicolae Paulescu developed an aqueous pancreatic extract which, when injected into a diabetic dog, had a normalizing effect on blood-sugar levels. He had to interrupt his experiments because of World War I, and in 1921 he wrote four papers about his work carried out in Bucharest and his tests on a diabetic dog. Later that year, he published \"Research on the Role of the Pancreas in Food Assimilation\".\n\nIn October 1920, Canadian Frederick Banting concluded that the digestive secretions that Minkowski had originally studied were breaking down the islet secretion, thereby making it impossible to extract successfully. A surgeon by training, Banting knew certain arteries could be tied off that would lead most of the pancreas to atrophy, while leaving the islets of Langerhans intact. He reasoned that a relatively pure extract could be made from the islets once most of the rest of the pancreas was gone. He jotted a note to himself: \"Ligate pancreatic ducts of the dog. Keep dogs alive till acini degenerate leaving islets. Try to isolate internal secretion of these and relieve glycosuria.\"\n\nIn the spring of 1921, Banting traveled to Toronto to explain his idea to J.J.R. Macleod, Professor of Physiology at the University of Toronto. Macleod was initially skeptical, since Banting had no background in research and was not familiar with the latest literature, but he agreed to provide lab space for Banting to test out his ideas. Macleod also arranged for two undergraduates to be Banting's lab assistants that summer, but Banting required only one lab assistant. Charles Best and Clark Noble flipped a coin; Best won the coin toss and took the first shift. This proved unfortunate for Noble, as Banting kept Best for the entire summer and eventually shared half his Nobel Prize money and credit for the discovery with Best. On 30 July 1921, Banting and Best successfully isolated an extract (\"isleton\") from the islets of a duct-tied dog and injected it into a diabetic dog, finding that the extract reduced its blood sugar by 40% in 1 hour.\n\nBanting and Best presented their results to Macleod on his return to Toronto in the fall of 1921, but Macleod pointed out flaws with the experimental design, and suggested the experiments be repeated with more dogs and better equipment. He moved Banting and Best into a better laboratory and began paying Banting a salary from his research grants. Several weeks later, the second round of experiments was also a success, and Macleod helped publish their results privately in Toronto that November. Bottlenecked by the time-consuming task of duct-tying dogs and waiting several weeks to extract insulin, Banting hit upon the idea of extracting insulin from the fetal calf pancreas, which had not yet developed digestive glands. By December, they had also succeeded in extracting insulin from the adult cow pancreas. Macleod discontinued all other research in his laboratory to concentrate on the purification of insulin. He invited biochemist James Collip to help with this task, and the team felt ready for a clinical test within a month.\n\nOn January 11, 1922, Leonard Thompson, a 14-year-old diabetic who lay dying at the Toronto General Hospital, was given the first injection of insulin. However, the extract was so impure that Thompson suffered a severe allergic reaction, and further injections were cancelled. Over the next 12 days, Collip worked day and night to improve the ox-pancreas extract. A second dose was injected on January 23, completely eliminating the glycosuria that was typical of diabetes without causing any obvious side-effects. The first American patient was Elizabeth Hughes, the daughter of U.S. Secretary of State Charles Evans Hughes. The first patient treated in the U.S. was future woodcut artist James D. Havens; Dr. John Ralston Williams imported insulin from Toronto to Rochester, New York, to treat Havens.\n\nBanting and Best never worked well with Collip, regarding him as something of an interloper, and Collip left the project soon after. Over the spring of 1922, Best managed to improve his techniques to the point where large quantities of insulin could be extracted on demand, but the preparation remained impure. The drug firm Eli Lilly and Company had offered assistance not long after the first publications in 1921, and they took Lilly up on the offer in April. In November, Lilly's head chemist, George B. Walden discovered isoelectric precipitation and was able to produce large quantities of highly refined insulin. Shortly thereafter, insulin was offered for sale to the general public.\n\nPurified animal-sourced insulin was initially the only type of insulin available to diabetics. The amino acid structure of insulin was characterized in the early 1950s by Frederick Sanger, and the first synthetic insulin was produced simultaneously in the labs of Panayotis Katsoyannis at the University of Pittsburgh and Helmut Zahn at RWTH Aachen University in the early 1960s. Synthetic crystalline bovine insulin was achieved by Chinese researchers in 1965.\n\nThe first genetically engineered, synthetic \"human\" insulin was produced using \"E. coli\" in 1978 by Arthur Riggs and Keiichi Itakura at the Beckman Research Institute of the City of Hope in collaboration with Herbert Boyer at Genentech. Genentech, founded by Swanson, Boyer and Eli Lilly and Company, went on in 1982 to sell the first commercially available biosynthetic human insulin under the brand name Humulin. The vast majority of insulin currently used worldwide is now biosynthetic recombinant \"human\" insulin or its analogues.\n\nRecombinant insulin is produced either in yeast (usually \"Saccharomyces cerevisiae\") or \"E. coli\". In yeast, insulin may be engineered as a single-chain protein with a KexII endoprotease (a yeast homolog of PCI/PCII) site that separates the insulin A chain from a c-terminally truncated insulin B chain. A chemically synthesized c-terminal tail is then grafted onto insulin by reverse proteolysis using the inexpensive protease trypsin; typically the lysine on the c-terminal tail is protected with a chemical protecting group to prevent proteolysis. The ease of modular synthesis and the relative safety of modifications in that region accounts for common insulin analogs with c-terminal modifications (e.g. lispro, aspart, glulisine). The Genentech synthesis and completely chemical synthesis such as that by Bruce Merrifield are not preferred because the efficiency of recombining the two insulin chains is low, primarily due to competition with the precipitation of insulin B chain.\n\nThe Nobel Prize committee in 1923 credited the practical extraction of insulin to a team at the University of Toronto and awarded the Nobel Prize to two men: Frederick Banting and J.J.R. Macleod. They were awarded the Nobel Prize in Physiology or Medicine in 1923 for the discovery of insulin. Banting, insisted that Best was not mentioned, shared his prize with him, and Macleod immediately shared his with James Collip. The patent for insulin was sold to the University of Toronto for one dollar.\n\nTwo other Nobel Prizes have been awarded for work on insulin. British molecular biologist Frederick Sanger determined the primary structure of insulin in 1955, making it the first protein to be sequenced. Sanger was awarded the 1958 Nobel Prize in Chemistry for this work. Rosalyn Sussman Yalow received the 1977 Nobel Prize in Medicine for the development of the radioimmunoassay for insulin.\n\nSeveral Nobel Prizes also have an indirect connection with insulin. George Minot, co-recipient of the 1934 Nobel Prize for the development of the first effective treatment for pernicious anemia, had diabetes mellitus. Dr. William Castle observed that the 1921 discovery of insulin, arriving in time to keep Minot alive, was therefore also responsible for the discovery of a cure for pernicious anemia. Dorothy Hodgkin was awarded a Nobel Prize in Chemistry in 1964 for the development of crystallography. In 1969, after decades of work, Hodgkin determined the spatial conformation of insulin, the so-called tertiary structure, by means of X-ray diffraction studies.\n\nThe work published by Banting, Best, Collip and Macleod represented the preparation of purified insulin extract suitable for use on human patients. Although Paulescu discovered the principles of the treatment, his saline extract could not be used on humans; he was not mentioned in the 1923 Nobel Prize. Professor Ian Murray was particularly active in working to correct \"the historical wrong\" against Nicolae Paulescu. Murray was a professor of physiology at the Anderson College of Medicine in Glasgow, Scotland, the head of the department of Metabolic Diseases at a leading Glasgow hospital, vice-president of the British Association of Diabetes, and a founding member of the International Diabetes Federation. Murray wrote:\n\nInsufficient recognition has been given to Paulescu, the distinguished Romanian scientist, who at the time when the Toronto team were commencing their research had already succeeded in extracting the antidiabetic hormone of the pancreas and proving its efficacy in reducing the hyperglycaemia in diabetic dogs.\nIn a private communication, Professor Arne Tiselius, former head of the Nobel Institute, expressed his personal opinion that Paulescu was equally worthy of the award in 1923.\n\n\n"}
{"id": "5737502", "url": "https://en.wikipedia.org/wiki?curid=5737502", "title": "International Permafrost Association", "text": "International Permafrost Association\n\nThe International Permafrost Association (IPA), founded in 1983, has as its objectives to foster the dissemination of knowledge concerning permafrost and to promote cooperation among persons and national or international organisations engaged in scientific investigation and engineering work related to permafrost and seasonally frozen ground. The IPA became an Affiliated Organisation of the International Union of Geological Sciences in July 1989.\n\nPermafrost or perennially frozen ground is defined as earth material that remains at or below 0 °C for at least two consecutive years. As such, upwards of 25% of Planet Earth is underlain to some degree by permafrost and in extreme conditions reaches depths of 1500 meters. Permafrost occurs in the high latitudes and mountains and plateaus of both hemispheres.\n\nThe Association’s primary responsibilities are to convene International Permafrost Conferences, undertake special projects such as preparing databases, maps, bibliographies, and glossaries, and coordinate international field programmes and networks. International conferences were held at: West Lafayette, Indiana, U.S.A. (1963); Yakutsk, Siberia (1973); Edmonton, Canada (1978); Fairbanks, Alaska (1983); Trondheim, Norway (1988); Beijing, China (1993); Yellowknife, Canada (1998); and Zurich, Switzerland (2003: ICOP 2003). The Ninth International Conference on Permafrost (NICOP) was held in Fairbanks, Alaska, June 29-July 3, 2008 and the Tenth took place in Salekhard, Russia, June 25–29, 2012 (TICOP 2012). The Eleventh International Conference on Permafrost will be held in Potsdam, Germany in 2016. Field excursions are an integral part of each Conference, and are organised by the host country. Regional conferences are organised between the main conferences (in Europe, Russia, China).\n\nMembership is through adhering national or multinational organisations or as Associate Members in countries where no Adhering Body exists. The IPA is governed by an Executive Committee and a Council consisting of representatives from 26 Adhering Bodies having interests in some aspect of theoretical, basic and applied frozen ground research, including permafrost, periglacial phenomena, seasonal frost, and artificial ground freezing. Members are: Argentina, Austria, Canada, China, Denmark, Finland, France, Germany, Iceland, Italy, Japan, Kyrgyzstan, Mongolia, The Netherlands, New Zealand, Norway, Poland, Portugal, Romania, Russia, South Korea, Spain, Sweden, Switzerland, United Kingdom, United States of America. News from the IPA members are posted on the IPA webpage.\n\nThe officers of the seven-member Executive Committee (2012–2016) are:\n\nThe Executive Director is Dr. Inga May (Germany).\n\nThe IPA Constitution provides for three categories of Working Group Parties: standing committees (long-term), working groups (5–10 years) and action groups (1–2 years) that organise and coordinate research activities and special projects. The first category includes a Standing Committee for Data, Information and Communication, an International Advisory Committee for the International Permafrost Conferences, and a Standing Committee for Education and Outreach. There are ten Working Groups, each with two co-chairs and some with subgroups. These are:\n\nDetails of the Working Parties goals and activities are found on the IPA website.\n\nThe International Secretariat is based at the Alfred Wegener Institute for Polar and Marine Research under the direction of Dr. Inga May (Germany). Annual membership contributions are used for producing and distributing \"Frozen Ground\", and support of Working Parties and committee activities and representations at international meetings.\n\nProceedings of peer-reviewed papers are produced for each International Permafrost Conference by the host country, as are field trip guidebooks. A list of publications is found on the IPA website. The News Bulletin \"Frozen Ground\" is published annually and has a distribution of over 2500. Current and back issues are posted online.\n\nThe Circum-Arctic Map of Permafrost and Ground-Ice Conditions at a scale of 1:10,000,000 was prepared by an international team and published in the Circum-Pacific map series in 1997.\n\nThe CAPS: Circumpolar Active-Layer Permafrost System, version 2.0 CD-ROM is a compilation of global frozen ground data and information.\n\nPermafrost and Frozen Ground: Bibliography 1978 – 2003, Glaciological Data Report GD-31, is an online bibliography of the world’s literature. Both are produced in conjunction with the International Permafrost Conferences and available from the National Snow and Ice Data Center.\n\nAn illustrated Glossary of Permafrost and Related Ground-Ice Terms in 12 languages (Chinese, English, French, German, Icelandic, Italian, Norwegian, Polish, Romanian, Russian, Swedish, and Spanish), 278 pages, was produced in 1998.\n\nIPA cooperates with the American Geological Institute by providing literature to its Cold Regions Bibliography Project.\n\nThe IPA coordinates and cooperates with several other major international programmes and organizations. Briefly these are:\n\nGlobal Terrestrial Network for Permafrost (GTN-P) is a WMO network for monitoring of the active layer and the Thermal State of Permafrost (TSP). The IPA manages the GTN-P. The Circumpolar Active Layer Monitoring (CALM) program has 125 reporting stations and TSP has identified over 800 boreholes; both include a total of over 15 participating countries.\n\nArctic Coastal Dynamics (ACD) is a joint programme with the International Arctic Science Committee (IASC) and the IGBP(International Geosphere-Biosphere Programme) - LOICZ (Land-Ocean Interactions in the Coastal Zone) programme to estimate the organic carbon content and mineral transfer for eroding permafrost onto the Arctic shelves. The IPA is collaborating with the International Union for Quaternary Research (INQUA).\n\nThe IPA has a Memorandum of Understanding with the Climate and Cryosphere (CliC) programme of the World Climate Research Programme (WCRP). The main areas of cooperation are on the roles of permafrost on water and carbon balances, and data assimilation and modelling.\n\nBeginning in 1995 the IPA and the International Geographical Union (IGU) developed an Agreement of Cooperation, thus making IPA an affiliate of the IGU. The current IGU collaboration is within its Commission on Cold Regions Environments.\n\nCoordination of activities on permafrost, soils and periglacial environments of the Antarctic and sub-Antarctic islands is a joint programme with an Expert Group of the Scientific Committee for Antarctic Research (SCAR).\n\nActivities related to glaciers and permafrost hazards in the high mountains (GAPHAZ) are a joint activity with the International Union of Geodesy and Geophysics (IUGG) and its newly designated Commission for the Cryospheric Sciences.\n\nThe topic of carbon sources and sinks in cold regions soils (cryosols) and permafrost is a joint program with the Global Carbon Project (GPC) and the joint working group on Cryosol of the International Union of Soil Sciences (IUSS).\n\nIPA was actively involved in the International Polar Year International Polar Year (IPY) by participating with four coordinated projects. The Thermal State of Permafrost (TSP) proposes to obtain a ‘snapshot’ of permafrost temperatures throughout Planet Earth during the period 2007-2008. Another objective of TSP is to establish a permanent International Network of Permafrost Observatories (INPO) within the framework of the Global Terrestrial Network for Permafrost (GTN-P). The three other IPY projects are concerned with Antarctic and sub-Antarctic Permafrost, Periglacial and Soil Environments (ANTPAS) and with the Arctic Circum-Polar Coastal Observatory Network (ACCO-Net) and Carbon Pools in Permafrost Regions (CAPP), and revised regional permafrost maps of Central Asia and the Nordic region. For updated news about IPA activities linked to IPY, see the IPA website.\n\nSeveral regional permafrost and soils conferences were held and are planned, including:\n\n\nJ. Brown & H.H. Christiansen. International Permafrost Association. \"Episodes\", Volume 28, no.4, pp. 301–302, December 2005.\n\n\n"}
{"id": "2580132", "url": "https://en.wikipedia.org/wiki?curid=2580132", "title": "Kanzelhoehe Solar Observatory", "text": "Kanzelhoehe Solar Observatory\n\nThe Kanzelhoehe Solar Observatory or KSO is an astronomical observatory affiliated with the Institute of Geophysics, Astrophysics and Meteorology out of the University of Graz. It is located near Villach on the southern border of Austria.\n\nIts Web page at http://www.kso.ac.at/ usually posts current images of the sun, especially in the hydrogen-alpha line that is the strongest visible-light line of hydrogen and that reveals the solar chromosphere.\n\nFounded in 1941 by the German Luftwaffe to research the effects of the Sun on the Earth's ionosphere, the KSO focuses on multispectral synoptic observations of the sun using several telescope on the same mount.\n\n"}
{"id": "1660047", "url": "https://en.wikipedia.org/wiki?curid=1660047", "title": "Laurentian Mixed Forest Province", "text": "Laurentian Mixed Forest Province\n\nThe Laurentian Mixed Forest Province, also known as the North Woods, is a forested ecoregion in the United States and Canada. Among others, this terminology has been adopted by the Minnesota Department of Natural Resources. Similar, though not necessarily entirely identical regions, are identified by the United States Environmental Protection Agency as Northern Lakes and Forests, and by the World Wildlife Fund by regions such as the Western Great Lakes forests and Eastern forest-boreal transition.\n\nIn the United States, it consists of a broad region of northern Minnesota, Wisconsin and Michigan (Northern Michigan and the Upper Peninsula) and the forested areas of New England. In Canada, it is found in Ontario around the Great Lakes and the Saint Lawrence River through Quebec to Quebec City.\n\nNearly all of the region was covered by glaciers during the last Ice age, which created many lakes and wetlands throughout the region. The poor soils and cool climate in this region were not conducive to farming for early settlers, which resulted in the regrowth of most of the forest after being cleared during the 19th and early 20th centuries. With the abundant lakes and streams and regrowth of the forests, the region became a major tourist and recreation area for the larger population centers just to the south.\n\nThe area is a Temperate broadleaf and mixed forests biome transition zone between the true boreal forest to the north and the Big Woods and Carolinian forest to the south, with characteristics of each. It has areas of both broadleaf and conifer forest cover, and bodies of water ranging from lakes to conifer bogs and swamps. Conifers include pines, spruces, firs, and junipers; deciduous types include aspens, oaks, paper birches, mountain ash and maples. It is often said to have a distinct smell, which is attributed partially to the presence of sweet fern.\n\nThe climate for the region roughly corresponds with USDA Plant Hardiness Zones 3a through 4b, although in New England the region extends into zone 5a. The forest region is adapted to the Humid continental climate with warm, humid summers and cold, snowy winters. The Köppen climate classification is Dfb.\n\nDuring the lumbering era, unrestricted logging and the resulting fires destroyed much of the forest. With the advent of fire suppression and forest management, the resulting second-growth forest differed substantially from the original forest cover. Conifer tree species became less common in the resulting forest. Early successional tree species such as Aspen and Birch became much more prevalent and replaced much of the mixed conifer and deciduous forests that originally existed prior to the logging era.\n\nMajor animal species inhabiting the forest include White-tailed deer, Moose, Porcupine, Beaver, the American red squirrel, the Eastern gray squirrel, Chipmunk, Opossum, Raccoon, Bobcat, Canada lynx, Fisher (animal), American marten, Long-tailed weasel, Ruffed grouse, Spruce grouse, Bald eagle, Red-tailed hawk, Osprey, Common loon, Duck, Canada goose, Wild turkey, Sandhill crane, Snowshoe hare, the American black bear, Coyote, Red fox, and (in the Canadian and upper Great Lakes states regions) the Gray wolf. Elk have been reintroduced in northern Wisconsin, Michigan, and Ontario after having been extirpated prior to the 20th century by overhunting and habitat loss. Lone Cougar have been documented moving through the region, but these appear to consist of solitary young males dispersing from the Great plains, and little evidence of breeding populations is currently known to exist in the region.\n\nThe Boreal woodland caribou used to inhabit the American portions of the region, but with the destruction of the original forest during the late 19th and early 20th centuries and the resulting expansion of the White-tailed deer population (which carry the deadly Parelaphostrongylus tenuis brain worm parasite), the species is now confined to Canada.\n\nEarly French, British, and American trappers exploited the large populations of furbearers in the region, especially the American Beaver. As a result, populations of these furbearers plummeted, only recovering after the institution of modern game management.\n\nDuring the 19th century, Native Americans in the United States were forcibly removed from their lands and/or confined to small reservations after signing treaties with the U.S. government. As a result of these treaties, most of the land in this region was opened to white settlement. However, many of the treaties guaranteed off-reservation hunting, fishing, and gathering rights on the ceded lands. After several court cases during the 20th century in which these rights were upheld, Native Americans continue to exercise their hunting and gathering rights.\n\nThe initial economy of the region was heavily dependent on logging. The logging progressed from east to west across the region during the 19th and early 20th centuries as the supply of virgin timber was exhausted. The logging provided the lumber necessary to build cities and towns throughout the eastern and central U.S., and communities sprang up around the saw mills that were built to process the lumber. The massive scale of the logging greatly reduced the prevalence of the large Eastern White Pines (Pinus strobus) and Red Pines (Pinus resinosa) formerly found throughout the region.\n\nMajor wildfires feeding on the Slash (logging) during the late 19th century resulted from the unrestricted lumbering throughout the region. The Peshtigo Fire on October 8, 1871 in northeastern Wisconsin was the deadliest wildfire in American history, with more than 1,500 deaths resulting from the fire.\n\nAfter loggers left in late 19th century, lodges were built as fishing camps and affluent sportsmen arrived by rail. In the 1920s, roads were built and automobiles became more affordable. The next four decades were the golden age of lake resorts of the North Woods. The influx of families continued towards the north, and the now familiar log-cabin look with massive beams, fieldstone fireplace, and boulder foundations became synonymous with the North Woods. In the late 1940s and 1950s, the expansion continued. However, today, only a few classic-styled lodges survive to give a feel of the golden era.\n\nIn addition to the many private resorts in the region, many of the cutover forest lands were acquired by state and federal governments for forestry purposes after early attempts at farming the region mostly failed. The resulting national, state, and county parks and forests throughout the region provide sustainable logging and recreational opportunities.\n\n"}
{"id": "22785887", "url": "https://en.wikipedia.org/wiki?curid=22785887", "title": "List of Gymnopilus species", "text": "List of Gymnopilus species\n\nThis is a list of species in the agaric fungi genus \"Gymnopilus\". There are about 200 species in the widespread genus.\n\n"}
{"id": "5871885", "url": "https://en.wikipedia.org/wiki?curid=5871885", "title": "List of Sites of Special Scientific Interest in West Yorkshire", "text": "List of Sites of Special Scientific Interest in West Yorkshire\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in West Yorkshire. English Nature, the designating body for SSSIs in England, uses the 1974–1996 county system, and the same approach is followed here, rather than, for example, merging all Yorkshire sites into a single list.\n\nFor other counties, see List of SSSIs by Area of Search.\n"}
{"id": "33331993", "url": "https://en.wikipedia.org/wiki?curid=33331993", "title": "List of energy regulatory bodies", "text": "List of energy regulatory bodies\n\n\n\nThe Energy Regulators Regional Association (ERRA) is a voluntary organization of independent energy regulatory bodies primarily from the Central European and Eurasian region, with Affiliates from Asia the Middle East and the US. ERRA began as a cooperative exchange among 12 energy regulatory bodies to improve national energy regulation in member countries.\n\nAssociated members:\n\n\nThe Council of European Energy Regulators (CEER) is a not-for-profit organisation in which Europe's national regulators of electricity and gas voluntarily cooperate to protect consumers' interests and to facilitate the creation of a single, competitive and sustainable internal market for gas and electricity in Europe.\n\nCEER currently has 29 members - the national energy regulators from the 27 EU-Member States, plus Iceland and Norway.\n\nThe first formal meeting on 3 March 2012 in Bangkok, Thailand, formally established the ASEAN Energy Regulators' Network (AERN) among the ASEAN energy regulators. The objective of AERN is to forge closer cooperation among ASEAN Energy Regulators with a view to promoting sustainability and economic development of the region in support of the vision of the ASEAN Economic Community 2015.\n\n"}
{"id": "7975888", "url": "https://en.wikipedia.org/wiki?curid=7975888", "title": "List of mountains in Cape Verde", "text": "List of mountains in Cape Verde\n\nThis is a list of mountains in Cabo Verde:\n\n"}
{"id": "58511133", "url": "https://en.wikipedia.org/wiki?curid=58511133", "title": "List of pipeline accidents in the United States in 2004", "text": "List of pipeline accidents in the United States in 2004\n\nThe following is a list of pipeline accidents in the United States in 2004. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n"}
{"id": "58527608", "url": "https://en.wikipedia.org/wiki?curid=58527608", "title": "List of pipeline accidents in the United States in 2007", "text": "List of pipeline accidents in the United States in 2007\n\nThe following is a list of pipeline accidents in the United States in 2007. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n\n"}
{"id": "50881632", "url": "https://en.wikipedia.org/wiki?curid=50881632", "title": "List of plants with indehiscent fruits", "text": "List of plants with indehiscent fruits\n\nIndehiscent fruit do not open at maturity in a pre-defined way, but rely on predation or decomposition to release the seeds.\n\nSome, but not all, indehiscent fruits are included in specialized morphological categories such as achene, berry, caryopsis, cypsela, drupe, hesperidium, loment, pepo, pome, samara, syconium. \n\nSee also: List of plants with dehiscent fruits\n\n"}
{"id": "356095", "url": "https://en.wikipedia.org/wiki?curid=356095", "title": "Longest English sentence", "text": "Longest English sentence\n\nThere have been several claims for the longest sentence in the English language, usually with claims that revolve around the longest \"printed\" sentence, because there is no limit on the possible length of a written English sentence. \n\nAt least one linguistics textbook concludes that, in theory, \"there is no longest English sentence.\" A sentence can be made arbitrarily long by successive iterations, such as \n\"Someone thinks that someone thinks that someone thinks that...,\" or by combining shorter clauses in various ways. \n\nFor example, sentences can be extended by recursively embedding clauses one into another, such as \n\nThe ability to embed structures within larger ones is called recursion. This also highlights the difference between linguistic performance and linguistic competence, because the language can support more variation than can reasonably be created or recorded.\n\nOne of the longest sentences in literature is contained in William Faulkner's \"Absalom, Absalom!\" (1936). The sentence is composed of 1,288 words (In the 1951 Random House version).\n\nAnother sentence that is often claimed to be the longest sentence ever written is Molly Bloom's soliloquy in the James Joyce novel \"Ulysses\" (1922), which contains a sentence of 3,687 words. However, this sentence is simply many sentences without punctuation.\n\nJonathan Coe's \"The Rotters' Club\" appears to hold the record at 13,955 words. It was inspired by Bohumil Hrabal's \"Dancing Lessons for the Advanced in Age\": a Czech language novel written in one long sentence.\n\n"}
{"id": "29175652", "url": "https://en.wikipedia.org/wiki?curid=29175652", "title": "MARKAL", "text": "MARKAL\n\nMARKAL is a numerical model used to carry out economic analysis of different energy related systems at the country level to represent its evolution over a period of usually of 40–50 years. The word MARKAL was generated by concatenating two words (MARKet and ALlocation). Various parameters such as energy costs, plant costs, plant performances, building performance and so on, can be input and the software will choose an optimal technology mix to meet that demand at minimum cost. It is available from the International Energy Agency. TIMES is an evolution of MARKAL and both energy models share many similarities.\n\nA 2009 report describes a newly developed and updated UK MARKAL elastic demand (MED) model, used to explore low carbon pathways for the United Kingdom.\n\n"}
{"id": "40705174", "url": "https://en.wikipedia.org/wiki?curid=40705174", "title": "Marselisborg Forests", "text": "Marselisborg Forests\n\nMarselisborg Forests (), or simply Marselisborg Forest, is a forest to the south of Aarhus City in the Kingdom of Denmark. Many present day sources now includes the forest of Fløjstrup, as part of the Marselisborg Forests, upping the total area with another . Marselisborg Forests runs along the coastline of the Aarhus Bay in a hilly terrain with steep slopes and deep gullies, especially at the shoreline. There are many traces of prehistoric activities here and the landscape have been covered by woodlands for thousands of years.\n\nMarselisborg Forests is comprised by a collection of small patches of woodland, that have been allowed to merge into a single entity, mainly after 1820. The woodland patches originally belonged to the barony of Marselis, residing at the now gone Marselisborg (meaning \"Marselis-castle\"), where Marselisborg Gymnasium is located today. Before 1820, there was an extensive forestry in the woods, so most of the trees are no more than 200 years old. Aarhus Municipality took ownership of the land and forests in 1896, when they acquired the Marselis estate.\n\nMarselisborg Forests have a long cultural history as a recreational area for Aarhus and its citizens and today it is among the most frequented forests in Denmark. The forests are used heavily for a variety of purposes, such as kindergarten excursions, camping, mountainbiking, scouting activities and headquarters, horse trails, running events, celebrations, picnics and more everyday unorganized activities like strolling, mushroom hunting, surf fishing, etc.. The forestry business is rather limited today. Marselisborg Forests are home to several important facilities for the citizens of the Aarhus area, such as:\n\n\nThe many sources to the cultural history and recreational use of the Marselisborg Forests through the ages, suggests that the forests were used even more extensively in previous times, than today. In the early 1900s, it even housed a zoo, and two steamboats regularly transported citizens from the Aarhus harbour to various spots, just for recreational purposes. Many of the historic documents of the booming restaurants and scenes, reports dramatic drops in turnovers, when television was introduced in the 1950-60's.\n\nAs one of the more popular and noteworthy happenings of our time, the northern parts of Marselisborg Forests housed a Sculpture by the Sea event once every second year in June from 2009 to 2015. The tradition was initiated by crown princess Mary and crown prince Frederik of Denmark, inspired by Australia; Mary's country of birth. The event called 'Sculpture by the Sea, Aarhus - Denmark' was financially and legally independent of 'Sculpture by the Sea Incorporated' and it was produced by the city of Aarhus in collaboration with ARoS Aarhus Artmuseum under the patronage of the crown prince couple. The exhibitions attracted an estimated half a million visitors each.\n\nIn spite of the high level of human activity, Marselisborg Forests does have an interesting flora and fauna and it is not impossible to find the peace and tranquillity, that one normally associates with wild nature. It is one of the few big forests in Denmark, that can present larger connected areas of natural beech wood. Most of the forest is a mixed deciduous forest, with species like beech, ash and maple as the most common. There are other areas dominated by conifer, and oak trees are mixed in throughout the forest.\n\nAt Moesgård Museum, sections of the forest have been raised in the 1970s, to recreate various forest-types associated with different epochs since the last ice age. These sections includes ash, birch and alder swamps, linden, elm and oak woods. Mixed in is also the versatile hazel and forest apple.\n\nMarselisborg Forests is home to very rich and varied fungi colonies, with several rare or threatened species. The forests is known for its Mycorrhiza, but also the rare and poisonous Satan's mushrooms, Warted Amanita, and bleeding corals are to be found here. In some years, larger quantities of the edible porcino, trumpet of the dead and charcoal burner can be found, to mention a few examples.\n\nThe forests supports a population of northern crested newts. They are common in Denmark, preferring the south-eastern parts of the country, and is thus not a Red List species, but their habitats are threatened on an international scale. Based partly on these facts, a larger part of Marselisborg Forests is to be protected under the EU Habitats Directive and have been designated as area H234.\n\n"}
{"id": "38120789", "url": "https://en.wikipedia.org/wiki?curid=38120789", "title": "Mid-Atlantic Soft Matter Workshop", "text": "Mid-Atlantic Soft Matter Workshop\n\nMid-Atlantic Soft Matter Workshop or MASM is an interdisciplinary meeting on soft matter routinely hosted and participated in by research and educational organizations in the mid-Atlantic region of the United States.\n\nThe workshop consists of several talks relating to the field of soft matter given by invited lecturers, and these talks are interspersed with sessions of short, three-minute \"sound-bite\" talks that can be delivered by any participant.\n\nMASM was started at Georgetown University and organized by Daniel Blair and Jeffrey Urbach of Georgetown University.\n\nThere have been 16 MASM meetings. The 16th MASM meeting was hosted at National Institutes of Health (NIH) on July 29, 2015.\n\n\n"}
{"id": "33398457", "url": "https://en.wikipedia.org/wiki?curid=33398457", "title": "Mistrals", "text": "Mistrals\n\nMISTRALS (\"Integrated Mediterranean Studies at Regional and Local Scales\") is a research program dedicated to the study of the Mediterranean basin and its surroundings, with the aim to \"better understand the impact of global factors on this region and to anticipate changes over a century of living conditions\".\n\nEstablished in August 2008 under the auspices of the CNRS, it quickly became a collaborative program of many research organizations from France and other Mediterranean countries. \"Corporate governance is provided by an International Steering Committee and a Steering Inter-Agency Committee, which validate the scientific plans of each program and provide working capital finance\".\n\nIts scientific direction and financial support are provided by CNRS and IRD (the co-directors are Etienne Ruellan CNRS / INSU and Abdelghani Chehbouni IRD), together with eleven other French research institutions (ADEME, BRGM, CEA, Cemagref, CIRAD, CNES, IFP énergies nouvelles, IFREMER, INRA, IRSN and Météo-France).\n\nIn 2011, over 1,000 scientists worked in seven scientific programs. Research programs are interdisciplinary. The themes include terrestrial and marine ecosystems, fisheries, climate, geology, urban planning, agriculture, the availability of fresh water, economics, sociology and anthropology.\n\n"}
{"id": "57367053", "url": "https://en.wikipedia.org/wiki?curid=57367053", "title": "Mobile metering", "text": "Mobile metering\n\nMobile metering (recording of data using a mobile meter) is a technology which enables mobile recording of metering data. While railway companies such as the German Deutsche Bahn have been using this technology for years in their trains, it is now also being used for recording the charging transactions of electric vehicles (EVs). \n\nIn the latter case, a mobile electricity meter is integrated either into the vehicle itself or into the respective charging cable. This, together with the necessary communication technology (SIM card), makes it possible to transmit charging data (down to the kWh) to a matching backend. Lean, switchable system sockets suffice for charging – they serve as outlets for the power grid. These system sockets can be reduced to a technical minimum, as the vehicle or the cable, respectively, already carry the necessary billing and communication technology. This makes these sockets especially affordable and avoids running costs compared to conventional charging infrastructure, such as costs for maintenance or meter point operation. \n\nAs a result, precise metering, secure data transmission and efficient billing fulfill all preconditions for a comprehensive and future-proof charging and billing solution for electric mobility. \n\nThe mobile meter was developed in the projects „On Board Metering I & II“, that kicked off in March 2003, sponsored by the German ministry for economic affairs and technology. Participants of the project were:\n\n\nThe project’s approach to electric mobility was not tackling it as a singular challenge. The aim was rather to make a significant contribution to the energy transition by taking electric mobility one step further.\n\nFor this to happen, the EV was to become a system-relevant factor as an energy storage device. The goal was to create as much power grid connection points as possible while at the same time safeguarding exact metering and billing of electricity. This way, the vehicle could gain access to the grid anytime it is parked (ratio of EVs to grid connection points greater than 1). Up to that date, charging infrastructure for electric cars was thought of as stationary, similar to conventional gas stations for combustion cars. \n\nFor this, a shift of technology from the infrastructure to the vehicle side (or the cable, respectively) was needed. Such a network of ubiquitous charging spots was only to be realized with charging infrastructure that would cause comparably low costs over longer periods of time. \n\nThe disruptive approach of mobile metering has by now opened up new possibilities and business models for electric mobility. \n\n"}
{"id": "23921427", "url": "https://en.wikipedia.org/wiki?curid=23921427", "title": "NOAAS Okeanos Explorer", "text": "NOAAS Okeanos Explorer\n\nNOAAS \"Okeanos Explorer\" (R 337) is a converted United States Navy ship (formerly ), now an exploratory vessel for the National Oceanic and Atmospheric Administration (NOAA), officially launched in 2010. Starting in 2010, NOAA entered into a five-year partnership with the San Francisco Exploratorium. The focus is on gathering scientific information about oceans for the public as well as for scientific uses. As much as 95% of the ocean remains unexplored, NOAA officials said. The ship is equipped with cameras and will provide real-time viewing of the ocean floor for scientists and for the public.\n\nThis is a pioneering use of what NOAA calls \"telepresence technology\". The \"Okeanos Explorer\" is the only vessel owned by the U.S. government that is dedicated to exploring the seabed and ocean crust. The ship is named after Okeanos, the Ancient Greek god of the sea, from which also comes the word \"ocean\".\n\nThe converted vessel is covered with cameras capable of high definition images, and sophisticated devices such as 3-D sonar mapping systems. It looks like an average ship, except for a white ball, its satellite dome, mounted high above its bridge. The \"Okeanos Explorer\" is the only NOAA ship to have a dedicated ROV. Although ROVs have been used on other NOAA ships, they are typically removed at the end of a cruise. Having a permanent ROV makes it easier to deploy at any time throughout the field season. On the \"Okeanos Explorer\", there is an integrated control room for operating the multibeam, ROV and telepresence communication equipment. Having the screens and computers permanently wired to the ship makes it more efficient to sustain long-term exploration in remote areas of the world.\n\nFrom June to August 2010, an international team led by scientists from the United States and Indonesia collaborated to explore the depths of Indonesian waters. The expedition, Indonesia-USA Deep-Sea Exploration of the Sangihe Talaud Region (INDEX 2010), featured a number of firsts including: the maiden voyage of the \"Okeanos Explorer\"; the first joint Indonesia-USA ocean exploration expedition; and the first joint international mission with two ships sending live video to scientists in Exploration Command Centers ashore.\n\nFrom June to July 2011, a team of scientists and technicians both at-sea and on shore conducted exploratory investigations on the diversity and distribution of deep-sea habitats and marine life in the vicinity of the Galápagos Islands. The 50-day expedition is divided into two ‘legs’ and includes work in Ecuador, Costa Rica, and international waters.\nMapping and water column information collected during Leg I identified a number of exciting targets to explore with the Institute for Exploration’s Little Hercules ROV during Leg II. Between July 11 and July 28, the ship explored seamounts, the oldest known vent fields, off-axis sulfide mounds, deep fracture zones, and newly discovered vents.\nThe expedition also marks the debut of a new camera sled and lighting platform named Seirios. When deployed from the ship with the Little Hercules, provided scientists and the audiences onshore with the very first video footage from a number of deepwater areas around the Galápagos.\n\nIn August 2011, NOAA Ship Okeanos Explorer embarked on an expedition to the deepest part of the Caribbean Sea, where a team of international scientists both at-sea and on shore will conduct interdisciplinary investigations of the Mid-Cayman Rise – an ultra-slow spreading center where two plates gradually move apart and upwelling magma creates new crust and the adjacent Cayman Trough. Our exploration will focus on the oceanic core complexes that appear to dominate construction of the rift valley walls along much of the Mid-Cayman Rise, and the inner wall of the Cayman Trough fracture zone immediately to the north – investigating the geology, marine life, and hydrothermal systems that these areas might host. \nOver the course of 10 days, the ship mapped the rift valley walls of the Mid-Cayman Rise and northern wall of the Cayman Trough Fracture zone using the ship’s deep-water multibeam sonar; explored the water column for hydrothermal activity using a CTD rosette, in situ sensors, and shipboard dissolved methane analysis; and conducted detailed seafloor investigations the ship’s ROV. During the expedition, live video of ongoing operations were to be streamed to shore where a team of scientists from the U.S. and U.K. joined the expedition remotely through the use of telepresence technology.\n\n\"EX1201\" was the first expedition of the 2012 field season for NOAA Ship Okeanos Explorer. The primary objective of this shakedown cruise was to operationally test the vessel, its systems, and all mission equipment. A secondary objective of the expedition was to use the ship’s technologically advanced multibeam sonar to map three Northeast and Mid-Atlantic canyon areas in support of the benthic habitat identification and management efforts of NOAA's Northeast Fisheries Science Center (NEFSC).After departing its homeport of Davisville, Rhode Island, on February 14, Okeanos Explorer proceeded offshore to Veatch Canyon where a sonar patch test was conducted to ensure the proper calibration of the vessel’s multibeam echosounder. As part of successful patch test operations, approximately 280 square kilometers of Veatch Canyon were bathymetrically mapped, including the majority of the first priority area identified by NEFSC. Later in the cruise, focused survey operations conducted at Block, Ryan, and McMaster Canyons mapped approximately 900 square kilometers of seafloor, including 65 percent of the second NEFSC priority area.Focused mapping operations at Hendrickson, Toms, and Berkeley Canyons covered approximately 1,400 square kilometers of seafloor. Finally, focused mapping operations at Hendrickson, Toms, and Berkeley Canyons covered approximately 1400 square kilometers of seafloor, including 85 percent of the third NEFSC priority area.While transiting along the continental shelf break between focused mapping areas, multibeam, single beam, and sub-bottom sonar data was collected, adding to the seafloor mapping coverage established on Okeanos Explorer cruise EX1106. In addition to successful shakedown testing and mapping operations, EX1201 also yielded a number of positive educational outcomes. During the cruise, two graduate interns were trained in seafloor mapping operations as part of the Okeanos Explorer internship program. Additionally, approximately 50 high school students, educators, and local media toured the vessel and learned about its mission at the conclusion of the expedition, in Charleston, South Carolina.\n\nFrom March to April 2012, a team of scientists and technicians both at-sea and on shore will conducted exploratory investigations on the diversity and distribution of deep-sea habitats and marine life in the northern Gulf of Mexico. The 56-day expedition was divided into three 'legs.' Through discussions and information stemming from the May 2011 Atlantic Basin Workshop, Fall 2011 Gulf of Mexico mapping expedition, and Leg I of the 2012 Gulf of Mexico expedition, NOAA and the broader science community identified a number of exciting targets to explore during Legs II and III. We explored cold seeps, deep coral communities, undersea canyons, and shipwrecks.The expedition also marked the return of the Institute for Exploration’s Little Hercules remotely operated vehicle (ROV) and NOAA's Seirios camera sled and lighting platform to the NOAA Ship Okeanos Explorer. When these systems were deployed from the ship, we were able to provide scientists and the audiences onshore with real-time video footage from deepwater areas in important, yet largely unknown, U.S. waters.\n\nFrom July to August 2013, a team of scientists and technicians both at-sea and on shore will conduct exploratory investigations on the diversity and distribution of deep-sea habitats and marine life along the Northeast U.S. Canyons and at Mytilus Seamount, located within the U.S. Exclusive Economic Zone. The 36-day expedition is composed of two cruise ‘legs.’The exploration area for this community-driven expedition was identified based on the discussions and information stemming from the May 2011 Atlantic Basin Workshop and priority area input received from other NOAA programs and the management community. Using this input, and data acquired during previous Atlantic Canyon Undersea Mapping Expeditions (ACUMEN Project), NOAA and the broader science community have identified a number of exciting targets to explore during the two cruise legs, commencing the next steps in systematic exploration. In the coming weeks, we expect to explore cold seeps, deep coral communities, undersea canyons, landslide features, and a seamount.The expedition also marks the first time NOAA’s new 6,000 meter remotely operated vehicle (ROV), Deep Discoverer and the Seirios camera sled and lighting platform will be used to in a full telepresence-enabled ocean exploration with NOAA Ship Okeanos Explorer. When these systems are deployed from the ship, we will be able to provide scientists and the audiences onshore with real-time video footage from deepwater areas in important, yet largely unknown, U.S. waters.\n\nFrom August through October 2014, NOAA Ship Okeanos Explorer will explore the largely unknown deep-sea ecosystems of the U.S. Atlantic coast. Our at-sea and shore-based science team will collect baseline data in the Atlantic submarine canyons and along the New England Seamount Chain.\n\nSecond of three exploration cruises of NOAA Ship Okeanos Explorer in the Caribbean in the 2015 field season. The primary focus of this cruise is high-resolution mapping of areas near Puerto Rico and the Puerto Rico Trench.\n\nFrom February 25 to March 18, scientists will continue 2015 Hohonu Moana expedition efforts to explore deep-water habitats in and around Papahānaumokuākea Marine National Monument. The expedition will include work on seamounts in the Mid-Pacific Mountains while en route to port in Kwajalein.\n\nFrom February to April 2017, NOAA and partners will conducted two telepresence-enabled ocean exploration cruises on NOAA Ship Okeanos Explorer to collect critical baseline information of unknown and poorly known deepwater areas in American Samoa and Samoa, with an emphasis on Rose Atoll Marine National Monument, National Marine Sanctuary of American Samoa, and National Park of American Samoa.\n\nThe 2017 American Samoa Expedition Part 1: Suesuega o le Moana o Amerika Samoa, was a 14-day telepresence-enabled expedition to explore unknown and poorly understood areas in American Samoa and Samoa with a focus on Rose Atoll Marine National Monument and National Marine Sanctuary of American Samoa. 11 ROV dives were conducted from 250 to 4,000 meters depth. 101 biological specimens (31 primary specimens and 80 associates), were collected including corals, anemones, zoanthids, hydroids, sponges, sea stars, feather stars, brittle stars, urchins, squat lobsters, amphipods, shrimp, barnacles, snails, and polychaete worms. As many as 30 of these specimens could represent new species, and most of the specimens from known species will represent new range records. Along with biological specimens, 30 rock samples for age dating and geochemical analysis were collected to help reveal the geologic history of American Samoa seamounts, atolls, and islands. The dives also obtained samples from the youngest volcanic structures that formed in the Vailulu'u volcanic crater since the previous mapping survey in 2012. More than 12,000 square kilometers of seafloor was mapped, an area 60 times the land area of American Samoa.\n\nFrom May 22 through July 2, NOAA and partners conducted a two-part, telepresence-enabled ocean exploration expedition on NOAA Ship Okeanos Explorer to collect critical baseline information about unknown and poorly understood deepwater areas of the Southeastern United States.\nThe first leg of this expedition took place from May 22 to June 6 and involved the collection of seafloor and water column mapping data to further improve fundamental understanding in this region and to facilitate remotely operated vehicle (ROV) dive planning. Mapping operations were focused offshore Florida, Georgia, and South Carolina, in the South Atlantic Bight - and more specifically, the Blake Plateau - over some of the least explored ares along the East Coast of the United States. During the mission, the team mapped over 4,900 linear kilometers, resulting in complete seafloor bathymetric coverage of over 16,000 square kilometers- an area larger than the states of Connecticut and Rhode Island combined, or nearly the size of Lake Ontario. ROV dives were conducted from July 14 to July 1 during the expedition's second leg.\n\nDuring the second leg of the expedition, 24-hour operations consisting of daytime ROV dives with the Deep Discoverer(D2) and overnight mapping. Prior to the first leg of the expedition, there was limited high-resolution multibeam mapping in the deepwater areas offshore the southeastern U.S. Live feeds of the dives could be viewed online in real time.\n\nThis was the ninth expedition to test telepresence-enabled mapping operations on NOAA Ship Okeanos Explorer. For this expedition, the mapping team was split between shipboard and shore-based mission teams based at the Exploration Command Center (ECC) at the University of New Hampshire's Center for Coastal and Ocean Mapping (UNH CCOM).\n\nNOAA is seeking to provide data for, and interact with, students, teachers, and anyone else who is interested. For this purpose it is collaborating with San Francisco's Exploratorium to develop an online educational programs with live broadcast for all ages and for the general public. They will \"join the scientific expertise and observational resources of the NOAA with the creativity and educational expertise of the Exploratorium\", a press release said. The \"Okeanos Explorer\" is designed as an educational tool that can be followed on Twitter.\n\n\nNOAAS Okeanos Explorer Gulf of Mexico 2018 Expedition\nNOAAS Okeanos Explorer Gulf of Mexico 2017 Expedition\n\n"}
{"id": "32955700", "url": "https://en.wikipedia.org/wiki?curid=32955700", "title": "Natural history museum", "text": "Natural history museum\n\nA natural history museum or museum of natural history is a scientific institution with natural history collections that include current and historical records of animals, plants, fungi, ecosystems, geology, paleontology, climatology, and more.\n\nThe primary role of a natural history museum is to provide the scientific community with current and historical specimens for their research, which is to improve our understanding of the natural world. Some museums have public exhibits to share the beauty and wonder of the natural world with the public; these are referred to as 'public museums'. Some museums feature non-natural history collections in addition to their primary collections, such as ones related to history, art, and science. \n\nRenaissance cabinets of curiosities were private collections that typically included exotic specimens of natural history, sometimes faked, along with other types of object. The first natural history museum was possibly that of Swiss scholar Conrad Gessner, established in Zürich in the mid-16th century. The Muséum National d'Histoire Naturelle, established in Paris in 1635, was the first natural history museum to take the form that would be recognized as a natural history museum today. Early natural history museums offered limited accessibility, as they were generally private collections or holdings of scientific societies. The Ashmolean Museum, opened in 1683, was the first natural history museum to grant admission to the general public.\n"}
{"id": "43392145", "url": "https://en.wikipedia.org/wiki?curid=43392145", "title": "Pharping Hydropower Station", "text": "Pharping Hydropower Station\n\nPharping Hydro Power Project is the first hydro-power project of Nepal and second of Asia. This is situated in Kathmandu district. In 2010, it was declared a Living Museum by government of Nepal and was open for public.\n\nPharping Hydro Power was established in 1911 as Chandrajyoti Hydro-electric power station by Prime Minister Chandra Shamsher Jang Bahadur Rana.Plant was inaugurated by King Prithvi Bir Bikram Shah Dev on Monday, 22 May, 1911 at around 6: 30 Pm by turning the lights on during a program in Tudhikhel, Kathmandu.\n\nCurrently water from the reservoir lake is used for drinking water supply for Lalitpur District. Water is supplied to places like Bhaisipati, Saindu, Kupondole, etc.\n\nNepal Electricity Authority took over Chandrajyoti Hydro-electric Power Station and renamed it Pharping Hydro Power Station and had since been smoothly running the power station till the late 1990s when it was considered that the aging power station needed to be converted into a heritage site. In 2010, it was declared a Living Museum by government of Nepal and was open for public.\n\n"}
{"id": "7979270", "url": "https://en.wikipedia.org/wiki?curid=7979270", "title": "Raised-relief map", "text": "Raised-relief map\n\nA raised-relief map or terrain model is a three-dimensional representation, usually of terrain, materialized as a physical artifact. When representing terrain, the vertical dimension is usually exaggerated by a factor between five and ten; this facilitates the visual recognition of terrain features and velocity.\n\nIf the account of Sima Qian (c. 145–86 BCE) in his \"Records of the Grand Historian\" is proven correct upon the unearthing of Qin Shi Huang's tomb, the raised-relief map has existed since the Qin dynasty (221–206 BCE) of China. Joseph Needham suggests that certain pottery vessels of the Han dynasty (202 BCE – 220 CE) showing artificial mountains as lid decorations may have influenced the raised-relief map.\n\nThe Han dynasty general Ma Yuan made a raised-relief map of valleys and mountains in a rice-constructed model of 32 CE. Such rice models were expounded on by the Tang dynasty (618–907) author Jiang Fang in his \"Essay on the Art of Constructing Mountains with Rice\" (c. 845). A raised-relief map made of wood representing all the provinces of the empire and put together like a giant () jigsaw puzzle was invented by Xie Zhuang (421–466) during the Liu Song dynasty (420–479).\n\nShen Kuo (1031-1095) created a raised-relief map using sawdust, wood, beeswax, and wheat paste. His wooden model pleased Emperor Shenzong of Song, who later ordered that all the prefects administering the frontier regions should prepare similar wooden maps which could be sent to the capital and stored in an archive.\n\nIn 1130, Huang Shang made a wooden raised-relief map which later caught the attention of the Neo-Confucian philosopher Zhu Xi, who tried to acquire it but instead made his own map out of sticky clay and wood. The map, made of eight pieces of wood connected by hinges, could be folded up and carried around by one person.\n\nLater, Ibn Battuta (1304–1377) described a raised-relief map while visiting Gibraltar\n\nIn his 1665 paper for the \"Philosophical Transactions of the Royal Society\", John Evelyn (1620–1706) believed that wax models imitating nature and bas relief maps were something entirely new from France. Some later scholars attributed the first raised-relief map to one Paul Dox, who represented the area of Kufstein in his raised-relief map of 1510.\n\nThere are a number of ways to create a raised-relief map. Each method has advantages and disadvantages in regards to accuracy, price, and relative ease of creation.\nStarting with a topographic map, one can cut out successive layers from some sheet material, with edges following the contour lines on the map. These may be assembled in a stack to obtain a rough approximation of the terrain. This method is commonly used as the base for architectural models, and is usually done without vertical exaggeration. For models of landforms, the stack can then be smoothed by filling with some material. This model may be used directly, or for greater durability a mold may be made from it. This mold may then be used to produce a plaster model.\nA combination of computer numerical control (CNC) machining a master model, and vacuum forming copies from this, can be been used to rapidly mass-produce raised-relief maps. The Vacuum Forming technique, invented in 1947 by the Army Map Service in Washington, D.C., uses vacuum-formed plastic sheets and heat to increase the production rate of these maps. To make the Vacuum-Formed plastic maps, first a master model made of resin or other materials is created with a computer guided milling machine using a digital terrain model. Then a reproduction mold is cast using the master mold and a heat and pressure resistant material. Fine holes are put into the reproduction mold so that the air can later be removed by a vacuum. Next, a plastic sheet is applied to the mold so that they are airtight, and a heater is placed above the plastic for about 10 seconds. The vacuum is then applied to remove the remaining air. After letting the plastic cool, it can be removed and the terrain is complete. After this step, a color map can be overlaid/printed onto the bases that were created to make it realistic. \n\nVacuum-formed plastic maps have many advantages and disadvantages. They can be quickly produced, which can be beneficial in time of war or disaster. However, the accuracy of certain points throughout the model can vary. The points that touch the mold first are the most accurate, while the points that touch the mold last can become bulged and slightly distorted. Also, the effectiveness of this particular construction method varies by the terrain being represented. They are not good at representing sharp-edged land forms like high mountain ranges or urban areas. \n\nAnother method which is becoming more widespread is the use of 3D printing. With the rapid development of this technology its use is becoming increasingly economic. In order to create a raised-relief map using a 3D printer, Digital Elevation Models (DEM) are rendered into a 3D computer model, which can then be sent to a 3D printer. Most consumer-level 3D printers extrude plastic layer by layer to create a 3D object. However, if a map is needed for commercial and professional uses, higher-end printers can be used. These 3D printers use a combination of powders, resins, and even metals to create higher-quality models. After the model is created, color can be added to show different land cover characteristics, providing a more realistic view of the area. Some benefits of using a 3D printed model include the technology and DEMs being more prevalent easier to find, and that they are easier to understand than a typical topographic map.\n\nFor appropriate mathematical functions and especially for certain types of statistics displays, a similar model may be constructed as an aid to understanding a function or as an aid to studying the statistical data.\n\nThe Great Polish Map of Scotland is claimed to be the largest terrain relief model, constructed out of brick and concrete in the grounds of a hotel near Peebles, Scotland. It measures .\n\nHowever, a site in Ningxia province, China was spotted in 2006 using satellite imagery. It measured , had a perimeter and appeared to be a large scale relief model of Aksai Chin, a disputed territory between China and India.\n\n\n\n"}
{"id": "27926536", "url": "https://en.wikipedia.org/wiki?curid=27926536", "title": "Samoan tropical moist forests", "text": "Samoan tropical moist forests\n\nThe Samoan tropical moist forests are a tropical moist broadleaf forest ecoregion in the Samoan Islands. They cover an area of .\n\nThe Central Savai'i Rainforest, comprising an area of on the island of Savai'i in the Samoan Islands, is the largest continuous patch of rainforest in Polynesia. The area contains more than 100 volcanic craters including recent lava flows. There are three broad types of rainforests, the most extensive is lowland forest, followed by montane and cloud forest. The rainforest spans the inland region of the island and contains most of Samoa's endemic native species, many of which are threatened or near extinction.\n\nThese include the rare and unusual tooth-billed pigeon (\"Didinculus strigirostris\") known locally as manumea, the national bird of Samoa and other birds such as the maomao honeyeater (\"Gymnomyza samoensis\"). The Samoan white-eye (\"Zosterops samoensis\") and Samoan moorhen (\"Gallinula pacifica\") are both endemic to Savai'i. The Samoan moorhen was last recorded in 1873 with possible sightings in 1984 at the upland forests and at Mount Silisili in 2003.\n\nIn 1994, Samoa ratified the international and legally binding treaty, the Convention on Biological Diversity to develop national strategies for conservation and sustainable use of biological diversity. As of 2010, protected areas in the country covered 5% of land.\n\nApproximately 30% of Samoa's biodiversity is endemic, found only in Samoa, with new species still being discovered including two new butterflies in 2009 and freshwater fish new to science. The country has more native species of ferns and butterflies than New Zealand, a country 85 times larger.\n\nMost of Samoa's land is under customary ownership, about 81% of which is governed at the local level by \"matai\", the chiefly heads of families. Conservation projects therefore take place in partnership with \"matai\", such as the lowland rainforest preserve in Falealupo village, at the western tip of Savai'i and Tafua village on the south east coast.\n\n"}
{"id": "11291159", "url": "https://en.wikipedia.org/wiki?curid=11291159", "title": "Solar power in India", "text": "Solar power in India\n\nSolar power in India is a fast developing industry. \nThe country's solar installed capacity reached 26 GW as of 30 September 2018.\nIndia expanded its solar-generation capacity 8 times from 2,650 MW on 26 May 2014 to over 20 GW as on 31 January 2018. \nThe 20 GW capacity was initially targeted for 2022 but the government achieved the target four years ahead of schedule.\n\nThe country added 3 GW of solar capacity in 2015-2016, 5 GW in 2016-2017 and over 10 GW in 2017-2018, with the average current price of solar electricity dropping to 18% below the average price of its coal-fired counterpart.\n\nIn January 2015 the Indian government expanded its solar plans, targeting 100 billion in investment and 100 GW of solar capacity (including 40 GW from rooftop solar) by 2022. India's initiative of 100 GW of solar energy by 2022 is an ambitious target, since the world's installed solar-power capacity in 2017 is expected to be 303 GW. The improvements in solar thermal storage power technology in recent years has made this task achievable as the cheaper solar power need not depend on costly and polluting coal/gas/nuclear based power generation for ensuring stable grid operation.\n\nIn addition to its large-scale grid-connected solar PV initiative, India is developing off-grid solar power for local energy needs. Solar products have increasingly helped to meet rural needs; by the end of 2015 just under one million solar lanterns were sold in the country, reducing the need for kerosene. That year, 118,700 solar home lighting systems were installed and 46,655 solar street lighting installations were provided under a national program; just over 1.4 million solar cookers were distributed in India.\n\nIn January 2016, Prime Minister Narendra Modi and French President François Hollande laid the foundation stone for the headquarters of the International Solar Alliance (ISA) in Gwal Pahari, Gurgaon. The ISA will focus on promoting and developing solar energy and solar products for countries lying wholly or partially between the Tropic of Cancer and the Tropic of Capricorn. The alliance of over 120 countries was announced at the Paris COP21 climate summit. One hope of the ISA is that wider deployment will reduce production and development costs, facilitating the increased deployment of solar technologies to poor and remote regions.\n\nA report published by the Institute for Energy Economics and Financial Analysis (IEEFA) found that India installed 10 GW of solar in 2017, almost double its record in 2016. Crucially, India’s “Scheme for Development of Solar Parks” has proven successful at attracting foreign capital toward construction of the world’s largest ultra-mega solar parks.\nWith about 300 clear and sunny days in a year, the calculated solar energy incidence on India's land area is about 5000 trillion kilowatt-hours (kWh) per year (or 5 EWh/yr). The solar energy available in a single year exceeds the possible energy output of all of the fossil fuel energy reserves in India. The daily average solar-power-plant generation capacity in India is 0.20 kWh per m of used land area, equivalent to 1400–1800 peak (rated) capacity operating hours in a year with available, commercially-proven technology.\n\nKarnataka is the top solar state in India exceeding 5,000 MW installed capacity by the end of financial year 2017-18. The installed capacity of Pavagada Solar Park is 600 MW and its ultimate 2,000 MW installed capacity is expected by the end of year 2020.\n\nInstalled photovoltaic capacity in Andhra Pradesh is more than 2049 MW as on 31 July 2017. In 2014, the IPPs agreed with APTransCo to install 619 MW. The following year, NTPC agreed with APTransCo to install the 250-MW NP Kunta Ultra Mega Solar Power Project near Kadiri in Anantapur district. In October 2017, 1000 MW was commissioned at Kurnool Ultra Mega Solar Park which has become the world's largest solar power plant.\n\nRajasthan is one of Indias most solar-developed states, with its total photovoltaic capacity reaching 2289 MW by the end of June, 2018. Rajasthan is also home to the worlds largest Fresnel type 125 MW CSP plant at the Dhirubhai Ambani Solar Park. Jodhpur district leads the state with installed capacity of over 1,500 MW, followed by Jaisalmer and Bikaner.\n\nThe Bhadla Solar Park, with a total ultimate capacity of 2,255 MW, is being developed in four phases of which 260 MW capacity was commissioned by NTPC Limited. Total installed capacity at the end of June, 2018 is 745 MW and the remaining capacity is expected to be commissioned by March, 2019. In September 2018 Acme Solar announced that it had commissioned India's cheapest solar power, 200 MW at Bhadla\n\nThe only tower type solar thermal power plant (2.5 MW) in India is located in Bikaner district.\n\nTamil Nadu has the 5th highest installed solar-power capacity in India on May 2018. The total installed capacity in Tamil Nadu is 1,8 GW. On 1 July 2017, Solar power tariff in Tamil Nadu has hit an all-time low of Rs 3.47 per unit when bidding for 1500 MW capacity was held.\n\nThe 648-MW Kamuthi Solar Power Project is the biggest operating project in the state. On 1 January 2018, NLC India Limited (NCIL) commissioned a new 130 MW solar power project in Neyveli.\n\nGujarat is one of India's most solar-developed states, with its total photovoltaic capacity reaching 1,262 MW by the end of July 2017. Gujarat has been a leader in solar-power generation in India due to its high solar-power potential, availability of vacant land, connectivity, transmission and distribution infrastructure and utilities. According to a report by the Low Emission Development Strategies Global Partnership (LEDS GP) report, these attributes are complemented by political will and investment. The 2009 Solar Power of Gujarat policy framework, financing mechanism and incentives have contributed to a green investment climate in the state and targets for grid-connected solar power.\n\nThe state has commissioned Asia's largest solar park near the village of Charanka in Patan district. The park is generating 2 MW of its total planned capacity of 500 MW, and has been cited as an innovative and environmentally-friendly project by the Confederation of Indian Industry.\n\nTo make Gandhinagar a solar-power city, the state government has begun a rooftop solar-power generation scheme. Under the scheme, Gujarat plans to generate 5 MW of solar power by putting solar panels on about 50 state-government buildings and 500 private buildings.\n\nIt also plans to generate solar power by putting solar panels along the Narmada canals. As part of this scheme, the state has commissioned the 1 MW Canal Solar Power Project on a branch of the Narmada Canal near the village of Chandrasan in Mehsana district. The pilot project is expected to stop of water per year from evaporating from the Narmada River.\n\nThe 125-MW Sakri solar plant is the largest solar-power plant in Maharashtra. The Shri Saibaba Sansthan Trust has the world's largest solar steam system. It was constructed at the Shirdi shrine at an estimated cost of , which was paid as a subsidy by the renewable-energy ministry. The system is used to cook 50,000 meals per day for pilgrims visiting the shrine, resulting in annual savings of 100,000  kg of cooking gas, and was designed to generate steam for cooking even in the absence of electricity to run the circulating pump. The project to install and commission the system was completed in seven months, and the system has a design life of 25 years. The Osmanabad region in Maharashtra has abundant sunlight, and is ranked the third-best region in India in solar insolation. A 10 MW solar power plant in Osmanabad was commissioned in 2013. The total power capacity of Maharashtra is about 500 MW.\n\nMadhya Pradesh is one of India's most solar-developed states, with its total photovoltaic capacity reaching 1,117 MW by the end of July 2017. The Welspun Solar MP project, the largest solar-power plant in the state, was built at a cost of on of land and will supply power at per kWh. A 130 MW solar power plant project at Bhagwanpura, a village in Neemuch district, was launched by Prime Minister Narendra Modi. It is the largest solar producer, and Welspun Energy is one of the top three companies in India's renewable-energy sector.\nA planned 750 MW solar-power plant in Rewa district is also planned and expected to be completed in 2018. This is developing by Rewa Ultra Mega Solar Limited.\n\nIndia's largest floating solar power plant was set upon the Banasura Sagar reservoir in Wayanad, Kerala. The 500 kW (kilowatt peak) solar plant of the Kerala state electricity board (KSEB) floats on 1.25 acres of the water surface of the reservoir. The solar plant has 1,938 solar panels which have been installed on 18 Ferro cement floaters with hollow insides.\n\nGrid-connected solar electricity generation has reached nearly 2% of total utility electricity generation. Solar generation meets the daytime peak load in non-monsoon months when electricity spot prices exceed the daily average price.\n\nBelow is a list of solar power generation facilities with a capacity of at least 10 MW.\n\nIn August 2016, the forecast for solar photovoltaic installations was about 4.8 GW for the calendar year. About 2.8 GW was installed in the first eight months of 2016, more than all 2015 solar installations. India's solar projects stood at about 21 GW, with about 14 GW under construction and about 7 GW to be auctioned. The country's solar capacity reached 19.7 GW by the end of 2017, making it the third-largest global solar market.\n\nIn mid-2018 the Indian power minister RK Singh flagged a tender for a 100GW solar plant at an event in Delhi, while discussing a 10GW tender due to be issued in July that year (at the time, a world record). He also increased the government target for installed renewable energy by 2022 to 227GW.\n\nThe installed capacity of commercial solar thermal power plants (non storage type) in India is 227.5 MW with 50 MW in Andhra Pradesh and 177.5 MW in Rajasthan. \nSolar thermal plants with thermal storage are emerging as cheaper (US 6.1 ¢/kWh or Rs 3.97/KWh) and clean load following power plants to supply electricity round the clock, working as dispatchable generation. \nProper mix of solar thermal (thermal storage type) and solar PV can fully match the load fluctuations without the need of costly battery storage.\n\nThe existing solar thermal power plants (non-storage type) in India, which are generating costly intermittent power on daily basis, can be converted into storage type solar thermal plants to generate 3 to 4 times more baseload power at cheaper cost and not depend on government subsidies.\n\nSolar power, generated mainly during the daytime in the non-monsoon period, complements wind which generate power during the monsoon months in India.\nSolar panels can be located in the space between the towers of wind-power plants. It also complements hydroelectricity, generated primarily during India's monsoon months. Solar-power plants can be installed near existing hydropower and pumped-storage hydroelectricity, utilizing the existing power transmission infrastructure and storing the surplus secondary power generated by the solar PV plants.\n\nDuring the daytime, the additional auxiliary power consumption of a solar thermal storage power plant is nearly 10% of its rated capacity for the process of extracting solar energy in the form of thermal energy. This auxiliary power requirement can be made available from cheaper solar PV plant by envisaging hybrid solar plant with a mix of solar thermal and solar PV plants at a site. Also to optimise the cost of power, generation can be from the cheaper solar PV plant (33% generation) during the daylight whereas the rest of the time in a day is from the solar thermal storage plant (67% generation from Solar power tower and parabolic trough types) for meeting 24 hours baseload power. When solar thermal storage plant is forced to idle due to lack of sunlight locally during cloudy days in monsoon season, it is also possible to consume (similar to a lesser efficient, huge capacity and low cost battery storage system) the cheap excess grid power when the grid frequency is above 50 hz for heating the hot molten salt to higher temperature for converting stored thermal energy in to electricity during the peak demand hours when the electricity sale price is profitable.\n\nGenerating hot water or air or steam using concentrated solar reflectors, is increasing rapidly. Presently concentrated solar thermal installation base for heating applications is about 20 MW in India and expected to grow rapidly. Cogeneration of steam and power round the clock is also feasible with solar thermal CHP plants with storage capacity.\n\nBengaluru has the largest deployment of roof-top solar water heaters in India, generating an energy equivalent of 200 MW. It is India's first city to provide a rebate of on monthly electricity bills for residents using roof-top thermal systems, which are now mandatory in all new structures. Pune has also made solar water heaters mandatory in new buildings. Photovoltaic thermal (PVT) panels produce simultaneously the required warm water/air along with electricity under sunlight.\n\nThe lack of an electricity infrastructure is a hurdle to rural India's development. India's power grid is under-developed, with large groups of people still living off the grid. In 2004, about 80,000 of the nation's villages still did not have electricity; of them, 18,000 could not be electrified by extending the conventional grid. A target of electrifying 5,000 such villages was set for the 2002–2007 Five-Year Plan. By 2004 more than 2,700 villages and hamlets were electrified, primarily with solar photovoltaic systems. The development of inexpensive solar technology is considered a potential alternative, providing an electricity infrastructure consisting of a network of local-grid clusters with distributed electricity generation. It could bypass (or relieve) expensive, long-distance, centralized power-delivery systems, bringing inexpensive electricity to large groups of people. In Rajasthan during FY2016-17, 91 villages have been electrified with a solar standalone system and over 6,200 households have received a 100W solar home-lighting system.\n\nIndia has sold or distributed about 1.2 million solar home-lighting systems and 3.2 million solar lanterns, and has been ranked the top Asian market for solar off-grid products.\n\nBy 2012, a total of 4,600,000 solar lanterns and 861,654 solar-powered home lights were installed. Typically replacing kerosene lamps, they can be purchased for the cost of a few months' worth of kerosene with a small loan. The Ministry of New and Renewable Energy is offering a 30- to 40-percent subsidy of the cost of lanterns, home lights and small systems (up to 210 W). Twenty million solar lamps are expected by 2022.\n\nSolar photovoltaic water-pumping systems are used for irrigation and drinking water. Most pumps are fitted with a motor powered with a 1,800 W PV array which can deliver about of water per day from a total hydraulic head of . By 30 September 2006 a total of 7,068 solar photovoltaic water pumping systems were installed, and 7,771 were installed by March 2012. During hot sunny daytime when the water needs are more for watering the fields, solar pumps performance can be improved by maintaining pumped water flowing/sliding over the solar panels to keep them cooler and clean. Solar driers are used to dry harvests for storage. Low cost solar powered bicycles are also available to ply between fields and village for agricultural activity, etc.\n\nIn addition to solar energy, rainwater is a major renewable resource of any area. \nIn India, large areas are being covered by solar PV panels every year. \nSolar panels can also be used for harvesting most of the rainwater falling on them and drinking-quality water, free from bacteria and suspended matter, can be generated by simple filtration and disinfection processes, as rainwater is very low in salinity. \nGood quality water resources, closer to populated areas, are becoming a scarcity and increasingly costly for consumers. \nExploitation of rainwater for value-added products like bottled drinking water makes solar PV power plants profitable even in high rainfall and cloudy areas by the increased income from drinking water generation.\n\nThin-film solar cell panels offer better performance than crystalline silica solar panels in tropical hot and dusty places like India; there is less deterioration in conversion efficiency with increased ambient temperature, and no partial shading effect. These factors enhance the performance and reliability (fire safety) of thin-film panels. Maximum solar-electricity generation during the hot hours of the day can be used for meeting residential air-conditioning requirements regardless of other load requirements, such as refrigeration, lighting, cooking and water pumping. Power generation of photovoltaic modules can be increased by 17 to 20 percent by equipping them with a tracking system.\n\nResidential electricity consumers who are paying higher slab rates more than per unit, can form in to local groups to install collectively roof top off-grid solar power units (without much battery storage) and replace the costly power used from the grid with the solar power as and when produced. Hence power drawl from the grid which is an assured power supply without much power cuts nowadays, serves as cheaper back up source when grid power consumption is limited to lower slab rate by using solar power during the day time. The maximum power generation of solar panels during the sunny daytime is complementary with the enhanced residential electricity consumption during the hot/summer days due to higher use of cooling appliances such as fans, refrigerators, air conditioners, desert coolers, etc. It would discourage the Discoms to extract higher electricity charges selectively from its consumers. There is no need of any permission from Discoms similar to DG power sets installation. Cheaper discarded batteries of electric vehicle can also be used economically to store the excess solar power generated in the daylight.\n\nSolar-power plants equipped with battery-storage systems where net energy metering is used can feed stored electricity into the power grid when its frequency is below the rated parameter (50 Hz) and draw excess power from the grid when its frequency is above the rated parameter. Excursions above and below the rated grid frequency occur about 100 times daily. The solar-plant owner would receive nearly double the price for electricity sent into the grid compared to that consumed from the grid if a frequency-based tariff is offered to rooftop solar plants or plants dedicated to a distribution substation. A power-purchase agreement (PPA) is not needed for solar plants with a battery storage system to serve ancillary-service operations and transmit generated electricity for captive consumption using an open-access facility. Battery storage is popular in India, with more than 10 million households using battery backup during load shedding. Battery storage systems are also used to improve the power factor. Solar PV or wind paired with four-hour battery storage systems is already cost competitive, without subsidy, as a source of dispatchable generation compared with new coal and new gas plants in India”.\n\nBattery storage is also used economically to reduce daily/monthly peak power demand for minimising the monthly demand charges from the utility to the commercial and industrial establishments. Construction power tariffs are very high in India. Construction power needs of long gestation mega projects can be economically met by installing solar PV plants for permanent service in the project premises with or without battery storage for minimising use of Standby generator sets or costly grid power.\n\nThe land is scarce in India, and per-capita land availability is low. Dedication of land for the installation of solar arrays must compete with other needs. The amount of land required for utility-scale solar power plants is about for every 40–60 MW generated. One alternative is to use the water-surface area on canals, lakes, reservoirs, farm ponds and the sea for large solar-power plants. These water bodies can also provide water to clean the solar panels. Highways and railways may also avoid the cost of land nearer to load centres, minimising transmission-line costs by having solar plants about 10 meters above the roads or rail tracks. Solar power generated by road areas may also be used for in-motion charging of electric vehicles, reducing fuel costs. Highways would avoid damage from rain and summer heat, increasing comfort for commuters.\n\nThe architecture best suited to most of India would be a set of rooftop power-generation systems connected via a local grid. Such an infrastructure, which does not have the economy of scale of mass, utility-scale solar-panel deployment, needs a lower deployment price to attract individuals and family-sized households. Photovoltaics are projected to continue their cost reductions, becoming able to compete with fossil fuels.\n\nGreenpeace recommends that India adopt a policy of developing solar power as a dominant component of its renewable-energy mix, since its identity as a densely-populated country in the tropical belt of the subcontinent has an ideal combination of high insolation and a large potential consumer base. In one scenario India could make renewable resources the backbone of its economy by 2030, curtailing carbon emissions without compromising its economic-growth potential. A study suggested that 100 GW of solar power could be generated through a mix of utility-scale and rooftop solar, with the realizable potential for rooftop solar between 57 and 76 GW by 2024.\n\nDuring the 2015-16 fiscal year NTPC, with 110 MW solar power installations, generated 160.8 million kWh at a capacity utilisation of 16.64 percent (1,458 kWh per kW)—more than 20 percent below the claimed norms of the solar-power industry.\n\nIt is considered prudent to encourage solar-plant installations up to a threshold (such as 7,000 MW) by offering incentives. Otherwise, substandard equipment with overrated nameplate capacity may tarnish the industry. The purchaser, transmission agency and financial institution should require capacity utilisation and long-term performance guarantees for the equipment backed by insurance coverage in the event that the original equipment manufacturer ceases to exist. Alarmed by the low quality of equipment, India issued draft quality guide lines in May 2017 to be followed by the solar plant equipment suppliers conforming to Indian standards.\n\nFifty-one solar radiation resource assessment stations have been installed across India by the Ministry of New and Renewable Energy (MNRE) to create a database of solar-energy potential. Data is collected and reported to the Centre for Wind Energy Technology (C-WET) to create a solar atlas. In June 2015, India began a project to measure solar radiation with a spatial resolution of . This solar-radiation measuring network will provide the basis for the Indian solar-radiation atlas. According to National Institute of Wind Energy officials, the Solar Radiation Resource Assessment wing (121 ground stations) would measure solar radiation's three parameters—Global Horizontal Irradiance (GHI), Direct Normal Irradiance (DNI) and Diffuse Horizontal Irradiance (DHI)—to accurately measure a region's solar radiation.\n\nThe Indian government is promoting solar energy. It announced an allocation of for the Jawaharlal Nehru National Solar Mission and a clean-energy fund for the 2010-11 fiscal year, an increase of from the previous budget. The budget encouraged private solar companies by reducing the import duty on solar panels by five percent. This is expected to reduce the cost of a rooftop solar-panel installation by 15 to 20 percent.\n\nThe average bid in reverse auctions in April 2017 is per kWh, compared with per kWh in 2010, which is around 73% drop over the time window. The current prices of solar PV electricity is around 18% lower than the average price for electricity generated by coal-fired plants. Competitive reverse auctions, falling panel and component prices, the introduction of solar parks, lower borrowing costs and large power companies have contributed to the fall in prices. The cost of solar PV power in India, China, Brazil and 55 other emerging markets fell to about one-third of its 2010 price, making solar the cheapest form of renewable energy and cheaper than power generated from fossil fuels such as coal and gas.\n\nThe levelized cost of solar PV electricity fell below 1.77¢ US per kWh in November 2017, cheaper than fuel cost of any pit head coal-based power plants in India. The intermittent / non-dispatchable solar PV at the prevailing low tariffs clubbed with Pumped-heat electricity storage can offer cheapest dispatchable power round the clock on demand.\n\nThe Indian government has reduced the solar PV power purchase price from the maximum allowed per KWh to per KWh, reflecting the steep fall in cost of solar power-generation equipment. The applicable tariff is offered after applying viability gap funding (VGF) or accelerated depreciation (AD) incentives.\n\nSolar PV generation cost fell to per kWh for the 750 MW Rewa Ultra Mega Solar power project, India's lowest electricity-generation cost. Solar panel prices are lower than those of mirrors by unit area.\n\nIn an auction of 250 MW capacity of the second phase in Bhadla solar park, South Africa's Phelan Energy Group and Avaada Power were awarded 50 MW and 100 MW of capacity respectively in May 2017 at per kilowatt hour. The tariff is also lower than NTPC's average coal power tariff of 3.20 per kilowatt hour. SBG Cleantech, a consortium of Softbank Group, Airtel and Foxconn, was awarded the remaining 100 MW capacity at a rate of per kWh. Few days later in a second auction for another 500 MW at the same park, solar tariff has further fallen to per kilowatt hour which are the lowest tariffs for any solar power project in India. These tariffs are lower than traded prices for day time in non-monsoon period in IEX and also for meeting peak loads on daily basis by using cheaper solar PV power in pumped-storage hydroelectricity stations indicating there is no need of any power purchase agreements and any incentives for the solar PV power plants in India. Solar PV power plant developers are forecasting that solar power tariff would drop to /unit in near future. \n\nThe lowest solar tariff in May 2018 is Rs 2.71/kWh (without incentives) which is less than the tariff of Badla solar park ( per kWh with VGF incentive) after the clarification that any additional taxes are pass through cost with hike in the tariff. In early July 2018 bids, the lowest solar PV tariff has touched per kWh without VGF incentive. The tariff for roof top installations are also falling with the recent offer of with 100% locally made components.\n\nAt the end of July 2015, the chief incentives were:\n\nThe 2018 manufacturing capacity of solar cells and solar modules in India was 1,590 MW and 5,620 MW, respectively. Except for crystalline silicon wafers or cadmium telluride photovoltaics or Float-zone silicon, nearly 80 percent of solar-panel weight is flat glass. One hundred to 150 tons of flat glass is used to manufacture a one-MW solar panel. Low-iron flat or float glass is manufactured from soda ash and iron-free silica. Soda-ash manufacturing from common salt is an energy-intensive process if it is not extracted from soda lakes or glasswort cultivation in alkali soil. To increase installation of photovoltaic solar-power plants, the production of flat glass and its raw materials must expand commensurately to eliminate supply constraints or future imports.\n\nFor utility scale solar projects, top solar module suppliers in 2016-17 were: Trina Solar, JA Solar, Canadian Solar, Risen, Hanwha and GCL Poly.\nFor rooftop solar projects, international companies with the largest market share in the Indian market were: Trina Solar, Canadian Solar, Renesola, REC Solar and Jinko Solar. Similarly, top Indian suppliers were: Vikram Solar, Waaree, Adani, Emmvee and Jakson.\n\n \n"}
{"id": "2610779", "url": "https://en.wikipedia.org/wiki?curid=2610779", "title": "Special Period", "text": "Special Period\n\nThe Special Period in Time of Peace () in Cuba was an extended period of economic crisis that began in 1989 primarily due to the dissolution of the Soviet Union and, by extension, the Comecon. The economic depression of the Special Period was at its most severe in the early to mid-1990s before slightly declining in severity towards the end of the decade once Hugo Chávez's Venezuela emerged as Cuba's primary trading partner and diplomatic ally and especially after the year 2000 once Cuba-Russia relations improved under the presidency of Vladimir Putin. It was defined primarily by the severe shortages of hydrocarbon energy resources in the form of gasoline, diesel, and other petroleum derivatives that occurred upon the implosion of economic agreements between the petroleum-rich Soviet Union and Cuba. The period radically transformed Cuban society and the economy, as it necessitated the successful introduction of sustainable agriculture, decreased use of automobiles, and overhauled industry, health, and diet countrywide. People were forced to live without many goods they had become used to.\n\nThe dissolution of the Soviet Union hit the Cuban economy severely. The country lost approximately 80% of its imports, 80% of its exports and its Gross Domestic Product dropped by 34%. Food and medicine imports stopped or severely slowed. The largest immediate impact was the loss of nearly all of the petroleum imports from the USSR; Cuba's oil imports dropped to 10% of pre-1990 amounts. Before this, Cuba had been re-exporting any Soviet petroleum it did not consume to other nations for profit, meaning that petroleum had been Cuba's second largest export product before 1990. Once the restored Russian Federation emerged from the former Soviet Union, its administration immediately made clear that it had no intention of delivering petroleum that had been guaranteed the island by the USSR; this resulted in a decrease in Cuban consumption by 20% of its previous level within two years. The effect was felt immediately. Entirely dependent on fossil fuels to operate, the major underpinnings of Cuban society—its transport, industrial and agricultural systems—were paralyzed. There were extensive losses of productivity in both Cuban agriculture—which was dominated by modern industrial tractors, combines, and harvesters, all of which required petroleum to run—and in Cuban industrial capacity.\n\nThe early stages of the Special Period were defined by a general breakdown in transportation and agricultural sectors, fertilizer and pesticide stocks (both of those being manufactured primarily from petroleum derivatives), and widespread food shortages. Australian and other permaculturists arriving in Cuba at the time began to distribute aid and taught their techniques to locals, who soon implemented them in Cuban fields, raised beds, and urban rooftops across the nation. Organic agriculture was soon after mandated by the Cuban government, supplanting the old industrialized form of agriculture Cubans had grown accustomed to. Relocalization, permaculture, and innovative modes of mass transit had to be rapidly developed. For a time, waiting for a bus could take three hours, power outages could last up to sixteen hours, food consumption was cut back to one-fifth of its previous level and the average Cuban lost about nine kilograms (twenty pounds). Although starvation was avoided, persistent hunger, something not seen since before the Cuban Revolution, suddenly became a daily experience, and initially, malnutrition in children under five was evident after just a few weeks of these food shortages.\n\nAt the time, United States law allowed humanitarian aid in the form of food and medicine by private groups. Then in March 1996, the Helms-Burton Act imposed further penalties on foreign companies doing business in Cuba, and allowed U.S. citizens to sue foreign investors who use American-owned property seized by the Cuban government.\n\nThe Cuban government was also forced to contract out more lucrative economic and tourism deals with various Western European and South American nations in an attempt to earn the foreign currency necessary to replace the lost Soviet petroleum via the international markets. Additionally faced with a near-elimination of imported steel and other ore-based supplies, Cuba closed refineries and factories across the country, eliminating the country's industrial arm and millions of jobs. The government then proceeded to replace these lost jobs with employment in industrial agriculture and other homegrown initiatives, but these jobs often did not pay as well, and Cubans on the whole became economically poorer. Alternative transport, most notably the Cuban \"camels\"—immense 18-wheeler tractor trailers retrofitted as passenger buses meant to carry hundreds of Cubans each—flourished. Food-wise, meat and dairy products, having been extremely fossil fuel dependent in their former factory farming methods, soon diminished in the Cuban diet. In a shift notable for being generally anathema to Latin American food habits, the people of the island by necessity adopted diets higher in fiber, fresh produce, and ultimately more vegan in character. No longer needing sugar as desperately for a cash crop—the oil-for-sugar program the Soviets had contracted with Cuba had, of course, dissipated—Cuba hurriedly diversified its agricultural production, utilizing former cane fields to grow consumables such as oranges and other fruit and vegetables. The Cuban government also focused more intensely on cooperation with Venezuela once the socialist Hugo Chávez was elected president in 1998.\n\nCubans had to resort to eating anything they could find. In the Havana zoo, \"The peacocks, the buffalo and even the rhea\" were reported to have disappeared. Cuban domestic cats disappeared from streets to dinner tables.\n\nCows in the island were eaten. Before 1959, Cuba boasted as many cattle as people. Today meat is so scarce that it is a crime to kill and eat a cow. To combat illegal cow eating, the government established harsh penalties. A person can get more jail time for killing a cow (10 years in prison) than killing a human. Those who sell beef without government permission can get three to eight years in prison. Eaters of illegal beef can get three months to one year in prison.\n\nThe Special Period's malnutrition created epidemics, but it had positive effects too. Manuel Franco describes the Special Period as \"the first, and probably the only, natural experiment, born of unfortunate circumstances, where large effects on diabetes, cardiovascular disease and all-cause mortality have been related to sustained population-wide weight loss as a result of increased physical activity and reduced caloric intake\".\n\nA paper in the American Journal of Epidemiology, says that \"during 1997-2002, there were declines in deaths attributed to diabetes (51%), coronary heart disease (35%), stroke (20%), and all causes (18%). An outbreak of neuropathy and a modest increase in the all-cause death rate among the elderly were also observed.\" This was caused by how the population tried to reduce the energy store without reducing the nutritional value of the food.\n\nA letter published in the Canadian Medical Association Journal (CMAJ) criticizes the American Journal of Epidemiology for not taking all factors into account and says that \"The famine in Cuba during the Special Period was caused by political and economic factors similar to the ones that caused a famine in North Korea in the mid-1990s. Both countries were run by authoritarian regimes that denied ordinary people the food to which they were entitled when the public food distribution collapsed; priority was given to the elite classes and the military. In North Korea, 3%–5% of the population died; in Cuba the death rate among the elderly increased by 20% from 1982 to 1993.\" All editorial matter in CMAJ represents the opinions of the authors and not necessarily those of the Canadian Medical Association or its subsidiaries. In this article the author's name has been withheld in order to safeguard her or his right to free communication. The regime did not accept American donations of food, medicines and cash until 1993. Thirty thousand Cubans fled the country; thousands drowned or were killed by sharks.\"\n\nNutrition fell from 3,052 calories per day in 1989 to 2,099 calories per day in 1993. Other reports indicate even lower figures, 1,863 calories per day. Some estimated that the very old and children received only 1,450 calories per day. The recommended minimum is 2,100–2,300 calories\n\nHundreds of Cubans protested in Havana on August 5, 1994, some chanting \"Libertad!\" (\"Freedom\"). The protest, in which some threw rocks at police, was dispersed by the police after a few hours. A paper published in the \"Journal of Democracy\" argues that this was the closest that the Cuban opposition could come to asserting itself decisively.\n\nImmediate actions taken by the government included televising an announcement of the expected energy crisis a week before the USSR notified the Cuban government that they would not be delivering the expected quota of crude oil. Citizens were asked to reduce their consumption in all areas and to use public transport and carpooling. As time went on, the administration developed more structured strategies to manage the long-term energy/economic crisis as it stretched into the 21st century.\n\nFood rationing intensified. Monthly allocations for families were based on basic minimum requirements as recommended by the United Nations. However, at the worst of times, the rations comprised only one fifth of these consumption amounts.\n\nThe cost of producing cement and the scarcity of tools and of building materials increased the pressure on already overcrowded housing. Even before the energy crisis, extended families lived in small apartments (many of which were in very poor condition) to be closer to an urban area. To help alleviate this situation, the government engaged in land-distribution where they supplemented larger government-owned farms with privately owned ones. Small homes were built in rural areas and land was provided to encourage families to move, to assist in food production for themselves, and to sell in local farmers' markets. As the film \"\" discusses, co-ops developed which were owned and managed by groups, as well as creating opportunities for allowing them to form \"service co-ops\" where credit was exchanged and group purchasing-power was used to buy seeds and other scarce items.\n\nCubans were accustomed to cars as a convenient mode of transport. It was a difficult shift during the Special Period to adjust to a new way of managing the transport of thousands of people to school, to work and to other daily activities. With the realization that food was the key to survival, transport became a secondary worry and walking, hitch-hiking, and carpooling became the norm. Privately owned vehicles are not common; ownership is not seen as a right but as a privilege awarded for performance. Public transport is creative and takes on the following forms:\n\nCuba's history of colonization included deforestation and overuse of its agricultural land. Before the crisis, Cuba used more pesticides than the U.S. Lack of fertiliser and agricultural machinery caused a shift towards organic farming and urban farming. Cuba still has food rationing for basic staples. Approximately 69% of these rationed basic staples (wheat, vegetable oils, rice, etc.) are imported. Overall, however, approximately 16% of food is imported from abroad.\nInitially, this was a very difficult situation for Cubans to accept; many came home from studying abroad to find that there were no jobs in their fields. It was pure survival that motivated them to continue and contribute to survive through this crisis. The documentary, \"The Power of Community: How Cuba Survived Peak Oil\", states that today, farmers make more money than most other occupations.\n\nDue to a poor economy, there were many crumbling buildings that could not be repaired. These were torn down and the empty lots lay idle for years until the food shortages forced Cuban citizens to make use of every piece of land. Initially, this was an \"ad-hoc\" process where ordinary Cubans took the initiative to grow their own food in any available piece of land. The government encouraged this practice and later assisted in promoting it. Urban gardens sprung up throughout the capital of Havana and other urban centers on roof-tops, patios, and unused parking lots in raised beds as well as \"squatting\" on empty lots. These efforts were furthered by Australian agriculturalists who came to the island in 1993 to teach permaculture, a sustainable agricultural system, and to \"train the trainers\". The Cuban government then sent teams throughout the country to train others.\n\nDowntown Havana kiosks provided advice and resources for individual residents. Widespread farmers' markets gave easy access to locally grown produce; less travel time required less energy use.\n\nThe ideological changes of the Special Period had effects on Cuban society and culture, beyond those on the country. A comprehensive review of these effects concerning ideology, art and popular culture can be found in Ariana Hernandez-Reguant's \"Cuba in the Special Period\". As a result of increased travel and tourism, popular culture developed in new ways. Lisa Knauer, in that volume, describes the circulation of rumba between New York and Havana, and their mutual influences. Antonio Eligio Tonel has described the contemporary art networks that shaped the Cuban art market, and Esther Whitfield the channels through which Cuban literature accessed the wider Spanish speaking world during that period. Elsewhere, Deborah Pacini, Marc Perry, Geoffrey Baker and Sujatha Fernandes extensively wrote about Cuban rap music as a result of these transnational exchanges. In recent years, that is, not in the 1990s which is the period identified with the Special Period, reggaeton has replaced timba as the genre of choice among youth, taking on the explicitly sexual dance moves that originated with timba.\n\nWhereas timba music was a Cuban genre that evolved out of traditional son and jazz, emphasizing blackness and sexuality through sensual dancing, with lyrics that reflected the socio-cultural situation of the period with humor [Hernandez-Reguant 2006], Cuban hip hop evolved as a socially conscious movement influenced heavily by its kin genre in the United States. Thus it was not so much a product of the Special Period—as timba was—as one of globalization.[Fernandes 2004] The Revolution and the blockage of all imports from the US had made the dissemination of American music difficult during the sixties and seventies, as it was often \"tainted as music of the enemy and began to disappear from the public view.\" But all of that changed in the 1990s, when American rappers flocked regularly to Cuba, tourists brought CDs, and North American stations, perfectly audible in Cuba, brought its sounds. Nonetheless, hip hop circulated through informal networks, thus creating a small underground scene of rap enthusiasts located mostly in Havana's Eastern neighborhoods that called the attention of foreign scholars and journalists. Eventually, rappers were offered a space within state cultural networks. The lack of resources to purchase the electronic equipment to produce beats and tracks gives Cuban rap a raw feel that paralleled that of \"old school\" music in the US.\n\n\n"}
{"id": "18718706", "url": "https://en.wikipedia.org/wiki?curid=18718706", "title": "Temperate Northern Pacific", "text": "Temperate Northern Pacific\n\nThe Temperate Northern Pacific is a biogeographic region of the Earth's seas, comprising the temperate waters of the northern Pacific Ocean.\n\nThe Temperate Northern Pacific connects, via the Bering Sea, to the Arctic marine realm, which includes the polar waters of the Arctic Sea. To the south, it transitions to the tropical marine realms of the Pacific, including the Tropical Eastern Pacific along the Pacific coast of the Americas, the Eastern Indo-Pacific in the central Pacific Ocean, and the Central Indo-Pacific of the western Pacific basin. The Taiwan Strait forms the boundary between the Temperate Northern Pacific and the Central Indo-Pacific.\n\nCharacteristic fauna include the Pacific salmon and trout \"(Oncorhynchus\" spp.), gray whale \"(Eschrichtius robustus)\", and North Pacific right whale \"(Eubalaena japonica).\"\n\nThe Temperate Northern Pacific is further subdivided into marine provinces, and the marine provinces divided into marine ecoregions:\n\n\n\n\n\n"}
{"id": "94858", "url": "https://en.wikipedia.org/wiki?curid=94858", "title": "Thunder", "text": "Thunder\n\nThunder is the sound caused by lightning. Depending on the distance from and nature of the lightning, it can range from a sharp, loud crack to a long, low rumble (brontide). The sudden increase in pressure and temperature from lightning produces rapid expansion of the air surrounding and within a bolt of lightning. In turn, this expansion of air creates a sonic shock wave, similar to a sonic boom, often referred to as a \"thunderclap\" or \"peal of thunder\".\n\nThe cause of thunder has been the subject of centuries of speculation and scientific inquiry. The first recorded theory is attributed to the Greek philosopher Aristotle in the fourth century BC, and an early speculation was that it was caused by the collision of clouds. Subsequently, numerous other theories were proposed. By the mid-19th century, the accepted theory was that lightning produced a vacuum.\n\nIn the 20th century a consensus evolved that thunder must begin with a shock wave in the air due to the sudden thermal expansion of the plasma in the lightning channel. The temperature inside the lightning channel, measured by spectral analysis, varies during its 50 μs existence, rising sharply from an initial temperature of about 20,000 K to about 30,000 K, then dropping away gradually to about 10,000 K. The average is about . This heating causes a rapid outward expansion, impacting the surrounding cooler air at a speed faster than sound would otherwise travel. The resultant outward-moving pulse is a shock wave,\nsimilar in principle to the shock wave formed by an explosion, or at the front of a supersonic aircraft.\n\nExperimental studies of simulated lightning have produced results largely consistent with this model, though there is continued debate about the precise physical mechanisms of the process. Other causes have also been proposed, relying on electrodynamic effects of the massive current acting on the plasma in the bolt of lightning. The shockwave in thunder is sufficient to cause injury, such as internal contusion, to individuals nearby.\n\nInversion thunder results when lightning strikes between cloud and ground occur during a temperature inversion. In such an inversion, the air near the ground is cooler than the higher air. The sound energy is prevented from dispersing vertically as it would in a non-inversion and is thus concentrated in the near-ground layer. Inversions often occur when warm moist air passes above a cold front; the resulting thunder sound is significantly louder than it would be if heard at the same distance in a non-inversion condition.\nThe \"d\" in Modern English \"thunder\" (from earlier Old English \"þunor\") is epenthetic, and is now found as well in Modern Dutch \"donder\" (cp Middle Dutch \"donre\", and Old Norse \"þorr\", Old Frisian \"þuner\", Old High German \"donar\" descended from Proto-Germanic *\"þunraz\"). In Latin the term was \"tonare\" \"to thunder\". The name of the Germanic god Thor comes from the Old Norse word for thunder.\n\nThe shared Proto-Indo-European root is *\"tón-r̥\" or \"*\", also found Gaulish \"Taranis\" and Hittite \"Tarhunt\".\n\nA flash of lightning, followed after some time by a rumble of thunder, illustrates the fact that sound travels significantly slower than light. Using this difference, one can estimate how far away the bolt of lightning is by timing the interval between seeing the flash and hearing thunder. The speed of sound in dry air is approximately 343 m/s or 1,127 ft/s or at 20 °C (68 °F). This translates to approximately 3 seconds per kilometer (or 5 seconds per mile); saying \"One-Mississippi... Two-Mississippi...\" (or \"One One-thousand... Two One-Thousand...) is a useful method of counting the seconds from the perception of a given lightning flash to the perception of its thunder (which can be used to gauge the proximity of lightning for the sake of safety).\nThe speed of light is high enough that it can be taken as infinite in this calculation because of the relatively small distance involved. Therefore, the lightning is approximately one kilometer distant for every three seconds that elapse between the visible flash and the first sound of thunder (or one mile for every five seconds). In the same five seconds, the light could have traveled the Lunar distance four times. (In this calculation, the initial shock wave, which travels at a rate faster than the speed of sound, but only extends outward for the first , is ignored.) Thunder is seldom heard at distances over . A very bright flash of lightning and an almost simultaneous sharp \"crack\" of thunder, a \"thundercrack\", therefore indicates that the lightning strike was very near.\n\n\n"}
{"id": "174387", "url": "https://en.wikipedia.org/wiki?curid=174387", "title": "Tree of life (biblical)", "text": "Tree of life (biblical)\n\nThe tree of life (, : \"\") is a term used in the Hebrew Bible that is a component of the world tree motif.\n\nIn the Book of Genesis, the tree of life is first described in as being \"in the midst of the Garden of Eden\" with the tree of the knowledge of good and evil (). After the fall of man, \"lest he put forth his hand, and take also of the tree of life, and eat, and live for ever\", cherubim are placed at the east end of the Garden to guard the way to the tree of life. The tree of life has become the subject of some debate as to whether or not the tree of the knowledge of good and evil is the same tree.\n\nIn the Bible outside of Genesis, the term \"tree of life\" appears in Proverbs () and Revelation (). It also appears in 2 Esdras () and 4 Maccabees (), which are included among the Jewish apocrypha.\n\nKarl Budde, in his critical research of 1883, outlined that there was only one tree in the body of the Genesis narrative and it qualified in two ways: one as the tree in the middle of the Garden, and two as the forbidden tree. Claus Westermann gave recognition to Budde's theory in 1976.\n\nEllen van Wolde noted in her 1994 survey that among Bible scholars \"the trees are almost always dealt with separately and not related to each other” and that “attention is almost exclusively directed to the tree of knowledge of good and evil, whereas the tree of life is paid hardly any attention.\"\n\nThe Eastern Orthodox Church has traditionally understood the tree of life in Genesis as a prefiguration of the Cross, which humanity could not partake of until after the incarnation, death and resurrection of Jesus.\n\nIn \"The City of God\" (xiii.20-21), Augustine of Hippo offers great allowance for \"spiritual\" interpretations of the events in the garden, so long as such allegories do not rob the narrative of its historical reality. Enlightenment theologians (culminating perhaps in Brunner and Niebuhr in the twentieth century) sought for figurative interpretations because they had already dismissed the historical possibility of the story.\n\nOthers sought very pragmatic understandings of the tree. In the \"Summa Theologica\" (Q97), Thomas Aquinas argued that the tree served to maintain Adam's biological processes for an extended earthly animal life. It did not provide immortality as such, for the tree, being finite, could not grant infinite life. Hence after a period of time, the man and woman would need to eat again from the tree or else be \"transported to the spiritual life.\" The common fruit trees of the garden were given to offset the effects of \"loss of moisture\" (note the doctrine of the humors at work), while the tree of life was intended to offset the inefficiencies of the body. Following Augustine in the \"City of God\" (xiv.26), “man was furnished with food against hunger, with drink against thirst, and with the tree of life against the ravages of old age.”\n\nJohn Calvin (\"Commentary on Genesis\" 2:8), following a different thread in Augustine (\"City of God\", xiii.20), understood the tree in sacramental language. Given that humanity cannot exist except within a covenantal relationship with God, and all covenants use symbols to give us \"the attestation of his grace\", he gives the tree, \"not because it could confer on man that life with which he had been previously endued, but in order that it might be a symbol and memorial of the life which he had received from God.\" God often uses symbols - He doesn’t transfer his power into these outward signs, but \"by them He stretches out His hand to us, because, without assistance, we cannot ascend to Him.\" Thus he intends man, as often as he eats the fruit, to remember the source of his life, and acknowledge that he lives not by his own power, but by God's kindness. Calvin denies (contra Aquinas and without mentioning his name) that the tree served as a biological defense against physical aging. This is the standing interpretation in modern Reformed theology as well.\n\nAccording to Jewish mythology, in the Garden of Eden there is a tree of life or the \"tree of souls\" that blossoms and produces new souls, which fall into the Guf, the \"Treasury of Souls\". The Angel Gabriel reaches into the treasury and takes out the first soul that comes into his hand. Then Lailah, the Angel of Conception, watches over the embryo until it is born.\nThe tree of life is represented in several examples of sacred geometry and is central in particular to the Kabbalah, where it is represented as a diagram of ten points.\n\n\n\n"}
{"id": "973007", "url": "https://en.wikipedia.org/wiki?curid=973007", "title": "USDA soil taxonomy", "text": "USDA soil taxonomy\n\nUSDA soil taxonomy (ST) developed by United States Department of Agriculture and the National Cooperative Soil Survey provides an elaborate classification of soil types according to several parameters (most commonly their properties) and in several levels: \"Order\", \"Suborder\", \"Great Group\", \"Subgroup\", \"Family\", and \"Series\". The classification was originally developed by Guy Donald Smith, former director of the U.S. Department of Agriculture's soil survey investigations.\n\nOrder: Entisols \n\nAnother Example\n\nOrder: Alfisols\nLink to Official Series Description: ftp://ftp-fc.sc.egov.usda.gov/NSSC/StateSoil_Profiles/ca_soil.pdf\n\nSoil temperature regimes, such as frigid, mesic, and thermic, are used to classify soils at some of the lower levels of the Soil Taxonomy. The cryic temperature regime distinguishes some higher-level groups. These regimes are based on the mean annual soil temperature (MAST), mean summer temperature, and the difference between mean summer and winter temperatures all at a soil depth of 50 cm. It is normally assumed that the MAST (in °C) equals the sum of the mean annual air temperature plus 2°C. If the difference between mean summer and winter temperatures is less than 6°C, then add \"Iso\" at the front of the name of the Soil Temperature Class.\n\nThe soil moisture regime, often reflective of climatic factors, is a major determinant of the productivity of terrestrial ecosystems, including agricultural systems. The soil moisture regimes are defined based on the levels of the groundwater table and the amounts of soil water available to plants during a given year in a particular region. Several moisture regime classes are used to characterize soils.\n\n\n"}
{"id": "2839592", "url": "https://en.wikipedia.org/wiki?curid=2839592", "title": "Xiazhi", "text": "Xiazhi\n\nThe traditional East Asian calendars divide a year into 24 solar terms. Xiàzhì, \"Geshi\", \"Haji\", or \"Hạ chí\" is the 10th solar term, and marks the summer solstice. It begins when the Sun reaches the celestial longitude of 90° and ends when it reaches the longitude of 105°. It more often refers in particular to the day when the Sun is exactly at the celestial longitude of 90°.\n\nIn the Gregorian calendar, it usually begins around 21 June and ends around 7 July.\n\nThe solstices (as well as the equinoxes) mark the middle of the seasons in traditional East Asian calendars. Here, the Chinese character 至 means \"extreme\", which implies \"solstices\", so the term for the summer solstice directly signifies the summit of summer.\n\n"}
