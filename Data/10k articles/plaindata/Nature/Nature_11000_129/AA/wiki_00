{"id": "37854", "url": "https://en.wikipedia.org/wiki?curid=37854", "title": "Antimatter rocket", "text": "Antimatter rocket\n\nAn antimatter rocket is a proposed class of rockets that use antimatter as their power source. There are several designs that attempt to accomplish this goal. The advantage to this class of rocket is that a large fraction of the rest mass of a matter/antimatter mixture may be converted to energy, allowing antimatter rockets to have a far higher energy density and specific impulse than any other proposed class of rocket.\n\nAntimatter rockets can be divided into three types of application: those that directly use the products of antimatter annihilation for propulsion, those that heat a working fluid or an intermediate material which is then used for propulsion, and those that heat a working fluid or an intermediate material to generate electricity for some form of electric spacecraft propulsion system.\nThe propulsion concepts that employ these mechanisms generally fall into four categories: solid core, gaseous core, plasma core, and beamed core configurations. The alternatives to direct antimatter annihilation propulsion offer the possibility of feasible vehicles with, in some cases, vastly smaller amounts of antimatter but require a lot more matter propellant.\nThen there are hybrid solutions using antimatter to catalyze fission/fusion reactions for propulsion.\n\nAntiproton annihilation reactions produce charged and uncharged pions, in addition to neutrinos and gamma rays. The charged pions can be channelled by a magnetic nozzle, producing thrust. This type of antimatter rocket is a pion rocket or beamed core configuration. It is not perfectly efficient; energy is lost as the rest mass of the charged (22.3%) and uncharged pions (14.38%), lost as the kinetic energy of the uncharged pions (which can't be deflected for thrust), and lost as neutrinos and gamma rays (see antimatter as fuel).\n\nPositron annihilation has also been proposed for rocketry. Annihilation of positrons produces only gamma rays. Early proposals for this type of rocket, such as those developed by Eugen Sänger, assumed the use of some material that could reflect gamma rays, used as a light sail or parabolic shield to derive thrust from the annihilation reaction, but no known form of matter (consisting of atoms or ions) interacts with gamma rays in a manner that would enable specular reflection. The momentum of gamma rays can, however, be partially transferred to matter by Compton scattering. A recent approach is to utilize an ultra-intense laser capable of generating positrons when striking a high atomic number target, such as gold.\n\nThe only concept known to reach relativistic velocities uses a matter-antimatter GeV gamma ray laser photon rocket made possible by a relativistic proton-antiproton pinch discharge, where the recoil from the laser beam is transmitted by the Mössbauer effect to the spacecraft.\n\nThis type of antimatter rocket is termed a thermal antimatter rocket as the energy or heat from the annihilation is harnessed to create an exhaust from non-exotic material or propellant.\n\nThe solid core concept uses antiprotons to heat a solid, high-atomic weight (Z), refractory metal core. Propellant is pumped into the hot core and expanded through a nozzle to generate thrust. The performance of this concept is roughly equivalent to that of the nuclear thermal rocket (formula_1 ~ 10 sec) due to temperature limitations of the solid. However, the antimatter energy conversion and heating efficiencies are typically high due to the short mean path between collisions with core atoms (efficiency formula_2 ~ 85%).\nSeveral methods for the liquid-propellant thermal antimatter engine using the gamma rays produced by antiproton or positron annihilation have been proposed. These methods resemble those proposed for nuclear thermal rockets. One proposed method is to use positron annihilation gamma rays to heat a solid engine core. Hydrogen gas is ducted through this core, heated, and expelled from a rocket nozzle. A second proposed engine type uses positron annihilation within a solid lead pellet or within compressed xenon gas to produce a cloud of hot gas, which heats a surrounding layer of gaseous hydrogen. Direct heating of the hydrogen by gamma rays was considered impractical, due to the difficulty of compressing enough of it within an engine of reasonable size to absorb the gamma rays. A third proposed engine type uses annihilation gamma rays to heat an ablative sail, with the ablated material providing thrust. As with nuclear thermal rockets, the specific impulse achievable by these methods is limited by materials considerations, typically being in the range of 1000–2000 seconds.\n\nThe gaseous core system substitutes the low-melting point solid with a high temperature gas (i.e. tungsten gas/plasma), thus permitting higher operational temperatures and performance (formula_1 ~ 2 × 10 sec). However, the longer mean free path for thermalization and absorption results in much lower energy conversion efficiencies (formula_2 ~ 35%).\n\nThe plasma core allows the gas to ionize and operate at even higher effective temperatures. Heat loss is suppressed by magnetic confinement in the reaction chamber and nozzle. Although performance is extremely high (formula_1 ~ 10-10 sec), the long mean free path results in very low energy utilization (formula_2 ~ 10%)\n\nThe idea of using antimatter to power an electric space drive has also been proposed. These proposed designs are typically similar to those suggested for nuclear electric rockets. Antimatter annihilations are used to directly or indirectly heat a working fluid, as in a nuclear thermal rocket, but the fluid is used to generate electricity, which is then used to power some form of electric space propulsion system. The resulting system shares many of the characteristics of other charged particle/electric propulsion proposals (typically high specific impulse and low thrust).\n\nThis is a hybrid approach in which antiprotons are used to catalyze a fission/fusion reaction or to \"spike\" the propulsion of a fusion rocket or any similar applications.\n\nThe antiproton-driven Inertial confinement fusion (ICF) Rocket concept uses pellets for the D-T reaction. The pellet consists of a hemisphere of fissionable material such as U with a hole through which a pulse of antiprotons and positrons is injected. It is surrounded by a hemisphere of fusion fuel, for example deuterium-tritium, or lithium deuteride. Antiproton annihilation occurs at the surface of the hemisphere, which ionizes the fuel. These ions heat the core of the pellet to fusion temperatures.\n\nThe antiproton-driven Magnetically Insulated Inertial Confinement Fusion Propulsion (MICF) concept relies on self-generated magnetic field which insulates the plasma from the metallic shell that contains it during the burn. The lifetime of the plasma was estimated to be two orders of magnitude greater than implosion inertial fusion, which corresponds to a longer burn time, and hence, greater gain.\n\nThe antimatter-driven P-B concept uses antiprotons to ignite the P-B reactions in an MICF scheme. Excessive radiation losses are a major obstacle to ignition and require modifying the particle density, and plasma temperature to increase the gain. It was concluded that it is entirely feasible that this system could achieve I~10s.\n\nA different approach was envisioned for AIMStar in which small fusion fuel droplets would be injected into a cloud of antiprotons confined in a very small volume within a reaction Penning trap. Annihilation takes place on the surface of the antiproton cloud, peeling back 0.5% of the cloud. The power density released is roughly comparable to a 1 kJ, 1 ns laser depositing its energy over a 200 µm ICF target.\n\nThe ICAN-II project employs the antiproton catalyzed microfission (ACMF) concept which uses pellets with a molar ratio of 9:1 of D-T:U for Nuclear pulse propulsion.\n\nThe chief practical difficulties with antimatter rockets are the problems of creating antimatter and storing it. Creating antimatter requires input of vast amounts of energy, at least equivalent to the rest energy of the created particle/antiparticle pairs, and typically (for antiproton production) tens of thousands to millions of times more. Most storage schemes proposed for interstellar craft require the production of frozen pellets of antihydrogen. This requires cooling of antiprotons, binding to positrons, and capture of the resulting antihydrogen atoms - tasks which have, , been performed only for small numbers of individual atoms. Storage of antimatter is typically done by trapping electrically charged frozen antihydrogen pellets in Penning or Paul traps. There is no theoretical barrier to these tasks being performed on the scale required to fuel an antimatter rocket. However, they are expected to be extremely (and perhaps prohibitively) expensive due to current production abilities being only able to produce small numbers of atoms, a scale approximately 10 times smaller than needed for a 10-gram trip to Mars.\n\nGenerally, the energy from antiproton annihilation is deposited over such a large region that it cannot efficiently drive nuclear capsules. Antiproton-induced fission and self-generated magnetic fields may greatly enhance energy localization and efficient use of annihilation energy.\n\nA secondary problem is the extraction of useful energy or momentum from the products of antimatter annihilation, which are primarily in the form of extremely energetic ionizing radiation. The antimatter mechanisms proposed to date have for the most part provided plausible mechanisms for harnessing energy from these annihilation products. The classic rocket equation with its \"wet\" mass (formula_7)(with propellant mass fraction) to \"dry\" mass (formula_8)(with payload) fraction (formula_9), the velocity change (formula_10) and specific impulse (formula_1) no longer holds due to the mass loses occurring in antimatter annihilation.\n\nAnother general problem with high powered propulsion is excess heat or waste heat, and as with antimatter-matter annihilation also includes extreme radiation. A proton-antiproton annihilation propulsion system transforms 39% of the propellant mass into an intense high-energy flux of gamma radiation. The gamma rays and the high-energy charged pions will cause heating and radiation damage if they are not shielded against. Unlike neutrons, they will not cause the exposed material to become radioactive by transmutation of the nuclei. The components needing shielding are the crew, the electronics, the cryogenic tankage, and the magnetic coils for magnetically assisted rockets. Two types of shielding are needed: radiation protection and thermal protection (different from Heat shield or thermal insulation).\n\nFinally, relativistic considerations have to be taken into account. As the by products of annihilation move at relativistic velocities the rest mass changes according to relativistic mass-energy. For example, the total mass-energy content of the neutral pion is converted into gammas, not just its rest mass. It is necessary to use a relativistic rocket equation that takes into account the relativistic effects of both the vehicle and propellant exhaust (charged pions) moving near the speed of light. These two modifications to the two rocket equations result in a mass ratio (formula_9) for a given (formula_10) and (formula_1) that is much higher for a relativistic antimatter rocket than for either a classical or relativistic \"conventional\" rocket.\n\nThe loss of mass specific to antimatter annihilation requires a modification of the relativistic rocket equation given as\n\nwhere formula_15 is the speed of light, and formula_1 is the specific impulse (i.e. formula_1=0.69formula_15).\n\nThe derivative form of the equation is\n\n}{M_{\\text{ship}}}=\n\nwhere formula_19 is the non-relativistic (rest) mass of the rocket ship, and formula_20 is the fraction of the original (on-board) propellant mass (non-relativistic) remaining after annihilation (i.e., formula_20=0.22 for the charged pions).\n\n}{M_{\\text{ship}}}=\n\nThe resulting relativistic rocket equation with loss of propellant is\n\nThe cosmic background hard radiation will ionize the rocket's hull over time and poses a health threat. Also, gas plasma interactions may cause space charge. The major interaction of concern is differential charging of various parts of a spacecraft, leading to high electric fields and arcing between spacecraft components. This can be resolved with well placed plasma contactor. However, there is no solution yet for when plasma contactors are turned off to allow maintenance work on the hull. Long term space flight at interstellar velocities causes erosion of the rocket's hull due to collision with particles, gas, dust and micrometeorites. At 0.2formula_15 for a 6 light year distance, erosion is estimated to be in the order of about 30 kg/m or about 1 cm of aluminum shielding.\n\n"}
{"id": "2326866", "url": "https://en.wikipedia.org/wiki?curid=2326866", "title": "Asia-Pacific Partnership on Clean Development and Climate", "text": "Asia-Pacific Partnership on Clean Development and Climate\n\nThe Asia-Pacific Partnership on Clean Development and Climate, also known as APP, was an international, voluntary, public-private partnership among Australia, Canada, India, Japan, the People's Republic of China, South Korea, and the United States announced July 28, 2005 at an Association of South East Asian Nations (ASEAN) Regional Forum meeting and launched on January 12, 2006 at the Partnership's inaugural Ministerial meeting in Sydney. As of 5 April 2011, the Partnership formally concluded although a number of individual projects continue. The conclusion of the APP and cancellation of many of its projects attracted almost no media comment.\n\nForeign, Environment and Energy Ministers from partner countries agreed to co-operate on the development and transfer of technology which enables reduction of greenhouse gas emissions that is consistent with and complementary to the UN Framework Convention on Climate Change and other relevant international instruments, and is intended to complement but not replace the Kyoto Protocol., Ministers agreed to a Charter, Communique and Work Plan that \"outline a ground-breaking new model of private-public task forces to address climate change, energy security and air pollution.\" \n\nMember countries account for over 50% of the world's greenhouse gas emissions, energy consumption, GDP and population. Unlike the Kyoto Protocol (currently unratified by the United States), which imposes mandatory limits on greenhouse gas emissions, the Partnership engages member countries to accelerate the development and deployment of clean energy technologies, with no mandatory enforcement mechanism. This has led to criticism that the Partnership is worthless, by other governments, climate scientists and environmental groups. Proponents, on the other hand, argue that unrestricted economic growth and emission reductions can only be brought about through active engagement by all major polluters, which includes India and China, within the Kyoto Protocol framework neither India nor China are yet required to reduce emissions. \n\nCanada became the 7th member of the APP at the Second Ministerial Meeting in New Delhi on October 15, 2007. Canada's Prime Minister Stephen Harper earlier expressed his intention to join the Partnership in August 2007, despite some domestic opposition.\n\nU.S. former President George W. Bush called it a \"new results-oriented partnership\" that he said \"will allow our nations to develop and accelerate deployment of cleaner, more efficient energy technologies to meet national pollution reduction, energy security and climate change concerns in ways that reduce poverty and promote economic development.\" John Howard, the former Australian Prime Minister, described the pact as \"fair and effective\"\n\nHowever, the Worldwide Fund for Nature stated that \"a deal on climate change that doesn't limit pollution is the same as a peace plan that allows guns to be fired\" whilst the British Governments' chief scientific adviser, Sir David King, in a BBC interview said he doubted the new deal could work without setting caps on emissions, but added it should be seen as a sign of progress on climate change. Compared to the Kyoto Protocol, which so far requires no emission reductions from India and China, the APP actively engages both countries through building market incentives to reduce greenhouse emissions along with building capacity and providing clean technology transfers. Proponents argue that this approach creates a greater likelihood that both India and China will, sooner rather than later, effectively cut their greenhouse emissions even though they are not required to do so under the Kyoto Protocol.\n\nThe intent is to create a voluntary, non-legally binding framework for international cooperation to facilitate the development, diffusion, deployment, and transfer of existing, emerging and longer term cost- effective, cleaner, more efficient technologies and practices among the Partners through concrete and substantial cooperation so as to achieve practical results; promote and create enabling environments to assist in such efforts; facilitate attainment of the Partners' respective national pollution reduction, energy security and climate change objectives; and provide a forum for exploring the Partners’ respective policy approaches relevant to addressing interlinked development, energy, environment, and climate change issues within the context of clean development goals, and for sharing experiences in developing and implementing respective national development and energy strategies.\nThe Partnership's inaugural Ministerial meeting established eight government/business taskforces through its Work Plan, posted on the APP website.\n\nThe inaugural ministerial meeting was held at the Four Seasons Hotel and Government House in Sydney, Australia on January 11 and 12, 2006.\n\nAsia-Pacific Partnership Ministers agreed and released a: \n\nPartnership Ministers met again in New Delhi, India on October 15, 2007, and released a second communique and admitted Canada as a Partner.\nThe Ministers also met in Shanghai, China on October 26–27, 2009 where they discussed the accomplishments of the Partnership since the New Delhi Ministerial, and received the results of a report analyzing and evaluating the progress of the APP flagship projects.\n\nThe Partnership has been publicly supported as an alternative to the Kyoto Protocol by governments and business groups in some countries, particularly in countries where the Kyoto Protocol has not been ratified. Many commentators have particularly welcomed the fact that the Partnership overcomes the impasse between developed and developing countries under the United Nations Framework Convention on Climate Change and the Kyoto Protocol and has led to India and China taking some steps to address their greenhouse gas emissions. Mexico, Russia, and several ASEAN members have expressed interest in joining the Partnership in the future.\n\nThe Partnership has been criticized by environmentalists who have rebuked the proceedings as ineffectual without mandatory limits on greenhouse-gas emissions. A coalition of national environment groups and networks from all of the APP countries issued a challenge to their governments to make the APP meaningful by agreeing to mandatory targets, creating financial mechanisms with incentives for the dissemination of clean energy technologies, and create an action plan to overcome the key barriers to technology transfer. U.S. Senator John McCain said the Partnership \"[amounted] to nothing more than a nice little public relations ploy.\", while the Economist described the Partnership as \"patent fig-leaf for the refusal of America and Australia to ratify Kyoto\".\n\nProponents of the Partnership have lauded the APP’s achievements since its inception in 2006. In its over three years, the Partnership has established a record of achievement in promoting collaboration between our governments and private sector in key energy-intensive sectors and activities. The Partnership has worked to develop and implement detailed action plans across key sectors of the energy economy, and to date has endorsed 175 collaborative projects including 22 flagship projects across all the seven Partner countries. These projects have, inter alia, helped power plant managers improve the efficiency of their operations, trained cement plant operators how to save energy at their facilities, assisted in pushing solar photovoltaics toward commercialization, and improved design, equipment and operations of buildings and appliances. The Partnership has been widely noted for its innovative work in public-private sector cooperation, and stands as an example of the benefits of international cooperative efforts in addressing climate change.\n\n\n"}
{"id": "46801209", "url": "https://en.wikipedia.org/wiki?curid=46801209", "title": "Bosque Andino Patagónico", "text": "Bosque Andino Patagónico\n\nThe Bosque Andino Patagónico is a type of temperate to cold forest located in southern Chile and western Patagonia in Chile and Argentina at the southern end of South America. The climate here is influenced by humid air masses moving in from the Pacific Ocean which lose most of their moisture as they rise over the Andes. The flora is dominated by trees, usually of the genus \"Nothofagus\". \nThe Bosque Andino Patagónico represents one of the few relatively undisturbed temperate forests that has been little changed by man. It occupies approximately 6.5 million hectares of land. It is culturally significant because of its archaeological and historical value.\nThe Andean Patagonian forest region is located between 37°S and 55°S latitude, and clothes both sides of the Cordillera. The forest extends for about parallel to the Pacific coast, extending from just south of Mendoza in Argentina to the southern end of Santa Cruz Province and to the island portion of the province of Tierra del Fuego. It is a narrow strip of land, with a maximum width of , dwindling to zero in places near the Chubut River and Santa Cruz Province. It is bounded to the east by the Patagonian Desert. Ecologically the forest have no equivalent in the northern hemisphere. This is indebted to distinct biota histories and environmental differences.\n\nThe forest is divided into four regions; deciduous forest, Magellan forest, Valdivia forest and\nDel Pehuén. Rainfall is more plentiful in the Andean area than in other parts of Patagonia and there are several lakes, reservoirs and hydro-electric projects within the forested area. There is also an increase in population in nearby urban areas and greater tourism in the forest, resulting in increasing pollution.\n\nThe climate of these forests is governed by atmospheric, topographical and oceanic factors. The prevailing westerly winds are laden with moisture and come from the Pacific Ocean, being linked to the South Pacific subtropical anticyclone. Frontal systems move inland and precipitation is high at low altitudes in southern Patagonia, making the fiord region one of the wettest places on earth outside the tropics, with rainfall topping per year in places. To the east of the Andes precipitation declines to nearly nil. In northern Patagonia, the seasons vary as the anticyclone oscillates northwards in summer and southwards in winter. More frontal systems cross the coast during the winter and that is the season when most precipitation occurs on the western slopes of the mountains. Some places suffer prolonged droughts in summer, and further east, on the Patagonian plateau, the air is no longer moist and little rain falls at any time of year. Over a west-east distance of , precipitation can fall from per year to .\n\nOn the western side of the Andes, the cold Humboldt Current and prevailing westerly winds help to reduce temperature variations and the mean air temperature is at 36°S and is at 55°S. On the eastern side of the mountains the temperature variations are much more severe and there is a more continental climate.\n\nMuch of the soils around the Andes are derived from volcanic material. In this soils weathering has produced water-retentive allophane that can allow the vegetation to withstand drought. These soils develop chiefly from volcanic ash and wind-redeposited ash deposits that can reach several meters in height. These kind of soils have high porosity and low bulk density. Locally they are known as trumao which are Andosols. Other soil types include ñadis which are aquepts usually developed on top of glacifluvial material. While \"trumaos\" originate from recent ashes on ancient ashes brown and red-clay soils develop, these soils are not restricted to old ash parent material but do also originate from metamorphic rocks in the Chilean Coast Range.\n\nIn the high Andes, the soils are known as lithosols and contain little organic matter. Much of the ground here may be subject to permafrost and the vegetation is scant, with mosses and lichens growing on the rocks and scree. Generally speaking soils in the northern Argentine area are arranged in the following pattern by decreasing longitude and altitude lithosols, andosols, cambisols and regosols. The last ones occur at the boundary with the steppe.\n\nThis is the easternmost and driest part of the forest and extends from Neuquén to Tierra del Fuego. The three main species here are all deciduous, the lenga beech (\"Nothofagus pumilio\"), the Antarctic beech (\"Nothofagus antarctica\"), which is found typically in lower and damp locations, and the mountain cypress (\"Austrocedrus chilensis\"). Other species present are the radal (\"Lomatia hirsuta\"), the raulí beech (\"Nothofagus alpina\") and the roble beech (\"Nothofagus obliqua\"). Underneath these large trees there is usually a lower layer which includes such shrubs and small trees as boxleaf azara (\"Azara microphylla\"), Chilean wineberry (\"Aristotelia chilensis\"), Darwin's barberry (\"Berberis darwinii\", box-leaved barberry (\"Berberis microphylla\") and the bamboo coligüe cane (\"Chusquea culeou\"), as well as a wide range of herbs.\n\nThe southern end of the forest is characterised by a colder, dry climate and the number of species growing here are more limited. The predominant tree is the evergreen Magellan's beech (\"Nothofagus betuloides\"), sometimes accompanied by the canelo (\"Drimys winteri\"). The understorey shrubs are similar to the deciduous forest apart from the absence of bamboo. There are also peat bogs with various species of sphagnum moss and plants such as the sundew (\"Drosera uniflora\").\n\nThe Valdivian forest lies in a discontinuous belt on the west of the area. The precipitation is greater here. Some parts, such as at Puerto Blest near the Rio Negro, have an average rainfall of per year, and the impression is of jungle. Here the most common trees are the coihue (\"Nothofagus dombeyi\"), the Patagonian cypress (\"Fitzroya cupressoides\"), the Chilean hazel (\"Gevuina avellana\"), the ulmo (\"Eucryphia cordifolia\"), the Guaitecas cypress (\"Pilgerodendron uviferum\"), the podocarp (\"Podocarpus nubigenus\") and the female maniu (\"Saxegothaea conspicua\"). There is a dense understorey of bamboos, the hardy fuchsia (\"Fuchsia magellanica\"), reeds and ferns.\n\nMammals present in the Valdivian forest include an arboreal marsupial, the monito del monte (\"Dromiciops gliroides\"), the world's smallest deer, the southern pudú (\"Pudu pudu\"), and South America's smallest cat, the kodkod (\"Leopardus guigna\"). The giant snail (\"Macrocyclis peruvianus\"), with a shell length of , is also found here.\n\nThis region lies in the centre of the forest to the west of Neuquén. The dominant species here is the Chilean pine (\"Araucaria araucana\"). This area has been the location of much logging in the past. The understorey is scanty and consists of such shrubs as the bamboo caña coligüe (\"Chusquea culeou\") and the box-leaved barberry (\"Berberis buxifolia\").\n\nThe fauna of the Andean Patagonian forest region is very varied with many endemic species. Predatory mammals include the cougar (\"Felis concolor\"), the culpeo (\"Lycalopex culpaeus\"), the southern river otter (\"Lontra provocax\") and the Geoffroy's cat (\"Leopardus geoffroyi\"). Other mammals include Humboldt's hog-nosed skunk (\"Conepatus humboldtii\"), the Patagonian mara (\"Dolichotis patagonum\"), the guanaco (\"Lama guanicoe\"), the south Andean deer (\"Hippocamelus bisulcus\") and numerous species of rodents.\n\nThere is a wide range of bird life, ranging from the black-chested buzzard-eagle (\"Geranoaetus melanoleucus\") and Magellanic woodpecker (\"Campephilus magellanicus\") to the Chilean swallow (\"Tachycineta meyeni\") and the torrent duck (\"Merganetta armata\"). Amphibians are represented by the toad (\"Rhinella spinulosa\"), the large four-eyed frog (\"Pleurodema bufoninum\"), \"Pleurodema bibroni\" and the marsupial Darwin's frog (\"Rhinoderma darwinii\"), and there are also numerous lizards and the Peru slender snake (\"Tachymenis peruviana\"). There are many species of native fish in the lakes and rivers.\n\nRock paintings have been found in the valley of the Rio Manso in north Patagonia, both in Argentina and in Chile. The paintings are largely abstract and mostly consist of complex patterns of straight lines, with zig-zag lines, diamonds and triangles, and sometimes concentric circles. There is some figurative art showing humans and animals and the colour most often used is different shades of red. The style of the work makes it likely to have been performed by hunter-gatherers in the late Holocene era.\n"}
{"id": "10987119", "url": "https://en.wikipedia.org/wiki?curid=10987119", "title": "Bromochloromethane", "text": "Bromochloromethane\n\nBromochloromethane or methylene bromochloride and Halon 1011 is a mixed halomethane. It is a heavy low-viscosity liquid with refractive index 1.4808.\n\nIt was invented for use in fire extinguishers by the Germans during the mid-1940s, in an attempt to create a less-toxic, more effective alternative to carbon tetrachloride. This was a concern in aircraft and tanks as carbon tetrachloride produced highly toxic by-products when discharged onto a fire. CBM was slightly less toxic, and used up until the late 1960s, being officially banned by the NFPA for use in fire extinguishers in 1969, as safer and more effective agents such as halon 1211 and 1301 were developed. Due to its ozone depletion potential its production was banned from January 1, 2002, at the \"Eleventh Meeting of the Parties for the Montreal Protocol on Substances that Deplete the Ozone Layer\".\n\nIt can be biologically decomposed using the hydrolase enzyme alkylhalidase by the reaction:\n\nIn the \"Enter the Matrix\" video game, the player can find a \"Chloro-Bromo Methane Gun\", which is used as a fire extinguisher. It fires a pressurized cartridge of CBM gas to put out fires. However, due to its chemical properties, firing a cartridge near people causes their lungs to fill with liquid, effectively drowning them.\n\n"}
{"id": "869677", "url": "https://en.wikipedia.org/wiki?curid=869677", "title": "Buru", "text": "Buru\n\nBuru (formerly spelled Boeroe, Boro, or Bouru) is the third largest island within Maluku Islands of Indonesia. It lies between the Banda Sea to the south and Seram Sea to the north, west of Ambon and Seram islands. The island belongs to Maluku province () and includes the Buru () and South Buru () regencies. Their administrative centers, Namlea and Namrole, respectively, have ports and the largest towns of the island. There is a military airport at Namlea which supports civilian cargo transportation.\n\nAbout a third of the population is indigenous, mostly Buru, but also Lisela, Ambelau and Kayeli people. The rest of population are immigrants from Java and nearby Maluku Islands. Religious affiliation is evenly split between Christianity and Sunni Islam, with some remnants of traditional beliefs. While local languages and dialects are spoken within individual communities, the national Indonesian language is used among the communities and by the administration. Most of the island is covered with forests rich in tropical flora and fauna. From the present 179 bird and 25 mammal species, about 14 are found either on Buru only or also on a few nearby islands, the most notable being the wild pig Buru babirusa. There is little industry on the island, and most population is engaged in growing rice, maize, sweet potato, beans, coconuts, cocoa, coffee, clove and nutmeg. Other significant activities are animal farming and fishing.\n\nThe island was first mentioned around 1365. Between 1658 and 1942, it was colonised by the Dutch East India Company and then by the Crown of the Netherlands. The Dutch administration relocated many local villages to the newly built island capital at Kayeli Bay for working at clove plantations. It also promoted the hierarchy among the indigenous people with selected loyal rajas placed above the heads of the local clans. The island was occupied by the Japanese forces between 1942 and 1945 and in 1950 became part of independent Indonesia. During former president Suharto's New Order administration in the 1960s–1970s, Buru was the site of a prison used to hold thousands of political prisoners. While held at Buru, writer Pramoedya Ananta Toer wrote most of his novels, including \"Buru Quartet\".\n\nBuru island lies between two seas of the Pacific Ocean – Seram Sea () on the north and Banda Sea () to the south and west. To the east, it is separated by the Manipa Strait () from Seram Island (). With an area of , Buru is the third largest among the Maluku Islands () after Halmahera and Seram.\n\nBuru is shaped like an oval, elongated from west to east. The maximum length is about from east to west and from north to south. The coastline is smooth, with the only indentation being Kayeli Bay located on the eastern coast. The bay also has a smooth, oval shape. It extends into the island to 8–12 km and has a maximum width of 17 km; the width decreases to 9 km at the mouth; the coastal length of the bay is about 50 km. At the northern part of the mouth stands Namlea, the largest town of the island.\n\nThe highest point on the island () is the peak of Mount Kapalatmada (also called Kepala Madan, Kepalat Mada or Ghegan). Off the coast of Buru there are several smaller islands; those permanently inhabited are Ambelau (the largest, about 10 km in diameter, located about 20 km south-east of Buru) and Tengah (). The largest uninhabited islands are Fogi (), Oki () and Tomahu ().\n\nThe island is mostly mountainous, especially in the central and western parts. Of the 9,505 km of the island area, 1,789 km lie 900 m above mean sea level, 872 km above 1,200 m and 382 km above 1,500 m. Flat areas are located in narrow strips near the coast and along the banks of river Apo. There they form a valley of the same name. Much of the island is covered with tropical rain forest.\n\nWith a length of about , Apo is the longest river of Buru. It flows nearly straight to the north-east and empties into Kayeli Bay; however, its bed is very winding for hundreds of meters, with loops all along its length. Two other permanent rivers are Geren and Nibe; the rest are intermittent rivers with discontinuous flow. The river discharge varies significantly through the year, reaching a maximum in the rainy season. Indonesian sources often include \"wae\" (meaning river) before the river names; thus Apo is often referred to as Waeapo or Wae Apo, or Apu in some local dialects. In the center of the island, at an altitude of , lies freshwater Lake Rana (). This is the only significant lake on Buru; it has a nearly rectangular shape with the length of about 4.3 km, width of about 2.7 km and an area of \n\nThe crust consists of several types of deposits. It is dominated by Cenozoic sedimentary rocks, probably originating from the Australian continent; Also present are younger volcanic rocks and more recent alluvial deposits. Sedimentary deposits in the form of silt, peat, sand and mud are mostly found in the river valleys. Metamorphic rocks of slate, schist and arkose dominate the northern part of the island. Very few mineral deposits of Buru have industrial value, and only limestone is mined commercially. However, significant reserves of oil and gas were discovered in the shelf in 2009. There are numerous coral reefs around the island. The soil mostly consists of yellow-red Podsol, Organosol, Grumosol and various mixtures.\n\nThe climate is equatorial monsoonal, wet, and in general typical for the Maluku Islands. The rainy season extends from October to April with the highest rainfall in December–February. Despite the relatively small size of the island, its mountainous terrain results in several climatic zones. Apart from temperature reduction with altitude, the temperature variations across these zones are negligible, with the annual average of about 26 °C, However the annual precipitation differs and amounts to 1,400–1,800 mm in the north, 1,800–2,000 mm in the center, 2,000–2,500 mm in the south and 3,000–4,000 mm in the mountains, at elevation above 500 m.\n\nAs Buru is located at the boundary between the biogeographic zones of Australia and Asia, its flora and fauna are unique and are the subject of national and international scientific research. Of 25 species of mammals found on the island, at least four are endemic to Buru and closest to it islands. The local species of the wild pig named Buru babirusa (\"Babyrousa babyrussa\") is distinguished from the other \"Babyrousa\" species by having relatively long and thick body-hair. It also has very low fat content in their meat (only 1.27% compared to 5–15% for domestic pigs) and is regarded as a delicacy among the local population, which favours it to other wild pigs or deer in terms of texture and flavour. Also endemic to Buru are three types of bats: Moluccan flying fox (\"Pteropus chrysoproctus\"), Ceram fruit bat \"Pteropus ocularis\" and lesser tube-nosed bat (\"Nyctimene minutus\").\n\nOf the 178 recorded species of birds, 10 are endemic to Buru and nearby islands: Buru racket-tail (\"Prioniturus mada\"), black-lored parrot (\"Tanygnathus gramineus\"), blue-fronted lorikeet (\"Charmosyna toxopei\"), Buru honeyeater (\"Lichmera deningeri\"), Buru cuckooshrike (\"Coracina fortis\"), streaky-breasted jungle-flycatcher (\"Rhinomyias addita\"), madanga (\"Madanga ruficollis\"), Buru white-eye (\"Zosterops buruensis\"), tawny-backed fantail (\"Rhipidura superflua\") and black-tipped monarch (\"Monarcha loricatus\"). Among those, the rufous-throated white-eye is regarded as endangered and the black-lored parrot and vulnerable (threatened) by the International Union for Conservation of Nature; both species were observed only in very limited areas of Buru island. There are another 19 birds that are near-endemic to Buru: rufous-necked sparrowhawk (\"Accipiter erythrauchen\"), dusky megapode (\"Megapodius forstenii\"), Moluccan megapode (\"Megapodius wallacei\"), white-eyed imperial pigeon (\"Ducula perspicillata\"), long-tailed mountain pigeon (\"Gymnophaps mada\"), red lory (\"Eos bornea\"), Moluccan hawk-owl (\"Ninox squamipila\"), Moluccan masked owl (\"Tyto sororcula\"), Wakolo myzomela (\"Myzomela wakoloensis\"), black-faced friarbird (\"Philemon moluccensis\"), drab whistler (\"Pachycephala griseonota\"), white-naped monarch (\"Monarcha pileatus\"), dark-grey flycatcher (\"Myiagra galeata\"), black-eared oriole (\"Oriolus bouroensis\"), pale cicadabird (\"Coracina ceramensis\"), Buru thrush (\"Zoothera dumasi\"), cinnamon-chested flycatcher (\"Ficedula buruensis\"), chestnut-backed bush-warbler (\"Bradypterus castaneus\") and flame-breasted flowerpecker (\"Dicaeum erythrothorax\"). Among butterflies, 25% of the Pieridae and 7% of the Papilionidae found on Buru are endemic to the island.\n\nThe vegetation is characteristic of tropical lowland evergreen and semi-evergreen rain forests, with the dominant family of Dipterocarpaceae, genera of \"Hopea\", \"Shorea\" and \"Vatica\", and the individual species of \"Anisoptera thurifera\", \"Hopea gregaria\", \"Hopea iriana\", \"Hopea novoguineensis\", \"Shorea assamica\", \"Shorea montigena\", \"Shorea selanica\" and \"Vatica rassak\". Some of these trees may grow to more than and are usually bound by thick lianas and other epiphytes. Open forest, woodland, and savanna areas also exist on Buru. The fire-resistant paper bark tree (\"Melaleuca cajuputi\") is common in dry areas. The northwestern part of the island has steep limestone cliffs covered by mixed forests that include \"Shorea\" trees, and stunted \"Dacrydium novo-guineense\" is present at the mountain tops.\n\nPrimary forests constitute 60% of the island, and are mostly found in the areas of Air Buaya and Waeapo. There is only 0.51% of secondary forests, in the district Batabual, and 0.9% of mangroves, at Waeapo, Air Buaya, Batabual and Namlea. A significant part of the island (23.10%) is taken by shrubs, and only 5.83% is open land, which is spread over most districts of Buru.\n\nThe island belongs to the Indonesian province of Maluku (). Until 1999, it was part of the Central Maluku Regency () and then became a separate regency of the same name. In 2008, it was split into Buru Regency () and South Buru Regency ().\n\nBuru Regency has an area of 5,578 km and the administrative center at Namlea. It is divided into 5 districts: Namlea (center in Namlea), Waplau (center – Waplau), Waeapo (center – Waenetat), Air Buaya (center – Air Buaya) and Batubual (center – Ilath). The governor (Regent or , as of October 2009) is Husni Hentihu and Vice Regent is Ramli Umasugi.\n\nSouth Buru Regency (administrative center Namrole) has an area of 5,060 km and includes the Ambelau and other small islands south of Buru. Its governing structure has not been completed as of February 2010, and the current acting Regent is A. R. Uluputti. The regency is divided into 5 districts: Namrole (center – Namrole), Kepalamadan (center – Biloro), Leksula (center – Leksula), Wae Sama (center – Wamsisi),\nand Ambalau (center – Wailua); the last district is entirely located on the island Ambalau.\n\nAs of the 2010 Census, the population of the islands administered as Buru was 161,828 people, with about ⅔ in the northern regency and ⅓ in the southern; this number also includes some other small islands to the south. In the early 2000s, most of the population was concentrated in the coastal areas, and comprised the following major groups: indigenous Buru (33,000 people), Lisela (13,000), Ambelau (6,000) and Kayeli people (800); migrants from Java, and migrants from other Maluku Islands. The migration to Buru was stimulated by the Dutch colonial administration in the 1900s, and by Indonesian authorities in the 1950s–1990s. The local communities speak Buru, Lisela and Ambelau languages in everyday life, however, the national Indonesian language is used as a means of international communication. It is also used for writing, as none of the local languages (except for Buru) have a writing system. Also common is Ambon dialect of Malay language (\"Melayu Ambon\"). The latter is widely used in the Maluku Islands as a second language and is a simplified form of Indonesian language with additions of the local lexicon. Some local dialects, such as the Hukumina and Kayeli, became extinct in the second half of the 20th century.\n\nReligious composition of the population is heterogeneous: the number of islanders practising Sunni Islam and Christianity is almost the same at about 40–45% each, and the rest – mainly residents of remote mountain areas – still follow traditional local cults or do not have a clear religious affiliation. Most Christians are indigenous islanders and migrants from Maluku Islands, while most Muslims originate from Java. The economical crisis of the 1990s resulted in frequent conflicts among Buru people over religious grounds. So within a few days in December 1999, 43 people were killed and at least 150 houses burned in the Wainibe village.\n\nOne of the first mentions of Buru occurred in the Nagarakretagama – an Old Javanese eulogy to Hayam Wuruk, the ruler of the Majapahit Kingdom, dating back to 1365. The island appears in the third line of 15th song in the list of lands subordinate to Majapahit under the name Hutan Kadali.\n\nIn the 16th–17th centuries Buru was claimed by the rulers of Ternate island and by the Portuguese; both claims were, however, symbolic, as neither party controlled the island, but only visited it on trade matters. More active were Makassar people from Sulawesi island, who had built fortifications on Buru and forced the natives to grow valuable spices, such as clove.\n\nThe rivalry between the Makassar and Dutch East India Company for control over production and trade in spices in the east of the Malay archipelago resulted in military conflict. In 1648, a Dutch expedition to Buru expelled the Makassar from the island and destroyed their buildings and boats; instead of re-using the existing clove plantations, the Dutch burned more than three thousand trees, as they were not in position to settle on the island and were afraid that the Makassar might return after their departure. They returned after several years and raised a fortress armed with four cannon and staffed by 24 soldiers in 1658 at the southern coast of Kayeli Bay, in the eastern part of Buru. A permanent settlement was established at the fortress, which became the administrative center of the island. About 2,000 native inhabitants of the island were forcibly relocated to this area from other parts of the island, including much of the tribal nobility, and about thirteen large villages were built around the fort: Lumaite, Hukumina, Palamata, Tomahu, Masarete, Waisama, Marulat, Leliali, Tagalisa, Ilat, Kayeli, Bara and Lisela. The relocation was designed to facilitate control over the local population and provide a workforce for clove fields which were being planted by the Dutch in this part of the island. The Kayeli ethnicity with its own language was formed as a mixture of the newly arriving settlers and the native population of the fort area.\n\nThe presence among the ancestors of the tribal aristocracy and interaction with the Dutch colonial administration resulted in a special position for the Kayeli over the next centuries, who claimed the role of indigenous elite of the island. In particularly, they requested donations from each clan of Buru, which could be rice, millet, coconuts, sago, sweet potatoes and tobacco, as well as supplying men to work exclusively for the Kayeli rajas.\n\nThe Dutch East India Company was abolished in the early 18th century, and all its possessions in the Malay archipelago came under the direct control of the Dutch crown. In 1824, as part of the reform of the colonial administration, Buru was divided into 14 regencies (this number was gradually reduced to seven over the next 100 years). They were headed by the local rulers, rajas, who were subordinate to the Dutch advisors. All rajas were selected from the Kayeli tribal nobles, who had by this time proved their loyalty to the Dutch.\n\nThe demise of Kayeli dominance began in the 1880s, when the leaders of Leliali, Wae Sama and Fogi clans moved significant parts of their ethnic groups to their original settlements; they were joined in the early 1900s by Tagalisa. By then, many other of the original 13 Kayeli villages were either abandoned or had lost their rajas. By about 1910, the leading role of the Kayeli clan had almost disappeared.\n\nFrom the spring of 1942 to the summer of 1945, the entire Dutch East Indies, including Buru, were occupied by the Japanese army. During this period, the island was raided by the bombers of the Allies aiming to incapacitate the Japanese military infrastructure, in particular the airport at Namlea, the major town of Buru.\n\nAfter the capitulation of Japan on 2 September 1945, control over the island fell back to the Netherlands. In December 1946, Buru, along with the other Maluku Islands, Sulawesi and Lesser Sunda Islands, was included in a quasi-independent State of East Indonesia () which was established by the Dutch government to gradually transform their former colonial possessions in the East Indies into a dependent state. In December 1949, eastern Indonesia joined the Republic of the United States of Indonesia ( RIS) established at the Dutch–Indonesian Round Table Conference of 23 August – 2 November 1949.\n\nIn April 1950, just before the abolition of RIS and inclusion of most of eastern Indonesia to the Republic of Indonesia, the local authorities of Buru, Ambon, Seram and several smaller nearby islands proclaimed the establishment of an independent Republic of South Moluccas (, RMS) and committed to maintain close political ties with the Netherlands. After unsuccessful attempts to absorb the RMS through negotiations, the Republic of Indonesia initiated a six-month military conflict in July 1950. In December 1950, Buru was brought under the control of Indonesian troops and proclaimed part of the Republic of Indonesia.\n\nBetween 1950 and 1965, the policy of the new government was aimed at rapid social, political and economic integration of Buru into Indonesia. In the 1960s and 1970s, during the military regime of General Suharto (), Buru became one of the main places of exile and imprisonment of political dissidents – primarily Communists and other left-wing representatives, as well as dissident intellectuals. Most camps were closed on Buru in 1980. More than 12,000 people had served a prison sentence in those camps by then, and at least several hundred had died or been killed.\n\nOne of the prisoners was a prominent Indonesian writer, Pramoedya Ananta Toer, who spent 14 years (1965–1979) in prison, mostly on Buru, and wrote there many of his novels. Those included a large part of \"Buru Quartet\" (), in particular its first part \"The world of humanity\" (). Until 1975, Toer was deprived of writing tools. He committed his novels to memory and was reciting them to his cellmates, partly relying on their memory.\n\nEconomic development of the island was depressed in the late 1990s as a result of the national and regional crisis. The growth resumed in the early 2000s, however, unemployment remains high (9.92% of the population in 2008), and more than 37% of islanders are living below the national poverty line ().\n\nThe basis of the island's economy is agriculture which contributed 51.22% to the GDP in 2008. The major crop is rice with plantations taking an area of more than 5,700 hectares and yielding about 23,000 tonnes per the year (for 2008). Most rice fields are located on the northern coasts of the island, in the districts of Namlea, Waeapo and Air Buaya. With the total area of 135 hectares, maize dominates the southern field of districts Waisama, Kepalamadan and Namrole, yielding 176 tonnes per year (). Other crops of the southern part are sweet potato (211 hectares, 636 tonnes), beans (926 hectares, 946 tonnes ) and soybeans (965 hectares, 965 tonnes). Coco (5,724 ha, 2,742 tonnes), cocoa (4,453 ha, 2,368 tonnes), clove (947 ha, 881 tonnes) and coffee (, 1223 tonnes) are grown in the Namlea, Air Buaya, Waeapo, Batubual and Waplau areas, whereas nutmeg (143 ha, 75 tonnes) is restricted to Batubual. Teak plantations are found almost everywhere on Buru and complement the natural sources of timber.\n\nAnimal farming is of secondary importance, but its role is gradually increasing. The major animals are cows (41,349 animals ), buffalo (3,699), chickens (more than 1,346,000), ducks (195,000), sheep (26,950), domestic pigs (1,276) and horses (497). In 2008, there were 410 fishing enterprises with the annual catch of 3,891 tonnes of fish and seafood. The major commercial species are tuna (900 tonnes), sardines (420 tonnes) and mackerel (385 tonnes).\n\nThe industry employs only about 2,700 islanders and contributes about 7% to the GDP. Among the 537 enterprises registered in 2008, 482 were engaged in processing of agricultural products and 44 in engineering, chemicals and repair. In January 2010, the Ministry of Industry of Indonesia has approved a plan to build major cement plants on the island. The expansion of tourism is hindered by the lack of infrastructure on the island.\n\nApart from agriculture and engineering, other significant economic areas are trade, hotel industry and catering (19.19% of GDP in 2008), custom services (12.74%), transport and communication (3.10%), construction (3.13%), financial sector (2.64%) and energy and water (0.49%).\n\nBuru is linked with other parts of Indonesia via sea routes and has two main ports in Namlea and Namrole. With 866 registered cargo and passenger vessels, the average transport rate in 2008 was 400 tonnes per day. Speedboats \"Bahari Express\" run daily between Namlea and the capital of Maluku Province, Ambon (distance 160 km, travel time three hours).\n\nBy agreement between the administration of Buru district and local military authorities, the military airfield at Namlea (runway 750 meters) is used for air transportation. Aircraft CASA C-212 Aviocar make 96 passenger flights a year between Namlea and several towns of Maluku. In 2007, construction began of a civil airport near the villages of Suva and Namrole, about 30 km south-west of Namlea.\n\nIn absence of railways, most local transportation occurs via the roads. In 2008, their total length was 1,310 km, of which 278 km was covered with asphalt, 282 km with gravel and the rest were laid in soil. The construction project of a modern, highway across the island, connecting its two major towns of Namlea and Namrole and several other towns is delayed due to underfunding. There is a regular long-distance bus service supported by a park of 18 units.\n\n, the medical system of the island was in a poor state due to underfunding and lack of qualified medical personnel. There were 2 hospitals and 16 clinics, 5 of which were first-aid clinics and 11 were serving non-emergency patients who could arrive there on their own. The medical staff consisted of 22 doctors (two with the medical degree), 65 obstetricians and 303 nurses. The authorities plan to increase the number of medical facilities and staff by 2–4 times by 2012.\n\nWhereas local people traditionally occupied villages, variation of their seasonal activities – predominantly hunting and farming – tended to either disperse or gather them. Men hunted the wild pig Buru babirusa, deer and possum, in the forests, mostly during the peak of east monsoon (June and July); meanwhile women were gathering wild vegetables. However, during the west monsoon (November to April) both men and women were working together in the fields. The position of the villages was also changing with time, mostly because of the low soil fertility on the island – recovery of the soil took significant time urging long-distance travel to the new fields. It was rather common for a family to leave the village for most of the week to their field and return only for the religious service on Sunday. It was also rather common to move the entire village after some 20 years of exploiting a plot of land. Partly because of this, most settlements were rather small, with the smallest type accommodating one-two families (\"hum-hawa\" or \"hum-tapa\"), middle-type (\"hum-lolin\") consisting of 3–10 houses and accommodating 20–50 people, and larger ones of 30–50 houses and 150–300 people (\"fen-lale\"). On the coast, there were several multi-ethnic settlements with more than 200 houses (\"ibu kota kecamatan\"). This local variety of terms for a \"settlement\" puzzled the Dutch colonizers trying to systematise the local registries.\n\nTraditional Buru houses were made from bamboo, often on stilts. The roofs were covered with palm leaves or reeds. Cement, metal and tiles were introduced in the 20th century and urged to build more permanent dwelling, but with limited results, as the locals continued to relocate. This was partly due to the habits of relocation and partly due to local disputes or superstitions, such as cursing a place where a certain number of people died within a short period. Presence of a church in a village might hinder relocation for a century, but not always. Traditional Buru costumes are similar those of most other Indonesia peoples. Men wear sarong (a kind of kilt) and a long-skirted tunic, and women are dressed in sarong and a shorter jacket. However, the dress color systematically varies between the different tribes of the island.\n\nThe unique flora, fauna and tropical forest ecosystem of the island are systematically researched by both Indonesian and foreign scientific bodies. The original vegetation on the coastal plain has been cleared, and much of the mountain forest on the northern side of the island has been cut and burned out for timber and creating new farming fields, but two large areas of stable rain forest still exist in the mountains. These are currently protected areas, Gunung Kapalat Mada (1,380 km) and Waeapo (50 km).\n\nMost of the published studies on the history, culture and languages of the island were conducted in the 1980s out by spouses Charles E. Grimes and Barbara Dix Grimes – Australian missionaries and ethnographers and active members of SIL International (they should not be confused with Joseph E. Grimes and Barbara F. Grimes, Charles' parents, also known Australian ethnographers). They have also completed translation of the Bible to Buru language, which was started by the early Dutch missionaries.\n\n\n"}
{"id": "22196710", "url": "https://en.wikipedia.org/wiki?curid=22196710", "title": "Camcon binary actuator", "text": "Camcon binary actuator\n\nThe Camcon binary actuator is a digital valve device used by oil and gas operators and sold by Camcon Technology.\n\n"}
{"id": "8530595", "url": "https://en.wikipedia.org/wiki?curid=8530595", "title": "Chrismon tree", "text": "Chrismon tree\n\nA Chrismon tree is an evergreen tree often found in the chancel or nave of a church during Advent and Christmastide. The Chrismon tree was first used by North American Lutherans in 1957, although the practice has spread to other Christian denominations, including Anglicans, Catholics, Methodists, and the Reformed. As with the Christmas tree, the evergreen tree itself, for Christians, \"symbolizes the eternal life Jesus Christ provides\". However, the Chrismon tree differs from the traditional Christmas tree in that it \"is decorated only with clear lights and Chrismons made from white and gold material\", the latter two being the liturgical colours of the Christmas season. The Chrismon tree is adorned with Chrismons, \"ancient symbols for Christ or some part of Christ's ministry: the crow, descending down, fish, Celtic cross, Jerusalem cross, shepherd's crook, chalice, shell, and others.\" Laurence Hull Stookey writes that \"because many symbols of the Chrismon tree direct our attention to the nature and ultimate work of Christ, they can be helpful in calling attention to Advent themes.\" \n\n\n"}
{"id": "7870371", "url": "https://en.wikipedia.org/wiki?curid=7870371", "title": "Dispersion (geology)", "text": "Dispersion (geology)\n\nDispersion is a process that occurs in soils that are particularly vulnerable to erosion by water. In soil layers where clays are saturated with sodium ions (\"sodic soils\"), soil can break down very easily into fine particles and wash away. This can lead to a variety of soil and water quality problems, including:\n\nDispersive soils are more common in older landscapes where leaching and illuviation processes have had more time to work. A source of sodium is also required. Possible sources can include weathering from soil parent materials or wind-blown salt deposition. Sodium ions are highly mobile in the soil solution and so they accumulate in the lower parts of the landscape.\n\nThe dispersive portion of a soil profile is generally confined to the subsoil, where soil-forming processes concentrate clay minerals and sodium. This means that dispersive soils may not be identified until they are disturbed in a way that exposes the subsoil to running water.\n\nWhen observed in situ, dispersive soil textures may feel 'soapy', and in many cases the physical structure of subsoil layers will be prismatic or columnar. A simplified version of the Emerson soil dispersion test can be completed in the field on a 20-minute to two-hour timescale.\n\nLaboratory tests used to disagnose a soil as dispersive focus on the cation exchange capacity of a soil sample and its cation breakdown. Soil cations are dominated by Ca, Mg, K, and Na, as well as H in acidic soils. The exchangeable sodium percentage ( \"ESP\", (sodium / (total cations)) * 100 ) is a key indicator derived from these measurements. Where ESP exceeds 5%, dispersive behaviour becomes possible, and is highly likely where ESP exceeds 15%.\n\nThe best management approach to these soils is simply to avoid their disturbance. maintaining vegetation cover of the soil is also important to minimize soil dispersion. In agricultural systems, this may mean making an effort to prevent surface soil loss and/or damage, as the surface soil and ground cover form a protective barrier. Landscape-scale management of excess salt is also important, as secondary salinisation can induce dispersive behaviour in soil.\n\nIn places where disturbance has occurred, amelioration with gypsum can be helpful. Other options include:\n\n"}
{"id": "804071", "url": "https://en.wikipedia.org/wiki?curid=804071", "title": "Dutch pollutant standards", "text": "Dutch pollutant standards\n\nDutch Standards are environmental pollutant reference values (i.e., concentrations in environmental medium) used in environmental remediation, investigation and cleanup.\n\nBarring a few exceptions, the target values are underpinned by an environmental risk analysis wherever possible and apply to individual substances. In most cases, target values for the various substances are related to a national background concentration that was determined for the Netherlands.\n\nGroundwater target values provide an indication of the benchmark for environmental quality in the long term, assuming that there are negligible risks for the ecosystem. For metals a distinction is made between deep and shallow groundwater. This is because deep and shallow groundwater contain different background concentrations. An arbitrary limit of 10 metres has been adopted. The target values shown below are for 'shallow' groundwater, 0 – 10 m depth.\n\nThe soil remediation intervention values indicate when the functional properties of the soil for\nhumans, plants and animals is seriously impaired or threatened. They are representative of the\nlevel of contamination above which a serious case of soil contamination is deemed to exist. The target values for soil are adjusted for the organic matter (humus) content and soil fraction <0.2 µm (\"lutum\" - Latin, meaning \"mud\" or \"clay\"). The values below are calculated for a 'Standard Soil' with 10% organic matter and 25% \"lutum\".\n\nA case of environmental contamination is defined as 'serious' if >25 m³ soil or >100 m³ groundwater is contaminated above the intervention value.\n\nThe values presented below are from Annex 1, Table 1, \"Groundwater target values and soil and groundwater intervention values\". In previous versions of the Dutch Standards, target values for soil were also present. However, in the 2009 version, target values for soils have been deleted for all compounds except metals.\n"}
{"id": "3007213", "url": "https://en.wikipedia.org/wiki?curid=3007213", "title": "Electric discharge", "text": "Electric discharge\n\nAn electric discharge is the release and transmission of electricity in an applied electric field through a medium such as a gas.\n\nThe properties and effects of electric discharges are useful over a wide range of magnitudes. Tiny pulses of current are used to detect ionizing radiation in a Geiger–Müller tube. A low steady current can be used to illustrate the spectrum of gases in a gas-filled tube. A neon lamp is an example of a gas-discharge lamp, useful both for illumination and as a voltage regulator. A flashtube generates a short pulse of intense light useful for photography by sending a heavy current through a gas arc discharge. Corona discharges are used in photocopiers. \n\nElectric discharges can convey substantial energy to the electrodes at the ends of the discharge. A spark gap is used in internal combustion engines to ignite the fuel/air mixture on every power stroke. Spark gaps are also used to switch heavy currents in a Marx generator and to protect electrical apparatus. In electric discharge machining, multiple tiny electric arcs are used to erode a conductive workpiece to a finished shape. Arc welding is used to assemble heavy steel structures, where the base metal is heated to melting by the heat of the arc. An electric arc furnace sustains arc currents of tens of thousands of amperes and is used for steelmaking and production of alloys and other products.\n\nExamples of electric discharge phenomena include:\n\n"}
{"id": "21866204", "url": "https://en.wikipedia.org/wiki?curid=21866204", "title": "Energy in Belgium", "text": "Energy in Belgium\n\nEnergy in Belgium describes energy and electricity production, consumption and import in Belgium.\n\nIt is governed by the energy policy of Belgium, which is split over several levels of government. For example, the regional level is responsible for awarding green certificates (except for offshore wind parks) and the national level for anything concerning nuclear power. As a member country of the European Union Belgium also complies to its energy policy.\n\nPrimary energy is the amount of extractable energy present in fuels as they are found in nature. It is often expressed in tonnes of oil equivalent (toe) or watt-hour (Wh). Unless stated otherwise the lower heating value is used in the remainder of this text. A portion of primary energy is converted into other forms before it is used, depending on the energy conversion efficiency of the installation and method employed this number differs significantly from the final energy as consumed by end users.\n\nIn recent years own production is about 25%. However this is mostly due to nuclear power generation, which by convention is counted as domestic production despite the uranium being imported. Renewable energy sources account for the rest of own production.\n\nHistorically, up until 1992, coal was mined in Belgium. Since 2000 the quantities of Coalbed methane have been researched. These show that a concession of Limburgse Reconversie Maatschappij would contain 7 billion cubic meters of mine gas. In 2012 a deal was made with the Australian Dart Energy to investigate the practical extraction of this mine gas.\n\nIn 2010, crude oil was imported mainly from Russia (44%), and OPEC countries (23%). Norway was the largest (37%) supplier of natural gas followed by The Netherlands (29%). While 39% of coal was imported from the United States and 22% from Australia.\n\nAt the end of 2011 Belgium had a distillation capacity 41 Mt. That year 72% of the capacity was used.\n\nElectrabel is main producer of electricity, followed by EDF Luminus.\n\nWith the exception of 2009 Belgium has been a net importer in recent years.\n\nShort term trading is done via the Belpex energy exchange, which is now part of APX-ENDEX. The Belgian transmission grid, operated by Elia System Operator, has a central position in the Synchronous grid of Continental Europe. This allows Belgium to trade electricity with its neighbours. Though currently there are only physical connections with the Netherlands and France. Direct links with Germany(Alegro) and The United Kingdom(Nemo) are planned. Currently a maximum of 3500 MW can be imported. In comparison, the net installed generation capacity in Belgium is estimated to be 19,627 MW.\n\nIn 2010, Belgium produced 95 TWh of electricity. After subtraction of the electricity used in the production process and losses and addition of the net import 84 TWh was consumed by end users.\n\nAccording to the GEMIX report the potential of renewable energy sources is 17 TWh per year. Edora, the union of renewable energy producers in Wallonia refers to studies claiming that 7.8 TWh may come from renewable sources in 2015. \nIn 2000, renewable energy (including biomass) was used for producing 0.95% of the 78.85 TWh of electricity produced domestically\nIn 2011 wind and solar power generated 3.8 TWh.\n\nNuclear power typically contributes between 50% and 60% of the electricity produced domestically (50.4% in 2010).\n\nBelgium has two nuclear power plants:\n\nBy law the nuclear power plants are to be phased-out. The first reactors are scheduled to be taken out of service in 2014, the last ones by 2025. It is widely assumed that the government will replace that law.\n\nTwo reactors are scheduled to be shut down by 2015. The lifetime of one old reactor has been extended through to 2025.\n\nThe use of coal in thermal power plants has been decreasing, in 2000 it was still used to produce 14.25% of electricity, by 2006 this had dropped to about 10%. In 2010 it was down to 6.3%. The conventional coal units of the thermal power plants in Mol and Kallo were closed. \nOil is also playing an increasingly less prominent role in 2010 it accounted only for 0.4% of gross electricity production.\n\nThis reduction is mostly compensated by a rise in popularity of natural gas. Where in 2000 gas only accounted for 23% it was up to 33%(including blast furnace gas) of gross electricity generated in 2010. Natural gas power plants are less polluting and have a short start-up time,\n\nFluxys is the main operator in natural gas transmission.\n\nSeveral power stations use a combined cycle including: Drogenbos, , Tessenderlo. Building permits are processed for plants in Seneffe and Visé.\n\nAt the start of 2012, there were 498 operational wind turbines in Belgium, with a capacity of 1080 MW. The amount of electricity generated from wind energy has surpassed 2 TWh per year.\n\nOn Sunday 6 February 2011, due to high winds and lower electricity consumption, 12% of the country's consumed electricity was generated by wind turbines.\nThere are seven large-scale offshore wind farm projects. Northwind (216MW), Thorntonbank Wind Farm (325 MW), Belwind Wind Farm (330 MW) are operational. The others are in various stages of planning.\n\nThe exploitation of Solar power is on the rise in Belgium. In 2014 the installed capacity expanded to almost 3 GWp, nearly all of it was grid connected. Belgium's 2953 MWp of photovoltaics produced an estimated 2752 GWh of electricity in 2014.\nSources:\n\nIn 2009, biomass and biogas were used to generate 3.5 TWh or 3.8% of gross domestic electricity production.\n\nIn 2010 5.07 million tonnes of waste was produced in Belgium, of which 1.75 Mt was incinerated. Nearly always (99.8% of the time) energy was recovered during incineration. Non renewable waste was used for producing 1.4% of the gross domestic electricity production. 1.9 Mt was recycled and 1 Mt was composted or fermented; only 0.062 Mt was dumped. \nTen years earlier this was only 0.71%.\n\nBelgium has two pumped storage hydroelectric power stations: Coo-Trois-Ponts (1164 MW) and Plate-Taille (143 MW). Pumped storage stations are a net consumer of electricity, but they contributed 1.4% to the gross electricity production in 2010.\n\nDespite the limited potential there are also a number of stations generating hydroelectric power. With a combined capacity of about 100 MW. Contributing 0.3% of gross domestic production in 2010.\n\nAlmost all of this capacity is realised in the Walloon Region. Even though hydroelectric power was used extensively in Flanders prior to the industrial revolution, there are no rivers where it can be generated on a large scale. The region's 15 installations have a combined capacity just shy of 1 MW (994 kW).\n\nIn 2010 the largest share (34%) of final energy was for domestic use (this includes: households, service sector, commerce, and agriculture). Transport and industrial sector both consumed about a quarter. Fossil fuels are also used as raw material in several manufacturing processes, this non-energetic use accounts for the remainder of the final energy.\n\nA more detailed picture of the energy and type of fuel used by various activities is given in the table below.\n\nIn the Brussels-Capital Region, the electricity and natural gas net are operated by Sibelga. In 2011, the natural gas consumption was 10,480 GWh and the electricity consumption was 5,087 GWh.\n\nSibelga invests in combined heat and power (CHP) installations for which it receives green certificates. In 2011 its eleven installations had a combined capacity of 17.8 MWe and 19.7 MWth and generated 50.5 GWh of electricity.\n\nThe Region of Brussels-Capital also encourages MicroCHP and implemented the European directive of 2002/91/CE on Energy Performance of Buildings.\n\nThe companies Umicore, BASF, Solvay, , , ArcelorMittal, and Air Liquide together account for about 15% of the total electricity consumption of Belgium in 2006.\n\nIn 1990, the greenhouse gas (GHG) emissions were 146.9 million tons of equivalent (Mt eq), whose 88 million tons came from the Flemish Region, 54.8 from the Walloon Region and 4 Mt from the Brussels-capital Region.\n\nBeing a member of the European Union, Belgium, applied the European Union Emission Trading Scheme set up by the Directive 2003/87/EC. The Kyoto protocol sets a 7.5% reduction of greenhouse gas emission target compared to 1990. Belgium set up a National Allocation Plan at the federal level with target for each of the three regions.\n\nBelgium takes part in the United Nations Framework Convention on Climate Change and has ratified the Kyoto Protocol.\n\nOn 14 November 2002, Belgium signed the \"Cooperation Agreement for the implementation of a National Climate Plan and reporting in the context of the UNFCCC and the Kyoto protocol\". The first National Allocation Plan was for the period from 2005 to 2007. The European commission approved it on 20 October 2004. The second allocation plan was for the period 2008–2012 and aims a reduction of 7.5% of green house gas emissions compared to 1990.\n\nAccording to Forbes list of billionaires (2011), the Belgian billionaire Wang Xingchun ($1 B 2011) made his wealth in coal business. Wang is a resident of Singapore who holds a Belgian citizenship. Wang is chairman of minerals concern Winsway Coking Coal, an importer of coal to China from Mongolia that went public in Hong Kong 2010.\n\n"}
{"id": "48059540", "url": "https://en.wikipedia.org/wiki?curid=48059540", "title": "Energy in Kyrgyzstan", "text": "Energy in Kyrgyzstan\n\nKyrgyzstan had a total primary energy supply (TPES) of 4.13 Mtoe in 2012. Electricity consumption was 10.14 TWh. The majority of primary energy came from fossil fuels, with coal (25%) and oil products (38%) the main sources. Hydroelectricity, the only significant renewable source in the country, accounted for 8.6% of the primary energy supply and almost all of the electricity supply.\n"}
{"id": "31904062", "url": "https://en.wikipedia.org/wiki?curid=31904062", "title": "Energy policy of Belgium", "text": "Energy policy of Belgium\n\nEnergy policy of Belgium describes the politics of Belgium related to energy. Energy in Belgium describes energy and electricity production. consumption and import in Belgium. Electricity sector in Belgium is the main article of electricity in Belgium.\n\nBelgium is a federal state. The energy policy is mainly set by the three regions. Belgian federal government, if in place, have jurisdiction in some areas. The federal government have responsibility of the nuclear policy - and closing the nuclear plants. Belgium is a member of the European Union compling the energy policy of the European Union.\n\nAt the federal level, Minister Paul Magnette is in charge of politics of global warming.\nBelgium takes part in the European Union Emission Trading Scheme and United Nations Framework Convention on Climate Change. Belgium has signed the Kyoto Protocol.\nBelgium emissions in 2007 were 10 tonnes of carbon dioxide per capita, when the EU 27 average was 7.9 tonnes per capita. \n\nEU renewable energy targets of final energy for Belgium is 13% by 2020.\n\nElectricity was 13% of primary energy use in 2008. According to the IEA statistics the fuels of electricity consumption in 2009 were: 53,8% nuclear, 40,8% fossil, 7,4% renewable and 2% export of production.\n\nIn October 2009, the final version of GEMIX report commissioned by Minister Paul Magnette was published. The report studied the energy sources for the years 2020 and 2030. It recommended not to close the old nuclear reactors in 2015 as previously planned. Ecolo the French-speaking green party was against keeping them after 2015.\n\nExisting share of renewable energy of electricity in Belgium was 2.8% in 2006 and target 6.0% by 2010.\n\nBelgium was an exception in the central Europe by not having feed-in tariff FiT for wind energy in 2007, like e.g. the Netherlands, Denmark, Germany, Italy, Spain, France and Portugal. Wind electricity production per inhabitant in 2009 was in Belgium 93 kWh/person compared to Spain 794, Germany 461, the Netherlands 278, Britain 138 and France 121. \n\nBelgium had in 2007 FiT for small installations of photovoltaic solar power.\n\nTarget for biofuels was in Belgium was 5.75% by 2010. The EU Biofuels Directive (2001) requires 5.75% biofuels of transport fuels excluding aviation by December 31, 2010.\n\nAs of 2009, there was 3.3% of electricity coming from renewable energy.\nThe Flemish Region published various \"Beleidsnota energie\". The 2004-2009 version was published by Minister Kris Peeters. The 2004-2009 version was published by Minister Freya Van den Bossche,on 27 October 2009. This report was partially based on research conducted by VITO. \n\nThe Flemish region stated five goals for the government of 2009-2014:\n\nThe government target is 9% of renewable electricity in 2014 and 13% in 2020.\n\n"}
{"id": "56049870", "url": "https://en.wikipedia.org/wiki?curid=56049870", "title": "Galactic superwind", "text": "Galactic superwind\n\nThe galactic superwind is an unusual outflow of highly energetic radiation and material normally observed in starburst galaxies.\n\nGalactic superwinds are theorized to occur when ejecta from supernovae or stellar winds interact with enough force that the ejecta's kinetic energy is converted to thermal energy. The resulting effect is a massive gust of rapidly expanding super-heated gases that can span the length of a galaxy.\n\nSuperwinds are theorized to form in compact starburst galaxies in which star growth is much higher than in other types of galaxies. This accelerated star growth results in more prevalent solar winds being present in starburst galaxies. Superwinds form when ejecta released either by supernovae or solar winds collide with such force that the shock from the impact converts the kinetic energy of the ejecta into thermal energy. The violent conversion from kinetic to thermal energy prevents a significant amount of energy from being radiated away. This in turn creates an incredibly hot bubble of gas that is under much greater pressure than it surroundings are. Eventually the gas bubble will expand to encompass other particles of ejected gases, further increasing the force and size of its expansion. This \"snowplow\" effect results in a gust of stellar wind and gas that can span the width of a galaxy. A paper produced by the California Institute of Technology theorized that superwinds can potentially be traveling at a velocity of several thousand kilometers per second by the time they enter the intergalactic medium.\n"}
{"id": "12207", "url": "https://en.wikipedia.org/wiki?curid=12207", "title": "Geology", "text": "Geology\n\nGeology (from the Ancient Greek γῆ, \"gē\"(\"earth\") and -λoγία, \"-logia\", (\"study of\", \"discourse\")) is an earth science concerned with the solid Earth, the rocks of which it is composed, and the processes by which they change over time. Geology can also refer to the study of the solid features of any terrestrial planet or natural satellite such as Mars or the Moon. Modern geology significantly overlaps all other earth sciences, including hydrology and the atmospheric sciences, and so is treated as one major aspect of integrated earth system science and planetary science.\n\nGeology describes the structure of the Earth beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.\n\nGeologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding of natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology, a major academic discipline, also plays a role in geotechnical engineering.\n\nThe majority of geological data comes from research on solid Earth materials. These typically fall into one of two categories: rock and unconsolidated material.\n\nThe majority of research in geology is associated with the study of rock, as rock provides the primary record of the majority of the geologic history of the Earth. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle \nillustrates the relationships among them (see diagram).\n\nWhen a rock crystallizes from melt (magma or lava), it is an igneous rock. This rock can be weathered and eroded, then redeposited and lithified into a sedimentary rock. It can then be turned into a metamorphic rock by heat and pressure that change its mineral content, resulting in a characteristic fabric. All three types may melt again, and when this happens, new magma is formed, from which an igneous rock may once more crystallize.\n\nTo study all three types of rock, geologists evaluate the minerals of which they are composed. Each mineral has distinct physical properties, and there are many tests to determine each of them. The specimens can be tested for:\n\n\nGeologists also study unlithified materials (referred to as \"drift\"), which typically come from more recent deposits. These materials are superficial deposits that lie above the bedrock. This study is often known as Quaternary geology, after the Quaternary period of geologic history.\n\nIn the 1960s, it was discovered that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. This theory is supported by several types of observations, including seafloor spreading and the global distribution of mountain terrain and seismicity.\n\nThere is an intimate coupling between the movement of the plates on the surface and the convection of the mantle (that is, the heat transfer caused by bulk movement of molecules within fluids). Thus, oceanic plates and the adjoining mantle convection currents always move in the same direction — because the oceanic lithosphere is actually the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics.\n\nThe development of plate tectonics has provided a physical basis for many observations of the solid Earth. Long linear regions of geologic features are explained as plate boundaries. For example:\n\n\nTransform boundaries, such as the San Andreas Fault system, resulted in widespread powerful earthquakes. Plate tectonics also has provided a mechanism for Alfred Wegener's theory of continental drift, in which the continents move across the surface of the Earth over geologic time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle.\n\nAdvances in seismology, computer modeling, and mineralogy and crystallography at high temperatures and pressures give insights into the internal composition and structure of the Earth.\n\nSeismologists can use the arrival times of seismic waves in reverse to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model.\n\nMineralogists have been able to use the pressure and temperature data from the seismic and modelling studies alongside knowledge of the elemental composition of the Earth to reproduce these conditions in experimental settings and measure changes in crystal structure. These studies explain the chemical changes associated with the major seismic discontinuities in the mantle and show the crystallographic structures expected in the inner core of the Earth.\n\nThe geologic time scale encompasses the history of the Earth. It is bracketed at the earliest by the dates of the first Solar System material at 4.567 Ga (or 4.567 billion years ago) and the formation of the Earth at \n4.54 Ga\n(4.54 billion years), which is the beginning of the informally recognized Hadean eona division of geologic time. At the later end of the scale, it is marked by the present day (in the Holocene epoch).\n\n\nMethods for relative dating were developed when geology first emerged as a natural science. Geologists still use the following principles today as a means to provide information about geologic history and the timing of geologic events.\n\nThe principle of uniformitarianism states that the geologic processes observed in operation that modify the Earth's crust at present have worked in much the same way over geologic time. A fundamental principle of geology advanced by the 18th century Scottish physician and geologist James Hutton is that \"the present is the key to the past.\" In Hutton's words: \"the past history of our globe must be explained by what can be seen to be happening now.\"\n\nThe principle of intrusive relationships concerns crosscutting intrusions. In geology, when an igneous intrusion cuts across a formation of sedimentary rock, it can be determined that the igneous intrusion is younger than the sedimentary rock. Different types of intrusions include stocks, laccoliths, batholiths, sills and dikes.\n\nThe principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault.\n\nThe principle of inclusions and components states that, with sedimentary rocks, if inclusions (or \"clasts\") are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock that contains them.\nThe principle of original horizontality states that the deposition of sediments occurs as essentially horizontal beds. Observation of modern marine and non-marine sediments in a wide variety of environments supports this generalization (although cross-bedding is inclined, the overall orientation of cross-bedded units is horizontal).\n\nThe principle of superposition states that a sedimentary rock layer in a tectonically undisturbed sequence is younger than the one beneath it and older than the one above it. Logically a younger layer cannot slip beneath a layer previously deposited. This principle allows sedimentary layers to be viewed as a form of vertical time line, a partial or complete record of the time elapsed from deposition of the lowest layer to deposition of the highest bed.\n\nThe principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist during the same period throughout the world, their presence or (sometimes) absence provides a relative age of the formations where they appear. Based on principles that William Smith laid out almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils formed globally at the same time.\n\nGeologists also use methods to determine the absolute age of rock samples and geological events. These dates are useful on their own and may also be used in conjunction with relative dating methods or to calibrate relative methods.\n\nAt the beginning of the 20th century, advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geologic events using radioactive isotopes and other methods. This changed the understanding of geologic time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates, it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages.\n\nFor many geologic applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice. These are used in geochronologic and thermochronologic studies. Common methods include uranium-lead dating, potassium-argon dating, argon-argon dating and uranium-thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units that do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement. \nThermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleotopography.\n\nFractionation of the lanthanide series elements is used to compute ages since rocks were removed from the mantle.\n\nOther methods are used for more recent events. Optically stimulated luminescence and cosmogenic radionuclide dating are used to date surfaces and/or erosion rates. Dendrochronology can also be used for the dating of landscapes. Radiocarbon dating is used for geologically young materials containing organic carbon.\n\nThe geology of an area changes through time as rock units are deposited and inserted, and deformational processes change their shapes and locations.\n\nRock units are first emplaced either by deposition onto the surface or intrusion into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.\n\nAfter the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.\n\nWhen rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which causes deeper rock to move on top of shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating \"antiforms\", or where it buckles downwards, creating \"synforms\". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms and synforms.\n\nEven higher pressures and temperatures during horizontal shortening can cause both folding and metamorphism of the rocks. This metamorphism causes changes in the mineral composition of the rocks; creates a foliation, or planar surface, that is related to mineral growth under stress. This can remove signs of the original textures of the rocks, such as bedding in sedimentary rocks, flow features of lavas, and crystal patterns in crystalline rocks.\n\nExtension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units ending up below older units. Stretching of units can result in their thinning. In fact, at one location within the Maria Fold and Thrust Belt, the entire sedimentary sequence of the Grand Canyon appears over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as \"boudins\", after the French word for \"sausage\" because of their visual similarity.\n\nWhere rock units slide past one another, strike-slip faults develop in shallow regions, and become shear zones at deeper depths where the rocks deform ductilely.\n\nThe addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment, and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano.\n\nAll of these processes do not necessarily occur in a single environment, and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is undiscernable without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area.\n\nGeologists use a number of field, laboratory, and numerical modeling methods to decipher Earth history and to understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Sub-specialities of geology may distinguish endogenous and exogenous geology.\n\nGeological field work varies depending on the task at hand. Typical fieldwork could consist of:\n\nIn addition to identifying rocks in the field (lithology), petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, petrologists analyze thin sections of rock samples using a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens. In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals. Stable and radioactive isotope studies provide insight into the geochemical evolution of rock units.\n\nPetrologists can also use fluid inclusion data and perform high temperature and pressure physical experiments to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks. This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution.\n\nStructural geologists use microscopic analysis of oriented thin sections of geologic samples to observe the fabric within the rocks, which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures to better understand the orientations of faults and folds to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings.\n\nThe analysis of structures is often accomplished by plotting the orientations of various features onto stereonets. A stereonet is a stereographic projection of a sphere onto a plane, in which planes are projected as lines and lines are projected as points. These can be used to find the locations of fold axes, relationships between faults, and relationships between other geologic structures.\n\nAmong the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries. In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge. Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt. This helps to show the relationship between erosion and the shape of a mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time.\n\nIn the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.\n\nIn the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them. These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section to provide better absolute bounds on the timing and rates of deposition.\nMagnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores. Other scientists perform stable-isotope studies on the rocks to gain information about past climate.\n\nWith the advent of space exploration in the twentieth century, geologists have begun to look at other planetary bodies in the same ways that have been developed to study the Earth. This new field of study is called planetary geology (sometimes known as astrogeology) and relies on known geologic principles to study other bodies of the solar system.\n\nAlthough the Greek-language-origin prefix \"geo\" refers to Earth, \"geology\" is often used in conjunction with the names of other planetary bodies when describing their composition and internal processes: examples are \"the geology of Mars\" and \"Lunar geology\". Specialised terms such as \"selenology\" (studies of the Moon), \"areology\" (of Mars), etc., are also in use.\n\nAlthough planetary geologists are interested in studying all aspects of other planets, a significant focus is to search for evidence of past or present life on other worlds. This has led to many missions whose primary or ancillary purpose is to examine planetary bodies for evidence of life. One of these is the Phoenix lander, which analyzed Martian polar soil for water, chemical, and mineralogical constituents related to biological processes.\n\nEconomic geology is a branch of geology that deals with aspects of economic minerals that humankind uses to fulfill various needs. Economic minerals are those extracted profitably for various practical uses. Economic geologists help locate and manage the Earth's natural resources, such as petroleum and coal, as well as mineral resources, which include metals such as iron, copper, and uranium.\n\nMining geology consists of the extractions of mineral resources from the Earth. Some resources of economic interests include gemstones, metals such as gold and copper, and many minerals such as asbestos, perlite, mica, phosphates, zeolites, clay, pumice, quartz, and silica, as well as elements such as sulfur, chlorine, and helium.\n\nPetroleum geologists study the locations of the subsurface of the Earth that can contain extractable hydrocarbons, especially petroleum and natural gas. Because many of these reservoirs are found in sedimentary basins, they study the formation of these basins, as well as their sedimentary and tectonic evolution and the present-day positions of the rock units.\n\nEngineering geology is the application of the geologic principles to engineering practice for the purpose of assuring that the geologic factors affecting the location, design, construction, operation, and maintenance of engineering works are properly addressed.\n\nIn the field of civil engineering, geological principles and analyses are used in order to ascertain the mechanical principles of the material on which structures are built. This allows tunnels to be built without collapsing, bridges and skyscrapers to be built with sturdy foundations, and buildings to be built that will not settle in clay and mud.\n\nGeology and geologic principles can be applied to various environmental problems such as stream restoration, the restoration of brownfields, and the understanding of the interaction between natural habitat and the geologic environment. Groundwater hydrology, or hydrogeology, is used to locate groundwater, which can often provide a ready supply of uncontaminated water and is especially important in arid regions, and to monitor the spread of contaminants in groundwater wells.\n\nGeologists also obtain data through stratigraphy, boreholes, core samples, and ice cores. Ice cores and sediment cores are used to for paleoclimate reconstructions, which tell geologists about past and present temperature, precipitation, and sea level across the globe. These datasets are our primary source of information on global climate change outside of instrumental data.\n\nGeologists and geophysicists study natural hazards in order to enact safe building codes and warning systems that are used to prevent loss of property and life. Examples of important natural hazards that are pertinent to geology (as opposed those that are mainly or only pertinent to meteorology) are:\n\nThe study of the physical material of the Earth dates back at least to ancient Greece when Theophrastus (372–287 BCE) wrote the work \"Peri Lithon\" (\"On Stones\"). During the Roman period, Pliny the Elder wrote in detail of the many minerals and metals then in practical use – even correctly noting the origin of amber.\n\nSome modern scholars, such as Fielding H. Garrison, are of the opinion that the origin of the science of geology can be traced to Persia after the Muslim conquests had come to an end. Abu al-Rayhan al-Biruni (973–1048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea. Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981–1037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science. In China, the polymath Shen Kuo (1031–1095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by erosion of the mountains and by deposition of silt.\n\nNicolas Steno (1638–1686) is credited with the law of superposition, the principle of original horizontality, and the principle of lateral continuity: three defining principles of stratigraphy.\n\nThe word \"geology\" was first used by Ulisse Aldrovandi in 1603, then by Jean-André Deluc in 1778 and introduced as a fixed term by Horace-Bénédict de Saussure in 1779. The word is derived from the Greek γῆ, \"gê\", meaning \"earth\" and λόγος, \"logos\", meaning \"speech\". But according to another source, the word \"geology\" comes from a Norwegian, Mikkel Pedersøn Escholt (1600–1699), who was a priest and scholar. Escholt first used the definition in his book titled, \"Geologia Norvegica\" (1657).\n\nWilliam Smith (1769–1839) drew some of the first geological maps and began the process of ordering rock strata (layers) by examining the fossils contained in them.\n\nJames Hutton is often viewed as the first modern geologist. In 1785 he presented a paper entitled \"Theory of the Earth\" to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795 (Vol. 1, Vol. 2).\n\nFollowers of Hutton were known as \"Plutonists\" because they believed that some rocks were formed by \"vulcanism\", which is the deposition of lava from volcanoes, as opposed to the \"Neptunists\", led by Abraham Werner, who believed that all rocks had settled out of a large ocean whose level gradually dropped over time.\n\nThe first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times. The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled \"Observations on the Geology of the United States explanatory of a Geological Map\", and published in the \"Society's Transactions\", together with the nation's first geological map. This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks.\n\nSir Charles Lyell first published his famous book, \"Principles of Geology\", in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time.\n\nMuch of 19th-century geology revolved around the question of the Earth's exact age. Estimates varied from a few hundred thousand to billions of years. By the early 20th century, radiometric dating allowed the Earth's age to be estimated at two billion years. The awareness of this vast amount of time opened the door to new theories about the processes that shaped the planet.\n\nSome of the most significant advances in 20th-century geology have been the development of the theory of plate tectonics in the 1960s and the refinement of estimates of the planet's age. Plate tectonics theory arose from two separate geological observations: seafloor spreading and continental drift. The theory revolutionized the Earth sciences. Today the Earth is known to be approximately 4.5 billion years old.\n\n"}
{"id": "10180397", "url": "https://en.wikipedia.org/wiki?curid=10180397", "title": "Hapke parameters", "text": "Hapke parameters\n\nThe Hapke parameters are a set of parameters for an empirical model that is commonly used to describe the directional reflectance properties of the airless regolith surfaces of bodies in the solar system. The model has been developed by astronomer Bruce Hapke at the University of Pittsburgh.\n\nThe parameters are:\n\n\nThe Hapke parameters can be used to derive other albedo and scattering properties such as the geometric albedo, the phase integral, and the Bond albedo.\n\n"}
{"id": "52792009", "url": "https://en.wikipedia.org/wiki?curid=52792009", "title": "K Carinae", "text": "K Carinae\n\nThe Bayer designations k Carinae and K Carinae are distinct.\n\n"}
{"id": "2001079", "url": "https://en.wikipedia.org/wiki?curid=2001079", "title": "Lacus Odii", "text": "Lacus Odii\n\nLacus Odii (Latin for \"Lake of Hate\") is a small lunar mare in the Terra Nivium region on the Moon. It is located at 19.0° N, 7.0° E and is 70 km in diameter.\n"}
{"id": "35887065", "url": "https://en.wikipedia.org/wiki?curid=35887065", "title": "Lambda Fornacis", "text": "Lambda Fornacis\n\nThe Bayer designation Lambda Fornacis (λ For / λ Fornacis) is shared by two stars, in the constellation Fornax:\n"}
{"id": "2731431", "url": "https://en.wikipedia.org/wiki?curid=2731431", "title": "List of Alpine peaks by prominence", "text": "List of Alpine peaks by prominence\n\nThis is a list of the mountains of the Alps, ordered by their topographic prominence. For a list by height, see the list of mountains of the Alps. By descending to 1500m of prominence, this list includes all the Ultras of the Alps. Some famous peaks, such as the Matterhorn and Eiger, are not Ultras because they are connected to higher mountains by high cols and therefore do not achieve enough topographic prominence.\n\nWhere the \"prominence parent\" and the \"island parent\" differ, the prominence parent is marked with \"\" and the island parent with \"\" (with Mont Blanc abbreviated to \"MB\"). The column \"Col height\" denotes the lowest elevation to which one must descend from a peak in order to reach peaks with higher elevations; note that the elevation of any peak is the sum of its prominence and col. The column \"Col location\" denotes the pass where the col height is located.\n\n\n"}
{"id": "8215238", "url": "https://en.wikipedia.org/wiki?curid=8215238", "title": "List of Cortinarius species", "text": "List of Cortinarius species\n\nWith around 2000 species, \"Cortinarius\" is the biggest genus of fungi that form mushrooms. Apart from a few species such as \"C. caperatus\", many even so-called edible species appear to have very similar species that are at least inedible if not poisonous, or otherwise may differ in edibility geographically.\n\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21299962", "url": "https://en.wikipedia.org/wiki?curid=21299962", "title": "List of Dungeons &amp; Dragons 4th edition monsters", "text": "List of Dungeons &amp; Dragons 4th edition monsters\n\n\"Dungeons & Dragons\" 4th edition (see editions of \"Dungeons & Dragons\") was released in 2008. The first book containing monsters to be published was the Heroic Tier adventure \"Keep on the Shadowfell\", followed closely with the release of the first set of \"core\" rulebooks.\n\nKeep on the Shadowfell is the first official product from the 4th edition \"Dungeons & Dragons\" line. It is part one of a three part series of adventures. The monsters appear in the 80-page adventure book booklet and are organized as they appear in the adventure encounters.\n\nThis is the first monster book for the Dungeons & Dragons 4th edition, published in 2008. This book features an alphabetical listing of monsters on page 3, an introduction on pages 4–7, the monster descriptions on pages 8–275, instructions for converting some monsters into NPCs and PCs on pages 276–279, a glossary on pages 280–283, and a list of the monsters in the book ranked by level on pages 284–287.\n\nReleased in August 2008, this book details the Forgotten Realms campaign setting. It includes unique creatures, such as the zairtail, as well as a number of servants of the various groups and cults across the Realms. The table below is organized by the order the creatures first appear.\n\nThis book, published in September 2008, details various weapons, armor, vehicles, mounts, and other magical items. All of the monsters found in this book are mounts and/or creatures summoned through the use of magic items. The table below is organized by the order the creatures first appear.\n\nA boxed set that includes a book with a revised and streamlined core listing of monsters for the Dungeons & Dragons 4th edition game, published in 2010 as part of the Essentials line. The book features an introduction on pages 4–11, the monster descriptions on pages 12–295, an appendix of animals on pages 296–304, a glossary on pages 305–312, and a list of the monsters in the book ranked by level on pages 313–319.\n\nA sourcebook for monsters and NPCs from the Nentir Vale setting for the Dungeons & Dragons 4th edition game, published in 2011 as part of the Essentials line. The book features an introduction (including details on the Nentir Vale) on pages 4–13, the monster descriptions on pages 14–121, a glossary on pages 122–125, and a list of the monsters in the book ranked by level on pages 126–127.\n"}
{"id": "38017428", "url": "https://en.wikipedia.org/wiki?curid=38017428", "title": "List of Sargassum species", "text": "List of Sargassum species\n\nThe genus \"Sargassum\" contains over 300 species of brown algae:\n\n"}
{"id": "25496890", "url": "https://en.wikipedia.org/wiki?curid=25496890", "title": "List of Superfund sites in Nebraska", "text": "List of Superfund sites in Nebraska\n\nThis is a list of Superfund sites in Nebraska designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). The NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of March 26, 2010, there were 13 Superfund sites on the National Priorities List in Nebraska. No sites are currently proposed for entry on the list. One site has been cleaned up and removed from the list.\n\n\n"}
{"id": "31099339", "url": "https://en.wikipedia.org/wiki?curid=31099339", "title": "List of national parks of the Gambia", "text": "List of national parks of the Gambia\n\nThe Gambia has the following National parks and nature reserves, managed by the Department of Parks and Wildlife Management:\n"}
{"id": "3430069", "url": "https://en.wikipedia.org/wiki?curid=3430069", "title": "List of retired Atlantic hurricane names", "text": "List of retired Atlantic hurricane names\n\nThis is a cumulative list of previously used tropical cyclone (tropical storm and hurricane) names which have been permanently removed from reuse in the North Atlantic region.\nThe naming of North Atlantic tropical cyclones is currently under the oversight of the Hurricane Committee of the World Meteorological Organization. This group maintains six alphabetic lists of names, with one list used each year. This normally results in each name being reused every six years. However, in the case of a particularly deadly or damaging storm, that storm's name is retired, and a replacement starting with the same letter is selected to take its place. The decision whether to remove a name in a given season is made at the annual session of the Hurricane Committee in the spring of the following year.\n\nThe practice of retiring storm names was begun by the United States Weather Bureau in 1955, after major hurricanes Carol, Edna, and Hazel struck the Northeastern United States during the previous year. Initially their names were retired for 10 years, after which time they could be reintroduced; however, in 1969, the policy was changed to have the names retired permanently. In 1977, the United States National Oceanic and Atmospheric Administration (NOAA) transferred control of the naming lists to the Hurricane Committee.\n\nSince the formal start of naming during the 1947 Atlantic hurricane season, an average of one Atlantic storm name has been retired each year, though many seasons (most recently 2014) did not have any names retired. The deadliest storm to have its name retired was Hurricane Mitch, which caused over 10,000 fatalities when it struck Central America during October 1998. The costliest storms were hurricanes Katrina in August 2005 and Harvey in August 2017; each storm struck the U.S. Gulf Coast, causing $125 billion in damage, much of it from flooding. The most recently retired storm names are Harvey, Irma, Maria, and Nate.\n\nBy 1947, tropical cyclones developing in the North Atlantic Ocean were named by the United States Army Air Forces in private communications between weather centres and aircraft using the Phonetic alphabet. This practice continued until September 1950, when the names started to be used publicly after three hurricanes (Baker, Dog, Easy) had occurred simultaneously and caused confusion within the media and the public. Over the next two years, the public use of the phonetic alphabet to name systems continued before at the 1953 Interdepartmental Hurricane Conference it was decided to start using a new list of female names during that season, as a second phonetic alphabet had been developed. During the active but mild 1953 Atlantic hurricane season, the names were readily used in the press with few objections recorded; as a result, the same names were reused during the next year with only one change: Gilda for Gail. Over the next six years a new list of names was developed ahead of each season, before in 1960 forecasters developed four alphabetical sets and repeated them every four years. These new sets followed the example of the typhoon names and excluded names beginning with the letters Q, U, X, Y, and Z, and keeping them to female names only.\n\nIn 1955, it was decided to start retiring the names of significant tropical cyclones for 10 years after which they might be reintroduced, with the names Carol and Edna reintroduced ahead of the 1965 and 1968 hurricane seasons respectively. At the 1969 Interdepartmental hurricane conference the naming lists were revised after it was decided that the names Carol, Edna and Hazel would be permanently retired because of their importance to the research community. It was also decided that any significant hurricane in the future would also be permanently retired. Ahead of the 1971 Atlantic hurricane season, 10 lists of hurricane names were inaugurated, by the National Oceanic and Atmospheric Administration. In 1977 it was decided that the World Meteorological Organization's Hurricane Committee (WMO) would control the names used, who subsequently decided that six lists of names would be used in the Atlantic Ocean from 1979 onwards with male names included. Since 1979 the same six lists have been used by the United States National Hurricane Center to name systems, with names of significant tropical cyclones retired from the lists permanently and replaced with new names as required at the following year's hurricane committee meeting.\n\nAt present, the name of any tropical cyclone may be retired or withdrawn from the list of names at the request of a member state, if it acquires notoriety for various reasons including the number of deaths, amount of damages or other impacts. The committee subsequently discuss the proposal and either through building consensus or a majority vote decides if the name should be retired or withdrawn. In March 2017, members of the British Caribbean Territories proposed that a third retirement criterion be added: the tropical cyclone must have sustained winds of at least . This came in light of the retirement of Tropical Storm Erika in 2015 which caused catastrophic flooding and mudslides in Dominica without producing sustained tropical storm-force winds on the island. No action has been taken on this proposal yet.\n\nBetween 1954 and 1959, eight names were deemed significant enough to be retired for 10 years due to their impact, before being permanently retired after 1969. There were no names retired for the 1956, 1958, and 1959 seasons. Collectively, these storms resulted in at least 2317 fatalities and over in damage. The deadliest hurricane was Hurricane Janet, which killed at least 1,023 people, while the costliest was Hurricane Diane, which caused in damage.\n\nIn 1960, four rotating lists of names were developed to avoid having to create new lists each year, while the practice of retiring any particularly damaging storm names for 10 years continued, with 11 names deemed significant enough to be retired during the decade. At the 1969 Hurricane Warning Conference, the National Hurricane Center requested that Carol, Edna, Hazel, and Inez be permanently retired due to their importance to the research community. This request was subsequently accepted and led to today's practice of retiring names of significant tropical cyclones permanently. There were no names retired for the 1962 and 1968 seasons. Collectively, the 11 systems were responsible for at least 9082 fatalities and in excess of in damage.\n\nStarting in 1979, the World Meteorological Organization began assigning both male and female names to tropical cyclones. This decade featured hurricanes David and Frederic, the first male Atlantic hurricane names to be retired. During this decade, 9 storms were deemed significant enough to have their names retired. Together these 9 systems caused at least in damage, while at least 10527 people lost their lives. There were no names retired for the 1971, 1973, and 1976 seasons.\n\nAfter control of the naming scheme was turned over to the World Meteorological Organization's Hurricane Committee during the mid-1970s, the 1980s marked the least prolific decade in terms of the number of retired storms with 7 names warranting removal. Between them the 7 systems caused over in damage while over 891 people lost their lives. Hurricane Gilbert was the most intense tropical cyclone during the decade by pressure, with a minimum value of 888 hPa (26.22 inHg). This was the lowest recorded pressure in a North Atlantic hurricane until Hurricane Wilma surpassed it during 2005. In addition, Hurricane Allen was the most intense tropical cyclone during the decade by wind speed, with maximum 1–minute sustained winds of 190 mph (305 km/h). This remains the highest sustained wind speed of any Atlantic hurricane on record. There were no names retired for the 1981, 1982, 1984, 1986, and 1987 seasons, which was the most of any decade since the introduction of the practice of retiring hurricane names.\n\nDuring the 1990s, the Atlantic Ocean moved into its active era, which led to more tropical cyclones forming during the hurricane seasons. The decade featured Hurricane Andrew which at the time was the costliest hurricane on record, and also Hurricane Mitch which is considered to be the deadliest tropical cyclone to have its name retired killing over 11,000 people in Central America. A total of 15 names were retired in this decade, with seven of those during the 1995 and 1996 seasons. Cumulatively, the 15 systems caused over in damage while over 12145 people lost their lives. There were no names retired for the 1993, 1994 and 1997 seasons.\n\nAfter the Atlantic basin had moved into the warm phase of the Atlantic multidecadal oscillation during the mid-1990s, the 2000s marked the most prolific decade in terms of the number of retired storms, with 24 names warranting removal. The decade featured one of the costliest tropical cyclones on record, Hurricane Katrina, which inflicted roughly in damage across the Gulf Coast of the United States. Katrina was also the deadliest hurricane to strike the United States since the 1928 Okeechobee hurricane. After stalling over and flooding southeastern Texas, and causing approximately in damage, Tropical Storm Allison became the first tropical storm in this basin to have its name retired, while subtropical storms started to be named during 2002. Hurricane Jeanne was the deadliest storm during the decade and was responsible for over 3000 deaths, when it impacted Haiti and other parts of the Caribbean as a tropical storm and minimal hurricane. During October 2005, Hurricane Wilma became the most intense tropical cyclone in the Atlantic basin on record, with a central pressure of . There were no names retired for the 2006 and 2009 seasons. Collectively, the 24 systems were responsible for nearly 7,900 fatalities and in excess of in damage.\n\nSo far during the current decade, 13 tropical cyclone names have been retired. Collectively, these systems killed at least 4413 people and caused at least worth of damage. So far, of those storms whose names have been retired during the decade, Hurricane Maria is the most intense tropical cyclone by pressure, with a minimum value of 908 hPa (26.81 inHg), as well as the deadliest, with 3,057 fatalities directly or indirectly caused by Maria. Hurricane Irma is the most intense in terms of wind speed, with maximum sustained winds of 180 mph (290 km/h). Hurricane Harvey is currently the costliest Atlantic hurricane to have its name retired during the 2010s, as well as the costliest to have its name retired overall, tied with Katrina in 2005. There were no names retired for the 2014 season.\n\n\n"}
{"id": "57037447", "url": "https://en.wikipedia.org/wiki?curid=57037447", "title": "List of tornadoes by province (Canada)", "text": "List of tornadoes by province (Canada)\n\nThis page lists all the confirmed and unconfirmed tornadoes which have touched down in every Canadian province/territory.\n\nOn average, around 80 confirmed and unconfirmed tornadoes touch down in Canada each year, with most occurring in Southern Ontario, the southern Canadian Prairies and southern Quebec. Canada ranks as the second country in the world with the most tornadoes per year, after the US. The most common types are EF0 to EF2 (F0 to F2) in damage intensity level and usually result in minor structural damage to barns, wood fences, roof shingles, chimneys, uprooted or snapped tree limbs and downed power lines. Fewer than 5% of tornadoes in Canada are rated EF3 (F3) or higher in intensity, where wind speeds are in excess of 225 km/h (140 mph).\n\nOntario, Alberta, Manitoba and Saskatchewan all average between 8 and 14 tornadoes per season, followed by Quebec with numbers normally between 4–10. New Brunswick is also a recognized tornado zone. All other provinces and territories have significantly less threat from tornadoes. The peak season in Canada is in the summer months when clashing air masses move north, as opposed to the spring season in the United States southern-central plains, although tornadoes in Canada have occurred in spring, fall and very rarely winter.\n\nDue to increasing detection (i.e. Doppler weather radar, social media and satellite imagery), particularly in the US and southern Canada, numbers of counted tornadoes have increased markedly in recent decades although number of actual tornadoes and counted significant tornadoes has not. In older events, the number of tornadoes officially counted is likely underestimated. The upswing could also be attributed to other factors, such as improved aerial and ground damage assessment after the fact in sparsely populated areas (particularly the case in remote parts of the Canadian Prairies and Northern Ontario, for example), better trained spotter capabilities and increased use of digital recording devices by citizens.\n\nFor a variety of reasons, such as Canada's lower population density and generally stronger housing construction due to the colder climate, Canadian tornadoes have historically caused far fewer fatalities than tornadoes in the United States.\n\nTornadoes in Canada are enough of a threat for a public warning system to be in place, overseen by the national weather agency, Environment Canada (EC). With connections between Environment Canada and the Government of Canada, AlertReady is used for the public alerting method for severe weather such as tornadoes or wildfires.\n\n\n\n"}
{"id": "85557", "url": "https://en.wikipedia.org/wiki?curid=85557", "title": "Marduk", "text": "Marduk\n\nMarduk (cuneiform: AMAR.UTU; Sumerian: \"amar utu.k\" \"calf of the sun; solar calf\"; Greek , \"Mardochaios\"; ) was a late-generation god from ancient Mesopotamia and patron deity of the city of Babylon. When Babylon became the political center of the Euphrates valley in the time of Hammurabi (18th century BC), he slowly started to rise to the position of the head of the Babylonian pantheon, a position he fully acquired by the second half of the second millennium BC. In the city of Babylon, Marduk was worshiped in the temple Esagila. Marduk is associated with the divine weapon Imhullu. \"Marduk\" is the Babylonian form of his name.\n\nThe name \"Marduk\" was probably pronounced \"Marutuk\". The etymology of the name \"Marduk\" is conjectured as derived from \"amar-Utu\" (\"immortal son of Utu\") or (\"bull calf of the sun god Utu\"). The origin of Marduk's name may reflect an earlier genealogy, or have had cultural ties to the ancient city of Sippar (whose god was Utu, the sun god), dating back to the third millennium BC.\n\nBy the Hammurabi period, Marduk had become astrologically associated with the planet Jupiter.\n\nMarduk's original character is obscure but he was later associated with water, vegetation, judgment, and magic. His consort was the goddess Sarpanit. He was also regarded as the son of Ea (Sumerian Enki) and Damkina and the heir of Anu, but whatever special traits Marduk may have had were overshadowed by the political development through which the Euphrates valley passed and which led to people of the time imbuing him with traits belonging to gods who in an earlier period were recognized as the heads of the pantheon. There are particularly two gods—Ea and Enlil—whose powers and attributes pass over to Marduk.\n\nIn the case of Ea, the transfer proceeded pacifically and without effacing the older god. Marduk took over the identity of Asarluhi, the son of Ea and god of magic, so that Marduk was integrated in the pantheon of Eridu where both Ea and Asarluhi originally came from. Ea, Marduk's father, voluntarily recognized the superiority of the son and hands over to him the control of humanity. This association of Marduk and Ea, while indicating primarily the passing of the supremacy once enjoyed by Eridu to Babylon as a religious and political centre, may also reflect an early dependence of Babylon upon Eridu, not necessarily of a political character but, in view of the spread of culture in the Euphrates valley from the south to the north, the recognition of Eridu as the older centre on the part of the younger one.\nWhile the relationship between Ea and Marduk is marked by harmony and an amicable abdication on the part of the father in favour of his son, Marduk's absorption of the power and prerogatives of Enlil of Nippur was at the expense of the latter's prestige. Babylon became independent in the early 19th century BC, and was initially a small city state, overshadowed by older and more powerful Mesopotamian states such as Isin, Larsa and Assyria. However, after Hammurabi forged an empire in the 18th century BC, turning Babylon into the dominant state in the south, the cult of Marduk eclipsed that of Enlil; although Nippur and the cult of Enlil enjoyed a period of renaissance during the over four centuries of Kassite control in Babylonia (c. 1595 BC–1157 BC), the definite and permanent triumph of Marduk over Enlil became felt within Babylonia.\n\nThe only serious rival to Marduk after c. 1750 BC was the god Aššur (Ashur) (who had been the supreme deity in the northern Mesopotamian state of Assyria since the 25th century BC) which was the dominant power in the region between the 20th and 18th centuries BC, and between the 14th to the late 7th century BC. In the south, Marduk reigned supreme. He is normally referred to as \"Bel\" \"Lord\", also \"bel rabim\" \"great lord\", \"bêl bêlim\" \"lord of lords\", \"ab-kal ilâni bêl terêti\" \"leader of the gods\", \"aklu bêl terieti\" \"the wise, lord of oracles\", \"muballit mîte\" \"reviver of the dead\", etc.\n\nWhen Babylon became the principal city of southern Mesopotamia during the reign of Hammurabi in the 18th century BC, the patron deity of Babylon was elevated to the level of supreme god. In order to explain how Marduk seized power, \"Enûma Elish\" was written, which tells the story of Marduk's birth, heroic deeds and becoming the ruler of the gods. This can be viewed as a form of Mesopotamian apologetics. Also included in this document are the fifty names of Marduk.\n\nIn \"Enûma Elish\", a civil war between the gods was growing to a climactic battle. The Anunnaki gods gathered together to find one god who could defeat the gods rising against them. Marduk, a very young god, answered the call and was promised the position of head god.\n\nTo prepare for battle, he makes a bow, fletches arrows, grabs a mace, throws lightning before him, fills his body with flame, makes a net to encircle Tiamat within it, gathers the four winds so that no part of her could escape, creates seven nasty new winds such as the whirlwind and tornado, and raises up his mightiest weapon, the rain-flood. Then he sets out for battle, mounting his storm-chariot drawn by four horses with poison in their mouths. In his lips he holds a spell and in one hand he grasps a herb to counter poison.\n\nFirst, he challenges the leader of the Anunnaki gods, the dragon of the primordial sea Tiamat, to single combat and defeats her by trapping her with his net, blowing her up with his winds, and piercing her belly with an arrow.\n\nThen, he proceeds to defeat Kingu, who Tiamat put in charge of the army and wore the Tablets of Destiny on his breast, and \"wrested from him the Tablets of Destiny, wrongfully his\" and assumed his new position. Under his reign humans were created to bear the burdens of life so the gods could be at leisure.\n\nMarduk was depicted as a human, often with his symbol the snake-dragon which he had taken over from the god Tishpak. Another symbol that stood for Marduk was the spade.\n\nBabylonian texts talk of the creation of Eridu by the god Marduk as the first city, \"the holy city, the dwelling of their [the other gods'] delight\". However, Eridu was founded in the 5th millennium BC and Marduk's ascendancy only occurred in the second millennium BC, so this is clearly a revisionist back-dating to inflate the prestige of Marduk.\n\nLeonard W. King in \"The Seven Tablets of Creation\" (1902) included fragments of god lists which he considered essential for the reconstruction of the meaning of Marduk's name. Franz Bohl in his 1936 study of the fifty names also referred to King's list.\nRichard Litke (1958) noticed a similarity between Marduk's names in the An:Anum list and those of the Enuma elish, albeit in a different arrangement.\nThe connection between the An:Anum list and the list in Enuma Elish were established by Walther Sommerfeld (1982), who used the correspondence to argue for a Kassite period composition date of the Enuma elish, although the direct derivation of the Enuma elish list from the An:Anum one was disputed in a review by Wilfred Lambert (1984).\n\nThe Marduk Prophecy is a \"vaticinium ex eventu\" text describing the travels of the Marduk cult statue from Babylon. It relates his visit to the land of Ḫatti, corresponding to the statue's seizure during the sack of the city by Mursilis I in 1531 BC, Assyria, and when Tukulti-Ninurta I overthrew Kashtiliash IV, taking the image to Assur and Elam in 1225 BC. Kudur-nahhunte then ransacked the city and pilfered the statue around 1160 BC. He addresses an assembly of the gods.\nThe first two sojourns are described in glowing terms as good for both Babylon and the other places Marduk has graciously agreed to visit. The episode in Elam, however, is a disaster, where the gods have followed Marduk and abandoned Babylon to famine and pestilence. Marduk prophesies that he will return once more to Babylon to a messianic new king, who will bring salvation to the city and who will wreak a terrible revenge on the Elamites. This king is understood to be Nabu-kudurri-uṣur I, 1125-1103 BC. Thereafter the text lists various sacrifices.\nA copy was found in the House of the Exorcist at Assur, whose contents date from 713-612 BC and is closely related thematically to another \"vaticinium ex eventu\" text called the Shulgi prophecy, which probably followed it in a sequence of tablets. Both compositions present a favorable view of Assyria.\n\nDuring the first millennium BC, the Babylonians worshipped a deity under the title \"Bel\", meaning \"lord\", who was a syncretization of Marduk, Enlil, and the dying god Dumuzid. Bel held all the cultic titles of Enlil and his status in the Babylonian religion was largely the same. Eventually, Bel came to be seen as the god of order and destiny. The cult of Bel is a major component of the Jewish story of \"Bel and the Dragon\" from the apocryphal additions to Daniel. In the account, the Babylonians offer \"twelve bushels of fine flour, twenty sheep, and fifty gallons of wine\" every day to an idol of Bel and the food miraculously disappears overnight. The Persian king Cyrus the Great tells the Jewish wise man Daniel that the idol is clearly alive, because it eats the food that is offered to it, but Daniel objects that it \"is only clay on the inside, and bronze on the outside, and has never tasted a thing.\" Daniel proves this by secretly covering the floor of the temple with ash. Daniel and Cyrus leave the temple and, when they return, Daniel shows the king the human footprints that have been left on the floor, proving that the food is really being eaten by the seventy priests of Bel. Bel is also mentioned in the writings of several Greek historians.\n\n\n\n"}
{"id": "39093199", "url": "https://en.wikipedia.org/wiki?curid=39093199", "title": "Mathematical Q models", "text": "Mathematical Q models\n\nMathematical Q models provide a model of the earth's response to seismic waves. In reflection seismology, the anelastic attenuation factor, often expressed as seismic quality factor or Q, which is inversely proportional to attenuation factor, quantifies the effects of anelastic attenuation on the seismic wavelet caused by fluid movement and grain boundary friction. When a plane wave propagates through a homogeneous viscoelastic medium, the effects of amplitude attenuation and velocity dispersion may be combined conveniently into the single dimensionless parameter, Q. As a seismic wave propagates through a medium, the elastic energy associated with the wave is gradually absorbed by the medium, eventually ending up as heat energy. This is known as absorption (or anelastic attenuation) and will eventually cause the total disappearance of the seismic wave.\n\nThe frequency-dependent attenuation of seismic waves leads to decreased resolution of seismic images with depth. Transmission losses may also occur due to friction or fluid movement, and for a given physical mechanism, they can be conveniently described with an empirical formulation where elastic moduli and propagation velocity are complex functions of frequency. Bjørn Ursin and Tommy Toverud published an article where they compared different Q models.\n\nIn order to compare the different models they considered plane-wave propagation in a homogeneous viscoelastic medium. They used the Kolsky–Futterman model as a reference and studied several other models. These other models were compared with the behavior of the Kolsky–Futterman model.\n\nThe Kolsky–Futterman model was first described in the article ‘Dispersive body waves’ by Futterman (1962).\n\n'Seismic inverse Q-filtering' by Yanghua Wang (2008) contains an outline discussing the theory of Futterman, beginning with the wave equation:\n\nwhere U(r,w) is the plane wave of radial frequency w at travel distance r, k is the wavenumber and i is the imaginary unit. Reflection seismograms record the reflection wave along the propagation path r from the source to reflector and back to the surface.\n\nEquation (1.1) has an analytical solution given by:\n\nwhere k is the wave number. When the wave propagates in inhomogeneous seismic media the propagation constant k must be a complex value that includes not only an imaginary part, the frequency-dependent attenuation coefficient, but also a real part, the dispersive wave number. We can call this K(w) a propagation constant in line with Futterman.\n\nk(w) can be linked to the phase velocity of the wave with the formula:\n\nTo obtain a solution that can be applied to seismic \"k\"(\"w\") must be connected to a function that represents the way in which \"U\"(\"r\",\"w\") propagates in the seismic media. This function can be regarded as a Q-model.\n\nIn his outline Wang calls the Kolsky–Futterman model the Kolsky model. The model assumes the attenuation \"α\"(\"w\") to be strictly linear with frequency over the range of measurement:\n\nAnd defines the phase velocity as:\n\nwhere \"c\" and \"Q\" are the phase velocity and the \"Q\" value at a reference frequency \"w\".\n\nFor a large value of \"Qr\" » 1 the solution (1.6) can be approximated to\n\nwhere\n\nKolsky’s model was derived from and fit well with experimental observations. The theory for materials satisfying the linear attenuation assumption requires that the reference frequency w is a finite (arbitrarily small but nonzero) cut-off on the absorption. According to Kolsky, we are free to choose w following the phenomenological criterion that it be small compared with the lowest measured frequency w in the frequency band. More information regarding this concept can be found in Futterman (1962)\n\nFor each of the Q models Ursin B. and Toverud T. presented in their article they computed the attenuation (1.5) and phase velocity (1.6) in the frequency band 0–300 Hz. Fig.1. presents the graph for the Kolsky model – attenuation (left) and phase velocity (right) with \"c\" = 2000 m/s, \"Q\" = 100 and \"w\" = 2100 Hz.\n\nWang listed the different Q models that Ursin B. and Toverud T. applied in their study, classifying the models into two groups. The first group consists of models 1-5 below, the other group including models 6-8. The main difference between these two groups is the behaviour of the phase velocity when the frequency approaches zero. Whereas the first group has a zero-valued phase velocity, the second group has a finite, nonzero phase velocity.\n\n1) the Kolsky model (linear attenuation)\n\n2) the Strick–Azimi model (power-law attenuation)\n\n3) the Kjartansson model (constant Q)\n\n4) Azimi's second and third models (non-linear attenuation)\n\n5) Müller's model (power-law Q)\n\n6) Standard linear solid Q model for attenuation and dispersion the Zener model (the standard linear solid)\n\n7) the Cole–Cole model (a general linear-solid)\n\n8) a new general linear model\n\n\n"}
{"id": "2461754", "url": "https://en.wikipedia.org/wiki?curid=2461754", "title": "Mount Taylor (New Mexico)", "text": "Mount Taylor (New Mexico)\n\nMount Taylor (Navajo: Tsoodził) is a stratovolcano in northwest New Mexico, northeast of the town of Grants. It is the high point of the San Mateo Mountains and the highest point in the Cibola National Forest. It was named in 1849 for then president Zachary Taylor. Prior to that, it was called \"Cebolleta\" (tender onion) by the Spanish; the name persists as one name for the northern portion of the San Mateo Mountains, a large mesa. Mount Taylor is largely forested, rising like a blue cone above the desert below. Its slopes were an important source of lumber for neighboring pueblos.\n\nMount Taylor is the cone in a larger volcanic field, including Mesa Chivato. The Mount Taylor volcanic field is composed primarily of basalt (with 80% by volume) and straddles the extensional transition zone between the Colorado Plateau and the Rio Grande rift. The largest volcanic plug in the volcanic field is Cabezon Peak, which rises nearly 2,000 feet above the surrounding plain. According to Robert Julyan's \"The Place Names of New Mexico\", the Navajos identify Cabezon Peak \"as the head of a giant killed by the Twin War Gods\" with the lava flow to the south of Grants believed to be the congealed blood of the giant.\n\nTo the Navajo people, Mount Taylor is \"\", the blue bead mountain, sometimes translated Torquoise Mountain, one of the four sacred mountains marking the cardinal directions and the boundaries of the Dinetah, the traditional Navajo homeland. Mount Taylor marks the southern boundary, and is associated with the direction south and the color blue; it is gendered female. In Navajo mythology, First Man created the sacred mountains from soil from the Fourth World, together with sacred matter, as replicas of mountains from that world. He fastened Mount Taylor to the earth with a stone knife. The supernatural beings Black God, Turquoise Boy, and Turquoise Girl are said to reside on the mountain.\nMount Taylor is also sacred to the Acoma, Hopi, Laguna and Zuni people.\n\nMount Taylor was active from 3.3 to 1.5 million years ago\nduring the Pliocene, and is surrounded by a field of smaller inactive volcanoes. Repeated eruptions built lava domes and produced lava flows, ash plumes, and mudflows. The mountain is surrounded by a great volume of volcanic debris, suggesting multiple major eruptions, possibly similar to that of Mount Saint Helens and the San Francisco Peaks near Flagstaff, Arizona.\nEstimates vary about how high the mountain was at its highest. Conservative estimates place its maximum near a similar pre-explosion height for the San Francisco Peaks of , and an extreme estimate places it near .\n\nMount Taylor is the site of the Mount Taylor Quadrathlon, an endurance event which has been held at this location for almost thirty years. The event includes running, skiing, bicycling and snowshoeing for 43 miles.\n\nMount Taylor and the surrounding area is home to large elk herds, mule deer, black bear, mountain lion and wild turkey. Bird species are particularly diverse in the area and include great blue heron, white-faced ibis, canvasback, common merganser, rough-legged hawk, red-tailed hawk, ferruginous hawk, sharp-shinned hawk, osprey, golden eagle, barn owl, great horned owl, and kestrel, whip-poor-will, white-throated swift, western kingbird, warbling vireo, western meadowlark, house finch, swifts, swallows, prairie falcon, gray-headed junco, Steller's jay, and pinyon jay. Furthermore, the area offers excellent raptor-nesting habitat on the various cliffs that spill down into the Rio Puerco valley below.\nMount Taylor is very rich in a uranium-vanadium bearing mineral, and was mined extensively for it from 1979 to 1990. The Mount Taylor and the hundreds of other uranium mines on Pueblo lands have provided over thirteen million tons of uranium ore to the United States since 1945.\n\nConcern has arisen regarding the impact of future mining activities on the site. In June 2008 the New Mexico Cultural Properties Review Committee voted in favor of a one-year emergency listing of more than surrounding the mountain's summit on the state Register of Cultural Properties. \"The Navajo Nation, the Acoma, Laguna and Zuni pueblos, and the Hopi tribe of Arizona asked the state to approve the listing for a mountain they consider sacred to protect it from an anticipated uranium mining boom, according to the nomination report.\" In April 2009, Mount Taylor was added to the National Trust for Historic Preservation's list of America's Most Endangered Places.\n\nOn 3 September 1929, the Transcontinental Air Transport Ford 5-AT-B Tri-Motor \"City of San Francisco\" struck Mount Taylor during a thunderstorm while on a scheduled passenger flight from Albuquerque Airport in Albuquerque, New Mexico, to Los Angeles, California, killing all eight people on board.\n\n"}
{"id": "20548535", "url": "https://en.wikipedia.org/wiki?curid=20548535", "title": "Nameplate capacity", "text": "Nameplate capacity\n\nNameplate capacity, also known as the rated capacity, nominal capacity, installed capacity, or maximum effect, is the intended full-load sustained output of a facility such as a power plant, electric generator, a chemical plant, fuel plant, metal refinery, mine, and many others. Nameplate capacity is the number registered with authorities for classifying the power output of a power station usually expressed in megawatts (MW).\n\nPower plants with an output consistently near their nameplate capacity have a high capacity factor.\n\nFor dispatchable power, this capacity depends on the internal technical capability of the plant to maintain output for a reasonable amount of time (for example, a day), neither momentarily nor permanently, and without considering external events such as lack of fuel or internal events such as maintenance. Actual output can be different from nameplate capacity for a number of reasons depending on equipment and circumstances. \n\nFor non-dispatchable power, particularly renewable energy, nameplate capacity refers to generation under ideal conditions. Output is generally limited by weather conditions, hydroelectric dam water levels, tidal variations and other outside forces. Equipment failures and maintenance usually contribute less to capacity factor reduction than the innate variation of the power source.\nIn photovoltaics, capacity is rated under \"Standard Test Conditions\" usually expressed as watt-peak (W). In addition, a PV system's nameplate capacity is sometimes denoted by a subindex, for example, MW or MW, to identify the raw DC power or converted AC power output. \n\nThe term is connected with nameplates on electrical generators as these plates describing the model name and manufacturer usually also contain the rated output, but the rated output of a power station to the electrical grid is invariably less than the generator nameplate capacity, because the components connecting the actual generator to the \"grid\" also use power. Thus there is a distinction between component capacity and facility capacity.\n\n"}
{"id": "36970013", "url": "https://en.wikipedia.org/wiki?curid=36970013", "title": "National Air Pollution Symposium", "text": "National Air Pollution Symposium\n\nThe National Air Pollution Symposium was held on November 10–11, 1949 in Pasadena, California by the Stanford Research Institute (now SRI International), along with assistance from the California Institute of Technology, the University of Southern California and the University of California.\n\nSRI had performed much of the early research on air pollution and the formation of ozone in the lower atmosphere. About 400 scientists, businessmen, and politicians attended the event, which was the first event of its kind. The conference (and subsequent conferences) spurred some of the early pollution and climate change research, as well as early climate change legislation including the Air Pollution Control Act of 1955.\n"}
{"id": "8507619", "url": "https://en.wikipedia.org/wiki?curid=8507619", "title": "Needle ice", "text": "Needle ice\n\nNeedle ice is a phenomenon that occurs when the temperature of the soil is above and the surface temperature of the air is below . The subterranean liquid water is brought to the surface via capillary action, where it freezes and contributes to a growing needle-like ice column.\n\nNeedle ice requires a flowing form of water underneath the surface, from that point it comes into contact with air that is below freezing. This area of the process usually occurs at night when temperature peaks its low point. From then on, it produces the needle like structure known as \"Needle Ice\".\n\nThe ice needles are typically a few centimetres long. While growing, they may lift or push away small soil particles. On sloped surfaces, needle ice may be a factor contributing to soil creep.\n\nAlternate names for needle ice are \"frost pillars\" (\"Säuleneis\" in German), \"frost column\", \"Kammeis\" (a German term meaning \"comb ice\"), \"Stängeleis\" (another German term referring to the stem-like structures), \"shimobashira\" (霜柱, a Japanese term meaning frost pillars), or \"pipkrake\" (from Swedish \"pipa\" (tube) and \"krake\" (weak, fine), coined in 1907 by Henrik Hesselman).\n\nThe similar phenomena of frost flowers and hair ice can occur on living or dead plants, especially on wood.\n\nIn order for needle ice to form there needs to be a process of Ice Segregation, which only occurs in a porous medium when supercooled water freezes into existing ice, growing away from the ice/water interface. As water permeates the ice, it becomes segregated into separate pieces of ice in the form of lenses, ribbons, needles, layers or strands of ice.\n\nNeedle ice is commonly found along stream banks or soil terraces. It is also found by gaps around stones and others areas of patterned ground. The variety of soil properties also affects where its found. Places where the soil is much deeper and richer can affect the growth of the ice. Consequently, the deeper the soil, the larger the water content allows it to develop. It can be evidently formed anywhere where underground water is exposed to open (freezing) air.\n\nNeedle ice is most suitable in soils with a high silt and organic matter content. Needle ice consists of groups of narrow ice slivers that are up to several centimeters long. The largest recorded needle ice was at 10 cm in length.\n\nNeedle ice grows up slowly from the moist and water-penetrable soil, and melts gradually in the sun. It can vary in appearance but always shows the consistent growth of ice perpendicular to the land surface. Needle ice looks like a bunch of filamentous crystals, and is in straight or curve shape. It usually forms in the morning when temperature drops below freezing point (0 °C).\n\nThe emergence of needle ice has been recognized as a geomorphic agent of soil disturbance. The growth of needle ice lifts a detached, frozen soil crust riding on top of the layer of ice. When the crust and the ice melt, the soil surface settles back irregularly. This process is what loosens the soil surface, destroying the cohesion of the soil. Once all of it dries, the wind can just remove the soil particles resulting in erosion. This erosion is most commonly seen on banks, which has a major effect on them. Needle ice tends to move rocks in the soil upwards toward the surface. Then, the soil becomes infertile in those areas.\n\nThe Fukushima Daiichi nuclear disaster was an explosion of the Fukushima Nuclear Power Plant on March 11, 2011. The disaster was caused by a tsunami that quickly followed an earthquake resulting in one of Japan’s largest nuclear damages. The land closest to the plant and nearby areas are contaminated indefinitely until Japan can contain and remove radiocaesium from the area. And, one of the major ideas to contain any further chemical leakage was to build an ice wall within the plant’s building to slow down groundwater spreading. But recent research has shown that the ice wall was proven to be ineffective to the surrounding areas. The reason why, was because of the areas’ soil migration, or chemical adsorption. Because the soil was slow absorb contaminated particles downward into the ground, it remains at the uppermost level of soil, vulnerable to erosion. Making the formation of needle ice, one of the main reason of redistribution of contaminated soil, easier to transfer the contaminated soil to surrounding areas. In addition to the ice wall, the land in the area would receive constant water from the ice, adding to the increase chance in precipitation.\n\nThe phenomenon of needle ice was observed in East Tennessee. Through the observation, the air temperature was above freezing where it leads to the fact that a form of ice called Pebble ice grew on a few rocks. In fact, the phenomenon has been studied in a climatologic experience at Vancouver, Canada. In this sense, the experiment involves the analysis of the soil surface temperature, net radiation. Therefore, the researchers inform that the growth of needle ice occur during a single freeze-thaw cycle. In addition, the needle ice may become less abundant as the soil surface dried out. The heat released in the conversion of the water into ice truly plays an important role in the process of the growth of needle ice. through the experiment, the formation of needle ice may result from the soil cooling as the conversion of the water into ice releases a heat, and that there are many changes experienced from the soil moisture which are considered to be the main causes of the growth of needle ice In addition, the phenomenon was also observed in Chiltern Hills which is located in England as the temperature was freezing during the winter. Obviously, the temperature dropped down to 5 degrees .\n\nPhenomenon in West Glamorgan, South Wales\nNeedle ice process in the erosion of the sites of the River, Ilston, West Glamorgan, South Wales was taken into account as obvious experiment. Needle Ice often occurred through the process of mild freezing temperatures with an average growth rate of 1mm. In some cases, Needle ice may be defined as “vertical Filaments of ice measuring 8 to 10 cm in length”. It is obviously formed by the separation of ice near the surface in case that the surface soil has been unfrozen. In overall meaning, it is the external form taken by the segregated ice.\n\nNeedle ice affects the growth of plants. Seedlings are often heaved to this surface by needle ice. When the ground hardens the stems and roots of the seedling, they are gripped by the soil and then the formation of needle ice is what pushes them up and out the ground. When the needle ice melts, the seedlings do not settle correctly back into the ground causing these seedling to die. Even if the seedlings are partially heaved by the needle ice, they can still die due to root desiccation. Heaving accounts for 95% of all seedlings deaths. Not only does needle ice affect the soil around it but it also kills the vegetation.\n\n\n"}
{"id": "30099659", "url": "https://en.wikipedia.org/wiki?curid=30099659", "title": "Preconsolidation pressure", "text": "Preconsolidation pressure\n\nPreconsolidation pressure is the maximum effective vertical overburden stress that a particular soil sample has sustained in the past. This quantity is important in geotechnical engineering, particularly for finding the expected settlement of foundations and embankments. Alternative names for the preconsolidation pressure are preconsolidation stress, pre-compression stress, pre-compaction stress, and preload stress. A soil is called overconsolidated if the current effective stress acting on the soil is less than the historical maximum.\n\nThe preconsolidation pressure can help determine the largest overburden pressure that can be exerted on a soil without irrecoverable volume change. This type of volume change is important for understanding shrinkage behavior, crack and structure formation and resistance to shearing stresses. Previous stresses and other changes in a soil's history are preserved within the soil's structure. If a soil is loaded beyond this point the soil is unable to sustain the increased load and the structure will break down. This breakdown can cause a number of different things depending on the type of soil and its geologic history.\n\nPreconsolidation pressure cannot be measured directly, but can be estimated using a number of different strategies. Samples taken from the field are subjected to a variety of tests, like the constant rate of strain test (CRS) or the incremental loading test (IL). These tests can be costly due to expensive equipment and the long period of time they require. Each sample must be undisturbed and can only undergo one test with satisfactory results. It is important to execute these tests precisely to ensure an accurate resulting plot. There are various methods for determining the preconsolidation pressure from lab data. The data is usually arranged on a semilog plot of the effective stress (frequently represented as σ') versus the void ratio. This graph is commonly called the e log p curve or the consolidation curve.\n\nThe preconsolidation pressure can be estimated in a number of different ways but not measured directly. It is useful to know the range of expected values depending on the type of soil being analyzed. For example, in samples with natural moisture content at the liquid limit (liquidity index of 1), preconsolidation ranges between about 0.1 and 0.8 tsf, depending on soil sensitivity (defined as the ratio of undisturbed peak undrained shear strength to totally remolded undrained shear strength). For natural moisture at the plastic limit (liquidity index equal to zero), preconsolidation ranges from about 12 to 25 tsf.\n\nSee Atterberg limits for information about soil properties like liquidity index and liquid limit.\n\nUsing a consolidation curve:(Casagrande 1936)\n\n\nThe point where the lines in part 4 and part 5 intersect is the preconsolidation pressure. \n\nGregory et al. proposed an analytical method to calculate preconsolidation stress that avoids subjective interpretations of the location of the maximum curvature point (i.e. Minimum radius of curvature). Tomás et al. used this method to calculate the preconsolidation pressure of 139 undisturbed soil samples to generate preconsolidation pressure maps of the Vega Baja of the Segura (Spain).\n\nUsing a consolidation curve, intersect the horizontal portion of the recompression curve and a line tangent to the compression curve. This point is within the range of probable preconsolidation pressures. It can be used in calculations that require less accuracy or if a rough estimate is all that is required.\n\nSee \"Modeling Volume Change and Mechanical Properties with Hydraulic Models,\" from the Soil Science Society of America (link in references) for a more involved mathematical model based on Casagrande's method combining principles from soil mechanics and hydraulics.\n\nVarious different factors can cause a soil to approach its preconsolidation pressure:\n\nPreconsolidation pressure is used in many calculations of soil properties essential for structural analysis and soil mechanics. One of the primary uses is to predict settlement of a structure after loading. This is required for any construction project such as new buildings, bridges, large roads and railroad tracks. All of these require site evaluation before construction. Preparing a site for construction requires an initial compression of the soil to prepare for foundation to be added. It is important to know the preconsolidation pressure because it will help to determine the amount of loading that is appropriate for the site. It will also help to determine whether recompression (after excavation), if the conditions allow, soil can exhibit volumetric expansion, recompression, due to the removal of load conditions need to be considered.\n\n"}
{"id": "34496131", "url": "https://en.wikipedia.org/wiki?curid=34496131", "title": "River Styx (Michigan)", "text": "River Styx (Michigan)\n\nThere are two rivers named the River Styx in the U.S. state of Michigan:\n\n"}
{"id": "31372715", "url": "https://en.wikipedia.org/wiki?curid=31372715", "title": "Safety code (nuclear reactor)", "text": "Safety code (nuclear reactor)\n\nIn the context of nuclear reactors, a safety code is a computer program used to analyze the safety of a reactor, or to simulate possible accident conditions.\n\n\n"}
{"id": "54094280", "url": "https://en.wikipedia.org/wiki?curid=54094280", "title": "Salt tolerance of crops", "text": "Salt tolerance of crops\n\nSalt tolerance of crops is the maximum salt level a crop tolerates without losing its productivity while it is affected negatively at higher levels. The salt level is often taken as the soil salinity or the salinity of the irrigation water.\n\nSalt tolerance is of importance in irrigated lands in (semi)arid regions where the soil salinity problem can be extensive as a result of the salinization occurring here. It concerns hundreds of millions of hectares. A regional distribution of the 3,230,000 km² of saline land worldwide is shown in salt affected area based on the FAO/UNESCO Soil Map of the World.\n\nAdditionally, in areas where sprinkler irrigation is practiced, salty sprinkler water can cause considerable damage by leaf burning, whether the soil is saline or not.\n\nOne of the first studies made on soil salinity and plant response was published in the USDA Agriculture Handbook No. 60 , 1954.\nMore than 20 years later Maas and Hoffman published the results of an extensive study on salt tolerance. In 2001, a Canadian study provided a substantial amount of additional data. A comprehensive survey of tolerances reported world wide was made by the FAO in 2002.\n\nMost studies were made with pot or drum experiments or in lysimeters under controlled conditions. The collection of field data under farmers' conditions was rare, probably due to the greater efforts and higher costs involved, the lack of control of plant growing conditions other than soil salinity, and the larger random variation in crop yields and soil salinity. Yet, with statistical methods, it is possible to detect the tolerance level from field data. Salt Farm Texel, a Dutch based research company has identified various crops that have considerable amount of salt tolerance. \n\nSoil and water salinity can be expressed in various ways. The most common parameter used in soil salinity is the electric conductivity of the extract (ECe) of a saturated soil paste in units of deciSiemens per metre (dS/m) (previously measured in millimhos per centimeter (mmho/cm)).\nBernstein presented the following soil classification based on ECe in dS/m:\n\nA common way to present crop – salinity data is according to the Maas–Hoffman model (see above figure): initially a horizontal line connected to a downward sloping line. The breakpoint is also called threshold or tolerance. For field data with random variation the tolerance level can be found with segmented regression. As the Maas-Hoffman model is fitted to the data by the method of least squares, the data at the tail-end influence the position of the breakpoint.\n\nAnother method was described by Van Genuchten and Gupta. It uses an inverted S-curve as shown in the left-hand figure. This model recognizes that the tail-end may have a flatter slope than the middle part. It does not provide a sharp tolerance level.\n\nUsing the Maas–Hoffman model in situations with a flat trend in the tail-end may lead to a breakpoint with a low ECe value, owing to the employment of the condition to minimize the deviations of the model values from the observed values over the entire domain (i.e. including the tail-end).\n\nUsing the logistic sigmoid function for the same data applied in the van Genuchten-Gupta model, the curvature becomes more pronounced and a better fit is obtained. \n\nA third model is based on the method of partial regression, whereby one finds the longest horizontal stretch (the range of \"no effect\") of the yield-ECe relation while beyond that stretch the yield decline sets in (figure below). With this method the trend at the tail-end plays no role. As a result the tolerance level (breakpoint, threshold) is larger (4.9 dS/m) than according to the Maas-Hoffman model (3.3 dS/m, see the second figure above with the same data). Akso a better fit is achieved. \n\nPresently a considerable amount of research is undertaken to develop agricultural crops with a higher salt tolerance to enhance crop cultivation in salinity stricken regions.\n\nIn Australia the following classification of sprinkler irrigation water salinity was developed:\n\n"}
{"id": "2508976", "url": "https://en.wikipedia.org/wiki?curid=2508976", "title": "Sea Life Sunshine Coast", "text": "Sea Life Sunshine Coast\n\nSea Life Sunshine Coast at Mooloolaba, Sunshine Coast, Queensland, Australia is a marine mammal park, oceanarium and wildlife sanctuary. Sea Life Sunshine Coast is an institutional member of the Zoo and Aquarium Association (ZAA). The attraction is a Sea Life Centre owned by Merlin Entertainments, and is globally referred to as Sea Life Sunshine Coast by the firm. It was formerly known as UnderWater World.\n\nUnderWater World officially opened in 1989.\n\nSeveral scenes in the Jackie Chan 1996 film \"\" were shot in the park, including a scene in the main aquarium. In one of the scenes a tourist family is visible in the reflection on one of the tanks.\n\nDuring the period of Christmas on 2006 UnderWater World had a Giant Squid preserved in a block of ice on display.\n\nIn July 2013, UnderWater World's owners, Merlin Entertainments, announced that they would be spending $6.5 million on the refurbishment of the facilities. As part of the process, the aquarium was rebranded as a Sea Life Centre and relaunched in December 2013.\n\n\nA moving walkway in a shark tunnel under the oceanarium takes visitors past several viewing windows, with fish swimming all around the walkway. The exhibit includes three separate habitats: coral reef, cave and open ocean. Its residents include rays, moray eels, grey nurse sharks, sandbar whalers, whitetip reef sharks, and zebra sharks.\n\nIn November 2011, the shark tank and moving walkway were closed for an upgrade.\n\n\nThere are about seven seals at Sea Life Sunshine Coast, one New Zealand fur seal, two Australian fur seals and four Australian sea lions which make up part of the Seal Island Exhibit.\n\n\nThis exhibit is an interactive journey from land to deep sea. It features the world’s largest marine, land and migratory crab species. Species of crabs in the exhibit include giant Japanese spider crabs, robber crabs (also known as coconut crabs) and Christmas Island red and blue crabs.\n\n\nSea Life Sunshine Coast is also home to a fresh water Crocodile.\n\nSea Life Sunshine Coast presents seal shows and wildlife information talks, and has hands-on marine displays. Shows at the park include:\n\n\nSea Life Sunshine Coast is a rehabilitation centre for turtles, seals, and other marine animals. \n"}
{"id": "488154", "url": "https://en.wikipedia.org/wiki?curid=488154", "title": "Secondary emission", "text": "Secondary emission\n\nSecondary emission in physics is a phenomenon where primary incident particles of sufficient energy, when hitting a surface or passing through some material, induce the emission of secondary particles. The term often refers to the emission of electrons when charged particles like electrons or ions in a vacuum tube strike a metal surface; these are called secondary electrons. In this case, the number of secondary electrons emitted per incident particle is called secondary emission yield. If the secondary particles are ions, the effect is termed secondary ion emission. Secondary electron emission is used in photomultiplier tubes and image intensifier tubes to amplify the small number of photoelectrons produced by photoemission, making the tube more sensitive. It also occurs as an undesirable side effect in electronic vacuum tubes when electrons from the cathode strike the anode, and can cause parasitic oscillation.\n\nCommonly used secondary emissive materials include\n\nIn a photomultiplier tube, one or more electrons are emitted from a photocathode and accelerated towards a polished metal electrode (called a dynode). \nThey hit the electrode surface with sufficient energy to release a number of electrons through secondary emission. These new electrons are then accelerated towards another dynode, and the process is repeated several times, resulting in an overall gain ('electron multiplication') in the order of typically one million and thus generating an electronically detectable current pulse at the last dynodes.\n\nSimilar electron multipliers can be used for detection of fast particles like electrons or \"ions\".\n\nIn the 1930s special amplifying tubes were developed which deliberately \"folded\" the electron beam, by having it strike a dynode to be reflected into the anode. This had the effect of increasing the plate-grid distance for a given tube size, increasing the transconductance of the tube and reducing its noise figure. A typical such \"orbital beam hexode\" was the RCA 1630, introduced in 1939. Because the heavy electron current in such tubes damaged the dynode surface rapidly, their lifetime tended to be very short compared to conventional tubes.\n\nThe first random access computer memory used a type of cathode ray tube called the Williams tube that used secondary emission to store bits on the tube face. Another random access computer memory tube based on secondary emission was the Selectron tube. Both were made obsolete by the invention of magnetic core memory.\n\nSecondary emission can be undesirable such as in the tetrode thermionic valve (tube). In this instance the positively charged screen grid can accelerate the electron stream sufficiently to cause secondary emission at the anode (plate). This can give rise to excessive screen grid current. It is also partly responsible for this type of valve (tube), particularly early types with anodes not treated to reduce secondary emission, exhibiting a 'negative resistance' characteristic, which could cause the tube to become unstable. This side effect could be put to use by using some older valves (e.g., type 77 pentode) as dynatron oscillators. This effect was prevented by adding a third grid to the tetrode, called the suppressor grid, to repel the electrons back toward the plate. This tube was called the pentode.\n\n"}
{"id": "184101", "url": "https://en.wikipedia.org/wiki?curid=184101", "title": "Singing sand", "text": "Singing sand\n\nSinging sand, also called whistling sand or barking sand, is sand that produces sound. The sound emission may be caused by wind passing over dunes or by walking on the sand.\n\nCertain conditions have to come together to create singing sand:\n\nThe most common frequency emitted seems to be close to 450 Hz.\n\nThere are various theories about the singing sand mechanism. It has been proposed that the sound frequency is controlled by the shear rate. Others have suggested that the frequency of vibration is related to the thickness of the dry surface layer of sand. The sound waves bounce back and forth between the surface of the dune and the surface of the moist layer, creating a resonance that increases the sound's volume. The noise may be generated by friction between the grains or by the compression of air between them.\n\nOther sounds that can be emitted by sand have been described as \"roaring\" or \"booming\".\n\nSinging sand dunes, an example of the phenomenon of singing sand, produce a sound described as roaring, booming, squeaking, or the \"Song of Dunes\". This is a natural sound phenomenon of up to 105 decibels, lasting as long as several minutes, that occurs in about 35 desert locations around the world. The sound is similar to a loud low-pitch rumble. It emanates from crescent-shaped dunes, or barchans. The sound emission accompanies a slumping or avalanching movement of sand, usually triggered by wind passing over the dune or by someone walking near the crest.\n\nExamples of singing sand dunes include California's Kelso Dunes and Eureka Dunes; AuTrain Beach in Northern Michigan; sugar sand beaches and Warren Dunes in southwestern Michigan; Sand Mountain in Nevada; the Booming Dunes in the Namib Desert, Africa; Porth Oer (also known as Whistling Sands) near Aberdaron in Wales; Indiana Dunes in Indiana; Barking Sands in Hawaiʻi; Mingsha Shan in Dunhuang, China; Kotogahama Beach in Odashi, Japan; Singing Beach in Manchester-by-the-Sea, Massachusetts; near Mesaieed in Qatar; and Gebel Naqous, near el-Tor, South Sinai, Egypt.\n\nThe phenomena also inspired a song called \"The Singing Sands of Alamosa\" in Bing Crosby's album Drifting and Dreaming(1947).\n\nOn some beaches around the world, dry sand will make a singing, squeaking, whistling, or screaming sound if a person scuffs or shuffles their feet with sufficient force. The phenomenon is not completely understood scientifically, but it has been found that quartz sand will do this if the grains are very well-rounded and highly spherical. It is believed by some that the sand grains must be of similar size, so the sand must be well sorted by the actions of wind and waves, and that the grains should be close to spherical and have dust-, pollution-, and organic-matter-free surfaces. The \"singing\" sound is then believed to be produced by shear, as each layer of sand grains slides over the layer beneath it. The similarity in size, the uniformity, and the cleanness means that grains move up and down in unison over the layer of grains below them. Even small amounts of pollution on the sand grains reduce the friction enough to silence the sand.\n\nOthers believe that the sound is produced by the friction of grain against grain that have been coated with dried salt, in a way that is analogous to the way that the rosin on the bow produces sounds from a violin string. It has also been speculated that thin layers of gas trapped and released between the grains act as \"percussive cushions\" capable of vibration, and so produce the tones heard.\n\nNot all sands sing, whistle or bark alike. The sounds heard have a wide frequency range that can be different for each patch of sand. Fine sands, where individual grains are barely visible to the naked eye, produce only a poor, weak sounding bark. Medium-sized grains can emit a range of sounds, from a faint squeak or a high-pitched sound, to the best and loudest barks when scuffed enthusiastically.\n\nWater also influences the effect. Wet sands are usually silent because the grains stick together instead of sliding past each other, but small amounts of water can actually raise the pitch of the sounds produced. The most common part of the beach on which to hear singing sand is the dry upper beach above the normal high tide line, but singing has been reported on the lower beach near the low tide line as well.\n\nSinging sand has been reported on 33 beaches in the British Isles, including in the north of Wales and on the little island of Eigg in the Scottish Hebrides. It has also been reported at a number of beaches along North America's Atlantic coast. Singing sands can be found at Souris, on the eastern tip of Prince Edward Island, at the Singing Sands beach in Basin Head Provincial Park; on Singing Beach in Manchester-by-the-Sea, Massachusetts, as well as in the fresh waters of Lake Superior and Lake Michigan and in other places.\n\n\n\n\n\n\n"}
{"id": "59413", "url": "https://en.wikipedia.org/wiki?curid=59413", "title": "Soil science", "text": "Soil science\n\nSometimes terms which refer to branches of soil science, such as pedology (formation, chemistry, morphology, and classification of soil) and edaphology (how soils interact with living things, especially plants), are used as if synonymous with soil science. The diversity of names associated with this discipline is related to the various associations concerned. Indeed, engineers, agronomists, chemists, geologists, physical geographers, ecologists, biologists, microbiologists, silviculturists, sanitarians, archaeologists, and specialists in regional planning, all contribute to further knowledge of soils and the advancement of the soil sciences.\n\nSoil scientists have raised concerns about how to preserve soil and arable land in a world with a growing population, possible future water crisis, increasing per capita food consumption, and land degradation.\n\nSoil occupies the pedosphere, one of Earth's spheres that the geosciences use to organize the Earth conceptually. This is the conceptual perspective of pedology and edaphology, the two main branches of soil science. Pedology is the study of soil in its natural setting. Edaphology is the study of soil in relation to soil-dependent uses. Both branches apply a combination of soil physics, soil chemistry, and soil biology. Due to the numerous interactions between the biosphere, atmosphere and hydrosphere that are hosted within the pedosphere, more integrated, less soil-centric concepts are also valuable. Many concepts essential to understanding soil come from individuals not identifiable strictly as soil scientists. This highlights the interdisciplinary nature of soil concepts.\n\nDependence on and curiosity about soil, exploring the diversity and dynamics of this resource continues to yield fresh discoveries and insights. New avenues of soil research are compelled by a need to understand soil in the context of climate change, greenhouse gases, and carbon sequestration. Interest in maintaining the planet's biodiversity and in exploring past cultures has also stimulated renewed interest in achieving a more refined understanding of soil.\n\nMost empirical knowledge of soil in nature comes from soil survey efforts. Soil survey, or soil mapping, is the process of determining the soil types or other properties of the soil cover over a landscape, and mapping them for others to understand and use. It relies heavily on distinguishing the individual influences of the five classic soil forming factors. This effort draws upon geomorphology, physical geography, and analysis of vegetation and land-use patterns. Primary data for the soil survey are acquired by field sampling and supported by remote sensing.\n\nAs of 2006, the World Reference Base for Soil Resources, via its Land & Water Development division, is the pre-eminent soil classification system. It replaces the previous FAO soil classification.\n\nThe WRB borrows from modern soil classification concepts, including USDA soil taxonomy. The classification is based mainly on soil morphology as an expression pedogenesis. A major difference with USDA soil taxonomy is that soil climate is not part of the system, except insofar as climate influences soil profile characteristics.\n\nMany other classification schemes exist, including vernacular systems. The structure in vernacular systems are either nominal, giving unique names to soils or landscapes, or descriptive, naming soils by their characteristics such as red, hot, fat, or sandy. Soils are distinguished by obvious characteristics, such as physical appearance (e.g., color, texture, landscape position), performance (e.g., production capability, flooding), and accompanying vegetation. A vernacular distinction familiar to many is classifying texture as heavy or light. Light soil content and better structure, take less effort to turn and cultivate. Contrary to popular belief, light soils do not weigh less than heavy soils on an air dry basis nor do they have more porosity.\n\nVasily Dokuchaev, a Russian geologist, geographer and early soil scientist, is credited with identifying soil as a resource whose distinctness and complexity deserved to be separated conceptually from geology and crop production and treated as a whole.\n\nPreviously, soil had been considered a product of chemical transformations of rocks, a dead substrate from which plants derive nutritious elements. Soil and bedrock were in fact equated. Dokuchaev considers the soil as a natural body having its own genesis and its own history of development, a body with complex and multiform processes taking place within it. The soil is considered as different from bedrock. The latter becomes soil under the influence of a series of soil-formation factors (climate, vegetation, country, relief and age). According to him, soil should be called the \"daily\" or outward horizons of rocks regardless of the type; they are changed naturally by the common effect of water, air and various kinds of living and dead organisms.\n\nA 1914 encyclopedic definition: \"the different forms of earth on the surface of the rocks, formed by the breaking down or weathering of rocks\". serves to illustrate the historic view of soil which persisted from the 19th century. Dokuchaev's late 19th century soil concept developed in the 20th century to one of soil as earthy material that has been altered by living processes. A corollary concept is that soil without a living component is simply a part of earth's outer layer.\n\nFurther refinement of the soil concept is occurring in view of an appreciation of energy transport and transformation within soil. The term is popularly applied to the material on the surface of the Earth's moon and Mars, a usage acceptable within a portion of the scientific community. Accurate to this modern understanding of soil is Nikiforoff's 1959 definition of soil as the \"excited skin of the sub aerial part of the earth's crust\".\n\nAcademically, soil scientists tend to be drawn to one of five areas of specialization: microbiology, pedology, edaphology, physics or chemistry. Yet the work specifics are very much dictated by the challenges facing our civilization's desire to sustain the land that supports it, and the distinctions between the sub-disciplines of soil science often blur in the process. Soil science professionals commonly stay current in soil chemistry, soil physics, soil microbiology, pedology, and applied soil science in related disciplines\n\nOne interesting effort drawing in soil scientists in the USA is the Soil Quality Initiative. Central to the Soil Quality Initiative is developing indices of soil health and then monitoring them in a way that gives us long term (decade-to-decade) feedback on our performance as stewards of the planet. The effort includes understanding the functions of soil microbiotic crusts and exploring the potential to sequester atmospheric carbon in soil organic matter. The concept of soil quality, however, has not been without its share of controversy and criticism, including critiques by Nobel Laureate Norman Borlaug and World Food Prize Winner Pedro Sanchez.\n\nA more traditional role for soil scientists has been to map soils. Most every area in the United States now has a published soil survey, which includes interpretive tables as to how soil properties support or limit activities and uses. An internationally accepted soil taxonomy allows uniform communication of soil characteristics and functions. National and international soil survey efforts have given the profession unique insights into landscape scale functions. The landscape functions that soil scientists are called upon to address in the field seem to fall roughly into six areas:\n\n\nThere are also practical applications of soil science that might not be apparent from looking at a published soil survey.\n\n\n\n\nDepression storage capacity, in soil science, is the ability of a particular area of land to retain water in its pits and depressions, thus preventing it from flowing. Depression storage capacity, along with infiltration capacity, is one of the main factors involved in Horton overland flow, whereby water volume surpasses both infiltration and depression storage capacity and begins to flow horizontally across land, possibly leading to flooding and soil erosion. The study of land's depression storage capacity is important in the fields of geology, ecology, and especially hydrology.\n\n\n"}
{"id": "9646584", "url": "https://en.wikipedia.org/wiki?curid=9646584", "title": "Stilbe", "text": "Stilbe\n\nStilbe () in Greek mythology was a nymph, daughter of the river god Peneus and the Naiad Creusa. She bore Apollo twin sons, Centaurus, ancestor of the Centaurs, and Lapithus, ancestor of the Lapiths. In another version of the myth, Centaurus was instead the son of Ixion and Nephele. Aineus, father of Cyzicus, was also said to have been a son of Apollo and Stilbe. By Cychreus she became mother of the nymph Chariclo, wife of Chiron. \n\nA different Stilbe was a daughter of Eosphoros and a possible mother of Autolycus by Hermes, and of Callisto by Ceteus.\n\n"}
{"id": "36198576", "url": "https://en.wikipedia.org/wiki?curid=36198576", "title": "Sunbelt Publications", "text": "Sunbelt Publications\n\nSunbelt Publications, incorporated in 1988 with roots in the book business since 1973, publishes and distributes multi-language pictorials, natural science and outdoor guidebooks, regional references, and stories that celebrate the land and its people. The company is located in El Cajon,San Diego County, California, while sometimes being cited as in the city of San Diego.\n\nDiana and Lowell Lindsay created Sunbelt Books based on their passion for the natural world. Diana Lindsay's love for the desert began in 1966, when she “figuratively went into the desert and never came out”. The Anza-Borrego Desert was the subject for her master's thesis from SDSU that was subsequently published as a book entitled “Our Historic Desert” by Copley Books (1973). She is a noted desert scholar and author.\n\nLowell Lindsay is the CEO of Sunbelt Publications. He majored in geology at UCLA and received his master's degree from West Texas A&M University in political science, specializing in environmental education. He is past president of the San Diego Association of Geologists (SDAG) and a board member of the national Association of Earth Science Editor (AESE). Lowell is a former naval aviator and wilderness training officer.\n\n"}
{"id": "416666", "url": "https://en.wikipedia.org/wiki?curid=416666", "title": "Surface energy", "text": "Surface energy\n\nSurface Free energy, or interfacial free energy, quantifies the disruption of intermolecular bonds that occurs when a surface is created. In the physics of solids, surfaces must be intrinsically less energetically favorable than the bulk of a material (the molecules on the surface have more energy compared with the molecules in the bulk of the material), otherwise there would be a driving force for surfaces to be created, removing the bulk of the material (see sublimation). The surface energy may therefore be defined as the excess energy at the surface of a material compared to the bulk, or it is the work required to build an area of a particular surface. Another way to view the surface energy is to relate it to the work required to cut a bulk sample, creating two surfaces.\n\nCutting a solid body into pieces disrupts its bonds, and therefore increases free energy. If the cutting is done reversibly, then conservation of energy means that the energy consumed by the cutting process will be equal to the energy inherent in the two new surfaces created. The unit surface energy of a material would therefore be half of its energy of cohesion, all other things being equal; in practice, this is true only for a surface freshly prepared in vacuum. Surfaces often change their form away from the simple \"cleaved bond\" model just implied above. They are found to be highly dynamic regions, which readily rearrange or react, so that energy is often reduced by such processes as passivation or adsorption.\n\nMeasuring the surface energy of a solid\n\nThe surface energy of a liquid may be measured by stretching a liquid membrane (which increases the surface area and hence the surface energy). In that case, in order to increase the surface area of a mass of liquid by an amount, δA, a quantity of work, γδA, is needed (where γ is the surface energy density of the liquid). However, such a method cannot be used to measure the surface energy of a solid because stretching of a solid membrane induces elastic energy in the bulk in addition to increasing the surface energy.\n\nThe surface energy of a solid is usually measured at high temperatures. At such temperatures the solid creeps and even though the surface area changes, the volume remains approximately constant. If γ is the surface energy density of a cylindrical rod of radius formula_1 and length formula_2 at high temperature and a constant uniaxial tension formula_3, then at equilibrium, the variation of the total Helmholtz free energy vanishes and we have\n\nwhere formula_5 is the Helmholtz free energy and formula_6 is the surface area of the rod:\n\nAlso, since the volume (formula_8) of the rod remains constant, the variation (formula_9) of the volume is zero, i.e.,\n\nTherefore, the surface energy density can be expressed as\nThe surface energy density of the solid can be computed by measuring formula_3, formula_1, and formula_2 at equilibrium.\n\nThis method is valid only if the solid is isotropic, meaning the surface energy is the same for all crystallographic orientations. While this is only strictly true for amorphous solids (glass) and liquids, isotropy is a good approximation for many other materials. In particular, if the sample is polygranular (most metals) or made by powder sintering (most ceramics) this is a good approximation.\n\nIn the case of single-crystal materials, such as natural gemstones, anisotropy in the surface energy leads to faceting. The shape of the crystal (assuming equilibrium growth conditions) is related to the surface energy by the Wulff construction. The surface energy of the facets can thus be found to within a scaling constant by measuring the relative sizes of the facets.\n\nCalculating the surface energy of a deformed solid\n\nIn the deformation of solids, surface energy can be treated as the \"energy required to create one unit of surface area\", and is a function of the difference between the total energies of the system before and after the deformation:\n\nCalculation of surface energy from first principles (for example, density functional theory) is an alternative approach to measurement. Surface energy is estimated from the following variables: width of the d-band, the number of valence d-electrons, and the coordination number of atoms at the surface and in the bulk of the solid.\n\nCalculating the surface formation energy of a crystalline solid\n\nIn density functional theory, surface energy can be calculated from the following expression\n\nwhere formula_17 is the total energy of surface slab obtained using density functional theory.\nformula_18 is the number of atoms in the surface slab. \nformula_19 is the bulk energy per atom. formula_6 is the surface area.\nFor a slab, we have two surfaces and they are of the same type, which is reflected by the number 2 in the denominator.\nTo guarantee this, we need to create the slab carefully to make sure that \nthe upper and lower surfaces are of the same type.\n\nStrength of adhesive contacts is determined by the work of adhesion which is also called \"relative surface energy\" of two contacting bodies. The relative surface energy can be determined by detaching of bodies of well defined shape made of one material from the substrate made from the second material. For example, the relative surface energy of the interface \"acrylic glass - gelatine\" is equal to 0.03 N·m. Experimental set-up for measuring relative surface energy and its function can be seen in the video\n\nEstimating surface energy from the heat of sublimation\n\nTo estimate the surface energy of a pure, uniform material, an individual molecular component of the material can be modeled as a cube. In order to move a cube from the bulk of a material to the surface, energy is required. This energy cost is incorporated into the surface energy of the material, which is quantified by:\n\nwhere formula_22 and formula_23 are coordination numbers corresponding to the surface and the bulk regions of the material, and are equal to 5 and 6, respectively; formula_24 is the surface area of an individual molecule, and formula_25 is the pairwise intermolecular energy.\n\nSurface area can be determined by squaring the cube root of the volume of the molecule:\n\nHere, formula_27 corresponds to the molar mass of the molecule, formula_28 corresponds to the density, and formula_29 is Avogadro’s number.\n\nIn order to determine the pairwise intermolecular energy, all intermolecular forces in the material must be broken. This allows thorough investigation of the interactions that occur for single molecules. During sublimation of a substance, intermolecular forces between molecules are broken, resulting in a change in the material from solid to gas. For this reason, considering the enthalpy of sublimation can be useful in determining the pairwise intermolecular energy. Enthalpy of sublimation can be calculated by the following equation:\n\nUsing empirically tabulated values for enthalpy of sublimation, it is possible to determine the pairwise intermolecular energy. Incorporating this value into the surface energy equation allows for the surface energy to be estimated.\n\nThe following equation can be used as a reasonable estimate for surface energy:\n\nThe presence of an interface influences generally all thermodynamic parameters of a system. There are two models that are commonly used to demonstrate interfacial phenomena, which includes the Gibbs ideal interface model and the Guggenheim model. In order to demonstrate the thermodynamics of an interfacial system using the Gibb’s model, the system can be divided into three parts: two immiscible liquids with volumes formula_32 and formula_33 and an infinitesimally thin boundary layer known as the Gibbs dividing plane (σ) separating these two volumes.\n\nThe total volume of the system is:\n\nAll extensive quantities of the system can be written as a sum of three components: bulk phase a, bulk phase b, and the interface, sigma. Some examples include internal energy (formula_35), the number of molecules of the ith substance (formula_36), and the entropy (formula_37).\n\nWhile these quantities can vary between each component, the sum within the system remains constant. At the interface, these values may deviate from those present within the bulk phases. The concentration of molecules present at the interface can be defined as:\n\nwhere formula_42 and formula_43 represent the concentration of substance formula_44 in bulk phase formula_45 and formula_46, respectively.\nIt is beneficial to define a new term interfacial excess formula_47 which allows us to describe the number of molecules per unit area:\n\nSpreading Parameter:\nSurface energy comes into play in wetting phenomena. To examine this, consider a drop of liquid on a solid substrate. If the surface energy of the substrate changes upon the addition of the drop, the substrate is said to be wetting. The spreading parameter can be used to mathematically determine this:\n\nwhere formula_37 is the spreading parameter, formula_51 the surface energy of the substrate, formula_52 the surface energy of the liquid, and formula_53 the interfacial energy between the substrate and the liquid.\n\nContact angle:\nA way to experimentally determine wetting is to look at the contact angle (formula_56), which is the angle connecting the solid–gas interface and the solid–liquid interface [figure].\n\nThe Young equation relates the contact angle to interfacial energy:\n\nwhere formula_62 is the interfacial energy between the solid and gas phases, formula_53 the interfacial energy between the substrate and the liquid, formula_64 is the interfacial energy between the liquid and gas phases, and formula_56 is the contact angle between the solid–gas and the solid–liquid interface.\n\nWetting of high and low energy substrates:\nThe energy of the bulk component of a solid substrate is determined by the types of interactions that hold the substrate together. High energy substrates are held together by bonds, while low energy substrates are held together by forces. Covalent, ionic, and metallic bonds are much stronger than forces such as van der Waals and hydrogen bonding. High energy substrates are more easily wet than low energy substrates. In addition, more complete wetting will occur if the substrate has a much higher surface energy than the liquid.\n\nThe most commonly used surface modification protocols are plasma activation, wet chemical treatment, including grafting, and thin-film coating. Surface energy mimicking is a technique that enables merging the device manufacturing and surface modifications, including patterning, into a single processing step using a single device material.\nMany techniques can be used to enhance wetting. Surface treatments, such as Corona treatment, plasma treatment and acid etching, can be used to increase the surface energy of the substrate. Additives can also be added to the liquid to decrease its surface energy. This technique is employed often in paint formulations to ensure that they will be evenly spread on a surface.\n\nAs a result of the surface tension inherent to liquids, curved surfaces are formed in order to minimize the area. This phenomenon arises from the energetic cost of forming a surface. As such the Gibbs free energy of the system is minimized when the surface is curved.\nThe Kelvin equation is based on thermodynamic principles and is used to describe changes in vapor pressure caused by liquids with curved surfaces. The cause for this change in vapor pressure is the Laplace pressure. The vapor pressure of a drop is higher than that of a planar surface because the increased Laplace pressure causes the molecules to evaporate more easily. Conversely, in liquids surrounding a bubble, the pressure with respect to the inner part of the bubble is reduced, thus making it more difficult for molecules to evaporate. The Kelvin equation can be stated as:\n\nwhere formula_67 is the vapor pressure of the curved surface, formula_68 is the vapor pressure of the flat surface, formula_69 is the surface tension, formula_70 is the molar volume of the liquid, formula_71 is the universal gas constant, formula_72 is temperature (K), and formula_73 and formula_74 are the principal radii of curvature of the surface.\n\nPigments offer great potential in modifying the application properties of a coating. Due to their fine particle size and inherently high surface energy, they often require a surface treatment in order to enhance their ease of dispersion in a liquid medium. A wide variety of surface treatments have been previously used, including the adsorption on the surface of a molecule in the presence of polar groups, monolayers of polymers, and layers of inorganic oxides on the surface of organic pigments.\n\nNew surfaces are constantly being created as larger pigment particles get broken down into smaller subparticles. These newly formed surfaces consequently contribute to larger surface energies, whereby the resulting particles often become cemented together into aggregates. Because particles dispersed in liquid media are in constant thermal or Brownian motion, they exhibit a strong affinity for other pigment particles nearby as they move through the medium and collide. This natural attraction is largely attributed to the powerful short-range Van der Waals forces, as an effect of their surface energies.\n\nThe chief purpose of pigment dispersion is to break down aggregates and form stable dispersions of optimally sized pigment particles. This process generally involves three distinct stages: wetting, deaggregation, and stabilization. A surface that is easy to wet is desirable when formulating a coating that requires good adhesion and appearance. This also minimizes the risks of surface tension related defects, such as crawling, catering, and orange peel. This is an essential requirement for pigment dispersions; for wetting to be effective, the surface tension of the vehicle must be lower than the surface free energy of the pigment. This allows the vehicle to penetrate into the interstices of the pigment aggregates, thus ensuring complete wetting. Finally, the particles are subjected to a repulsive force in order to keep them separated from one another and lowers the likelihood of flocculation.\n\nDispersions may become stable through two different phenomena: charge repulsion and steric or entropic repulsion. In charge repulsion, particles that possess the same like electrostatic charges repel each other. Alternatively, steric or entropic repulsion is a phenomenon used to describe the repelling effect when adsorbed layers of material (e.g. polymer molecules swollen with solvent) are present on the surface of the pigment particles in dispersion. Only certain portions (i.e. anchors) of the polymer molecules are adsorbed, with their corresponding loops and tails extending out into the solution. As the particles approach each other their adsorbed layers become crowded; this provides an effective steric barrier that prevents flocculation. This crowding effect is accompanied by a decrease in entropy, whereby the number of conformations possible for the polymer molecules is reduced in the adsorbed layer. As a result, energy is increased and often gives rise to repulsive forces that aid in keeping the particles separated from each other.\n\n\n"}
{"id": "34080404", "url": "https://en.wikipedia.org/wiki?curid=34080404", "title": "Sustainable Energy for All", "text": "Sustainable Energy for All\n\nSustainable Energy for All (SEforALL) is an international organization launched by the then Secretary-General of the United Nations, Ban Ki-moon to help mobilize achievement of universal energy access, improve energy efficiency, and increase the use of renewable energy. \n\nIt was formally launched as an initiative in September 2011 and is headquartered in Vienna, Austria, with an executive office in Washington, DC. \nRachel Kyte took office as SEforALL’s CEO and Special Representative of the UN Secretary-General in January 2016. \n\nUN General Assembly resolution 65/151, which was passed on 20 December 2010, designated 2012 as the International Year of Sustainable Energy for All. This was in recognition of the growing importance of energy for economic development and climate change mitigation. It also attempted to correct what many working on energy and development issues had for many years argued was a major error in not including action on energy poverty in the Millennium Development Goals.\n\nIn June 2009, Ban Ki-moon established the Secretary-General's Advisory Group on Energy and Climate Change (AGECC), which published its final report on 28 April 2010. The recommendations from this report form the basis of the Sustainable Energy for All initiative launched in 2011.\n\nSEforALL’s mission ties closely to the 2015 Sustainable Development Goals (SDGs) – specifically, Sustainable Development Goal 7, which calls for universal access to sustainable energy by 2030 – and the Paris Climate Agreement, which calls for reducing greenhouse gas emissions to limit climate warming to below 2 degrees Celsius. \n\nA number of activities have been initiated, both within and outside the UN system. These include:\n\n\n"}
{"id": "39402132", "url": "https://en.wikipedia.org/wiki?curid=39402132", "title": "Tom Hoblyn", "text": "Tom Hoblyn\n\nTom Hoblyn is a British landscape and garden designer who has been designing gardens since 2005. He has won five medals, including Gold, at the Chelsea Flower Show as well as the RHS People’s Choice Award for The Arthritis Research UK Garden 2012.\n\nHoblyn trained at the Royal Botanic Gardens, Kew and set up his company Thomas Hoblyn Landscape & Garden Design Ltd in 2002. His commissions are geographically diverse and varied. He is returning the gardens of the former Hunting Lodge of Edward and Mrs Simpson to its past glory and has been appointed to create a new landmark garden attraction and sculpture park at Hillersdon House.\n\nHoblyn is a teacher and commentator. He has contributed to Radio 4’s Gardeners’ Question Time and written features for Gardens Illustrated, The English Garden and Country Living magazines. He writes the monthly \"Diary of a Garden Designer\" blog for The Guardian and has blogged for the BBC Gardening website and the Financial Times.\n\nA garden inspired by his childhood in Cornwall is permanently sited at the Eden Project in Cornwall.\n"}
{"id": "7982330", "url": "https://en.wikipedia.org/wiki?curid=7982330", "title": "World population estimates", "text": "World population estimates\n\nThis article lists estimates of world population, as well as projections of future developments.\nIn summary, estimates for the progression of world population since the late medieval period are in the following ranges:\nEstimates for pre-modern times are necessarily fraught with great uncertainties, and few of the published estimates have confidence intervals; in the absence of a straightforward means to assess the error of such estimates, a rough idea of expert consensus can be gained by comparing the values given in independent publications. Population estimates cannot be considered accurate to more than two decimal digits;\nfor example, world population for the year 2012 was estimated at\n7.02, 7.06 and 7.08 billion by the United States Census Bureau, the Population Reference Bureau and the United Nations Department of Economic and Social Affairs, respectively, corresponding to \na spread of estimates of the order of 0.8%.\n\nAs a general rule, the confidence of estimates on historical world population decreases for the more distant past. \nRobust population data only exists for the last two or three centuries. Until the late 18th century, few governments had ever performed an accurate census. In many early attempts, such as in Ancient Egypt and the Persian Empire, the focus was on counting merely a subset of the population for purposes of taxation or military service. \nPublished estimates for the 1st century (\"AD 1\") suggest an uncertainty of the order of 50% (estimates range between 150 and 330 million).\nSome estimates extend their timeline into deep prehistory, to \"10,000 BC\", i.e. the early Holocene, when world population estimates range roughly between one and ten million (with an uncertainty of up to an order of magnitude).\n\nEstimates for yet deeper prehistory, into the Paleolithic, are of a different nature. At this time human populations consisted entirely of non-sedentary hunter-gatherer populations, with anatomically modern humans existing alongside archaic human varieties, some of which are still ancestral to the modern human population due to interbreeding with modern humans during the Upper Paleolithic. \nEstimates of the size of these populations are a topic of paleoanthropology. A late human population bottleneck is postulated by some scholars at approximately 70,000 years ago, during the Toba catastrophe, when \"Homo sapiens\" population may have dropped to as low as between 1,000 and 10,000 individuals.\nFor the time of speciation of \"Homo sapiens\", some 200,000 years ago, an effective population size of the order of 10,000 to 30,000 individuals has been estimated, with an actual \"census population\" of early \"Homo sapiens\" of roughly 100,000 to 300,000 individuals.\nThe question of \"how many people have ever lived?\" or \"what percentage of people who have ever lived are alive today\" can be traced to the 1970s.\nThe more dramatic phrasing of \"the living outnumber the dead\" also dates to the 1970s, a time of population explosion and growing fears of human overpopulation in the wake of decolonization and before the adoption of China's one-child policy.\nThe claim that \"the living outnumber the dead\" was never accurate (although it may be roughly accurate if only ancestral population is considered). Arthur C. Clarke in \"\" (1968) has the claim that \"Behind every man now alive stand 30 ghosts, for that is the ratio by which the dead outnumber the living\", which was roughly accurate at the time of writing.\n\nEstimates of the \"total number of people who have ever lived\" is 107.6 billion as of 2011. \nThe answer naturally depends on the definition of \"people\", i.e. is only \"Homo sapiens\" to be counted, or all of genus \"Homo\", but due to the small population sizes in the Lower Paleolithic, the order of magnitude of the estimate is not affected by the choice of cut-off date substantially more than by the uncertainty of estimates throughout the Neolithic to Iron Age.\nThe estimate is more crucially affected by the estimate of infant mortalities vs. stillborn infants, due to the very high infant mortality throughout the pre-modern period. An estimate on the \"total number of people who have ever lived\" as of 1995 was calculated by Haub (1995) at \"about 105 billion births since the dawn of the human race\" with a cut-off date at 50,000 BC (beginning of the Upper Paleolithic), and an inclusion of a high infant mortality rate throughout pre-modern history.\n\nThe following table uses astronomical year numbering for dates, negative numbers corresponding roughly to the corresponding year BC (i.e. -10000 = 10,001 BC, etc.). The table starts counting around the Late Glacial Maximum period, in which ice retreated and humans started to spread into the northern hemisphere.\n\nFrom the beginning of the early modern period until the 20th century, world population has been characterized by a faster-than-exponential growth.\nFor the period of Classical Antiquity to the Middle Ages, roughly 500 BC to AD 1500, there has also been a general tendency of growth (estimated at roughly a factor 4 to 5 over the 2,000 year period), but not strictly monotonic: A noticeable dip in world population is assumed due to the Black Death in the mid-14th century.\n\nFor times after World War II, demographic data of some accuracy becomes available for a significant number of countries, and population estimates are often given as grand totals of numbers (typically given by country) of widely diverging accuracies. Some sources give these numbers rounded to the nearest million or the nearest thousand, while others give them without any rounding.\n\nTaking these numbers at face value would \nbe false precision; in spite of being stated to four, seven or even ten digits, they should not be interpreted as accurate to more than three digits at best (estimates by the United States Census Bureau and by the United Nations differ by about 0.5–1.5%).\n\n, the population of the world is projected to reach 8 billion in 2025, and 9 billion by about 2040/42. Kapitza (1996) estimated an asymptotic limit of population growth of 14 billion, 90% of which (12.6 billion) expected to be reached by 2135.\n\nReasonable predictions of population development are possible for the next 30 years or so, representing the period of fertility of the children alive today. Projections of population reaching more than one generation into the future are highly speculative: Thus, the United Nations Department of Economic and Social Affairs report of 2004 projected the world population to peak at 9.22 billion in 2075 and then stabilise at a value close to 9 billion; \nBy contrast, a 2014 projection by the United Nations Population Division predicts a population close to 11 billion by 2100 without any declining trend in the foreseeable future. On the other hand, a conservative scenario published in 2012 assumes that a maximum of 8 billion will be reached before 2040.\n\nThe following table shows projections of world population for the 21st century.\nOther, historical projections include\n\nPopulation estimates for world regions based on Maddison (2007), in millions.\nThe row showing total world population includes the average growth rate per year over the period separating each column from the preceding one.\n\nWhen considering population estimates by world region, it is worth noting that population history of the indigenous peoples of the Americas before the 1492 voyage of Christopher Columbus has proven difficult to establish, with many historians arguing for an estimate of 50 million people throughout the Americas, and some estimating that populations may have reached 100 million people or more. It is therefore estimated by some that populations in Mexico, Central, and South America could have reached 37 million by 1492. Additionally, the population estimate of 2 million for North America for the same time period represents the low end of modern estimates, and some estimate the population to have been as high as 18 million.\n\n\n"}
{"id": "1719876", "url": "https://en.wikipedia.org/wiki?curid=1719876", "title": "Yellowstone hotspot", "text": "Yellowstone hotspot\n\nThe Yellowstone hotspot is a volcanic hotspot in the United States responsible for large scale volcanism in Idaho, Montana, Nevada, Oregon, and Wyoming as the North American tectonic plate moved over it. It formed the eastern Snake River Plain through a succession of caldera-forming eruptions. The resulting calderas include the Island Park Caldera, the Henry's Fork Caldera, and the Bruneau-Jarbidge caldera. The hotspot currently lies under the Yellowstone Caldera. The hotspot's most recent caldera-forming supereruption, known as the Lava Creek eruption, took place 640,000 years ago and created the Lava Creek Tuff, and the most recent Yellowstone Caldera. The Yellowstone hotspot is one of a few volcanic hotspots underlying the North American tectonic plate; others include the Anahim and Raton hotspots.\n\nThe eastern Snake River Plain is a topographic depression that cuts across Basin and Range Mountain structures, more or less parallel to North American plate motion. Beneath more recent basalts are rhyolite lavas and ignimbrites that erupted as the lithosphere passed over the hotspot. Younger volcanoes that erupted after passing over the hotspot covered the plain with young basalt lava flows in places, including Craters of the Moon National Monument and Preserve.\n\nThe central Snake River plain is similar to the eastern plain, but differs by having thick sections of interbedded lacustrine (lake) and fluvial (stream) sediments, including the Hagerman Fossil Beds.\n\nAlthough the McDermitt volcanic field on the Nevada–Oregon border is frequently shown as the site of the initial impingement of the Yellowstone Hotspot, new geochronology and mapping demonstrates that the area affected by this mid-Miocene volcanism is significantly larger than previously appreciated. Three silicic calderas have been newly identified in northwest Nevada, west of the McDermitt volcanic field as well as the Virgin Valley Caldera. These calderas, along with the Virgin Valley Caldera and McDermitt Caldera, are interpreted to have formed during a short interval 16.5–15.5 million years ago, in the waning stage of the Steens flood basalt volcanism. The northwest Nevada calderas have diameters ranging from 15–26 km and deposited high temperature rhyolite ignimbrites over approximately 5000 km.\n\nThe Bruneau-Jarbidge caldera erupted between ten and twelve million years ago, spreading a thick blanket of ash in the Bruneau-Jarbidge event and forming a wide caldera. Animals were suffocated and burned in pyroclastic flows within a hundred miles of the event, and died of slow suffocation and starvation much farther away, notably at Ashfall Fossil Beds, located 1000 miles downwind in northeastern Nebraska, where a foot of ash was deposited. There, two hundred fossilized rhinoceros and many other animals were preserved in two meters of volcanic ash. By its characteristic chemical fingerprint and the distinctive size and shape of its crystals and glass shards, the volcano stands out among dozens of prominent ashfall horizons laid down in the Cretaceous, Paleogene, and Neogene periods of central North America. The event responsible for this fall of volcanic ash was identified as Bruneau-Jarbidge. Prevailing westerlies deposited distal ashfall over a vast area of the Great Plains.\n\nThe Twin Falls and Picabo volcanic fields were active about 10 million years ago. The Picabo Caldera was notable for producing the Arbon Valley Tuff 10.2 million years ago.\n\nThe Heise volcanic field of eastern Idaho produced explosive caldera-forming eruptions which began 6.6 million years ago and lasted for more than 2 million years, sequentially producing four large-volume rhyolitic eruptions. The first three caldera-forming rhyolites — Blacktail Tuff, Walcott Tuff and Conant Creek Tuff — totaled at least 2250 km of erupted magma. The final, extremely voluminous, caldera-forming eruption — the Kilgore Tuff — which erupted 1800 km of ash, occurred 4.5 million years ago.\n\nThe Yellowstone Plateau volcanic field is composed of four adjacent calderas. West Thumb Lake is itself formed by a smaller caldera which erupted 174,000 years ago. (See .) The Henry's Fork Caldera in Idaho was formed in an eruption of more than 1.3 million years ago, and is the source of the Mesa Falls Tuff. The Henry's Fork Caldera is nested inside of the Island Park Caldera and the calderas share a rim on the western side. The earlier Island Park Caldera is much larger and more oval and extends well into Yellowstone Park. Although much smaller than the Island Park Caldera, the Henry's Fork Caldera is still sizeable at long and wide and its curved rim is plainly visible from many locations in the Island Park area.\n\nOf the many calderas formed by the Yellowstone Hotspot, including the later Yellowstone Caldera, the Henry's Fork Caldera is the only one that is currently clearly visible. The Henry's Fork of the Snake River flows through the Henry's Fork Caldera and drops out at Upper and Lower Mesa Falls. The caldera is bounded by the Ashton Hill on the south, Big Bend Ridge and Bishop Mountain on the west, by Thurburn Ridge on the North and by Black Mountain and the Madison Plateau on the east. The Henry's Fork caldera is in an area called Island Park. Harriman State Park is situated in the caldera.\n\nThe Island Park Caldera is older and much larger than the Henry's Fork Caldera with approximate dimensions of 58 miles (93 km) by 40 miles (64 km). It is the source of the Huckleberry Ridge Tuff that is found from southern California to the Mississippi River near St. Louis. This supereruption occurred 2.1 million years BP and produced 2500 km of ash. The Island Park Caldera is sometimes referred to as the First Phase Yellowstone Caldera or the Huckleberry Ridge Caldera. The youngest of the hotspot calderas, the Yellowstone Caldera, formed 640,000 years ago and is about 34 miles (55 km) by 45 miles (72 km) wide. Non-explosive eruptions of lava and less-violent explosive eruptions have occurred in and near the Yellowstone Caldera since the last super eruption. The most recent lava flow occurred about 70,000 years ago, while the largest violent eruption excavated the West Thumb of Lake Yellowstone around 150,000 years ago. Smaller steam explosions occur as well – an explosion 13,800 years ago left a 5 kilometer diameter crater at Mary Bay on the edge of Yellowstone Lake.\n\nBoth the Heise and Yellowstone volcanic fields produced a series of caldera-forming eruptions characterised by magmas with so-called \"normal\" oxygen isotope signatures (with heavy oxygen-18 isotopes) and a series of predominantly post-caldera magmas with so-called \"light\" oxygen isotope signatures (characterised as low in heavy oxygen-18 isotopes). The final stage of volcanism at Heise was marked by \"light\" magma eruptions. If Heise is any indication, this could mean that the Yellowstone Caldera has entered its final stage, but the volcano might still exit with a climactic fourth caldera event analogous to the fourth and final caldera-forming eruption of Heise (the Kilgore Tuff) – which was also made up of so-called \"light\" magmas. The appearance of \"light\" magmas would seem to indicate that the uppermost portion of the continental crust has largely been consumed by the earlier caldera- forming events, exhausting the melting potential of the crust above the mantle plume. In this case Yellowstone could be expiring. It could be another 1–2 million years (as the North American Plate moves across the Yellowstone hotspot) before a new supervolcano is born to the northeast, and the Yellowstone Plateau volcanic field joins the ranks of its deceased ancestors in the Snake River Plain. (References to be added: Kathryn Watts (Nov 2007) GeoTimes \"Yellowstone and Heise: Supervolcanoes that Lighten Up\": Kathryn E. Watts, Ilya N. Bindeman and Axel K. Schmitt (2011) Petrology, Vol. 52, No. 5, \"Large-volume Rhyolite Genesis in Caldera Complexes of the Snake River Plain: Insights from the Kilgore Tuff of the Heise Volcanic Field, Idaho, with Comparison to Yellowstone and Bruneau-Jarbidge Rhyolites\" pp. 857–890).\n\n\n\n\n\n\n"}
{"id": "53034532", "url": "https://en.wikipedia.org/wiki?curid=53034532", "title": "Yumurtalık Lagoon", "text": "Yumurtalık Lagoon\n\nYumurtalık Lagoon () is a lagoon located in Adana Province, southern Turkey. The area is a nature reserve, a former national park, and is a Ramsar site.\n"}
{"id": "53390774", "url": "https://en.wikipedia.org/wiki?curid=53390774", "title": "Zeta Librae", "text": "Zeta Librae\n\nThe Bayer designation ζ Librae, Latinised as zeta Librae (abbreviated ζ Lib / zeta Lib) is shared by several star systems in the constellation Libra. Sources differ about the Flamsteed and Bayer designations that should be applied to four stars:\n\n\nIn light of the obvious confusion, the Bright Star Catalogue recommends the use of HR designations to unambiguously identify these four stars. HD 137949 does not have an HR number although it is included in the Bright Star Catalogue Supplement.\n"}
