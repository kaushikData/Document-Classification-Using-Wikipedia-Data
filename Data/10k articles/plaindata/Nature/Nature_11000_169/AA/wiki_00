{"id": "57393594", "url": "https://en.wikipedia.org/wiki?curid=57393594", "title": "2010 WC9", "text": "2010 WC9\n\n, unofficially designated , is a sub-kilometer near-Earth asteroid of the Apollo group, approximately in diameter. First observed for eleven days by the Catalina Sky Survey in 2010, the asteroid was recovered in May 2018 during its sub-lunar close encounter with Earth. \n\n was first observed by astronomers with the Catalina Sky Survey on 30 November 2010 with a 1-day observation arc and was observed through 10 December 2010. By 10 December 2010 the asteroid was more than 24 million kilometers from Earth at apparent magnitude 21.8 and was becoming too faint to be practical to track.\n\nThe preliminary 10-day observation arc generated a line of variation roughly 15 million km long for May 2018 that did not intersect Earth's orbit and thus was not a 2018 impact threat. The 10-day observation arc showed the asteroid would pass about from Earth around late 14 May 2018. The asteroid was recovered on 8 May 2018 when it was 8 million kilometers from Earth and given the temporary NEOCP designation ZJ99C60. It was removed from the Sentry Risk Table on 10 May 2018 and is not an impact threat for the next 100 years or more. The asteroid now has a secure 7-year observation arc.\n\n is an Apollo asteroid, the largest dynamical group of near-Earth objects with nearly 10,000 known members. It orbits the Sun at a distance of 0.78–1.4 AU once every 13 months (409 days; semi-major axis of 1.08 AU). Its orbit has an eccentricity of 0.28 and an inclination of 18° with respect to the ecliptic.\n\nUsing an epoch of 23 March 2018, the object had a minimum orbital intersection distance with Earth of , or 0.55 lunar distances (LD).\n\nOn 15 May 2018, 22:05 UT, the asteroid approached Earth at just over 0.5 LD, the closest approach of this asteroid in nearly 300 years.. It was expected to reach apparent magnitude +11 at closest approach, bright enough to be seen in a small telescope if you have a custom ephemeris for your location. At closest approach, it was best seen from the Southern hemisphere such as South Africa and southern South America. The asteroid passed Earth going .\n\nThis was the third closest approach ever observed by an asteroid with absolute magnitude (H) brighter than 24.\n\nAs the asteroid has not been directly resolved by telescope, its diameter can only be estimated based on the distance and brightness. Based on a generic magnitude-to-diameter conversion, it is estimated to measure between 60–130 meters in diameter, for an absolute magnitude of 23.5, and an assumed albedo of 0.04–0.20. \n\nAs of 2018, this minor planet has neither been numbered nor named by the Minor Planet Center.\n\n\n"}
{"id": "10100830", "url": "https://en.wikipedia.org/wiki?curid=10100830", "title": "Acis and Galatea", "text": "Acis and Galatea\n\nAcis and Galatea is a story from Greek mythology that originally appeared in Ovid's \"Metamorphoses\". The story tells of the love between the mortal Acis and the Nereid (sea-nymph) Galatea; when the jealous Cyclops Polyphemus kills Acis, Galatea transforms her lover into an immortal river spirit. The episode was made the subject of poems, operas, paintings, and statues in the Renaissance and after.\n\nGalatea (Γαλάτεια; \"she who is milk-white\"), daughter of Nereus and Doris, was a sea-nymph anciently attested in the work of both Homer and Hesiod, where she is described as the fairest and most beloved of the 50 Nereids. In Ovid's \"Metamorphoses\" she appears as the beloved of Acis, the son of Faunus and the river-nymph Symaethis, daughter of the River Symaethus. When a jealous rival, the Sicilian Cyclops Polyphemus, killed him with a boulder, Galatea then turned his blood into the Sicilian River Acis, of which he became the spirit. This version of the tale occurs nowhere earlier and may be a fiction invented by Ovid, \"suggested by the manner in which the little river springs forth from under a rock\". According to Athenaeus, ca 200 CE, the story was first concocted by Philoxenus of Cythera as a political satire against the Sicilian tyrant Dionysius I of Syracuse, whose favourite concubine, Galatea, shared her name with the nymph. Others claim the story was invented to explain the presence of a shrine dedicated to Galatea on Mount Etna.\n\nDuring Renaissance and Baroque times the story emerged once more as a popular theme. In Spain, Luis de Góngora y Argote wrote the much-admired narrative poem, \"Fábula de Polifemo y Galatea\", published in 1627. It is particularly noted for its depiction of landscape and for the sensual description of the love of Acis and Galatea. It was written in homage to an earlier and rather shorter narrative with the same title by Luis Carillo y Sotomayor (1611) The story was also given operatic treatment in the very popular zarzuela of Antoni Lliteres Carrió (1708). The atmosphere here is lighter and enlivened by the inclusion of the clowns Momo and Tisbe.\n\nIn France, Jean-Baptiste Lully devoted his opera Acis et Galatée (1686) to their love. Described by him as a pastoral-heroic work, it depicts a love triangle between the three main characters - Acis, Galatea, and Poliphème. Poliphème murders Acis out of jealousy, but Acis is revived and turned into a river by Neptune. In Italy Giovanni Bononcini's one-act opera \"Polifemo\" followed in 1703. Shortly afterwards George Frideric Handel was working in that country and composed the cantata \"Aci, Galatea e Polifemo\" (1708), laying as much emphasis on the part of Polifemo as on the lovers. Written in Italian, Polifemo's deep bass solo \"Fra l'ombre e gl'orrori\" (From horrid shades) establishes his character from the start.\n\nAfter Handel's move to England, he gave the story a new treatment in his pastoral opera \"Acis and Galatea\" with an English libretto provided by John Gay. Initially composed in 1718, the work went through many revisions and was later to be given updated orchestrations by both Mozart and Mendelssohn. As a pastoral work where Polyphemus plays only a minor, though decisive part, it largely centres on the two lovers. In Austria later in the century, Joseph Haydn composed \"Acide e Galatea\" (1763). Designed for an imperial wedding, it was given a happier ending centred on the transformation scene after the murder of Acis as the pair declare their undying love.\n\nPaintings featuring Acis and Galatea can be grouped according to their themes. Most notably the story takes place within a pastoral landscape in which the figures are almost incidental. This is particularly so in Nicolas Poussin's \"Landscape with Polyphemus\" (1649)(Hermitage Museum) and Claude Lorrain's seaside landscape (Dresden) of 1657, in both of which the lovers play a minor part in the foreground. In an earlier painting by Poussin (National Gallery of Ireland, 1630) the couple is among several embracing figures in the foreground, shielded from view of Polyphemus, who is playing his flute higher up the slope.\n\nIn all of these Polyphemus is somewhere in the background, but many others feature Galatea alone, as in Perino del Vaga's painting of her being drawn by sea beasts over the waves while riding on a seashell. Generally, though, the nymph is carried through the sea by adoring attendants in paintings generally titled \"The Triumph of Galatea\", of which the most renowned treatment is by Raphael. In general these follow the 3rd-century description given of such a painting by Philostratus the Younger in his \"Imagines\":\n\nIn those cases where the rejected lover Polyphemus appears somewhere ashore, the division between them is emphasised by their being identified with their respective elements, sea, and land. Typical examples of this were painted by Francois Perrier, Giovanni Lanfranco and Jean-Baptiste van Loo.\n\nSensual portrayals of the lovers embracing in a landscape were provided by French painters especially, as in those by Charles de La Fosse (c. 1700), Jean-François de Troy and Alexandre Charles Guillemot (1827). Polyphemus lurks in the background of these and in the example by De Troy his presence plainly distresses Galatea. Other French examples by Antoine Jean Gros (1833) and Édouard Zier (1877) show the lovers hiding in a cave and peering anxiously out at him.\n\nThey anticipate the tragic moment when he looms menacingly over the pair, having discovered the truth they have tried to conceal. The threat is as apparent in Jean-Francois de Troy's softly outlined 18th-century vision as it is in Odilon Redon's almost Surrealist painting of 1900. The brooding atmosphere in these suggests the violent action which is to follow. That had been portrayed in earlier paintings of Polyphemus casting a rock at the fleeing lovers, such as those by Annibale Carracci, and Carle van Loo.\n\nStatues of Galatea, sometimes in the company of Acis, began to be made in Europe from the 17th century. There is a fanciful description of a fountain that incorporates them both in John Barclay's Latin novel \"Argenis\", dating from 1621:\n\nAn actual statue by a pool in the public gardens of Acireale, the Sicilian town where the transformation of Acis is supposed to have taken place, is less ingenious. He lies beneath the boulder that has killed him while Galatea crouches to one side, an arm raised to heaven in supplication.\n\nFrench sculptors have also been responsible for some memorable statues. There are a pair by Jean-Baptiste Tuby in the Bosquet des Dômes in the Versailles gardens. Acis leans on a rock, casually playing the flute, as the half-clad Galatea comes upon him with hands lifted in surprise (1667–75). A similar gesture is displayed in the statue of her alone in the fountain to the right of the great staircase at Château de Chantilly. The lovers are portrayed together as part of the Medici Fountain in the Luxembourg Garden in Paris. Designed by Auguste Ottin in 1866, the marble group embrace inside a grotto while above them is crouched a huge Polyphemus in weathered bronze, peering down in jealousy.\n\nMany other statues feature Galatea alone, but there is a complication. Some time after the Renaissance, the same name was given to Pygmalion's animated statue and one has to distinguish between representations of her and of the nymph Galatea. One pointer is given by the introduction of features mentioned in the description of the nymph by Philostratus that is quoted above. These include one hand raised and holding a billowing scarf; sea imagery, including shells, dolphins and tritons; and often the fact that the statue is incorporated into a fountain. In the work by Gabriel de Grupello in the castle park at Schwetzingen, the triton at Galatea's feet holds up a garland threaded with shells and pearls. The Galatea in the grounds of Tsarskoye Selo in Russia has sea pearls threaded into her hair. There is also a statue of her by Nicola Michetti that forms part of the cascade at the Peterhof Palace in St Petersburg.\n\nThe nymph reclines on a large shell carried by tritons in the 18th-century fountain at the Villa Borromeo Visconti Litta in Milan. It is on the back of a dolphin that she reclines in the statue by the 19th-century Italian sculptor Leopoldo Ansiglioni (1832–1894). There are two versions of this, one at the centre of a fish pool in the East House of the University of Greenwich's Winter Gardens, and a later copy installed at Hearst Castle in California. In this, one of the arms bent back to support her head is encircled by the dolphin's tail. There is also a German fountain by Karl Friedrich Moest now installed in Karlsruhe in which Galatea sits on the back of a triton. Over her head she balances the huge shell from which the water pours. Another statue was erected at the head of an impressive cascade in Stuttgart's Eugenplatz. A work of Otto Rieth (1858–1911) dating from 1890, it features the nymph crowned with seaweed and surging up from the dolphin and young cupids playing at her feet.\n\nIn the applied arts, three-dimensional representations of Raphael's triumph theme were often incorporated into artifacts for aristocratic use and were painted on majolica ware.\n\n"}
{"id": "38052921", "url": "https://en.wikipedia.org/wiki?curid=38052921", "title": "Ad Hoc Working Group on Further Commitments for Annex I Parties under the Kyoto Protocol", "text": "Ad Hoc Working Group on Further Commitments for Annex I Parties under the Kyoto Protocol\n\nThe Ad Hoc Working Group on Further Commitments for Annex I Parties under the Kyoto Protocol (AWG-KP) is one of the two negotiating tracks established under the Bali Road Map at the United Nations Framework Convention on Climate Change's 13th Conference of the Parties (COP13) in Bali in 2007. The other one is the Ad Hoc Working Group on Long-term Cooperative Action (AWG-LCA).\n"}
{"id": "49482835", "url": "https://en.wikipedia.org/wiki?curid=49482835", "title": "Aerobic fermentation", "text": "Aerobic fermentation\n\nAerobic fermentation is a metabolic process by which cells metabolize sugars via fermentation in the presence of oxygen and occurs through the repression of normal respiratory metabolism (also referred to as the crabtree effect in yeast). This phenomenon is fairly rare and is primarily observed in yeasts. Aerobic fermentation evolved independently in at least three yeast lineages (\"Saccharomyces\", \"Dekkera\", \"Schizosaccharomyces\"). It has also been observed in plant pollen, trypanosomatids, mutated \"E. coli\", and tumor cells. Crabtree-positive yeasts will respire when grown with very low concentrations of glucose or when grown on most other carbohydrate sources. The Crabtree effect is a regulatory system whereby respiration is repressed by fermentation, except in low sugar conditions. When \"Saccharomyces cerevisiae\" is grown below the sugar threshold and undergoes a respiration metabolism, the fermentation pathway is still fully expressed, while the respiration pathway is only expressed relative to the sugar availability. This contrasts with the pasteur effect, which is the inhibition of fermentation in the presence of oxygen, and observed in most organisms.\n\nThe evolution of aerobic fermentation likely involved multiple successive molecular steps, which included the expansion of hexose transporter genes, copy number variation (CNV) and differential expression in metabolic genes, and regulatory reprogramming. Research is still needed to fully understand the genomic basis of this complex phenomenon. Many crabtree-positive yeast species are used for their fermentation ability in industrial processes in the production of wine, beer, sake, bread, and bioethanol. Through domestication, these yeast species have evolved, often through artificial selection, to better fit their environment. Strains evolved through mechanisms that include interspecific hybridization, horizontal gene transfer (HGT), gene duplication, pseudogenization, and gene loss.\n\nApproximately 100 million years ago (mya), within the yeast lineage there was a whole genome duplication (WGD). A majority of Crabtree-positive yeasts are post-WGD yeasts. It was believed that the WGD was a mechanism for the development of Crabtree effect in these species due to the duplication of alcohol dehydrogenase (ADH) encoding genes and hexose transporters. However, recent evidence has shown that aerobic fermentation originated before the WGD and evolved as a multi-step process, potentially aided by the WGD. The origin of aerobic fermentation, or the first step, in \"Saccharomyces\" crabtree-positive yeasts likely occurred in the interval between the ability to grow under anaerobic conditions, horizontal transfer of anaerobic DHODase (encoded by URA1 with bacteria), and the loss of respiratory chain Complex I. A more pronounced Crabtree effect, the second step, likely occurred near the time of the WGD event. Later evolutionary events that aided in the evolution of aerobic fermentation are better understood and outlined in the Genomic basis of the crabtree effect section.\n\nIt is believed that a major driving force in the origin of aerobic fermentation was its simultaneous origin with modern fruit (~125 mya). These fruit provided an abundance of simple sugar food source for microbial communities, including both yeast and bacteria. Bacteria, at that time, were able to produce biomass at a faster rate than the yeast. Producing a toxic compound, like ethanol, can slow the growth of bacteria, allowing the yeast to be more competitive. However, the yeast still had to use a portion of the sugar it consumes to produce ethanol. Crabtree-positive yeasts also have increased glycolytic flow, or increased uptake of glucose and conversion to pyruvate, which compensates for using a portion of the glucose to produce ethanol rather than biomass. Therefore, it is believed that the original driving force was to kill competitors. This is supported by research that determined the kinetic behavior of the ancestral ADH protein, which was found to be optimized to make ethanol, rather than consume it.\n\nFurther evolutionary events in the development of aerobic fermentation likely increased the efficiency of this lifestyle, including increased tolerance to ethanol and the repression of the respiratory pathway. In high sugar environments, \"S. cerevisiae\" outcompetes and dominants all other yeast species, except its closest relative \"Saccharomyces paradoxus\". The ability of \"S. cerevisiae\" to dominate in high sugar environments evolved more recently than aerobic fermentation and is dependent on the type of high-sugar environment. Other yeasts' growth is dependent on the pH and nutrients of the high-sugar environment.\n\nThe genomic basis of the crabtree effect is still being investigated, and its evolution likely involved multiple successive molecular steps that increased the efficiency of the lifestyle.\n\nHexose transporters (HXT) are a group of proteins that are largely responsible for the uptake of glucose in yeast. In \"S. cerevisiae\", 20 \"HXT\" genes have been identified and 17 encode for glucose transporters (\"HXT1-HXT17\"), \"GAL2\" encodes for a galactose transporter, and \"SNF3\" and \"RGT2\" encode for glucose sensors. The number of glucose sensor genes have remained mostly consistent through the budding yeast lineage, however glucose sensors are absent from \"Schizosaccharomyces pombe\". \"Sch. pombe\" is a Crabtree-positive yeast, which developed aerobic fermentation independently from \"Saccharomyces\" lineage, and detects glucose via the cAMP-signaling pathway. The number of transporter genes vary significantly between yeast species and has continually increased during the evolution of the \"S. cerevisiae\" lineage. Most of the transporter genes have been generated by tandem duplication, rather than from the WGD. \"Sch. pombe\" also has a high number of transporter genes compared to its close relatives. Glucose uptake is believed to be a major rate-limiting step in glycolysis and replacing \"S. cerevisiae\"'s \"HXT1-17\" genes with a single chimera \"HXT\" gene results in decreased ethanol production or fully respiratory metabolism. Thus, having an efficient glucose uptake system appears to be essential to ability of aerobic fermentation. There is a significant positive correlation between the number of hexose transporter genes and the efficiency of ethanol production.\n\nAfter a WGD, one of the duplicated gene pair is often lost through fractionation; less than 10% of WGD gene pairs have remained in \"S. cerevisiae\" genome. A little over half of WGD gene pairs in the glycolysis reaction pathway were retained in post-WGD species, significantly higher than the overall retention rate. This has been associated with an increased ability to metabolize glucose into pyruvate, or higher rate of glycolysis. After glycolysis, pyruvate can either be further broken down by pyruvate decarboxylase (Pdc) or pyruvate dehydrogenase (Pdh). The kinetics of the enzymes are such that when pyruvate concentrations are high, due to a high rate of glycolysis, there is increased flux through Pdc and thus the fermentation pathway. The WGD is believed to have played a beneficial role in the evolution of the Crabtree effect in post-WGD species partially due to this increase in copy number of glycolysis genes.\n\nThe fermentation reaction only involves two steps. Pyruvate is converted to acetaldehyde by Pdc and then acetaldehyde is converted to ethanol by alcohol dehydrogenase (Adh). There is no significant increase in the number of \"Pdc\" genes in Crabtree-positive compared to Crabtree-negative species and no correlation between number of \"Pdc\" genes and efficiency of fermentation. There are five \"Adh\" genes in \"S. cerevisiae\". Adh1 is the major enzyme responsible for catalyzing the fermentation step from acetaldehyde to ethanol. Adh2 catalyzes the reverse reaction, consuming ethanol and converting it to acetaldehyde. The ancestral, or original, Adh had a similar function as Adh1 and after a duplication in this gene, Adh2 evolved a lower K for ethanol. Adh2 is believed to have increased yeast species' tolerance for ethanol and allowed Crabtree-positive species to consume the ethanol they produced after depleting sugars. However, Adh2 and consumption of ethanol is not essential for aerobic fermentation. \"Sch. pombe\" and other Crabtree positive species do not have the \"ADH2\" gene and consumes ethanol very poorly.\n\nIn Crabtree-negative species, respiration related genes are highly expressed in the presence of oxygen. However, when \"S. cerevisiae\" is grown on glucose in aerobic conditions, respiration-related gene expression is repressed. Mitochondrial ribosomal proteins expression is only induced under environmental stress conditions, specifically low glucose availability. Genes involving mitochondrial energy generation and phosphorylation oxidation, which are involved in respiration, have the largest expression difference between aerobic fermentative yeast species and respiratory species. In a comparative analysis between \"Sch. pombe\" and \"S. cerevisiae\", both of which evolved aerobic fermentation independently, the expression pattern of these two fermentative yeasts were more similar to each other than a respiratory yeast, \"C. albicans\". However, \"S. cerevisiae\" is evolutionarily closer to \"C. albicans\". Regulatory rewiring was likely important in the evolution of aerobic fermentation in both lineages.\n\nAerobic fermentation is also essential for multiple industries, resulting in human domestication of several yeast strains. Beer and other alcoholic beverages, throughout human history, have played a significant role in society through drinking rituals, providing nutrition, medicine, and uncontaminated water. During the domestication process, organisms shift from natural environments that are more variable and complex to simple and stable environments with a constant substrate. This often favors specialization adaptations in domesticated microbes, associated with relaxed selection for non-useful genes in alternative metabolic strategies or pathogenicity. Domestication might be partially responsible for the traits that promote aerobic fermentation in industrial species. Introgression and HGT is common in \"Saccharomyces\" domesticated strains. Many commercial wine strains have significant portions of their DNA derived from HGT of non-\"Saccharomyces\" species. HGT and introgression are less common in nature than is seen during domestication pressures. For example, the important industrial yeast strain \"Saccharomyces pastorianus\", is an interspecies hybrid of \"S. cerevisiae\" and the cold tolerant \"S. eubayanus.\" This hybrid is commonly used in lager-brewing, which requires slow, low temperature fermentation.\n\nAlcoholic fermentation is often used by plants in anaerobic conditions to produce ATP and regenerate NAD to allow for glycolysis to continue. For most plant tissues, fermentation only occurs in anaerobic conditions, but there are a few exceptions. In the pollen of maize (\"Zea mays\") and tobacco (\"Nicotiana tabacum\" & \"Nicotiana plumbaginifolia\"), the fermentation enzyme ADH is abundant, regardless of the oxygen level. In tobacco pollen, PDC is also highly expressed in this tissue and transcript levels are not influenced by oxygen concentration. Tobacco pollen, similar to Crabtree-positive yeast, perform high levels of fermentation dependent on the sugar supply, and not oxygen availability. In these tissues, respiration and alcoholic fermentation occur simultaneously with high sugar availability. Fermentation produces the toxic acetaldehyde and ethanol, that can build up in large quantities during pollen development. It has been hypothesized that acetaldehyde is a pollen factor that causes cytoplasmic male sterility. Cytoplasmic male sterility is a trait observed in maize, tobacco and other plants in which there is an inability to produce viable pollen. It is believed that this trait might be due to the expression of the fermentation genes, ADH and PDC, a lot earlier on in pollen development than normal and the accumulation of toxic aldehyde.\n\nWhen grown in glucose-rich media, trypanosomatid parasites degrade glucose via aerobic fermentation. In this group, this phenomenon is not a pre-adaptation to/or remnant of anaerobic life, shown through their inability to survive in anaerobic conditions. It is believed that this phenomenon developed due to the capacity for a high glycolytic flux and the high glucose concentrations of their natural environment. The mechanism for repression of respiration in these conditions is not yet known.\n\nA couple \"Escherichia coli\" mutant strains have been bioengineered to ferment glucose under aerobic conditions. One group developed the ECOM3 (\"E. coli\" cytochrome oxidase mutant) strain by removing three terminal cytochrome oxidases (cydAB, cyoABCD, and cbdAB) to reduce oxygen uptake. After 60 days of adaptive evolution on glucose media, the strain displayed a mixed phenotype. In aerobic conditions, some populations' fermentation solely produced lactate, while others did mixed-acid fermentation.\n\nOne of the hallmarks of cancer is altered metabolism or deregulating cellular energetics. Cancers cells often have reprogrammed their glucose metabolism to perform lactic acid fermentation, in the presence of oxygen, rather than send the pyruvate made through glycolysis to the mitochondria. This is referred to as the Warburg effect, and is associated with high consumption of glucose and a high rate of glycolysis. ATP production in these cancer cells is often only through the process of glycolysis and pyruvate is broken down by the fermentation process in the cell's cytoplasm. This phenomenon is often seen as counterintuitive, since cancer cells have higher energy demands due to the continued proliferation and respiration produces significantly more ATP than glycolysis alone (fermentation produces no additional ATP). Typically, there is an up-regulation in glucose transporters and enzymes in the glycolysis pathway (also seen in yeast). There are many parallel aspects of aerobic fermentation in tumor cells that are also seen in Crabtree-positive yeasts. Further research into the evolution of aerobic fermentation in yeast such as \"S. cerevisiae\" can be a useful model for understanding aerobic fermentation in tumor cells. This has a potential for better understanding cancer and cancer treatments.\n"}
{"id": "32343186", "url": "https://en.wikipedia.org/wiki?curid=32343186", "title": "Bratschen", "text": "Bratschen\n\nBratschen are weathering products that occur as a result of frost and aeolian corrasion almost exclusively on the calc-schists of the Upper Slate Mantle (\"Obere Schieferhülle\") in the High Tauern mountains of Austria. The term is German but is used untranslated in English sources. It may be the equivalent of the New Zealand climbers' term 'weet-bix' for a rock that disintegrates easily and so is difficult to climb on.\n\nThe calc-schist, that appears blue-gray when freshly broken, weathers to a yellow to brown colour and flakes off on the surface to form \"bratschen\".\n\nThese form steep (up to 40°), rocky, almost unvegetated mountainsides with an odd and rough-textured surface, caused by wind erosion. \"Bratschen\" are found on the mountains such as the Fuscherkarkopf, the Großer Bärenkopf, the Kitzsteinhorn, the Schwerteck or on the – eponymous – Bratschenköpfen.\n"}
{"id": "29442599", "url": "https://en.wikipedia.org/wiki?curid=29442599", "title": "Cap and dividend", "text": "Cap and dividend\n\nCap and dividend is a market-based trading system which retains the original capping method of cap and trade, but also includes compensation for energy consumers. This compensation is to offset the cost of products produced by companies that raise prices to consumers as a result of this policy.\n\nThe process begins with some governments setting aggregate pollution quotas (e.g., for carbon emissions) and selling pollution permits to the public respectively. Polluters are required to buy those credits to match their pollution outputs. Some of the cost producers pay for pollution will result in higher costs for consumers, who as citizens are additionally faced with the environmental costs of the pollution. Under the cap and dividend system, public revenues raised from the sale of pollution credits is rebated to citizens or to consumers as a subsidy for increasing efficiency.\n\nThe goal of this type of pseudo-tax is to reduce carbon emission rates. This is similar to the cap-and-trade system, with the main difference being that citizens receive dividend payments financed from pollution rents that are publicly captured, as opposed to leaving the value of pollution privileges to become financialized as private assets. The dividend payments can also finance the addition of incentives designed to encourage consumers to increase energy efficiency, whereas cap-and-trade does not directly involve the consumer. The Healthy Climate Trust Fund is the agency in the U.S. government who are overseeing the cap-and-dividend policy. They will accomplish this by collecting and distributing the funds from the capping process.\n\nProvided are convenient definitions pertaining to cap-and-dividend:\n\nFor definitions on Cap-and-Trade, see emissions trading\n\nThe history of the cap and trade system is very recent; it was created and is especially relevant in the political world recently.\n\nLittle is known about the origin of the idea, but most likely the idea came from multiple sources as an answer to the problems of cap and trade. It was popularized by American entrepreneur Peter Barnes and such groups as On the Commons, a network group which promotes environmental, community-related solutions.\n\nChris Van Hollen (D-Md.) put a bill before Congress on April 1, 2009, pertaining to carbon reductions and including the cap and dividend system. The cosponsors of the bill were Rep Earl Blumenauer [OR-3], Rep Lloyd Doggett [TX-25], Rep Mike Thompson [CA-1], Rep Lynn C. Woolsey [CA-6].\n\nThe goals of the Cap and Dividend Act of 2009 are as follows:\n\nThis bill looked to create the Healthy Climate Trust Fund, the potential agency for managing and distributing dividend funds.\n\nThe status of the bill included a referral to House Ways and Means Committee on 4/1/2009, then a referral to House Energy and Commerce Committee also on 4/1/2009, and then on 4/2/2009 a referral to the United States House Energy Subcommittee on Energy and Environment. The bill was reviewed by the United States House Energy Subcommittee on Energy and Environment but it didn't pass as an Act.\n\nCongressman Van Hollen again tried to introduce a Cap and Dividend bill in July 2014, but it did not pass as an Act.\n\nSeveral elder statesmen of the Grand Old Party (GOP), namely former Secretaries of State James Baker and George Shultz and former Treasury Secretary Hank Paulson, co-authored a report with Ted Halstead of Climate Leadership Council; and economists Martin Feldstein and Greg Mankiw; calling upon President Trump to introduce cap and dividend. Mankiw was interviewed about the use of a carbon tax in the 2016 Leonardo DiCaprio documentary \"Before the Flood\".\n\nCap and dividend, like cap and trade, would have had a direct impact on the economy. With a policy like this it will affect not only the major companies that will be taxed but also every household through a chain reaction of product price increases. There will be variations on how much of an impact this policy would have on different geographical areas based on population and how industrialized the area is.\n\nCaps will be placed on carbon emissions and every company that use carbon-based fuel to produce some sort of product will have to buy carbon permits. In the cap and dividend policy, every company will have to buy a carbon permit and this differs from the cap and trade policy because there will be no permits given away for free. The permits, collected by the government, will then be used to account for the dividends given back to the people. However, because of the permits' cost, the companies will be forced to raise the prices of their products so they can still make profit. The price increase will be felt by all customers of these various products.\n\nThe cost of everything made using carbon-based fuels will increase and will be felt by everyone. However, the people most affected by the price increases are the people emitting more carbon. For example: Someone that drives a Hummer is going to have to buy more gas, which the price is increased as well, than a family that drives a fuel-efficient car. Every month the government will automatically send a dividend to offset the cost of the high prices. The people that conserve the most and produced the least amount of carbon emissions will get a bigger dividend than a person who has been producing a large amount of carbon emissions.\n\nThere have been numerous ideas and attempts to reduce the amount of carbon emissions. Policies have been proposed and rejected. One of the policies that has been actually used is known as the cap and trade system. It is currently being used in Europe and has influenced the people living there.\n\nThere are several differences between cap and trade and cap and dividend that ultimately define each of them. A cap is placed on carbon emissions and green house gas (GHG) emissions in both policies. Based on the caps there are carbon emission permits that give the companies the ability to produce more carbon emissions then the cap would permit. These permits are auctioned off to different companies. In the cap and trade system only a set amount are auctioned off and the rest are given away for free. In the cap and dividend system all the permits are auctioned off. In both systems the cost of buying the permits will increase the price of the product made by the companies; oil, electric, and products that make carbon emissions. The thought process behind this is to dissuade the purchase of mass quantities of these products i.e. less carbon/GHG emissions. While the cap and trade system uses the high prices to control the amount of carbon emissions they do not provide a good incentive to limit carbon emission. Cap and dividend uses the dividends to reward the people that conserve the most. This will benefit the poor the most because of their living situations and this is found to be a problem with lots of people critiquing the policy.\n\n\n"}
{"id": "32000094", "url": "https://en.wikipedia.org/wiki?curid=32000094", "title": "Compensatory growth (organism)", "text": "Compensatory growth (organism)\n\nCompensatory growth, known as catch-up growth and compensatory gain, is an accelerated growth of an organism following a period of slowed development, particularly as a result of nutrient deprivation. The growth may be with respect to weight or length (or height in humans). For example, oftentimes the body weights of animals who experience nutritional restriction will over time become similar to those of animals who did not experience such stress. It is possible for high compensatory growth rates to result in overcompensation, where the organism exceeds normal weight and often has excessive fat deposition.\n\nAn organism can recover to normal weight without additional time. Sometimes when the nutrient restriction is severe, the growth period is extended to reach the normal weight. If the nutrient restriction is severe enough, the organism may have permanent stunted growth where it does not ever reach normal weight. Usually in animals, complete recovery from carbohydrate and protein restriction occurs.\n\nCompensatory growth has been observed in a number of organisms including humans, other species of mammals, birds, reptiles, fish, plants (especially grasses and young tree seedlings and saplings), fungi, microbes, and damselflies.\n\nIn 1911, Hans Aron performed the earliest study of growth after periods of undernourishment. He underfed a dog and found that it still had the capacity to rapidly gain weight, though it did not reach the final weight of a dog that was fed normally. In 1915, Osborne and Mendel were the first to demonstrate that rats fed after growth restriction had an accelerated growth rate. In 1945, Brody developed the idea of “homoestasis of growth” in the book \"Bioenergetics and Growth\". In 1955, Verle Bohman was the first to use the term “compensatory growth” in an article pertaining to beef cattle.\n\nIn animals, homeostatic and homeorhetic processes are involved in the abnormally high growth rates. Homeostatic processes usually affect compensatory growth in the short term, whereas homeorhetic processes usually have a long-term effect.\n\nThe exact biological mechanisms for compensatory growth are poorly understood, though it is clear that in some animals the endocrine system is involved in the metabolism and nutrient partitioning in the tissues.\nFirst, during nutrient starvation, a reduction of basal metabolism takes place. The gut tissues are the first tissues to be reduced in weight and activity. Then, during the realimentation (re-feeding) phase, an increase in feeding enables more dietary protein and energy to be contributed for tissue growth instead of basal metabolism. The gut tissues are the first to increase in weight, followed by muscle tissue and finally adipose tissue.\n\nAnorexia nervosa can have serious implications if its duration and severity are significant and if onset occurs before the completion of growth, pubertal maturation or prior to attaining peak bone mass. Both height gain and pubertal development are dependent on the release of growth hormone and gonadotrophins (LH and FSH) from the pituitary gland. Suppression of gonadotropins in patients with anorexia nervosa has been frequently documented. In some cases, especially where onset is pre-pubertal, physical consequences such as stunted growth and pubertal delay are usually fully reversible. Height potential is normally preserved if the duration and severity of anorexia nervosa are not significant and/or if the illness is accompanied with delayed bone age (especially prior to a bone age of approximately 15 years), as hypogonadism may negate the deleterious effects of undernutrition on stature by allowing for a longer duration of growth compared to controls. In such cases, appropriate early treatment can preserve height potential and may even help to increase it in some post-anorexic subjects due to the aforementioned reasons in addition to factors such as long-term reduced estrogen-producing adipose tissue levels compared to premorbid levels.\n\nIn 1960, Wilson and Osborne outlined six factors that could affect compensatory growth in a review article. The importance of each, some, or all of these factors is not well understood. These factors are as follows:\n\nAnimal factors that can affect compensatory growth may include the maturity level and fat proportion of the animal at the time of nutrient deprivation, the genotype, the gender, and the metabolic changes. The stage of development of the animal when the nutrient restriction occurs greatly affects its body composition.\n\n"}
{"id": "11219282", "url": "https://en.wikipedia.org/wiki?curid=11219282", "title": "Costa v ENEL", "text": "Costa v ENEL\n\nFlaminio Costa v ENEL (1964) Case 6/64 was a landmark decision of the European Court of Justice which established the primacy of European Union law (then Community law) over the laws of its member states'.\n\nMr. Costa was an Italian citizen who had owned shares in an electricity company, Edisonvolta, and opposed the nationalisation of the electricity sector in Italy. He asked to two lower courts in Milano (two different \"Giudice Conciliatore\") to ascertain that the real creditor of his electricity bill (a relatively small amount of money, 1,925 lire) was the nationalised company, Edisonvolta, and not the newly established state company, Enel. He argued that the nationalisation of the electricity industry violated the Treaty of Rome and the Italian Constitution. The first \"Giudice Conciliatore\" of Milan referred the case to the Italian Constitutional Court and the second \"Giudice Conciliatore\" referred it to the European Court of Justice.\n\nThe Italian Constitution Court gave judgement in March 1964, ruling that while the Italian Constitution allowed for the limitation of sovereignty for international organisation like the European Economic Community, it did not upset that normal rule of statutory interpretation that where two statutes conflict the subsequent one prevails (\"lex posterior derogat legi anteriori/priori\"). As a result the Treaty of Rome which was incorporated into Italian law in 1958 could not prevail over the electricity nationalisation law which was enacted in 1962.\n\nIn light of the decision of the constitutional court, the Italian government submitted to the ECJ that the Italian court's request for a preliminary ruling from the ECJ was inadmissible on the grounds that as the Italian court was not empowered to set aside the national law in question, a preliminary ruling would not serve any valid purpose.\n\nThe ECJ held the Treaty of Rome rule on an undistorted market was one on which the Commission alone could challenge the Italian government. As an individual, Costa had no standing to challenge the decision because that Treaty provision had no direct effect. However, Costa could raise a point of EC law against a national government in legal proceeding before the courts in that Member State: EC law would not be effective if Costa could not challenge national law on the basis of its alleged incompatibility with EC law.\n\nThis case is additional confirmation that under Article 267 of the Treaty on the Functioning of the European Union (TFEU) a court has an obligation to refer cases that have reached the highest point of appeal in their respective country if there is a question of the application of EU Law. Costa had reached its highest point of appeal.\n\n\n"}
{"id": "17303574", "url": "https://en.wikipedia.org/wiki?curid=17303574", "title": "Ecological network", "text": "Ecological network\n\nAn ecological network is a representation of the biotic interactions in an ecosystem, in which species (nodes) are connected by pairwise interactions (links). These interactions can be trophic or symbiotic. Ecological networks are used to describe and compare the structures of real ecosystems, while network models are used to investigate the effects of network structure on properties such as ecosystem stability.\n\nHistorically, research into ecological networks developed from descriptions of trophic relationships in aquatic food webs; however, recent work has expanded to look at other food webs as well as webs of mutualists. Results of this work have identified several important properties of ecological networks.\n\nComplexity ([linkage density): the average number of links per species. Explaining the observed high levels of complexity in ecosystems has been one of the main challenges and motivations for ecological network analysis, since early theory predicted that complexity should lead to instability.\n\nConnectance: the proportion of possible links between species that are realized (links/species). In food webs, the level of connectance is related to the statistical distribution of the links per species. The distribution of links changes from (partial) power-law to exponential to uniform as the level of connectance increases. The observed values of connectance in empirical food webs appear to be accountable for by constraints on an organisms diet breadth driven by optimal foraging behaviour.This links the structure of these ecological networks to the behaviour of individual organisms.\n\nDegree distribution: the degree distribution of an ecological network is the cumulative distribution for the number of links each species has. The degree distributions of food webs have been found to display the same universal functional form. The degree distribution can be split into its two component parts, links to a species' prey (aka. in degree) and links to a species' predators (aka- out degree). Both the in degree and out degree distributions display their own universal functional forms. As there is a faster decay of the out-degree distribution than the in degree distribution we can expect that on average in a food web a species will have more in links than out links.\n\nClustering: the proportion of species that are directly linked to a focal species. A focal species in the middle of a cluster may be a keystone species, and its loss could have large effects on the network.\n\nCompartmentalization: the division of the network into relatively independent sub-networks. Some ecological networks have been observed to be compartmentalized by body size and by spatial location. Evidence also exists which suggests that compartmentilization in food webs appears to result from patterns of species' diet contiguity and adaptive foraging \n\nNestedness: the degree to which species with few links have a sub-set of the links of other species, rather than a different set of links. In highly nested networks, guilds of species that share an ecological niche contain both generalists (species with many links) and specialists (species with few links, all shared with the generalists). In mutualistic networks, nestedness is often asymmetrical, with specialists of one guild linked to the generalists of the partner guild. The level of nestedness is determined not by species features but overall network depictors (e.g. network size and connectance) and can be predicted by a dynamic adaptive model with species rewiring to maximize individual fitness or the fitness of the whole community.\n\nNetwork motif: Motifs are unique sub-graphs composed of n-nodes found embedded in a network. For instance there exist thirteen unique motif structures containing three species, some of these correspond to familiar interaction modules studied by population ecologists such as food chains, apparent competition, or intraguild predation. Studies investigating motif structures of ecological networks, by examining patterns of under/over representation of certain motifs compared to a random graph, have found that food webs have particular motif structures \n\nTrophic coherence: The tendency of species to specialise on particular trophic levels leads to food webs displaying a significant degree of order in their trophic structure, known as \"trophic coherence\", which in turn has important effects on properties such as stability and prevalence of cycles.\n\nThe relationship between ecosystem complexity and stability is a major topic of interest in ecology. Use of ecological networks makes it possible to analyze the effects of the network properties described above on the stability of an ecosystem. Ecosystem complexity was once thought to reduce stability by enabling the effects of disturbances, such as species loss or species invasion, to spread and amplify through the network. However, other characteristics of network structure have been identified that reduce the spread of indirect effects and thus enhance ecosystem stability. The relationship between complexity and stability can even be inverted in food webs with sufficient trophic coherence, so that increases in biodiversity would make a community more stable rather than less.\n\nInteraction strength may decrease with the number of links between species, damping the effects of any disturbance and cascading extinctions are less likely in compartmentalized networks, as effects of species losses are limited to the original compartment. Furthermore, as long as the most connected species are unlikely to go extinct, network persistence increases with connectance and nestedness. No consensus on the links between network nestedness and community stability in mutualistic species has however been reached among several investigations in recent years. Recent findings suggest that a trade-off between different types of stability may exist. The nested structure of mutualisitic networks was shown to promote the capacity of species to persist under increasingly harsh circumstances. Most likely, because the nested structure of mutualistic networks helps species to indirectly support each other when circumstances are harsh. This indirect facilitation helps species to survive, but it also means that under harsh circumstances one species cannot survive without the support of the other. As circumstances become increasingly harsh, a tipping point may therefore be passed at which the populations of a large number of species may collapse simultaneously.\n\nAdditional applications of ecological networks include exploration of how the community context affects pairwise interactions. The community of species in an ecosystem is expected to affect both the ecological interaction and coevolution of pairs of species. Related, spatial applications are being developed for studying metapopulations, epidemiology, and the evolution of cooperation. In these cases, networks of habitat patches (metapopulations) or individuals (epidemiology, social behavior), make it possible to explore the effects of spatial heterogeneity.\n\n\n\n"}
{"id": "41181", "url": "https://en.wikipedia.org/wiki?curid=41181", "title": "F region", "text": "F region\n\nThe F region of the ionosphere is home to the F layer of ionization, also called the Appleton–Barnett layer, after the English physicist Edward Appleton and New Zealander Miles Barnett. As with other ionospheric sectors, 'layer' implies a concentration of plasma (physics), while 'region' is the volume that contains the said layer. The F region contains ionized gases at a height of around 150–800 km above sea level, placing it in the Earth’s thermosphere, a hot region in the upper atmosphere, and also in the heterosphere, where chemical composition varies with height. Generally speaking, the F region has the highest concentration of free electrons and ions anywhere in the atmosphere. It may be thought of as comprising two layers, the F1-and F2-layers.\n\nThe F-region is located directly above the E region (formerly the Kennelly-Heaviside layer) and below the protonosphere. It acts as a dependable reflector of HF radio signals as it is not affected by atmospheric conditions, although its ionic composition varies with the sunspot cycle. It reflects normal-incident frequencies at or below the critical frequency (approximately 10 MHz) and partially absorbs waves of higher frequency.\n\nThe F1 layer is the lower sector of the F layer and exists from about 150 to 220 km above the surface of the Earth and only during daylight hours. It is composed of a mixture of molecular ions O and NO, and atomic ions O. Above the F1 region, atomic oxygen becomes the dominant constituent because lighter particles tend to occupy higher altitudes above the turbopause (at ~100 km). This atomic oxygen provides the O atomic ions that make up the F2 layer.\nThe F1 layer has approximately 5 × 10 e/cm (free electrons per cubic centimeter) at noontime and minimum sunspot activity, and increases to roughly 2 × 10 e/cm during maximum sunspot activity. The density falls off to below 10 e/cm at night.\n\nCritical F layer frequencies are the ones that will not go through the F layer.\n\n"}
{"id": "53617628", "url": "https://en.wikipedia.org/wiki?curid=53617628", "title": "Fanny Elizabeth de Mole", "text": "Fanny Elizabeth de Mole\n\nFanny Elizabeth de Mole (1 March 1835 - 26 December 1866) was a botanical artist who illustrated the first book of flora from the colony of South Australia.\n\nDe Mole was born in the residential rooms of Merchant Taylors Hall, London. Her father was John Bamber De Mole and her mother Isabel was the daughter of engineer Henry Maudslay. She migrated to South Australia in 1857 to join two of her brothers.\n\nShe wrote and illustrated the first book on wildflowers in that state, \"Wild flowers of South Australia\" in 1861. In order to achieve colour quality which would not weaken through the printing process, lithographs of the illustrations were prepared in England by Paul Jerrard & Son and hand-coloured in Australia. The book contains 20 hand-coloured lithographed plates. There were an estimated 100 copies made of the original edition of \"Wildflowers of South Australia,\" and it is quite difficult to obtain. In 2014 a copy of the book sold at Christie's for $3,750. A facsimile edition of the book was published by Queensbury Hill Press in 1981.\n\nDe Mole won prizes for her paintings at the 1865 exhibition of the South Australian Society of Arts.\n\nFive years after her book was published Fanny Elizabeth de Mole died from tuberculosis at Willunga, South Australia.\n"}
{"id": "3948316", "url": "https://en.wikipedia.org/wiki?curid=3948316", "title": "Grid dip oscillator", "text": "Grid dip oscillator\n\nGrid dip oscillator (GDO), also called grid dip meter, dip meter, dipmeter, or just dipper, is a measuring instrument to measure resonant frequency of radio frequency circuits. It measures the amount of absorption of a high frequency inductively coupled magnetic field by nearby objects. It is an oscillator whose amplitude changes when near a resonant circuit which is tuned to the frequency the oscillator generates. At the heart of the instrument is a tunable LC circuit with a coil that serves as a loose inductive coupling to the measured LC resonant circuit. Resonance is indicated by a dip in the meter indicator on the device, usually based on a microammeter.\n\nGrid dip oscillators were first developed in the 1920s, and were built with vacuum tubes. The devices measured the value of the tube's grid current. Modern grid dip oscillators are solid-state devices and are more versatile. Solid-state versions of the grid dip oscillator are sometimes called gate dip oscillators or emitter dip oscillators in reference to the parts of the semiconductor whose current is being measured instead of the grid current on a vacuum tube.\n\nCentral to the circuit is a variable frequency oscillator with interchangeable coil and tuning capacitors (ganged) as shown. When the oscillator's coil is in the vicinity of another resonant circuit, the pair behaves as a low Q transformer, especially if the respective resonant frequencies match; this affects the frequency and amplitude of oscillation of the GDO, which is sensed in any of several ways, the simplest of which is to attach an ammeter.\n\nGrid dip oscillators have been widely used by amateur radio operators for measuring the properties of resonant circuits, filters, and antennas.\n\n\n"}
{"id": "51408006", "url": "https://en.wikipedia.org/wiki?curid=51408006", "title": "Haze (optics)", "text": "Haze (optics)\n\nThere are two different types of haze that can occur in materials:\n\nThe measurement and control of both types during manufacture is essential to ensure optimum quality, acceptability and suitability for purpose of the product.\nFor instance, in automotive manufacturing, a high quality reflective appearance is desirable with low reflection haze and high contrast whilst in packaging clear, low haze, highly transmissive films are required so that the contents, foods etc., can be clearly observed.\n\nReflection Haze is an optical phenomenon usually associated with high gloss surfaces, it is a common surface problem that can affect appearance quality. The reflection from an ideal high gloss surface should be clear and radiant, however due to imperfections in the surface caused by microscopic structures or textures (≈ 0.01mm wavelength) the reflection can appear milky or hazy reducing the quality of its overall appearance.\nCauses of this could be due to a number of factors –\n\nA high gloss surface with haze exhibits a milky finish with low reflective contrast- reflected highlights and lowlights are less pronounced.\nOn surfaces with haze, halos are visible around the reflections of strong light sources.\n\nMeasurement of reflection haze is primarily defined under three International test standards:\n\nASTM E430 \ncomprising three test methods:\n\nTest method A specifies a 30° angle for specular gloss measurement, 28° or 32° for narrow-angle reﬂection haze measurement and 25° or 35° for wide-angle reﬂection haze measurement.\n\nTest method B specifies a 20° angle for specular gloss measurement and 18.1° and 21.9° for narrow-angle reﬂection haze measurement.\n\nTest method C specifies a 30° angle for specular gloss measurement, 28° or 32° for narrow-angle reﬂection haze measurement and 15° wide-angle reﬂection haze measurement.\n\nASTM D4039 \n\nTest method specifies gloss measurements to be made at 20° and 60°, the haze index is then calculated as the difference between the 60° and 20° measurements.\n\nISO 13803 \n\nTest method specifies a 20° angle for specular gloss measurement and 18.1° and 21.9° for narrow-angle reﬂection haze measurement.\n\nAll test methods specify that measurements should be made with visible light according to CIE spectral luminous efficiency function V(λ) in the CIE 1931 standard observer and CIE standard illuminant C.\n\nAs most commercially available glossmeters have gloss measurement angles of 20°, 60° and 85° haze measurement is incorporated at either 20° (ISO 13803 / ASTM E430 method B) or at 20° and 60° ( ASTM D4039). There are however some manufacturers that offer glossmeters with measurement angles of 30° and haze measurement in accordance with ASTM E430 Method A and C but are fewer in number, therefore for the purposes of detailing haze measurement theory only the first three methods will be included.\nISO 13803 / ASTM E430 method B\n\nBoth test methods measure specular gloss and haze together at 20° that means light is transmitted and received at an equal but opposite angle of 20°. \n\nSpecular gloss is measured over an angular range that is limited by aperture dimensions as defined in ASTM Test Method D523. The angular measurement range for this at 20° is ±0.9° (19.1° - 20.9°). For haze measurement additional sensors are used either side of this range at 18.1° and 21.9° to measure the intensity of the scattered light. Both solid colours and those containing metallics can be measured using this method provided haze compensation is used (as detailed later).\n\nASTM D4039\n\nThis method can only be used on nonmetallic materials having a 60° specular gloss value greater than 70 in accordance with ASTM Test Method D523 / ISO 2813. Haze Index is calculated from gloss measurements made at 20 and 60 degrees as the difference between the two measurements (HI = G60-G20).\n\nAs measurements of specular gloss depend largely on the refractive index of the material being measured 20° gloss will change more noticeably than 60° gloss, therefore as haze index is calculated using these two measurements it too will be affected by the refractive index of the material. Evaluations of reflection haze using this test method are therefore confined to samples of roughly the same refractive index.\nHaze compensation\n\nIt is important to note that the colour (luminous reflectance) of a material can greatly influence the measurement of reflection haze. As colour and haze are both components of scattered light (diffuse reflectance) they must be separated so that only the haze value is quantified; this is also true for metallics or coatings containing metallic pigments where a higher scattering exists.\n\nAs test method ASTM D4039 is only suitable for nonmetallic materials of more or less the same refractive index separation of the colour and haze components is not detailed. Haze index calculations and measurements using this test method will therefore produce higher haze results on brighter coloured materials than darker with the same level of haze present. The chart below shows these differences for various colours:-\n\nBoth ISO 13803 and ASTM E430 method B require a separate measurement of luminous reﬂectance, Y, to calculate compensated haze. The tri-stimulus value Y gives a measure of the lightness of the material as defined in ISO 7724-2 requiring a 45°/0° geometry to be used with standard illuminant C and 2° observer (although it is mentioned that slightly different conditions will not result in significant errors). Luminous reflectance measurements, Y, are required on both the sample material and a reference white; ISO 13803 details the use of a BaSO4 standard - Barium sulphate, a white crystalline solid having a white opaque appearance and high density as this material is a good substitute for a perfectly reflecting diffusor as defined under ISO 7724-2.\n\nCompensated haze can then be calculated as -\n\nH Comp = H Linear – Y Sample / Y BaSO4\nUsing the ISO / ASTM method therefore to measure luminous reflectance produces a reliable measurement of Y for non-metallic surfaces as the diffuse component is lambertian, i.e. it is equal in amplitude at all angles in relation to the sample surface. \n\nHowever for metallic coatings and those containing speciality pigments, as the particles within the coating reflect the light directionally around the specular angle, little or no metallic reflection is present at the angle at which the luminosity is measured, therefore these types of coatings have an unexpectedly high haze reading. Using a measurement angle which is closer to the region adjacent to the haze angle has proven successful in providing compatible readings on solid colours and also compensating for directional reflection from metallic coatings and speciality pigments\nGenerally measurement of reflection haze is confined to high gloss paints and coatings and highly polished metals. Although there has been some degree of success using this measurement method for films it has proven unreliable due to variability caused by changes in the film thickness (internal refraction variations) and the background colour on which the film sample is placed. Generally haze measurement of films is performed using a transmission type hazemeter as described hereafter.\n\nLight and transparent materials\n\nWhen light strikes the surface of a transparent material the following interactions occur –\n• Light is reflected from the front surface of the material\n\n• Some light is refracted within the material (depending on thickness) and reflected from the second surface \n\n• Light passes through the material at an angle which is determined by the refractive index of the material and the angle of illumination.\nThe light that passes through the transparent material can be affected by irregularities within it; these can include poorly dispersed particles, contaminants (i.e. dust particles) and/or air spaces. This causes the light to scatter in different directions from the normal the degree of which being related to the size and number of irregularities present. Small irregularities cause the light to scatter, or diffuse, in all directions whilst large ones cause the light to be scattered forward in a narrow cone shape. These two types of scattering behaviour are known as Wide Angle Scattering, which causes haze due to the loss of transmissive contrast, and Narrow Angle Scattering a measure of clarity or the “see through quality” of the material based on a reduction of sharpness.\n\nThese factors are therefore important for defining the transmitting properties of a transparent material-\nTransmission – The amount of light that passes through the material without being scattered\n\nHaze – The amount of light that is subject to Wide Angle Scattering (At an angle greater than 2.5° from normal (ASTM D1003))\n\nClarity – The amount of light that is subject to Narrow Area Scattering (At an angle less than 2.5° from normal)\n\nMeasurement of these factors is defined in two International test standards-\nASTM D1003 – Comprising two test methods –\n\nProcedure A – using a Hazemeter\n\nProcedure B – using a Spectrophotometer\nBS EN ISO 13468 Parts 1 and 2 \n\nPart 1 – Using a single beam Hazemeter\n\nPart 2 – Using a dual beam Hazemeter\nThe test methods specify the use of a Hazemeter as shown below -\n\nA collimated beam of light from a light source (ASTM D1003 - Illuminant C, BS EN ISO 13468 Parts 1 and 2 - Illuminant D65 ) passes through a sample mounted on the entrance port of an integrating sphere. \n\nThe light, which is uniformly distributed by a matte white highly reflective coating on the sphere walls, is measured by a photodetector positioned at 90° from the entrance port. A baffle mounted between the photodetector and the entrance port prevents direct exposure from the port.\n\nThe exit port immediately opposite the entrance port contains a light trap to absorb all light from the light source when no sample is present. A shutter in this exit port coated with the same coating as the sphere walls allows the port to be opened and closed as required. \n\nTotal transmittance is measured with the exit port closed.\n\nTransmittance haze is measured with the exit port open. \n\nCommercially available Hazemeters of this type perform both measurements automatically, the only operator interaction being the placement of the sample material on the measurement (entrance) port of the device.\n\n"}
{"id": "2932854", "url": "https://en.wikipedia.org/wiki?curid=2932854", "title": "High Earth orbit", "text": "High Earth orbit\n\nA high Earth orbit is a geocentric orbit with an altitude entirely above that of a geosynchronous orbit (). The orbital periods of such orbits are greater than 24 hours, therefore satellites in such orbits have an apparent retrograde motion – that is, even if they are in a prograde orbit (90° > inclination ≥ 0°), their orbital velocity is lower than Earth's rotational speed, causing their ground track to move westward on Earth's surface.\n"}
{"id": "40203", "url": "https://en.wikipedia.org/wiki?curid=40203", "title": "Hubble Space Telescope", "text": "Hubble Space Telescope\n\nThe Hubble Space Telescope (HST) is a space telescope that was launched into low Earth orbit in 1990 and remains in operation. Although not the first space telescope, Hubble is one of the largest and most versatile and is well known as both a vital research tool and a public relations boon for astronomy. The HST is named after the astronomer Edwin Hubble and is one of NASA's Great Observatories, along with the Compton Gamma Ray Observatory, the Chandra X-ray Observatory and the Spitzer Space Telescope.\n\nWith a mirror, Hubble's four main instruments observe in the near ultraviolet, visible, and near infrared spectra. Hubble's orbit outside the distortion of Earth's atmosphere allows it to take extremely high-resolution images, with substantially lower background light than ground-based telescopes. Hubble has recorded some of the most detailed visible light images ever, allowing a deep view into space and time. Many Hubble observations have led to breakthroughs in astrophysics, such as accurately determining the rate of expansion of the universe.\n\nThe HST was built by the United States space agency NASA, with contributions from the European Space Agency. The Space Telescope Science Institute (STScI) selects Hubble's targets and processes the resulting data, while the Goddard Space Flight Center controls the spacecraft.\n\nSpace telescopes were proposed as early as 1923. Hubble was funded in the 1970s, with a proposed launch in 1983, but the project was beset by technical delays, budget problems, and the \"Challenger\" disaster (1986). When finally launched in 1990, Hubble's main mirror was found to have spherical aberration, compromising the telescope's capabilities. The optics were corrected to their intended quality by a servicing mission in 1993.\n\nHubble is the only telescope designed to be serviced in space by astronauts. After launch by in 1990, five subsequent Space Shuttle missions repaired, upgraded, and replaced systems on the telescope, including all five of the main instruments. The fifth mission was initially canceled on safety grounds following the \"Columbia\" disaster (2003). However, after spirited public discussion, NASA administrator Mike Griffin approved the fifth servicing mission, completed in 2009. The telescope is operating , and could last until 2030–2040. Its scientific successor, the James Webb Space Telescope (JWST), is scheduled for launch in March 2021.\n\nIn 1923, Hermann Oberth—considered a father of modern rocketry, along with Robert H. Goddard and Konstantin Tsiolkovsky—published \"\" (\"The Rocket into Planetary Space\"), which mentioned how a telescope could be propelled into Earth orbit by a rocket.\n\nThe history of the Hubble Space Telescope can be traced back as far as 1946, to the astronomer Lyman Spitzer's paper \"Astronomical advantages of an extraterrestrial observatory\". In it, he discussed the two main advantages that a space-based observatory would have over ground-based telescopes. First, the angular resolution (the smallest separation at which objects can be clearly distinguished) would be limited only by diffraction, rather than by the turbulence in the atmosphere, which causes stars to twinkle, known to astronomers as seeing. At that time ground-based telescopes were limited to resolutions of 0.5–1.0 arcseconds, compared to a theoretical diffraction-limited resolution of about 0.05 arcsec for a telescope with a mirror in diameter. Second, a space-based telescope could observe infrared and ultraviolet light, which are strongly absorbed by the atmosphere.\n\nSpitzer devoted much of his career to pushing for the development of a space telescope. In 1962, a report by the US National Academy of Sciences recommended the development of a space telescope as part of the space program, and in 1965 Spitzer was appointed as head of a committee given the task of defining scientific objectives for a large space telescope.\n\nSpace-based astronomy had begun on a very small scale following World War II, as scientists made use of developments that had taken place in rocket technology. The first ultraviolet spectrum of the Sun was obtained in 1946, and the National Aeronautics and Space Administration (NASA) launched the Orbiting Solar Observatory (OSO) to obtain UV, X-ray, and gamma-ray spectra in 1962. An orbiting solar telescope was launched in 1962 by the United Kingdom as part of the Ariel space program, and in 1966 NASA launched the first Orbiting Astronomical Observatory (OAO) mission. OAO-1's battery failed after three days, terminating the mission. It was followed by OAO-2, which carried out ultraviolet observations of stars and galaxies from its launch in 1968 until 1972, well beyond its original planned lifetime of one year.\n\nThe OSO and OAO missions demonstrated the important role space-based observations could play in astronomy, and in 1968, NASA developed firm plans for a space-based reflecting telescope with a mirror in diameter, known provisionally as the Large Orbiting Telescope or Large Space Telescope (LST), with a launch slated for 1979. These plans emphasized the need for manned maintenance missions to the telescope to ensure such a costly program had a lengthy working life, and the concurrent development of plans for the reusable Space Shuttle indicated that the technology to allow this was soon to become available.\n\nThe continuing success of the OAO program encouraged increasingly strong consensus within the astronomical community that the LST should be a major goal. In 1970, NASA established two committees, one to plan the engineering side of the space telescope project, and the other to determine the scientific goals of the mission. Once these had been established, the next hurdle for NASA was to obtain funding for the instrument, which would be far more costly than any Earth-based telescope. The U.S. Congress questioned many aspects of the proposed budget for the telescope and forced cuts in the budget for the planning stages, which at the time consisted of very detailed studies of potential instruments and hardware for the telescope. In 1974, public spending cuts led to Congress deleting all funding for the telescope project.\n\nIn response a nationwide lobbying effort was coordinated among astronomers. Many astronomers met congressmen and senators in person, and large scale letter-writing campaigns were organized. The National Academy of Sciences published a report emphasizing the need for a space telescope, and eventually the Senate agreed to half of the budget that had originally been approved by Congress.\n\nThe funding issues led to something of a reduction in the scale of the project, with the proposed mirror diameter reduced from 3 m to 2.4 m, both to cut costs and to allow a more compact and effective configuration for the telescope hardware. A proposed precursor space telescope to test the systems to be used on the main satellite was dropped, and budgetary concerns also prompted collaboration with the European Space Agency. ESA agreed to provide funding and supply one of the first generation instruments for the telescope, as well as the solar cells that would power it, and staff to work on the telescope in the United States, in return for European astronomers being guaranteed at least 15% of the observing time on the telescope. Congress eventually approved funding of for 1978, and the design of the LST began in earnest, aiming for a launch date of 1983. In 1983 the telescope was named after Edwin Hubble, who made one of the greatest scientific breakthroughs of the 20th century when he discovered that the universe is expanding.\n\nOnce the Space Telescope project had been given the go-ahead, work on the program was divided among many institutions. Marshall Space Flight Center (MSFC) was given responsibility for the design, development, and construction of the telescope, while Goddard Space Flight Center was given overall control of the scientific instruments and ground-control center for the mission. MSFC commissioned the optics company Perkin-Elmer to design and build the Optical Telescope Assembly (OTA) and Fine Guidance Sensors for the space telescope. Lockheed was commissioned to construct and integrate the spacecraft in which the telescope would be housed.\n\nOptically, the HST is a Cassegrain reflector of Ritchey–Chrétien design, as are most large professional telescopes. This design, with two hyperbolic mirrors, is known for good imaging performance over a wide field of view, with the disadvantage that the mirrors have shapes that are hard to fabricate and test. The mirror and optical systems of the telescope determine the final performance, and they were designed to exacting specifications. Optical telescopes typically have mirrors polished to an accuracy of about a tenth of the wavelength of visible light, but the Space Telescope was to be used for observations from the visible through the ultraviolet (shorter wavelengths) and was specified to be diffraction limited to take full advantage of the space environment. Therefore, its mirror needed to be polished to an accuracy of , or about 1/65 of the wavelength of red light. On the long wavelength end, the OTA was not designed with optimum IR performance in mind—for example, the mirrors are kept at stable (and warm, about 15 °C) temperatures by heaters. This limits Hubble's performance as an infrared telescope.\n\nPerkin-Elmer intended to use custom-built and extremely sophisticated computer-controlled polishing machines to grind the mirror to the required shape. However, in case their cutting-edge technology ran into difficulties, NASA demanded that PE sub-contract to Kodak to construct a back-up mirror using traditional mirror-polishing techniques. (The team of Kodak and Itek also bid on the original mirror polishing work. Their bid called for the two companies to double-check each other's work, which would have almost certainly caught the polishing error that later caused such problems.) The Kodak mirror is now on permanent display at the National Air and Space Museum. An Itek mirror built as part of the effort is now used in the 2.4 m telescope at the Magdalena Ridge Observatory.\n\nConstruction of the Perkin-Elmer mirror began in 1979, starting with a blank manufactured by Corning from their ultra-low expansion glass. To keep the mirror's weight to a minimum it consisted of top and bottom plates, each thick, sandwiching a honeycomb lattice. Perkin-Elmer simulated microgravity by supporting the mirror from the back with 130 rods that exerted varying amounts of force. This ensured that the mirror's final shape would be correct and to specification when finally deployed. Mirror polishing continued until May 1981. NASA reports at the time questioned Perkin-Elmer's managerial structure, and the polishing began to slip behind schedule and over budget. To save money, NASA halted work on the back-up mirror and put the launch date of the telescope back to October 1984. The mirror was completed by the end of 1981; it was washed using of hot, deionized water and then received a reflective coating of aluminum and a protective coating of magnesium fluoride.\n\nDoubts continued to be expressed about Perkin-Elmer's competence on a project of this importance, as their budget and timescale for producing the rest of the OTA continued to inflate. In response to a schedule described as \"unsettled and changing daily\", NASA postponed the launch date of the telescope until April 1985. Perkin-Elmer's schedules continued to slip at a rate of about one month per quarter, and at times delays reached one day for each day of work. NASA was forced to postpone the launch date until March and then September 1986. By this time, the total project budget had risen to .\n\nThe spacecraft in which the telescope and instruments were to be housed was another major engineering challenge. It would have to withstand frequent passages from direct sunlight into the darkness of Earth's shadow, which would cause major changes in temperature, while being stable enough to allow extremely accurate pointing of the telescope. A shroud of multi-layer insulation keeps the temperature within the telescope stable and surrounds a light aluminum shell in which the telescope and instruments sit. Within the shell, a graphite-epoxy frame keeps the working parts of the telescope firmly aligned. Because graphite composites are hygroscopic, there was a risk that water vapor absorbed by the truss while in Lockheed's clean room would later be expressed in the vacuum of space; resulting in the telescope's instruments being covered by ice. To reduce that risk, a nitrogen gas purge was performed before launching the telescope into space.\n\nWhile construction of the spacecraft in which the telescope and instruments would be housed proceeded somewhat more smoothly than the construction of the OTA, Lockheed still experienced some budget and schedule slippage, and by the summer of 1985, construction of the spacecraft was 30% over budget and three months behind schedule. An MSFC report said that Lockheed tended to rely on NASA directions rather than take their own initiative in the construction.\n\nThe two initial, primary computers on the HST were the 1.25 MHz DF-224 system, built by Rockwell Autonetics, which contained three redundant CPUs, and two redundant NSSC-1 (NASA Standard Spacecraft Computer, Model 1) systems, developed by Westinghouse and GSFC using diode–transistor logic (DTL). A co-processor for the DF-224 was added during Servicing Mission 1 in 1993, which consisted of two redundant strings of an Intel-based 80386 processor with an 80387 math co-processor. The DF-224 and its 386 co-processor were replaced by a 25 MHz Intel-based 80486 processor system during Servicing Mission 3A in 1999.\n\nAdditionally, some of the science instruments and components had their own embedded microprocessor-based control systems. The MATs (Multiple Access Transponder) components, MAT-1 and MAT-2, utilize Hughes Aircraft CDP1802CD microprocessors. The Wide Field and Planetary Camera (WFPC) also utilized an RCA 1802 microprocessor (or possibly the older 1801 version). The WFPC-1 was replaced by the WFPC-2 during Servicing Mission 1 in 1993, which was then replaced by the Wide Field Camera 3 (WFC3) during Servicing Mission 4 in 2009.\n\nWhen launched, the HST carried five scientific instruments: the Wide Field and Planetary Camera (WF/PC), Goddard High Resolution Spectrograph (GHRS), High Speed Photometer (HSP), Faint Object Camera (FOC) and the Faint Object Spectrograph (FOS). WF/PC was a high-resolution imaging device primarily intended for optical observations. It was built by NASA's Jet Propulsion Laboratory, and incorporated a set of 48 filters isolating spectral lines of particular astrophysical interest. The instrument contained eight charge-coupled device (CCD) chips divided between two cameras, each using four CCDs. Each CCD has a resolution of 0.64 megapixels. The \"wide field camera\" (WFC) covered a large angular field at the expense of resolution, while the \"planetary camera\" (PC) took images at a longer effective focal length than the WF chips, giving it a greater magnification.\n\nThe GHRS was a spectrograph designed to operate in the ultraviolet. It was built by the Goddard Space Flight Center and could achieve a spectral resolution of 90,000. Also optimized for ultraviolet observations were the FOC and FOS, which were capable of the highest spatial resolution of any instruments on Hubble. Rather than CCDs these three instruments used photon-counting digicons as their detectors. The FOC was constructed by ESA, while the University of California, San Diego, and Martin Marietta Corporation built the FOS.\n\nThe final instrument was the HSP, designed and built at the University of Wisconsin–Madison. It was optimized for visible and ultraviolet light observations of variable stars and other astronomical objects varying in brightness. It could take up to 100,000 measurements per second with a photometric accuracy of about 2% or better.\n\nHST's guidance system can also be used as a scientific instrument. Its three Fine Guidance Sensors (FGS) are primarily used to keep the telescope accurately pointed during an observation, but can also be used to carry out extremely accurate astrometry; measurements accurate to within 0.0003 arcseconds have been achieved.\n\nThe Space Telescope Science Institute (STScI) is responsible for the scientific operation of the telescope and the delivery of data products to astronomers. STScI is operated by the Association of Universities for Research in Astronomy (AURA) and is physically located in Baltimore, Maryland on the Homewood campus of Johns Hopkins University, one of the 39 US universities and seven international affiliates that make up the AURA consortium. STScI was established in 1981 after something of a power struggle between NASA and the scientific community at large. NASA had wanted to keep this function in-house, but scientists wanted it to be based in an academic establishment. The Space Telescope European Coordinating Facility (ST-ECF), established at Garching bei München near Munich in 1984, provided similar support for European astronomers until 2011, when these activities were moved to the European Space Astronomy Centre.\n\nOne rather complex task that falls to STScI is scheduling observations for the telescope. Hubble is in a low-Earth orbit to enable servicing missions, but this means that most astronomical targets are occulted by the Earth for slightly less than half of each orbit. Observations cannot take place when the telescope passes through the South Atlantic Anomaly due to elevated radiation levels, and there are also sizable exclusion zones around the Sun (precluding observations of Mercury), Moon and Earth. The solar avoidance angle is about 50°, to keep sunlight from illuminating any part of the OTA. Earth and Moon avoidance keeps bright light out of the FGSs, and keeps scattered light from entering the instruments. If the FGSs are turned off, however, the Moon and Earth can be observed. Earth observations were used very early in the program to generate flat-fields for the WFPC1 instrument. There is a so-called continuous viewing zone (CVZ), at roughly 90° to the plane of Hubble's orbit, in which targets are not occulted for long periods. Due to the precession of the orbit, the location of the CVZ moves slowly over a period of eight weeks. Because the limb of the Earth is always within about 30° of regions within the CVZ, the brightness of scattered earthshine may be elevated for long periods during CVZ observations.\n\nHubble orbits in low Earth orbit at an altitude of approximately and an inclination of 28.5°. The position along its orbit changes over time in a way that is not accurately predictable. The density of the upper atmosphere varies according to many factors, and this means that Hubble's predicted position for six weeks' time could be in error by up to . Observation schedules are typically finalized only a few days in advance, as a longer lead time would mean there was a chance that the target would be unobservable by the time it was due to be observed.\n\nEngineering support for HST is provided by NASA and contractor personnel at the Goddard Space Flight Center in Greenbelt, Maryland, south of the STScI. Hubble's operation is monitored 24 hours per day by four teams of flight controllers who make up Hubble's Flight Operations Team.\n\nBy early 1986, the planned launch date of October that year looked feasible, but the \"Challenger\" accident brought the U.S. space program to a halt, grounding the Space Shuttle fleet and forcing the launch of Hubble to be postponed for several years. The telescope had to be kept in a clean room, powered up and purged with nitrogen, until a launch could be rescheduled. This costly situation (about per month) pushed the overall costs of the project even higher. This delay did allow time for engineers to perform extensive tests, swap out a possibly failure-prone battery, and make other improvements. Furthermore, the ground software needed to control Hubble was not ready in 1986, and was barely ready by the 1990 launch.\n\nEventually, following the resumption of shuttle flights in 1988, the launch of the telescope was scheduled for 1990. On April 24, 1990, successfully launched the telescope into its planned orbit during the STS-31 mission.\n\nFrom its original total cost estimate of about , the telescope cost about by the time of its launch. Hubble's cumulative costs were estimated to be about in 2010, twenty years after launch.\n\nHubble accommodates five science instruments at a given time, plus the Fine Guidance Sensors, which are mainly used for aiming the telescope but are occasionally used for science (astrometry). Early instruments were replaced with more advanced ones during the Shuttle servicing missions. COSTAR was strictly a corrective optics device rather than a true science instrument, but occupied one of the five instrument bays.\n\nSince the final servicing mission in 2009, the four active instruments have been ACS, COS, STIS and WFC3. NICMOS is kept in hibernation, but may be revived if WFC3 were to fail in the future.\n\nOf the former instruments, three (COSTAR, FOS and WFPC2) are displayed in the Smithsonian National Air and Space Museum. The FOC is in the Dornier museum, Germany. The HSP is in the Space Place at the University of Wisconsin–Madison. The first WFPC was dismantled, and some components were then re-used in WFC3. The current location of GHRS is unclear.\n\nWithin weeks of the launch of the telescope, the returned images indicated a serious problem with the optical system. Although the first images appeared to be sharper than those of ground-based telescopes, Hubble failed to achieve a final sharp focus and the best image quality obtained was drastically lower than expected. Images of point sources spread out over a radius of more than one arcsecond, instead of having a point spread function (PSF) concentrated within a circle 0.1 arcsec in diameter as had been specified in the design criteria.\n\nAnalysis of the flawed images showed that the cause of the problem was that the primary mirror had been polished to the wrong shape. Although it was probably the most precisely figured optical mirror ever made, smooth to about , at the perimeter it was too flat by about . This difference was catastrophic, introducing severe spherical aberration, a flaw in which light reflecting off the edge of a mirror focuses on a different point from the light reflecting off its center.\n\nThe effect of the mirror flaw on scientific observations depended on the particular observation—the core of the aberrated PSF was sharp enough to permit high-resolution observations of bright objects, and spectroscopy of point sources was only affected through a sensitivity loss. However, the loss of light to the large, out-of-focus halo severely reduced the usefulness of the telescope for faint objects or high-contrast imaging. This meant that nearly all of the cosmological programs were essentially impossible, since they required observation of exceptionally faint objects. NASA and the telescope became the butt of many jokes, and the project was popularly regarded as a white elephant. For instance, in the 1991 comedy \"\", Hubble was pictured with \"Lusitania\", the \"Hindenburg\", and the Edsel. Nonetheless, during the first three years of the Hubble mission, before the optical corrections, the telescope still carried out a large number of productive observations of less demanding targets. The error was well characterized and stable, enabling astronomers to partially compensate for the defective mirror by using sophisticated image processing techniques such as deconvolution.\n\nA commission headed by Lew Allen, director of the Jet Propulsion Laboratory, was established to determine how the error could have arisen. The Allen Commission found that a reflective null corrector, a testing device used to achieve a properly shaped non-spherical mirror, had been incorrectly assembled—one lens was out of position by . During the initial grinding and polishing of the mirror, Perkin-Elmer analyzed its surface with two conventional refractive null correctors. However, for the final manufacturing step (figuring), they switched to the custom-built reflective null corrector, designed explicitly to meet very strict tolerances. The incorrect assembly of the device resulted in the mirror being ground very precisely but to the wrong shape. A few final tests, using the conventional null correctors, correctly reported spherical aberration. But these results were dismissed, thus missing the opportunity to catch the error, because the reflective null corrector was considered more accurate.\n\nThe commission blamed the failings primarily on Perkin-Elmer. Relations between NASA and the optics company had been severely strained during the telescope construction, due to frequent schedule slippage and cost overruns. NASA found that Perkin-Elmer did not review or supervise the mirror construction adequately, did not assign its best optical scientists to the project (as it had for the prototype), and in particular did not involve the optical designers in the construction and verification of the mirror. While the commission heavily criticized Perkin-Elmer for these managerial failings, NASA was also criticized for not picking up on the quality control shortcomings, such as relying totally on test results from a single instrument.\n\nThe design of the telescope had always incorporated servicing missions, and astronomers immediately began to seek potential solutions to the problem that could be applied at the first servicing mission, scheduled for 1993. While Kodak had ground a back-up mirror for Hubble, it would have been impossible to replace the mirror in orbit, and too expensive and time-consuming to bring the telescope back to Earth for a refit. Instead, the fact that the mirror had been ground so precisely to the wrong shape led to the design of new optical components with exactly the same error but in the opposite sense, to be added to the telescope at the servicing mission, effectively acting as \"spectacles\" to correct the spherical aberration.\n\nThe first step was a precise characterization of the error in the main mirror. Working backwards from images of point sources, astronomers determined that the conic constant of the mirror as built was , instead of the intended . The same number was also derived by analyzing the null corrector used by Perkin-Elmer to figure the mirror, as well as by analyzing interferograms obtained during ground testing of the mirror.\n\nBecause of the way the HST's instruments were designed, two different sets of correctors were required. The design of the Wide Field and Planetary Camera 2, already planned to replace the existing WF/PC, included relay mirrors to direct light onto the four separate charge-coupled device (CCD) chips making up its two cameras. An inverse error built into their surfaces could completely cancel the aberration of the primary. However, the other instruments lacked any intermediate surfaces that could be figured in this way, and so required an external correction device.\n\nThe Corrective Optics Space Telescope Axial Replacement (COSTAR) system was designed to correct the spherical aberration for light focused at the FOC, FOS, and GHRS. It consists of two mirrors in the light path with one ground to correct the aberration. To fit the COSTAR system onto the telescope, one of the other instruments had to be removed, and astronomers selected the High Speed Photometer to be sacrificed. By 2002, all of the original instruments requiring COSTAR had been replaced by instruments with their own corrective optics. COSTAR was removed and returned to Earth in 2009 where it is exhibited at the National Air and Space Museum. The area previously used by COSTAR is now occupied by the Cosmic Origins Spectrograph.\n\nHubble was designed to accommodate regular servicing and equipment upgrades while in orbit. Instruments and limited life items were designed as orbital replacement units. Five servicing missions (SM 1, 2, 3A, 3B, and 4) were flown by NASA space shuttles, the first in December 1993 and the last in May 2009. Servicing missions were delicate operations that began with maneuvering to intercept the telescope in orbit and carefully retrieving it with the shuttle's mechanical arm. The necessary work was then carried out in multiple tethered spacewalks over a period of four to five days. After a visual inspection of the telescope, astronauts conducted repairs, replaced failed or degraded components, upgraded equipment, and installed new instruments. Once work was completed, the telescope was redeployed, typically after boosting to a higher orbit to address the orbital decay caused by atmospheric drag.\n\nAfter the problems with Hubble's mirror were discovered, the first servicing mission assumed greater importance, as the astronauts would need to do extensive work to install corrective optics. The seven astronauts for the mission were trained to use about a hundred specialized tools. SM1 flew aboard \"Endeavour\" in December 1993, and involved installation of several instruments and other equipment over ten days.\n\nMost importantly, the High Speed Photometer was replaced with the COSTAR corrective optics package, and WFPC was replaced with the Wide Field and Planetary Camera 2 (WFPC2) with an internal optical correction system. The solar arrays and their drive electronics were also replaced, as well as four gyroscopes in the telescope pointing system, two electrical control units and other electrical components, and two magnetometers. The onboard computers were upgraded with added coprocessors, and Hubble's orbit was boosted.\n\nOn January 13, 1994, NASA declared the mission a complete success and showed the first sharper images. The mission was one of the most complex performed up until that date, involving five long extra-vehicular activity periods. Its success was a boon for NASA, as well as for the astronomers who now had a more capable space telescope.\n\nServicing Mission 2, flown by \"Discovery\" in February 1997, replaced the GHRS and the FOS with the Space Telescope Imaging Spectrograph (STIS) and the Near Infrared Camera and Multi-Object Spectrometer (NICMOS), replaced an Engineering and Science Tape Recorder with a new Solid State Recorder, and repaired thermal insulation. NICMOS contained a heat sink of solid nitrogen to reduce the thermal noise from the instrument, but shortly after it was installed, an unexpected thermal expansion resulted in part of the heat sink coming into contact with an optical baffle. This led to an increased warming rate for the instrument and reduced its original expected lifetime of 4.5 years to about 2 years.\n\nServicing Mission 3A, flown by \"Discovery\", took place in December 1999, and was a split-off from Servicing Mission 3 after three of the six onboard gyroscopes had failed. The fourth failed a few weeks before the mission, rendering the telescope incapable of performing scientific observations. The mission replaced all six gyroscopes, replaced a Fine Guidance Sensor and the computer, installed a Voltage/temperature Improvement Kit (VIK) to prevent battery overcharging, and replaced thermal insulation blankets. The new computer is 20 times faster, with six times more memory, than the DF-224 it replaced. It increases throughput by moving some computing tasks from the ground to the spacecraft and saves money by allowing the use of modern programming languages.\n\nServicing Mission 3B flown by \"Columbia\" in March 2002 saw the installation of a new instrument, with the FOC (which, except for the Fine Guidance Sensors when used for astrometry, was the last of the original instruments) being replaced by the Advanced Camera for Surveys (ACS). This meant that COSTAR was no longer required, since all new instruments had built-in correction for the main mirror aberration. The mission also revived NICMOS by installing a closed-cycle cooler and replaced the solar arrays for the second time, providing 30 percent more power.\n\nPlans called for Hubble to be serviced in February 2005, but the \"Columbia\" disaster in 2003, in which the orbiter disintegrated on re-entry into the atmosphere, had wide-ranging effects on the Hubble program. NASA Administrator Sean O'Keefe decided that all future shuttle missions had to be able to reach the safe haven of the International Space Station should in-flight problems develop. As no shuttles were capable of reaching both HST and the ISS during the same mission, future crewed service missions were canceled. This decision was assailed by numerous astronomers, who felt that Hubble was valuable enough to merit the human risk. HST's planned successor, the James Webb Telescope (JWST), was not expected to launch until at least 2018. A gap in space-observing capabilities between a decommissioning of Hubble and the commissioning of a successor was of major concern to many astronomers, given the significant scientific impact of HST. The consideration that JWST will not be located in low Earth orbit, and therefore cannot be easily upgraded or repaired in the event of an early failure, only makes these concerns more acute. On the other hand, many astronomers felt strongly that the servicing of Hubble should not take place if the expense were to come from the JWST budget.\n\nIn January 2004, O'Keefe said he would review his decision to cancel the final servicing mission to HST due to public outcry and requests from Congress for NASA to look for a way to save it. The National Academy of Sciences convened an official panel, which recommended in July 2004 that the HST should be preserved despite the apparent risks. Their report urged \"NASA should take no actions that would preclude a space shuttle servicing mission to the Hubble Space Telescope\". In August 2004, O'Keefe asked Goddard Space Flight Center to prepare a detailed proposal for a robotic service mission. These plans were later canceled, the robotic mission being described as \"not feasible\". In late 2004, several Congressional members, led by Senator Barbara Mikulski, held public hearings and carried on a fight with much public support (including thousands of letters from school children across the country) to get the Bush Administration and NASA to reconsider the decision to drop plans for a Hubble rescue mission.\n\nThe nomination in April 2005 of a new NASA Administrator with an engineering rather than accounting background, Michael D. Griffin, changed the situation, as Griffin stated he would consider a manned servicing mission. Soon after his appointment Griffin authorized Goddard to proceed with preparations for a manned Hubble maintenance flight, saying he would make the final decision after the next two shuttle missions. In October 2006 Griffin gave the final go-ahead, and the 11-day mission by \"Atlantis\" was scheduled for October 2008. Hubble's main data-handling unit failed in September 2008, halting all reporting of scientific data until its back-up was brought online on October 25, 2008. Since a failure of the backup unit would leave the HST helpless, the service mission was postponed to incorporate a replacement for the primary unit.\n\nServicing Mission 4, flown by \"Atlantis\" in May 2009, was the last scheduled shuttle mission for HST. SM4 installed the replacement data-handling unit, repaired the ACS and STIS systems, installed improved nickel hydrogen batteries, and replaced other components. SM4 also installed two new observation instruments—Wide Field Camera 3 (WFC3) and the Cosmic Origins Spectrograph (COS)—and the Soft Capture and Rendezvous System, which will enable the future rendezvous, capture, and safe disposal of Hubble by either a crewed or robotic mission. Except for the ACS's High Resolution Channel which could not be repaired and was disabled, the work accomplished during SM4 rendered the telescope fully functional, and it remains so .\n\nSince the start of the program, a number of research projects have been carried out, some of them almost solely with Hubble, others coordinated facilities such as Chandra X-ray Observatory and ESO's Very Large Telescope. Although the Hubble observatory is nearing the end of its life, there are still major projects scheduled for it. One example is the upcoming Frontier Fields program, inspired by the results of Hubble's deep observation of the galaxy cluster Abell 1689.\nIn an August 2013 press release, CANDELS was referred to as \"the largest project in the history of Hubble\". The survey \"aims to explore galactic evolution in the early Universe, and the very first seeds of cosmic structure at less than one billion years after the Big Bang.\" The CANDELS project site describes the survey's goals as the following:\nThe Cosmic Assembly Near-IR Deep Extragalactic Legacy Survey is designed to document the first third of galactic evolution from z = 8 to 1.5 via deep imaging of more than 250,000 galaxies with WFC3/IR and ACS. It will also find the first Type Ia SNe beyond z > 1.5 and establish their accuracy as standard candles for cosmology. Five premier multi-wavelength sky regions are selected; each has multi-wavelength data from Spitzer and other facilities, and has extensive spectroscopy of the brighter galaxies. The use of five widely separated fields mitigates cosmic variance and yields statistically robust and complete samples of galaxies down to 10 solar masses out to z ~ 8.\n\nThe program, officially named \"Hubble Deep Fields Initiative 2012\", is aimed to advance the knowledge of early galaxy formation by studying high-redshift galaxies in blank fields with the help of gravitational lensing to see the \"faintest galaxies in the distant universe.\" The Frontier Fields web page describes the goals of the program being:\nAnyone can apply for time on the telescope; there are no restrictions on nationality or academic affiliation, but funding for analysis is only available to US institutions. Competition for time on the telescope is intense, with about one-fifth of the proposals submitted in each cycle earning time on the schedule.\n\nCalls for proposals are issued roughly annually, with time allocated for a cycle lasting about one year. Proposals are divided into several categories; \"general observer\" proposals are the most common, covering routine observations. \"Snapshot observations\" are those in which targets require only 45 minutes or less of telescope time, including overheads such as acquiring the target. Snapshot observations are used to fill in gaps in the telescope schedule that cannot be filled by regular GO programs.\n\nAstronomers may make \"Target of Opportunity\" proposals, in which observations are scheduled if a transient event covered by the proposal occurs during the scheduling cycle. In addition, up to 10% of the telescope time is designated \"director's discretionary\" (DD) time. Astronomers can apply to use DD time at any time of year, and it is typically awarded for study of unexpected transient phenomena such as supernovae.\n\nOther uses of DD time have included the observations that led to views of the Hubble Deep Field and Hubble Ultra Deep Field, and in the first four cycles of telescope time, observations that were carried out by amateur astronomers.\n\nPublic image processing of Hubble data is encouraged as most of the data in the archives has not been processed into color imagery.\n\nThe first director of STScI, Riccardo Giacconi, announced in 1986 that he intended to devote some of his director discretionary time to allowing amateur astronomers to use the telescope. The total time to be allocated was only a few hours per cycle but excited great interest among amateur astronomers.\n\nProposals for amateur time were stringently reviewed by a committee of amateur astronomers, and time was awarded only to proposals that were deemed to have genuine scientific merit, did not duplicate proposals made by professionals, and required the unique capabilities of the space telescope. Thirteen amateur astronomers were awarded time on the telescope, with observations being carried out between 1990 and 1997. One such study was \"Transition Comets—UV Search for OH\". The very first proposal, \"A Hubble Space Telescope Study of Posteclipse Brightening and Albedo Changes on Io\", was published in \"Icarus\", a journal devoted to solar system studies. A second study from another group of amateurs was also published in \"Icarus\". After that time, however, budget reductions at STScI made the support of work by amateur astronomers untenable, and no additional amateur programs have been carried out.\n\nIn the early 1980s, NASA and STScI convened four panels to discuss key projects. These were projects that were both scientifically important and would require significant telescope time, which would be explicitly dedicated to each project. This guaranteed that these particular projects would be completed early, in case the telescope failed sooner than expected. The panels identified three such projects: 1) a study of the nearby intergalactic medium using quasar absorption lines to determine the properties of the intergalactic medium and the gaseous content of galaxies and groups of galaxies; 2) a medium deep survey using the Wide Field Camera to take data whenever one of the other instruments was being used and 3) a project to determine the Hubble constant within ten percent by reducing the errors, both external and internal, in the calibration of the distance scale.\n\nHubble has helped resolve some long-standing problems in astronomy, while also raising new questions. Some results have required new theories to explain them. Among its primary mission targets was to measure distances to Cepheid variable stars more accurately than ever before, and thus constrain the value of the Hubble constant, the measure of the rate at which the universe is expanding, which is also related to its age. Before the launch of HST, estimates of the Hubble constant typically had errors of up to 50%, but Hubble measurements of Cepheid variables in the Virgo Cluster and other distant galaxy clusters provided a measured value with an accuracy of ±10%, which is consistent with other more accurate measurements made since Hubble's launch using other techniques. The estimated age is now about 13.7 billion years, but before the Hubble Telescope scientists predicted an age ranging from 10 to 20 billion years.\n\nWhile Hubble helped to refine estimates of the age of the universe, it also cast doubt on theories about its future. Astronomers from the High-z Supernova Search Team and the Supernova Cosmology Project used ground-based telescopes and HST to observe distant supernovae and uncovered evidence that, far from decelerating under the influence of gravity, the expansion of the universe may in fact be accelerating. Three members of these two groups have subsequently been awarded Nobel Prizes for their discovery. The cause of this acceleration remains poorly understood; the most common cause attributed is dark energy.\n\nThe high-resolution spectra and images provided by the HST have been especially well-suited to establishing the prevalence of black holes in the nuclei of nearby galaxies. While it had been hypothesized in the early 1960s that black holes would be found at the centers of some galaxies, and astronomers in the 1980s identified a number of good black hole candidates, work conducted with Hubble shows that black holes are probably common to the centers of all galaxies. The Hubble programs further established that the masses of the nuclear black holes and properties of the galaxies are closely related. The legacy of the Hubble programs on black holes in galaxies is thus to demonstrate a deep connection between galaxies and their central black holes.\n\nThe collision of Comet Shoemaker-Levy 9 with Jupiter in 1994 was fortuitously timed for astronomers, coming just a few months after Servicing Mission 1 had restored Hubble's optical performance. Hubble images of the planet were sharper than any taken since the passage of Voyager 2 in 1979, and were crucial in studying the dynamics of the collision of a comet with Jupiter, an event believed to occur once every few centuries.\n\nOther discoveries made with Hubble data include proto-planetary disks (proplyds) in the Orion Nebula; evidence for the presence of extrasolar planets around Sun-like stars; and the optical counterparts of the still-mysterious gamma-ray bursts. HST has also been used to study objects in the outer reaches of the Solar System, including the dwarf planets Pluto and Eris.\n\nA unique window on the Universe enabled by Hubble are the Hubble Deep Field, Hubble Ultra-Deep Field, and Hubble Extreme Deep Field images, which used Hubble's unmatched sensitivity at visible wavelengths to create images of small patches of sky that are the deepest ever obtained at optical wavelengths. The images reveal galaxies billions of light years away, and have generated a wealth of scientific papers, providing a new window on the early Universe. The Wide Field Camera 3 improved the view of these fields in the infrared and ultraviolet, supporting the discovery of some of the most distant objects yet discovered, such as MACS0647-JD.\n\nThe non-standard object SCP 06F6 was discovered by the Hubble Space Telescope in February 2006. During June and July 2012, US astronomers using Hubble discovered a tiny fifth moon moving around icy Pluto.\n\nIn March 2015, researchers announced that measurements of aurorae around Ganymede revealed that the moon has a subsurface ocean. Using Hubble to study the motion of its aurorae, the researchers determined that a large saltwater ocean was helping to suppress the interaction between Jupiter's magnetic field and that of Ganymede. The ocean is estimated to be deep, trapped beneath a ice crust.\n\nOn December 11, 2015, Hubble captured an image of the first-ever predicted reappearance of a supernova, dubbed \"Refsdal\", which was calculated using different mass models of a galaxy cluster whose gravity is warping the supernova's light. The supernova was previously seen in November 2014 behind galaxy cluster MACS J1149.5+2223 as part of Hubble's Frontier Fields program. Astronomers spotted four separate images of the supernova in an arrangement known as an Einstein Cross. The light from the cluster has taken about five billion years to reach Earth, though the supernova exploded some 10 billion years ago. The detection of Refsdal's reappearance served as a unique opportunity for astronomers to test their models of how mass, especially dark matter, is distributed within this galaxy cluster.\n\nOn March 3, 2016, researchers using Hubble data announced the discovery of the farthest known galaxy to date: GN-z11. The Hubble observations occurred on February 11, 2015, and April 3, 2015, as part of the CANDELS/GOODS-North surveys.\n\nMany objective measures show the positive impact of Hubble data on astronomy. Over 15,000 papers based on Hubble data have been published in peer-reviewed journals, and countless more have appeared in conference proceedings. Looking at papers several years after their publication, about one-third of all astronomy papers have no citations, while only 2% of papers based on Hubble data have no citations. On average, a paper based on Hubble data receives about twice as many citations as papers based on non-Hubble data. Of the 200 papers published each year that receive the most citations, about 10% are based on Hubble data.\n\nAlthough the HST has clearly helped astronomical research, its financial cost has been large. A study on the relative astronomical benefits of different sizes of telescopes found that while papers based on HST data generate 15 times as many citations as a ground-based telescope such as the William Herschel Telescope, the HST costs about 100 times as much to build and maintain.\n\nDeciding between building ground- versus space-based telescopes is complex. Even before Hubble was launched, specialized ground-based techniques such as aperture masking interferometry had obtained higher-resolution optical and infrared images than Hubble would achieve, though restricted to targets about 10 times brighter than the faintest targets observed by Hubble. Since then, advances in adaptive optics have extended the high-resolution imaging capabilities of ground-based telescopes to the infrared imaging of faint objects. The usefulness of adaptive optics versus HST observations depends strongly on the particular details of the research questions being asked. In the visible bands, adaptive optics can only correct a relatively small field of view, whereas HST can conduct high-resolution optical imaging over a wide field. Only a small fraction of astronomical objects are accessible to high-resolution ground-based imaging; in contrast Hubble can perform high-resolution observations of any part of the night sky, and on objects that are extremely faint.\n\nIn addition to its scientific results, Hubble has also made significant contributions to aerospace engineering, in particular the performance of systems in low Earth orbit. These insights result from Hubble's long lifetime on orbit, extensive instrumentation, and return of assemblies to the Earth where they can be studied in detail. In particular, Hubble has contributed to studies of the behavior of graphite composite structures in vacuum, optical contamination from residual gas and human servicing, radiation damage to electronics and sensors, and the long term behavior of multi-layer insulation. One lesson learned was that gyros assembled using pressurized oxygen to deliver suspension fluid were prone to failure due to electric wire corrosion. Gyros are now assembled using pressurized nitrogen.\n\nHubble data was initially stored on the spacecraft. When launched, the storage facilities were old-fashioned reel-to-reel tape recorders, but these were replaced by solid state data storage facilities during servicing missions 2 and 3A. About twice daily, the Hubble Space Telescope radios data to a satellite in the geosynchronous Tracking and Data Relay Satellite System (TDRSS), which then downlinks the science data to one of two 60-foot (18-meter) diameter high-gain microwave antennas located at the White Sands Test Facility in White Sands, New Mexico. From there they are sent to the Space Telescope Operations Control Center at Goddard Space Flight Center, and finally to the Space Telescope Science Institute for archiving. Each week, HST downlinks approximately 140 gigabits of data.\n\nAll images from Hubble are monochromatic grayscale, taken through a variety of filters, each passing specific wavelengths of light, and incorporated in each camera. Color images are created by combining separate monochrome images taken through different filters. This process can also create false-color versions of images including infrared and ultraviolet channels, where infrared is typically rendered as a deep red and ultraviolet is rendered as a deep blue.\n\nAll Hubble data is eventually made available via the Mikulski Archive for Space Telescopes at STScI, CADC and ESA/ESAC. Data is usually proprietary—available only to the principal investigator (PI) and astronomers designated by the PI—for six months after being taken. The PI can apply to the director of the STScI to extend or reduce the proprietary period in some circumstances.\n\nObservations made on Director's Discretionary Time are exempt from the proprietary period, and are released to the public immediately. Calibration data such as flat fields and dark frames are also publicly available straight away. All data in the archive is in the FITS format, which is suitable for astronomical analysis but not for public use. The Hubble Heritage Project processes and releases to the public a small selection of the most striking images in JPEG and TIFF formats.\n\nAstronomical data taken with CCDs must undergo several calibration steps before they are suitable for astronomical analysis. STScI has developed sophisticated software that automatically calibrates data when they are requested from the archive using the best calibration files available. This 'on-the-fly' processing means that large data requests can take a day or more to be processed and returned. The process by which data is calibrated automatically is known as 'pipeline reduction', and is increasingly common at major observatories. Astronomers may if they wish retrieve the calibration files themselves and run the pipeline reduction software locally. This may be desirable when calibration files other than those selected automatically need to be used.\n\nHubble data can be analyzed using many different packages. STScI maintains the custom-made Space Telescope Science Data Analysis System (STSDAS) software, which contains all the programs needed to run pipeline reduction on raw data files, as well as many other astronomical image processing tools, tailored to the requirements of Hubble data. The software runs as a module of IRAF, a popular astronomical data reduction program.\n\nIt has always been important for the Space Telescope to capture the public's imagination, given the considerable contribution of taxpayers to its construction and operational costs. After the difficult early years when the faulty mirror severely dented Hubble's reputation with the public, the first servicing mission allowed its rehabilitation as the corrected optics produced numerous remarkable images.\n\nSeveral initiatives have helped to keep the public informed about Hubble activities.\nIn the United States, outreach efforts are coordinated by the Space Telescope Science Institute (STScI) Office for Public Outreach, which was established in 2000 to ensure that U.S. taxpayers saw the benefits of their investment in the space telescope program. To that end, STScI operates the HubbleSite.org website. The Hubble Heritage Project, operating out of the STScI, provides the public with high-quality images of the most interesting and striking objects observed. The Heritage team is composed of amateur and professional astronomers, as well as people with backgrounds outside astronomy, and emphasizes the aesthetic nature of Hubble images. The Heritage Project is granted a small amount of time to observe objects which, for scientific reasons, may not have images taken at enough wavelengths to construct a full-color image.\n\nSince 1999, the leading Hubble outreach group in Europe has been the Hubble European Space Agency Information Centre (HEIC). This office was established at the Space Telescope European Coordinating Facility in Munich, Germany. HEIC's mission is to fulfill HST outreach and education tasks for the European Space Agency. The work is centered on the production of news and photo releases that highlight interesting Hubble results and images. These are often European in origin, and so increase awareness of both ESA's Hubble share (15%) and the contribution of European scientists to the observatory. ESA produces educational material, including a videocast series called Hubblecast designed to share world-class scientific news with the public.\n\nThe Hubble Space Telescope has won two Space Achievement Awards from the Space Foundation, for its outreach activities, in 2001 and 2010.\n\nA replica of the Hubble Space Telescope is on the courthouse lawn in Marshfield, Missouri, the hometown of namesake Edwin P. Hubble.\n\nThe Hubble Space Telescope celebrated its 20th anniversary in space on April 24, 2010. To commemorate the occasion, NASA, ESA, and the Space Telescope Science Institute (STScI) released an image from the Carina Nebula.\n\nTo commemorate Hubble's 25th anniversary in space on April 24, 2015, STScI released images of the Westerlund 2 cluster, located about away in the constellation Carina, through its Hubble 25 website. The European Space Agency created a dedicated 25th anniversary page on its website. In April 2016, a special celebratory image of the Bubble Nebula was released for Hubble's 26th \"birthday\".\n\nPast servicing missions have exchanged old instruments for new ones, both avoiding failure and making possible new types of science. Without servicing missions, all of the instruments will eventually fail. In August 2004, the power system of the Space Telescope Imaging Spectrograph (STIS) failed, rendering the instrument inoperable. The electronics had originally been fully redundant, but the first set of electronics failed in May 2001. This power supply was fixed during Servicing Mission 4 in May 2009.\n\nSimilarly, the Advanced Camera for Surveys (ACS) main camera primary electronics failed in June 2006, and the power supply for the backup electronics failed on January 27, 2007. Only the instrument's Solar Blind Channel (SBC) was operable using the side-1 electronics. A new power supply for the wide angle channel was added during SM 4, but quick tests revealed this did not help the high resolution channel. The Wide Field Channel (WFC) was returned to service by STS-125 in May 2009 but the High Resolution Channel (HRC) remains offline.\n\nHST uses gyroscopes to detect and measure any rotations so it can stabilize itself in orbit and point accurately and steadily at astronomical targets. (The gyroscopes are part of the Pointing Control System which has five types of sensor: magnetic sensors, various optical sensors and the gyros, and has two types of actuator: reaction wheels and magnetic torquers.) It carries six gyroscopes in total. Three gyroscopes are normally required for operation; observations are still possible with two or one, but the area of sky that can be viewed would be somewhat restricted, and observations requiring very accurate pointing are more difficult. In 2018, the plan is to drop into one-gyro mode if less than three working gyroscopes are operational. \n\nAfter the Columbia disaster in 2003, it was unclear whether another servicing mission would be possible, and gyro life became a concern again, so engineers developed new software for two-gyro and one-gyro modes to maximize the potential lifetime. The development was successful, and in 2005, it was decided to switch to two-gyroscope mode for regular telescope operations as a means of extending the lifetime of the mission. The switch to this mode was made in August 2005, leaving Hubble with two gyroscopes in use, two on backup, and two inoperable. One more gyro failed in 2007.\n\nBy the time of the final repair mission in May 2009, during which all six gyros were replaced (with two new pairs and one refurbished pair), only three gyros were still working. Engineers determined that the gyro failures were caused by corrosion of electric wires powering the motor that was initiated by oxygen-pressurized air used to deliver the thick suspending fluid. The new gyro models were assembled using pressurized nitrogen and they should be much more reliable. In the 2009 servcing mission all six gyroscopes were replaced, and after almost ten years only three gryos failed, but only after exceeding the average expected run time for the design.\n\nOf the six gyroscopes replaced in 2009, three were of the old design susceptible for flex-lead failure, and three were of the new design with a longer expected lifetime. The first of the old-style gyros failed in March 2014, and the second of the old-style gyros failed in April 2018. On October 5, 2018, the last of the old-style gyros failed, and one of the new-style gyros was powered-up from standby state. However, that reserve gyro did not immediately perform within operational limits, and so the observatory was placed into \"safe\" mode while scientists attempted to fix the problem. NASA tweeted on October 22, 2018 that the \"rotation rates produced by the backup gyro have reduced and are now within a normal range. Additional tests to be performed to ensure Hubble can return to science operations\". \n\nThe solution that restored the backup new-style gyro to operational range was widely reported as \"turning it off and on again\". A \"running restart\" of the gyroscope was performed, but this had no effect, and the final resolution to the failure was more complex. The failure was attributed to an inconsistency in the fluid surrounding the float within the gyroscope (e.g., an air bubble). On October 18, 2018 the Hubble Operations Team directed the spacecraft into a series of maneuvers - moving the spacecraft in opposite directions - in order to mitigate the inconsistency. Only after the maneuvers, and a subsequent set of maneuvers on October 19, did the gyroscope truly operate within its normal range.\n\nHubble orbits the Earth in the extremely tenuous upper atmosphere, and over time its orbit decays due to drag. If it is not re-boosted, it will re-enter the Earth's atmosphere within some decades, with the exact date depending on how active the Sun is and its impact on the upper atmosphere. If Hubble were to descend in a completely uncontrolled re-entry, parts of the main mirror and its support structure would probably survive, leaving the potential for damage or even human fatalities. In 2013, deputy project manager James Jeletic projected that Hubble could survive into the 2020s. Based on solar activity and atmospheric drag, or lack thereof, a natural atmospheric reentry for Hubble will occur between 2028 and 2040. In June 2016, NASA extended the service contract for Hubble until June 2021.\n\nNASA's original plan for safely de-orbiting Hubble was to retrieve it using a space shuttle. Hubble would then have most likely been displayed in the Smithsonian Institution. This is no longer possible since the space shuttle fleet has been retired, and would have been unlikely in any case due to the cost of the mission and risk to the crew. Instead, NASA considered adding an external propulsion module to allow controlled re-entry. Ultimately, in 2009, as part of Servicing Mission 4, the last servicing mission by the Space Shuttle, NASA installed the Soft Capture Mechanism (SCM), to enable deorbit by either a crewed or robotic mission. The SCM together with the Relative Navigation System (RNS) - mounted on the Shuttle to collect data to \"enable NASA to pursue numerous options for the safe de-orbit of Hubble\" - constitute the Soft Capture and Rendezvous System (SCRS).\n\n, the Trump Administration is considering a proposal by the Sierra Nevada Corporation to use a manned version of its Dream Chaser spacecraft to service \"Hubble\" some time in the 2020s both as a continuation of its scientific capabilities and as insurance against any malfunctions in the to-be-launched James Webb Space Telescope.\n\nThere is no direct replacement to Hubble as an ultraviolet and visible light space telescope, because near-term space telescopes do not duplicate Hubble's wavelength coverage (near-ultraviolet to near-infrared wavelengths), instead concentrating on the farther infrared bands. These bands are preferred for studying high redshift and low-temperature objects, objects generally older and farther away in the universe. These wavelengths are also difficult or impossible to study from the ground, justifying the expense of a space-based telescope. Large ground-based telescopes can image some of the same wavelengths as Hubble, sometimes challenge HST in terms of resolution by using adaptive optics (AO), have much larger light-gathering power, and can be upgraded more easily, but cannot yet match Hubble's excellent resolution over a wide field of view with the very dark background of space.\n\nPlans for a Hubble successor materialized as the Next Generation Space Telescope project, which culminated in plans for the James Webb Space Telescope (JWST), the formal successor of Hubble. Very different from a scaled-up Hubble, it is designed to operate colder and farther away from the Earth at the L2 Lagrangian point, where thermal and optical interference from the Earth and Moon are lessened. It is not engineered to be fully serviceable (such as replaceable instruments), but the design includes a docking ring to enable visits from other spacecraft. A main scientific goal of JWST is to observe the most distant objects in the universe, beyond the reach of existing instruments. It is expected to detect stars in the early Universe approximately 280 million years older than stars HST now detects. The telescope is an international collaboration between NASA, the European Space Agency, and the Canadian Space Agency since 1996, and is planned for launch on an Ariane 5 rocket. Although JWST is primarily an infrared instrument, its coverage extends down to 600 nm wavelength light, or roughly orange in the visible spectrum. A typical human eye can see to about 750 nm wavelength light, so there is some overlap with the longest visible wavelength bands, including orange and red light.\nA complementary telescope, looking at even longer wavelengths than Hubble or JWST, was the European Space Agency's Herschel Space Observatory, launched on May 14, 2009. Like JWST, Herschel was not designed to be serviced after launch, and had a mirror substantially larger than Hubble's, but observed only in the far infrared and submillimeter. It needed helium coolant, of which it ran out on April 29, 2013.\n\nFurther concepts for advanced 21st-century space telescopes include the Advanced Technology Large-Aperture Space Telescope, a conceptualized optical space telescope that if realized could be a more direct successor to HST, with the ability to observe and photograph astronomical objects in the visible, ultraviolet, and infrared wavelengths, with substantially better resolution than Hubble or the Spitzer Space telescope. This effort is being planned for the 2025–2035 time frame.\n\nExisting ground-based telescopes, and various proposed Extremely Large Telescopes, can exceed the HST in terms of sheer light-gathering power and diffraction limit due to larger mirrors, but other factors affect telescopes. In some cases, they may be able to match or beat Hubble in resolution by using adaptive optics (AO). However, AO on large ground-based reflectors will not make Hubble and other space telescopes obsolete. Most AO systems sharpen the view over a very narrow field—Lucky Cam, for example, produces crisp images just 10\" to 20\" wide, whereas Hubble's cameras produce crisp images across a 2½' (150\") field. Furthermore, space telescopes can study the universe across the entire electromagnetic spectrum, most of which is blocked by Earth's atmosphere. Finally, the background sky is darker in space than on the ground, because air absorbs solar energy during the day and then releases it at night, producing a faint—but nevertheless discernible—airglow that washes out low-contrast astronomical objects.\n\n\n\n"}
{"id": "858101", "url": "https://en.wikipedia.org/wiki?curid=858101", "title": "Inorganic ions", "text": "Inorganic ions\n\nInorganic ions in animals and plants are ions necessary for vital cellular activity. In body tissues, ions are also known as electrolytes, essential for the electrical activity needed to support muscle contractions and neuron activation. They contribute to osmotic pressure of body fluids as well as performing a number of other important functions. Below is a list of some of the most important ions for living things as well as examples of their functions:\n\n\n"}
{"id": "18650951", "url": "https://en.wikipedia.org/wiki?curid=18650951", "title": "Isabelle Autissier", "text": "Isabelle Autissier\n\nIsabelle Autissier (born 18 October 1956) is a French sailor, navigator, writer, and broadcaster. She is celebrated for being the first woman to have completed a solo world navigation in competition (BOC Challenge 1990–91).\n\nWhile competing in the 1994–95 BOC Challenge Autissier's boat \"Ecureuil Poitou Charentes II\" was dismasted and severely damaged approximately south of Adelaide, Australia. Autissier was rescued on 1 January 1995 by a Seahawk helicopter launched from the Royal Australian Navy frigate, .\n\nShe competed in the 1997–98 Whitbread Round the World Race on board \"EF Education\".\n\nIn the 1998–99 Around Alone race Autissier was rescued by fellow competitor when her boat \"PRB\" capsized approximately west of Cape Horn.\n\n"}
{"id": "6402034", "url": "https://en.wikipedia.org/wiki?curid=6402034", "title": "Jogah", "text": "Jogah\n\nJogah (Drum Dancers) are the mythical \"little people\" in Iroquois lore. Usually invisible, there are ways of telling if they are around. For example, drumming with no visible drummers around. They also leave rings of bare earth and \"bowls\" in stones or mud. offerings like tobacco and fingernails can be offered at these \"bowls\". They are also used to explain disembodied lights and bad luck. When people, usually children, elders, and spiritual healers, see the Jagoh, they are described as \"Knee high\" to around four feet tall. Behaviorally the Jagoh love games and playing tricks, which can be dangerous if they are disrespected. They have been claimed to cause illness in homes and neighborhoods that are built on sites to which that they are attracted.\n\nThe Jagoh are divided into multiple groups. The Gahongas \"stone throwers or rollers\" live in rocky areas like streams. Their favorite game is to play \"catch\" with people using stones, often the size of boulders. The Gandayah care for the flora of an area, telling it when to grow and how good its yield will be. They are known to help respectful Iroquois farmers. They are also known to love strawberries and take the forms of American robins, if their news is good, or owls, if it is bad. Lastly the Ohdows. These are the subterranean guardians of our world. They protect it from creatures of the underworld which would spread disease and, in the case of the \"White Buffalo\", chaos. The Ohdow come out of the underground at night to dance and hunt any underworld creatures that have escaped. To help with this task, fingernails were left as offerings as the animals knew what Ohdow smelled like and would hide from them.\n\nSources: \n"}
{"id": "212235", "url": "https://en.wikipedia.org/wiki?curid=212235", "title": "Lamb of God", "text": "Lamb of God\n\nLamb of God (, \"Amnos tou Theou\"; ) is a title for Jesus that appears in the Gospel of John. It appears at , where John the Baptist sees Jesus and exclaims, \"Behold the Lamb of God who takes away the sin of the world.\"\n\nChristian doctrine holds that divine Jesus chose to suffer crucifixion at Calvary as a sign of his full obedience to the will of his divine Father, as an \"agent and servant of God\". In Christian theology the \"Lamb of God\" is viewed as foundational and integral to the message of Christianity.\n\nA lion-like lamb that rises to deliver victory after being slain appears several times in the Book of Revelation. It is also referred to in Pauline writings: suggests that Saint Paul intends to refer to the death of Jesus, who is the Paschal Lamb, using the theme found in Johannine writings. The lamb metaphor is also in line with Psalm 23, which depicts God as a shepherd leading his flock (mankind).\n\nThe Lamb of God title is widely used in Christian prayers, and the Agnus Dei is used as a standard part of the Catholic Mass, as well as the classical Western Liturgies of the Anglican and Lutheran Churches. It also is used in liturgy and as a form of contemplative prayer. The Agnus Dei also forms a part of the musical setting for the Mass.\n\nAs a visual motif the lamb has been most often represented since the Middle Ages as a standing haloed lamb with a foreleg cocked \"holding\" a pennant with a red cross on a white ground, though many other ways of representing it have been used.\n\nThe title \"Lamb of God\" for Jesus appears in the Gospel of John, with the initial proclamation: \"Behold the Lamb of God who takes away the sin of the world\" in , the title reaffirmed the next day in . The second use of the title Lamb of God takes place in the presence of the first two apostles of Jesus, who immediately follow him, address him as Rabbi with respect and later in the narrative bring others to meet him.\n\nThese two proclamations of Jesus as the Lamb of God closely bracket the Baptist's other proclamation in : \"I have borne witness that this is the Son of God\". From a Christological perspective, these proclamations and the descent of the Holy Spirit as a dove in John 1:32 reinforce each other to establish the divine element of the Person of Christ. In Johannine Christology the proclamation \"who takes away the sin of the world\" begins the unfolding of the salvific theme of the redemptive and sacrificial death of Jesus followed by his resurrection which is built upon in other proclamations such as \"this is indeed the Saviour of the world\" uttered by the Samaritans in .\n\nThe Book of Revelation includes over twenty-nine references to a lion-like lamb (\"slain but standing\") which delivers victory in a manner reminiscent of the resurrected Christ. In the first appearance of the lamb in Revelation () only the lamb (which is of the tribe of Judah, and the root of David) is found worthy to take the judgment scroll from God and break the seals. The reference to the lamb in relates it to the Seven Spirits of God which first appear in and are associated with Jesus who holds them along with seven stars.\n\nIn the lamb is said to have twelve apostles. The handing of the scroll (i.e. the book containing the names of those who will be saved) to the risen lamb signifies the change in the role of the lamb. In Calvary, the lamb submitted to the will of the Father to be slain, but now is trusted with the judgment of mankind.\n\nFrom the outset, the book of Revelation is presented as a \"revelation of Jesus Christ\" and hence the focus on the lamb as both redeemer and judge presents the dual role of Jesus: he redeems man through self-sacrifice, yet calls man to account on the day of judgment.\n\nThe concept of the Lamb of God fits well within John's \"agent Christology\", in which sacrifice is made as an \"agent of God\" or \"servant of God\" for the sake of eventual victory.\n\nThe theme of a sacrificial lamb which rises in victory as the Resurrected Christ was employed in early Christology. For example in 375 Saint Augustine wrote: \"Why a lamb in his passion? Because he underwent death without being guilty of any iniquity. Why a lion in his passion? Because in being slain, he slew death. Why a lamb in his resurrection? Because his innocence is everlasting. Why a lion in his resurrection? Because everlasting also is his might.\"\n\nThe 11th century Christology of Saint Anselm of Canterbury specifically disassociates the Lamb of God from the Old Testament concept of a scapegoat, which is subjected to punishment for the sins of others without knowing it or willing it. Anselm emphasized that as Lamb of God, Jesus chose to suffer in Calvary as a sign of his full obedience to the will of the Father.\n\nJohn Calvin presented the same Christological view, of \"The Lamb as the agent of God\", by arguing that in his trial before Pilate and while at Herod's Court Jesus could have argued for his innocence, but instead remained mostly quiet and submitted to crucifixion in obedience to the Father, for he knew his role as the Lamb of God.\n\nIn modern Eastern Orthodox Christology, Sergei Bulgakov argued that the role of Jesus as the Lamb of God was \"pre-eternally\" determined by the Father, before the creation of the world, by considering the scenario that it would be necessary to send The Son as an agent to redeem humanity disgraced by the fall of Adam, and that this is a sign of His love.\n\nMultiple hypotheses about the suitable symbolism for the Lamb of God have been offered, within various Christological frameworks, ranging from the interpretation of Old Testament references to those of the Book of Revelation. One view suggests the symbolism of Leviticus 16 as scapegoat, coupled with Romans 3:21-25 for atonement, while another view draws parallels with the Paschal Lamb in Exodus 12:1-4, coupled with John 1:29-36, and yet another symbolism relies on Revelation 5:5-14 in which the lamb is viewed as a lion who destroys evil. However, as above, the view adopted by Saint Anselm and John Calvin rejects the scapegoat symbolism. They view Jesus as making a knowing sacrifice as an agent of God, unlike an unwitting scapegoat.\n\nIn modern Roman Catholic Christology, Karl Rahner has continued to elaborate on the analogy that the blood of the Lamb of God, and the water flowing from the side of Christ on Calvary, had a cleansing nature, similar to baptismal water. In this analogy, the blood of the Lamb washed away the sins of humanity in a new baptism, redeeming it from the fall of Adam.\n\nIn the Mass of the Roman Rite and also in the Eucharist of the Anglican Communion, the Lutheran Church, and the Western Rite of the Orthodox Church the \"Agnus Dei\" is the invocation to the Lamb of God sung or recited during the fraction of the Host. It is said to have been introduced into the Mass by Pope Sergius I (687–701).\n\n\"Agnus Dei\" has been set to music by many composers, usually as part of a Mass setting.\n\nIn Christian iconography, an Agnus Dei is a visual representation of Jesus as a lamb, since the Middle Ages, usually holding a standard or banner with a cross. This normally rests on the lamb's shoulder and is held in its right foreleg. Often the cross will have a white banner suspended from it charged with a red cross (similar to St George's Cross), though the cross may also be rendered in different colors. Sometimes the lamb is shown lying atop a book with seven seals hanging from it. This is a reference to the imagery in the Book of Revelation , ff. Occasionally, the lamb may be depicted bleeding from the area of the heart (Cf. ), symbolizing Jesus' shedding of his blood to take away the sins of the world (Cf. , ).\n\nIn Early Christian art the symbol appears very early on. Several mosaics in churches include it, some showing a row of twelve sheep representing the apostles flanking the central Agnus Dei, as in Santi Cosma e Damiano, Rome (526-30).\n\nThe Moravian Church uses an Agnus Dei as their seal with the surrounding inscription \"Vicit agnus noster, eum sequamur\" (\"Our Lamb has conquered, let us follow him\").\n\nAlthough the depiction of Jesus as the Lamb of God is of ancient origin, it is not used in the liturgical iconography of the Eastern Orthodox Church. The reason for this is that the depictions of Jesus in the Orthodox Church are anthropomorphic rather than symbolic, as a confession of the Orthodox belief in the Incarnation of the Logos. However, there is no objection to the application of the term \"Lamb of God\" to Jesus. In fact, the Host used in the Orthodox Divine Liturgy is referred to as the Lamb (; ).\n\nA \"Paschal Lamb\" is a charge used in heraldry, for example as the crest of the Davie Baronets, and is blazoned: \"A paschal lamb\" This charge is depicted as a lamb standing with body facing towards the dexter (viewer's left), with nimbus, and with head facing forwards (or turned looking backwards to sinister, termed \"reguardant\") holding under its right foreleg a flagpole, tipped with a small cross, resting at a diagonal angle over its shoulder, flying a banner of the Cross of St. George (except in Perth's coat of arms, where it flies a banner of the Cross of St Andrew).\n\nIn the Roman Catholic Church, an \"Agnus Dei\" is a disc of wax, stamped with an image of Jesus as a lamb bearing a cross, that is consecrated by the Pope as a sacramental. These were often set in jewelry, and might be worn round the neck on a chain, or as a brooch.\n\n"}
{"id": "276661", "url": "https://en.wikipedia.org/wiki?curid=276661", "title": "Leave No Trace", "text": "Leave No Trace\n\nLeave No Trace is a set of outdoor ethics promoting conservation in the outdoors. It consists of seven principles:\n\nThese principles have been adapted to different activities, ecosystems and environments. Since 1994, Leave No Trace Center For Outdoor Ethics, a non-profit organization also known as Leave No Trace, exists to educate people about their recreational impact on nature as well as the principles of Leave No Trace to prevent and minimize such impacts.\n\nIn the mid 20th-century there was a cultural shift in wilderness ethics from woodcraft where wilderness travelers prided themselves on their ability to rely on the resources of wild lands to a post-WWII ethics of minimal impact on the environment. Leave No Trace began in the 1960s and 1970s. There was a large increase of wilderness visitation following the creation of new recreational equipment such as white gas stoves, synthetic tents, and sleeping pads. This began a commercial interest in outdoor recreation which in turn caused more visitors to national parks. In those decades, the United States Forest Service, the Bureau of Land Management, and the National Park Service started to teach their non-motorized visitors how to have a minimal impact on the land. Wilderness Informational Specialists were trained to educate visitors on minimal impact camping in the different parks. In 1987 the three departments cooperatively developed a pamphlet titled \"Leave No Trace Land Ethics\".\n\nAlso in the 1970s, groups such as the Sierra Club were advocating minimum impact camping techniques. The Boy Scouts of America (BSA) had been actively advocating training and implementation of Leave No Trace and outdoor ethics principles early in the 1970s at such places as Philmont Scout Ranch in Northern New Mexico. A pilot program in the 1980s between the BSA and the Bureau of Land Management in the High Uintas Wilderness tried to reach a wide audience.\n\nThe national education program of Leave No Trace was developed in 1990 by the United States Forest Service in conjunction with the National Outdoor Leadership School (NOLS). At the time the USFS also created other programs such as Smokey Bear, Woodsy Owl, and previously in 1985 the Tread Lightly! program which was geared towards motorized recreation. The Bureau of Land Management joined the program in 1993 followed by the National Park Service and U.S. Fish and Wildlife Service in 1994.\n\nLeave No Trace provides a framework for outdoor recreation decision making, which is summarized in the following seven principles. Originally developed for the \"backcountry\", there are now also seven \"frontcountry\" principles as well:\n\n\n\nTwo primary scientific disciplines form the foundation of the Leave No Trace program: recreation ecology and human dimensions of natural resources. Recreation ecology research, \"a field of study that examines, assesses and monitors visitor impacts, typically to protected natural areas, and their relationships to influential factors\" has provided the foundation for Leave No Trace messaging because of its focus on recreational impacts. Recreation ecology has dominated most minimum-impact research, and reviews suggest that there have been over one thousand recreation ecology articles published within recent decades. Examples include both ecological and social impacts of visitors. Yet, the behavior of outdoor enthusiasts is perhaps the largest determinant of impact, and human dimensions research, which focuses on the sociological, psychological, cultural, and economic aspects of recreationists is limited but growing with regard to Leave No Trace-related studies.\n\nThe majority of human dimensions research related to Leave No Trace has evaluated educational effectiveness through various communication strategies in an effort to increase knowledge and influence behavioral change. For example, studies have evaluated communication strategies to mitigate human and wildlife conflict, reduce litter, minimize removal of natural objects or deter off-trail hiking. Few studies have addressed Leave No Trace specifically, instead focusing on minimum-impact behaviors broadly, and even fewer studies have evaluated the most common user-group, frontcountry visitors. More recently, however, social scientists have explored concepts such as knowledge, attitudes, beliefs, values, and behaviors of outdoor enthusiasts in the context of Leave No Trace practices, and have begun examining the perceptions of frontcountry visitors.\n\nSince 1994, a Leave No Trace program has been managed by the Leave No Trace Center for Outdoor Ethics. The Leave No Trace Center for Outdoor Ethics, a 501(c)(3) non-profit organization, is dedicated to the responsible enjoyment and active stewardship of the outdoors worldwide. \n\nThe Center's mission is to teach people how to enjoy the outdoors responsibly. This mission is accomplished through education, research, volunteerism and partnerships. Leave No Trace tries to build awareness, appreciation and respect for wildlands. \n\nThe Center has partnerships with the National Park Service, the U.S. Forest Service, the Bureau of Land Management, the U.S. Fish and Wildlife Service, US Army Corps of Engineers, and other partners such as colleges, universities, outfitter/guide services, small businesses, non-profits and youth serving organizations such as the Boy Scouts of America and the American Camp Association.\n\nThere are also formal Leave No Trace organisations in Australia, Canada, Ireland and New Zealand.\n\nDespite the fact that Leave No Trace is a widely accepted conservationist ethic, there has been a good amount of criticism. Environmental historian James Morton Turner argued in 2002 that Leave No Trace focused attention of backpackers \"largely on protecting wilderness as a recreational landscape\" rather than tackling larger questions, such as \"the modern economy, consumerism, and the environment\". Turner also argues that this ethic omitted the calculus behind it, and the code itself, in his view, \"helped ally the modern backpacker with the wilderness recreation industry\" by encouraging backpackers to practice the Leave No Trace ethic while in the wilderness and to keep an eye out for the logo of this ethic in shopping malls.\n\nGregory Simon and Peter Alagona had a different argument, saying in a 2009 article that there should be a move beyond Leave No Trace. They argued that this ethic \"disguises much about human relationships with non-human nature\" because it makes it seem that parks and wilderness areas are \"pristine nature\" which in their view, \"erases their human histories, and prevents people from understanding how these landscapes\nhave developed over time through complex human–environment interactions\". In the place of this ethic, they say that there should be a new environmental ethic \"that transforms the critical scholarship of social science into a critical practice of wilderness recreation, addresses the global economic system...and reinvents wilderness recreation as a more collaborative, participatory, productive, democratic, and radical form of political action\". In the article they also write about how certain corporations in the outdoor recreation industry and magazines like \"National Geographic\" support Leave No Trace (LNT), but that in stores for outdoor recreation products like REI, \"the LNT logo becomes both a corporate brand and an official stamp of approval\".\n\nThe authors also argue that because LNT \"focuses on the immediate, local impacts of recreational use while ignoring larger issues of change over time and connections through space\" it has what they consider \"two conceptual flaws\": the idea that the current wilderness is in a \"natural, pristine condition\" and the obscuring of the \"spatial connections between what takes place inside parks and wilderness areas and what occurs outside\". The authors note that additionally, this ethic is limited in scope, offering \"a code of conduct calibrated to the particular, limited, and arbitrary geographic scale of parks and wilderness areas\". They articulate their new environmental ethic as expanding LNT, not rejecting it all together, expanding its \"spatial scale beyond the boundaries of parks and wilderness areas...broaden[ing] LNT’s ethical purview to include the global economic systems that make contemporary American wilderness recreation possible\", redefining recreationists rather than \"passive ethical subjects and consumers, to active participants in collaborative programs\" and in sum, \"a more democratic, more participatory, and more radical vision of outdoor recreation as a form of political action\". Near the end of the article, the authors articulate the seven principles of what they call 'Beyond Leave No Trace':\n\nThree years later, Simon and Alagona responded to critiques of their 2009 article calling for a 'Beyond Leave No Trace' approach. They argued that they were not the first to explore LNT's history, that they \"joined a growing chorus of researchers\", and importantly that they \"remain steadfast in our endorsement of LNT’s value and potential\" but that they believe that \"this simple ethic is not enough in a world of global capital circulation where the goods we produce and consume in order to enjoy the outdoors can have long-term and far-reaching social and environmental ramifications\". While dismissing the concerns of critics, they write that, firstly, Leave No Trace \"could not exist in its current form without a plethora of consumer products;\" secondly, that \"the use of such products does not erase environmental impacts;\" and thirdly that LNT \"systematically obscures these impacts, displacements, and connections by encouraging the false belief that it is possible to 'leave no trace'\".\n\nOther critics of Leave No Trace have argued that it is impractical, displaces environmental impacts to other locations, \"obscures connections between the uses of outdoor products and their production and disposal impacts\" and have questioned how much the ethic affects everyday environmental behavior.\n\n\n"}
{"id": "4401404", "url": "https://en.wikipedia.org/wiki?curid=4401404", "title": "List of Lepidoptera that feed on Centaurea", "text": "List of Lepidoptera that feed on Centaurea\n\nCentaurea species (knapweeds, starthistles and relatives) are used as food plants by the caterpillars of a numerous Lepidoptera species, including:\n\nSpecies which feed exclusively on \"Centaurea\". Some are used in biological control of knapweeds that have become invasive weeds.\n\nBucculatricidae\nCochylidae\nColeophoridae\nGelechiidae\n\nSpecies which feed on \"Centaurea\" and other plants\n\nColeophoridae\nCrambidae\nGeometridae\nNoctuidae\nNymphalidae\nOecophoridae\nSphingidae\n\n"}
{"id": "402111", "url": "https://en.wikipedia.org/wiki?curid=402111", "title": "List of United States natural gas companies", "text": "List of United States natural gas companies\n\nThis is a list of natural gas companies in the United States.\n\n\n"}
{"id": "4655598", "url": "https://en.wikipedia.org/wiki?curid=4655598", "title": "List of boiling and freezing information of solvents", "text": "List of boiling and freezing information of solvents\n\n"}
{"id": "35133776", "url": "https://en.wikipedia.org/wiki?curid=35133776", "title": "List of mountains in South Africa", "text": "List of mountains in South Africa\n\nList of Mountains in South Africa is a general list of mountains in South Africa, with elevation. This list includes mountains in two other sovereign states, in the Stormberg-Drakensberg range, where the highest elevations are to be found in Lesotho, as well as Emlembe, the highest mountain in Swaziland, located at the border with South Africa. The highest mountain in South Africa is high Mafadi, located on the border of South Africa and Lesotho. \n\nSeveral of the highest peaks have snow in the Southern hemisphere winter season.\n\nA Khulu is a peak above and not within of another \"Khulu\", as defined by the Mountain Club of South Africa. Seweweekspoortpiek and Du Toits Peak are among the ultra prominent peaks of Africa.\n\nA few mountains, such as Spion Kop or Isandlwana are historically important hills, even though they are relatively not very high. Table Mountain is important because of its emblematic value.\n\n\n"}
{"id": "2863193", "url": "https://en.wikipedia.org/wiki?curid=2863193", "title": "List of rare species in the British National Vegetation Classification", "text": "List of rare species in the British National Vegetation Classification\n\nThe following is a list of vascular plants, bryophytes and lichens which were regarded as rare species by the authors of \"British Plant Communities\", together with the communities in which they occur.\n\n\n\n\n"}
{"id": "386433", "url": "https://en.wikipedia.org/wiki?curid=386433", "title": "List of rivers of France", "text": "List of rivers of France\n\nThis is a list of rivers that are at least partially in France. The rivers are grouped by sea or ocean. The rivers flowing into the sea are sorted along the coast. Rivers flowing into other rivers are listed by the rivers they flow into. Some rivers (e.g. Sûre/Sauer) do not flow through France themselves, but they are mentioned for having French tributaries. They are given in \"italics\". For clarity, only rivers that are longer than 50 km (or have longer tributaries) are shown.\n\nIn French, rivers are traditionally classified either as \"fleuves\" when they flow into the sea (or into a desert or lake), or as \"rivières\" when they flow into another river. The \"fleuves\" are shown in bold.\n\nFor an alphabetical overview of rivers of France, see the category .\n\nThe rivers in this section are sorted north-east (Netherlands) to south-west (Calais).\n\n\nThe rivers in this section are sorted east (Calais) to west (Brest).\n\n\nThe rivers in this section are sorted north (Brest) to south (Spain).\n\n\nThe rivers in this section are sorted west (Spain) to east (Italy).\n\n\nThe rivers in this section are listed counterclockwise along the Corsican coast starting from Cap Corse.\n\n\n\n\n"}
{"id": "20515837", "url": "https://en.wikipedia.org/wiki?curid=20515837", "title": "List of rivers of Jamaica", "text": "List of rivers of Jamaica\n\nThis is a list of rivers of Jamaica, arranged from west to east, with respective tributaries indented under each larger stream's name.\n\n\n\n\n\n"}
{"id": "41907076", "url": "https://en.wikipedia.org/wiki?curid=41907076", "title": "Lists of constellations", "text": "Lists of constellations\n\nThe following lists of constellations are available:\n\n"}
{"id": "2120798", "url": "https://en.wikipedia.org/wiki?curid=2120798", "title": "Lithium economy", "text": "Lithium economy\n\nThe lithium economy is a concept analogous to other element-based economies, such as the hydrogen economy, methanol economy, ethanol economy, electron economy, vegetable oil economy, or liquid nitrogen economy but where the energy storage medium is lithium. Analogous \"economies\" are the \"aluminium economy\" where the energy storage medium (fuel) is aluminium (typically aluminium-gallium).\n\nThe hydrogen economy as a low-carbon solution to land transport has problems in generation, distribution (infrastructure), on-board storage and cost of power converter (fuel cell). The lithium economy has analogous problems in all four areas, but considered separately, the routes to their solution have different absolute limits and different timescales for their solutions.\n\nThe lithium economy concept is used primarily as a political argument to prevent over-domination of the post-carbon energy future by oil companies; and as a post-carbon economy on which action can be taken now instead of deferred to some future date (see FreedomCAR project).\n\nThe lithium economy differs from the other proposed future fuel economies in that the transition roadmap begins with convencional recreable batteries using conventional Li-ion or Lithium polymer cell batteries and progressing to chemistries (such as Li-S and Li-iron-phosphate) and cell types with higher energy densities. Eventually, anode replacement Li-air or Li-water cells are envisaged where only anodes (lithium metal) are replaced.\n\nThe energy is stored in unoxidised lithium atoms, which release energy when oxidised. A lithium atom is seven times as heavy as a hydrogen atom, and at room temperature, hydrogen is a gas, while lithium is solid. This means energy per mass is much worse, but since lithium is much more compact, it has more energy per volume. In fact, storing hydrogen requires so much ancillary equipment or material that lithium is also competitive in energy per mass when the whole system is considered.\n\n\n"}
{"id": "19776285", "url": "https://en.wikipedia.org/wiki?curid=19776285", "title": "Little Vermilion River (Illinois)", "text": "Little Vermilion River (Illinois)\n\nThe Little Vermilion River may refer to two different rivers in Illinois, in the United States:\n\n"}
{"id": "392304", "url": "https://en.wikipedia.org/wiki?curid=392304", "title": "Mathieu Kérékou", "text": "Mathieu Kérékou\n\nMathieu Kérékou () (2 September 1933 – 14 October 2015) was a Beninese politician who served as President of Benin from 1972 to 1991 and again from 1996 to 2006. After seizing power in a military coup, he ruled the country for 19 years, for most of that time under an officially Marxist–Leninist ideology, before he was stripped of his powers by the National Conference of 1990. He was defeated in the 1991 presidential election but was returned to the presidency in the 1996 election and controversially re-elected in 2001.\n\nKérékou was born in 1933 in Kouarfa village, in north-west French Dahomey. After having studied at military schools in modern-day Mali and Senegal, Kérékou served in the military by joining the French Army in 1960. Following independence, from 1961 to 1963 he was an \"aide-de-camp\" to Dahomeyan President Hubert Maga. Following Maurice Kouandété's coup d'état in December 1967, Kérékou, who was his cousin, was made chairman of the Military Revolutionary Council. After Kérékou attended French military schools from 1968 to 1970, Maga made him a major, deputy chief of staff, and commander of the Ouidah paratroop unit.\n\nKérékou seized power in Dahomey in a military coup on 26 October 1972, ending a system of government in which three members of a presidential council were to rotate power (earlier in the year Maga had handed over power to Justin Ahomadegbé).\n\nDuring his first two years in power, Kérékou expressed only nationalism and said that the country's revolution would not \"burden itself by copying foreign ideology ... We do not want communism or capitalism or socialism. We have our own Dahomean social and cultural system.\" On 30 November 1974, however, he announced the adoption of Marxism-Leninism by the state. The country was renamed from the Republic of Dahomey to the People's Republic of Benin a year later; the banks and petroleum industry were nationalized. The People's Revolutionary Party of Benin (\"Parti de la révolution populaire du Bénin\", PRPB) was established as the sole ruling party. In 1980, Kérékou was elected president by the Revolutionary National Assembly; he retired from the army in 1987.\n\nIt has been suggested that Kérékou's move to Marxism-Leninism was motivated mainly by pragmatic considerations, and that Kérékou himself was not actually a leftist radical; the new ideology offered a means of legitimization, a way of distinguishing the new regime from those that had preceded it, and was based on broader unifying principles than the politics of ethnicity. Kérékou's regime initially included officers from both the north and south of the country, but as the years passed the northerners (like Kérékou himself) became clearly dominant, undermining the idea that the regime was not based in ethnicity. By officially adopting Marxism-Leninism, Kérékou may also have wanted to win the support of the country's leftists.\n\nKérékou's regime was rigid and vigorous in pursuing its newly adopted ideological goals from the mid-1970s to the late 1970s. Beginning in the late 1970s, the regime jettisoned much of its radicalism and settled onto a more moderately socialist course as Kérékou consolidated his personal control.\n\nKérékou survived numerous attempts to oust him, including an invasion of the port city of Cotonou by mercenaries contracted by a group of exiled Beninese political rivals in January 1977, as well as two coup attempts in 1988.\n\nIt was hoped that the nationalizations of the 1970s would help develop the economy, but it remained in a very poor condition, with the state sector being plagued by inefficiency and corruption. Kérékou began reversing course in the early 1980s, closing down numerous state-run companies and attempting to attract foreign investment. He also accepted an IMF structural readjustment programme in 1989, agreeing to austerity measures that severely cut state expenditure. The economic situation continued to worsen during the 1980s, provoking widespread unrest in 1989. A student strike began in January of that year; subsequently strikes among various elements of society increased in frequency and the nature of their demands grew broader: whereas initially they had focused on economic issues such as salary arrears, this progressed to include demands for political reform.\n\nIn the period of reforms towards multiparty democracy in Africa at the beginning of the 1990s, Benin moved onto this path early, with Kérékou being forced to make concessions to popular discontent. Benin's early and relatively smooth transition may be attributed to the particularly dismal economic situation in the country, which seemed to preclude any alternative. In the midst of increasing unrest, Kérékou was re-elected as president by the National Assembly in August 1989, but in December 1989 Marxism-Leninism was dropped as the state ideology, and a national conference was held in February 1990. The conference turned out to be hostile to Kérékou and declared its own sovereignty; despite the objections of some of his officers to this turn of events, Kérékou did not act against the conference, although he labelled the conference's declaration of sovereignty a \"civilian coup\". During the transition that followed, Kérékou remained president but lost most of his power.\n\nDuring the 1990 National Conference, which was nationally televised, Kérékou spoke to the Archbishop of Cotonou, Isidore de Souza, confessing guilt and begging forgiveness for the flaws of his regime. An observer described it as a \"remarkable piece of political theater\", full of cultural symbolism and significance; in effect, Kérékou was seeking forgiveness from his people. Such a gesture, so unusual for the African autocrats of the time, could have fatally weakened Kérékou's political standing, but he performed the gesture in such a way that, far from ending his political career, it instead served to symbolically redeem him and facilitate his political rehabilitation, while also \"securing him immunity from prosecution\". Kérékou shrewdly utilized the timing and setting: \"Culturally as well as theologically it was impossible to refuse forgiveness on these terms.\"\n\nWorld Bank economist Nicéphore Soglo, chosen as prime minister by the conference, took office in March, and a new constitution was approved in a December 1990 referendum. Multi-party elections were held in March 1991, which Kérékou lost, obtaining only about 32% of the vote in the second round against Prime Minister Soglo; while he won very large vote percentages in the north, in the rest of the country he found little support. Kérékou was thus the first mainland African president to lose power through a popular election. He apologized for \"deplorable and regrettable incidents\" that occurred during his rule.\n\nAfter losing the election in March 1991, Kérékou left the political scene and \"withdrew to total silence\", another move that was interpreted as penitential.\n\nKérékou reclaimed the presidency in the March 1996 election. Soglo's economic reforms and his alleged dictatorial tendencies had caused his popularity to suffer. Although Kérékou received fewer votes than Soglo in the first round, he then defeated Soglo in the second round, taking 52.5% of the vote. Kérékou was backed in the second round by third place candidate Adrien Houngbédji and fourth place candidate Bruno Amoussou; as in 1991, Kérékou received very strong support from northern voters, but he also improved his performance in the south. Soglo alleged fraud, but this was rejected by the Constitutional Court, which confirmed Kérékou's victory. When taking the oath of office, Kérékou left out a portion that referred to the \"spirits of the ancestors\" because he had become a born-again Christian after his defeat by Soglo. He was subsequently forced to retake the oath including the reference to spirits.\n\nKérékou was re-elected for a second five-year term in the March 2001 presidential election under controversial circumstances. In the first round he took 45.4% of the vote; Soglo, who took second place, and parliament speaker Houngbédji, who took third, both refused to participate in the second round, alleging fraud and saying that they did not want to legitimize the vote by participating in it. This left the fourth-place finisher, Amoussou, to face Kérékou in the run-off, and Kérékou easily won with 83.6% of the vote. It was subsequently discovered that the American corporation Titan gave more than two million dollars to Kérékou's re-election campaign as a bribe.\n\nDuring Kérékou's second period in office his government followed a liberal economic path. The period also saw Benin take part in international peacekeeping missions in other African states.\n\nKérékou was barred from running again in 2006 on two counts. The constitution not only limited the president to two terms, but also required that presidential candidates be younger than 70 (he turned 70 in 2003, through his second term). Kérékou said in July 2005 that he would not attempt to amend the constitution to allow him to run for a third term. \"If you don't leave power,\" he said, \"power will leave you.\" There was, however, speculation that he had wanted it to be changed, but faced too much opposition.\n\nOn 5 March 2006, voters went to the polls to decide who would succeed Kérékou as President of Benin. Yayi Boni defeated Adrien Houngbédji in a run-off vote on 19 March, and Kérékou left office at the end of his term, at midnight on 6 April 2006.\n\nBorn and baptized in the Roman Catholic faith, although he was a lapsed adherent, Kérékou allegedly converted to Islam in 1980 while on a visit to Libya, and changed his first name to Ahmed, but he later returned to the use of the name Mathieu. This alleged conversion may have been designed to please the Libyan leader Muammar Gaddafi and obtain financial and military support. Alternatively, the conversion story may have been a rumor planted by some of his opponents in order to destabilize his regime. He subsequently became a born-again Christian. Some Vodun believers in Benin regarded him as having magical powers, explaining his ability to survive repeated coup attempts during his military rule.\n\nNicknamed \"the chameleon\" from an early point in his career, Kérékou's motto was \"the branch will not break in the arms of the chameleon\". The nickname and motto he adopted were full of cultural symbolism, articulating and projecting his power and ability. Unlike some past rulers who had adopted animal symbolism intending to project a violent, warlike sense of power, Kérékou's symbolic animal suggested skill and cleverness; his motto suggested that he would keep the branch from breaking, but implicitly warned of what could happen to \"the branch\" if it was not \"in the arms of the chameleon\"—political chaos. To some, his nickname seemed particularly apt as he successfully adapted himself to a new political climate and neoliberal economic policies in the 1990s.\n\nHe used the campaign slogan, \"Experience in the service of youth.\"\n\nAfter leaving office in 2006, Kérékou stayed out of politics and spent time at his homes in Cotonou and Natitingou in northwestern Benin, his native region. He suffered a health crisis in 2014 and was taken to Paris for treatment. Although he recovered, he continued to suffer health problems, and he died in Benin on 14 October 2015 at the age of 82. His death was announced in a statement by President Thomas Boni Yayi. No cause of death was stated. A week of national mourning was declared.\n"}
{"id": "381925", "url": "https://en.wikipedia.org/wiki?curid=381925", "title": "Mount Carmel", "text": "Mount Carmel\n\nMount Carmel (, \"Har HaKarmel\" \"Har ha Karmell\"; , \"Al-Kurmul\", or , \"Jabal Mar Elyas\" (\"lit.\" Mount Saint Elias/Elijah) is a coastal mountain range in northern Israel stretching from the Mediterranean Sea towards the southeast. The range is a UNESCO biosphere reserve. A number of towns are situated there, most notably the city of Haifa, Israel's third largest city, located on the northern slope.\n\nThe name is presumed to be directly from the Hebrew language word Carmel (כַּרְמֶל), which means \"fresh\" (planted), or \"vineyard\" (planted).\n\nThe phrase \"Mount Carmel\" has been used in three distinct ways:\n\nThe Carmel range is approximately wide, sloping gradually towards the southwest, but forming a steep ridge on the northeastern face, high. The Jezreel Valley lies to the immediate northeast. The range forms a natural barrier in the landscape, just as the Jezreel Valley forms a natural passageway, and consequently the mountain range and the valley have had a large impact on migration and invasions through the Levant over time. The mountain formation is an admixture of limestone and flint, containing many caves, and covered in several volcanic rocks. The sloped side of the mountain is covered with luxuriant vegetation, including oak, pine, olive, and laurel trees.\n\nSeveral modern towns are located on the range, including Yokneam on the eastern ridge, Zikhron Ya'akov on the southern slope, the Druze communities of Daliyat al-Karmel and Isfiya on the more central part of the ridge, and the towns of Nesher, Tirat Hakarmel, and the city of Haifa, on the far northwestern promontory and its base. There is also a small kibbutz called Beit Oren, which is located on one of the highest points in the range to the southeast of Haifa.\n\nAs part of a 1929–1934 campaign, between 1930 and 1932, Dorothy Garrod excavated four caves, and a number of rock shelters, in the Carmel mountain range at el-Wad, el-Tabun, and Es Skhul. Garrod discovered Neanderthal and early modern human remains, including the skeleton of a Neanderthal female, named Tabun I, which is regarded as one of the most important human fossils ever found. The excavation at el-Tabun produced the longest stratigraphic record in the region, spanning 600,000 or more years of human activity. The four caves and rock-shelters (Tabun, Jamal, el-Wad, and Skhul) together yield results from the Lower Paleolithic to the present day, representing roughly a million years of human evolution. There are also several well-preserved burials of Neanderthals and Homo sapiens and passage from nomadic hunter-gatherer groups to complex, sedentary agricultural societies is extensively documented at the site. Taken together, these emphasize the paramount significance of the Mount Carmel caves for the study of human cultural and biological evolution within the framework of palaeo-ecological changes.\"\n\nIn 2012, UNESCO's World Heritage Committee added sites of human evolution at Mount Carmel to the List of World Heritage Sites. The World Heritage Site includes four caves (Tabun, Jamal, el-Wad, and Skhul) on the southern side of the Nahal Me’arot/Wadi El-Mughara Valley. The site fulfils criteria in two separate categories, \"natural\" and \"cultural\".\n\nDue to the lush vegetation on the sloped hillside, and many caves on the steeper side, Carmel became the haunt of criminals; Carmel was seen as a place offering an escape from God, as implied by the Book of Amos. According to the Books of Kings, Elisha travelled to Carmel straight after cursing a group of young men because they had mocked him and the ascension of Elijah by jeering, \"Go on up, bald man!\" After this, bears came out of the forest and mauled 42 of them. This does not necessarily imply that Elisha had sought asylum there from any potential backlash, although the description in the Book of Amos, of the location being a refuge, is dated by textual scholars to be earlier than the accounts of Elisha in the Book of Kings, and according to Strabo it had continued to be a place of refuge until at least the first century.\n\nAccording to Epiphanius, and Josephus, Mount Carmel had been the stronghold of the Essenes that came from a place in Galilee named \"Nazareth\"; this Essene group are sometimes referred to as \"Nazareans\", possibly akin to Nazarenes, which followed the teachings of Jesus (Yeshua).\n\nDuring World War I, Mount Carmel played a significant strategic role. The (20th century) Battle of Megiddo took place at the head of a pass through the Carmel Ridge, which overlooks the Valley of Jezreel from the south. General Allenby led the British in the battle, which was the turning point in the war against the Ottoman Empire. The Jezreel Valley had played host to many battles before, including the very historically significant Battle of Megiddo between the Egyptians and Canaanites, but it was only in the 20th century battle that the Carmel Ridge itself played a significant part, due to the developments in munitions.\n\nArchaeologists have discovered ancient wine and oil presses at various locations on Mt. Carmel.\n\nIn ancient Canaanite culture, \"high places\" were frequently considered to be sacred, and Mount Carmel appears to have been no exception; Thutmose III lists a \"holy headland\" among his Canaanite territories, and if this equates to Carmel, as Egyptologists such as Maspero believe, then it would indicate that the mountain headland was considered sacred from at least the 15th century BC. According to the Books of Kings, there was an altar to Yahweh on the mountain, which had fallen into ruin by the time of Ahab, but Elijah built a new one. Iamblichus describes Pythagoras visiting the mountain on account of its reputation for sacredness, stating that it was \"the most holy of all mountains, and access was forbidden to many\", while Tacitus states that there was an oracle situated there, which Vespasian visited for a consultation; Tacitus states that there was an altar there, but without any image upon it, and without a temple around it.\n\nIn mainstream Jewish, Christian, and Islamic thought, Elijah is indelibly associated with the mountain, and he is regarded as having sometimes resided in a grotto on the mountain. Indeed, one name for Mount Carmel is جبل مار إلياس (\"Jabal Mar Elyas\"; \"Mount Saint Elias\"). In the Books of Kings, Elijah challenges 450 prophets of a particular \"Baal\" to a contest at the altar on Mount Carmel to determine whose deity was genuinely in control of the Kingdom of Israel; since the narrative is set during the rule of Ahab and his association with the Phoenicians, biblical scholars suspect that the \"Baal\" in question was probably Melqart.\n\nAccording to the Bible in 1 Kings 18, the challenge was to see which deity could light a sacrifice by fire. After the prophets of Baal had failed to achieve this, Elijah had water poured on his sacrifice to saturate the altar and then he prayed; fire fell and consumed the sacrifice, wood, stones, soil, and water which prompted the Israelite witnesses to proclaim, \"The LORD, He is God! The LORD, He is God!\" In the account, Elijah announced the end to a long drought; clouds gathered, the sky turned black, and it rained heavily.\n\nThough there is no biblical reason to assume that the account of Elijah's victory refers to any particular part of Mount Carmel, Islamic tradition places it at a point known as \"El-Maharrakah\", meaning \"the burning\".\n\nTwo places have been appointed as possible site for the story about the battle against the priests of Baal. The slaughter could have taken place near the river Kishon, at the mountain base, in an amphitheater-like flat area. The site where the offering took place is traditionally placed on the mountain above Yokneam, on the road to the Druze village of Daliyat el-Karmil, where there is a monastery built in 1868 called El-Muhraqa (\"the Sacrifice\"). It is praised as one of the must-visit tour site in Haifa.\n\nAlthough archeological clues are absent, it has a point in its favor because it has a spring, from which water could have been drawn to wet Elijah's offering, and secondly there is a sea view, where Elijah looked out to see the cloud announcing rain. On the other hand, in the Bible text it says that Elijah had to climb up to see the sea. There is an altar in the monastery which is claimed to be the one that Elijah built up in God's honor, but that is unlikely as it's not made of the local limestone.\n\nA Catholic religious order was founded on Mount Carmel in the 12th century, named the Carmelites, in reference to the mountain range; the founder of the Carmelites is unknown; in the original Rule or 'Letter of Life' given by Albert, the Latin Patriarch of Jerusalem who was resident in Acre, around the year 1210 this hermit is referred to simply as 'Brother B'; he probably died around the date 1210 and could have been either a pilgrim, someone serving out a penance or a crusader who had stayed in the Holy Land. The Order was founded at the site that it claimed had been the location of Elijah's cave, above sea level at the northwestern end of the mountain range; this, perhaps not coincidentally, is also the highest natural point of the mountain range. Though there is no documentary evidence to support it, Carmelite tradition suggests that a community of Jewish hermits had lived at the site from the time of Elijah until the Carmelites were founded there; prefixed to the Carmelite Constitution of 1281 was the claim that from the time when Elijah and Elisha had \"dwelt devoutly on Mount Carmel\", priests and prophets, Jewish and Christian, had lived \"praiseworthy lives in holy penitence\" adjacent to the site of the \"fountain of Elisha\" in an uninterrupted succession.\n\nA Carmelite monastery was founded at the site shortly after the Order itself was created, and was dedicated to the Blessed Virgin Mary under the title of \"Star of the Sea\" (\"stella maris\" in Latin), a common medieval presentation of her. Although Louis IX of France is sometimes named as the founder, he was not, and had merely visited it in 1252. The Carmelite Order grew to be one of the major Catholic religious orders worldwide, although the monastery at Carmel has had a less successful history. During the Crusades the monastery often changed hands, frequently being converted into a mosque; under Islamic control the location came to be known as \"El-Maharrakah\", meaning \"place of burning\", in reference to the account of Elijah's challenge to the priests of Hadad. In 1799 the building was finally converted into a hospital, by Napoleon, but in 1821 the surviving structure was destroyed by the pasha of Damascus. A new monastery was later constructed directly over a nearby cave, after funds were collected by the Carmelite Order for restoration of the monastery. The cave, which now forms the crypt of the monastic church, is termed \"Elijah's grotto\" by the Discalced Carmelite friars who have custody of the monastery.\n\nOne of the oldest scapulars is associated with Mount Carmel and the Carmelites. According to Carmelite tradition, the Scapular of Our Lady of Mount Carmel was first given to St. Simon Stock, an English Carmelite, by the Blessed Virgin Mary. The Carmelites refer to her under the title \"Our Lady of Mount Carmel,\" and celebrate 16 July as her feast day.\n\nMount Carmel is considered a sacred place for Bahá'ís around the world, and is the location of the Bahá'í World Centre and the Shrine of the Báb. The location of the Bahá'í holy places has its roots in the imprisonment of the religion's founder, Bahá'u'lláh, near Haifa by the Ottoman Empire during the Ottoman Empire's rule over Palestine.\n\nThe Shrine of the Báb is a structure where the remains of the Báb, the founder of Bábism and forerunner of Bahá'u'lláh in the Bahá'í Faith, have been laid to rest. The shrine's precise location on Mount Carmel was designated by Bahá'u'lláh himself and the Báb's remains were laid to rest on March 21, 1909 in a six-room mausoleum made of local stone. The construction of the shrine with a golden dome was completed over the mausoleum in 1953, and a series of decorative terraces around the shrine were completed in 2001. The white marbles used were from the same ancient source that most Athenian masterpieces were using, the Penteliko Mountain.\n\nBahá'u'lláh, the founder of the Bahá'í Faith, writing in the \"Tablet of Carmel\", designated the area around the shrine as the location for the administrative headquarters of the religion; the Bahá'í administrative buildings were constructed adjacent to the decorative terraces, and are referred to as \"the Arc\", on account of their physical arrangement.\n\nThe Ahmadiyya Muslim Community has its largest Israeli mosque on Mount Carmel known as the Mahmood Mosque in Kababir. It is a unique structure composed of two minarets. The mosque was once visited by the president of Israel, Shimon Peres, for an iftar dinner.\n\n\n"}
{"id": "2189140", "url": "https://en.wikipedia.org/wiki?curid=2189140", "title": "Mount Zuqualla", "text": "Mount Zuqualla\n\nMount Zuqualla (also spelled Zuquala or Chuqqaala) is an extinct volcano in the Oromia region of Ethiopia. Situated in Ada'a Chukala woreda of the (East) Shewa Zone, it rises from the plain south of Bushooftuu. With a height of , it is known for its crater lake, lake Dembel, an elliptical crater lake with a maximum diameter of about one kilometer, but the trail around the crater is about 6 kilometers long. Both the mountain and the lake is a holy site to the Oromo living nearby. The ambivalent attitude regarding the holiness of the mountain is seen in the Oromo proverb: \"Those who live far away worship it, those who live nearby plow it.\"\n\nThe lake in the crater has an island Tulluu Irreechaa, said to have been founded by Abba Gadaa of Tuulama on the site of a hermitage used by Saint Mercurius. This monastery was destroyed, and a church at the foot of the mountain looted, by Imam Ahmad Gragn in 1531; two churches were later built at the monastery, destroying the Galama of Tuulama Oromo, one dedicated to St Gebre Menfas built by Menelik II in 1880 and designed by the Italian Sebastian Castagna, and the other dedicated to Kidane Mihret built during the reign of Haile Selassie. Various other holy sites are found around the mountain, mostly rock formations, while the monastery is the site of a biannual festival.\n\nThe explorers Orazio Antinori, Antonelli and Antonio Cecchi used Zuqualla to determine various geographical locations in May 1881. Dr Scott, on behalf of Cambridge University and the British Museum, secured a large and valuable entomological collection near Zuqualla in 1926. Three of the leaders of the attempted 1960 Ethiopian coup fled to Zuqualla from the capital, where the Moja family had land. Two of them lost their lives 24 December while Mengistu Neway, seriously wounded, was captured and brought to the capital for trial.\n\n\n"}
{"id": "2208292", "url": "https://en.wikipedia.org/wiki?curid=2208292", "title": "Neoevolutionism", "text": "Neoevolutionism\n\nNeoevolutionism as a social theory attempts to explain the evolution of societies by drawing on Charles Darwin's theory of evolution while discarding some dogmas of the previous theories of social evolutionism. Neoevolutionism is concerned with long-term, directional, evolutionary social change and with the regular patterns of development that may be seen in unrelated, widely-separated cultures.\n\nSociological neoevolutionism emerged in the 1930s. It developed extensively in the period after the Second World War—and was incorporated into anthropology as well as into sociology in the 1960s.\n\nNeoevolutionary theories are based on empirical evidence from fields such as archaeology, paleontology, and historiography. Proponents say neoevolutionism is objective and simply descriptive, eliminating any references to a moral or cultural system of values.\n\nWhile the 19th-century cultural evolutionism explained how culture develops by describing general principles of its evolutionary process, it was dismissed by historical particularism as unscientific in the early 20th century. Neoevolutionary thinkers brought back evolutionary ideas and developed them, with the result that they became acceptable to contemporary anthropology.\n\nNeoevolutionism discards many ideas of classical social evolutionism, notably the emphasis on social progress, so dominant in previous sociological evolution-related theories. Neoevolutionism discards the determinism argument and introduces probability, arguing that accidents and free will have much impact on the process of social evolution. It also supports counterfactual history—asking \"what if?\" and considering different possible paths that social evolution may (or might) have taken, and thus allows for the fact that various cultures may develop in different ways, some skipping entire \"stages\" others have passed through. Neoevolutionism stresses the importance of empirical evidence. While 19th-century social evolutionism used value judgments and assumptions when interpreting data, neoevolutionism relies on measurable information for analyzing the process of cultural evolution.\n\nImportant thinkers for neoevolutionism include:\n\n\n"}
{"id": "37562515", "url": "https://en.wikipedia.org/wiki?curid=37562515", "title": "Northern Zanzibar-Inhambane coastal forest mosaic", "text": "Northern Zanzibar-Inhambane coastal forest mosaic\n\nNorthern Zanzibar-Inhambane coastal forest mosaic is a tropical moist broadleaf forest ecoregion of coastal East Africa.\n\nThe ecoregion extends along the East African coast from southern Somalia through Kenya to the Lukuledi river in Tanzania, which forms its southern boundary. It also encompasses the Zanzibar Archipelago, including Unguja (Zanzibar) Pemba, and the surrounding smaller islands. The ecoregion includes a variety of habitats, including forest, savanna and swamps.\n\nThe ecoregion is bounded on the east by the Indian Ocean. It transitions to drier open woodlands and shrublands to the north and west: the Somali Acacia-Commiphora bushlands and thickets in the north, the Northern Acacia-Commiphora bushlands and thickets and Southern Acacia-Commiphora bushlands and thickets west of the central portion, and the Eastern Miombo woodlands to the southwest. To the south, it borders the Southern Zanzibar-Inhambane coastal forest mosaic across the Lukuledi River.\n\nThe ecoregion is home to ten endemic species of birds. Four are restricted to the island of Pemba (\"Treron pembaensis, Nectarinia pembae, Zosterops vaughani\" and \"Otus pembaensis\"). The remaining six are found on the mainland; one in the lower Tana River of Kenya (\"Cisticola restrictus\"), and four in the mainland coastal forest remnants – \"Erythrocercus holochlorus, Anthus sokokensis,\" Clarke's Weaver (\"Ploceus golandi\"), and \"Campethera mombassica\". \"Anthus melindae\" is endemic to the coastal grasslands in Kenya.\n\n\n\n"}
{"id": "13165796", "url": "https://en.wikipedia.org/wiki?curid=13165796", "title": "Ocean heat content", "text": "Ocean heat content\n\nOceanic heat content (OHC) is the heat stored in the ocean. Oceanography and climatology are the science branches which study ocean heat content. Changes in the ocean heat content play an important role in the sea level rise, because of thermal expansion. It is with high confidence that ocean warming accounts for 90% of the energy accumulation from global warming between 1971 and 2010. About one third of that extra heat has been estimated to propagate to depth below 700 meters.\n\nThe areal density of ocean heat content between two depth levels is defined using a definite integral:\n\nformula_1\n\nwhere formula_2 is seawater density, formula_3 is the specific heat of sea water, h2 is the lower depth, h1 is the upper depth, and formula_4 is the temperature profile. In SI units, formula_5 has units of J·m. Integrating this density over an ocean basin, or entire ocean, gives the total heat content, as indicated in the figure to right. Thus, the total heat content is the product of the density, specific heat capacity, and the volume integral of temperature over the three-dimensional region of the ocean in question.\n\nOcean heat content can be estimated using temperature measurements obtained by a Nansen bottle, an ARGO float, or ocean acoustic tomography. The World Ocean Database Project is the largest database for temperature profiles from all of the world’s oceans.\n\nThe upper Ocean heat content in most North Atlantic regions is dominated by heat transport convergence (a location where ocean currents meet), without large changes to temperature and salinity relation.\n\nSeveral studies in recent years have found a multi-decadal oscillation increase in OHC of the deep and upper ocean regions and attribute the heat uptake to anthropogenic warming. Studies based on \"ARGO\" indicate that ocean surface winds, especially the subtropical trade winds in the Pacific Ocean, change ocean heat vertical distribution. This results in changes among ocean currents, and an increase of the subtropical overturning, which is also related to the El Niño and La Niña phenomenon. Depending on stochastic natural variability fluctuations, during La Niña years around 30% more heat from the upper ocean layer is transported into the deeper ocean.\nModel studies indicate that ocean currents transport more heat into deeper layers during La Niña years, following changes in wind circulation. Years with increased ocean heat uptake have been associated with negative phases of the interdecadal Pacific oscillation (IPO). This is of particular interest to climate scientists who use the data to estimate the \"ocean heat uptake\".\n\nA study in 2015 concluded that ocean heat content increases by the Pacific Ocean, were compensated by an abrupt distribution of OHC into the Indian Ocean.\n\n\n"}
{"id": "616752", "url": "https://en.wikipedia.org/wiki?curid=616752", "title": "Ozone–oxygen cycle", "text": "Ozone–oxygen cycle\n\nThe ozone–oxygen cycle is the process by which ozone is continually regenerated in Earth's stratosphere, converting ultraviolet radiation (UV) into heat. In 1930 Sydney Chapman resolved the chemistry involved. The process is commonly called the Chapman cycle by atmospheric scientists.\n\nMost of the ozone production occurs in the tropical upper stratosphere and mesosphere. The total mass of ozone produced per day over the globe is about 400 million metric tons. The global mass of ozone is relatively constant at about 3 billion metric tons, meaning the Sun produces about 12% of the ozone layer each day.\n\nAnd if two oxygen atoms meet, they react to form one oxygen molecule:\nThis reaction is known to have a negative order of reaction of -1.\nThe overall amount of ozone in the stratosphere is determined by a balance between production by solar radiation and removal. The removal rate is slow, since the concentration of O atoms is very low.\n\nCertain free radicals, the most important being hydroxyl (OH), nitric oxide (NO) and atoms of chlorine (Cl) and bromine (Br), catalyze the recombination reaction, leading to an ozone layer that is thinner than it would be if the catalysts were not present.\n\nMost of the OH and NO are naturally present in the stratosphere, but human activity, especially emissions of chlorofluorocarbons (CFCs) and halons, has greatly increased the Cl and Br concentrations, leading to ozone depletion. Each Cl or Br atom can catalyze tens of thousands of decomposition reactions before it is removed from the stratosphere.\n\n"}
{"id": "5589335", "url": "https://en.wikipedia.org/wiki?curid=5589335", "title": "Penetration depth", "text": "Penetration depth\n\nPenetration depth is a measure of how deep light or any electromagnetic radiation can penetrate into a material. It is defined as the depth at which the intensity of the radiation inside the material falls to 1/e (about 37%) of its original value at (or more properly, just beneath) the surface.\n\nWhen electromagnetic radiation is incident on the surface of a material, it may be (partly) reflected from that surface and there will be a field containing energy transmitted into the material. This electromagnetic field interacts with the atoms and electrons inside the material. Depending on the nature of the material, the electromagnetic field might travel very far into the material, or may die out very quickly. For a given material, penetration depth will generally be a function of wavelength.\n\nAccording to Beer-Lambert law, the intensity of an electromagnetic wave inside a material falls off exponentially from the surface as\n\nIf formula_2 denotes the penetration depth, we have \n\n\"Penetration depth\" is one term that describes the decay of electromagnetic waves inside of a material. The above definition refers to the depth formula_4 at which the intensity or power of the field decays to 1/e of its surface value. In many contexts one is concentrating on the field quantities themselves: the electric and magnetic fields in the case of electromagnetic waves. Since the power of a wave in a particular medium is proportional to the \"square\" of a field quantity, one may speak of a penetration depth at which the magnitude of the electric (or magnetic) field has decayed to 1/e of its surface value, and at which point the \"power\" of the wave has thereby decreased to formula_5 or about 13% of its surface value:\n\nNote that formula_7 is identical to the skin depth, the latter term usually applying to metals in reference to the decay of electrical currents (which follow the decay in the electric or magnetic field due to a plane wave incident on a bulk conductor). The attenuation constant formula_8 is also identical to the (negative) real part of the propagation constant, which may also be referred to as formula_9 using a notation inconsistent with the above use. When referencing a source one must always be careful to note whether a number such as formula_9 or formula_11 refers to the decay of the field itself, or of the intensity (power) associated with that field. It can also be ambiguous as to whether a positive number describes attenuation (reduction of the field) or gain; this is usually obvious from the context.\n\nThe attenuation constant for an electromagnetic wave at normal incidence on a material is also proportional to the imaginary part of the material's refractive index \"n\". Using the above definition of formula_9 (based on intensity) the following relationship holds:\n\nwhere formula_14 denotes the \"complex\" index of refraction, formula_15 is the radian frequency of the radiation, \"c\" is the speed of light in vacuum and formula_16 is the wavelength. Note that formula_17 is very much a function of frequency, as is its imaginary part which is often not mentioned (it is essentially zero for transparent dielectrics). The complex refractive index of \"metals\" is also infrequently mentioned but has the same significance, leading to a penetration depth (or skin depth formula_7) accurately given by a formula which is valid up to microwave frequencies.\n\nRelationships between these and other ways of specifying the decay of an electromagnetic field are further detailed in the article: Mathematical descriptions of opacity.\n\nIt should also be noted that we are only specifying the decay of the field which may be due to absorption of the electromagnetic energy in a lossy medium or may simply describe the penetration of the field in a medium where no loss occurs (or a combination of the two). For instance, a hypothetical substance may have a complex index of refraction formula_19. A wave will enter that medium without significant reflection and will be totally absorbed in the medium with a penetration depth (in field strength) offormula_20, where formula_21 is the vacuum wavelength. A different hypothetical material with a complex index of refraction formula_22 will \"also\" have a penetration depth of 16 wavelengths, however in this case the wave will be perfectly reflected from the material! No actual absorption of the radiation takes place, however the electric and magnetic fields extend well into the substance. In either case the penetration depth is found directly from the imaginary part of the material's refractive index as is detailed above.\n\n"}
{"id": "52662393", "url": "https://en.wikipedia.org/wiki?curid=52662393", "title": "ProSTEP iViP", "text": "ProSTEP iViP\n\nprostep ivip is an association with its headquarter in Darmstadt, Germany. It is a globally active, independent association of 180 member companies from industry, IT and research. As an industry-driven association it focuses on the digital transformation in product creation and production. By designing the digital transformation in the manufacturing industry prostep ivip defines and aggregates the requirements of manufacturers and suppliers, with the goal of defining standards and interfaces primarily for the digitalization of the entire product creation process – from idea to implementation. Founded in 1993 as ProSTEP Association for the Promotion of Product Data Standards and renamed to ProSTEP iViP Association in 2002. Since May 2017, the association's name is written as \"prostep ivip\".\n\nAfter the end of the ProSTEP Initiative of the German Federal Ministry for Economic Affairs and Energy (German acronym: BMWi), the ProSTEP Association was founded in 1993. Leading IT managers at BMW, Bosch, Continental, Daimler, Delphi, Opel, Siemens, Volkswagen and 30 other companies realized that the development of modern processes for efficient product data management was crucial to ensuring the ability of German companies to compete in the global marketplace and that they can address their common aims at best when joining under the neutral umbrella of an association.\n\nThe starting point for this endeavor was the joint development of the STEP data format (ISO 10303). In 2002, it merged with the initiative \"Integrated Virtual Product Creation (German acronym: iViP)\" of the German Federal Ministry of Education and Research (German acronym: BMBF), which led to a massive scope-extension. Until today, the prostep ivip association remains committed to developing new approaches to end-to-end process, system and data integration for its members and providing digital support for all the phases of the product creation process.\n\n40% of today's 180 member companies in ProSTEP iViP are manufacturing companies (manufacturers and suppliers), 40% are IT companies and service providers and 20% are research institutions and other standardization bodies. This tripartism is also reflected within the by-annually elected board of the association: one representative of the manufacturers, one of the suppliers, one of the IT and one of the research institutions. prostep ivip's Technical Programm, with its currently over 20 running project groups, is governed be the Technical Steering Committee (TSC).\n\nprostep ivip maintains and continuously expands its network toward like-minded organizations. Examples for these organizations are AIA, ISO, OMG as well as associations like the French GALIA, the Japanese JAMA, the US-based PDES, Inc., the German VDA etc.\n\nProSTEP iViP publishes Standards, Recommendations, White Paper and Best Practices, also together with its partner organizations. For example:\n\nEach year in spring prostep ivip conducts one of world's largest neutral PLM Congresses: the prostep ivip symposium. Beside this, it invites to smaller topic-specific events and Webinars.\n\n"}
{"id": "16538579", "url": "https://en.wikipedia.org/wiki?curid=16538579", "title": "Renewable Electricity and the Grid", "text": "Renewable Electricity and the Grid\n\nRenewable Electricity and the Grid: The Challenge of Variability is a 2007 book edited by Godfrey Boyle which examines the significance of the issue of variability of renewable energy supplies in the electricity grid.\n\nThe energy available from sun, wind, waves, and tides varies in ways which may not match variations in consumer energy demand. Assimilating these fluctuations can affect the operation and economics of electricity networks and markets. There are many myths and misunderstandings surrounding this topic. \"Renewable Electricity and the Grid\" presents technical and operational solutions to the problem of reconciling the differing patterns of power supply and demand.\n\nChapters of \"Renewable Electricity and the Grid\" are authored by leading experts, who explain and quantify the impacts of renewable energy variability. Godfrey Boyle (editor) is Director of the Energy and Environment Research Unit at the UK Open University and has written the textbooks \"Energy Systems and Sustainability\" (2003) and \"Renewable Energy: Power for a Sustainable Future\" (2004). He is a Fellow of the Institution of Engineering and Technology and a Trustee of the National Energy Foundation.\n\nOther authors include:\nDr Bob Everett, open University,\nDr Mark Barret, Open University,\nDr Fred Starr, EU Energy Center at Petten\nDave Andrews, Wessex Water, Energy Manager\nBrian Hurley - Airtricity\n\n"}
{"id": "8912761", "url": "https://en.wikipedia.org/wiki?curid=8912761", "title": "Restinga", "text": "Restinga\n\nRestinga () — a spit and a distinct type of coastal Tropical and subtropical moist broadleaf forest, found in eastern Brazil.\n\n\"Restingas\" form on sandy, acidic, and nutrient-poor soils, and are characterized by medium-sized trees and shrubs adapted to the drier and nutrient-poor conditions.\n\nOne of the most notable restingas is the Restinga da Marambaia (in Rio de Janeiro), which is owned and kept by the Brazilian Army.\n\nThe World Wildlife Fund distinguishes two \"Restinga\" ecoregions. \n\n\n"}
{"id": "58826853", "url": "https://en.wikipedia.org/wiki?curid=58826853", "title": "Surface rupture", "text": "Surface rupture\n\nSurface rupture (or ground rupture, or ground displacement) is the visible offset of the ground surface when an earthquake rupture along a fault affects the Earth's surface. Surface rupture is opposed by buried rupture, where there is no displacement at ground level. This is a major risk to any structure that is built across a fault zone that may be active, in addition to any risk from ground shaking. Surface rupture entails vertical or horizontal movement, on either side of a fault which has ruptured. Surface rupture can affect large areas of land.\n\nNot every earthquake results in surface rupture, particularly for smaller and deeper earthquakes.. In some cases, however, the lack of surface effects is because the fault that moved does not reach the surface. For example, the 1994 Northridge earthquake had a moment magnitude of 6.7, caused major damage in the Los Angeles area, occurred at below the Earth's surface, but did not cause surface rupture, because it was a blind thrust earthquake.\n\nSurface ruptures commonly occur on pre-existing faults. Only rarely are earthquakes (and surface ruptures) associated with faulting on entirely new fault structures. There is shallow hypocenter, and large fracture energy on the asperities, the asperity shallower than . Examples of such earthquakes are San Fernando earthquake, Tabas earthquake, and Chi-Chi earthquake.\n\nIn surface rupture earthquakes, the large slips of land are concentrated in the shallow parts of the fault. And, notably, permanent ground displacements which are measureable can be produced by shallow earthquakes, of magnitude M5 and greater.\n\nThe form that surface rupturing takes depends on two things: the nature of the material at the surface and the type of fault movement.\n\nWhere there are thick superficial deposits overlying the trace of the faults the resulting surface effects are typically more discontinuous. Where there is little or no superficial deposits the surface rupture is generally continuous, except where the earthquake rupture affects more than one fault, which can lead to complex patterns of surface faulting, such as in the 1992 Landers earthquake.\n\nSurface ruptures associated with normal faults are typically simple fault scarps. Where there are significant superficial deposits, sections with more oblique faulting may from sets of en-echelon scarp segments. Antithetic faults may also develop giving rise to surface grabens.\n\nReverse faulting (or particularly thrust faulting) is associated with more complex surface rupture patterns as the protruding unsupported part of the hanging-wall of the fault is liable to collapse. In addition there may be surface folding and back-thrust development.\n\nStrike-slip faults are associated with dominantly horizontal movement, leading to relatively simple linear zones of surface rupture where the fault is a simple planar structure. However, many strike-slip faults are formed of overlapping segments, leading to complex zones of normal or reverse faulting depending on the nature of the overlap. Additionally, where there are thick superficial deposits, the rupture typically appears as a set of en-echelon faults.\n\nTo retrofit a house to survive surface rupture requires engineered design by geotechnical, and structural or civil engineers. This can be quite expensive.\n\n\n\n"}
{"id": "3572234", "url": "https://en.wikipedia.org/wiki?curid=3572234", "title": "Terrace (geology)", "text": "Terrace (geology)\n\nIn geology, a terrace is a step-like landform. A terrace consists of a flat or gently sloping geomorphic surface, called a tread, that is typically bounded one side by a steeper ascending slope, which is called a \"riser\" or \"scarp.\" The tread and the steeper descending slope (riser or scarp) together constitute the terrace. Terraces can also consist of a tread bounded on all sides by a descending riser or scarp. A narrow terrace is often called a bench.\n\nThe sediments underlying the tread and riser of a terrace are also commonly, but incorrectly, called terraces, leading to confusion.\n\nTerraces are formed in various ways.\n\nFluvial terraces are remnants of the former floodplain of a stream or river. They are formed by the downcutting of a river or stream channel into and the abandonment and lateral erosion of its former floodplain. The downcutting, abandonment, and lateral erosion of a former floodplain can be the result of either changes in sea level, local or regional tectonic uplift; changes in local or regional climate; changes the amount of sediment being carried by the river or stream; change in discharge of the river; or a complex mixture of these and other factors. The most common sources of the variations in rivers and streams that create fluvial terraces are vegetative, geomorphic, and hydrologic responses to climate. More recently, the direct modification of rivers and streams and their watersheds by cultural processes have result in the development of terraces along many rivers and streams.\n\nKame terraces are formed on the side of a glacial valley and are the deposits of meltwater streams flowing between the ice and the adjacent valley side.\n\nA marine terrace represents the former shoreline of a sea or ocean. It can be formed by marine abrasion or erosion of materials comprising the shoreline (marine-cut terraces or wave-cut platforms); the accumulations of sediments in the shallow-water to slightly emerged coastal environments (marine-built terraces or raised beach); or the bioconstruction by coral reefs and accumulation of reef materials (reef flats) in intertropical regions.\n\nThe formation of a marine terrace follows this general process: A wave cut platform must be carved into bedrock (high wave energy is needed for this process). Although this is the first step to the process for the formation of a marine terrace, not all wave cut platforms will become a marine terrace. After the wave cut platform is formed it must be removed from interaction with the high wave energy. This process happens by either change in sea level due to glacial-interglacial cycles or tectonically rising landmasses. When the wave cut has been raised above sea level it is preserved. The terraces are most commonly preserved in flights along the coastline.\n\nA lake (lacustrine) terrace represents the former shoreline of either a nonglacial, glacial, or proglacial lake. As in case of marine terraces, a lake terrace can be formed by either the abrasion or erosion of materials comprising the shoreline, the accumulations of sediments in the shallow-water to slightly emerged environments, or some combination of these. Given the smaller size of lakes relative to the size of typical marine water bodies, lake terraces are overall significantly narrower and less well developed than marine terraces. However, not all lake terraces are relict shorelines. In case of the lake terraces of ancient ice-walled lakes, some proglacial lakes, and alluvium-dammed (slackwater) lakes, they often represent the relict bottom of these lakes. Finally, glaciolacustrine kame terraces are either the relict deltas or bottoms of ancient ice marginal lakes.\n\nIn geomorphology, a structural terrace is a terrace created by the differential erosion of flat-lying or nearly flat-lying layered strata. The terrace results from preferential stripping by erosion of a layer of softer strata from an underlying layer of harder strata. The preferential removal of softer material exposes the flat surface of the underlying harder layer creating the tread of a structural terrace. Structural terraces are commonly paired and not always associated with river valleys.\n\nA travertine terrace is formed when geothermally heated supersaturated alkaline waters emerge to the surface and form waterfalls of precipitated carbonates.\n\n\n"}
{"id": "37462359", "url": "https://en.wikipedia.org/wiki?curid=37462359", "title": "Timeline of the Magellan–Elcano circumnavigation", "text": "Timeline of the Magellan–Elcano circumnavigation\n\nThe Magellan–Elcano circumnavigation was the first voyage around the world in human history. It was a Spanish expedition that sailed from Seville in 1519 under the command of Ferdinand Magellan, a Portuguese, in search of a maritime path from Spain to East Asia through the Americas and across the Pacific Ocean, and concluded by Spanish navigator Juan Sebastian Elcano in 1522. Elcano and the 18 survivors of the expedition were the first men to circumnavigate the globe in a single expedition. \n\nThe Spanish fleet, the \"Armada de Molucca\", that left Spain on 20 September 1519 consisted of five ships with 270 men: \"Trinidad\" under Magellan, Captain General; \"San Antonio\" under Juan de Cartagena; \"Concepcion\" under Gaspar de Quesada; \"Santiago\" under João Serrão; and \"Victoria\" under Luiz Mendoza. \n\nAfter crossing the Atlantic and wintering in South America the expedition navigated the Straits of Magellan, then crossed the Pacific to the Philippines.\n\nFollowing Magellan's death in Mactan (Philippines) in 1521, Juan Sebastián Elcano took command of the ship \"Victoria\", sailing back to Spain across the Indian Ocean, round the Cape of Good Hope and north along the west coast of Africa.\nThe circumnavigation was completed by Elcano and a crew of 18 men in \"Victoria\", returning to Spain three years after they left on 6 September 1522.\n\nAugust 10: Departure from Seville. \n\nSeptember 20: Departure from Sanlúcar de Barrameda.\n\nDecember 13: Entering the bay of Rio de Janeiro.\n\nDecember 31: Departure from Rio de Janeiro.\n\nJanuary 10: Entering the Río de la Plata.\n\nFebruary 27: Entering Bahia de los Patos.\n\nMarch 31: Begin of the overwintering stay at Puerto San Julián.\n\nApril 1 and 2: Mutiny on \"Victoria\", \"Concepcion\" and \"San Antonio\"; death of Louis de Mendoza. Later execution of de Quesada, marooning of de Cartagena. Alvaro de Mesquita becomes captain of \"San Antonio\", Duarte Barbosa of \"Victoria\". \n\nEnd of April: \"Santiago\" is sent on a mission to find the passage. The ship is caught in a storm and wrecked. Survivors return to Puerto San Julián. Serrano (João Serrão) becomes captain of the \"Concepcion\".\n\nJuly: Encounters with the “Patagonian giants” (likely the Tehuelche people).\n\nAugust 24: Departure from Puerto San Julián.\n\nOctober 21: Arriving at the Cape of the Eleven Thousand Virgins, entry to what would be known as Strait of Magellan.\n\nEnd of October: \"San Antonio\", charged to explore Magdalen Sound, fails to return to the fleet, instead sails back to Spain under Estêvão Gomes who imprisoned captain de Mesquita. The ship arrives in Spain on May 21, 1521.\n\nNovember 28: The fleet leaves the strait and enters the Pacific Ocean.\n\nMarch 6: Arrival at Guam and encounters with the Chamorro people.\n\nMarch 16: Arrival of Magellan's expedition to one of the Philippine Islands. They headed to Suluan and dropped anchor for a few hours of respite. Suluan is a small island in the province of Eastern Samar. They then next dropped anchor at Homonhon, another small island in the province of Eastern Samar. They were detected by the boats of Rajah Kolambu who was visiting Mazaua, who later guided them to Cebu, on April 7.\n\nApril 7: Arrival at the Rajahnate of Cebu.\n\nApril 27: Death of Magellan in the Battle of Mactan. Serrano and Barbosa are voted co-commanders.\n\nMay 1: At a local banquet Barbosa and 27 sailors (including Afonso de Góis, the new captain of \"Victoria\" after the election of Barbosa and Serrão) are murdered and Serrao captured, later killed. The three remaining ships escape.\n\nMay 2: There are not enough men to handle three ships, thus the worm-infested \"Concepcion\" is burned down. Two ships remain: \"Victoria\" and \"Trinidad\". Gonzalo Gomez de Espinosa becomes captain of \"Victoria\". Joao Lopez Carvalho is Captain General. The ships sail to Mindanao and Brunei.\n\nSeptember 21: Carvalho is replaced by Martín Méndez as Captain General, Espinosa becomes captain of the \"Trinidad\" and Juan Sebastián Elcano captain of \"Victoria\".\n\nNovember 8: Arriving at Tidore in the Moluccas.\n\nDecember 21: \"Victoria\" under the command of Elcano leaves the Moluccas to return home, sailing west towards the Cape of Good Hope. \"Trinidad\" remains at Tidore for repairs.\n\nJanuary 25: \"Victoria\" reaches Timor and starts to cross the Indian Ocean.\n\nApril 6: \"Trinidad\" under the command of Espinosa leaves the Moluccas heading home sailing east. After five weeks, Espinosa decides to return to the Moluccas where he and his ship are captured by a Portuguese fleet under Antonio de Brito. However, the ship was wrecked during a storm.\nMay 22: \"Victoria\" passes the Cape of Good Hope and enters the Atlantic Ocean.\n\nJuly 9: \"Victoria\" reaches Santiago, Cape Verde.\n\nSeptember 6: \"Victoria\" returns to Sanlúcar de Barrameda under the command of Elcano, two weeks shy of three years after setting sail.\n\nSeptember 8: \"Victoria\" arrives at Seville, technically completing the circumnavigation.\n\n"}
{"id": "49614667", "url": "https://en.wikipedia.org/wiki?curid=49614667", "title": "Tistarite", "text": "Tistarite\n\nTistarite is an exceedingly rare mineral with the formula TiO, thus being the natural analogue of titanium(III) oxide. In terms of chemistry it is the titanium-analogue of hematite, corundum, eskolaite, and karelianite. Other minerals with the general formula AO are arsenolite, avicennite, claudetite, bismite, bixbyite, kangite, sphaerobismoite, yttriaite-(Y) and valentinite. Tistarite and grossmanite - both found in the famous Allende meteorite (so is kangite) - are the only currently known minerals with trivalent titanium. Titanium in minerals is almost exclusively tetravalent. The only known terrestrial occurrence of tistarite was found during minerals exploration by Shefa Yamim Ltd. in the upper mantle beneath Mount Carmel, Israel.\n"}
{"id": "42145537", "url": "https://en.wikipedia.org/wiki?curid=42145537", "title": "Transpolar Sea Route", "text": "Transpolar Sea Route\n\nThe Transpolar Sea Route (TSR) is a future Arctic shipping route running from the Atlantic Ocean to the Pacific Ocean across the center of the Arctic Ocean.\n\nThe route is also sometimes called Trans-Arctic Route. In contrast to the Northeast Passage (including the Northern Sea Route) and the North-West Passage it largely avoids the territorial waters of Arctic states and lies in international high seas. The route is currently only navigable by heavy icebreakers. However, due to the increasing decline of Arctic sea ice extent, the route is slated to emerge as the predominant Arctic shipping route by 2030.\n\nThe TSR is about long and offers significant distance savings between Europe and Asia. It is the shortest of the Arctic shipping routes. In contrast to the Northern Sea Route and the North-West Passage, which are both coastal routes, the TSR is a mid-ocean route and passes close to the North Pole. Due to high seasonal variability of ice conditions throughout the entire Arctic basin, the TSR will not exist as one fixed shipping lane, but will follow a number of navigational routes.\n\nThe TSR passes outside the exclusive economic zones of Arctic coastal states making it of special geopolitical importance to countries looking towards the Arctic as a future trade route. While a number of legal disagreement and uncertainties revolve around both the North-West Passage and the Northern Sea Route, the TSR lies outside the territorial jurisdiction of any state. The Chinese icebreaker \"Snow Dragon\" was one of the first major vessels to utilize the route during its 2012 journey through the Arctic Ocean.\n"}
{"id": "13249308", "url": "https://en.wikipedia.org/wiki?curid=13249308", "title": "USS Sea Gull (1838)", "text": "USS Sea Gull (1838)\n\nUSS \"Sea Gull\" was a schooner in the service of the United States Navy. The \"Sea Gull\" was one of six ships that sailed in the US Exploring Expedition (known as the US Ex. Ex.) in 1838 to survey the coast of the then-unknown continent of Antarctica and the Pacific Islands. The specimens collected on the voyage would later form the backbone of the Smithsonian Institution.\n\nFormerly the New York pilot boat \"New Jersey\", the ship was purchased by the Navy in July 1838 and renamed USS \"Sea Gull\". She was outfitted with a new mast and sails in three days' time, and under the command of Passed Midshipman James W. E. Reid, sailed for Hampton Roads to join the expedition as a tender.\n\nAt Norfolk, Virginia the \"Sea Gull\" joined the other ships of the expedition: flagship , , , the schooner , and the supply ship .\n\nThe ships left Norfolk on August 18, 1838, for the tip of South America, where they would await the slower \"Relief\" and then continue to Antarctica and the Pacific Islands. After surveying and collecting specimens, the remaining ships would sail to Hawaii and then the Columbia River to survey that area then return to the United States via the Cape of Good Hope on June 9, 1842.\n\nThe \"Sea Gull\", commanded by Lieutenant Robert Johnson and in the company of the \"Porpoise\", headed south from Orange Bay on the tip of South America on February 25, 1839, to explore the area of the South Shetland Islands. Both ships encountered heavy seas which resulted in a broken gaff for the \"Sea Gull\"; the crew was constantly drenched by huge waves. Soon they encountered snow squalls and penguins. Huge icebergs were sighted, some said to be as large as the U.S. Capitol building. On March 1, some islands of the South Shetlands were sighted. Attempts were made to land on the islands and gather specimens but the seas proved too rough to make a landing. On March 5, the wind increased to a whole gale and the commander of the expedition — Lt. Charles Wilkes — ordered the ships about and headed north. Wilkes ordered Johnson to proceed back to Orange Bay after stopping at Deception Island to attempt to retrieve a self-reading thermometer left there by an earlier British expedition. After removing ice from the ship's rigging, the crew of the \"Sea Gull\" headed for Deception. The crew didn't find the thermometer but did experience the volcanism of the island finding it frightening and unnerving to know they were standing on an active volcano. Eventually the ship made its way to Orange Bay. After reaching Orange Bay the \"Sea Gull\" participated in a search for a missing crew in a survey launch, finding the launch eventually safe and sound with all aboard.\n\nOn April 17, 1839, Wilkes left Orange Bay in \"Vincennes\" with \"Porpoise\" for Valparaíso, Chile and ordered the schooners \"Flying Fish\" and \"Sea Gull\" to wait ten days for the supply ship \"Relief\". If the \"Relief\" didn't arrive they were to transport the scientists aboard to Valparaíso. On May 19, the \"Flying Fish\" arrived in Valparaíso and the \"Sea Gull\" was nowhere in sight. The \"Sea Gull\", under the command of passed midshipman James Reid, was last seen waiting out a gale in the lee of Staten Island off Cape Horn. After a month or so, the officers of the Ex. Ex. assumed the \"Sea Gull\" was lost and took up a collection for a monument to their memory. That monument stands in the Mount Auburn Cemetery in Cambridge, Massachusetts. It lists the names of the officers who were lost on the \"Sea Gull\" and the names of two officers who were killed during the survey of the Pacific Islands. No mention was made of the other 24 sailors and marines who died during the expedition.\n\n"}
{"id": "4599491", "url": "https://en.wikipedia.org/wiki?curid=4599491", "title": "Upper Guinean forests", "text": "Upper Guinean forests\n\nThe Upper Guinean forests is a tropical seasonal forest region of West Africa. The Upper Guinean forests extend from Guinea and Sierra Leone in the west through Liberia, Côte d'Ivoire and Ghana to Togo in the east, and a few hundred kilometers inland from the Atlantic coast. A few enclaves of montane forest lie further inland in the mountains of central Guinea and central Togo and Benin. \n\nIn the drier interior, the Upper Guinean forests yield to the Guinean forest-savanna mosaic, a belt of dry forests and savannas that lies between the coastal forests and the savannas and grasslands of the Sudan further north. The Dahomey Gap, a region of Togo and Benin where the Guinean forest-savanna mosaic extends to the Atlantic coast, separates the Upper Guinean forests from the Lower Guinean forests to the east, which extend from eastern Benin through Nigeria, Cameroon, and south along the coast of the Gulf of Guinea. The Upper Guinean forests are a Global 200 ecoregion.\n\nThe Guinean moist forests are much affected by winds from the hot dry area to the north and the cool Atlantic currents. This gives the region a very seasonal climate with over of rain falling in some areas in the wet season. Over 2000 species of vascular plant have been recorded in the ecoregion, and mammals found here include the chimpanzee (\"Pan troglodytes\"), leopard (\"Panthera pardus\"), pygmy hippopotamus (\"Hexaprotodon liberiensis\"), Ogilby's duiker (\"Cephalophus ogilbyi\"), Nimba otter shrew (\"Micropotamogale lamottei\") and the African golden cat (\"Profelis aurata\"). There are twenty-one endemic and near-endemic forest birds in the ecoregion of which three, Nimba Flycatcher \"Melaenornis annamarulae,\" Gola Malimbe \"Malimbus ballmanni\" and Spot-winged Greenbul \"Phyllastrephus leucolepis\" are further restricted in distribution to the western forests only. \n\nThe World Wide Fund for Nature (WWF) designated the Upper Guinean forests, which it calls the Guinean moist forests, as one of its Global 200 critical regions for conservation.\nThe WWF divides the Upper Guinean forests into three ecoregions:\n\n\n"}
{"id": "1337327", "url": "https://en.wikipedia.org/wiki?curid=1337327", "title": "Vernal pool", "text": "Vernal pool\n\nVernal pools, also called vernal ponds or ephemeral pools, are seasonal pools of water that provide habitat for distinctive plants and animals. They are considered to be a distinctive type of wetland usually devoid of fish, and thus allow the safe development of natal amphibian and insect species unable to withstand competition or predation by fish. Certain tropical fish lineages (such as killifishes) have however adapted to this habitat specifically.\n\nDuring most years, a vernal pool basin will experience inundation from local surface runoff, followed by desiccation from evapotranspiration. These conditions are commonly associated with Mediterranean climate. Most pools are dry for at least part of the year, and fill with the winter rains or snow melt. Some pools may remain at least partially filled with water over the course of a year or more, but all vernal pools dry up periodically. A key time during vernal pool development between the flooding and evaporation phases is the flowering of native species, which attracts pollinators and seed distribution patterns.\n\nSome authorities restrict the definition of vernal pools to exclude seasonal wetlands that have defined inlet and outlet channels. The justification is that such seasonal wetlands tend to be qualitatively different from isolated vernal pools; this is because they are fed by larger drainage basins so that firstly, inflow contributes higher concentrations of dissolved minerals. Secondly, flow patterns increase the periodic scouring and silting effect of flows through or simply into the wetland. Thirdly, longer distance inflow and outflow make for less strictly endemic populations and plants. Low dissolved mineral concentrations of smaller vernal pool basins may be characterized as oligotrophic, and poorly buffered with rapid pH shifts due to carbon dioxide uptake during photosynthesis.\n\nVernal pools are so called because they are often, though not necessarily, at their maximum depth in the spring (\"vernal\" meaning of, relating to, or occurring in the spring). There are many local names for such pools, depending upon the part of the world in which they occur. Vernal pools may form in forests, but they are more typically associated with grasslands and rocky plains or basins. While many vernal pools are only a few meters in width, playas and prairie potholes are usually much larger, but still are otherwise similar in many respects, with high water in wet periods, followed by dry conditions. Some exclude desert playas from the definition of vernal pools because their larger closed drainage basins in areas with high evaporation rates produce higher concentrations of dissolved minerals, with salinity and alkalinity favoring different species. Playas may be inundated less frequently than vernal pools, and inundation typically coincides with colder weather unfavorable for plant growth.\n\nDespite being dry at times, vernal pools teem with life when filled. The most obvious inhabitants are various species of breeding frogs and toads. Some salamanders also utilize vernal pools for reproduction, but the adults may visit the pool only briefly. Other notable inhabitants are \"Daphnia\" and fairy shrimp, the latter often used as an indicator species to decisively define a vernal pool. Other indicator species, at least in New England, are the wood frog, the spadefoot toad, and some species of mole salamanders. Certain plant species are also associated with vernal pools, although the particular species depend upon the ecological region. The flora of South African vernal pools, for example, are different from those of Californian vernal pools, and they have characteristic Anostraca, such as various Branchipodopsis species. In some northern areas, tadpole shrimp are more common.\n\nVernal pools harbor a distinct assemblage of flora and fauna that, in some cases, aren't found anywhere else on the planet. Despite this fact, about 90% of vernal pool ecosystems in California have been destroyed. Disturbingly, much of this destruction has occurred in recent years, with about 13% of remaining vernal pools being lost in the short interval from 1995-2005. The major threats to vernal pool habitats in the Central Valley are agriculture, urbanization, changes in hydrology, climate change, and improperly managed grazing by livestock.\n\nVernal pools are prime habitats to be targeted for restoration work due to their value as hotpots of biodiversity as well as recent history of extensive destruction and degradation. However, there have been varying rates of success attributed to various restoration efforts. A number of hypotheses exists as to why:\n\nHypothesis 1: Constructed pools are too deep.\n\nHypothesis 2: Edges of constructed pools narrower than natural ones.\n\nHypothesis 3: Constructed pools have steeper slopes than natural ones.\n\nResults: Research suggest that the last two details (Hypothesis 2 & 3) are crucial in determining the habitat value of man-made vernal pools. In general, most constructed pools were too steep and did not have wide enough edges.\n\nThere has been a fair amount of controversy surrounding the practice of mitigation, which is the destruction of protected or endangered species and habitats, such as vernal pools, on the condition that whatever entity (business, land manager, etc.) is destroying the habitat will undertake the construction of a replacement habitat to \"mitigate\" their impacts. This concept is difficult to apply to vernal pools, which represent a tremendous habitat value- but are difficult to successfully replicate using construction methods (as mentioned above). Thus, it has been very controversial to apply mitigation strategies to vernal pool systems due to the obvious risks inherent in trying to reconstruct this kind of habitat. Although, some agencies are now requiring two replacements for every vernal pool that is destroyed, in order to compensate for the low quality of man-made habitat.\n\nVernal pools can form anywhere that a depression fills with water. They can be found on bedrock of many kinds, or in grasslands that form over a variety of soil types containing silts and clays. They can develop hydric soils which are typical of flooded areas, including accumulations of organic matter, but this may not happen in drier areas. In some cases there is a hard pan layer which causes the retention of water in the pools. The hardpan clay basin accumulates water due to the small particle size and therefore reduced porosity. This permits flooding and development of vernal pools.\n\nIn vernal pools, flowering occurs simultaneously because of the seasonality of favorable conditions. Vernal pool ecosystems may include both cosmopolitan species and endemic species adapted to unique environmental conditions. These include moisture gradients, salinity gradients, and reduced levels of competition. Mircrotopographical gradients also contribute to species distribution in vernal pool communities, where plants that flower sooner in the season are more likely to be found at slightly higher elevations than later flowering species. Many vernal pool plants have buried seeds which accumulate in the soil. Different species are suited to different moisture levels, and as water evaporates from the edges of a pool, distinctive zonation of species can be seen.\n\nMany upland perennial plants are unable to withstand the duration of vernal pool inundation; while many wetland plants are unable to withstand desiccation. Wetland plants may also be nutrient limited within small drainage basins. When dissolved carbon dioxide is depleted by daytime photosynthesis, vernal pool species like Howell's quillwort (\"Isoetes howellii\") and pygmyweed (\"Crassula aquatica\") collect carbon dioxide nocturnally using Crassulacean acid metabolism. Vernal pool basin habitats favor annual plants with some uniquely adapted perennials which suffer extensive mortality resembling annual reproduction. Annuals comprise approximately 80 percent of vernal pool flora. Listed below are some genera of the approximately one hundred vascular plant species associated with California vernal pool habitats; although a typical pool will include only 15 to 25 species.\n\nUpland plants commonly found at vernal pools in California include yellow pansies, several sweet-scented clovers, yellow and bright lavender monkeyflowers, star lilies, and yarrow.\n\nVernal pools are often threatened by development in the same way that other wetlands are. As a result, most pools have been converted into residential zones, roads, and industrial parks. That is why most extant pools occur on protected or private land such as national parks, and ranches.\n\nA large number of rare, endangered species, and endemic species occur in vernal pool areas. For example, the San Diego mesa mint, a highly endangered plant, is found exclusively in vernal pools in the San Diego area. Another example is the wildflower \"Lasthenia conjugens\", which is found in limited parts of the San Francisco Bay Area. A third example is the herb \"Limnanthes vinculans\" endemic to Sonoma County, California.\n\nMany of the amphibians that breed only in vernal pools spend most of their lives in the uplands within hundreds of feet of the vernal pool. Eggs are laid in the vernal pool, then the juveniles leave the pool two or three months later, not to return until the following spring to breed. Therefore, the upland areas surrounding a vernal pool are critical for the survival of these species. In New York state, the endangered tiger salamander (\"Ambystoma tigrinum\") is dependent on vernal pools to breed as described above. A few other obligate vernal pool species are the marbled salamander (\"Ambystoma opacum\"), Jefferson's salamander (\"Ambystoma jeffersonianum\"), the blue-spotted salamander (\"Ambystoma laterale\") and the spotted salamander (\"Ambystoma maculatum\").\n\nSome other species, notably Anostraca, fairy shrimp and their relatives, lay eggs capable of entering a state of cryptobiosis. They hatch when rains replenish the water of the pool, and no stage of the animals' life cycle leaves the pool, except when eggs are accidentally transported by animal phoresis, wind, or rarely, by flood. Such animal populations may be very old indeed, when the conditions for seasonal vernal waters are stable enough. As an extreme example, \"Branchipodopsis relictus\" on the main island of the Socotra archipelago, which is exceedingly remote for what it is, a continental fragment of Gondwana, is believed to have been isolated since the Miocene. \"Branchipodopsis relictus\" is correspondingly isolated genetically as well as geographically.\n\n\n"}
{"id": "1192008", "url": "https://en.wikipedia.org/wiki?curid=1192008", "title": "Wolf number", "text": "Wolf number\n\nThe Wolf number (also known as the International sunspot number, relative sunspot number, or Zürich number) is a quantity that measures the number of sunspots and groups of sunspots present on the surface of the sun.\n\nThe idea of computing sunspot numbers was originated by Rudolf Wolf in 1848 in Zurich, Switzerland and, thus, the procedure he initiated bears his name (or place). The combination of sunspots and their grouping is used because it compensates for variations in observing small sunspots.\n\nThis number has been collected and tabulated by researchers for over 150 years. They have found that sunspot activity is cyclical and reaches its maximum around every 9.5 to 11 years. This cycle was first noted by Heinrich Schwabe in 1843.\n\nDue to weather and researcher unavailability, \"the\" sunspot count is actually an average of observations by multiple people in multiple locations with different equipment, with a scaling factor \"k\" assigned to each observer to compensate for their differing ability to resolve small sunspots and their subjective division of groups of sunspots.\n\nThe relative sunspot number formula_1 is computed using the formula (collected as a daily index of sunspot activity):\n\nwhere\n\n\nSince 1 July 2015 a revised and updated list of the sunspot numbers has been made available. The biggest difference is an overall increase by a factor of 1.6 to the entire series. Traditionally, a scaling of 0.6 was applied to all sunspot counts after 1893, to compensate for Alfred Wolfer's better equipment, after taking over from Wolf. This scaling has been dropped from the revised series, making modern counts closer to their raw values. Also, counts were reduced slightly after 1947 to compensate for bias introduced by a new counting method adopted that year, in which sunspots are weighted according to their size.\n\nHowever, alternative (group) sunspot number series exist suggesting different behavior of sunspot activity before the 20th century. Thus, the solar variability before the 20th centuries remains quite uncertain.\n\n\n"}
