{"id": "4801410", "url": "https://en.wikipedia.org/wiki?curid=4801410", "title": "Albert E. Sleeper State Park", "text": "Albert E. Sleeper State Park\n\nAlbert E. Sleeper State Park is a public recreation area on Lake Huron in Lake Township, Huron County, Michigan. The state park encompasses four miles northeast of Caseville, close to the tip of The Thumb of Michigan.\nThe park was created in 1925 by Huron County; it became a state park in 1927. The Civilian Conservation Corps was active in the park in the 1940s, building the park's Outdoor Center. In 1944, the park was renamed to honor former Michigan Governor Albert E. Sleeper, who signed the legislation authorizing the state park system.\nAccording to a 2008 economic impact analysis, Albert E. Sleeper State Park has a significant effect on the economy of the surrounding county, accounting for $5.476 million in direct spending, 144 direct jobs, and $2.881 million in value added. There is additional economic effect from secondary effects, totaling $7.905 million (44.9 percent over direct spending), 181 jobs (25.7 percent over direct job impacts), and $4.396 million (52.6 percent over direct value added).\n\nThe park is considered a multi-use park with recreational features that include a half-mile beach on Saginaw Bay, four miles of trails for hiking, biking and cross-country skiing, picnicking areas, cabins, and campground.\n\n"}
{"id": "1616044", "url": "https://en.wikipedia.org/wiki?curid=1616044", "title": "Auxetics", "text": "Auxetics\n\nAuxetics are structures or materials that have a negative Poisson's ratio. When stretched, they become thicker perpendicular to the applied force. This occurs due to their particular internal structure and the way this deforms when the sample is uniaxially loaded. Auxetics can be single molecules, crystals, or a particular structure of macroscopic matter.\nSuch materials and structures are expected to have mechanical properties such as high energy absorption and fracture resistance. Auxetics may be useful in applications such as body armor, packing material, knee and elbow pads, robust shock absorbing material, and sponge mops.\n\nThe term \"auxetic\" derives from the Greek word αὐξητικός (auxetikos) which means \"that which tends to increase\" and has its root in the word αὔξησις, or \"auxesis\", meaning \"increase\" (noun). This terminology was coined by Professor Ken Evans of the University of Exeter.\nOne of the first artificially produced auxetic materials, the RFS structure (diamond-fold structure) , was invented in 1978 by the Berlin researcher K. Pietsch. Although he did not use the term auxetics, he describes for the first time the underlying lever mechanism and its non-linear mechanical reaction is therefore considered the inventor of the auxetic net.\nThe earliest published example of a material with negative Poisson's constant is due to A. G. Kolpakov in 1985, \"Determination of the average characteristics of elastic frameworks\"; the next synthetic auxetic material was described in \"Science\" in 1987, entitled \"Foam structures with a Negative Poisson's Ratio\" by R.S. Lakes from the University of Wisconsin Madison. The use of the word \"auxetic\" to refer to this property probably began in 1991.\n\nDesigns of composites with inverted hexagonal periodicity cell (auxetic hexagon), possessing negative Poisson ratios, were published in 1985.\n\nTypically, auxetic materials have low density, which is what allows the hinge-like areas of the auxetic microstructures to flex.\n\nAt the macroscale, auxetic behaviour can be illustrated with an inelastic string wound around an elastic cord. When the ends of the structure are pulled apart, the inelastic string straightens while the elastic cord stretches and winds around it, increasing the structure's effective volume. Auxetic behaviour at the macroscale can also be employed for the development of products with enhanced characteristics such as footwear based on the auxetic rotating triangles structures developed by Grima and Evans.\n\nExamples of auxetic materials include:\n\n"}
{"id": "7193566", "url": "https://en.wikipedia.org/wiki?curid=7193566", "title": "Birds Directive", "text": "Birds Directive\n\nThe Birds Directive (formally known as Council Directive 2009/147/EC on the conservation of wild birds) is a European Union directive adopted in 2009. It replaces Council Directive 79/409/EEC of 2 April 1979 on the conservation of wild birds which was modified several times and had become very unclear. It aims to protect all European wild birds and the habitats of listed species, in particular through the designation of Special Protection Areas (often known by the acronym SPA).\n\nThe Birds Directive is one of the EU's two directives in relation to wildlife and nature conservation, the other being the Habitats Directive. The Habitats Directive led to the setting up of a network of Special Areas of Conservation, which together with the existing Special Protection Areas form a network of protected sites across the European Union called Natura 2000.\n\n\n\n"}
{"id": "2148577", "url": "https://en.wikipedia.org/wiki?curid=2148577", "title": "Cooks' Cottage", "text": "Cooks' Cottage\n\nCooks' Cottage, previously known as Captain Cook's Cottage, is located in the Fitzroy Gardens, Melbourne, Australia. The cottage was constructed in 1755 in the English village of Great Ayton, North Yorkshire, by the parents of Captain James Cook, James and Grace Cook, and was brought to Melbourne in 1934 by Sir Russell Grimwade. It is a point of conjecture among historians whether James Cook, the famous navigator, ever lived in the house, but almost certainly he visited his parents at the house.\n\nThe inside of the cottage includes centuries-old antiques and is stylised in the way of the 18th century, as are the clothes of the volunteer guides.\n\nIn 1933 the owner of the cottage decided to sell it with a condition of sale that the building remain in England. She was persuaded to change \"England\" to \"the Empire\", and accepted an Australian bid of £800, by Russell Grimwade, as opposed to the highest local offer of £300.\n\nThe cottage was deconstructed brick by brick and packed into 253 cases and 40 barrels for shipping on board the \"Port Dunedin\" from Hull. Cuttings from ivy that adorned the house were also taken and planted when the house was re-erected in Melbourne. Grimwade, a notable businessman and philanthropist, donated the house to the people of Victoria for the centenary anniversary of the settlement of Melbourne in October 1934.\n\nThe cottage immediately became a popular tourist attraction. In 1978 further restoration work was carried out on the cottage. An English cottage garden has been established around the house, further adding to its period reconstruction. Very few of the items in the house are from the Cook family, but all are representative furnishings of the period.\n\nThe cottage features in two scenes of the 2012 movie \"Any Questions for Ben?\"\n\n"}
{"id": "54438478", "url": "https://en.wikipedia.org/wiki?curid=54438478", "title": "Criddle/Vane Homestead Provincial Park", "text": "Criddle/Vane Homestead Provincial Park\n\nCriddle/Vane Homestead Provincial Park was designated a provincial park by the Government of Manitoba in 2004. The park is in size. The park is considered to be a Class III protected area under the IUCN protected area management categories.\n\n\n"}
{"id": "21102002", "url": "https://en.wikipedia.org/wiki?curid=21102002", "title": "Cèdre Gouraud Forest", "text": "Cèdre Gouraud Forest\n\nCèdre Gouraud Forest is a woodland area in the Middle Atlas Mountain Range in Morocco. It was named for the French general Henri Gouraud. This forest is located on National Route 8 between Azrou and Ifrane. The forest is notable as a habitat for a sub-population of Barbary macaques, \"Macaca sylvanus\".\n\n\n"}
{"id": "6993454", "url": "https://en.wikipedia.org/wiki?curid=6993454", "title": "Darts world rankings", "text": "Darts world rankings\n\nDarts World Rankings are a system designed to determine a list of the best darts players in the world based on their performances in tournaments. However, in 1993, a group of former world champions and other high-profile players separated from the British Darts Organisation, meaning there are now two major governing bodies.\n\nEach organisation has its own players, and each has its own ranking system. The ranking systems are used to arrange tournament seedings, which are so arranged, that the number one player in the world will not face the number two player until the final of a tournament, providing they both reach that final.\n\nThe Professional Darts Corporation's Ranking is based on the amount of prize money won over the past two years.\n\nThe rankings are based on a cumulative points system, calculated on a rolling one year basis. When a tournament is played, the previous year's results are removed from the rankings and the new tournament scores are used. This list is used to determine seeds for some of the WDF Opens. The World Darts Federation ranking system is designed to provide a measure of the global activities of darts players in every WDF recognised darts event. It used to be very similar to the BDO system but was revised in January 2007 to include categories by country and by events, and the distribution of ranking points reflect the levels of prize money on offer and the numbers of entries in a tournament. Players gain points in six different levels of categorized events and prize money and at the end of the season the leading players receive monetary bonus rewards from the WDF.\n\n\"Points are currently awarded as follows:\"\n\nThe following is a list of players who ended the year ranked number one in the WDF.\n\n\n\nThe BDO Invitational Table relates only to registered playing members of the BDO, who comply with BDO and World Darts Federation (WDF) eligibility rules, including terms and conditions of the 1997 Tomlin Order.\n\nThe BDO awards points depending on a player's performances within each of its events and other BDO-recognised qualifying events. Forty-nine points are awarded to the champion of each of its three Major Events - The Lakeside World Professional Championship, The Bavaria World Darts Trophy and the Winmau World Masters and A+ events. Lower points are awarded for each round of the tournament reached. Other events are placed into Category A, B, C and D based on the prize-pool and number of payouts, with points for placings slowly decreasing. Only the best 12 placings are added for a player's ranking.\n\n\"Points are currently awarded as follows:\"\n\nFor the current men ranking see: BDO Men's Invitational Table at 16/12/2017\n\nFor the current women's rankings see:\n\n"}
{"id": "306064", "url": "https://en.wikipedia.org/wiki?curid=306064", "title": "Ecosophy", "text": "Ecosophy\n\nEcosophy or ecophilosophy (a portmanteau of ecological philosophy) is a philosophy of ecological harmony or equilibrium. The term was coined by the Norwegian father of deep ecology, Arne Næss, and French post-structuralist philosopher and psychoanalyst Félix Guattari.\n\nNaess defined ecosophy in the following way:\n\nWhile a professor at University of Oslo in 1972, Arne Næss, introduced the terms \"deep ecology movement\" and \"ecosophy\" into environmental literature. Naess based his article on a talk he gave in Bucharest in 1972 at the Third World Future Research Conference. As Drengson notes in \"Ecophilosophy, Ecosophy and the Deep Ecology Movement: An Overview\", \"In his talk Næss discussed the longer-range background of the ecology movement and its connection with respect for Nature and the inherent worth of other beings.\" Naess's view of humans as an integral part of a \"total-field image\" of Nature contrasts with the alternative (and more anthropocentric) construction of ecosophy outlined by Guattari.\n\nThe term ecological wisdom, synonymous with ecosophy, was introduced by Næss in 1973. The concept has become one of the foundations of the deep ecology movement. All expressions of values by Green Parties list ecological wisdom as a key value—it was one of the original Four Pillars of the Green Party and is often considered the most basic value of these parties. It is also often associated with indigenous religion and cultural practices. In its political context, it is necessarily not as easily defined as ecological health or scientific ecology concepts.\n\nEcosophy also refers to a field of practice introduced by psychoanalyst, poststructuralist philosopher, and political activist Félix Guattari. In part Guattari's use of the term demarcates a necessity for the proponents of social liberation, whose struggles in the 20th century were dominated by the paradigm of social revolution, to embed their arguments within an ecological framework which understands the interconnections of social and environmental spheres.\n\nGuattari holds that traditional environmentalist perspectives obscure the complexity of the relationship between humans and their natural environment through their maintenance of the dualistic separation of human (cultural) and nonhuman (natural) systems; he envisions ecosophy as a new field with a monistic and pluralistic approach to such study. Ecology in the Guattarian sense, then, is a study of complex phenomena, including human subjectivity, the environment, and social relations, all of which are intimately interconnected. Despite this emphasis on interconnection, throughout his individual writings and more famous collaborations with Gilles Deleuze, Guattari has resisted calls for holism, preferring to emphasize heterogeneity and difference, synthesizing assemblages and multiplicities in order to trace rhizomatic structures rather than creating unified and holistic structures.\n\nGuattari's concept of the three interacting and interdependent ecologies of mind, society, and environment stems from the outline of the three ecologies presented in \"Steps to an Ecology of Mind\", a collection of writings by cyberneticist Gregory Bateson.\n\n\n\n"}
{"id": "2264346", "url": "https://en.wikipedia.org/wiki?curid=2264346", "title": "Electron configurations of the elements (data page)", "text": "Electron configurations of the elements (data page)\n\nThis page shows the electron configurations of the neutral gaseous atoms in their ground states. For each atom the subshells are given first in concise form, then with all subshells written out, followed by the number of electrons per shell. Electron configurations of elements beyond hassium (element 108) are predicted.\n\nAs an approximate rule, electron configurations are given by the . However there are numerous exceptions; for example the lightest exception is chromium, which would be predicted to have the configuration , written as , but whose actual configuration given in the table below is .\n\nAll sources concur with the data above except in the separately listed instances:\n\nThis website is also cited in the CRC Handbook as source of Section 1, subsection Electron Configuration of Neutral Atoms in the Ground State.\n\n\n\n\n\n\nThis book contains predicted electron configurations for the elements 119–172 and 184, based on relativistic Dirac–Fock calculations by B. Fricke in \n\nPredicted electron configurations for element 173 based on relativistic Dirac–Fock calculations by B. Fricke\n"}
{"id": "10469240", "url": "https://en.wikipedia.org/wiki?curid=10469240", "title": "Energy policy of Canada", "text": "Energy policy of Canada\n\nCanada has access to all main sources of energy including oil and gas, coal, hydropower, biomass, solar, geothermal, wind, marine and nuclear. It is the world's second largest producer of uranium, third largest producer of hydro-electricity, fourth largest natural gas producer, and the fifth largest producer of crude oil. Only Russia, the People's Republic of China, the United States and Saudi Arabia produce more total energy than Canada.\n\nThe United States is Canada's major trade market for energy products and services. Canada sends around 98% of its total energy exports to the United States, meaning that Canada is the largest supplier of energy exports to the world's largest economy. Canada also exports significant amounts of uranium and coal to Asia, Europe and Latin America.\n\nDespite being a net energy exporter, Canada also imports large/small amounts of energy products.\n\nCanada has a robust energy profile with abundant and diverse resources. The energy and climate policies in Canada are interrelated. These energy and climate policies are implemented at both the federal and provincial government level. The federal government is responsible for establishing objectives for the entire country and the provincial governments are responsible for enforcing these objectives and developing the methods to achieve these goals. In 2015, the federal and provincial governments created a national agreement for cooperating in boosting the nation's energy industry while transitioning to a low-carbon economy. Today, provincial governments are developing their own strategies in order to reach the national goals. The Prince Edward Island Strategy is one of the first provincial strategies developed in response to the federal agreement goals.\n\nIn Canada's federal system of government, jurisdiction over energy is divided between the federal and provincial and territorial governments. Provincial governments have jurisdiction over the exploration, development, conservation, and management of non-renewable resources, as well as the generation and production of electricity. Federal jurisdiction in energy is primarily concerned with regulation of inter-provincial and international trade and commerce, and the management of non-renewable resources on federal lands.\n\nThe National Energy Board (NEB) is an independent federal regulatory agency that regulates the Canadian energy industry. The NEB was created in 1959 and reports through the Minister of Natural Resources to the Parliament of Canada. Its primary responsibilities include:\nIn 1985, the federal government and the provincial governments in Alberta, British Columbia and Saskatchewan agreed to deregulate the prices of crude oil and natural gas. Offshore oil Atlantic Canada is administered under joint federal and provincial responsibility in Nova Scotia and Newfoundland and Labrador.\n\nProvincial regulation of oil and natural gas activities, pipelines, and distribution systems is administered by provincial utility boards. The producing provinces impose royalties and taxes on oil and natural gas production; provide drilling incentives; and grant permits and licenses to construct and operate facilities. The consuming provinces regulate distribution systems and oversee the retail price of natural gas to consumers. The key regulations with respect to the wholesale and retail electricity competition are at the provincial level. To date, two provinces (Alberta and Ontario) have initiated retail competition. In Alberta, the electricity sector is largely privatized, in Ontario the process is ongoing. In other provinces electricity is mostly generated and distributed by provincially owned utilities.\n\nCanadian energy policy reflects the constitutional division of powers between the federal government and the provincial governments. The Constitution of Canada places natural resources under the jurisdiction of the provinces. However, the three prairie provinces originally did not control the natural resources in the provinces as a condition of their entry into Confederation, until the Natural Resources Acts of 1930. The provincial governments own most of the petroleum, natural gas and coal reserves, and control most of the electricity production. This means that the national government must coordinate its energy policies with those of the provincial governments, and intergovernmental conflicts sometimes arise. The problem is particularly acute since, while the energy consuming provinces have the bulk of the population and are able to elect federal governments which introduce policies favouring energy consumers, the energy producing provinces have the ability to defeat such policies by exercising their constitutional authority over natural resources.\n\nSection 92A of the Constitution Act, 1867 assigned to the provincial governments the exclusive authority to make laws in relation to non-renewable resources and electrical energy, while Section 125 prevented the federal government from taxing any provincial government lands or property. On the other hand, the federal government has the power to make treaties with foreign countries. This has important implications for treaties involving energy production, like the Kyoto Protocol, which the Canadian government signed in 2002. Although the federal government had the authority to sign the treaty, it may require the cooperation of the provincial governments to enforce it.\n\nCanada has a robust energy profile with abundant and diverse resources. Energy and climate policies are interrelated. These policies are implemented at both the federal and provincial governmental level. A recent SWOT (Strengths, Weaknesses, Opportunities, and Threats) analysis conducted in 2013 of a Canadian energy and climate policies has shows that there is a lack of consistency between federal and regional strategies. The reason for this lack of consistency was attributed to the economic and environmental realities, the diversity of energy sources and energy demands that vary greatly among the Canadian provinces. As a result of the differing energy characteristics of the provinces there is creation of multiple federal and provincial strategies, sometimes complementary, but often contradictory.\n\nThe Canadian energy policy is based on three important principles. These principles are (1) competitive markets to ensure a successful and innovative energy system capable of meeting Canadian energy needs, (2) respecting the jurisdictions of provinces and the federal government and (3) targeted federal interventions in the energy trading process ensuring the specific energy-policy objectives are achieved.\n\nIn order to improve the coherence of provinces and federal policies a combination of policy tools have been instituted to facilitate collaboration between the federal and provincial governments. These policies tools have resulted in equal balance of federal and provincial government in the creation of energy policies. The federal government is responsible for establishing objectives for the entire country and the provincial governments are responsible for enforcing these objectives and developing the methods to achieve these goals.\n\nIn 2015, the federal government worked with Canada's provincial leaders and reached an agreement for cooperating in boosting the nation's industry while transitioning to a low-carbon economy. The critics of this agreement doubted that the provincial leaders would be to reach an agreement and they also doubted that they would be successful in forming a joint energy policy. However, this was not the case. After a three-day meeting in St. John's, Newfoundland and Labrador, the Council of the Federation released this report that set out their vision for a national energy strategy. This agreement is meant to guide energy policy among the provincial governments. This agreement seeks out to influence provinces to promote energy efficiency and conservation, transition to lower carbon economy and enhance energy information and awareness. The Prince Edward Island Strategy is a provincial strategy that was in response to meeting the federal government goals presented in this agreement.\n\nCoal has been mined in Canada since 1639 when a small mine was opened at Grand Lake, New Brunswick. In 1720 French soldiers opened a mine in Cape Breton, Nova Scotia to supply the fortress of Louisbourg. Cape Breton later supplied coal to Boston and other American ports. Commercial mining in New Brunswick began in 1825 although most of the province's coal production has been used locally. In western Canada, coal was first mined on Vancouver Island from 1853. Starting in the 1880s, the building of the transcontinental railways through Alberta and British Columbia caused coal mines to be developed in various locations near railway lines in the prairies and mountains. By 1911 western mines produced most of the coal in Canada and, despite downturns, gradually expanded to produce over 95% of Canadian coal.\nCoal was subsidised in Canada from 1887. The mines of Cape Breton were involved in this tariff protection to help it compete against American coal entering Ontario via the Great Lakes. Cape Breton coal was dug underground then shipped to Toronto and Montreal. The vast industries of the east, including steel mills, were fuelled with this coal. While there were difficulties and strikes, coal powered Canada into the Second World War. There were several Royal Commissions into coal: one in 1947 and other in 1965.\n\nFederal involvement in Cape Breton, continued with the Cape Breton Development Corporation, or Devco which was in reality a large subsidy. The completion of the trans-Canada pipeline, nuclear reactors and the Hibernia oil fields have finished coal in Nova Scotia. On the other side of the country, Vancouver Island is covered in coal: there are coal fields in Cassidy, Nanaimo, Campbell River and Fort Rupert. Coal was mined at Nanaimo for one hundred years from 1853 to 1955. Coal was fed in ship's furnaces, railroad engines, and industry. In BC's interior coal was mined at Merritt, Coalmont, Fernie and Hudson's Hope. The development of coal mines in the west is integrally mixed with the building of railways—the Canadian Pacific Railway was directly involved with the Fermie mines. A separate railway—the Crow's Nest Line—was built to move coal from the Rockies to the smelter at Trail. Alberta's bedrock is literally a layer of coal—coal underlays much of the Rocky Mountains. Historically, there were underground pits in Lethbridge, Pincher Creek, Canmore and Nordegg.\n\nThe discovery of huge oil fields in western Canada starting with the Leduc, Alberta field in 1947, and growing imports of cheap foreign oil into eastern Canada drastically affected the demand for Canadian coal. Beginning about 1950, almost all the coal used for heating, industry, and transportation was replaced by petroleum products and natural gas. This had a devastating effect on the coal mining communities of Atlantic Canada, although in western Canada the loss of jobs in the coal industry was more than compensated for by gains in the oil industry.\n\nCoal mining began an expansion phase in the late 1960s with the signing of long-term contracts to supply metallurgical coal to the booming Japanese steel industry. This was of little benefit to Atlantic Canada, but led to the re-opening of closed mines and the development of new mines in Alberta and BC. Around the same time, Alberta and Saskatchewan began to use their substantial coal resources to generate electricity. Crude oil price increases in the 1970s and early 1980s increased the demand for coal worldwide. New mines opened in Alberta and BC, and new port facilities were built in BC to supply the growing demand in Asia.\n\nCanada has the tenth largest coal reserves in the world, an enormous amount considering the sparse population of the country. However, the vast majority of those reserves are located hundreds or thousands of kilometres from the country's industrial centers and seaports, and the effect of high transportation costs is that they remain largely unexploited. As with other natural resources, regulation of coal production is within the exclusive jurisdiction of the provincial governments, and it only enters federal jurisdiction when it is imported or exported from Canada.\n\nOver 90% of Canada's coal reserves and 99% of its production are located in the Western provinces of Alberta, British Columbia, and Saskatchewan. Alberta has 70% of Canada's coal reserves, and 48% of the province is underlaid by coal deposits. The Hat Creek deposit in British Columbia has one of the thickest coal deposits in the world, about thick. There are also smaller, but substantial, coal deposits in the Yukon and Northwest Territories and the Arctic Islands, which are even further from markets. The Atlantic provinces of Nova Scotia and New Brunswick have coal deposits that were historically a very important source of energy, and Nova Scotia was once the largest coal producer in Canada, but these deposits are much smaller and much more expensive to produce than the Western coal, so coal production in the Atlantic provinces has virtually ceased. Nova Scotia now imports most of the coal for its steel mills and power plants from other countries like Colombia. At the same time, the Western provinces export their coal to 20 different countries, particularly Japan, Korea, and China, in addition to using it in their own thermal power plants. Elk Valley Coal mine is the second biggest coal mine in the world.\n\nThe region between New Brunswick and Saskatchewan, a distance of thousands of kilometres which includes the major industrial centers of Ontario and Quebec, is largely devoid of coal. As a result, these provinces import almost all of the coal for their steel mills and thermal power plants from the United States. Unfortunately coal from the Eastern United States is high in sulfur content, and this had contributed to a serious air quality problem, particularly in heavily populated Southwestern Ontario until they phased out the last coal fired power plant in 2014. In Alberta the coal fired Sundance Power Station and Genesee Generating Station are the second and third largest sources of greenhouse gases in Canada.\n\nIn 1858 James Miller Williams dug the first oil well in North America at Oil Springs, Ontario, preceding Edwin Drake who drilled the first one in the United States one year later. By 1870 Canada had 100 refineries in operation and was exporting oil to Europe. However, the oil fields of Ontario were shallow and small, and oil production peaked and started to decline around 1900. In contrast, oil production in the United States grew rapidly in the first part of the 20th century after huge discoveries were made in Texas, Oklahoma, California and elsewhere.\n\nIn 1914, Turner Valley became the first significant field found in Alberta. Eastern Canadian investors and the federal government showed little interest and the field was developed primarily by subsidiaries of U.S. companies. It was originally believed to be a gas field with a small amount of naptha condensed in the gas, but due to the lack of regulations, about 90% of the gas was flared off to extract the small amount of petroleum liquids, an amount of gas that today would be worth billions of dollars.\n\nIn 1930, crude oil was discovered in the Turner Valley field, below and to the west of the gas cap. This came as a shock to geologists because the free gas cap, which could have provided the reservoir drive to produce the oil, had largely been produced and flared off by that time. As a result, less than 12% of the original oil in place at Turner Valley will ever be recovered.\n\nThe Alberta provincial government became upset by the conspicuous waste so in 1931 it passed the Oil and Gas Wells Act, followed in 1932 by the Turner Valley Conservation Act. However, the federal government declared both Acts unconstitutional, and the wasteful burning of natural gas continued. However, in 1938 the provincial government established the Alberta Petroleum and Natural Gas Conservation Board (today known as the Energy Resources Conservation Board) to initiate conservation measures, and this time was successful in implementing it.\n\nThis body was the regulator of oil and gas production in Alberta, and therefore of most production in Canada. As the provincial regulatory authority with the most experience in the industry, it became a model for the other oil and gas producing provinces - indeed, it has been used as a model by many national petroleum industries around the world.\n\nAt the end of World War II, Canada was importing 90% of its oil from the U.S. The situation changed dramatically in 1947 when, after drilling 133 consecutive dry holes, Imperial Oil decided to drill into a peculiar anomaly on its newly developed seismic recordings near the then-village of Leduc to see what it was. The Leduc No. 1 well identified a large oil field, and provided the geological key for other important discoveries within Alberta. Geologists soon began to identify and drill other Devonian reefs within the province - mostly in the north-central portion of the province. The Alberta oil rush began, and drillers quickly began to identify other important oil-bearing formations like the one hosting the giant Pembina oilfield.\n\nThe Leduc discovery and the string of even bigger ones that followed rapidly backed imported oil out of the Canadian prairies and produced a huge surplus of oil which had no immediate market. In 1949, Imperial Oil applied to the federal government to build the Interprovincial Pipeline (IPL) to Lake Superior, and in 1950 it was completed to the port of Superior, Wisconsin. Many people questioned why it was built to an American port rather than a Canadian one, but the federal government was more interested in the fact that oil exports made a huge difference to Canada's trade balance and completely erased the country's balance of trade deficit.\n\nBy 1956 the pipeline was extended via Sarnia, Ontario to Toronto and became, at 3,100 km, the longest oil pipeline in the world. In the interest of increasing oil exports, extensions were built to Chicago and other refinery locations in the Midwestern United States during the 1960s. In the other direction, in 1950 the federal government gave approval to build a pipeline west, and in 1953 the 1,200 km Transmountain Pipeline was built from Edmonton to the port of Vancouver, British Columbia with an extension to Seattle, Washington. These pipelines did more to improve the energy security of the United States than that of Canada, since the Canadian government was more interested in the country's trade balance than in military or energy security.\n\nAfter the big discoveries of the 1940s and 1950s, the U.S. noticed that Alberta was protected from invasion by the wall of the Rocky Mountains to the west, the vast boreal forest to the north, and the bottomless swamps of the Canadian shield to the east, but was highly accessible from the vast industrial areas of the U.S. Midwest to the south. Its landlocked location was easier to defend from foreign attack than the United States own oil fields in Texas, Alaska and California. As a result, the U.S. gave preference to oil imports from Canada, and for the purposes of energy policy treated Alberta as if it were a U.S. state. Since this resulted in producers in Alberta receiving better treatment from the United States government than the Canadian government, producers asked the federal government for access to the Eastern Canadian oil market. Oil producers in Alberta calculated they could deliver Alberta oil to the refineries at Montreal for a cost equal to or only slightly higher than the price of imported oil. However, the Montreal area refineries and the Quebec government balked at the restriction, so the result was the National Oil Policy of 1961. This drew a dividing line at the Ottawa River and gave Canadian producers exclusive rights to the areas to sell oil to the west of the line. Refineries to the east of the line could continue to process imported oil.\n\nNot everyone was happy with the arrangement. The aim of the National Oil Policy was to promote the Alberta oil industry by securing for it a protected share of the domestic market. Under the policy, Canada was divided into two oil markets. The market east of the Ottawa Valley (the Borden Line) would use imported oil, while west of the Borden Line, consumers would use the more expensive Alberta supplies. For most of the 1961-73 period, consumers to the West paid between $1.00 and $1.50 per barrel above the world price, which, just before the 1973 OPEC oil embargo and price increase, stood at around $3.00. They also paid proportionately higher prices at the pump than Canadians east of the Borden line.\n\nIn 1970, Quebec created a provincially owned petroleum company called SOQUIP. A year later, the Gordon Commission's nationalist flavour found practical expression with the creation of the Canada Development Corporation, to \"buy back\" Canadian industries and resource with deals that included a takeover of the Western operations of France's Aquitaine and their conversion into Canterra Energy. Also in 1971, the federal government blocked a proposed purchase of Canadian-controlled Home Oil by American-based Ashland Oil.\n\nThe wave of direct action spread to Alberta when Premier Peter Lougheed and his Conservatives won power in 1971, ending 36 years of Social Credit rule. Lougheed's elaborate election platform, titled New Directions, sounded themes common among OPEC countries by pledging to create provincial resources and oil growth companies, collect a greater share of energy revenues, and foster economic diversification to prepare for the day when petroleum reserves ran out. The idea of limited resources emerged from the realm of theory into hard facts of policy when the NEB rejected natural-gas export applications in 1970 and 1971, on grounds that there was no surplus and Canada needed the supplies. The strength of the new conservationist sentiment was underlined when the NEB stuck to its guns despite a 1971 declaration by the federal Department of Energy that it thought Canada had a 392-year supply of natural gas and enough oil for 923 years.\n\nIn 1973, this situation changed abruptly.\n\nThe Canadian government had already begun to change its energy policy. Inflation had become a national problem and oil prices were rising, and on September 4, 1973 Pierre Trudeau asked the western provinces to agree to a voluntary freeze on oil prices. Nine days later, his government imposed a 40-cent tax on every barrel of exported Canadian oil. The tax equalled the difference between domestic and international oil prices, and the revenues were used to subsidize imports for eastern refiners. At a stroke, Ottawa began subsidizing eastern consumers while reducing the revenues available to producing provinces and the petroleum industry. Alberta premier Peter Lougheed soon announced that his government would revise its royalty policy in favour of a system linked to international oil prices.\n\nTwo days later, on October 6, the Yom Kippur War broke out – a nail-biting affair between Israel and the Arab states. OPEC used the conflict to double the posted price for a barrel of Saudi Arabian light oil, to US$5.14. Saudi and the other Arab states then imposed embargoes on countries supporting Israel, and oil prices rose quickly to $12.\n\nThese events aggravated tensions among provincial, federal and industry leaders. The rest of the 1970s were marked by rapid-fire, escalating moves and counter-moves by Ottawa, Western provinces and even Newfoundland. The atmosphere was one of urgency, alarm and crisis, with global conflicts adding gravity to the federal-provincial quarrelling.\n\nIn 1979-1980, further crises in the Middle East led to panic-driven pricing. The Iranian Revolution came first. War between that country and Iraq soon followed. Oil prices more than doubled, to US$36 per barrel.\n\nIntroduced by the Liberal government under Pierre Trudeau on October 28, 1980, the controversial National Energy Program (NEP) had three objectives: energy self-sufficiency; redistributing wealth from a non-sustainable resource to benefit the country as a whole; and increased ownership of the oil industry by Canadians. As implemented, the NEP gave the Federal government control over petroleum prices, imposing a price ceiling and export duties.\n\nThe federal government had two major challenges in creating a truly national energy program. The first problem was that Canada is both an importer and an exporter of oil. It imports oil from offshore sources such as Venezuela and the Middle East into its Eastern provinces, while simultaneously exporting oil from its Western provinces into the United States. While it was popular in Eastern and Central Canada, the program incurred strong resentment in the province of Alberta where oil and gas production are concentrated. The second problem was that provincial governments, rather than the federal government, have constitutional jurisdiction over natural resources. The Government of Alberta actually owned most of the oil in Canada. This provoked a confrontation with the government of Alberta, since any reduction in oil prices came directly out of Alberta government revenues. The conflict was made worse by the fact that the Alberta government had constitutional mechanisms available to it by which it could remove oil from federal taxation and shift the costs of oil subsidies onto the federal government. This increased the federal government deficit.\n\nThe National Energy Program had a number of other flaws. It was based on a world price steadily increasing to $100 per barrel. The world oil price declined to as little as $10 per barrel in the years following. Since the federal government based its spending on the larger figure, the result was that it spent a great deal of money on subsidies that could not be recovered in taxes on production. Furthermore, due to proximity to the U.S. market companies had opportunities to make money by playing differentials in prices. For instance, refiners in Eastern Canada would import oil subsidized down to half the world price, refine it into products, and export the products to the U.S. at full world price. Airlines flying between Europe and the U.S. via the polar route would take off with as little fuel as possible, and stop briefly in Canada to fill up before continuing on to their destination. Trucking companies operating between locations in the Northern U.S. would detour their trucks through Canada to refuel. None of these transactions was illegal, or even unusual considering the integrated nature of the economies, but all had the effect of transferring billions of Canadian tax dollars to the balance sheets of (mostly foreign owned) companies. A third flaw was that the NEP assumed that future oil discoveries would be made in areas under federal jurisdiction, such as the Arctic and offshore. As it turned out, most of the major oil discoveries in Canada had already been made, and the subsidies given by the federal government to companies exploring in federal jurisdiction were not productive. All of these flaws resulted in large, and unexpected, increases in the federal budget deficit.\n\nThe final result of the NEP was that the federal government failed to keep fuel prices low while incurring financial losses. In the subsequent election in 1984, the governing Liberal party was defeated. The winning Progressive Conservative party dismantled the policy two and a half years after its election.\n\nIn 1975 the Liberal government reacted to the 1973 oil crisis by creating a federally owned oil company, Petro-Canada. The Crown corporation was originally developed to be an \"eye on the petroleum industry\" during a period of perceived energy crisis. Initially, its assets consisted only of the federal government's share of the oil sands company Syncrude and the Arctic oil explorer Panarctic Oils.\n\nHowever, the government quickly expanded it by buying the Canadian assets of foreign-owned oil companies, such as Atlantic Richfield in 1976, Pacific Petroleums in 1979, Petrofina in 1981, the refining and marketing assets of BP in 1983 and of Gulf Oil in 1985.\n\nFederal ownership brought Petro-Canada into conflict with the provincial governments which had control over the largest and lowest cost oil production in the country. They objected to federal intrusion into their constitutional jurisdiction, and tried to block federal incursions. For instance, when Petro-Canada attempted to buy Husky Oil in 1978, the Alberta government surreptitiously got control of Husky stock through Alberta Gas Trunk Line, and successfully blocked the takeover. In 1979 Petro-Canada acquired Westcoast Transmission Co. Ltd. and Pacific Petroleums Ltd., its parent company, as a fully integrated oil company for the then-record purchase price of $1.5 billion.\n\nPetro-Canada overestimated the future price of oil, and consequently paid high prices for the oil assets it acquired, which subsequently fell considerably in value. Its assumption that big new oil discoveries would be made in the Arctic and off the Atlantic coast turned out to be incorrect. Petro-Canada has since abandoned all the wells Panarctic drilled, and the discoveries it did make off the Atlantic coast were fewer, more expensive, and took longer to develop than expected. Hibernia did not produce oil until 1997 and Terra Nova until 2002. The government also expected Petro-Canada to force down what it considered the high price of gasoline to consumers, but Petro-Canada's oil production was more expensive and its oil refineries less efficient than those of the competing multi-national companies, and it found itself losing money on all aspects of the oil industry.\n\nWhen the Conservatives replaced the Liberals in power in 1984, they began to reverse the nationalization process. In 1991, they passed legislation allowing privatization and began selling shares to the public. The Liberals returned to power in 1993, but had lost interest in having a national oil company, and continued the privatization process. In 1995 the federal government reduced its interest to 20 percent, and in 2004 sold the remaining shares. Petro-Canada has done better since privatization because oil price increases since 2003 make its high-cost production profitable, and consolidation of its refining operations to fewer but larger refineries reduced its downstream costs even as prices increased.\n\nOn March 23, 2009, Petro-Canada and Suncor Energy announced they would merge to create Canada's largest oil company. At the time of the announcement, combined market capitalization of the two corporations was $43 billion. The merged organization would operate under the Suncor name, but would use the Petro-Canada brand in its retail operations. The companies estimated that the merger would save $1.3 billion per year in capital and operating costs, and said that the larger company will have the financial resources to move ahead with the most promising oilsands projects.\n\nCanada has oil sands deposits greater than the world's total supply of conventional oil at to . Of these, are extractable at current prices using current technology, which makes Canada's proven oil reserves second only to Saudi Arabia. Production costs are considerably higher than in the Middle East, but this is offset by the fact that the geological and political risks are much lower than in most major oil-producing areas. Almost all of the Canadian oil sands are located in Alberta. The Athabasca oil sands are the only major oil sands deposits in the world which are shallow enough for surface mining.\n\nCommercial production began in 1967 when Great Canadian Oil Sands (now Suncor) launched the world's first major oil sands mine. Syncrude opened the second major facility in 1978. The third, by Shell Canada, started in 2003. The oil price increases of 2004-2007 made the oil sands much more profitable, and by 2007 over $100 billion worth of new mines and thermal projects were under construction or on the drawing boards. Royal Dutch Shell announced that in 2006 its Canadian oil sands operations were almost twice as profitable on a per-barrel basis as its international conventional oil operations and in July 2007, it announced it would start a massive $27 billion expansion of its oil sands plants in Alberta.\n\nCost of production in the oil sands, from raw tar sand to fractionate in the pipe feed, was $18 per barrel; now with improvements it is in the 12-15 dollar range. Rapid price increases in recent years have greatly contributed to the profitability of an industry which has traditionally focused on reducing operating costs, and continues to do so. Environmental economists point out that the focus on operating costs does not sufficiently address environmental issues - for example, \"ravaged landscapes, despoiled rivers, diseased denizens, and altered atmospheric chemistry.\"\n\nOil sands operations differ from conventional oil in that the initial profitability is somewhat lower, but the geological and political risks are low, the reserves are vast, and the expected lifetime of production extends for generations rather than just a few years. Governments have an incentive to subsidize the start-up costs since they will recover their initial subsidies from tax revenues over a long period of time. From the standpoint of federal-provincial revenues, they also differ in that the federal government will receive larger higher share and higher return on its incentives than it would from conventional oil, while the provincial share, although substantial, will be proportionally smaller. Consequently, there has tended to be much less intergovernmental conflict and more agreement on how these projects should be handled.\n\nIf global oil prices remain high, it is likely that Canada will become one of the largest oil producers in the world in the next few decades. If so, there will be environmental issues, resulting more from the vast scale of the operations rather than the toxicity of the products. The oil sands deposits are roughly the size of Florida and the operations would drastically alter the landscape, which until recently was largely wilderness. In addition, concerns have been raised about water supplies, since the mines and steam projects would use a large portion of the flow of several major rivers. The most serious problem in the short term is an acute labor and housing shortage which has driven vacancy rates in the oil sands area to zero and wages to extremely high levels. However, given the hundreds of billions of dollars in revenue expected to be generated by the oil sands in the next few decades, it is likely that future projects will be approved regardless of the problems.\n\nAlso 19 deposits of oil shales have been identified in Canada. The most explored deposits are in Nova Scotia and New Brunswick. These are not as large as those in the Western United States, and will probably remain undeveloped in the foreseeable future since they are much more expensive and much smaller than the oil sands.\n\nThe natural gas industry is older in Alberta than oil, dating from 1883 discoveries near Medicine Hat. During the first half of the twentieth century, those who applied for permits to export Alberta natural gas often made the painful discovery that it was politically more complex to export gas than oil. Canadians tend to view oil as a commodity. However, through much of Canadian history, they have viewed natural gas as a patrimony, an essential resource to husband with great care for tomorrow. Although the reasons behind this attitude are complex, they are probably rooted in its value for space heating. This trend goes back as far as an incident at the end of the nineteenth century, when Ontario revoked export licenses for natural gas to the United States.\n\nBy the late 1940s Alberta, through its Conservation Board, eliminated most of the wasteful production practices associated with the Turner Valley oil and gas field. As new natural gas discoveries greeted drillers in the Leduc-fuelled search for oil, the industry agitated for licenses to export natural gas. In response, the provincial government appointed the Dinning Natural Gas Commission to inquire into Alberta's likely reserves and future demand.\n\nIn its March 1949 report, the Dinning Commission supported the principle that Albertans should have first call on provincial natural gas supplies, and that Canadians should have priority over foreign users if an exportable surplus developed. Alberta accepted the recommendations of the Dinning Commission, and later declared it would only authorize exports of gas in excess of a 30-year supply. Shortly thereafter, Alberta's Legislature passed the Gas Resources Conservation Act, which gave Alberta greater control over natural gas at the wellhead, and empowered the Oil and Gas Conservation Board to issue export permits.\n\nThe federal government's policy objectives at the time reflected concern for national integration and equity among Canadians. In 1949, Ottawa created a framework for regulating interprovincial and international pipelines with its Pipe Lines Act. Alberta once again agreed to authorize exports. The federal government, like Alberta, treated natural gas as a Canadian resource to protect for the foreseeable future before permitting international sales.\n\nAlthough Americans were interested in Canadian exports, they only wanted very cheap natural gas. After all, their natural gas industry was a major player in the American economy, and American policy-makers were not eager to allow foreign competition unless there was clear economic benefit.\n\nBecause of these combined factors, proposals for major gas transportation projects carried political as well as economic risks. Not until the implementation of the Canada-United States Free Trade Agreement (signed in 1988) did natural gas become a freely traded commodity between the US and Canada.\n\nIn 2016 as well as being Canada's largest producer, Alberta consumed more natural gas than any other province at 3.9 billion cubic feet per day. Part of the high consumption is due to generating 40% of the provinces electricity using gas.\n\nThe provincial government has stated \"natural gas is a climate solution\", under the LiveSmart BC initiative, natural gas furnaces and water heaters receive cash back thereby promoting the burning of fossil fuel in the province. The province states that an important part of new natural gas production will come from the Horn River basin where about 500 million tonnes of CO2 will be released into the atmosphere. Natural gas production in BC tripled between 1990 and 2010.\n\nTotal BC petroleum and natural gas emissions in 2014 were 50 million tonnes of carbon dioxide equivalent. The city of Vancouver in 2015 issued a report stating that for buildings, natural gas supplied 59% of all energy use, while electricity made up the remainder. BC has committed to reducing greenhouse gases to 33 per cent below 2007 levels by 2020, however the province is far short of that goal, only achieving a 6.5% reduction as of 2015. Although the new Site C dam is expected to have a large initial electricity surplus, the former Liberal government of the province proposed to sell this power rather than using it to cut the 2.3 billion cubic feet per day of natural gas consumption.\n\nThe use of electricity in Canada began with a few trial installations of electric arc lights in Montreal and Toronto in 1878 and 1879. A permanent arc lighting system was installed in Toronto in 1881 and used to illuminate a number of stores, including Eaton's. In Ottawa, arc lights were installed in several mills. By 1883 arc lights were installed in the streets of Toronto, Montreal and Winnipeg, and by 1890 numerous cities from St. John's, Newfoundland and Labrador to Victoria, British Columbia had arc lighting.\n\nThe first successful installations of Thomas Edison's incandescent lighting systems began in Ontario and Quebec starting in 1882. In 1886 a small plant supplying incandescent lights was installed in the Parliament Buildings in Ottawa. These direct current (DC) systems could serve only a radius of from the power plant. However, in 1888 the first permanent installation of a Westinghouse alternating current (AC) system was installed in Cornwall, Ontario.\n\nThe competition between AC and DC came to a head during the development of the potential of Niagara Falls because AC systems could supply electricity over much longer distances than DC systems. This was enormously important to Canada, which had numerous potential hydroelectric sites in remote locations. In 1897 a transmission system was built from the Batiscan River to Trois-Rivières, Quebec. In 1901 Shawinigan Falls was harnessed, and by 1903 a 50,000 volt power line carried electricity from it to Montreal.\n\nIn 1906, influenced by Adam Beck, the Ontario Legislature created the Hydro-Electric Power Commission (HEPC) to build transmissions lines to supply municipal utilities with power generated at Niagara Falls by private companies. In 1910 the HEPC began building 110,000 volt electric power lines to supply electricity to numerous municipalities in southwestern Ontario. In 1922 it started building its own generating stations, and gradually it took over most power generation in Ontario. In 1926 it signed long-term contracts to buy electricity from power companies in Quebec, but these proved controversial when jurisdictional disputes impeded development of the St. Lawrence and Ottawa Rivers and the Great Depression reduced demand. However, during World War II they proved an extremely important source of power for war production.\n\nAfter WWII, the development of the Saint Lawrence Seaway in conjunction with American power authorities allowed the development of the potential of the St. Lawrence River, and agreements with Quebec allowed Ontario to develop sites on the upper Ottawa River. However, hydroelectric capacity in Ontario was inadequate to meet growing demand, so coal burning power stations were built near Toronto and Windsor in the early 1950s. In the 1960s, Ontario turned to nuclear power. In 1962 the HEPC and Atomic Energy of Canada Limited started operating a 25-megawatt Nuclear Power Demonstrator, and in 1968 they brought the 200-megawatt Douglas Point Nuclear Generating Station into service. This was followed by the Pickering Nuclear Generating Station in 1971, the Bruce Nuclear Generating Station in 1977, and the Darlington Nuclear Generating Station in 1989. In 1974, toward the beginning of this expansion, the HEPC was renamed Ontario Hydro, which had long been its informal name. Eventually, Pickering grew to eight 540 MW nuclear reactors, Bruce to eight 900+ MW reactors, and Darlington to four 935 MW units.\n\nIn the 1990s, the enormous debt from building nuclear power stations, combined with lower than expected reliability and life span, became a political issue. The Ontario government decided to open the market to competition. In the meantime, the closure of many of Ontario's nuclear reactors for rehabilitation, combined with increasing demand resulted in a substantial increase in coal-fired power generation, with resulting increases in air pollution levels. In 2003 a new government came into power in Ontario and pledged to phase out coal as a generation source, leaving open the question of how Ontario was to meet future demand.\n\nThe Quebec government followed the example of Ontario in nationalizing its electrical sector, and in 1944 expropriated the assets of the monopoly \"Montreal Light, Heat and Power Company\" to create a new crown corporation called Hydro-Québec. In the post-war era, Hydro-Québec set about expanding and improving the reliability of the electric power grid, and demonstrated it could transmit electricity over long distances at extremely high voltages. Under Maurice Duplessis the Quebec government preferred to leave electrification of rural areas to the Rural Electrification Agency., however after Jean Lesage took power in 1960, Hydro-Québec gained exclusive rights to develop new hydroelectric projects, and in 1963 it began the gradual takeover of all private distributors in the province. Driven by rapidly growing demand, Hydro-Québec built three major hydroelectric complexes in rapid succession: Manicouagan-Outardes on the North Shore of the Saint Lawrence River, and the James Bay Project on La Grande River. This, combined with lower than projected demand, created a surplus of electricity in Quebec, so in 1997, Hydro-Québec began wholesale marketing of electricity to the United States.\n\nThe development of electric power in British Columbia began with the installation of electric lights in Victoria in 1883. Created in 1897, the BC Electric Company built BC's first hydroelectric plant near Victoria the following year, and created subsidiaries to supply electricity to Victoria and Vancouver, the province's two largest cities. BC Electric was taken over by Montreal-based Power Corporation in 1928. Before and during World War II, BC Electric primarily supplied power to the main cities of Vancouver and Victoria, leaving other regions with spotty and unreliable supply. In 1938, the BC government created the British Columbia Utilities Commission, which limited BC Electric's profit margins. In 1945, the provincial government created a crown corporation, the BC Power Commission (BCPC), to acquire small utilities and extended electrification to rural and isolated areas. BCPC grew to supply more than 200 small communities throughout the province.\n\nThe American and Canadian governments signed the Columbia River Treaty in 1961 and ratified it in 1964, agreeing to share power from hydroelectric dams on the Columbia River. To enable development of major hydroelectric sites on the Columbia and Peace Rivers, the BC government under Premier W. A. C. Bennett bought BC Electric in 1961, and the following year merged it with the BCPC to create the British Columbia Hydro and Power Authority, commonly known as BC Hydro. During the 60s and 70s, BC Hydro built some of the largest hydroelectric projects in the world, notably the W. A. C. Bennett Dam. More than 80% of BC Hydro's electricity is produced by 61 dams at 43 locations on the Columbia and Peace Rivers. Since that time the company's developments have been much smaller. During the 1980s BC Hydro changed its focus from building new hydroelectric plants to promoting energy conservation.\n\nIn 2010 the province enacted the Clean Energy Act which puts it on a path toward electricity self-sufficiency and energy conservation, while opening the door to energy exports, further investments in clean, renewable energy and a requirement that 93 percent of its electricity must come from clean or renewable sources. After the first application to build the Site C Dam was denied by the BC Utilities Commission in 1983, BC Hydro began purchasing from independent power producers which provide 20% of BC Hydro's supply.\n\nWith its earliest beginnings in the 1890s, Alberta's electricity system evolved as combination of a municipally and privately owned and operated systems based on coal-fired generation supplemented with some hydro. Most major municipalities operated municipally owned distribution systems.\n\nBeginning as early as 1887, Alberta had numerous small, privately owned firms that supplied towns across the province with electricity. However, service was often inconsistent—limited to a select number of businesses and provided only for a few evening hours.\n\nAs of 2008, Alberta's electricity sector was the most carbon-intensive of all Canadian provinces and territories, with total emissions of 55.9 million tonnes of equivalent in 2008, accounting for 47% of all Canadian emissions in the electricity and heat generation sector.\n\nCalgary Power's first major project became the construction of the province's first large-scale hydroelectric plant, located at the Horseshoe Falls. The Horseshoe Falls Plant's opening on May 21, 1911, allowed Calgary Power to meet the needs of the city. According to the Morning Albertan, Calgary mayor J. W. Mitchell was aroused from a Sunday nap to flip the switch which officially opened the plant and connected the city with its first large-scale source of electricity. In 1911, Calgary Power supplied 3,000 horsepower of electricity to the city at a cost of $30 per horsepower. The city had 44,000 people in 1911, and the emerging need for mass transportation was met by the booming streetcar industry, which accounted for a significant share of the city's electric usage. By 1913, Calgary Power had constructed the Kananaskis Falls Plant as an additional source of power.\n\nIn 1947, two years after the war ended, Calgary Power moved its head office from Montreal—then the nation's largest city and prime business center—to Calgary, reorganized, and incorporated as Calgary Power Ltd. At that time, Calgary Power supplied the province of Alberta with 99 percent of its hydroelectric power. Also in 1947, Calgary Power built its Barrier Hydro Plant and used it to test the use of a newly developed remote-control operation system. The automation efforts worked well enough that Calgary Power soon converted all of its plants to the Barrier Plant system. A control center that could operate the company's entire system was built in Seebe in 1951.\n\nCalgary Power continued to expand through the 1950s and 1960s, developing its first underground distribution lines and building dams on the Brazeau and North Saskatchewan rivers. The reservoir built on the North Saskatchewan project, Lake Abraham, became the largest man-made lake in the province. Also at this time, Calgary Power began exploring thermal energy generation, since few sites remained that were suitable for hydro power development. The company built its first thermal generating plant in 1956 near Wabamun Lake, west of Edmonton and near large coal reserves.\n\nOn July 19, 1911, Canadian Western Natural Gas, Light, Heat, and Power Company Limited was incorporated to provide natural gas from near Medicine Hat to other communities in southern Alberta. Electricity was also provided.\n\nIn 1954, International Utilities became the corporate owner of Canadian, Northwestern and Canadian Western Utilities. Canadian Utilities purchased the McMurray Light and Power Company Limited and Slave Lake Utilities. Northland Utilities Limited was added in 1961. In the early 1970s, Canadian Utilities became the corporate parent of Canadian Western, Northwestern, Northland, and Alberta Power Limited, which was the electrical operations of Canadian Utilities.\n\nOn October 23, 1891 a group of entrepreneurs obtain a 10-year permit to build the Edmonton Electric Lighting and Power Company on the banks of the North Saskatchewan River. The Edmonton Electrical Lighting and Power Company became a municipally owned electric utility in 1902, then the Electrical Distribution and Power Plant departments combined to form Edmonton Power in 1970. Electrical generation capacity was also expanded in 1970 with the coal fired Clover Bar Generating Station construction. Within the next eight years, another three units are added, bringing the combined generating capacity of the Clover bar and Rossdale generating stations to 1050 megawatts by 1979. Expansion occurred again in 1989 with the first Genesee unit is operating at full load and in 1994 with a second Genesee unit to a total capacity of both units to 850 megawatts. Electricity generated at Genesee was made commercially available through the Alberta Interconnected Grid in the early 1990s. EPCOR was formed from the merger of Edmonton's municipal natural gas, power and water utilities in 1996 and converted into a public company in 2006. Then EPCOR Utilities Inc. spun off its power generation business to create Capital Power Corporation in 2009.\n\nThis electrical system changed in 1996, when Alberta began to restructure its electricity market away from traditional regulation to a market-based system. The market now includes a host of buyers and sellers, and an increasingly diverse infrastructure.\n\nConsumers range from residential buyers to huge industrial consumers mining the oil sands, operating pipelines and milling forest products. On the supply side, generators range from wind farms east of Crowsnest Pass to oilsands plants and other petroleum processing facilities which generate marketable electricity surplus to their own needs, to coal-fired plants near Edmonton. Because of lower altitude, cooler temperatures, greater supplies of water for cooling and steam generation, and large near surface supplies of thermal coal, central Alberta is thermodynamically the best place in Alberta to generate hydrocarbon-fuelled electricity.\n\nThe diversity of Alberta’s electricity supply has increased substantially in recent years. To a large extent because of deregulation, the province has more technology, fuels, locations, ownership, and maintenance diversity than in the past and the rest of Canada. The system’s reliability, its cost structure and Alberta’s collective exposure to risk are now met by a complex system based on diverse power sources. However, overloaded power lines between northern Alberta and the south of the province are wasting enough electricity to power half the city of Red Deer, Alberta.\n\nElectric power generation in Canada draws on hydroelectric, nuclear, coal and natural gas, with a small but growing contribution from wind power. The electrification of Canada, was spurred from the US. The Niagara electrical power plant spurred industrial development in Southern Ontario.\nSoon major rivers across Canada had hydro schemes on them. The Canadian electrical grid was closely connected to and supplied large amounts of energy to the U.S. electrical grid. Many provinces have had a provincially owned monopoly power generator, such as Ontario Hydro, Manitoba Hydro, Hydro-Québec, Sask Power and BC Hydro. Many major provincial hydroelectric schemes also included federal involvement and subsidies. These concerns embarked on vast building schemes in the postwar years raising some of the largest dams in the world.\n\nOntario, Canada's most populous province, generates some 9,600 MW annually, over half of that coming from one dozen nuclear reactors. Ontario also has natural gas, and hydro facilities. However, Ontario faces a challenge as it must replace 80% of its generating capacity in the next twenty years—the old stations have time-expired and the nuclear reactors are overstressed. A debate continues over whether to go largely nuclear or go with renewables. Since the Green Energy Act 2009, the debate has become even more heated.\n\nCanada is a leader in the field of nuclear energy. Nuclear power in Canada is provided by 19 commercial reactors with a net capacity of 13.5 Gigawatts (GWe), producing a total of 95.6 Terawatt-hours (TWh) of electricity, which accounted for 16.6% of the nation's total electric energy generation in 2015. All but one of these reactors are located in Ontario where they produced 61% of the province's electricity in 2016 (91.7 TWh). Seven smaller reactors are used for research and to produce radioactive isotopes for nuclear medicine.\n\nCanadian nuclear reactors are a type of pressurized heavy-water reactor (PHWR) of indigenous design, the CANDU reactor. CANDU reactors have been exported to India, Pakistan, Argentina, South Korea, Romania, and China.\n\nUranium mining in Canada took off with the Great Bear Lake deposit furnishing some material for the Manhattan Project. Today Cameco and Areva Resources Canada are major produces of uranium for nuclear power. Cameco mines the world's largest high-grade uranium deposit at the McArthur River mine in Northern Saskatchewan.\n\nZEEP was Canada's first nuclear reactor built in 1945. Canada set up its NRX research reactor at Chalk River Laboratories in 1947. In 1962 the NPD reactor in Rolphton, Ontario was the first prototype power reactor in Canada. From this the NRC and the AECL developed the CANDU reactor. Ontario Hydro's first production power reactor was constructed at the Douglas Point in 1956. Eighteen reactors were then built in the following four decades in Ontario, Quebec and New Brunswick. In 2008 Nuclear power re-emerged with approved plans to build new reactors at existing stations at Darlington and Pickering, Ontario; another new station is planned for Peace River, AB. All are subject to planning and environmental reviews.\n\nCanada generates a significant part of its electricity from hydroelectric dams, but has otherwise limited renewable energy generation, although wind power is growing quickly. The first commercial wind farm in Canada was built in Alberta in 1993. A 20 megawatt tidal plant sits at Annapolis, Nova Scotia, and uses the daily tides of the Bay of Fundy.\n\nThe first commercial solar project was built in Stone Mills, Ontario in 2009.Skypower Ltd, used over 120,000 thin film photovoltaic solar panels, for a total of 9,1 megawatt, creating clean solar energy for 1000 homes annually.\n\nPoliticians have been willing to subsidize renewable methods using taxpayer funds to increase the amount and percentage of Canada's electricity generated.\n\nAfter the 1973 Oil Crisis, energy conservation became practical with smaller cars and insulated homes.\nAppliances were improved to use less energy. In the recent years, this successfully lead to both a reduction in energy use and CO2-emissions.\n\nHowever, the adaptation of new technologies in civil engineering also caused new issues, such as the Urea-formaldehyde insulation disaster and the ongoing Leaky condo crisis.\n\n\n"}
{"id": "11876825", "url": "https://en.wikipedia.org/wiki?curid=11876825", "title": "Fiery flying serpent", "text": "Fiery flying serpent\n\nThe fiery flying serpent is a creature mentioned in the Book of Isaiah (30:6).\n\n\nReferences to \"fiery serpents\" lacking a mention of flight can be found in several places in the Hebrew Bible.\n\n\n\nIn the History of Herodotus, Book 2, Herodotus describes flying winged serpents.\n\nIn Cicero's \"On the Nature of the Gods\" Book 1 XXXVI, Cotta describes how the Egyptian ibis was deified because it \"protects Egypt from plague, by killing and eating the flying serpents that are brought from the Libyan desert...\"\n"}
{"id": "34050147", "url": "https://en.wikipedia.org/wiki?curid=34050147", "title": "Flora Hiperspectral", "text": "Flora Hiperspectral\n\nThe (Flora Hiperspectral) satellite, initially scheduled to launch in 2017, is a proposed Brazilian–American earth observation satellite. It will produce hyperspectral surface images of comparable resolution to Landsat satellites. The Flora Hiperspectral is a joint project between the Jet Propulsion Laboratory (JPL) of NASA and the National Institute for Space Research of Brazil.\n\nThis mission will produce the first time in orbit, hyperspectral images with a spatial resolution and global coverage comparable to that of the Landsat satellite and CBERS program. Data will study the biochemical and biophysical properties of soil cover and the action of man (for example, deforestation and fires) on ecosystem functioning.\n\nHyperspectral data are obtained in narrow strips, contiguous and in sufficient number to build spectra similar to the laboratory. Indicate biochemical characteristics (e.g., nutrient levels, moisture) and biophysical plants. Detect water stress, diseases, or difficulties in adapting to the ground culture. They measure of mineral absorption in exposed surfaces of rocks. In coastal and inland waters, measure chlorophyll and suspended sediment. The satellite monitoring currently only allows to observe if there is forest or not. The Plant will be able to check the chemical and physical characteristics of the vegetation, which today no satellite can do. According to reports, it will be 10 times more advanced than any satellite of the same type.\n\nJPL will provide an instrument, which cost 150 million dollars and Brazil come with providing about US$110 million, concerning a part of the satellite and part of the launch.\n\nHigh spectral resolution camera (200 bands between 400 and 2500 nm along a 150 km swath.\nSpatial resolution is 30 meters, with 14 bits quantization.\n\n"}
{"id": "6783609", "url": "https://en.wikipedia.org/wiki?curid=6783609", "title": "Freshwater fish", "text": "Freshwater fish\n\nFreshwater fish are those that spend some or all of their lives in fresh water, such as rivers and lakes, with a salinity of less than 0.05%. These environments differ from marine conditions in many ways, the most obvious being the difference in levels of salinity. To survive fresh water, the fish need a range of physiological adaptations.\n\n41.24% of all known species of fish are found in fresh water. This is primarily due to the rapid speciation that the scattered habitats make possible. When dealing with ponds and lakes, one might use the same basic models of speciation as when studying island biogeography.\n\nFreshwater fish differ physiologically from salt water fish in several respects. Their gills must be able to diffuse dissolved gasses while keeping the salts in the body fluids inside. Their scales reduce water diffusion through the skin: freshwater fish that have lost too many scales will die. They also have well developed kidneys to reclaim salts from body fluids before excretion.\n\nMany species of fish do reproduce in freshwater, but spend most of their adult lives in the sea. These are known as anadromous fish, and include, for instance, salmon, trout, sea lamprey and three-spined stickleback. Some other kinds of fish are, on the contrary, born in salt water, but live most of or parts of their adult lives in fresh water; for instance the eels. These are known as catadromous fish.\n\nSpecies migrating between marine and fresh waters need adaptations for both environments; when in salt water they need to keep the bodily salt concentration on a level lower than the surroundings, and vice versa. Many species solve this problem by associating different habitats with different stages of life. Both eels, anadromous salmoniform fish and the sea lamprey have different tolerances in salinity in different stages of their lives.\n\nAmong fishers in the United States, freshwater fish species are usually classified by the water temperature in which they survive. The water temperature affects the amount of oxygen available as cold water contains more oxygen than warm water.\n\nColdwater fish species survive in the coldest temperatures, preferring a water temperature of . In North America, air temperatures that result in sufficiently cold water temperatures are found in the northern United States, Canada, and in the southern United States at high elevation. Common coldwater fish include brook trout, rainbow trout, and brown trout.\n\nWarmwater fish species can survive in a wide range of conditions, preferring a water temperature around . Warmwater fish can survive cold winter temperatures in northern climates, but thrive in warmer water. Common warmwater fish include catfish, largemouth bass, bluegill, crappies, and many other species from the Centrarchidae family.\n\nCoolwater fish species prefer water temperature between the coldwater and warmwater species, around . They are found throughout North America except for the southern portions of the United States. Common coolwater species include muskellunge, northern pike, walleye, and yellow perch\n\nAbout four in ten North American freshwater fish are endangered, according to a pan-North American study, the main cause being human pollution. The number of fish species and subspecies to become endangered has risen from 40 to 61, since 1989.\n\n\n\n"}
{"id": "8605182", "url": "https://en.wikipedia.org/wiki?curid=8605182", "title": "Galkynysh Gas Field", "text": "Galkynysh Gas Field\n\nThe Galkynysh Gas Field, formerly known as Iolotan gas field or South Yolotan – Osman field, is a large natural gas field near Ýolöten in Mary Province of Turkmenistan. It is the world's second-largest gas field.\n\nThe discovery of the gas field was announced on 2 November 2006. Late Turkmen president Saparmurat Niyazov invited Chinese CNPC and Turkish Çalik Enerji to participate in the exploration and development of the Ýolöten field. In 2008, the gas field was audited by Gaffney, Cline & Associates. According to Gaffney, Cline and Associates (GCA), Galkynysh is five times larger than the Dauletabad gas field and fourth- or fifth-largest gas field in the world.\n\nIn December 2009, the contracts to develop the field were awarded to CNPC, Hyundai Engineering and Petrofac.\n\nIn November 2011, the field was renamed as Galkynysh. Production started in September 2013.\n\nThe gas field ranks among the world's five largest with estimated reserves of between of natural gas and proven commercial reserves of . It lies on zone of in length and in width in the depth of . Galkynysh consist of Iolotan, Minara, Osman and Yashlar fields. Other nearby gas areas are Gunorta Garakel, Garakel, Giurgiu, Gazanly, Gundogar Eloten and Gunbatar Yandakly.\n\nOil reserves are 300 million tons. The crude oil extracted at Galkynysh is transported to the Seýdi Oil Refinery.\n\nThe field is developed by Türkmengaz. CNPC, Hyundai Engineering and Petrofac built gas processing plants.\n"}
{"id": "9134092", "url": "https://en.wikipedia.org/wiki?curid=9134092", "title": "Geodynamics", "text": "Geodynamics\n\nGeodynamics is a subfield of geophysics dealing with dynamics of the Earth. It applies physics, chemistry and mathematics to the understanding of how mantle convection leads to plate tectonics and geologic phenomena such as seafloor spreading, mountain building, volcanoes, earthquakes, faulting and so on. It also attempts to probe the internal activity by measuring magnetic fields, gravity, and seismic waves, as well as the mineralogy of rocks and their isotopic composition. Methods of geodynamics are also applied to exploration of other planets.\n\nGeodynamics is generally concerned with processes that move materials throughout the Earth. In the Earth’s interior, movement happens when rocks melt or deform and flow in response to a stress field. This deformation may be brittle, elastic, or plastic, depending on the magnitude of the stress and the material’s physical properties, especially the stress relaxation time scale. Rocks are structurally and compositionally heterogeneous and are subjected to variable stresses, so it is common to see different types of deformation in close spatial and temporal proximity. When working with geological timescales and lengths, it is convenient to use the continuous medium approximation and equilibrium stress fields to consider the average response to average stress.\n\nExperts in geodynamics commonly use data from geodetic GPS, InSAR, and seismology, along with numerical models, to study the evolution of the Earth's lithosphere, mantle and core.\n\nWork performed by geodynamicists may include:\n\nRocks and other geological materials experience strain according to three distinct modes, elastic, plastic, and brittle depending on the properties of the material and the magnitude of the stress field. Stress is defined as the average force per unit area exerted on each part of the rock. Pressure is the part of stress that changes the volume of a solid; shear stress changes the shape. If there is no shear, the fluid is in hydrostatic equilibrium. Since, over long periods, rocks readily deform under pressure, the Earth is in hydrostatic equilibrium to a good approximation. The pressure on rock depends only on the weight of the rock above, and this depends on gravity and the density of the rock. In a body like the Moon, the density is almost constant, so a pressure profile is readily calculated. In the Earth, the compression of rocks with depth is significant, and an equation of state is needed to calculate changes in density of rock even when it is of uniform composition.\n\nElastic deformation is always reversible, which means that if the stress field associated with elastic deformation is removed, the material will return to its previous state. Materials only behave elastically when the relative arrangement along the axis being considered of material components (e.g. atoms or crystals) remains unchanged. This means that the magnitude of the stress cannot exceed the yield strength of a material, and the time scale of the stress cannot approach the relaxation time of the material. If stress exceeds the yield strength of a material, bonds begin to break (and reform), which can lead to ductile or brittle deformation.\n\nDuctile or plastic deformation happens when the temperature of a system is high enough so that a significant fraction of the material microstates (figure 1) are unbound, which means that a large fraction of the chemical bonds are in the process of being broken and reformed. During ductile deformation, this process of atomic rearrangement redistributes stress and strain towards equilibrium faster than they can accumulate. Examples include bending of the lithosphere under volcanic islands or sedimentary basins, and bending at oceanic trenches. Ductile deformation happens when transport processes such as diffusion and advection that rely on chemical bonds to be broken and reformed redistribute strain about as fast as it accumulates.\n\nWhen strain localizes faster than these relaxation processes can redistribute it, brittle deformation occurs. The mechanism for brittle deformation involves a positive feedback between the accumulation or propagation of defects especially those produced by strain in areas of high strain, and the localization of strain along these dislocations and fractures. In other words, any fracture, however small, tends to focus strain at its leading edge, which causes the fracture to extend.\n\nIn general, the mode of deformation is controlled not only by the amount of stress, but also by the distribution of strain and strain associated features. Whichever mode of deformation ultimately occurs is the result of a competition between processes that tend to localize strain, such as fracture propagation, and relaxational processes, such as annealing, that tend to delocalize strain.\n\nStructural geologists study the results of deformation, using observations of rock, especially the mode and geometry of deformation to reconstruct the stress field that affected the rock over time. Structural geology is an important complement to geodynamics because it provides the most direct source of data about the movements of the Earth. Different modes of deformation result in distinct geological structures, e.g. brittle fracture in rocks or ductile folding.\n\nThe physical characteristics of rocks that control the rate and mode of strain, such as yield strength or viscosity, depend on the thermodynamic state of the rock and composition. The most important thermodynamic variables in this case are temperature and pressure. Both of these increase with depth, so to a first approximation the mode of deformation can be understood in terms of depth. Within the upper lithosphere, brittle deformation is common because under low pressure rocks have relatively low brittle strength, while at the same time low temperature reduces the likelihood of ductile flow. After the brittle-ductile transition zone, ductile deformation becomes dominant. Elastic deformation happens when the time scale of stress is shorter than the relaxation time for the material. Seismic waves are a common example of this type of deformation. At temperatures high enough to melt rocks, the ductile shear strength approaches zero, which is why shear mode elastic deformation (S-Waves) will not propagate through melts.\n\nThe main motive force behind stress in the Earth is provided by thermal energy from radioisotope decay, friction, and residual heat. Cooling at the surface and heat production within the Earth create a metastable thermal gradient from the hot core to the relatively cool lithosphere. This thermal energy is converted into mechanical energy by thermal expansion. Deeper hotter and often have higher thermal expansion and lower density relative to overlying rocks. Conversely, rock that is cooled at the surface can become less buoyant than the rock below it. Eventually this can lead to a Rayleigh-Taylor instability (Figure 2), or interpenetration of rock on different sides of the buoyancy contrast.\n\nNegative thermal buoyancy of the oceanic plates is the primary cause of subduction and plate tectonics, while positive thermal buoyancy may lead to mantle plumes, which could explain intraplate volcanism. The relative importance of heat production vs. heat loss for buoyant convection throughout the whole Earth remains uncertain and understanding the details of buoyant convection is a key focus of geodynamics.\n\nGeodynamics is a broad field which combines observations from many different types of geological study into a broad picture of the dynamics of Earth. Close to the surface of the Earth, data includes field observations, geodesy, radiometric dating, petrology, mineralogy, drilling boreholes and remote sensing techniques. However, beyond a few kilometers depth, most of these kinds of observations become impractical. Geologists studying the geodynamics of the mantle and core must rely entirely on remote sensing, especially seismology, and experimentally recreating the conditions found in the Earth in high pressure high temperature experiments.(see also Adams–Williamson equation). \n\nBecause of the complexity of geological systems, computer modeling is used to test theoretical predictions about geodynamics using data from these sources.\n\nThere are two main ways of geodynamic numerical modeling .\n\n\nBasic fluid dynamics modelling can further be subdivided into instantaneous studies, which aim to reproduce the instantaneous flow in a system due to a given buoyancy distribution, and time-dependent studies, which either aim to reproduce a possible evolution of a given initial condition over time or a statistical (quasi) steady-state of a given system.\n\n\n"}
{"id": "23558063", "url": "https://en.wikipedia.org/wiki?curid=23558063", "title": "Geomagnetic jerk", "text": "Geomagnetic jerk\n\nIn geophysics, a geomagnetic jerk or secular geomagnetic variation impulse is a relatively sudden change in the second derivative of the Earth's magnetic field with respect to time.\n\nThese events were noted by Courtillot and others in 1978. The clearest ones, observed all over the world, happened in 1969, 1978, 1991, and 1999. Data before 1969 is scarcer, but there is evidence of other global jerks in 1901, 1913, and 1925. Other events in 1932, 1949, 1958, 1986, and 2003 were detected only in some parts of the world. These events are believed to originate in the interior of the Earth (rather than being due to external phenomena such as the solar wind); but their precise cause is still a matter of research.\n\nThe name \"jerk\" was borrowed from dynamics, where it means the rate of change of the acceleration of a body, that is, the third derivative of its position with respect to time (the acceleration being the second derivative); or, more specifically, a sudden and momentary spike (or dip) in that rate.\n\nJerks seem to occur in irregular intervals, on average about once every 10 years. In the period between jerks, each component of the field at a specific location changes with time \"t\" approximately as a fixed polynomial of the second degree, \"A\" \"t\" + \"B\" \"t\" + \"C\". Each jerk is a relatively sudden change (spread over a period of a few months to a couple of years) in the \"A\" coefficient of this formula, which determines the second derivative; and usually in \"B\" and \"C\" coefficients as well.\n\nThe strength of each jerk varies from location to location, and some jerks are observed only in some regions. For example, the 1949 jerk was clearly observed at Tucson (North America, long. 110.93°), but not at Chambon la Forêt (Europe, long. 2.27°). Moreover, the global jerks seem to occur at slightly different times in different regions; often earlier in the Northern hemisphere than in the Southern hemisphere.\n\nThese events are believed to be caused by changes in the flow patterns of the liquid outer core of the Earth. Another theory is that they are due to torsional oscillations in the solid inner core of the Earth. There have been claims that they are connected to strong earthquakes.\n"}
{"id": "31178484", "url": "https://en.wikipedia.org/wiki?curid=31178484", "title": "George Joseph (scientist)", "text": "George Joseph (scientist)\n\nGeorge Joseph (born 4 August 1938) is an Indian space scientist, best known for his contributions to the development of remote sensing technology in India, especially in the field of earth observation sensors. He is a former chairman of the Lunar Mission Study Task Force of the Indian Space Research Organization and an elected fellow of the National Academy of Sciences, India, Indian Academy of Sciences and Indian National Academy of Engineering. The Government of India awarded him the Padma Bhushan, the third highest civilian award, in 1999.\n\nGeorge Joseph was born on 4 August 1938 to Advocate M. G. Joseph and Alice in Madras Presidency, British India. After completing schooling, he joined St.Berchmans College, Changanassery and also attended Alagappa Chettiar College in Karaikudi and University College, Trivandrum. He served as lecturer in Union Christian College, Aluva and at CMS College Kottayam. Afterwards he was a trainee at Bhabha Atomic Research Center (BARC), Bombay.\n\nJoseph is married to Mercy and they have two sons.\n\nJoseph started his research career in 1962 at the Tata Institute of Fundamental Research(TIFR), Bombay, where he was involved in the study of cosmic rays. Based on his research work at TIFR he was awarded PhD degree by the Bombay University. A novel detector system designed by him was flown on the first Indian satellite Aryabhata (satellite) to detect solar neutrons.\n\nIn 1973, Joseph was invited to join the Space Applications Centre, Ahmedabad-one of the centers of the Indian Space Research Organization (ISRO),and initiated the development of remote sensing technology particularly sensors of various types. He has been the guiding force in the design and development of all the earth observation cameras on board Indian Remote Sensing Satellite(IRS) and INSAT. He served ISRO in various capacities including Director, Space Applications Centre and took keen interest and initiative to ensure the fruits of space technology reach common man. One of the noteworthy contributions of Joseph is the study report on Indian Mission to Moon in the capacity as Chairman, Lunar Mission Study Task Force.\n\nJoseph has served in a number of national and international committees/organizations including President of Technical Commission -I of the International Society for Photogrammetry and Remote Sensing (ISPRS) during 1996-2000 and Director, Centre for Space Science and Technology Education in Asia and the Pacific (CSSTEAP), (affiliated to the United Nations), with headquarters at Dehradun during 2006-2009.\n\n\n\n\n"}
{"id": "14341696", "url": "https://en.wikipedia.org/wiki?curid=14341696", "title": "Haar (fog)", "text": "Haar (fog)\n\nIn meteorology, haar or sea fret is a cold sea fog. It occurs most often on the east coast of England or Scotland between April and September, when warm air passes over the cold North Sea.\n\nHaar is typically formed over the sea and is blown to the land by the wind. This commonly occurs when warmer moist air moves over the relatively cooler North Sea causing the moisture in the air to condense, forming haar. \n\nSea breezes and easterly winds then bring the haar into the east coast of Scotland and North-East England where it can continue for several miles inland. This can be common in the UK summer when heating of the land creates a sea breeze, bringing haar in from the sea and as a result can significantly reduce temperatures compared to those just a few miles inland.\n\nThe term \"haar\" is used along certain lands bordering the North Sea, primarily eastern Scotland and the north-east of England. Variants of the term in Scots and northern English include har, hare, harl, harr and hoar. The origin may be Low German/Middle Dutch \"hare\" or Saxon. In Yorkshire and Northumberland it is commonly referred to as a sea fret.\n"}
{"id": "23210613", "url": "https://en.wikipedia.org/wiki?curid=23210613", "title": "Holy places", "text": "Holy places\n\nHoly places are sites that religions considers to be of special religious significance. Holy places are often visited by pilgrims.\n\nLocated in Bahji near Acre, Israel, the Shrine of Bahá'u'lláh is the most holy place for Bahá'ís and their Qiblih, or direction of prayer. It contains the remains of Bahá'u'lláh, founder of the Bahá'í Faith and is near the spot where he died in the Mansion of Bahjí. The second holiest site is the Shrine of the Báb in Haifa.\n\nIn Iran, the Baha'is possessed several important holy sites, including the House of the Bab in Shiraz; the House of Baha' Allah's father in Tehran; the shrine and grave of Quddus. Since the Revolution, all these places have been demolished, and a mosque has been built on the site of the Bab's house.\n\nThe Buddha is said to have identified four sites most worthy of pilgrimage for Buddhists, saying that they would produce a feeling of spiritual urgency. These are:\n\nIn the later commentarial tradition, four more sites were added to make Eight Great Places, places where a miraculous event is reported to have occurred:\n\nThere are various other locations in India and Nepal associated with the Buddha, and there are holy sites located throughout Asia for each Buddhist tradition, for instance in Afghanistan, Cambodia, China, India, Indonesia, Japan, Laos, Myanmar, Nepal, Sri Lanka, South Korea and Thailand. Lhasa in Tibet contains many culturally significant Tibetan Buddhist sites such as the Potala Palace, Jokhang temple and Norbulingka palaces.\n\nIn Christianity, the holy places are significant because they are the place of birth, ministry, crucifixion and resurrection of Jesus, the Saviour or Messiah to Christianity.\nHoly cities for Christians of all denominations\n\nDuring the Crusades, Christian pilgrims often sought out the holy places in the Outremer, especially early in the 12th century immediately after Jerusalem was captured. The holy places included sites in Jerusalem and Bethlehem as well as:\n\n\nHoly places of Confucianism include Temple of Confucius, Cemetery of Confucius and the Kong Family Mansion in Qufu, Shandong Province of China.\n\nStonehenge is a site of religious significance in Neo-Druidism. Druids perform pilgrimage there, The first modern Druids to make ceremonies at this site is the Ancient Order of Druids.\n\nThe Seven sacred ancient Holy towns are Ayodhya, Mathura, Haridwar, Varanasi, Kanchipuram, Dvārakā, and Ujjain.\n\nThe Holy Dhamas (Char Dham to Smartas)\n\nThe Four Maha Kumbha Mela sites\n\nVaishnavas\nShaivites\n\nKaumaram (Arupadaiveedu (Six sacred abodes of Murugan; Tamil\n\nShaktas (Shakti piths)\n\nTirth Kshetra\n\nThe three holiest sites in Islam are the Masjid al-Haram, or Grand Mosque, (in Mecca) ; the Al-Masjid al-Nabawi, or Prophet's Mosque, (in Medina) and Al-Aqsa Mosque in Jerusalem. Every year millions of Muslims from all over the world visit Masjid al-Haram and Al-Masjid al-Nabawi to perform Hajj.\n\nMuslims are also required, if able, to undertake a pilgrimage to Mecca (Hajj) (; \"\" \"pilgrimage\") at least once in one's life: \n\nThe next two holiest sites especially for Shia Islam are the Imam Ali Mosque and the Imam Husayn shrine (which is visited by more pilgrims than any other holy site in any religion. More than 18 millions people visited on the day of Ashura in 2018)\n\nIn Jewish tradition, the Temple Mount is regarded as the place where God chose the Divine Presence to rest. Jewish tradition regards the Mount, or the Foundation Stone, as the location of a number of important events mentioned in the Bible, including the location from which the world expanded into its present form, Abraham's binding of Isaac, Jacob's dream, the threshing floor which King David purchased from Araunah the Jebusite, and the location of the two Jewish Temples. Jewish texts record that the Mount will be the site of the Third Temple, which will be rebuilt with the coming of the messiah.\n\nThe Temple Mount is the holiest site in Judaism and is the place to which Jews turn during prayer. Due to its extreme sanctity, many Jews will not walk on the Mount itself, to avoid unintentionally entering the area where the Holy of Holies stood. The Temple is mentioned extensively in Orthodox services. Conservative Judaism mentions the Temple and its restoration, but not its sacrifices. The destruction of the Temple is mourned on the Jewish fast day of Tisha B'Av, and is remembered on a number of occasions such as in the breaking of a glass at the end of a wedding ceremony. Due to religious restrictions on entering the most sacred areas of the Temple Mount (see following section), the Western Wall, a retaining wall of the Temple Mount and remnant of the Second Temple structure, is considered by some rabbinical authorities the holiest accessible site for Jews to pray.\n\nJerusalem itself is also the holiest city in Judaism, and the spiritual center of the Jewish people since the 10th century BCE. The Four Holy Cities in Jewish tradition are the cities of Jerusalem, Hebron, Tiberias, and Safed: The Jewish Encyclopedia in 1906 noted: \"Since the sixteenth century the Holiness of Palestine, especially for burial, has been almost wholly transferred to four cities—Jerusalem, Hebron, Tiberias, and Safed.\"\n\nThe Panj Takht are the five gurudwaras which are revered as the seats of power in Sikhism. They are all located in India - the Akal Takht Sahib in Amritsar, Punjab; the Keshgarh Sahib in Anandpur Sahib, Punjab; the Damdama Sahib in Bhatinda, Punjab; the Takht Sri Patna Sahib in Patna, Bihar and the Hazur Sahib Nanded in Nanded, Maharashtra.\n\nSri Harmandir Sahib, also known as Sri Darbar Sahib or Golden Temple, (on account of its scenic beauty and golden coating for English speaking world), is named after Hari (God) the temple of God. The Sikhs all over the world, daily wish to pay visit to Sri Amritsar and to pay obeisance at Sri Harmandir Sahib in their Ardas.\n\nGuru Arjan's father Guru Ram Das founded the town named after him \"Ramdaspur\", around a large man-made water pool called \"Ramdas Sarovar\". Guru Arjan continued the infrastructure building effort of his father. The town expanded during the time of Guru Arjan, financed by donations and constructed by voluntary work. The pool area grew into a temple complex with the gurdwara Harmandir Sahib near the pool. Guru Arjan installed the scripture of Sikhism inside the new temple in 1604. The city that emerged is now known as Amritsar, and is the holiest pilgrimage site in Sikhism.\n\nFour sacred mountains of Taoism：\n\n\n"}
{"id": "2898710", "url": "https://en.wikipedia.org/wiki?curid=2898710", "title": "Hot-filament ionization gauge", "text": "Hot-filament ionization gauge\n\nThe hot-filament ionization gauge, sometimes called a hot-filament gauge or hot-cathode gauge, is the most widely used low-pressure (vacuum) measuring device for the region from 10 to 10 Torr. It is a triode, with the filament being the cathode.\n\n\"Note: Principles are mostly the same for hot-cathode ion sources in particle accelerators to create electrons.\"\n\nA regulated electron current (typically 10 mA) is emitted from a heated filament. The electrons are attracted to the helical grid by a DC potential of about +150 V. Most of the electrons pass through the grid and collide with gas molecules in the enclosed volume, causing a fraction of them to be ionized. The gas ions formed by the electron collisions are attracted to the central ion collector wire by the negative voltage on the collector (typically −30 V). Ion currents are on the order of 1 mA/Pa. This current is amplified and displayed by a high-gain differential amplifier/electrometer.\n\nThis ion current differs for different gases at the same pressure; that is, a hot-filament ionization gauge is composition-dependent. Over a wide range of molecular density, however, the ion current from a gas of constant composition is directly proportional to the molecular density of the gas in the gauge.\n\nA hot-cathode ionization gauge is composed mainly of three electrodes, all acting as a triode, wherein the cathode is the filament. The three electrodes are a collector or plate, a filament, and a grid. The collector current is measured in picoamperes by an electrometer. The filament voltage to ground is usually at a potential of 30 volts, while the grid voltage at 180–210 volts DC, unless there is an optional electron bombardment feature, by heating the grid, which may have a high potential of approximately 565 volts.\nThe most common ion gauge is the hot-cathode Bayard–Alpert gauge, with a small collector inside the grid. A glass envelope with an opening to the vacuum can surround the electrodes, but usually the nude gauge is inserted in the vacuum chamber directly, the pins being fed through a ceramic plate in the wall of the chamber. Hot-cathode gauges can be damaged or lose their calibration if they are exposed to atmospheric pressure or even low vacuum while hot.\n\nElectrons emitted from the filament move several times in back-and-forth movements around the grid before finally entering the grid. During these movements, some electrons collide with a gas molecule to form a pair of an ion and an electron (electron ionization). The number of these ions is proportional to the gas molecule density multiplied by the electron current emitted from the filament, and these ions pour into the collector to form an ion current. Since the gas molecule density is proportional to the pressure, the pressure is estimated by measuring the ion current.\n\nThe low-pressure sensitivity of hot-cathode gauges is limited by the photoelectric effect. Electrons hitting the grid produce X-rays that produce photoelectric noise in the ion collector. This limits the range of older hot-cathode gauges to 10 Torr and the Bayard–Alpert gauges to about 10 Torr. Additional wires at cathode potential in the line of sight between the ion collector and the grid prevent this effect. In the extraction type the ions are not attracted by a wire but by an open cone. As the ions cannot decide which part of the cone to hit, they pass through the hole and form an ion beam. This ion beam can be passed on to a\n\n\n\n"}
{"id": "23721650", "url": "https://en.wikipedia.org/wiki?curid=23721650", "title": "Index of energy articles", "text": "Index of energy articles\n\nThis is an index of energy articles.\n\nActivation energy\n- Alternative energy\n- Alternative energy indexes\n- American Museum of Science and Energy (AMSE)\n- Anisotropy energy\n- Atomic energy\n\nBinding energy\n- Black hole\n- Breeder reactor\n- Brown energy\n\nCharacteristic energy\n- Conservation of energy\n- Consol Energy\n\nDark energy\n- Decay energy\n- Direct Energy\n- Dirichlet's energy\n- Dyson's sphere\n\nEcological energetics\n- Electric potential energy\n- Electrochemical energy conversion\n- Embodied energy\n- Encircled energy\n- Energy\n- Energy accidents\n- Energy accounting\n- Energy amplifier\n- Energy analyser\n- Energy applications of nanotechnology\n- Energy balance (biology)\n- Energy bar\n- Energy barrier\n- Energy being\n- Energy carrier\n- Energy Catalyzer\n- Energy cell\n- Energy charge\n- Energy conservation\n- Energy conversion efficiency\n- Energy crop\n- Energy current\n- Energy density\n- Energy-depth relationship in a rectangular channel\n- Energy development\n- Energy-dispersive X-ray spectroscopy\n- Energy distance\n- Energy drift\n- Energy drink\n- Energy efficiency gap\n- Energy-Efficient Ethernet\n- Energy-efficient landscaping\n- Energy elasticity\n- Energy engineering\n- Energy (esotericism)\n- Energy expenditure\n- Energy factor\n- Energy field disturbance\n- Energy filtered transmission electron microscopy\n- Energy transfer\n- Energy flow (ecology)\n- Energy flux\n- Energy forestry\n- Energy functional\n- Energy gel\n- Energy harvesting\n- Energy input labeling\n- Energy landscape\n- Energy level\n- Energy level splitting\n- Energy management software\n- Energy management system\n- Energy–maneuverability theory\n- Energy Manufacturing Co. Inc\n- Energy medicine\n- Energy–momentum relation\n- Energy monitoring and targeting\n- Energy Probe\n- Energy profile (chemistry)\n- Energy quality\n- Energy recovery ventilation\n- Energy security\n- Energy (signal processing)\n- Energy Slave\n- Energy Star\n- Energy statistics\n- Energy Storage Challenge\n- Energy storage\n- Energy system\n- Energy technology\n- Energy tower (downdraft)\n- Energy transfer\n- Energy transfer upconversion\n- Energy transformation\n- Energy value of coal\n- Energy vortex (stargate)\n- Enthalpy\n- Entropy\n- Equipartition theorem\n- E-statistic\n- Exertion\n\nFermi energy\n- Forms of energy\n- Fuel\n- Fusion power\n\nGeothermal energy\n- Gravitational energy\n- Gravitational potential\n\nHistory of energy\n- Hydroelectricity\n\nInteraction energy\n- Intermittent energy source\n- Internal energy\n- Invariant mass\n\nJosephson energy\n\nKinetic energy\n\nLatent heat\n\nMagnetic confinement fusion \n- Marine energy\n- Mass–energy equivalence\n- Mechanical energy\n- Möbius energy\n\nNegative energy\n- Nuclear fusion\n- Nuclear power\n- Nuclear reactor\n\nOrders of magnitude (energy)\n- Osmotic power\n\nPhotosynthesis\n- Potential energy\n- Power (physics)\n- Primary energy\n\nQi\n- Quasar\n\nRelativistic jet\n- Renewable energy - Rotational energy\n\nSeismic scale\n- Solar energy\n- Solar thermal energy\n- Sound energy\n- Specific energy\n- Specific kinetic energy\n- Specific orbital energy\n- Surface energy\n\nThermodynamic free energy\n- Threshold energy\n- Tidal power\n- Turbulence kinetic energy\n\nUnits of energy\n- Universe of Energy\n\nVacuum energy\n\nWork (physics)\n- World energy resources and consumption\n- World Forum on Energy Regulation\n\nZero-energy building\n- Zero-energy universe\n- Zero-point energy\n\n"}
{"id": "25322095", "url": "https://en.wikipedia.org/wiki?curid=25322095", "title": "Indian Ocean trade", "text": "Indian Ocean trade\n\nIndian Ocean Trade has been a key factor in East–West exchanges throughout history. Long distance trade in dhows and sailboats made it a dynamic zone of interaction between peoples, cultures, and civilizations stretching from Java in the East to Zanzibar and Mombasa in the West. Cities and states on the Indian Ocean rim were Janus-faced. They looked outward to the sea as much as they looked inward to the hinterland.\n\nThere was an extensive maritime trade network operating between the Harappan and Mesopotamian civilizations as early as the middle Harappan Phase (2600-1900 BCE), with much commerce being handled by \"middlemen merchants from Dilmun\" (modern Bahrain and Failaka located in the Persian Gulf). Such long-distance sea trade became feasible with the development of plank-built watercraft, equipped with a single central mast supporting a sail of woven rushes or cloth.\n\nSeveral coastal settlements like Sotkagen-dor (astride Dasht River, north of Jiwani), Sokhta Koh (astride Shadi River, north of Pasni), and Balakot (near Sonmiani) in Pakistan along with Lothal in western India, testify to their role as Harappan trading outposts. Shallow harbours located at the estuaries of rivers opening into the sea allowed brisk maritime trade with Mesopotamian cities.\n\nPrior to Roman expansion, the various peoples of the subcontinent had established strong maritime trade with other countries. The dramatic increase in South Asian ports, however, did not occur until the opening of the Red Sea by the Greeks and the Romans and the attainment of geographical knowledge concerning the region’s seasonal monsoons. In fact, the first two centuries of the Common Era indicate this increase in trade between present-day western India and Rome. This expansion of trade was due to the comparative peace established by the Roman Empire during the time of Augustus (23 September 63 BC – 19 August AD 14), which allowed for new explorations. \n\nThe replacement of Greece by the Roman empire as the administrator of the Mediterranean basin led to the strengthening of direct maritime trade with the east and the elimination of the taxes extracted previously by the middlemen of various land-based trading routes. Strabo's mention of the vast increase in trade following the Roman annexation of Egypt indicates that monsoon was known and manipulated for trade in his time.\n\nThe trade started by Eudoxus of Cyzicus in 130 BCE kept increasing, and according to Strabo (II.5.12.), writing some 150 years later:\n\nBy the time of Augustus up to 120 ships were setting sail every year from Myos Hormos to India. So much gold was used for this trade, and apparently recycled by the Kushan Empire (Kushans) for their own coinage, that Pliny the Elder (NH VI.101) complained about the drain of specie to India:\n\nThe three main Roman ports involved with eastern trade were Arsinoe, Berenice and Myos Hormos. Arsinoe was one of the early trading centers but was soon overshadowed by the more easily accessible Myos Hormos and Berenice.\n\nThe Ptolemaic dynasty exploited the strategic position of Alexandria to secure trade with the subcontinent. The course of trade with the east then seems to have been first through the harbor of Arsinoe, the present day Suez. The goods from the East African trade were landed at one of the three main Roman ports, Arsinoe, Berenice or Myos Hormos. The Romans repaired and cleared out the silted up canal from the Nile to harbor center of Arsinoe on the Red Sea. This was one of the many efforts the Roman administration had to undertake to divert as much of the trade to the maritime routes as possible.\n\nArsinoe was eventually overshadowed by the rising prominence of Myos Hormos. The navigation to the northern ports, such as Arsinoe-Clysma, became difficult in comparison to Myos Hormos due to the northern winds in the Gulf of Suez. Venturing to these northern ports presented additional difficulties such as shoals, reefs and treacherous currents.\n\nMyos Hormos and Berenice appear to have been important ancient trading ports, possibly used by the Pharaonic traders of ancient Egypt and the Ptolemaic dynasty before falling into Roman control.\n\nThe site of Berenice, since its discovery by Belzoni (1818), has been equated with the ruins near Ras Banas in Southern Egypt. However, the precise location of Myos Hormos is disputed with the latitude and longitude given in Ptolemy's \"Geography\" favoring Abu Sha'ar and the accounts given in classical literature and satellite images indicating a probable identification with Quseir el-Quadim at the end of a fortified road from Koptos on the Nile. The Quseir el-Quadim site has further been associated with Myos Hormos following the excavations at el-Zerqa, halfway along the route, which have revealed ostraca leading to the conclusion that the port at the end of this road may have been Myos Hormos.\n\nThe regional ports of Barbaricum (modern Karachi), Sounagoura (central Bangladesh) Barygaza, Muziris in Kerala, Korkai, Kaveripattinam and Arikamedu on the southern tip of present-day India were the main centers of this trade, along with Kodumanal, an inland city. The Periplus Maris Erythraei describes Greco-Roman merchants selling in Barbaricum \"thin clothing, figured linens, topaz, coral, storax, frankincense, vessels of glass, silver and gold plate, and a little wine\" in exchange for \"costus, bdellium, lycium, nard, turquoise, lapis lazuli, Seric skins, cotton cloth, silk yarn, and indigo\". In Barygaza, they would buy wheat, rice, sesame oil, cotton and cloth.\n\nTrade with Barigaza, under the control of the Indo-Scythian Western Satrap Nahapana (\"Nambanus\"), was especially flourishing:\n\nMuziris is a lost port city on the south-western coast of India which was a major center of trade in the ancient Tamil land between the Chera kingdom and the Roman Empire. Its location is generally identified with modern-day Cranganore (central Kerala). Large hoards of coins and innumerable shards of amphorae found at the town of Pattanam (near Cranganore) have elicited recent archeological interest in finding a probable location of this port city.\n\nAccording to the Periplus, numerous Greek seamen managed an intense trade with Muziris:\n\nThe Periplus Maris Erythraei mentions a marketplace named Poduke (ch. 60), which G.W.B. Huntingford identified as possibly being Arikamedu in Tamil Nadu, a centre of early Chola trade (now part of Ariyankuppam), about from the modern Pondicherry. Huntingford further notes that Roman pottery was found at Arikamedu in 1937, and archeological excavations between 1944 and 1949 showed that it was \"a trading station to which goods of Roman manufacture were imported during the first half of the 1st century AD\".\n\nFollowing the Roman-Persian Wars, the areas under the Roman Byzantine Empire were captured by Khosrow II of the Persian Sassanian Dynasty, but the Byzantine emperor Heraclius reconquered them (628). The Arabs, led by 'Amr ibn al-'As, crossed into Egypt in late 639 or early 640 CE. This advance marked the beginning of the Islamic conquest of Egypt and the fall of ports such as Alexandria, used to secure trade with the subcontinent by the Roman world since the Ptolemaic dynasty.\n\nThe decline in trade saw the ancient Tamil country turn to Southeast Asia for international trade, where it influenced the native culture to a greater degree than the impressions made on Rome.\n\nThe Satavahanas developed shipping ventures in Southeast Asia.\n\nThe 8th century depiction of a wooden double outrigger and sailed Borobudur ship in ancient Java suggests that there were ancient trading links across the Indian Ocean between Indonesia and Madagascar and East Africa sometimes referred to as the 'Cinnamon Route.' The single or double outrigger is a typical feature of vessels of the seafaring Austronesians and the most likely vessel used for their voyages and exploration across Southeast Asia, Oceania, and Indian Ocean. During this period, between 7th to 13th century in Indonesian archipelago flourished the Srivijaya thalassocracy empire that rule the maritime trade network in maritime Southeast Asia and connecting India and China.\n\nDuring the Muslim period, in which the Muslims had dominated the trade across the Indian Ocean, the Gujaratis were bringing spices from the Moluccas as well as silk from China, in exchange for manufactured items such as textiles, and then selling them to the Egyptians and Arabs. Calicut was the center of Indian pepper exports to the Red Sea and Europe at this time with Egyptian and Arab traders being particularly active.\n\nIn Madagascar, merchants and slave traders from the Middle East (Shirazi Persians, Omani Arabs, Arabized Jews, accompanied by Bantus from southeast Africa) and from Asia (Gujaratis, Malays, Javanese, Bugis) were sometimes integrated within the indigenous Malagasy clans New waves of Austronesian migrants arrived in Madagascar at this time leaving behind a lasting cultural and genetic legacy.\n\nChinese fleets under Zheng He criscrossed the Indian Ocean during the early part of the 15th century. The missions were diplomatic rather than commercial, but many exchanges of gift and produces were made.\n\nThe Portuguese under Vasco da Gama discovered a naval route to the Indian Ocean through the southern tip of Africa in 1497–98. Initially, the Portuguese were mainly active in Calicut, but the northern region of Gujarat was even more important for trade, and an essential intermediary in east-west trade.\nVenetian interests were directly threatened as the traditional trade patterns were eliminated and the Portuguese became able to undersell the Venetians in the spice trade in Europe. Venice broke diplomatic relations with Portugal and started to look at ways to counter its intervention in the Indian Ocean, sending an ambassador to the Egyptian court. Venice negotiated for Egyptian tariffs to be lowered to facilitate competition with the Portuguese, and suggested that \"rapid and secret remedies\" be taken against the Portuguese. The Mamluks sent a fleet in 1507 under Amir Husain Al-Kurdi, which would fight in the Battle of Chaul.\n\nThe Ottomans tried to challenge Portugal's hegemony in the Persian Gulf region by sending an armada against the Portuguese under Ali Bey in 1581. They were supported in this endeavor by the chiefs of several local principalities and port towns such as Muscat, Gwadar, and Pasni. However, the Portuguese successfully intercepted and destroyed the Ottoman Armada. Subsequently, the Portuguese attacked Gwadar and Pasni on the Mekran Coast and sacked them in retaliation for providing aid and comfort to the enemy.\n\nDuring the 16th century the Portuguese had established bases in the Persian Gulf. In 1602, the Iranian army under the command of Imam-Quli Khan Undiladze managed to expel the Portuguese from Bahrain. In 1622, with the help of four English ships, Abbas retook Hormuz from the Portuguese in the capture of Ormuz. He replaced it as a trading centre with a new port, Bandar Abbas, nearby on the mainland, but it never became as successful.\n\nDuring the 16th and 17th century, Japanese ships also made forays into Indian Ocean trade through the Red Seal ship \nsystem.\n\n"}
{"id": "7982164", "url": "https://en.wikipedia.org/wiki?curid=7982164", "title": "Intermittent energy source", "text": "Intermittent energy source\n\nAn intermittent energy source is any source of energy that is not continuously available for conversion into electricity and outside direct control because the used primary energy cannot be stored. Intermittent energy sources may be predictable but cannot be dispatched to meet the demand of an electric power system.\n\nThe use of intermittent sources in an electric power system usually displaces storable primary energy that would otherwise be consumed by other power stations. Another option is to store electricity generated by non-dispatchable energy sources for later use when needed, e.g. in the form of pumped storage, compressed air or in batteries. A third option is the sector coupling e.g. by electric heating for district heating schemes.\n\nThe use of small amounts of intermittent power has little effect on grid operations. Using larger amounts of intermittent power may require upgrades or even a redesign of the grid infrastructure.\n\nSeveral key terms are useful for understanding the issue of intermittent power sources. These terms are not standardized, and variations may be used. Most of these terms also apply to traditional power plants.\n\n\nIntermittency inherently affects solar energy, as the production of renewable electricity from solar sources depends on the amount of sunlight at a given place and time. Solar output varies throughout the day and through the seasons, and is affected by dust, fog, cloud cover, frost or snow. Many of the seasonal factors are fairly predictable, and some solar thermal systems make use of heat storage to produce grid power for a full day.\n\n\nThe impact of intermittency of solar-generated electricity will depend on the correlation of generation with demand. For example, solar thermal power plants such as Nevada Solar One are somewhat matched to summer peak loads in areas with significant cooling demands, such as the south-western United States. Thermal energy storage systems like the small Spanish Gemasolar Thermosolar Plant can improve the match between solar supply and local consumption. The improved capacity factor using thermal storage represents a decrease in maximum capacity, and extends the total time the system generates power.\n\nWind-generated power is a variable resource, and the amount of electricity produced at any given point in time by a given plant will depend on wind speeds, air density, and turbine characteristics (among other factors). If wind speed is too low (less than about 2.5 m/s) then the wind turbines will not be able to make electricity, and if it is too high (more than about 25 m/s) the turbines will have to be shut down to avoid damage. While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable.\n\n\nAccording to a 2007 study of wind in the United States, ten or more widely separated wind farms connected through the grid could be relied upon for from 33 to 47% of their average output (15–20% of nominal capacity) as reliable, baseload power, as long as minimum criteria are met for wind speed and turbine height. When calculating the generating capacity available to meet summer peak demand, ERCOT (manages Texas grid) counts wind generation at 8.7% of nameplate capacity.\n\nWind generates about 16% (EWEA – 2011 European Statistics, February 2012) of electric energy in Spain and Portugal, 9% in Ireland, and 7% in Germany. Wind provides around 40% of the annual electricity generated in Denmark (up from 20% in 2005); to meet this percentage Denmark exports surpluses and imports during shortfalls to and from the EU grid, particularly Norwegian Hydro, to balance supply with demand. \n\nBecause wind power is generated by large numbers of small generators, individual failures do not have large impacts on power grids. This feature of wind has been referred to as resiliency.\n\nWind power is affected by air temperature because colder air is more dense and therefore more effective at producing wind power. As a result, wind power is affected seasonally (more output in winter than summer) and by daily temperature variations. During the 2006 California heat wave output from wind power in California significantly decreased to an average of 4% of capacity for seven days. A similar result was seen during the 2003 European heat wave, when the output of wind power in France, Germany, and Spain fell below 10% during peak demand times. Heat waves are partially caused by large amounts of solar radiation.\nAccording to an article in EnergyPulse, \"the development and expansion of well-functioning day-ahead and real time markets will provide an effective means of dealing with the variability of wind generation.\"\n\nSeveral authors have said that no energy resource is totally reliable. Amory Lovins says that nuclear power plants are intermittent in that they will sometimes fail unexpectedly, often for long periods of time. For example, in the United States, 132 nuclear plants were built, and 21% were permanently and prematurely closed due to reliability or cost problems, while another 27% have at least once completely failed for a year or more. The remaining U.S. nuclear plants produce approximately 90% of their full-time full-load potential, but even they must shut down (on average) for 39 days every 17 months for scheduled refueling and maintenance. To cope with such intermittence by nuclear (and centralized fossil-fuelled) power plants, utilities install a \"reserve margin\" of roughly 15% extra capacity spinning ready for instant use.\n\nThe penetration of intermittent renewables in most power grids is low, global electricity production in 2014 was supplied by 3.1% wind, and 1% solar. Wind generates roughly 16% of electric energy in Spain and Portugal, 15.3% in Ireland, and 7% in Germany. , wind provides 39% of the electricity generated in Denmark. To operate with this level of penetration, Denmark exports surpluses and imports during shortfalls to and from neighbouring countries, particularly hydroelectric power from Norway, to balance supply with demand. It also uses large numbers of combined heat and power (CHP) stations which can rapidly adjust output.\n\nThe intermittency and variability of renewable energy sources can be reduced and accommodated by diversifying their technology type and geographical location, forecasting their variation, and integrating them with dispatchable renewables (such as hydropower, geothermal, and biomass). Combining this with energy storage and demand response can create a power system that can reliably match real-time energy demand. The integration of ever-higher levels of renewables has already been successfully demonstrated:\nIn 2009, eight American and three European authorities, writing in the leading electrical engineers' professional journal, didn't find \"a credible and firm technical limit to the amount of wind energy that can be accommodated by electricity grids\". In fact, not one of more than 200 international studies, nor official studies for the eastern and western U.S. regions, nor the International Energy Agency, has found major costs or technical barriers to reliably integrating up to 30% variable renewable supplies into the grid, and in some studies much more.\nA research group at Harvard University quantified the meteorologically defined limits to reduction in the variability of outputs from a coupled wind farm system in the Central US:\nThe problem with the output from a single wind farm located in any particular region is that it is variable on time scales ranging from minutes to days posing difficulties for incorporating relevant outputs into an integrated power system. The high frequency (shorter than once per day) variability of contributions from individual wind farms is determined mainly by locally generated small scale boundary layer. The low frequency variability (longer than once per day) is associated with the passage of transient waves in the atmosphere with a characteristic time scale of several days. The high frequency variability of wind-generated power can be significantly reduced by coupling outputs from 5 to 10 wind farms distributed uniformly over a ten state region of the Central US. More than 95% of the remaining variability of the coupled system is concentrated at time scales longer than a day, allowing operators to take advantage of multi-day weather forecasts in scheduling projected contributions from wind.\nMark Z. Jacobson has studied how wind, water and solar technologies can be integrated to provide the majority of the world's energy needs. He advocates a \"smart mix\" of renewable energy sources to reliably meet electricity demand:\nBecause the wind blows during stormy conditions when the sun does not shine and the sun often shines on calm days with little wind, combining wind and solar can go a long way toward meeting demand, especially when geothermal provides a steady base and hydroelectric can be called on to fill in the gaps.\nMark A. Delucchi and Mark Z. Jacobson argue that there are at least seven ways to design and operate renewable energy systems so that they will reliably satisfy electricity demand:\nTechnological solutions to mitigate large-scale wind energy type intermittency exist such as increased interconnection (the European super grid), Demand response, load management, diesel generators (in the British National Grid, Frequency Response / National Grid Reserve Service type schemes, and use of existing power stations on standby. Studies by academics and grid operators indicate that the cost of compensating for intermittency is expected to be high at levels of penetration above the low levels currently in use today Large, distributed power grids are better able to deal with high levels of penetration than small, isolated grids. For a hypothetical European-wide power grid, analysis has shown that wind energy penetration levels as high as 70% are viable, and that the cost of the extra transmission lines would be only around 10% of the turbine cost, yielding power at around present day prices. Smaller grids may be less tolerant to high levels of penetration.\n\nMatching power demand to supply is not a problem specific to intermittent power sources. Existing power grids already contain elements of uncertainty including sudden and large changes in demand and unforeseen power plant failures. Though power grids are already designed to have some capacity in excess of projected peak demand to deal with these problems, significant upgrades may be required to accommodate large amounts of intermittent power. The International Energy Agency (IEA) states:\n\"In the case of wind power, operational reserve is the additional generating reserve needed to ensure that differences between forecast and actual volumes of generation and demand can be met. Again, it has to be noted that already significant amounts of this reserve are operating on the grid due to the general safety and quality demands of the grid. Wind imposes additional demands only inasmuch as it increases variability and unpredictability. However, these factors are nothing completely new to system operators. By adding another variable, wind power changes the degree of uncertainty, but not the kind...\"\n\nWith sufficient energy storage, highly variable and intermittent sources can supply all of a regions electrical power. For solar to provide half of all electricity and using a solar capacity factor of 20%, the total capacity for solar would be 250% of the grids average daily load. For wind to provide half of all electricity and using a wind capacity factor of 30% the total capacity for wind would be 160% of the grids average daily load.\n\nA pumped storage facility would then store enough water for the grids weekly load, with a capacity for peak demand i.e.:200% of the grid average. This would allow for one week of overcast and windless conditions. There are unusual costs associated with building storage and total generating capacity being six times the grid average.\n\nAll sources of electrical power have some degree of variability, as do demand patterns which routinely drive large swings in the amount of electricity that suppliers feed into the grid. Wherever possible, grid operations procedures are designed to match supply with demand at high levels of reliability, and the tools to influence supply and demand are well-developed. The introduction of large amounts of highly variable power generation may require changes to existing procedures and additional investments.\n\nThe capacity of a reliable renewable power supply, can be fulfilled by the use of backup or extra infrastructure and technology, using mixed renewables to produce electricity above the intermittent average, which may be used to meet regular and unanticipated supply demands. Additionally, the storage of energy to fill the shortfall intermittency or for emergencies can be part of a reliable power supply.\n\nAll managed grids already have existing operational and \"spinning\" reserve to compensate for existing uncertainties in the power grid. The addition of intermittent resources such as wind does not require 100% \"back-up\" because operating reserves and balancing requirements are calculated on a system-wide basis, and not dedicated to a specific generating plant.\n\n\n\nAt times of low load where non-dispatchable output from wind and solar may be high, grid stability requires lowering the output of various dispatchable generating sources or even increasing controllable loads, possibly by using energy storage to time-shift output to times of higher demand. Such mechanisms can include:\n\n\nStorage of electrical energy results in some lost energy because storage and retrieval are not perfectly efficient. Storage may also require substantial capital investment and space for storage facilities.\n\nThe variability of production from a single wind turbine can be high. Combining any additional number of turbines (for example, in a wind farm) results in lower statistical variation, as long as the correlation between the output of each turbine is imperfect, and the correlations are always imperfect due to the distance between each turbine. Similarly, geographically distant wind turbines or wind farms have lower correlations, reducing overall variability. Since wind power is dependent on weather systems, there is a limit to the benefit of this geographic diversity for any power system.\n\nMultiple wind farms spread over a wide geographic area and gridded together produce power more constantly and with less variability than smaller installations. Wind output can be predicted with some degree of confidence using weather forecasts, especially from large numbers of turbines/farms. The ability to predict wind output is expected to increase over time as data is collected, especially from newer facilities.\n\nIn the past electrical generation was mostly dispatchable and consumer demand led how much and when to dispatch power. The trend in adding intermittent sources such as wind, solar, and run-of-river hydro means the grid is beginning to be led by the intermittent supply. The use of intermittent sources relies on electric power grids that are carefully managed, for instance using highly dispatchable generation that is able to shut itself down whenever an intermittent source starts to generate power, and to successfully startup without warning when the intermittents stop generating. Ideally the capacity of the intermittents would grow to be larger than consumer demand for periods of time, creating excess low price electricity to displace heating fuels or be converted to mechanical or chemical storage for later use.\n\nThe displaced dispatchable generation could be coal, natural gas, biomass, nuclear, geothermal or storage hydro. Rather than starting and stopping nuclear or geothermal it is cheaper to use them as constant base load power. Any power generated in excess of demand can displace heating fuels, be converted to storage or sold to another grid. Biofuels and conventional hydro can be saved for later when intermittents are not generating power. Alternatives to burning coal and natural gas which produce fewer greenhouse gases may eventually make fossil fuels a stranded asset that is left in the ground. Highly integrated grids favor flexibility and performance over cost, resulting in more plants that operate for fewer hours and lower capacity factors.\n\n\nPenetration refers to the proportion of a primary energy (PE) source in an electric power system, expressed as a percentage. There are several methods of calculation yielding different penetrations. The penetration can be calculated either as:\n\n\nThe level of penetration of intermittent variable sources is significant for the following reasons:\n\n\nRenewable electricity supply in the 20-50+% penetration range has already been implemented in several European systems, albeit in the context of an integrated European grid system:\nIn 2010, four German states, totaling 10 million people, relied on wind power for 43-52% of their annual electricity needs. Denmark isn't far behind, supplying 22% of its power from wind in 2010 (26% in an average wind year). The Extremadura region of Spain is getting up to 25% of its electricity from solar, while the whole country meets 16% of its demand from wind. Just during 2005-2010, Portugal vaulted from 17% to 45% renewable electricity.\nThere is no generally accepted maximum level of penetration, as each system's capacity to compensate for intermittency differs, and the systems themselves will change over time. Discussion of acceptable or unacceptable penetration figures should be treated and used with caution, as the relevance or significance will be highly dependent on local factors, grid structure and management, and existing generation capacity.\n\nFor most systems worldwide, existing penetration levels are significantly lower than practical or theoretical maximums; for example, a UK study found that \"it is clear that intermittent generation need not compromise electricity system reliability at any level of penetration foreseeable in Britain over the next 20 years, although it may increase costs.\"\n\nThere is no generally accepted maximum penetration of wind energy that would be feasible in any given grid. Rather, economic efficiency and cost considerations are more likely to dominate as critical factors; technical solutions may allow higher penetration levels to be considered in future, particularly if cost considerations are secondary.\n\nHigh penetration scenarios may be feasible in certain circumstances:\n\n\nStudies have been conducted to assess the viability of specific penetration levels in specific energy markets.\n\nA series of detailed modelling studies by Dr. Gregor Czisch, which looked at the European wide adoption of renewable energy and interlinking power grids the European super grid using HVDC cables, indicates that the entire European power usage could come from renewables, with 70% total energy from wind at the same sort of costs or lower than at present. This proposed large European power grid has been called a \"super grid.\"\n\nThe model deals with intermittent power issues by using base-load renewables such as hydroelectric and biomass for a substantial portion of the remaining 30% and by heavy use of HVDC to shift power from windy areas to non-windy areas. The report states that \"electricity transport proves to be one of the keys to an economical electricity supply\" and underscores the importance of \"international co-operation in the field of renewable energy use [and] transmission.\"\n\nDr. Czisch described the concept in an interview, saying \"For example, if we look at wind energy in Europe. We have a winter wind region where the maximum production is in winter and in the Sahara region in northern Africa the highest wind production is in the summer and if you combine both, you come quite close to the needs of the people living in the whole area - let's say from northern Russia down to the southern part of the Sahara.\"\n\nA study of the grid in Ireland indicates that it would be feasible to accommodate 42% (of demand) renewables\nin the electricity mix. This acceptable level of renewable penetration was found in what the study called Scenario 5, provided 47% of electrical capacity (different from demand) with the following mix of renewable energies:\n\n\nThe study cautions that various assumptions were made that \"may have understated dispatch restrictions, resulting in an underestimation of operational costs, required wind curtailment, and CO emissions\" and that \"The limitations of the study may overstate the technical feasibility of the portfolios analyzed...\"\n\nScenario 6, which proposed renewables providing 59% of electrical capacity and 54% of demand had problems. Scenario 6 proposed the following mix of renewable energies:\n\n\nThe study found that for Scenario 6, \"a significant number of hours characterized by extreme system situations occurred where load and reserve requirements could not be met. The results of the network study indicated that for such extreme renewable penetration scenarios, a system re-design is required, rather than a reinforcement exercise.\" The study declined to analyze the cost effectiveness of the required changes because \"determination of costs and benefits had become extremely dependent on the assumptions made\" and this uncertainty would have impacted the robustness of the results.\n\nA study published in October 2006, by the Ontario Independent Electric System Operator (IESO) found that \"there would be minimal system operation impacts for levels of wind capacity up to 5,000 MW,\" which corresponds to a peak penetration of 17%\n\nA November 2006 analysis, found that \"wind power may be able to cover more than 50% of the Danish electricity consumption in 2025\" under conditions of high oil prices and higher costs for CO allowances. Denmark's two grids (covering West Denmark and East Denmark separately) each incorporate high-capacity interconnectors to neighbouring grids where some of the variations from wind are absorbed. In 2012 the Danish government adopted a plan to increase the share of electricity production from wind to 50% by 2020, and to 84% in 2035.\n\nEstimates of the cost of wind energy may include estimates of the \"external\" costs of wind variability, or be limited to the cost of production. All electrical plant has costs that are separate from the cost of production, including, for example, the cost of any necessary transmission capacity or reserve capacity in case of loss of generating capacity. Many types of generation, particularly fossil fuel derived, will also have cost externalities such as pollution, greenhouse gas emission, and habitat destruction which are generally not directly accounted for. The magnitude of the economic impacts is debated and will vary by location, but is expected to rise with higher penetration levels. At low penetration levels, costs such as operating reserve and balancing costs are believed to be insignificant.\n\nIntermittency may introduce additional costs that are distinct from or of a different magnitude than for traditional generation types. These may include:\n\n\nStudies have been performed to determine the costs of variability. RenewableUK states:\n\nAn official at Xcel Energy claimed that at 20 percent penetration, additional standby generators to compensate for wind in Colorado would cost $8 per MWh, adding between 13% and 16% to the US$50–60 cost per MWh of wind energy.\n\nThe Union of Concerned Scientists conducted a study of the costs to increase the renewable penetration in Colorado to 10% and found that for an average residential bill \"customers of municipal utilities and rural electric cooperatives that opt out of the solar energy requirement\" would save 4 cents per month, but that for Xcel Energy customers there would be additional cost of about 10 cents per month. Total impact on all consumers would be $4.5 million or 0.01% over two decades.\n\nA detailed study for UK National Grid (a private power company) states \"We have estimated that for the case with 8,000 MW of wind needed to meet the 10% renewables target for 2010, balancing costs can be expected to increase by around £2 per MWh of wind production. This would represent an additional £40million per annum, just over 10% of existing annual balancing costs.\"\n\nIn evidence to the UK House of Lords Economic Affairs Select Committee, National Grid have quoted estimates of balancing costs for 40% wind and these lie in the range £500-1000M per annum. \"These balancing costs represent an additional £6 to £12 per annum on average consumer electricity bill of around £390.\"\n\nNational Grid notes that \"increasing levels of such renewable generation on the system would increase the costs of balancing the system and managing system frequency.\"\n\nA 2003 report, by Carbon Trust and the UK Department of Trade and Industry (DTI), projected costs of £1.6 to £2.4 billion for reinforcement and new build of transmission and distribution systems to support 10% renewable electricity in the UK by 2010, and £3.2bn to £4.5bn for 20% by 2020. The study classified \"Intermittency\" as \"Not a significant issue\" for the 2010 target but a \"Significant Issue\" for the 2020 target. \"See grid balancing\"\n\nA Minnesota study on wind penetration levels and found that \"total integration operating cost for up to 25% wind energy\" would be less than $0.0045 per kWh (additional).\n\nThere are differing views about some sources of renewable energy and intermittency. The World Nuclear Association argues that the sun, wind, tides and waves cannot be controlled to provide directly either continuous base-load power, or peak-load power when it is needed. Proponents of renewable energy use argue that the issue of intermittency of renewables is over-stated, and that practical experience demonstrates this. In any case, geothermal renewable energy has, like nuclear, no intermittency (but they both use the energy in radioactive materials like uranium, thorium and potassium).\n\nFor many years there was a consensus within the electric utilities in the U.S. that renewable electricity generators such as wind and solar are so unreliable and intermittent that they will never be able to contribute significantly to electric supply or provide baseload power. Thomas Petersnik, an analyst with the U.S. Energy Information Administration put it this way: \"by and large, renewable energy sources are too rare, too distant, too uncertain, and too ill-timed to provide significant supplies at times and places of need\".\nAccording to a transatlantic collaborative research paper on Energy return on energy Invested(EROEI), conducted by 6 analysts and led by D. Weißbach, as published in the peer reviewed journal \"Energy\" in 2013. The uncorrected for their intermittency(\"unbuffered\") EROEI for each energy source analyzed is as depicted in the attached table at right, while the buffered(corrected for their intermittency) EROEI stated in the paper for all low carbon power sources, with the exception of nuclear and biomass, were yet lower still. As when corrected for their weather intermittency/\"buffered\", the EROEI figures for intermittent energy sources as stated in the paper is diminished - a reduction of EROEI dependent on how reliant they are on back up energy sources.\n\nThe U.S. Federal Energy Regulatory Commission (FERC) Chairman Jon Wellinghoff has stated that \"baseload capacity is going to become an anachronism\" and that no new nuclear or coal plants may ever be needed in the United States. Some renewable electricity sources have identical variability to coal-fired power stations, so they are base-load, and can be integrated into the electricity supply system without any additional back-up. Examples include:\n\n\nGrid operators in countries like Denmark and Spain now integrate large quantities of renewable energy into their electricity grids, with Denmark receiving 40% of its electricity from wind power during some months.\n\nSupporters say that the total electricity generated from a large-scale array of dispersed wind farms, located in different wind regimes, cannot be accurately described as intermittent, because it does not start up or switch off instantaneously at irregular intervals. With a small amount of supplementary peak-load plant, which operates infrequently, large-scale distributed wind power can substitute for some base-load power and be equally reliable.\n\nHydropower can be intermittent and/or dispatchable, depending on the configuration of the plant. Typical hydroelectric plants in the dam configuration may have substantial storage capacity, and be considered dispatchable. Run of the river hydroelectric generation will typically have limited or no storage capacity, and will be variable on a seasonal or annual basis (dependent on rainfall and snow melt).\n\nAmory Lovins suggests a few basic strategies to deal with these issues:\n\nMoreover, efficient energy use and energy conservation measures can reliably reduce demand for base-load and peak-load electricity.\n\nInternational groups are studying much higher penetrations (30-100% renewable energy), and conclusions are that these levels are also technically feasible. In the UK, one summary of other studies indicated that if assuming that wind power contributed less than 20% of UK power consumption, then the intermittency would cause only moderate cost.\n\nMethods to manage wind power integration range from those that are commonly used at present (e.g. demand management) to potential new technologies for grid energy storage. Improved forecasting can also contribute as the daily and seasonal variations in wind and solar sources are to some extent predictable. The Pembina Institute and the World Wide Fund for Nature state in the Renewable is Doable plan that resilience is a feature of renewable energy:\n\n\nThese peer-reviewed papers examine the impacts of intermittency:\n\n\n"}
{"id": "17857226", "url": "https://en.wikipedia.org/wiki?curid=17857226", "title": "Jan DeBlieu", "text": "Jan DeBlieu\n\nJan DeBlieu is an American writer whose work often focuses on how people are shaped by the landscapes in which they live. Her own writing has been influenced by her adopted home on the Outer Banks of North Carolina.\n\nShe is the author of four books including \"Hatteras Journal\", \"Meant to Be Wild\", \"Wind\" (which won the John Burroughs Medal for Distinguished Natural History Writing, the highest national honor for that genre) and \"Year of the Comets.\" Her fifth book, which examines living in service to others, is forthcoming. She also has published dozens of essays and magazine articles, both in literary journals and mainstream publications like \"The New York Times Magazine,\" \"Smithsonian\", \"Audubon\", and \"Orion.\" Her work has been widely anthologized.\n\nIn 2003, she was named the Cape Hatteras Coastkeeper for the North Carolina Coastal Federation, a post she held until 2012. She has since returned to full-time writing.\n\nWhile DeBlieu's work has mostly focused on naturally history, landscape, and place, this shifted after the death of her son in a car accident in 2009. Since then she has concentrated on exploring how ordinary people can help change the lives of people in need. This is the subject of her forthcoming book, \"Searching for Seva\".\n\nIn 2006, she was featured in the book \"Bedrock: Writers on the Wonders of Geology\" edited by Lauret E. Savoy, Eldridge M. Moores, and Judith E. Moores (Trinity University Press) which looks at how writing pays a tribute to the Earth's geological features.\n\n"}
{"id": "57470435", "url": "https://en.wikipedia.org/wiki?curid=57470435", "title": "Laze (geology)", "text": "Laze (geology)\n\nLaze is acid rain and air pollution arising from steam explosions and large plume clouds containing extremely acidic condensate (mainly hydrochloric acid), which occur when molten lava flows enter cold oceans. The term \"laze\" is a portmanteau of \"lava\" and \"haze\".\n\nLaze, created by the interaction of lava and cold seawater, differs from vog, which originates from volcanic vents.\n\nThe extremely high temperatures of lava flows (1200 °C; 2200 °F) cause sea water to decompose into hydrogen and oxygen. The hydrogen combines with chloride ions dissolved in sea water, forming hydrogen chloride gas (hydrochloric acid). The rapidly rising plume of gas also carries with it fine particles of volcanic glass. The hydrochloric acid and other contaminants can precipitate out rapidly and the plume may become relatively safe a few hundred meters away, however, laze plumes should be shown a degree of respect as they can and have killed people who come in contact with them. The USGS has reported that, in 2000, two people were killed by exposure to laze clouds.\n"}
{"id": "42395495", "url": "https://en.wikipedia.org/wiki?curid=42395495", "title": "Leatheroid", "text": "Leatheroid\n\nLeatheroid is cellulose material very similar to vulcanized fibre in physical properties and uses. It is prepared using unsized cotton rag paper (as is vulcanized fibre) and mineral acid.\n\nLeatheroid was made Leatheroid Manufacturing Company and its successors the Mousam Manufacturing Company, the National Fibreboard Company and the Rogers Fibre Company. For nearly 50 years from 1881 to 1930 Leatheroid was made in Kennebunk, Maine.\n\nThe Leatheroid Manufacturing Company's first existence can be traced to Wheeling, West \nVirginia. The 1879 Session Laws of West Virginia showed the following individuals to hold shares:\nThe West Virginia law incorporated the company with $20,200 in initial capitalization, each share having a $100 value. The company was authorized to increase its capitalization to $100,000 through the sale of additional shares with the value of $100 each. Leatheroid operated in Wheeling for a year and then moved on to Pittsburgh, Pennsylvania, for a year and next to Philadelphia, Pennsylvania. Finally Leatheroid was purchased by the Mousam Manufacturing Company and its equipment and manufacturing operations removed to Kennebunk, Maine.\n\nThe Mousam Manufacturing Company was organized in 1875 to manufacture leatherboard at Kennebunk, Maine. Among those organizing Mousam Manufacturing Company were Emery Andrews, S. B. Rogers, Stephen Moore, and Homer Rogers. The company's name was the same as a cotton mill that had operated in Kennebunk from 1838 to 1850 and ceased operations after being destroyed in a fire. Mousam Manufacturing Company used most of its leatherboard in the production of shoe counters, which are used to stiffen the heel area of boots and shoes.\n\nIn 1881 when Mousam Manufacturing Company purchased Leatheroid and moved its manufacturing equipment into nearby mill buildings adjacent to its own leatherboard mill in Kennebunk, Maine, it organized it as a separate company, the Leatheroid Manufacturing Company. Emery Andrews, S.B. Rogers, Stephen Moore, and Homer Rogers were board members and controlling stock holders in the Leatheroid Manufacturing Company as they were in Mousam Manufacturing Company. In 1891 the Mousam Manufacturing Company and the Leatheroid Manufacturing Company along with Harwood Manufacturing Company of Leominster, Massachusetts, the Towne Manufacturing Company of Boston, Massachusetts, and the Clegg and Fisher Mill at Lawrence, Massachusetts, were merged into a new company known as the Consolidated Fibre Board and Leatheroid Company. This consolidation was known as the National Fibre Board Company with Emery Andrews as president and the board of directors consisting of Charles H. Allen, Homer Rogers, J. A. Harwood, Stephen Moore, and W.C. Gogswell. In 1918 the National Fibre Board Company changed its name to Rogers Fibre Company. The officers of Rogers Fibre Company in 1920 were Elliott Rogers, Pres., Kennebunk, Me.; Louis Rogers, V-P; E. W. Freeman, Clerk, Portland, Me.; L. B.Rogers, Treas. & Gen. Mgr.; E. O. Hallberg, Asst. Treas. Boston, Mass. In 1930 the Leatheroid business of Rogers Fibre Company and its equipment were sold to the Delaware Hard Fibre Company. All usable equipment was removed from the Leatheroid operations in the Island and Dirigo buildings at Kennebunk, thus bringing an end to Leatheroid production in Kennebunk, Maine.\n\nThe name Leatheroid is still used for seal and gasket material on the market today. \n\nThe key patents for the development of leatheroid were:\nThese patents centered on using mineral acids (sulfuric and hydrochloric) to produce the high level of bonding for parchmentized paper and leatheroid as compared to using zinc chloride to produce vulcanized fibre. The basic process for producing parchmentized paper is to pass the paper through an acid bath, press it, wash the paper, neutralize the remaining acids in the paper with a caustic bath of ammonium hydroxide, sodium hydroxide, or other suitable caustic solution, wash the paper again, and then dry the paper in the normal way. This process is used to this day to produce parchmentized paper. However the use of this process to produce the highly bonded fibre board known as leatheroid had problems to be surmounted. Using the cutdown machine to build up multiple layers of paper to various board weights entailed the problem of how to keep the acid from destroying the cellulose chains. The key to the using the parchmentizing process for paper in the production of paperboard seems to center around using other chemical agents in the parchmentizing acid solution to retard or delay the destruction of the cellulose chains until such time as the acid is washed out or neutralized. Examples of such agents are given patent number 198,382 as zinc and dextrin (dextrine). Other organic matters that may substitute for dextrin mentioned in 198382 were crude petroleum, blood, albumen, and paper and pulp. Another technique used to retard or delay the action of the parchmentizing acid solution was to keep the reaction cold. The technique and equipment designed to accomplish it are described patent number 312,945. The importance of keeping the reaction cold was so important that leatheroid manufacturing in Kennebunk was suspended every summer until 1889 when the Leatheroid Manufacturing Company dug a well that provided a source of cold water year round as an alternative to using Mousam River water.\n"}
{"id": "3232180", "url": "https://en.wikipedia.org/wiki?curid=3232180", "title": "List of Lepidoptera that feed on Malus", "text": "List of Lepidoptera that feed on Malus\n\nApples (Malus species) are used as food plants by the caterpillars of a number of Lepidoptera (butterflies and moths). These include:\n\n"}
{"id": "3291776", "url": "https://en.wikipedia.org/wiki?curid=3291776", "title": "List of Lepidoptera that feed on willows", "text": "List of Lepidoptera that feed on willows\n\nWillows, sallows and osiers (\"Salix\" species) are used as food plants by the larvae (caterpillars) of a large number of Lepidoptera species including the following.\n\nSpecies that feed exclusively on \"Salix\".\n\n\nSpecies that feed on \"Salix\" and other plants.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10821804", "url": "https://en.wikipedia.org/wiki?curid=10821804", "title": "List of Tillandsia species", "text": "List of Tillandsia species\n\n"}
{"id": "44716411", "url": "https://en.wikipedia.org/wiki?curid=44716411", "title": "List of mountain passes in Kyrgyzstan", "text": "List of mountain passes in Kyrgyzstan\n\nThis is a list of mountain passes in Kyrgyzstan.\n"}
{"id": "6230216", "url": "https://en.wikipedia.org/wiki?curid=6230216", "title": "List of rivers of Catalonia", "text": "List of rivers of Catalonia\n\nThe rivers of Catalonia can be classified into four groups according to their source.\n\nThe Llobregat deserves a special mention because of its importance: its basin covers most of the province of Barcelona. It source is in the pyreneean Serra del Cadí in the municipality of Castellar de n'Hug (Berguedà) and its valley is one of the main north-south communication routes in Catalonia.\n\nThe Aran Valley is the upper valley of the Garonne (\"la Garona\" in Catalan, \"era Garona\" in Aranese), an Atlantic river which meets the sea near Bordeaux, France. The boundaries of the \"comarca\" do not correspond exactly with the watershed, and both the Noguera Ribagorçana and the Noguera Pallaresa have their sources on its territory.\n\nThe frontier between Catalonia and the Valencian Community is formed for much of its length by the Sénia, while the Noguera Ribagorçana forms the frontier with Aragon for much of its length.\n\nRivers are listed according to their estuaries, from north to south along the Catalan coastline, or according to the point of confluence, from the sea to the source, for rivers which do not flow directly into the sea. Names are given in Catalan with the appropriate definite article: Spanish names are given in parentheses. Catalan names are usually preferred in English language works, except for the Ebre which is usually referred to by its Spanish name \"Ebro\".\n\n\nThe Catalan language has two words for \"river\", \"riu\" and \"riera\". The word \"riera\" is used for smaller rivers and for streams, and often indicates a seasonal river. Of the above list, only the Gavarresa is commonly classified as a \"riera\", all the others being qualified as \"rius\". The grammatical gender of Anoia is feminine, while that of Ebre is masculine.\n\nThe main economic importance of the Catalan rivers is probably the role of their valleys as communication routes, particularly through the Coastal and Prelitteral Ranges. Hence routes inland from Barcelona pass either through the valley of the Llobregat or that of the Besòs, and the Francolí valley is an important route inland from Tarragona.\n\nIrrigation is important in the drier areas of Catalonia, notably in the Central Depression and in the south. The Canal de Aragó i Catalunya and the Canal d'Urgell distribute the waters of the Segre across the \"comarques\" of Noguera and Segrià where it is used for \ngrowing cereals, almonds and olives. Irrigation is also important in the \"comarques\" of Baix Ebre and Montsià, where the cultivation of rice is widespread.\n\nAll of the larger Catalan rivers, with the exception of the Llobregat, have been dammed for hydroelectric power. By far the largest dams are those on the Ebre at Riba-roja and at Flix (Ribera d'Ebre). Other dams include\n\n"}
{"id": "36900802", "url": "https://en.wikipedia.org/wiki?curid=36900802", "title": "List of shipwrecks in the mid-Atlantic Ocean", "text": "List of shipwrecks in the mid-Atlantic Ocean\n\nThis is a list of shipwrecks located in the main body of the Atlantic Ocean, rather than in one of its marginal seas.\n"}
{"id": "2926584", "url": "https://en.wikipedia.org/wiki?curid=2926584", "title": "Malcolm Ogilvie", "text": "Malcolm Ogilvie\n\nDr Malcolm Alexander Ogilvie is a British ornithologist and freelance natural history author and consultant. One of his areas of expertise is wildfowl.\n\nOgilvie was a research scientist with the Wildfowl and Wetlands Trust 1960-1986, also editing their journal, \"Wildfowl\", 1966-1986. Until 1997 he was a member of the \"British Birds\" editorial board and a member of the editorial board of the handbook \"The Birds of the Western Palearctic\". He has been a fully qualified bird ringer since 1958. He is a past regional representative for the British Trust for Ornithology, and the vice-county plant recorder for South Ebudes for the Botanical Society of Britain and Ireland.\n\nOgilvie has been resident on the island of Islay since 1986. He is married to Carol and has two daughters, Isla and Heather.\n\n\nOgilvie's positions have included:\n\n\n\n\n\n"}
{"id": "14147401", "url": "https://en.wikipedia.org/wiki?curid=14147401", "title": "Mesophase", "text": "Mesophase\n\nIn physics, a mesophase is a state of matter intermediate between liquid and solid. Gelatin is a common example of a partially ordered structure in a mesophase. Further, biological structures such as the lipid bilayers of cell membranes are examples of mesophases.\n\nGeorges Friedel (1922) called attention to the \"mesomorphic states of matter\" in his scientific assessment of observations of the so-called liquid crystals. Conventionally a crystal is solid, and crystallization converts liquid to solid. The oxymoron of the liquid crystal is resolved through the notion of mesophases. The observations noted an optic axis persisting in materials that had been melted and had begun to flow. The term \"liquid crystal\" persists as a colloquialism, but use of the term was criticized in 1993: In \"The Physics of Liquid Crystals\" the mesophases are introduced from the beginning:\n\nFurther, \"The classification of mesophases (first clearly set out by G. Friedel in 1922) is essentially based on symmetry.\"\n\nMolecules that demonstrate mesophases are called mesogens.\n\nIn technology, molecules in which the optic axis is subject to manipulation during a mesophase have become commercial products as they can be used to manufacture display devices, known as liquid-crystal displays (LCDs). The susceptibility of the optical axis, called a \"director\", to an electric or magnetic field produces the potential for an optical switch that obscures light or lets it pass. Methods used include the Freedericksz transition and the twisted nematic field effect. From early liquid crystal displays the buying public has embraced the low-power optical switch facility of mesophases with director.\n\nConsider a solid consisting of a single molecular species and subjected to melting. Ultimately it is rendered to an isotropic state classically referred to as liquid. Mesophases occur before then when an intermediate state of order is still maintained as in the nematic, smectic, and columnar phases of liquid crystals. Mesophases thus exhibit anisotropy. LCD devices work as an optical switch which is turned off and on by an electric field applied to the mesogen with director. The response of the director to the field is expressed with viscosity parameters, as in the Ericksen-Leslie theory in continuum mechanics developed by Jerald Ericksen and Frank Matthews Leslie.\n\nMesophase phenomena are important in many scientific fields. The publishing arms of professional societies have academic journals as needed. For instance, the American Chemical Society has both \"Macromolecules\" and \"Langmuir\", while Royal Society of Chemistry has \"Soft Matter\", and American Physical Society has \"Physical Review E\", and Elsevier has \"Advances in Colloid and Interface Science\".\n\n\n\n"}
{"id": "54260933", "url": "https://en.wikipedia.org/wiki?curid=54260933", "title": "Moleanos", "text": "Moleanos\n\nMoleanos is a Portuguese limestone with light beige colored background and a slight grayish tonality, with thin to medium grain and disperse brownish fine spots. With a medium hardness, it’s frequently used for cladding, pavements, interior decor, street furniture and other stoneworks. The main and most popular type is Moleanos Classic, however there several types of this limestone such as Moleanos Blue Light Water, Moleanos Shelly, Moleanos Moon and Moleanos White.\nMoleanos is currently one of the most popular limestones in the world, being used on a large scale a little throughout the world due to its lower price and uniform color. There are several types of Moleanos, which vary depending on where they are extracted. The most popular are, Moleanos White, Moleanos Moon, Moleanos Blue Light Water, and Moleanos Shelly.\n\nThis stone can be used in projects that require large quantities because it is a stone that exists in abundance and it is explored by numerous quarries. On the other hand, it is an extremely versatile stone, can be used in flooring and facing slabs, for interior and exterior pavings, cladding, masonry, stonework, and many others.\n\nThe Moleanos limestone is extracted in Portugal, next to a small town called Moleanos. The city is located in the center of the country and is next to “Serra de Aire e Candeiros”, which is the most important limestone extraction area in Portugal. The quarries of this stone are approximately 1 hour away from the capital of the country, Lisbon.\n\nMoleanos is a very versatile stone, it has many uses. Due to the properties of this stone, with enormous hardness for a limestone and with a uniform beige color, it allows a beautiful look wherever applied. This stone is very used in exterior and interior floors with moderate or intense use. It is also very popular in external and internal claddings, mainly due to its resistance. Other uses like fireplaces, street chairs, bathtubs, balusters and columns, window and door frames, garden decoration and fountains are very popular too, but there are many others.\n"}
{"id": "10189501", "url": "https://en.wikipedia.org/wiki?curid=10189501", "title": "Multifunction platform", "text": "Multifunction platform\n\nThe Multifunction Platform (MFP) is a concept and a structure developed by UNDP and deployed in a number of West African countries, and Tanzania and Zambia. The idea has been to place an MFP in a village which, driven by a diesel engine, powers devices such as pumps and grain mills and generators. The UNDP has produced a number of reports on this project in Mali which can be accessed here. There are few independent analyses of the development impact of the concept of the multifunctional platform, but recently an article in the journal Energy Policy,which can be accessed here, provide a more nuanced view of programme achievements than impact assessments made public at earlier stages of the project.\n\nThe primary impact of the MFP has been on women's work (on reducing daily drudgery and opening up new opportunities in life) and the UNDP's deployment has been to women's organisations, with part local funding and part local grant.\n\nIn places where Jatropha is grown, a device, powered by the MFP, can crush the Jatropha seed. The oil produced is suitable for running the diesel engine, allowing the MFP to produce fuel for its own operation. The company Mali Biocarburant has carried out a pilot with in a number of villages in Mali, as reported here. Also, The Mali Folke Centre has reported on its success in this field.\n\nMore recently, FACT Foundation has implemented a pilot programme in which biogas is used for co-fuelling MFP diesel engines. In five villages in Mali, biogas systems were installed and connected to the MFP diesel engines. As reported here, first results look promising, although further work is required to improve the management of the biogas systems.\n\nVolunteers from Engineers Without Borders have been involved with assessment of some MFP projects, as reported here.\n\nWhereas the UNDP's projects have involved substantial injection of foreign expert advice, a low budget self-initiated approach was in recent times taken in Kiliba, DRC at Farm of Hope, a project of Fondation chirezi. However, that project failed to materialise.\n"}
{"id": "28671504", "url": "https://en.wikipedia.org/wiki?curid=28671504", "title": "Northwest Atlantic Mid-Ocean Channel", "text": "Northwest Atlantic Mid-Ocean Channel\n\nThe Northwest Atlantic Mid-Ocean Channel (NAMOC) is the main body of a turbidity current system of channels and canyons running on the sea bottom from the Hudson Strait, through the Labrador Sea and ending at the Sohm Abyssal Plain in the Atlantic Ocean. Contrary to most other such systems which fan away from the main channel, numerous tributaries run into the NAMOC and end there. The density of those tributaries is the highest near the Labrador Peninsula, but the longest tributary, called Imarssuak Mid-Ocean Channel (IMOC), originates in the Atlantic Ocean. \n\nMost topography data on the NAMOC originate from wide-range sonar scans. With a total length of about , NAMOC is one of the longest underwater channels in the world. It is 100–200 m deep and 2–5 km wide at the channel floor. The rising levees of the NAMOC (about 100 m above the sea bed) often hinder confluence of some tributaries, which instead run along NAMOC for hundreds of km. Its western (right-hand, max. height 250 m) levee rises some 100 m above the eastern one (max. height 150 m). This asymmetry is attributed to the Coriolis effect affecting the turbidity currents, which reach velocities of 6–8.5 m/s and deposit silt and clay over the channel. The levee is absent in some parts of the NAMOC, for example between 56°N and 57°N, due to the local side-flows of sand. \n\nThe meandering of the NAMOC is relatively small compared to other underwater channels, such as Amazon Canyon. It is more developed in the northern part with a period increasing from 25 km between 59°45'N and 56°N to 50 km between 56°N and 54°30'N. The channel becomes on average more straight towards the south, but it still contains abrupt turns due to local seamounts sea bed fractures.\n\n"}
{"id": "42393289", "url": "https://en.wikipedia.org/wiki?curid=42393289", "title": "Ocean Networks Canada", "text": "Ocean Networks Canada\n\nOcean Networks Canada is a University of Victoria initiative that operates the NEPTUNE and VENUS cabled ocean observatories in the northeast Pacific Ocean and the Salish Sea. Additionally, Ocean Networks Canada operates smaller community-based observatories offshore from Cambridge Bay, Nunavut., Campbell River, Kitamaat Village and Digby Island. These observatories collect data on physical, chemical, biological, and geological aspects of the ocean over long time periods. As with other ocean observatories such as ESONET, Ocean Observatories Initiative, MACHO and DONET, scientific instruments connected to Ocean Networks Canada are operated remotely and provide continuous streams of freely available data to researchers and the public. Over 200 gigabytes of data are collected every day.\n\nThe VENUS Observatory is situated at three main sites in the Salish Sea, including Saanich Inlet (depth 100 m), the eastern and central Strait of Georgia (depths 170–300 m), and the Fraser River delta.\n\nThe NEPTUNE observatory is situated off the west coast of Vancouver Island in Barkley Sound, along the Cascadia subduction zone, on the Cascadia Basin abyssal plain, and on the Endeavour segment of the Juan de Fuca Ridge.\n\nAltogether, the system includes 3 observatories, 5 shore stations, 850+ km of seafloor backbone cables, 11 instrumented sites, 32 instrument platforms, 6 mobile instrument platforms, 400+ instruments and over 2000 scientific sensors deployed.\n\nScientific topics of study that are enabled by data from these observatories include Arctic oceanography, deep-sea biodiversity, marine ecosystem function, marine forensics, gas hydrates, hydrothermal vents, marine mammals, sediment and benthic dynamics and tsunami studies.\n\nOcean Networks Canada instrumentations are installed in the following seafloor locations:\n\nSaanich Inlet, at the southern end of Vancouver Island, Canada, is a naturally hypoxic basin. A shallow sill (70 m) at the mouth isolates the deep basin (215 m) that experiences seasonal deep-water anoxia as a result of high primary productivity and subsequent degradation of sedimented organic matter. The Saanich Inlet network consists of cabled arrays of instruments in Mill Bay, Patricia Bay and an autonomous mooring at the entrance (sill) to the inlet. Two cabled surface buoys are connected to the Patricia Bay installation, supporting a technology testbed facility (Patricia Bay) and a full water column observation station (Coles Bay). Patricia Bay installations include:\n\nIn Mill Bay, a mini-observatory is installed at Brentwood College with basic sensors that measure water properties at 8 m depth.\n\nA network with three sites on seafloor at Central, East and Fraser Delta locations of the southern Strait of Georgia, and surface-based systems on BC Ferries, Iona Causeway, and Coal Port terminal. Installations in the Strait of Georgia include:\n\nBarkley Canyon extends from the continental shelf edge at 400 m down the continental slope to the canyon axis at 985 m water depth. Located at the leading edge of the Cascadia subduction zone, this site supports the study of the accretionary prism, where the sediments pile along the continental slope as they are scraped off the subducting or descending tectonic plate. This is also a location where pressure, temperature, gas saturation, and local biological and chemical conditions are just right for exposed gas hydrates to be stable on the seafloor. Gas hydrates have gas molecules, typically methane in marine environments, trapped within “cages” of water molecules. This gives them a crystalline structure that resembles ice and can appear as white to yellow mounds covered by sediment on the seafloor.\n\nThe region is influenced by a major ocean current system. Off the coast, the west wind drift current splits to create the Alaska and the California currents (the California current system). The direction and strength of the currents regulate the upwelling/downwelling regime along the coast, with a flow towards the equator in summer (California current) and reversal in winter (Alaska current). In addition to these two currents, a large submarine canyon acts as a primary conduit for the transfer of sediment from the continental slope to the deep-sea.\n\nOrganisms living in the depths of Barkley Canyon have evolved to be able to persist in areas with high pressure, no light, and low nutrients/food availability. The Barkley Canyon instruments span a diversity of habitats, each of which is associated with its own specialized biological community. Most of the areas within Barkley Canon are characterised by a soft, finely sedimented seafloor. Video observations suggest that animal densities are higher at the shallower sites compared with the deeper sites, although most species are present at all sites. A number of fish including sablefish, thornyheads, rockfish, flatfish, sharks, skates, hagfish and eelpouts have been observed throughout Barkley Canyon. The area is rich with invertebrates as well including molluscs (bivalves, octopus, snails), echinoderms (sea stars, brittle stars, sea cucumbers, and urchins) as well as arthropods (crabs and shrimp). Cnidarians are seen both on the seafloor (anemones, sea pens, and corals) as well as floating in the water column (jellyfish) along with other organisms such as salps, ctenophores, and tunicates.\n\nWhere gas hydrates are observed on the seafloor, there are mats of chemosynthetic bacteria which derive their energy from hydrogen sulphide produced by the oxidate of methane by a second group of microbes living deep within the sediments. Other chemosynthetic sulphide-oxidizing bacteria live in symbiosis with clams who live at these hydrate mounts. Many of the species observed elsewhere in Barkley Canyon are not dependent on this system but are frequently observed in the area.\n\nThe Cascadia Basin is the heavily sediment part of the Juan de Fuca Plate that extends from the base the continental margin to the west where the sediments lap on to the Juan de Fuca Ridge flank. The Juan de Fuca Plate is one of the last remnants of the Farallon Plate, the original eastern Pacific oceanic plate, which has been almost entirely subducted beneath North America. The flat sediment surface constitutes an abyssal plain, an exceedingly vast environment that covers over 50% of the planet’s surface. Seemingly inhospitable, with temperatures below 2 °C, high-pressures, and a total absence of light, the Cascadia Basin is nevertheless home to an assortment of well-adapted organisms.\n\nDepending mostly on marine snow—the continuous shower of mostly organic detritus falling from surface waters—little is known about the complicated food web connecting the organisms of the deep. Because of the harsh environment, there is a low density of organisms observed. Despite the low density, there is a fairly diverse community living on the abyssal plain. Installation and maintenance work has enabled a number of these organisms to be observed. The main groups of organisms observed include fish (skates and rattails), echinoderms (sea cucumbers, sea stars, brittle stars, and crinoids), molluscs (octopus and squid), sea pens, crabs, and squat lobsters. A number of pelagic (living in the water column) organisms were observed such as squid, jellyfish, ostracods, ctenophores, and salps.\n\nA few isolated outcropping seamounts that pierce through more than 200 m of impermeable sediments, are conduits that allow hydrologic exchange between the open ocean and the upper oceanic crust—the largest aquifer of the planet. The Cascadia Basin site is placed in the vicinity of several circulation obviation retrofit kit (CORK) borehole observatories, which are designed to study the hydrology, geochemistry and microbiology of the upper oceanic crust. CORKs are also being used to investigate changes in regional plate strain that are caused by earthquakes on the plate boundaries. The seafloor pressure measurements of the CORK borehole observatories constitute the center of a “tsunami-meter”, a network of several high precision, rapid sampling bottom pressure recorders (BPRs), that allows precise determination of deep water tsunami amplitude, direction of propagation, and speed.\n\nThe name Clayoquot (pronounced “Clah-quot”) is an anglicized version of Tla-o-qui-aht, the largest nation in the Nuu-chah-nulth (Nootka) First Nations, whose people have resided in the Clayoquot Sound region near Tofino and Ucluelet for at least the last 2000 years. The Clayoquot Slope site lies about 1250 m below sea level and approximately 20 km landward of the toe of the Cascadia subduction zone. The Cascadia subduction zone is the area at which the Juan de Fuca plate is subducting (descending) beneath the North American plate. This is a zone where much of the thick layer of sediments deposited on the eastern flank of the Juan de Fuca Ridge are scraped off and accreted as the tectonic plates converge (move together). As sediments thicken and compact from accretion, pore waters are expelled from the sediment, and gases — primarily biogenic methane — contribute to the formation of gas hydrates in the upper few hundred metres of the sediment. At this site, a cold vent, known as Bullseye Vent, has formed along with significant concentrations of gas hydrates.\n\nClayoquot Slope is home to a variety of deep-sea organisms. Many demersal fish (fish which live very near the bottom) were observed (rockfish, flatfish, throny heads, and rattails) along with echinoderms (sea cucumbers, brittle stars, sea stars), octopus, crabs, cnidarians (sea pens, corals, anemones), and bacterial mats. In the water column, organisms such as squid, krill, jellyfish, siphonophores, and larvaceans have been observed during installation and maintenance work.\n\nEndeavour (depth 2200–2400 m) is a northern segment of the Juan de Fuca Ridge which, in turn, is part of the complex, 80,000 km long mid-ocean ridge system spanning the World Ocean. Juan de Fuca Ridge is a medium rate spreading centre (~6 cm/yr) forming the divergent boundary between the Pacific (to the west) and the Juan de Fuca (to the east) tectonic plates . At these divergent boundaries, convection currents in the mantle rise up as magma, emerge through the rifts as lava, and crystallise as new rock (basalts and gabbro). These processes continually create new ocean crust. Hydrothermal vents, which typically form along these mid-ocean ridges, are fissures from which geothermally heated water flows. The water flowing out of vents is dominantly seawater drawn into the system through the faults, porous sediments, and volcanic rocks. As the cool seawater moves through the sediment and rock towards the hot magma, the water becomes super heated (300-400 °C) and rich in dissolved mineral elements (such as sulphur, iron, zinc and copper) from the young ocean crust. When the hot effluent is expelled, it encounters the cold, ambient seawater (about 2 °C) minerals precipitate from the element-rich vent water. At the Endeavour segment, a notably vigorous venting area, black smokers form at the high temperature vents,where the effluent is precipitating iron sulphides.This gives the plumeits dark colour and deposits sulphide-minerals, creating chimneys up to 30 m in height. There are 6 known vent fields with distinct morphologies spaced about 2 km apart along the axial rift valley of the segment.\n\nThese tall sulphide chimneys (hydrothermal vents) host some unique ecological communities. While most of the deep sea depends on near-surface productivity with photosynthesis as its fundamental energy source, vent communities are completely independent from the surface and sunlight. Bacteria are able to use reduced compounds from the vent effluent as an energy source (chemosynthesis). These bacteria can be free-living or symbiotic and are the base of the food web of these communities where 90% of the species are endemic to this special environment. The tubeworm Ridgeia piscesae grows in large colonies in diffuse venting areas, supported by the symbiotic chemosynthetic bacteria developing in their cells. These worms have no mouth and rely on their internal symbiotic bacteria to survive. Other species living within hydrothermal communities include limpets, worms (scale worms and sulphide worms), fish, and sea spiders.\n\nFolger Passage is located at the mouth of Barkley Sound, offshore Vancouver Island near Bamfield, British Columbia. The seafloor composition includes cobble, gravel, soft sandy sediment, and carbonate-rich detritus. Two instrument platforms, Folger Deep (100 m) and Folger Pinnacle (23 m), are installed at Folger Passage. Folger Deep is situated on soft sediment at the mouth of an inlet channel while the Folger Pinnacle platform is secured to the top of a rocky reef within a rockfish conservation area.\n\nThis coastal zone is ideal for studies of land-ocean interactions and coastal physical oceanography. Estuarine circulation from Barkley Sound is influenced by the shelf dynamics of an eastern boundary current, creating a complex physical environment. Surface outflow drives a deep water inflow which is strongly influenced by upwelling and downwelling conditions on the nearby continental shelf. The nutrient-rich, terrestrial freshwater discharge and the nutrient-rich, cool, salty upwelled water support a diverse and abundant ecosystem\n\nFolger Pinnacle, located atop a shallow reef, has dense mats of sponges, ascidians and encrusting algae. There are numerous types of sessile (bottom attached) organisms including sponges, anemones, bryozoans, tunicates, and barnacles. Since this is a rockfish conservation area, there is a wide variety of rockfish (yellowtail, China, quillback, Puget Sound, black, and blue) in addition to many other fish (kelp greenling, lingcod, flatfish, wolfeels), molluscs (giant Pacific octopus, mussles, swimming scallops, and snails), and echinoderms (seastars, sea cucumbers, and urchins). An echosounder installed at Folger Deep shows evidence of a dense zooplankton community and schools of fish in the water column, while hydrophones regularly record the songs of whales and dolphins in the area.\n\n\n"}
{"id": "50566309", "url": "https://en.wikipedia.org/wiki?curid=50566309", "title": "Oil pollution toxicity to marine fish", "text": "Oil pollution toxicity to marine fish\n\nOil pollution toxicity to marine fish has been observed from oil spills such as the \"Exxon Valdez\" disaster, and from nonpoint sources, such as surface runoff, which is the largest source of oil pollution in marine waters.\n\nCrude oil entering waterways from spills or runoff contain polycyclic aromatic hydrocarbons (PAHs), the most toxic components of oil. The route of PAH uptake into fish depends on many environmental factors and the properties of the PAH. The common routes are ingestion, ventilation of the gills, and dermal uptake. Fish exposed to these PAHs exhibit an array of toxic effects including genetic damage, morphological deformities, altered growth and development, decreased body size, inhibited swimming abilities and mortality. The morphological deformities of PAH exposure, such as fin and jaw malformations, result in significantly reduced survival in fish due to the reduction of swimming and feeding abilities. While the exact mechanism of PAH toxicity is unknown, there are four proposed mechanisms. The difficulty in finding a specific toxic mechanism is largely due to the wide variety of PAH compounds with differing properties.\n\nResearch on the environmental impact of the petroleum industry began in earnest, during the mid to late 20th century, as the oil industry developed and expanded. Large scale transport of crude oil increased as a result of the increasing worldwide demand for oil, subsequently increasing the number of oil spills. Oil spills provided perfect opportunities for scientists to examine the in situ effects of crude oil exposure to marine ecosystems, and collaborative efforts between the National Oceanic and Atmospheric Administration (NOAA) and the United States Coast Guard resulted in improved response efforts and detailed research on oil pollution's effects. The Exxon Valdez oil spill in 1989, and the Deepwater Horizon oil spill in 2010, both resulted in increased scientific knowledge on the specific effects of oil pollution toxicity to marine fish.\n\nFocused research on oil pollution toxicity to fish began in earnest in 1989, after the \"Exxon Valdez\" tanker struck a reef in Prince William Sound, Alaska and spilled approximately 11 million gallons of crude oil into the surrounding water. At the time, the Exxon Valdez oil spill was the largest in the history of the United States. There were many adverse ecological impacts of the spill including the loss of the loss of billions of Pacific herring and pink salmon eggs. Pacific herring were just beginning to spawn in late March when the spill occurred, resulting in nearly half of the populations eggs being exposed to crude oil. Pacific herring spawn in the intertidal and subtidal zones, making the vulnerable eggs easily exposed to pollution.\n\nAfter April 20, 2010, when an explosion on the \"Deepwater Horizon\" Macondo oil drilling platform triggered the largest oil spill in US history, another opportunity for oil toxicity research was presented. Approximately 171 million gallons of crude oil flowed from the seafloor into the Gulf of Mexico, exposing the majority of surrounding biota. The Deepwater Horizon oil spill also coincided directly with spawning window of various ecologically and commercially important fish species, including yellowfin and Atlantic bluefin tuna. The oil spill directly affected Atlantic bluefin tuna, as approximately 12% of larval tuna were located in oil contaminated waters, and Gulf of Mexico is the only known spawning grounds for the western population of bluefin tuna.\n\nOil spills, as well daily oil runoff from urbanized areas can lead to polycyclic aromatic hydrocarbon (PAHs) entering marine ecosystems. Once PAHs enter the marine environment, fish can be exposed to them via ingestion, ventilation of the gills, and dermal uptake. The major route of uptake will depend on the behavior of the species of fish and the physicochemical properties of the PAH of concern. Habitat can be a major deciding factor for the route of exposure. For example, demersal fish or fish that consume demersal fish are highly likely to ingest PAHs that have sorbed to the sediment, whereas fish that swim at the surface are at a higher risk for dermal exposure. Upon coming in contact with a PAH, bioavailability will effect how readily the PAH is taken up. The EPA identifies 16 major PAHs of concern and each of these PAHs has a different degree of bioavailability. For instance, PAHs with lower molecular weight are more bioavailable because they dissolve more readily in water and are therefore more bioavailable for fish within the water column. Similarly, hydrophilic PAHs are more bioavailable for uptake by fish. For this reason, usage of oil dispersants, like Corexit, to treat oil spills can increase the uptake of PAHs by increasing their solubility in water and making them more available for uptake via the gills. Once a PAH is taken up, the fish's metabolism can affect the duration and intensity of the exposure to target tissues. Fish are able to readily metabolize 99% of PAHs to a more hydrophilic metabolite through their hepato-bilary system. This allows for the excretion of PAHs. The rate of metabolism of PAHs will depend on the sex and size of the species. The ability to metabolize PAHs into a more hydrophillic form can prevent bioaccumulation and halt PAHs from being passed on to organisms further up the food web. Because oil can persist in the environment long after oil spills via sedimentation, demersal fish are likely to be continually exposed to PAHs many years after oil spills. This has been proven by looking at the biliary PAH metabolites of bottom dwelling fish. For instance, bottom dwelling fish still showed elevated levels of low molecular weight PAH metabolites 10 years after \"Exxon Valdez\" oil spill.\n\nCrude oil is composed of more than 17,000 compounds. Among these 17,000 compounds are PAHs, which are considered the most toxic components of oil. PAHs are formed by pyrogenic and petrogenic processes. Petrogenic PAHs are formed by the elevated pressure of organic material. In contrast, pyrogenic PAHs are formed through the incomplete combustion of organic material. Crude oil naturally contains petrogenic PAHs and these PAH levels are increased significantly through the burning of oil which creates pyrogenic PAHs. The level of PAHs found in crude oil differs with the type of crude oil. For example, crude oil from the \"Exxon Valdez\" oil spill had PAH concentrations of 1.47%, while PAH concentrations from the North Sea have much lower PAH concentrations of 0.83%.\n\nCrude oil contamination in marine ecosystems can lead to both pyrogenic and petrogenic PAHs entering these ecosystems. Petrogenic PAHs can enter waterways through oil seeps, major oil spills, creosote and fuel oil runoff from urban areas. Pyrogenic PAH sources consist of diesel soot tire rubber and coal dust. Although there are natural sources of PAHs such as volcanic activity and seepage of coal deposits, anthropogenic sources pose the most significant input of PAHs into the environment. These anthroprogenic sources include residential heating, asphalt production, coal gasification, and petroleum usage. Petrogenic PAH contamination is more common from crude oil spills such as \"Exxon Valdez\", or oil seeps; however, with runoff pyrogenic PAHs can also be prevalent. Although major oil spills such as \"Exxon Valdez\" can introduce a large amount of crude oil to a localized area in a short time span, daily runoff comprises most of the oil pollution to marine ecosystems. Atmospheric deposition can also be a source of PAHs into marine ecosystems. The deposition of PAHs from the atmosphere into a water body is largely influenced by the gas-particle partitioning of the PAH.\n\nMany effects of PAH exposure have been observed in marine fish. Specifically, studies have been conducted on the embryonic and larval fish, the development of fish exposed to PAHs, and uptake of PAHs by fish via various routes of exposure. One study on found that Pacific herring eggs exposed to conditions mimicking the ‘’Exxon Valdez’’ oil spill resulted in premature hatching of eggs, reduced size as fish matured and significant teratogenic effects, including skeletal, cardiovascular, fin and yolk sac malformations. Yolk sac edema was responsible for the majority of herring larval mortality. The teratogenic malformations in the dorsal fin and spine, and in the jaw were observed to effectively decrease the survival of developing fish, through the impairing of swimming and feeding ability respectively. Feeding and prey avoidance via swimming are crucial for the survival of larval and juvenile fish. All effects observed in herring eggs in the study were consistent with effects observed in exposed fish eggs following the \"Exxon Valdez\" oil spill. Zebrafish embryos exposed to oil were observed to have severe teratogenic defects similar to those seen in herring embryos, including edema, cardiac dysfunction, and intracranial hemorrhages. In a study focused on the uptake of PAHs by fish, salmon embryos were exposed to crude oil in three various situations, including via effluent from oil coated gravel. PAH concentrations in embryos directly exposed to oil and those exposed to PAH effluent were not significantly different. PAH exposure was observed to lead to death, even when the PAHs were exposed to fish via effluent. From the results, it was determined that fish embryos near the \"Exxon Valdez\" spill in Prince William Sound that were not directly in contact with oil still may have accumulated lethal levels of PAHs. While many laboratory and natural studies have observed significant adverse effects of PAH exposure to fish, a lack of effects has also been observed for certain PAH compounds, which could be due to a lack of uptake during exposure to the compound.\n\nWhile it has been proven that different classes of PAHs act through distinct toxic mechanisms due to the variations in their molecular weight, ring arrangements, and water solubility properties, the specific mechanisms of PAH toxicity to fish and fish development are still unknown. Toxicity depends on the extent to which chemical in the oil will mix with water: this is referred to as the water associated fraction of the oil. The proposed mechanisms of toxicity of PAHs are toxicity through narcosis, interaction with the AhR pathway, alkyl phenanthrene toxicity, and additive toxicity by multiple mechanisms.\n\n\n"}
{"id": "53497", "url": "https://en.wikipedia.org/wiki?curid=53497", "title": "Optical illusion", "text": "Optical illusion\n\nAn optical illusion (also called a visual illusion)\nis an illusion caused by the visual system and characterized by a visual percept that (loosely said) appears to differ from reality. Illusions come in a wide variety; their categorization is difficult because the underlying cause is often not clear but a classification proposed by Richard Gregory is useful as an orientation. According to that, there are three main classes: physical, physiological, and cognitive illusions, and in each class there are four kinds: Ambiguities, distortions, paradoxes, and fictions. A classical example for a physical distortion would be the apparent bending of a stick half immerged in water; an example for a physiological paradox is the motion aftereffect (where despite movement position remains unchanged). An example for a physiological fiction is an afterimage. Three typical cognitive distortions are the Ponzo, Poggendorff, and Müller-Lyer illusion. Physical illusions are caused by the physical environment, e.g. by the optical properties of water. Physiological illusions arise in the eye or the visual pathway, e.g. from the effects of excessive stimulation of a specific receptor type. Cognitive visual illusions are the result of unconscious inferences and are perhaps those most widely known.\n\nPathological visual illusions arise from pathological changes in the physiological visual perception mechanisms causing the aforementioned types of illusions; they are discussed e.g. under visual hallucinations.\n\nA familiar phenomenon and example for a physical visual illusion is when mountains appear to be much nearer in clear weather with low humidity (Foehn) than they are. This is because haze is a cue for depth perception for far-away objects (Aerial perspective).\n\nThe classical example of a physical illusion is when a stick that is half immerged in water appears bent. This phenomenon has already been discussed by Ptolemy (ca. 150) and was often a prototypical example for an illusion.\n\nPhysiological illusions, such as the afterimages following bright lights, or adapting stimuli of excessively longer alternating patterns (contingent perceptual aftereffect), are presumed to be the effects on the eyes or brain of excessive stimulation or interaction with contextual or competing stimuli of a specific type—brightness, color, position, tile, size, movement, etc. The theory is that a stimulus follows its individual dedicated neural path in the early stages of visual processing, and that intense or repetitive activity in that or interaction with active adjoining channels cause a physiological imbalance that alters perception.\n\nThe Hermann grid illusion and Mach bands are two illusions that are best explained using a biological approach. Lateral inhibition, where in the receptive field of the retina light and dark receptors compete with one another to become active, has been used to explain why we see bands of increased brightness at the edge of a color difference when viewing Mach bands. Once a receptor is active, it inhibits adjacent receptors. This inhibition creates contrast, highlighting edges. In the Hermann grid illusion the gray spots appear at the intersection because of the inhibitory response which occurs as a result of the increased dark surround. Lateral inhibition has also been used to explain the Hermann grid illusion, but this has been disproved.\n\nMore recent empirical approaches to optical illusions have had some success in explaining optical phenomena with which theories based on lateral inhibition have struggled.\n\nCognitive illusions are assumed to arise by interaction with assumptions about the world, leading to \"unconscious inferences\", an idea first suggested in the 19th century by the German physicist and physician Hermann Helmholtz. Cognitive illusions are commonly divided into ambiguous illusions, distorting illusions, paradox illusions, or fiction illusions.\n\nTo make sense of the world it is necessary to organize incoming sensations into information which is meaningful. Gestalt psychologists believe one way this is done is by perceiving individual sensory stimuli as a meaningful whole. Gestalt organization can be used to explain many illusions including the rabbit–duck illusion where the image as a whole switches back and forth from being a duck then being a rabbit and why in the figure–ground illusion the figure and ground are reversible.\nIn addition, Gestalt theory can be used to explain the illusory contours in the Kanizsa's Triangle. A floating white triangle, which does not exist, is seen. The brain has a need to see familiar simple objects and has a tendency to create a \"whole\" image from individual elements. Gestalt means \"form\" or \"shape\" in German. However, another explanation of the Kanizsa's Triangle is based in evolutionary psychology and the fact that in order to survive it was important to see form and edges. The use of perceptual organization to create meaning out of stimuli is the principle behind other well-known illusions including impossible objects. Our brain makes sense of shapes and symbols putting them together like a jigsaw puzzle, formulating that which isn't there to that which is believable.\n\nThe Gestalt principles of perception govern the way we group different objects. Good form is where the perceptual system tries to fill in the blanks in order to see simple objects rather than complex objects. Continuity is where the perceptual system tries to disambiguate which segments fit together into continuous lines. Proximity is where objects that are close together are associated. Similarity is where objects that are similar are seen as associated. Some of these elements have been successfully incorporated into quantitative models involving optimal estimation or Bayesian inference.\nThe double-anchoring theory, a popular but recent theory of lightness illusions, states that any region belongs to one or more frameworks, created by Gestalt grouping principles, and within each framework is independently anchored to both the highest luminance and the surround luminance. A spot's lightness is determined by the average of the values computed in each framework.\n\nIllusions can be based on an individual's ability to see in three dimensions even though the image hitting the retina is only two dimensional. The Ponzo illusion is an example of an illusion which uses monocular cues of depth perception to fool the eye. But even with two dimensional images, the brain exaggerates vertical distances when compared with horizontal distances, as in the vertical-horizontal illusion where the two lines are exactly the same length.\n\nIn the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away therefore the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting \"Waterfall\" exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.\n\nThe ambiguity of direction of motion due to lack of visual references for depth is shown in the spinning dancer illusion. The spinning dancer appears to be moving clockwise or counterclockwise depending on spontaneous activity in the brain where perception is subjective. Recent studies show on the fMRI that there are spontaneous fluctuations in cortical activity while watching this illusion, particularly the parietal lobe, because it is involved in perceiving movement.\n\nPerceptual constancies are sources of illusions. Color constancy and brightness constancy are responsible for the fact that a familiar object will appear the same color regardless of the amount of light or color of light reflecting from it. An illusion of color difference or luminosity difference can be created when the luminosity or color of the area surrounding an unfamiliar object is changed. The luminosity of the object will appear brighter against a black field (that reflects less light) compared to a white field, even though the object itself did not change in luminosity. Similarly, the eye will compensate for color contrast depending on the color cast of the surrounding area.\n\nIn addition to the Gestalt principles of perception, water-color illusions contribute to the formation of optical illusions. Water-color illusions consist of object-hole effects and coloration. Object-hole effects occur when boundaries are prominent where there is a figure and background with a hole that is 3D volumetric in appearance. Coloration consists of an assimilation of color radiating from a thin-colored edge lining a darker chromatic contour. The water-color illusion describes how the human mind perceives the wholeness of an object such as top-down processing. Thus, contextual factors play into perceiving the brightness of an object.\n\nJust as it perceives color and brightness constancies, the brain has the ability to understand familiar objects as having a consistent shape or size. For example, a door is perceived as rectangle regardless of how the image may change on the retina as the door is opened and closed. Unfamiliar objects, however, do not always follow the rules of shape constancy and may change when the perspective is changed. The Shepard illusion of the changing table is an example of an illusion based on distortions in shape constancy.\n\nResearcher Mark Changizi of Rensselaer Polytechnic Institute in New York has a more imaginative take on optical illusions, saying that they are due to a neural lag which most humans experience while awake. When light hits the retina, about one-tenth of a second goes by before the brain translates the signal into a visual perception of the world. Scientists have known of the lag, yet they have debated how humans compensate, with some proposing that our motor system somehow modifies our movements to offset the delay.\n\nChangizi asserts that the human visual system has evolved to compensate for neural delays by generating images of what will occur one-tenth of a second into the future. This foresight enables humans to react to events in the present, enabling humans to perform reflexive acts like catching a fly ball and to maneuver smoothly through a crowd. In an interview with ABC Changizi said, \"Illusions occur when our brains attempt to perceive the future, and those perceptions don't match reality.\" For example, an illusion called the Hering illusion looks like bicycle spokes around a central point, with vertical lines on either side of this central, so-called vanishing point.\nThe illusion tricks us into thinking we are looking at a perspective picture, and thus according to Changizi, switches on our future-seeing abilities. Since we aren't actually moving and the figure is static, we misperceive the straight lines as curved ones.\nChangizi said:\nEvolution has seen to it that geometric drawings like this elicit in us premonitions of the near future. The converging lines toward a vanishing point (the spokes) are cues that trick our brains into thinking we are moving forward—as we would in the real world, where the door frame (a pair of vertical lines) seems to bow out as we move through it—and we try to perceive what that world will look like in the next instant.\n\nA pathological visual illusion is a distortion of a real external stimulus and are often diffuse and persistent. Pathological visual illusions usually occur throughout the visual field, suggesting global excitability or sensitivity alterations. Alternatively visual hallucination is the perception of an external visual stimulus where none exists. Visual hallucinations are often from focal dysfunction and are usually transient.\n\nTypes of visual illusions include oscillopsia, halos around objects, illusory palinopsia (visual trailing, light streaking, prolonged indistinct afterimages), akinetopsia, visual snow, micropsia, macropsia, teleopsia, pelopsia, Alice in Wonderland syndrome, metamorphopsia, dyschromatopsia, intense glare, blue field entoptic phenomenon, and purkinje trees.\n\nThese symptoms may indicate an underlying disease state and necessitate seeing a medical practitioner. Etiologies associated with pathological visual illusions include multiple types of ocular disease, migraines, hallucinogen persisting perception disorder, head trauma, and prescription drugs. If a medical work-up does not reveal a cause of the pathological visual illusions, the idiopathic visual disturbances could be analogous to the altered excitability state seen in visual aura with no migraine headache. If the visual illusions are diffuse and persistent, they often affect the patient's quality of life. These symptoms are often refractory to treatment and may be caused by any of the aforementioned etiologes, but are often idiopathic. There is no standard treatment for these visual disturbances.\n\nThere are a variety of different types of optical illusions. Many are included in the following list.\nArtists who have worked with optical illusions include M. C. Escher, Bridget Riley, Salvador Dalí, Giuseppe Arcimboldo, Patrick Bokanowski, Marcel Duchamp, Jasper Johns, Oscar Reutersvärd, Victor Vasarely and Charles Allan Gilbert. Contemporary artists who have experimented with illusions include Jonty Hurwitz, Sandro del Prete, Octavio Ocampo, Dick Termes, Shigeo Fukuda, Patrick Hughes, István Orosz, Rob Gonsalves, Gianni A. Sarcone, Ben Heine and Akiyoshi Kitaoka. Optical illusion is also used in film by the technique of forced perspective.\n\nOp art is a style of art that uses optical illusions to create an impression of movement, or hidden images and patterns. \"Trompe-l'œil\"\nuses realistic imagery to create the optical illusion that depicted objects exist in three dimensions.\n\nThe hypothesis claims that visual illusions occur because the neural circuitry in our visual system evolves, by neural learning, to a system that makes very efficient interpretations of usual 3D scenes based in the emergence of simplified models in our brain that speed up the interpretation process but give rise to optical illusions in unusual situations. In this sense, the cognitive processes hypothesis can be considered a framework for an understanding of optical illusions as the signature of the empirical statistical way vision has evolved to solve the inverse problem.\n\nResearch indicates that 3D vision capabilities emerge and are learned jointly with the planning of movements. After a long process of learning, an internal representation of the world emerges that is well-adjusted to the perceived data coming from closer objects. The representation of distant objects near the horizon is less \"adequate\". In fact, it is not only the Moon that seems larger when we perceive it near the horizon. In a photo of a distant scene, all distant objects are perceived as smaller than when we observe them directly using our vision.\n\nThe retinal image is the main source driving vision but what we see is a \"virtual\" 3D representation of the scene in front of us. We don't see a physical image of the world; we see objects, and the physical world is not itself separated into objects. We see it according to the way our brain organizes it. The names, colours, usual shapes and other information about the things we see pop up instantaneously from our neural circuitry and influence the representation of the scene. We \"see\" the most relevant information about the elements of the best 3D image that our neural networks can produce. The illusions arise when the \"judgments\" implied in the unconscious analysis of the scene are in conflict with reasoned considerations about it.\n\n\n\n"}
{"id": "48065", "url": "https://en.wikipedia.org/wiki?curid=48065", "title": "Polder", "text": "Polder\n\nA polder () is a low-lying tract of land enclosed by dikes that form an artificial hydrological entity, meaning it has no connection with outside water other than through manually operated devices. There are three types of polder:\n\nThe ground level in drained marshes subsides over time. All polders will eventually be below the surrounding water level some or all of the time. Water enters the low-lying polder through infiltration and water pressure of ground water, or rainfall, or transport of water by rivers and canals. This usually means that the polder has an excess of water, which is pumped out or drained by opening sluices at low tide. Care must be taken not to set the internal water level too low. Polder land made up of peat (former marshland) will sink in relation to its previous level, because of peat decomposing when exposed to oxygen from the air. \nPolders are at risk from flooding at all times, and care must be taken to protect the surrounding dikes. Dikes are typically built with locally available materials, and each material has its own risks: sand is prone to collapse owing to saturation by water; dry peat is lighter than water and potentially unable to retain water in very dry seasons. Some animals dig tunnels in the barrier, allowing water to infiltrate the structure; the muskrat is known for this activity and hunted in certain European countries because of it. Polders are most commonly, though not exclusively, found in river deltas, former fenlands and coastal areas.\n\nFlooding of polders has also been used as a military tactic in the past. One example is the flooding of the polders along the Yser river during World War I. Opening the sluices at high tide and closing them at low tide turned the polders into an inaccessible swamp which allowed the Allied armies to stop the German army.\n\nFrom Dutch \"polder\" (\"polder\"), from Middle Dutch \"polre\", from Old Dutch \"polra\", ultimately from \"pol-\" \"part of land, elevated above its surroundings\"; with augmentative suffix \"-er\" and epenthetical \" -d-\".\n\nThe Netherlands is frequently associated with polders, as its engineers became noted for developing techniques to drain wetlands and make them usable for agriculture and other development. This is illustrated by the saying: \"God created the world, but the Dutch created the Netherlands\". \n\nThe Dutch have a long history of reclamation of marshes and fenland, resulting in some 3,000 \"polders\" nationwide. By 1961 , about half of the country's land, was reclaimed from the sea. About half the total surface area of \"polders\" in north-west Europe is in the Netherlands. The first embankments in Europe were constructed in Roman times. The first polders were constructed in the 11th century. \n\nAs a result of flooding disasters, water boards called \"waterschap\" (when situated more inland) or \"hoogheemraadschap\" (near the sea, mainly used in the Holland region) were set up to maintain the integrity of the water defences around polders, maintain the waterways inside a polder, and control the various water levels inside and outside the polder. Water boards hold separate elections, levy taxes, and function independently from other government bodies. Their function is basically unchanged even today. As such they are the oldest democratic institution in the country. The necessary cooperation among all ranks to maintain polder integrity gave its name to the Dutch version of third way politics—the \"Polder Model\".\n\nThe 1953 flood disaster prompted a new approach to the design of dikes and other water-retaining structures, based on an acceptable probability of overflowing. Risk is defined as the product of probability and consequences. The potential damage in lives, property and rebuilding costs is compared to the potential cost of water defences. From these calculations follows an acceptable flood risk from the sea at one in 4,000–10,000 years, while it is one in 100–2,500 years for a river flood. The particular established policy guides the Dutch government to improve flood defences as new data on threat levels becomes available. \n\nSome famous Dutch polders and the year they were laid dry are:\n\nBangladesh has 123 polders, of which 49 are sea-facing. These were constructed in the 1960s to protect the coast from tidal flooding and reduce salinity incursion. They reduce long-term flooding and waterlogging following storm surges from tropical cyclones. They are also cultivated for agriculture.\n\n\n\n\n\n\nIn Germany, land reclaimed by dyking is called a \"koog\". The German \"Deichgraf\" system was similar to the Dutch and is widely known from Theodor Storm's novella \"The Rider on the White Horse\".\n\n\nIn southern Germany, the term \"polder\" is used for retention basins recreated by opening dikes during river floodplain restoration, a meaning somewhat opposite to that in coastal context.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1227392", "url": "https://en.wikipedia.org/wiki?curid=1227392", "title": "Protected areas of Romania", "text": "Protected areas of Romania\n\nThis is a list of protected areas of Romania.\n\nAbout 5.18% of the area of Romania has a protected status (12,360 km²), including the Danube Delta, which makes half of these areas (2.43% of Romania's area).\n\nThere are 14 national parks totaling 3,223 km²:\n\nThe Romanian parliament discussed in September 2008 a bill aiming to open 13 national parks to sustainable hunting, in order to manage the wildlife biodiversity in these areas and promote greater tourism and the accompanying revenue necessary to support and maintain the parks. However, after several protests from environmental organizations, the law was rejected by President Traian Băsescu. Nowadays, hunting is prohibited in national parks of Romania.\n\nThere are 17 natural parks totaling 5,492.33 km²:\n\nNatural reserves are natural areas protected by law in order to protect and conserve important habitats and natural species. The dimensions of natural reserves vary and depend on the area needed by protected natural elements. Besides scientific activities, the administrations of natural reserves encourage traditional activities and ecotourism that do not affect the natural landscape. Here is not permitted to use natural resources.\n\nThere are 617 natural reservations totaling 2,043.55 km².\n\nScientific reserves are also protected areas that, like the previous ones, aim at protection and preservation of natural habitats. The difference is that scientific reserves can not be visited by tourists. These areas typically contain rare plant and animal species or particular natural elements, reason why here are prohibited any human activities, excepting research and education activities. Entry without permission in scientific reserves is punishable by considerable fines.\n\nThere are 55 such reservations totaling 1,112.77 km².\n\nThere are 234 natural monuments totaling 77.05 km².\n\n"}
{"id": "25198", "url": "https://en.wikipedia.org/wiki?curid=25198", "title": "Quaternary", "text": "Quaternary\n\nQuaternary () is the current and most recent of the three periods of the Cenozoic Era in the geologic time scale of the International Commission on Stratigraphy (ICS). It follows the Neogene Period and spans from 2.588 ± 0.005 million years ago to the present. The Quaternary Period is divided into two epochs: the Pleistocene (2.588 million years ago to 11.7 thousand years ago) and the Holocene (11.7 thousand years ago to today). The informal term \"Late Quaternary\" refers to the past 0.5–1.0 million years.\n\nThe Quaternary Period is typically defined by the cyclic growth and decay of continental ice sheets associated with Milankovitch cycles and the associated climate and environmental changes that occurred.\n\nIn 1759 Giovanni Arduino proposed that the geological strata of northern Italy could be divided into four successive formations or \"orders\" (). The term \"quaternary\" was introduced by Jules Desnoyers in 1829 for sediments of France's Seine Basin that seemed clearly to be younger than Tertiary Period rocks.\n\nThe Quaternary Period follows the Neogene Period and extends to the present. The Quaternary covers the time span of glaciations classified as the Pleistocene, and includes the present interglacial time-period, the Holocene.\n\nThis places the start of the Quaternary at the onset of Northern Hemisphere glaciation approximately 2.6 million years ago. Prior to 2009, the Pleistocene was defined to be from 1.805 million years ago to the present, so the current definition of the Pleistocene includes a portion of what was, prior to 2009, defined as the Pliocene.\n\nQuaternary stratigraphers usually worked with regional subdivisions. From the 1970s, the International Commission on Stratigraphy (ICS) tried to make a single geologic time scale based on GSSP's, which could be used internationally. The Quaternary subdivisions were defined based on biostratigraphy instead of paleoclimate.\n\nThis led to the problem that the proposed base of the Pleistocene was at 1.805 Mya, long after the start of the major glaciations of the northern hemisphere. The ICS then proposed to abolish use of the name Quaternary altogether, which appeared unacceptable to the International Union for Quaternary Research (INQUA).\n\nIn 2009, it was decided to make the Quaternary the youngest period of the Cenozoic Era with its base at 2.588 Mya and including the Gelasian stage, which was formerly considered part of the Neogene Period and Pliocene Epoch.\n\nThe Anthropocene has been proposed as a third epoch as a mark of the anthropogenic impact on the global environment starting with the Industrial Revolution, or about 200 years ago. The Anthropocene is not officially designated by the ICS, however, but a working group is currently aiming to complete a proposal for the creation of an epoch or sub-period by 2016.\n\nThe 2.6 million years of the Quaternary represents the time during which recognizable humans existed. Over this geologically short time period, there has been relatively little change in the distribution of the continents due to plate tectonics.\n\nThe Quaternary geological record is preserved in greater detail than that for earlier periods.\n\nThe major geographical changes during this time period included the emergence of the Strait of Bosphorus and Skagerrak during glacial epochs, which respectively turned the Black Sea and Baltic Sea into fresh water, followed by their flooding (and return to salt water) by rising sea level; the periodic filling of the English Channel, forming a land bridge between Britain and the European mainland; the periodic closing of the Bering Strait, forming the land bridge between Asia and North America; and the periodic flash flooding of Scablands of the American Northwest by glacial water.\n\nThe current extent of Hudson Bay, the Great Lakes and other major lakes of North America are a consequence of the Canadian Shield's readjustment since the last ice age; different shorelines have existed over the course of Quaternary time.\n\nThe climate was one of periodic glaciations with continental glaciers moving as far from the poles as 40 degrees latitude. There was a major extinction of large mammals in Northern areas at the end of the Pleistocene Epoch. Many forms such as saber-toothed cats, mammoths, mastodons, glyptodonts, etc., became extinct worldwide. Others, including horses, camels and American cheetahs became extinct in North America.\n\nGlaciation took place repeatedly during the Quaternary Ice Age – a term coined by Schimper in 1839 that began with the start of the Quaternary about 2.58 Mya and continues to the present day.\n\nIn 1821, a Swiss engineer, Ignaz Venetz, presented an article in which he suggested the presence of traces of the passage of a glacier at a considerable distance from the Alps. This idea was initially disputed by another Swiss scientist, Louis Agassiz, but when he undertook to disprove it, he ended up affirming his colleague's hypothesis. A year later, Agassiz raised the hypothesis of a great glacial period that would have had long-reaching general effects. This idea gained him international fame and led to the establishment of the Glacial Theory.\n\nIn time, thanks to the refinement of geology, it has been demonstrated that there were several periods of glacial advance and retreat and that past temperatures on Earth were very different from today.\nIn particular, the Milankovitch cycles of Milutin Milankovitch are based on the premise that variations in incoming solar radiation are a fundamental factor controlling Earth's climate.\n\nDuring this time, substantial glaciers advanced and retreated over much of North America and Europe, parts of South America and Asia, and all of Antarctica. The Great Lakes formed and giant mammals thrived in parts of North America and Eurasia not covered in ice. These mammals became extinct when the glacial period Age ended about 11,700 years ago. Modern humans evolved about 315,000 years ago. During the Quaternary Period, mammals, flowering plants, and insects dominated the land. \n\n\n\n"}
{"id": "41445803", "url": "https://en.wikipedia.org/wiki?curid=41445803", "title": "Renewable energy in Armenia", "text": "Renewable energy in Armenia\n\nRenewable energy in Armenia ranges from geothermal, hydroelectric, solar and wind energy in Armenia.\n\n"}
{"id": "19790402", "url": "https://en.wikipedia.org/wiki?curid=19790402", "title": "Rosenfeld Effect", "text": "Rosenfeld Effect\n\nThe Rosenfeld Effect is not a scientific phenomena, but an empirical fact that electricity use per capita in California (CA) had been almost flat from 1973 to 2006, whereas use in the United States has risen 50%. The effect is attributed to energy efficiency, a cause pioneered by Arthur H. Rosenfeld. Up until 2010 Dr. Rosenfeld was the commissioner and a very prominent member of the California Energy Commission board and presided over the Research, Development, and Demonstration Committee; the Dynamic Pricing Committee; and the Energy Efficiency Committee, whose main purposes are to promote energy efficiency and conservation, to support cutting edge research and, to look towards developing renewable energy sources. \n\nA conference in 2006 at UC Berkeley was dedicated to the Rosenfeld Effect. The purpose of the Energy Symposium conference on the Rosenfeld Effect was to inform people of the role increased energy efficiency plays in California, China, Russia, and on a much larger global scale.\n\nAccording to Dr. Rosenfeld, as time progresses, new technological breakthroughs make electrical appliances more efficient and longer lasting than their older counterparts. For example, when refrigerators were made in 1974, the model consumed four times as much energy compared to refrigerators manufactured in 2001. As the appliances became more efficient, they would save more energy, which consequently lowered the amount of money the average person paid for electricity to keep the appliance running. The cheaper cost of higher efficiency is also the premise behind Rosenfeld's Law, which is also attributed to Rosenfeld. Rosenfeld believed that reasonable standards for energy efficiency in numerous appliances could guarantee a drastic reduction in energy consumption. As opposed to national precedents set in the early 1990s, California's earlier standards for energy efficiency contributed much to these technological discoveries.\n\nThe Rosenfeld Effect is often associated with the following two charts:\n\nThe first graph is Figure 1 of the 2007 California Integrated Energy Policy Report and is sometimes referred to as the Rosenfeld Curve. The second is widely used in Powerpoint presentations.\n\nThe Natural Resource Defense Council (NRDC), a special interest group independent of the California Energy Commission, released a document in 2005 detailing the energy per capita of the state of California as well as the energy per capita of the United States as a whole. From 1976 to 2005, California's energy per capita fluctuated around 6,500 kilowatt-hours per capita whereas the US roughly went from 8,000 kilowatt-hours to 12,000 kilowatt-hours per capita. The document also states that Rosenfeld's titles, such as titles 20 and 24, which are responsible for setting energy efficiency standards, have saved California from needing to build 21 power plants. For California's per capita energy consumption to remain constant as the growing population increased its dependence on electricity, increases in efficiency had to keep up with electricity's increasing utility. The article also credits Rosenfeld's energy saving targets, his focus on research and development, his ability to integrate energy efficiency into resource procurement, his provision of \"performance-based incentives\", his capability to motivate utility companies to segregate revenues from sales, his willingness to have progress towards targets measured by a third party, and his implementation of \"well-designed programs\" as additional reasons for California's leadership in energy efficiency. One \"well-designed program\" the article cites is the subsidization of compact fluorescent lamps and how different methods were undertaken to make the cost drop from $25 a bulb in the 1980s to $3 a bulb in 2005, which saved a typical Californian family on average of about $1000 per year. The NRDC estimates California saves approximately $42 billion a year from just four of Rosenfeld's initiatives: the invention of DOE-2 (a computer program that builds energy analysis--$10 billion per year) and the implementation of high-frequency ballasts ($5 billion per year), low-e windows (estimated $5-$10 billion per year), and more efficient refrigerators ($17 billion per year). Beyond saving energy and money, Rosenfeld's actions at California's Energy Commission have also helped paint California as an example to the United States in carbon dioxide emissions, setting California's per capita emissions to half the national average in 2000.\n\nBeyond setting standards and implementing programs, Rosenfeld and the California Energy Commission have invested money into research and development. The NRDC cites the following programs: the California Energy Commission's Public Interest Energy Research Program (PIER), Lawrence Berkeley National Laboratory's Center for Building Science, California Public Utilities Commission's Emerging Technologies Fund (CETF), and the California Clean Energy Fund (CalCEF), all of which are focused on finding new ways to save energy, by finding new technology or ways that can help the everyday person use less power from the grid.\n\nAccording to its staff, PIER allocates grants to various companies investing in energy efficiency. The document PIER staff provides on the California Energy Commission's website includes an example of a funded company: Altex Technologies Corporation, which researches technologies for fuel cells and boiler burners. The staff behind PIER estimates the program has also directly and indirectly created roughly 50,000 jobs since its imitation in 1996.\n\nFunding for the Lawrence Berkeley National Laboratory's Center for Building Science was to discover ways to limit energy waste. After sufficient testing, the applications team released a guide on how to design a research laboratory in a more energy efficiency manner. The guide includes suggestions for how to properly set air filtration, lighting, and supply, exhaust, and distribution systems for optimal efficiency. \n\nThe CETF stated its primary goals include managing telecommunication mergers and helping strengthen the broadband Internet infrastructure of California, with saving energy an indirect result of its procedures. The California Public Utilities Commission writes that in 2010 the CETF considered its work complete and disbanded.\n\nCalCEF is a non-profit program that hopes to transition to a clean energy economy by creating and supporting institutions that grow markets for clean energy technologies. CalCEF is partnered with Lawrence Berkeley National Laboratory, and according to a CalCEF press release published in December 2012, the U.S. Department of Energy granted $120 million to upgrade the California storage accelerator.\n\nWhile the energy per capita began to rise post-2006 for California, Rosenfeld still took initiative to manage California's energy consumption. In September 2005, the California Public Utilities Commission authorized a $2.7 billion investment from 2006-2008 to save ratepayers $5.4 billion and to save California from having to expend resources in order to build three additional power plants.\n\nRosenfeld's work has motivated others to conserve energy. For example, In April 2006, Ashok Gadgil of Lawrence Berkelely International Laboratory gave a presentation detailing international examples of the Rosenfeld Effect in practical use, which included programs making compact fluorescent lamps more cost-effective for poorer countries; distributing energy efficient water filters to the Philippines, Mexico, and India; and distributing metal cook-stoves with an increased efficiency factor of four in contrast to its predecessor to poor citizens of Sudan and Darfur refugees displaced by conflict in that region.\n\nTo understand the extensive work and research put behind the Rosenfeld Effect, information presented by Dr. Rosenfeld himself illustrates how several areas of crucial interest were taken into account: investigating the science and engineering of energy end-use, assessing potential and theoretical opportunities for energy efficiency, developing analytic and economic models to quantify opportunities , and researching and developing new equipment and processes to make these opportunities a success. Dr. Rosenfeld’s main motivation for improving energy efficiency was to save money as well as save resources such as oil, gas, and forests for future generations. The purpose behind the Rosenfeld effect was not only to set an example in high efficiency standards, but also to curb the threat of carbon-emissions which lead to greenhouse gases and thus further threat of global warming. By making energy use more efficient humans would be burning less fossil fuel for energy consumption. This decreased carbon emissions by more than 3 million tons per year by the year 2008, which is the equivalent of taking 650,000 cars off the road.\n"}
{"id": "4972467", "url": "https://en.wikipedia.org/wiki?curid=4972467", "title": "State of the Climate", "text": "State of the Climate\n\nThe State of the Climate is an annual report that is primarily led by the National Oceanic and Atmospheric Administration National Climatic Data Center (NOAA/NCDC), located in Asheville, North Carolina, but whose leadership and authorship spans roughly 100 institutions in about 50 countries.\n\nThe report appears as a supplement to a summer issue of the \"Bulletin of the American Meteorological Society\" (BAMS), a publication of the American Meteorological Society. The State of the Climate report, known until 2001 as the \"Climate Assessment\", is an international effort.\n\nThe 2010 edition (released June 28, 2011) contained submissions from 368 authors from 45 nations and covered 41 climate indicators. The 2010 edition contained a highlights document that summarized the major findings of the report. The State of the Climate summarizes the global and regional climate of the preceding calendar year and places it into a historical context. In addition, notable climatic anomalies and events are discussed.\n\nMajor findings in the 2010 report were:\n\nThe 2010 issue included a sidebar detailing the multiple lines of evidence (major variables besides global temperature) consistent with the conclusion of a warming planet. An image associated with this sidebar has been recreated many times since, as the 11 (or ten) indicators of a warming planet.\n\nThe 2011 edition contained submissions from 376 authors from 46 nations/territories. The La Nina event of that year was a major focus of the report. The cover featured East African women walking to retrieve water in a dust storm. East African drought is not atypical of La Nina episodes.\n\nThe 2012 edition contained submissions from 394 authors from 54 nations/territories. It cover featured an Arctic scene, reflecting major events in that region during the year. \n\nMajor findings in the 2012 report were:\n\nThe 2013 edition has been released on July 17, 2014. The American Meteorological Society published a supplemental paper online. The report was compiled by 425 scientists from 57 countries.\n\nMajor findings in the 2013 report include:\n\nA report was released for the year of 2014.\n\nA report was released in August 2016 for 2015.\n\n2015 was the hottest year to date. Greenhouse gases were highest on record. Global upper ocean heat content was highest on record. Global sea level was highest on record.\n\nCurrent as of report, 2016 surpassed 2015 as the warmest year in 137 years of recordkeeping. \n\nConcentrations of carbon dioxide () in the Earth's atmosphere surged by a record amount in 2016, according to the World Meteorological Organization.\n\n2017 was recorded as the third warmest year on record. 2017 was the warmest non-El Niño year in the instrumental record.\n\n\n"}
{"id": "248078", "url": "https://en.wikipedia.org/wiki?curid=248078", "title": "Stingray (1964 TV series)", "text": "Stingray (1964 TV series)\n\nStingray is a British children's Supermarionation television series, created by Gerry and Sylvia Anderson and produced by AP Films for Associated Television and ITC Entertainment between 1964 and 1965. Its 39 half-hour episodes were originally broadcast on ITV in the United Kingdom and in syndication in the United States and Canada.\n\nThe series was written by the Andersons, Alan Fennell and Dennis Spooner. Its music was composed by Barry Gray and its special effects were directed by Derek Meddings. It was filmed in Eastmancolor at a cost of approximately £1 million.\n\n\"Stingray\" was the first Supermarionation series to feature marionette characters with interchangeable heads that enabled them to show a variety of expressions. It was also the first British TV series to be filmed entirely in colour, primarily to increase its appeal to the lucrative American market.\n\n\"Stingray\", a nuclear-powered combat submarine built for speed and manoeuvrability, is the flagship of the World Aquanaut Security Patrol (WASP), a branch of the World Security Patrol (WSP) responsible for policing the Earth's oceans in the mid-2060s. The vessel is armed with \"sting missile\" torpedoes and can travel at up to underwater, while its pressure compensators allow it to reach depths of over .\n\nThe WASP's base is Marineville, located several miles inland somewhere on the West Coast of North America. It is connected to the Pacific Ocean via a tunnel leading to an \"ocean door\", through which \"Stingray\" is launched. Alerts such as \"action stations\", \"launch stations\", and \"battle stations\" are sounded by rapid drum beats that are played over the base's public address system. In emergency situations, the entire base is lowered into underground bunkers on giant hydraulic jacks while interceptor missiles and fighter aircraft are launched to counter threats. WASP personnel acknowledge commands with the phrase \"P.W.O.R.\" – short for \"Proceeding With Orders Received\".\n\n\"Stingray\" is piloted by the square-jawed Captain Troy Tempest. He is paired with Southern navigator Lieutenant George Lee Sheridan, nicknamed \"Phones\" for his role as \"Stingray\"<nowiki>'</nowiki>s hydrophone operator. Troy and Phones board \"Stingray\" by sitting on twin injector seats in Marineville's stand-by lounge, which are lowered into the vessel via injector tubes and then clamped into place. They answer to the crusty \"hoverchair\"-bound Commander Sam Shore, whose daughter, Lieutenant Atlanta Shore, takes shifts in the Marineville control tower and is enamoured of Troy.\n\nAt the start of the series, the WASP learns that the ocean floor is home to many underwater civilisations. Among these is the city of Titanica – whose tyrannical ruler, King Titan, commands a brutal warrior race called the Aquaphibians and possesses a fleet of lethal submersibles known as \"Mechanical Fish\" (referred to as \"Terror Fish\" in tie-in media). In the first episode, \"Stingray\" is attacked by Titan's forces and Troy and Phones are captured. They are rescued by Titan's slave, Marina, a mute young woman from the undersea city of Pacifica who can breathe underwater. Marina returns to Marineville with Troy and Phones and becomes a regular member of the \"Stingray\" crew. Troy becomes infatuated with her, making Atlanta jealous. Meanwhile, Titan, furious at Marina's betrayal, vows revenge on \"terraineans\" (land people) in general and Troy and the WASP in particular.\n\nMany later episodes revolve around Titan's schemes to destroy \"Stingray\" and Marineville; however, these usually fail due to the incompetence of his spy on land, Surface Agent X-2-0. Based on the Pacific island of Lemoy, X-2-0 lives in a dilapidated house whose walls conceal banks of highly-sophisticated surveillance and tracking equipment. Other episodes feature encounters with other races living under the sea or within the Earth, some friendly and others hostile, or investigation of natural phenomena.\n\n\n\nBetween 1980 and 1981, two compilation films were produced. Each of these comprised re-edited versions of four of the original episodes. The films were produced for the American market and aired in the United States as part of an ITC Entertainment package titled \"Super Space Theater\"; other Supermarionation series were given similar treatments. On 24 November (Thanksgiving) 1988, the second \"Stingray\" compilation feature, \"Invaders from the Deep\", appeared as the first broadcast episode of movie-mocking TV series \"Mystery Science Theater 3000\" in the US.\n\n\n\n\nMarina is unique among Supermarionation characters in that she has no dialogue. In the episode \"Raptures of the Deep\" she appears to communicate telepathically with Troy (her thoughts voiced by Sylvia Anderson), but this is later revealed to be a part of a dream that Troy experienced while delirious, having passed out underwater due to a lack of oxygen. In the dream sequence in question, Marina's lips do not move because her puppet was not equipped with a speech mechanism.\n\nAs filming on \"Fireball XL5\" came to an end in late 1962, producer Gerry Anderson considered a series set underwater to be the next logical step for AP Films: \"We had been on land and in space, so where could we go next? One possibility was underwater.\" He was inspired by childhood memories of U-boats in the Second World War, as well as by the mysteries of the ocean: \"I was ... fascinated by trenches in the ocean that are as deep as mountains are high. There are features that man has never seen and pressures that are almost impossible to withstand. I began to wonder if there were areas of the Earth which had been little explored and felt justified in writing some wacky stuff.\" \n\nLew Grade, who had been financing APF since the production of \"Supercar\" and had bought the company following the commercial success of \"Fireball XL5\", approved the new concept and commissioned 26 episodes. Anderson named the series \"Stingray\" in part due to a mistaken belief that stingrays are dangerous animals, but also because it \"seemed an exciting title.\"\n\nIn preparation for the new series, APF moved to larger facilities on a different part of the Slough Trading Estate at a cost of £75,000 (approximately £ million in ). The new studios, built inside a factory unit, were located half a mile from the site where APF had filmed \"Four Feather Falls\", \"Supercar\" and \"Fireball XL5\". They contained three shooting stages, each measuring : two for puppet filming and one for special effects filming.\n\nProduction began in the spring of 1963 and the series was completed in ten months, with each episode taking an average of 16.5 days to film. The total cost of the production was approximately £1 million (approximately £ million in ). It was budgeted at £20,000 (approximately £ in ) per episode, which enabled APF, whose earlier productions had been in black and white, to film in Eastmancolor. The switch to colour filming was intended to increase the series' chances of being bought by a network in the United States, where colour TV broadcasts were already common. Sets were re-painted after NBC supplied APF with a list of colours believed to cause problems such as flaring or bleeding; according to Anderson, this was unnecessary because if filmed in Eastmancolour, a set \"would appear on screen exactly as you had painted it.\" Nevertheless, some colours were avoided as they did not come out well in black and white. During the production of \"Stingray\", APF became the largest consumer of colour film in the UK. \n\nAs filming progressed, Grade extended his commission to 39 episodes. As shooting on the final 13 episodes commenced, Don Mason and Robert Easton, who had been told that all members of the voice cast were being paid the same amount, discovered that they were actually earning less than their co-star David Graham. Mason and Easton did not commit to the remaining episodes until their fees had been re-negotiated.\n\nIn a first for a Supermarionation series, \"Stingray\" features a Christmas-themed episode (\"A Christmas to Remember\") and a clip show final episode. The latter, \"Aquanaut of the Year\", incorporates elements from the documentary series \"This Is Your Life\", which required APF to obtain the approval of its creator, Ralph Edwards. With negotiations between Edwards and APF taking longer than expected, work commenced on an alternative series finale in which Commander Shore and Admiral Denver view highlights from a selection of \"Stingray\"s missions on a film projector; however, the production of this episode was halted when Edwards gave APF permission to proceed with \"Aquanaut of the Year\". The linking material from the abandoned episode was re-discovered in either 2000 or 2001 and subsequently included on \"Stingray\" DVD releases. Several years later, it was combined with footage from \"Stingray\", \"An Echo of Danger\", and \"Emergency Marineville\" to create a new 29-minute clip show titled \"The Reunion Party\", which was first broadcast in 2008.\n\nGerry Anderson said that the character of Phones was inspired by a sound engineer with whom he used to work: \"He spent so long with his headphones plugged in to various bits of equipment that he used to leave them on all the time, earning himself the nickname 'Phones'.\" Voice actor Robert Easton based the character's Southern American tones on his performance in the 1961 film \"Voyage to the Bottom of the Sea\", in which he had played a Southern radio operator called Sparks. The voice of Surface Agent X-2-0 was Easton's impression of actor Peter Lorre, who had appeared in the same film.\n\nSylvia Anderson, who had voiced the regular character Dr Venus on \"Fireball XL5\" and was credited for \"characterisation supervision\" for \"Stingray\", stated in her 2007 autobiography that she devised Marina as mute because she wanted to take a break from voice acting and \"concentrate on the scripts and characters\". The Aquaphibians were based on a villainous alien from the \"Fireball XL5\" episode \"XL5 to HO\".\n\nThe process of designing and making the puppets took four months and each of the main characters was sculpted in duplicate to allow two episodes to be filmed simultaneously using both puppet stages. The likenesses of some of the puppets were inspired by real-life actors: Titan was based on a young Laurence Olivier and Surface Agent X-2-0 on either Claude Rains or Peter Lorre. Troy Tempest was modelled on James Garner at Gerry Anderson's suggestion. Atlanta Shore has been likened to Lois Maxwell (who voiced the character) and Marina to both Brigitte Bardot and Ursula Andress.\n\n\"Stingray\" was the first Supermarionation series to feature puppets with glass eyes and poseable hands (both specially made by outside contractors) for greater realism. To make the puppets' eyes sparkle in a lifelike way, they were polished with silicon and illuminated using an \"eye light\" (a small lamp). Another innovation was the creation of alternative heads to allow puppets to express emotions: in addition to their \"normal\" heads, which had neutral expressions, the main characters could also be fitted with \"smiling\" and \"frowning\" heads. The wigs of female puppets were made of human hair; for male puppets, mohair was used as it was softer and easier to style.\n\nMost of the series' special effects shots were filmed on high-speed cameras with the footage slowed down in post-production to convey greater weight and scale.\n\nThe \"Stingray\" submarine was designed by Reg Hill and built by Feltham-based company Mastermodels. The Marineville model, which was created in-house, was made of wood and cardboard augmented with pieces of model kits purchased from a toy shop. It was lowered and raised by hydraulics.\n\nThe series' underwater scenes were filmed not in a water tank, as Anderson had originally envisaged, but by shooting a model of the ocean floor, mounted against a cyclorama, through a thin aquarium and \"flying\" the puppets and miniature models across the set on wires from an overhead gantry. A similar technique had been used for the underwater scenes in \"Supercar\". Several aquaria were used; constructed by a company that supplied fish tanks to London Zoo, they were re-built with thicker glass after one of them burst due to the water pressure. Wires were painted over to make them non-reflective, while vegetable dye was added to the aquaria to give the water a murky appearance. Fans were used to simulate currents passing over characters' hair and clothing.\n\nThe illusion of scenes being set underwater was enhanced by populating the aquaria with tropical fish of various sizes to create forced perspective. Fish food was dropped at various points around the aquaria to keep the animals in shot. A disc with various portions cut out was mounted in front of a lamp and rotated to simulate light being refracted through the ocean surface, while the water inside the aquaria was disturbed to create \"ripple\" effects. For the conclusion of the \"Stingray\" launch sequence, in which the submarine shoots out of an underwater tunnel, part of the set was painted onto the aquarium to conceal the air line that was used to produce the accompanying bubbles. The move away from black-and-white filming was sometimes problematic as build-ups of algae caused the water in the aquaria to change colour.\n\nWater surface shots in \"Supercar\" and \"Fireball XL5\" had been filmed in a single outdoor water tank, but for \"Stingray\" a number of tanks were built inside the studio. As the crew were unable to use lighting effects to make the water look blue, it was dyed to give it a realistic colour. Various powders were added to create whitewater and foam effects for undersea explosions and storm scenes. Miniature models were controlled using wires, poles and underwater tracks and rigs.\n\nEach tank incorporated a weir system whereby one or more walls, including the back wall, were built lower and the tank was filled to a higher level to create waterfalls. With the camera mounted at water level, this produced an artificial horizon at the back of the tank. The overflowing water was collected in troughs and then pumped back into the tank to keep the water level constant and sustain the effect. To conserve studio space, some scenes were filmed in a wedge-shaped tank that was built to match the field of view of the camera. One of the effects shots in the opening titles, featuring a complex manoeuvre in which \"Stingray\" and a pursuing Mechanical Fish leaping out of the sea, was filmed in a single take.\n\nShots of aircraft in flight were filmed using a technique known as the \"rolling sky\", whereby the miniature model remained stationary and an illusion of movement was created by continuously running a loop of painted canvas background around two electrically-driven rollers. This system, devised by effects director Derek Meddings, made aerial shots easier to film as it took up little studio space.\n\nThe title sequence consists of a series of action shots featuring undersea explosions, Marineville going to red alert and \"Stingray\" being launched to do battle with a Mechanical Fish. This is accompanied by dramatic narration from the character of Commander Shore, who warns the audience to \"Stand by for action!\" and declares that \"Anything can happen in the next half-hour!\" In the first 26 episodes, the title sequence opens in black and white before switching to colour; for the final 13 episodes, the first few seconds were replaced with all-colour footage. \n\nJim Sangster and Paul Condon, authors of \"Collins Telly Guide\", praise the opening titles, writing that \"Of all the programmes we've looked at for this book, there is none with a title sequence as thrilling as \"Stingray\".\" According to John Peel, the \"Stingray\" title sequence contrasts greatly with those of \"Supercar\" and \"Fireball XL5\", which he describes as \"straight narrative openings\". Peel also argues that \"Stingray\" has influenced the \"rapid cutting, pounding rhythms and extreme stylising\" of subsequent TV title sequences. \n\nThe series' closing titles focus on the love triangle between Atlanta, Troy, and Marina, with Troy singing \"Aqua Marina\" – a song about his romantic feelings for Marina, performed by Gary Miller with backing vocals by soprano Joan Brown – while Atlanta gazes wistfully at his photograph.\n\nIn the UK, \"Stingray\" was first broadcast on 4 October 1964 in the Anglia, Border, Grampian, London and Southern regions. Although it received little publicity it replicated the success of earlier Supermarionation series. Having debuted in black and white, it was transmitted in colour for the first time in December 1969. It was repeated on ITV in 1981 and BBC2 in the early 1990s. It was also shown on Sky One from 2002 to 2003. \n\nIn the US, the series was first broadcast in 1965. Premiering in colour, it was syndicated across more than 100 markets with total sales exceeding £3 million. Sci-Fi Channel aired the series between 1992 and 1997 as part of its \"Sci-Fi Cartoon Quest\" programming block.\n\n\"Stingray\" was featured in the Supermarionation tie-in comic \"TV Century 21\" from its first issue, published by AP Films (Merchandising) in January 1965. The 1960s also saw the publication of two original novels by Armada Books: \"Stingray\" and \"Stingray and the Monster\", written by John William Jennison under the pseudonym \"John Theydon\".\n\nCoinciding with the revival of the show on BBC2, a self-titled \"Stingray\" comic was launched in 1993 and ran until 1995, published by Fleetway Publications. The strips featured were direct reprints from \"TV Century 21\" with minimal new content such as competitions and letter pages. In 1995 the comic was cancelled and the remaining \"Stingray\" strips were featured in the then ongoing \"Thunderbirds\" comic until it's eventual cancellation. \n\nTo supplement the 39 TV episodes, in 1965 AP Films (Merchandising) released three \"audio adventures\" as 7-inch vinyl EP records (marketed as \"mini-albums\"). These audio episodes, each running to approximately 21 minutes and featuring the voice cast from the TV series, are included as special features on the UK \"Stingray\" DVD box set.\n\n\"Marina Speaks\" reveals that Marina is in fact not mute at all. In fact, her race has been cursed by Titan – should any one of them speak, another will die. They are not certain that this is true, but none of them dares find out; thus, for years they have lived in complicit silence.\n\nMedia historian Marcus Hearn argues that \"Stingray\" essentially \"[transfers] the format of \"Fireball XL5\" to an underwater setting.\" Writing in 2006, Robert Sellers described \"Stingray\" as the \"first truly classic Anderson show\", whose special effects \"have stood the test of time remarkably well.\" Daniel O'Brien, author of \"SF:UK: How British Science Fiction Changed the World\", considers it to be \"perhaps the archetypal Gerry Anderson series\".\n\nReviewing the DVD box set in 2001, Mike Fillis of \"TV Zone\" magazine conceded that \"Stingray\" was less \"ambitious\" than its immediate follow-up, \"Thunderbirds\", but compared its \"self-awareness\" and \"looseness\" favourably to the \"po-faced rigidity\" of \"Captain Scarlet and the Mysterons\". He also praised the series' \"well-drawn\" characters and described its water-based special effects sequences as \"surprisingly elegant\" given the \"uncooperative\" nature of the medium.\n\nPaul Cornell, Martin Day and Keith Topping, authors of \"The Guinness Book of Classic British TV\", view \"Stingray\"s effects as more \"realistic\" than those of earlier Supermarionation productions. They also argue that while many episodes were \"predictable and corny\", the series contained a \"knowingness and a love of character that made the whole thing charming.\" According to Jon E. Lewis and Penny Stempel, authors of \"Cult TV: The Essential Critical Guide\", the series featured \"plenty of kiddie-time exciting narrative action, while the more sophisticated could enjoy its proclivity to spoof virtually everything which passed its periscope.\" Sangster and Condon argue that while elements such as the animal character, Oink, mean that \"Stingray\" is aimed primarily at children, it is the first Anderson series in which the \"sophistication of the production\" creates appeal for adults. They regard \"Stingray\" as \"much less po-faced\" than \"Thunderbirds\", with episodes that are \"mercifully shorter, leading to tighter plotting and an engaging simplicity\". Peel suggests that the \"tongue-in-cheek humour that [Gerry] Anderson favoured probably reached its peak with \"Stingray\".\"\n\nMedia historian Nicholas J. Cull observes that through its depiction of the World Aquanaut Security Patrol (WASP), \"Stingray\" is one of several Gerry Anderson series to \"assume the development of world government and world security institutions\" and \"reflect the 1960s vogue for stories set in secret organisations with extravagant acronyms.\" He compares the premise to the Cold War, noting the conflict between WASP and the various undersea races and the latter's use of spies to infiltrate human society. Cull cites \"Marineville Traitor\", whose plot concerns the hunt for an \"enemy within\", as an episode with an \"especially strong Cold War flavour\". He also notes that while Anderson's series often focus on the dangers of nuclear technology, \"Stingray\" also presents it in a positive light: \"Stingray\" itself, for example, is a nuclear-powered submarine. \n\nSarah Kurchak of The A.V. Club argues that compared to villains in previous Anderson series Titan and the Aquaphibians represent a \"more classically Cold War-style villainous Other\". She adds: \"Throughout the 39 episodes, the battle lines between land and sea are clearly defined, the enemy is always watching, and the target of their aggression is always close to home.\" Kurchak also suggests that the character of Troy Tempest serves as an embodiment of Cold War anxieties through his \"multiple nightmares\" involving threats against Marineville. O'Brien remarks that \"Stingray\" contains \"more than a touch of the Cold War ethos\", suggesting that Titan \"could have easily belonged to an underwater branch of the Soviet Bloc, hungering for the destruction of the Tempest.\"\n\nAccording to the \"Stingray\" comic strip in the weekly \"Countdown\" comic, more than one \"Stingray\"-class submarine was in service in the Marineville fleet. These vessels had names such as \"Spearfish\", \"Barracuda\", \"Moray\", and \"Thornback\" and were identified by different numbers on their fins, suggesting that the \"3\" painted on \"Stingray\"<nowiki>'</nowiki>s tail fin did not indicate that the submarine was a \"Mark III\" after all.\n\nA similar idea had been adopted by author John Theydon for his second \"Stingray\" tie-in novel, \"Stingray and the Monster\", some years prior. In the novel, another WASP submarine (unnamed and referred to as \"Number Thirteen\") is hi-jacked by an old enemy of Commander Shore. Theydon's description of the hi-jacked boat, both inside and out, is recognisably similar to that of \"Stingray\", with the exception that \"Number Thirteen\" is stated not to possess \"Stingray\"<nowiki>'</nowiki>s exceptional performance, being limited to roughly instead of the that \"Stingray\" is quoted as being able to attain. The implication, not explicitly stated, is that \"Stingray\" is an upgraded version of the design. Later, \"TV21\" comic mentioned a second \"super-sub\" due to enter service under the WASP that is stolen by a Mysteron agent as part of the plot of a \"Captain Scarlet and the Mysterons\" story.\n\n\n\n"}
{"id": "22712279", "url": "https://en.wikipedia.org/wiki?curid=22712279", "title": "The Birds of the Malay Peninsula", "text": "The Birds of the Malay Peninsula\n\nThe Birds of the Malay Peninsula is a major illustrated ornithological reference work conceived and started by Herbert Christopher Robinson. The full title is The Birds of the Malay Peninsula: a general account of the birds inhabiting the region from the isthmus of Kra to Singapore with the adjacent islands. It comprises five substantial (large octavo) hardbound volumes of text, with 125 plates (123 in colour) by Henrik Grönvold and 11 maps. It was published by H. F. and G. Witherby, London. The binding of the first four volumes was red buckram; the fifth was red cloth with a dust jacket.\n\nRobinson served as Director of Museums in the Federated Malay States in the early 20th century. On his retirement in 1926 he initiated the production of the work, though he died in 1929 after the first two volumes were published. With the help of Robinson's notes and papers, the third and fourth volumes were prepared by Frederick Chasen, the Director of the Raffles Museum in Singapore. Completion was much delayed, first by Robinson's death, and then by Chasen's early in 1942. The fifth and final volume, by Lord Medway and David Wells, was eventually published in 1976, in a review of which D.G. Robertson says:\n\"After the war E. Banks, former Curator of the Sarawak Museum, wrote a replacement text and deposited it in the British Museum (Natural History). In 1964, Ken Scriven, a long-time resident of Malaysia, was in London and quite by chance discovered not only Banks's text but also the coloured plates by H. Gronvold. He informed Lord Medway and David Wells who in turn decided to complete the series. Witherby, publisher of Volumes I to IV, agreed to produce Volume V using typesetting, paper and layout identical with the previous volumes and in 1976 the task was completed, almost fifty years after its inception. The authors have revised and updated Banks's text and, because Volumes I to IV are almost priceless, have amended Chasen's original intentions so that Volume V can stand on its own.\"\nThe complete set contains:\n\n\n"}
{"id": "10330451", "url": "https://en.wikipedia.org/wiki?curid=10330451", "title": "Uri Bin Nun", "text": "Uri Bin Nun\n\nUri Bin Nun is an Israeli inventor and the current CEO of the Israel Electric Corporation succeeding Dr. Jacob Ravon on March 20, 2006. He is a former student of Kerem B'Yavneh.\n\nMr. Nun has applied for five patents, including one for a regenerator matrix and one for a refrigeration device with an improved DC motor.\n\n"}
{"id": "12282627", "url": "https://en.wikipedia.org/wiki?curid=12282627", "title": "Water-in-water emulsion", "text": "Water-in-water emulsion\n\nWater-in-water (W/W) emulsion is a system that consists of droplets of water-solvated molecules in another continuous aqueous solution; both the droplet and continuous phases contain different molecules that are entirely water-soluble. As such, when two entirely aqueous solutions containing different water-soluble molecules are mixed, water droplets containing predominantly one component are dispersed in water solution containing another component. Recently, such a water-in-water emulsion was demonstrated to exist and be stable from coalescence by the separation of different types of non-amphiphilic, but water-soluble molecular interactions. These molecular interactions include hydrogen bonding, pi stacking, and salt bridging. This w/w emulsion was generated when the different water-solvated molecular functional groups get segregated in an aqueous mixture consisting of polymer and liquid crystal molecules. \n\nThis water-in-water emulsion consists of liquid crystals suspended as water-solvated droplets dispersed in a solution of polymer whose solvent is also water. The liquid crystal component of the emulsion is disodium cromolyn glycate (DSCG). This molecule is an anti-asthmatic drug, but also exists as a special type of liquid crystal when the concentration of DSCG is ~9-21 wt%. Unlike conventional lyotropic liquid crystals which consist of oily molecules such as 5CB, DSCG molecules are not amphiphilic, but entirely water-soluble. Thus, the separation of hydrophobic/hydrophilic groups cannot be applied to DSCG. The polymer solution serves as the medium or continuous phase of the w/w emulsion. Apart from being water-soluble, one important criterium for the generation of this w/w emulsion system is that the polymer cannot bear functional groups that interact strongly with DSCG. As such, ionic polymer when mixed with DSCG does not form w/w emulsion, but gives rise to a homogeneous solution or a precipitate solution. Consequently, the known polymers that afford w/w emulsion include polyacrylic amides and polyols. Surprisingly, some of these water-in-water emulsions can be exceptionally stable from coalescence for up to 30 days. \nBecause molecules of liquid crystal assume a preferred common orientation among themselves, the overall orientation of liquid crystals in a droplet is only stable in certain configurations (Fig. 3). As water solvated droplets in a w/w emulsion, DSCG molecules would align in a preferred direction on the surface of the droplet. To minimize the overall energy of the system, the DSCG molecules in the droplet prefer to align either parallel or perpendicular to the surfaces of the droplets.(Fig. 4A,B).\n\nThe stability of this water-in-water emulsion from coalescence is attributed to three molecular forces:\n1. The separation of different molecular forces at the beginning of the droplet formation. Similar forces tend to stay together: pi-stacking and salt bridging are the two dominant forces in the liquid crystal droplet phase, while hydrogen bonding governs in the continuous polymer phase. \n\n2. As the droplet size increases, the molecular interactions at the interface of the droplet phase and the continuous phase become stronger through multivalent interactions. The strengthening of interfacial molecular interactions in w/w emulsions results in the formation of a layer of polymer that coats the surface of the droplet which consequently prevents droplets from clumping together.\n3. In addition, it is also proposed that when two liquid crystal droplets merge (coalescence), the orientation of the liquid crystal molecules in the two merging droplets must change to “adapt” to each other, and thus incur an energy penalty which prevent the occurrence of coalescence.\n\nThis w/w emulsion also represents a new class of polymer dispersed liquid crystals(PDLC). Traditionally known PDLC consists of oil-in-water emulsion where the oily droplet is a thermotropic liquid crystal such as 4-pentyl-4'-cyanobiphenyl (5CB), and the water phase contains certain polymers. In comparison, this water-in-water emulsion consists of Polymer-Dispersed Lyotropic Liquid Crystals, where the lyotropic liquid crystal is DSCG molecules solvated in water. Traditional PDLCs have found application, from switchable windows to projection displays. The water-in-water emulsion of polymer-dispersed lyotropic liquid crystals has the potential for building highly bio-functional materials because of its compatibility with protein structure.\n\nOther known types of water-in-water emulsions involve the separation of different biopolymers in aqueous solution.\n\n4. (a) Terentjev, E. M. Europhys. Lett. 1995, 32, 607–612. (b) Poulin,P.; Stark, H.; Lubensky, T. C.; Weitz, D. A. Science 1997, 275, 1770–1773.\n\n5. Scholten, E.; Sagis, L. M. C.; Van der Linden, E., Effect of Bending Rigidity and Interfacial Permeability on the Dynamical Behavior of Water-in-Water Emulsions. Journal of Physical Chemistry B 2006, 110, (7), 3250–3256.\n\n1. Salt bridging and example of salt bridges http://www.cryst.bbk.ac.uk/PPS2/projects/day/TDayDiss/SaltBridges.html\n\n2. Tutorial on liquid crystals http://outreach.lci.kent.edu/\n\n3. Introduction to polymer dispersed liquid crystals (PDLC) \n\n4. Droplet configuration of PDLC’s http://plc.cwru.edu/tutorial/enhanced/files/pdlc/droplet/droplet.htm\n"}
