{"id": "8906668", "url": "https://en.wikipedia.org/wiki?curid=8906668", "title": "Acousto-optics", "text": "Acousto-optics\n\nAcousto-optics is a branch of physics that studies the interactions between sound waves and light waves, especially the diffraction of laser light by ultrasound (or sound in general) through an ultrasonic grating.\n\nOptics has had a very long and full history, from ancient Greece, through the renaissance and modern times. As with optics, acoustics has a history of similar duration, again starting with the ancient Greeks. In contrast, the acousto-optic effect has had a relatively short history, beginning with Brillouin predicting the diffraction of light by an acoustic wave, being propagated in a medium of interaction, in 1922. This was then confirmed with experimentation in 1932 by Debye and Sears, and also by Lucas and Biquard.\n\nThe particular case of diffraction on the first order, under a certain angle of incidence, (also predicted by Brillouin), has been observed by Rytow in 1935. Raman and Nath (1937) have designed a general ideal model of interaction taking into account several orders. This model was developed by Phariseau (1956) for diffraction including only one diffraction order.\n\nIn general, acousto-optic effects are based on the change of the refractive index of a medium due to the presence of sound waves in that medium. Sound waves produce a refractive index grating in the material, and it is this grating that is \"seen\" by the light wave. These variations in the refractive index, due to the pressure fluctuations, may be detected optically by refraction, diffraction, and interference effects, reflection may also be used.\n\nThe acousto-optic effect is extensively used in the measurement and study of ultrasonic waves. However, the growing principal area of interest is in acousto-optical devices for the deflection, modulation, signal processing and frequency shifting of light beams. This is due to the increasing availability and performance of lasers, which have made the acousto-optic effect easier to observe and measure. Technical progress in both crystal growth and high frequency piezoelectric transducers has brought valuable benefits to acousto-optic components' improvements.\n\nAlong with the current applications, acousto-optics presents interesting possible application. It can be used in nondestructive testing, structural health monitoring and biomedical applications, where optically generated and optical measurements of ultrasound gives a non-contact method of imaging.\n\nThe acousto-optic effect is a specific case of photoelasticity, where there is a change of a material's permittivity, formula_1, due to a mechanical strain formula_2. Photoelasticity is the variation of the optical indicatrix coefficients formula_3 caused by the strain formula_4 given by,\n\nwhere formula_6 is the photoelastic tensor with components, formula_7,formula_8 = 1,2,…,6.\n\nSpecifically in the acousto-optic effect, the strains formula_4 are a result of the acoustic wave which has been excited within a transparent medium. This then gives rise to the variation of the refractive index. For a plane acoustic wave propagating along the z axis, the change in the refractive index can be expressed as,\n\nwhere formula_11 is the undisturbed refractive index, formula_12 is the angular frequency, formula_13 is the wavenumber of the acoustic wave, and formula_14 is the amplitude of variation in the refractive index generated by the acoustic wave, and is given as,\n\nThe generated refractive index, (2), gives a diffraction grating moving with the velocity given by the speed of the sound wave in the medium. Light which then passes through the transparent material, is diffracted due to this generated refraction index, forming a prominent diffraction pattern. This diffraction pattern corresponds with a conventional diffraction grating at angles formula_16 from the original direction, and is given by,\n\nwhere formula_18 is the wavelength of the optical wave, formula_19 is the wavelength of the acoustic wave and formula_20 is the integer order maximum.\n\nLight diffracted by an acoustic wave of a single frequency produces two distinct diffraction types. These are Raman-Nath diffraction and Bragg diffraction.\n\nRaman-Nath diffraction is observed with relatively low acoustic frequencies, typically less than 10 MHz, and with a small acousto-optic interaction length, ℓ, which is typically less than 1 cm. This type of diffraction occurs at an arbitrary angle of incidence, formula_21.\n\nIn contrast, Bragg diffraction occurs at higher acoustic frequencies, usually exceeding 100 MHz. The observed diffraction pattern generally consists of two diffraction maxima; these are the zeroth and the first orders. However, even these two maxima only appear at definite incidence angles close to the Bragg angle, formula_22. The first order maximum or the Bragg maximum is formed due to a selective reflection of the light from the wave fronts of ultrasonic wave. The Bragg angle is given by the expression,\n\nwhere formula_18 is the wavelength of the incident light wave (in a vacuum), formula_25 is the acoustic frequency, formula_26 is the velocity of the acoustic wave, formula_27 is the refractive index for the incident optical wave, and formula_28 is the refractive index for the diffracted optical waves.\n\nIn general, there is no point at which Bragg diffraction takes over from Raman-Nath diffraction. It is simply a fact that as the acoustic frequency increases, the number of observed maxima is gradually reduced due to the angular selectivity of the acousto-optic interaction. Traditionally, the type of diffraction, Bragg or Raman-Nath, is determined by the conditions \"Q\" » 1 and \"Q\" « 1 respectively, where Q is given by,\n\nwhich is known as the Klein-Cook parameter. Since, in general, only the first order diffraction maximum is used in acousto-optic devices, Bragg diffraction is preferable due to the lower optical losses. However, the acousto-optic requirements for Bragg diffraction limit the frequency range of acousto-optic interaction. As a consequence, the speed of operation of acousto-optic devices is also limited.\n\nThree categories of acousto-optic devices will be discussed. They include the acousto-optic modulator, filter and deflector.\n\nBy varying the parameters of the acoustic wave, including the amplitude, phase, frequency and polarization, properties of the optical wave may be modulated. The acousto-optic interaction also makes it possible to modulate the optical beam by both temporal and spatial modulation.\n\nA simple method of modulating the optical beam travelling through the acousto-optic device is done by switching the acoustic field on and off. When off the light beam is undiverted, the intensity of light directed at the Bragg diffraction angle is zero. When switched on and Bragg diffraction occurs, the intensity at the Bragg angle increases. So the acousto-optic device is modulating the output along the Bragg diffraction angle, switching it on and off. The device is operated as a modulator by keeping the acoustic wavelength (frequency) fixed and varying the drive power to vary the amount of light in the deflected beam.\n\nThere are several limitations associated with the design and performance of acousto-optic modulators. The acousto-optic medium must be designed carefully to provide maximum light intensity in a single diffracted beam. The time taken for the acoustic wave to travel across the diameter of the light beam gives a limitation on the switching speed, and hence limits the modulation bandwidth. The finite velocity of the acoustic wave means the light cannot be fully switched on or off until the acoustic wave has traveled across the light beam. So to increase the bandwidth the light must be focused to a small diameter at the location of the acousto-optic interaction. This minimum focused size of the beam represents the limit for the bandwidth.\n\nThe principle behind the operation of acousto-optic filters is based on the wavelength of the diffracted light being dependent on the acoustic frequency. By tuning the frequency of the acoustic wave, the desired wavelength of the optical wave can be diffracted acousto-optically.\n\nThere are two types of the acousto-optic filters, the collinear and non-collinear filters. The type of filter depends on geometry of acousto-optic interaction.\n\nThe polarization of the incident light can be either ordinary or extraordinary. For the definition, we assume ordinary polarization. Here the following list of symbols is used,\n\nformula_30: the angle between the acoustic wave vector and the crystallographic axis \"z\" of the crystal;\n\nformula_31: the wedge angle between the input and output faces of the filter cell (the wedge angle is necessary for eliminating the angular shift of the diffracted beam caused by frequency changing);\n\nformula_32: the angle between the incident light wave vector and [110] axis of the crystal;\n\nformula_33: the angle between the input face of the cell and acoustic wave vector;\n\nformula_34: the angle between deflected and non-deflected light at the central frequency;\n\nformula_35: the transducer length. \nThe incidence angle formula_32 and the central frequency formula_37 of the filter are defined by the following set of equations,\n\nRefractive indices of the ordinary (formula_11) and extraordinary (formula_41) polarized beams are determined by taking into account their dispersive dependence.\n\nThe sound velocity, formula_26, depends on the angle α, such that,\n\nformula_44 and formula_45 are the sound velocities along the axes [110] and [001], consecutively. The value of formula_46 is determined by the angles formula_32 and formula_30,\n\nThe angle formula_34 between the diffracted and non-diffracted beams defines the view field of the filter; it can be calculated from the formula,\n\nInput light need not be polarized for a non-collinear design. Unpolarized input light is scattered into orthogonally polarized beams separated by the scattering angle for the particular design and wavelength. If the optical design provides an appropriate beam block for the unscattered light, then two beams (images) are formed in an optical passband that is nearly equivalent in both orthogonally linearly polarized output beams (differing by the Stokes and Anti-Stokes scattering parameter). Because of dispersion, these beams move slightly with scanning rf frequency.\n\nAn acousto-optic deflector spatially controls the optical beam. In the operation of an acousto-optic deflector the power driving the acoustic transducer is kept on, at a constant level, while the acoustic frequency is varied to deflect the beam to different angular positions. The acousto-optic deflector makes use of the acoustic frequency dependent diffraction angle, where a change in the angle formula_52 as a function of the change in frequency formula_53 is given as,\n\nwhere formula_18 is the optical wavelength of the beam and formula_56 is the velocity of the acoustic wave.\n\nAOD technology has made practical the Bose–Einstein condensation for which the 2001 Nobel Prize in Physics was awarded to Eric A. Cornell, Wolfgang Ketterle and Carl E. Wieman. Another application of acoustic-optical deflection is optical trapping of small molecules.\n\nAODs are essentially the same as acousto-optic modulators (AOMs). In an AOM, only the amplitude of the sound wave is modulated (to modulate the intensity of the diffracted laser beam), whereas in an AOD, both the amplitude and frequency are adjusted, making the engineering requirements tighter for an AOD than an AOM.\n\nSome materials displaying acousto-optic effect include fused silica,lithium niobate, arsenic trisulfide, tellurium dioxide and tellurite glasses, lead silicate, GeAsS, mercury(I) chloride, lead(II) bromide, and other materials.\n\n"}
{"id": "37205291", "url": "https://en.wikipedia.org/wiki?curid=37205291", "title": "Aesthetics of nature", "text": "Aesthetics of nature\n\nAesthetics of nature is a sub-field of philosophical ethics, and refers to the study of natural objects from their aesthetical perspective.\n\nAesthetics of nature developed as a sub-field of philosophical ethics. In the 18th and 19th century, the aesthetics of nature advanced the concepts of disinterestedness, the pictures, and the introduction of the idea of positive aesthetics. The first major developments of nature occurred in the 18th century. The concept of disinterestedness had been explained by many thinkers. Anthony Ashley-Cooper introduced the concept as a way of characterizing the notion of the aesthetic, later magnified by Francis Hutcheson, who expanded it to exclude personal and utilitarianism interests and associations of a more general nature from aesthetic experience. This concept was further developed by Archibald Alison who referred it to a particular state of mind.\n\nThe theory of disinterestedness opened doors for a better understanding of the aesthetics dimensions of nature in terms of three conceptualizations: \n\nObjects experienced as beautiful tend to be small, smooth, and fair in color. In contrast, objects viewed as sublime tend to be powerful, intense and terrifying. Picturesque items are a mixture of both, which can be seen as varied and irregular, rich and forceful, and even vibrant.\n\nCognitive and non-cognitive approaches of nature have directed their focus from natural environments to the consideration of human and human influenced environments and developed aesthetic investigations of everyday life.(Carlson and Lintott, 2007; Parsons 2008a; Carlson 2010)\n\nPeople may be mistaken by the art object analogy. For instance, a sandhill crane is not an art object; an art object is not a sandhill crane. In fact, an art object should be called an \"artifact\". The crane is wildlife on its own and is not an art object. This can be related to Satio's definition of the cognitive view. In elaboration, the crane lives through various ecosystems such as Yellowstone. Nature is a living system which includes animals, plants, and Eco-systems. In contrast, an art object has no regeneration, evolutionary history, or metabolism. An individual may be in the forest and perceive it as beautiful because of the plethora of colors such as red, green, and yellow. This is a result of the chemicals interacting with chlorophyll. An individual's aesthetic experience may increase; however, none of the things mentioned have anything to do with what is really going on in the forest. The chlorophyll is capturing solar energy and the residual chemicals protect the trees from insect grazing.\n\nAny color perceived by human visitors for a few hours is entirely different from what is really happening. According to Leopold, the three features of ecosystems that generate land ethic are integrity, stability and beauty. None of the mentioned features are real in nature. Ecosystems are not stable: they are dramatically changing and they have little integration; ergo, beauty is in the eye of the beholder.\n\nIn a Post-Modern approach, when an individual engages in aesthetically appreciating a natural thing, we give meaning to the thing we appreciate and in that meaning, we express and develop our own attitudes, values and beliefs. Our interest in natural things are not only a passive reflection of our inclinations, as Croce describes as the appreciation of nature as looking in a mirror, or what we might call our inward life; but may instead be the things we come across in nature that engage and stimulate our imagination. As a result, we are challenged to think differently and apply thoughts and associations to in new situations and ways.\nAs a characterization of the appreciation of art, nature aestheticists argue that post modernism is a mistaken view because we do not have a case of anything goes.The aesthetics appreciation of art is governed by some normative standards. In the world of art, criticism may take place when people come together and discuss books and films or critics write appraisals for publications. On the contrary, there are not obvious instances of debate and appraisals where different judgments about the aesthetics of character of nature are evaluated.\n"}
{"id": "48476957", "url": "https://en.wikipedia.org/wiki?curid=48476957", "title": "Afella", "text": "Afella\n\nAfella is a Mountain located in the Western High-Atlas range. Its highest peak is at 4043 metres.\n"}
{"id": "1396650", "url": "https://en.wikipedia.org/wiki?curid=1396650", "title": "Anticyclonic storm", "text": "Anticyclonic storm\n\nAn anticyclonic storm is a weather storm where winds around the storm flow in the direction opposite to that of the flow about a region of low pressure. \n\nIn the Northern Hemisphere, anticyclonic storms involve clockwise wind flow; in the Southern Hemisphere, they involve counterclockwise wind flow.\n\nAnticyclonic storms usually form around high-pressure systems. These do not \"contradict\" the Coriolis effect; it predicts such anticyclonic flow about high-pressure regions. Anticyclonic storms, as high-pressure systems, usually accompany cold weather and are frequently a factor in large snowstorms. Jupiter's Great Red Spot is a well-known non-terrestrial example of an anticyclonic system.\n\nAnticyclonic tornadoes often occur; while tornadoes' vortices are low-pressure regions, this occurs because tornadoes occur on a small enough scale such that the Coriolis effect is negligible.\n\n\n"}
{"id": "418257", "url": "https://en.wikipedia.org/wiki?curid=418257", "title": "Araucaria bidwillii", "text": "Araucaria bidwillii\n\nAraucaria bidwillii, the bunya pine, is a large evergreen coniferous tree in the plant family Araucariaceae. It is found naturally in south-east Queensland Australia and two small disjunct populations in north eastern Queensland's World Heritage listed Wet Tropics. There are many old planted specimens in New South Wales, and around the Perth, Western Australia metropolitan area. They can grow up to . The tallest presently living is one in Bunya Mountains National Park, Queensland which was reported by Robert Van Pelt in January 2003 to be in height.\n\nThe bunya pine is the last surviving species of the Section \"Bunya\" of the genus \"Araucaria\". This section was diverse and widespread during the Mesozoic with some species having cone morphology similar to \"A. bidwillii\", which appeared during the Jurassic. Fossils of Section \"Bunya\" are found in South America and Europe. The scientific name honours the botanist John Carne Bidwill, who came across it in 1842 and sent the first specimens to Sir William Hooker in the following year.\n\nNative in Queensland, historically trees were found in populations recorded as abundant and widespread in suitable habitats of South East Queensland and Wide Bay-Burnett (regions). In these regions of Queensland the natural ecosystems growing Bunya Pines have sustained European agricultural occupation and have been fragmented now into the areas of the Blackall Range, Bunya Mountains, upper Brisbane River reaches and upper Mary River valley. Natural ecosystems having Bunya pines are found again approximately to the north, in the wet tropics region of north eastern Queensland. There the species natural populations are rare and restricted. Two outlying restricted populations are known in the Cannabullen Falls and Mt. Lewis areas.\n\n\"A. bidwillii\" has a limited distribution within Australia in part because of the drying out of Australia with loss of rainforest and poor seed dispersal. The remnant sites at the Bunya Mountains and Mount Lewis in Queensland have genetic diversity. The cones are large, soft-shelled and nutritious and fall intact to the ground beneath the tree before dehiscing. The suggestion that extinct large animals – perhaps dinosaurs and later, large mammals – may have been dispersers for the Bunya is reasonable, given the seeds' size and energy content, but difficult to confirm given the incompleteness of the fossil record for coprolites.\n\nAt the start of European occupation, \"A. bidwillii\" occurred in great abundance in southern Queensland, to the extent that a Bunya Bunya Reserve was declared in 1840 to protect its habitat. The tree once grew as large groves or sprinkled regularly as an emergent species throughout other forest types on the Upper Stanley and Brisbane Rivers, Sunshine Coast hinterland (especially the Blackall Range near Montville and Maleny), and also towards and on the Bunya Mountains. Today, the species is usually encountered as very small groves or single trees in its former range, except on and near the Bunya Mountains, where it is still fairly prolific.\n\n\"A. bidwillii\" has unusual cryptogeal seed germination in which the seeds develop to form an underground tuber from which the aerial shoot later emerges. The actual emergence of the seed is then known to occur over several years presumably as a strategy to allow the seedlings to emerge under optimum climatic conditions or, it has been suggested, to avoid fire. This erratic germination has been one of the main problems in silviculture of the species.\n\nThe cones are in diameter, and can weigh as much as 18 kg (40 lb) and are opened by large birds, such as cockatoos, or disintegrate when mature to release the large seeds or nuts.\n\nAlthough there are no reported dispersal agents for the seeds of \"A. bidwillii\", macropods and various species of rats are known as predators of the seeds and tubers. The bush rat (\"Rattus fuscipes\") was observed caching bunya seeds some distance uphill from parent trees, possibly allowing ridge-top germination. Brushtail possums (\"Trichosurus spp.)\" were mentioned as carrying the seeds up trees. In a study in 2006, the short-eared possum (\"Trichosurus caninus\") was shown to disperse the seed of \"A. bidwillii\".\n\nNatural populations of this species have been reduced in extent and abundance through exploitation for its timber, the construction of dams and historical clearing. Most populations are now protected in formal reserves and national parks.\n\nA recent problem in small forestry plantations of \"A. bidwilli\" in Southeast Queensland is the introduction of red deer (\"Cervus elaphus\"). Red deer, unlike possums and rodents, eat bunya cones while still intact, preventing their dispersal.\n\nThe \"bunya\", \"bonye\", \"bunyi\" or \"bunya-bunya\" in various Australian Aboriginal languages was colloquially named the Bunya Pine by Europeans. However, \"Araucaria bidwillii\" is not a pine tree (of the genus \"Pinus\"). It belongs to the same genus as the monkey puzzle tree (\"Araucaria araucana\") and is commonly referred to as the \"false monkey puzzle\".\n\nThe Bunya tree grows to a height of 30–45 metres, and the cones, which contain the edible kernels, are the size of footballs.\n\nThe ripe cones fall to the ground. Each segment contains a kernel in a tough protective shell, which will split when boiled or put in a fire. The flavour of the kernel is similar to a chestnut.\n\nA Bunya festival was recorded by Thomas (Tom) Petrie (1831–1910), who went with the Aboriginal people of Brisbane at the age of 14 to the festival at the Bunya Range (now the Blackall Range in the hinterland area of the Sunshine Coast). His daughter, Constance Petrie, put down his stories in which he said that the trees fruited at three-year intervals. The three-year interval may not be correct. Ludwig Leichhardt wrote in 1844 of his expedition to the Bunya feast. \n\nThe 1889 book 'The Useful Native Plants of Australia' records that \"The cones shed their seeds, which are two to two and a-half inches long by three-quarters of an inch broad ; they are sweet before being perfectly ripe, and after that resemble roasted chestnuts in taste. They are plentiful once in three years, and when the ripening season arrives, which is generally in the month of January. \n\nThe Bunya trees pollinate in South East Queensland in September, October and the cones fall seventeen to eighteen months later in late January to early March from the coast to the current Bunya Mountains. When there is heavy rainfall or drought, pollination may vary. The large festival harvests may vary between two and seven years. When the fruit was ripe, the people of the region would set aside differences and gather in the Bon-yi Mountains (Bunya Mountains) to feast on the kernels.\n\nAs the fruit ripened, locals, who were bound by custodial obligations and rights, sent\nout messengers to invite people from hundreds of kilometres to meet at specific sites. The meetings involved ceremonies, dispute settlements and fights, marriage arrangements and the trading of goods. The Aborigines' fierce protection of the trees and recognition of the value of the timber, led to colonial authorities prohibiting settlers from cutting the trees in the 1842. The resource was too valuable, and the aboriginals were driven out of the forests along with the ability to run the festivals. The forests were felled for timber and cleared to make way for cultivation.\n\nIn what was probably Australia's largest indigenous event, diverse tribes – up to thousands of people – once travelled great distances (from as far as Charleville, Dubbo, Bundaberg and Grafton) to the gatherings. They stayed for months, to celebrate and feast on the bunya nut. The bunya gatherings were an armistice accompanied by much trade exchange, and discussions and negotiations over marriage and regional issues. Due to the sacred status of the bunyas, some tribes would not camp amongst these trees. Also in some regions, the tree was never to be cut.\n\nIndigenous groups such as the Wakawaka, Githabul, Kabi Kabi, Jarowair, Goreng goreng, Butchulla, Quandamooka, Baruŋgam , Yiman and Wulili have continued cultural and spiritual connections to the Bunya Mountains to this day, a number of strategies including the use of traditional ecological knowledge have been incorporated into the current management practices of the national park and conservation reserves with the Bunya Murri Ranger project currently operating in the mountains.\n\nIndigenous Australians eat the nut of the bunya tree both raw and cooked (roasted, and in more recent times boiled), and also in its immature form. Traditionally, the nuts were additionally ground and made into a paste, which was eaten directly or cooked in hot coals to make bread. The nuts were also stored in the mud of running creeks, and eaten in a fermented state. This was considered a delicacy.\n\nApart from consuming the nuts, indigenous Australians ate bunya shoots, and utilised the tree's bark as kindling.\n\nBunya nuts are still sold as a regular food item in grocery stalls and street-side stalls around rural southern Queensland. Some farmers in the Wide Bay/ Sunshine Coast regions have experimented with growing bunya trees commercially for their nuts and timber.\n\nSince the mid-1990s, the Australian company Maton has used bunya for the soundboards of its BG808CL Performer acoustic guitars. The Cole Clark company (also Australian) uses bunya for the majority of its acoustic guitar soundboards. The timber is valued by cabinet makers and woodworkers, and has been used for that purpose for over a century.\n\nHowever, its most popular use is as a 'bushfood' by indigenous foods enthusiasts. A huge variety of home-invented recipes now exists for the bunya nut; from pancakes, biscuits and breads, to casseroles, to 'bunya nut pesto' or hoummus. The nut is considered nutritious, with a unique flavour similar to starchy potato and chestnut.\n\nWhen the nuts are boiled in water, the water turns red, making a flavoursome tea.\n\nThe nutritional content of the bunya nut is: 40% water, 40% complex carbohydrates, 9% protein, 2% fat, 0.2% potassium, 0.06% magnesium. It is also gluten free, making bunya nut flour a substitute for people with gluten intolerance.\n\nBunya nuts are slow to germinate. A set of 12 seeds sown in Melbourne took an average of about six months to germinate (with the first germinating in 3 months) and only developed roots after 1 year. The first leaves form a rosette and are dark brown. The leaves only turn green once the first stem branch occurs. Unlike the mature leaves, the young leaves are relatively soft. As the leaves age they become very hard and sharp. Cuttings can be successful, though they must be taken from erect growing shoots, as cuttings from side shoots will not grow upright.\n\nIn the highly variable Australian climate, the spread of actual emergence of the bunya maximises the possibility of at least successful replacement of the parent tree. A test of germination was carried out by Smith starting in 1999. Seeds were extracted from two mature cones collected from the same tree, a cultivated specimen at Petrie, just north of Brisbane (originally the homestead of Thomas Petrie, the son of the first European to report the species). One hundred apparently full seeds were selected and planted into 30 cm by 12 cm plastic tubes commercially filled with sterile potting mix in early February 1999. These were then placed in a shaded area and watered weekly. Four tubes were lost due to being knocked over. Of a total of 100 seeds placed, 87 germinated. The tubes were checked monthly for emergence over 3 years. Of these seeds, 55 emerged from April to December, 1999; 32 emerged from January to September in 2000, 1 seed emerged in January 2001, and the last 1 appeared in February 2001.\nOnce established bunyas are quite hardy and can be grown as far south as Hobart in Australia (42° S) and Christchurch in New Zealand (43° S) and (at least) as far north as Sacramento in California (38° N) and Lisbon (in the botanical garden) and even in Dublin area in Ireland (53ºN) in a microclimate protected from arctic winds and moderated by the Gulf Stream. They will reach a height of 35 to 40 metres, and live for about 500 years.\n\n"}
{"id": "5787655", "url": "https://en.wikipedia.org/wiki?curid=5787655", "title": "British Entomological and Natural History Society", "text": "British Entomological and Natural History Society\n\nThe British Entomological and Natural History Society or BENHS is a British entomological society. It is based at Dinton Pastures Country Park in Reading.\n\nBENHS was founded in 1872 as the South London Entomological and Natural History Society.\n\nBENHS publishes a quarterly journal, the British Journal of Entomology and Natural History (), formally Proceedings and Transactions of the British Entomological and Natural History Society, and Proceedings and Transactions of the South London Entomological and Natural History Society.\n\nBENHS has published a number of books. Among the most well-known are two illustrated identification guides to British flies:\n\nAnother title published by BENHS was \"New British Beetles - species not in Joy's practical handbook\" by Peter J. Hodge and Richard A. Jones, a companion volume to Norman H. Joy's \"A Practical Handbook of British Beetles.\n\nThe following groups are affiliated to BENHS:\n\n"}
{"id": "6017710", "url": "https://en.wikipedia.org/wiki?curid=6017710", "title": "Ciudad Encantada", "text": "Ciudad Encantada\n\nThe Ciudad Encantada (English: Enchanted City) is a geological site near the city of Cuenca, in the autonomous community of Castilla-La Mancha, Spain in which the erosive forces of weather and the waters of the nearby Júcar river have formed rocks into distinctive and memorable shapes.\n\nIt was declared a Natural Site of National Interest on 11 June 1929.\n\nThe rock formations of Ciudad Encantada are karst formations made of limestone and dolomite, which date back to the Cretaceous period, approximately 90 million years ago. Rain falling on the original limestone plateau wore down the porous limestone, leaving behind the more resistant dolomite. Because the dolomite was not always distributed evenly in the original rock, the result was the irregularly eroded shapes that form the Ciudad Encantada.\n\nThe rock formations that have been named include:\n\nCiudad Encantada appears as a location in the following films:\n\n\n"}
{"id": "13677019", "url": "https://en.wikipedia.org/wiki?curid=13677019", "title": "Cold drop", "text": "Cold drop\n\nThe cold drop () is a weather phenomenon often occurring in the Spanish autumn. It is experienced particularly along the western Mediterranean and as such, most frequently affects the east coast of Spain. It is a closed upper-level low which has become completely displaced (cut off) from basic westerly current, and moves independently of that current. Cutoff lows may remain nearly stationary for days, or on occasion may move westward opposite to the prevailing flow aloft (i.e., retrogression). The term is also used to describe the meteorological phenomenon associated. In Spain, it appears when a front of very cold polar air, a jet stream, advances slowly over Western Europe, at high altitude (normally 5–9 km or 3–5.5 mi).\n\nIf a sudden cut off in the stream takes place, caused by various reasons, like the effect of the high pressures, a pocket of cold air detaches from the main jet stream, penetrating to the south over the Pyrenees into the warm air in Spain, causing its most dramatic effects in the Southeast of Spain, particularly along the Spanish Mediterranean coast, especially in the Valencian Community.\n\nThis phenomenon is associated with extremely violent downpours and storms, with speeds of 100–200 km (60–120 mi)/hour, but not always accompanied by significant rainfall. For this it is necessary that the high atmospheric torrential rain instability in the lower air layers to combine with a significant amount of water vapors. Such a combination causes the masses of cold air to rapidly discharge up to 500 liters per square meter in extremely rapid rain episodes. This phenomenon usually lasts a very short time, (from a few hours to a maximum of four days) as it exhausts its water reserves without receiving a new supply.\n\nThe clouds are formed in the Atlantic Ocean. The more extreme the difference in temperature, the more water is stored in the clouds. The Cold Drop can produce snow or hail.\n\nThis way a great mass of cold air rotates and floats like a drop over a warm area.\n\nThe torrential rain caused by cold drop can result in devastation caused by torrents and flash floods. For instance, the great Valencia flood of 1957 was the result of a 3-day-long cold drop.\n\nA sudden rain over Valencia is a plot element in the 2016 Spanish thriller \"Cien años de perdón\".\n\n\n"}
{"id": "19406460", "url": "https://en.wikipedia.org/wiki?curid=19406460", "title": "Cubic mile of oil", "text": "Cubic mile of oil\n\nThe cubic mile of oil (CMO) is a unit of energy. It was created by Hew Crane of SRI International to aid in public understanding of global-scale energy consumption and resources.\n\nSignificant sources of energy include oil, coal, natural gas, nuclear, hydroelectric, and biomass (primarily the burning of wood). Other energy sources include geothermal, wind, photovoltaic, and solar thermal. The various energy units commonly used to measure these sources (e.g., joules, BTUs, kilowatt hours, therms) are only somewhat familiar to the general public, and their relationships can be confusing. These common energy units are sized for everyday activities (a joule is the energy required to lift a small apple one metre vertically). For regional, national, and global scales, larger energy units, such as the exajoule, the billion barrels of oil equivalent (BBOE) and the quad are used. Derived by multiplying the small common units by large powers of ten these larger units pose additional conceptual difficulties for many citizens.\n\nCrane intended the cubic mile of oil to provide a visualizable scale for comparing the contributions of these diverse energy components as a percentage of total worldwide, energy use.\n\nThe global economy consumes approximately 30 billion barrels of oil (1.26 trillion U.S. gallons or 4.75 trillion litres) each year. Numbers of this magnitude are difficult to conceive by most people. The volume occupied by one trillion U.S. gallons is about one cubic mile. Crane felt that a cubic mile would be an easier concept for the general public than a trillion gallons.\n\nThe CMO is the energy released by burning a cubic mile of oil. Conversions to other units may be calculated based on the barrel of oil equivalent (BOE), an approximation of the energy released by burning one 42-US-gallon barrel of crude oil. Since one BOE is about and one cubic mile is about :\n\nThe world consumes approximately 3 CMO annually from all sources. The table\n\nA CMO/yr is about 5.084 TW continuous, making current world energy use around 15 TW.\n\nProved oil reserves are those that can be extracted with reasonable certainty under existing conditions using existing technology. Global proved oil reserves are estimated at approximately . This corresponds to roughly 43 cubic miles, or 43 CMO. At the current rate of use, this would last about 40 years. Technological advances, new discoveries, and political changes will likely lead to additional proved oil reserves in the future. Concurrently, the International Energy Agency predicted in its 2005 World Energy Outlook that the annual consumption will increase by 50% by 2030. Coal and natural gas currently provide 1.42 CMO of energy per year. Global reserves of these fossil resources are as follows:\n\nWhile oil has many other important uses (lubrication, plastics, roadways, roofing) this section considers only its use as an energy source.\n\nThe CMO is a powerful means of understanding the difficulty of replacing oil energy by other sources. SRI International chemist Ripudaman Malhotra, working with Crane and colleague Ed Kinderman, used it to describe the looming energy crisis in sobering terms. Malhotra illustrates the problem of producing one CMO energy that we currently derive from oil each year from five different alternative sources. Installing capacity to produce 1 CMO per year requires long and significant development.\n\nAllowing fifty years to develop the requisite capacity, 1 CMO of energy per year could be produced by any one of these developments:\nThe energy produced is the power rating of the source multiplied by the duration it is operational. These comparisons take into account the variability of available power (solar panels work only during the day, turbines work only when the wind blows). Also, whereas 1 kWh is equivalent to 3412 BTU of primary energy, in practice it takes closer to 10,000 BTU to produce 1 kWh of electricity from coal and other fossil sources. Thus, when considering sources such as wind and solar which directly produce electricity, the required installed capacity was calculated by using 1 kWh as equivalent to 10,000 BTU.\n\nThe environmental, social, and financial costs of such development projects are immense:\n\nFor comparison, US$3.2 trillion is the approximate gross domestic product of Germany, China, or the United Kingdom. The total land area of New Zealand is approximately .\n\nAt a 2008 market price of US$120 per barrel (US$750/m), the cost of one CMO was about US$3 trillion. So for the cost of about one year's global oil consumption at 2008 market prices, enough wind turbines could be built to generate the same energy for 40 years, assuming sites are available.\n\nSpace-based solar power offers one way StratoSolar offers another. At ~5 TW per CMO/yr, it would take about 1000 five GW power satellites to replace a CMO. To compete with coal, the maximum cost for space based solar power plants would be $2.4 B/GW. At that, the cost would be ~$2.4 T for one CMO or ~$7.2 T to replace the entire fossil fuel use of humans. There is room in GEO for more than ten times the current energy use. At a peak production of 315 new power satellites per year, it would take less than a decade to get off fossil fuels.\n\nStratoSolar has the advantage that solar can be tapped at 20 km regardless of the local weather.\n\nIn both cases, a huge energy flow to carbon neutral synthetic fuel plants would be required to replace the oil used for transportation. The technology for making synthetic transport fuels is well understood. See Oryx GTL\n\n"}
{"id": "50482513", "url": "https://en.wikipedia.org/wiki?curid=50482513", "title": "Cultural evolution", "text": "Cultural evolution\n\nCultural evolution is an evolutionary theory of social change. It follows from the definition of culture as \"information capable of affecting individuals' behavior that they acquire from other members of their species through teaching, imitation and other forms of social transmission\". Cultural evolution is the change of this information over time.\n\nCultural evolution, historically also known as sociocultural evolution, was originally developed in the 19th century by anthropologists stemming from Charles Darwin's research on evolution. Today, cultural evolution has become the basis for a growing field of scientific research in the social sciences, including anthropology, economics, psychology and organizational studies. Previously, it was believed that social change resulted from biological adaptations, but anthropologists now commonly accept that social changes arise in consequence of a combination of social, evolutionary and biological influences.\n\nThere have been a number of different approaches to the study of cultural evolution, including dual inheritance theory, sociocultural evolution, memetics, cultural evolutionism and other variants on cultural selection theory. The approaches differ not just in the history of their development and discipline of origin but in how they conceptualize the process of cultural evolution and the assumptions, theories and methods that they apply to its study. In recent years, there has been a convergence of the cluster of related theories towards seeing cultural evolution as a unified discipline in its own right.\n\nAristotle thought that development of cultural form (such as poetry) stops when it reaches its maturity. In 1873 in \"Harper's New Monthly Magazine\", it was written: \"By the principle which Darwin describes as natural selection short words are gaining the advantage over long words, direct forms of expression are gaining the advantage over indirect, words of precise meaning the advantage of the ambiguous, and local idioms are everywhere in disadvantage\".\n\nCultural evolution, in the Darwinian sense of variation and selective inheritance, could be said to trace back to Darwin himself. He argued for both customs (1874 p. 239) and \"inherited habits\" as contributing to human evolution, grounding both in the innate capacity for acquiring language.\n\nDarwin's ideas, along with those of such as Comte and Quetelet, influenced a number of what would now be called social scientists in the late nineteenth and early twentieth centuries. Hodgson and Knudsen single out David George Ritchie and Thorstein Veblen, crediting the former with anticipating both dual inheritance theory and universal Darwinism. Despite the stereotypical image of social Darwinism that developed later in the century, neither Ritchie nor Veblen were on the political right.\n\nThe early years of the 20th century and particularly the First World War saw biological concepts and metaphors shunned by most social sciences. Even uttering the word \"evolution\" carried \"serious risk to one's intellectual reputation.\" Darwinian ideas were also in decline following the rediscovery of Mendelian genetics but were revived, especially by Fisher, Haldane and Wright, who developed the first population genetic models and as it became known the modern synthesis.\n\nCultural evolutionary concepts, or even metaphors, revived more slowly. If there was one influential individual in the revival it was probably Donald T. Campbell. In 1960 he drew on Wright to draw a parallel between genetic evolution and the \"blind variation and selective retention\" of creative ideas; work that was developed into a full theory of \"socio-cultural evolution\" in 1965 (a work that includes references to other works in the then current revival of interest in the field. Campbell (1965 26) was clear that he understood cultural evolution not as an analogy \"from organic evolution per se, but rather from a general model for quasiteleological processes for which organic evolution is but one instance\".\n\nOthers pursued more specific analogies notably the anthropologist F. T. (Ted) Cloak who argued in 1975 for the existence of learnt cultural instructions (cultural corpuscles or i-culture) resulting in material artefacts (m-culture) such as wheels. The argument thereby introduced as to whether cultural evolution requires neurological instructions continues to the present day .\n\nIn the 19th century cultural evolution was thought to follow a unilineal pattern whereby all cultures progressively develop over time. The underlying assumption being that Cultural Evolution itself led to the growth and development of civilization \n\nThomas Hobbes in the 17th Century declared indigenous culture to have \"no arts, no letters, no society\" and he described facing life as \"solitary, poor, nasty, brutish, and short.\" He, like other scholars of his time, reasoned that everything positive and esteemed resulted from the slow development away from this poor lowly state of being.\n\nUnder the theory of unilinear Cultural Evolution, all societies and cultures develop on the same path. The first to present a general unilineal theory was Herbert Spencer. Spencer suggested that humans develop into more complex beings as culture progresses, where people originally lived in \"undifferentiated hordes\" culture progresses and develops to the point where civilization develops hierarchies. The concept behind unilinear theory is that the steady accumulation of knowledge and culture leads to the separation of the various modern day sciences and the build-up of cultural norms present in modern-day society \n\nIn Lewis H. Morgan's book \"Ancient Society\" (1877), Morgan labels seven differing stages of human culture: lower, middle, and upper savagery; lower, middle, and upper barbarism; and civilization. He justifies this staging classification by referencing societies whose cultural traits resembled those of each of his stage classifications of the cultural progression. Morgan gave no example of lower savagery, as even at the time of writing few examples remained of this cultural type. At the time of expounding his theory, Morgan's work was highly respected and became a foundation for much of anthropological study that was to follow.\n\nThere began a widespread condemnation of unilinear theory in the late 19th century. Unilinear cultural evolution implicitly assumes that culture was borne out of the United States and Western Europe. That was seen by many to be racist, as it assumed that some individuals and cultures were more evolved than others.\n\nFranz Boas, a German-born anthropologist, was the instigator of the movement known as 'cultural particularism' in which the emphasis shifted to a multilinear approach to cultural evolution. That differed to the unilinear approach that used to be favoured in the sense that cultures were no longer compared, but they were assessed uniquely. Boas, along with several of his pupils, notably A.L. Kroeber, Ruth Benedict and Margaret Mead, changed the focus of anthropological research to the effect that instead of generalizing cultures, the attention was now on collecting empirical evidence of how individual cultures change and develop.\n\nCultural particularism dominated popular thought for the first half of the 20th century before American anthropologists, including Leslie A. White, Julian H. Steward, Marshall D. Sahlins, and Elman R. Service, revived the debate on cultural evolution. These theorists were the first to introduce the idea of multilinear cultural evolution.\n\nUnder multilinear theory, there are no fixed stages (as in unilinear theory) towards cultural development. Instead, there are several stages of differing lengths and forms. Although, individual cultures develop differently and cultural evolution occurs differently, multilinear theory acknowledges that cultures and societies do tend to develop and move forward.\n\nLeslie A. White focused on the idea that different cultures had differing amounts of 'energy', White argued that with greater energy societies could possess greater levels of social differentiation. He rejected separation of modern societies from primitive societies. In contrast, Steward argued, much like Darwin's theory of evolution, that culture adapts to its surroundings. 'Evolution and Culture' by Sahlins and Service is an attempt to condense the views of White and Steward into a universal theory of multilinear evolution.\n\nRichard Dawkins' 1976 book \"The Selfish Gene\" proposed the concept of the \"meme\", which is analogous to that of the gene. A meme is an idea-replicator that can reproduce itself, by jumping from mind to mind via the process of one human learning from another via imitation. Along with the \"virus of the mind\" image, the meme might be thought of as a \"unit of culture\" (an idea, belief, pattern of behaviour, etc.), which spreads among the individuals of a population. The variation and selection in the copying process enables Darwinian evolution among memeplexes and therefore is a candidate for a mechanism of cultural evolution. As memes are \"selfish\" in that they are \"interested\" only in their own success, they could well be in conflict with their biological host's genetic interests. Consequently, a \"meme's eye\" view might account for certain evolved cultural traits, such as suicide terrorism, that are successful at spreading meme of martyrdom, but fatal to their hosts and often other people.\n\n\"Evolutionary epistemology\" can also refer to a theory that applies the concepts of biological evolution to the growth of human knowledge and argues that units of knowledge themselves, particularly scientific theories, evolve according to selection. In that case, a theory, like the germ theory of disease, becomes more or less credible according to changes in the body of knowledge surrounding it.\n\nEvolutionary epistemology is a naturalistic approach to epistemology, which emphasizes the importance of natural selection in two primary roles. In the first role, selection is the generator and maintainer of the reliability of our senses and cognitive mechanisms, as well as the \"fit\" between those mechanisms and the world. In the second role, trial and error learning and the evolution of scientific theories are construed as selection processes.\n\nOne of the hallmarks of evolutionary epistemology is the notion that empirical testing alone does not justify the pragmatic value of scientific theories but rather that social and methodological processes select those theories with the closest \"fit\" to a given problem. The mere fact that a theory has survived the most rigorous empirical tests available does not, in the calculus of probability, predict its ability to survive future testing. Karl Popper used Newtonian physics as an example of a body of theories so thoroughly confirmed by testing as to be considered unassailable but were nevertheless overturned by Einstein's bold insights into the nature of space-time. For the evolutionary epistemologist, all theories are true only provisionally, regardless of the degree of empirical testing they have survived.\n\nPopper is considered by many to have given evolutionary epistemology its first comprehensive treatment, bur Donald T. Campbell had coined the phrase in 1974.\n\nTaken from the main page:\n\nDual inheritance theory (DIT), also known as gene–culture coevolution or biocultural evolution, was developed in the 1960s through early 1980s to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. Genes and culture continually interact in a feedback loop, changes in genes can lead to changes in culture which can then influence genetic selection, and vice versa. One of the theory's central claims is that culture evolves partly through a Darwinian selection process, which dual inheritance theorists often describe by analogy to genetic evolution.\"\n\nAs a relatively new and growing scientific field, cultural evolution is undergoing much formative debate. Some of the prominent conversations are revolving around Universal Darwinism, dual inheritance theory, and memetics.\n\nMore recently, cultural evolution has drawn conversations from multi-disciplinary sources with movement towards a unified view between the natural and social sciences. There remains some accusation of biological reductionism, as opposed to cultural naturalism, and scientific efforts are often mistakenly associated with Social Darwinism. However, some useful parallels between biological and social evolution still appear to be found.\n\nCultural evolution has been criticized over the past two centuries that it has advanced its development into the form it holds today. Morgan's theory of evolution implies that all cultures follow the same basic pattern. Human culture is not linear, different cultures develop in different directions and at differing paces, and it is not satisfactory or productive to assume cultures develop in the same way.\n\nA further key critique of cultural evolutionism is what is known as \"armchair anthropology\". The name results from the fact that many of the anthropologists advancing theories had not seen first hand the cultures they were studying. The research and data collected was carried out by explorers and missionaries as opposed to the anthropologists themselves. Edward Tylor was the epitome of that and did very little of his own research.Cultural evolution is also criticized for being ethnocentric, cultures are still seen to be attempting to emulate western civilization. Under ethnocentricity, primitive societies are said to be not yet at the cultural levels of other western societies\n\nMuch of the criticism aimed at cultural evolution is focused on the unilinear approach to social change. Broadly speaking in the second half of the 20th century the criticisms of cultural evolution have been answered by the multilinear theory. Ethnocentricity, for example, is more prevalent under the unilinear theory.\n\nSome recent approaches, such as Dual Inheritance Theory, make use of empirical methods including psychological and animal studies, field site research, and computational models.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "16865821", "url": "https://en.wikipedia.org/wiki?curid=16865821", "title": "Distonic ion", "text": "Distonic ion\n\nDistonic ions are chemical species that contain 2 ionic charges on the same molecule. The charges must be separated by two or more carbon\nor hetero atoms to count as distonic (distant). Otherwise it is a gitonic (close) ion. For example, the acid-catalyzed Grewe-cyclization is a well-known reaction based on formation of a distonic ion, initiating the ring closing step as a superelectrophile. Distonic radical ions were first discovered in the gas phase by Michael L. Gross. In recent years there has been a tremendous effort to identify new distonic species, characterize their reactivity, and measure their stability.\n\n"}
{"id": "9482345", "url": "https://en.wikipedia.org/wiki?curid=9482345", "title": "Dose profile", "text": "Dose profile\n\nIn External Beam Radiotherapy, transverse and longitudinal dose measurements are taken by a radiation detector in order to characterise the radiation beams from medical linear accelerators. Typically, an ionisation chamber and water phantom are used to create these radiation dose profiles. Water is used due to its tissue equivalence. \n\nTransverse dose measurements are performed in the x (crossplane) or y (inplane) directions perpendicular to the radiation beam, and at a given depth (z) in the phantom. These are known dose profiles.\nDose measurements taken along the z direction create radiation dose distribution known as a depth-dose curve.\n"}
{"id": "2069060", "url": "https://en.wikipedia.org/wiki?curid=2069060", "title": "Earth (Noon Universe)", "text": "Earth (Noon Universe)\n\nThe Noon Universe is the setting of a series of science-fiction books written by Boris and Arkady Strugatsky. Earth is one of the planets populated by humans in this setting, and is implied to be their origin. \"Noon\" Earth is identical in concept to real-life Earth, but set in an alternate future, with the story taking place in the 22nd century. Earth is described in detail in \"\", which serves as the first in the \"Noon Universe\" series.\n\nWhile humans also inhabit other planets in the Noon Universe, the humans of Earth and make up the most scientifically advanced civilization in the known universe. A single governing body called the World Council, composed of Earth's brightest scientists, philosophers, historians and strategists governs all the citizens of Earth. Many aspects of this governing system resemble the idealistic vision of communism, which is strongly implied by the humans of Earth, who often refer to themselves as \"communars.\"\n\nEarth in the Noon Universe is set in an alternate future, where the Soviet Union has managed to realize the ideals of communism proclaimed by Karl Marx and Vladimir Lenin in the second half of 20th century, and spread these ideals peacefully over the planet. Soon afterwards the entire planet was united, all wars ceased and a (seemingly) utopian future began.\n\nFirst of all, it was marked by extensive exploration of Earth's own secrets such as those the ocean and deep underground. However, more fascinating were the studies of space. At the beginning they were limited to exploration of the Solar System (see \"The Land of Crimson Clouds\"), but with the invention of the photon engine, inter-stellar flights became possible (see Ruzhena). Simultaneously, due to various technological advances in other fields, by the end of the 21st century Earth was capable of satisfying all material needs of its citizens including spaceships for amateur exploration of the galaxy.\n\nFor more information about 22nd century, see Noon Universe Chronology.\n\nThe Earth's humans are the most technologically advanced civilization in the Noon Universe (except for Wanderers and maybe Tagorians). Some of their technologies utilize the well-known principles of physics while the others are rather of sci-fi nature.\n\n\n\nApart from these there are also more conventional transport means used on Earth like submarines and helicopters, that probably remained from older times, but were, of course, modernized to match the safety requirements.\n\n\n\n\nThe World Council is the main governing body on Earth as well as on its colonies all over the Noon Universe. However, each of the colonies has its own World Council to handle local problems, while the Earth's one usually only suggests the overall policy. On Earth, the Council determines the distribution of energy and other resources between various institutions like institutes, laboratories and universities. Famous members of the World Council include: August-Iogann-Maria Bader, Leonid Gorbovsky, Gennady Komov and Rudolf Sikorski.\n\nIRU (or FSG - \"Free Search Group\") was a label for all non-professional explorers who went into space to discover and study new planets and stars. According to the authors of \"The time of the disciples\" (Время учеников), a collection of stories by authors other than the Strugatsky brothers set in the Noon Universe, the IRU existed during the period between 2114 and 2193 AD. To become a member of IRU one needed mere a piloting license and a flight registration issued by COMCON. After that, one was free to visit the planet he decided to and study it, unless there was a native civilization on it in which case he or she was supposed to leave the planet immediately after destroying all traces of one's presence.\n\nCommittee for Contacts with Other Civilizations (ComCon or COMCON) is an organization that directs the diplomatic negotiations between Earth and alien civilizations as well as the progressors' actions towards human ones. Officially, the Earth World Council makes all decisions, but usually it's up to COMCON to decide which actions should be discussed there and which not.\n\nCommittee for Control of Scientific Achievements was founded in 2137 AD to ensure the safety of the human civilization on Earth. Unlike COMCON-1, COMCON-2 is surrounded by much more mystery, which inevitably leads to a certain suspicion from the general public of Earth and even casual comparisons with Soviet KGB. The first COMCON-2 chief was \"Rudolf Sikorski\" (until 2178 AD) and after that - his apprentice, Maxim Kammerer.\n\nGSC is a security organization like COMCON-2 founded in 2142 AD. While COMCON-2 deals with extraterrestrial threats to Earth and mankind, GSC deals with civilizations that were potentially influenced by Wanderers, such as Saraksh or Saula. Following the events of \"Prisoners of Power\", GSC made the Land of Fathers its protectorate.\n\nFounded in 2110s on Kotlin Island (Gulf of Finland) ISP always concentrated on the most difficult and nearly unsolvable problems of modern physics. Some of these include: gravitation problems, deritrinitration, theory of discrete space, etc. The latter problem was the leading one in 2130s when the scientists tried to prove \"the theory of intersecting universes\" with the help from the \"readers\", a handful of people who were able to read others' thoughts. See \"\" for more info.\n\nAfter half a century of idle, importance of IEH rose drastically with the discovery of Saula and other human planets. The specialization of this Institute is the history of human civilizations and means to speed it up (to \"progress\" it). The field agents of IEH are therefore referred to as \"progressors\". In the middle of the 22nd century IEH studies were considered to be the most important ones (along with \"null-physics\") on Earth. See \"Hard to Be a God\" for more info.\n\nA brief summary of Earth's relationships with other civilizations:\n\n\n"}
{"id": "57248187", "url": "https://en.wikipedia.org/wiki?curid=57248187", "title": "Echigo-jofu", "text": "Echigo-jofu\n\nEchigo-jofu () is a fabric of Echigo, Japan on national Important Cultural Properties listing in 1955, and UNESCO's Intangible Cultural Heritage of Humanity list since 2009. It is made from fine bast fiber from the ramie plant (\"Boehmeria nivea\"), also called hemp, although not directly related to \"cannabis\" hemp. After it is woven on a \"jibata\" backstrap loom (), the fabric is spread on snowfields (\"yuki-zarashi\") where ultraviolet light from the sun creates ozone and bleaches it white. The fabric is used to make summer kimonos and other traditional garments, cushions and bed linens.\n\n"}
{"id": "14105333", "url": "https://en.wikipedia.org/wiki?curid=14105333", "title": "Electric power system", "text": "Electric power system\n\nAn electric power system is a network of electrical components deployed to supply, transfer, and use electric power. An example of an electric power system is \"the grid\" that provides power to an extended area. An electrical grid power system can be broadly divided into the generators that supply the power, the transmission system that carries the power from the generating centres to the load centres, and the distribution system that feeds the power to nearby homes and industries. Smaller power systems are also found in industry, hospitals, commercial buildings and homes. The majority of these systems rely upon three-phase AC power—the standard for large-scale power transmission and distribution across the modern world. Specialised power systems that do not always rely upon three-phase AC power are found in aircraft, electric rail systems, ocean liners and automobiles.\n\nIn 1881, two electricians built the world's first power system at Godalming in England. It was powered by two waterwheels and produced an alternating current that in turn supplied seven Siemens arc lamps at 250 volts and 34 incandescent lamps at 40 volts. However, supply to the lamps was intermittent and in 1882 Thomas Edison and his company, The Edison Electric Light Company, developed the first steam-powered electric power station on Pearl Street in New York City. The Pearl Street Station initially powered around 3,000 lamps for 59 customers. The power station generated direct current and operated at a single voltage. Direct current power could not be transformed easily or efficiently to the higher voltages necessary to minimise power loss during long-distance transmission, so the maximum economic distance between the generators and load was limited to around half a mile (800 m).\n\nThat same year in London, Lucien Gaulard and John Dixon Gibbs demonstrated the \"secondary generator\", namely the first transformer suitable for use in a real power system. The practical value of Gaulard and Gibbs' transformer was demonstrated in 1884 at Turin where the transformer was used to light up forty kilometres (25 miles) of railway from a single alternating current generator. Despite the success of the system, the pair made some fundamental mistakes. Perhaps the most serious was connecting the primaries of the transformers in series so that active lamps would affect the brightness of other lamps further down the line.\n\nIn 1885, Ottó Titusz Bláthy (1860–1939) of Ganz & Co.(Budapest) perfected the secondary generator of Gaulard and Gibbs, providing it with a closed iron core, and thus obtained the first true power transformer, which he dubbed with its present name. The same year, Bláthy and two other engineers of the company set up the ZBD system (from their initials) by implementing the parallel AC distribution proposed by British scientist R. Kennedy in 1883, in which several power transformers have their primary windings fed in parallel from a high-voltage distribution line. The system was presented at the 1885 National General Exhibition of Budapest.\n\nIn 1885 George Westinghouse, an American entrepreneur, obtained the patent rights to the Gaulard-Gibbs transformer and imported a number of them along with a Siemens generator, and set his engineers to experimenting with them in hopes of improving them for use in a commercial power system. In 1886, one of Westinghouse's engineers, William Stanley, also recognised the problem with connecting transformers in series as opposed to parallel and also realised that making the iron core of a transformer a fully enclosed loop would improve the voltage regulation of the secondary winding. Using this knowledge he built the first practical transformer-based alternating-current power system at Great Barrington, Massachusetts in 1886. Westinghouse would begin installing multi-voltage AC transformer systems in competition with the Edison company later that year. In 1888 Westinghouse also licensed Nikola Tesla's US patents for a polyphase AC induction motor and transformer designs and hired Tesla for one year to be a consultant at the Westinghouse Electric & Manufacturing Company's Pittsburgh labs.\n\nBy 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse's companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a synchronous electric motor, not just provide electric lighting, at Telluride, Colorado. On the other side of the Atlantic, Mikhail Dolivo-Dobrovolsky of AEG and Charles Eugene Lancelot Brown of Maschinenfabrik Oerlikon, built the very first long-distance (175 km, a distance never tried before) high-voltage (15 kV, then a record) three-phase transmission line from Lauffen am Neckar to Frankfurt am Main for the Electrical Engineering Exhibition in Frankfurt, where power was used light lamps and move a water pump. In the US the AC/DC competition came to an end when Edison General Electric was taken over by their chief AC rival, the Thomson-Houston Electric Company, forming General Electric. In 1895, after a protracted decision-making process, alternating current was chosen as the transmission standard with Westinghouse building the Adams No. 1 generating station at Niagara Falls and General Electric building the three-phase alternating current power system to supply Buffalo at 11 kV.\n\nDevelopments in power systems continued beyond the nineteenth century. In 1936 the first experimental high voltage direct current (HVDC) line using mercury arc valves was built between Schenectady and Mechanicville, New York. HVDC had previously been achieved by series-connected direct current generators and motors (the Thury system) although this suffered from serious reliability issues. The first solid-state metal diode suitable for general power uses was developed by Ernst Presser at TeKaDe, Germany, in 1928. It consisted of a layer of selenium applied on an aluminum plate. \nIn 1957, a General Electric research group developed a solid-state p-n-p-n switch device that was successfully marketed in early 1958, starting a revolution in power electronics. In 1957, also Siemens demonstrated a solid-state rectifier, but it was not until the early 1970s that solid-state devices became the standard in HVDC, when GE emerged as one of the top suppliers of thyristor-based HVDC. \nIn 1979, a European consortium including Siemens, Brown Boveri & Cie and AEG realized the record HVDC link from Cabora Bassa (Mozambique) to Johannesburg (South Africa), extending more than 1,420 km and rated 1.9 GW at ±533 kV, that resorted to top performing 3.2-kV\nthyristors, developed by AEG under GE’s license, In recent times, many important developments have come from extending innovations in the information and communications technology (ICT) field to the power engineering field. For example, the development of computers meant load flow studies could be run more efficiently allowing for much better planning of power systems. Advances in information technology and telecommunication also allowed for remote control of a power system's switchgear and generators.\n\nElectric power is the product of two quantities: current and voltage. These two quantities can vary with respect to time (AC power) or can be kept at constant levels (DC power).\n\nMost refrigerators, air conditioners, pumps and industrial machinery use AC power whereas most computers and digital equipment use DC power (the digital devices you plug into the mains typically have an internal or external power adapter to convert from AC to DC power). AC power has the advantage of being easy to transform between voltages and is able to be generated and utilised by brushless machinery. DC power remains the only practical choice in digital systems and can be more economical to transmit over long distances at very high voltages (see HVDC).\n\nThe ability to easily transform the voltage of AC power is important for two reasons: Firstly, power can be transmitted over long distances with less loss at higher voltages. So in power systems where generation is distant from the load, it is desirable to step-up (increase) the voltage of power at the generation point and then step-down (decrease) the voltage near the load. Secondly, it is often more economical to install turbines that produce higher voltages than would be used by most appliances, so the ability to easily transform voltages means this mismatch between voltages can be easily managed.\n\nSolid state devices, which are products of the semiconductor revolution, make it possible to transform DC power to different voltages, build brushless DC machines and convert between AC and DC power. Nevertheless, devices utilising solid state technology are often more expensive than their traditional counterparts, so AC power remains in widespread use.\n\nOne of the main difficulties in power systems is that the amount of active power consumed plus losses should always equal the active power produced. If more power is produced than consumed the frequency wil rise and vice versa. Even small deviations from the nominal frequency value will damage synchronous machines and other appliances. Making sure the frequency is constant is usually the task of a transmission system operator. In some countries (for example in the European Union) this is achieved through a balancing market using ancillary services.\n\nAll power systems have one or more sources of power. For some power systems, the source of power is external to the system but for others, it is part of the system itself—it is these internal power sources that are discussed in the remainder of this section. Direct current power can be supplied by batteries, fuel cells or photovoltaic cells. Alternating current power is typically supplied by a rotor that spins in a magnetic field in a device known as a turbo generator. There have been a wide range of techniques used to spin a turbine's rotor, from steam heated using fossil fuel (including coal, gas and oil) or nuclear energy, falling water (hydroelectric power) and wind (wind power).\n\nThe speed at which the rotor spins in combination with the number of generator poles determines the frequency of the alternating current produced by the generator. All generators on a single synchronous system, for example, the national grid, rotate at sub-multiples of the same speed and so generate electric current at the same frequency. If the load on the system increases, the generators will require more torque to spin at that speed and, in a typical power station, more steam must be supplied to the turbines driving them. Thus the steam used and the fuel expended are directly dependent on the quantity of electrical energy supplied. An exception exists for generators incorporating power electronics such as gearless wind turbines or linked to a grid through an asynchronous tie such as a HVDC link — these can operate at frequencies independent of the power system frequency.\n\nDepending on how the poles are fed, alternating current generators can produce a variable number of phases of power. A higher number of phases leads to more efficient power system operation but also increases the infrastructure requirements of the system.\n\nElectricity grid systems connect multiple generators and loads operating at the same frequency and number of phases, the commonest being three-phase at 50 or 60 Hz. However, there are other considerations. These range from the obvious: How much power should the generator be able to supply? What is an acceptable length of time for starting the generator (some generators can take hours to start)? Is the availability of the power source acceptable (some renewables are only available when the sun is shining or the wind is blowing)? To the more technical: How should the generator start (some turbines act like a motor to bring themselves up to speed in which case they need an appropriate starting circuit)? What is the mechanical speed of operation for the turbine and consequently what are the number of poles required? What type of generator is suitable (synchronous or asynchronous) and what type of rotor (squirrel-cage rotor, wound rotor, salient pole rotor or cylindrical rotor)?\n\nPower systems deliver energy to loads that perform a function. These loads range from household appliances to industrial machinery. Most loads expect a certain voltage and, for alternating current devices, a certain frequency and number of phases. The appliances found in your home, for example, will typically be single-phase operating at 50 or 60 Hz with a voltage between 110 and 260 volts (depending on national standards). An exception exists for centralized air conditioning systems as these are now typically three-phase because this allows them to operate more efficiently. All devices in your house will also have a wattage, this specifies the amount of power the device consumes. At any one time, the net amount of power consumed by the loads on a power system must equal the net amount of power produced by the supplies less the power lost in transmission.\n\nMaking sure that the voltage, frequency and amount of power supplied to the loads is in line with expectations is one of the great challenges of power system engineering. However it is not the only challenge, in addition to the power used by a load to do useful work (termed real power) many alternating current devices also use an additional amount of power because they cause the alternating voltage and alternating current to become slightly out-of-sync (termed reactive power). The reactive power like the real power must balance (that is the reactive power produced on a system must equal the reactive power consumed) and can be supplied from the generators, however it is often more economical to supply such power from capacitors (see \"Capacitors and reactors\" below for more details).\n\nA final consideration with loads is to do with power quality. In addition to sustained overvoltages and undervoltages (voltage regulation issues) as well as sustained deviations from the system frequency (frequency regulation issues), power system loads can be adversely affected by a range of temporal issues. These include voltage sags, dips and swells, transient overvoltages, flicker, high-frequency noise, phase imbalance and poor power factor. Power quality issues occur when the power supply to a load deviates from the ideal: For an AC supply, the ideal is the current and voltage in-sync fluctuating as a perfect sine wave at a prescribed frequency with the voltage at a prescribed amplitude. For DC supply, the ideal is the voltage not varying from a prescribed level. Power quality issues can be especially important when it comes to specialist industrial machinery or hospital equipment. \n\nConductors carry power from the generators to the load. In a grid, conductors may be classified as belonging to the transmission system, which carries large amounts of power at high voltages (typically more than 69 kV) from the generating centres to the load centres, or the distribution system, which feeds smaller amounts of power at lower voltages (typically less than 69 kV) from the load centres to nearby homes and industry.\n\nChoice of conductors is based on considerations such as cost, transmission losses and other desirable characteristics of the metal like tensile strength. Copper, with lower resistivity than Aluminum, was the conductor of choice for most power systems. However, Aluminum has a lower cost for the same current carrying capacity and is the primary metal used for transmission line conductors. Overhead line conductors may be reinforced with steel or aluminium alloys.\n\nConductors in exterior power systems may be placed overhead or underground. Overhead conductors are usually air insulated and supported on porcelain, glass or polymer insulators. Cables used for underground transmission or building wiring are insulated with cross-linked polyethylene or other flexible insulation. Large conductors are stranded for ease of handling; small conductors used for building wiring are often solid, especially in light commercial or residential construction.\n\nConductors are typically rated for the maximum current that they can carry at a given temperature rise over ambient conditions. As current flow increases through a conductor it heats up. For insulated conductors, the rating is determined by the insulation. For overhead conductors, the rating is determined by the point at which the sag of the conductors would become unacceptable.\n\nThe majority of the load in a typical AC power system is inductive; the current lags behind the voltage. Since the voltage and current are out-of-phase, this leads to the emergence of an \"imaginary\" form of power known as reactive power. Reactive power does no measurable work but is transmitted back and forth between the reactive power source and load every cycle. This reactive power can be provided by the generators themselves, through the adjustment of generator excitation, but it is often cheaper to provide it through capacitors, hence capacitors are often placed near inductive loads to reduce current demand on the power system (i.e., increase the power factor), which may never exceed 1.0, and which represents a purely resistive load. Power factor correction may be applied at a central substation, through the use of so-called \"synchronous condensers\" (synchronous machines which act as condensers which are variable in VAR value, through the adjustment of machine excitation) or adjacent to large loads, through the use of so-called \"static condensers\" (condensers which are fixed in VAR value).\n\nReactors consume reactive power and are used to regulate voltage on long transmission lines. In light load conditions, where the loading on transmission lines is well below the surge impedance loading, the efficiency of the power system may actually be improved by switching in reactors. Reactors installed in series in a power system also limit rushes of current flow, small reactors are therefore almost always installed in series with capacitors to limit the current rush associated with switching in a capacitor. Series reactors can also be used to limit fault currents.\n\nCapacitors and reactors are switched by circuit breakers, which results in moderately large steps in reactive power. A solution comes in the form of static VAR compensators and static synchronous compensators. Briefly, static VAR compensators work by switching in capacitors using thyristors as opposed to circuit breakers allowing capacitors to be switched-in and switched-out within a single cycle. This provides a far more refined response than circuit breaker switched capacitors. Static synchronous compensators take a step further by achieving reactive power adjustments using only power electronics.\n\nPower electronics are semiconductor based devices that are able to switch quantities of power ranging from a few hundred watts to several hundred megawatts. Despite their relatively simple function, their speed of operation (typically in the order of nanoseconds) means they are capable of a wide range of tasks that would be difficult or impossible with conventional technology. The classic function of power electronics is rectification, or the conversion of AC-to-DC power, power electronics are therefore found in almost every digital device that is supplied from an AC source either as an adapter that plugs into the wall (see photo in \"Basics of Electric Power\" section) or as component internal to the device. High-powered power electronics can also be used to convert AC power to DC power for long distance transmission in a system known as HVDC. HVDC is used because it proves to be more economical than similar high voltage AC systems for very long distances (hundreds to thousands of kilometres). HVDC is also desirable for interconnects because it allows frequency independence thus improving system stability. Power electronics are also essential for any power source that is required to produce an AC output but that by its nature produces a DC output. They are therefore used by many photovoltaic installations both industrial and residential.\n\nPower electronics also feature in a wide range of more exotic uses. They are at the heart of all modern electric and hybrid vehicles—where they are used for both motor control and as part of the brushless DC motor. Power electronics are also found in practically all modern petrol-powered vehicles, this is because the power provided by the car's batteries alone is insufficient to provide ignition, air-conditioning, internal lighting, radio and dashboard displays for the life of the car. So the batteries must be recharged while driving using DC power from the engine—a feat that is typically accomplished using power electronics. Whereas conventional technology would be unsuitable for a modern electric car, commutators can and have been used in petrol-powered cars, the switch to alternators in combination with power electronics has occurred because of the improved durability of brushless machinery.\n\nSome electric railway systems also use DC power and thus make use of power electronics to feed grid power to the locomotives and often for speed control of the locomotive's motor. In the middle twentieth century, rectifier locomotives were popular, these used power electronics to convert AC power from the railway network for use by a DC motor. Today most electric locomotives are supplied with AC power and run using AC motors, but still use power electronics to provide suitable motor control. The use of power electronics to assist with the motor control and with starter circuits cannot be overestimated and, in addition to rectification, is responsible for power electronics appearing in a wide range of industrial machinery. Power electronics even appear in modern residential air conditioners.\n\nPower electronics are also at the heart of the variable speed wind turbine. Conventional wind turbines require significant engineering to ensure they operate at some ratio of the system frequency, however by using power electronics this requirement can be eliminated leading to quieter, more flexible and (at the moment) more costly wind turbines. A final example of one of the more exotic uses of power electronics comes from the previous section where the fast-switching times of power electronics were used to provide more refined reactive compensation to the power system.\n\nPower systems contain protective devices to prevent injury or damage during failures. The quintessential protective device is the fuse. When the current through a fuse exceeds a certain threshold, the fuse element melts, producing an arc across the resulting gap that is then extinguished, interrupting the circuit. Given that fuses can be built as the weak point of a system, fuses are ideal for protecting circuitry from damage. Fuses however have two problems: First, after they have functioned, fuses must be replaced as they cannot be reset. This can prove inconvenient if the fuse is at a remote site or a spare fuse is not on hand. And second, fuses are typically inadequate as the sole safety device in most power systems as they allow current flows well in excess of that that would prove lethal to a human or animal.\n\nThe first problem is resolved by the use of circuit breakers—devices that can be reset after they have broken current flow. In modern systems that use less than about 10 kW, miniature circuit breakers are typically used. These devices combine the mechanism that initiates the trip (by sensing excess current) as well as the mechanism that breaks the current flow in a single unit. Some miniature circuit breakers operate solely on the basis of electromagnetism. In these miniature circuit breakers, the current is run through a solenoid, and, in the event of excess current flow, the magnetic pull of the solenoid is sufficient to force open the circuit breaker's contacts (often indirectly through a tripping mechanism). A better design, however, arises by inserting a bimetallic strip before the solenoid—this means that instead of always producing a magnetic force, the solenoid only produces a magnetic force when the current is strong enough to deform the bimetallic strip and complete the solenoid's circuit.\n\nIn higher powered applications, the protective relays that detect a fault and initiate a trip are separate from the circuit breaker. Early relays worked based upon electromagnetic principles similar to those mentioned in the previous paragraph, modern relays are application-specific computers that determine whether to trip based upon readings from the power system. Different relays will initiate trips depending upon different protection schemes. For example, an overcurrent relay might initiate a trip if the current on any phase exceeds a certain threshold whereas a set of differential relays might initiate a trip if the sum of currents between them indicates there may be current leaking to earth. The circuit breakers in higher powered applications are different too. Air is typically no longer sufficient to quench the arc that forms when the contacts are forced open so a variety of techniques are used. One of the most popular techniques is to keep the chamber enclosing the contacts flooded with sulfur hexafluoride (SF)—a non-toxic gas that has sound arc-quenching properties. Other techniques are discussed in the reference.\n\nThe second problem, the inadequacy of fuses to act as the sole safety device in most power systems, is probably best resolved by the use of residual current devices (RCDs). In any properly functioning electrical appliance, the current flowing into the appliance on the active line should equal the current flowing out of the appliance on the neutral line. A residual current device works by monitoring the active and neutral lines and tripping the active line if it notices a difference. Residual current devices require a separate neutral line for each phase and to be able to trip within a time frame before harm occurs. This is typically not a problem in most residential applications where standard wiring provides an active and neutral line for each appliance (that's why your power plugs always have at least two tongs) and the voltages are relatively low however these issues do limit the effectiveness of RCDs in other applications such as industry. Even with the installation of an RCD, exposure to electricity can still prove lethal.\n\nIn large electric power systems, supervisory control and data acquisition (SCADA) is used for tasks such as switching on generators, controlling generator output and switching in or out system elements for maintenance. The first supervisory control systems implemented consisted of a panel of lamps and switches at a central console near the controlled plant. The lamps provided feedback on the state of the plant (the data acquisition function) and the switches allowed adjustments to the plant to be made (the supervisory control function). Today, SCADA systems are much more sophisticated and, due to advances in communication systems, the consoles controlling the plant no longer need to be near the plant itself. Instead, it is now common for plants to be controlled with equipment similar (if not identical) to a desktop computer. The ability to control such plants through computers has increased the need for security—there have already been reports of cyber-attacks on such systems causing significant disruptions to power systems.\n\nDespite their common components, power systems vary widely both with respect to their design and how they operate. This section introduces some common power system types and briefly explains their operation.\n\nResidential dwellings almost always take supply from the low voltage distribution lines or cables that run past the dwelling. These operate at voltages of between 110 and 260 volts (phase-to-earth) depending upon national standards. A few decades ago small dwellings would be fed a single phase using a dedicated two-core service cable (one core for the active phase and one core for the neutral return). The active line would then be run through a main isolating switch in the fuse box and then split into one or more circuits to feed lighting and appliances inside the house. By convention, the lighting and appliance circuits are kept separate so the failure of an appliance does not leave the dwelling's occupants in the dark. All circuits would be fused with an appropriate fuse based upon the wire size used for that circuit. Circuits would have both an active and neutral wire with both the lighting and power sockets being connected in parallel. Sockets would also be provided with a protective earth. This would be made available to appliances to connect to any metallic casing. If this casing were to become live, the theory is the connection to earth would cause an RCD or fuse to trip—thus preventing the future electrocution of an occupant handling the appliance. Earthing systems vary between regions, but in countries such as the United Kingdom and Australia both the protective earth and neutral line would be earthed together near the fuse box before the main isolating switch and the neutral earthed once again back at the distribution transformer.\n\nThere have been a number of minor changes over the year to practice of residential wiring. Some of the most significant ways modern residential power systems tend to vary from older ones include:\n\n\nCommercial power systems such as shopping centers or high-rise buildings are larger in scale than residential systems. Electrical designs for larger commercial systems are usually studied for load flow, short-circuit fault levels, and voltage drop for steady-state loads and during starting of large motors. The objectives of the studies are to assure proper equipment and conductor sizing, and to coordinate protective devices so that minimal disruption is cause when a fault is cleared. Large commercial installations will have an orderly system of sub-panels, separate from the main distribution board to allow for better system protection and more efficient electrical installation.\n\nTypically one of the largest appliances connected to a commercial power system is the HVAC unit, and ensuring this unit is adequately supplied is an important consideration in commercial power systems. Regulations for commercial establishments place other requirements on commercial systems that are not placed on residential systems. For example, in Australia, commercial systems must comply with AS 2293, the standard for emergency lighting, which requires emergency lighting be maintained for at least 90 minutes in the event of loss of mains supply. In the United States, the National Electrical Code requires commercial systems to be built with at least one 20A sign outlet in order to light outdoor signage. Building code regulations may place special requirements on the electrical system for emergency lighting, evacuation, emergency power, smoke control and fire protection.\n\n\n"}
{"id": "23567924", "url": "https://en.wikipedia.org/wiki?curid=23567924", "title": "Environmental impact of shipping", "text": "Environmental impact of shipping\n\nThe environmental impact of shipping includes greenhouse gas emissions, acoustic, and oil pollution. The International Maritime Organization (IMO) estimates that Carbon dioxide emissions from shipping were equal to 2.2% of the global human-made emissions in 2012 and expects them to rise 50 to 250 percent by 2050 if no action is taken.\n\nThe First Intersessional Meeting of the IMO Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway on 23–27 June 2008. It was tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO's Marine Environment Protection Committee (MEPC).\n\nBallast water discharges by ships can have a negative impact on the marine environment.\n\nCruise ships, large tankers, and bulk cargo carriers use a huge amount of ballast water, which is often taken on in the coastal waters in one region after ships discharge wastewater or unload cargo, and discharged at the next port of call, wherever more cargo is loaded. Ballast water discharge typically contains a variety of biological materials, including plants, animals, viruses, and bacteria. These materials often include non-native, nuisance, invasive, exotic species that can cause extensive ecological and economic damage to aquatic ecosystems along with serious human health problems.\n\nNoise pollution caused by shipping and other human enterprises has increased in recent history. The noise produced by ships can travel long distances, and marine species who may rely on sound for their orientation, communication, and feeding, can be harmed by this sound pollution.\n\nThe Convention on the Conservation of Migratory Species has identified ocean noise as a potential threat to marine life.\nThe disruption of whales' ability to communicate with one another is an extreme threat and is affecting their ability to survive. According to Discovery Channel's article on Sonic Sea Journeys Deep Into the Ocean, over the last century, extremely loud noise from commercial ships, oil and gas exploration, naval sonar exercises and other sources has transformed the ocean's delicate acoustic habitat, challenging the ability of whales and other marine life to prosper and ultimately to survive. Whales are starting to react to this in ways that are life-threatening. Kenneth C. Balcomb, a whale researcher and a former U.S Navy officer, states that the day March 15, 2000, is the day of infamy. As Discovery says, where him and his crew discovered whales swimming dangerously close to the shore. They're supposed to be in deep water. So I pushed it back out to sea, says Balcomb. Although sonar helps to protect us, it is destroying marine life. According to IFAW Animal Rescue Program Director Katie Moore, \"There's different ways that sounds can affect animals. There's that underlying ambient noise level that's rising, and rising, and rising that interferes with communication and their movement patterns. And then there's the more acute kind of traumatic impact of sound, that's causing physical damage or a really strong behavioral response. It's fight or flight\".\n\nMarine mammals, such as whales and manatees, risk being struck by ships, causing injury and death. For example, if a ship is traveling at a speed of only 15 knots, there is a 79 percent chance of a collision being lethal to a whale.\n\nOne notable example of the impact of ship collisions is the endangered North Atlantic right whale, of which 400 or less remain. The greatest danger to the North Atlantic right whale is injury sustained from ship strikes. Between 1970 and 1999, 35.5 percent of recorded deaths were attributed to collisions. During 1999 to 2003, incidents of mortality and serious injury attributed to ship strikes averaged one per year. In 2004 to 2006, that number increased to 2.6. Deaths from collisions has become an extinction threat. The United States' National Marine Fisheries Service (NMFS) and National Oceanic and Atmospheric Administration (NOAA) introduced vessel speed restrictions to reduce ship collisions with North Atlantic right whales in 2008, which expired in 2013. However, in 2017 an unprecedented mortality event occurred, resulting in the deaths of 17 North Atlantic Right Whales caused primarily from ship-strikes and entanglement in fishing gear.\n\nExhaust gases from ships are considered to be a significant source of air pollution, both for conventional pollutants and greenhouse gases.\n\nThere is a perception that cargo transport by ship is low in air pollutants, because for equal weight and distance it is the most efficient transport method, according to shipping researcher Alice Bows-Larkin. This is particularly true in comparison to air freight; however, because sea shipment accounts for far more annual tonnage and the distances are often large, shipping's emissions are globally substantial. A difficulty is that the year-on-year increasing amount shipping overwhelms gains in efficiency, such as from slow-steaming or the use of kites. The growth in tonne-kilometers of sea shipment has averaged 4 percent yearly since the 1990s. And it has grown by a factor of 5 since the 1970s. There are now over 100,000 transport ships at sea, of which about 6,000 are large container ships.\n\nAir pollution from cruise ships is generated by diesel engines that burn high sulfur content fuel oil, also known as bunker oil, producing sulfur dioxide, nitrogen oxide and particulate, in addition to carbon monoxide, carbon dioxide, and hydrocarbons. Diesel exhaust has been classified by EPA as a likely human carcinogen. EPA recognizes that these emissions from marine diesel engines contribute to ozone and carbon monoxide nonattainment (i.e., failure to meet air quality standards), as well as adverse health effects associated with ambient concentrations of particulate matter and visibility, haze, acid deposition, and eutrophication and nitrification of water. EPA estimates that large marine diesel engines accounted for about 1.6 percent of mobile source nitrogen oxide emissions and 2.8 percent of mobile source particulate emissions in the United States in 2000. Contributions of marine diesel engines can be higher on a port-specific basis. Ultra-low sulfur diesel (ULSD) is a standard for defining diesel fuel with substantially lowered sulfur contents. As of 2006, almost all of the petroleum-based diesel fuel available in Europe and North America is of a ULSD type.\n\nIn 2016 the IMO has made new sulfur regulations which must be implemented by larger ships by 2020.\n\nOf total global air emissions, shipping accounts for 18 to 30 percent of the nitrogen oxide and 9 percent of the sulphur oxides. Sulfur in the air creates acid rain which damages crops and buildings. When inhaled, sulfur is known to cause respiratory problems and even increases the risk of a heart attack. According to Irene Blooming, a spokeswoman for the European environmental coalition Seas at Risk, the fuel used in oil tankers and container ships is high in sulfur and cheaper to buy compared to the fuel used for domestic land use. \"A ship lets out around 50 times more sulfur than a lorry per metric tonne of cargo carried.\" Cities in the U.S. like Long Beach, Los Angeles, Houston, Galveston, and Pittsburgh see some of the heaviest shipping traffic in the nation and have left local officials desperately trying to clean up the air. Increasing trade between the U.S. and China is helping to increase the number of vessels navigating the Pacific and exacerbating many of the environmental problems. To maintain the level of growth China is experiencing, large amounts of grain are being shipped to China by the boat load. The number of voyages are expected to continue increasing.\n\n3.5 to 4 percent of all climate change emissions are caused by shipping, primarily carbon dioxide.\n\nAs one way to reduce the impact of greenhouse gas emissions from shipping, vetting agency RightShip developed an online \"Greenhouse Gas (GHG) Emissions Rating\" as a systematic way for the industry to compare a ship's CO emissions with peer vessels of a similar size and type. Based on the International Maritime Organisation's (IMO) Energy Efficiency Design Index (EEDI) that applies to ships built from 2013, RightShip's GHG Rating can also be applied to vessels built prior to 2013, allowing for effective vessel comparison across the world's fleet. The GHG Rating utilises an A to G scale, where A represents the most efficient ships. It measures the theoretical amount of carbon dioxide emitted per tonne nautical mile travelled, based on the design characteristics of the ship at time of build such as cargo carrying capacity, engine power and fuel consumption. Higher rated ships can deliver significantly lower CO emissions across the voyage length, which means they also use less fuel and are cheaper to run.\n\nOne source of environmental stresses on maritime vessels recently has come from states and localities, as they assess the contribution of commercial marine vessels to regional air quality problems when ships are docked at port.\nFor instance, large marine diesel engines are believed to contribute 7 percent of mobile source nitrogen oxide emissions in Baton Rouge/New Orleans. Ships can also have a significant impact in areas without large commercial ports: they contribute about 37 percent of total area nitrogen oxide emissions in the Santa Barbara area, and that percentage is expected to increase to 61 percent by 2015. Again, there is little cruise-industry specific data on this issue. They comprise only a small fraction of the world shipping fleet, but cruise ship emissions may exert significant impacts on a local scale in specific coastal areas that are visited repeatedly. Shipboard incinerators also burn large volumes of garbage, plastics, and other waste, producing ash that must be disposed of. Incinerators may release toxic emissions as well.\n\nIn 2005, MARPOL Annex VI came into force to combat this problem. As such cruise ships now employ CCTV monitoring on the smokestacks as well as recorded measuring via opacity meter while some are also using clean burning gas turbines for electrical loads and propulsion in sensitive areas.\n\nMost commonly associated with ship pollution are oil spills. While less frequent than the pollution that occurs from daily operations, oil spills have devastating effects. While being toxic to marine life, polycyclic aromatic hydrocarbons (PAHs), the components in crude oil, are very difficult to clean up, and last for years in the sediment and marine environment. Marine species constantly exposed to PAHs can exhibit developmental problems, susceptibility to disease, and abnormal reproductive cycles. One of the more widely known spills was the Exxon Valdez incident in Alaska. The ship ran aground and dumped a massive amount of oil into the ocean in March 1989. Despite efforts of scientists, managers and volunteers, over 400,000 seabirds, about 1,000 sea otters, and immense numbers of fish were killed.\n\nSome of the major international efforts in the form of treaties are the Marine Pollution Treaty, Honolulu, which deals with regulating marine pollution from ships, and the UN Convention on Law of the Sea, which deals with marine species and pollution. While plenty of local and international regulations have been introduced throughout maritime history, much of the current regulations are considered inadequate. \"In general, the treaties tend to emphasize the technical features of safety and pollution control measures without going to the root causes of sub-standard shipping, the absence of incentives for compliance and the lack of enforceability of measures.\" The most common problems encountered with international shipping arise from paperwork errors and customs brokers not having the proper information about your items. Cruise ships, for example, are exempt from regulation under the US discharge permit system (NPDES, under the Clean Water Act) that requires compliance with technology-based standards. In the Caribbean, many ports lack proper waste disposal facilities, and many ships dump their waste at sea. Moreover, due to the complexities of shipping trade and the difficulties involved in regulating this business, a comprehensive and generally acceptable regulatory framework on corporate responsibility for reducing GHG emissions is unlikely to be achieved soon. In fact, emissions are continuing to increase. Under these circumstances, it is necessary for the states, the shipping industry and global organizations to explore and discuss market based mechanisms for vessel-sourced GHG emissions reduction.\n\nThe cruise line industry dumps of greywater and of blackwater into the sea every day. Blackwater is sewage, wastewater from toilets and medical facilities, which can contain harmful bacteria, pathogens, viruses, intestinal parasites, and harmful nutrients. Discharges of untreated or inadequately treated sewage can cause bacterial and viral contamination of fisheries and shellfish beds, producing risks to public health. Nutrients in sewage, such as nitrogen and phosphorus, promote excessive algal blooms, which consumes oxygen in the water and can lead to fish kills and destruction of other aquatic life. A large cruise ship (3,000 passengers and crew) generates an estimated 55,000 to 110,000 liters per day of blackwater waste.\n\nDue to the environmental impact of shipping, and sewage in particular marpol annex IV was brought into force September 2003 strictly limiting untreated waste discharge. Modern cruise ships are most commonly installed with a membrane bioreactor type treatment plant for all blackwater and greywater, such as (https://web.archive.org/web/20130408054426/http://www.gertsen-olufsen.com/Ship-Offshore/Products/G-O_Brands/G-O_Bioreactor.aspx), Zenon or Rochem which produce near drinkable quality effluent to be re-used in the machinery spaces as technical water.\n\nGreywater is wastewater from the sinks, showers, galleys, laundry, and cleaning activities aboard a ship. It can contain a variety of pollutant substances, including fecal coliforms, detergents, oil and grease, metals, organic compounds, petroleum hydrocarbons, nutrients, food waste, medical and dental waste. Sampling done by the EPA and the state of Alaska found that untreated greywater from cruise ships can contain pollutants at variable strengths and that it can contain levels of fecal coliform bacteria several times greater than is typically found in untreated domestic wastewater. Greywater has potential to cause adverse environmental effects because of concentrations of nutrients and other oxygen-demanding materials, in particular. Greywater is typically the largest source of liquid waste generated by cruise ships (90 to 95 percent of the total). Estimates of greywater range from 110 to 320 liters per day per person, or 330,000 to 960,000 liters per day for a 3,000-person cruise ship.\n\nSolid waste generated on a ship includes glass, paper, cardboard, aluminium and steel cans, and plastics. It can be either non-hazardous or hazardous in nature. Solid waste that enters the ocean may become marine debris, and can then pose a threat to marine organisms, humans, coastal communities, and industries that utilize marine waters. Cruise ships typically manage solid waste by a combination of source reduction, waste minimization, and recycling. However, as much as 75 percent of solid waste is incinerated on board, and the ash typically is discharged at sea, although some is landed ashore for disposal or recycling. Marine mammals, fish, sea turtles, and birds can be injured or killed from entanglement with plastics and other solid waste that may be released or disposed off of cruise ships. On average, each cruise ship passenger generates at least two pounds of non-hazardous solid waste per day. With large cruise ships carrying several thousand passengers, the amount of waste generated in a day can be massive. For a large cruise ship, about 8 tons of solid waste are generated during a one-week cruise. It has been estimated that 24 percent of the solid waste generated by vessels worldwide (by weight) comes from cruise ships. Most cruise ship garbage is treated on board (incinerated, pulped, or ground up) for discharge overboard. When garbage must be off-loaded (for example, because glass and aluminium cannot be incinerated), cruise ships can put a strain on port reception facilities, which are rarely adequate to the task of serving a large passenger vessel.\n\nOn a ship, oil often leaks from engine and machinery spaces or from engine maintenance activities and mixes with water in the bilge, the lowest part of the hull of the ship, but there is a filter to clean bilge water before being discharged. Oil, gasoline, and by-products from the biological breakdown of petroleum products can harm fish and wildlife and pose threats to human health if ingested. Oil in even minute concentrations can kill fish or have various sub-lethal chronic effects. Bilge water also may contain solid wastes and pollutants containing high levels of oxygen-demanding material, oil and other chemicals. A typically large cruise ship will generate an average of 8 metric tons of oily bilge water for each 24 hours of operation. To maintain ship stability and eliminate potentially hazardous conditions from oil vapors in these areas, the bilge spaces need to be flushed and periodically pumped dry. However, before a bilge can be cleared out and the water discharged, the oil that has been accumulated needs to be extracted from the bilge water, after which the extracted oil can be reused, incinerated, and/or offloaded in port. If a separator, which is normally used to extract the oil, is faulty or is deliberately bypassed, untreated oily bilge water could be discharged directly into the ocean, where it can damage marine life. A number of cruise lines have been charged with environmental violations related to this issue in recent years.\n\n\n\nIt is expected that, (from 2004) \"...shipping traffic to and from the United States is projected to double by 2020.\" However, many shipping companies and port operators in North America (Canada and the United States) have adopted the Green Marine Environmental Program to limit operational impacts on the environment.\n\n\n\n\n"}
{"id": "144283", "url": "https://en.wikipedia.org/wiki?curid=144283", "title": "Ertholmene", "text": "Ertholmene\n\nErtholmene (formerly spelled \"Ærtholme(ne)\" and generally known as Christiansø) is a small archipelago situated northeast of Gudhjem, Bornholm and east of Allinge-Sandvig in the Baltic Sea. It is southeast of the capital of Denmark, Copenhagen. Its name derives from the Danish for \"pea islands\".\n\nErtholmene belongs to Denmark and has the country's easternmost point. Its permanent population was 83 people in 2018, and its area is (0.16 sq mi). Together with Bornholm it is part of Landsdel Bornholm. Ertholmene makes up the Parish of Christiansø in the Church of Denmark and is served by Christiansø Church. On 1 January 2017 out of 78 inhabitants in total there were 69 members of the Church of Denmark (88.46% of the population). Ertholmene has never been part of a municipality, county, or region. The islands are administered by the Ministry of Defence.\n\nErtholmene consists of three main islands, \"Christiansø\" (named after King Christian V), \"Frederiksø\" (named after King Frederick IV) and \"Græsholm\", plus a number of minor rocks and skerries, including Tat and Østerskær, Denmark's easternmost point. Christiansø makes up 22.3 hectares (0.0861 sq mi), Frederiksø 4 (0.01544 sq mi), and Græsholm 11 hectares (0.04247 sq mi). Only Christiansø and Frederiksø are inhabited, while Græsholm is a bird reserve. The sound between Christiansø and Frederiksø, a well-sheltered natural harbour, is crossed by a pedestrian bridge that can be pulled aside to accommodate larger vessels.\n\nFishermen from Bornholm have used Ertholmene for temporary shelter since the Middle Ages. The first permanent settlement was the result of the Danish-Swedish conflicts in the late 17th century. As Denmark needed a naval base in the central Baltic Sea, a fort was built on Christiansø and Frederiksø in 1684 which served as an outpost for the Danish Navy until 1855. Christiansø Church originally served the garrison. The population peaked at the census in 1810 which showed 829 inhabitants. They were soldiers and were there because of the Gunboat War. Many of the historical buildings now serve as living quarters for the local population, and some are rented, year after year, to regular summer residents. The islands' external appearance has changed very little in over 300 years. Girdled by thick granite walls with old cannons pointed seaward, Christiansø is a picturesque tourist spot seemingly frozen in time. A former part of the fort, \"Store Tårn\" has housed the Christiansø Lighthouse for the past 200 years, and a small round tower on Frederiksø, \"Lille Tårn,\" serves as a museum.\n\nThe islands form an unincorporated area that does not belong to either a municipality or a region. Instead they are state property governed by an administrator, appointed by the Danish ministry of defence, with the responsibility being the tasks normally performed by municipalities and other public sector civil services. The major sources of income are fishery and tourism. There are 80,000 tourists per year, mostly day visitors arriving via Bornholm. Ertholmene is also a popular destination for yachts.\n\nThe islanders, who do not pay municipal taxes of any kind and were never part of a municipality, are exempt from the central government \"Health Contribution\" tax (\"Sundhedsbidrag\") which stood at 8% when it was introduced on 1 January 2007 with \"Kommunalreformen\" (\"The Municipal Reform\" of 2007), because it was thought that the relatively isolated islets would otherwise be abandoned. This tax replaced the county tax, which the islanders were also exempt from paying, not being part of a county. From 1 January 2019 this tax will be abolished, while income taxes in the lowest bracket will be raised simultaneously instead with this tax being phased out. From 2012, the tax was lowered by one percentage point per year, reaching 1% in 2018.\n"}
{"id": "172331", "url": "https://en.wikipedia.org/wiki?curid=172331", "title": "Gene pool", "text": "Gene pool\n\nThe gene pool is the set of all genes, or genetic information, in any population, usually of a particular species.\n\nA large gene pool indicates extensive genetic diversity, which is associated with robust populations that can survive bouts of intense selection. Meanwhile, low genetic diversity (see inbreeding and population bottlenecks) can cause reduced biological fitness and an increased chance of extinction, although as explained by genetic drift new genetic variants, that may cause an increase in the fitness of organisms, are more likely to fix in the population if it is rather small.\n\nWhen all individuals in a population are identical with regard to a particular phenotypic trait, the population is said to be 'monomorphic'. When the individuals show several variants of a particular trait they are said to be polymorphic.\n\nThe Russian geneticist Aleksandr Sergeevich Serebrovskii first formulated the concept in the 1920s as \"genofond\" (gene fund), a word that was imported to the United States from the Soviet Union by Theodosius Dobzhansky, who translated it into English as “gene pool.”\n\nHarlan and de Wet (1971) proposed classifying each crop and its related species by gene pools rather than by formal taxonomy.\n\nGene pool centres refers to areas on the earth where important crop plants and domestic animals originated. They have an extraordinary range of the wild counterparts of cultivated plant species and useful tropical plants.\nGene pool centres also contain different sub tropical and temperate region species.\n\n"}
{"id": "35313285", "url": "https://en.wikipedia.org/wiki?curid=35313285", "title": "Genoa low", "text": "Genoa low\n\nA Genoa low (also known as Genoa Cyclogenesis, Ligurian Depression, or V(5)-track cyclone) is a cyclone that forms or intensifies from a pre-existing cyclone to the south of the Alps over the Gulf of Genoa, Ligurian Sea, Po Valley and northern Adriatic. Vb cyclones are rare events which occur on average only 2.3 times per year.\n\nThe northwestern Mediterranean and the Gulf of Genoa in particular, are not only a transition area for passing cyclones, but are frequently areas of cyclogenesis. \nLow pressure areas move into or are formed as a result of North Atlantic air entering the Mediterranean Sea between the Alps and the Massif Central, via the Rhone Valley, or via the Carcassonne gap between the Pyrenees and Massif Central. This cold and moist air enters the Mediterranean basin, and is deflected by the high mountains of northwest Corsica, which divert the air mass to the northeast, triggering cool and wet Libeccio winds in response into to the Ligurian Sea, which in turn hit the western Apennines located in the immediate vicinity of the sea.\n\nSeveral factors that have special relevance in the development of depressions south of the Alps are:\n\n\nA complex interaction is established between the orography of Liguria and the contrast between the cold and humid air mass and the warmer water of the Ligurian Sea, the process ends with the formation of a low pressure area over the Ligurian Sea, just near the city of Genoa. Genoa low cyclogenesis can occur at any time of year, though usually situated further south during the summer. Cyclogenesis is shifted to the east depending on the amount of cold air entering the Po Valley, and generally shifts to the Gulf of Venice when little enters the valley.\n\nThe depressions bear rain, often intense, on the Ligurian coast and hills of Tuscany, due to orographic lift which affects the southern side of the Apennines. The area of low pressure is slow moving, and may follow a trajectory from west to east, then going on to affect the regions of the Adriatic, or move from the north-west to south-east down along the Tyrrhenian Sea: in this last case, the structure will reach the same cyclonic area of formation of Tyrrhenian depressions, although not related to the latter.\n\nMost Genoa lows remain stationary or leave a residual trough to the south of the Alps. Three principal tracks which they typically follow were identified by Wilhelm Jakob van Bebber who classified European windstorm tracks (\"Zyklonenbahnen\" in German) in 1891. To this date, track V of the latter group has remained in common use, unlike the large majority of van Bebber's tracks. The V track is linked to flooding events in central and eastern Europe, low pressure areas (south of the Alps) can track across France into the Mediterranean Sea where they pick up additional moisture, or form, and then move into central and eastern or southern Europe. the tracks diverge from the Genoa low formation area along the following pathways.\n\nA strong southwesterly flow in the upper atmosphere leads the lows to the northeast and north-northeast, (\"Zugstrasse Vb\" Van Bebber) towards the Vienna Basin. The lows then glide over colder and denser air from the northwest and are lifted orographically by the Bohemian Massif, Ore Mountains, Sudetes, Beskids and Tatra Mountains. The warm and moist air masses cause prolonged and abundant precipitation during slow Meridional flow over the upper catchments of both the southern and northern European Watershed. Flooding then progresses down their major rivers of central Europe. This 'Vb-track' displays a high potential for large summer floods in Europe. Although the link between large summer floods and the Vb track have also been described as having a significant but weak correlation.\n\nExamples of flooding events which follow this pattern are: \n\nThe Vc track draws the lows across the Panonian plain towards the Carpathian Mountains and on towards western Ukraine and Moldova.\n\nIf there is a strong anticyclone over the Balkans, Turkey and the Black Sea, the usual track of the low is southeasterly, skirting the northern coast of the Mediterranean Sea. It is this track that moves the low across the Tyrrhenian Sea. Even after the primary low has moved out of the Tyrrhenian Sea-central Mediterranean area, if a residual trough remains south of the Alps, as is often the case, new centers can develop and occasionally move southeastward along the west coast of Italy. It is also common that a Genoa Low that has moved to the southwest will stall and become stationary, just to the west of the foot of the Italian boot, and this often will be associated with new low centers developing to the east over the Ionian Sea. In this case, gale force Bora are typically generated by the time the depression moves into the Ionian Sea.\n\n"}
{"id": "14458", "url": "https://en.wikipedia.org/wiki?curid=14458", "title": "Hail", "text": "Hail\n\nHail is a form of solid precipitation. It is distinct from ice pellets (American English \"sleet\"), though the two are often confused. It consists of balls or irregular lumps of ice, each of which is called a hailstone. Ice pellets fall generally in cold weather while hail growth is greatly inhibited during cold surface temperatures.\n\nUnlike other forms of water ice such as graupel, which is made of rime, and ice pellets, which are smaller and translucent, hailstones usually measure between and in diameter. The METAR reporting code for hail or greater is GR, while smaller hailstones and graupel are coded GS.\n\nHail is possible within most thunderstorms as it is produced by cumulonimbus, and within of the parent storm. Hail formation requires environments of strong, upward motion of air with the parent thunderstorm (similar to tornadoes) and lowered heights of the freezing level. In the mid-latitudes, hail forms near the interiors of continents, while in the tropics, it tends to be confined to high elevations.\n\nThere are methods available to detect hail-producing thunderstorms using weather satellites and weather radar imagery. Hailstones generally fall at higher speeds as they grow in size, though complicating factors such as melting, friction with air, wind, and interaction with rain and other hailstones can slow their descent through Earth's atmosphere. Severe weather warnings are issued for hail when the stones reach a damaging size, as it can cause serious damage to human-made structures and, most commonly, farmers' crops.\n\nAny thunderstorm which produces hail that reaches the ground is known as a hailstorm. Hail has a diameter of or more. Hailstones can grow to and weigh more than .\n\nUnlike ice pellets, hailstones are layered and can be irregular and clumped together. Hail is composed of transparent ice or alternating layers of transparent and translucent ice at least thick, which are deposited upon the hailstone as it travels through the cloud, suspended aloft by air with strong upward motion until its weight overcomes the updraft and falls to the ground. Although the diameter of hail is varied, in the United States, the average observation of damaging hail is between 2.5 cm (1 in) and golf ball-sized (1.75 in).\n\nStones larger than 2 cm (0.80 in) are usually considered large enough to cause damage. The Meteorological Service of Canada issues severe thunderstorm warnings when hail that size or above is expected. The US National Weather Service has a 2.5 cm (1 in) or greater in diameter threshold, effective January 2010, an increase over the previous threshold of ¾-inch hail. Other countries have different thresholds according to local sensitivity to hail; for instance grape growing areas could be adversely impacted by smaller hailstones. Hailstones can be very large or very small, depending on how strong the updraft is: weaker hailstorms produce smaller hailstones than stronger hailstorms (such as supercells).\n\nHail forms in strong thunderstorm clouds, particularly those with intense updrafts, high liquid water content, great vertical extent, large water droplets, and where a good portion of the cloud layer is below freezing . These types of strong updrafts can also indicate the presence of a tornado. The growth rate of hailstones is impacted by factors such as higher elevation, lower freezing zones, and wind shear.\n\nLike other precipitation in cumulonimbus clouds, hail begins as water droplets. As the droplets rise and the temperature goes below freezing, they become supercooled water and will freeze on contact with condensation nuclei. A cross-section through a large hailstone shows an onion-like structure. This means the hailstone is made of thick and translucent layers, alternating with layers that are thin, white and opaque. Former theory suggested that hailstones were subjected to multiple descents and ascents, falling into a zone of humidity and refreezing as they were uplifted. This up and down motion was thought to be responsible for the successive layers of the hailstone. New research, based on theory as well as field study, has shown this is not necessarily true.\n\nThe storm's updraft, with upwardly directed wind speeds as high as , blows the forming hailstones up the cloud. As the hailstone ascends it passes into areas of the cloud where the concentration of humidity and supercooled water droplets varies. The hailstone’s growth rate changes depending on the variation in humidity and supercooled water droplets that it encounters. The accretion rate of these water droplets is another factor in the hailstone’s growth. When the hailstone moves into an area with a high concentration of water droplets, it captures the latter and acquires a translucent layer. Should the hailstone move into an area where mostly water vapour is available, it acquires a layer of opaque white ice.\n\nFurthermore, the hailstone’s speed depends on its position in the cloud’s updraft and its mass. This determines the varying thicknesses of the layers of the hailstone. The accretion rate of supercooled water droplets onto the hailstone depends on the relative velocities between these water droplets and the hailstone itself. This means that generally the larger hailstones will form some distance from the stronger updraft where they can pass more time growing. As the hailstone grows it releases latent heat, which keeps its exterior in a liquid phase. Because it undergoes 'wet growth', the outer layer is \"sticky\" (i.e. more adhesive), so a single hailstone may grow by collision with other smaller hailstones, forming a larger entity with an irregular shape.\n\nHail can also undergo 'dry growth' in which the latent heat release through freezing is not enough to keep the outer layer in a liquid state. Hail forming in this manner appears opaque due to small air bubbles that become trapped in the stone during rapid freezing. These bubbles coalesce and escape during the 'wet growth' mode, and the hailstone is more clear. The mode of growth for a hailstone can change throughout its development, and this can result in distinct layers in a hailstone's cross-section.\n\nThe hailstone will keep rising in the thunderstorm until its mass can no longer be supported by the updraft. This may take at least 30 minutes based on the force of the updrafts in the hail-producing thunderstorm, whose top is usually greater than 10 km high. It then falls toward the ground while continuing to grow, based on the same processes, until it leaves the cloud. It will later begin to melt as it passes into air above freezing temperature.\n\nThus, a unique trajectory in the thunderstorm is sufficient to explain the layer-like structure of the hailstone. The only case in which multiple trajectories can be discussed is in a multicellular thunderstorm, where the hailstone may be ejected from the top of the \"mother\" cell and captured in the updraft of a more intense \"daughter\" cell. This, however, is an exceptional case.\n\nHail is most common within continental interiors of the mid-latitudes, as hail formation is considerably more likely when the freezing level is below the altitude of . Movement of dry air into strong thunderstorms over continents can increase the frequency of hail by promoting evaporational cooling which lowers the freezing level of thunderstorm clouds giving hail a larger volume to grow in. Accordingly, hail is less common in the tropics despite a much higher frequency of thunderstorms than in the mid-latitudes because the atmosphere over the tropics tends to be warmer over a much greater altitude. Hail in the tropics occurs mainly at higher elevations.\n\nHail growth becomes vanishingly small when air temperatures fall below as supercooled water droplets become rare at these temperatures. Around thunderstorms, hail is most likely within the cloud at elevations above . Between and , 60 percent of hail is still within the thunderstorm, though 40 percent now lies within the clear air under the anvil. Below , hail is equally distributed in and around a thunderstorm to a distance of .\n\nHail occurs most frequently within continental interiors at mid-latitudes and is less common in the tropics, despite a much higher frequency of thunderstorms than in the mid-latitudes. Hail is also much more common along mountain ranges because mountains force horizontal winds upwards (known as orographic lifting), thereby intensifying the updrafts within thunderstorms and making hail more likely. The higher elevations also result in there being less time available for hail to melt before reaching the ground. One of the more common regions for large hail is across mountainous northern India, which reported one of the highest hail-related death tolls on record in 1888. China also experiences significant hailstorms. Central Europe and southern Australia also experience a lot of hailstorms. Regions where hailstorms frequently occur are southern and western Germany, northern and eastern France, and southern and eastern Benelux. In southeastern Europe, Croatia and Serbia experience frequent occurrences of hail.\n\nIn North America, hail is most common in the area where Colorado, Nebraska, and Wyoming meet, known as \"Hail Alley\". Hail in this region occurs between the months of March and October during the afternoon and evening hours, with the bulk of the occurrences from May through September. Cheyenne, Wyoming is North America's most hail-prone city with an average of nine to ten hailstorms per season. To the north of this area and also just downwind of the Rocky Mountains is the Hailstorm Alley region of Alberta, which also experiences an increased incidence of significant hail events.\n\nWeather radar is a very useful tool to detect the presence of hail-producing thunderstorms. However, radar data has to be complemented by a knowledge of current atmospheric conditions which can allow one to determine if the current atmosphere is conducive to hail development.\n\nModern radar scans many angles around the site. Reflectivity values at multiple angles above ground level in a storm are proportional to the precipitation rate at those levels. Summing reflectivities in the Vertically Integrated Liquid or VIL, gives the liquid water content in the cloud. Research shows that hail development in the upper levels of the storm is related to the evolution of VIL. VIL divided by the vertical extent of the storm, called VIL density, has a relationship with hail size, although this varies with atmospheric conditions and therefore is not highly accurate. Traditionally, hail size and probability can be estimated from radar data by computer using algorithms based on this research. Some algorithms include the height of the freezing level to estimate the melting of the hailstone and what would be left on the ground.\n\nCertain patterns of reflectivity are important clues for the meteorologist as well. The three body scatter spike is an example. This is the result of energy from the radar hitting hail and being deflected to the ground, where they deflect back to the hail and then to the radar. The energy took more time to go from the hail to the ground and back, as opposed to the energy that went directly from the hail to the radar, and the echo is further away from the radar than the actual location of the hail on the same radial path, forming a cone of weaker reflectivities.\n\nMore recently, the polarization properties of weather radar returns have been analyzed to differentiate between hail and heavy rain. The use of differential reflectivity (formula_1), in combination with horizontal reflectivity (formula_2) has led to a variety of hail classification algorithms. Visible satellite imagery is beginning to be used to detect hail, but false alarm rates remain high using this method.\n\nThe size of hailstones is best determined by measuring their diameter with a ruler. In the absence of a ruler, hailstone size is often visually estimated by comparing its size to that of known objects, such as coins. Using the objects such as hen's eggs, peas, and marbles for comparing hailstone sizes is imprecise, due to their varied dimensions. The UK organisation, TORRO, also scales for both hailstones and hailstorms. When observed at an airport, METAR code is used within a surface weather observation which relates to the size of the hailstone. Within METAR code, GR is used to indicate larger hail, of a diameter of at least . GR is derived from the French word grêle. Smaller-sized hail, as well as snow pellets, use the coding of GS, which is short for the French word grésil.\n\nTerminal velocity of hail, or the speed at which hail is falling when it strikes the ground, varies. It is estimated that a hailstone of in diameter falls at a rate of , while stones the size of in diameter fall at a rate of . Hailstone velocity is dependent on the size of the stone, friction with air it is falling through, the motion of wind it is falling through, collisions with raindrops or other hailstones, and melting as the stones fall through a warmer atmosphere. As hail stones are not perfect spheres it is difficult to calculate their speed accurately.\n\nMegacryometeors, large rocks of ice that are not associated with thunderstorms, are not officially recognized by the World Meteorological Organization as \"hail,\" which are aggregations of ice associated with thunderstorms, and therefore records of extreme characteristics of megacryometeors are not given as hail records.\n\n\nHail can cause serious damage, notably to automobiles, aircraft, skylights, glass-roofed structures, livestock, and most commonly, crops. Hail damage to roofs often goes unnoticed until further structural damage is seen, such as leaks or cracks. It is hardest to recognize hail damage on shingled roofs and flat roofs, but all roofs have their own hail damage detection problems. Metal roofs are fairly resistant to hail damage, but may accumulate cosmetic damage in the form of dents and damaged coatings.\n\nHail is one of the most significant thunderstorm hazards to aircraft. When hailstones exceed in diameter, planes can be seriously damaged within seconds. The hailstones accumulating on the ground can also be hazardous to landing aircraft. Hail is also a common nuisance to drivers of automobiles, severely denting the vehicle and cracking or even shattering windshields and windows. Wheat, corn, soybeans, and tobacco are the most sensitive crops to hail damage. Hail is one of Canada's most expensive hazards. Rarely, massive hailstones have been known to cause concussions or fatal head trauma. Hailstorms have been the cause of costly and deadly events throughout history. One of the earliest known incidents occurred around the 9th century in Roopkund, Uttarakhand, India, where 200 to 600 nomads seem to have died of injuries from hail the size of cricket balls. The largest hailstone in terms of diameter and weight ever recorded in the United States fell on July 23, 2010 in Vivian, South Dakota; it measured in diameter and in circumference, weighing in at . This broke the previous record for diameter set by a hailstone 7 inches diameter and 18.74 inches circumference (still the greatest \"circumference\" hailstone) which fell in Aurora, Nebraska in the United States on June 22, 2003, as well as the record for weight, set by a hailstone of that fell in Coffeyville, Kansas in 1970.\n\nNarrow zones where hail accumulates on the ground in association with thunderstorm activity are known as hail streaks or hail swaths, which can be detectable by satellite after the storms pass by. Hailstorms normally last from a few minutes up to 15 minutes in duration. Accumulating hail storms can blanket the ground with over of hail, cause thousands to lose power, and bring down many trees. Flash flooding and mudslides within areas of steep terrain can be a concern with accumulating hail.\n\nDepths of up to have been reported. A landscape covered in accumulated hail generally resembles one covered in accumulated snow and any significant accumulation of hail has the same restrictive effects as snow accumulation, albeit over a smaller area, on transport and infrastructure. Accumulated hail can also cause flooding by blocking drains, and hail can be carried in the floodwater, turning into a snow-like slush which is deposited at lower elevations.\n\nOn somewhat rare occasions, a thunderstorm can become stationary or nearly so while prolifically producing hail and significant depths of accumulation do occur; this tends to happen in mountainous areas, such as the July 29, 2010 case of a foot of hail accumulation in Boulder County, Colorado. On June 5, 2015, hail up to four feet deep fell on one city block in Denver, Colorado. The hailstones, described as between the size of bumble bees and ping pong balls, were accompanied by rain and high winds. The hail fell in only the one area, leaving the surrounding area untouched. It fell for one and a half hours between 10 p.m. and 11:30 p.m. A meteorologist for the National Weather Service in Boulder said, \"It's a very interesting phenomenon. We saw the storm stall. It produced copious amounts of hail in one small area. It's a meteorological thing.\" Tractors used to clear the area filled more than 30 dump-truck loads of hail.\n\nResearch focused on four individual days that accumulated more than 5.9 inches (15 cm) of hail in 30 minutes on the Colorado front range has shown that these events share similar patterns in observed synoptic weather, radar, and lightning characteristics, suggesting the possibility of predicting these events prior to their occurrence. A fundamental problem in continuing research in this area is that, unlike hail diameter, hail depth is not commonly reported. The lack of data leaves researchers and forecasters in the dark when trying to verify operational methods. A cooperative effort between the University of Colorado and the National Weather Service is in progress. The joint project's goal is to enlist the help of the general public to develop a database of hail accumulation depths.\n\nDuring the Middle Ages, people in Europe used to ring church bells and fire cannons to try to prevent hail, and the subsequent damage to crops. Updated versions of this approach are available as modern hail cannons. Cloud seeding after World War II was done to eliminate the hail threat, particularly across the Soviet Union – where it was claimed a 70 to 98 percent reduction in crop damage from hail storms was achieved by deploying silver iodide in clouds using rockets and artillery shells. Hail suppression programs have been undertaken by 15 countries between 1965 and 2005.\n\n\n\n"}
{"id": "998156", "url": "https://en.wikipedia.org/wiki?curid=998156", "title": "Haze", "text": "Haze\n\nHaze is traditionally an atmospheric phenomenon in which dust, smoke, and other dry particulates obscure the clarity of the sky. The World Meteorological Organization manual of codes includes a classification of horizontal obscuration into categories of fog, ice fog, steam fog, mist, haze, smoke, volcanic ash, dust, sand, and snow. Sources for haze particles include farming (ploughing in dry weather), traffic, industry, and wildfires.\n\nSeen from afar (e.g. an approaching airplane) and depending on the direction of view with respect to the Sun, haze may appear brownish or bluish, while mist tends to be bluish grey. Whereas haze often is thought of as a phenomenon of dry air, mist formation is a phenomenon of humid air. However, haze particles may act as condensation nuclei for the subsequent formation of mist droplets; such forms of haze are known as \"wet haze.\"\n\nHaze also occurs when there is too much pollution in the air while there is also dust\n\nIn meteorological literature, the word \"haze\" is generally used to denote visibility-reducing aerosols of the wet type. Such aerosols commonly arise from complex chemical reactions that occur as sulfur dioxide gases emitted during combustion are converted into small droplets of sulfuric acid. The reactions are enhanced in the presence of sunlight, high relative humidity, and stagnant air flow. A small component of wet-haze aerosols appear to be derived from compounds released by trees, such as terpenes. For all these reasons, wet haze tends to be primarily a warm-season phenomenon. Large areas of haze covering many thousands of kilometers may be produced under favorable conditions each summer.\n\nHaze often occurs when dust and smoke particles accumulate in relatively dry air. When weather conditions block the dispersal of smoke and other pollutants they concentrate and form a usually low-hanging shroud that impairs visibility and may become a respiratory health threat. Industrial pollution can result in dense haze, which is known as smog.\n\nSince 1991, haze has been a particularly acute problem in Southeast Asia. The main source of the haze has been fires occurring in Sumatra and Borneo. In response to the 1997 Southeast Asian haze, the ASEAN countries agreed on a Regional Haze Action Plan (1997). In 2002, all ASEAN countries signed the Agreement on Transboundary Haze Pollution, but the pollution is still a problem today. Under the agreement, the ASEAN secretariat hosts a co-ordination and support unit. During the 2013 Southeast Asian haze, Singapore experienced a record high pollution level, with the 3-hour Pollution Standards Index reaching a record high of 401.\n\nIn the United States, the Interagency Monitoring of Protected Visual Environments (IMPROVE) program was developed as a collaborative effort between the US EPA and the National Park Service in order to establish the chemical composition of haze in National Parks and establish air pollution control measures in order to restore the visibility to pre-industrial levels. Additionally, the Clean Air Act requires that any current visibility problems be remedied, and future visibility problems be prevented, in 156 Class I Federal areas located throughout the United States. A full list of these areas is available on EPA's website.\n\nHaze is no longer a domestic problem. It has become one of the causes of international disputes among neighboring countries. Haze migrates to adjacent countries and thereby pollutes other countries as well.\nOne of the most recent problems concerned the two neighboring countries Malaysia and Indonesia. In 2013, due to forest fires in Indonesia, the capital city of Malaysia Kuala Lumpur and surrounding areas became shrouded in a pall of noxious fumes, smelling of ash and coal for more than a week, in the country’s worst environmental crisis since 1997. The main sources of the haze are Indonesia’s Sumatra Island, Kalimantan, and Riau, where farmers, plantation owners and miners have set hundreds of fires in the forests to clear land during dry weather. Winds blow most of the fumes across the narrow Strait of Malacca to Malaysia, although parts of Indonesia are also affected. The 2015 Southeast Asian haze was a major crisis.\n\nHaze causes issues in the area of terrestrial photography, where the penetration of large amounts of dense atmosphere may be necessary to image distant subjects. This results in the visual effect of a loss of contrast in the subject, due to the effect of light scattering through the haze particles. For these reasons, sunrise and sunset colors appear subdued on hazy days, and stars may be obscured at night. In some cases, attenuation by haze is so great that, toward sunset, the sun disappears altogether before reaching the horizon. \n\nHaze can be defined as an aerial form of the Tyndall effect therefore unlike other atmospheric effects such as cloud and fog, haze is spectrally selective: shorter (blue) wavelengths are scattered more, and longer (red/infrared) wavelengths are scattered less. For this reason, many super-telephoto lenses often incorporate yellow filters or coatings to enhance image contrast. Infrared (IR) imaging may also be used to penetrate haze over long distances, with a combination of IR-pass optical filters (such as the Wratten 89B) and IR-sensitive detector. \n\n\n"}
{"id": "18611260", "url": "https://en.wikipedia.org/wiki?curid=18611260", "title": "Hermaphrodite", "text": "Hermaphrodite\n\nIn biology, a hermaphrodite () is an organism that has complete or partial reproductive organs and produces gametes normally associated with both male and female sexes. Many taxonomic groups of animals (mostly invertebrates) do not have separate sexes. In these groups, hermaphroditism is a normal condition, enabling a form of sexual reproduction in which either partner can act as the \"female\" or \"male.\" For example, the great majority of tunicates, pulmonate snails, opisthobranch snails, earthworms and slugs are hermaphrodites. Hermaphroditism is also found in some fish species and to a lesser degree in other vertebrates. Most plants are also hermaphrodites.\n\nHistorically, the term \"hermaphrodite\" has also been used to describe ambiguous genitalia and gonadal mosaicism in individuals of gonochoristic species, especially human beings. The word \"intersex\" has come into preferred usage for humans, since the word \"hermaphrodite\" is considered to be misleading and stigmatizing, as well as \"scientifically specious and clinically problematic.\"\n\nA rough estimate of the number of hermaphroditic animal species is 65,000. Since the estimated total number of animal species is 8.6 million, the percentage of animal species that are hermaphroditic is about 0.7%. Arthropods are the phylum with the largest number of species. Most hermaphroditic species exhibit some degree of self-fertilization. The distribution of self-fertilization rates among animals is similar to that of plants, suggesting that similar processes are operating to direct the evolution of selfing in animals and plants.\n\nThe term derives from the , from , which derives from Hermaphroditus (Ἑρμαφρόδιτος), the son of Hermes and Aphrodite in Greek mythology. According to Ovid, he fused with the nymph Salmacis resulting in one individual possessing physical traits of male and female sexes; according to the earlier Diodorus Siculus, he was born with a physical body combining male and female sexes. The word \"hermaphrodite\" entered the English lexicon as early as the late fourteenth century. Alexander ab Alexandro stated, using the term \"hermaphrodite,\" that the people who bore the sexes of both man and woman were regarded by the Athenians and the Romans as monsters, and thrown into the sea at Athens and into the Tiber at Rome.\n\nSequential hermaphrodites (dichogamy) occur in species in which the individual is born as one sex, but can later change into the opposite sex. This contrasts simultaneous hermaphrodites, in which an individual may possess fully functional male and female genitalia. Sequential hermaphroditism is common in fish (particularly teleost fish) and many gastropods (such as the common slipper shell), and some flowering plants. Sequential hermaphrodites can only change sex once. Sequential hermaphroditism can best be understood in terms of behavioral ecology and evolutionary life history theory, as described in the size-advantage mode first proposed by Michael T. Ghiselin which states that if an individual of a certain sex could significantly increase its reproductive success after reaching a certain size, it would be to their advantage to switch to that sex.\n\nSequential hermaphrodites can be divided into three broad categories:\nDichogamy can have both conservation-related implications for humans, as mentioned above, as well as economic implications. For instance, groupers are favoured fish for eating in many Asian countries and are often aquacultured. Since the adults take several years to change from female to male, the broodstock are extremely valuable individuals.\n\nA simultaneous (or synchronous) hermaphrodite (or homogamous) is an adult organism that has both male and female sexual organs at the same time. Self-fertilization often occurs.\n\nWhen spotted hyenas were first discovered by explorers, they were thought to be hermaphrodites. Early observations of spotted hyenas in the wild led researchers to believe that all spotted hyenas, male and female, were born with what appeared to be a penis. The apparent penis in female spotted hyenas is in fact an enlarged clitoris, which contains an external birth canal. It can be difficult to determine the sex of wild spotted hyenas until sexual maturity, when they may become pregnant. When a female spotted hyena gives birth, they pass the cub through the cervix internally, but then pass it out through the elongated clitoris.\n\n\"Hermaphrodite\" is used in older literature to describe any person whose physical characteristics do not neatly fit male or female classifications, but some people advocate to replace the term with intersex. Intersex describes a wide variety of combinations of what are considered male and female biology. Intersex biology may include, for example, ambiguous-looking external genitalia, karyotypes that include mixed XX and XY chromosome pairs (46XX/46XY, 46XX/47XXY or 45X/XY mosaic).\n\nClinically, medicine currently describes intersex people as having disorders of sex development, a term vigorously contested. This is particularly because of a relationship between medical terminology and medical intervention. Intersex civil society organizations, and many human rights institutions, have criticized medical interventions designed to make intersex bodies more typically male or female.\n\nSome people who are intersex, such as some of those with androgen insensitivity syndrome, outwardly appear completely female or male, frequently without realizing they are intersex. Other kinds of intersex conditions are identified immediately at birth because those with the condition have a sexual organ larger than a clitoris and smaller than a penis.\n\nSome humans were historically termed true hermaphrodites if their gonadal tissue contained both testicular and ovarian tissue, or pseudohermaphrodites if their external appearance (phenotype) differed from sex expected from internal gonads. This language has fallen out of favor due to misconceptions and pejorative connotations associated with the terms, and also a shift to nomenclature based on genetics.\n\nIntersex is in some caused by unusual sex hormones; the unusual hormones may be caused by an atypical set of sex chromosomes. One possible pathophysiologic explanation of intersex in humans is a parthenogenetic division of a haploid ovum into two haploid ova. Upon fertilization of the two ova by two sperm cells (one carrying an X and the other carrying a Y chromosome), the two fertilized ova are then fused together resulting in a person having dual genitalial, gonadal (ovotestes) and genetic sex. Another common cause of being intersex is the crossing over of the SRY from the Y chromosome to the X chromosome during meiosis. The SRY is then activated in only certain areas, causing development of testes in some areas by beginning a series of events starting with the upregulation of SOX9, and in other areas not being active (causing the growth of ovarian tissues). Thus, testicular and ovarian tissues will both be present in the same individual.\n\nFetuses before sexual differentiation are sometimes described as female by doctors explaining the process. This is technically not true. Before this stage, humans are simply undifferentiated and possess a Müllerian duct, a Wolffian duct, and a genital tubercle.\n\nHermaphrodite is used in botany to describe a flower that has both staminate (male, pollen-producing) and carpellate (female, ovule-producing) parts. This condition is seen in many common garden plants. A closer analogy to hermaphroditism in botany is the presence of separate male and female flowers on the same individual—such plants are called monoecious. Monoecy is especially common in conifers, but occurs in only about 7% of angiosperm species. The condition also occurs in some algae.\n\n\n\n"}
{"id": "22769053", "url": "https://en.wikipedia.org/wiki?curid=22769053", "title": "IIHF World Ranking", "text": "IIHF World Ranking\n\nThe IIHF World Ranking is a ranking of the performance of the national ice hockey teams of member countries of the International Ice Hockey Federation (IIHF). It is based on a formula giving points for each team's placings at IIHF-sanctioned tournaments over the previous four years. The ranking is used to determine seedings and qualification requirements for future IIHF tournaments. The current leader in rankings is Canada in men's play and the United States in women's play.\n\nThe system was approved at the IIHF congress of September 2003. According to IIHF President René Fasel, the system was designed to be simple to understand and \"reflect the long-term quality of all national hockey programs and their commitment to international hockey.\"\n\nThe ranking is used to determine the seeding of the teams for the next World Championship and to select the teams which can participate in Winter Olympics without playing in the qualifying round. For example, for the 2014 Winter Olympics, the first nine teams of the Men's World Ranking and the first six of the Women's World Ranking were pre-qualified. Qualification for the men's tournament at the 2014 Winter Olympics was structured around the 2012 ranking. Twelve spots were made available for teams. The top nine teams in the World Ranking after the 2012 Men's World Ice Hockey Championships received automatic berths into the Ice Hockey event. All IIHF teams had an opportunity to qualify for the event. Teams that wished to participate ranked below 29th played a preliminary qualification in September 2012. The winner of the preliminary and teams ranked 19-29th were divided in three groups to play in the pre-qualification round in November 2012. The winner of each pre-qualification group and teams ranked 10-18 were divided in three groups to play in the final qualification in February 2013. The winner of each group then joined the nine top-ranked teams in the Olympics in 2014.\n\nThe women's tournament uses a similar qualification format. The top six teams in the IIHF Women's World Ranking after the 2012 IIHF Women's World Championship received automatic berths into the ice hockey event. Lower ranked teams had an opportunity to qualify for the event. Teams ranked 19th and below were divided into two groups where they played in a preliminary qualification round in the autumn of 2012. The two winners and teams ranked 13-18 were divided into two groups where they played in the pre-qualification round in November 2012. The two group winners from the round advanced to the final qualification round, where the teams ranked seventh through twelfth joined them.\n\nThe world ranking is based on the final positions of the last four Ice Hockey World Championships and last Olympic ice hockey tournament. Points are assigned according to a team's final placement in the World Championship or the Olympic tournament. The world champion receives 1200 points and then a 20-point interval is used between teams. However, a 40-point interval is used between gold and silver, silver and bronze, fourth and fifth, and eighth and ninth. This is used as a bonus for the teams who reach the quarter-finals, the semi-finals, the final and for winning the gold medal.\n\nPoints awarded in the current year are valued at the full amount. Points award in the prior years decline linearly by 25% until the fifth year when they are dropped from the calculation. For example, if after the 2018 Olympics a team had won the gold medal in the last four championships and the last Olympic tournament their score would be 4200:\n\nThe Men's 2018 ranking is based on the performance at the World Championships of 2018, 2017, 2016, and 2015, and at the 2018 Olympic Ice Hockey Tournament in Pyeongchang, South Korea.\n\nFor the 2018 Winter Olympics, Russia's Olympic Committee and many of its athletes were banned for illegal doping. The IIHF opposed an outright ban on all Russian players, concerned that the KHL would disallow its players from participating in the tournament, as the NHL had done. The ice hockey team from the Russian hockey federation played under the banner of the International Olympic Committee as \"Olympic Athletes from Russia\", along with other Russian athletes not banned for doping.\n\nThe following table lists the current ranking following the 2018 Winter Olympics and the 2018 Men's World Ice Hockey Championships. Scores in \"italics\" represent minimum possible scores for unfinished tournaments. All tournament's points have their full value displayed, while the ranking is calculated by adding the current year's tournament points to the depreciated previous three years' tournament points as explained above. The depreciated percentages are shown in the column headings, first for the current total, then for the new total. The \"Total\" columns are the sums of the current tournament points and the depreciated values for past tournaments. The \"+/–\" columns indicate the increase or decrease in ranking since the last tournament. A dash in a tournament column indicates that the country did not participate.\nSource: IIHF\n\nThe Women's 2018 ranking is based on the performance at the World Championships of 2017, 2016, 2015, and 2014, and at the 2018 Olympic Ice Hockey Tournament in Pyeongchang, South Korea.\n\nFor the 2018 Winter Olympics, Russia's Olympic Committee and many of its athletes were banned due to systemic illegal doping. The IIHF opposed an outright ban on all Russian players and the ice hockey team from the Russian hockey federation played under the banner of the International Olympic Committee as \"Olympic Athletes from Russia\", along with other Russian athletes not banned for doping.\n\nThe following table lists the ranking following the 2018 Winter Olympic qualification tournaments and the 2018 Winter Olympics. Scores in \"italics\" represent minimum possible scores for unfinished tournaments. All tournament's points have their full value displayed, while the ranking is calculated by adding the current year's tournament points to the depreciated previous three years' tournament points as explained above. The depreciated percentages are shown in the column headings, first for the current total, then for the new total. The \"Total\" columns are the sums of the current tournament points and the depreciated values for past tournaments. The \"+/–\" columns indicate the increase or decrease in ranking since the last tournament. A dash in a tournament column indicates that the country did not participate.\n\nSource: IIHF.\n\n\n"}
{"id": "31577242", "url": "https://en.wikipedia.org/wiki?curid=31577242", "title": "Idosawa Fault", "text": "Idosawa Fault\n\nThe , also referred to as the Shionihara Fault, is an active earthquake fault system located in Fukushima Prefecture of Japan, to the west of Iwaki city. It mainly consists of a trace of three separate striations.\n\nThe fault was first mapped by the Active Fault Research Group in 1991 as a complex of north-northwest-striking inactive traces of fault in the Hamadōri region. It has since been compartmentalized into separate striations near Tabito-cho west of Iwaki city. The northernmost and largest of the faultlines, the North Fault, was identified in 2009 and extends roughly 24 km (15 mi) from the southeast to the northwest (N45˚W). To its southwest, two parallel faultlines, the East and Shionihara faults, extend from the south-southeast to the north-northwest (N10˚W). The faultlines are separated by 1 km (0.6 mi) and span roughly 23 km and 22 km (14 mi), respectively. The westernmost of the two, the Shionihara Fault lies near Tabito-cho and borders the small village of Shionohira, after which it was named. \n\nThe main structural trend is north-northwest–south-southeast, with sinking observed only to the south on the east side of the fault. Metamorphic rock and Cretaceous strata, as well as granite and epidiorite are distributed in the region; the fault is described as a limit to the distribution of tuff from the Neogene Period.\n\nTo the northeast of the Idosawa Fault complex lies a separate normal fault trace, which was named the Yunodake Fault (also Yunotake) in 2011. Distanced approx. 50 km (30 mi) from the Fukushima Daiichi Nuclear Power Plant, the fault had been dormant for 120,000–130,000 before it ruptured during the magnitude 7.1 M Fukushima Hamadori earthquake on 11 April 2011. Several geological surveys have since been conducted in its vicinity. Evidence of sedimentary rock layers deposited after the Late Pleistocene beneath the fault suggests that the Yunotake Fault had been in the active in the past.\n\n\n"}
{"id": "32214581", "url": "https://en.wikipedia.org/wiki?curid=32214581", "title": "Interdunal wetland", "text": "Interdunal wetland\n\nAn interdunal wetland,interdunal pond or dune slack is a water-filled depression between coastal sand dunes. It may be formed either by wind erosion or by dunal encroachment on an existing wetland. The wind erosion process involves wind scooping out sufficient sand to reach the water table, and typically occurs behind the first line of foredunes.\n\nThe Indiana Dunes contain interdunal wetlands. Many conservation efforts have been made to preserve parts of the Indiana Dunes.\n\nBecause they are typically very shallow, interdunal wetlands warm quickly, and provide an abundant source of invertebrates eaten by many species of shorebirds. Many interdunal wetlands are ephemeral, drying out during periods of low rain or low water.\n\nIn the Great Lakes region of North America, interdunal communities are typically mildly calcareous and dominated by rushes, sedges and shrubs. They are tentatively classified as G2, or globally imperiled, under the NatureServe rankings.\n\nA distinction is sometimes made between interdunal and intradunal wetlands such as pannes, which form within a single dune as part of a blowout.\n"}
{"id": "25037904", "url": "https://en.wikipedia.org/wiki?curid=25037904", "title": "Lake Nemrut", "text": "Lake Nemrut\n\nLake Nemrut () is a freshwater crater lake in Bitlis Province, eastern Turkey. It is part of Nemrut Caldera (), a volcanic caldera atop Volcano Nemrut. The caldera is a registered natural monument and the wetlandsis registered as a Ramsar site of the country.\n\nThe caldera is located west of Lake Van in the Tatvan, Ahlat and Güroymak districts of Bitlis Province. It is named after the biblical figure King Nimrod. The caldera is far from Tatvan, and from Ahlat. With its width of nearly , the crater of Nemrut Volcano is one of the largest calderas of the world. Nemrut Caldera is world's second largest one. The western half of the crater is covered by the lake. AT the summit, there are five lakes, two of them existing permanently and the others seasonal. The biggest of the lakes is Lake Nemrut in the form of crescent. It contains freshwater of colorless, odorless and drink water quality.\n\nLake Nemrut is situated at an elevation of about above main sea level. It has an area of , and its average depth is about with a maximum depth of .\n\nNemrut Caldera is on the youngest volcanic cone in Turkey, which is in a not-eroded state. This unique structural geomorphology make it a subject of scientific research.\n\nAbout 450 plant species were recorded in and around Nemrut Caldera, around 200 (44%) of them belonging to the region. The diversity of the flora points out to the variation of climate conditions in the past. Around 38 (8.4%) of the existing plant species are endemic. The upside down tulip, which grows here, is a world-famous flower. The climax vegetation of Nemrut Caldera forms the haired birch (\"Betula\") and the trembling aspen (\"Populus tremula\"). Other notable plants growing around two lakes of the caldera are the trees dwarf juniper (\"Juniperus communis\"), Norway maple (\"Acer platanoides\"), European mountain ash (\"Sorbus aucuparia\"), common buckthorn (\"Rhamnus cathartica\"), sessile oak (\"Quercus petraea\"), pedunculate oak (\"Quercus robur\"), white willow (\"Salix alba\"), and the shrubs coinwort cotoneaster (\"Cotoneaster nummularius\"), cherry plum (\"Prunus divaricata\"), grey willow (\"Salix cinerea\"), Greek juniper (\"Juniperus excelsa\"), breaking buckthorn (\"Frangula alnus\"), alder buckthorn (\"Frangula alnus\") and mahaleb cherry (\"Prunus mahaleb\"). Steppe-like vegetation is spread over wide areas in the caldera. Those are mainly species of milkvetch (\"Astragalus\"). Other subshrubs and herbaceous plants are prickly thrift (\"Acantholimon\"), sainfoin (\"Onobrychis\"), sheep's sorrel (\"Rumex acetosella\"), \"Thymus\", \"Alyssum\", sheep's fescue (\"Festuca ovina\"), \"Salvia\", \"Ranunculus\", \"Silene\", rabbitfoot clover (\"Trifolium arvense\"), \"Pimpinella\", \"Artemisia\", squarrose (\"Centaurea triumfettii\"). Reedy areas are present In the northwestern part of Lake Nemrıt.\n\nThe griffon vulture brood their eggs at the caldera, which earned its status of special protected area as habitat for breeding of velvet duck and golden eagle. The number of bird species decreased at Lake Nemrut, which is a stopover site for a lot of migrating birds, due to irregular and uncontrolled hunting. Currently observed animals in the region are the bird species partridge, duck, bee-eater, Armenian gull and mammals hare, fox, bear. The chamois has been extinct.\n\nThe caldera was registered a natural monument in 2003. The protected area around the crater lake covers . Nemrut Caldera Natural Monument () is protected in the status of a tourist attraction, a protected area of first degree and a wetland.\n\nThe government of Turkey designated the wetland of the caldera as the 14th Ramsar site of the country on April 17, 2013.\n\nIt is not permitted to cut reed in the caldera and to fish in the lake, although some livestock grazing takes place around the caldera. A winter sports and ski center was established on the southern slope of the caldera in 2007. The main thread in terms of ecology is overgrazing.\n\n"}
{"id": "25499611", "url": "https://en.wikipedia.org/wiki?curid=25499611", "title": "List of Superfund sites in Virginia", "text": "List of Superfund sites in Virginia\n\nThis is a list of Superfund sites in Virginia designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of November 2, 2017, there were 35 Superfund sites on the National Priorities List in Virginia. No additional sites are currently proposed for entry on the list. Four sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "55115440", "url": "https://en.wikipedia.org/wiki?curid=55115440", "title": "List of cycad species by country", "text": "List of cycad species by country\n\nBelow is a list of cycad species by ordered by country.\n\n\n\n\n\n\n\n\n\n\"Cycas thouarsii\" is the most geographically widespread species, and is found in Indian Ocean islands as well.\n\n\n\n\n\n\n\n\n\"Encephalartos barteri\" is the only cycad species recorded in West Africa.\n\n\n\n\n\n\"Cycas pectinata\" has the most widespread distribution in South Asia, and is the only South Asian cycad species found outside India and Sri Lanka.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe eastern coast of Australia contains the most diversity. \"Cycas seemannii\" is found in Melanesia and western Polynesia. \"Cycas micronesica\" is found in Micronesia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "29749126", "url": "https://en.wikipedia.org/wiki?curid=29749126", "title": "List of national parks of Honduras", "text": "List of national parks of Honduras\n\nNational parks of Honduras is an incomplete list of the National parks in Honduras.\n\n\n"}
{"id": "37949418", "url": "https://en.wikipedia.org/wiki?curid=37949418", "title": "List of parties to the Ramsar Convention", "text": "List of parties to the Ramsar Convention\n\nThis is a list of parties contracting to the Ramsar Convention \"on Wetlands of International Importance especially as Waterfowl Habitat\", which is also known as the Convention on Wetlands. The Convention's mission is \"“the conservation and wise use of all wetlands through local and national actions and international cooperation, as a contribution towards achieving sustainable development throughout the world”. It calls upon contracting parties to recognize the interdependence of humans and the environment as well as the ecological functions of wetlands, such as wildlife habitat, nutrient cycling, and flood control. \n\nThe Ramsar Convention is the oldest multilateral international conservation convention and the only one to deal with one habitat or ecosystem type, wetlands. The Convention's headquarters are in Gland, Switzerland, and it works closely with the International Union for the Conservation of Nature.\n\nThe Convention was held in the city of Ramsar, Iran in February 1971 and was originally contracted by seven countries when it came into force on 21 December 1975. As of February 2018 there are 170 contracting parties and over 2,000 designated sites covering over . Every contracting country has at least one Ramsar site, and 31 of the contracting countries have only one site. The country with the most sites is the United Kingdom with 170. To become a Ramsar site, a site must be nominated by a contracting country, meet at least one of nine criteria, and undergo scientific review. The Convention was most recently ratified by North Korea in 2018.\n\nThe table lists the countries contracting to the convention, the entry date of each country to the convention, the number of Ramsar sites in each country, and the total area of all Ramsar sites in each country.\n\n"}
{"id": "5759586", "url": "https://en.wikipedia.org/wiki?curid=5759586", "title": "List of psychoactive plants", "text": "List of psychoactive plants\n\nA list of plants that are used as psychoactive drugs. Some of them have been used entheogenically for millennia. The plants are listed according to the substances they contain.\n\n\"Cannabis\" (Marijuana) is a popular psychoactive plant that is often used medically and recreationally. The psychoactive substance in \"Cannabis\", THC, is unique in that it contains no nitrogen and is not an indole, tryptamine, phenethylamine, anticholinergic (deliriant), or a dissociative drug. \"Cannabis\" plants tend to vary, with different strains producing dynamic balances of cannabinoids (THC, CBD, etc.) yielding markedly different effects. Popular strains are often hybrids of \"Cannabis sativa\" and \"Cannabis indica\".\n\nSome universities and research firms currently study the medicinal effects of cannabis. Many jurisdictions have laws regulating or prohibiting the sale and use of medical and recreational cannabis.\n\nMany of the psychedelic plants contain dimethyltryptamine (DMT), which is either snorted (Virola, Yopo snuffs), vaporized, or drunk with MAOIs (Ayahuasca). It cannot simply be eaten as it is not orally active without an MAOI and it needs to be extremely concentrated to be vaporized.\n\n\"Species\", \"Alkaloid content, where given, refers to dried material\"\n\n\n\n\n1,2,3,4-Tetrahydro-6-methoxy-2,9-dimethyl-beta-carboline, Plant, 1,2,3,4-Tetrahydro-6-methoxy-2-methyl-beta-carboline, Plant, 5-Methoxy-N,N-dimethyltryptamine, Bark, 5-Methoxy-N-methyltryptamine, Bark, Bufotenin, plant, beans, Bufotenin N-oxide, Fruit, beans, N,N-Dimethyltryptamine-oxide, Fruit\n\n\n\n\n\n\nSome Graminae (grass) species contain gramine, which can cause brain damage, other organ damage, central nervous system damage and death in sheep.\n\n\n\n\n\nSpecies, Alkaloid Content (Fresh) - Alkaloid Content (Dried)\n\nBeta-carbolines are \"reversible\" MAO-A inhibitors. They are found in some plants used to make Ayahuasca. In high doses the harmala alkaloids are somewhat hallucinogenic on their own. β-carboline is a benzodiazepine receptor inverse agonist and can therefore have convulsive, anxiogenic and memory enhancing effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22477406", "url": "https://en.wikipedia.org/wiki?curid=22477406", "title": "List of rivers of the Democratic Republic of the Congo", "text": "List of rivers of the Democratic Republic of the Congo\n\nThis is a list of rivers in the Democratic Republic of the Congo. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n\n"}
{"id": "31023913", "url": "https://en.wikipedia.org/wiki?curid=31023913", "title": "Lute (material)", "text": "Lute (material)\n\nLute (from Latin \"Lutum\", meaning mud, clay etc.) was a substance used to seal and affix apparatus employed in chemistry and alchemy, and to protect component vessels against heat damage by fire; it was also used to line furnaces. \"Lutation\" was thus the act of \"cementing vessels with lute\".\n\nIn pottery, luting is a technique for joining pieces of unfired leather-hard clay together, using a wet clay slip or slurry as adhesive. The complete object is then fired. Large objects are often built up in this way, for example the figures of the Terracotta Army in ancient China. The edges being joined might be scored or cross-hatched to promote adhesion, but clay and water are the only materials used. \n\nLute was commonly used in distillation, which required airtight vessels and connectors to ensure that no vapours were lost; thus it was employed by chemists and alchemists, the latter being known to refer to it as \"\"lutum sapientiae\" or the \"lute of Wisdom\"\".\nThe earthen and glass vessels commonly employed in these processes were very vulnerable to cracking, both on heating and on cooling; one way of protecting them was by coating the vessels with lute and allowing it to set. One mixture for this purpose included \"fat earth\" (terra pinguis), Windsor loam, sand, iron filings or powdered glass, and cow's hair.\n\nAnother use for lute was to act as a safety valve, preventing the buildup of vapour pressure from shattering a vessel and possibly causing an explosion. For this purpose, a hole was bored in the flask and covered with \"luting\" material of a particular composition, which was kept soft so that excessive buildup of vapour would cause it to come away from the vessel, thus releasing the pressure safely. This process could also be performed manually by the operator removing and reaffixing the lute as required. Lute was also used to effect repairs to cracked glass vessels. In \"The Alchemist’s Experiment Takes Fire\", 1687, one alembic is exploding; the luting used to seal a receiving bottle to another alembic can be seen behind the alchemist's upraised arm.\n\nLute was frequently applied to the joints between vessels (such as retorts and receivers), making them airtight and preventing vapour from escaping; this was especially important for more penetrating \"spiritous\" vapours and required a mixture that would set hard - such as a mix of quicklime and either egg white or size etc. However a stronger lute had to be used to confine acid vapours, and for this purpose fat earth and linseed oil were mixed to form \"fat lute\", which could be rolled into cylinders of convenient size, ready for use. Where the vapour was more \"aqueous\", and less penetrating, strips of paper affixed with sizing would suffice or \"bladder long steeped in water\".\n\nAnother related use for lute was for lining furnaces, and was described as far back as the 16th century by Georg Agricola in his \"De re metallica\".\n\n\"Fat Lute\" was made of clay mixed with oil and beaten until it had the consistency of putty. It could be stored in a sealed earthenware vessel, which retained moisture and kept the material pliable. An alchemical writer of the 16th century recommended a lute made up of \"loam mixed to a compost with horse dung\" while the French chemist Chaptal used a similar mixture of \"fat earth\" and horse dung, mixed in water and formed into a soft paste.\n\n\"Linseed meal\" or \"Almond meal\" could be made into a lute by mixing with water \"or\" dissolved starch \"or\" weak glue, and used in combination with strips of rag \"or\" moistened bladder; however, it was combustible which limited its range of applications.\n\n\"Lime\" could be made into an effective lute by mixing it with egg white \"or\" glue; for sealing joints it was used in conjunction with strips of rag.\n\nLinen rags mixed with paste, \"or\" strips of \"Bladder\" soaked in warm water, then coated with paste or egg white, also served as a lute.\n\n\"Fire Lute\" was used to protect vessels from heat damage. It consisted of clay mixed with sand and either horse-hair \"or\" straw \"or\" tow (coarse, broken fibre of crops such as flax, hemp, or jute). It had to be allowed to dry thoroughly before use to be effective.\n\n\"Fusible lute\" was used to coat earthenware vessels to ensure impermeability. A mixture of Borax and slaked lime, mixed with water into a fine paste, served this purpose.\n\n\"Parker's Cement\", \"Plaster of Paris\" and \"Fusible fluxes\" (a clay and Borax mixture in 10:1 proportion, mixed to a paste in water) could all be used as lutes, rendering heat protection and air-tightness. Stourbridge clay mixed with water could withstand the highest heat of any lute.\n\n\"Hard cement\" was also commonly used to join glass vessels and fix cracks; it was composed of resin, beeswax and either brick dust or \"bole earth\", or red ochre or venetian red. \"Soft cement\", made of yellow wax, turpentine and venetian red, was also used for repair.\n\n"}
{"id": "14255345", "url": "https://en.wikipedia.org/wiki?curid=14255345", "title": "Lyonsite", "text": "Lyonsite\n\nLyonsite (CuFe(VO)) is a rare black vanadate mineral that is opaque with a metallic lustre. It crystallizes in the orthorhombic crystal system. Lyonsite often occurs as small tabular typically well formed crystals. Lyonsite has a good cleavage and a dark gray streak.\n\nLyonsite occurs as a sublimate in volcanic fumaroles. It is often associated with howardevansite and thenardite. It was first described in 1987 for an occurrence on the Izalco volcano, El Salvador. It was named for mineralogist John Bartholomew Lyons (1916–1998) of Dartmouth College. It has also been reported from a mine dump in the Lichtenberg Absetzer Mine of Thuringia, Germany.\n"}
{"id": "32500130", "url": "https://en.wikipedia.org/wiki?curid=32500130", "title": "Machar Marshes", "text": "Machar Marshes\n\nThe Machar Marshes are a large area of wetlands in the state of Upper Nile, South Sudan. Estimates of their size vary. A 1950 study put the area of swamp at 6,500 km. A 1980 study put the area of permanent swamp at 8,700 km., 60% of which was grass and forest.\n\nThe marshes are fed by waters from the Khor Machar (a distributary of the Baro River), the Yabus River and the Daga River. At times of high water they are also fed by spill from the Pibor River. The marshes are drained by the Adar River, a tributary of the White Nile.\n\n"}
{"id": "1131690", "url": "https://en.wikipedia.org/wiki?curid=1131690", "title": "Malabar Coast moist forests", "text": "Malabar Coast moist forests\n\nThe Malabar Coast moist forests is a tropical moist broadleaf forest ecoregion of southwestern India. It lies along India's Konkan and Malabar coasts, in a narrow strip between the Arabian Sea and the Western Ghats range, which runs parallel to the coast. It has an area of , and extends from northern Maharashtra through Goa, Karnataka and Kerala to Kanniyakumari in southernmost Tamil Nadu.\n\nThe ecoregion extends from sea level to the 250 meter contour of the Western Ghats. It is bounded on the east by the North Western Ghats moist deciduous forests in Maharashtra and Karnataka, and the South Western Ghats moist deciduous forests in Kerala.\n\nVery little of the natural vegetation of the ecoregion remains; it has largely been cleared for agriculture, grazing, and teak plantations.\n\nIn 1997, the World Wildlife Fund identified three protected areas in the ecoregion, with a combined area of approximately 300 km², encompassing less than 1% of the ecoregion's area. \n\n\n\n"}
{"id": "22023250", "url": "https://en.wikipedia.org/wiki?curid=22023250", "title": "Mid-Atlantic Ridge Ecosystem Project", "text": "Mid-Atlantic Ridge Ecosystem Project\n\nThe Mid-Atlantic Ridge Ecosystem Project MAR-ECO is an international research project in which scientists from 16 nations take part. Norway, represented by the Institute of Marine Research and the University of Bergen, co-ordinates the project which will enhance our understanding of occurrence, distribution and ecology of animals and animal communities along the Mid-Atlantic Ridge between Iceland and the Azores. The Mid-Atlantic Ridge is the volcanic mountain range in the middle of the ocean, marking the spreading zone between the Eurasian and American continental plates. New ocean floor is constantly being formed, and Iceland and the Azores are volcanic islands created when the mid-ocean ridge breaks the sea surface. The groups of animals to be studied are fishes, crustaceans, cephalopods (squids) and a wide range of gelatinous animals (e.g. jellyfish) living either near the seabed or in midwater above the ridge.\n\nThe research programme Census of Marine Life seriously addresses this situation and challenges marine biologists to utilize the most advanced technology to achieve true new information in areas of the ocean that were poorly studied previously. The project MAR-ECO, an element of the Census of Marine Life, rises to the challenge and investigates the diverse animal life along the vast underwater mountain chains of the open ocean.\n\nMAR-ECO adopts the most advanced technology and instruments for observing and sample the animals and to tackle the challenge of working to 3500 m depth and in rugged terrain. An international multidisciplinary team of biologists, oceanographers, and engineers is offered this rare opportunity. A number of countries have committed their best research vessels, and in the 2003-2005 and 2007-2010 field phases a number of research cruise were conducted. In 2004, a two-month major international expedition was carried out by the new Norwegian vessel RV G.O. Sars, but vessels from Iceland, Russia, Germany, the United Kingdom, USA, and Portugal have also made major contributions. In June 2003 a Russian-US cruise using the manned submersibles MIR-1 and -2 took scientists to areas never before visited by humans at 4500m below the surface.\nContributing to sustainable development\n\nMAR-ECO shall enhance the basic knowledge of ocean life and thereby contribute to a sustainable international management of marine resources and the priceless biodiversity of the marine environment. Knowledge obtained by a unified international effort carries greater weight in the policy-making processes than information gathered by isolated national research. Good science may hopefully lead to international consensus on appropriate action.\n\nMAR-ECO and the Census of Marine Life emphasises public outreach and even in the planning phase MAR-ECO has enjoyed considerable public attention and support. Expeditions to unknown depths of the oceans appear to have great appeal, both to scientists and the interested laymen of all ages.\n\nThe MAR-ECO management consists of a Norwegian secretariat, a public outreach group, and an international steering group. The co-ordinating institutions are Institute of Marine Research and the University of Bergen in Norway. \n\nMembers of the international steering group are experienced scientists from key institutions in Norway, Iceland, Portugal (Azores), France, Germany, United Kingdom, USA, Russia and Brazil. Chair: Odd Aksel Bergstad, Norway.\n\nThe project consists of 10 integrated science components dealing with different key objectives, each with dedicated teams and principal investigators. In addition, a range of education and outreach components facilitates dissemination of results to a wide audience. \n\nCommon critical tasks are funded by A.P.Sloan Foundation (USA), and national sources.\n\nThe Public Outreach Group is based in Bergen, Norway, but has associates among project participants in other countries. The group works closely with the Education and Outreach team of the Census of Marine Life based in the USA.\n\nThe Mar-ECO project presented an exhibit in the Sant Ocean Hall of the Smithsonian Museum of Natural History in Washington, DC in 2010. The exhibit featured specimens, photography, art, models, and multimedia about the discoveries of the program.\n\n"}
{"id": "47278951", "url": "https://en.wikipedia.org/wiki?curid=47278951", "title": "Northeast Georgia Rise", "text": "Northeast Georgia Rise\n\nThe Northeast Georgia Rise is an oceanic plateau located in the South Atlantic Ocean northeast of South Georgia Island and west of the Falkland Plateau.\n\nThe rise is separated from South Georgia Island by the Northeast Georgia Passage. The Georgia Basin surrounds the northern end of the rise.\nThe Agulhas-Falkland Fracture Zone (AFFZ) stretches across the Atlantic north of the Northeast Georgia Rise. A group of small seamounts north of the rise are aligned with a gap in the AFFZ. East of this gap the AFFZ is a single ridge with an average height of but west of the gap the AFFZ is a double ridge with an average height of .\n\nOn the eastern flank of the rise is a prominent ridge, the Soledad Ridge, about tall. It has the same orientation as the southeastern part of the rise. It is a basement-feature in which bottom-water have scoured a channel. Both the Northeast Georgia Rise and the Islas Orcadas Rise east of it, are seemingly dissected by transverse valleys that extend to the fracture zones of the Mid-Atlantic Ridge.\n\nThe Northeast Georgia Rise is made of oceanic crust that formed when Africa and South America spread apart after the Gondwana breakup. 100 Ma the Northeast Georgia Rise was part of the Agulhas Plateau-Northeast Georgia Rise-Maud Rise large igneous province (LIP) in what today is the southwestern-most Indian Ocean south of South Africa. This LIP, often called the southeast African LIP, formed at the triple junction where Gondwana passed over the Bouvet Hotspot and broke-up into Antarctica, South America, and Africa. This volcanism lasted until 94 Ma after which seafloor spreading detached the Northeast Georgia Rise and Maude Rise from the Agulhas Plateau and the Northeast Georgia Rise migrated westward to its current location and Maud Rise south towards the Weddell Sea.\n\nNortheast Georgia Rise and the Agulhas Plateau were always located on different tectonic plates, the South American and African plates respectively. Because of this, these two plateaus can be used to reconstruct the movements of the two plates from the formation of the southeast African LIP.\n\nNortheast Georgia rise collided with the South Georgia microcontinental block about 10 Ma which caused the uplift of this block and the creation of the present islands. The collision coincided with the termination of spreading at the West Scotia Sea and resulted in a bathymetric obstacle that still steers the Antarctic Circumpolar Current northward.\n\nTwo or more episodes of deformation have modified the topography of the ridge. Late Oligocene faulting coincides with the opening of the Scotia Sea; the western part of the Northeast Georgia Rise was uplifted by during the Neogene (23-3 Ma); the topography of the southwestern part of the rise may have formed by interaction with the advancing South Sandwich Trench or the South Sandwich block.\n\nAs a part of the Scotia Plate, the South Georgia block has been moving eastward. It is possible that when Northeast Georgia Rise, with its thickened, buoyant crust, reached the convergent South American-Scotia margin, the rise stopped the South Georgia block, transformed it into a series of fault blocks, and forced the margin to relocate south of the South Georgia block — effectively making it part of the South American Plate.\n\nIn the Scotia Sea the Antarctic Circumpolar Current (ACC) is deflected north by the South Scotia Ridge. It then widens extensively before passing over the North Scotia Ridge. North of South Georgia the southern boundary of the ACC is retroflected around the Northeast Georgia Rise.\nThe Southern Antarctic Circumpolar Current Front (SACCF) meanders across the Scotia Sea from the western shelf of the Antarctic Peninsula to the southwestern side of South Georgia. From there SACCF wraps the island anti-cyclonically, retroflects north of it, and flows across the Northeast Georgia Rise before looping cyclonically into the South Atlantic. The retroflection north of the island and across the rise shows a strong seasonal variability but SACCF remains constrained by these bathymetric obstacles.\n\nWeddell Sea Deep Water (WSDW) circulates cyclonically in the Weddell Gyre from where it escapes through gaps in the South Scotia Ridge, such as the South Sandwich Trench. It then flows across the Scotia Sea which it can only escape through the Georgia Passage. WSDW can reach the Georgia Basin by two routes: either by circumnavigating the Northeast Georgia Rise on its eastern side or by passing through the Northeast Georgia Passage.\n\n"}
{"id": "28324910", "url": "https://en.wikipedia.org/wiki?curid=28324910", "title": "Office of the Federal Coordinator, Alaska Natural Gas Transportation Projects", "text": "Office of the Federal Coordinator, Alaska Natural Gas Transportation Projects\n\nThe Office of the Federal Coordinator, Alaska Natural Gas Transportation Projects is an independent agency of the U.S. government. Congress created the Office in the Alaska Natural Gas Pipeline Act of 2004.\n\nThe intent of that law is to help ease forward a multibillion-dollar Alaska natural gas pipeline project that would deliver North Slope gas to consumers in the 48 contiguous states.\n\nCongress directed the Office of the Federal Coordinator to expedite and coordinate federal permitting for construction of a pipeline. The Office coordinates with over 20 federal agencies, the Canadian federal government, the State of Alaska, tribal governments and other stakeholders.\n\nWith some exceptions, the law empowers the Office to prohibit a federal agency from including “in any certificate, right-of-way, permit, lease, or other authorization issued to an Alaska natural gas transportation project any term or condition that may be permitted, but is not required, by any applicable law,” if the term or condition would prevent or significantly impair construction, operation or expansion of the natural gas pipeline project.\n\nThe Federal Coordinator (director of the agency) is nominated with advice and consent of the Senate by the President of the United States. Drue Pearce was the first federal coordinator; she was nominated by George W. Bush and served from December 12, 2006 through January 3, 2010. Larry Persily is the current coordinator; Barack Obama nominated him on December 9, 2009, and was confirmed by the United States Senate on March 10, 2010.\n\n\n"}
{"id": "22265", "url": "https://en.wikipedia.org/wiki?curid=22265", "title": "Ordovician", "text": "Ordovician\n\nThe Ordovician () is a geologic period and system, the second of six periods of the Paleozoic Era. The Ordovician spans 41.2 million years from the end of the Cambrian Period million years ago (Mya) to the start of the Silurian Period Mya.\n\nThe Ordovician, named after the Celtic tribe of the Ordovices, was defined by Charles Lapworth in 1879 to resolve a dispute between followers of Adam Sedgwick and Roderick Murchison, who were placing the same rock beds in northern Wales into the Cambrian and Silurian systems, respectively. Lapworth recognized that the fossil fauna in the disputed strata were different from those of either the Cambrian or the Silurian systems, and placed them in a system of their own. The Ordovician received international approval in 1960 (forty years after Lapworth's death), when it was adopted as an official period of the Paleozoic Era by the International Geological Congress.\n\nLife continued to flourish during the Ordovician as it did in the earlier Cambrian period, although the end of the period was marked by the Ordovician–Silurian extinction events. Invertebrates, namely molluscs and arthropods, dominated the oceans. The Great Ordovician Biodiversification Event considerably increased the diversity of life. Fish, the world's first true vertebrates, continued to evolve, and those with jaws may have first appeared late in the period. Life had yet to diversify on land. About 100 times as many meteorites struck the Earth per year during the Ordovician compared with today.\n\nThe Ordovician Period began with a major extinction called the Cambrian–Ordovician extinction event, about Mya (million years ago). It lasted for about 42 million years and ended with the Ordovician–Silurian extinction events, about Mya (ICS, 2004) which wiped out 60% of marine genera. The dates given are recent radiometric dates and vary slightly from those found in other sources. This second period of the Paleozoic era created abundant fossils that became major petroleum and gas reservoirs.\nThe boundary chosen for the beginning of both the Ordovician Period and the Tremadocian stage is highly significant. It correlates well with the occurrence of widespread graptolite, conodont, and trilobite species. The base (start) of the Tremadocian allows scientists to relate these species not only to each other, but also to species that occur with them in other areas. This makes it easier to place many more species in time relative to the beginning of the Ordovician Period.\n\nA number of regional terms have been used to subdivide the Ordovician Period. In 2008, the ICS erected a formal international system of subdivisions. There exist Baltoscandic, British, Siberian, North American, Australian, Chinese Mediterranean and North-Gondwanan regional stratigraphic schemes.\nThe Ordovician Period in Britain was traditionally broken into Early (Tremadocian and Arenig), Middle (Llanvirn (subdivided into Abereiddian and Llandeilian) and Llandeilo) and Late (Caradoc and Ashgill) epochs. The corresponding rocks of the Ordovician System are referred to as coming from the Lower, Middle, or Upper part of the column. The faunal stages (subdivisions of epochs) from youngest to oldest are:\n\nLate Ordovician\nMiddle Ordovician\nEarly Ordovician\n\nThe Tremadoc corresponds to the (modern) Tremadocian. The Floian corresponds to the lower Arenig; the Arenig continues until the early Darriwilian, subsuming the Dapingian. The Llanvirn occupies the rest of the Darriwilian, and terminates with it at the base of the Late Ordovician.\nThe Sandbian represents the first half of the Caradoc; the Caradoc ends in the mid-Katian, and the Ashgill represents the last half of the Katian, plus the Hirnantian.\n\nDuring the Ordovician, the southern continents were collected into Gondwana. Gondwana started the period in equatorial latitudes and, as the period progressed, drifted toward the South Pole.\n\nEarly in the Ordovician, the continents of Laurentia (in present-day North America), Siberia, and Baltica (present-day northern Europe) were still independent continents (since the break-up of the supercontinent Pannotia earlier), but Baltica began to move towards Laurentia later in the period, causing the Iapetus Ocean between them to shrink. The small continent Avalonia separated from Gondwana and began to move north towards Baltica and Laurentia, opening the Rheic Ocean between Gondwana and Avalonia.\n\nThe Taconic orogeny, a major mountain-building episode, was well under way in Cambrian times. In the early and middle Ordovician, temperatures were mild, but at the beginning of the Late Ordovician, from 460 to 450 Ma, volcanoes along the margin of the Iapetus Ocean spewed massive amounts of carbon dioxide, a greenhouse gas, into the atmosphere, turning the planet into a hothouse.\n\nInitially, sea levels were high, but as Gondwana moved south, ice accumulated into glaciers and sea levels dropped. At first, low-lying sea beds increased diversity, but later glaciation led to mass extinctions as the seas drained and continental shelves became dry land. During the Ordovician, in fact during the Tremadocian, marine transgressions worldwide were the greatest for which evidence is preserved.\n\nThese volcanic island arcs eventually collided with proto North America to form the Appalachian mountains. By the end of the Late Ordovician the volcanic emissions had stopped. Gondwana had by that time neared the South Pole and was largely glaciated.\n\nThe Ordovician meteor event is a proposed shower of meteors that occurred during the Middle Ordovician period, roughly 470 million years ago. It is not associated with any major extinction event.\n\nThe Ordovician was a time of calcite sea geochemistry in which low-magnesium calcite was the primary inorganic marine precipitate of calcium carbonate. Carbonate hardgrounds were thus very common, along with calcitic ooids, calcitic cements, and invertebrate faunas with dominantly calcitic skeletons. Biogenic aragonite, like that composing the shells of most molluscs, dissolved rapidly on the sea floor after death.\n\nUnlike Cambrian times, when calcite production was dominated by microbial and non-biological processes, animals (and macroalgae) became a dominant source of calcareous material in Ordovician deposits.\n\nThe Ordovician saw the highest sea levels of the Paleozoic, and the low relief of the continents led to many shelf deposits being formed under hundreds of metres of water. The sea level rose more or less continuously throughout the Early Ordovician, leveling off somewhat during the middle of the period. Locally, some regressions occurred, but sea level rise continued in the beginning of the Late Ordovician. Sea levels fell steadily in accord with the cooling temperatures for ~30 million years leading up to the Hirnantian glaciation. During this icy stage, sea level seems to have risen and dropped somewhat, but despite much study the details remain unresolved.\n\nAt the beginning of the period, around million years ago, the climate was very hot due to high concentration of (4200 ppm) in the atmosphere, which gave a strong greenhouse effect. By contrast, today the concentration is just above 400 ppm. Marine water temperatures are assumed to have averaged , which restricted the diversification of complex multi-cellular organisms. But over time, the climate became cooler, and around 460 million years ago, the ocean temperatures became comparable to those of present-day equatorial waters.\n\nAs with North America and Europe, Gondwana was largely covered with shallow seas during the Ordovician. Shallow clear waters over continental shelves encouraged the growth of organisms that deposit calcium carbonates in their shells and hard parts. The Panthalassic Ocean covered much of the northern hemisphere, and other minor oceans included Proto-Tethys, Paleo-Tethys, Khanty Ocean, which was closed off by the Late Ordovician, Iapetus Ocean, and the new Rheic Ocean.\n\nAs the Ordovician progressed, we see evidence of glaciers on the land we now know as Africa and South America, which were near the South Pole at the time, and covered by ice caps.\n\nFor most of the Late Ordovician life continued to flourish, but at and near the end of the period there were mass-extinction events that seriously affected planktonic forms like conodonts and graptolites. The trilobites Agnostida and Ptychopariida completely died out, and the Asaphida were much reduced. Brachiopods, bryozoans and echinoderms were also heavily affected, and the endocerid cephalopods died out completely, except for possible rare Silurian forms. The Ordovician–Silurian extinction events may have been caused by an ice age that occurred at the end of the Ordovician period, due to the expansion of the first terrestrial plants, as the end of the Late Ordovician was one of the coldest times in the last 600 million years of Earth's history.\n\nOn the whole, the fauna that emerged in the Ordovician were the template for the remainder of the Palaeozoic. The fauna was dominated by tiered communities of suspension feeders, mainly with short food chains. The ecological system reached a new grade of complexity far beyond that of the Cambrian fauna, which has persisted until the present day.\n\nThough less famous than the Cambrian explosion, the Ordovician radiation was no less remarkable; marine faunal genera increased fourfold, resulting in 12% of all known Phanerozoic marine fauna. Another change in the fauna was the strong increase in filter-feeding organisms. The trilobite, inarticulate brachiopod, archaeocyathid, and eocrinoid faunas of the Cambrian were succeeded by those that dominated the rest of the Paleozoic, such as articulate brachiopods, cephalopods, and crinoids. Articulate brachiopods, in particular, largely replaced trilobites in shelf communities. Their success epitomizes the greatly increased diversity of carbonate shell-secreting organisms in the Ordovician compared to the Cambrian.\n\nIn North America and Europe, the Ordovician was a time of shallow continental seas rich in life. Trilobites and brachiopods in particular were rich and diverse. Although solitary corals date back to at least the Cambrian, reef-forming corals appeared in the early Ordovician, corresponding to an increase in the stability of carbonate and thus a new abundance of calcifying animals.\n\nMolluscs, which appeared during the Cambrian or even the Ediacaran, became common and varied, especially bivalves, gastropods, and nautiloid cephalopods.\n\nNow-extinct marine animals called graptolites thrived in the oceans. Some new cystoids and crinoids appeared.\n\nIt was long thought that the first true vertebrates (fish — Ostracoderms) appeared in the Ordovician, but recent discoveries in China reveal that they probably originated in the Early Cambrian. The very first gnathostome (jawed fish) appeared in the Late Ordovician epoch.\n\nDuring the Middle Ordovician there was a large increase in the intensity and diversity of bioeroding organisms. This is known as the Ordovician Bioerosion Revolution. It is marked by a sudden abundance of hard substrate trace fossils such as \"Trypanites\", \"Palaeosabella\", \"Petroxestes\" and \"Osprioneides\". Several groups of endobiotic symbionts appeared in the Ordovician.\n\nIn the Early Ordovician, trilobites were joined by many new types of organisms, including tabulate corals, strophomenid, rhynchonellid, and many new orthid brachiopods, bryozoans, planktonic graptolites and conodonts, and many types of molluscs and echinoderms, including the ophiuroids (\"brittle stars\") and the first sea stars. Nevertheless, the trilobites remained abundant, all the Late Cambrian orders continued, and were joined by the new group Phacopida. The first evidence of land plants also appeared (see evolutionary history of life).\n\nIn the Middle Ordovician, the trilobite-dominated Early Ordovician communities were replaced by generally more mixed ecosystems, in which brachiopods, bryozoans, molluscs, cornulitids, tentaculitids and echinoderms all flourished, tabulate corals diversified and the first rugose corals appeared. The planktonic graptolites remained diverse, with the Diplograptina making their appearance. Bioerosion became an important process, particularly in the thick calcitic skeletons of corals, bryozoans and brachiopods, and on the extensive carbonate hardgrounds that appear in abundance at this time. One of the earliest known armoured agnathan (\"ostracoderm\") vertebrate, \"Arandaspis\", dates from the Middle Ordovician.\n\nTrilobites in the Ordovician were very different from their predecessors in the Cambrian. Many trilobites developed bizarre spines and nodules to defend against predators such as primitive eurypterids and nautiloids while other trilobites such as \"Aeglina prisca\" evolved to become swimming forms. Some trilobites even developed shovel-like snouts for ploughing through muddy sea bottoms. Another unusual clade of trilobites known as the trinucleids developed a broad pitted margin around their head shields. Some trilobites such as \"Asaphus kowalewski\" evolved long eyestalks to assist in detecting predators whereas other trilobite eyes in contrast disappeared completely. Molecular clock analyses suggest that early arachnids started living on land by the end of the Ordovician.\n\nThe earliest-known octocorals date from the Ordovician.\n\nGreen algae were common in the Late Cambrian (perhaps earlier) and in the Ordovician. Terrestrial plants probably evolved from green algae, first appearing as tiny non-vascular forms resembling liverworts. Fossil spores from land plants have been identified in uppermost Ordovician sediments.\n\nAmong the first land fungi may have been arbuscular mycorrhiza fungi (Glomerales), playing a crucial role in facilitating the colonization of land by plants through mycorrhizal symbiosis, which makes mineral nutrients available to plant cells; such fossilized fungal hyphae and spores from the Ordovician of Wisconsin have been found with an age of about 460 million years ago, a time when the land flora most likely only consisted of plants similar to non-vascular bryophytes.\n\nThe Ordovician came to a close in a series of extinction events that, taken together, comprise the second largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. The only larger one was the Permian–Triassic extinction event.\n\nThe extinctions occurred approximately 447–444 million years ago and mark the boundary between the Ordovician and the following Silurian Period. At that time all complex multicellular organisms lived in the sea, and about 49% of genera of fauna disappeared forever; brachiopods and bryozoans were greatly reduced, along with many trilobite, conodont and graptolite families.\n\nThe most commonly accepted theory is that these events were triggered by the onset of cold conditions in the late Katian, followed by an ice age, in the Hirnantian faunal stage, that ended the long, stable greenhouse conditions typical of the Ordovician.\n\nThe ice age was possibly not long-lasting. Oxygen isotopes in fossil brachiopods show its duration may have been only 0.5 to 1.5 million years. Other researchers (Page et al.) estimate more temperate conditions did not return until the late Silurian.\n\nThe late Ordovician glaciation event was preceded by a fall in atmospheric carbon dioxide (from 7000 ppm to 4400 ppm). The dip was triggered by a burst of volcanic activity that deposited new silicate rocks, which draw CO out of the air as they erode. This selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it, which have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.\n\nAs glaciers grew, the sea level dropped, and the vast shallow intra-continental Ordovician seas withdrew, which eliminated many ecological niches. When they returned, they carried diminished founder populations that lacked many whole families of organisms. They then withdrew again with the next pulse of glaciation, eliminating biological diversity with each change. Species limited to a single epicontinental sea on a given landmass were severely affected. Tropical lifeforms were hit particularly hard in the first wave of extinction, while cool-water species were hit worst in the second pulse.\n\nThose species able to adapt to the changing conditions survived to fill the ecological niches left by the extinctions.\n\nAt the end of the second event, melting glaciers caused the sea level to rise and stabilise once more. The rebound of life's diversity with the permanent re-flooding of continental shelves at the onset of the Silurian saw increased biodiversity within the surviving Orders.\n\nAn alternate extinction hypothesis suggested that a ten-second gamma-ray burst could have destroyed the ozone layer and exposed terrestrial and marine surface-dwelling life to deadly ultraviolet radiation and initiated global cooling.\n\nRecent work considering the sequence stratigraphy of the Late Ordovician argues that the mass extinction was a single protracted episode lasting several hundred thousand years, with abrupt changes in water depth and sedimentation rate producing two pulses of last occurrences of species.\n\n"}
{"id": "8171788", "url": "https://en.wikipedia.org/wiki?curid=8171788", "title": "Peninsular Malaysian peat swamp forests", "text": "Peninsular Malaysian peat swamp forests\n\nThe Peninsular Malaysian peat swamp forests ecoregion, in the tropical and subtropical moist broadleaf forests biome, are of the Malay Peninsula, which includes portions of Malaysia and southern Thailand.\n\nThe ecoregion covers an area of on both the eastern and western sides of the peninsula. The peat swamp forests have formed over hundreds of years, as sediment and organic debris deposited by rivers are trapped behind mangroves, gradually building up a layer of waterlogged, acidic, nutrient-poor soil. These forests are less diverse than the surrounding Peninsular Malaysian rain forests, but are home to many endangered animals.\n\nDipterocarps, including \"Shorea albida,\" are the dominant trees, while strangler figs (\"Ficus\" spp.) are common at the edges of the swamp forests. Pandan (\"Pandanus amaryllifolius\") and the sealing wax palm (\"Cyrtostachys lakka\") are common understory plants. These and other plants provide a source of food for a host of animals, including birds.\n\nThe ecoregion home to many endangered species, including tigers, Malayan tapirs (\"Tapirus indicus\"), clouded leopards (\"Neofelis nebulosa\"), Asian elephants (\"Elephas maximus\"), and Sumatran rhinoceroses (\"Dicerorhinus sumatrensis\").\n\nThreats to the forests include: mining, rubber harvesting, and clearing of forest for oil palm and coconut plantations. Land draining has also opened a great deal of space in these forests. Many trees are cut down for development of more space, although this has increased peat forest fires.\n"}
{"id": "35139871", "url": "https://en.wikipedia.org/wiki?curid=35139871", "title": "Q-Vectors", "text": "Q-Vectors\n\nQ-vectors are used in atmospheric dynamics to understand physical processes such as vertical motion and frontogenesis. Q-vectors are not physical quantities that can be measured in the atmosphere but are derived from the quasi-geostrophic equations and can be used in the previous diagnostic situations. On meteorological charts, Q-vectors point toward upward motion and away from downward motion. Q-vectors are an alternative to the omega equation for diagnosing vertical motion in the quasi-geostrophic equations.\n\nFirst derived in 1978, Q-vector derivation can be simplified for the midlatitudes, using the midlatitude β-plane quasi-geostrophic prediction equations:\n\n\nAnd the thermal wind equations:\n\nformula_4 (x component of thermal wind equation)\n\nformula_5 (y component of thermal wind equation)\n\nwhere formula_6 is the Coriolis parameter, approximated by the constant 1e s; formula_7 is the atmospheric ideal gas constant; formula_8 is the latitudinal change in the Coriolis parameter formula_9; formula_10 is a static stability parameter; formula_11 is the specific heat at constant pressure; formula_12 is pressure; formula_13 is temperature; anything with a subscript formula_14 indicates geostrophic; anything with a subscript formula_15 indicates ageostrophic; formula_16 is a diabatic heating rate; and formula_17 is the Lagrangian rate change of pressure with time. formula_18. Note that because pressure decreases with height in the atmosphere, a formula_19 is upward vertical motion, analogous to formula_20.\n\nFrom these equations we can get expressions for the Q-vector:\n\nformula_21\n\nformula_22\n\nAnd in vector form:\n\nformula_23\n\nformula_24\n\nPlugging these Q-vector equations into the quasi-geostrophic omega equation gives:\n\nformula_25\n\nWhich in an adiabatic setting gives:\n\nformula_26\n\nExpanding the left-hand side of the quasi-geostrophic omega equation in a Fourier Series gives the formula_27 above, implying that a formula_27 relationship with the right-hand side of the quasi-geostrophic omega equation can be assumed.\n\nThis expression shows that the divergence of the Q-vector (formula_29) is associated with downward motion. Therefore, convergent formula_30 forces ascend and divergent formula_30 forces descend. Q-vectors and all ageostrophic flow exist to preserve thermal wind balance. Therefore, low level Q-vectors tend to point in the direction of low-level ageostrophic winds.\n\nQ-vectors can be determined wholly with: geopotential height (formula_32) and temperature on a constant pressure surface. Q-vectors always point in the direction of ascending air. For an idealized cyclone and anticyclone in the Northern Hemisphere (where formula_33), cyclones have Q-vectors which point parallel to the thermal wind and anticyclones have Q-vectors that point antiparallel to the thermal wind. This means upward motion in the area of warm air advection and downward motion in the area of cold air advection.\n\nIn frontogenesis, temperature gradients need to tighten for initiation. For those situations Q-vectors point toward ascending air and the tightening thermal gradients. In areas of convergent Q-vectors, cyclonic vorticity is created, and in divergent areas, anticyclonic vorticity is created.\n"}
{"id": "1660393", "url": "https://en.wikipedia.org/wiki?curid=1660393", "title": "Resting metabolic rate", "text": "Resting metabolic rate\n\nResting metabolic rate (RMR) is whole-body mammal (and other vertebrate) metabolism during a time period of strict and steady \"resting conditions\" that are defined by a combination of assumptions of physiological homeostasis and biological equilibrium. RMR differs from basal metabolic rate (BMR) because BMR measurements must meet total physiological equilibrium whereas RMR conditions of measurement can be altered and defined by the contextual limitations. Therefore, BMR is measured in the elusive \"perfect\" steady state, whereas RMR measurement is more accessible and thus, represents most, if not all measurements or estimates of daily energy expenditure.\n\nIndirect calorimetry is the study or clinical use of the relationship between respirometry and bioenergetics, where the measurement of the rates of change in oxygen consumption, sometimes carbon dioxide production, and less often urea production is transformed to energy expenditure and expressed as the ratio between \"i) energy\" and \"ii) the time frame of the measurement\". For example, following analysis of oxygen consumption of a human subject, if 5.5 kilocalories of energy were estimated during a 5-minute measurement from a rested individual, then the resting metabolic rate equals = 1.1 kcal/min rate.\n\nA comprehensive treatment of confounding factors on BMR measurements is demonstrated as early as 1922 in Massachusetts by Engineering Professor Frank B Sanborn, wherein descriptions of the effects of food, posture, sleep, muscular activity, and emotion provide criteria for separating BMR from RMR.\n\nIn the 1780s for the French Academy of Sciences, Lavoisier, Laplace, and Seguin investigated and published relationships between direct calorimetry and respiratory gas exchanges from mammalian subjects. 100 years later in the 19th century for the Connecticut-based Wesleyan University, Professors Atwater and Rosa provided ample evidence of nitrogen, carbon dioxide, and oxygen transport during the metabolism of amino acids, glucose, and fatty acids in human subjects, further establishing the value of indirect calorimetry in determining bioenergetics of free-living humans. The work of Atwater and Rosa also made it possible to calculate the caloric values of foods, which eventually became the criteria adopted by the USDA to create the food calorie library.\n\nIn the early 20th century at Oxford University, physiology researcher Claude Gordon Douglas developed an inexpensive and mobile method of collecting exhaled breath (partly in preparation for experiments to be conducted on Pike's Peak, Colorado). In this method, the subject exhales into a nearly impermeable and large volume collection bag over a recorded period of time. The entire volume is measured, the oxygen and carbon dioxide content are analyzed, and the differences from inspired \"ambient\" air are calculated to determine the rates of oxygen uptake and carbon dioxide output.\n\nTo estimate energy expenditure from the exhaled gases, several algorithms were developed. One of the most widely used was developed in 1949 at University of Glasgow by research physiologist J. B. de V. Weir. His abbreviated equation for estimating metabolic rate was written with rates of gas exchange being volume/time, excluded urinary nitrogen, and allowed for the inclusion of a time conversion factor of 1.44 to extrapolate to 24-hour energy expenditure from 'kcal per minute\" to \"kcal per day.\" Weir used the Douglas Bag method in his experiments, and in support of neglecting the effect of protein metabolism under normal physiological conditions and eating patterns of ~12.5% protein calories, he wrote:\n\nIn the early 1970s, computer technology enabled on-site data processing, some real-time analysis, and even graphical displays of metabolic variables, such as O, CO, and air-flow, thereby encouraging academic institutions to test accuracy and precision in new ways. A few years later in the decade, battery-operated systems made debuts. For example, a demonstration of the mobile Oxylog with digital display of both cumulative and past-minute oxygen consumption was presented in 1977 at the Proceedings of the Physiological Society. As manufacturing and computing costs dropped over the next few decades, various universal calibration methods for preparing and comparing various models in the 1990s brought attention to short-comings or advantages of various designs. In addition to lower costs, the metabolic variable CO was often ignored, promoting instead a focus on oxygen-consumption models of weight management and exercise training.\n\nIn the new millennium, smaller \"desktop-sized\" indirect calorimeters, such as the New Leaf system from Medical Graphics were being distributed with fully dedicated personal computers & printers, and running modern windows-based software such as BreezeSuite for Windows OS. Sophisticated software were made available to empower nutritionists and end-consumers alike to track and manage calorie intake. For example, \"in 2003\", HealtheTech provided BalanceLog(TM) Weight Management and Nutrition Monitoring software and its BalanceLog Pro(TM) web product, both of which were oriented for use with their handheld & disposable BodyGem(R), which measured oxygen consumption and reported 24-hr resting energy expenditure.\n\nAt this time, several health and wellness companies brought resting and exercise-conditions measurements as a service to the end consumer, which helped shape sales and service teams to keep these systems online and ready for gym-goers and weight management clinics.\n\nWide variety of tools and devices available in the market nowadays to track RMR based fitness and exercises. Tools can also target day to day activities and how RMR can be used to determine the amount of energy expended by individuals, personalised to their body measurements and types of activities. For instance, \"In 2017\" TryAround introduced Scientific Fitness Tracker that uses RMR to estimate energy expended for nearly 900 physical activities. The tool is available as an app for iPhone users purely based on metabolic equivalents for standard vs corrected RMRs.\n\nRMR measurements are recommended when estimating total daily energy expenditure (TEE). Since BMR measures are restricted to the narrow time frame (and strict conditions) upon waking, the looser-conditions RMR measure is more typically conducted. In the review organized by the USDA, most publications documented specific conditions of resting measurements, including time from latest food intake or physical activities; this comprehensive review estimated RMR is 10 – 20% higher than BMR due to thermic effect of feeding and residual burn from activities that occur throughout the day.\n\nThermochemistry aside, the rate of metabolism and an amount of energy expenditures can be mistakenly interchanged, for example, when describing RMR and REE.\n\nThe Academy of Nutrition and Dietetics (AND) provides clinical guidance for preparing a subject for RMR measures, in order to mitigate possible confounding factors from feeding, stressful physical activities, or exposure to stimulants such as caffeine or nicotine:\n\n\"In preparation, a subject should be fasting for 7 hrs or greater, and mindful to avoid stimulants and stressors, such as caffeine, nicotine, and hard physical activities such as purposeful exercises.\"\n\"For 30 minutes before conducting the measurement, a subject should be laying supine without physical movements, no reading nor listening to music. The ambiance should reduce stimulation by maintaining constant quiet, low lighting, and steady temperature. These conditions continue during the measurement stage.\"\n\nFurther, the correct use of a well-maintained indirect calorimeter includes achieving a natural and steady breathing pattern in order to reveal oxygen consumption and carbon dioxide production rates under a reproducible resting condition. Indirect calorimetry is considered the gold-standard method to measure RMR. Indirect calorimeters are usually found in laboratory and clinical settings, but technological advancements are bringing RMR measurement to free-living conditions.\n\nLong-term weight management is directly proportional to calories absorbed from feeding; nevertheless, myriad non-caloric factors also play biologically significant roles (not covered here) in estimating energy intake. In counting energy expenditure, the use of a resting measurement (RMR) is the most accurate method for estimating the major portion of Total daily energy expenditure (TEE), thereby giving the closest approximations when planning & following a Calorie Intake Plan. Thus, estimation of REE by indirect calorimetry is strongly recommended for accomplishing long-term weight management, a conclusion reached and maintained due to ongoing observational research by well-respected institutions such as the USDA, AND (previously ADA), ACSM, and internationally by the WHO.\n\nEnergy expenditure is correlated to a number of factors, listed in alphabetical order.\n"}
{"id": "3165280", "url": "https://en.wikipedia.org/wiki?curid=3165280", "title": "Sabicu wood", "text": "Sabicu wood\n\nSabicu wood or sabicu is the wood of at least two species of the genus \"Lysiloma\". \"Lysiloma sabicu\" (L.) Benth. occurs sparingly in the Bahamas, Jamaica, Haiti and the Dominican Republic, and Cuba. It was named by George Bentham (1800-1884) from a Cuban specimen examined in 1854. Bentham went on to identify a second species, \"Lysiloma latisiliquum\" (L.) Benth., which grows best in the Bahamas. The latter is commonly known as 'wild tamarind' or 'false tamarind'. The wood of both species is similar, being mid-brown in colour, sometimes with a reddish hue, heavy (specific gravity of 0.40-0.75) hard and durable. Some timber is well figured, but most relatively plain. The wood has been used in construction, shipbuilding and in furniture making, although its weight is a distinct drawback for the latter purpose. The stairs of The Crystal Palace in London, in which The Great Exhibition of 1851 was held, were made of sabicu due to its durability. Despite the enormous traffic that passed over them, the wood at the end was found to be little affected by wear.\n\nThere is some confusion in the published literature between \"L. sabicu\" and \"L. latisiliquum\", although there is little doubt that the former was the most important commercial species. There is also confusion with other Bahamian species colloquially known as 'tamarind', several of which were also called sabicu. The most common of these are \"Peltophorum adnatum\" Griseb. and \"Cojoba arborea\" (L.) Britton & Rose. At various times their wood has also been called 'horseflesh mahogany'. The Economic Botany Collection at the Royal Botanic Gardens, Kew, London, contains specimens of wood from all these species collected at various times in the 19th and early 20th centuries. The labelling evinces considerable confusion, but it seems likely that 'horseflesh mahogany' properly applies to \"Peltophorum\" and \"Cojoba\", while sabicu applies to \"Lysiloma\". \n"}
{"id": "42413701", "url": "https://en.wikipedia.org/wiki?curid=42413701", "title": "Sparrowhawk Hill", "text": "Sparrowhawk Hill\n\nSparrowhawk Hill lies in the centre of Little Cayman, one of the Cayman Islands, a British Overseas Territory in the Caribbean Sea. It is one of the territory’s Important Bird Areas (IBAs).\n\nSparrowhawk Hill is a 255 ha tract of pristine native dry forest with a maximum elevation of 20 m above sea level. Dominant tree species are \"Calyptranthes pallens\", \"Canella winterana\", \"Chionanthus caymanensis\", \"Dipholis salicifolia\" and \"Erythroxylum areolatum\".\n\nThe IBA was identified as such by BirdLife International because it supports significant populations of white-crowned pigeons, Caribbean elaenias and vitelline warblers.\n"}
{"id": "30547984", "url": "https://en.wikipedia.org/wiki?curid=30547984", "title": "Thermal wheel", "text": "Thermal wheel\n\nA thermal wheel, also known as a rotary heat exchanger, or rotary air-to-air enthalpy wheel, or heat recovery wheel, is a type of energy recovery heat exchanger positioned within the supply and exhaust air streams of an air-handling system or in the exhaust gases of an industrial process, in order to recover the heat energy. Other variants include enthalpy wheels and desiccant wheels. A cooling-specific thermal wheel is sometimes referred to as a Kyoto wheel.\n\nA thermal wheel consists of a circular honeycomb matrix of heat-absorbing material, which is slowly rotated within the supply and exhaust air streams of an air-handling system. As the thermal wheel rotates, heat is picked up from the exhaust air stream in one half of the rotation and given up to the fresh air stream in the other half of the rotation. Thus waste heat energy from the exhaust air stream is transferred to the matrix material and then from the matrix material to the fresh air stream, raising the temperature of the supply air stream by an amount proportional to the temperature differential between air streams, or \"thermal gradient\", and depending upon the efficiency of the device. Heat exchange is most efficient when the streams flow in opposite directions, since this causes a favourable temperature gradient across the thickness of the wheel. The principle of course works in reverse, and \"cooling\" energy can be recovered to the supply air stream if so desired and the temperature differential allows.\n\nThe heat exchange matrix may be aluminium, plastic, or synthetic fiber. The heat exchanger is rotated by a small electric motor and belt drive system. The motors are often inverter speed-controlled for improved control of the leaving air temperature. If no heat exchange is required, then the motor can be stopped altogether.\n\nBecause of the nature of thermal wheels in the way that heat is transferred from the exhaust air stream to the supply air stream without having to pass directly through an exchange medium, the gross efficiencies are usually much higher than that of any other air-side heat recovery system. The shallower depth of the heat exchange matrix, as compared to that, say, for a plate heat exchanger, means that the pressure drop through the device is normally lower in comparison. Generally, a thermal wheel will be selected for face velocities between , and with equal air volume flow rates, gross \"sensible\" efficiencies of 85% can be expected. Although there is a small extra energy requirement to rotate the wheel, the motor energy consumption is usually very low and has little effect upon the seasonal efficiency of the device. In addition, the ability to recover \"latent\" heat, depending upon the materials and coatings used, can improve gross efficiencies by 10–15%.\n\nNormally the heat transfer between airstreams provided by the device is termed as \"sensible\", which is the exchange of energy, or enthalpy, resulting in a change in temperature of the medium (air in this case), but with no change in moisture content. However, if moisture or relative humidity levels in the return air stream are high enough to allow condensation to take place in the device, then this will cause \"latent\" heat to be released, and the heat transfer material will be covered with a film of water. Despite a corresponding absorption of latent heat, as some of the water film is evaporated in the opposite air stream, the water will reduce the thermal resistance of the boundary layer of the heat exchanger material and thus improve the heat transfer coefficient of the device, and hence increase efficiency. The energy exchange of such devices now comprises both sensible and latent heat transfer; in addition to a change in temperature, there is also a change in moisture content of the air streams.\n\nHowever, the film of condensation will also slightly increase pressure drop through the device, and depending upon the spacing of the matrix material, this can increase resistance by up to 30%. This will increase fan energy consumption and reduce the seasonal efficiency of the device.\n\nAluminium matrices are also available with an applied hygroscopic coating, and the use of this, or the use of porous synthetic fiber matrices, allows for the adsorption and release of water vapour, at moisture levels much lower than that normally required for condensation and latent heat transfer to occur. The benefit of this is an even higher heat transfer efficiency, but it also results in the drying or humidification of air streams, which may also be desired for the particular process being served by the supply air.\n\nFor this reason these devices are also commonly known as an enthalpy wheel.\n\nDuring the automotive industry's interest in gas turbines for vehicle propulsion (around 1965), Chrysler invented a unique type of rotary heat exchanger that consisted of a rotary drum constructed from corrugated metal (similar in appearance to corrugated cardboard). This drum was continuously rotated by reduction gears driven by the turbine. The hot exhaust gasses were directed through a portion of the device, which would then rotate to a section that conducted the induction air, where this intake air was heated. This recovery of the heat of combustion significantly increased the efficiency of the turbine engine. This engine proved impractical for an automotive application due to its poor low-rpm torque. Even such an efficient engine, if large enough to deliver the proper performance, would have a low average fuel efficiency. Such an engine may at some future time be attractive when combined with an electric motor in a hybrid vehicle owing to its robust longevity and an ability to burn a wide variety of liquid fuels.\n\nA desiccant wheel is very similar to a thermal wheel, but with a coating applied for the sole purpose of dehumidifying, or \"drying\", the air stream. The desiccant is normally silica gel. As the wheel turns, the desiccant passes alternately through the incoming air, where the moisture is adsorbed, and through a “regenerating” zone, where the desiccant is dried and the moisture expelled. The wheel continues to rotate, and the adsorbent process is repeated. Regeneration is normally carried out by the use of a heating coil, such as a water or steam coil, or a direct-fired gas burner.\n\nThermal wheels and desiccant wheels are often used in series configuration to provide the required dehumidification as well as recovering the heat from the regeneration cycle.\n\nThermal wheels are not suitable for use where total separation of supply and exhaust air streams is required, since air will bypass at the interface between the air streams at the heat exchanger boundary, and at the point where the wheel passes from one air stream to the other during its normal rotation. The former is reduced by brush seals, and the latter is reduced by a small purge section, formed by plating off a small segment of the wheel, normally in the exhaust air stream.\n\nMatrices made from fibrous materials, or with hygroscopic coatings, for the transfer of latent heat, are far more susceptible to damage and degradation by \"fouling\" than plain metal or plastic materials, and are difficult or impossible to effectively clean if dirty. Care must be taken to properly filter the air streams on both exhaust and fresh air sides of the wheel. Any dirt attaching on either air side will invariably be transported into the air stream of the other side.\n\n\n"}
{"id": "6816191", "url": "https://en.wikipedia.org/wiki?curid=6816191", "title": "Timeline of the geologic history of the United States", "text": "Timeline of the geologic history of the United States\n\nThis time line of the geologic history of the United States chronologically lists important events occurring within the present political boundaries of United States (including territories) before 12,000 years ago. This time line segment may include some events that occurred outside these borders that profoundly influenced later American life and its present landscape. It also includes evidence of Native American communities predating the Clovis culture.\n\nBecause of the inaccuracies inhernet in radiocarbon dating and other methods of interpreting the geologic (and archaeological) record, most dates in this time line represent approximations that may vary considerably from source to source. The assumptions implicit in geologic dating methods also may yield a general bias in the dating in this time line.\n\n"}
{"id": "1814556", "url": "https://en.wikipedia.org/wiki?curid=1814556", "title": "Transatlantic tunnel", "text": "Transatlantic tunnel\n\nA transatlantic tunnel is a theoretical tunnel that would span the Atlantic Ocean between North America and Europe possibly for such purposes as mass transit. Some proposals envision technologically advanced trains reaching speeds of . Most conceptions of the tunnel envision it between the United States and the United Kingdom ‒ or more specifically between New York City and London.\n\nAdvantages compared to air travel could be increased speed, and use of electricity instead of scarce oil based fuel, considering a future time long after peak oil.\n\nThe main barriers to constructing such a tunnel are cost with estimates of between $88 billion to $175 billion as well as the limits of current materials science. Existing major tunnels, such as the Channel Tunnel, Seikan Tunnel and the Gotthard Base Tunnel, despite using less expensive technology than any yet proposed for the transatlantic tunnel, struggle financially.\n\nMany variations of the concept exist, including a tube above the seabed, a tunnel beneath the ocean floor, or some combination of the two.\n\nA 1960s proposal has a -long near-vacuum tube with vactrains, a theoretical type of maglev train, which could travel at speeds up to . At this speed, the travel-time between New York City and London would be less than one hour. Another modern variation, intended to reduce costs, is a submerged floating tunnel about below the ocean surface, in order to avoid ships, bad weather, and the high pressure associated with a much deeper tunnel near the sea bed. It would consist of 54,000 prefabricated sections held in place by 100,000 tethering cables. Each section would consist of a layer of foam sandwiched between concentric steel tubes, and the tunnel would also have reduced air pressure.\n\nIdeas proposing rocket, jet, scramjet, and air-pressurized tunnels for train transportation have also been put forward. In the proposal described in an \"Extreme Engineering\" episode, trains would take 18 minutes to reach top speed, and 18 minutes at the end to come to a halt. During the deceleration phase, the resultant 0.2g acceleration would lead to an unpleasant feeling of tilting downward, and it was proposed that the seats would individually rotate to face backwards at the midpoint of the journey, in order to make the deceleration more pleasant.\n\nSuggestions for such a structure go back to Michel Verne, son of Jules Verne, who wrote about it in 1888 in a story entitled \"Un Express de l'avenir\" (\"An Express of the Future\"). This story was published in English in \"Strand Magazine\" in 1895, where it was incorrectly attributed to Jules Verne, a mistake frequently repeated today.\n1913 saw the publication of the novel \"Der Tunnel\" by German author Bernhard Kellermann. It inspired four films of the same name: one in 1915 by William Wauer, and separate German, French, and British versions released in 1933 and 1935. The German and French versions were directed by Curtis Bernhardt, and the British one was written in part by science fiction writer Curt Siodmak. Perhaps suggesting contemporary interest in the topic, an original poster for the American release of the British version (renamed \"Transatlantic Tunnel\") was, in 2006, estimated for auction at $2,000–3,000.\n\nRobert H. Goddard, the father of rocketry, was issued two of his 214 patents for the idea.\nArthur C. Clarke mentioned intercontinental tunnels in his 1946 short story \"Rescue Party\" and again in his 1956 novel \"The City and the Stars\". Harry Harrison's 1975 novel \"Tunnel Through the Deeps\" (also published as \"A Transatlantic Tunnel, Hurrah!\") describes a vacuum/maglev system on the ocean floor.\nThe April 2004 issue of \"Popular Science\" suggests that a transatlantic tunnel is more feasible than previously thought, and without major engineering challenges. It compares it favorably with laying transatlantic pipes and cables, but with a cost of 88 to 175 billion dollars. In 2003, the Discovery Channel's show \"Extreme Engineering\" aired a program, titled \"Transatlantic Tunnel\", which discussed the proposed tunnel concept in detail.\n"}
{"id": "48099717", "url": "https://en.wikipedia.org/wiki?curid=48099717", "title": "Tumchaite", "text": "Tumchaite\n\nTumchaite, Na(Zr,Sn)SiO•HO, is a colorless to white monoclinic phyllosilicate mineral. It is associated with calcite, dolomite, and pyrite in the late dolomite-calcite carbonatites. It can be transparent to translucent; has a vitreous luster; and has perfect cleavage on {100}. Its hardness is 4.5, between fluorite and apatite. Tumchaite is isotypic with penkvilksite. The structure of the mineral is identified by silicate sheets parallel {100}, formed by alternation of clockwise and counterclockwise growing spiral chains of corner-sharing SiO tetrahedra. Tumchaite is named for the river Tumcha near Vuoriyarvi massif.\n\nTumchaite is found in carbonatites of the alkaline-ultramafic Vuoriyarvi massif, the Murmansk Region, Russia. This is located in the North Karelia on the north shore of the Vuoriyarvi lake and occupies an area of about 19.5 km. The mineral has been found as lens-like segregation of .5 x 1.0 x 1.5 cm in size in core sample from the bore hole. Tumchaite is associated with calcite, dolomite, a mineral of the serpentine group, and pyrite, and is formed during the hydrothermal alteration of carbonatites.\n\nTumchaite occurs as tabular monoclinic crystals, but other habits include massive and granular. The size of individual crystals is approximately 0.2 x 1.2 x 2.5 mm. It is colorless to white, transparent to translucent. The streak is white and the luster is vitreous. Cleavage on {100} is perfect, no parting is observed, and fracture is uneven as tumchaite is very brittle. The Mohs hardness is 4.5, between fluorite and apatite. The Vickers hardness test yielded 365 to 445- averaging 410- kg/mm with a 40 g load. The density, which was determined by flotation of the mineral using a dilute Clerici-HO solution, was 2.78 (2) g/cm versus the 2.77 g/cm determined by the empirical formula.\n\nSubbotin, et al., performed quantitative electron microprobe analysis with MS 46 CAMECA instrument operated at 20kV (30 for Zr) and sample current 15-30 nA. The following standards were used for the elements shown in parentheses: lorenzenite (for Na and Ti), diopside (Ca and Si), wadeite (Zr), hematite (Fe), MnCO (Mn), YAlO (Y), metallic Sn, Hf, and Nb.\n\nWater contents could not be determined due to little amount of homogeneous material. The presence of molecular water in the mineral was confirmed by the refinement of the crystal structure. The empirical formula, calculated on the basis of 13 O atoms is (NaCa)(ZrSnTiHf)SiO•2HO, simplified to Na(Zr,Sn)SiO•HO.\n\nA single crystal with the approximate dimensions of 0.20 x 0.20 x 0.20 mm was selected for the X-ray study. This study was performed using a Siemens P4 four circle diffractometer, using graphite monochromatized Mo\"K\"α radiation (λ=0.71073 Å). Unit cell parameters were found through a least square fit using 25 medium θ reflections: \"a\"=9.144 (4), \"b\"=8.818 (3), \"c\"=7.537 (3) Å, β=113.22 (3)°, \"V\"=558.49 Å, \"Z\"=2. The full diffraction pattern can be found in Table 2 of Subbotin, et al.\n\nTumchaite is isotypic with penkvilksite-1\"M\" and is chemically related to vlasovite. The dominant structural feature of tumchaite is the silicate sheet [SiO] parallel to {100}, which can be considered as a result of condensation of the tetrahedral spiral chains running along [010] with six tetrahedra in the repeat unit. Adjacent spirals are oriented in alternate clockwise and counterclockwise ways. Tumchaite contains two symmetrically independent SiO tetrahedra: Si1 and Si2 have two and three bridging O atoms, respectively.\n\nThe tetrahedral silicate sheets are connected by cationic octahedra with a disordered distribution of Zr and Sn (at 0, 1/2, 0), with the Zr/Sn ratio in these octahedra being 4. Figure 2 of Subbotin, et al., shows an illustration of this structure.\n"}
{"id": "58306275", "url": "https://en.wikipedia.org/wiki?curid=58306275", "title": "Whitehill Formation", "text": "Whitehill Formation\n\nThe Whitehill Formation, alternatively written as White Hill Formation and formerly known as White Band or Whitehill or White Hill Member, is a regional Early Permian (Artinskian to Kungurian, dating to around 282 to 275 Ma) geologic formation belonging to the Ecca Group in the southeastern ǁKaras Region of southeastern Namibia and Eastern, Northern and Western Cape provinces of South Africa.\n\nThe formation comprises black shales, mudstones, siltstones, dolomite beds, gypsum and halite layers and a layer of tuff within the formation. With a thickness between and present in an area of , the formation is considered the primary target for shale gas potential in the Southern Karoo. Total Organic Carbon (TOC) values average 4.5% with a range from 0.5 to 14.7%, placing the formation in the same range as the well-known Barnett Shale and Marcellus Formation of the United States.\n\nThe Whitehill Formation of the Karoo and Nama or Kalahari Basin is contemporaneous with the Huab Formation of the Huab Basin and is correlated with a series of formations in the Pelotas and Paraná Basins in southeastern Brazil, deposited in a larger basinal area, 150 million years before the break-up of Pangea. The abundance of \"Glossopteris\" and \"Mesosaurus\" fossils are characteristic of the Gondwanan correlation across present-day South America, Africa, Antarctica and Australia. The Whitehill Formation has provided fossil reptiles, insects, fish and flora.\n\nThe Whitehill Formation is an extensive Lower Permian unit, cropping out in a thin band stretching from the Western Cape in South Africa through southeastern Namibia and the Northern Cape in the north to the Eastern Cape of South Africa in the east. The formation is found at the edge of the Karoo Basin at distances of north to south and east to west. The formation is part of the Karoo Supergroup, more precisely the Ecca Group, where it overlies the Prince Albert Formation and is overlain by the Collingham Formation, and in the southern Karoo by the Tierberg Formation.\n\nSingle zircon U-Pb SHRIMP dating yielded an age of 279.1 ± 1.5 Ma for the Uhabis River Tuff present in the upper strata of the underlying Prince Albert Formation. The Khabus Tuff within the Whitehill Formation provided a weighted mean Pb/U age of 280.5 ± 2.1 Ma. Other authors use an estimated age for the whole formation of around 275 Ma.\n\nIn the central part of the Karoo Basin, the formation ranges in thickness from and is highly organic with up to 14 weight percent of Total Organic Carbon (TOC). The formation is highly conductive and can be traced in seismic profiles across nearly the whole basin.\n\nThe Whitehill Formation has been subdivided into two major subunits according to their weathering color in outcrops. The lower and thicker part consists mainly of bluish- to greenish-grey shales and mudstones, which grade upward into more light brownish, buff weathering, slightly coarser grained siltstones. This zone is conformably overlain by white weathering shales, with intermittent chert lenses and pyritic stringers; the latter rarely exceeding in thickness. The sedimentary structure is generally massive, however laminations do occur that resemble algal lamellae. The formation appears white due to weathering of pyrite (sulfide) at surface to sulfate (gypsum). This section is grading upward into dark carbonaceous, bluish-grey weathering shales. From a lithological point of view only the upper part of the succession consists of the Whitehill-characteristic carbonaceous black shales.\n\nA tuffaceous zone occurs within the Whitehill Formation a few metres below the stratigraphic interval, which contains several dolomitic limestones. This limestone interval forms a mappable unit in all outcrops of the Whitehill Formation in central southern Namibia. In the Northern Cape, the formation is intruded by dolerite sills. The sediments are considered to be deposited in an anoxic environment. Besides pyrite, the shales contain dolomite lenses near the base. The rocks are highly folded and faulted by the Cape orogeny forming the Cape Fold Belt, and thus, interpreted as \"decollement\".\n\nThe Whitehill Formation was deposited in a shallow inland sea with little to no connection to the open ocean. This sea, stretching from the Paraná Basin in current southeastern Brazil to the Karasberg and Karoo Basins of southwestern Africa, probably represents the transition from marine to brackish or freshwater conditions, in sediment-starved, stratified and anoxic embayments.\n\nFrom outcrops in southernmost Namibia (Aussenkjer-Noordoewer area) it became evident that the boundary between the Prince Albert and the Whitehill Formation represents the turning point from a progradational to a retrogradational succession. The tuffs within the Whitehill Formation, as well as other tuffaceous beds found in the underlying and overlying formations, were possibly sourced by volcanoes located in present-day South America, although other interpretations of the tuffs of the Dwyka and Ecca Groups propose a general source along the southern Panthalassian margin of current southern Africa.\n\nThe Whitehill Formation is time equivalent with the Pietermaritzburg Formation of the Waterberg Basin of southern Namibia. To the north in the Karoo Basin, the shales of the Whitehill Formation are laterally equivalent with the Vryheid Formation. The upper part of the formation is correlated with the Irati Formation of the Paraná and Pelotas Basins in Rio Grande do Sul, Brazil, the Black Rock Member of the Falkland Islands, and with the Huab Formation in the Huab Basin of northwestern Namibia. The lower section correlates with the Palermo and Rio Bonito Formations of the Paraná Basin and the Prince Albert Formation of the Karoo. The Permian sequence in the Huab Basin is much thinner than those of the Paraná and Karoo Basins. The fossil assemblages of \"Glossopteris\" and \"Mesosaurus\" are known from other parts of Gondwana; the Vryheid Formation of South Africa and coal deposits of the Lower Permian in Australia.\n\nThe Whitehill Formation has provided a variety of fossil insects, rare flora and \"Mesosaurus\" fossils, typical for the Permian of Gondwana. The mesosaurids are preserved as molds filled with gypsum crystals; they are commonly disarticulated. More rarely, casts of mesosaurids are found. Plant stems are similarly rare, and most commonly occur as chloritized fragments floating in the pink lower, more massive mudstone succession of the Whitehill Formation, while coprolites containing either palaeoniscoid scales or fragmentary crustacean carapaces are preserved on bedding planes.\n\nThe following fossils have been reported from the Whitehill Formation:\n\n\n\n\n\nThe high concentration of organic matter in the black shales of the Whitehill Formation make it an interesting target for shale gas exploration. The formation is considered the prime focus for potential shale gas prospects in the Southern Karoo. TOC levels average at 4.5% TOC, and range from 0.5 to 14.7%, comparable to known shale gas producing formations as the Marcellus Formation and the Barnett Shale. The formation is considered to have economic potential in an area around Beaufort West to Graaff-Reinet.\n\nResults from Rock-Eval pyrolysis, vitrinite reflectance measurements, open pyrolysis and thermovaporization analyses carried out on core samples drilled through the formation show that organic matter has reached an advanced stage of kerogen development. These rocks can therefore be classified as overmature, likely because of the thermotectonic processes related to the Cape orogeny forming the Cape Fold Belt overprint on lower Karoo rocks in the study area. It is possible that the maturity of the shales decreases farther north in the Karoo basin.\n\n\n\n"}
