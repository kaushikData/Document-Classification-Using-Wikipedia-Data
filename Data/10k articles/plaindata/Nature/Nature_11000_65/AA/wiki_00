{"id": "4082574", "url": "https://en.wikipedia.org/wiki?curid=4082574", "title": "12AU7", "text": "12AU7\n\nThe 12AU7 and its variants are a miniature nine-pin (B9A base) medium-gain dual triode vacuum tube. It belongs to a large family of dual triode vacuum tubes which share the same pinout (RETMA 9A). 12AU7 is also known in Europe under its Mullard–Philips tube designation ECC82. There are many equivalent tubes with different names, some identical, some designed for ruggedness, long life, or other characteristics; examples are the US military 5814A and the European special-quality E82CC and E182CC.\n\nThe tube is popular in hi-fi vacuum tube audio as a low-noise line amplifier, driver (especially for tone stacks), and phase-inverter in vacuum tube push–pull amplifier circuits. It was widely used, in special-quality versions such as E82CC and 5814A, in pre-semiconductor digital computer circuitry. Use of special-quality versions outside of the purpose they were designed for may not be optimal; for example, a version for digital computers may be designed for long life without cathode poisoning when mostly switched to low-current mode in switching applications, but with little attention to parameters of interest only for linear applications such as linearity of transfer characteristic, matching between the two sections, microphony, etc.\n\nThis tube is essentially two 6C4s or two EC90s in the same envelope.\nDouble triodes of the 12AU7 family have a center-tapped filament for use in either 6.3V 300mA or 12.6V 150mA heater circuits.\n\n\n"}
{"id": "53188828", "url": "https://en.wikipedia.org/wiki?curid=53188828", "title": "A Serpentis", "text": "A Serpentis\n\nThe Bayer designation A Serpentis is shared by two stars in the head of the constellation Serpens:\n"}
{"id": "23548517", "url": "https://en.wikipedia.org/wiki?curid=23548517", "title": "Air pollutant concentrations", "text": "Air pollutant concentrations\n\nAir pollutant concentrations, as measured or as calculated by air pollution dispersion modeling, must often be converted or corrected to be expressed as required by the regulations issued by various governmental agencies. Regulations that define and limit the concentration of pollutants in the ambient air or in gaseous emissions to the ambient air are issued by various national and state (or provincial) environmental protection and occupational health and safety agencies.\n\nSuch regulations involve a number of different expressions of concentration. Some express the concentrations as ppmv (parts per million by volume) and some express the concentrations as mg/m (milligrams per cubic meter), while others require adjusting or correcting the concentrations to reference conditions of moisture content, oxygen content or carbon dioxide content. This article presents methods for converting concentrations from ppmv to mg/m (and vice versa) and for correcting the concentrations to the required reference conditions.\n\nAll of the concentrations and concentration corrections in this article apply only to air and other gases. They are not applicable for liquids.\n\nThe conversion equations depend on the temperature at which the conversion is wanted (usually about 20 to 25 °C). At an ambient sea level atmospheric pressure of 1 atm (101.325 kPa or 1.01325 bar), the general equation is:\n\nand for the reverse conversion:\n\nNotes:\n\nAir pollutant concentrations expressed as mass per unit volume of atmospheric air (e.g., mg/m, µg/m, etc.) at sea level will decrease with increasing altitude. The concentration decrease is directly proportional to the pressure decrease with increasing altitude. Some governmental regulatory jurisdictions require industrial sources of air pollution to comply with sea level standards corrected for altitude. In other words, industrial air pollution sources located at altitudes well above sea level must comply with significantly more stringent air quality standards than sources located at sea level (since it is more difficult to comply with lower standards). For example, New Mexico's Department of the Environment has a regulation with such a requirement.\n\nThe change of atmospheric pressure with altitude (<20 km) can be obtained from this equation:\n\nGiven an air pollutant concentration at sea-level atmospheric pressure, the concentration at higher altitudes can be obtained from this equation:\n\nAs an example, given an air pollutant concentration of 260 mg/m at sea level, calculate the equivalent pollutant concentration at an altitude of 2800 meters:\n\nNote:\n\nMany environmental protection agencies have issued regulations that limit the concentration of pollutants in gaseous emissions and define the reference conditions applicable to those concentration limits. For example, such a regulation might limit the concentration of NOx to 55 ppmv in a dry combustion exhaust gas (at a specified reference temperature and pressure) corrected to 3 volume percent O in the dry gas. As another example, a regulation might limit the concentration of total particulate matter to 200 mg/m of an emitted gas (at a specified reference temperature and pressure) corrected to a dry basis and further corrected to 12 volume percent CO in the dry gas.\n\nEnvironmental agencies in the USA often use the terms \"dscf\" or \"scfd\" to denote a \"standard\" cubic foot of dry gas. Likewise, they often use the terms \"dscm\" or \"scmd\" to denote a \"standard\" cubic meter of gas. Since there is no universally accepted set of \"standard\" temperature and pressure, such usage can be and is very confusing. It is strongly recommended that the reference temperature and pressure always be clearly specified when stating gas volumes or gas flow rates.\n\nIf a gaseous emission sample is analyzed and found to contain water vapor and a pollutant concentration of say 40 ppmv, then 40 ppmv should be designated as the \"wet basis\" pollutant concentration. The following equation can be used to correct the measured \"wet basis\" concentration to a \"dry basis\" concentration:\n\nAs an example, a wet basis concentration of 40 ppmv in a gas having 10 volume percent water vapor would have a:\n\nThe following equation can be used to correct a measured pollutant concentration in a dry emitted gas with a measured O content to an equivalent pollutant concentration in a dry emitted gas with a specified reference amount of O:\n\nAs an example, a measured NOx concentration of 45 ppmv in a dry gas having 5 volume % O is:\n\nwhen corrected to a dry gas having a specified reference O content of 3 volume %.\n\nNote:\n\nThe following equation can be used to correct a measured pollutant concentration in an emitted gas (containing a measured CO content) to an equivalent pollutant concentration in an emitted gas containing a specified reference amount of CO:\n\nAs an example, a measured particulates concentration of 200 mg/m in a dry gas that has a measured 8 volume % CO is:\n\nwhen corrected to a dry gas having a specified reference CO content of 12 volume %.\n"}
{"id": "19282634", "url": "https://en.wikipedia.org/wiki?curid=19282634", "title": "Anthousai", "text": "Anthousai\n\nAnthousai ( from ἄνθος \"ánthos\", meaning \"flower, blossom\") are nymphs of flowers in Greek mythology. They were described as having hair that resembled hyacinth flowers. Chloris the nymph is an example of a flower nymph.\n\n"}
{"id": "1028817", "url": "https://en.wikipedia.org/wiki?curid=1028817", "title": "Auloniad", "text": "Auloniad\n\nThe names of different species of nymphs varied according to their natural abodes. The Auloniad (; Αὐλωνιάς from the classical Greek αὐλών \"valley, ravine\") was a nymph who could be found in mountain pastures and vales, often in the company of Pan, the god of nature.\n\nEurydice, for whom Orpheus traveled into dark Hades, was an Auloniad, and it was in the valley of the Thessalian river Pineios where she met her death, indirectly, at the hands of Aristaeus. Aristaeus, son of the god Apollo and the nymph Cyrene, desired to ravish Eurydice. Either disgust or fear made the nymph run away from him without looking where she was going. Eurydice trod on a poisonous serpent and died.\n\n"}
{"id": "12438068", "url": "https://en.wikipedia.org/wiki?curid=12438068", "title": "Baptism in Mormonism", "text": "Baptism in Mormonism\n\nIn Mormonism, baptism is recognized as the first of several ordinances (rituals) of the gospel.\n\nMuch of the theology of Mormon baptism was established during the early Latter Day Saint movement founded by Joseph Smith. Baptism must be by immersion and is for the remission of sins (meaning that through baptism, past sins are forgiven), and occurs after one has shown faith and repentance. Mormon baptism does not purport to remit any sins other than personal ones, as adherents do not believe in original sin. Mormon baptisms also occur only after an \"age of accountability\" which is defined as the age of eight years. The theology thus rejects infant baptism. According to the account in Joseph Smith–History 1:68, the first Mormon baptisms occurred on May 15, 1829, when Joseph Smith and Oliver Cowdery baptized each other in the Susquehanna River near Harmony, Pennsylvania shortly after receiving the Aaronic priesthood from John the Baptist.\n\nIn addition, Mormon theology requires baptism only be performed by a priesthood holder. The minimum required priesthood level to perform a baptism in Mormonism is Priest, who is a worthy male members at least 16 years old, and the rite is presided by a Bishop. Because the churches of the Latter Day Saint movement operate under a lay priesthood, children raised in a Mormon family are usually baptized by a father or close male friend or family member.\n\nMormons view baptism as symbolic of Jesus' death, burial and resurrection, and also symbolic of the baptized individual separating from the \"natural\" or sinful aspects of humanity and becoming spiritually reborn as a disciple of Jesus.\n\nMembership into a Latter Day Saint church is granted only by baptism. Most Latter Day Saint churches do not recognize the baptisms of other faiths as they believe baptisms must be performed under the church's unique priesthood authority. Therefore, any converts are baptized at their conversion to Mormonism.\n\nBaptism (and any subsequent callings in the church) are preceded by a baptismal interview.\n\nDoctrine and Covenants Section 20 first documented the instructions for Mormon baptism.\n\nPeople being baptized or performing the baptism typically wear a \"one-piece suit\" with \"short sleeves, and is lined to the knee.\" Baptisms are usually performed in a baptismal font, but any body of water in which the person may be completely immersed is acceptable. The person administering the baptism must recite the prayer exactly, and immerse every part, limb, hair and clothing of the person being baptized. If there are any mistakes, or if any part of the person being baptized is not fully immersed, the baptism is repeated until it is performed correctly. In addition to the baptizer, two priesthood holders witness the baptism to ensure that it is performed properly.\n\nFollowing baptism, Latter Day Saints receive the Gift of the Holy Ghost by the laying on of hands of a Melchizedek Priesthood holder.\n\nThe Church of Jesus Christ of Latter-day Saints (LDS Church) practices baptism for the dead \"vicariously\" or \"by proxy\" in their temples for anyone who did not receive these ordinances while living.\n\nAfter the death of Joseph Smith, the founder of the Latter Day Saint movement, in 1844, rebaptism became a more important ordinance in The Church of Jesus Christ of Latter-day Saints (LDS Church), as led by Brigham Young. Young led his group to the Great Basin in what is now Utah, and most of his followers were rebaptised soon after arriving as a sign that they would rededicate their lives to Christ. During the \"Mormon Reformation\" of 1856–57, rebaptism became an extremely important ordinance, signifying that the church member confessed their sins and would live a life of a Latter-day Saint. Church members were rebaptized prior to new covenants and ordinances, such as ordination to a new office of the priesthood, receiving temple ordinances, getting married, or entering plural marriage.\n\nRebaptism remains a practice in the LDS Church today, but is practiced when a previously excommunicated member rejoins the church. In such cases, the wording of the ordinance is identical to that of the first baptismal ordinance.\n\n"}
{"id": "5260710", "url": "https://en.wikipedia.org/wiki?curid=5260710", "title": "Beta plane", "text": "Beta plane\n\nIn geophysical fluid dynamics, an approximation whereby the Coriolis parameter, \"f\", is set to vary linearly in space is called a beta plane approximation.\n\nOn a rotating sphere such as the Earth, \"f\" varies with the sine of latitude; in the so-called f-plane approximation, this variation is ignored, and a value of \"f\" appropriate for a particular latitude is used throughout the domain. This approximation can be visualized as a tangent plane touching the surface of the sphere at this latitude.\n\nA more accurate model is a linear Taylor series approximation to this variability about a given latitude formula_1:\n\nformula_2, where formula_3 is the Coriolis parameter at formula_1, formula_5 is the Rossby parameter, formula_6 is the meridional distance from formula_1, formula_8 is the angular rotation rate of the Earth, and formula_9 is the Earth's radius.\n\nIn analogy with the f-plane, this approximation is termed the beta plane, even though it no longer describes dynamics on a hypothetical tangent plane. The advantage of the beta plane approximation over more accurate formulations is that it does not contribute nonlinear terms to the dynamical equations; such terms make the equations harder to solve. The name 'beta plane' derives from the convention to denote the linear coefficient of variation with the Greek letter β.\n\nThe beta plane approximation is useful for the theoretical analysis of many phenomena in geophysical fluid dynamics since it makes the equations much more tractable, yet retains the important information that the Coriolis parameter varies in space. In particular, Rossby waves, the most important type of waves if one considers large-scale atmospheric and oceanic dynamics, depend on the variation of \"f\" as a restoring force; they do not occur if the Coriolis parameter is approximated only as a constant.\n\n\n"}
{"id": "12870568", "url": "https://en.wikipedia.org/wiki?curid=12870568", "title": "Bullaun", "text": "Bullaun\n\nA bullaun (; from a word cognate with \"bowl\" and French \"bol\") is the term used for the depression in a stone which is often water filled. Natural rounded boulders or pebbles may sit in the bullaun. The size of the bullaun is highly variable and these hemispherical cups hollowed out of a rock may come as singles or multiples with the same rock.\n\nLocal folklore often attaches religious or magical significance to bullaun stones, such as the belief that the rainwater collecting in a stone's hollow has healing properties. Ritual use of some bullaun stones continued well into the Christian period and many are found in association with early churches, such as the 'Deer' Stone at Glendalough, County Wicklow. The example at St Brigit's Stone, County Cavan, still has its 'cure' or 'curse' stones. These would be used by turning them whilst praying for or cursing somebody. In May 2012 the second cursing stone to be found in Scotland was discovered on Canna and drawn soon after by archaeological illustrator Thomas Small. It has been dated to c. 800. The first was found on the Shiant Isles. It has been dated to c. 800. The stones were latterly known as 'Butterlumps'.\nSt. Aid or Áed mac Bricc was Bishop of Killare in 6th-century. At Saint Aid's birth his head had hit a stone, leaving a hole in which collected rainwater that cured all ailments, thus identifying it with the Irish tradition of Bullaun stones.\n\nBullauns are not unique to Ireland and Scotland, being also found on the Swedish island of Gotland, and in Lithuania and France. Possibly enlarged from already-existing solution-pits caused by rain, bullauns are reminiscent of the cup-marked stones which occur all over Atlantic Europe, and their significance (if not their precise use) must date from Neolithic times.\n\n"}
{"id": "54342619", "url": "https://en.wikipedia.org/wiki?curid=54342619", "title": "Central Basin Spreading Center", "text": "Central Basin Spreading Center\n\nCentral Basin Spreading Center (formerly Central Basin Fault) is a seafloor spreading center of the West Philippine Sea Basin.\n"}
{"id": "3567939", "url": "https://en.wikipedia.org/wiki?curid=3567939", "title": "Coalescence (physics)", "text": "Coalescence (physics)\n\nCoalescence is the process by which two or more droplets, bubbles or particles merge during contact to form a single daughter droplet, bubble or particle. It can take place in many processes, ranging from meteorology to astrophysics. For example, it is seen in the formation of raindrops as well as planetary and star formation.\n\nIn meteorology, its role is crucial in the formation of rain. As droplets are carried by the updrafts and downdrafts in a cloud, they collide and coalesce to form larger droplets. When the droplets become too large to be sustained on the air currents, they begin to fall as rain. Adding to this process, the cloud may be seeded with ice from higher altitudes, either via the cloud tops reaching , or via the cloud being seeded by ice from cirrus clouds.\n\nIn Cloud physics the main mechanism of collision is the different terminal velocity that between the droplets, the terminal velocity is a function of the droplet size. The other factors that determine the collision rate is the droplet concentration and turbulence.\n\n\n"}
{"id": "16082692", "url": "https://en.wikipedia.org/wiki?curid=16082692", "title": "Conrad discontinuity", "text": "Conrad discontinuity\n\nThe Conrad discontinuity corresponds to the sub-horizontal boundary in continental crust at which the seismic wave velocity increases in a discontinuous way. This boundary is observed in various continental regions at a depth of 15 to 20 km, however it is not found in oceanic regions.\n\nThe Conrad discontinuity (named after the seismologist Victor Conrad) is considered to be the border between the upper continental crust and the lower one. It is not as pronounced as the Mohorovičić discontinuity, and absent in some continental regions. Up to the middle 20th Century the upper crust in continental regions was seen to consist of felsic rocks such as granite (sial, for silica-aluminium), and the lower one to consist of more magnesium-rich mafic rocks like basalt (sima, for silica-magnesium). Therefore, the seismologists of that time considered that the Conrad discontinuity should correspond to a sharply defined contact between the chemically distinct two layers, sial and sima.\n\nHowever, from the 1960s onward this theory was strongly contested among geologists. The exact geological significance of the Conrad discontinuity is still not clarified. The possibility that it represents the transition from amphibolite facies to granulite facies metamorphism has been given some support from observations of the uplifted central part of the Vredefort crater and the surrounding Kaapvaal Craton.\n"}
{"id": "1839990", "url": "https://en.wikipedia.org/wiki?curid=1839990", "title": "Crypton (particle)", "text": "Crypton (particle)\n\nIn particle physics, the crypton is a hypothetical superheavy particle, thought to exist in a hidden sector of string theory. It has been proposed as a candidate particle to explain the dark matter content of the universe. Cryptons arising in the hidden sector of a superstring-derived flipped SU(5) GUT model have been shown to be metastable with a lifetime exceeding the age of the universe. Their slow decays may provide a source for the ultra-high-energy cosmic rays (UHECR).\n\n"}
{"id": "16702346", "url": "https://en.wikipedia.org/wiki?curid=16702346", "title": "Dead Sea Transform", "text": "Dead Sea Transform\n\nThe Dead Sea Transform (DST) fault system, also sometimes referred to as the Dead Sea Rift, is a series of faults that run from the Maras Triple Junction (a junction with the East Anatolian Fault in southeastern Turkey) to the northern end of the Red Sea Rift (just offshore of the southern tip of the Sinai Peninsula). The fault system forms the transform boundary between the African Plate to the west and the Arabian Plate to the east. It is a zone of left lateral displacement, signifying the relative motions of the two plates. Both plates are moving in a general north-northeast direction, but the Arabian Plate is moving faster, resulting in the observed left lateral motions along the fault of approximately 107 km. A component of extension is also present in the southern part of the transform, which has contributed to a series of depressions, or pull-apart basins, forming the Gulf of Aqaba, Dead Sea, Sea of Galilee and Hula basins.\n\nThe DST fault system is generally considered to be a transform fault that has accommodated a 105 km northwards displacement of the Arabian Plate. This interpretation is based on observation of offset markers, such as river terraces, gullies and archaeological features, giving horizontal slip rates of several mm per year over the last few million years. GPS data give similar rates of present-day movement of the Arabian Plate relative to the Africa Plate. It has also been proposed that the fault zone is a rift system that is an incipient oceanic spreading center, the northern extension of the Red Sea Rift.\n\nThe Dead Sea Transform began to form during the early to mid-Miocene, when there was a change in plate motions and rifting stopped in the Gulf of Suez Rift. The initial phase of northward propagation reached as far as southernmost Lebanon and was followed by a period in the Late Miocene where continuing displacement across the plate boundary was taken up mainly by shortening in the Palmyride fold belt. A total displacement of 64 km has been estimated for this early phase of motion. In the Pliocene the DST propagated northwards once more through Lebanon into northwestern Syria before reaching the East Anatolian Fault.\n\nThe southern section of the DST is about 400 km long, extending from the spreading center in the Red Sea at southern end of the Gulf of Aqaba to just north of the Hula basin in southernmost Lebanon.\n\nThe Gulf of Aqaba was created by movement on four left-stepping strike-slip fault segments in a diagonal stepwise sequence known as echelon formation. In the areas where these segments overlap, pull-apart basins have developed, forming three bathymetric lows known as the Daka Deep, the Aragonese Deep and the Elat Deep. Parts of three of these faults ruptured during the 1995 Gulf of Aqaba earthquake.\n\nThe Wadi Arabah (Arava Valley) segment of the DST extends for about 160 km from the Gulf of Aqaba to the southern end of the Dead Sea. Some researchers have further broken down this segment, recognising two separate segments, Avrona and Arava. The Avrona fault extends from the northern part of the Gulf of Aqaba for about 50 km along the Arava Valley. The Arava fault runs from just north of the Avrona fault segment for about 100 km.\n\nA slip rate of 4 ±2 mm per year has been estimated from the offset of gullies across the fault. Four major earthquakes are thought to have occurred due to movement on this fault in the last 1,000 years, in 1068, 1212, 1293 and 1458.\n\nThe Dead Sea is formed in a pull-apart basin due to the left-stepping offset between the Wadi Arabah and Jordan Valley segments. The part of the basin with a sedimentary fill of more than 2 km is 150 km long and 15–17 km wide in its central part. In the north, the fill reaches its maximum thickness of about 10 km. The sequence includes Miocene fluvial sandstones of the Hazeva Formation overlain by a sequence of Late Miocene to early Pliocene evaporites, mainly halite, the Sedom Formation, and a lacustrine to fluvial sequence of Pliocene to recent age.\n\nThe Jordan valley segment of the DST, the Jordan Rift Valley, runs for about 100 km from the northwestern part of the Dead Sea to the southeastern part of the Sea of Galilee along the Jordan Valley. A slip rate of between 4.7 and 5.1 mm per year has been estimated over the last 47,500 years. The entire segment is thought to have ruptured during the earthquake in 749 and again in 1033, the most recent major earthquake along this structure. The deficit in slip that has built up since the 1033 event is sufficient to cause an earthquake of ~7.4.\n\nThe Sea of Galilee Basin or Kinneret Basin is a pull apart formed between Jordan valley fault along its eastern edge and a set of smaller faults to the north. The central site of the basin's deepest sedimentary fill (its \"depocentre\" in geologists' jargon) lies on the eastern side, against the continuation of the Jordan valley fault. The thickness of the fill is estimated as 3 km down to the deepest mapped seismic reflection, correlated with the top of a basalt layer that was extruded about four million years ago.\n\nThe Hula pull-apart basin lies to the north of the Sea of Galilee basin and is formed between several short fault segments. The currently active part of the basin is relatively narrow. The Hula Western Border Fault defines the western side of the basin and splays to the north into several faults, including the Roum fault and the Yammouneh fault. The Hula Eastern Border Fault continues northwards from the northeastern part of the Sea of Galilee, forming the eastern edge of the basin and linking eventually to the Rachaya fault.\n\nThe DST splays within the area of the restraining bend, with several distinct active fault segments recognised.\n\nThe Yammouneh fault is the main fault strand within the Lebanon restraining bend, carrying most of the plate boundary displacement. It is SSW-NNE trending and runs for about 170 km from the northwestern end of the Hula Basin to its junction with the Missyaf Fault. It has been the location of several major historical earthquakes, such as the 1202 Syria event. The estimated average slip rate along the Yammouneh fault is 4.0 to 5.5 mm per year, with a major earthquake recurrence interval of 1020 to 1175 years. There have been no major earthquakes since that in 1202.\nThe Roum fault branches away from the Yammouneh fault at the northwestern part of the Hula Basin. It can be traced from there northwards for about 35 km before becoming indistinct. Movement on this fault has been linked to the 1837 Galilee earthquake. A slip-rate of 0.86—1.05 mm per year has been estimated.\n\nThis fault zone comprises two main fault strands, the Rachaya and Serghaya faults. The Serghaya fault branches off the Hula Eastern Border Fault, continuing northeastwards to the south of Mount Hermon into the Anti-Lebanon range where it becomes SSW-NNE trending. The fault has a slip rate of about 1.4 mm per year. Movement on this fault is thought to be responsible for the November 1759 earthquake. The Rachaya fault also branches off the Hula Eastern Border Fault, trending SSW-NNE, passing to the north of Mount Hermon. No slip rate has yet been estimated for this fault. The Rachaya fault is the interpreted location of the October 1759 earthquake.\n\nThe northern section of the DST extends from the northern end of the Yammouneh fault up to the triple junction with the East Anatolian Fault. The over all deformation style is transpressional, in keeping with the relative plate motions as determined from GPS measurements.\n\nThis fault segment, also known as the Ghab fault, runs for about 70 km from the northern end of the Yammouneh fault into the Ghab basin. The estimated slip rate for this segment is 6.9 mm per year. Major historical earthquakes interpreted to have occurred along this structure include the >7 events in AD 115 and 1170. No major earthquakes have been recorded since 1170, suggesting that such an event is overdue.\n\nThe Ghab basin was formed in the Pliocene and is interpreted to be a pull-apart basin formed due to the overlap at the left-stepping offset between the Missyaf fault and the Hacıpaşa fault. The basin is about 60 km long and 15 km wide. Based on the interpretation of seismic reflection data and a single well penetration (Ghab-1) the fill of the basin is thought to be entirely Pliocene to recent in age. There are two main depocentres in the basin at the northern and southern end, separated by an intrabasinal high.\n\nThe Hacıpaşa fault extends from the Ghab basin into the Amik basin. It is thought to carry the bulk of the plate boundary displacement linking through on to the Karasu fault. Major earthquakes in 1408 and 1872 have been linked to movement on this fault.\n\nThe Karasu fault or Amanos fault has SW-NE trend and represents part of the transition from the DST to the East Anatolian Fault. It has an estimated slip rate of 1.0 to 1.6 mm per year for the whole Quaternary. No historical earthquakes have been linked with movement on this fault.\n\n\n"}
{"id": "50758294", "url": "https://en.wikipedia.org/wiki?curid=50758294", "title": "Des singularités de la nature", "text": "Des singularités de la nature\n\nDes singularités de la nature is an essay on natural history by the French philosopher and author Voltaire, first published in 1768. In it, he defends Preformationism, the idea that organisms develop from tiny versions of themselves. He defends the idea of a supreme being, and the idea that many features of the natural world have been made to benefit people, including noses for smelling and mountains for forming the landscape.\n\n"}
{"id": "23099899", "url": "https://en.wikipedia.org/wiki?curid=23099899", "title": "Environmental issue", "text": "Environmental issue\n\nEnvironmental issues are harmful effects of human activity on the biophysical environment. Environmental protection is a practice of protecting the natural environment on individual, organizational or governmental levels, for the benefit of both the environment and humans. Environmentalism, a social and environmental movement, addresses environmental issues through advocacy, education and activism.\n\nThe carbon dioxide equivalent of greenhouse gases (GHG) in the atmosphere has already exceeded 400 parts per million (NOAA) (with total \"long-term\" GHG exceeding 455 parts per million) (Intergovernmental Panel on Climate Change Report). This level is considered a tipping point. \"The amount of greenhouse gas in the atmosphere is already above the threshold that can potentially cause dangerous climate change. We are already at risk of many areas of pollution...It's not next year or next decade, it's now.\" The UN Office for the Coordination of Humanitarian Affairs (OCHA) has stated \"Climate change is not just a distant future threat. It is the main driver behind rising humanitarian needs and we are seeing its impact. The number of people affected and the damages inflicted by extreme weather has been unprecedented.\" Further, OCHA has stated:Climate disasters are on the rise. Around 70 percent of disasters are now climate related – up from around 50 percent from two decades ago. These disasters take a heavier human toll and come with a higher price tag. In the last decade, 2.4 billion people were affected by climate related disasters, compared to 1.7 billion in the previous decade. The cost of responding to disasters has risen tenfold between 1992 and 2008.Destructive sudden heavy rains, intense tropical storms, repeated flooding and droughts are likely to increase, as will the vulnerability of local communities in the absence of strong concerted action.Environment destruction caused by humans is a global problem, and this is a problem that is on going every day. By year 2050, the global human population is expected to grow by 2 billion people, thereby reaching a level of 9.6 billion people (Living Blue Planet 24). The human effects on Earth can be seen in many different ways. A main one is the temperature rise, and according to the report ”Our Changing Climate”, the global warming that has been going on for the past 50 years is primarily due to human activities (Walsh, et al. 20). Since 1895, the U.S. average temperature has increased from 1.3 °F to 1.9 °F, with most of the increase taken place since around year 1970 (Walsh, et al. 20).\n\nMajor current environmental issues may include climate change, pollution, environmental degradation, and resource depletion etc. The conservation movement lobbies for protection of endangered species and protection of any ecologically valuable natural areas, genetically modified foods and global warming.\n\nThe level of understanding of Earth has increased markebly in recent times through science especially with the application of the scientific method. Environmental science is now a multi-disciplinary academic study taught and researched at many universities. This is used as a basis for addressing environmental issues.\n\nLarge amounts of data have been gathered and these are collated into reports, of which a common type is the State of the Environment publications. A recent major report was the Millennium Ecosystem Assessment, with input from 1200 scientists and released in 2005, which showed the high level of impact that humans are having on ecosystem services.\n\nEnvironmental issues are addressed at a regional, national or international level by government organizations.\n\nThe largest international agency, set up in 1972, is the United Nations Environment Programme. The International Union for Conservation of Nature brings together 83 states, 108 government agencies, 766 Non-governmental organizations and 81 international organizations and about 10,000 experts and scientists from countries around the world. International non-governmental organizations include Greenpeace, Friends of the Earth and World Wide Fund for Nature. Governments enact environmental policy and enforce environmental law and this is done to differing degrees around the world.\n\nSustainability is the key to prevent or reduce the effect of environmental issues. There is now clear scientific evidence that humanity is living unsustainably, and that an unprecedented collective effort is needed to return human use of natural resources to within sustainable limits. For humans to live sustainably, the Earth's natural resources must be used at a rate at which they can be replenished (and by limiting global warming).\n\nConcerns for the environment have prompted the formation of green parties, political parties that seek to address environmental issues. Initially these were formed in Australia, New Zealand and Germany but are now present in many other countries.\n\nThere are an increasing number of films being produced on environmental issues, especially on climate change and global warming. Al Gore's 2006 film \"An Inconvenient Truth\" gained commercial success and a high media profile.\n\n\nIssues\n\nSpecific issues\n"}
{"id": "1840762", "url": "https://en.wikipedia.org/wiki?curid=1840762", "title": "Fossil water", "text": "Fossil water\n\nFossil water or paleowater is an ancient body of water that has been contained in some undisturbed space, typically groundwater in an aquifer, for millennia. Other types of fossil water can include subglacial lakes, such as Antarctica's Lake Vostok, and even ancient water on other planets.\n\nUNESCO defines \"fossil groundwater\" as\nDetermining the time since water infiltrated usually involves analyzing isotopic signatures. Determining \"fossil\" status—whether or not that particular water has occupied that particular space since the distant past—involves modeling the flow, recharge, and losses of aquifers, which can involve significant uncertainty. Some aquifers are hundreds of meters deep and underlie vast areas of land. Research techniques in the field are developing quickly and the scientific knowledge base is growing. In the cases of many aquifers, research is lacking or disputed as to the age of the water and the behavior of the water inside the aquifer.\n\nLarge, prolific aquifers (notably the Nubian Sandstone Aquifer System and the Ogallala Aquifer) containing fossil water are of significant socio-economic value. Fossil water is extracted from these aquifers for many human purposes, notably, agriculture, industry, and consumption. In arid regions, some aquifers containing available and usable water receive little to no significant recharge, effectively making groundwater in those aquifers a non-renewable resource. Extraction rates greater than recharge rates result in lowering of the water table and can lead to groundwater depletion. Extraction of non-renewable groundwater resources is referred to as groundwater \"mining\" because of their finite nature.\n\nIn other regions that do receive significant precipitation and where recharge does occur, fossil water may be extracted and eventually replaced by younger water, in which case the water in those aquifers could be considered a renewable resource.\n\nAquifers are typically composed of semi-porous rock or unconsolidated material whose pore space has been filled with water. In the relatively rare cases of confined aquifers, an impermeable geologic layer (e.g. clay or calcrete) encloses an aquifer, isolating the water within, sometimes for millennia. More commonly, fossil water is found in arid or semi-arid regions where the climate was significantly more humid in recent geologic history. In some semi-arid regions, the majority of precipitation evaporates before it can infiltrate and result in any significant aquifer recharge.\n\nMost fossil groundwater has been estimated to have originally infiltrated within the Holocene and Pleistocene (10,000-40,000 years ago). Some fossil groundwater is associated with the melting of ice in the time since the last glacial maximum. Dating of groundwater relies on measuring concentrations of certain stable isotopes, including (tritium) and (\"heavy\" oxygen), and comparing values with known concentrations of the geologic past.\n\nFossil water can potentially dissolve and absorb a number of ions from its host rock. Salinity in groundwater can be higher than seawater. In some cases, some form of treatment is required to make these waters suitable for human use. Saline fossil aquifers can also store significant quantities of oil and natural gas.\n\nThe Ogallala or High Plains Aquifer sits under 450,000 km of 8 states of the United States of America. It is one of the largest freshwater deposits in the world. The aquifer is composed of unconsolidated alluvial deposits. Groundwater in this aquifer has been dated to have been deposited in the humid time following the last glacial maximum. In much of the aquifer’s area, an impermeable layer of calcrete prevents precipitation from infiltrating. In other regions of the aquifer, some relatively small rates of recharge have been measured.\n\nThe aquifer supplies water for the many people who live above it and for widespread agricultural uses. In many areas, the water table has dropped drastically due to heavy extraction. Depletion rates are not stabilizing, in fact, they have been increasing in recent decades.\n\nThe Nubian Sandstone Aquifer System is located in northeastern Africa, under the nations of Sudan, Libya, Egypt, and Chad, covering about 2,000,000 km. It is largely composed of many hydraulically interconnected sandstone aquifers. Some parts of the system are considered to be confined, if not somewhat leaky, due to impermeable layers such as marine shales. The water was deposited between 4,000 and 20,000 years ago, varying by specific locality.\n\nThe water in the Nubian Sandstone Aquifer System is of high importance to the people living above it, and has been for millennia. In modern times, as demand increases, avoiding depletion and conflict will depend on careful cross-boundary monitoring and planning. Libya and Egypt are currently planning development projects to withdraw significant amounts of the aquifer’s fossil water for use.\n\nOther fossil aquifers have been identified throughout Northern Africa.\n\nThe Kalahari Desert in central southern Africa (Botswana, Namibia, and South Africa). Geology of the area includes significant karst formations. Most of the precipitation in the region evaporates before it can contribute to significant recharge of the aquifers below. Whether or not the region’s aquifers receive any significant recharge has long been the subject of debate and research. In the northern region of the Kalahari, a deep aquifer in Cave sandstone was found to have isotopic signatures that suggested it had been confined with little to no leakage for long periods of time.\n\n"}
{"id": "27879368", "url": "https://en.wikipedia.org/wiki?curid=27879368", "title": "Functional divergence", "text": "Functional divergence\n\nFunctional divergence is the process by which genes, after gene duplication, shift in function from an ancestral function. Functional divergence can result in either subfunctionalization, where a paralog specializes one of several ancestral functions, or neofunctionalization, where a totally new functional capability evolves. It is thought that this process of gene duplication and functional divergence is a major originator of molecular novelty and has produced the many large protein families that exist today.\n\nFunctional divergence is just one possible outcome of gene duplication events. Other fates include nonfunctionalization where one of the paralogs acquires deleterious mutations and becomes a pseudogene and superfunctionalization (reinforcement), where both paralogs maintain original function. While gene, chromosome, or whole genome duplication events are considered the canonical sources of functional divergence of paralogs, orthologs (genes descended from speciation events) can also undergo functional divergence and horizontal gene transfer can also result in multiple copies of a gene in a genome, providing the opportunity for functional divergence.\n\nMany well known protein families are the result of this process, such as the ancient gene duplication event that led to the divergence of hemoglobin and myoglobin, the more recent duplication events that led to the various subunit expansions (alpha and beta) of vertebrate hemoglobins, or the expansion of G-protein alpha subunits \n\n"}
{"id": "40398208", "url": "https://en.wikipedia.org/wiki?curid=40398208", "title": "GSAT-7", "text": "GSAT-7\n\nGSAT-7 or INSAT-4F is a multi-band military communications satellite developed by ISRO. The Indian Navy is the user of the multi-band communication spacecraft, which has been operational since September 2013. According to defense experts, the satellite will enable the navy to extend its blue water capabilities and stop relying on foreign satellites like Inmarsat, which provide communication services to its ships.\n\nGSAT-7, the multi-band communication satellite named \"Rukmini\" carries the payloads in UHF, C band and . It is the first dedicated military communication satellite (unlie earlier dual use ones) built by ISRO that will provide services to the Indian defence forces with the main user being the Indian Navy. The GSAT-7 is the last of ISRO’s seven fourth-generation satellites. Its foreign launch cost has been put at Rs. 480 crore, with the satellite costing Rs. 185 crore. The multiple-band spacecraft will be used exclusively by the Navy to shore up secure, real-time communications among its warships, submarines, aircraft and land systems. GSAT-7/ INSAT-4F is said to significantly improve the country’s maritime security and intelligence gathering in a wide swathe on the eastern and western flanks of the Indian Ocean region, among others. ISRO is expected to launch the second satellite, GSAT-7A, in the second half of 2018. GSAT 7a will be used by the Indian air force . The state-of-the-art GSAT 7 satellite carrying payloads operating in UHF, S, C and Ku bands, had a lift-off mass of and is based on ISRO's satellite bus with some new technological elements, including the antennae. After a flight of almost 34 minutes, the satellite was injected into a geosynchronous transfer orbit (GTO) of perigee, apogee and an inclination of 3.5 degree with respect to the equator.\n\nThe satellite was launched early on 30 August 2013 atop an Ariane 5 ECA rocket from Kourou, French Guiana.\n\nIndia's first dedicated military satellite was put into a geosynchronous orbit, about above Earth, five days after it was launched after three orbit-raising manoeuvres from ISRO's Master Control Facility at Hassan in Karnataka. The 2.5-tonne spacecraft's antennae, including the ultra high frequency Helix antenna were deployed before it was stabilised on its three-axis in the orbit. All of the on-board transponders were switched on successfully on September 18, 2013\n\nRukmini will provide networking capabilities to various Indian Naval assets. During Theater-level Readiness and Operational Exercise (Tropex) in the Bay of Bengal in 2014, Rukmini was able to network about 60 ships and 75 aircraft seamlessly. The intention of the Indian Navy is to use this geostationary naval communication and surveillance satellite to especially cover activities up to the Malacca Straits in the east and the Hormuz Strait to the west. Rukmini has a nearly 2,000 nautical mile 'footprint' over the Indian Ocean Region.\n\n\n"}
{"id": "34920670", "url": "https://en.wikipedia.org/wiki?curid=34920670", "title": "Hologenome theory of evolution", "text": "Hologenome theory of evolution\n\nThe hologenome theory of evolution recasts the individual animal or plant (and other multicellular organisms) as a community or a \"holobiont\" – the host plus all of its symbiotic microbes. Consequently, the collective genomes of the holobiont form a \"hologenome\". Holobionts and hologenomes are structural entities that replace misnomers in the context of host-microbiota symbioses such as superorganism (i.e., an integrated social unit composed of conspecifics), organ, and metagenome. Variation in the hologenome may encode phenotypic plasticity of the holobiont and can be subject to evolutionary changes caused by selection and drift, if portions of the hologenome are transmitted between generations with reasonable fidelity. One of the important outcomes of recasting the individual as a holobiont subject to evolutionary forces is that genetic variation in the hologenome can be brought about by changes in the host genome and also by changes in the microbiome, including new acquisitions of microbes, horizontal gene transfers, and changes in microbial abundance within hosts. Although there is a rich literature on binary host–microbe symbioses, the hologenome concept distinguishes itself by including the vast symbiotic complexity inherent in many multicellular hosts. For recent literature on holobionts and hologenomes published in an open access platform, see the following reference.\n\nLynn Margulis coined the term holobiont in her 1991 book \"Symbiosis as a Source of Evolutionary Innovation: Specation and Morphogenesis\" (MIT Press). The term holobiont is derived from the Ancient Greek ὅλος (hólos, \"whole\"), and the word biont for a unit of life.\n\nIn September 1994, Richard Jefferson first introduced the term hologenome at a presentation at Cold Spring Harbor Laboratory. At the CSH Symposium and earlier, the unsettling number and diversity of microbes that were being discovered through the powerful tool of PCR-amplification of 16S ribosomal RNA genes was exciting, but confusing interpretations in diverse studies. A number of speakers referred to microbial contributions to mammalian or plant DNA samples as 'contamination'. In his lecture, Jefferson argued that these were likely not contamination, but rather essential components of the samples that reflected the actual genetic composition of the organism being studied, and that the logic of the organism's performance and capabilities would be embedded only in the hologenome. Observations on the ubiquity of microbes in plant and soil samples as well as laboratory work on molecular genetics of vertebrate-associated microbial enzymes informed this assertion .\n\nIn 2008, Eugene Rosenberg and Ilana Zilber-Rosenberg independently derived the term hologenome and developed the hologenome theory of evolution. This theory was originally based on their observations of Vibrio shiloi-mediated bleaching of the coral \"Oculina patagonica\". Since its first introduction, the theory has been promoted as a fusion of Lamarckism and Darwinism and expanded to all of evolution, not just that of corals. The history of the development of the hologenome theory and the logic undergirding its development was the focus of a cover article by Carrie Arnold in New Scientist in January, 2013. The most comprehensive treatment of the theory, including updates by the Rosenbergs on neutrality, pathogenesis and multi-level selection, can be found in their 2013 book.\n\nIn 2013, Robert Brucker and Seth Bordenstein re-invigorated the hologenome concept by showing that the gut microbiomes of closely related \"Nasonia\" wasp species are distinguishable, and contribute to hybrid death. This set interactions between hosts and microbes in a conceptual continuum with interactions between genes in the same genome. In 2015, Bordenstein and Kevin R. Theis outlined a conceptual framework that aligns with pre-existing theories in biology.\n\nMulticellular life is made possible by the coordination of physically and temporally distinct processes, most prominently through hormones. Hormones mediate critical activities in vertebrates, including ontogeny, somatic and reproductive physiology, sexual development, performance and behaviour. \nMany of these hormones – including most steroids and thyroxines – are secreted in inactive form through the endocrine and apocrine systems into epithelial corridors in which microbiota are widespread and diverse, including gut, urinary tract, lung and skin. There, the inactive hormones can be re-activated by cleavage of the glucuronide or sulfate residue, allowing them to be reabsorbed. Thus the concentration and bioavailability of many of the hormones is impacted by microbial cleavage of conjugated intermediaries, itself determined by a diverse population with redundant enzymatic capabilities. Aspects of enterohepatic circulation have been known for decades, but had been viewed as an ancillary effect of detoxification and excretion of metabolites and xenobiotics, including effects on lifetimes of pharmaceuticals, including birth control formulations.\n\nThe basic premise is that a spectrum of hormones can be re-activated and resorbed from epithelia, potentially modulating effective time and dose relationships of many vertebrate hormones. The ability to alter and modulate, amplify and suppress, disseminate and recruit new capabilities as microbially-encoded 'traits' means that sampling, sensing and responding to the environment become intrinsic features and emergent capabilities of the holobiont, with mechanisms that can provide rapid, sensitive, nuanced and persistent performance changes. \nStudies by Froebe et al. in 1990 indicating that essential mating pheromones, including androstenols, required activation by skin-associated microbial glucuronidases and sulfatases. In the absence of microbial populations in the skin, no detectable aromatic pheromone was released, as the pro-pheromone remained water-soluble and non-volatile. This effectively meant that the microbes in the skin were essential to produce a mating signal.\n\nSubsequent re-articulation describing the hologenome theory by Rosenberg and Zilber-Rosenberg, published 13 years after Jefferson's definition of the theory, was based on their observations of corals, and the coral probiotic hypothesis.\n\nCoral reefs are the largest structures created by living organisms, and contain abundant and highly complex microbial communities. A coral \"head\" is a colony of genetically identical polyps, which secrete an exoskeleton near the base. Depending on the species, the exoskeleton may be hard, based on calcium carbonate, or soft and proteinaceous. Over many generations, the colony creates a large skeleton that is characteristic of the species. Diverse forms of life take up residence in a coral colony, including photosynthetic algae such as \"Symbiodinium\", as well as a wide range of bacteria including nitrogen fixers, and chitin decomposers, all of which form an important part of coral nutrition. The association between coral and its microbiota is species dependent, and different bacterial populations are found in mucus, skeleton and tissue from the same coral fragment.\n\nOver the past several decades, major declines in coral populations have occurred. Climate change, water pollution and overfishing are three stress factors that have been described as leading to disease susceptibility. Over twenty different coral diseases have been described, but of these, only a handful have had their causative agents isolated and characterized.\n\nCoral bleaching is the most serious of these diseases. In the Mediterranean Sea, the bleaching of \"Oculina patagonica\" was first described in 1994 and, through a rigorous application of Koch's Postulates, determined to be due to infection by \"Vibrio shiloi\". From 1994 to 2002, bacterial bleaching of \"O. patagonica\" occurred every summer in the eastern Mediterranean. Surprisingly, however, after 2003, \"O. patagonica\" in the eastern Mediterranean has been resistant to \"V. shiloi\" infection, although other diseases still cause bleaching.\n\nThe surprise stems from the knowledge that corals are long lived, with lifespans on the order of decades, and do not have adaptive immune systems. Their innate immune systems do not produce antibodies, and they should seemingly not be able to respond to new challenges except over evolutionary time scales. Yet multiple researchers have documented variations in bleaching susceptibility that may be termed 'experience-mediated tolerance'. The puzzle of how corals managed to acquire resistance to a specific pathogen led Eugene Rosenberg and Ilana Zilber-Rosenberg to propose the Coral Probiotic Hypothesis. This hypothesis proposes that a dynamic relationship exists between corals and their symbiotic microbial communities. Beneficial mutations can arise and spread among the symbiotic microbes much faster than in the host corals. By altering its microbial composition, the \"holobiont\" can adapt to changing environmental conditions far more rapidly than by genetic mutation and selection in the host species alone.\n\nExtrapolating the coral probiotic hypothesis to other organisms, including higher plants and animals, led to the Rosenberg's support for and publications around the hologenome theory of evolution.\n\nThe framework of the hologenome theory of evolution is as follows (condensed from Rosenberg \"et al.\", 2007):\n\nSome authors supplement the above principles with an additional one. If a given holobiont is to be considered a unit of natural selection: \n\nTen principles of holobionts and hologenomes were presented in PLOS Biology: \n\n\nMany case studies clearly demonstrate the importance of an organism's associated microbiota to its existence. (For example, see the numerous case studies in the Microbiome article.) However, horizontal \"versus\" vertical transmission of endosymbionts must be distinguished. Endosymbionts whose transmission is predominantly vertical may be considered as contributing to the heritable genetic variation present in a host species.\n\nIn the case of colonial organisms such as corals, the microbial associations of the colony persist even though individual members of the colony, reproducing asexually, live and die. Corals also have a sexual mode of reproduction, resulting in planktonic larva; it is less clear whether microbial associations persist through this stage of growth. Also, the bacterial community of a colony may change with the seasons.\n\nMany insects maintain heritable obligate symbiosis relationships with bacterial partners. For example, normal development of female wasps of the species \"Asobara tabida\" is dependent on \"Wolbachia\" infection. If \"cured\" of the infection, their ovaries degenerate. Transmission of the infection is vertical through the egg cytoplasm.\n\nIn contrast, many obligate symbiosis relationships have been described in the literature where transmission of the symbionts is via horizontal transfer. A well-studied example is the nocturnally feeding squid \"Euprymna scolopes\", which camouflages its outline against the moonlit ocean surface by emitting light from its underside with the aid of the symbiotic bacterium \"Vibrio fischeri\". The Rosenbergs cite this example within the context of the hologenome theory of evolution. Squid and bacterium maintain a highly co-evolved relationship. The newly hatched squid collects its bacteria from the sea water, and lateral transfer of symbionts between hosts permits faster transfer of beneficial mutations within a host species than are possible with mutations within the host genome.\n\nAnother traditional distinction between endosymbionts has been between primary and secondary symbionts. Primary endosymbionts reside in specialized host cells that may be organized into larger, organ-like structures (in insects, the bacteriome). Associations between hosts and primary endosymbionts are usually ancient, with an estimated age of tens to hundreds of millions of years. According to endosymbiotic theory, extreme cases of primary endosymbionts include mitochondria, plastids (including chloroplasts), and possibly other organelles of eukaryotic cells. Primary endosymbionts are usually transmitted exclusively vertically, and the relationship is always mutualistic and generally obligate for both partners. Primary endosymbiosis is surprisingly common. An estimated 15% of insect species, for example, harbor this type of endosymbiont. In contrast, secondary endosymbiosis is often facultative, at least from the host point of view, and the associations are less ancient. Secondary endosymbionts do not reside in specialized host tissues, but may dwell in the body cavity dispersed in fat, muscle, or nervous tissue, or may grow within the gut. Transmission may be via vertical, horizontal, or both vertical and horizontal transfer. The relationship between host and secondary endosymbiont is not necessarily beneficial to the host; indeed, the relationship may be parasitic.\n\nThe distinction between vertical and horizontal transfer, and between primary and secondary endosymbiosis is not absolute, but follows a continuum, and may be subject to environmental influences. For example, in the stink bug \"Nezara viridula\", the vertical transmission rate of symbionts, which females provide to offspring by smearing the eggs with gastric caeca, was 100% at 20 °C, but decreased to 8% at 30 °C. Likewise, in aphids, the vertical transmission of bacteriocytes containing the primary endosymbiont \"Buchnera\" is drastically reduced at high temperature. In like manner, the distinction between commensal, mutualistic, and parasitic relationships is also not absolute. An example is the relationship between legumes and rhizobial species: N uptake is energetically more costly than the uptake of fixed nitrogen from the soil, so soil N is preferred if not limiting. During the early stages of nodule formation, the plant-rhizobial relationship actually resembles a pathogenesis more than it does a mutualistic association.\n\nLamarckism, the concept that an organism can pass on characteristics that it acquired during its lifetime to its offspring (also known as inheritance of acquired characteristics or soft inheritance) incorporated two common ideas of its time:\n\n\nAlthough Lamarckian theory was rejected by the neo-Darwinism of the modern evolutionary synthesis in which evolution occurs through random variations being subject to natural selection, the hologenome theory has aspects that harken back to Lamarckian concepts. In addition to the traditionally recognized modes of variation (\"i.e.\" sexual recombination, chromosomal rearrangement, mutation), the holobiont allows for two additional mechanisms of variation that are specific to the hologenome theory: (1) changes in the relative population of existing microorganisms (\"i.e.\" amplification and reduction) and (2) acquisition of novel strains from the environment, which may be passed on to offspring.\n\nChanges in the relative population of existing microorganisms corresponds to Lamarckian \"use and disuse\", while the ability to acquire novel strains from the environment, which may be passed on to offspring, corresponds to Lamarckian \"inheritance of acquired traits\". The hologenome theory, therefore, is said by its proponents to incorporate Lamarckian aspects within a Darwinian framework.\n\nThe pea aphid \"Acyrthosiphon pisum\" maintains an obligate symbiotic relationship with the bacterium \"Buchnera aphidicola\", which is transmitted maternally to the embryos that develop within the mother's ovarioles. Pea aphids live on sap, which is rich in sugars but deficient in amino acids. They rely on their \"Buchnera\" endosymbiotic population for essential amino acids, supplying in exchange nutrients as well as a protected intracellular environment that allows \"Buchnera\" to grow and reproduce. The relationship is actually more complicated than mutual nutrition; some strains of \"Buchnera\" increases host thermotolerance, while other strains do not. Both strains are present in field populations, suggesting that under some conditions, increased heat tolerance is advantageous to the host, while under other conditions, decreased heat tolerance but increased cold tolerance may be advantageous. One can consider the variant \"Buchnera\" genomes as alleles for the larger hologenome. The association between \"Buchnera\" and aphids began about 200 million years ago, with host and symbiont co-evolving since that time; in particular, it has been discovered that genome size in various \"Buchnera\" species has become extremely reduced, in some cases down to 450 kb, which is far smaller even than the 580 kb genome of \"Mycoplasma genitalium\".\n\nDevelopment of mating preferences, \"i.e.\" sexual selection, is considered to be an early event in speciation. In 1989, Dodd reported mating preferences in \"Drosophila\" that were induced by diet. It has recently been demonstrated that when otherwise identical populations of \"Drosophila\" were switched in diet between molasses medium and starch medium, that the \"molasses flies\" preferred to mate with other molasses flies, while the \"starch flies\" preferred to mate with other starch flies. This mating preference appeared after only one generation and was maintained for at least 37 generations. The origin of these differences were changes in the flies' populations of a particular bacterial symbiont, \"Lactobacillus plantarum\". Antibiotic treatment abolished the induced mating preferences. It appears that the symbiotic bacteria changed the levels of cuticular hydrocarbon sex pheromones.\n\nZilber-Rosenberg and Rosenberg (2008) have tabulated many of the ways in which symbionts are transmitted and their contributions to the fitness of the holobiont, beginning with mitochondria found in all eukaryotes, chloroplast in plants, and then various associations described in specific systems. The microbial contributions to host fitness included provision of specific amino acids, growth at high temperatures, provision of nutritional needs from cellulose, nitrogen metabolism, recognition signals, more efficient food utilization, protection of eggs and embryos against metabolism, camouflage against predators, photosynthesis, breakdown of complex polymers, stimulation of the immune system, angiogenesis, vitamin synthesis, fiber breakdown, fat storage, supply of minerals from the soil, supply of organics, acceleration of mineralization, carbon cycling, and salt tolerance.\n\nThe hologenome theory is debated. A major criticism by Ainsworth \"et al.\" has been their claim that \"V. shiloi\" was misidentified as the causative agent of coral bleaching, and that its presence in bleached \"O. patagonica\" was simply that of opportunistic colonization.\n\nIf this is true, the original observation that led to Rosenberg's later articulation of the theory would be invalid. On the other hand, Ainsworth \"et al.\" performed their samplings in 2005, two years after the Rosenberg group discovered \"O. patagonica\" no longer to be susceptible to \"V. shiloi\" infection; therefore their finding that bacteria are not the primary cause of present-day bleaching in Mediterranean coral \"O. patagonica\" should not be considered surprising. The rigorous satisfaction of Koch's postulates, as employed in Kushmaro \"et al.\" (1997), is generally accepted as providing a definitive identification of infectious disease agents.\n\nBaird \"et al.\" (2009) have questioned basic assumptions made by Reshef \"et al.\" (2006) in presuming that (1) coral generation times are too slow to adjust to novel stresses over the observed time scales, and that (2) the scale of dispersal of coral larvae is too large to allow for adaptation to local environments. They may simply have underestimated the potential rapidity of conventional means of natural selection. In cases of severe stress, multiple cases have been documented of ecologically significant evolutionary change occurring over a handful of generations. Novel adaptive mechanisms such as switching symbionts might not be necessary for corals to adjust to rapid climate change or novel stressors.\nOrganisms in symbiotic relationships evolve to accommodate each other, and the symbiotic relationship increases the overall fitness of the participant species. Although the hologenome theory is still being debated, it has gained a significant degree of popularity within the scientific community as a way of explaining rapid adaptive changes that are difficult to accommodate within a traditional Darwinian framework.\n\nDefinitions and uses of the words holobiont and hologenome also differ between proponents and skeptics, and the misuse of the terms has led to confusions over what comprises evidence related to the hologenome. Ongoing discourse is attempting to clear this confusion. Theis et al. clarify that \"critiquing the hologenome concept is not synonymous with critiquing coevolution, and arguing that an entity is not a primary unit of selection dismisses the fact that the hologenome concept has always embraced multilevel selection.\"\n\nFor instance, Chandler and Turelli (2014) criticize the conclusions of Brucker and Bordenstein (2013), noting that their observations are also consistent with an alternative explanation. Brucker and Bordenstein (2014) responded to these criticisms, claiming they were unfounded because of factual inaccuracies and altered arguments and definitions that were not advanced by Brucker and Bordenstein (2013).\n\nRecently, Forest L Rohwer and colleagues developed a novel statistical test to examine the potential for the hologenome theory of evolution in coral species. They found that coral species do not inherit microbial communities, and are instead colonized by a core group of microbes that associate with a diversity of species. The authors conclude: \"Identification of these two symbiont communities supports the holobiont model and calls into question the hologenome theory of evolution.\" However, other studies in coral adhere to the original and pluralistic definitions of holobionts and hologenomes. David Bourne, Kathleen Morrow and Nicole Webster clarify that \"The combined genomes of this coral holobiont form a coral hologenome, and genomic interactions within the hologenome ultimately define the coral phenotype.\"\n"}
{"id": "13585", "url": "https://en.wikipedia.org/wiki?curid=13585", "title": "Hugo de Garis", "text": "Hugo de Garis\n\nHugo de Garis (born 1947, Sydney, Australia) is a retired researcher in the sub-field of artificial intelligence (AI) known as evolvable hardware. He became known in the 1990s for his research on the use of genetic algorithms to evolve artificial neural networks using three-dimensional cellular automata inside field programmable gate arrays. He claimed that this approach would enable the creation of what he terms \"artificial brains\" which would quickly surpass human levels of intelligence.\n\nHe has more recently been noted for his belief that a major war between the supporters and opponents of intelligent machines, resulting in billions of deaths, is almost inevitable before the end of the 21st century. He suggests AIs may simply eliminate the human race, and humans would be powerless to stop them because of technological singularity. This prediction has attracted debate and criticism from the AI research community, and some of its more notable members, such as Kevin Warwick, Bill Joy, Ken MacLeod, Ray Kurzweil, and Hans Moravec, have voiced their opinions on whether or not this future is likely.\n\nDe Garis originally studied theoretical physics, but he abandoned this field in favour of artificial intelligence. In 1992 he received his PhD from Université Libre de Bruxelles, Belgium. He worked as a researcher at ATR (Advanced Telecommunications Research institute international, ), Japan from 1994–2000, a researcher at Starlab, Brussels from 2000–2001, and associate professor of computer science at Utah State University from 2001–2006. Until his retirement in late 2010 he was a professor at Xiamen University, where he taught theoretical physics and computer science, and ran the Artificial Brain Lab.\n\nFrom 1993 to 2000 de Garis participated in a research project at ATR's Human Information Processing Research Laboratories (ATR-HIP) which aimed to create a billion-neuron artificial brain by the year 2001. The project was known as \"cellular automata machine brain,\" or \"CAM-Brain.\" During this 8-year span he and his fellow researchers published a series of papers in which they discussed the use of genetic algorithms to evolve neural structures inside 3D cellular automata. They argued that existing neural models had failed to produce intelligent behaviour because they were too small, and that in order to create \"artificial brains\" it was necessary to manually assemble tens of thousands of evolved neural modules together, with the billion neuron \"CAM-Brain\" requiring around 10 million modules; this idea was rejected by Igor Aleksander, who said \"The point is that these puzzles are not puzzles because our neural models are not large enough.\"\n\nThough it was initially envisaged that these cellular automata would run on special computers, such as MIT's \"Cellular Automata Machine-8\" (CAM-8), by 1996 it was realised that the model originally proposed, which required cellular automata with thousands of states, was too complex to be realised in hardware. The design was considerably simplified, and in 1997 the \"collect and distribute 1 bit\" (\"CoDi-1Bit\") model was published, and work began on a hardware implementation using Xilinx XC6264 FPGAs. This was to be known as the \"CAM Brain Machine\" (CBM).\n\nThe researchers evolved cellular automata for several tasks (using software simulation, not hardware):\n\n\nUltimately the project failed to produce a functional robot control system, and ATR terminated it along with the closure of ATR-HIP in February 2001.\n\nThe original aim of de Garis' work was to establish the field of \"brain building\" (a term of his invention) and to \"create a trillion dollar industry within 20 years\". Throughout the 90s his papers claimed that by 2001 the ATR \"Robokoneko\" (translation: kitten robot) project would develop a billion-neuron \"cellular automata machine brain\" (CAM-brain), with \"computational power equivalent to 10,000 pentiums\" that could simulate the brain of a real cat. de Garis received a US$0.4 million \"fat brain building grant\" to develop this. The first \"CAM-brain\" was delivered to ATR in 1999. After receiving a further US$1 million grant at Starlab de Garis failed to deliver a working \"brain\" before Starlab's bankruptcy. At USU de Garis announced he was establishing a \"brain builder\" group to create a second generation \"CAM-brain\".\n\nde Garis published his last \"CAM-Brain\" research paper in 2002. He still works on evolvable hardware. Using a Celoxica FPGA board he says he can create up to 50,000 neural network modules for less than $3000.\n\nSince 2002 he has co-authored several papers on evolutionary algorithms.\n\nHe believes that topological quantum computing is about to revolutionize computer science, and hopes that his teaching will help his students to understand its principles.\n\nIn 2008 de Garis received a 3 million Chinese yuan grant (around $436,000) to build an artificial brain for China (the \"China-Brain Project\"), as part of the \"Brain Builder Group\" at Wuhan University.\n\nHugo de Garis retired in 2010. Before that he was director of the artificial brains lab at Xiamen University in China. In 2013 he was studying Maths and Physics at PhD level and over the next 20 years plans to publish 500 graduate level free lecture videos. This is called \"degarisMPC\" and some lectures are already available.\n\nde Garis's original work on \"CAM-brain\" machines was part of an 8-year research project, from 1993 to 2000, at the ATR Human Information Processing Research Laboratories (ATR-HIP) in Kyoto Prefecture, Japan. de Garis left in 2000, and ATR-HIP was closed on 28 February 2001. de Garis then moved to Starlab in Brussels, where he received a million dollars in funding from the government of Belgium (\"over a third of the Brussels government's total budget for scientific research\", according to de Garis). Starlab went bankrupt in June 2001. A few months later de Garis was employed as an associate professor at the computer science department of Utah State University. In May 2006 he became a professor at Wuhan University's international school of software, teaching graduate level pure mathematics, theoretical physics and computer science.\n\nSince June 2006 he has been a member of the advisory board of Novamente, a commercial company which aims to create artificial general intelligence.\n\nde Garis believes that a major war before the end of the 21st century, resulting in billions of deaths, is almost inevitable. Intelligent machines (or 'artilects', a shortened form of 'artificial intellects') will be far more intelligent than humans and will threaten to attain world domination, resulting in a conflict between 'Cosmists', who support the artilects, and 'Terrans', who oppose them (both of these are terms of his invention). He describes this conflict as a 'gigadeath' war, reinforcing the point that billions of people will be killed. This scenario has been criticised by other AI researchers, including Chris Malcolm, who described it as \"entertaining science fiction horror stories which happen to have caught the attention of the popular media\". Kevin Warwick called it a \"hellish nightmare, as portrayed in films such as the Terminator\".\nIn 2005 de Garis published a book describing his views on this topic entitled \"The Artilect War: Cosmists vs. Terrans: A Bitter Controversy Concerning Whether Humanity Should Build Godlike Massively Intelligent Machines\".\n\nCosmism is a moral philosophy that favours building or growing strong artificial intelligence and ultimately leaving Earth to the Terrans, who oppose this path for humanity. The first half of the book describes technologies which he believes will make it possible for computers to be billions or trillions of times more intelligent than humans. He predicts that as artificial intelligence improves and becomes progressively more human-like, differing views will begin to emerge regarding how far such research should be allowed to proceed. Cosmists will foresee the massive, truly astronomical potential of substrate-independent cognition, and will therefore advocate unlimited growth in the designated fields, in the hopes that \"super intelligent\" machines might one day colonise the universe. It is this \"cosmic\" view of history, in which the fate of one single species, on one single planet, is seen as insignificant next to the fate of the known universe, that gives the Cosmists their name.\nHugo identifies with that group and noted that it \"would be a cosmic tragedy if humanity freezes evolution at the puny human level\".\n\nTerrans on the other hand, will have a more \"terrestrial\" Earth-centred view, in which the fate of the Earth and its species (like humanity) are seen as being all-important. To Terrans, a future without humans is to be avoided at all costs, as it would represent \"the\" worst-case scenario. As such, Terrans will find themselves unable to ignore the possibility that super intelligent machines might one day cause the destruction of the human race—being very immensely intelligent and so cosmically inclined, these artilect machines may have no more moral or ethical difficulty in exterminating humanity than humans do in using medicines to cure diseases. So, Terrans will see themselves as living during the closing of a window of opportunity, to disable future artilects before they are built, after which humans will no longer have a say in the affairs of intelligent machines.\n\nIt is these two extreme ideologies which de Garis believes may herald a new world war, wherein one group with a 'grand plan' (the Cosmists) will be rabidly opposed by another which feels itself to be under deadly threat from that plan (the Terrans). The factions, he predicts, may eventually war to the death because of this, as the Terrans will come to view the Cosmists as \"arch-monsters\" when they begin seriously discussing acceptable risks, and the probabilities of large percentages of Earth-based life going extinct. In response to this, the Cosmists will come to view the Terrans as being reactionary extremists, and will stop treating them and their ideas seriously, further aggravating the situation, possibly beyond reconciliation.\n\nThroughout his book, de Garis states that he is ambivalent about which viewpoint he ultimately supports, and attempts to make convincing cases for both sides. He elaborates towards the end of the book that the more he thinks about it, the more he feels like a Cosmist, because he feels that despite the horrible possibility that humanity might ultimately be destroyed, perhaps inadvertently or at least indifferently, by the artilects, he cannot ignore the fact that the human species is just another link in the evolutionary chain, and must become extinct in their current form anyway, whereas the artilects could very well be the \"next\" link in that chain and therefore would be excellent candidates to carry the torch of science and exploration forward into the rest of the universe.\n\nHe relates a morally isomorphic scenario in which extraterrestrial intelligences visit the earth three billion years ago and discover two domains of life living there, one domain which is \"older\" but \"simpler\" and contemporarily dominant, but which upon closer study appears to be incapable of much further evolutionary development; and one \"younger\" domain which is struggling to survive, but which upon further study displays the potential to evolve into all the varieties of life existing on the Earth today, including humanity, and then queries the reader as to whether they would feel ethically compelled to destroy the dominant domain of life to ensure the survival of the younger one, or to destroy the younger one in order to ensure the survival of the older and more populous domain which was \"there first\". He states that he believes that, like himself, most of the public would feel torn or at least ambivalent about the outcome of artilects at first, but that as the technology advances, the issue would be forced and most would feel compelled to choose a side, and that as such the public consciousness of the coming issue should be raised now so that society can \"choose\", hopefully before the factions becomes irreconcilably polarised, which outcome it prefers.\n\nDe Garis also predicts a third group that will emerge between the two. He refers to this third party as Cyborgians or \"Cyborgs\", because they will not be opposed to artilects as such, but desire to become artilects themselves by adding components to their own human brains, rather than falling into obsolescence. They will seek to become artilects by gradually merging themselves with machines and think that the dichotomy between the Cosmists and Terrans can be avoided because all human beings would become artilects.\nThe transhumanist movement are usually identified as Cyborgians.\n\nDe Garis' concept of the Cyborgians might have stemmed from a conversation with Kevin Warwick: in 2000, de Garis noted, \"Just out of curiosity, I asked Kevin Warwick whether he was a Terran or a Cosmist. He said he was against the idea of artilects being built (i.e., he is Terran). I was surprised, and felt a shiver go up my spine. That moment reminded me of a biography of Lenin that I had read in my 20s in which the Bolsheviks and the Mensheviks first started debating the future government of Russia. What began as an intellectual difference ended up as a Russian civil war after 1917 between the white and the red Russians\".\n\n\n\n\n\n"}
{"id": "2026869", "url": "https://en.wikipedia.org/wiki?curid=2026869", "title": "Incoherent scatter", "text": "Incoherent scatter\n\nIncoherent scattering is a type of scattering phenomenon in physics. The term is most commonly used when referring to the scattering of an electromagnetic wave (usually light or radio frequency) by random fluctuations in a gas of particles (most often electrons).\n\nThe most well known practical application is known as incoherent scatter radar theory, a ground-based technique for studying the Earth's ionosphere first proposed by Professor Bill Gordon in 1958. A radar beam scattering off electrons in the ionospheric plasma creates an incoherent scatter return. The distribution function of the ionospheric electrons is modified by the much slower and more massive positive ions — electron density fluctuations relate to ion temperature, mass distribution, and motion. The incoherent scatter signal allows measurement of electron density, ion temperature and electron temperatures, ion composition and plasma velocity.\n\n\n"}
{"id": "35955099", "url": "https://en.wikipedia.org/wiki?curid=35955099", "title": "Insecurity Insight", "text": "Insecurity Insight\n\nEstablished in 2008 by Nathan Taback, Christina Willie and Robin Coupland, Insecurity Insight is a Geneva-based non-profit organisation that has developed the ”Taback-Coupland model of armed violence”, a model that is used to generate data on the impact of armed violence and insecurity on people's lives and wellbeing with a view to generating preventive policies.\n\nWhile working in conflict zones as a general surgeon treating war wounds (1987-1995), Coupland developed a theory about armed violence and its effects. The theory states that the effects of use of weapons on health have certain identifiable determinants:\n\n\nWhile teaching a course at the Harvard School of Public Health (2003), Coupland began collaborating with Taback, a statistician with an interest in global health. Their collaboration involved refining Coupland's original theory and developing the means to make quantifiable the various determinants for a given outcome of armed violence. The result has come to be known as the Taback-Coupland model of armed violence.\n\nCoupland and Taback later met Wille while she was working for the Small Arms Survey (2006), and applied the model to map the nature and extent of armed violence on a geographical basis. Willie worked on refining the model further and the structure of the corresponding databases to allow application of the model to analyze different kinds of violent events. Later Taback, Willie and Coupland founded Insecurity Insight (2008).\n\nThe core activity of Insecurity Insight is to promote, use and make available to others the Taback-Coupland model. Because the model can be applied to any act of violence using any weapons resulting in any effect, the model has been applied to subjects such as explosive force in populated areas, attacks on journalists and sexual violence in the Democratic Republic of Congo and in Zimbabwe and attacks on health care workers and facilities.\n\nA major ongoing project is the building of the “Security in Numbers Database.” This is a collaborative effort initiated by Insecurity Insight involving a number of major international humanitarian NGOs. By taking part in this project, these organisations share their security incidents with Insecurity Insight on a confidential basis; this permits analysis of these incidents by the model and therefore comparison of any single agency’s security issues with what is known globally about aid worker security. This has already generated findings.\n\nBased on a standard public health model, the Taback-Coupland model takes as a starting point that use of weapons has a health impact and that this health impact is amenable to a preventive approach as any other public health issue. By generating data about threats to and vulnerabilities of people’s lives and well-being through violence and weapons, appropriate preventive policies can be proposed.\n\nThe model is constructed around four “risk factors” or determinants for any given effects of armed violence. The four determinants are the nature of the weapon, the number of weapons used (or potentially used), the way the weapon is used (the psychological aspect of the violence), and the victims’ vulnerability. An example is the lethality of attacks with firearms. This will be determined by:\n\n\nThis example also shows the important interaction at a psychological level between how the weapon is used, the other determinants and perceptions of the potential outcome. (Whether the trigger is pulled to cause a fatal injury is influenced, for example, by whether the user of the firearm believes he or she can hit the victim(s) with it, whether there are others similarly armed, how easy it is to hit the victim(s) in that context, and whether the victim(s) have already been hit.)\n\nThe method uses written reports of real incidents of armed violence as the source for the necessary values. Information from a report on an individual incident is entered into a spreadsheet specially designed to capture and process information about the people committing violence and about the victim(s) or potential victim(s). In this way, qualitative sources of data about violence i.e., reports, can be fed into a specially prepared relational database; this uses binaries and surrogates to give a numeric value to determinants such as intent and vulnerability. (For example, for vulnerability: Were the people herded into a confined space before being shot? Yes / No) A “report” can be a very short text as indicated by the following sentence: “Two masked men entered Hospital X last night and shot dead a sleeping patient.” In terms of entry into the database, this can be read as: Two masked men [number of people armed] entered Hospital X [where] last night [when] and shot [weapon (firearms)] dead [outcome] a [outcome] sleeping patient [vulnerability]. (\"Shot dead\" is also a comment on intent.)\n\nWhen the database has been fed with a sufficient number of reports of real events about a certain issue, this permits an “evidence-based” dialogue about the perpetrators and their intents and about the victims and their vulnerabilities. Taback and Coupland tested the model using global news reports about violence in general and about attacks on journalists. They went further and termed such an approach “the science of human security.”\n\nAll important limitations of the data gathering method based on the model relate to the completeness and accuracy of the qualitative reports which are fed into the model. However, as Taback and Coupland have pointed out, reports, especially media reports may be the only information available about a given context or form of violence in multiple contexts. It is nevertheless possible to generate policy-relevant quantitative data from such sources. Further, they point out that while there are international surveillance systems for monitoring infectious disease outbreaks, no such system exists (yet) to evaluate on a regular basis the effects of armed violence on peoples’ lives. With that in mind, the method developed by Taback and Coupland is a useful contribution to international efforts to enhance human security, including in disarmament.\n\nThe model has generated original and policy-relevant data in relation to subjects such as explosive force in populated areas, and sexual violence in the Democratic Republic of Congo and in Zimbabwe. A major study involving analysis of 655 attacks on health care workers and facilities was commissioned by the International Committee of the Red Cross (ICRC); its publication marked the launch of the ICRC’s “Health Care in Danger” project. Another major ongoing project is the “Security in Numbers Database.”\n\n\n"}
{"id": "53903009", "url": "https://en.wikipedia.org/wiki?curid=53903009", "title": "Laboratoire atmosphères, milieux, observations spatiales", "text": "Laboratoire atmosphères, milieux, observations spatiales\n\nThe Laboratoire atmosphères, milieux, observations spatiales (LATMOS) is a French research laboratory specialized in the study of the physical and chemical processes of the Earth's atmosphere, the study of planets and small bodies of the solar system (atmospheres, surfaces, sub-surfaces) as well as the physics of the heliosphere, the exosphere of the planets and the plasmas of the solar system. It employs 230 people.\n\n"}
{"id": "8437923", "url": "https://en.wikipedia.org/wiki?curid=8437923", "title": "Liquid-to-gas ratio", "text": "Liquid-to-gas ratio\n\nAn important parameter in wet scrubbing systems is the rate of liquid flow. It is common in wet scrubber terminology to express the liquid flow as a function of the gas flow rate that is being treated. This is commonly called the liquid-to-gas ratio (L/G ratio) and uses the units of gallons per 1,000 actual cubic feet or litres per cubic metre (L/m). \n\nExpressing the amount of liquid used as a ratio enables systems of different sizes to be readily compared.\nFor particulate removal, the liquid-to-gas ratio is a function of the mechanical design of the system; while for gas absorption this ratio gives an indication of the difficulty of removing a pollutant. Most wet scrubbers used for particulate control operate with liquid-to-gas ratios in the range of 4 to 20 gallons per 1,000 actual cubic foot (0.5 to 3 litres per actual cubic metre). \n\nDepending on scrubber design, a minimum volume of liquid is required to \"wet\" the scrubber internals and create sufficient collection targets. After a certain optimum point, adding excess liquid to a particulate wet scrubber does not increase efficiency and in fact, could be counter-productive by causing excessive pressure loss. Liquid-to-gas ratios for gas absorption are often higher, in the range of 20 to 40 gallons per 1,000 actual cubic foot (3 to 6 litres per actual cubic metre).\n\nL/G ratio illustrates a number of points about the choice of wet scrubbers used for gas absorption. For example, because flue-gas desulfurization systems must deal with heavy particulate loadings, open, simple designs (such as venturi, spray chamber and moving bed) are used.\nAlso, the liquid-to-gas ratio for the absorption process is higher than for particle removal and gas velocities are kept low to enhance the absorption process.\n\nSolubility is a very important factor affecting the amount of a pollutant that can be absorbed. Solubility governs the amount of liquid required (liquid-to-gas ratio) and the necessary contact time. More soluble gases require less liquid. Also, more soluble gases will be absorbed faster.\n\n\n"}
{"id": "57152834", "url": "https://en.wikipedia.org/wiki?curid=57152834", "title": "List of Ascochyta species", "text": "List of Ascochyta species\n\nThis is a list of the fungus species in the genus \"Ascochyta\". Many are plant pathogens. \"Ascochyta\" is an anamorph for many species placed in \"Didymella\" (the teleomorph) but both names refer to the same organism. there are 1,184 species included. There is also one subgenus: \"Ascochyta\" subgen. \"Ascochyta\" .\n"}
{"id": "3406753", "url": "https://en.wikipedia.org/wiki?curid=3406753", "title": "List of Lepidoptera that feed on buckthorns", "text": "List of Lepidoptera that feed on buckthorns\n\nBuckthorns (\"Rhamnus\" species) are used as food plants by the larvae of a number of Lepidoptera species, including:\n\nSpecies which feed exclusively on \"Rhamnus\"\n\n\nSpecies which feed on \"Rhamnus\" and other plants\n\n\n"}
{"id": "4432878", "url": "https://en.wikipedia.org/wiki?curid=4432878", "title": "List of Norwegian fjords", "text": "List of Norwegian fjords\n\nThis list of Norwegian fjords shows many of the fjords in Norway. In total, there are about 1,190 fjords in Norway and the Svalbard islands. The sortable list includes the lengths and locations of those fjords.\n\n"}
{"id": "448990", "url": "https://en.wikipedia.org/wiki?curid=448990", "title": "List of Pinus species", "text": "List of Pinus species\n\nPinus, the pines, is a genus of approximately 111 extant tree and shrub species. The genus is currently split into two subgenera: subgenus Pinus (hard pines), and subgenus Strobus (soft pines). Each of the subgenera have several sections within based on chloroplast DNA sequencing. Older classifications split the genus into three subgenera – subgenus \"Pinus\", subgenus \"Strobus\", and subgenus \"Ducampopinus\" (pinyon, bristlecone and lacebark pines) – based on cone, seed and leaf characteristics. DNA phylogeny has shown that species formerly in subgenus \"Ducampopinus\" are members of subgenus \"Strobus\", so \"Ducampopinus\" is no longer used.\n\nThe species of subgenus \"Ducampopinus\" were regarded as intermediate between the other two subgenera. In the modern classification, they are placed into subgenus \"Strobus\", yet they did not fit entirely well in either so they were classified in a third subgenus. In 1888 the Californian botanist John Gill Lemmon placed them in subgenus \"Pinus\". In general, this classification emphasized cone, cone scale, seed, and leaf fascicle and sheath morphology, and species in each subsection were usually recognizable by their general appearance. Pines with one fibrovascular bundle per leaf, (the former subgenera \"Strobus\" and \"Ducampopinus\") were known as \"haploxylon pines\", while pines with two fibrovascular bundles per leaf, (subgenus \"Pinus\") were called \"diploxylon pines\". Diploxylon pines tend to have harder timber and a larger amount of resin than the haploxylon pines.\n\nSeveral features are used to distinguish the subgenera, sections, and subsections of pines: the number of leaves (needles) per fascicle, whether the fascicle sheaths are deciduous or persistent, the number of fibrovascular bundles per needle, the position of the resin ducts in the needles, the presence or shape of the seed wings, and the position of the umbo and presence of a prickle on the scales of the seed cones.\n\nSubgenus \"Pinus\" includes the yellow and hard pines. Pines in this subgenus have one to five needles per fascicle and two fibrovascular bundles per needle, and the fascicle sheaths are persistent, except in \"P. leiophylla\" and \"P. lumholtzii\". Cone scales are thicker and more rigid than those of subgenus \"Strobus\", and cones either open soon after they mature or are serotinous.\n\nSection \"Pinus\" has two or three needles per fascicle. Cones of all species have thick scales, and all except those of \"P. pinea\" open at maturity. Species in this section are native to Europe, Asia, and the Mediterranean, except for \"P. resinosa\" in northeastern North America and \"P. tropicalis\" in western Cuba.\n\nAll but two species in Subsection \"Pinus\" are native to Eurasia.\n\nSubsection \"Pinaster\" contains species native to the Mediterranean, as well as \"P. roxburghii\" from the Himalayas. The scales of its cones lack spines. It is named after \"P. pinaster\".\n\nSection \"Trifoliae\" (American hard pines), despite its name (which means \"three-leaved\"), has two to five needles per fascicle, or rarely eight. The cones of most species open at maturity, but a few are serotinous. All but two American hard pines belong to this section.\n\nSubsection \"Australes\" is native to North and Central America and islands in the Caribbean.\nThe closed-cone (serotinous) species of California and Baja California, \"P. attenuata\", \"P. muricata\", and \"P. radiata\", are sometimes placed in a separate subsection, \"Attenuatae\".\n\nSubsection \"Contortae\" is native to North America and Mexico.\n\nSubsection \"Ponderosae\" is native to Central America, Mexico, the western United States, and southwestern Canada.\n\nSubgenus \"Strobus\" includes the white and soft pines. Pines in this subgenus have one to five needles per fascicle and one fibrovascular bundle per needle, and the fascicle sheaths are deciduous, except in \"P. nelsonii\", where they are persistent. Cone scales are thinner and more flexible than those of subgenus \"Pinus\", except in some species like \"P. maximartinezii\", and cones usually open soon after they mature.\n\nSection \"Parrya\" has one to five needles per fascicle. The seeds either have articulate (jointed) wings or no wings at all. In all species except for \"P. nelsonii\", the fascicle sheaths curl back to form a rosette before falling away. The cones have thick scales and release the seeds at maturity. This section is native to the southwestern United States and Mexico.\n\nSubsection \"Balfourianae (bristlecone pines) is native to southwest United States.\n\nSubsection \"Cembroides (pinyons or piñons) is native to Mexico and the southwestern United States.\n\nSubsection \"Nelsonianae\" is native to northeastern Mexico. It consists of the single species with persistent fascicle sheaths.\n\nSection \"Quinquefoliae\" (white pines), as its name (which means \"five-leaved\") suggests, has five needles per fascicle except for \"P. krempfii\", which has two, and \"P. gerardiana\" and \"P. bungeana\", which have three. All species have cones with thin or thick scales that open at maturity or do not open at all; none are serotinous. Species in this section are found in Eurasia and North America, and one species, \"P. chiapensis\" reaches Guatemala.\n\nSubsection \"Gerardianae\" is native to East Asia. It has three or five needles per fascicle.\n\nSubsection \"Krempfianae\" is native to Vietnam. It has two needles per fascicle, and they are atypically flattened. The cone scales are thick and have no prickles.\n\nSubsection \"Strobus\" has five needles per fascicle and thin cone scales with no prickles. It is native to North and Central America, Europe, and Asia.\n\nSpecies which are not placed in a subgenus at this time.\n\n\n"}
{"id": "36815043", "url": "https://en.wikipedia.org/wiki?curid=36815043", "title": "List of Sites of Special Scientific Interest in Wrexham", "text": "List of Sites of Special Scientific Interest in Wrexham\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in the Wrexham Area of Search (AoS).\n"}
{"id": "22274781", "url": "https://en.wikipedia.org/wiki?curid=22274781", "title": "List of earthquakes in Australia", "text": "List of earthquakes in Australia\n\nThis is a list of significant earthquakes recorded within Australia and its territories. The currency used is the Australian dollar (A$) unless noted otherwise.\n\n\n\nSources\n\n"}
{"id": "9018416", "url": "https://en.wikipedia.org/wiki?curid=9018416", "title": "List of peanut diseases", "text": "List of peanut diseases\n\nThis article is a list of diseases of peanuts (\"Arachis hypogaea\").\n\n"}
{"id": "58483486", "url": "https://en.wikipedia.org/wiki?curid=58483486", "title": "List of pipeline accidents in the United States in 2000", "text": "List of pipeline accidents in the United States in 2000\n\nThe following is a list of pipeline accidents in the United States in 2000. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n"}
{"id": "20598064", "url": "https://en.wikipedia.org/wiki?curid=20598064", "title": "List of rivers discharging into the North Sea", "text": "List of rivers discharging into the North Sea\n\n\n\n\n\n\"From Foreness Point to Shoeburyness\"\n\n\"From Shoeburyness to St Abb's Head\"\n\n\nThe Aa is an 89 km long river in northern France. Its source is near the village Bourthes. It flows through the \"départements\" and cities of Pas-de-Calais: Saint-Omer and Nord: Gravelines.\n\nThe three main rivers in Germany are the Rhine () (main tributaries including the Neckar, the Main and the Moselle (\"Mosel\")); the Elbe (also drains into the North Sea); and, the Danube (\"Donau\").\n\nThe rivers in this section are sorted south-west (Netherlands) to east (Danish border).\n\n\n\n\n\n\n\n\nThe Rhine, together with its tributaries the Aare and the Thur drain about two thirds of the water into the North Sea.\n\n"}
{"id": "3954327", "url": "https://en.wikipedia.org/wiki?curid=3954327", "title": "Maritime territory", "text": "Maritime territory\n\nMaritime territory is a term used in international law to denote coastal waters which are not Territorial Waters though in immediate contact with the sea. In the case of Territorial Waters, the dominion of the adjacent state is subject to a limitation. Dominion over maritime territory is not subject to any limitation. Thus any strait through which the right of passage of foreign vessels can be forbidden, or bays so land-locked that they cannot be held to form part of any ocean-highway, are maritime territory.\n\n"}
{"id": "3523310", "url": "https://en.wikipedia.org/wiki?curid=3523310", "title": "Mizora", "text": "Mizora\n\nMizora is a feminist science fiction utopian novel by Mary E. Bradley Lane, first published in 1880–81, when it was serialized in the \"Cincinnati Commercial\" newspaper. It appeared in book form in 1890. \"Mizora\" is \"the first portrait of an all-female, self-sufficient society,\" and \"the first feminist technological Utopia.\"\n\nThe book's full title is Mizora: A Prophecy: A Mss. Found Among the Private Papers of Princess Vera Zarovitch: Being a True and Faithful Account of her Journey to the Interior of the Earth, with a Careful Description of the Country and its Inhabitants, their Customs, Manners, and Government.\n\n\"Mizora\" is one element in the wave of utopian and dystopian fiction that distinguished the later decades of the nineteenth century.\n\nThe novel is \"the second known feminist utopian novel written by a woman,\" after \"Man's Rights\" (1870) by Annie Denton Cridge. The concept of an all-female society dates back at least to the Amazons of ancient Greek mythology — though the Amazons still needed men for procreation. In Lane's \"Mizora\", reproduction is by parthenogenesis.\n\nThe book depicts an all-female \"utopia\" existing within the Earth. The Mizorans practice eugenics; all of them are blonde \"Aryans,\" who disdain people of darker skin. (In modern terms their society is deliberately racist. That term is perhaps applicable to the book as well.) In its ancient history, the land was ruled by a military general elected president (a version of Ulysses Grant). When the general ran for a third term (as Grant was urged to do in 1880), the society of Mizora descended into chaos. Eventually a new all-female social order arose in Mizora. The last men were \"eliminated\" — though it is not clear whether they were overtly killed or left to die out. It is said that men are more forgotten than hated.\n\nThe novel also refers to political repression in contemporary Russia, and the suppression of the Polish revolt of 1863. The first-person narrator, Vera Zarovitch, is a young wife and mother, but she has fallen foul of the Czarist regime and has been sentenced to exile in Siberia. She escapes northward into the Arctic, where her kayak is swept over a vast waterfall to Mizora. She spends fifteen years there, learning the ways of the culture; at the end of that time she longs to return to her husband and child, and teach her own society what she has learned.\n\nAs a utopian novel, the book devotes some time to the futuristic technology such as \"videophones.\" The Mizorans can make rain by discharging electricity into the air. Though Mizora has no domestic animals, its women eat chemically-prepared artificial meat — an innovation that is only under development in the early twenty-first century. \n\nLane plays with the customs and conventions of her own society, as utopian writers normally do. In Mizora, a narrow waist is considered a \"disgusting deformity\" — reversing the preference of Lane's own time for tightly-corseted women.\n\nLane's book anticipates some of the features of Charlotte Perkins Gilman's famous \"Herland\" by three decades. It was closely followed by other feminist utopian works, Mrs. George Corbett's \"New Amazonia: A Foretaste of the Future\" (1889), and \"Unveiling a Parallel\" (1893) by collaborators Alice Ilgenfritz Jones and Ella Merchant. Simultaneously, some male utopian writers published works that involve feminist issues and questions of gender roles; Charles Bellamy's \"An Experiment in Marriage\" (1889) and Linn Boyd Porter's \"Speaking of Ellen\" (1890) are examples.\n\n\"Mizora\" also belongs to the curious class of hollow Earth literature.\n\nThe second edition of \"Mizora\" appeared in 1975, and it was re-released in 1999 by the University of Nebraska Press. Little is known of the author; Mrs. Lane did not want her husband to find out she was writing about the world being better off without men.\n\n\n"}
{"id": "35132050", "url": "https://en.wikipedia.org/wiki?curid=35132050", "title": "Neaera (mythology)", "text": "Neaera (mythology)\n\nNeaera (; Ancient Greek: Νέαιρα), also Neaira (), is the name of multiple female characters in Greek mythology:\n"}
{"id": "37297196", "url": "https://en.wikipedia.org/wiki?curid=37297196", "title": "Opening of the North Atlantic Ocean", "text": "Opening of the North Atlantic Ocean\n\nThe opening of the North Atlantic Ocean is a geological event that occurred over millions of years, during which the supercontinent Pangea broke up. As modern-day Europe (Eurasian plate) and North America (North American Plate) separated during the final breakup of Pangea in the early Cenozoic Era, they formed the North Atlantic Ocean. Geologists believe the breakup occurred either due to primary processes of the Iceland plume or secondary processes of lithospheric extension from plate tectonics.\n\nRocks from the North Atlantic Igneous Province have been found in Greenland, the Irminger Basin, Faroe Islands, Vøring Plateau (off Norway), Faroe-Shetland Basin, Hebrides, Outer Moray Firth and Denmark. The supercontinent known as Pangea existed during the late Paleozoic and early Mesozoic eras and began to rift around 200 million years ago. Pangea had three major phases of breakup. The first major phase began in the Early-Middle Jurassic, taking place between North America and Africa. The second major phase of breakup began in the Early Cretaceous. The South Atlantic Ocean opened around 140 million years ago as Africa separated from South America, and about the same time, India separated from Antarctica and Australia, forming the central Indian Ocean. The final major phase of breakup occurred in the early Cenozoic, as Laurentia separated from Eurasia. As the two plates broke free from each other, the Atlantic Ocean continued to expand.\n\nThe Iceland plume is a mantle plume under Iceland that carries hot material from the deep within Earth's mantle upwards to the crust. The rising hot material weakens the lithosphere, making the separation of plates easier. The flow of hot plume material creates volcanism under the continental lithosphere. Iceland extends across the Mid-Atlantic Ridge. The Mid-Atlantic Ridge is a divergent plate boundary, and it separates the Eurasian and North American plates. The ages of the earliest volcanic rocks from this plume lie in the late Paleocene, and both sides of the Atlantic Ocean contain these rocks. Since these rocks have been dated to the late Paleocene, this lines up with the time of breakup of the North Atlantic continent, so some think it could have been a contributing factor.\n\nThis theory views volcanism as the resultant of lithospheric processes rather than heat from the mantle rising up. Instead of heat coming up from deep in the mantle, volcanic anomalies come from a shallow source. Volcanism thus occurs where the crust is easier to break up because it has been stretched by lithospheric extension, allowing melt to reach the surface. Volcanic anomalies are created by plate tectonics such as spreading plate boundaries or subduction zones. The location of the volcanism is governed by the stress field in the plate and the amount of melt is governed by the fusibility of the mantle beneath. Plate tectonics can explain most of the volcanism on Earth.\n\nActive rifting, such as is formed by the Iceland plume, is driven by hotspot or mantle plume activity. From deep within the Earth, hot mantle rises to force doming of the crust.\nThis causes thinning of the crust and lithosphere, then melting and underplating occur. Finally, there is rifting at the crest of the domed crust and volcanism occurs. In passive rifting, driven by plate tectonics, the crust and lithosphere extend as a result of plate boundary forces such as slab pull. Far field stresses thin the crust and lithospheric mantle, and hot asthenospheric mantle passively enters the thinned area. The upwelling of asthenosphere is not involved in the actual rifting process. The upward flow of the asthenosphere results in decompression melting, magmatic underplating and some volcanism that may occur in the rift area.\n\n"}
{"id": "23473", "url": "https://en.wikipedia.org/wiki?curid=23473", "title": "Paleomap", "text": "Paleomap\n\nPaleomaps are maps of continents and mountain ranges in the past based on plate reconstructions. Until the 1960s, paleomaps were not very satisfactory, since it was difficult to understand many quite distinctive features. For example, huge river deltas seemed to be associated with what must have been rather small drainage basins. With the discovery of plate tectonics, it became apparent that land masses move relative to one another over time. Ancient geologic features started to make far more sense. It is now possible to construct maps that are probably fairly accurate for continental positions over several hundred million years. Before the Cambrian Period, it becomes much more difficult since there are fewer rock exposures preserved. The state of large regions of the Earth becomes unknowable in the distant past. Where rocks are exposed, latitudes can often be determined from the orientation of preserved magnetic fields (see paleomagnetism) but longitudes are based on projections that are increasingly uncertain as one gets further from the present.\n\nMany published maps are associated in one way or another with the work of Christopher Scotese. The maps are useful since it is usually quite difficult to describe the location and orientation of geographical features using words alone.\n\n"}
{"id": "8787661", "url": "https://en.wikipedia.org/wiki?curid=8787661", "title": "Palisade Head", "text": "Palisade Head\n\nPalisade Head is a large rock formation on the North Shore of Lake Superior in the U.S. state of Minnesota. It is within Tettegouche State Park but not contiguous with the rest of that park. Palisade Head is located at milepost 57 on scenic Minnesota State Highway 61 in Beaver Bay Township, Lake County, approximately 54 miles (86 km) northeast of Duluth and three miles (five km) northeast of Silver Bay.\n\nPalisade Head is formed from a rhyolitic lava flow which was extruded some 1.1 billion years ago. During the Mesoproterozoic era of the Precambrian eon, the continent spread apart on the Midcontinent Rift System extending from what is now eastern Lake Superior through Duluth to Kansas; this rifting process stopped before an ocean developed. A flow some 200 feet (60 meters) thick formed extremely hard volcanic rock which resisted a billion years of erosion which cut down surrounding formations. This formed both Palisade Head and Shovel Point, which is within the main part of Tettegouche State Park about two miles (three kilometers) to the east. The feature is a shallow headland, with Lake Superior to the southwest, southeast, and northeast. Its high point is over 300 feet (approximately 100 meters) above the level of the lake; the lakeside cliffs stand up to several hundred feet (60 meters) above water level.\n\nThe headland is covered by a mixed forest of white spruce, mountain ash, aspen, paper birch, and oak. Raptors can often be seen soaring over the cliffs. Peregrine falcons nest on Palisdade Head, Bald eagles nest in the area, and thousands of hawks of several species can be seen migrating along the shoreline in the fall.\nTrees are sparse, but there are plentiful wild blueberries and, less commonly, gooseberries.\n\nPalisade Head is undeveloped; there are no improvements except for an access road, antenna tower, short-term parking, and a few low rock walls near the edge of the cliffs. On clear days there are views of the Sawtooth Mountains to the northeast, Split Rock Lighthouse to the southwest, the Bayfield Peninsula and Apostle Islands of Wisconsin across the lake to the south, and the ship traffic on Lake Superior. It is a regional center for rock climbing with many routes up the lakeshore cliffs. These cliffs were used for more sinister, albeit fictional purposes in The Good Son, partially filmed on location at Palisade Head.\n\n"}
{"id": "57632191", "url": "https://en.wikipedia.org/wiki?curid=57632191", "title": "Paramanu", "text": "Paramanu\n\nParamanu is a technical term in Jainism. It is defined as the smallest and indivisible particle of matter. It is one of the two types of Pudgala (matter), the other being Skandha.\nIt also helps define smallest measure of space. All the Parmanus occupy exactly same amount of space. The measure of the space occupied by one Parmanu is called Pradesha.\n"}
{"id": "16430619", "url": "https://en.wikipedia.org/wiki?curid=16430619", "title": "Parijaat tree, Kintoor", "text": "Parijaat tree, Kintoor\n\nThe Parijaat tree is a sacred baobab tree in the village of Kintoor, near Barabanki, Uttar Pradesh, India, about which there are several legends.\n\nIt is a protected tree situated in Barabanki district of Uttar Pradesh, India. By the order of local district magistrate, any kind of damage to the tree is strictly prohibited. The tree is known as baobab in modern science which is originated in Sub-sahara Africa and hence its presence in the fertile land of India makes it rare. Also the age of the tree is still not determined, which makes it quite possible that the tree may have been planted by someone who used to travel between India and Africa. The tree needs international attention of scientists to find out more about it. The tree is also known as 'the tree from paradise' due to its mythological significance. \n\nKintur, about east of the district headquarters, Barabanki, was named after Kunti, mother of the Pandavas. There are a number ancient temples and their remains around this place. Near a temple established by Kunti, is a special tree called Parijaat which is said to grow from Kunti's ashes. There are a number of legends about this tree which have popular acceptance. One being that Arjun brought this tree from heavens and Kunti used to offer and crown Lord Shiva with its flowers. Another saying being, that Lord Krishna brought this tree for his beloved queen Satyabhama or Rukmini. Historically, though these saying may have some bearing or not, but it is true that this tree is from a very ancient background.\n\nAccording to the \"Harivansh Puraan\" the Parijaat Tree is a Kalpavriksha, or wish bearing tree, which, apart from this tree, is only found in heaven. Newly-weds visit the tree for blessings, and every Tuesday a fair is held where local people worship the tree.\n\n"}
{"id": "25684865", "url": "https://en.wikipedia.org/wiki?curid=25684865", "title": "Pore pressure gradient", "text": "Pore pressure gradient\n\nPore pressure gradient is a dimensional petrophysical term used by drilling engineers and mud engineers during the design of drilling programs for drilling (constructing) oil and gas wells into the earth. It is the pressure gradient inside the pore space of the rock column from the surface of the ground down to the total depth (TD), as compared to the pressure gradient of seawater in deep water.\n\nWhereas in \"pure math,\" the gradient of a scalar function expressed by the math notation grad(\"f\") may not have physical units associated with it; in drilling engineering the pore pressure gradient is usually expressed in API-type International Association of Drilling Contractors (IADC) physical units of measurement, namely \"psi per foot.\" In the well-known formula\n\ntaught in almost all petroleum engineering courses worldwide, the mud weight (MW) is expressed in pounds per U.S. gallon, and the true vertical depth (TVD) is expressed in feet, and 0.052 is a commonly used conversion constant that can be derived by dimensional analysis:\n\nformula_1\n\nIt would be more accurate to divide a value in lb/gal by 19.25 than to multiply that value by 0.052. The magnitude of the error caused by multiplying by 0.052 is approximately 0.1%.\n\nExample: For a column of fresh water of 8.33 pounds per gallon (lb/U.S. gal) standing still hydrostatically in a 21,000 feet vertical cased wellbore from top to bottom (vertical hole), the pressure gradient would be\n\nand the hydrostatic bottom hole pressure (BHP) is then\n\nHowever, the formation fluid pressure (pore pressure) is usually much greater than a column of fresh water, and can be as much as 19 lb/U.S. gal (e.g., in Iran). For an onshore vertical wellbore with an exposed open hole interval at 21,000 feet with a pore pressure gradient of 19 lb/U.S. gal, the BHP would be\n\nThe calculation of a bottom hole pressure and the pressure induced by a static column of fluid are the most important and basic calculations in all well control courses taught worldwide for the prevention of oil and gas well blowouts.\n\nUsing the figures above, we can calculate the maximum pressure at various depths in an offshore oil well.\n\nA well with 5,000 feet of seawater and 15,000 feet of rock could have an overburden pressures at the bottom as high as 17,220 psi (5000 * 0.444 + 15000 * 1.0). That pressure is reduced at the surface by the weight of oil and gas the riser pipe, but this is only a small percentage of the total. It takes heavy mud (drilling fluid) inserted at the bottom to control the well when pressures are this high.\n"}
{"id": "16780796", "url": "https://en.wikipedia.org/wiki?curid=16780796", "title": "Rinay", "text": "Rinay\n\nRinay is a malacological museum of natural history in Baku (Azerbaijan). It is also the first private museum in the country. It exhibits approximately 5,000 clam shells (the shells of bivalve molluscs) from 86 genera worldwide. \nThe museum also displays hundreds of fossil clam shells. Some of these were obtained by exchange from the Austrian, Canadian, Argentine, Spanish, German, Ukrainian, Russian, Turkish and Indonesian paleontological university museums. \n\nThe basis for the museum collection was laid down in the early 1930s by Sadykh Karayev, an Honored Engineer and Honored Inventor of the USSR. Karayev's clam shell collection was preserved by his wife Prof. Jeyran Ibadova, and was transferred from Rinay to the History Museum of Azerbaijan in June 2007.\nSadykh Karayev's son Tofik Karayev continued the effort to improve his father's collection. T. Karayev previously worked at the Geological Institute of the Academy of Sciences of Azerbaijan SSR. He systematized the collection, and conducted a taxonomic revision of modern molluscs. In 1989 the museum received official status. \n\nRinay is open on Mondays, Tuesdays, and Fridays between 10:00 and 14:00.\n\n"}
{"id": "12765380", "url": "https://en.wikipedia.org/wiki?curid=12765380", "title": "Satellite formation flying", "text": "Satellite formation flying\n\nSatellite formation flying is the concept that multiple satellites can work together in a group to accomplish the objective of one larger, usually more expensive, satellite. Coordinating smaller satellites has many benefits over single satellites including simpler designs, faster build times, cheaper replacement creating higher redundancy, unprecedented high resolution, and the ability to view research targets from multiple angles or at multiple times. These qualities make them ideal for astronomy, communications, meteorology, and environmental uses.\n\nDepending on the application, there are three formations possible: trailing, cluster, and constellation. \n\nUsually, these formations are made up of numerous small satellites. A micro satellite weighs under 100 kg and a nano satellite weighs under 10 kg. Magnetosheric Constellation, for instance, would be composed of 100 micro satellites.(see )\n\nThis technology has become more viable thanks to the development of autonomous flying. With an onboard computer and this algorithm, satellites may autonomously position themselves into a formation. Previously, ground control would have to adjust each satellite to maintain formations. Now, satellites may arrive at and maintain formations with faster response time and have the ability to change the formation for varied resolution of observations. Also, satellites may be launched from different spacecraft and rendezvous on a particular path. This advance was made possible by Dave Folta, John Bristow, and Dave Quinn at NASA’s Goddard Space Flight Center (GSFC).\n\n"}
{"id": "1499550", "url": "https://en.wikipedia.org/wiki?curid=1499550", "title": "Sillar", "text": "Sillar\n\nSillar is a variety of rhyolite, which is a type of volcanic rock. Although sillar is of rhyolitic composition, it has been erupted from volcanoes which mostly erupt andesite lava, and sillar contains small fragments of andesite. A pink variety of sillar owes its colour to crystals of hematite within the rock. A white variety lacks these hematite crystals. Sillar is found as pyroclastic flow deposits of tuff near volcanoes in southern Peru, for example the now-extinct Chachani volcano which erupted flows of sillar during the Pleistocene epoch.\n\nSillar facies Orvieto-Bagnoregio Ignimbrite (black blocks of scoria in red tuff) occurs at Civita di Bagnoregio in the Vulsini volcanic district of central Italy.\n\nSillar has been used as a building stone in Peru. Many colonial buildings in the city of Arequipa are made of sillar, for example, the arches of the \"Mirador of Yanahuara\" in Arequipa, from which the entire city can be appreciated.\n\n\n"}
{"id": "4156794", "url": "https://en.wikipedia.org/wiki?curid=4156794", "title": "Soil compaction", "text": "Soil compaction\n\nIn geotechnical engineering, soil compaction is the process in which a stress applied to a soil causes densification as air is displaced from the pores between the soil grains. When stress is applied that causes densification due to water (or other liquid) being displaced from between the soil grains, then consolidation, not compaction, has occurred. Normally, compaction is the result of heavy machinery compressing the soil, but it can also occur due to the passage of (e.g.) animal feet. \n\nIn soil science and agronomy, soil compaction is usually a combination of both engineering compaction and consolidation, so may occur due to a lack of water in the soil, the applied stress being internal suction due to water evaporation as well as due to passage of animal feet. Affected soils become less able to absorb rainfall, thus increasing runoff and erosion. Plants have difficulty in compacted soil because the mineral grains are pressed together, leaving little space for air and water, which are essential for root growth. Burrowing animals also find it a hostile environment, because the denser soil is more difficult to penetrate. The ability of a soil to recover from this type of compaction depends on climate, mineralogy and fauna. Soils with high shrink-swell capacity, such as vertisols, recover quickly from compaction where moisture conditions are variable (dry spells shrink the soil, causing it to crack). But clays which do not crack as they dry cannot recover from compaction on their own unless they host ground-dwelling animals such as earthworms — the Cecil soil series is an example. \n\nSoil compaction is a vital part of the construction process. It is used for support of structural entities such as building foundations, roadways, walkways, and earth retaining structures to name a few. For a given soil type certain properties may deem it more or less desirable to perform adequately for a particular circumstance. In general, the preselected soil should have adequate strength, be relatively incompressible so that future settlement is not significant, be stable against volume change as water content or other factors vary, be durable and safe against deterioration, and possess proper permeability.\n\nWhen an area is to be filled or backfilled the soil is placed in layers called lifts. The ability of the first fill layers to be properly compacted will depend on the condition of the natural material being covered. If unsuitable material is left in place and backfilled, it may compress over a long period under the weight of the earth fill, causing settlement cracks in the fill or in any structure supported by the fill. In order to determine if the natural soil will support the first fill layers, an area can be proofrolled. Proofrolling consists of utilizing a piece heavy construction equipment (typically, heavy compaction equipment or hauling equipment) to roll across the fill site and watching for deflections to be revealed. These areas will be indicated by the development of rutting, pumping, or ground weaving.\n\nTo ensure adequate soil compaction is achieved, project specifications will indicate the required soil density or degree of compaction that must be achieved. These specifications are generally recommended by a geotechnical engineer in a geotechnical engineering report.\n\nThe soil type - that is, grain-size distributions, shape of the soil grains, specific gravity of soil solids, and amount and type of clay minerals, present - has a great influence on the maximum dry unit weight and optimum moisture content. It also has a great influence on how the materials should be compacted in given situations. Compaction is accomplished by use of heavy equipment. In sands and gravels, the equipment usually vibrates, to cause re-orientation of the soil particles into a denser configuration. In silts and clays, a sheepsfoot roller is frequently used, to create small zones of intense shearing, which drives air out of the soil.\nDetermination of adequate compaction is done by determining the in-situ density of the soil and comparing it to the maximum density determined by a laboratory test. The most commonly used laboratory test is called the Proctor compaction test and there are two different methods in obtaining the maximum density. They are the standard Proctor and modified Proctor tests; the modified Proctor is more commonly used. For small dams, the standard Proctor may still be the reference.\n\nWhile soil under structures and pavements needs to be compacted, it is important after construction to decompact areas to be landscaped so that vegetation can grow.\n\nThere are several means of achieving compaction of a material. Some are more appropriate for soil compaction than others, while some techniques are only suitable for particular soils or soils in particular conditions. Some are more suited to compaction of non-soil materials such as asphalt. Generally, those that can apply significant amounts of shear as well as compressive stress, are most effective. \n\nThe available techniques can be classified as:\n\nThe construction plant available to achieve compaction is extremely varied and is described elsewhere.\n\nSoil compactors are used to perform test methods which cover laboratory compaction methods used to determine the relationship between molding water content and dry unit weight of soils. Soil placed as engineering fill is compacted to a dense state to obtain satisfactory engineering properties such as, shear strength, compressibility, or permeability. In addition, foundation soils are often compacted to improve their engineering properties. Laboratory compaction tests provide the basis for determining the percent compaction and molding water content needed to achieve the required engineering properties, and for controlling construction to assure that the required compaction and water contents are achieved. Test methods such as EN 13286-2, EN 13286-47, ASTM D698, ASTM D1557, AASHTO T99, AASHTO T180, AASHTO T193, BS 1377:4 provide soil compaction testing procedures.\n\n"}
{"id": "49854401", "url": "https://en.wikipedia.org/wiki?curid=49854401", "title": "Titanomagnetite", "text": "Titanomagnetite\n\nTitanomagnetite is a mineral containing oxides of titanium and iron. It is also known as titaniferous magnetite.\n"}
{"id": "384213", "url": "https://en.wikipedia.org/wiki?curid=384213", "title": "Vampire pumpkins and watermelons", "text": "Vampire pumpkins and watermelons\n\nVampire pumpkins and watermelons are a folk legend from the Balkans, in southeastern Europe, described by ethnologist Tatomir Vukanović. The story is associated with the Romani people of the region, from whom much of traditional vampire folklore originated.\n\nThe belief in vampire fruit is similar to the belief that any inanimate object left outside during the night of a full moon will become a vampire. One of the main indications that a pumpkin or melon is about to undergo a vampiric transformation (or has just completed one) is said to be the appearance of a drop of blood on its skin.\n\nThe only known reference in scholarship is Tatomir Vukanović's account of his journeys in Serbia from 1933 to 1948. He wrote several years later:\n\nThe majority of Vukanović's article discusses human vampires; vampiric agricultural tools are also mentioned. Though modern readers may be skeptical that such beliefs ever existed, the superstitions of Roma (Gypsy) culture are well documented. The \"Journal of the Gypsy Lore Society\" has many articles that are collections of Roma tales, presumably oral history. However others are horror stories that allegedly include the direct involvement of the source (e.g., the fatal consequences of disrespecting the dead). In this context, vampire pumpkins and watermelons are not necessarily any more implausible than other superstitious beliefs.\n\nThe story was popularized by Terry Pratchett's 1998 book \"Carpe Jugulum\", a comic fantasy novel making extensive use of vampire legends. Pratchett has stated that he did not invent the vampire watermelon story himself. It is found in several other works: Jan Perkowski's 1976 book reprinted Vukanović's account, the webcomic \"Digger\" incorporates a field of vampire squash (most of which resemble butternut squashes in appearance), and recent popular books on the topic of vampirism include a mention.\n\nVampire vegetables play a central role in the \"Bunnicula\" series of children's books written by James and Deborah Howe.\n\n"}
{"id": "2846507", "url": "https://en.wikipedia.org/wiki?curid=2846507", "title": "Worst-Case Scenario series", "text": "Worst-Case Scenario series\n\nWorst-Case Scenario is the name of a series of merchandise based on a 1999 book written by Joshua Piven and David Borgenicht and published by Chronicle Books. The series first entry, \"The Worst-Case Scenario Survival Handbook,\" sold over 10 million copies worldwide and prompted a series of related books, games, calendars, and two television series. Later works have been published via Quirk Books and has been credited with giving the company enough funding to expand. \n\nA worst-case scenario is the most severe possible outcome that can be projected to occur in a given situation. Conceiving of worst-case scenarios is a common form of strategic planning to prepare for and minimize contingencies that could result in accidents, quality problems, or other issues. The books provide instructions for situations like landing a plane and surviving a shark attack.\n\nThere have been two television series that have been loosely based on the book series, a 2002 TBS reality show and a 2010 survival skills television series on the Discovery Channel. Both series ran for only one season. \n\nThe show \"Worst Case Scenarios\" ran on TBS from July 10, 2002 through November 13, 2002 and had a total of 13 episodes. It was narrated by Mike Rowe and featured demonstrations by stunt men and stunt women. Each episode was hosted by new experts on how to survive worst-case scenarios. It also aired clips of real life situations, tips, volunteer challenges, and gadgets shown by Gear Girl (Danielle Burgio). \n\n\"Worst-Case Scenario\" ran on the Discovery Channel beginning on May 5, 2010 and had a total of 12 episodes. It was hosted by Bear Grylls, who was shown using skills he learned in the 21 SAS, which included self-defense, evasive driving, parkour, urban survival, and wilderness survival.\n\n\n\n\n\n\n"}
