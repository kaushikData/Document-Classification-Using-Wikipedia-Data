{"id": "56656103", "url": "https://en.wikipedia.org/wiki?curid=56656103", "title": "2018 BD", "text": "2018 BD\n\n\"2018 BD\" is an Apollo asteroid. It orbits the Sun at a distance of 0.75–1.36 AU once every 13 months (395 days; semi-major axis of 1.05 AU). Its orbit has an eccentricity of 0.29 and an inclination of 2° with respect to the ecliptic.\n\nThe object has an exceptionally low minimum orbital intersection distance with Earth of , or 0.002 lunar distances.\n\nBased on a generic magnitude-to-diameter conversion, measures between 2 and 6 meters in diameter, for an absolute magnitude of 30.154, and an assumed albedo between 0.05 and 0.20, which represent typical values for carbonaceous and a bright E-type asteroids, respectively. As of 2018, no rotational lightcurve of this object has been obtained from photometric observations. The body's rotation period, pole and shape remain unknown.\n\nThis minor planet has neither been numbered nor named.\n\n\n"}
{"id": "39376462", "url": "https://en.wikipedia.org/wiki?curid=39376462", "title": "Ab-polar current", "text": "Ab-polar current\n\nAb-polar current, an obsolete term sometimes found in 19th century meteorological literature, refers to any air current moving away from either the North Pole or the South Pole. In the Northern Hemisphere, this term indicates a northerly wind. The Latin prefix \"ab-\" means \"from\" or \"away from\".\n"}
{"id": "52784842", "url": "https://en.wikipedia.org/wiki?curid=52784842", "title": "Axiarchism", "text": "Axiarchism\n\nAxiarchism () is a metaphysical position that everything that exists, including the universe itself, exists for a good purpose.The word was coined by Canadian philosopher John Leslie.\n"}
{"id": "3573137", "url": "https://en.wikipedia.org/wiki?curid=3573137", "title": "Batea (mythology)", "text": "Batea (mythology)\n\nIn Greek mythology, the name Batea or Bateia (; Ancient Greek: Βάτεια) refers to the following individuals:\n"}
{"id": "31695373", "url": "https://en.wikipedia.org/wiki?curid=31695373", "title": "Bathometer", "text": "Bathometer\n\nA bathometer (also bathymeter) is an instrument for measuring water depth. It was previously used mainly in oceanographical studies, but is rarely employed nowadays. The term originates from Greek \"βαθύς\" (\"bathys\"), \"deep\" and \"μέτρον\" (\"métron\"), \"measure\".\n\n"}
{"id": "50688613", "url": "https://en.wikipedia.org/wiki?curid=50688613", "title": "Boardman Hill Solar Farm", "text": "Boardman Hill Solar Farm\n\nThe Boardman Hill Solar Farm \"BHSF\" is a 150 kW AC community solar farm project. The Boardman Hill Solar Farm is the first community solar array in Vermont that fulfills the “Vermont Grown, Vermont Green” mission: complete member-ownership, democratic management of ongoing operations, and retirement of the RECs generated by the solar farm. It is located on the Boardman Hill Farm in West Rutland, Vermont.In 2015 it was awarded the Vermont Energy and Climate Action Network Best Project of 2015\n\nProject Overview\n\n"}
{"id": "233772", "url": "https://en.wikipedia.org/wiki?curid=233772", "title": "Camp Taliaferro", "text": "Camp Taliaferro\n\nCamp Taliaferro was a World War I flight-training center run under the direction of the Air Service, United States Army in the Fort Worth, Texas, area. Camp Taliaferro had an administration center near what is now the Will Rogers Memorial Center complex in Fort Worth's cultural area near University Drive and W Lancaster Avenue.\n\nAfter the United States' entry into World War I in April 1917, General John J. \"Blackjack\" Pershing invited the British Royal Flying Corps (RFC) to establish training fields in the southern United States where the warmer weather would be more conducive for flying year-round. In June, the War Department inspected six sites around Fort Worth which had been offered by the Chamber of Commerce and by July, RFC representatives from Canada inspected five potential sites in Texas and Louisiana for use during the winter.\n\nAfter looking at sites in Dallas, Fort Worth, Waco, Austin, Wichita Falls, and Midland, in August the War Department signed leases with the RFC on three sites around Fort Worth. Known as the Flying Triangle, these sites were Hicks Field (#1), Barron Field (#2), and Benbrook Field (#3) based on their locations; and construction began in late August and early September.\n\nThe Canadians named the training complex Camp Taliaferro after 1st Lieutenant Walter R. Taliaferro, a U.S. Army aviator. Taliaferro was killed in an accident at [San Diego], California, on 11 October 1915. The Camp Taliaferro offices for the Air Service and RFC Canada were initially located in the basement of the Chamber of Commerce building in Fort Worth to handle pay, purchasing, and administrative services for their own personnel assigned at the three fields.\n\nWork on constructing the airfield had to be done quickly. Cattle were moved out, and construction crews worked feverishly at the site. US Air Service squadrons which had been training in Canada began arriving in October 1917, and the RFC squadrons began to arrive in early November.\n\nThe first winter of 1917-1918 was difficult. Many men lived in tents in this snowy winter. RFC instructors trained about 6000 men there. In six months, 1,960 pilots were trained, completing 67,000 flying hours on the Curtiss JN4 Canuck, a two-seater biplane weighing 2,100 lb (950 kg) with a maximum speed of 75 mph (120 km/h). About 69 ground officers and 4,150 others received training in ground trades and skills. During the winter, eight deaths were due to influenza and 39 RFC personnel died as the result of aircraft accidents, influenza, or other illnesses.\n\nCanadian cadets were at Benbrook and Everman Fields, while the US cadets and the Canadian aerial gunnery school went to Hicks. The training was a major life experience, sufficiently so for veterans to keep memorabilia long afterwards. For example, the wife of Harry Kuhlmann from San Jose, California, (22d Aero Squadron 1917-9) died in 1974, still holding Camp Taliaferro Post Exchange Tickets. In Cornell's \"Alumni News\", L. H. Germer announced as his permanent address: Squadron 139, Field No. 1, Camp Taliaferro, Hicks, Texas. At Christmas, special postcards were available from the camp for residents.\n\nFor those who survived the training, combat life expectancy was short. Only two US Air Service squadrons—the 17th and 148th Aero Squadrons—saw active service with the British, flying with them until November 1918, after which they were absorbed into the US Air Service. The obituary of one veteran may be typical: \n\nOthers survived the war, but to continue in aviation was almost as perilous:\n\nThirty-nine officers and cadets died in Texas. Eleven British, Canadians, and Americans remain there, reinterred in 1924 at a Commonwealth War Graves Commission cemetery plot in Greenwood Memorial Park, Fort Worth. The plot is in Section 5 of the cemetery, at 32-45-47, 97-21-48. Also interred there are one of their comrades who died in 1975, and the daughter of a Canadian instructor who died as a baby in 1918. A stone monument serves as a focal point on Memorial Day in May of odd-numbered years, when friends of the cemetery support a moving Remembrance Service, at which people from the three nations remember the sacrifice of those buried there.\n\nFollowing the departure of the Royal Air Force in April 1918, Camp Taliaferro was closed and each of the 3 fields operated as separate sites. When the RFC Canada arrived in Fort Worth in November 1917, they brought 254 Curtiss JN-4 (Can) license-built aircraft with them for training at the 3 fields around Fort Worth. When they returned to Canada in April 1918, they turned over 180 serviceable aircraft to the Army Air Service. By then, American and Canadian license-built JN-4s were keeping up with demand.\n\nBesides molding men into lean, mean fighting machines, Camp Taliaferro also molded some long lasting friendships between the Canadians and Americans. One of the most vivid memories of Hicks Field was the remarkable friendship that existed between the RFC pilots and the American flyers. The townspeople of the surrounding communities opened their homes, their hearts and numerous facilities to the young aviators.\n\n\n"}
{"id": "7204913", "url": "https://en.wikipedia.org/wiki?curid=7204913", "title": "Carbon-to-nitrogen ratio", "text": "Carbon-to-nitrogen ratio\n\nA carbon-to-nitrogen ratio (C/N ratio or C:N ratio) is a ratio of the mass of carbon to the mass of nitrogen in a substance. It can, amongst other things, be used in analysing sediments and compost. A useful application for C/N ratios is as a proxy for paleoclimate research, having different uses whether the sediment cores are terrestrial-based or marine-based. Carbon-to-nitrogen ratios are an indicator for nitrogen limitation of plants and other organisms and can identify whether molecules found in the sediment under study come from land-based or algal plants. Further, they can distinguish between different land-based plants, depending on the type of photosynthesis they undergo. Therefore, the C/N ratio serves as a tool for understanding the sources of sedimentary organic matter, which can lead to information about the ecology, climate, and ocean circulation at different times in Earth’s history.\n\nC/N ratios in the range 4-10:1 are usually from marine sources, whereas higher ratios are likely to come from a terrestrial source. Vascular plants from terrestrial sources tend to have C/N ratios greater than 20. The lack of cellulose, which has a chemical formula of (CHO), and greater amount of proteins in algae versus vascular plants causes this significant difference in the C/N ratio.\n\nWhen composting, microbial activity utilizes a C/N ratio of 30-35:1 and a higher ratio will result in slower composting rates. However, this assumes that carbon is completely consumed, which is often not the case. Thus, for practical agricultural purposes, a compost should have an initial C/N ratio of 20-30:1.\n\nExample of devices that can be used to measure this ratio are the CHN analyzer and the continuous-flow isotope ratio mass spectrometer (CF-IRMS). However, for more practical applications, desired C/N ratios can be achieved by blending common used substrates of known C/N content, which are readily available and easy to use.\n\nOrganic matter that is deposited in marine sediments contains a key indicator as to its source and the processes it underwent before reaching the floor as well as after deposition, its carbon to nitrogen ratio. In the global oceans, freshly produced algae in the surface ocean typically have a carbon to nitrogen ratio of about 4 to 10. However, it has been observed that only 10% of this organic matter (algae) produced in the surface ocean sinks to the deep ocean without being degraded by bacteria in transit, and only about 1% is permanently buried in the sediment. An important process called sediment diagenesis accounts for the other 9% of organic carbon that sank to the deep ocean floor, but was not permanently buried, that is 9% of the total organic carbon produced is degraded in the deep ocean. The microbial communities utilizing the sinking organic carbon as an energy source are partial to nitrogen-rich compounds because much of these bacterium are nitrogen-limited and much prefer it over carbon. As a result, the carbon to nitrogen ratio of sinking organic carbon in the deep ocean is elevated compared to fresh surface ocean organic matter that had not been degraded. An exponential increase in C/N ratios is observed with increasing water depth—with C/N ratios reaching 10 at intermediate water depths of about 1000 meters, and up to 15 in the deep ocean (~ >2500 meters) . This elevated C/N signature is preserved in the sediment, until another form of diagenesis, post-depositional diagenesis, alters its C/N signature once again. Post-depositional diagenesis occurs in organic-carbon-poor marine sediments where bacteria are able to oxidize organic matter in aerobic conditions as an energy source. The oxidation reaction proceeds as follows: CHO + HO → CO + 4H + 4e, with a standard free energy of –27.4 kJ mol (half reaction). Once all of the oxygen is used up, bacteria are able to carry out an anoxic sequence of chemical reactions as an energy source, all with negative ∆G°r values, with the reaction becoming less favorable as the chain of reactions proceeds.\n\nThe same principle described above explaining the preferential degradation of nitrogen-rich organic matter occurs within the sediments, as they are more labile and are in higher demand. This principle has been utilized in paleoceanographic studies in order to identify core sites that have not experienced much microbial activity, or contamination by terrestrial sources with much higher C/N ratios.\n\nLastly, it should be noted that ammonia, the product of the second reduction reaction, which reduces nitrate and produces nitrogen gas and ammonia, is easily adsorbed on clay mineral surfaces and protected from bacteria. This has been proposed as an explanation for lower than expected C/N signatures of organic carbon in sediments that have undergone post-depositional diagenesis.\n\nAmmonium produced from the remineralisation of organic material, exists in elevated concentrations (1 - >14μM) within cohesive shelf sea sediments found in the Celtic Sea (depth: 1–30 cm). The depth of sediment exceeds 1m and would be a suitable study site to carry out paleolimnology experiments with C:N.\n\nUnlike in marine sediments, diagenesis does not pose a large threat to the integrity of the C/N ratio in lacustrine sediments. Though wood from living trees around lakes have consistently higher C/N ratios than wood buried in sediment, the change in elemental composition is not large enough to remove the vascular versus non-vascular plant signals due to the refractory nature of terrestrial organic matter. Abrupt shifts in the C/N ratio down-core can be interpreted as shifts in the organic source material.\n\nFor example, two separate studies on Mangrove Lake, Bermuda and Lake Yunoko, Japan show irregular, abrupt fluctuations between C/N around 11 to around 18. These fluctuations are attributed to shifts from mainly algal dominance to land-based vascular dominance. Results of studies that show abrupt shifts in algal dominance and vascular dominance often lead to conclusions about the state of the lake during these distinct periods of isotopic signatures. Times in which lakes are dominated by algal signals suggest the lake is a deep-water lake, while times in which lakes are dominated by vascular plant signals suggest the lake is shallow, dry, or marshy. Using the C/N ratio in conjunction with other sediment observations, such as physical variations, D/H isotopic analyses of fatty acids and alkanes, and δ13C analyses on similar biomarkers can lead to further regional climate interpretations that describe the larger phenomena at play.\n\n"}
{"id": "12283839", "url": "https://en.wikipedia.org/wiki?curid=12283839", "title": "Chivela Pass", "text": "Chivela Pass\n\nThe Chivela Pass is a narrow mountain pass in the Sierra Madre Mountains that funnels cooler, drier air from the North American continent, through southern Mexico, into the Pacific. These northeasterly winds, specifically the Tehuano wind, which periodically blows across the Isthmus of Tehuantepec in southern Mexico, have important climatic effects, influencing the formation of hurricanes and typhoons, as well as contributing to worldwide climatological events, such as \"El Nino\".\n\nIn extreme circumstances during the winter, truly cold, dense, air occasionally flows from the Bay of Campeche in the Gulf of Mexico through the Chivela Pass into the Gulf of Tehuantepec on the Pacific side. These winds can be strong enough to sandblast paint off ships at sea.\n\n"}
{"id": "3247983", "url": "https://en.wikipedia.org/wiki?curid=3247983", "title": "Clay court", "text": "Clay court\n\nA clay court is one of many different types of tennis court. Clay courts are made of crushed shale, stone, brick, or other unbound mineral aggregates. The French Open uses clay courts, making it unique among the Grand Slam tournaments. Clay courts are more common in Continental Europe and Latin America than in North America, Asia or Britain. Two main types exist: red clay, the more common variety, and green clay, also known as \"rubico\", which is a harder surface. Although less expensive to construct than other types of tennis courts, the maintenance costs of clay are high as the surface must be rolled to preserve flatness.\n\nClay courts favor the \"full western grip\" for more topspin. \"Clay-courters\" generally play in a semicircle about behind the baseline.\n\nClay courts are considered \"slow\" because the balls bounce relatively high and lose much of their initial speed when contacting the surface, making it more difficult for a player to deliver an unreturnable shot. Points are usually longer as there are fewer winners. Therefore, clay courts heavily favor baseliners who are consistent and have a strong defensive game, which has allowed players such as Rafael Nadal, Björn Borg, Chris Evert, and Justine Henin to find success at the French Open.\n\nClay court players use topspins to throw off their opponents. Movement on gravel courts is very different from movement on any other surface. Playing on clay often involves the ability to slide into the ball during the stroke, as opposed to running and stopping like on a hard or grass court. Players who excel on clay courts but struggle to replicate the same form on fast courts are known as clay-court specialists.\n\nClay courts are unique in that the ball bounce leaves an impression in the ground, which can help determine whether a shot was in or out. Critics of red clay courts point to the constant need to wet them down, problems renewing the surface if it dries out, and the damage caused to clothing and footwear through stains. All clay courts, not just red clay, tend to cause a build-up of clay on the bottom of the shoes of the players, needing constant removal.\n\nAlmost all red clay courts are made not of natural clay but of crushed brick that is packed to make the court, with the top most layers consisting of finely crushed loose particles. Such courts are most common in Europe and Latin America.\n\n\"En tout cas\" (Fr. \"in any case\") is a version of red clay with a coarser top layer to improve drainage. The coarser surface allows more water to run through the surface of the court drying the surface more quickly after rain. \n\nNatural clay courts are rare because this type of surface does not absorb water easily and takes two to three days to dry. A good example of natural red clay can be seen at the Frick Park Clay Courts in Pittsburgh, a public facility of six red clay courts that has been in continual use since 1930.\n\nGreen clay, also called rubico and the brand name Har-Tru, is similar to coarse red clay, but is made from crushed metabasalt rather than brick, making the surface slightly harder and faster. Green clay is packed to make the subsurface. It is then covered with a topping. These courts are located primarily in the eastern and southern United States.\n\nThere is one WTA tournament played on green Har-Tru clay courts, the Volvo Car Open in Charleston, South Carolina.\n\nRafael Nadal, winner of eleven French Open men's singles titles, is noted for his success on clay; since his debut in 2005, he has only lost twice at the tournament – in 2009 and 2015. Nadal holds the record for the longest winning streak by any male player on a single surface since the tennis open era began in 1968: 81 clay court wins between April 2005 and May 2007. He also holds the record for most titles won on clay in the open era with 55 trophies. Thomas Muster is also considered a successful clay court player; although he only won the French Open once, 40 out of his 44 career singles titles were won on clay.\n\nOn the women's side, Justine Henin and Monica Seles hold the record for the number of consecutive French Open titles won at three (1990–1992 for Seles and 2005–2007 for Henin). In the pre-open era this feat was first achieved by Helen Wills Moody (1928–1930) and followed by Hilde Krahwinkel Sperling (1935–1937).\n\nChris Evert holds the record for longest winning streak on clay for either gender in the open era: from August 1973 to May 1979, she won 125 consecutive clay court matches. During this time Evert skipped three editions of the French Opens (1976–78), to participate in World Team Tennis. She also has the highest career win percentage on clay courts (94.28%) during the open era.\n\nThe most successful currently active female player on clay is Serena Williams, who won the French Open thrice, in 2002, 2013 and 2015. In 2013, Williams went undefeated throughout the clay court season, winning five titles on the surface.\n\nA clay-court specialist is a tennis player who excels on clay courts.\n\nDue in part to advances in racquet technology, current clay-court specialists are known for employing long, winding groundstrokes that generate heavy topspin, strokes which are less effective when the surface is faster and the balls do not bounce as high. Clay-court specialists tend to slide more effectively on clay than other players. Many of them are also very adept at hitting the drop shot, which can be effective because rallies on clay courts often leave players pushed far beyond the baseline. Additionally, the slow, long rallies require a great degree of mental focus and physical stamina from the players.\n\nThe definition of \"clay-court specialist\" has varied, with some placing players such as Thomas Muster, Gustavo Kuerten, and Juan Carlos Ferrero in that category, even though these players have won tournaments (including Masters Series events) on other surfaces. However, since these players won major titles only at the French Open, they are sometimes labeled as such. Other players, such as Sergi Bruguera, Albert Costa and Gastón Gaudio were French Open champions who won all or very nearly all of their career titles on clay. Among female players, there have been very few whose best results were confined exclusively to clay. Virginia Ruzici, Anastasia Myskina, Iva Majoli, Sue Barker, Ana Ivanovic, Francesca Schiavone, Jeļena Ostapenko and Simona Halep are the only female players to have won major titles at only the French Open since the beginning of the open era.\n\nIncreasingly, clay courters have attempted to play better on other surfaces with some success. Ferrero reached the US Open Final in 2003, the same year he won the French Open, and also won several hardcourt tournaments. Nadal was considered a clay court specialist until a string of successes on other surfaces, including completing a golden career grand slam, led to a broadening of his reputation, while 2016 French Open winner Garbiñe Muguruza reached the 2015 Wimbledon final and won the 2017 Wimbledon title.\n\nThe professional clay court season comprises many more tournaments than the brief grass court season, but is still shorter than the hard court seasons. There are three distinct clay court seasons during the year.\n\nThe first is the men's winter clay season consisting of tournaments in South America. Played primarily in February between the Australian Open and the Indian Wells Masters, the ATP has four tournaments in this swing, although other ATP tournaments played on hardcourt occur the same weeks. The WTA discontinued its participation in Rio de Janeiro after 2016, so there are no clay court women's tournaments during this period.\n\nThe second is the long spring clay season that starts in the Americas and Morocco before moving to mainland Europe and finishing with the French Open. It is usually played over two months between April and June, after the Miami Open. Unlike the other two clay seasons this swing does not share the majority of its time with simultaneous hard court tournaments.\n\nThe third is the brief summer clay season that takes place after Wimbledon. It is entirely in Europe, and usually takes place in July. Near the end of the swing, it overlaps with the beginning of the US Open Series.\n\n"}
{"id": "37268324", "url": "https://en.wikipedia.org/wiki?curid=37268324", "title": "Darvaza gas crater", "text": "Darvaza gas crater\n\nThe Darvaza gas crater (, \"Җәхеннем дервезеси\"), known locally as the \"Door to Hell\" or <nowiki>\"</nowiki>Gates of Hell\", is a natural gas field collapsed into an underground cavern located in Derweze, Turkmenistan. Geologists set it on fire to prevent the spread of methane gas, and it is thought to have been burning continuously since 1971. The diameter of the crater is , and its depth is .\n\nThe crater is a popular tourist attraction. Since 2009, 50,000 tourists have visited the site. The gas crater has a total area of . The surrounding area is also popular for wild desert camping.\n\nThe gas crater is located near the village of Derweze, also known as \"Darvaza\". It is in the middle of the Karakum Desert, about north of Ashgabat, the capital of Turkmenistan. The gas reserve found here is one of the largest in the world. The name \"Door to Hell\" was given to the field by the locals, referring to the fire, boiling mud, and orange flames in the large crater, which has a diameter of . The hot spots range over an area with a width of and to a depth of about .\n\nAccording to Turkmen geologist Anatoly Bushmakin, the site was identified by Soviet engineers in 1971. It was originally thought to be a substantial oil field site. The engineers set up a drilling rig and operations to assess the quantity of oil available at the site. Soon after the preliminary survey found a natural gas pocket, the ground beneath the drilling rig and camp collapsed into a wide crater and was buried.\n\nExpecting dangerous releases of poisonous gases from the cavern into nearby towns, the engineers thought it best to burn the gas off. It was estimated that the gas would burn out within a few weeks, but it has instead continued to burn for more than four decades. \n\nThe early years of the crater's history are uncertain: local geologists say the collapse into a crater happened in the 1960s, and the gases were not set on fire until the 1980s. There are, however, no records available of the Soviet or Turkmen version of events.\n\nIn April 2010, the President of Turkmenistan, Gurbanguly Berdimuhamedow, visited the site and ordered that the hole should be closed. In 2013, he declared the part of the Karakum Desert with the crater a nature reserve. \n\nThe crater was featured in an episode of the short-lived (2014) National Geographic Channel series \"Die Trying\". In the July 16, 2014 episode \"Crater of Fire\", explorer George Kourounis became the first person to ever set foot at the bottom, gathering samples of extremophile microorganisms. An edited photograph of the crater was also released as publicity for the then-upcoming 2014 \"Godzilla\" film, with the image depicting MONARCH agents and vehicles investigating the site.\n\nOn President Berdimuhamedow's April 2010 visit, he recommended that measures be taken to limit the crater's influence on the development of other natural gas fields in the area. At that time, Turkmenistan announced plans to increase its production of natural gas, intending to increase its export of gas to many countries such as Pakistan, China, India, Iran, Russia, and Western Europe, from its then yearly production level to a new production level of by 2030.\n\n"}
{"id": "5597613", "url": "https://en.wikipedia.org/wiki?curid=5597613", "title": "Directional-hemispherical reflectance", "text": "Directional-hemispherical reflectance\n\nDirectional-hemispherical reflectance is the reflectance of a surface under direct illumination (with no diffuse component). Directional-hemispherical reflectance is the integral of the BRDF over all viewing directions. It is sometimes called \"black-sky albedo\". See also: Bi-hemispherical reflectance\n"}
{"id": "9144140", "url": "https://en.wikipedia.org/wiki?curid=9144140", "title": "Dopplergraph", "text": "Dopplergraph\n\nThe word dopplergraph is a combination of the words doppler and photograph. Dopplergraphs are two-dimensional records of variations in the doppler shift in light intensity. Dopplergraphs do not need to be a record of the shift of visible light, but of any radiated wave, which includes electromagnetic waves and acoustic waves. Since the doppler shift is caused by the velocity of the radiating source towards or away from the viewer, a dopplergraph is a picture of the velocities associated with the sources being viewed.\n"}
{"id": "10368520", "url": "https://en.wikipedia.org/wiki?curid=10368520", "title": "Downsizer", "text": "Downsizer\n\nDownsizer is a virtual community, run on a not-for-profit basis, which describes itself as \"a resource for people who want to live more sustainably\". Its website includes articles on sustainable living and a popular forum with over 4,500 registered members.\n\nThe site was set up in October 2004 by the founder members. Its name came from a term used in present-day English to describe a person, whose aim it is to cut down on consumer goods and live a more self-reliant and sustainable lifestyle. A year after Downsizer's establishment, the \"Essex Chronicle\" remarked on the site's growth, observing that it had \"tapped into an increasing awareness and interest in the impact our lives have on the environment\". It has continued to grow, and has over 4,500 users registered on its forum as of July 2011.\n\nDownsizer.net is a not-for-profit online community, a resource for people who want to live more sustainably. Like-minded individuals can visit the forums to discuss matters of self-sufficiency and sustainability amongst other things. The website also includes many articles related to downsizing issues including the following:\n\n\nThe website received a positive assessment from Alison Cork in \"The Observer\", who commented on the variety of topics discussed by members and concluded, \"Whether you're an individual wanting to live a more sustainable lifestyle or a small business trying to get an appreciative market for your ethical product or service, you'd be hard pushed not to find something of interest.\"\n\nA more negative stance was taken by Steve Lowe and Alan McArthur, authors of \"Is It Just Me Or Is Everything Shit?\" (2005). In an article in \"The Sunday Times\" to promote their book, they presented the interest of \"burnt-out stockbrokers\" in the advice provided by Downsizer as an instance of the \"mediocrity\" of contemporary British culture.\n"}
{"id": "28490374", "url": "https://en.wikipedia.org/wiki?curid=28490374", "title": "Eastern Canadian Shield taiga", "text": "Eastern Canadian Shield taiga\n\nThe Eastern Canadian Shield taiga is an ecoregion of Canada as defined by the World Wildlife Fund (WWF) categorization system.\n\nLocated in northeastern Canada, this ecoregion covers a large part of northern Quebec and most of Labrador, reaching from Hudson Bay and James Bay in the west, across to Ungava Bay and east to the Atlantic Ocean coast of Labrador. This is a taiga ecoregion and therefore stops at the treeline, beyond which is tundra. This is a rugged rocky landscape including an area fjords on the Atlantic coast of Labrador. The hills and plateaus are dotted with many lakes and string bogs, and patches of tundra on the Mealy Mountains and elsewhere.\n\nThis is a cold part of the world with average annual temperatures ranging from -6°C in Hudson Bay to 1°C on the Labrador coast.\n\nThe dominant trees of the taiga are black spruce (\"Picea mariana\") and tamarack (\"Larix laricina\"), mixed with smaller numbers of white spruce (\"Picea glauca\"), dwarf birches, willows, laurels, and rhododendrons. The boglands are a habitat of sedges and sphagnum moss.\n\nThe ecoregion is home to wildlife including caribou, moose (\"Alces alces\"), American black bear (\"Ursus americanus\"), grey wolf (\"Canis lupus\"), red fox (\"Vulpes vulpes\"), Arctic fox (\"Alopex lagopus\"), wolverine (\"Gulo gulo\"), snowshoe hare (\"Lepus americanus\") and colonies of seals. Of particular interest are the inland (and therefore freshwater) harbor seals of Lacs des Loups Marins and the world's largest herd of caribou, the George River herd of up to 400,000 animals. Birds include grouse, osprey (\"Pandion haliaetus\"), raven (\"Corvus corax\") and many waterbirds. In particular the rocky coast is home to breeding colonies of seabirds, including the endangered eastern population of the harlequin duck and is also on the Atlantic Flyway migratory route for birds.\n\nThis ecoregion is almost entirely in its natural state, apart from some areas damaged by hydro-electric power generation projects and mining activities such as the Voisey's Bay Mine. There are no substantial protected areas although the proposed Mealy Mountains National Park Reserve will be the largest park in Atlantic Canada and other parks are proposed for Richmond Gulf and the Clearwater Lakes on Hudson Bay, and Atikonak Lake on the Quebec-Labrador border.\n"}
{"id": "17945330", "url": "https://en.wikipedia.org/wiki?curid=17945330", "title": "Elizabeth Royte", "text": "Elizabeth Royte\n\nElizabeth Royte is an American science/nature writer. She is best known for her books \"Garbage Land\" (a New York Times Notable Book of the Year 2005), \"The Tapir's Morning Bath: Solving the Mysteries of the Tropical Rain Forest\" (a New York Times Notable Book of the Year, 2001), \"Bottlemania: How Water Went on Sale and Why We Bought It\" (a \"Best of\" or \"Top 10\" book of 2008 in Entertainment Weekly, Seed and Plenty magazines) and \"A Place to Go\" \n\nRoyte's articles have appeared in \"The New York Times Magazine\", \"Harper's\", \"National Geographic\", \"The New York Times Book Review\", \"The New Yorker\", \"The Nation\", \"Outside\", \"Smithsonian\", and other magazines. Her work has been featured in the \"Best American Science Writing 2004\" and the \"Best American Science Writing 2009.\" Royte is a former Alicia Patterson Foundation fellow and a recipient of Bard College's John Dewey Award for Distinguished Public Service.\n\nHer article about women who survived the genocide in Rwanda attracted a good deal of attention. She has traveled throughout the world to research her articles and books.\n\nRoyte won an Alicia Patterson Journalism Fellowship in 1990 to research and write about life at a biological research station in the tropics.\n\nRoyte began her career as an intern at \"The Nation\". She did freelance copy editing and writing for other magazines.\n\nRoyte lives in Park Slope, Brooklyn with her husband and their daughter. Her brother is an ecologist. Her uncle is theater director/producer Robert Kalfin.\n\n\n"}
{"id": "28810049", "url": "https://en.wikipedia.org/wiki?curid=28810049", "title": "Environmental magnetism", "text": "Environmental magnetism\n\nEnvironmental magnetism is the study of magnetism as it relates to the effects of climate, sediment transport, pollution and other environmental influences on magnetic minerals. It makes use of techniques from rock magnetism and magnetic mineralogy. The magnetic properties of minerals are used as proxies for environmental change in applications such as paleoclimate, paleoceanography, studies of the provenance of sediments, pollution and archeology. The main advantages of using magnetic measurements are that magnetic minerals are almost ubiquitous and magnetic measurements are quick and non-invasive.\n\nEnvironmental magnetism was first identified as a distinct field in 1978 and was introduced to a wider audience by the book \"Environmental Magnetism\" in 1986. Since then it has grown rapidly, finding application in and making major contributions to a range of diverse fields, especially paleoclimate, sedimentology, paleoceanography, and studies of particulate pollution.\n\nEnvironmental magnetism is built on two parts of rock magnetism: magnetic mineralogy, which looks at how basic magnetic properties depend on composition; and magnetic hysteresis, which can provide details on particle size and other physical properties that also affect the hysteresis. Several parameters such as magnetic susceptibility and various kinds of remanence have been developed to represent certain features of the hysteresis. These parameters are then used to estimate mineral size and composition. The main contributors to the magnetic properties of rocks are the iron oxides, including magnetite, maghemite, hematite; and iron sulfides (particularly greigite and pyrrhotite). These minerals are strongly magnetic because, at room temperature, they are magnetically ordered (magnetite, maghemite and greigite are ferrimagnets while hematite is a canted antiferromagnet).\n\nTo relate magnetic measurements to the environment, environmental magnetists have identified a variety of processes that give rise to each magnetic mineral. These include erosion, transport, fossil fuel combustion, and bacterial formation. The latter includes extracellular precipitation and formation of magnetosomes by magnetotactic bacteria.\n\nMagnetic measurements have been used to investigate past climate. A classic example is the study of loess, which is windblown dust from the edges of glaciers and semiarid desert margins. In north-central China, blankets of loess that were deposited during glacial periods alternate with paleosols (fossil soils) that formed during warmer and wetter interglacials. The magnetic susceptibility profiles of these sediments have been dated using magnetostratigraphy, which identifies geomagnetic reversals, and correlated with climate indicators such as oxygen isotope stages. Ultimately, this work allowed environmental magnetists to map out the variations in the monsoon cycle during the Quaternary. Magnetic measurements of lacustrine sediments can also be used to reconstruct the upland surfical processes that were associated with past climate.\n\n\n"}
{"id": "41398498", "url": "https://en.wikipedia.org/wiki?curid=41398498", "title": "Filtered Rayleigh scattering", "text": "Filtered Rayleigh scattering\n\nFiltered Rayleigh scattering (FRS) is a diagnostic technique which measures velocity, temperature, and pressure by determining Doppler shift, total intensity, and spectral line shape of laser induced Rayleigh-Brillouin scattering.\n"}
{"id": "2775695", "url": "https://en.wikipedia.org/wiki?curid=2775695", "title": "First Landing State Park", "text": "First Landing State Park\n\nFirst Landing State Park (formerly Seashore State Park) offers recreational opportunities at Cape Henry in the independent city of Virginia Beach, Virginia. As the first planned state park of Virginia, First Landing is listed on the National Register of Historic Places as Seashore State Park Historic District. A portion of the park is listed as a National Natural Landmark as part of the Seashore Natural Area.\n\nThe state park is near the site of the first landing on April 26, 1607 of Christopher Newport and the Virginia Company colonists before establishing themselves at Jamestown. The park includes cabins, areas for camping, fishing, and swimming, a public beach, and over of trails for hiking and biking. Virginia's most popular state park, it's visited by over a million people each year. Its main entrance is located on Shore Drive across from the beach camping entrance. First Landing State Park charges a fee for its camping facilities and for the overnight use of its cabins.\n\nOriginally built by the Civilian Conservation Corps in the 1930s, First Landing State Park is located on the Chesapeake Bay. The park, which is Virginia's most visited state park, is nestled in Virginia Beach. First Landing offers boating, swimming, nature and history programs, hiking, biking, picnicking, a boat launch, cabins and of trails on . It also has campsites that have water and electric hook-ups and nearby access to restrooms and showers. The park's name was changed in 1997 from Seashore State Park to First Landing State Park to reflect its heritage as the first place where members of the Virginia Company landed (though its former name is still commonly used by Tidewater locals).\n\nDevelopment of the Seashore State Park began in 1933 by the Civilian Conservation Corps on of donated land. Most of the workers were African American men. The new park opened on June 15, 1936. Several additions of land and adjustments of the border with Joint Expeditionary Base East resulted in the current total area of the park of .\n\nThe Seashore Natural Area, a portion of which is located within the park, was listed as a National Natural Landmark in 1965. The designation recognized the significance of the park's forested dunes and semitropical vegetation. In 1997, the park's name was changed from Seashore State Park to First Landing State Park to reflect its heritage as the first place where members of the Virginia Company landed.\nA cross was erected in the state park by the Daughters of the American Colonists on April 26, 1935 to mark the location of the first landing by English colonists at the site, along with a placard at the base of the cross with the dates of the landing and the settlement at Jamestown, Virginia.\n\nThe Seashore State Park Historic District is a historic district (comprising the entire park) with significance dating to 1933 for its architecture/engineering. It includes work by CEGG Assoc. LC, the National Park Service, and others. It includes eight contributing buildings, six contributing sites and 10 contributing structures.\n\nThe park was the first planned state park of the Virginia State Park system. Its plan was designed and developed with extensive consultation of the National Park Service, which provided architectural drawings and plans, and which educated about traffic circulation and other aspects of already-designed U.S. national parks. The work of actually building roads and buildings and other structures and features of the park was accomplished by Civilian Conservation Corps workers, who were housed in three camps of 200 men each.\n\n\n"}
{"id": "3784614", "url": "https://en.wikipedia.org/wiki?curid=3784614", "title": "Flexible Mechanisms", "text": "Flexible Mechanisms\n\nFlexible mechanisms, also sometimes known as Flexibility Mechanisms or Kyoto Mechanisms), refers to Emissions Trading, the Clean Development Mechanism and Joint Implementation. These are mechanisms defined under the Kyoto Protocol intended to lower the overall costs of achieving its emissions targets. These mechanisms enable Parties to achieve emission reductions or to remove carbon from the atmosphere cost-effectively in other countries. While the cost of limiting emissions varies considerably from region to region, the benefit for the atmosphere is in principle the same, wherever the action is taken.\n\nMuch of the negotiations on the mechanisms has been concerned with ensuring their integrity. There was concern that the mechanisms do not confer a \"right to emit\" on Annex 1 Parties or lead to exchanges of fictitious credits which would undermine the Protocol’s environmental goals. The negotiators of the Protocol and the Marrakesh Accords therefore sought to design a system that fulfilled the cost-effectiveness promise of the mechanisms, while addressing concerns about environmental integrity and equity.\n\nTo participate in the mechanisms, Annex 1 Parties must meet the following eligibility requirements:\n\n\nThe Emissions Trading-mechanism allows parties to the Kyoto Protocol to buy 'Kyoto units'(emission permits for greenhouse gas) from other countries to help meet their domestic emission reduction targets.\n\nThe Protocol defines two project-based mechanisms that allow Annex I countries to meet their GHG emission reduction commitments by acquiring GHG emission reductions \"credits.\" The credits are acquired by an Annex I country financing projects that reduce emissions in non-Annex I countries or other Annex I countries, or by purchasing credits from Annex I countries with excess credits. The project-based mechanisms are the Clean Development Mechanism (CDM) and Joint Implementation (JI).\n\nThe project-based mechanisms allow Annex I countries with efficient, low GHG-emitting industries, and high prevailing environmental standards to purchase carbon credits on the world market instead of reducing greenhouse gas emissions domestically. Annex I countries typically will want to acquire carbon credits as cheaply as possible, while non-Annex I countries want to maximize the value of carbon credits generated from their domestic greenhouse gas reducing projects.\n\nThrough the Joint Implementation, any Annex I country can invest in emission reduction projects (referred to as \"Joint Implementation Projects\") in any other Annex I country as an alternative to reducing emissions domestically.\n\nThrough the CDM, countries can meet their domestic emission reduction targets by buying greenhouse gas reduction units from (projects in) non Annex I countries to the Kyoto protocol (mostly developing countries). Non-Annex I countries have no GHG emission restrictions, but have financial incentives to develop GHG emission reduction projects to receive Certified Emission Reductions that can then be sold to Annex I countries, encouraging sustainable development.\n\nKyoto provides for a 'cap and trade' system which imposes national caps on the emissions of annex I countries. On average, this cap requires countries to reduce their emissions by 5.2% below their 1990 baseline over the 2008 to 2012 period. Although these caps are national-level commitments, in practice, most countries will devolve their emissions targets to individual industrial entities, such as a power plant or paper factory. One example of a 'cap and trade' system is the 'EU ETS'. Other schemes may follow suit in time.\n\nThe ultimate buyers of credits are often individual companies that expect emissions to exceed their quota, their assigned allocation units, AAUs or 'allowances' for short. Typically, they will purchase credits directly from another party with excess allowances, from a broker, from a JI/CDM developer, or on an exchange.\n\nNational governments, some of whom may not have devolved responsibility for meeting Kyoto obligations to industry, and that have a net deficit of allowances, will buy credits for their own account, mainly from JI/CDM developers. These deals are occasionally done directly through a national fund or agency, as in the case of the Dutch government's ERUPT programme, or via collective funds such as the World Bank’s Prototype Carbon Fund (PCF). The PCF, for example, represents a consortium of six governments and 17 major utility and energy companies on whose behalf it purchases credits.\n\nSince allowances and carbon credits are tradeable instruments with a transparent price, financial investors can buy them on the spot market for speculation purposes, or link them to futures contracts. A high volume of trading in this secondary market helps price discovery and liquidity, and in this way helps to keep down costs and set a clear price signal in CO which helps businesses to plan investments. This market has grown substantially, with banks, brokers, funds, arbitrageurs and private traders now participating in a market valued at about $60 billion in 2007. Emissions Trading PLC, for example, was floated on the London Stock Exchange's AIM market in 2005 with the specific remit of investing in emissions instruments.\n\nAlthough Kyoto created a framework and a set of rules for a global carbon market, there are in practice several distinct schemes or markets in operation today, with varying degrees of linkages among them.\n\nKyoto enables a group of several annex I countries to create a market-within-a-market together. The EU elected to be treated as such a group, and created the EU Emissions Trading Scheme (ETS). The EU ETS uses EAUs (EU Allowance Units), each equivalent to a Kyoto AAU. The scheme went into operation on 1 January 2005, although a forward market has existed since 2003.\n\nThe UK established its own learning-by-doing voluntary scheme, the UK ETS, which ran from 2002 through 2006. This market existed alongside the EU's scheme, and participants in the UK scheme have the option of applying to opt out of the first phase of the EU ETS, which lasts through 2007.\n\nThe sources of Kyoto credits are the Clean Development Mechanism (CDM) and Joint Implementation (JI) projects. The CDM allows the creation of new carbon credits by developing emission reduction projects in non-annex I countries, while JI allows project-specific credits to be converted from existing credits within annex I countries. CDM projects produce Certified Emission Reductions (CERs), and JI projects produce Emission Reduction Units (ERUs), each equivalent to one AAU. Kyoto CERs are also accepted for meeting EU ETS obligations, and ERUs will become similarly valid from 2008 for meeting ETS obligations (although individual countries may choose to limit the number and source of CER/JIs they will allow for compliance purposes starting from 2008). CERs/ERUs are overwhelmingly bought from project developers by funds or individual entities, rather than being exchange-traded like allowances.\n\nSince the creation of Kyoto is subject to a lengthy process of registration and certification by the UNFCCC, and the projects themselves require several years to develop, this market is at this point largely a forward market where purchases are made at a discount to their equivalent currency, the EUA, and are almost always subject to certification and delivery (although up-front payments are sometimes made). According to IETA, the market value of CDM/JI credits transacted in 2004 was EUR 245 m; it is estimated that more than EUR 620 m worth of credits were transacted in 2005.\n\nSeveral non-Kyoto carbon markets are in existence or being planned, and these are likely to grow in importance and numbers in the coming years. These include the New South Wales Greenhouse Gas Abatement Scheme, the Regional Greenhouse Gas Initiative and Western Climate Initiative in the United States and Canada, the Chicago Climate Exchange and the State of California’s recent initiative to reduce emissions.\n\nThese initiatives taken together may create a series of partly linked markets, rather than a single carbon market. The common theme is the adoption of market-based mechanisms centered on carbon credits that represent a reduction of CO emissions. The fact that some of these initiatives have similar approaches to certifying their credits makes it possible that carbon credits in one market may in the long run be tradeable in other schemes. The scheme would broaden the current carbon market far more than the current focus on the CDM/JI and EU ETS domains. An obvious precondition, however, is a realignment of penalties and fines to similar levels,since these create an effective ceiling for each market.\n\nAs stated in the lede, one of the main arguments made in favour of the flexibility mechanisms is that of cost-effectiveness. The principle of cost-effectiveness is included in the UN Framework Convention on Climate Change (UNFCCC). The economic basis of costs being reduced through flexibility is discussed in emissions trading#Applying the economic theory and economics of climate change mitigation#Flexibility.\n\nA number of concerns were raised about flexibility in the lead-up to negotiations of the Kyoto Protocol. Two examples of issues raised were that of domestic emissions reductions in the developed countries, and the issue of developed countries effectively taking up all the low-cost emissions reductions in developing countries. The idea behind the first view was that most emissions reductions should occur first in the developed countries - this would encourage the development of low-carbon energy technologies which could then be taken up later on by developing counties. The second idea was that all of the low-cost emissions reductions in developing countries would, in effect, be stolen by the developed countries. Thus, when it came time for developing countries to take on their own commitments to reduce emissions, it would be more costly for them to do so.\n\nDiffering views on flexibility were summarized in the Intergovernmental Panel on Climate Change's (IPCC) Second Assessment Report. The basic economic argument in favour of flexibility was that, in principle at least, issues to do with fairness (\"equity\" in the language of economics) could be separated from efficiency (i.e., reducing emissions most cheaply). From this viewpoint, flexibility through emissions trading could promote efficiency, while arguments of equity could be partially addressed through, for example, the allocations of emissions rights between different countries.\n\nDuring negotiations, the US was a supporter of flexibility, while several other negotiating parties were in favour of uniform emissions cuts (e.g., the Alliance of Small Island States, ASIS). In the end, flexibility was incorporated into Kyoto's design, but the treaty still places an emphasis on developed countries achieving the bulk of their emissions reductions domestically, rather than in developing countries (i.e., by using the Clean Development Mechanism, CDM). The balance between domestic emissions reductions in developed countries and reductions through the CDM is not, however, quantified.\n\nSince the implementation of the flexibility mechanisms, a number of other concerns have been raised. There have been various criticisms of the CDM (see Clean Development Mechanism for details). These include excess profits generated by CDM projects designed to reduce emissions of industrial gases, adverse effects of projects on local communities, and the failure of the CDM to promote development in the poorest regions of the world. Criticisms have also been made of the various emissions trading schemes set-up by developed countries to meet their first-round Kyoto targets. These criticisms are discussed in the individual articles on these trading schemes: see Kyoto Protocol#International Emissions Trading for a list of these trading schemes. For example, the environmental organization Friends of the Earth (EWNI) has called for the EU Emissions Trading System (EU ETS) to be scrapped, and be replaced by other policies (e.g., energy efficiency standards), which they argue would be more effective than the EU ETS at reducing emissions.\n\nThe articles referred to above also contain policy measures proposed by governments and commentators to address some of these criticisms.\n\nThe second commitment period of the Kyoto Protocol ends in 2020. Due to this, negotiations are underway to have the role of Flexible Mechanisms continue in some form under the Paris_Agreement. All though not yet worked out (at the moment of writing; november 2016), This should be possible as both article 5 and 6 call for a similar mechanism to be created. \n\n\n\n"}
{"id": "23702872", "url": "https://en.wikipedia.org/wiki?curid=23702872", "title": "Fossil Fuel Beta", "text": "Fossil Fuel Beta\n\nFossil Fuel Beta (FFß) measures the percent change in excess (market-adjusted) stock returns for every 1 percent increase in fossil fuel prices. \nFor example, if a company (or industry) has an FFß of –0.20, then a 1 percent increase in fossil fuel prices should produce, on average, a 0.2% decline in the firm’s stock price over and above the impact arising from fossil fuel price swing on the stock market as a whole. (Conversely, a 1 percent decrease in fossil fuel prices should produce, on average, an equivalent increase in stock price.) \n\nConverting the FFß into a hypothetical ‘Earnings per share-equivalent’ based on a fixed percent change in fossil fuel prices, it is possible to compare earnings-at-risk for individual companies with their competitors, or even industries with each other.\n\nThree common fossil fuels — coal, petroleum and natural gas — produce more than four-fifths of all carbon dioxide (CO) emissions. Achieving meaningful reductions in greenhouse gas (GHG) emissions requires abating CO emissions.\n\nIn countries such as the US, companies are the largest emitters of CO. By linking earnings-per-share (EPS) to fossil fuel price volatility, FFß highlights a potential economic payoff for reducing or hedging the use of fossil fuels, and hence CO emissions. FFß can also underscore adverse economic consequences to companies that fail to manage carbon footprints.\n\nThe FFß was created by Professor Anant Sundaram of the Tuck School of Business at Dartmouth College, with support from the Allwin Initiative for Corporate Citizenship. \n\n\"CFO Magazine\" (a division of the \"Economist\") published the first FFß scorecard in its December 2008 issue. The scorecard was computed for 135 S&P 500 firms in ten industries, using a simple market model in which a firm's excess returns are calculated by subtracting aggregate market returns from the firm's returns. A slightly more sophisticated analysis calculates the firm’s excess returns by subtracting returns predicted by an asset pricing model such as the Capital Asset Pricing Model, or CAPM. This latter analysis, for all the firms in the S&P500 can be found here.\n\nCalculating market value and EPS impact of fossil fuel price changes uses publicly available data on stock returns, analyst earnings estimates, and fossil fuel producer prices. Steps are:\n\n\nIf we expect that fossil fuel prices will rise over time and that there will be a meaningful price on GHG emissions from a carbon tax or cap-and-trade system, energy-consuming businesses will generally want to reduce their dependence on GHG-producing fossil fuels. In that context, a firm that is focused on its underlying product-market and competitive strategy (rather than extraneous variables such as energy prices) might attempt to decouple itself from cash flow and return effects of fossil fuel price changes. Over the long run, such a firm would seek to gravitate to a zero FFß. Actions it might take include implementing operational strategies such as achieving greater fuel efficiency and shifting to renewable energy sources, or financial strategies such as hedging fossil fuel purchases.\n\nSuccess in decoupling stock returns from fossil fuel price changes will have implications for statistical significance of the coefficient in the regression to calculate the FFß: the regression coefficient will not be significantly different from zero for companies that have successfully gravitated to a zero-FFß. (Of course, the normal tests of statistical significance of difference from zero will apply for betas that are inferred to be positive or negative).\n\nEnergy-consuming companies should generally have neutral-to-negative FFßs.\nEnergy is not only an input cost, but higher energy prices also increase the price of other raw materials. If a company cannot pass through in its pricing the effects of energy and raw material price increases, its profit margin will decline. There is also a substitution effect. Because consumers pay more for energy use when fossil fuel prices rise, they have less to spend on the firm’s products, thus negatively impacting profits and margins. The combination of these two effects for energy-consuming firms produces a decline in stock price when energy prices rise. \n\nSome energy-consuming firms could be insulated from these impacts, and may have neutral (or even positive) FFßs. They may have hedged their cash flow exposure to price increases via financial hedges or by operational hedges to reduce dependence on fossil fuel use. Their market power might be higher, or the price elasticity of demand for their products might be so low as to enable them to pass on the effect of cost increases. It is also possible that a company’s business model benefits from the macroeconomic effects of an energy price increase: for instance, a company with lower-priced product offerings might find demand increasing in harder economic times as consumers switch from higher-priced alternatives.\n\nEnergy-producing companies, on the other hand, should generally have neutral-to-positive betas. If the fossil fuel price increase is the result of an increase in demand for energy, higher prices will lead to higher profits. Price elasticity of demand may be lower than average in energy-producing industries since it is a widely used (and essential) good with no short-term substitutes. \n\nSome energy producers, e.g., electric utilities, may have margins protected by regulation that allows them to price on a ‘cost-plus’ basis. Such factors allow energy producers to pass on cost increases to consumers. Energy-producing industries also generally tend to be capital intensive. This capital-intensity could create entry barriers, giving them greater pricing power. For all these reasons, the net impact of fossil fuel price increases on energy producers is to maintain or increase margins, and thereby, their stock prices. Finally, many energy producers own fossil fuel reserves. Higher fossil fuel prices will have a direct positive impact on their share prices by increasing the market value of reserve assets.\n\nHere is an example showing two companies in the retailing sector as it appeared in the \"CFO scorecard\". It shows the hypothetical EPS impact of a 10% increase in fossil fuel prices.\n\nGiven likely high dependence on fossil fuels because of reliance on transportation and storage, as well as heating or cooling commercial buildings, firms in the retailing sector would be expected to have a negative FFß, on average. Yet, there is considerable variance within the industry. Some firms in the industry – e.g., Wal-mart – even have non-negative FFßs.\n\nAs the scorecard shows, petroleum producers and electric utilities have generally positive FFßs. On the other hand, heavy energy-consuming industries such as heavy manufacturing, retailing, aerospace/defense, and airlines have negative FFßs, while industries such as food/beverages and drugs with their neutral FFßs, seem relatively immune to fossil fuel price changes.\n\n"}
{"id": "4662308", "url": "https://en.wikipedia.org/wiki?curid=4662308", "title": "Föhn cloud", "text": "Föhn cloud\n\nA Föhn cloud is any cloud associated with a föhn, usually an orographic cloud, a mountain wave cloud, or a Lenticular cloud.\n\nFöhn is a regional term referring to winds in the Alps.\n\n\nDefant, F., 1951: Compendium of Meteorology, 667–669.\n\n"}
{"id": "22353740", "url": "https://en.wikipedia.org/wiki?curid=22353740", "title": "Geothermal power in Denmark", "text": "Geothermal power in Denmark\n\nDenmark has two geothermal district heating plants, one in Thisted which started in 1988, and one in Copenhagen which started in 2005. They produce no electricity.\n\n"}
{"id": "7149688", "url": "https://en.wikipedia.org/wiki?curid=7149688", "title": "Hard inheritance", "text": "Hard inheritance\n\nHard inheritance was a model of heredity that explicitly excludes any acquired characteristics, such as of Lamarckism. It is the exact opposite of soft inheritance, coined by Ernst Mayr to contrast ideas about inheritance.\n\nHard inheritance states that characteristics of an organism's offspring (passed on through DNA) will not be affected by the actions that the parental organism performs during its lifetime. For example: a medieval blacksmith who uses only his right arm to forge steel will not sire a son with a stronger right arm than left because the blacksmith's actions do not alter his genetic code. Inheritance due to usage and non-usage is excluded. Inheritance works as described in the modern synthesis of evolutionary biology.\n\nThe existence of inherited epigenetic variants has led to renewed interest in soft inheritance.\n"}
{"id": "5361557", "url": "https://en.wikipedia.org/wiki?curid=5361557", "title": "Kautsky effect", "text": "Kautsky effect\n\nKautsky effect (also named, \"fluorescence transient\", \"fluorescence induction\" or \"fluorescence decay\") is a phenomenon consisting on a typical variation on the behavior of a plant fluorescence when is exposed to light. It was discovered in 1931 by H. Kautsky and A. Hirsch.\n\nWhen dark-adapted photosynthesising cells are illuminated with continuous light, chlorophyll fluorescence displays characteristic changes in intensity accompanying the induction of photosynthetic activity.\n\nThe quantum yield of photosynthesis, which is also the photochemical quenching of fluorescence, is calculated through the following equation:\n\nΦ = (F-F)/F = F/F\n\nF is the low fluorescence intensity, which is measured by a short light flash that is not strong enough to cause photochemistry, and thus induces fluorescence. F is the maximum fluorescence that can be obtained from a sample by measuring the highest intensity of fluorescence after a saturating flash. The difference between the measured values is the variable fluorescence F.\n\nWhen a sample (leaf or algal suspension) is illuminated, the fluorescence intensity increases with a time constant in the microsecond or millisecond range. After a few seconds the intensity decreases and reaches a steady-state level. The initial rise of the fluorescence intensity is attributed to the progressive saturation of the reaction centers of photosystem 2 (PSII). Therefore, photochemical quenching increases with the time of illumination, with a corresponding increase of the fluorescence intensity. The slow decrease of the fluorescence intensity at later times is caused, in addition to other processes, by non-photochemical quenching. Non-photochemical quenching is a protection mechanism in photosynthetic organisms as they have to avoid the adverse effect of excess light. Which components contribute and in which quantities remains an active but controversial area of research. It is known that carotenoids and the special pigment pairs (e.g. P700) have functions in photoprotection.\n\n"}
{"id": "36989349", "url": "https://en.wikipedia.org/wiki?curid=36989349", "title": "Khalwat al-Bayada", "text": "Khalwat al-Bayada\n\nKhalwat is the name of the prayer-houses of the Druze. The primary sanctuary of the Druze is at Khalwat al-Bayada.\n\nThe \"Khalwat al-Bayada, Khalwet el Biyad, Khalwat al-Biyyada\" or \"White houses of communion\" is the central sanctuary, and theological school of the Druze, located in Lebanon. Located near Hasbaya, the khalwat is the location where Ad-Darazi is supposed to have settled and taught from during the first Druze call. It features a large, stone, circular bench next to an ancient oak tree known as \"Areopagus of the Elders\" that is secluded amongst nature and trees. The Kalwaat provides around forty hermitages for Al-ʻuqqāl (the initiated) at various times of the year. In 1838, copies of the Epistles of Wisdom were taken from the site by invading Egyptians. Visitors are politely requested to seek permission from the resident sheikh before entering the site and female visitors are requested to cover their heads as a courtesy.\n\n\n"}
{"id": "20813552", "url": "https://en.wikipedia.org/wiki?curid=20813552", "title": "Kitadani Formation", "text": "Kitadani Formation\n\nThe Kitadani Formation is a unit of Lower Cretaceous sedimentary rock which crops out near the city of Katsuyama in the Fukui Prefecture of Japan, and it is the primary source of Cretaceous-aged non-marine vertebrate fossils in Japan. Dinosaur remains are among the fossils that have been recovered from the formation, but it also preserves a diverse assemblage of plants, invertebrates, and other vertebrates. Most, if not all, of the fossil specimens collected from the Kitadani Formation are reposited at the Fukui Prefectural Dinosaur Museum.\n\nThe Kitadani Formation is a unit within the Akaiwa Subgroup of the Tetori Group, a major sequence of Mesozoic-aged rocks which is distributed across the Fukui, Ishikawa, and Gifu prefectures of western-central Honshu, Japan. The Tetori Group exhibits marked lateral variation, and the Kitadani Formation is only present in the Fukui Prefecture. The Kitadani Formation comprises interbedded tuffs, sandstones, and shales and reaches a maximum thickness of approximately 100 m (~328 feet). It conformably overlies the Akaiwa Formation and is unconformably overlain by the Omichidani Formation. The Kitadani Formation is significant because it is the major source of dinosaur fossils in Japan and because of Japan's unique position along the northeastern margin of Eurasia during the Early Cretaceous.\n\nThe Kitadani Formation is a unit within the Tetori Group, a Middle Jurassic to Lower Cretaceous sequence of predominantly sedimentary rock which crops out in the Fukui, Ishikawa, and Gifu prefectures of west-central Honshu, Japan in the region surrounding Mount Haku. The Tetori Group comprises, in ascending stratigraphic order:\n\n\nThe formations present within the Akaiwa Subgroup vary laterally, and the Kitadani Formation crops out only in the Kuzuryū River district of the Fukui Prefecture. In this region, the Akaiwa Subgroup comprises, in ascending stratigraphic order, the Akaiwa Formation and Kitadani Formation. Here, the Kitadani Formation comprises alternating horizons of red-brown tuffs, blackish shales and sandstones, and thin coal beds. The sandstones within the Kitadani Formation are light gray and green and range in clast size from fine to coarse. The type section of the Kitadani Formation occurs along the Nakanomatadani branch of the Takinami River near the city of Katsuyama, where it is approximately 100 m (~328 feet) in thickness. The Kitadani Formation conformably overlies the Akaiwa Formation and is unconformably overlain by the Omichidani Formation.\n\nThe Kitadani Formation has had varying nomenclature throughout the history of its study. In the early stratigraphic literature on the Tetori Group, the Kitadani Formation was variably referred to as the \"Lower part of the Omichidani Formation\", the \"Chinaboradani Alternation of Tuff, Shale, and Sandstone\", the \"Kitadani Alternation of Sandstone, Shale, and Tuff\", and simply the \"Kitadani Alternation\" prior to its designation as a formation.\n\nThe Kitadani Formation was biostratigraphically dated to the late Barremian and early Aptian ages of the Early Cretaceous Epoch in 2002 based upon the presence of the freshwater bivalve \"Nippononaia ryosekiana\". In 2005, part of the Kitadani Formation was biostratigraphically dated to the Barremian Age based upon the occurrence of the charophyte gyrogonite \"Clavator harrisii reyi\" in association with other charophytes. These biostratigraphic age assignments are supported by zircon fission track radioisotopic ages of tuff, which date the Kitadani Formation the 127-115 Ma.\n\nThe Kitadani Formation preserves a diverse assemblage of plant fossils; invertebrate fossils; and vertebrate body and trace fossils, including mammals, turtles, neosuchian reptiles, and dinosaurs. Many vertebrate specimens from the Kitadani Formation are incomplete and poorly preserved, so taxonomic diversity is likely higher than it seems.\n\nThe plant fossil assemblage of the Kitadani Formation is characterized by a rarity of ferns and an abundance of cycadales and conifers represented mostly by cones and shoots. A palynological study in 2013 resulted in the identification of greater than 40 species of spores, pollen grains, and plant fragments from the Kitadani Formation representing gymnosperms, freshwater algae, and epiphyllous fungus; however, no angiosperm pollen was identified. Branches of the conifer \"Brachyphyllum obesum\" have been recovered, which was interpreted to represent the warming and possible drying of the climate toward the upper Tetori Group. This interpretation is supported by the lack of plants from lower in the Tetori Group, such as ginkgos, in the Kitadani Formation.\n\nThe invertebrate fossil assemblage of the Kitadani Formation mostly comprises freshwater and brackish water bivalve and gastropod mollusks.\n\nAt least three mammalian taxa have been recovered from the Kitadani Formation, represented by rare teeth and partial jaws. In 2004, the spalacotheriid \"symmetrodont\" \"Symmetrolestes parvus\" was reported based upon a fragmentary right mandible with the first incisor and five postcanine teeth preserved. Two non-therian mammal specimens were reported in 2015, including an eobaatarid multituberculate and a triconodontid eutriconodontan.. These specimens were noted as possessing non-tribosphenic dentition, and were interpreted being taxonomically distinct to closely related taxa from elsewhere in the Tetori Group, but neither specimen was named. The eobaatarid is represented only by a damaged left premolar. The triconodontid is represented by a partial right dentary possessing a faint Meckelian groove.\n\nTurtles are represented mostly by shell fragments within the Kitadani Formation. In 2002, a preliminary evaluation of fragmentary specimens resulted in the identification of the taxa \"Adocus\", \"Basilemys\", and Trionychidae. A more recent evaluation of all 700+ turtle specimens from the Kitadani Formation was conducted in 2015. In that study, the authors concluded that the turtle assemblage of the Kitadani Formation consists of four trionychoid taxa and two other eucryptodires. In addition to the genera reported in 2002, \"Perochelys\", \"Gobiapalone\", \"Apalonina\", and an unnamed nanhsiungchelyid were identified.\n\nA nearly complete skeleton of a goniopholidid eusuchian has been noted from the Kitadani Formation, but this material remains formally unpublished and unnamed. This specimen was discovered in 1982, and it was hypothesized to be closely related to \"Sunosuchus\", \"Goniopholis\", and \"Eutretauranosuchus\" based on a preliminary phylogenetic analysis.\n\nDinosaurs are among the most well-known vertebrate taxa from the Kitadani Formation. Taxa from all three majors dinosaurian clades have been recovered, including theropods, sauropodomorphs, and ornithischians. \n\nAmong theropods, two taxa have been named, and other indeterminate dromaeosaurid and other theropod material is preserved. \"Fukuiraptor kitadanensis\" was reported in 2000 based upon a partial skull and associated partial postcranium. The phylogenetic relationships of \"Fukuiraptor\" are poorly resolved. It was originally assigned the wastebasket taxon Megaraptora, but subsequent phylogenetic analyses have suggested that it and all other \"megaraptorans\" may be allosauroids or non-tyrannosaurian coelurosaurs. \"Fukuivenator paradoxus\" was reported in 2016, and it is interpreted to be an early-diverging maniraptoran. However, its unique combination of ancestral and derived features associated with several different coelurosaurian clades precludes referral to a more exclusive clade than Maniraptora.\n\nThe titanosaurian sauropod dinosaur \"Fukuititan nipponensis\" was reported from the Kitadani Formation in 2010 based upon a single partial skeleton including cranial and postcranial material. Other indeterminate sauropod material is also preserved. \n\nOrnithischian dinosaur are represented in the Kitadani Formation by two named taxa and other indeterminate specimens, including an unnamed psittacosaurid ceratopsian. \"Fukuisaurus tetoriensis\" was named in 2003 based upon sparse cranial material, but more complete specimens have been recovered in the time since. \"Fukuisaurus\" is interpreted to be a non-hadrosauroid hadrosauriform. Another hadrosauriform \"Koshisaurus katsuyama\" was reported in 2015 and interpreted to be an early-branching hadrosauroid, not especially closely related to \"Fukuisaurus\".\n\nAdditionally, non-avialan theropod, avialan theropod, sauropod, ornithischian tracks have been reported from the Kitadani Formation. \n\nDinosaur egg fragments have also been reported from the Kitadani Formation, including \"Plagioolithus fukuensis\", a three-layered eggshell interpreted as a fossil avian egg. If indeed avian, \"Plagioolithus\" would represent the oldest known fossil bird egg.\n\n"}
{"id": "23163934", "url": "https://en.wikipedia.org/wiki?curid=23163934", "title": "Laminar–turbulent transition", "text": "Laminar–turbulent transition\n\nThe process of a laminar flow becoming turbulent is known as laminar–turbulent transition. This is an extraordinarily complicated process, which at present is not fully understood. However, as the result of many decades of intensive research, certain features have become gradually clear, and it is known that the process proceeds through a series of stages. \"Transitional flow\" can refer to transition in either direction, that is laminar–turbulent transitional or turbulent–laminar transitional flow.\n\nWhile the process applies to any fluid flow, it is most often used in the context of boundary layers due to their ubiquity in real fluid flows and their importance in many fluid-dynamic processes.\n\nIn 1883 Osborne Reynolds demonstrated the transition to turbulent flow in a classic experiment in which he examined the behaviour of water flow under different flow rates using a small jet of dyed water introduced into the centre of flow in a larger pipe.\n\nThe larger pipe was glass, so the behaviour of the layer of dyed flow could be observed, and at the end of this pipe was a flow-control valve used to vary the water velocity inside the tube. When the velocity was low, the dyed layer remained distinct through the entire length of the large tube. When the velocity was increased, the layer broke up at a given point and diffused throughout the fluid's cross-section. The point at which this happened was the transition point from laminar to turbulent flow. Reynolds identified the governing parameter for the onset of this effect, which was a dimensionless constant later called the Reynolds number.\n\nReynolds found that the transition occurred between Re = 2000 and 13000, depending on the smoothness of the entry conditions. When extreme care is taken, the transition can even happen with Re as high as 40000. On the other hand, Re = 2000 appears to be about the lowest value obtained at a rough entrance.\n\nReynolds' publications in fluid dynamics began in the early 1870s. His final theoretical model published in the mid-1890s is still the standard mathematical framework used today. Examples of titles from his more groundbreaking reports are:\n\nA boundary layer can transition to turbulence through a number of paths. Which path is realized physically depends on the initial conditions such as initial disturbance amplitude and surface roughness. The level of understanding of each phase varies greatly, from near complete understanding of primary mode growth to a near-complete lack of understanding of bypass mechanisms.\n\nThe initial stage of the natural transition process is known as the Receptivity phase and consists of the transformation of environmental disturbances – both acoustic (sound) and vortical (turbulence) – into small perturbations within the boundary layer. The mechanisms by which these disturbances arise are varied and include freestream sound and/or turbulence interacting with surface curvature, shape discontinuities and surface roughness. These initial conditions are small, often unmeasurable perturbations to the basic state flow. From here, the growth (or decay) of these disturbances depends on the nature of the disturbance and the nature of the basic state. Acoustic disturbances tend to excite two-dimensional instabilities such as Tollmien–Schlichting waves (T-S waves), while vortical disturbances tend to lead to the growth of three-dimensional phenomena such as the crossflow instability.\n\nNumerous experiments in recent decades have revealed that the extent of the amplification region, and hence the location of the transition point on the body surface, is strongly dependent not only upon the amplitude and/or the spectrum of external disturbances but also on their physical nature. Some of the disturbances easily penetrate into the boundary layer whilst others do not. Consequently, the concept of boundary layer transition is a complex one and still lacks a complete theoretical exposition.\n\nIf the initial, environmentally-generated disturbance is small enough, the next stage of the transition process is that of primary mode growth. In this stage, the initial disturbances grow (or decay) in a manner described by linear stability theory. The specific instabilities that are exhibited in reality depend on the geometry of the problem and the nature and amplitude of initial disturbances. Across a range of Reynolds numbers in a given flow configuration, the most amplified modes can and often do vary.\n\nThere are several major types of instability which commonly occur in boundary layers. In subsonic and early supersonic flows, the dominant two-dimensional instabilities are T-S waves. For flows in which a three-dimensional boundary layer develops such as a swept wing, the crossflow instability becomes important. For flows navigating concave surface curvature, Görtler vortices may become the dominant instability. Each instability has its own physical origins and its own set of control strategies - some of which are contraindicated by other instabilities – adding to the difficulty in controlling laminar-turbulent transition.\n\nSimple harmonic sound as a precipitating factor in the sudden transition from laminar to turbulent flow might be attributed to Elizabeth Barrett Browning. Her poem, Aurora Leigh (1856), revealed how musical notes (the pealing of a particular church bell), triggered wavering turbulence in the previously steady laminar-flow flames of street gaslights (“...gaslights tremble in the streets and squares”: Hair 2016). Her instantly acclaimed poem might have alerted scientists (e.g., Leconte 1859) to the influence of simple harmonic (SH) sound as a cause of turbulence. A contemporary flurry of scientific interest in this effect culminated in Sir John Tyndall (1867) deducing that specific SH sounds, directed perpendicular to the flow had waves that blended with similar SH waves created by friction along the boundaries of tubes, amplifying them and triggering the phenomenon of high-resistance turbulent flow. His interpretation re-surfaced over 100 years later (Hamilton 2015).\n\nTollmien (1931) and Schlichting (1929) proposed that friction (viscosity) along a smooth flat boundary, created SH boundary layer (BL) oscillations that gradually increased in amplitude until turbulence erupted. Although contemporary wind tunnels failed to confirm the theory, Schubauer and Skramstad (1943) created a refined wind tunnel that deadened the vibrations and sounds that might impinge on the wind tunnel flat plate flow studies. They confirmed the development of SH long-crested BL oscillations, the dynamic shear waves of transition to turbulence. They showed that specific SH fluttering vibrations induced electromagnetically into a BL ferromagnetic ribbon could amplify similar flow-induced SH BL flutter (BLF) waves, precipitating turbulence at much lower flow rates. Furthermore, certain other specific frequencies interfered with the development of the SH BLF waves, preserving laminar flow to higher flow rates.\n\nAn oscillation of a mass in a fluid is a vibration that creates a sound wave. SH BLF oscillations in boundary layer fluid along a flat plate must produce SH sound that reflects off the boundary perpendicular to the fluid laminae. In late transition, Schubauer and Skramstad found foci of amplification of BL oscillations, associated with bursts of noise (“turbulent spots”). Focal amplification of the transverse sound in late transition was associated with BL vortex formation.\n\nThe focal amplified sound of turbulent spots along a flat plate with high energy oscillation of molecules perpendicularly through the laminae, might suddenly cause localized freezing of laminar slip. The sudden braking of “frozen” spots of fluid would transfer resistance to the high resistance at the boundary, and might explain the head-over-heels BL vortices of late transition. Osborne Reynolds described similar turbulent spots during transition in water flow in cylinders (“flashes of turbulence,” 1883).\n\nWhen many random vortices erupt as turbulence onsets, the generalized freezing of laminar slip (laminar interlocking) is associated with noise and a dramatic increase in resistance to flow. This might also explain the parabolic isovelocity profile of laminar flow abruptly changing to the flattened profile of turbulent flow – as laminar slip is replaced by laminar interlocking as turbulence erupts (Hamilton 2015).\n\nThe primary modes themselves don't actually lead directly to breakdown, but instead lead to the formation of secondary instability mechanisms. As the primary modes grow and distort the mean flow, they begin to exhibit nonlinearities and linear theory no longer applies. Complicating the matter is the growing distortion of the mean flow, which can lead to inflection points in the velocity profile a situation shown by Lord Rayleigh to indicate absolute instability in a boundary layer. These secondary instabilities lead rapidly to breakdown. These secondary instabilities are often much higher in frequency than their linear precursors.\n"}
{"id": "1536956", "url": "https://en.wikipedia.org/wiki?curid=1536956", "title": "Lineworker", "text": "Lineworker\n\nA lineworker (lineman (American English), linesman (British English), powerline technician (PLT), or powerline worker) is a tradesperson who constructs and maintains electric power transmission, telecommunications lines (cable, internet and phone) and distribution lines. \n\nA lineworker generally does outdoor installation and maintenance jobs. Those who install and maintain electrical wiring inside buildings are electricians.\n\nThe occupation began with the widespread use of the telegraph in the 1840s. Telegraph lines could be strung on trees, but wooden poles were quickly adopted as the method of choice. The term 'lineman' was used for those who set wooden poles and strung the wire. The term continued in use with the invention of the telephone in the 1870s and the beginnings of electrification in the 1890s.\n\nThis new electrical power work was more hazardous than telegraph or telephone work because of the risk of electrocution. Between the 1890s and the 1930s, line work was considered one of the most hazardous jobs. This led to the formation of labor organizations to represent the workers and advocate for their safety. This also led to the establishment of apprenticeship programs and the establishment of more stringent safety standards, starting in the late 1930s. The union movement in the United States was led by lineman Henry Miller, who in 1890 was elected president of the Electrical Wiremen and Linemen's Union, No. 5221 of the American Federation of Labor.\n\nThe rural electrification drive during the New Deal led to a wide expansion in the number of jobs in the electric power industry. Many power linemen during that period traveled around the country following jobs as they became available in tower construction, substation construction, and wire stringing. They often lived in temporary camps set up near the project they were working on, or in boarding houses if the work was in a town or city, and relocating every few weeks or months. The occupation was lucrative at the time, but the hazards and the extensive travel limited its appeal.\n\nA brief drive to electrify some railroads on the East Coast of the US led to the development of specialization of linemen who installed and maintained catenary overhead lines. Growth in this branch of line work declined after most railroads favored diesel over electric engines for replacement to steam engines.\n\nThe occupation evolved during the 1940s and 1950s with expansion of residential electrification. This led to an increase in the number of linemen needed to maintain power distribution circuits and provide emergency repairs. Maintenance linemen mostly stayed in one place, although sometimes they were called to travel to assist repairs. During the 1950s, some electric lines began to be installed in underground tunnels, expanding the scope of the work.\n\nPower linemen work on electrically energized (live) and de-energized (dead) power lines. They may perform a number of tasks associated with power lines, including installation or replacement of distribution equipment such as capacitor banks, distribution transformers on poles, insulators and fuses. These duties include the use of ropes, knots, and lifting equipment. These tasks may have to be performed with primitive manual tools where accessibility is limited. Such conditions are common in rural or mountainous areas that are inaccessible to trucks.\n\nHigh voltage transmission lines can be worked live with proper setups. The lineman must be isolated from the ground. The lineman wears special conductive clothing that is connected to the live power line, at which point the line and the lineman are at the same potential, allowing the lineman to handle the wire. The lineman may still be electrocuted if he completes an electrical circuit, for example by handling both ends of a broken conductor. Such work is often done by helicopter by specially trained linemen. Isolated line work is only used for transmission-level voltages and sometimes for the higher distribution voltages. Live wire work is common on low voltage distribution systems within the UK and Australia as all linesmen are trained to work 'live'. Live wire work on high voltage distribution systems within the UK and Australia is carried out by specialist teams.\n\nWork on outdoor tower construction or wire installation are not performed exclusively by linemen. A crew of linemen will include several helpers. Helpers assist with on-the-ground tasks needed to support the linemen, but may work above ground or on electrical circuits. \n\nBecoming a lineworker usually involves starting as an apprentice and a four-year training program before becoming a \"Journey Lineworker\". Apprentice linemen are trained in all types of work from operating equipment and climbing to proper techniques and safety standards. Schools throughout the United States offer a pre-apprentice lineman training program such as Southeast Lineman Training Center and Northwest Lineman College.\n\nLineworkers, especially those who deal with live electrical apparatus, use personal protective equipment (PPE) as protection against inadvertent contact. This includes rubber gloves, rubber sleeves, bucket liners and protective blankets.\n\nWhen working with energized power lines, linemen must use protection to eliminate any contact with the energized line. The requirements for PPEs and associated permissible voltage depends on applicable regulations in jurisdiction as well as company policy. Voltages higher than those that can be worked using gloves are worked with special sticks known as hot-line tools or hot sticks, with which power lines can be safely handled from a distance. Linemen must also wear special rubber insulating gear when working with live wires to protect against any accidental contact with the wire. The buckets linemen sometimes work from are also insulated with fiberglass.\n\nDe-energized power lines can be hazardous as they can still be energized from another source such as interconnection or interaction with another circuit even when they appear shut off. For example: A higher-voltage distribution level circuit may feed several lower-voltage distribution circuits through transformers. If the higher voltage circuit is de-energized, but if lower-voltage circuits connected remain energized, the higher voltage circuit will remain energized. Another problem can arise when de-energized wires become energized through electrostatic or electromagnetic induction from energized wires in close proximity.\n\nAll live line work PPE must be kept clean from contaminants and regularly tested for di-electric integrity. This is done by the use of high voltage electrical testing equipment.\n\nOther general items of PPE such as helmets are usually replaced at regular intervals.\n\n\n\n"}
{"id": "41138208", "url": "https://en.wikipedia.org/wiki?curid=41138208", "title": "List of Galium species", "text": "List of Galium species\n\nThe genus \"Galium\" (Rubiaceae) contains around 650 species, making it one of the largest genera of flowering plants.\n\n\n\n\n"}
{"id": "25512467", "url": "https://en.wikipedia.org/wiki?curid=25512467", "title": "List of Superfund sites in Washington (state)", "text": "List of Superfund sites in Washington (state)\n\nThis is a list of Superfund sites in Washington State designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. \n\nThese locations are known as Superfund sites, and are placed on the National Priorities List (NPL). The NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of May 1, 2010, there were 48 Superfund sites on the National Priorities List in Washington. Seventeen others have been cleaned up and removed from the list; no sites are currently proposed for addition.\n\n\n"}
{"id": "25519622", "url": "https://en.wikipedia.org/wiki?curid=25519622", "title": "List of Superfund sites in Wisconsin", "text": "List of Superfund sites in Wisconsin\n\nThis is a list of Superfund sites in Wisconsin designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of March 26, 2010, there were 38 Superfund sites on the National Priorities List in Wisconsin. One additional site has been proposed for entry on the list. Six sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "15563334", "url": "https://en.wikipedia.org/wiki?curid=15563334", "title": "List of ecoregions in Angola", "text": "List of ecoregions in Angola\n\nThe following is a list of ecoregions in Angola, according to the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\n\n\n\n\n\"by bioregion\"\n\n\n\n\n\n\n"}
{"id": "26188539", "url": "https://en.wikipedia.org/wiki?curid=26188539", "title": "List of supernova candidates", "text": "List of supernova candidates\n\nThis is a list of supernova candidates, or stars that astronomers have suggested are supernova progenitors. Type II supernova progenitors include stars with at least 10 solar masses that are in the final stages of their evolution. Prominent examples of stars in this mass range include Antares, Spica, Gamma Velorum, Mu Cephei, and members of the Quintuplet Cluster. Type Ia supernova progenitors are white dwarf stars that are close to the Chandrasekhar limit of about 1.44 solar masses and are accreting matter from a binary companion star. The list includes massive Wolf–Rayet stars, which may become Type Ib/Ic supernovae.\n"}
{"id": "24159290", "url": "https://en.wikipedia.org/wiki?curid=24159290", "title": "Measurement of biodiversity", "text": "Measurement of biodiversity\n\nConservation biologists have designed a variety of objective means to measure biodiversity empirically. Each measure of biodiversity relates to a particular use of the data. For practical conservationists, measurements should include . For others, a more economically defensible definition should allow the ensuring of continued possibilities for both adaptation and future use by humans, assuring environmental sustainability.\n\nAs a consequence, biologists argue that this measure is likely to be associated with the variety of genes. Since it cannot always be said which genes are more likely to prove beneficial, the best choice for conservation is to assure the persistence of as many genes as possible. For ecologists, this latter approach is sometimes considered too restrictive, as it prohibits ecological succession.\n\nBiodiversity is usually plotted as taxonomic richness of a geographic area, with some reference to a temporal scale. Whittaker described three common metrics used to measure species-level biodiversity, encompassing attention to species richness or species evenness:\n\nRecently, another new index has been invented called the Mean Species Abundance Index (MSA); this index calculates the trend in population size of a cross section of the\nspecies. It does this in line with the CBD 2010 indicator for species abundance.\n\nAlternatively, other types of diversity may be plotted against a temporal timescale:\n\nThese different types of diversity may not be independent. There is, for example, a close link between vertebrate taxonomic and ecological diversity.\n\nOther authors tried to organize the measurements of biodiversity in the following way:\n\nDiversity may be measured at different scales. These are three indices used by ecologists: \n"}
{"id": "57463049", "url": "https://en.wikipedia.org/wiki?curid=57463049", "title": "Michael Harwood (author)", "text": "Michael Harwood (author)\n\nMichael Harwood (1934, Boston – 24 November 1989, San Diego) was a naturalist, environmentalist, and author.\n\nHarwood received his secondary education from The Putney School in Vermont and graduated from Harvard University in 1956. He became the third husband of the author Mary B. Durant in 1966.\n\nHe was a co-winner of the 1981 John Burroughs Medal for \"On the Road With John James Audubon\", which he co-authored with his wife Mary Durant. The book is organized as a travel journal which recounts how the two authors spent more than a year camping along the various North American itineraries recorded in Audubon's journals.\n\nHarwood attended the 1974 foundational meeting and was a president of the Hawk Migration Association of North America. He was a member of the board of the Hawk Mountain Sanctuary at Kempton, Pennsylvania.\n\n"}
{"id": "1117836", "url": "https://en.wikipedia.org/wiki?curid=1117836", "title": "Modernization theory", "text": "Modernization theory\n\nModernization theory is used to explain the process of modernization within societies. Modernization refers to a model of a progressive transition from a 'pre-modern' or 'traditional' to a 'modern' society. Modernization theory originated from the ideas of German sociologist Max Weber (1864–1920), which provided the basis for the modernization paradigm developed by Harvard sociologist Talcott Parsons (1902–1979). The theory looks at the internal factors of a country while assuming that with assistance, \"traditional\" countries can be brought to development in the same manner more developed countries have been. Modernization theory was a dominant paradigm in the social sciences in the 1950s and 1960s, then went into a deep eclipse. It made a comeback after 1991 but remains a controversial model.\n\nModernization theory both attempts to identify the social variables that contribute to social progress and development of societies and seeks to explain the process of social evolution. Modernization theory is subject to criticism originating among socialist and free-market ideologies, world-systems theorists, globalization theorists and dependency theorists among others. Modernization theory stresses not only the process of change but also the responses to that change. It also looks at internal dynamics while referring to social and cultural structures and the adaptation of new technologies.\nModernization theory maintains that traditional societies will develop as they adopt more modern practices. Proponents of modernization theory claim that modern states are wealthier and more powerful and that their citizens are freer to enjoy a higher standard of living. Developments such as new data technology and the need to update traditional methods in transport, communication and production, it is argued, make modernization necessary or at least preferable to the status quo. That view makes critique of modern difficult since it implies that such developments control the limits of human interaction, not vice versa. It also implies that human agency controls the speed and severity of modernization. Supposedly, instead of being dominated by tradition, societies undergoing the process of modernization typically arrive at forms of governance dictated by abstract principles. Traditional religious beliefs and cultural traits, according to the theory, usually become less important as modernization takes hold.\n\nHistorians link modernization to the processes of urbanization and industrialization and the spread of education. As Kendall (2007) notes, \"Urbanization accompanied modernization and the rapid process of industrialization.\" In sociological critical theory, modernization is linked to an overarching process of rationalisation. When modernization increases within a society, the individual becomes increasingly important, eventually replacing the family or community as the fundamental unit of society.\n\nSociological theories of the late 19th century such as Social Darwinism provided a basis for asking what were the laws of evolution of human society. The current modernization theory originated with the ideas of German sociologist Max Weber (1864–1920) regarding the role of rationality and irrationality in the transition from traditional to modern society. Weber's approach provided the basis for the modernization paradigm as popularized by Harvard sociologist Talcott Parsons (1902–1979), who translated Weber's works into English in the 1930s and provided his own interpretation.\n\nAfter 1945 the Parsonian version became widely used in sociology and other social sciences. By the late 1960s opposition developed because the theory was too general and did not fit all societies in quite the same way.\n\nGlobalization can be defined as the integration of economic, political and social cultures. It is argued that globalization is related to the spreading of modernization across borders.\n\nGlobal trade has grown continuously since the European discovery of new continents in the Early modern period; it increased particularly as a result of the Industrial Revolution and the mid-20th century adoption of the shipping container.\n\nAnnual trans-border tourist arrivals rose to 456 million by 1990 and almost tripled since, reaching a total of over 1.2 billion in 2016. Communication is another major area that has grown due to modernization. Communication industries have enabled capitalism to spread throughout the world. Telephony, television broadcasts, news services and online service providers have played a crucial part in globalization. Former U.S president Lyndon B. Johnson was a supporter of the modernization theory and believed that television had potential to provide educational tools in development.\n\nWith the many apparent positive attributes to globalization there are also negative consequences. The dominant, neoliberal model of globalization often increases disparities between a society's rich and its poor. In major cities of developing countries there exist pockets where technologies of the modernised world, computers, cell phones and satellite television, exist alongside stark poverty. Globalists are globalization modernization theorists and argue that globalization is positive for everyone, as its benefits must eventually extend to all members of society, including vulnerable groups such as women and children.\n\nThe relationship between modernization and democracy is one of the most researched studies in comparative politics. There is academic debate over the drivers of democracy because there are theories that support economic growth as both a cause and effect of the institution of democracy. “Lipset’s observation that democracy is related to economic development, first advanced in 1959, has generated the largest body of research on any topic in comparative politics,” (Przeworski and Limongi, 1997).\n\nLarry Diamond and Juan Linz, who worked with Lipset in the book, \"Democracy in Developing Countries: Latin America\", argue that economic performance affects the development of democracy in at least three ways. First, they argue that economic growth is more important for democracy than given levels of socioeconomic development. Second, socioeconomic development generates social changes that can potentially facilitate democratization. Third, socioeconomic development promotes other changes, like organization of the middle class, which is conducive to democracy.\n\nAs Seymour Martin Lipset put it, \"All the various aspects of economic development — industrialization, urbanization, wealth and education — are so closely interrelated as to form one major factor which has the political correlate of democracy\". The argument also appears in Walt W. Rostow, \"Politics and the Stages of Growth\" (1971); A. F. K. Organski, \"The Stages of Political Development\" (1965); and David Apter, \"The Politics of Modernization\" (1965). In the 1960s, some critics argued that the link between modernization and democracy was based too much on the example of European history and neglected the Third World. Recent demonstrations of the emergence of democracy in South Korea, Taiwan and South Africa have been cited as support for Lipset's thesis.\n\nOne historical problem with that argument has always been Germany whose economic modernization in the 19th century came long before the democratization after 1918. Berman, however, concludes that a process of democratization was underway in Imperial Germany, for \"during these years Germans developed many of the habits and mores that are now thought by political scientists to augur healthy political development\".\n\nRonald Inglehart and Christian Welzel (2009) contend that the realization of democracy is not based solely on an expressed desire for that form of government, but democracies are born as a result of the admixture of certain social and cultural factors. They argue the ideal social and cultural conditions for the foundation of a democracy are born of significant modernization and economic development that result in mass political participation.\n\nPeerenboom (2008) explores the relationships among democracy, the rule of law and their relationship to wealth by pointing to examples of Asian countries, such as Taiwan and South Korea, which have successfully democratized only after economic growth reached relatively high levels and to examples of countries such as the Philippines, Bangladesh, Cambodia, Thailand, Indonesia and India, which sought to democratize at lower levels of wealth but have not done as well.\n\nAdam Przeworski and others have challenged Lipset's argument. They say political regimes do not transition to democracy as per capita incomes rise. Rather, democratic transitions occur randomly, but once there, countries with higher levels of gross domestic product per capita remain democratic. Epstein et al. (2006) retest the modernization hypothesis using new data, new techniques, and a three-way, rather than dichotomous, classification of regimes. Contrary to Przeworski, this study finds that the modernization hypothesis stands up well. Partial democracies emerge as among the most important and least understood regime types.\n\nHighly contentious is the idea that modernization implies more human rights, with China in the 21st century being a major test case.\n\nNew technology is a major source of social change. (Social change refers to any significant alteration over time in behavior patterns and cultural values and norms.) Since modernization entails the social transformation from agrarian societies to industrial ones, it is important to look at the technological viewpoint; however, new technologies do not change societies by itself. Rather, it is the \"response\" to technology that causes change. Frequently, technology is recognized but not put to use for a very long time such as the ability to extract metal from rock. Although that initially went unused, it later had profound implications for the developmental course of societies. Technology makes it possible for a more innovated society and broad social change. That dramatic change through the centuries that has evolved socially, industrially, and economically, can be summed up by the term modernization. Cell phones, for example, have changed the lives of millions throughout the world. That is especially true in Africa and other parts of the Middle East, where there is a low cost communication infrastructure. With cell phone technology, widely dispersed populations are connected, which facilitates business-to-business communication and provides internet access to remoter areas, with a consequential rise in literacy.\n\nDevelopment, like modernization, has become the orienting principle of modern times. Countries that are seen as modern are also seen as developed, which means that they are generally more respected by institutions such as the United Nations and even as possible trade partners for other countries. The extent to which a country has modernized or developed dictates its power and importance on the international level.\n\nModernization of the health sector of developing nations recognizes that transitioning from 'traditional' to 'modern' is not merely the advancement in technology and the introduction of Western practices; implementing modern healthcare requires the reorganization of political agenda and, in turn, an increase in funding by feeders and resources towards public health. However, rather than replicating the stages of developed nations, whose roots of modernization are found with the context of industrialization or colonialism, underdeveloped nations should apply proximal interventions to target rural communities and focus on prevention strategies rather than curative solutions. That has been successfully exhibited by the Christian Medical Commission and in China through 'barefoot doctors'. Additionally, a strong advocate of the DE-emphasis of medical institutions was Halfdan T. Mahler, the WHO General Director from 1973 to 1988. Related ideas have been proposed at international conferences such as Alma-Ats and the \"Health and Population in Development\" conference, sponsored by the Rockefeller Foundation in Italy in 1979, and selective primary healthcare and GOBI were discussed (although they have both been strongly criticized by supporters of comprehensive healthcare). Overall, however, this is not to say that the nations of the Global South can function independently from Western states; significant funding is received from well-intention programs, foundations, and charities that target epidemics such as HIV/AIDS, malaria, and tuberculosis that have substantially improved the lives of millions of people and impeded future development.\n\nModernization theorists often saw traditions as obstacles to economic growth. According to Seymour Martin Lipset, economic conditions are heavily determined by the cultural, social values present in that given society. Furthermore, while modernization might deliver violent, radical change for traditional societies, it was thought worth the price. Critics insist that traditional societies were often destroyed without ever gaining the promised advantages if, among other things, the economic gap between advanced societies and such societies actually increased. The net effect of modernization for some societies was therefore the replacement of traditional poverty by a more modern form of misery, according to these critics. Others point to improvements in living standards, physical infrastructure, education and economic opportunity to refute such criticisms.\n\nFrom the 1960s, modernization theory has been criticized by numerous scholars, including Andre Gunder Frank (1929 – 2005) and Immanuel Wallerstein (born 1930). In this model, the modernization of a society required the destruction of the indigenous culture and its replacement by a more Westernized one. By one definition, \"modern\" simply refers to the present, and any society still in existence is therefore modern. Proponents of modernization typically view only Western society as being truly modern and argue that others are primitive or unevolved by comparison. That view sees unmodernized societies as inferior even if they have the same standard of living as western societies. Opponents argue that modernity is independent of culture and can be adapted to any society. Japan is cited as an example by both sides. Some see it as proof that a thoroughly modern way of life can exist in a non western society. Others argue that Japan has become distinctly more western as a result of its modernization.\n\nAs Tipps has argued, by conflating modernization with other processes, with which theorists use interchangeably (democratization, liberalization, development), the term becomes imprecise and therefore difficult to disprove.\n\nThe theory has also been criticised empirically, as modernization theorists ignore external sources of change in societies. The binary between traditional and modern is unhelpful, as the two are linked and often interdependent, and 'modernization' does not come as a whole.\n\nModernization theory has also been accused of being Eurocentric, as modernization began in Europe, with the Industrial Revolution, the French Revolution and the Revolutions of 1848 (Macionis 953) and has long been regarded as reaching its most advanced stage in Europe. Anthropologists typically make their criticism one step further and say that the view is ethnocentric and is specific to Western culture.\n\nOne alternative model on the left is Dependency theory. It emerged in the 1950s and argues that the underdevelopment of poor nations in the Third World derived from systematic imperial and neo-colonial exploitation of raw materials. Its proponents argue that resources typically flow from a \"periphery\" of poor and underdeveloped states to a \"core\" of wealthy states, enriching the latter at the expense of the former. It is a central contention of dependency theorists such as Andre Gunder Frank that poor states are impoverished and rich ones enriched by the way poor states are integrated into the \"world system\".\n\nDependency models arose from a growing association of southern hemisphere nationalists (from Latin America and Africa) and Marxists. It was their reaction against modernization theory, which held that all societies progress through similar stages of development, that today's underdeveloped areas are thus in a similar situation to that of today's developed areas at some time in the past, and that, therefore, the task of helping the underdeveloped areas out of poverty is to accelerate them along this supposed common path of development, by various means such as investment, technology transfers, and closer integration into the world market. Dependency theory rejected this view, arguing that underdeveloped countries are not merely primitive versions of developed countries, but have unique features and structures of their own; and, importantly, are in the situation of being the weaker members in a world market economy.\n\n"}
{"id": "2291190", "url": "https://en.wikipedia.org/wiki?curid=2291190", "title": "Natrocarbonatite", "text": "Natrocarbonatite\n\nNatrocarbonatite is a rare carbonatite lava which erupts from the Ol Doinyo Lengai volcano in Tanzania within the East African Rift of eastern Africa.\n\nWhereas most lavas are rich in silicate minerals, the natrocarbonatite lavas of Ol Doinyo Lengai are rich in the rare sodium and potassium carbonate minerals, nyerereite and gregoryite. \n\nDue to this unusual composition, the lava is erupted at relatively low temperatures (approximately 500-600 °C). This temperature is so low that the molten lava appears black in sunlight, rather than having the red glow common to most lavas. It is also much more fluid than silicate lavas.\n\nThe sodium and potassium carbonate minerals of the lavas erupted at Ol Doinyo Lengai are unstable at the Earth's surface and susceptible to rapid weathering, the minerals are anhydrous and when they come into contact with the moisture of the atmosphere, they begin to react extremely quickly. \n\nThe black or dark brown lava and ash erupted begins to turn white within a few hours. The resulting volcanic landscape is different from any other in the world.\n\nNotable appearances\n\n"}
{"id": "169889", "url": "https://en.wikipedia.org/wiki?curid=169889", "title": "Paleoproterozoic", "text": "Paleoproterozoic\n\nPaleoproterozoic Era (;), spanning the time period from (2.5–1.6 Ga), is the first of the three sub-divisions (eras) of the Proterozoic Eon. The Paleoproterozoic is also the longest era of the Earth's geological history. It was during this era that the continents first stabilized.\n\nPaleontological evidence suggests that the Earth's rotational rate during this era resulted in 20-hour days ~1.8 billion years ago, implying a total of ~450 days per year.\n\nBefore the enormous increase in atmospheric oxygen, almost all existing lifeforms were anaerobic, i.e., their metabolism was based upon a form of cellular respiration that did not require oxygen. Indeed, free oxygen in large amounts is toxic to most anaerobic organisms. Consequently, the majority of the anaerobic lifeforms on Earth died when the atmospheric free-oxygen levels soared. The only lifeforms that survived were either those resistant to the oxidizing and poisonous effects of oxygen, or those sequestered in oxygen-free environments. The sudden increase of atmospheric free oxygen and the ensuing extinction of the vulnerable lifeforms (an event called, among numerous other similarly suggestive titles, the Oxygen Holocaust or Oxygen Catastrophe) is widely considered to be the first of the most significant mass extinctions in the history of the Earth.\n\nMany crown node eukaryotes (from which the modern-day eukaryotic lineages would have arisen)—or the divergences that imply them between various groups of eukaryotes—have been ostensibly dated to around the time of the Paleoproterozoic era. However, these conclusions (as is the case in virtually any contemporaneously \"hot\" areas of biological study and research) are likely to be readjusted—if not outright abandoned—as more data become available, and should not be considered conclusive proof by any means. Nevertheless, given the number of and the peer respect assigned to many of the authors of these studies (and related analyses corroborating the validity of the methodologies used by those studies ...even though those very analyses are themselves also sometimes called into question), the final revisions will likely place the emergence of the oldest eukaryotic divergences around this period of time.\n\nDuring this era, the earliest global-scale continent-continent collision belts developed.\n\nThese continent and mountain building events are represented by the 2.1–2.0 Ga Trans-Amazonian and Eburnean orogens in South America and West Africa; the ~2.0 Ga Limpopo Belt in southern Africa; the 1.9–1.8 Ga Trans-Hudson, Penokean, Taltson–Thelon, Wopmay, Ungava and Torngat orogens in North America, the 1.9–1.8 Ga Nagssugtoqidain Orogen in Greenland; the 1.9–1.8 Ga Kola–Karelia, Svecofennian, Volhyn-Central Russian, and Pachelma orogens in Baltica (Eastern Europe); the 1.9–1.8 Ga Akitkan Orogen in Siberia; the ~1.95 Ga Khondalite Belt and ~1.85 Ga Trans-North China Orogen in North China.\n\nThese continental collision belts are interpreted as having resulted from one or more 2.0–1.8 Ga global-scale collision events that then led to the assembly of a Proterozoic supercontinent named Columbia or Nuna.\n\nThe lithospheric mantle of Patagonia's oldest blocks formed.\n\n(Impact events)\n\n"}
{"id": "11570540", "url": "https://en.wikipedia.org/wiki?curid=11570540", "title": "Phaeolus schweinitzii", "text": "Phaeolus schweinitzii\n\nPhaeolus schweinitzii, commonly known as velvet-top fungus, dyer's polypore, or dyer's mazegill, is a fungal plant pathogen that causes butt rot on conifers such as Douglas-fir, spruce, fir, hemlock, pine, and larch. \"P. schweinitzii\" is a polypore, although unlike bracket fungi the fruiting body may appear terrestrial when growing from the roots or base of the host tree. The fruiting bodies, appearing in late summer or fall, commonly incorporate blades of grass, twigs, or fallen pine needles as they grow. As these fruiting bodies age, the pore surface turns from yellow to greenish yellow, the top becomes darker, and the flesh becomes harder and more wood-like.\n\nThe effect, impact and significance of infection by this fungus is rooted in the fact that it causes brown rot, which degrades the cellulose. Thus there is a loss of tensile strength which often leads to brittle fracture near the stem base, even at a fairly early stage of decay. Decay initiated above ground can lead to branch snap or breakout.\n\n\"P. schweinitzii\" is native to North America and Eurasia, and has been identified as an exotic species in New Zealand, Australia, and South Africa.\n\nAs its common name suggests, the dyer's polypore is an excellent natural source of green, yellow, gold, or brown dye, depending on the material dyed and the mordant used.\n\n\"P. schweinitzii\" is named after Lewis David de Schweinitz, a Pennsylvania-born Moravian minister and important early American mycologist.\n"}
{"id": "2174083", "url": "https://en.wikipedia.org/wiki?curid=2174083", "title": "Power-system automation", "text": "Power-system automation\n\nPower-system automation is the act of automatically controlling the power system via instrumentation and control devices. Substation automation refers to using data from Intelligent electronic devices (IED), control and automation capabilities within the substation, and control commands from remote users to control power-system devices.\n\nSince full substation automation relies on substation integration, the terms are often used interchangeably. Power-system automation includes processes associated with generation and delivery of power. Monitoring and control of power delivery systems in the substation and on the pole reduce the occurrence of outages and shorten the duration of outages that do occur. The IEDs, communications protocols, and communications methods, work together as a system to perform power-system automation.\nThe term “power system” describes the collection of devices that make up the physical systems that generate, transmit, and distribute power. The term “instrumentation and control (I&C) system” refers to the collection of devices that monitor, control, and protect the power system.\n\nPower-system automation is composed of several tasks. \nIn addition, another task is power-system integration, which is the act of communicating data to, from, or among IEDs in the I&C system and remote users. Substation integration refers to combining data from the IED’s local to a substation so that there is a single point of contact in the substation for all of the I&C data.\n\nPower-system automation processes rely on data acquisition; power-system supervision and power-system control all working together in a coordinated automatic fashion. The commands are generated automatically and then transmitted in the same fashion as operator initiated commands.\n\nThe instrument transformers with protective relays are used to sense the power-system voltage and current. They are physically connected to power-system apparatus and convert the actual power-system signals. The transducers convert the analog output of an instrument transformer from one magnitude to another or from one value type to another, such as from an ac current to dc voltage. Also the input data is taken from the auxiliary contacts of switch gears and power-system control equipment.\n\nThe I&C devices built using microprocessors are commonly referred to as intelligent electronic devices (IEDs). Microprocessors are single chip computers that allow the devices into which they are built to process data, accept commands, and communicate information like a computer. Automatic processes can be run in the IEDs. Some IEDs used in power-system automation are:\n\n\n\nAll lines and all electrical equipment must be protected against prolonged overcurrent. If the cause of the overcurrent is nearby then automatically that current is interrupted immediately. But if the cause of the overcurrent is outside the local area then a backup provision automatically disconnects all affected circuits after a suitable time delay.\n\nNote that disconnection can, unfortunately, have a cascade effect, leading to overcurrent in other circuits that then also must therefore disconnect automatically.\n\nAlso note that generators that suddenly have lost their load because of such a protection operation will have to shut down automatically immediately, and it may take many hours to restore a proper balance between demand and supply in the system, partly because there must be proper synchronization before any two parts of the system can be reconnected.\n\nReclosing operations of circuit breakers usually are attempted automatically, and often are successful during thunderstorms, for example.\n\nA \"supervisory control and data acquisition system\" (SCADA) transmits and receives logic or data from events of controls, metering, measuring, safety and monitoring of process devices such as Electrical equipment, Instrumentation devices, telecommunication on industrial applications. Power system elements ranging from pole-mounted switches to entire power plants can be controlled remotely over long distance communication links. Remote switching, telemetering of grids (showing voltage, current, power, direction, consumption in kWh, etc.), even automatic synchronization is used in some power systems.\n\nPower utility companies protect high voltage lines by monitoring them constantly. This supervision requires the transmission of information between the power substations in order to ensure correct operation while controlling every alarm and failure. Legacy telecom networks were interconnected with metallic wires, but the substation environment is characterized by a high level of electromagnetic fields that may disturb copper wires.\n\nAuthorities use a tele-protection scheme to enable substations to communicate with one another to selectively isolate faults on high voltage lines, transformers, reactors and other important elements of the electrical plants. This functionality requires the continuous exchange of critical data in order to assure correct operation. In order to warranty the operation the telecom network should always be in perfect conditions in terms of availability, performance, quality and delays.\n\nInitially these networks were made of metallic conductive media, however the vulnerability of the 56–64 kbit/s channels to electromagnetic interference, signal ground loops, and ground potential rise made them too unreliable for the power industry. Strong electromagnetic fields caused by the high voltages and currents in power lines occur regularly in electric substations.\n\nMoreover, during fault conditions electromagnetic perturbations may rise significantly and disturb those communications channels based on copper wires. The reliability of the communications link interconnecting the protection relays is critical and therefore must be resistant to effects encountered in high voltage areas, such as high frequency induction and ground potential rise.\n\nConsequently, the power industry moved to optical fibers to interconnect the different items installed in substations. Fiber optics need not be grounded and are immune to the interferences caused by electrical noise, eliminating many of the errors commonly seen with electrical connections. The use of fully optical links from power relays to multiplexers as described by IEEE C37.94 became standard.\n\nA more sophisticated architecture for the protection scheme emphasizes the notion of fault tolerant networks. Instead of using a direct relay connection and dedicated fibers, redundant connections make the protection process more reliable by increasing the availability of critical data interchanges.\n\nIEEE C37.94 , full title \"IEEE Standard for N Times 64 Kilobit Per Second Optical Fiber Interfaces Between Teleprotection and Multiplexer Equipment\", is an IEEE standard, published in 2002, that defines the rules to interconnect tele-protection and multiplexer devices of power utility companies. The standard defines a data frame format for optical interconnection, and references standards for the physical connector for multi-mode optical fiber. Furthermore, it defines behavior of connected equipment on failure of the link, and the timing and optical signal characteristics.\n\nTeleprotection systems must isolate faults very quickly to prevent damage to the network and power outages. The IEEE committee defined C37.94 as a programmable n x 64 kbit/s (n=1...12) multimode optical fiber interface to provide transparent communications between teleprotection relays and multiplexers for distances of up to 2 km. To reach longer distances, the power industry later adopted a single mode optical fiber interface as well.\n\nThe standard defines the protection and communications equipment inside a substation using optical fibers, the method for clock recovery, the jitter tolerances allowed in the signals, the physical connection method, and the actions the protection equipment must follow when any kind of network anomalies and faults occur. C37.94 was already implemented by many protection relay manufacturers such as ABB, SEL, RFL, and RAD. Teleprotection equipment once offered a choice of transmission interfaces, such as the IEEE C37.94 compliant optical fiber interface for transmission over fiber pairs, and G.703, 64kbit/s co-directional and E1 interfaces.\n\n"}
{"id": "51829", "url": "https://en.wikipedia.org/wiki?curid=51829", "title": "Rodinia", "text": "Rodinia\n\nRodinia (from the Russian родить, \"rodít\", meaning \"to beget, to give birth\", or родина, \"ródina\", meaning \"motherland, birthplace\") is a Neoproterozoic supercontinent that was assembled 1.1–0.9 billion years ago and broke up 750–633 million years ago.\n\nRodinia formed at c. 1.23 Ga by accretion and collision of fragments produced by breakup of an older supercontinent, Columbia, assembled by global-scale 2.0–1.8 Ga collisional events.\n\nRodinia broke up in the Neoproterozoic with its continental fragments reassembled to form Pannotia 633–573 million years ago. In contrast with Pannotia, little is known yet about the exact configuration and geodynamic history of Rodinia. Paleomagnetic evidence provides some clues to the paleolatitude of individual pieces of the Earth's crust, but not to their longitude, which geologists have pieced together by comparing similar geologic features, often now widely dispersed.\n\nThe extreme cooling of the global climate around 717–635 million years ago (the so-called Snowball Earth of the Cryogenian Period) and the rapid evolution of primitive life during the subsequent Ediacaran and Cambrian periods are thought to have been triggered by the breaking up of Rodinia or to a slowing down of tectonic processes.\n\nThe idea that a supercontinent existed in the early Neoproterozoic arose in the 1970s, when geologists determined that orogens of this age exist on virtually all cratons. Examples are the Grenville orogeny in North America and the Dalslandian orogeny in Europe.\n\nSince then, many alternative reconstructions have been proposed for the configuration of the cratons in this supercontinent. Most of these reconstructions are based on the correlation of the orogens on different cratons. Though the configuration of the core cratons in Rodinia is now reasonably well known, recent reconstructions still differ in many details. Geologists try to decrease the uncertainties by collecting geological and paleomagnetical data.\n\nMost reconstructions show Rodinia's core formed by the North American craton (the later paleocontinent of Laurentia), surrounded in the southeast with the East European craton (the later paleocontinent of Baltica), the Amazonian craton (\"Amazonia\") and the West African craton; in the south with the Río de la Plata and São Francisco cratons; in the southwest with the Congo and Kalahari cratons; and in the northeast with Australia, India and eastern Antarctica. The positions of Siberia and North and South China north of the North American craton differ strongly depending on the reconstruction:\n\nLittle is known about the paleogeography before the formation of Rodinia. Paleomagnetic and geologic data are only definite enough to form reconstructions from the breakup of Rodinia onwards. Rodinia is considered to have formed between 1.3 and 1.23 billion years ago and broke up again before 750 million years ago. Rodinia was surrounded by the superocean geologists are calling Mirovia (from Russian мировой, \"mirovoy\", meaning \"global\").\n\nAccording to J.D.A. Piper, Rodinia is one of two models for the configuration and history of the continental crust in the latter part of Precambrian times. The other is Paleopangea, Piper's own concept. Piper proposes an alternative hypothesis for this era and the previous ones. This idea rejects that Rodinia ever existed as a transient supercontinent subject to progressive break-up in the latter part of Proterozoic times and instead that this time and earlier times were dominated by a single, persistent \"Paleopangaea\" supercontinent. As evidence, he suggests an observation that the palaeomagnetic poles from the continental crust assigned to this time conform to a single path between 825 and 633 million years ago and latterly to a near-static position between 750 and 633 million years. This latter solution predicts that break-up was confined to the Ediacaran Period and produced the dramatic environmental changes that characterised the transition between Precambrian and Phanerozoic times.\n\nIn 2009 UNESCO's IGCP project 440, named 'Rodinia Assembly and Breakup', concluded that Rodinia broke up in four stages between 825–550 Ma:\n\nThe Rodinia hypothesis assumes that rifting did not start everywhere simultaneously. Extensive lava flows and volcanic eruptions of Neoproterozoic age are found on most continents, evidence for large scale rifting about 750 million years ago. As early as 850 and 800 million years ago, a rift developed between the continental masses of present-day Australia, East Antarctica, India and the Congo and Kalahari cratons on one side and later Laurentia, Baltica, Amazonia and the West African and Rio de la Plata cratons on the other. This rift developed into the Adamastor Ocean during the Ediacaran.\n\nAround 550 million years ago, on the boundary between the Ediacaran and Cambrian, the first group of cratons eventually fused again with Amazonia, West Africa and the Rio de la Plata cratons. This tectonic phase is called the Pan-African orogeny. It created a configuration of continents that would remain stable for hundreds of millions of years in the form of the continent Gondwana.\n\nIn a separate rifting event about 610 million years ago (halfway in the Ediacaran period), the Iapetus Ocean formed. The eastern part of this ocean formed between Baltica and Laurentia, the western part between Amazonia and Laurentia. Because the exact moments of this separation and the partially contemporaneous Pan-African orogeny are hard to correlate, it might be that all continental mass was again joined in one supercontinent between roughly 600 and 550 million years ago. This hypothetical supercontinent is called Pannotia.\n\nUnlike later supercontinents, Rodinia would have been entirely barren. Rodinia existed before complex life colonized dry land. Based on sedimentary rock analysis Rodinia's formation happened when the ozone layer was not as extensive as it is today. Ultraviolet light discouraged organisms from inhabiting its interior. Nevertheless, its existence did significantly influence the marine life of its time.\n\nIn the Cryogenian period the Earth experienced large glaciations, and temperatures were at least as cool as today. Substantial areas of Rodinia may have been covered by glaciers or the southern polar ice cap.\n\nLow temperatures may have been exaggerated during the early stages of continental rifting. Geothermal heating peaks in crust about to be rifted; and since warmer rocks are less dense, the crustal rocks rise up relative to their surroundings. This rising creates areas of higher altitude, where the air is cooler and ice is less likely to melt with changes in season, and it may explain the evidence of abundant glaciation in the Ediacaran period.\n\nThe eventual rifting of the continents created new oceans and seafloor spreading, which produces warmer, less dense oceanic lithosphere. Due to its lower density, hot oceanic lithosphere will not lie as deep as old, cool oceanic lithosphere. In periods with relatively large areas of new lithosphere, the ocean floors come up, causing the eustatic sea level to rise. The result was a greater number of shallower seas.\n\nThe increased evaporation from the larger water area of the oceans may have increased rainfall, which, in turn, increased the weathering of exposed rock. By inputting data on the ratio of stable isotopes O:O into computer models, it has been shown that, in conjunction with quick weathering of volcanic rock, this increased rainfall may have reduced greenhouse gas levels to below the threshold required to trigger the period of extreme glaciation known as Snowball Earth.\n\nIncreased volcanic activity also introduced into the marine environment biologically active nutrients, which may have played an important role in the development of the earliest animals.\n\n\n\n"}
{"id": "8590459", "url": "https://en.wikipedia.org/wiki?curid=8590459", "title": "Sahara pump theory", "text": "Sahara pump theory\n\nThe Sahara pump theory is a hypothesis that explains how flora and fauna migrated between Eurasia and Africa via a land bridge in the Levant region. It posits that extended periods of abundant rainfall lasting many thousands of years (pluvial periods) in Africa are associated with a \"wet-Sahara\" phase, during which larger lakes and more rivers existed. This caused changes in the flora and fauna found in the area. Migration along the river corridor was halted when, during a desert phase 1.8–0.8 million years ago (mya), the Nile ceased to flow completely and possibly flowed only temporarily in other periods due to the geologic uplift (Nubian Swell) of the Nile River region.\n\nDuring periods of a wet or \"Green Sahara\", the Sahara and Arabia become a savanna grassland and African flora and fauna become common. Following inter-pluvial arid periods, the Sahara area then reverts to desert conditions, usually as a result of the retreat of the West African Monsoon southwards. Evaporation exceeds precipitation, the level of water in lakes like Lake Chad falls, and rivers become dry wadis. Flora and fauna previously widespread as a result retreat northwards to the Atlas Mountains, southwards into West Africa, or eastwards into the Nile Valley and thence either southeast to the Ethiopian Highlands and Kenya or northeast across the Sinai into Asia. This separates populations of some of the species in areas with different climates, forcing them to adapt, possibly giving rise to allopatric speciation.\n\nThe Plio-Pleistocene migrations to Africa included the Caprinae in two waves at 3.2 Ma and 2.7–2.5 Ma; \"Nyctereutes\" at 2.5 Ma, and \"Equus\" at 2.3 Ma. \"Hippotragus\" migrated at 2.6 Ma from Africa to the Siwaliks of the Himalayas. Asian bovids moved to Europe and to and from Africa. The primate \"Theropithecus\" experienced contraction and its fossils are found only in Europe and Asia, while \"Homo\" and \"Macaca\" settled wide ranges.\n\nBetween about 133 and 122 thousand years ago (kya), the southern parts of the Saharan-Arabian Desert experienced the start of the Abbassia Pluvial, a wet period with increased monsoonal precipitation, around 200–100 mm/year. This allowed Eurasian biota to travel to Africa and vice versa. The growth of speleothems (which requires rainwater) was detected in Hol-Zakh, Ashalim, Even-Sid, Ma'ale-ha-Meyshar, Ktora Cracks, Nagev Tzavoa Cave. In Qafzeh and Es Skuhl caves, where at that time precipitation was 600–1000 mm/year, the remains of Qafzeh-Skhul type anatomically modern humans are dated from this period, but human occupation seems to end in the later arid period.\n\nThe Red Sea coastal route was extremely arid before 140 and after 115 kya. Slightly wetter conditions appear at 90–87 kya, but it still was just one tenth the rainfall around 125 kya. Speleothems are detected only in Even-Sid-2.\n\nIn the southern Negev Desert speleothems did not grow between 185–140 kya (MIS 6), 110–90 (MIS 5.4–5.2), nor after 85 kya nor during most of the interglacial period (MIS 5.1), the glacial period and Holocene. This suggests that the southern Negev was arid to hyper-arid in these periods.\n\nThe coastal route around the western Mediterranean may have been open at times during the last glacial; speleothems grew in Hol-Zakh and in Nagev Tzavoa Caves. Comparison of speleothem formation with calcite horizons suggests that the wet periods were limited to only tens or hundreds of years.\n\nFrom 60–30 kya there were extremely dry conditions in many parts of Africa.\n\nAn example of the Saharan pump has occurred after the Last Glacial Maximum (LGM). During the Last Glacial Maximum the Sahara desert was more extensive than it is now with the extent of the tropical forests being greatly reduced. During this period, the lower temperatures reduced the strength of the Hadley Cell whereby rising tropical air of the Inter-Tropical Convergence Zone (ITCZ) brings rain to the tropics, while dry descending air, at about 20 degrees north, flows back to the equator and brings desert conditions to this region. This phase is associated with high rates of wind-blown mineral dust, found in marine cores that come from the north tropical Atlantic.\n\nAround 12,500 BC, the amount of dust in the cores in the Bølling/Allerød phase suddenly plummets and shows a period of much wetter conditions in the Sahara, indicating a Dansgaard-Oeschger (DO) event (a sudden warming followed by a slower cooling of the climate). The moister Saharan conditions had begun about 12,500 BC, with the extension of the ITCZ northward in the northern hemisphere summer, bringing moist wet conditions and a savanna climate to the Sahara, which (apart from a short dry spell associated with the Younger Dryas) peaked during the Holocene thermal maximum climatic phase at 4000 BC when mid-latitude temperatures seem to have been between 2 and 3 degrees warmer than in the recent past. Analysis of Nile River deposited sediments in the delta also shows this period had a higher proportion of sediments coming from the Blue Nile, suggesting higher rainfall also in the Ethiopian Highlands. This was caused principally by a stronger monsoonal circulation throughout the sub-tropical regions, affecting India, Arabia and the Sahara. Lake Victoria only recently became the source of the White Nile and dried out almost completely around 15 kya.\n\nThe sudden subsequent movement of the ITCZ southwards with a Heinrich event (a sudden cooling followed by a slower warming), linked to changes with the El Niño-Southern Oscillation cycle, led to a rapid drying out of the Saharan and Arabian regions, which quickly became desert. This is linked to a marked decline in the scale of the Nile floods between 2700 and 2100 BC. One theory proposed that humans accelerated the drying out period from 6,000–2,500 BC by pastoralists overgrazing available grassland.\n\nThe Saharan pump has been used to date a number of waves of human migration from Africa, namely:\n\n\n"}
{"id": "3200977", "url": "https://en.wikipedia.org/wiki?curid=3200977", "title": "Sierra Madre Occidental pine-oak forests", "text": "Sierra Madre Occidental pine-oak forests\n\nThe Sierra Madre Occidental pine-oak forests are a subtropical coniferous forest ecoregion of the Sierra Madre Occidental range from the southwest USA region to the western part of Mexico. They are home to a large number of endemic plants and important habitat for wildlife.\n\nThe Sierra Madre Occidental run north to south in western Mexico from the center of the country towards the United States border. Coordinates of a latitude of formula_1N and longitude of formula_2 W. Where the northern extent of the ecoregion consists of patches of forest on the Madrean Sky Islands sticking up out of the desert of Sonora, Mexico and in the US in Arizona and New Mexico. A Landscape of steep mountain valleys with rivers flowing through the canyons. This is a dramatic landscape of steep mountains cut through with canyons including Copper Canyon, the deepest in North America.\n\nThe ecoregion consists of a complex of high elevation pine-oak forest enclaves surrounded at lower elevations by deserts and xeric shrublands and tropical dry forests at lower elevations, including the Sonoran Desert to the northwest, the Chihuahuan Desert to the northeast and east in Arizona, the Meseta Central matorral and Central Mexican matorral to the southeast, and the Sinaloan dry forests to the west and southwest. The Sierra Madre Occidental pine-oak forests are one of the Madrean pine-oak forests ecoregions, which are found throughout the Sierra Madre ranges of Mexico and the US Southwest.\n\nAn interesting fact about Mexico and its pine trees is that, this country has more pine species than any other country its size. The Sierra Madre Occidental being the largest mountain range in Mexico, is the home to a huge diversity of pine trees species. There are 24 different pine species in this mountain range and most are located in the highland forests of the Sierra Madre Occidental.\n\nThe original habitats of the Sierra Madre Occidental included forest of pines and Douglas-firs at the higher elevations with oak-wooded grassland on the lower slopes. The vegetation of the pine-oak forest differs between zones of lower temperatures and higher elevations. Some of the Pine trees flourish in higher elevation where more rainfall tends to occur during the summer seasons. The pines and oaks are especially important as there are so many different species of each including a number of endemics.\nPredominant conifers among the 27 species found here include Apache pine (\"Pinus engelmannii\"), Chihuahua pine (\"Pinus leiophylla\"), Mexican pinyon (\"Pinus cembroides\"), Lumholtz's pine (\"Pinus lumholtzii\"), Yécora pine (\"Pinus yecorensis\"), Rocky Mountain Douglas-fir (\"Pseudotsuga menziesii\" subsp. \"glauca\"), and Mexican Douglas-fir (\"Pseudotsuga lindleyana\").\n\nOaks (\"Quercus\") are the dominant broadleaf trees, with 21 different species found including \"Quercus albocincta\", Arizona oak (\"Q. arizonica\") \"Q. carmenensis\", \"Q. chihuahuensis\", \"Q. cordifolia\", \"Q. durifolia\", Emory oak (\"Q. emoryi\"), \"Q. grisea\", Mexican blue oak (\"Q. oblongifolia\"), \"Q. santaclarensis\", and \"Q. tarahumara\". Madroño (\"Arbutus xalapensis\" and \"A. arizonica\") is found in association with oaks. \"Q. carmenensis\" and \"Q. deliquescens\" are two oaks endemic to the sky islands portion of the ecoregion.\n\nThe forests are home to more than 300 species of birds including golden eagle, thick-billed parrot (which is now endangered having been trapped for collections), military macaw and tufted jay, and were once home to the imperial woodpecker which is now thought to be extinct.\n\nMammals include the American black bear and jaguar. The rare Mexican wolf was once common in the mountains but is now not found in the wild while the Mexican grizzly bear is now thought to be extinct.\n\nThe many reptiles include the rock rattlesnake (\"Crotalus lepidus\"), twin-spotted rattlesnake (\"Crotalus pricei\"), ridgenose rattlesnake (\"Crotalus willardi\") and some spiny lizards (\"Sceloporus clarki\", \"S. jarrovi\", \"S. scalaris\" and \"S. virgatus\").\n\nFinally the mountains are important stage in the migration route of monarch butterflies.\n\nThe pine forests have been cleared by the logging and paper industries since the 1800s and only a very small percentage of original forest remains. This removal of habitat has resulted in the presumed extinction of the imperial woodpecker, once the largest woodpecker on earth. Areas prioritised for conservation include Bavispe/Sierra del Tigre and Copper Canyon, while the only protected area is La Michilía. There are a number of protected areas of Madrean sky islands in Arizona which are part of the ecoregion although not the Sierra Madre Occidental themselves, including Chiricahua National Monument, Galiuro Wilderness, the Rincon Mountain district of Saguaro National Park, the Huachuca Mountains, Pusch Ridge Wilderness Area, the Santa Teresa Mountains and the Pajarito Mountains.\n\nThe increase of global warming is expected to have some devastating effects on the pine trees located in the Sierra Madre Occidental. The predicted outcome of global warming in this subtropical region, is the reduction of plant/animal productivity and growth. Another effect would be that the increase of climate warming would also lead to tree mortality affecting not only the pine trees located in the Sierra Madre Occidental but also other pine trees in the surrounding areas.\n\n\n3.González-Cásares, Marcos, Marín Pompa-García, and J Camarero. \"Differences in Climate-growth Relationship Indicate Diverse Drought Tolerances Among Five Pine Species Coexisting in Northwestern Mexico.\" \"Trees: Structure & Function\", 31.2 (2017): 531-544.\n\n4. González-Elizondo, Martha, Enrique Jurado, José Návar, M. Socorro González-Elizondo, José Villanueva, Oscar Aguirre, and Javier Jiménez. \"Tree-rings and Climate Relationships for Douglas-fir Chronologies from the Sierra Madre Occidental, Mexico: A 1681–2001 Rain Reconstruction.\" \"Forest Ecology & Management\", 213.1-3 (2005): 39-53.\n\n"}
{"id": "12186522", "url": "https://en.wikipedia.org/wiki?curid=12186522", "title": "Stanton Davis Kirkham", "text": "Stanton Davis Kirkham\n\nStanton Davis Kirkham (December 7, 1868 – January 6, 1944) was a naturalist, philosopher, ornithologist and author. Although widely travelled, he resided primarily in Canandaigua, Ontario County, New York. He was born in Nice, Alpes-Maritimes, France, the only child of Major Murray S. Davis (Commander, 8th Cavalry, Troop A, Camp Winfield Scott, Nevada, 1867) and Julia Edith Kirkham Davis, daughter of Gen. Ralph Wilson Kirkham, Union Army general, who adopted Kirkham and brought him to the United States. He was named after Secretary of War Edwin M. Stanton, whom his father had served as an aide. He attended public schools in California and later graduated from the Massachusetts Institute of Technology.\n\nKirkham was the author of \"As Nature Whispers\" (1902), \"The Ministry of Beauty\" (1907), \"Where Dwells the Soul Serene\" (1907), \"In The Open: Intimate Studies and Appreciations of Nature\" (1908), \"The Philosophy of Self-Help: An Application of Practical Psychology to Daily Life\", (1909), \"Mexican Trails: A Record of Travel in Mexico, 1904-7, and a Glimpse at the Life of the Mexican Indian\" (1909), \"Resources: An Interpretation of the Well-Rounded Life\" (1910), \"East and West: Comparative Studies of Nature in Eastern and Western States\" (1911), \"Outdoor Philosophy: The Meditations of a Naturalist\" (1912), \"North and South: Notes on the Natural History of a Summer Camp and a Winter Home\" (1913), \"Half-True Stories: For Little Folks of Just the Right Age\" (1916), \"After Thirty Years\" (1923), \"Animal Nature and Other Stories\" (1926), \"Cruising Around the World and the Seven Seas\" (1927) and \"Shut-In\" (1936).\n\nHe married Mary Clark Williams (1869 - 1911), granddaughter of New York Governor Myron H. Clark, on May 16, 1907. They had two children, a son, Paul Kirkham, and a daughter, Mary Clark Kirkham (1908-2000). He was quoted in Chapter 6 of \"As a Man Thinketh\" by James Allen. He died in New York City at the age of 75. An illness he contracted during a horseback journey across South America in 1914 rendered him an invalid for the latter part of his life. His philosophy, as stated in \"The Ministry of Beauty\" and other works, could be said to fall within the category of Transcendentalism. He is buried in Woodlawn Cemetery, Canandaigua, New York.\n\n\n"}
{"id": "28514827", "url": "https://en.wikipedia.org/wiki?curid=28514827", "title": "The war to end war", "text": "The war to end war\n\n\"The war to end war\" (sometimes called \"The war to end all wars\") was a term for the First World War of 1914–1918. Originally idealistic, it is now used mainly sardonically.\n\nDuring August 1914, immediately after the outbreak of the war, British author and social commentator H. G. Wells published a number of articles in London newspapers that subsequently appeared as a book entitled \"The War That Will End War\". Wells blamed the Central Powers for the coming of the war and argued that only the defeat of German militarism could bring about an end to war. Wells used the shorter form of the phrase, \"the war to end war\", in \"In the Fourth Year\" (1918), in which he noted that the phrase had \"got into circulation\" in the second half of 1914. In fact, it had become one of the most common catchphrases of the First World War.\n\nIn later years, the term became associated with Woodrow Wilson, despite the fact that Wilson used the phrase only once. Along with the phrase \"make the world safe for democracy\", it embodied Wilson's conviction that U.S. entry into the war was necessary to preserve human freedom.\n\nEven during World War I, the phrase met with some degree of skepticism; the British politician David Lloyd George is reputed to have said, \"This war, like the next war, is a war to end war.\" As it became apparent that the war had not succeeded in ending war, the phrase took on a more cynical tone. The British staff officer Archibald Wavell, a future field marshal and viceroy of India, said despondently of the Paris Peace Conference, \"After the 'war to end war', they seem to have been in Paris at making the 'Peace to end Peace'.\" Wells himself used the phrase in an ironic way in the novel \"The Bulpington of Blup\" (1932). Walter Lippmann wrote in \"Newsweek\" in 1967, \"The delusion is that whatever war we are fighting is the war to end war\", while U.S. President Richard Nixon, in his \"Silent Majority\" speech, said, \"I do not tell you that the war in Vietnam is the war to end wars\".\n\n\n"}
{"id": "28503451", "url": "https://en.wikipedia.org/wiki?curid=28503451", "title": "Transnational organization", "text": "Transnational organization\n\nTransnational organization is a term used in scholarly literature. It refers to international organizations (usually, international nongovernmental organizations) that \"transcend\" the idea of a nation-state.\n\nThe distinction between an international and a transnational organization is unclear and has been criticized by some scholars (ex. Colás, 2002).\n\nTransnational relations have been defined as “contacts, coalitions, and interactions across state boundaries that are not controlled by the central foreign policy organs of governments.” Examples of transnational entities are “multinational business enterprises and revolutionary movements; trade unions and scientific networks; international air transport cartels and communications activities in outer space.” Transnational social movements are “the broad tendencies that often manifest themselves in particular International Non-Governmental Organizations (INGOs).” Examples of transnational social movements include human rights, women’s, peace, labor, green, or student movements manifested in Amnesty International, the Peace Brigades International, the International Confederation of Free Trade Unions, etc. A further definition: “An organization is \"transnational\" rather than \"national\" if it carries on significant centrally-directed operations in the territory of two or more nation-states. Similarly, an organization will be called \"international\" rather than \"national\" only if the control of the organization is explicitly shared among representatives of two or more nationalities. And an organization is \"multinational\" rather than \"national\" only if people from two or more nationalities participate significantly in its operation.” “Transnational organizations are designed to facilitate the pursuit of a single interest within many national units.” \n\n\n"}
{"id": "507893", "url": "https://en.wikipedia.org/wiki?curid=507893", "title": "Viktor Khristenko", "text": "Viktor Khristenko\n\nViktor Borisovich Khristenko (; born 28 August 1957) is a Russian politician who was Chairman of the board of the Eurasian Economic Commission from 1 February 2012 to 1 February 2016. He was First Deputy Prime Minister of Russia from 31 May 1999 to 10 January 2000 and Minister of Industry from 9 March 2004 to 31 January 2012.\n\nKhristenko was born in Chelyabinsk on 28 August 1957. Kristenko graduated in 1979 from Chelyabinsk Mechanical Engineering Institute with a specialization in construction management and Economics. In 1983, he completed his Candidate of Sciences in Management at the Moscow Institute of Management. Khristenko has acknowledged the influence of Georgy Shchedrovitsky in his approach to management. He contributed three chapters to \"Methodological School of Management\", a book based on the work of Shchedrovitsky's Moscow Methodological Circle and its successors.\n\nIn 1998, Viktor Khristenko was appointed as Deputy Prime Minister for Economy and Finance in Sergei Kiriyenko's Cabinet. Described as a \"little-known reformist\", his appointment drew quite some attention as it was seen as a sign towards economic reform under the Yeltsin Presidency. He however didn't survive the government reshuffling under the following Prime Minister Primakov. From 1999 until early 2000, he was however appointed to Vladimir Putin's First Cabinet serving as First Deputy Prime Minister.\n\nIn February 2004, Khristenko briefly served as the acting Prime Minister of Russia, when President Vladimir Putin fired Prime Minister Mikhail Kasyanov on 24 February 2004.\n\nKhristenko was described as a \"broadly reformist technocrat,\" who had shown \"loyalty mixed with extreme caution,\" unlike the outgoing prime minister who had \"openly disagreed with Mr Putin several times, criticizing the criminal investigations into the owners of Yukos. \"The Washington Post\" called Kasyanov \"the most powerful ally of big business remaining in the Russian government.\" Khristenko, 46 at the time, was promoted from deputy prime minister to acting prime minister. Putin commented that Kasyanov's ousting was not related to the results of the government's activities, which he characterized as positive, but rather was caused by a necessity to once again confirm his position, which would guide the development of the country after 14 March 2004.\n\nTwo weeks ahead of the 2004 presidential election, Putin however nominated Mikhail Fradkov to become the next prime minister, four days later to be confirmed by the State Duma. On 9 March 2004, Kristenko was appointed Minister of Industry and Trade instead, a post which he held until 31 January 2012.\n\nKhristenko became the first Chairman of the Eurasian Economic Commission, which started operations in February 2012. He resigned on February 1, 2016.\n\nKhristenko's second wife, Tatyana Golikova, was Minister of Health and Social Development from 2007 to 2012. They married in 2003.\n\n"}
{"id": "581452", "url": "https://en.wikipedia.org/wiki?curid=581452", "title": "War Industries Board", "text": "War Industries Board\n\nThe War Industries Board (WIB) was a United States government agency established on July 28, 1917, during World War I, to coordinate the purchase of war supplies between the War Department (Department of the Army) and the Navy Department. Because the United States Department of Defense (The Pentagon) would only come into existence in 1947, this was an ad hoc construction to promote cooperation between the Army and the Navy (with regard to procurement), it was founded by the Council of National Defense (which on its turn came into existence by the appropriation bill of August 1916). The \"War Industries Board\" was preceded by the General Munitions Board —which didn't have the authority it needed and was later strengthened and transformed into the WIB.\n\nThe board was led initially by Frank A. Scott, who had previously been head of the General Munitions Board. He was replaced in November by Baltimore and Ohio Railroad president Daniel Willard. Finally, in January 1918, the board was reorganized under the leadership of financier Bernard M. Baruch.\n\nThe organization encouraged companies to use mass-production techniques to increase efficiency and urged them to eliminate waste by standardizing products. The board set production quotas and allocated raw materials. It also conducted psychological testing to help people find the right jobs.\n\nThe WIB dealt with labor-management disputes resulting from increased demand for products during World War I. The government could not negotiate prices and could not handle worker strikes, so the War Industries Board regulated the two to decrease tensions by stopping strikes with wage increases to prevent a shortage of supplies going to the war in Europe.\n\nUnder the War Industries Board, industrial production in the U.S. increased 20 percent, however, the vast majority of the war material was produced too late to do any good. The War Industries Board was decommissioned by an executive order on January 1, 1919.\n\nWith the war mobilization conducted under the supervision of the War Industries Board, unprecedented fortunes fell upon war producers and certain holders of raw materials and patents. Hearings in 1934 by the Nye Committee led by U.S. Senator Gerald Nye were intended to hold war profiteers to account.\n\nDespite its relatively brief existence, the WIB was a major step in the development of national planning and government-business cooperation in the United States, and its precedents —like the National Recovery Administration— were influential during the New Deal and World War II.\n\nThe original seven members of the War Industries Board were:\n\nOther later members included:\n\n"}
{"id": "570662", "url": "https://en.wikipedia.org/wiki?curid=570662", "title": "Wireless power transfer", "text": "Wireless power transfer\n\nWireless power transfer (WPT), wireless power transmission, wireless energy transmission (WET), or electromagnetic power transfer is the transmission of electrical energy without wires as a physical link. In a wireless power transmission system, a transmitter device, driven by electric power from a power source, generates a time-varying electromagnetic field, which transmits power across space to a receiver device, which extracts power from the field and supplies it to an electrical load. The technology of the wireless power transmission can eliminate the use of the wires and batteries, thus it increases the mobility of the electronic devices, more convenient and safer for all users. Wireless power transfer is useful to power electrical devices where interconnecting wires are inconvenient, hazardous, or are not possible.\n\nWireless power techniques mainly fall into two categories, near field and far-field. In \"near field\" or \"non-radiative\" techniques, power is transferred over short distances by magnetic fields using inductive coupling between coils of wire, or by electric fields using capacitive coupling between metal electrodes. Inductive coupling is the most widely used wireless technology; its applications include charging handheld devices like phones and electric toothbrushes, RFID tags, and wirelessly charging or continuous wireless power transfer in implantable medical devices like artificial cardiac pacemakers, or electric vehicles.\n\nIn \"far-field\" or \"radiative\" techniques, also called \"power beaming\", power is transferred by beams of electromagnetic radiation, like microwaves or laser beams. These techniques can transport energy longer distances but must be aimed at the receiver. Proposed applications for this type are solar power satellites, and wireless powered drone aircraft.\n\nAn important issue associated with all wireless power systems is limiting the exposure of people and other living things to potentially injurious electromagnetic fields.\n\nWireless power transfer is a generic term for a number of different technologies for transmitting energy by means of electromagnetic fields. The technologies, listed in the table below, differ in the distance over which they can transfer power efficiently, whether the transmitter must be aimed (directed) at the receiver, and in the type of electromagnetic energy they use: time varying electric fields, magnetic fields, radio waves, microwaves, infrared or visible light waves.\n\nIn general a wireless power system consists of a \"transmitter\" device connected to a source of power such as a mains power line, which converts the power to a time-varying electromagnetic field, and one or more \"receiver\" devices which receive the power and convert it back to DC or AC electric current which is used by an electrical load. At the transmitter the input power is converted to an oscillating electromagnetic field by some type of \"antenna\" device. The word \"antenna\" is used loosely here; it may be a coil of wire which generates a magnetic field, a metal plate which generates an electric field, an antenna which radiates radio waves, or a laser which generates light. A similar antenna or coupling device at the receiver converts the oscillating fields to an electric current. An important parameter that determines the type of waves is the frequency, which determines the wavelength.\n\nWireless power uses the same fields and waves as wireless communication devices like radio, another familiar technology that involves electrical energy transmitted without wires by electromagnetic fields, used in cellphones, radio and television broadcasting, and WiFi. In radio communication the goal is the transmission of information, so the amount of power reaching the receiver is not so important, as long as it is sufficient that the information can be received intelligibly. In wireless communication technologies only tiny amounts of power reach the receiver. In contrast, with wireless power transfer the amount of energy received is the important thing, so the efficiency (fraction of transmitted energy that is received) is the more significant parameter. For this reason, wireless power technologies are likely to be more limited by distance than wireless communication technologies.\n\nWireless power transfer may be used to power up wireless information transmitters or receivers. This type of communication is known as wireless powered communication (WPC). When the harvested power is used to supply the power of wireless information transmitters, the network is known as Simultaneous Wireless Information and Power Transfer (SWIPT); whereas when it is used to supply the power of wireless information receivers, it is known as a Wireless Powered Communication Network (WPCN).\n\nIn the United States, the Federal Communications Commission (FCC) provided its first certification for a wireless transmission charging system in December 2017.\n\nThese are the different wireless power technologies:\n\nElectric and magnetic fields are created by charged particles in matter such as electrons. A stationary charge creates an electrostatic field in the space around it. A steady current of charges (direct current, DC) creates a static magnetic field around it. The above fields contain energy, but cannot carry power because they are static. However time-varying fields can carry power. Accelerating electric charges, such as are found in an alternating current (AC) of electrons in a wire, create time-varying electric and magnetic fields in the space around them. These fields can exert oscillating forces on the electrons in a receiving \"antenna\", causing them to move back and forth. These represent alternating current which can be used to power a load.\n\nThe oscillating electric and magnetic fields surrounding moving electric charges in an antenna device can be divided into two regions, depending on distance \"D\" from the antenna.\n\n\nAt large relative distance, the near-field components of electric and magnetic fields are approximately quasi-static oscillating dipole fields. These fields decrease with the cube of distance: (D\"/D\") Since power is proportional to the square of the field strength, the power transferred decreases as (D\"/D\"). or 60 dB per decade. In other words, if far apart, doubling the distance between the two antennas causes the power received to decrease by a factor of 2 = 64. As a result, inductive and capacitive coupling can only be used for short-range power transfer, within a few times the diameter of the antenna device \"D\". Unlike in a radiative system where the maximum radiation occurs when the dipole antennas are oriented transverse to the direction of propagation, with dipole fields the maximum coupling occurs when the dipoles are oriented longitudinally.\n\nIn inductive coupling (\"electromagnetic induction\" or \"inductive power transfer\", IPT), power is transferred between coils of wire by a magnetic field. The transmitter and receiver coils together form a \"transformer\" \"(see diagram)\". An alternating current (AC) through the transmitter coil \"(L1)\" creates an oscillating magnetic field \"(B)\" by Ampere's law. The magnetic field passes through the receiving coil \"(L2)\", where it induces an alternating EMF (voltage) by Faraday's law of induction, which creates an alternating current in the receiver. The induced alternating current may either drive the load directly, or be rectified to direct current (DC) by a rectifier in the receiver, which drives the load. A few systems, such as electric toothbrush charging stands, work at 50/60 Hz so AC mains current is applied directly to the transmitter coil, but in most systems an electronic oscillator generates a higher frequency AC current which drives the coil, because transmission efficiency improves with frequency.\n\nInductive coupling is the oldest and most widely used wireless power technology, and virtually the only one so far which is used in commercial products. It is used in inductive charging stands for cordless appliances used in wet environments such as electric toothbrushes and shavers, to reduce the risk of electric shock. Another application area is \"transcutaneous\" recharging of biomedical prosthetic devices implanted in the human body, such as cardiac pacemakers and insulin pumps, to avoid having wires passing through the skin. It is also used to charge electric vehicles such as cars and to either charge or power transit vehicles like buses and trains.\n\nHowever the fastest growing use is wireless charging pads to recharge mobile and handheld wireless devices such as laptop and tablet computers, cellphones, digital media players, and video game controllers.\n\nThe power transferred increases with frequency and the mutual inductance formula_1 between the coils, which depends on their geometry and the distance formula_2 between them. A widely used figure of merit is the coupling coefficient formula_3. This dimensionless parameter is equal to the fraction of magnetic flux through the transmitter coil formula_4 that passes through the receiver coil formula_5 when L2 is open circuited. If the two coils are on the same axis and close together so all the magnetic flux from formula_4 passes through formula_5, formula_8 and the link efficiency approaches 100%. The greater the separation between the coils, the more of the magnetic field from the first coil misses the second, and the lower formula_9 and the link efficiency are, approaching zero at large separations. The link efficiency and power transferred is roughly proportional to formula_10. In order to achieve high efficiency, the coils must be very close together, a fraction of the coil diameter formula_11, usually within centimeters, with the coils' axes aligned. Wide, flat coil shapes are usually used, to increase coupling. Ferrite \"flux confinement\" cores can confine the magnetic fields, improving coupling and reducing interference to nearby electronics, but they are heavy and bulky so small wireless devices often use air-core coils.\n\nOrdinary inductive coupling can only achieve high efficiency when the coils are very close together, usually adjacent. In most modern inductive systems resonant inductive coupling \"(described below)\" is used, in which the efficiency is increased by using resonant circuits. This can achieve high efficiencies at greater distances than nonresonant inductive coupling. \n\nResonant inductive coupling (\"electrodynamic coupling\", \"strongly coupled magnetic resonance\") is a form of inductive coupling in which power is transferred by magnetic fields \"(B, green)\" between two resonant circuits (tuned circuits), one in the transmitter and one in the receiver \"(see diagram, right)\". Each resonant circuit consists of a coil of wire connected to a capacitor, or a self-resonant coil or other resonator with internal capacitance. The two are tuned to resonate at the same resonant frequency. The resonance between the coils can greatly increase coupling and power transfer, analogously to the way a vibrating tuning fork can induce sympathetic vibration in a distant fork tuned to the same pitch.\n\nNikola Tesla first discovered resonant coupling during his pioneering experiments in wireless power transfer around the turn of the 20th century, but the possibilities of using resonant coupling to increase transmission range has only recently been explored. In 2007 a team led by Marin Soljačić at MIT used two coupled tuned circuits each made of a 25 cm self-resonant coil of wire at 10 MHz to achieve the transmission of 60 W of power over a distance of (8 times the coil diameter) at around 40% efficiency. Soljačić founded the company WiTricity (the same name the team used for the technology) which is attempting to commercialize the technology.\n\nThe concept behind resonant inductive coupling systems is that high Q factor resonators exchange energy at a much higher rate than they lose energy due to internal damping. Therefore, by using resonance, the same amount of power can be transferred at greater distances, using the much weaker magnetic fields out in the peripheral regions (\"tails\") of the near fields (these are sometimes called evanescent fields). Resonant inductive coupling can achieve high efficiency at ranges of 4 to 10 times the coil diameter (\"D\"). This is considered \"mid-range\" transfer, in contrast to the \"short range\" of nonresonant inductive transfer, which can achieve similar efficiencies only when the coils are adjacent. Another advantage is that resonant circuits interact with each other so much more strongly than they do with nonresonant objects that power losses due to absorption in stray nearby objects are negligible.\n\nA drawback of resonant coupling theory is that at close ranges when the two resonant circuits are tightly coupled, the resonant frequency of the system is no longer constant but \"splits\" into two resonant peaks, so the maximum power transfer no longer occurs at the original resonant frequency and the oscillator frequency must be tuned to the new resonance peak.\n\nResonant technology is currently being widely incorporated in modern inductive wireless power systems. One of the possibilities envisioned for this technology is area wireless power coverage. A coil in the wall or ceiling of a room might be able to wirelessly power lights and mobile devices anywhere in the room, with reasonable efficiency. An environmental and economic benefit of wirelessly powering small devices such as clocks, radios, music players and remote controls is that it could drastically reduce the 6 billion batteries disposed of each year, a large source of toxic waste and groundwater contamination.\n\nCapacitive coupling also referred to as electric coupling, makes use of electric fields for the transmission of power between two electrodes (an anode and cathode) forming a capacitance for the transfer of power. In capacitive coupling (electrostatic induction), the conjugate of inductive coupling, energy is transmitted by electric fields between electrodes such as metal plates. The transmitter and receiver electrodes form a capacitor, with the intervening space as the dielectric. An alternating voltage generated by the transmitter is applied to the transmitting plate, and the oscillating electric field induces an alternating potential on the receiver plate by electrostatic induction, which causes an alternating current to flow in the load circuit. The amount of power transferred increases with the frequency the square of the voltage, and the capacitance between the plates, which is proportional to the area of the smaller plate and (for short distances) inversely proportional to the separation.\n\nCapacitive coupling has only been used practically in a few low power applications, because the very high voltages on the electrodes required to transmit significant power can be hazardous, and can cause unpleasant side effects such as noxious ozone production. In addition, in contrast to magnetic fields, electric fields interact strongly with most materials, including the human body, due to dielectric polarization. Intervening materials between or near the electrodes can absorb the energy, in the case of humans possibly causing excessive electromagnetic field exposure. However capacitive coupling has a few advantages over inductive coupling. The field is largely confined between the capacitor plates, reducing interference, which in inductive coupling requires heavy ferrite \"flux confinement\" cores. Also, alignment requirements between the transmitter and receiver are less critical. Capacitive coupling has recently been applied to charging battery powered portable devices as well as charging or continuous wireless power transfer in biomedical implants, and is being considered as a means of transferring power between substrate layers in integrated circuits.\n\nTwo types of circuit have been used:\n\nResonance can also be used with capacitive coupling to extend the range. At the turn of the 20th century, Nikola Tesla did the first experiments with both resonant inductive and capacitive coupling.\n\nIn this method, power is transmitted between two rotating armatures, one in the transmitter and one in the receiver, which rotate synchronously, coupled together by a magnetic field generated by permanent magnets on the armatures. The transmitter armature is turned either by or as the rotor of an electric motor, and its magnetic field exerts torque on the receiver armature, turning it. The magnetic field acts like a mechanical coupling between the armatures. The receiver armature produces power to drive the load, either by turning a separate electric generator or by using the receiver armature itself as the rotor in a generator.\n\nThis device has been proposed as an alternative to inductive power transfer for noncontact charging of electric vehicles. A rotating armature embedded in a garage floor or curb would turn a receiver armature in the underside of the vehicle to charge its batteries. It is claimed that this technique can transfer power over distances of 10 to 15 cm (4 to 6 inches) with high efficiency, over 90%. Also, the low frequency stray magnetic fields produced by the rotating magnets produce less electromagnetic interference to nearby electronic devices than the high frequency magnetic fields produced by inductive coupling systems. A prototype system charging electric vehicles has been in operation at University of British Columbia since 2012. Other researchers, however, claim that the two energy conversions (electrical to mechanical to electrical again) make the system less efficient than electrical systems like inductive coupling.\n\nFar field methods achieve longer ranges, often multiple kilometer ranges, where the distance is much greater than the diameter of the device(s). High-directivity antennas or well-collimated laser light produce a beam of energy that can be made to match the shape of the receiving area. The maximum directivity for antennas is physically limited by diffraction.\n\nIn general, visible light (from lasers) and microwaves (from purpose-designed antennas) are the forms of electromagnetic radiation best suited to energy transfer.\n\nThe dimensions of the components may be dictated by the distance from transmitter to receiver, the wavelength and the Rayleigh criterion or diffraction limit, used in standard radio frequency antenna design, which also applies to lasers. Airy's diffraction limit is also frequently used to determine an approximate spot size at an arbitrary distance from the aperture. Electromagnetic radiation experiences less diffraction at shorter wavelengths (higher frequencies); so, for example, a blue laser is diffracted less than a red one.\n\nThe Rayleigh criterion dictates that any radio wave, microwave or laser beam will spread and become weaker and diffuse over distance; the larger the transmitter antenna or laser aperture compared to the wavelength of radiation, the tighter the beam and the less it will spread as a function of distance (and vice versa). Smaller antennae also suffer from excessive losses due to side lobes. However, the concept of laser aperture considerably differs from an antenna. Typically, a laser aperture much larger than the wavelength induces multi-moded radiation and mostly collimators are used before emitted radiation couples into a fiber or into space.\n\nUltimately, beamwidth is physically determined by diffraction due to the dish size in relation to the wavelength of the electromagnetic radiation used to make the beam.\n\nMicrowave power beaming can be more efficient than lasers, and is less prone to atmospheric attenuation caused by dust or water vapor.\n\nHere, the power levels are calculated by combining the above parameters together, and adding in the gains and losses due to the antenna characteristics and the transparency and dispersion of the medium through which the radiation passes. That process is known as calculating a link budget.\n\nPower transmission via radio waves can be made more directional, allowing longer-distance power beaming, with shorter wavelengths of electromagnetic radiation, typically in the microwave range. A rectenna may be used to convert the microwave energy back into electricity. Rectenna conversion efficiencies exceeding 95% have been realized. Power beaming using microwaves has been proposed for the transmission of energy from orbiting solar power satellites to Earth and the beaming of power to spacecraft leaving orbit has been considered.\n\nPower beaming by microwaves has the difficulty that, for most space applications, the required aperture sizes are very large due to diffraction limiting antenna directionality. For example, the 1978 NASA study of solar power satellites required a transmitting antenna and a receiving rectenna for a microwave beam at 2.45 GHz. These sizes can be somewhat decreased by using shorter wavelengths, although short wavelengths may have difficulties with atmospheric absorption and beam blockage by rain or water droplets. Because of the \"thinned-array curse\", it is not possible to make a narrower beam by combining the beams of several smaller satellites.\n\nFor earthbound applications, a large-area 10 km diameter receiving array allows large total power levels to be used while operating at the low power density suggested for human electromagnetic exposure safety. A human safe power density of 1 mW/cm distributed across a 10 km diameter area corresponds to 750 megawatts total power level. This is the power level found in many modern electric power plants.\n\nFollowing World War II, which saw the development of high-power microwave emitters known as cavity magnetrons, the idea of using microwaves to transfer power was researched. By 1964, a miniature helicopter propelled by microwave power had been demonstrated.\n\nJapanese researcher Hidetsugu Yagi also investigated wireless energy transmission using a directional array antenna that he designed. In February 1926, Yagi and his colleague Shintaro Uda published their first paper on the tuned high-gain directional array now known as the Yagi antenna. While it did not prove to be particularly useful for power transmission, this beam antenna has been widely adopted throughout the broadcasting and wireless telecommunications industries due to its excellent performance characteristics.\n\nWireless high power transmission using microwaves is well proven. Experiments in the tens of kilowatts have been performed at Goldstone in California in 1975 and more recently (1997) at Grand Bassin on Reunion Island. These methods achieve distances on the order of a kilometer.\n\nUnder experimental conditions, microwave conversion efficiency was measured to be around 54% across one meter.\n\nA change to 24 GHz has been suggested as microwave emitters similar to LEDs have been made with very high quantum efficiencies using negative resistance, i.e., Gunn or IMPATT diodes, and this would be viable for short range links.\n\nIn 2013, inventor Hatem Zeine demonstrated how wireless power transmission using phased array antennas can deliver electrical power up to 30 feet. It uses the same radio frequencies as WiFi.\n\nIn 2015, researchers at the University of Washington introduced power over Wi-Fi, which trickle-charges batteries and powered battery-free cameras and temperature sensors using transmissions from Wi-Fi routers. Wi-Fi signals were shown to power battery-free temperature and camera sensors at ranges of up to 20 feet. It was also shown that Wi-Fi can be used to wirelessly trickle-charge nickel–metal hydride and lithium-ion coin-cell batteries at distances of up to 28 feet.\n\nIn 2017, the Federal Communication Commission (FCC) certified the first mid-field radio frequency (RF) transmitter of wireless power.\n\nIn the case of electromagnetic radiation closer to the visible region of the spectrum (tens of micrometers to tens of nanometers), power can be transmitted by converting electricity into a laser beam that is then pointed at a photovoltaic cell. This mechanism is generally known as 'power beaming' because the power is beamed at a receiver that can convert it to electrical energy. At the receiver, special photovoltaic laser power converters which are optimized for monochromatic light conversion are applied.\n\nAdvantages compared to other wireless methods are:\n\nDrawbacks include:\n\nLaser 'powerbeaming' technology was explored in military weapons and aerospace applications. Also, it is applied for powering of various kinds of sensors in industrial environment. Lately, it is developed for powering commercial and consumer electronics. Wireless energy transfer systems using lasers for consumer space have to satisfy laser safety requirements standardized under IEC 60825.\n\nFirst wireless power system using lasers for consumer applications was demonstrated in 2018. It can deliver power to stationary and moving devices across the room. This wireless power system complies with safety regulations according to IEC 60825 standard. It is also approved by the US Food and Drugs Administration (FDA).\n\nOther details include propagation, and the coherence and the range limitation problem.\n\nGeoffrey Landis is one of the pioneers of solar power satellites and laser-based transfer of energy especially for space and lunar missions. The demand for safe and frequent space missions has resulted in proposals for a laser-powered space elevator.\n\nNASA's Dryden Flight Research Center demonstrated a lightweight unmanned model plane powered by a laser beam. This proof-of-concept demonstrates the feasibility of periodic recharging using the laser beam system.\n\nScientists from Chinese Academy of Sciences developed a concept of utilizing the dual wavelength laser in wirelessly charging portable devices or UAVs, and a full-coupled model is established for the technology of laser power beaming.\n\nIn atmospheric plasma channel coupling, energy is transferred between two electrodes by electrical conduction through ionized air. When an electric field gradient exists between the two electrodes, exceeding 34 kilovolts per centimeter at sea level atmospheric pressure, an electric arc occurs. This atmospheric dielectric breakdown results in the flow of electric current along a random trajectory through an ionized plasma channel between the two electrodes. An example of this is natural lightning, where one electrode is a virtual point in a cloud and the other is a point on Earth. Laser Induced Plasma Channel (LIPC) research is presently underway using ultrafast lasers to artificially promote development of the plasma channel through the air, directing the electric arc, and guiding the current across a specific path in a controllable manner. The laser energy reduces the atmospheric dielectric breakdown voltage and the air is made less insulating by superheating, which lowers the density (formula_12) of the filament of air.\n\nThis new process is being explored for use as a laser lightning rod and as a means to trigger lightning bolts from clouds for natural lightning channel studies, for artificial atmospheric propagation studies, as a substitute for conventional radio antennas, for applications associated with electric welding and machining, for diverting power from high-voltage capacitor discharges, for directed-energy weapon applications employing electrical conduction through a ground return path, and electronic jamming.\n\nIn the context of wireless power, \"energy harvesting\", also called \"power harvesting\" or \"energy scavenging\", is the conversion of ambient energy from the environment to electric power, mainly to power small autonomous wireless electronic devices. The ambient energy may come from stray electric or magnetic fields or radio waves from nearby electrical equipment, light, thermal energy (heat), or kinetic energy such as vibration or motion of the device. Although the efficiency of conversion is usually low and the power gathered often minuscule (milliwatts or microwatts), it can be adequate to run or recharge small micropower wireless devices such as remote sensors, which are proliferating in many fields. This new technology is being developed to eliminate the need for battery replacement or charging of such wireless devices, allowing them to operate completely autonomously.\n\nThe 19th century saw many developments of theories, and counter-theories on how electrical energy might be transmitted. In 1826 André-Marie Ampère found Ampère's circuital law showing that electric current produces a magnetic field. Michael Faraday described in 1831 with his law of induction the electromotive force driving a current in a conductor loop by a time-varying magnetic flux. Transmission of electrical energy without wires was observed by many inventors and experimenters, but lack of a coherent theory attributed these phenomena vaguely to electromagnetic induction. A concise explanation of these phenomena would come from the 1860s Maxwell's equations by James Clerk Maxwell, establishing a theory that unified electricity and magnetism to electromagnetism, predicting the existence of electromagnetic waves as the \"wireless\" carrier of electromagnetic energy. Around 1884 John Henry Poynting defined the Poynting vector and gave Poynting's theorem, which describe the flow of power across an area within electromagnetic radiation and allow for a correct analysis of wireless power transfer systems. This was followed on by Heinrich Rudolf Hertz' 1888 validation of the theory, which included the evidence for radio waves.\n\nDuring the same period two schemes of wireless signaling were put forward by William Henry Ward (1871) and Mahlon Loomis (1872) that were based on the erroneous belief that there was an electrified atmospheric stratum accessible at low altitude. Both inventors' patents noted this layer connected with a return path using \"Earth currents\"' would allow for wireless telegraphy as well as supply power for the telegraph, doing away with artificial batteries, and could also be used for lighting, heat, and motive power. A more practical demonstration of wireless transmission via conduction came in Amos Dolbear's 1879 magneto electric telephone that used ground conduction to transmit over a distance of a quarter of a mile.\n\nAfter 1890 inventor Nikola Tesla experimented with transmitting power by inductive and capacitive coupling using spark-excited radio frequency resonant transformers, now called Tesla coils, which generated high AC voltages. Early on he attempted to develop a wireless lighting system based on near-field inductive and capacitive coupling and conducted a series of public demonstrations where he lit Geissler tubes and even incandescent light bulbs from across a stage. He found he could increase the distance at which he could light a lamp by using a receiving LC circuit tuned to resonance with the transmitter's LC circuit. using resonant inductive coupling. Tesla failed to make a commercial product out of his findings but his resonant inductive coupling method is now widely used in electronics and is currently being applied to short-range wireless power systems.\n\nTesla went on to develop a wireless power distribution system that he hoped would be capable of transmitting power long distance directly into homes and factories. Early on he seemed to borrow from the ideas of Mahlon Loomis, proposing a system composed of balloons to suspend transmitting and receiving electrodes in the air above in altitude, where he thought the pressure would allow him to send high voltages (millions of volts) long distances. To further study the conductive nature of low pressure air he set up a test facility at high altitude in Colorado Springs during 1899. Experiments he conducted there with a large coil operating in the megavolts range, as well as observations he made of the electronic noise of lightning strikes, led him to conclude incorrectly that he could use the entire globe of the Earth to conduct electrical energy. The theory included driving alternating current pulses into the Earth at its resonant frequency from a grounded Tesla coil working against an elevated capacitance to make the potential of the Earth oscillate. Tesla thought this would allow alternating current to be received with a similar capacitive antenna tuned to resonance with it at any point on Earth with very little power loss. His observations also led him to believe a high voltage used in a coil at an elevation of a few hundred feet would \"break the air stratum down\", eliminating the need for miles of cable hanging on balloons to create his atmospheric return circuit. Tesla would go on the next year to propose a \"World Wireless System\" that was to broadcast both information and power worldwide. In 1901, at Shoreham, New York he attempted to construct a large high-voltage wireless power station, now called Wardenclyffe Tower, but by 1904 investment dried up and the facility was never completed.\n\nInductive power transfer between nearby wire coils was the earliest wireless power technology to be developed, existing since the transformer was developed in the 1800s. Induction heating has been used since the early 1900s. With the advent of cordless devices, induction charging stands have been developed for appliances used in wet environments, like electric toothbrushes and electric razors, to eliminate the hazard of electric shock. One of the earliest proposed applications of inductive transfer was to power electric locomotives. In 1892 Maurice Hutin and Maurice Leblanc patented a wireless method of powering railroad trains using resonant coils inductively coupled to a track wire at 3 kHz. The first passive RFID (Radio Frequency Identification) technologies were invented by Mario Cardullo (1973) and Koelle et al. (1975) and by the 1990s were being used in proximity cards and contactless smartcards.\n\nThe proliferation of portable wireless communication devices such as mobile phones, tablet, and laptop computers in recent decades is currently driving the development of mid-range wireless powering and charging technology to eliminate the need for these devices to be tethered to wall plugs during charging. The Wireless Power Consortium was established in 2008 to develop interoperable standards across manufacturers. Its Qi inductive power standard published in August 2009 enables high efficiency charging and powering of portable devices of up to 5 watts over distances of 4 cm (1.6 inches). The wireless device is placed on a flat charger plate (which can be embedded in table tops at cafes, for example) and power is transferred from a flat coil in the charger to a similar one in the device.\n\nIn 2007, a team led by Marin Soljačić at MIT used a dual resonance transmitter with a 25 cm diameter secondary tuned to 10 MHz to transfer 60 W of power to a similar dual resonance receiver over a distance of (eight times the transmitter coil diameter) at around 40% efficiency. In 2008 the team of Greg Leyh and Mike Kennan of Nevada Lightning Lab used a grounded dual resonance transmitter with a 57 cm diameter secondary tuned to 60 kHz and a similar grounded dual resonance receiver to transfer power through coupled electric fields with an earth return circuit over a distance of .\n\nIn 2011, Dr. Christopher A. Tucker and Professor Kevin Warwick of the University of Reading, recreated Tesla’s 1900 patent 0,645,576 in miniature and demonstrated power transmission over with a coil diameter of at a resonant frequency of 27.50 MHz, with an effective efficiency of 60%. \n\nBefore World War 2, little progress was made in wireless power transmission. Radio was developed for communication uses, but couldn't be used for power transmission since the relatively low-frequency radio waves spread out in all directions and little energy reached the receiver. In radio communication, at the receiver, an amplifier intensifies a weak signal using energy from another source. For power transmission, efficient transmission required transmitters that could generate higher-frequency microwaves, which can be focused in narrow beams towards a receiver.\n\nThe development of microwave technology during World War 2, such as the klystron and magnetron tubes and parabolic antennas made radiative (far-field) methods practical for the first time, and the first long-distance wireless power transmission was achieved in the 1960s by William C. Brown. In 1964 Brown invented the rectenna which could efficiently convert microwaves to DC power, and in 1964 demonstrated it with the first wireless-powered aircraft, a model helicopter powered by microwaves beamed from the ground. A major motivation for microwave research in the 1970s and 80s was to develop a solar power satellite. Conceived in 1968 by Peter Glaser, this would harvest energy from sunlight using solar cells and beam it down to Earth as microwaves to huge rectennas, which would convert it to electrical energy on the electric power grid. In landmark 1975 experiments as technical director of a JPL/Raytheon program, Brown demonstrated long-range transmission by beaming 475 W of microwave power to a rectenna a mile away, with a microwave to DC conversion efficiency of 54%. At NASA's Jet Propulsion Laboratory he and Robert Dickinson transmitted 30 kW DC output power across 1.5 km with 2.38 GHz microwaves from a 26 m dish to a 7.3 x 3.5 m rectenna array. The incident-RF to DC conversion efficiency of the rectenna was 80%. In 1983 Japan launched MINIX (Microwave Ionosphere Nonlinear Interaction Experiment), a rocket experiment to test transmission of high power microwaves through the ionosphere.\n\nIn recent years a focus of research has been the development of wireless-powered drone aircraft, which began in 1959 with the Dept. of Defense's RAMP (Raytheon Airborne Microwave Platform) project which sponsored Brown's research. In 1987 Canada's Communications Research Center developed a small prototype airplane called Stationary High Altitude Relay Platform (SHARP) to relay telecommunication data between points on earth similar to a communication satellite. Powered by a rectenna, it could fly at 13 miles (21 km) altitude and stay aloft for months. In 1992 a team at Kyoto University built a more advanced craft called MILAX (MIcrowave Lifted Airplane eXperiment).\n\nIn 2003 NASA flew the first laser powered aircraft. The small model plane's motor was powered by electricity generated by photocells from a beam of infrared light from a ground-based laser, while a control system kept the laser pointed at the plane.\n\n\n\n\n\n"}
