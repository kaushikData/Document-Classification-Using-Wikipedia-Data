{"id": "30859025", "url": "https://en.wikipedia.org/wiki?curid=30859025", "title": "Al Hajar Mountains", "text": "Al Hajar Mountains\n\nAl-Hajar Mountains (, \"Rocky Mountains\" or \"Stone Mountains\") in northeastern Oman and also the eastern United Arab Emirates are the highest mountain range in the eastern Arabian peninsula. They separate the low coastal plain of Oman from the high desert plateau, and lie inland from the Gulf of Oman.\n\n\"Al\" () means \"The\", and \"Ḥajar\" () means \"Stone\" or \"Rock\". So \"Al-Hajar\" () would be defined as \"The Stone\" or \"The Rock\".\n\nGeologically, Al-Hajar Mountains are the continuation of the Zagros Mountains, and were mainly formed in the Miocene and Pliocene as the Arabian Plate collided with and pushed against the Iranian Plate. These mountains are chiefly made of Cretaceous limestones and ophiolites.\n\nThe mountains begin in the north, forming the peninsula. The central section of the Hajar is the highest and wildest terrain in the country. Jebel Akhdar and the smaller Jebel Nakhl range are bounded on the east by the low Samail Valley (which leads northeast to Muscat). East of Samail are the Eastern Hajar (Hajar ash Sharqi), which run east (much closer to the coast) to the fishing town of Sur, almost at the eastern point of Oman. The mountains extend for in total. \n\nThe low coastal land north and east of the Jebel Hajar is called \"Al-Batinah Region.\" The climate is cool and wet from December to March, and warmer but occasionally rainy from April to September.\n\nThe mountains are rich in plant life compared to most of Arabia, including a number of endemic species. The vegetation changes with altitude, the mountains are covered with shrubland at lower elevations, growing richer and then becoming woodland, including wild olive and fig trees between , and then higher still there are junipers. Fruit trees such as pomegranate and apricot are grown in the cooler valleys and in places there are rocky outcrops with little vegetation. The flora shows similarities with mountain areas of nearby Iran, as well as with areas along the Red Sea in the Horn of Africa. For example, the tree \"Ceratonia oreothauma\" is found here and also in Somalia.\n\nA number of birds are found in the mountains including Egyptian and lappet-faced vultures (\"Torgos tracheliotus\"). Mammals include mountain gazelles (\"Gazella gazella\") and the Arabian tahr (\"Arabitragus jayakari\"). Other endemic species include a number of geckos and lizards: \"Asaccus montanus\", \"Asaccus platyrhynchus\" and a subspecies of Wadi Kharrar rock gecko (\"Pristurus gasperetti gallagheri\") are found only in Oman while Musandam leaf-toed gecko (\"Asaccus caudivolvulus\"), Gallagher's leaf-toed gecko (\"Asaccus gallagheri\"), Oman rock gecko (\"Pristurus celirrimus\"), Jayakar lizard (\"Lacerta jayakari\") and Omman's lizard (\"Lacerta cyanura\") are found only in the Hajar mountains. The endangered Arabian leopard (\"Panthera pardus nimr\") had been recorded here.\n\nThe Hajar are extensively grazed by domestic goats, camels and donkeys and the landscape has been cleared in parts for urban areas and for mining, which has damaged both vegetation and water supplies and uprooted traditional rural land management behaviours. Poaching of wildlife is another issue. The Oman government has created the Wadi Sareen Reserve and an area of Jebel Qahwan-Jebal Sebtah in the Eastern Hajar, for the protection of Arabian tahr and mountain gazelle. For visitors, there is a road into the mountains from the town of Birkat al-Mawz (on the road to Nizwa from Muscat) and a walking route through Wadi al-Muaydin to the Saiq Plateau.\n\nThere are 11 marked trails/routes of varying intensity (between Grade 1 to 3) and duration (between 1.5 hours to 18 hours) published by Ministry of Tourism, Oman along the Hajar Mountain range.\n\n\n\n"}
{"id": "5811768", "url": "https://en.wikipedia.org/wiki?curid=5811768", "title": "Anticyclonic rotation", "text": "Anticyclonic rotation\n\nAnticyclonic rotation or circulation is movement in the direction opposite to the Earth's rotation. In the attached video an example of an anticyclonic couplet to a supercell is documented. Note that while the supercell itself is rotating in a cyclonic direction, the couplet is forced into an anticyclonic rotation due to the forward flank downdraft and inflow intersecting. Antiyclones on the synoptic scale are more often associated with higher pressure in relation to low pressure cyclones.\n\n\n"}
{"id": "17740081", "url": "https://en.wikipedia.org/wiki?curid=17740081", "title": "Australian Energy Market Commission", "text": "Australian Energy Market Commission\n\nThe \"'Australian Energy Market Commission (AEMC)\" was set up by the Council of Australian Governments through the Ministerial Council on Energy in 2005. The AEMC was established by the \"Australian Energy Market Commission Establishment Act 2004\" (SA), and commenced in July 2005. The Commission consists of one full-time and two part-time Commissioners. Two Commissioners are appointed by the participating State and Territory jurisdictions and one Commissioner is appointed by the Commonwealth.\n\nIn 2008, the AEMC completed a review of Victoria's and South Australia's retail gas and electricity markets. In both states it was found that competition in the market was adequate for deregulation. The authority is also involved in inter-regional electricity prices and access issues.\n\nThe AEMC has two roles in relation to the National Electricity Market - as rule maker and as a provider of advice to Ministers on how best to develop energy markets over time. The AEMC actively considers market development when it considers rule change proposals, policy advice and energy market reviews. These rules are binding on the Australian energy market and enforced by the Australian Energy Regulator.\n\nThe \"National Gas (South Australia) Act 2008\" introduced the National Gas Law (NGL) which commenced in all jurisdictions except Western Australia on 1 July 2008 (replacing the Gas Pipelines Access Law). \n\nThe National Gas Rules cover gas transmission and distribution in all participating Australian jurisdictions are developed and maintained by the AEMC. The AEMC is responsible for rule making, market development and policy advice concerning access to natural gas pipelines services and elements of the broader natural gas markets. This is narrower in scope than for the National Electricity Market.\n\nSince its establishment in 2005, the AEMC Commissioners have been:\n\n\nThe current Commissioners are:\n\n"}
{"id": "52952373", "url": "https://en.wikipedia.org/wiki?curid=52952373", "title": "B Cygni", "text": "B Cygni\n\nThe Bayer designation b Cygni is shared by three stars in the constellation Cygnus:\n\nAlbireo (β Cygni)\n"}
{"id": "7107998", "url": "https://en.wikipedia.org/wiki?curid=7107998", "title": "Bodies of water of Azerbaijan", "text": "Bodies of water of Azerbaijan\n\nThe water bodies of Azerbaijan were formed over a long geological timeframe and changed significantly throughout that period. This is particularly evidenced by remnants of ancient rivers found throughout the country. The country's water systems are continually changing under the influence of natural forces and human introduced industrial activities. Artificial rivers (canals) and ponds are a part of Azerbaijan's water systems.\n\nThe hydrography of Azerbaijan basically belongs to the Caspian Sea basin.\n\nRivers form the principal part of the water systems of Azerbaijan. There are 8,359 rivers of various lengths within Azerbaijan. Of them 8,188 rivers are less than long. Only 24 rivers are over long. \n\nThe largest rivers that flow through the country are:\n\nThe rivers in Azerbaijan can be divided into three groups:\n\nAzerbaijan river systems are changing and evolving under the influence of various physiographic factors: climate, landscape, geological structure, soil and vegetation. \nThe density of the river network increases, then gradually decreases later with higher altitudes. Except for the Talysh region (1.6-2.2 km/km²), the river system density is the highest (1-2 km/km²) at 1,000-2,500 kilometers, while in the area of the Talysh mountains it peaks at 1.6-2.2 km/km² at 500-1,000 km. The average density of the river system of Azerbaijan is 0.39 km/km². The density is even lower than 0.05 km/km² in the plains.\n\nThe Kur and Aras are the longest rivers in Azerbaijan. They run through the Kur-Araz Lowland. The rivers that directly flow into the Caspian Sea, originate mainly from the north-eastern slope of the Greater Caucasus and Talysh Mountains and run along the Samur-Devechi and Lenkeran lowlands.\n\nThe Kura River basin area (86,000 km²) up to the junction with the Aras River is smaller than the Aras water basin (101,937 km²). The river is still called Kura on the junction because the water level of the Kura is twice as high as that of the Aras River.\n\n\nOver 60 water reservoirs have been constructed in order to regulate the river flow in Azerbaijan. The formation of these reservoirs is one of the measures that has been undertaken in order to ration the utilization of water and energy resources. \n\nThe largest water reservoirs are:\n\nThe reservoirs in Azerbaijan are designed to be utilized for various purposes, while most other ponds are used exclusively for irrigation.\n\n\n"}
{"id": "26616666", "url": "https://en.wikipedia.org/wiki?curid=26616666", "title": "Building engineering physics", "text": "Building engineering physics\n\nThe term building engineering physics was introduced in a report released in January 2010 commissioned by The Royal Academy of Engineering. The report, entitled \"Engineering a Low Carbon Built Environment: The Discipline of Building Engineering Physics\", presents the initiative of many at the Royal Academy of Engineering in developing a field that addresses our fossil fuel dependence while working towards a more sustainably built environment for the future.\n\nThe field of building engineering physics combines the existing professions of building services engineering, applied physics and building construction engineering into a single field designed to investigate the energy efficiency of old and new buildings. The application of building engineering physics allows the construction and renovation of high performance, energy efficient buildings, while minimizing their environmental impacts.\n\nBuilding engineering physics addresses several different areas in building performance including: air movement, thermal performance, control of moisture, ambient energy, acoustics, light, climate and biology. This field employs creative ways of manipulating these principal aspects of a building’s indoor and outdoor environments so that a more eco-friendly standard of living is obtained. Building engineering physics is unique from other established applied sciences or engineering professions as it combines the sciences of architecture, engineering and human biology and physiology. Building engineering physics not only addresses energy efficiency and building sustainability, but also a building's internal environment conditions that affect the comfort and performance levels of its occupants.\n\nThroughout the 20th century, a large percentage of buildings were constructed completely dependent on fossil fuels. Rather than focusing on energy efficiency, architects and engineers were more concerned with experimenting with “new materials and structural forms” to further aesthetic ideals. Now in the 21st century, building energy performance standards are pushing towards a zero carbon standard in old and new buildings alike. The threat of global change and the need for energy independence and sustainability has prompted governments across the globe to adopt firm carbon reducing standards. A significant way to meet these stringent standards is in the construction of buildings that minimize environmental impacts, as well as the refurbishing of older buildings to meet carbon emission standards. The application of building engineering physics can aid in this transition to reduce energy dependent buildings, provide for the demands of a growing population and better standard of living. Growth in the application of this field is due in large part to the\" introduction of regulations requiring the calculation of carbon emissions to demonstrate compliance, principally the Energy Performance of Buildings Directive (EPBD)\".\n\nUnfortunately, the discipline of building engineering physics is severely impaired by lack of diverse education in the construction industry. Very few in the construction industry know how to apply the principles of building engineering physics and do not have interdisciplinary experience.\n\n"}
{"id": "21072589", "url": "https://en.wikipedia.org/wiki?curid=21072589", "title": "Carbon dioxide removal", "text": "Carbon dioxide removal\n\nCarbon dioxide removal (CDR) refers to a number of technologies, the objective of which is the large-scale removal of carbon dioxide from the atmosphere. Among such technologies are bio-energy with carbon capture and storage, biochar, ocean fertilization, enhanced weathering, and direct air capture when combined with storage. CDR is a different approach than removing from the stack emissions of large fossil fuel point sources, such as power stations. The latter reduces emission to the atmosphere but cannot reduce the amount of carbon dioxide already in the atmosphere. As CDR removes carbon dioxide from the atmosphere, it creates negative emissions, offsetting emissions from small and dispersed point sources such as domestic heating systems, airplanes and vehicle exhausts. It is regarded by some as a form of climate engineering, while other commentators describe it as a form of carbon capture and storage or extreme mitigation. Whether CDR would satisfy common definitions of \"climate engineering\" or \"geoengineering\" usually depends upon the scale on which it would be undertaken.\n\nThe likely need for CDR has been publicly expressed by a range of individuals and organizations involved with climate change issues, including IPCC chief Rajendra Pachauri, the UNFCCC executive secretary Christiana Figueres, and the World Watch Institute. Institutions with major programs focusing on CDR include the Lenfest Center for Sustainable Energy at the Earth Institute, Columbia University, and the Climate Decision Making Center, an international collaboration operated out of Carnegie-Mellon University's Department of Engineering and Public Policy.\n\nThe mitigation effectiveness of air capture is limited by societal investment, land use, availability of geologic reservoirs, and leakage. The reservoirs are estimated to be sufficient to for storing at least 545 GtC. Storing 771 GtC would cause a 186 ppm atmospheric reduction. In order to return the atmospheric CO content to 350 ppm we need atmospheric reduction of 50 ppm plus an additional 2 ppm per year of current emissions.\n\nCarbon dioxide removal is different from reducing emissions, as the former produces an outlet of carbon dioxide from Earth's atmosphere, whereas the latter decreases the inlet of carbon dioxide to the atmosphere. Both have the same net effect, but for achieving carbon dioxide concentration levels below present levels, carbon dioxide removal is critical. Also for meeting higher concentration levels, carbon dioxide removal is increasingly considered to be crucial as it provides the only possibility to fill the gap between needed reductions to meet mitigation targets and .\n\nIn the \"OECD Environmental Outlook to 2050\" released at the 2011 United Nations Climate Change Conference, the authors commented on the need for negative emissions, stating \"Achieving lower concentration targets (450 ppm) depends significantly on the use of BECCS\".\n\nA carbon dioxide sink such as a concentrated group of plants or any other primary producer that binds carbon dioxide into biomass, such as within forests and kelp beds, is not carbon negative, as sinks are not permanent. A carbon dioxide sink of this type moves carbon, in the form of carbon dioxide, from the atmosphere or hydrosphere to the biosphere. This process could be undone, for example by wildfires or logging.\n\nCarbon dioxide sinks that store carbon dioxide in the Earth's crust by injecting it into the subsurface, or in the form of insoluble carbonate salts (mineral sequestration), are considered carbon negative. This is because they are removing carbon from the atmosphere and sequestering it indefinitely and presumably for a considerable duration (thousands to millions of years). However, Carbon Capture technology remains, at best, theoretical and is yet to reach more than 33% efficiency. Furthermore, this process could be rapidly undone, for example by earthquakes or mining.\n\nBio-energy with carbon capture and storage, or BECCS, uses biomass to extract carbon dioxide from the atmosphere, and carbon capture and storage technologies to concentrate and permanently store it in deep geological formations.\n\nBECCS is currently (as of October 2012) the only CDR technology deployed at full industrial scale, with 550 000 tonnes CO/year in total capacity operating, divided between three different facilities (as of January 2012).\n\nThe Imperial College London, the UK Met Office Hadley Centre for Climate Prediction and Research, the Tyndall Centre for Climate Change Research, the Walker Institute for Climate System Research, and the Grantham Institute for Climate Change issued a joint report on carbon dioxide removal technologies as part of the \"AVOID: Avoiding dangerous climate change\" research program, stating that \"Overall, of the technologies studied in this report, BECCS has the greatest maturity and there are no major practical barriers to its introduction into today’s energy system. The presence of a primary product will support early deployment.\"\n\nAccording to the OECD, \"Achieving lower concentration targets (450 ppm) depends significantly on the use of BECCS\".\n\nBiochar is created by the pyrolysis of biomass, and is under investigation as a method of carbon sequestration.\nBiochar is a charcoal that is used for agricultural purposes which also aids in carbon sequestration, the capture or hold of carbon. It is created using a process called pyrolysis, which is basically the act of high temperature heating biomass in an environment with low oxygen levels. What remains is a material known as char, similar to charcoal but is made through a sustainable process, thus the use of biomass. Biomass is organic matter produced by living organisms or recently living organisms, most commonly plants or plant based material. The offset of GHG emission, if biochar were to be implemented, would be a maximum of 12%. This equates to about 106 metric tons of CO equivalents. On a medium conservative level, it would be 23% less than that, at 82 metric tons. A study done by the UK Biochar Research Center has stated that, on a conservative level, biochar can store 1 gigaton of carbon per year. With greater effort in marketing and acceptance of biochar, the benefit could be the storage of 5–9 gigatons per year of carbon in biochar soils.\n\nEnhanced weathering is a chemical approach to remove carbon dioxide involving land- or ocean-based techniques. One example of a land-based enhanced weathering technique is in-situ carbonation of silicates. Ultramafic rock, for example, has the potential to store from hundreds to thousands of years' worth of CO emissions, according to estimates. Ocean-based techniques involve alkalinity enhancement, such as grinding, dispersing, and dissolving olivine, limestone, silicates, or calcium hydroxide to address ocean acidification and CO sequestration. Enhanced weathering is considered one of the least expensive geoengineering options. One example of a research project on the feasibility of enhanced weathering is the CarbFix project in Iceland.\n\nCarbon dioxide can be removed from ambient air through chemical processes, sequestered, and stored. Traditional modes of carbon capture such as precombustion and postcombustion capture from large point sources can help slow the rate of increase of the atmospheric\n\nA few engineering proposals have been made for removing from the atmosphere, but work in this area is still in its infancy.\n\nAmong the main technologies proposed, three of them stand out: Causticization with alkali and alkali-earth hydroxides, carbonation,\nand organic−inorganic hybrid sorbents consisting of amines supported in porous adsorbents. A 2016 article reviews the research in these various areas.\n\nOne proposed method is by so-called \"artificial trees\". This concept, proposed by climate scientist Wallace S. Broecker and science writer Robert Kunzig, imagines huge numbers of artificial trees around the world to remove ambient CO. The technology is now being pioneered by Klaus Lackner, a researcher at the Earth Institute, Columbia University, whose artificial tree technology can suck up to 1,000 times more CO from the air than real trees can, at a rate of about one ton of carbon per day if the artificial tree is approximately the size of an actual tree. The CO would be captured in a filter and then removed from the filter and stored.\n\nThe chemistry used is a variant of that described below, as it is based on sodium hydroxide. However, in a more recent design proposed by Klaus Lackner, the process can be carried out at only 40 °C by using a polymer-based ion exchange resin, which takes advantage of changes in humidity to prompt the release of captured CO, instead of using a kiln. This reduces the energy required to operate the process.\n\nAnother substance which can be used are Metal-organic frameworks (or MOF's). A special MOF has been made specifically for locking CO by Joeri Denayer.\n\nIn 2008, the Discovery Channel covered the work of David Keith, of University of Calgary, who built a tower, 4 feet wide and 20 feet tall (1.2×6.1 meters), with a fan at the bottom that sucks air in, which comes out again at the top. In the process, about half the CO is removed from the air.\n\nThis device uses the chemical process described in detail below. The system demonstrated on the Discovery Channel was a 1/90,000th scale test system of the capture section; the reagents are regenerated in a separate facility. The main costs of a full plant will be the cost to build it, and the energy input to regenerate the chemicals and produce a pure stream of CO.\n\nTo put this into perspective, people in the U.S. emit about 20 tonnes of CO per person annually. In other words, each person in the U.S. would require a tower like the one featured by the Discovery Channel to remove this amount of CO from the air, requiring an annual 2 megawatt-hours of electricity to operate it. By comparison, a refrigerator consumes about 1.2 megawatt-hours annually (2001 figures). A modern, A++ rated refrigerator uses about 140 Kwh annually. But, by combining many small systems such as this into one large system, the construction costs and energy use can be reduced.\n\nIt has been proposed that the Solar updraft tower to generate electricity from thermal air currents also be used at the same time for amine gravity scrubbing of CO. Some heat would be required to regenerate the amine.\n\nA similar CO scrubber has also been built by Carbon Engineering. Besides simply focusing on capturing the CO, the company also puts emphasis on reuse of the CO, for example in the production of fuels, which would thus be carbon-neutral.\n\nDirect air capture has been proposed as a way of generating carbon-neutral organic chemicals, by harvesting the atmospheric compounds and then using them in the production and synthesis of polymers and fuels.\n\nThe Swiss-based Climeworks built the first industrial scale direct air capturing plant in Hinwil, in the canton of Zurich, Switzerland. It started up in May 2017, and could scrub up to 900 metric tons of CO per year using heat from a local waste incineration plant. The CO2 was then pumped into local greenhouses, where it was used to help grow vegetables like tomatoes and cucumbers.\n\nClimeworks and Reykjavik Energy also started up a small test direct air capturing plant in Hellisheidi, Iceland in October 2017. Using waste heat from a local geothermal power plant, the test plant captured up to 50 tons of CO per year which was bound with water and injected over 700 metres underground into basaltic bedrock. The CO reacted with the basalt to form solid carbonate minerals.\n\nOcean fertilization or ocean nourishment is a type of climate engineering based on the purposeful introduction of nutrients to the upper ocean to increase marine food production and to remove carbon dioxide from the atmosphere. A number of techniques, including fertilization by iron, urea and phosphorus have been proposed.\n\nIn their \"roadmap for rapid decarbonization\", Rockström et al. (2017) proposed in the journal Science that \"[a]fter 2030, all building construction must be carbon-neutral or carbon-negative. The construction industry must either use emissions-free concrete and steel or replace those materials with zero- or negative-emissions substances such as wood, stone, and carbon fiber. Especially wooden building constructions are a promising way to store emissions, as wood mainly contains carbon, grows faster due to the higher CO2 concentration and is established as a building material for several centuries. Projects like W350 in Tokyo demonstrate that even skyscrapers could potentially be built from wood and, thus, store large amounts of carbon.\n\nCalcium oxide (quicklime) will absorb CO from atmospheric air mixed with steam at 400 °C (forming calcium carbonate) and release it at 1,000 °C. This process, proposed by A. Steinfeld, can be performed using renewable energy from thermal concentrated solar power.\nQuicklime is made by heating limestone to release the within it.\nQuicklime is mixed with sand for brick building as mortar, where it hardens by absorption of .\n\nZeman and Lackner outlined a specific method of air capture using sodium hydroxide. Carbon Engineering, a Calgary, Alberta firm founded in 2009 and partially funded by Bill Gates, is developing a process to capture carbon dioxide using a solution of potassium hydroxide mixed with some water at their pilot plant. They hope to create and sell synthetic fuels at a cost of $100 a ton.\n\nAmong the technologies studied for direct air capture (DAC), the use of aqueous hydroxide sorbents is one of the most promising approaches.\nIn this process, CO from the air is chemically dissolved into NaOH(aq) solution as NaCO; the NaCO is then reacted with solid Ca(OH), which regenerates the solvent and produces CaCO crystals; lastly, heat is applied to the CaCO crystals to produce pure CO gas.\n\nAir is pumped through the CO absorber as the first step of this process. CO absorber for DAC are designed either as a counter-current spray tower or as a counter-current thin-falling-film contractor to maximize the contact area between the air and the solvent and thus maximize the absorption driving force. The solvent is regenerated in the causticization unit by reacting the NaCO with Ca(OH), which also transfers the captured CO to the form of CaCO solid crystals. A mechanical filter is then used to separate the CaCO crystals from the water. Since the crystals come out wet from the filter, they are dried in a steam dryer. Then the dry crystals are heated in a furnace to produce CaO and pure CO gas. The CaO is then hydrated to regenerate the Ca(OH) used for the causticization reaction. The pure CO stream is then compressed and ready to be transported for geologic sequestration, EOR, or other commercial applications.\n\n1 M NaOH (aq) is a typical solvent concentration because this concentrations is limited by the causticization reaction that regenerates the solvent it is not too far from the practical maximum of 2 M NaOH. The furnace/kiln can be powered renewably or by burning fuel on-site with pure oxygen produces in an on-site air separation unit.\n\nNaOH is economically competitive with other absorbents (e.g. Amines) used for DAC processes.\nDAC processes are energy intensive.\nCalcination (at the furnace) is the most energy intensive step of this process.\n\nA crucial issue for CDR methods is their cost, which differs substantially among the different technologies: some of these are not sufficiently developed to perform cost assessments. In 2011 the American Physical Society estimated the costs for direct air capture to be $600/tonne with optimistic assumptions. A 2018 study found this estimate lowered to between $94 and $232 per tonne. The IEA Greenhouse Gas R&D Programme and Ecofys provides an estimate that 3.5 billion tonnes could be removed annually from the atmosphere with BECCS (Bio-Energy with Carbon Capture and Storage) at carbon prices as low as €50 per tonne, while a report from Biorecro and the Global Carbon Capture and Storage Institute estimates costs \"below €100\" per tonne for large scale BECCS deployment.\n\nCDR is slow to act, and requires a long-term political and engineering program to effect. CDR is even slower to take effect on acidified oceans. In a \"Business as usual\" concentration pathway, the deep ocean will remain acidified for centuries, and as a consequence many marine species are in danger of extinction.\n\n"}
{"id": "42843259", "url": "https://en.wikipedia.org/wiki?curid=42843259", "title": "China Energy Label", "text": "China Energy Label\n\nThe China Energy Label (CEL) is an energy consumption label for products in China, similar to the European Union energy label. Manufacturers of specified electronic devices are obligated to attach a CEL label to their goods to inform China-based consumers of the product’s energy efficiency. The label includes the product’s energy efficiency class (1-5) as well as information regarding its energy consumption.\n\nThe CEL shows the level of energy consumption and, thus, the energy efficiency of a product. The CEL aims to encourage customers to buy energy efficient products. The target to increase energy efficiency and the acceptance of Chinese consumers towards energy efficient products is highly important, since China is the world's largest energy consumer.\n\nApplications are to be filed at the China Energy Label Center (CELC), which is the main authority for CEL-classification.\n\nSince the introduction of China Energy Label in 2005, more than 25 product groups have become CEL-mandatory, while additional classes continually being added to the product catalogue. For every product that requires CEL, a GB-Standard has been implemented.\n\nAmong the CEL mandatory products are:\n\n"}
{"id": "40950142", "url": "https://en.wikipedia.org/wiki?curid=40950142", "title": "Chukchi Plateau", "text": "Chukchi Plateau\n\nThe Chukchi Plateau or Chukchi Cap is a large subsea formation extending north from the Alaskan margin into the Arctic Ocean. The ridge is normally covered by ice year-round, and reaches an approximate bathymetric prominence of 3,400 m with its highest point at 246 m below sea level. As a subsea ridge extending from the continental shelf of the United States north of Alaska, the Chukchi Plateau is an important feature in maritime law of the Arctic Ocean and has been the subject of significant geographic research. The ridge has been extensively mapped by the USCGC \"Healy\", and by the Canadian icebreaker CCGS \"Louis S. St-Laurent\" (with the \"Healy\") in 2011 and RV \"Marcus Langseth\", a National Science Foundation vessel operated by the Lamont-Doherty Earth Observatory of Columbia University.\n\nThe cap is normally ice-covered, year-round. The cap lies roughly about 800 kilometres north of the Point Barrow, Alaska. The area is notable because it is believed to be rich in natural resources (especially oil, natural gas and manganese).\n\nThe geologic history of Arctic Ocean basins is a major source of debate among marine geophysicists. The difficulties associated with collecting marine geologic and geophysical data in this remote region has added to the debate on the tectonic history of the Arctic Ocean and the formation of its bathymetric features.\n\nThe Chukchi Borderland, which comprises the subsea region north of the Alaskan coast as well as the bathymetric highs of the Chukchi Plateau and the adjacent Northwind Ridge, is a continental fragment that is thought to have drifted from the Canadian continental margin. The geomorphology of the region is defined by north-south trending normal faulting –tectonic activity typical of continental rifting.\n\nAlthough there is no consensus as to the pre-rift location of the Chukchi Borderland, its tectonic migration could be attributed to an inferred spreading center indicated by a linear gravity low in the Canada Basin . Sediments transported from the Mackenzie River Delta would have buried the spreading center. The Chukchi Plateau, which could have been connected to Canada in the vicinity of Ellesmere Island, would have rifted along the spreading center to its current location. A competing hypothesis suggests that the Chukchi Plateau may have once been attached to the Siberian shelf.\n\nThe Chukchi Plateau also shows substantial evidence of pockmarks, which indicates subsurface hydrocarbon activity.\n\nUnder the United Nations Convention on the Law of the Sea, party states may submit claims to the Commission on the Limits of the Continental Shelf to extend its continental shelf beyond the 200-mile buffer that comprises a state’s exclusive economic zone. This requires that the proposed extension be a natural prolongation of the state’s continental shelf, which can be concluded through bathymetric mapping and analysis. If a state can prove that a subsea formation is a natural prolongation of its continental shelf, it must consequently locate the foot of the slope, the physical boundary between the natural prolongation and the abyssal plain of the ocean basin. The foot of the slope is then applied to two equations that inform the claim made by a state for the legal extension of its continental shelf.\n\nThe first equation calculates the Hedberg Line, which is derived by adding 60 nautical miles to the foot of the slope of the natural prolongation. The second yields the Gardiner Line, which refers to the point at which the measurement of subsea sediment thickness is 1% of the distance back to the foot of the slope. The two formulae for deriving these values can be substituted in order to form a composite continental shelf for a coastal state that yields the most advantageous possible maritime territory extension.\n\nThe United States has not ratified UNCLOS, although there is a concerted, bipartisan effort in Congress and among the various branches of the United States Military to do so. President Barack Obama also supports ratification of the treaty. If the United States were to accede to UNCLOS, they would accrue the exclusive rights to exploit the natural resources on and under the continental shelf. Some potentially extractable resources include manganese and metallic sulfides, oil, gas and gas hydrates, and sedentary species such as mollusks and clams.\n\nSince 2003, USCGC \"Healy\" has undertaken eight expeditions to the Chukchi Sea with researchers from the Center for Coastal and Ocean Mapping/Joint Hydrographic Center at the University of New Hampshire and the National Oceanic and Atmospheric Administration. The cruises investigated the bathymetry of the Chukchi Borderlands in order to inform discourse involving the potential ratification of UNCLOS by the U.S.\n\nThe latest cruise (HEALY-1202), lasting from 25 August to 27 September 2012, traversed 11,965 km of the Arctic Ocean and mapped about 68,600 km of the seafloor. The \"Healy\" was equipped with multi-beam sonar devices and seismic measurement devices, and was manned by a 34-person scientific party and around 100 US Coast Guardsmen. The expedition collected 10,030 km (5,416 nm) of multi-beam sonar data. HEALY-1202 began in Barrow, Alaska, reached a northernmost point of 83° 32’ N, 162° 36’ W, and culminated in Dutch Harbor, Alaska.\n\nPrior to the \"Healy\" cruises, the foot of the slope of the US continental shelf was believed to lie at the margin of the Chukchi Plateau. The cruises revealed that the foot of the slope was significantly further north at 81° 15’ N and a depth of approximately 3,800 m–the boundary of the Northern Chukchi Borderland with the Nautilus Basin.\n\nIn 2011 scientists aboard sea vessel, \"Marcus G. Langseth\", ran tests to increase understanding of the geology, structure and history of the continental shelves running underwater off Asia and North America, and the Chukchi Borderland, an adjoining region of dramatic deep-sea plateaus and ridges some 800 miles from the North Pole. One test includes sending sound pulses to the seabed and reading the echoes.\n"}
{"id": "6121014", "url": "https://en.wikipedia.org/wiki?curid=6121014", "title": "Cirque glacier", "text": "Cirque glacier\n\nA \"cirque glacier\" is formed in a cirque, a bowl-shaped depression on the side of or near mountains. Snow and ice accumulation in corries often occurs as the result of avalanching from higher surrounding slopes.\nIf a cirque glacier advances far enough, it may become a valley glacier. Additionally, if a valley glacier retreats enough that it is within the cirque, it becomes a cirque glacier again.\n\nIn these depressions, snow persists through summer months, and becomes glacier ice. Snow may be situated on the leeward slope of a mountain, where it is sheltered from wind. Rock fall from above slopes also plays an important role in sheltering the snow and ice from sunlight. If enough rock falls onto the glacier, it may become a rock glacier.\n\nRandklufts may form beneath corrie glaciers as open space between the ice and the bedrock, where meltwater can play a role in deposition of the rock.\n\n"}
{"id": "57063179", "url": "https://en.wikipedia.org/wiki?curid=57063179", "title": "Cuba during World War 1", "text": "Cuba during World War 1\n\nThe Republic of Cuba had maintained neutrality during much of World War I. However this changed in 1917, after German submarine warfare resumed on February 1, 1917. On April 7, 1917, one day after the United States of America entered the war, Cuba declares war on Germany and began to support the Allied war effort. Cuba also declared war on Austria-Hungary later in the year on December 16.\n\nA draft law was enacted and 25,000 soldiers were ready for shipment to France when the armistice intervened. A hospital unit of 100 Cuban doctors and nurses was equipped and sent to the Western Front (World War I).\n\nMario Garcia Menocal (December 17, 1866 - September 7, 1941) served as President of Cuba from 1913 until 1921. Supported by the USA and an economic boom, he maintained his office when World War I broke out. President Woodrow Wilson and the USA entered the war on April 6, 1917. With strong ties to its neighbours and put under preassure by New York Times reports of German U-boats resupplying in Cuba, the later followed suit an declared war on April 7.\n\nBecause of the total submarine warfare declared by Germany and the continued sinking of ships of different neutral nationalities on the American shores, Brazil and Cuba had sent angry protest to the Germans. President Menocal, encouraged by the United States` entry into the war, asked the congress of Cuba to declare war as well, enthusiastically stating that Cuba could not remain neutral. The Cuban senate unanimously passed a resolution that a state of war existed against Germany and the Cuban congress approved the declaration of war on April 7, 1917.\n\nThus Cuba was one of the few Latin American countries, along with Panama, Bolivia and Uruguay, who joined the Allies of World War I. Most countries in the region, including Mexico, maintained their neutral position.\n\nAfter the declaration of war all German ships within Havana Harbor were seized. Likewise Cuban ports opened to Allied warships. Likewise, a bill was being drafted to authorize the offering of a contingent of 12,000 men to the United States.\n\nInternal politics were strengthened by the declaration as liberals, who agreed with the move, decided to stop criticizing the government. In July 1917, the Menocal government suspended constitutional guarantees with the measure being claimed to be intended against German spies.\n\nThe Cuban government also agreed to the stationing of U.S. Marines on the island. However the Americans, afraid that this would undermine the national and international position of the Menocal government, announced that the goal of the intervention was to support the sugar harvest as major war contribution of Cuba, thus becoming known as Sugar Intervention.\n\nThe Cuban Red Cross was also reorganized, established operations in Europe and supported the Allied forces on the Western Front.\n\nThe United States declaration of war on Austria-Hungary on December 7, 1917. Panama followed on December 10, 1917 and Cuba would do the same on December 16.\n\nIn the midst of the diplomatic crisis caused by the disappearance of the diplomatic baggage of Mexican Ambassador Isidro Fabela, the government of Cuba officially acknowledged the constitutional Government of Venustiano Carranza. Federico Jimenez O'Farril presented a hand-written letter of the Cuban president to the Mexican President, in which he granted the recognition.\n\nDespite the diplomatic act, the relationship between the two countries cooled due to the treatment of the Mexican travelers in Havana.\n\nCuba during World War II"}
{"id": "58926806", "url": "https://en.wikipedia.org/wiki?curid=58926806", "title": "Domerian", "text": "Domerian\n\nThe Domerian is a sub-age of the ICS geologic timescale and a substage in the ICS stratigraphic column. It is named after Monte Domaro in the Lombard Alps and it is near the the bottom of the Jurassic system of rocks (part of the Lias series). The Domerian was identified by the Italian geologist G. Bonarelli in 1894 and it was regarded as a stratigraphic stage and age until the ICS timescale was devised about a century later. The Domerian is now the upper of two substages of the Pliensbachian stage of the Jurassic. A typical deposit of the Domerian in Europe is composed of limestones with cherts and black shales rich in ammonites. In Western Europe it is divided into the \"Amaltheus margaritatus\" and \"Pleuroceras spinatum\" zones.\n\n"}
{"id": "14135865", "url": "https://en.wikipedia.org/wiki?curid=14135865", "title": "Earthflow", "text": "Earthflow\n\nAn earthflow (earth flow) is a downslope viscous flow of fine-grained materials that have been saturated with water and moves under the pull of gravity. It is an intermediate type of mass wasting that is between downhill creep and mudflow. The types of materials that are susceptible to earthflows are clay, fine sand and silt, and fine-grained pyroclastic material.\n\nWhen the ground materials become saturated with enough water, they will start flowing (soil liquefaction). Its speed can range from being barely noticeable to rapid movement. The velocity of the flow is dictated by water content: the higher the water content is, the higher the velocity will be. Because of the dependency on water content for the velocity of the flow, it can take minutes or years for the materials to move down the slope.\n\nEarthflows are just one type of mass movement that can occur on a hill slope. It has been recognized as its own type of movement since the early 20th century. Earthflows are one of the most fluid types of mass movements. Earthflows occur on heavily saturated slopes like mudflows or a debris flow. Though earthflows are a lot like mudflows, overall they are slower and are covered with solid material carried along by flow from within. Earthflows are often made up of fine-grained materials so slopes consisting of clay and silt materials are more likely to create an earthflow.\n\nAs earthflows are usually water-dependent, the risk of one occurring is much higher in humid areas especially after a period of heavy rainfall or snowmelt. The high level of precipitation, which saturates the ground and adds water to the slope content, increases the pore-water pressure and reduces the shearing strength of the material. As the slope becomes wet, the earthflow may start as a creep downslope due to the clay or silt having less friction. As the material is increasingly more saturated, the slope will fail, which depends on slope stability. In earthflows, the slope does not fail along a clear shear plane and is instead more fluid as the material begins to move under the force of gravity as friction and slope stability is reduced.\n\nEarthflows vary in velocity of flow depending partly on the consistency of the flow for the speed of the entire movement, usually meaning how much water is in the material of the hill slope before the slope fails. Though water is often the key factor in slope failure, triggering an earthflow, there can also be dry granular flows made up of granular material. The speed also depends on the angle of slope as earthflows can happen on moderate or steep slopes. Because earthflows are usually water-dependent, they can take many years or just minutes to move a significant amount. An earthflow may affect as few as several square meters or up to several hectares in either time frame.\n\nEarthflows can have sudden impacts on the amount of sediment that is deposited into a river system, which can have effects on the life in and around the river itself. They can also cause damage to roads and constructions built near the slope. One of the best mitigation techniques to avoid serious earthflow and landslide damage is properly draining the slope of water, especially in places of high levels of precipitation.\n\nThe areas most at risk for earthflows are:\n\n\n"}
{"id": "8031668", "url": "https://en.wikipedia.org/wiki?curid=8031668", "title": "Electricity and Gas Regulation Commission", "text": "Electricity and Gas Regulation Commission\n\nThe Algerian Electricity and Gas Regulation Commission (CREG) has been created under the law n° 02-01 of 5 February 2002 relative to electricity and gas distribution.\nCREG is an independent body with a legal status and a financial autonomy. It is vested with three main missions:\n\n\nCREG has begun its activities after his installation by the Prime Minister on 24 January 2005.\nCREG is managed by a steering committee constituted of a chairman and three (03) directors appointed by presidential decree on proposition of the Minister in charge of Energy.\nAn Advisory Council is instituted within the regulation commission. It is composed of two (02) representatives of the relevant ministerial departments and of all interested parties (operators, consumers, employees).\n\nThe composition and financing of the advisory council are defined by legal ways.\n\nThe main competences and functions of the regulation commission are:\n\n1. Permits / Concessions\n\n2. Demand forecasts / Investment planning\n\n3. Remuneration of operators and tariffs\n\n4. Access to networks / Markets\n\n5. Quality, regulations /Technical and environmental control\n\n6. Consumer protection\n\nA conciliation department for conflicts resulting from the implementation of the regulations particularly those relative to access to networks, tariffs and remuneration of operators;\n\nAn arbitration chamber, on request from the parties, rules on conflicts which may arise among operators except for those related to contractual rights and obligations.\n\n"}
{"id": "14049047", "url": "https://en.wikipedia.org/wiki?curid=14049047", "title": "Energy in Chile", "text": "Energy in Chile\n\nChile's total primary energy supply (TPES) was 36.10 Mtoe in 2014. Energy in Chile is dominated by fossil fuels, with coal, oil and gas accounting for 73.4% of the total primary energy. Biofuels and waste account for another 20.5% of primary energy supply, with the rest sourced from hydro and other renewables.\n\nElectricity consumption was 68.90 TWh in 2014. Main sources of electricity in Chile are hydroelectricity, gas, oil and coal. Renewable energy in the forms of wind and solar energy are also coming into use, encouraged by collaboration since 2009 with the United States Department of Energy. The electricity industry is privatized with ENDESA as the largest company in the field.\n\nThe electricity sector in Chile relies mainly on hydro-electric power generation (33% of installed capacity as of May, 2012), oil (13%), gas (30%) and coal (20%). Wind has a small but growing presence with 198 MW. Faced with natural gas shortages that have the potential to jeopardize electricity supply, Chile is currently building its first LNG terminal to secure a supply for its existing and upcoming gas-fired thermal plants. In addition, it has engaged in the construction of several new hydropower and coal-fired thermal plants.\n\nChile’s successful electricity sector reform, which served as a model for other countries, was carried out in the first half of the 1980s. Vertical and horizontal unbundling of generation, transmission and distribution and large scale privatization led to soaring private investment. However, in recent years, there has been a substantial modification of the 1982 Electricity Act, to bring it line with developments of the last 20 years in the sector. This was the result of a pressing need to change.\n\nThe main companies involved, in terms of installed capacity, are the following:\n\n\nA number of other companies account for the remaining 14% (2418 MW)\n\nSince the introduction of the so-called \"short law II\", investments in generation have risen greatly. Currently, there are generation projects amounting to over 26,000 MW, in different stages of development. In 2013, Total S.A. announced the world's largest unsubsidised solar farm would be installed with assistance from SunPower Corp into Chile's Atacama desert.\n\nUSDOE announced on June 23, 2009 that U.S. Energy Secretary Chu has signed a Memorandum of Cooperation with Minister Marcelo Tokman of the Chilean National Energy Commission to further collaboration between the two nations. The memorandum establishes an institutional framework between Chile and the United States, allowing DOE to provide its technical expertise in support of a new Renewable Energy Center in Chile. The new center will work to identify developments and best practices in renewable energy technologies from around the world, disseminating its findings within Chile and throughout the region.\n\nThe two countries will also collaborate on other high-priority energy issues, including energy efficiency technologies and the establishment of two pilot solar power projects in northern Chile.\n\n"}
{"id": "21347660", "url": "https://en.wikipedia.org/wiki?curid=21347660", "title": "Generating Availability Data System", "text": "Generating Availability Data System\n\nThe Generating Availability Data System (GADS) is a database produced by the North American Electric Reliability Corporation (NERC). It includes annual summary reports comprising the statistics for power stations in the United States and Canada.\n\nGADS is the main source of power station outage data in North America. This reporting system, initiated by the electric utility industry in 1982, expands and extends the data collection procedures begun by the industry in 1963. NERC GADS is recognized today as a valuable source of reliability, availability, and maintainability (RAM) information.\n\nThis information, collected for both total unit and major equipment groups, is used by analysts' industry-wide in numerous applications. GADS maintains complete operating histories on more than 5,800 generating units representing 71% of the installed generating capacity of the United States and Canada. GADS is a mandatory industry program for conventional generating units 50 MW and larger starting January 1, 2012 and 20 MW and larger starting January 1, 2013. GADS remains open to all non-required participants in the Regional Entities (shown in Figure I-2 of the NERC GADS DRI) and any other organization (domestic or international) that operate electric generating facilities who is willing to follow the GADS mandatory requirements as presented in the document Final GADSTF Recommendations Report dated July 20, 2011.\n\nGADS data consists of three data types:\n\nOne example of such detail is that in its data pertaining to forced outages and unplanned unit failures, it makes the fine distinction between immediate, delayed, and postponed outages.\n\nAn important statistic calculated from the raw GADS data is the Equivalent Forced Outage Rate (EFOR), which is the hours of unit failure (unplanned outage hours and equivalent unplanned derated hours) given as a percentage of the total hours of the availability of that unit (unplanned outage, unplanned derated, and service hours).\n\nRecently, in response to the deregulated energy markets, the Equivalent Forced Outage Rate – Demand (EFORd) has taken on greater importance:\n\n\nIndustry Development of GADS\n\nBefore any data element was included in GADS, an industry committee to determine its applicability to utility operation and RAM analyses scrutinized it. A series of industry meetings were held to discuss the analytical usefulness of each element and to determine if utilities could reasonably provide that data to GADS. Consequently, the only data requested in the GADS Data Reporting Instructions (DRI) meet industry-prescribed needs.\n\nThe industry also realized a need to include standardized terminology in the GADS program if it were to function on an international scale. As a result, the definitions promulgated by The Institute of Electrical and Electronic Engineers' (IEEE) Standard 762, \"Definitions for Reporting Electric Generating Unit Reliability, Availability and Productivity\" were incorporated.\n\nUtilities started their reporting using the GADS guidelines on January 1, 1982.\n\nGADS superseded the earlier data collection procedures begun by the Edison Electric Institute (EEI), a program started in the mid-1960s. GADS contains many of the same elements previously collected by EEI in addition to the many new data items. This seeming duplication of data was done intentionally: the EEI information can be derived from GADS so analyses that include data from earlier than 1982 can be completed.\n\n"}
{"id": "8681974", "url": "https://en.wikipedia.org/wiki?curid=8681974", "title": "Georgina Basin", "text": "Georgina Basin\n\nThe Georgina Basin is a large (c. 330,000 km²) intracratonic sedimentary basin in central and northern Australia, lying mostly within the Northern Territory and partly within Queensland. It is named after the Georgina River which drains part of the basin. Deposition of locally up to c. 4 km of marine and non-marine sedimentary rocks took place from the Neoproterozoic to the late Paleozoic (c. 850-350 Ma). Along with other nearby sedimentary basins of similar age (Amadeus Basin, Officer Basin), the Georgina Basin is believed to have once been part of the hypothetical Centralian Superbasin, that was fragmented during episodes of tectonic activity.\n\n\n"}
{"id": "30630276", "url": "https://en.wikipedia.org/wiki?curid=30630276", "title": "Golosov Ravine", "text": "Golosov Ravine\n\nGolosov Ravine (Голосов Овраг), also known as Vlasov (Власов) ravine is a deep ravine in Moscow, Russia, between the Kolomenskoe Hill and Dyakovo Hill. The ravine has several springs and a brook streaming at its bottom. Up in the ravine, on the left side of it, there is a Neopagan shrine, organized around two venerated \"sacred stones\". In years 2006-2007, during the renovation of Kolomenskoe sides of the ravine were reinforced, and pedestrian paths and stairs were created on its sides.\n\nSince ancient times this ravine has always been a bit of a mystery. Inexplicable things constantly happened here in the past. For example, one amazing story was described in the 17th-century sources. In 1621, a small detachment of Tatar horsemen turned up at the walls of the Tsar’s palace in Kolomna. They were surrounded and captured immediately by the warriors, who guarded the entrance to the palace. Being lost and disoriented, they claimed to be part of the armies of the Crimean khan Devlet I Giray that had attacked Moscow in 1571. Sensing defeat and wanting to avoid capture, they retreated into Golosov ravine, where they were quickly enveloped in a thick mist. Spending what seemed like only a few minutes finding their way through the fog, they emerged to find that 50 years had already passed. One of the captives, named \"Murza\", said that the mist was unusual, of light green colour, but none of them paid attention to it, due to fear of the chasing. Tsar Mikhail Fedorovich ordered to undertake an inquiry, which showed that the men “probably told the truth.” They had even old-fashioned armor and ammunition, mostly of 1560s or 1570s types.\n\nThis wasn’t the only reported incident of time travel in the surroundings of Golosov Ravine. In the 19th century, such cases were documented by the Moscow province's police office. One story was published in the newspaper \"“Moscovkie vedomosti”\" in July 1832. Two local peasants, \"Arhip Kuzmin\" and \"Ivan Botchkarev\", had a big night out at a neighboring village and decided to take a shortcut through the ravine on their way home. There was the same thick green mist that filled the ravine, where a strange corridor suddenly appeared. The peasants stepped down into it and met large and hairy manlike creatures, who managed to put them back on the right path via hand signals. A few minutes later, the peasants left the ravine and continued their way. When they eventually came to their native village, it turned out that two decades had already passed. Their wives and children did not recognize them. The incident was reported to the police who conducted an investigation, during which one of the men dissolved into the fog and never returned.\n\nDown through the centuries, people have been periodically seen hairy human-resembling creatures round Golosov ravine. Such cases were mentioned not only in the ancient and medieval chronicles, but also in the Soviet periodical press. Large hairy manlike creatures most likely represent the leshy - a sort of bigfoot from Slavic mythology, who is more man than ape and more woodland spirit than mortal creature. In 1926, a policeman reported seeing a “hairy wild man” over two meters tall (6.6 ft) in Golosov Ravine. The policeman pulled out his pistol, but the strange creature instantly dissolved in the fog. Local schoolchildren joined the search for the unusual guest, but they were unable to detect his traces. However, one of the official Moscow newspapers published the article “Young Pioneers (Soviet Boy Scouts) are capturing a leshy”, by the journalist A.Ryazantsev.\n\nThe \"sacred stones of Kolomenskoe\" are a pair of local sandstone rocks of peculiar shape, located high in the ravine. Some sources claim them to be granite boulders of glacial origin, but this seems to be a misconception. Both rocks have traces of manual processing, both old (exaggerating the shape of the stones), and new (as they have been vandalized by modern graffiti). Initially the stones were located further down the ravine, closer to the springs, but during one of the renovations of the park in the Soviet era they were dragged to the place where they reside now.\n\nAccording to a recently popularized theory, Golosov Ravine might have initially hosted a shrine dedicated to the Slavic deity Veles. The name of Veles is said to be traceable in modern name of the ravine (Golosov or Vlasov, through Volosov, from Velesov). The shrine might have been later Christianized, with the stones re-interepreted by local inhabitants as traces of a famous battle between St. George (the patron saint of Moscow) and the dragon, thus preserving the ancient mythological motif under new names (see \"Enemy of Perun and storm myth\" section in Veles article).\n\nThe stones have their own names: one is called \"Deviy\" (or \"Devichiy\", , meaning \"Virgin\"), and is associated by modern worshipers with giving fertility to women, while the other one is called \"Gus’\" (\"Гусь\", meaning \"Goose\"). Local lore tells that they help to cure certain diseases, so people come and sit by them, and also tie small pieces of tissue onto nearby trees.\n\nAccording to some sources, the stones were not continuously venerated by locals in the 20th century, which would mean that the tradition is discontinuous, and may not follow the older patterns, whatever they might have been.\n\nThe nearby springs are also considered sacred (miracle-bearing) in contemporary Eastern Orthodoxy, Neopagan and New-Age traditions. Before the Revolution of 1917 there was a wooden chapel standing on top of (or near?) the springs, which implies that the springs were considered \"sacred\" or \"holy\" in the past as well. Several springs have (or had) their own names: Kadochka (literally: \"Little Tub\"; seemingly the most venerated one, with its sub-springs associated with St. George and Our Lady of Kazan); Peter and Paul's spring; the spring of the 12 apostles; St. Nicholas spring. Some of these springs were destroyed during the recent renovation works in the ravine.\n"}
{"id": "31362714", "url": "https://en.wikipedia.org/wiki?curid=31362714", "title": "Great Flood (China)", "text": "Great Flood (China)\n\nThe Great Flood of Gun-Yu (), also known as the Gun-Yu myth, was a major flood event in ancient China that allegedly continued for at least two generations, which resulted in great population displacements among other disasters, such as storms and famine. People left their homes to live on the high hills and mounts, or nest on the trees. According to mythological and historical sources, it is traditionally dated to the third millennium BCE, during the reign of Emperor Yao. Archaeological evidence of an outburst flood on the Yellow River (possibly among the worst anywhere in the world in the past 10,000 years) has been dated to about 1920 BCE (several centuries later than the traditional recorded beginning of the Xia dynasty), and is suggested to have been the basis for the myth.\n\nTreated either historically or mythologically, the story of the Great Flood and the heroic attempts of the various human characters to control it and to abate the disaster is a narrative fundamental to Chinese culture. Among other things, the Great Flood of China is key to understanding the history of the founding of both the Xia dynasty and the Zhou dynasty, it is also one of the main flood motifs in Chinese mythology, and it is a major source of allusion in Classical Chinese poetry.\n\nThe story of the Great Flood plays a dramatic role in Chinese mythology, and its various versions present a number of examples of the flood myth motif around the world. Flood narratives in Chinese mythology share certain common features, despite being somewhat lacking in internal consistency as well as incorporating various magical transformations and divine or semi-divine interventions like Nüwa. For example, the flood usually results from natural causes rather than \"universal punishment for human sin\". Another distinct motif of the myth of the Great Flood of China is an emphasis on the heroic and praiseworthy efforts to mitigate the disaster; flooding is alleviated by constructing dikes and dams (such as the efforts of Gun), digging canals (as devised by Yu the Great), widening or deepening existing channels, and teaching these skills to others.\n\nAnother key motif is the development of civilization and bettering the human situation despite the disaster of the deluge. During the course of fighting, surviving, and eventually getting the inundation problems under control, much progress was also made in terms of land management, beast control, and agricultural techniques. These and other developments are integral to the narrative, and exemplify a wider approach to human health and societal well being than emergency management of the flood and its immediate effects. According to legend, a comprehensive approach to societal development resulted not only in wide-scale cooperation and large-scale efforts to control the flood but also led to the establishment of the first state of China, the Xia dynasty (ca. 2070 – ca. 1600 BC).\n\nIt was during the reign of Emperor Yao that the Great Flood began, a flood so vast that no part of Yao's territory was spared, and both the Yellow River and the Yangtze valleys flooded. The alleged nature of the flood is shown in the following quote:\n\nAccording to both historical and mythological sources, the flooding continued relentlessly. Yao sought to find someone who could control the flood, and turned for advice to his special adviser, or advisers, the Four Mountains (四嶽 or 四岳, \"Sìyuè\"); who, after deliberation, gave Emperor Yao some advice which he did not especially welcome.\n\nUpon the insistence of Four Mountains, and over Yao's initial hesitation, the person Yao finally consented to appoint in charge of controlling the flood was Gun, the Prince of Chong, who was a distant relative of Yao's through common descent from the Yellow Emperor.\n\nAccording to the main mythological tradition, Gun's plan of flood control was through the use of a miraculously continuously self-expanding soil, Xirang. Gun chose to obtain the Xirang by stealing it from the Supreme Divinity, which he did; however, the Supreme Divinity became quite angered at this importunity. Year in and year out, many times, and to great extents; Gun applied the magical Xirang earth in attempt to block and barricade the flood waters with dams, dikes, and embankments (which he built with the special powers of the magic soil). However, Gun was never able to abate the problems of the Great Flood. Whether his failure to abate the flood was due to divine wrath or to engineering defects remains an unanswered question – although one pointed out over two thousand years ago by Qu Yuan, in his \"Heavenly Questions\".\n\nEven after nine years of the efforts of Gun, the flood continued to rage on, leading to the increase of all sorts of social disorders. The administration of the empire was becoming increasingly difficult; so, accordingly, at this point, Yao offered to resign the throne in favor of his special adviser(s), Four Mountains: however, Four Mountains declined, and instead recommended Shun – another distant relative to Yao through the Yellow Emperor; but one who was living in obscurity, despite his royal lineage.\n\nYao proceeded to put Shun through a series of tests, beginning with marrying his two daughters to Shun and ending by sending him down from the mountains to the plains below where Shun had to face fierce winds, thunder, and rain. After passing all of Yao's tests, not the least of which being establishing and continuing a state of marital harmony together with Yao's two daughters, Shun took on administrative responsibilities as co-emperor. Among these responsibilities, Shun had to deal with the Great Flood and its associated disruptions, especially in light of the fact that Yao's reluctant decision to appoint Gun to handle the problem had failed to fix the situation, despite having been working on it for the previous nine years. Shun took steps over the next four years to reorganize the empire, in such a way as to solve immediate problems and to put the imperial authority in a better position to deal with the flood and its effects.\n\nAlthough Shun's organization (or reorganization) of the flooded and increasingly flooded lands into \"zhou\" or islands (the political ancestors of the modern \"zhou\" or provinces, both of which may be written with the same character, 州) alleviated some of the administrative difficulties as a work around to various problems, the fact remained that despite the additional four years of effort, Gun still had not only failed to achieve any success towards solving the main problem of the ongoing flooding, but the water even kept on rising. Gun insisted on staying the course with the dikes, insisting that despite the overwhelming failure so far that the people work even harder and to continue to build more and higher. Not only that, but Gun questioned the legitimacy of Shun as a ruler due to his modest background.\n\nAfter the solemnities of his final accession to power, the first thing Shun did was to reform the calendar. Next, for the period of a month, Shun convoked a series of meetings, ceremonies, and interviews at the imperial capital with the Four Mountains and the heads, lords, or princes of the realm's houses, clans, surnames, tribes, and nations.\n\nShun then went to Mount Tai as the beginning of his tour of inspection of the flood-ravaged realm. Here, at Taishan, he met with the princes of the eastern regions; and, after certain religious ceremonies, he standardized weights, measures, and ritual. Then he went on to do the same to the south, the west, and the north, meeting at the sacred mountains of each region with the princes and leaders of each region, and standardizing their rules, measures, and practices. All of these acts can be seen as preparatory to the fighting of the flood, as this was an effort requiring extraordinary levels of synchronized and coordinated activity over a relatively large territory: the timing was synchronized through the calendar reform and the engineering measures were made possible by standardizing the weights and measures.\n\nTowards the end of the year, Shun returned to the imperial seat, and after a sacrificial offering of a bullock at his ancestral temple, he then put into action the plan that he had developed during his working tour of inspection. One of these was to divide the empire into twelve administrative units (\"zhou\"), each one administered from the highest mountain within that area. This was doubtlessly a useful expedient in the face of the rising and unpredictable flood waters. Another of Shun's acts was administrative reform.\n\nWith Gun's overwhelming failure to control the flood waters and his questioning of the legitimacy of Shun's rule, he became labeled as an intransigent. Accordingly, as part of his administrative reforms, Shun had Gun banished to Feather Mountain. Accounts vary considerably about the details of Gun's demise; but, in any case, the sources seem to agree that he met the end of his human existence at Feather Mountain (although again accounts vary as to whether this end was death, through execution by Zhurong, or through a metamorphic transformation into — depending on account — a yellow bear, a three-footed tortoise, or a yellow dragon.)\n\nSomehow, Gun had a son Yu. Various myths suggest that this occurred under circumstances that would not meet the normal criteria for historical fact. Yu would continue the struggle to contain the flood waters.\n\nYu tried a different approach to the project of flood control; which in the end having achieved success, earned Yu renown throughout Chinese history, in which the Gun-Yu Great Flood is commonly referred to as \"Great Yu Controls the Waters\" (). Yu's approach seems to have involved an approach more oriented toward drainage and less towards containment with dams and dikes. According to the more fancily embellished versions of the story it was also necessary for him to subdue various supernatural beings as well as recruit the assistance of others, for instance a channel-digging dragon and a giant mud-hauling tortoise (or turtle).\n\nVarious myths, or versions of myths, specify that Yu received help from various sources which helped him to succeed in controlling the Great Flood. Hebo, the god of the Yellow River is supposed to have provided Yu with a map of the river and its surroundings which helped him make his plans. Alternatively, the Yellow River map is said to have been provided to Yu by Houtu.\n\nAfter his work in controlling the flood waters, Yu became sole emperor and went on to found the Xia dynasty, when his son Qi of Xia succeeded him, thus establishing the beginning of a tradition of dynastic succession through primogeniture. But, before this, after ending his work against the flooding, Yu was said to have assembled all of the heroes/gods involved in fighting the flood together on Mount Guiji (in modern Zhejiang) at a certain time; but, when Fangfeng arrived late Yu had him executed — later it turned out that Fangfeng was late because he had stopped to fight a local flood which he encountered on his way.\n\nBesides the motif of controlling the flood waters another motif is particularly characteristic of the Chinese Gun Yu flood myth, namely the acquisition of the agricultural civilization. In some versions, this includes the appointment of Ji Qi (later called Houji) as Minister of Agriculture. Other versions go into the details of how a tiny remnant of people consisting of only two or a few individuals managed to survive the flood and the re-population/civilization process following the worldwide disaster, and/or how grain seeds or fire were obtained. Another figure in this regard is Yi, also known as Boyi or Bo Yi.\n\nThe narrative of the Great Flood of prehistoric China may provide some insight into social development during this era. David Hawkes comments on the way that the various versions of the Gun-Yu story seem to contrast the relative success or failure, or at least the differences, between Gun, the father, and his son, Yu. Hawkes proposes a symbolic interpretation of a societal transition. In this case, Gun represents a society at an earlier technological stage, which engages in small scale agriculture which involves raising areas of arable land sufficiently above the level of the marshes existing then in the flood plains of the Yellow River system, including tributaries: from this perspective the \"magically-expanding\" \"xirang\" soil can be understood as representing a type of floating garden, made up of soil, brushwood, and similar materials. Yu and his work in controlling the flood would symbolize a later type of society, a one which possessed of technological innovations allowing a much larger scale approach to transforming wetlands to arable fields. Hawkes explains the miraculous transformations of the landscape which appear in the mythological descriptions as symbolically representative of a gridded drainage system engineered to permanently eliminate entire marsh areas, in favor of agriculturally exploitable fields.\n\nRecent archaeological and geological discoveries may have some bearing on the story of the Great Flood. Archaeological evidence of a large outburst flood on the Yellow River has been dated to about 1920 BCE, and is suggested to have been the basis for the later myth. A colossal landslide created a natural dam across the river which was breached about a year later. The resulting flood could plausibly have travelled down the river and the resulting instability of river channels might have lasted up to twenty years. About this time, the Neolithic gave way to the Bronze Age in the Yellow River valley. The authors suggest that this coincided with the beginning of the Xia, several centuries later than traditionally thought, and that the Erlitou culture is an archaeological manifestation of the Xia dynasty.\n\nThe historian K. C. Wu believes that the \"Canon of Yao\" (\"yaodian\") in the Book of History (\"Shujing\") has historical value, despite being one of the \"second batch\" or \"new\" texts comprising this collection of documents, which despite the problematic nature of their textual transmission, and that they appear to be reconstructed or heavily edited and interpolated, as compared with the \"first\" or \"old\" batch, which supposedly survived the Fires of Qin (the Burning of books and burying of scholars together with the destruction by fire of the Qin imperial library at the collapse of its dynasty). The first batch documents allegedly remained hidden for about a century, until accidentally discovered and handed over to a descendent of Confucius. Wu accepts that the \"yaodian\" is not a direct copy of the original, however he argues that it was based on the same, authentic sources as the first batch documents, perhaps even being to some extent based on the actual original. However, the clinching factor which K. C. Wu claims is objective, extra-textual confirmation of \"Yao's Canon\" (and by implication, the rest of the second batch documents) has directly to do with dating the Great Flood, specifically to around the year 2200 BCE. This is based on comparing astronomical data from the text with modern astronomical or astrophysical analysis.\n\nAt the beginning of his reign, Yao was supposed to have appointed four ministerial officials (two sets of two brothers) to make the necessary astronomical observations for a reformed calendar. Each of these individuals were sent to the limits of the royal territory, one in each of the cardinal directions, where they were supposed to observe certain stars at sunset on each of the solstices and equinoxes, so the results could then be compared, and the calendar accordingly adjusted. K. C. Wu cites references from two modern astronomers that largely confirm a date of around 2200 for Yao's reign, which is in accord with traditional, accepted dating.\n\nIn a more mythological view of Yao and his reign, this evidence for accurate astronomical observations could be interpreted as an intrusion of archeoastronomy into the realm of myth.\n\n\n\n"}
{"id": "15097", "url": "https://en.wikipedia.org/wiki?curid=15097", "title": "Ionosphere", "text": "Ionosphere\n\nThe ionosphere () is the ionized part of Earth's upper atmosphere, from about to altitude, a region that includes the thermosphere and parts of the mesosphere and exosphere. The ionosphere is ionized by solar radiation. It plays an important role in atmospheric electricity and forms the inner edge of the magnetosphere. It has practical importance because, among other functions, it influences radio propagation to distant places on the Earth.\n\nAs early as 1839, the German mathematician and physicist Carl Friedrich Gauss postulated that an electrically conducting region of the atmosphere could account for observed variations of Earth's magnetic field. Sixty years later, Guglielmo Marconi received the first trans-Atlantic radio signal on December 12, 1901, in St. John's, Newfoundland (now in Canada) using a kite-supported antenna for reception. The transmitting station in Poldhu, Cornwall, used a spark-gap transmitter to produce a signal with a frequency of approximately 500 kHz and a power of 100 times more than any radio signal previously produced. The message received was three dits, the Morse code for the letter S. To reach Newfoundland the signal would have to bounce off the ionosphere twice. Dr. Jack Belrose has contested this, however, based on theoretical and experimental work. However, Marconi did achieve transatlantic wireless communications in Glace Bay, Nova Scotia, one year later.\n\nIn 1902, Oliver Heaviside proposed the existence of the Kennelly–Heaviside layer of the ionosphere which bears his name. Heaviside's proposal included means by which radio signals are transmitted around the Earth's curvature. Heaviside's proposal, coupled with Planck's law of black body radiation, may have hampered the growth of radio astronomy for the detection of electromagnetic waves from celestial bodies until 1932 (and the development of high-frequency radio transceivers). Also in 1902, Arthur Edwin Kennelly discovered some of the ionosphere's radio-electrical properties.\n\nIn 1912, the U.S. Congress imposed the Radio Act of 1912 on amateur radio operators, limiting their operations to frequencies above 1.5 MHz (wavelength 200 meters or smaller). The government thought those frequencies were useless. This led to the discovery of HF radio propagation via the ionosphere in 1923.\n\nIn 1926, Scottish physicist Robert Watson-Watt introduced the term \"ionosphere\" in a letter published only in 1969 in \"Nature\":\n\nIn the early 1930s, test transmissions of Radio Luxembourg inadvertently provided evidence of the first radio modification of the ionosphere; HAARP ran a series of experiments in 2017 using the eponymous Luxembourg Effect.\n\nEdward V. Appleton was awarded a Nobel Prize in 1947 for his confirmation in 1927 of the existence of the ionosphere. Lloyd Berkner first measured the height and density of the ionosphere. This permitted the first complete theory of short-wave radio propagation. Maurice V. Wilkes and J. A. Ratcliffe researched the topic of radio propagation of very long radio waves in the ionosphere. Vitaly Ginzburg has developed a theory of electromagnetic wave propagation in plasmas such as the ionosphere.\n\nIn 1962, the Canadian satellite Alouette 1 was launched to study the ionosphere. Following its success were Alouette 2 in 1965 and the two ISIS satellites in 1969 and 1971, further AEROS-A and -B in 1972 and 1975, all for measuring the ionosphere.\n\nOn July 26, 1963 the first operational geosynchronous satellite Syncom 2 was launched. The board radio beacons on this satellite (and its successors) enabled – for the first time – the measurement of total electron content (TEC) variation along a radio beam from geostationary orbit to an earth receiver. (The rotation of the plane of polarization directly measures TEC along the path.) Australian geophysicist Elizabeth Essex-Cohen from 1969 onwards was using this technique to monitor the atmosphere above Australia and Antarctica.\n\nThe ionosphere is a shell of electrons and electrically charged atoms and molecules that surrounds the Earth, stretching from a height of about to more than . It exists primarily due to ultraviolet radiation from the Sun.\n\nThe lowest part of the Earth's atmosphere, the troposphere extends from the surface to about . Above that is the stratosphere, followed by the mesosphere. In the stratosphere incoming solar radiation creates the ozone layer. At heights of above , in the thermosphere, the atmosphere is so thin that free electrons can exist for short periods of time before they are captured by a nearby positive ion. The number of these free electrons is sufficient to affect radio propagation. This portion of the atmosphere is partially \"ionized\" and contains a plasma which is referred to as the ionosphere.\n\nUltraviolet (UV), X-ray and shorter wavelengths of solar radiation are \"ionizing,\" since photons at these frequencies contain sufficient energy to dislodge an electron from a neutral gas atom or molecule upon absorption. In this process the light electron obtains a high velocity so that the temperature of the created electronic gas is much higher (of the order of thousand K) than the one of ions and neutrals. The reverse process to ionization is recombination, in which a free electron is \"captured\" by a positive ion. Recombination occurs spontaneously, and causes the emission of a photon carrying away the energy produced upon recombination. As gas density increases at lower altitudes, the recombination process prevails, since the gas molecules and ions are closer together. The balance between these two processes determines the quantity of ionization present.\n\nIonization depends primarily on the Sun and its activity. The amount of ionization in the ionosphere varies greatly with the amount of radiation received from the Sun. Thus there is a diurnal (time of day) effect and a seasonal effect. The local winter hemisphere is tipped away from the Sun, thus there is less received solar radiation. The activity of the Sun is associated with the sunspot cycle, with more radiation occurring with more sunspots. Radiation received also varies with geographical location (polar, auroral zones, mid-latitudes, and equatorial regions). There are also mechanisms that disturb the ionosphere and decrease the ionization. There are disturbances such as solar flares and the associated release of charged particles into the solar wind which reaches the Earth and interacts with its geomagnetic field.\n\nAt night the F layer is the only layer of significant ionization present, while the ionization in the E and D layers is extremely low. During the day, the D and E layers become much more heavily ionized, as does the F layer, which develops an additional, weaker region of ionisation known as the F layer. The F layer persists by day and night and is the main region responsible for the refraction and reflection of radio waves.\n\nThe D layer is the innermost layer, to above the surface of the Earth. Ionization here is due to Lyman series-alpha hydrogen radiation at a wavelength of 121.6 nanometre (nm) ionizing nitric oxide (NO). In addition, high solar activity can generate hard X-rays (wavelength ) that ionize N and O. Recombination rates are high in the D layer, so there are many more neutral air molecules than ions.\n\nMedium frequency (MF) and lower high frequency (HF) radio waves are significantly attenuated within the D layer, as the passing radio waves cause electrons to move, which then collide with the neutral molecules, giving up their energy. Lower frequencies experience greater absorption because they move the electrons farther, leading to greater chance of collisions. This is the main reason for absorption of HF radio waves, particularly at 10 MHz and below, with progressively less absorption at higher frequencies. This effect peaks around noon and is reduced at night due to a decrease in the D layer's thickness; only a small part remains due to cosmic rays. A common example of the D layer in action is the disappearance of distant AM broadcast band stations in the daytime.\n\nDuring solar proton events, ionization can reach unusually high levels in the D-region over high and polar latitudes. Such very rare events are known as Polar Cap Absorption (or PCA) events, because the increased ionization significantly enhances the absorption of radio signals passing through the region. In fact, absorption levels can increase by many tens of dB during intense events, which is enough to absorb most (if not all) transpolar HF radio signal transmissions. Such events typically last less than 24 to 48 hours.\n\nThe E layer is the middle layer, to above the surface of the Earth. Ionization is due to soft X-ray (1–10 nm) and far ultraviolet (UV) solar radiation ionization of molecular oxygen (O). Normally, at oblique incidence, this layer can only reflect radio waves having frequencies lower than about 10 MHz and may contribute a bit to absorption on frequencies above. However, during intense sporadic E events, the E layer can reflect frequencies up to 50 MHz and higher. The vertical structure of the E layer is primarily determined by the competing effects of ionization and recombination. At night the E layer weakens because the primary source of ionization is no longer present. After sunset an increase in the height of the E layer maximum increases the range to which radio waves can travel by reflection from the layer.\n\nThis region is also known as the Kennelly–Heaviside layer or simply the Heaviside layer. Its existence was predicted in 1902 independently and almost simultaneously by the American electrical engineer Arthur Edwin Kennelly (1861–1939) and the British physicist Oliver Heaviside (1850–1925). However, it was not until 1924 that its existence was detected by Edward V. Appleton and Miles Barnett.\n\nThe E layer (sporadic E-layer) is characterized by small, thin clouds of intense ionization, which can support reflection of radio waves, rarely up to 225 MHz. Sporadic-E events may last for just a few minutes to several hours. Sporadic E propagation makes VHF-operating radio amateurs very excited, as propagation paths that are generally unreachable can open up. There are multiple causes of sporadic-E that are still being pursued by researchers. This propagation occurs most frequently during the summer months when high signal levels may be reached. The skip distances are generally around . Distances for one hop propagation can be anywhere from to . Double-hop reception over is possible.\n\nThe F layer or region, also known as the Appleton–Barnett layer, extends from about to more than above the surface of Earth. It is the layer with the highest electron density, which implies signals penetrating this layer will escape into space. Electron production is dominated by extreme ultraviolet (UV, 10–100 nm) radiation ionizing atomic oxygen. The F layer consists of one layer (F) at night, but during the day, a secondary peak (labelled F) often forms in the electron density profile. Because the F layer remains by day and night, it is responsible for most skywave propagation of radio waves and long distances high frequency (HF, or shortwave) radio communications.\n\nAbove the F layer, the number of oxygen ions decreases and lighter ions such as hydrogen and helium become dominant. This region above the F layer peak and below the plasmasphere is called the topside ionosphere.\n\nFrom 1972 to 1975 NASA launched the AEROS and AEROS B satellites to study the F region.\n\nAn ionospheric model is a mathematical description of the ionosphere as a function of location, altitude, day of year, phase of the sunspot cycle and geomagnetic activity. Geophysically, the state of the ionospheric plasma may be described by four parameters: \"electron density, electron and ion temperature\" and, since several species of ions are present, \"ionic composition\". Radio propagation depends uniquely on electron density.\n\nModels are usually expressed as computer programs. The model may be based on basic physics of the interactions of the ions and electrons with the neutral atmosphere and sunlight, or it may be a statistical description based on a large number of observations or a combination of physics and observations. One of the most widely used models is the International Reference Ionosphere (IRI), which is based on data and specifies the four parameters just mentioned. The IRI is an international project sponsored by the Committee on Space Research (COSPAR) and the International Union of Radio Science (URSI). The major data sources are the worldwide network of ionosondes, the powerful incoherent scatter radars (Jicamarca, Arecibo, Millstone Hill, Malvern, St Santin), the ISIS and Alouette topside sounders, and in situ instruments on several satellites and rockets. IRI is updated yearly. IRI is more accurate in describing the variation of the electron density from bottom of the ionosphere to the altitude of maximum density than in describing the total electron content (TEC). Since 1999 this model is \"International Standard\" for the terrestrial ionosphere (standard TS16457).\n\nIonograms allow deducing, via computation, the true shape of the different layers. Nonhomogeneous structure of the electron/ion-plasma produces rough echo traces, seen predominantly at night and at higher latitudes, and during disturbed conditions.\n\nAt mid-latitudes, the F layer daytime ion production is higher in the summer, as expected, since the Sun shines more directly on the Earth. However, there are seasonal changes in the molecular-to-atomic ratio of the neutral atmosphere that cause the summer ion loss rate to be even higher. The result is that the increase in the summertime loss overwhelms the increase in summertime production, and total F ionization is actually lower in the local summer months. This effect is known as the winter anomaly. The anomaly is always present in the northern hemisphere, but is usually absent in the southern hemisphere during periods of low solar activity.\n\nWithin approximately ± 20 degrees of the \"magnetic equator\", is the \"equatorial anomaly\". It is the occurrence of a trough in the ionization in the F layer at the equator and crests at about 17 degrees in magnetic latitude. The Earth's magnetic field lines are horizontal at the magnetic equator. Solar heating and tidal oscillations in the lower ionosphere move plasma up and across the magnetic field lines. This sets up a sheet of electric current in the E region which, with the horizontal magnetic field, forces ionization up into the F layer, concentrating at ± 20 degrees from the magnetic equator. This phenomenon is known as the \"equatorial fountain\".\n\nThe worldwide solar-driven wind results in the so-called Sq (solar quiet) current system in the E region of the Earth's ionosphere (ionospheric dynamo region) ( altitude). Resulting from this current is an electrostatic field directed west–east (dawn–dusk) in the equatorial day side of the ionosphere. At the magnetic dip equator, where the geomagnetic field is horizontal, this electric field results in an enhanced eastward current flow within ± 3 degrees of the magnetic equator, known as the equatorial electrojet.\n\nWhen the Sun is active, strong solar flares can occur that will hit the sunlit side of Earth with hard X-rays. The X-rays will penetrate to the D-region, releasing electrons that will rapidly increase absorption, causing a high frequency (3–30 MHz) radio blackout. During this time very low frequency (3–30 kHz) signals will be reflected by the D layer instead of the E layer, where the increased atmospheric density will usually increase the absorption of the wave and thus dampen it. As soon as the X-rays end, the sudden ionospheric disturbance (SID) or radio black-out ends as the electrons in the D-region recombine rapidly and signal strengths return to normal.\n\nAssociated with solar flares is a release of high-energy protons. These particles can hit the Earth within 15 minutes to 2 hours of the solar flare. The protons spiral around and down the magnetic field lines of the Earth and penetrate into the atmosphere near the magnetic poles increasing the ionization of the D and E layers. PCA's typically last anywhere from about an hour to several days, with an average of around 24 to 36 hours. Coronal mass ejections can also release energetic protons that enhance D-region absorption in the polar regions.\n\nA geomagnetic storm is a temporary intense disturbance of the Earth's magnetosphere.\n\nLightning can cause ionospheric perturbations in the D-region in one of two ways. The first is through VLF (very low frequency) radio waves launched into the magnetosphere. These so-called \"whistler\" mode waves can interact with radiation belt particles and cause them to precipitate onto the ionosphere, adding ionization to the D-region. These disturbances are called \"lightning-induced electron precipitation\" (LEP) events.\n\nAdditional ionization can also occur from direct heating/ionization as a result of huge motions of charge in lightning strikes. These events are called early/fast.\n\nIn 1925, C. T. R. Wilson proposed a mechanism by which electrical discharge from lightning storms could propagate upwards from clouds to the ionosphere. Around the same time, Robert Watson-Watt, working at the Radio Research Station in Slough, UK, suggested that the ionospheric sporadic E layer (E) appeared to be enhanced as a result of lightning but that more work was needed. In 2005, C. Davis and C. Johnson, working at the Rutherford Appleton Laboratory in Oxfordshire, UK, demonstrated that the E layer was indeed enhanced as a result of lightning activity. Their subsequent research has focused on the mechanism by which this process can occur.\n\nDue to the ability of ionized atmospheric gases to refract high frequency (HF, or shortwave) radio waves, the ionosphere can reflect radio waves directed into the sky back toward the Earth. Radio waves directed at an angle into the sky can return to Earth beyond the horizon. This technique, called \"skip\" or \"skywave\" propagation, has been used since the 1920s to communicate at international or intercontinental distances. The returning radio waves can reflect off the Earth's surface into the sky again, allowing greater ranges to be achieved with multiple hops. This communication method is variable and unreliable, with reception over a given path depending on time of day or night, the seasons, weather, and the 11-year sunspot cycle. During the first half of the 20th century it was widely used for transoceanic telephone and telegraph service, and business and diplomatic communication. Due to its relative unreliability, shortwave radio communication has been mostly abandoned by the telecommunications industry, though it remains important for high-latitude communication where satellite-based radio communication is not possible. Some broadcasting stations and automated services still use shortwave radio frequencies, as do radio amateur hobbyists for private recreational contacts.\n\nWhen a radio wave reaches the ionosphere, the electric field in the wave forces the electrons in the ionosphere into oscillation at the same frequency as the radio wave. Some of the radio-frequency energy is given up to this resonant oscillation. The oscillating electrons will then either be lost to recombination or will re-radiate the original wave energy. Total refraction can occur when the collision frequency of the ionosphere is less than the radio frequency, and if the electron density in the ionosphere is great enough.\n\nA qualitative understanding of how an electromagnetic wave propagates through the ionosphere can be obtained by recalling geometric optics. Since the ionosphere is a plasma, it can be shown that the refractive index is less than unity. Hence, the electromagnetic \"ray\" is bent away from the normal rather than toward the normal as would be indicated when the refractive index is greater than unity. It can also be shown that the refractive index of a plasma, and hence the ionosphere, is frequency-dependent, see Dispersion (optics).\n\nThe critical frequency is the limiting frequency at or below which a radio wave is reflected by an ionospheric layer at vertical incidence. If the transmitted frequency is higher than the plasma frequency of the ionosphere, then the electrons cannot respond fast enough, and they are not able to re-radiate the signal. It is calculated as shown below:\n\nwhere N = electron density per m and f is in Hz.\n\nThe Maximum Usable Frequency (MUF) is defined as the upper frequency limit that can be used for transmission between two points at a specified time.\n\nwhere formula_3 = angle of attack, the angle of the wave relative to the horizon, and sin is the sine function.\n\nThe cutoff frequency is the frequency below which a radio wave fails to penetrate a layer of the ionosphere at the incidence angle required for transmission between two specified points by refraction from the layer.\n\nThe open system electrodynamic tether, which uses the ionosphere, is being researched. The space tether uses plasma contactors and the ionosphere as parts of a circuit to extract energy from the Earth's magnetic field by electromagnetic induction.\n\nScientists explore the structure of the ionosphere by a wide variety of methods. They include:\n\nA variety of experiments, such as HAARP (High Frequency Active Auroral Research Program), involve high power radio transmitters to modify the properties of the ionosphere. These investigations focus on studying the properties and behavior of ionospheric plasma, with particular emphasis on being able to understand and use it to enhance communications and surveillance systems for both civilian and military purposes. HAARP was started in 1993 as a proposed twenty-year experiment, and is currently active near Gakona, Alaska.\n\nThe SuperDARN radar project researches the high- and mid-latitudes using coherent backscatter of radio waves in the 8 to 20 MHz range. Coherent backscatter is similar to Bragg scattering in crystals and involves the constructive interference of scattering from ionospheric density irregularities. The project involves more than 11 different countries and multiple radars in both hemispheres.\n\nScientists are also examining the ionosphere by the changes to radio waves, from satellites and stars, passing through it. The Arecibo radio telescope located in Puerto Rico, was originally intended to study Earth's ionosphere.\n\nIonograms show the virtual heights and critical frequencies of the ionospheric layers and which are measured by an ionosonde. An ionosonde sweeps a range of frequencies, usually from 0.1 to 30 MHz, transmitting at vertical incidence to the ionosphere. As the frequency increases, each wave is refracted less by the ionization in the layer, and so each penetrates further before it is reflected. Eventually, a frequency is reached that enables the wave to penetrate the layer without being reflected. For ordinary mode waves, this occurs when the transmitted frequency just exceeds the peak plasma, or critical, frequency of the layer. Tracings of the reflected high frequency radio pulses are known as ionograms. Reduction rules are given in: \"URSI Handbook of Ionogram Interpretation and Reduction\", edited by William Roy Piggott and Karl Rawer, Elsevier Amsterdam, 1961 (translations into Chinese, French, Japanese and Russian are available).\n\nIncoherent scatter radars operate above the critical frequencies. Therefore, the technique allows probing the ionosphere, unlike ionosondes, also above the electron density peaks. The thermal fluctuations of the electron density scattering the transmitted signals lack coherence, which gave the technique its name. Their power spectrum contains information not only on the density, but also on the ion and electron temperatures, ion masses and drift velocities.\n\nRadio occultation is a remote sensing technique where a GNSS signal tangentially scrapes the Earth, passing through the atmosphere, and is received by a Low Earth Orbit (LEO) satellite. As the signal passes through the atmosphere, it is refracted, curved and delayed. An LEO satellite samples the total electron content and bending angle of many such signal paths as it watches the GNSS satellite rise or set behind the Earth. Using an Inverse Abel's transform, a radial profile of refractivity at that tangent point on earth can be reconstructed.\n\nMajor GNSS radio occultation missions include the GRACE, CHAMP, and COSMIC.\n\nIn empirical models of the ionosphere such as Nequick, the following indices are used as indirect indicators of the state of the ionosphere.\n\nF10.7 and R12 are two indices commonly used in ionospheric modelling. Both are valuable for their long historical records covering multiple solar cycles. F10.7 is a measurement of the intensity of solar radio emissions at a frequency of 2800 MHz made using a ground radio telescope. R12 is a 12 months average of daily sunspot numbers. Both indices have been shown to be correlated to each other.\n\nHowever, both indices are only indirect indicators of solar ultraviolet and X-ray emissions, which are primarily responsible for causing ionization in the Earth's upper atmosphere. We now have data from the GOES spacecraft that measures the background X-ray flux from the Sun, a parameter more closely related to the ionization levels in the ionosphere.\n\n\nThere are a number of models used to understand the effects of the ionosphere global navigation satellite systems. The Klobuchar model is currently used to compensate for ionospheric effects in GPS. This model was developed at the US Air Force Geophysical Research Laboratory circa 1974 by John (Jack) Klobuchar. The Galileo navigation system uses the NeQuick model.\n\nObjects in the Solar System that have appreciable atmospheres (i.e., all of the major planets and many of the larger natural satellites) generally produce ionospheres. Planets known to have ionospheres include Venus,\nUranus, Mars and Jupiter.\n\nThe atmosphere of Titan includes an ionosphere that ranges from about to in altitude and contains carbon compounds.\n\n\n\n"}
{"id": "48441126", "url": "https://en.wikipedia.org/wiki?curid=48441126", "title": "Islamo-leftism", "text": "Islamo-leftism\n\nIslamo-leftism (; , ), adjectivally Islamo-leftist (); is a neologism applied to the political alliance between leftists and Islamists.\n\nEssays in \"Libération\" and France 24 on the history of this term do not claim to find the definitive origin of this term, rather, both publications trace the term as far back as a 2002 use in \"New Judeophobia\", a book by Pierre-André Taguieff, historian of ideas, who describes Islamo-fascism as a type of anti-Zionism popular among \"the new third-worldist, neo-communist and neo-leftist configuration, better known as the 'anti-globalization movement.' \" Interviewed in 2016 by \"Liberation\" journalists Sonya Faure and Frantz Durupt, Taguieff is not certain whether he coined it or had heard it used, but he points out that the phrases Islamo-Progressives, and, in the 1980s, palestino-progressives were used as self-descriptions by the French left.\n\nAccording to Alain Badiou and Eric Hazan, Islamo-leftists was coined by French police for reasons of simple utility. Al Jazeera claims that the term Islamo-leftism was coined by Marine Le Pen, who uses it \"to describe what she considers an unhealthy alliance between \"Islamist fanatics\" and the French Left.\"\n\nFrench philosopher Pascal Bruckner understands Islamo-leftism as \"the fusion between the atheist Far Left and religious radicalism.\" According to Bruckner, Islamo-leftism was \"chiefly\" conceived by British Trotskyites of the Socialist Workers Party. Because these dedicated Leftists perceive Islam's potential for fomenting societal unrest, they promote tactical, temporary alliances with reactionary Muslim parties. According to Bruckner, Leftist adherents of Third-Worldism hope to use Islamism as a \"battering-ram\" to bring about the downfall of free-market capitalism, and they see the sacrifice of individual rights - in particular, of women's rights - as an acceptable trade-off in service of the greater goal of destroying capitalism. Bruckner contends that Islamists, for their part, pretend to join the left in its opposition to racism, neocolonialism, and globalization as a tactical and temporary means to achieve their true goal of imposing the \"totalitarian theocracy\" of Islamist government.\n\nPolitical scientist Maurice Fraser regards Islamo-leftism as part of a, \"striking and recent abdication of the Enlightenment project of human rights, freedom, secularism, science and progress,\" on the part of the political left, particularly among the anti-globalization activists of the New Left.\n\nBernard-Henri Lévy has described \"Islamo-leftism\" as, \"this grand new alliance between the reds and the new browns, of the axis which runs from Le Monde diplomatique to the death squads,\" and as a sort of \"anti-American religion.\"\n\nAccording to Mark Silinsky of the United States Army War College, Islamo-leftism is alliance of Islamists and leftists in opposition to Western values that can also be also referred to as the \"red-green axis.\" Silinsky characterizes the black-green alliance between Black Lives Matter and the Council on American–Islamic Relations as an example of Islamo-leftism.\n\nAccording to Robert S. Wistrich, \"A poisonous anti-Jewish legacy can be found in Marx, Fourier, and Proudhon, extending through the orthodox Communists and \"non-conformist\" Trotskyists to the Islamo-Leftist hybrids of today who... (are allied with) the Islamist anti-Semites of Hamas.\n\nAlvin Hirsch Rosenfeld describes Islamo Leftism as, \"the hope, entertained by a revolutionary fringe, of seeing Islam become the spearhead of a new insurrection, engaged in a 'Holy War against global capitalism.\"\n\nShireen Hunter credits Mahmoud Taleghani's reinterpretation of Islam in the light of Marxist theory in the 1970s with inspiring the \"Islamo-leftist\" group Mujahaedin-e-Khalq. According to Hunter, one of the goals of Islamo-leftism in the Muslim world during that period was to combat the Left by borrowing parts of its platform, thus hoping to attract economically disadvantaged groups like the Shia to Islamic politics. The result of these developments was the left-wing \"radicalization of Islam\" and \"the emergence of what could be described as a leftist Islam.\" Hunter describes the way in which \"Islamo-leftist intellectuals (the Mujahedin e Khalq) and Islamo-liberal intellectuals (the Islamic wing of the National Front and later the Freedom Movement) also directed anti-Pahlavi activities,\" which included guerrilla warfare. Frictions later between the various Islamic groups that had participated in the Iranian Revolution, with Morteza Motahhari, a socially progressive ally of Khomeini being assassinated by an Islamo-leftist followers of Ali Shariati. According to Olivier Roy, the three major Iranian political groups — leftist, Islamist, and Islamo-leftist — active in the 1970s had revolutionary rather than liberal democratic ideologies.\n\nIn his 2015 novel, \"Submission\", Michel Houellebecq has Robert Rediger, the fictional character who is a convert to Islam and university professor turned politician, describe Islamo-leftism as \"a desperate attempt by moldering, putrefying, brain-dead Marxists to hoist themselves out of the dustbin of history by latching onto the coattails of Islam.\"\n\n\n"}
{"id": "35108639", "url": "https://en.wikipedia.org/wiki?curid=35108639", "title": "Iulie Aslaksen", "text": "Iulie Aslaksen\n\nIulie Margrethe Nicolaysen Aslaksen (born 1956) is a Norwegian economist and Senior Researcher at Statistics Norway. She was a member of the Petroleum Price Board from 1990 to 2000. She is an expert on energy and environmental economics, including petroleum economics, climate policy and economics and sustainable development. She is cand.oecon. from the University of Oslo in 1981 and dr.polit. from 1990. She has been a visiting researcher and Fulbright Fellow at Harvard University and the University of California, Berkeley, and Associate Professor of Economics at the University of Oslo. She was a member of the government commissions resulting in the Norwegian Official Report 1988:21 \"Norsk økonomi i forandring\" (A Changing Norwegian Economy) and the Norwegian Official Report 1999:11 \"Analyse av investeringsutviklingen på kontinentalsokkelen\" (Analysis of Investments on the Norwegian Continental Shelf).\n\n\n\n\n"}
{"id": "17393545", "url": "https://en.wikipedia.org/wiki?curid=17393545", "title": "Janchi-guksu", "text": "Janchi-guksu\n\nJanchi-guksu (Korean: 잔치국수) or banquet noodles is a Korean noodle dish consisting of wheat flour noodles in a light broth made from anchovy and sometimes also \"dasima\" (kelp). Beef broth may be substituted for the anchovy broth. It is served with a sauce made from sesame oil, \"ganjang\" and small amounts of chili pepper powder and scallions. Thinly sliced \"jidan\" (지단, fried egg), \"gim\" (laver) and zucchini are added on top of the dish as garnishes.\n\nThe name derives from the Korean word \"janchi\" (잔치, literally \"feast\" or \"banquet\"), because the noodle dish has been eaten for special occasions like wedding feasts, birthday parties, or \"hwangap\" (60th birthday celebration) throughout Korea. The word \"guksu\" means \"noodles\" in Korean, and noodles symbolise longevity - in life, in a marriage.\n\nThere are records of \"guksu\" dating back to the Goryeo period. In the book \"Dongguk Isangguk Jeonjip\" Book 6 (hangul:동국이상국전집 6, hanja:東國李相國全集) there is a mention of \"guksu\" in a line of poetry, and in the book \"Goryeo Dogyeong\" (hangul:고려도경, hanja:高麗圖經) written by an envoy from the Chinese Song Dynasty it is mentioned that \"guksu\" was eaten on special occasions as wheat was rare and expensive in Goryeo. (The most common ingredients for noodles were buckwheat or starch.)\n\nBecause the noodles are traditionally eaten at weddings, the expression \"When are you going to feed us \"guksu\"?\" is a way of asking \"When are you going to get married?\" and a wedding day might be referred as \"a day to eat \"guksu\"\".\n\nFollowing the impeachment of Park Geun-hye, many Koreans ate fried chicken and janchi-guksu, which trended on Korean Twitter.\n\n\n"}
{"id": "218076", "url": "https://en.wikipedia.org/wiki?curid=218076", "title": "Keweenaw Peninsula", "text": "Keweenaw Peninsula\n\nThe Keweenaw Peninsula ( , sometimes locally /ˈkiːvənɔː/) is the northernmost part of Michigan's Upper Peninsula. It projects into Lake Superior and was the site of the first copper boom in the United States. As of the 2000 census, its population was roughly 43,200. Its major industries are now logging and tourism, as well as jobs related to Michigan Technological University and Finlandia University.\n\nThe ancient lava flows of the Keweenaw Peninsula were produced during the Mesoproterozoic Era as a part of the Midcontinent Rift between 1.096 and 1.087 billion years ago. This volcanic activity produced the only strata on Earth where large-scale economically recoverable 97 percent pure native copper is found.\n\nMuch of the native copper found in the Keweenaw comes in either the form of cavity fillings on lava flow surfaces, which has a \"lacy\" consistency, or as \"\"float\" copper\", which is found as a solid mass. Copper ore may occur within conglomerate or breccia as void or interclast fillings. The conglomerate layers occur as interbedded units within the volcanic pile.\n\nThe Keweenaw Peninsula and Isle Royale, formed by the Midcontinent Rift System, are the only sites in the United States with evidence of prehistoric aboriginal mining of copper. Artifacts made from this copper by these ancient Indians were traded as far south as present-day Alabama. These areas are also the unique location where chlorastrolite, the state gem of Michigan, can be found.\nThe northern end of the peninsula is sometimes referred to as Copper Island (or \"Kuparisaari\" by Finnish immigrants), although this term is becoming less common. It is separated from the rest of the peninsula by the Keweenaw Waterway, a natural waterway which was dredged and expanded in the 1860s across the peninsula between the cities of Houghton (named for Douglass Houghton) on the south side and Hancock on the north.\n\nA Keweenaw Water Trail has been established around Copper Island. The Water Trail stretches approximately and can be paddled in five to ten days, depending on weather and water conditions.\n\nThe Keweenaw Fault runs fairly lengthwise through both Keweenaw and neighboring Houghton counties. This ancient geological slip has given rise to cliffs. U.S. Highway 41 (US 41) and Brockway Mountain Drive, north of Calumet, were constructed along the cliff line.\n\nLake Superior significantly controls the climate of the Keweenaw Peninsula, keeping winters milder than those in surrounding areas. Spring is cool and brief, transitioning into a summer with highs near . Fall begins in September, with winter beginning in mid-November.\n\nThe peninsula receives copious amounts of lake-effect snow from Lake Superior. Official records are maintained close to the base of the peninsula in Hancock, Michigan, where the annual snowfall average is about . Farther north, in a community called Delaware, an unofficial average of about is maintained. At Delaware, the record snowfall for one season was in 1979. Averages over certainly occur in the higher elevations closer to the tip of the peninsula.\n\nBeginning as early as seven thousand years ago and apparently peaking around 3000 B.C., Native Americans dug copper from the southern shore of Lake Superior. This development was possible in large part because, in this region, large deposits of copper were easily accessible in surface rock and from shallow diggings. Native copper could be found as large nuggets and wiry masses. Copper as a resource for functional tooling achieved popularity around 3000 B.C., during the Middle Archaic Stage. The focus of copper working seems to have gradually shifted from functional tools to ornamental objects by the Late Archaic Stage c. 1200 B.C. Native Americans would build a fire to heat the rock around and over a copper mass and, after heating, pour on cold water to crack the rock. The copper was then pounded out, using rock hammers and stone chisels.\n\nThe Keweenaw's rich deposits of copper (and some silver) were extracted on an industrial scale beginning around the middle of the 19th century. The industry grew through the latter part of the century and employed thousands of people well into the 20th century. Hard rock mining in the region ceased in 1967 though copper sulfide deposits continued for some time after in Ontonagon. This vigorous industry created a need for educated mining professionals and directly led in 1885 to the founding of the Michigan Mining School (now Michigan Technological University) in Houghton. Although MTU discontinued its undergraduate mining engineering program in 2006, the university continues to offer engineering degrees in a variety of other disciplines. (In 2012 mining engineering was restarted in the re-formed Department of Geological and Mining Engineering and Sciences.)\n\nRunning concurrently with the mining boom in the Keweenaw was the white pine lumber boom. Trees were cut for timbers for mine shafts, to heat the communities around the large copper mines, and to help build a growing nation. Much of the logging at the time was done in winter due to the ease of operability with the snow. Due to the logging practices at that time, the forest of the Keweenaw looks much different today from 100 years ago.\n\nUS 41 terminates in the northern Keweenaw at the Michigan State Park housing Fort Wilkins. US 41 was the so-called \"Military Trail\" that started in Chicago in the 1900s and ended in the Keweenaw wilderness. The restored fort has numerous exhibits.\n\nFor detailed information on the region's mineralogical history, see the virtual tour of the peninsula written by the Mineralogical Society of America, found in \"External links\" on this page. Information on the geological formations of the region are also detailed.\n\nFrom 1964 to 1971, the University of Michigan cooperated with NASA and the U.S. Navy to run the Keweenaw Rocket launch site.\n\nA partial list of towns in the Keweenaw Peninsula:\n\n\n\n\n\n"}
{"id": "853113", "url": "https://en.wikipedia.org/wiki?curid=853113", "title": "List of European rivers with alternative names", "text": "List of European rivers with alternative names\n\nMany rivers in Europe have alternative names in different languages. Some rivers have also undergone name changes for political or other reasons. This article attempts to give all known alternative names for all major European rivers. It also includes some lesser rivers that are important because of their location or history.\n\nThis article does not offer any opinion about what the \"original\", \"official\", \"real\", or \"correct\" name of any river is or was. Rivers are listed alphabetically by their current best-known name in English. The English version is followed by variants in other languages, in alphabetical order by name, and then by any historical variants and former names.\n\nForeign names that are the same as their English equivalents may be listed, to provide an answer to the question \"What is that name in...?\".\n\n\n"}
{"id": "22346523", "url": "https://en.wikipedia.org/wiki?curid=22346523", "title": "List of environmental organisations topics", "text": "List of environmental organisations topics\n\nThis is a list of topics on which environmental organizations focus.\n\nHarmful substances in farming \n\n\n\n\n"}
{"id": "4466537", "url": "https://en.wikipedia.org/wiki?curid=4466537", "title": "List of national parks of South Korea", "text": "List of national parks of South Korea\n\nThe national parks of South Korea are preserved parcels of public land on which most forms of development are prohibited. They cover a total of 6.6% of the country's area, and are typically located in mountainous or coastal regions. The country's largest mountain park is Jirisan National Park in the southwest; this was also the first national park to be designated in 1967. The largest marine park is Dadohaehaesang, with an area of more than , but almost all of this is water. The smallest park is Wolchulsan, with an area of only .\n\nAs of 2016, there are 22 national parks in South Korea; the parks, with the exception of Hallasan National Park, are managed by the Korea National Park Service, established in 1987. The Authority operates its own police force, and since 1998 has been under the jurisdiction of the Ministry of Environment. It was previously under the jurisdiction of the Ministry of Construction.\n\n"}
{"id": "37486008", "url": "https://en.wikipedia.org/wiki?curid=37486008", "title": "Lists of organisms by population", "text": "Lists of organisms by population\n\nThis is a collection of lists of organisms by their population. While most of the numbers are estimates, they have been made by the experts in their fields. Species population is a science falling under the purview of population ecology and biogeography. Individuals are counted by census, as carried out for the piping plover; using the transect method, as done for the mountain plover; and beginning in 2012 by satellite, with the emperor penguin being first subject counted in this manner.\n\nMore than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. According to another study, the number of described species has been estimated at 1,899,587. 2000–2009 saw approximately 17,000 species described per year. The total number of undescribed organisms is unknown, but marine microbial species alone could number 20,000,000. The number of quantified species will \"ipso facto\" always lag behind the number of described species, and species contained in these lists tend to be on the K side of the r/K selection continuum. More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion [million million] tonnes of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nIt is estimated that the most numerous bacteria are of a species of the Pelagibacterales (or SAR11) clade, perhaps Pelagibacter ubique, and the most numerous viruses are bacteriophages infecting these species. It is estimated that the oceans contain about 2.4 × 10 (24 billion billion billion) SAR11 cells.\n\n\n\nThere are an estimated 3,500,000,000,000 (3.5 trillion) fish in the ocean. In the last 100 years, the number of small fish – such as pilchards, herrings, anchovies, sprats and sardines – has more than doubled. It is caused by a major decline in big ‘predator fish’ such as sharks, tuna and cod due to over-fishing.\n\nRecent figures indicate that there are more than 1.4 billion insects for each human on the planet An article in \"The New York Times\" claimed that the world holds 300 pounds of insects for every pound of humans. Ants have colonised almost every landmass on Earth. Their population is estimated as 10–10 (10-100 quadrillion).\n\nAccording to NASA in 2005, there were over 400 billion trees on our globe. However, more recently, in 2015, using better methods, the global tree count has been estimated at about 3 trillion. Other studies show that the Amazonian forest alone yields approximately 430 billion trees. Extrapolations from data compiled over a period of 10 years suggest that greater Amazonia, which includes the Amazon Basin and the Guiana Shield, harbors around 390 billion individual trees.\n"}
{"id": "551305", "url": "https://en.wikipedia.org/wiki?curid=551305", "title": "Lists of planets", "text": "Lists of planets\n\nThe following are lists of planets.\n\n\n\n\n\n\n\n\n\n"}
{"id": "51406305", "url": "https://en.wikipedia.org/wiki?curid=51406305", "title": "Macroinvertebrate Community Index", "text": "Macroinvertebrate Community Index\n\nMacroinvertebrate Community Index (MCI) is an index used in New Zealand to measure the water quality of fresh water streams. The presence or lack of macroinvertebrates such as insects, worms and snails in a river or stream can give a biological indicator on the health of that waterway. The MCI assigns a number to each species of macroinvertebrate based on the sensitivity of that species to pollution. The index then calculates an average score. A higher score on the MCI generally indicates a more healthy stream.\n\nThe MCI (Macro invertebrate Community Index) relies on an allocation of scores to freshwater macroinvertebrates based on their pollution tolerances. Freshwater macroinvertebrates found in pristine conditions would score higher than those found in polluted areas. MCI values can be calculated using macroinvertebrate presence-absence data using this equation:\n\nMCI = [(site score)/(# of scoring taxa)]*20\n\nPrevious water quality assessments have relied on both chemical and habitat analysis, however, these methods have been proven to be insufficient due to pollution from nonpoint sources. Species living in an aquatic environment may be the best natural indicator of environmental quality and reveal the effects of any habitat alteration or pollution, and have proved to respond to a wide range of stressors such as sedimentation, urbanization, agricultural practices and forest harvesting effects. Any changes that may occur in macroinvertebrate communities that lead to a reduction in diversity increase the dominance of pollution-tolerant invertebrates, such as oligochaetes and chironomids. Thus, a lack of species diversity and low biotic index scores of inhabitant macroinvertebrates may be an indicator of poor water quality. The risk of water quality degradation is the greatest in low-elevation areas, where high intensity agriculture and urban development are the dominant land uses.\n\nMacro invertebrate communities are the preferred indicators of aquatic ecosystem health because they are very easy to both collect and identify, and have short life spans, thus responding very quickly to changes in their environment. The MCI methods of utilizing macroinvertebrate communities to assess the overall health of an aquatic environment continues to be the most reliable, applicable, and widely acclaimed method around the world.\n\nVariations on the MCI\n\nIn addition to the MCI indexed defined above, there are also two other variations of the MCI. The QMCI (Quantitative Macroinvertebrate Community Index) and the SQMCI (Semi-Quantitative Macroinvertebrate Community Index). Both MCI and QMCI are widely used in countries like New Zealand. The combination of widespread use and good performance of the MCI and the QMCI in detecting water quality in aquatic ecosystems has sparked interest in further refinement of the methods in New Zealand. The QMCI, just like the MCI, was initially designed to evaluate the organic enrichment in aquatic ecosystems. The third index, the SQMCI, was created to reduce sampling and processing efforts required for the QMCI. The SQMCI will respond in a similar matter to the QMCI in community dominance, however, will require fewer samples to achieve the same precision. The SQMCI gives a comparative appraisal to the QMCI with under 40% of the exertion, in circumstances that macroinvertebrate densities are not required. This diminishes expenses and also enhances the logical solidness of biomonitoring projects. Both the QMCI and SQMCI are similar to the MCI in the way that they are graded on a 1 (extremely tolerant) to 10 (highly intolerant) scale. However, they differ in the way that MCI is calculated using presence-absence data whereas QMCI uses quantitative or percentage data. Having a qualitative, quantitative, and semi-quantitative version of the same index has raised some questions as to if this is a good thing or not. All three indexes have the same purpose, which is to measure the quality of an aquatic ecosystem, however, there are no clear recommendations about when each one is most appropriate to be used. In a study conducted on 88 rivers, Searsbrook et al. (2000) concluded MCI is more useful than the QMCI for recognizing changes in stream water quality over time. Having three forms of a similar index may prompt to various conclusions and also opens the route for specific utilization of either file to give bias to a specific position or position taken by a specialist. Consistency on the different versions of MCI have not yet been formally measured despite the communication that has been made to the public, managers and politicians.\n\nQMCI values can be calculated using:\nQMCI = ∑_(i=1)^(i=s)▒(n_i*a_i)/N\n\nSQMCI values can be calculated similar to QMCI except that coded abundances are substituted for actual counts. Example:\n\nSQMCI = ∑_(i=1)^(i=s)▒(n_i*a_i)/N\n\nFactors Influencing MCI\nThere are several factors which can affect the data acquisition of MCI when assessing the water quality of an aquatic ecosystem. Hard-bottom and Soft-bottom channels can often yield different results and many researchers will use two different versions of the MCI. For example, in a study by Stark & Mallard (2007) they discuss that hard and soft bottom channels have separate versions of the MCI and the two versions can not be combined into one data set because of the differences in taxa and tolerance values.\n\nSpatial variability is also of interest in terms of affecting the data acquired through MCI. Sites which are progressively down stream often tend to yield a lower MCI value. There may also be confounding influences between riffles, runs, or pools with a single stream reach.\n\nDepth and velocity have also been raised as a concern with regards to effecting results, however Stark (1993) investigated the influences of the sampling method, water depth, current velocity and substratum on the results and found that both MCI and QMCI are independent of depth, velocity, and substratum from macroinvertebrate samples collected from stony riffles. This finding is an advantage for the assessment of water pollution.\n\nThere have been several studies conducted on seasonal variability, which has been considered the main influential factor on the assessment of water quality. It has been concluded that all models should test data that has been collected in the season as the reference data, which is being used.\n\nThere have been several other factors such as water temperature, invertebrate life histories and dissolved oxygen levels that have all been explained as causes of seasonal variability. Warmer seasons have biotic indices that are indicative of poorer stream health. Warmer seasons such as summer, would have increased temperatures therefore increasing water temperature and decreasing the amount of dissolved oxygen in the water making the environment less ideal to aquatic macroinvertebrates. In return, this effects the density of macroinvertebrate population and changes the results of the indices.\n\n"}
{"id": "529974", "url": "https://en.wikipedia.org/wiki?curid=529974", "title": "Mediated transport", "text": "Mediated transport\n\nMediated transport refers to transport mediated by a membrane transport protein. Substances in the human body may be hydrophobic, electrophilic, contain a positively or negatively charge, or have another property. As such there are times when those substances may not be able to pass over the cell membrane using protein-independent movement. The cell membrane is imbedded with many membrane transport proteins that allow such molecules to travel in and out of the cell. There are three types of mediated transport: uniport, symport, and antiport. Things that can be transported are nutrients, ions, glucose, etc, all depending on the needs of the cell.\n\nMechanism of transport. A molecule will bind to a transporter protein, altering it's shape. The change of shape or other added substances such as ATP will, in turn, cause the transport protein to alter its shape and release the molecule onto the other side of the cell membrane.    \n\n\n"}
{"id": "55848604", "url": "https://en.wikipedia.org/wiki?curid=55848604", "title": "Moment distance index", "text": "Moment distance index\n\nThe moment distance index (MDI) is a shape-based metric or shape index that can be used to analyze spectral reflectance curves and waveform LiDAR, proposed by Salas and Henebry in 2014. In the case of spectral data, the shape of the reflectance curve should unmask fine points of the spectra usually not considered by existing band-specific indices. It has been used to identify spectral regions for chlorophyll and carotenoids, detect greenhouses using WorldView-2 and Landsat satellite data, compute canopy heights, and estimate green vegetation fraction.\n\nVarious approaches have been devised to analyze medium and fine spectral resolution data and maximize their use to extract specific information for vegetation biophysical and biochemical properties. Combinations of spectral bands, called indices, have been used to diminish the effects of soil background and/or atmospheric conditions while highlighting specific spectral features associated with plant or canopy properties. Vegetation indices (VIs) use the concept of band ratio and differences or weighted linear combinations to take advantage of the visible and NIR bands, two important spectral bands for vegetation studies, in measuring the photosynthetic activity of the plant and explore vegetation dynamics. There is an extensive list of such indices, including the normalized difference vegetation index (NDVI), ratio-based indices such as the modified simplerRatio, soil-distance-based indices such as the modified soil adjusted vegetation index (MSAVI), and many others. Whereas most indices incorporate two-band or three-band relations – slope-based, distance-based on soil line or optimized (slope-based and distance-based concepts combined) – no approach deals with the raw shape of the spectral curve. MDI, however, investigates the shape of the reflectance curve using multiple spectral bands not considered by other indices, which could carry additional spectral information useful for vegetation monitoring. \n\nA full-waveform Light Detection And Ranging (LiDAR) system has the ability to record many returns per emitted pulse, as a function of time, to reveal the vertical structure of the illuminated object, showing position of the individual targets, and finer details of the signature of intercepted surfaces or the proportion of the canopy complexity. Information associated with the illuminated object can be decoded from the generated backscattered waveform, as key features of the waveform such as the shape, area, and power are directly related to the geometry of the illuminated object. The richness of the LiDAR waveform holds a promise to address the challenge of characterizing in detail the geometric and reflection characteristics of vegetation structure, e.g., the vertical canopy volume distribution. MDI utilizes the raw waveform and place importance on its shape and its return power. MDI departs from the usual Gaussian modeling in detecting peaks (canopy and ground) for example in canopy height estimation and focus more on the full geometry (raw shape) and radiometry (raw power) of the LiDAR waveform to retain richness of the data. \n\nThe moment distance is a matrix of distances computed from two reference locations (pivots) to each spectral or waveform point within the specified range. \n\nAssume that a curve (reflectance or absorption curve or backscattered waveform) is displayed in Cartesian coordinates with the abscissa displaying the wavelength \"λ\" or time lapse \"t\" and the ordinate displaying the reflectance \"ρ\" or the backscattered power \"p\". Let the subscript LP denote the \"left pivot\" (located in a shorter wavelength for the spectral curve and earlier temporal reference point for the waveform) and subscript RP denote the \"right pivot\" (located in a longer wavelength for the spectral curve and later temporal reference point for the waveform). Let \"λ\" and \"λ\" be the wavelength locations observed at the left and right pivots for a reflectance data, respectively, where left (right) indicates a shorter (longer) wavelength. Let \"t\" and \"t\" be the time value observed at the left and right pivots for a waveform data, respectively, where left (right) indicates an earlier (later) time. The proposed MD approach can be described in a set of equations.\nFor spectral data, the index is given as:\n\nformula_1\n\nformula_2\n\nformula_3\n\nwhere formula_4is the moment distance from the right pivot, formula_5is the moment distance from the left pivot, formula_6is the wavelength location at left pivot, formula_7is the wavelength location at right pivot, formula_8is the spectral reflectance at a given wavelength, and formula_9 is successive wavelength location.\n\nFor waveform LiDAR data, the index is given as:\n\nformula_10\n\nformula_11\n\nformula_12\n\nwhere the moment distance from the left pivot (MD) is the sum of the hypotenuses constructed from the left pivot to the power at successively later times (index \"formula_9\" from \"t\" to \"t\"): one base of each triangle is difference from the left pivot (\"formula_9\" − \"t\") along the abscissa and the other base is simply the backscattered power at \"formula_9\". Similarly, the moment distance from the right pivot (MD) is the sum of the hypotenuses constructed from the right pivot to the power at successively earlier times (index \"formula_9\" from \"t\" to \"t\"): one base of each triangle is the difference from the right pivot (\"t\" − \"formula_9\") along the abscissa and the other base is simply the backscattered power at \"formula_9\".\n\nMDI is an unbounded metric. It increases or decreases as a nontrivial function of the number of spectral bands or bins considered and the shape of the spectrum or waveform that spans those contiguous bands or bins. The number of bands or bins is a function of the spectral resolution of the imaging spectrometer or the temporal resolution of the LiDAR (digitization rate) and the length of the reference range (i.e., full extent or subsets of the curve) being analyzed. \n"}
{"id": "39340042", "url": "https://en.wikipedia.org/wiki?curid=39340042", "title": "Mossø", "text": "Mossø\n\nMossø is Denmark's third largest freshwater lake and Jutland's largest, as measured by surface area. The lake is located just west of the city of Skanderborg in east Jutland, but is part of both Skanderborg Municipality and Horsens Municipality. Mossø lies in the middle of the area and landscape known as Søhøjlandet (English: \"The Lake-highland\").\n\nThere is a small lake named Mossø in the forest of Rold Skov in Himmerland.\n\nBoth ospreys and white-tailed eagle is regularly observed at Mossø and the later have recently established here as a breeding bird, which is rare in Denmark.\n\nMossø is part of the 4,470 ha Natura 2000 protection area, designated as number 52. The lake is also designated as an international bird protection area, with number F35.\n\n\n"}
{"id": "34804482", "url": "https://en.wikipedia.org/wiki?curid=34804482", "title": "Occupied Enemy Territory Administration", "text": "Occupied Enemy Territory Administration\n\nThe Occupied Enemy Territory Administration (OETA) was a joint British and French military administration over Levantine and Mesopotamian provinces of the former Ottoman Empire between 1918–20, set up following the Sinai and Palestine Campaign of World War I. It was initially set up via the 1918 Anglo–French Modus Vivendi.\n\nThe administration ended following the assignment of the French Mandate of Syria and Lebanon and British Mandate for Palestine at the 19–26 April 1920 San Remo conference.\n\nFollowing British and French occupation, the region was split into three administrative sub-units, which varied very little from the previous Ottoman divisions. OETA South, consisting of the Ottoman Mutasarrifate of Jerusalem and the sanjaks of Nablus and Acre, OETA North (later renamed OETA West) consisting of the Ottoman sanjaks of Beirut, Lebanon, Latakia and a number of sub-districts, and OETA East consisting of the Ottoman Syria Vilayet and Hejaz Vilayet. But, success of Turkish War of Independence, Maraş, Antep and Urfa sanjaks of former Halep Eyalet remained in Turkey after 1921. Also, Antakya and İskenderun kazas of Halep Sanjak in one were separated as the Republic of Hatay in 1938. The republic joined to Turkey in 1939.\n\nWhen the British forces occupied Ethiopia, Libya and other Italian colonies during World War II, the OETA was revived as the administrative structure by which the British governed these territories. In Ethiopia, Emperor Haile Selassie was allowed to return and claim his throne, but the OETA authorities ruled the country for some time before full sovereignty was restored to Ethiopia.\n\nWhen Field Marshal Edmund Allenby first assumed command of the Egyptian Expeditionary Force he quickly joined the army in the field leaving the political and administrative problems related to the Egyptian Mandate to a Government appointee with a suitable staff. The area of formerly Ottoman territory now under occupation also required management, and with the approval of the Government, Allenby appointed a Chief Administrator for Palestine. He divided the country into four districts: Jerusalem, Jaffa, Majdal and Beersheba, each under a military governor. Under this administration the immediate needs of the people were provided for, seed grain and live-stock were imported and distributed, finance on easy terms was made available through the Army bankers, a stable currency was set up and postal services restored. Allenby insisted that as long as military administration was required, it was to remain his responsibility.\n\n\nOETA East was a joint Arab-British military administration. The Arab and British armies entered Damascus on 1 October 1918, and on 3 October 1918 Ali Rida al-Rikabi was appointed Military Governor of OETA East. Prince Faisal son of King Hussain of Mecca entered Damascus as on 4 October and appointed Rikabi Chief of the Council of Directors (i.e. prime minister) of Syria.\n\n"}
{"id": "29068576", "url": "https://en.wikipedia.org/wiki?curid=29068576", "title": "Orbital arc", "text": "Orbital arc\n\nAn orbital arc is an imaginary arc in the sky composed of the connected pointing angles of all the geostationary satellites seen from any given location on the surface of the earth.\n\nModern VSAT satellite receiver system installations take into consideration the orbital arc when conducting a site survey to determine the optimal location for the earth station. Best practice methods for factoring in the orbital arc during a site survey are contained in VSAT training:\n\n"}
{"id": "3425287", "url": "https://en.wikipedia.org/wiki?curid=3425287", "title": "Ore genesis", "text": "Ore genesis\n\nVarious theories of ore genesis explain how the various types of mineral deposits form within the Earth's crust. Ore-genesis theories vary depending on the mineral or commodity examined.\n\nOre-genesis theories generally involve three components: source, transport or conduit, and trap. (This also applies to the petroleum industry: petroleum geologists originated this analysis.)\n\n\nThe biggest deposits form when the source is large, the transport mechanism is efficient, and the trap is active and ready at the right time.\n\n\nThese processes are the physicochemical phenomena and reactions caused by movement of hydrothermal water within the crust, often as a consequence of magmatic intrusion or tectonic upheavals. The foundations of hydrothermal processes are the source-transport-trap mechanism.\n\nSources of hydrothermal solutions include seawater and meteoric water circulating through fractured rock, formational brines (water trapped within sediments at deposition), and metamorphic fluids created by dehydration of hydrous minerals during metamorphism.\n\nMetal sources may include a plethora of rocks. However most metals of economic importance are carried as trace elements within rock-forming minerals, and so may be liberated by hydrothermal processes. This happens because of:\n\nTransport by hydrothermal solutions usually requires a salt or other soluble species which can form a metal-bearing complex. These metal-bearing complexes facilitate transport of metals within aqueous solutions, generally as hydroxides, but also by processes similar to chelation.\n\nThis process is especially well understood in gold metallogeny where various thiosulfate, chloride, and other gold-carrying chemical complexes (notably tellurium-chloride/sulfate or antimony-chloride/sulfate). The majority of metal deposits formed by hydrothermal processes include sulfide minerals, indicating sulfur is an important metal-carrying complex.\n\nSulfide deposition:<br>\nSulfide deposition within the \"trap\" zone occurs when metal-carrying sulfate, sulfide, or other complexes become chemically unstable due to one or more of the following processes;\n\nMetal can also precipitate when temperature and pressure or oxidation state favour different ionic complexes in the water, for instance the change from sulfide to sulfate, oxygen fugacity, exchange of metals between sulfide and chloride complexes, et cetera.\n\nLateral secretion:<br>\nOre deposits formed by lateral secretion are formed by metamorphic reactions during shearing, which liberate mineral constituents such as quartz, sulfides, gold, carbonates, and oxides from deforming rocks, and focus these constituents into zones of reduced pressure or dilation such as faults. This may occur without much hydrothermal fluid flow, and this is typical of podiform chromite deposits.\n\nMetamorphic processes also control many physical processes which form the source of hydrothermal fluids, outlined above.\n\nSurficial processes are the physical and chemical phenomena which cause concentration of ore material within the regolith, generally by the action of the environment. This includes placer deposits, laterite deposits, and residual or eluvial deposits. The physical processes of ore deposit formation in the surficial realm include;\n\nOre deposits are usually classified by ore formation processes and geological setting. For example, sedimentary exhalative deposits (SEDEX), are a class of ore deposit formed on the sea floor (sedimentary) by exhalation of brines into seawater (exhalative), causing chemical precipitation of ore minerals when the brine cools, mixes with sea water, and loses its metal carrying capacity.\n\nOre deposits rarely fit neatly into the categories in which geologists wish to place them. Many may be formed by one or more of the basic genesis processes above, creating ambiguous classifications and much argument and conjecture. Often ore deposits are classified after examples of their type, for instance Broken Hill type lead-zinc-silver deposits or Carlin–type gold deposits.\n\nClassification of hydrothermal ore deposits is also achieved by classifying according to the temperature of formation, which roughly also correlates with particular mineralising fluids, mineral associations and structural styles. This scheme, proposed by Waldemar Lindgren (1933) classified hydrothermal deposits as \"hypothermal\", \"mesothermal\", \"epithermal\", and \"telethermal\". Epithermal deposits are hydrothermal ore deposits formed at low temperatures (50-200 °C) near the Earth´s surface (<1500 m), that fill veins, breccias, and stockworks. The age of formation of epithermal deposits in México is currently being investigated.\n\n\"As they require the conjunction of specific environmental conditions to form, particular mineral deposit types tend to occupy specific geodynamic niches,\" therefore, this page has been organised by metal commodity. It is also possible to organise theories the other way, namely according to geological criteria of formation. Often ores of the same metal can be formed by multiple processes, and this is described here under each metal or metal complex.\n\nIron ores are overwhelmingly derived from ancient sediments known as \"banded iron formations\" (BIFs). These sediments are composed of iron oxide minerals deposited on the sea floor. Particular environmental conditions are needed to transport enough iron in sea water to form these deposits, such as acidic and oxygen-poor atmospheres within the Proterozoic Era.\n\nOften, more recent weathering is required to convert the usual magnetite minerals into more easily processed hematite. Some iron deposits within the Pilbara of West Australia are \"placer deposits\", formed by accumulation of hematite gravels called \"pisolites\" which form channel-iron deposits. These are preferred because they are cheap to mine.\n\nLead-zinc deposits are generally accompanied by silver, hosted within the lead sulfide mineral galena or within the zinc sulfide mineral sphalerite.\n\nLead and zinc deposits are formed by discharge of deep sedimentary brine onto the sea floor (termed \"sedimentary exhalative\" or SEDEX), or by replacement of limestone, in skarn deposits, some associated with submarine volcanoes (called volcanogenic massive sulfide ore deposits or VMS), or in the aureole of subvolcanic intrusions of granite. The vast majority of SEDEX lead and zinc deposits are Proterozoic in age, although there are significant Jurassic examples in Canada and Alaska.\n\nThe carbonate replacement type deposit is exemplified by the Mississippi valley type (MVT) ore deposits. MVT and similar styles occur by replacement and degradation of carbonate sequences by hydrocarbons, which are thought important for transporting lead.\n\nGold deposits are formed via a very wide variety of geological processes. Deposits are classified as primary, alluvial or placer deposits, or residual or laterite deposits. Often a deposit will contain a mixture of all three types of ore.\n\nPlate tectonics is the underlying mechanism for generating gold deposits. The majority of primary gold deposits fall into two main categories: lode gold deposits or intrusion-related deposits.\n\n\"Lode gold deposits\" are generally high-grade, thin, vein and fault hosted. They are primarily made up of quartz veins also known as lodes or \"reefs\", which contain either native gold or gold sulfides and tellurides. Lode gold deposits are usually hosted in basalt or in sediments known as turbidite, although when in faults, they may occupy intrusive igneous rocks such as granite.\n\nLode-gold deposits are intimately associated with orogeny and other plate collision events within geologic history. Most lode gold deposits \"sourced\" from metamorphic rocks because it is thought that the majority are formed by dehydration of basalt during metamorphism. The gold is transported up faults by hydrothermal waters and deposited when the water cools too much to retain gold in solution.\n\n\"Intrusive related gold\" (Lang & Baker, 2001) is generally hosted in granites, porphyry, or rarely dikes. Intrusive related gold usually also contains copper, and is often associated with tin and tungsten, and rarely molybdenum, antimony, and uranium. Intrusive-related gold deposits rely on gold existing in the fluids associated with the magma (White, 2001), and the inevitable discharge of these hydrothermal fluids into the wall-rocks (Lowenstern, 2001). Skarn deposits are another manifestation of intrusive-related deposits.\n\n\"Placer\" deposits are sourced from pre-existing gold deposits and are secondary deposits. Placer deposits are formed by alluvial processes within rivers and streams, and on beaches. Placer gold deposits form via gravity, with the density of gold causing it to sink into trap sites within the river bed, or where water velocity drops, such as bends in rivers and behind boulders. Often placer deposits are found within sedimentary rocks and can be billions of years old, for instance the Witwatersrand deposits in South Africa. Sedimentary placer deposits are known as 'leads' or 'deep leads'.\n\nPlacer deposits are often worked by fossicking, and panning for gold is a popular pastime.\n\nLaterite gold deposits are formed from pre-existing gold deposits (including some placer deposits) during prolonged weathering of the bedrock. Gold is deposited within iron oxides in the weathered rock or regolith, and may be further enriched by reworking by erosion. Some laterite deposits are formed by wind erosion of the bedrock leaving a residuum of native gold metal at surface.\n\nA bacterium, Cupriavidus metallidurans plays a vital role in the formation of gold nuggets, by precipitating metallic gold from a solution of gold (III) tetrachloride, a compound highly toxic to most other microorganisms.\nSimilarly, Delftia acidovorans can form gold nuggets.\n\nPlatinum and palladium are precious metals generally found in ultramafic rocks. The source of platinum and palladium deposits is ultramafic rocks which have enough sulfur to form a sulfide mineral while the magma is still liquid. This sulfide mineral (usually pentlandite, pyrite, chalcopyrite, or pyrrhotite) gains platinum by mixing with the bulk of the magma because platinum is chalcophile and is concentrated in sulfides. Alternatively, platinum occurs in association with chromite either within the chromite mineral itself or within sulfides associated with it.\n\nSulfide phases only form in ultramafic magmas when the magma reaches sulfur saturation. This is generally thought to be nearly impossible by pure fractional crystallisation, so other processes are usually required in ore genesis models to explain sulfur saturation. These include contamination of the magma with crustal material, especially sulfur-rich wall-rocks or sediments; magma mixing; volatile gain or loss.\n\nOften platinum is associated with nickel, copper, chromium, and cobalt deposits.\n\nNickel deposits are generally found in two forms, either as sulfide or laterite.\n\nSulfide type nickel deposits are formed in essentially the same manner as platinum deposits. Nickel is a chalcophile element which prefers sulfides, so an ultramafic or mafic rock which has a sulfide phase in the magma may form nickel sulfides. The best nickel deposits are formed where sulfide accumulates in the base of lava tubes or volcanic flows — especially komatiite lavas.\n\nKomatiitic nickel-copper sulfide deposits are considered to be formed by a mixture of sulfide segregation, immiscibility, and thermal erosion of sulfidic sediments. The sediments are considered to be necessary to promote sulfur saturation.\n\nSome subvolcanic sills in the Thompson Belt of Canada host nickel sulfide deposits formed by deposition of sulfides near the feeder vent. Sulfide was accumulated near the vent due to the loss of magma velocity at the vent interface. The massive Voisey's Bay nickel deposit is considered to have formed via a similar process.\n\nThe process of forming \"nickel laterite\" deposits is essentially similar to the formation of gold laterite deposits, except that ultramafic or mafic rocks are required. Generally nickel laterites require very large olivine-bearing ultramafic intrusions. Minerals formed in laterite nickel deposits include gibbsite.\n\nCopper is found in association with many other metals and deposit styles. Commonly, copper is either formed within sedimentary rocks, or associated with igneous rocks.\n\nThe world's major copper deposits are formed within the granitic porphyry copper style. Copper is enriched by processes during crystallisation of the granite and forms as chalcopyrite — a sulfide mineral, which is carried up with the granite.\n\nSometimes granites erupt to surface as volcanoes, and copper mineralisation forms during this phase when the granite and volcanic rocks cool via hydrothermal circulation.\n\nSedimentary copper forms within ocean basins in sedimentary rocks. Generally this forms by brine from deeply buried sediments discharging into the deep sea, and precipitating copper and often lead and zinc sulfides directly onto the sea floor. This is then buried by further sediment. This is a process similar to SEDEX zinc and lead, although some carbonate-hosted examples exist.\n\nOften copper is associated with gold, lead, zinc, and nickel deposits.\n\nUranium deposits are usually \"sourced\" from radioactive granites, where certain minerals such as monazite are leached during hydrothermal activity or during circulation of groundwater. The uranium is brought into solution by acidic conditions and is deposited when this acidity is neutralised. Generally this occurs in certain carbon-bearing sediments, within an unconformity in sedimentary strata. The majority of the world's nuclear power is sourced from uranium in such deposits.\n\nUranium is also found in nearly all coal at several parts per million, and in all granites. Radon is a common problem during mining of uranium as it is a radioactive gas.\n\nUranium is also found associated with certain igneous rocks, such as granite and porphyry. The Olympic Dam deposit in Australia is an example of this type of uranium deposit. It contains 70% of Australia's share of 40% of the known global low-cost recoverable uranium inventory.\n\nMineral sands are the predominant type of titanium, zirconium, and thorium deposit. They are formed by accumulation of such heavy minerals within beach systems, and are a type of \"placer deposits\". The minerals which contain titanium are ilmenite, rutile, and leucoxene, zirconium is contained within zircon, and thorium is generally contained within monazite. These minerals are sourced from primarily granite bedrock by erosion and transported to the sea by rivers where they accumulate within beach sands. Rarely, but importantly, gold, tin, and platinum deposits can form in beach placer deposits.\n\nThese three metals generally form in a certain type of granite, via a similar mechanism to intrusive-related gold and copper. They are considered together because the process of forming these deposits is essentially the same. Skarn type mineralisation related to these granites is a very important type of tin, tungsten, and molybdenum deposit. Skarn deposits form by reaction of mineralised fluids from the granite reacting with wall rocks such as limestone. Skarn mineralisation is also important in lead, zinc, copper, gold, and occasionally uranium mineralisation.\n\nGreisen granite is another related tin-molybdenum and topaz mineralisation style.\n\nThe overwhelming majority of rare earth elements, tantalum, and lithium are found within pegmatite. Ore genesis theories for these ores are wide and varied, but most involve metamorphism and igneous activity. Lithium is present as spodumene or lepidolite within pegmatite.\n\nCarbonatite intrusions are an important source of these elements. Ore minerals are essentially part of the unusual mineralogy of carbonatite.\n\nPhosphate is used in fertilisers. Immense quantities of \"phosphate rock\" or phosphorite occur in sedimentary shelf deposits, ranging in age from the Proterozoic to currently forming environments. Phosphate deposits are thought to be sourced from the skeletons of dead sea creatures which accumulated on the seafloor. Similar to iron ore deposits and oil, particular conditions in the ocean and environment are thought to have contributed to these deposits within the geological past.\n\nPhosphate deposits are also formed from alkaline igneous rocks such as nepheline syenites, carbonatites, and associated rock types. The phosphate is, in this case, contained within magmatic apatite, monazite, or other rare-earth phosphates.\n\nDue to the presence of vanabins, concentration of vanadium found in the blood cells of \"Ascidia gemmata\" belonging to the suborder Phlebobranchia is 10,000,000 times higher than that in the surrounding seawater. A similar biological process might have played a role in the formation of vanadium ores.\nVanadium is also present in fossil fuel deposits such as crude oil, coal, oil shale, and oil sands. In crude oil, concentrations up to 1200 ppm have been reported.\n\n\n\n"}
{"id": "51370704", "url": "https://en.wikipedia.org/wiki?curid=51370704", "title": "Poetic naturalism", "text": "Poetic naturalism\n\nPoetic naturalism is a philosophical approach to naturalism which encourages a variety of ways to talk about the world, using language dependent upon the aspect of reality being discussed. The term was coined by Sean M. Carroll in his book \"The Big Picture: On the Origins of Life, Meaning, and the Universe Itself\" to offer richer insight into naturalism, with strategies for agreeing on useful ways of talking about the natural world. It acknowledges that the methods and terms used within one domain may not be coherent with those of another domain, yet both can be considered valid representations of reality.\n\nPoetic naturalism resolves many of the conflicts caused by the fundamental differences between the various branches of science and philosophy. For example, the universe can be described mechanically in terms of atoms obeying certain laws. This results in a deterministic universe incompatible with the concepts of choice or free will. Determinism is not a useful perspective for navigating the human condition, which includes value judgments and the concept of cause and effect, as it essentially posits an endless chain of unavoidable \"effects\" stemming from a single primordial \"cause\". In spite of determinism, humans can be said to make decisions and control aspects of their environment. This apparent conflict does not negate either viewpoint; they can both be said to be legitimate depending on the frame of reference in which we are currently operating.\n\nThe tenets of poetic naturalism can be summarized in three points:\nSean Carroll cites a quote which speaks to the essence of poetic naturalism's distinction from naturalism:\n\nWhile naturalism is the \"idea or belief that only natural (as opposed to supernatural or spiritual) laws and forces operate in the world\", poetic naturalism understands that the way we find personally relevant meaning to life does not naturally emerge from a purely scientific approach. Science is a rigorous method of finding what is true or false, while poetic naturalism encourages extending the conversation to include contemplation into what is right and wrong. It integrates scientific reasoning methods into our personal purpose-seeking and meaning-making, with an emphasis on Bayesian techniques.\n\n\n"}
{"id": "16189704", "url": "https://en.wikipedia.org/wiki?curid=16189704", "title": "Project Valkyrie", "text": "Project Valkyrie\n\nThe Valkyrie is a theoretical spacecraft designed by Charles Pellegrino and Jim Powell (a physicist at Brookhaven National Laboratory). The Valkyrie is theoretically able to accelerate to 92% the speed of light and decelerate afterward, carrying a small human crew to another star system.\n\nThe Valkyrie's high performance is attributable to its innovative design. Instead of a solid spacecraft with a rocket at the back, Valkyrie is built more like a cable car train, with the crew quarters, fuel tanks, radiation shielding, and other vital components being pulled between front and aft engines on long tethers. This greatly reduces the mass of the ship, because it no longer requires heavy structural members and radiation shielding. This is a considerable advantage because in a rocket every extra kilogram of payload (dry mass) will require a corresponding extra amount of propellant or fuel.\n\nThe Valkyrie would have a crew module trailing 10 kilometers behind the engine. A small 20-cm-thick tungsten shield would hang 100 meters behind the engine, to help protect the trailing crew module from its harmful radiation. The fuel tank might be placed between the crew module and the engine, to further protect it. At the trailing end of the ship would be a second engine, which the ship would use to decelerate. The forward engine and the tank holding its fuel supply might be jettisoned before deceleration, to reduce fuel consumption. The tether system requires that the elements of the ship must be moved \"up\" or \"down\" the tethers depending on flight direction.\n\nInitially the Valkyrie's engine would work by using small quantities of antimatter to initiate an extremely energetic fusion reaction. A magnetic coil captures the exhaust products of this reaction, expelling them with an exhaust velocity of 12-20% the speed of light (35,000-60,000 km/s). As the spacecraft approaches 20% the speed of light, more antimatter is fed into the engines until it switches over to pure matter-antimatter annihilation. It will use this mode to accelerate the remainder of the way to .92 c. Pellegrino estimates that the ship would require 100 tons of matter and antimatter to reach 0.1-0.2c, with an undetermined excess of matter to ensure the antimatter is efficiently utilized. To reach a speed of .92 c and decelerate afterward, Valkyrie would require a mass ratio of 22 (or 2200 tons of fuel for a 100-ton spacecraft).\n\nAt such high speeds, incident debris would be a major hazard. While accelerating, Valkyrie uses a device that combines the functions of a particle shield and a liquid droplet radiator. Waste heat is dumped into liquid droplets that are cast out in front of the ship. As the ship accelerates the droplets (now cool) effectively fall back into the ship, so the system is self-recycling. During deceleration the ship will be protected by ultra-thin umbrella shields, augmented by a dust shield, possibly made by grinding up pieces of the discarded first stage.\n\nThe chief feasibility issue of Valkyrie (or for \"any\" antimatter-beam drive) lies in its requirement of quantities of antimatter fuel measured in tons. Antimatter cannot be produced at an efficiency of more than 50% (that is to say, to produce one gram of antimatter requires twice as much energy as you would get from annihilating that gram with a gram of matter). Since a half a kilogram of antimatter would yield 9×10 J if annihilated with an equal amount of matter, this quickly adds up to enormous energy requirements for its production. To produce the 50 tons of antimatter Valkyrie would require 1.8×10 J. This is the same amount of energy that the entire human race currently uses in about forty years.\n\nThis may be solved by creating a truly enormous power plant for the antimatter factory, probably in the form of a vast array of solar panels with a combined area of millions of square kilometers or many fusion reactors. Alternately the antimatter-fusion hybrid drive the Valkyrie uses to accelerate up to 0.2 \"c\" would require much less antimatter and, with an exhaust velocity of 30-60,000 km·s, still compares quite favorably with competing engines such as the inertial confinement pulse drive used by Project Daedalus or Project Orion. The Valkyrie's lightweight construction could also be applied to a wide variety of space vehicle.\n\nBy using tethers there is no rigidity between ship elements and engines. Without active acceleration or thrust to pull and straighten the tethers the slightest imbalance, excess force, or the moving of the ship elements into different flight configurations pose a danger for collisions between ship elements and engines. As long term space flight at interstellar velocities causes erosion due to collision with particles, gas, dust and micrometeorites the tethers are literally lifelines. Changing course or turning the ship requires re-positioning or aligning every ship element and presumably consumes more fuel in doing so.\n\nAs the liquid droplet radiators (LDR) are deployed on the other side of propulsion and the main body, the droplets and the collectors are exposed to the other half of heat energy from the gamma radiation from the antimatter annihilation. If the total area of the collectors are larger than the radiation shield the LDR would serve to cool itself rather than the shield for the ship's main components.\n\nA superficially-similar starship is featured in the movie \"Avatar\".\n\n\n"}
{"id": "3004938", "url": "https://en.wikipedia.org/wiki?curid=3004938", "title": "Rupelian", "text": "Rupelian\n\nThe Rupelian is, in the geologic timescale, the older of two ages or the lower of two stages of the Oligocene epoch/series. It spans the time between . It is preceded by the Priabonian stage (part of the Eocene) and is followed by the Chattian stage.\n\nThe stage is named after the small river Rupel in Belgium, a tributary to the Scheldt. The Belgian Rupel Group derives its name from the same source. The name Rupelian was introduced in scientific literature by Belgian geologist André Hubert Dumont in 1850. The separation between the group and the stage was made in the second half of the 20th century, when stratigraphers saw the need to distinguish between lithostratigraphic and chronostratigraphic names.\n\nThe base of the Rupelian stage (which is also the base of the Oligocene series) is at the extinction of the foraminiferan genus \"Hantkenina\". An official GSSP for the base of the Rupelian has been assigned in 1992 (Massignano, Italy). The transition with the Chattian has also been marked with a GSSP in August 2017 (Monte Conero, Italy). \n\nThe top of the Rupelian stage (the base of the Chattian) is at the extinction of the foram genus \"Chiloguembelina\" (which is also the base of foram biozone P21b).\n\nThe Rupelian overlaps the Orellan, Whitneyan and lower Arikareean North American Land Mammal Ages, the upper Mustersan and Tinguirirican South American Land Mammal Ages, the uppermost Headonian, Suevian and lower Arvernian European Land Mammal Mega Zones (the Rupelian spans the Mammal Paleogene zones 21 through 24 and part of 25), and the lower Hsandgolian Asian Land Mammal Age. It is also coeval with the only regionally used upper Aldingan and lower Janjukian stages of Australia, the upper Refugian and lower Zemorrian stages of California and the lower Kiscellian Paratethys stage of Central and eastern Europe. Other regionally used alternatives include the Stampian, Tongrian, Latdorfian and Vicksburgian.\n\n\n"}
{"id": "39577606", "url": "https://en.wikipedia.org/wiki?curid=39577606", "title": "SN 1972E", "text": "SN 1972E\n\nSN1972E was a supernova in the galaxy NGC 5253 that was discovered 13 May 1972 with an apparent B magnitude of about 8.5, shortly after it had reached its maximum brightness. In terms of apparent brightness, it was the second-brightest supernova of any kind (fainter only than SN 1987A) of the 20th century. It was observed for nearly 700 days, and it became the prototype object for the development of theoretical understanding of Type Ia supernovae.\n\nThe supernova was discovered by Charles Kowal, about 56 arc seconds west and 85 arc seconds south of the center of NGC 5253. The position in the periphery of the galaxy aided observation, minimizing interference by background objects. Well-positioned for Southern Hemisphere observers, it was quite observable from Northern Hemisphere observatories as well. Attempts made to observe it in X-rays with Uhuru and OSO 7 and to detect gamma rays from it via Cherenkov radiation showers gave at best equivocal results.\n\nPhotometric and spectroscopic measurements were made in the visible and near infrared by many observers, extending to about 700 days after maximum light. Interstellar absorption lines of ionized calcium due to gas both in our galaxy and NGC 5253 were observed, allowing an estimate of the interstellar extinction.\n\nThe extended length of the observed light curve found a remarkably uniform 0.01 magnitudes per day decline starting about 60 days after discovery. Translated into other units, this is almost exactly a 77-day half-life, which is the half-life of Co. In the standard model for Type Ia supernovae, approximately a solar mass of Ni is formed and ejected from a white dwarf which accretes mass from a binary companion and is raised over the Chandrasekhar limit and explodes. This Ni decays with a half-life of about 6 days to Co, and the decay of the cobalt provides the energy radiated away by the supernova remnant. The model also produces an estimate for the luminosity of such a supernova. The observations of SN1972e, both peak brightness and fade rate, were in general agreement with these predictions, and led to rapid acceptance of this degenerate-explosion model.\n\n\n\n"}
{"id": "23241429", "url": "https://en.wikipedia.org/wiki?curid=23241429", "title": "Sciography", "text": "Sciography\n\nSciography, also spelled sciagraphy or skiagraphy (), is a branch of science of the perspective dealing with the projection of shadows, or delineation of an object in perspective with its gradations of light and shade. One of the major professional fields that use this technique is the architectural field. In architecture it is defined as a study of shades and shadows cast by simple architectural forms on plane surfaces.\n\nIn general sciography the light source is imagined as the sun inclined at 45 degrees to both vertical plane and horizontal plane coming from left hand side. The resultant shadow is then drawn. \n\n"}
{"id": "41236318", "url": "https://en.wikipedia.org/wiki?curid=41236318", "title": "Silkeborg Forests", "text": "Silkeborg Forests\n\nSilkeborg Forests is Denmark's largest forest. It comprises a collection of several independent private and public woodlands, that have been allowed to merge into a single connected forest south of the city of Silkeborg. At 22,400 ha, it is Denmark's largest forest since 2004, after Rold Skov, and it is located within the largest forest district in the country as well. The forests forms an important part of the larger regional landscape known as Søhøjlandet.\n\nSilkeborg Forests are named after the city of Silkeborg, situated in the northern outskirts and the forest is naturally divided into a north, west, east and south section, known as Nordskoven, Vesterskoven, Østerskoven and Sønderskoven respectively.\n\nA total area of 1,455 ha of the forest is to be protected under Natura 2000 and is designated as area 57 and EU Habitat area H181. \n\nThe protection comprises several conservationally important forest and aquatic habitats. Amongst these, habitat type 9120 defined as \"Beech forests (\"Fagus sylvatica\") with holly (\"Ilex\"), growing on acidic soils, in a humid Atlantic climate\", is the most prevalent forest habitat at 351 ha, while habitat type 3150, defined as \"Natural eutrophic lakes with \"Magnopotamion\" or \"Hydrocharition\"-type vegetation\" is the most prevalent aquatic habitat in the area, at 188 ha. There are other important - but less extent - habitat types present, like the 54 ha Lobelian lake of Almind Sø for example. \n\nThe designation is also based on the presence of European brook lamprey, Northern crested newt, Pond bat and the European otter living here.\n"}
{"id": "43936526", "url": "https://en.wikipedia.org/wiki?curid=43936526", "title": "Solid earth", "text": "Solid earth\n\nSolid earth refers to \"the earth beneath our feet\" or \"terra firma\", the planet's solid surface and its interior. It contrasts with the Earth's fluid envelopes, the atmosphere and hydrosphere (but includes the ocean basin), as well as the biosphere and interactions with the sun. It includes the liquid core.\n\nSolid-earth science refers to the corresponding methods of study, a subset of Earth sciences, predominantly geophysics and geology, excluding aeronomy, atmospheric sciences, oceanography, hydrology, and ecology.\n\n"}
{"id": "45451026", "url": "https://en.wikipedia.org/wiki?curid=45451026", "title": "Summer park", "text": "Summer park\n\nA summer park is a form of theme park with a summertime theme, open only during the summertime season. Instead of electricity-generated carousels, playgrounds and swimming pool make up most attractions.\n\nIn Denmark, several summer parks have been built.\n\nIn Sweden, the phenomenon was booming by the mid and late 1980s. After expanding during the economic boom, a decrease began in the early 1990s during the economc recession, but also because of the introduction of a wider entertainment market.\n"}
{"id": "7112992", "url": "https://en.wikipedia.org/wiki?curid=7112992", "title": "Terry Tempest Williams", "text": "Terry Tempest Williams\n\nTerry Tempest Williams (born 8 September 1955), is an American author, conservationist, and activist. Williams' writing is rooted in the American West and has been significantly influenced by the arid landscape of her native Utah and its Mormon culture. Her work ranges from issues of ecology and wilderness preservation, to women's health, to exploring our relationship to culture and nature.\n\nWilliams has testified before Congress on women's health, committed acts of civil disobedience in the years 1987–1992 in protest against nuclear testing in the Nevada Desert, and again, in March 2003 in Washington, D.C., with Code Pink, against the Iraq War. She has been a guest at the White House, has camped in the remote regions of the Utah and Alaska wildernesses and worked as \"a barefoot artist\" in Rwanda.\n\nWilliams is the author of a number of books: \"Refuge: An Unnatural History of Family and Place\"; \"An Unspoken Hunger: Stories from the Field\"; \"Desert Quartet\"; \"Leap\"; \"Red: Passion and Patience in the Desert\"; \"The Open Space of Democracy\"; and \"Finding Beauty in a Broken World\".\n\nIn 2006, Williams received the Robert Marshall Award from The Wilderness Society, their highest honor given to an American citizen. She also received the Distinguished Achievement Award from the Western American Literature Association and the 2005 Wallace Stegner Award given by Center of the American West (of the University of Colorado Boulder). She is the recipient of a Lannan Literary Award for Nonfiction and a 1997 Guggenheim Fellowship in creative nonfiction. Williams was featured in Ken Burns' PBS series \"\" (2009) and in Stephen Ives's PBS documentary series \"The West\", which was produced by Burns. In 2011, Williams received the 18th International Peace Award given by the Community of Christ.. In 2014, the Sierra Club honored her with their highest award, the Sierra Club John Muir Award.\n\nWilliams is the current Annie Clark Tanner Fellow in the Environmental Humanities Graduate program at the University of Utah \nand has written for \"The Progressive\". She has been a Montgomery Fellow at Dartmouth College where she continues to teach. She divides her time between Moose, Wyoming and Castle Valley, Utah, where her husband Brooke is field coordinator for the Southern Utah Wilderness Alliance.\n\nTerry Tempest Williams was born in Corona, California, to Diane Dixon Tempest and John Henry Tempest, III. Her father was serving in the United States Air Force in Riverside, California, for two years. She grew up in Salt Lake City, Utah, within sight of Great Salt Lake.\n\nAtomic testing at the Nevada Test Site (outside Las Vegas) between 1951 and 1962 exposed Williams' family to radiation like many Utahns (especially those living in the southern part of the state), which Williams believes is the reason so many members of her family have been affected by cancer. By 1994, nine members of the Tempest family had had mastectomies, and seven had died of cancer. Some of the family members affected by cancer included Williams' own mother, grandmother, and brother.\n\nWilliams met her husband Brooke Williams in 1974 while working part-time at Sam Weller's Bookstore, a Salt Lake City bookstore, where he was a customer. The two married six months after their first meeting and began their life together working at the Teton Science School in Grand Teton National Park.\n\nIn 1976 Williams was hired to teach science at Carden School of Salt Lake City (since renamed Carden Memorial School).\nShe often clashed with the conservative couple that led the school over her unorthodox teaching methods and environmental politics, but she respected their gift of teaching through storytelling and prized her five years there.\n\"Teaching helped me find my voice,\" she later wrote. \"The challenge was to impart large ecological concepts to young burgeoning minds in a language that wasn't polemical, but woven into a compelling story.\"\n\nIn 1978, Williams graduated from the University of Utah with a degree in English and a minor in biology, followed by a Master of Science degree in environmental education in 1984. After graduating from college, Williams worked as a teacher in Montezuma Creek, Utah, on the Navajo Reservation. She worked at the Utah Museum of Natural History from 1986–96, first as curator of education and later as naturalist-in-residence.\n\nWilliams published her first book, \"The Secret Language of Snow\" in 1984. A children’s book written with Ted Major, her mentor at the Teton Science School, it received a National Science Foundation Book Award. Over the next few years, she published three other books: \"Pieces of White Shell: A Journey to Navajo Land\" (1984, illustrated by Clifford Brycelea, a Navajo artist), \"Between Cattails\" (1985, illustrated by Peter Parnall), and \"Coyote’s Canyon\", (1989, with photographs by John Telford).\n\nIn 1991, Williams' memoir, \"\" was published by Pantheon Books. The book interweaves memoir and natural history, explores her complicated relationship to Mormonism, and recounts her mother's diagnosis with ovarian cancer along with the concurrent flooding of the Bear River Migratory Bird Refuge, a place special to Williams since childhood. The book's widely anthologized epilogue, \"The Clan of One-Breasted Women\", explores whether the high incidence of cancer in her family might be due to their status as downwinders during the U.S. Atomic Energy Commission's above-ground nuclear testing in the 1950s and 60s. \"Refuge\" received the 1991 Evans Biography Award from the Mountain West Center for Regional Studies at Utah State University. and the Mountain & Plains Booksellers' Reading the West Book Award for creative nonfiction in 1992.\n\nIn 1995, when the United States Congress was debating issues related to the Utah wilderness, Williams and writer Stephen Trimble edited the collection, \"Testimony: Writers Speak On Behalf of Utah Wilderness\", an effort by twenty American writers to sway public policy. A copy of the book was given to every member of Congress. On 18 September 1996, President Bill Clinton at the dedication of the new Grand Staircase-Escalante National Monument, held up this book and said, \"This made a difference.\"\n\nWilliams' writing on ecological and social issues has appeared in \"The New Yorker\", \"The New York Times\", and \"Orion\" magazine, among others. She has been published in numerous environmental, feminist, political, and literary anthologies. She has also collaborated in the creation of fine art books with photographers Emmet Gowin, Richard Misrach, Debra Bloomfield, Meridel Rubenstein, Rosalie Winard, and Edward Riddell.\n\nWilliams wrote and spoke about the impact of the BP oil spill.\n\nOn 13 June 2014, Williams posted an open letter to the leadership of the Church of Jesus Christ of Latter-day Saints expressing \"solidarity with Kate Kelly and her plea to grant women equal standing in the rights, responsibilities and privileges of the [LDS Church], including the right to hold the Priesthood.\"\n\n\n\n\n\n\n\n\n\n"}
{"id": "1722764", "url": "https://en.wikipedia.org/wiki?curid=1722764", "title": "Tropical Grasslands (journal)", "text": "Tropical Grasslands (journal)\n\nTropical Grasslands was a peer-reviewed scientific journal published by the Tropical Grassland Society of Australia which was formed in 1963. The journal was established in 1967 and was discontinued in 2010 when the Tropical Grassland Society of Australia closed due to declining membership. It covered ecological and agricultural aspects of grasslands. Archived digital copies of articles are available for free online. The founding editor-in-chief was N.H. Shaw and the final editor was Lyle Winks.\n\nIn 2012, a new peer-reviewed scientific journal, \"Tropical Grasslands – Forrajes Tropicales\", covering research on tropical pastures and forages was established as an initiative of scientists from Australia and the International Center for Tropical Agriculture. It succeeds the Spanish-only journal \"Pasturas Tropicales\", published from 1979 to 2007, and \"Tropical Grasslands\". Lyle Winks, the former editor of \"Tropical Grasslands\", is the new journal's English editor; Rainer Schultze-Kraft is in charge of submissions in Spanish.\n\n"}
{"id": "12633653", "url": "https://en.wikipedia.org/wiki?curid=12633653", "title": "Veracruz moist forests", "text": "Veracruz moist forests\n\nThe Veracruz moist forests are an ecoregion, in the tropical and subtropical moist broadleaf forest biome, in eastern Mexico.\n\nThe Veracruz moist forests cover an area of , occupying a portion of Mexico's Gulf Coastal Plain between the Sierra Madre Oriental and the Gulf of Mexico. The forests extend from southern Tamaulipas state across northern Veracruz, eastern San Luis Potosí, and portions of eastern Hidalgo and northeastern Puebla.\n\nTo the north, the forests transition to the dry lowland Tamaulipan mezquital and the upland Tamaulipan matorral. To the east, the Sierra Madre Oriental pine-oak forests occupy the higher elevations of the Sierra Madre Oriental. South of the gap where the Panuco River cuts through the Sierra Madre, the Veracruz montane forests and Oaxacan montane forests occupy middle slopes of the Sierra. The Veracruz dry forests separate the Veracruz moist forests from the Petén-Veracruz moist forests further south.\n\nThe northernmost extension of the Veracruz moist forests occurs in the El Cielo Biosphere and the Sierra de Tamaulipas at a latitude of about 23° 20′ degrees north.\n\nThe climate of the region is tropical and humid, with rains during seven months of the year and mild variation in temperature. Average annual rainfall is .\n\nThe canopy of this ecoregion is characterized by trees reaching a height of up to , such as Mayan breadnut (\"Brosimum alicastrum\"), sapodilla (\"Manilkara zapota\"), rosadillo (\"Celtis monoica\"), \"Bursera simaruba\", \"Dendropanax arboreus\", and \"Sideroxylon capiri\". The southern parts of the ecoregion feature mahogany (\"Swietenia macrophylla\"), \"Manilkara zapota\", \"Bernoullia flammea\", and \"Astronium graveolens\".\n\nEndemic birds include the red-crowned amazon (\"Amazona viridigenalis\"), Tamaulipas crow (\"Corvus imparatus\"), Altamira yellowthroat (\"Geothlypis flavovelata\"), and crimson-collared grosbeak (\"Rhodothraupis celaeno\"). Two rodent species, the El Carrizo deer mouse (\"Peromyscus ochraventer\") and the Tamaulipan woodrat (\"Neotoma angustapalata\"), are also endemic.\n\nThe forests have been heavily altered by human activity, so that only a few enclaves of mature forest remain. Forests have been cleared for timber harvesting, agriculture, and grazing, and much of the original forest has been replaced with scrubland or secondary forest.\n\nEl Cielo Biosphere Reserve in southern Tamaulipas is the only protected area in the ecoregion.\n"}
{"id": "16561206", "url": "https://en.wikipedia.org/wiki?curid=16561206", "title": "Water horse", "text": "Water horse\n\nA water horse (or \"waterhorse\" in some folklore) is a mythical creature, such as the Ceffyl Dŵr, Capaill Uisce, the bäckahäst and kelpie.\n\nThe term \"water horse\" was originally a name given to the kelpie, a creature similar to the hippocamp, which has the head, neck and mane of a normal horse, legs like a horse, webbed feet, and a long, two-lobed, whale-like tail. The term has also been used as a nickname for lake monsters, particularly Ogopogo and Nessie. The name \"kelpie\" has often been a nickname for many other Scottish lake monsters, such as each uisge and Morag of Loch Morar and Lizzie of Loch Lochy. Other names for these sea monsters include \"seahorse\" (not referring to the seahorse fish) and \"hippocampus\" (which is the genus name for seahorses).\n\nThe usage of \"water horse\" or \"kelpie\" can often be a source of confusion; some consider the two terms to be synonymous, while others distinguish the water horse as a denizen of lochs and the kelpie of turbulent water such as rivers, fords, and waterfalls. Some authors call one creature of a certain place a kelpie while others call it a water horse. The name \"water bull\" has been used for either creature.\n\nThe Breton King Gradlon's magical \"horse of the sea\" Morvarc'h (whose name literally means \"sea horse\" in Breton) was said to have the ability to gallop upon the waves of the sea, in a similar fashion to the water horses of Cornish legend.\n\nThe water horse has often become a basic description of other lake monsters such as the Canadian Lake Okanagan monster Ogopogo and the Lake Champlain monster Champ. Loch Morar is reputedly home to \"Morag\", a lake monster that has been portrayed as a water horse.\n\nWhilst most Scottish/Celtic folklore places the water horse in a loch (particularly a loch that is famous for a lake monster, such as Loch Ness, Loch Morar or Loch Lomond), some Breton and Cornish tales of water horses place them in the ocean, making them sea monsters.\n\nMost Highland loch have some kind of water-horse tradition, although a study of 19th-century literature of the time showed that only about sixty lochs and lochans merited a mention out of the thousands of bodies of water in Scotland. The water horse that was reputed to inhabit Loch Ness gained the most mentions in Highland literature.\n\nWater horse sightings were reported regularly during the 18th century, but it was not until the 19th century that sightings were recorded.\n"}
{"id": "35805453", "url": "https://en.wikipedia.org/wiki?curid=35805453", "title": "Δ15N", "text": "Δ15N\n\nIn geochemistry, hydrology, paleoclimatology and paleoceanography \"δ\"N (pronounced \"delta fifteen n\") or delta-N-15 is a measure of the ratio of the two stable isotopes of nitrogen, N:N.\n\nTwo very similar expressions for are in wide use in hydrology. Both have the form formula_1 ‰ (‰ = permil or parts per thousand) where \"s\" and \"a\" are the relative abundances of N in respectively the sample and the atmosphere. The difference is whether the relative abundance is with respect to all the nitrogen, i.e. N plus N, or just to N. Since the atmosphere is 99.6337% N and 0.3663% N, \"a\" is 0.003663 in the former case and 0.003663/0.996337 = 0.003676 in the latter. However \"s\" varies similarly; for example if in the sample N is 0.385% and N is 99.615%, \"s\" is 0.003850 in the former case and 0.00385/0.99615 = 0.003865 in the latter. The value of formula_1 is then 51.05‰ in the former case and 51.38‰ in the latter, an insignificant difference in practice given the typical range of -20 to 80 for .\n\nOne use of N is as a tracer to determine the path taken by fertilizers applied to anything from pots to landscapes. Fertilizer enriched in N to an extent significantly different from that prevailing in the soil (which may be different from the atmospheric standard \"a\") is applied at a point and other points are then monitored for variations in .\n\nAnother application is the assessment of human waste water discharge into bodies of water. The abundance of N is greater in human waste water than in natural water sources. Hence in benthic sediment gives an indication of the contribution of human waste to the total nitrogen in the sediment. Sediment cores analyzed for yield an historical record of such waste, with older samples at greater depths.\n\n"}
