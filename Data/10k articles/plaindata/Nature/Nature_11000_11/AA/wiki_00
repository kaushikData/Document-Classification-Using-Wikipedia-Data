{"id": "58016373", "url": "https://en.wikipedia.org/wiki?curid=58016373", "title": "2015 XY261", "text": "2015 XY261\n\nThis small dim asteroid approached Earth in opposition, having an elongation from the Sun of over 160 degrees from mid November of 2015. However due to its absolute magnitude even though it had a very low phase angle during the approach it was too dim for contemporary telescopes to spot it so early, as it was significantly fainter than apparent magnitude 23. However it got brighter as it approached and a few weeks later, on 6 December 2015 it was spotted by Pan-STARRS at an apparent magnitude of 21.5 using a Ritchey–Chrétien telescope.\n\nOn 15 December 2015 13:52 UT the asteroid passed from the Earth and three and a half hours later at 17:17 UT passed from the Moon. \n\nThis flyby is notable because the discovery was 9 days prior to closest approach. This is one of the earliest discoveries of an asteroid during approach. Discounting cataloged objects which were discovered during previous orbits, most asteroids are discovered with little or no warning. was the only asteroid to pass within 1 LD of Earth in 2015 that was discovered more than a week before closest approach. See List of asteroid close approaches to Earth in 2015.\n"}
{"id": "56784404", "url": "https://en.wikipedia.org/wiki?curid=56784404", "title": "Active fluid", "text": "Active fluid\n\nAn active fluid is a densely packed soft material whose constituent elements can self-propel. Examples include dense suspensions of bacteria, microtubule networks or artificial swimmers. These materials come under the broad category of active matter and differ significantly in properties when compared to passive fluids, which can be described using Navier-Stokes equation. Even though systems describable as active fluids have been observed and investigated in different contexts for a long time, scientific interest in properties directly related to the activity has emerged only in the past two decades. These materials have been shown to exhibit a variety of different phases ranging from well ordered patterns to chaotic states (see below). Recent experimental investigations have suggested that the various dynamical phases exhibited by active fluids may have important technological applications.\n\nThe terms “active fluids”, “active nematics” and “active liquid crystals” have been used almost synonymously to denote hydrodynamic descriptions of dense active matter. While in many respects they describe the same phenomenon, there are subtle differences between them. “Active nematics” and “active liquid crystals” refers to systems where the constituent elements have nematic order whereas “active fluids” is the more generic term combining systems with both nematic and polar interactions.\n\nThere are wide range of cellular and intracellular elements which form active fluids. This include systems of microtubule, bacteria, sperm cells as well as inanimate microswimmers. It is known that these systems form a variety of structures such as regular and irregular lattices as well as seemingly random states in two dimensions.\n\nActive fluids have been shown to organize into regular and irregular lattices in a variety of settings. These include irregular hexagonal lattices by microtubules and regular vortex lattice by sperm cells. From topological considerations, it can be seen that the constituent element in quasi stationary states of active fluids should necessarily be vortices. But very less is known, for instance, about the length scale selection in such systems.\n\nChaotic states exhibited by active fluids are termed as active turbulence. Such states are qualitatively similar to hydrodynamic turbulence, by virtue of which they are termed active turbulence. But recent research has indicated that the statistical properties associated with such flows are quite different from that of hydrodynamic turbulence.\n\nThe mechanism behind the formation of various structures in active fluids is an area of active research. It is well understood that the structure formation in active fluids is intimately related to defects or disclinations in the order parameter field (the orientational order of the constituent agents). An important part of research on active fluids involve modelling of dynamics of these defects to study its role in pattern formation and turbulent dynamics in active fluids. Modified versions of Vicsek model are among earliest and continually used approach to model active fluids. Such models have been shown to capture the various dynamical states exhibited by active fluids. More refined approaches include derivation of continuum limit hydrodynamic equations for active fluids and adaptation of liquid crystal theory by including the activity terms.\n\nA few technological applications for active fluids have been proposed such as powering of molecular motors through active turbulence and patterned state. Furthermore, given the innumerable applications liquid crystals find in various technologies, there have been proposals to augment them by using active liquid crystals.\n"}
{"id": "35048724", "url": "https://en.wikipedia.org/wiki?curid=35048724", "title": "Avalon Organic Gardens &amp; EcoVillage", "text": "Avalon Organic Gardens &amp; EcoVillage\n\nAvalon Organic Gardens & EcoVillage also known as Avalon Gardens, is a sustainable community and ecovillage started in 1994 by Global Community Communications Alliance. The ecovillage is located in Tumacacori, Arizona, south of Tubac, Arizona in the Santa Cruz Valley and consists of over 100 residents. Its sustainable practices include recycling, composting, organic gardening, rainwater harvesting, greywater recycling, alternative energy sources, eco-architecture, resource management, and human consumption management.\n\nAvalon Gardens was started in 1994 in northern Arizona, USA. In 2007 the ecovillage moved to the Potreros Ranch in Tumacacori, neighboring the Mission San José de Tumacácori (Tumacacori Mission), started by Father Eusebio Kino in 1691. The ecovillage incorporates some of the traditional agricultural practices employed by the Tumacacori ranchers and farmers in former decades as well as newer permaculture methods. They plan to extend their facilities and housing to accommodate visiting and resident WWOOFers, students, and workshop participants.\n\nAvalon Gardens has a year-round organic food production system that involves greenhouses, garden beds and forest gardening, drip irrigation, soil composting and development, food preservation, and seed saving. Avalon Gardens locally participates in the community-supported agriculture (CSA) program, being the first established in the state of Arizona. Their “Hands In The Soil” mornings are organized for public participation in organic gardening at the ecovillage, and workshops and tours are held regularly for public education and training. The gardens also supplies vegetables to the Food For Ascension Café, an organic vegan-vegetarian and slow food movement restaurant in Tucson.\n\nWater harvesting is important to the ecovillage because they are located in desert climate. At the ecovillage, rainwater is collected from house rooftops and water-catchment tanks. Thousands of gallons of run-off rainwater are redirected and stored to water high-water use trees, their food forest, gardens, and pastures. Through a greywater system, water is harvested from washing machines, indoor and outdoor showers, and house sinks and basins. This water is used to water flower beds, trees, and plants around the homes.\n\nRainwater harvesting has become significant in view of the realization that water has become scarce and it is becoming a critical problem globally. In big cities large residencies are required to have a rainwater harvesting system. For the present, drains are being constructed with conventional materials such as brick and stonemasonry and R.C.C. side drains. Rain water is made to flow through these drains to a centralized water storage tank. Water from this tank is designated for nondomestic and gardening purposes. The water storage tank is of brick masonry or R.C.C. The drains for the flow of rainwater and the tank for the storage of rainwater can be constructed using Ferrocement technology. Ferrocement drains are machine-made, precast products about 2.4 to 5 meters long with covers and perforations of the same length at about 25 mm thick. The surfaces of these drains are known to be strong and not easily eroded by the flow of water. The bottom and sides of the water storage tanks consist of ferrocement plates with space frames. These plates are about 25 mm thick, waterproof, strong, durable, and low-maintenance.\n\nThe ecovillage uses sustainable, renewable, and recycled building materials in their building and construction projects. Paper and cardboard products are reused by blending them into a fiber pulp that is mixed with cement, poured into molds, and dried. This reused paper-cement mix is called papercrete, and the blocks are used as building material for walls, benches, flower beds, and other types of structures. Some of the sustainable dwellings at Avalon Gardens include earth domes, monolithic domes, yurts, and Eco-friendly houses.\n\nTo reduce their ecological footprint, noise and air pollution, ecovillage residents carpool, group-transit, bike, walk, and use other means of sustainable transport. Assigned shoppers buy food and supplies in bulk, and designated shuttle drivers make daily pickups and deliveries for over 100 residents.\n\nEcovillage residents share resources like vehicles, washing machines, and household appliances to make the most and best use of them. One of their facilities is a community kitchen and dining room where most of the meals are prepared and served for the entire community. Bio-friendly cleaning products and personal care products are also used to avoid water and environmental contamination.\n\n\n"}
{"id": "13257984", "url": "https://en.wikipedia.org/wiki?curid=13257984", "title": "Bear Creek Lake State Park", "text": "Bear Creek Lake State Park\n\nBear Creek Lake State Park is a state park located in Cumberland, Virginia, United States. It is a recreational and camping facility that surrounds an artificial lake situated in the Cumberland State Forest.\n\nBear Creek Lake was built by members of the Civilian Conservation Corps in 1938 as a project of the Virginia Department of Agriculture through the State Forestry Division. The lake was built using labor from local carpenters, farmers and un-skilled laborers seeking jobs. In addition to the lake, two pavilions, a concession stand and six fireplaces were constructed. In 1940, the area opened as a forestry wayside with boat rentals and swimming. In 1958, the area was given to the Division of State Parks and was operated as a day-use recreation area. In 1962 the division added campgrounds and the area's name was changed to Bear Creek Lake State Park.\n\nThe park features a lake with a boat launch, fishing pier, boat rentals and a swimming beach. Visitors will also find a meeting facility, cabins, camping, picnicking, an archery range and playgrounds. Guests also enjoy the park's nine trails and access to the adjoining Cumberland State Forest, including the Cumberland Multi-use Trail. This trail is available for hiking, biking and horseback riding.\n\n"}
{"id": "33092462", "url": "https://en.wikipedia.org/wiki?curid=33092462", "title": "Cirrostratus nebulosus", "text": "Cirrostratus nebulosus\n\nCirrostratus nebulosus is a species of high-level cirrostratus cloud. The name \"cirrostratus nebulosus\" is derived from Latin, the adjective \"nebulosus\" meaning \"full of vapor, foggy, cloudy, dark\". Cirrostratus nebulosus is one of the two most common forms that cirrostratus often takes, with the other being cirrostratus fibratus. The nebulosus species is featureless and uniform, while the fibratus species has a fibrous appearance. Cirrostratus nebulosus are formed by gently rising air. The cloud is often hard to see unless the sun shines through it at the correct angle, forming a halo. While usually very light, the cloud may also be very dense, and the exact appearance of the cloud can vary from one formation to another. In the winter, precipitation often follows behind these clouds; however, they are not a precipitation-producing cloud.\n\n\n"}
{"id": "29082171", "url": "https://en.wikipedia.org/wiki?curid=29082171", "title": "Computation of radiowave attenuation in the atmosphere", "text": "Computation of radiowave attenuation in the atmosphere\n\nOne of the causes of attenuation of radio propagation is the absorption by the atmosphere. There are many well known facts on the phenomenon and qualitative treatments in textbooks. A document published by the International Telecommunication Union (ITU)\n\nprovides some basis for a quantitative assessment of the attenuation. That document describes a simplified model along with semi-empirical formulas based on data fitting. It also recommended an algorithm to compute the attenuation of radiowave propagation in the atmosphere. NASA also published a study on a related subject. Free software from CNES based on ITU-R recommendations is available for download and is available to the public.\n\nThe document ITU-R P.676-8 of the ITU-R section considers the atmosphere as being divided into spherical homogeneous layers; each layer has a constant refraction index. By the use of trigonometry, a couple of formulas and an algorithm were derived.\n\nThrough the use of an invariant, the same results can be directly derived:\n\nAn incident ray at A under the angle Φ hits the layer B at the angle \"θ\". From basic Euclidean geometry:\n\nBy Snell's law:\n\nso that\n\nNotes:\n\n\nThe ITU recommended algorithm consists of launching a ray from a radio source, then at each step, a layer is chosen and a new incidence angle is then computed. The process is iterated until the altitude of the target is reached. At each step, the covered distance \"dL\" is multiplied by a specific attenuation coefficient \"g\" expressed in dB/km. All the increments \"g\" \"dL\" are added to provide the total attenuation.\n\nNote that the algorithm does not guaranty that the target is actually reached. For this, a much harder boundary value problem would have to be solved.\n\nThis equation is discussed in the references. The equation is highly non-linear. Given that a smooth data fitting curve n(altitude) is provided by the ITU for the refraction index n, and that the values of n differs from 1 only by something of the order 10, a numerical solution of the eikonal equation can be considered. Usually the equation is presented under the self-adjoint form, a more tractable equation for the ray head position vector r is given in generic parametric form:\n\nThree implementations to compute the attenuations exist:\n\n\nThe first two are only of 1st order approximation (see Orders of approximation). For the eikonal equation, many numerical schemes are available. Here only a simple second order scheme was chosen. For most standard configurations of source-target, the three methods differ little from each other. It is only in the case of rays grazing the ground that the differences are meaningful. The following was used for testing:\n\nAt the latitude of 10°, when a ray starts at 5 km altitude with an elevation angle of −1° to hit a target at the same longitude but at latitude 8.84° and altitude 30 km. At 22.5 GHz, the results are:\n\nNote that 22.5 GHz is not a practical frequency but it is the most suitable for algorithms comparison. In the table, the first column gives the results in dB, the third gives the distance covered and the last gives the final altitude. Distances are in km. From the altitude 30 km up, the attenuation is negligible. The paths of the three are plotted:\n\nThe linear path is the highest on the figure, the eikonal is the lowest.\n\nNote: A MATLAB version for the uplink (Telecommunications link) is available from the ITU\n\nWhen a point S communicates with a point T, the orientation of the ray is specified by an elevation angle. In a naïve way, the angle can be given by tracing a straight line from S to T. This specification does not guaranty that the ray will reach T: the variation of refraction index bends the ray trajectory. The elevation angle has to be modified to take into account the bending effect.\n\nFor the Eikonal equation, this correction can be done by solving a boundary value problem. As the equation is of second order, the problem is well defined. In spite of the lack of a firm theoretical basis for the ITU method, a trial-error by dichotomy (or binary search) can also be used. The next figure shows the results of numerical simulations.\n\nThe curve labeled as bvp is the trajectory found by correcting the elevation angle. The other two are from a fix step and a variable steps (chosen in accordance to the ITU recommendations) solutions without the elevation angle correction. The nominal elevation angle for this case is -0.5 degree. The numerical results obtained at 22.5 GHz were:\n\nNote the way the solution bvp bents over the straight line. A consequence of this property is that the ray can reach locations situated below the horizon of S. This is consistent with observations. The trajectory is a Concave function is a consequence of the fact that the gradient of the refraction index is negative, so the Eikonal equation implies that the second derivative of the trajectory is negative. From the point where the ray is parallel to ground, relative to the chosen coordinates, the ray goes down but relative to ground level, the ray goes up.\n\nOften engineers are interested in finding the limits of a system. In this case, a simple idea is to try some low elevation angle and let the ray reach the desired altitude. This point of view has a problem: if suffice to take the angle for which the ray has a tangent point of lowest altitude. For instance with the case of a source at 5 km altitude, of nominal elevation angle -0.5 degree and the target is at 30 km altitude; the attenuation found by the boundary value method is 11.33 dB. The previous point of view of worst case leads to an elevation angle of -1.87 degree and an attenuation of 170.77 dB. With this kind of attenuation, every system would be unusable! It was found also for this case that with the nominal elevation angle, the distance of the tangent point to ground is 5.84 km; that of the worst case is 2.69 km. The nominal distance from source to target is 6383.84 km; for the worst case, it is 990.36 km.\n\nThere are many numerical methods to solve boundary value problems. For the Eikonal equation, due to the good behavior of the refraction index just a simple Shooting method can be used.\n\nOf the three methods, the linear and the ITU methods require some coding since they are not presented as differential equations. These methods do not benefit from the help of standard numerical packages; however, only high school mathematics are required to understand the methods. The more technical eikonal equation can be solved using standard differential equations solvers offered by a few numerical software packages mentioned in the Wikipedia List of numerical analysis software and it offers a higher precision order.\n\nThe attenuation mechanism as described here is only one amongst many others. The full problem is much more complex.\n\n\n"}
{"id": "5615", "url": "https://en.wikipedia.org/wiki?curid=5615", "title": "Cretaceous", "text": "Cretaceous\n\nThe Cretaceous (, ) is a geologic period and system that spans 79 million years from the end of the Jurassic Period million years ago (mya) to the beginning of the Paleogene Period mya. It is the last period of the Mesozoic Era, and the longest period of the Phanerozoic Eon. The Cretaceous Period is usually abbreviated K, for its German translation \"Kreide\" (chalk, \"creta\" in Latin).\n\nThe Cretaceous was a period with a relatively warm climate, resulting in high eustatic sea levels that created numerous shallow inland seas. These oceans and seas were populated with now-extinct marine reptiles, ammonites and rudists, while dinosaurs continued to dominate on land. During this time, new groups of mammals and birds, as well as flowering plants, appeared.\n\nThe Cretaceous (along with the Mesozoic) ended with the Cretaceous–Paleogene extinction event, a large mass extinction in which many groups, including non-avian dinosaurs, pterosaurs and large marine reptiles died out. The end of the Cretaceous is defined by the abrupt Cretaceous–Paleogene boundary (K–Pg boundary), a geologic signature associated with the mass extinction which lies between the Mesozoic and Cenozoic eras.\n\nThe Cretaceous as a separate period was first defined by Belgian geologist Jean d'Omalius d'Halloy in 1822, using strata in the Paris Basin and named for the extensive beds of chalk (calcium carbonate deposited by the shells of marine invertebrates, principally coccoliths), found in the upper Cretaceous of Western Europe. The name Cretaceous was derived from Latin \"creta\", meaning \"chalk\".\n\nThe Cretaceous is divided into Early and Late Cretaceous epochs, or Lower and Upper Cretaceous series. In older literature the Cretaceous is sometimes divided into three series: Neocomian (lower/early), Gallic (middle) and Senonian (upper/late). A subdivision in eleven stages, all originating from European stratigraphy, is now used worldwide. In many parts of the world, alternative local subdivisions are still in use.\n\nAs with other older geologic periods, the rock beds of the Cretaceous are well identified but the exact age of the system's base is uncertain by a few million years. No great extinction or burst of diversity separates the Cretaceous from the Jurassic. However, the top of the system is sharply defined, being placed at an iridium-rich layer found worldwide that is believed to be associated with the Chicxulub impact crater, with its boundaries circumscribing parts of the Yucatán Peninsula and into the Gulf of Mexico. This layer has been dated at 66.043 Ma.\n\nA 140 Ma age for the Jurassic-Cretaceous boundary instead of the usually accepted 145 Ma was proposed in 2014 based on a stratigraphic study of Vaca Muerta Formation in Neuquén Basin, Argentina. Víctor Ramos, one of the authors of the study proposing the 140 Ma boundary age sees the study as a \"first step\" toward formally changing the age in the International Union of Geological Sciences.\n\nFrom youngest to oldest, the subdivisions of the Cretaceous period are:\n\nLate Cretaceous\n\nMaastrichtian – (66-72.1 MYA)\n\nCampanian – (72.1-83.6 MYA)\n\nSantonian – (83.6-86.3 MYA)\n\nConiacian – (86.3-89.8 MYA)\n\nTuronian – (89.8-93.9 MYA)\n\nCenomanian – (93.9-100.5 MYA)\n\nEarly Cretaceous\n\nAlbian – (100.5-113.0 MYA)\n\nAptian – (113.0-125.0 MYA)\n\nBarremian – (125.0-129.4 MYA)\n\nHauterivian – (129.4-132.9 MYA)\n\nValanginian – (132.9-139.8 MYA)\n\nBerriasian – (139.8-145.0 MYA)\n\nThe high sea level and warm climate of the Cretaceous meant large areas of the continents were covered by warm, shallow seas, providing habitat for many marine organisms. The Cretaceous was named for the extensive chalk deposits of this age in Europe, but in many parts of the world, the deposits from the Cretaceous are of marine limestone, a rock type that is formed under warm, shallow marine circumstances. Due to the high sea level, there was extensive space for such sedimentation. Because of the relatively young age and great thickness of the system, Cretaceous rocks are evident in many areas worldwide.\n\nChalk is a rock type characteristic for (but not restricted to) the Cretaceous. It consists of coccoliths, microscopically small calcite skeletons of coccolithophores, a type of algae that prospered in the Cretaceous seas.\n\nIn northwestern Europe, chalk deposits from the Upper Cretaceous are characteristic for the Chalk Group, which forms the white cliffs of Dover on the south coast of England and similar cliffs on the French Normandian coast. The group is found in England, northern France, the low countries, northern Germany, Denmark and in the subsurface of the southern part of the North Sea. Chalk is not easily consolidated and the Chalk Group still consists of loose sediments in many places. The group also has other limestones and arenites. Among the fossils it contains are sea urchins, belemnites, ammonites and sea reptiles such as \"Mosasaurus\".\n\nIn southern Europe, the Cretaceous is usually a marine system consisting of competent limestone beds or incompetent marls. Because the Alpine mountain chains did not yet exist in the Cretaceous, these deposits formed on the southern edge of the European continental shelf, at the margin of the Tethys Ocean.\n\nStagnation of deep sea currents in middle Cretaceous times caused anoxic conditions in the sea water leaving the deposited organic matter undecomposed. Half the worlds petroleum reserves were laid down at this time in the anoxic conditions of what would become the Persian Gulf and the Gulf of Mexico. In many places around the world, dark anoxic shales were formed during this interval. These shales are an important source rock for oil and gas, for example in the subsurface of the North Sea.\n\nDuring the Cretaceous, the late-Paleozoic-to-early-Mesozoic supercontinent of Pangaea completed its tectonic breakup into the present-day continents, although their positions were substantially different at the time. As the Atlantic Ocean widened, the convergent-margin mountain building (orogenies) that had begun during the Jurassic continued in the North American Cordillera, as the Nevadan orogeny was followed by the Sevier and Laramide orogenies.\nThough Gondwana was still intact in the beginning of the Cretaceous, it broke up as South America, Antarctica and Australia rifted away from Africa (though India and Madagascar remained attached to each other); thus, the South Atlantic and Indian Oceans were newly formed. Such active rifting lifted great undersea mountain chains along the welts, raising eustatic sea levels worldwide. To the north of Africa the Tethys Sea continued to narrow. Broad shallow seas advanced across central North America (the Western Interior Seaway) and Europe, then receded late in the period, leaving thick marine deposits sandwiched between coal beds. At the peak of the Cretaceous transgression, one-third of Earth's present land area was submerged.\n\nThe Cretaceous is justly famous for its chalk; indeed, more chalk formed in the Cretaceous than in any other period in the Phanerozoic. Mid-ocean ridge activity—or rather, the circulation of seawater through the enlarged ridges—enriched the oceans in calcium; this made the oceans more saturated, as well as increased the bioavailability of the element for calcareous nanoplankton. These widespread carbonates and other sedimentary deposits make the Cretaceous rock record especially fine. Famous formations from North America include the rich marine fossils of Kansas's Smoky Hill Chalk Member and the terrestrial fauna of the late Cretaceous Hell Creek Formation. Other important Cretaceous exposures occur in Europe (e.g., the Weald) and China (the Yixian Formation). In the area that is now India, massive lava beds called the Deccan Traps were erupted in the very late Cretaceous and early Paleocene.\n\nThe cooling trend of last epoch of the Jurassic continued into the first age of the Cretaceous. There is evidence that snowfalls were common in the higher latitudes and the tropics became wetter than during the Triassic and Jurassic. Glaciation was however restricted to high-latitude mountains, though seasonal snow may have existed farther from the poles. Rafting by ice of stones into marine environments occurred during much of the Cretaceous but evidence of deposition directly from glaciers is limited to the Early Cretaceous of the Eromanga Basin in southern Australia.\n\nAfter the end of the first age, however, temperatures increased again, and these conditions were almost constant until the end of the period. The warming may have been due to intense volcanic activity which produced large quantities of carbon dioxide. Between 70–69 Ma and 66–65 Ma, isotopic ratios indicate elevated atmospheric CO2 pressures with levels of 1000–1400 ppmV and mean annual temperatures in west Texas between 21 and 23 °C (70-73 °F). Atmospheric CO2 and temperature relations indicate a doubling of pCO2 was accompanied by a ~0.6 °C increase in temperature. The production of large quantities of magma, variously attributed to mantle plumes or to extensional tectonics, further pushed sea levels up, so that large areas of the continental crust were covered with shallow seas. The Tethys Sea connecting the tropical oceans east to west also helped to warm the global climate. Warm-adapted plant fossils are known from localities as far north as Alaska and Greenland, while dinosaur fossils have been found within 15 degrees of the Cretaceous south pole.\n\nNonetheless, there is evidence of Antarctic marine glaciation in the Turonian Age.\n\nA very gentle temperature gradient from the equator to the poles meant weaker global winds, which drive the ocean currents, resulted in less upwelling and more stagnant oceans than today. This is evidenced by widespread black shale deposition and frequent anoxic events. Sediment cores show that tropical sea surface temperatures may have briefly been as warm as , warmer than at present, and that they averaged around . Meanwhile, deep ocean temperatures were as much as warmer than today's.\n\nFlowering plants (angiosperms) spread during this period, although they did not become predominant until the Campanian Age near the end of the period. Their evolution was aided by the appearance of bees; in fact angiosperms and insects are a good example of coevolution. The first representatives of many leafy trees, including figs, planes and magnolias, appeared in the Cretaceous.\n\nAt the same time, some earlier Mesozoic gymnosperms continued to thrive; pehuéns (monkey puzzle trees, \"Araucaria\") and other conifers being notably plentiful and widespread. Some fern orders such as Gleicheniales appeared as early in the fossil record as the Cretaceous and achieved an early broad distribution. Gymnosperm taxa like Bennettitales and hirmerellan conifers died out before the end of the period.\n\nOn land, mammals were generally small sized, but a very relevant component of the fauna, with cimolodont multituberculates outnumbering dinosaurs in some sites. Neither true marsupials nor placentals existed until the very end, but a variety of non-marsupial metatherians and non-placental eutherians had already begun to diversify greatly, ranging as carnivores (Deltatheroida), aquatic foragers (Stagodontidae) and herbivores (\"Schowalteria\", Zhelestidae). Various \"archaic\" groups like eutriconodonts were common in the Early Cretaceous, but by the Late Cretaceous northern mammalian faunas were dominated by multituberculates and therians, with dryolestoids dominating South America.\n\nThe apex predators were archosaurian reptiles, especially dinosaurs, which were at their most diverse stage. Pterosaurs were common in the early and middle Cretaceous, but as the Cretaceous proceeded they declined for poorly understood reasons (once thought to be due to competition with early birds, but now it is understood avian adaptive radiation is not consistent with pterosaur decline), and by the end of the period only two highly specialized families remained.\n\nThe Liaoning lagerstätte (Chaomidianzi formation) in China is a treasure chest of preserved remains of numerous types of small dinosaurs, birds and mammals, that provides a glimpse of life in the Early Cretaceous. The coelurosaur dinosaurs found there represent types of the group Maniraptora, which is transitional between dinosaurs and birds, and are notable for the presence of hair-like feathers.\n\nInsects diversified during the Cretaceous, and the oldest known ants, termites and some lepidopterans, akin to butterflies and moths, appeared. Aphids, grasshoppers and gall wasps appeared.\n\nIn the seas, rays, modern sharks and teleosts became common. Marine reptiles included ichthyosaurs in the early and mid-Cretaceous (becoming extinct during the late Cretaceous Cenomanian-Turonian anoxic event), plesiosaurs throughout the entire period, and mosasaurs appearing in the Late Cretaceous.\n\n\"Baculites\", an ammonite genus with a straight shell, flourished in the seas along with reef-building rudist clams. The Hesperornithiformes were flightless, marine diving birds that swam like grebes. Globotruncanid Foraminifera and echinoderms such as sea urchins and starfish (sea stars) thrived. The first radiation of the diatoms (generally siliceous shelled, rather than calcareous) in the oceans occurred during the Cretaceous; freshwater diatoms did not appear until the Miocene. The Cretaceous was also an important interval in the evolution of bioerosion, the production of borings and scrapings in rocks, hardgrounds and shells.\n\nThe impact of a large body with the Earth may have been the punctuation mark at the end of a progressive decline in biodiversity during the Maastrichtian Age of the Cretaceous Period. The result was the extinction of three-quarters of Earth's plant and animal species. The impact created the sharp break known as K–Pg boundary (formerly known as the K–T boundary). Earth's biodiversity required substantial time to recover from this event, despite the probable existence of an abundance of vacant ecological niches.\n\nDespite the severity of K-Pg extinction event, there was significant variability in the rate of extinction between and within different clades. Species which depended on photosynthesis declined or became extinct as atmospheric particles blocked solar energy. As is the case today, photosynthesizing organisms, such as phytoplankton and land plants, formed the primary part of the food chain in the late Cretaceous, and all else that depended on them suffered as well. Herbivorous animals, which depended on plants and plankton as their food, died out as their food sources became scarce; consequently, the top predators such as \"Tyrannosaurus rex\" also perished. Yet only three major groups of tetrapods disappeared completely; the non-avian dinosaurs, the plesiosaurs and the pterosaurs. The other Cretaceous groups that did not survive into the Cenozoic era, the ichthyosaurs and last remaining temnospondyls and non-mammalian cynodonts were already extinct millions of years before the event occurred.\n\nCoccolithophorids and molluscs, including ammonites, rudists, freshwater snails and mussels, as well as organisms whose food chain included these shell builders, became extinct or suffered heavy losses. For example, it is thought that ammonites were the principal food of mosasaurs, a group of giant marine reptiles that became extinct at the boundary.\n\nOmnivores, insectivores and carrion-eaters survived the extinction event, perhaps because of the increased availability of their food sources. At the end of the Cretaceous there seem to have been no purely herbivorous or carnivorous mammals. Mammals and birds which survived the extinction fed on insects, larvae, worms and snails, which in turn fed on dead plant and animal matter. Scientists theorise that these organisms survived the collapse of plant-based food chains because they fed on detritus.\n\nIn stream communities, few groups of animals became extinct. Stream communities rely less on food from living plants and more on detritus that washes in from land. This particular ecological niche buffered them from extinction. Similar, but more complex patterns have been found in the oceans. Extinction was more severe among animals living in the water column, than among animals living on or in the seafloor. Animals in the water column are almost entirely dependent on primary production from living phytoplankton, while animals living on or in the ocean floor feed on detritus or can switch to detritus feeding.\n\nThe largest air-breathing survivors of the event, crocodilians and champsosaurs, were semi-aquatic and had access to detritus. Modern crocodilians can live as scavengers and can survive for months without food and go into hibernation when conditions are unfavorable, and their young are small, grow slowly, and feed largely on invertebrates and dead organisms or fragments of organisms for their first few years. These characteristics have been linked to crocodilian survival at the end of the Cretaceous.\n\n\n\n"}
{"id": "212101", "url": "https://en.wikipedia.org/wiki?curid=212101", "title": "Diffuse sky radiation", "text": "Diffuse sky radiation\n\nDiffuse sky radiation is solar radiation reaching the Earth's surface after having been scattered from the direct solar beam by molecules or particulates in the atmosphere. Also called sky radiation, diffuse skylight, or just skylight, it is the reason for the color changes of the sky. Of the total light removed from the direct solar beam by scattering in the atmosphere (approximately 25% of the incident radiation when and where the Sun is high in the sky, depending on the amount of dust, haze, and other particulates in the atmosphere), about two-thirds ultimately reaches the earth as diffuse sky radiation. When the Sun is at the zenith in a cloudless sky, with 1361 W/m above the atmosphere, direct sunlight is about 1050 W/m, and total insolation about 1120 W/m. This implies that under these conditions the diffuse radiation is only about 70 W/m out of the original 1361 W/m.\n\nThe dominant radiative scattering processes in the atmosphere (Rayleigh scattering and Mie scattering) are elastic in nature, by which light can be deviated from its path without being absorbed and with no change in wavelength.\n\nThe sunlit sky is blue because air scatters short-wavelength light more than longer wavelengths. Since blue light is at the short-wavelength end of the visible spectrum, it is more strongly scattered in the atmosphere than long-wavelength red light. The result is that when looking toward parts of the sky other than the sun, the human eye perceives them to be blue. The color perceived is similar to that obtained by a monochromatic blue of a wavelength of mixed with white light, \"i.e.\", an unsaturated blue light. The explanation of the blue color by Rayleigh in 1871 is one of the most famous examples of the application of dimensional analysis in solving a problem in physics.\n\nNear sunrise and sunset, most of the sunlight arrives nearly tangentially to the Earth's surface; thus, the light's path through the atmosphere is so long that much of the blue and even green light is scattered out along the way, leaving the sun rays and the clouds it illuminates red. Therefore, when looking at the sunset and sunrise, we see the color red more than the other colors.\n\nScattering and absorption are major causes of the attenuation of radiation by the atmosphere. Scattering varies as a function of the ratio of the particle diameter to the wavelength of the radiation. When this ratio is less than about one-tenth, Rayleigh scattering occurs. In this case, the scattering coefficient varies inversely with the fourth power of the wavelength. At larger ratios, scattering varies in a complex fashion, described for spherical particles by the Mie theory; the laws of geometric optics begin to apply at a ratio of the order of 10.\n\nIn the example of the sky at the zenith, it is blue during broad daylight due to Rayleigh scattering involving diatomic gases (, ). Near sunset and especially during twilight, ozone () absorption significantly contributes to maintaining the sky's blue color.\n\nThere are four commonly detectable points of zero polarization of diffuse sky radiation (known as neutral points) lying along the vertical circle through the sun.\n\nThere is essentially no direct sunlight under an overcast sky, so all light is then diffuse sky radiation. The flux of light is not very wavelength dependent because the cloud droplets are larger than the light's wavelength and scatter all colors approximately equally. The light passes through the translucent clouds in a manner similar to frosted glass. The intensity ranges (roughly) from of direct sunlight for relatively thin clouds down to of direct sunlight under the extreme of thickest storm clouds.\n\nOne of the equations for total solar radiation is:\n\nwhere \"H\" is the beam radiation irradiance, \"R\" is the tilt factor for beam radiation, \"H\" is the diffuse radiation irradiance, \"R\" is the tilt factor for diffuse radiation and \"R\" is the tilt factor for reflected radiation.\n\n\"R\" is given by:\n\nwhere \"δ\" is the solar declination, \"Φ\" is the latitude, \"β\" is an angle from the horizontal and \"h\" is the solar hour angle.\n\n\"R\" is given by:\n\nand \"R\" by:\n\nwhere \"ρ\" is the reflectivity of the surface.\n\nThe eruption of the Philippines volcano - Mount Pinatubo in June 1991 ejected roughly of magma and \"17,000,000 metric tons\"(17 teragrams) of sulfur dioxide SO into the air, introducing ten times as much total SO as the 1991 Kuwaiti fires, mostly during the explosive Plinian/Ultra-Plinian event of June 15, 1991, creating a global stratospheric SO haze layer which persisted for years. This resulted in the global average temperature dropping by about . As volcanic ash falls out of the atmosphere rapidly, the negative agricultural effects of the eruption were largely immediate and localized to a relatively small area in close proximity to the eruption, as they were caused by the resulting thick ash cover that resulted. Globally however, despite a several-month 5% drop in overall solar irradiation, and a reduction in direct sunlight by 30%, there was no negative impact to global agriculture. Surprisingly, a 3-4 year increase in global Agricultural productivity and forestry growth was observed, excepting boreal forest regions. The means by which this was discovered, is that initially at the time, a mysterious drop in the rate at which carbon dioxide (CO) was filling the atmosphere was observed, which is charted in what is known as the \"Keeling Curve\". This led numerous scientists to assume that this reduction was due to the lowering of the Earth's temperature, and with that, a slow down in plant and soil respiration, indicating a deleterious impact to global agriculture from the volcanic haze layer. However upon actual investigation, the reduction in the rate at which carbon dioxide filled the atmosphere did not match up with the hypothesis that plant respiration rates had declined. Instead the advantageous anomaly was relatively firmly linked to an unprecedented increase in the growth/net primary production, of global plant life, resulting in the increase of the carbon sink effect of global photosynthesis. The mechanism by which the increase in plant growth was possible, was that the 30% reduction of direct sunlight can also be expressed as an increase or \"enhancement\" in the amount of diffuse sunlight.\n\nThis diffuse light, owing to its intrinsic nature, can illuminate under-canopy leaves permitting more efficient total whole-plant photosynthesis than would otherwise be the case. In stark contrast to the effect of totally clear skies and the direct sunlight that results from it, which casts shadows onto understorey leaves, strictly limiting plant photosynthesis to the top canopy layer. This increase in global agriculture from the volcanic haze layer also naturally results as a product of other aerosols that are not emitted by volcanoes, such as man-made \"moderately thick smoke loading\" pollution, as the same mechanism, the \"aerosol direct radiative effect\" is behind both.\n\n"}
{"id": "3214240", "url": "https://en.wikipedia.org/wiki?curid=3214240", "title": "Domesticated plants of Mesoamerica", "text": "Domesticated plants of Mesoamerica\n\nDomesticated plants of Mesoamerica, established by agricultural developments and practices over several thousand years of pre-Columbian history, include \"maize\" and \"capsicum\". A list of Mesoamerican cultivars and staples:\n\nMaize was domesticated in Western Mexico and Mesoamerican cultures expanded wherever it was cultivated. It became widespread in the Late Archaic Period and was grown wherever conditions allowed.\n\nThe early use of maize focused on the consumption of unripened kernels. Before people settled into villages and began farming, the amount of time it took to invest in maize was too great. The output of wild maize did not justify the time and work needed to grow the crop.\n\nHowever, maize could be both dried and stored which was very important to early Mesoamericans as it could be used on a year-round basis. Drying meant that it could be transported as well. The common bean (\"Phaseolus vulgaris\") was often grown with maize. These two plants provide complementary dietary amino acids. Improved bioavailability of maize was discovered using a special process involving limewater, which also added calcium.\n\nMaize is also associated with festival and feast foods. Before it was domesticated and became a main crop, maize was used as a basis for beer. Beer was transported in decorated vessels and ceramic pottery. These vessels could be taken to social and ritual occasions.\n\nRitual events or festivals, such as ball games, feasts, and calendar turnings, involved the royal members who took part in the sacrifice of blood-letting and piercing as repayment to the gods for having given maize to the people that year.\n\nAnother example of how maize played such a large role in Mesoamerica, is when deities were portrayed with maize. Quetzalcoatl is connected as being a creator of humans in Mesoamerica. This deity is also seen as the one who took maize from the underworld and gave it to humans in the present world.\n\nCapsicum is the generic name of the chili pepper plant, which is a native domesticated plant from Mesoamerica. Capsaicin reduces the bacterial load when something can not be refrigerated. In Mesoamerica, the capsaicin spice was also used to relieve joint pain, and as an intestinal stimulant, so capsicum is also known as a medicinal plant. The peppers from capsicum plants can be used in a fresh or dried state. A dried chile pepper is stronger and more effective than a fresh chile pepper.\n\nDuring the Middle Archaic Period or the Coxcatlán phase, between 5700-3825 BC, the domestication of plants, such as the chile, was thought to have begun. Mesoamerica's chile crops along with the majority of other food crops, were all domesticated by the Late Formative Period. When the domestication of crops began, the majority of people were working at cultivating fields and crops like the chile. Chiles were a relied on source of food in Mesoamerican times. Chile crops were combined with maize, beans, and squash crops.\n\nChiles were a part of trade and gift giving. Chiefs or other elite members would use foods and stews spiced with chiles when involved in a feast. Using such strongly spiced foods was to show a stylistic and powerful approach to those receiving the dishes.\n\nThe chile plant was featured in different stews including vegetables, turkey, and dog meats and in chile-spiced tomato salsa with tortillas. Chiles were also added at times to cacao, when it was in a beverage form. In Mesoamerica, chiles were used for ritual purposes and therefore, the chile crops did not extend into North and South America like maize, beans, and squash. A cuisine distinct to Mesoamerica was a maize-and-chile pepper based food.\n\nPumpkins, zucchini, acorn squash, butternut squash, others.\n\nFrijol pinto (\"painted/speckled\" bean) nitrogen-fixer traditionally planted in conjunction with the \"two sisters\", maize and squash, to help condition soil; runners grew on maize)\n\nA member of the nightshade or potato family. The word refers both to the plant and the fruit.\n\nA member of the nightshade family. The word refers to both the plant and the tuber. It is related to the tomato and eggplant.\n\nRelated to the laurel, cinnamon, and camphor plants. Also known as “aguacate” in Spanish.\n\nAlso known as chewing gum.\n\nVanilla is a flavoring derived from orchids of the genus \"Vanilla\" native to Mexico. Etymologically, \"vanilla\" derives from the Spanish word \"\", \"little pod\". Originally cultivated by Pre-Columbian Mesoamerican peoples, Spanish conquistador Hernán Cortés is credited with introducing both vanilla and chocolate to Europe in the 1520s. Attempts to cultivate the vanilla plant outside Mexico and Central America proved futile because of the symbiotic relationship between the tlilxochitl vine that produced the vanilla orchid and the local species of Melipona bee; it was not until 1837 that Belgian botanist Charles François Antoine Morren discovered this fact and pioneered a method of artificially pollinating the plant. The method proved financially unworkable and was not deployed commercially. In 1841, a 12-year-old French-owned slave by the name of Edmond Albius, who lived on Île Bourbon, discovered the plant could be hand pollinated, allowing global cultivation of the plant.\n\nThere are currently three major cultivars of vanilla grown globally, all derived from a species originally found in Mesoamerica, including parts of modern-day Mexico. The various subspecies are \"Vanilla planifolia\" (syn. \"V. fragrans\"), grown on Madagascar, Réunion and other tropical areas along the Indian Ocean; \"V. tahitensis\", grown in the South Pacific; and \"V. pompona\", found in the West Indies, Central and South America. The majority of the world's vanilla is the \"V. planifolia\" variety, more commonly known as \"Madagascar-Bourbon\" vanilla, which is produced in a small region of Madagascar and in Indonesia.\n\nVanilla is the second most expensive spice after saffron, due to the extensive labor required to grow the vanilla seed pods. Despite the expense, it is highly valued for its flavor, which author Frederic Rosengarten Jr. described in \"The Book of Spices\" as \"pure, spicy, and delicate\" and its complex floral aroma depicted as a \"peculiar bouquet.\" Despite its high cost, vanilla is widely used in both commercial and domestic baking, perfume manufacture and aromatherapy.\n\nCultivated extensively.\n\nStem segments of prickly pear, the Opuntia cactus.\n\nFruits of many different species of cultivated Opuntia cactus.\n\nJícama (; ; from Nahuatl \"xicamatl\", ), also Yam and Mexican Turnip, is the name of a native Mexican vine, although the name most commonly refers to the plant's edible tuberous root. Jícama is one species in the genus \"Pachyrhizus\". Plants in this genus are commonly referred to as yam bean, although the term \"yam bean\" can be another name for jícama. The other major species of yam beans are also indigenous within the Americas.\n\nOriginally from southern Mexico, particularly Chiapas and Veracruz, Central America and northern South America, the papaya is now cultivated in most tropical countries, such as Brazil, Bangladesh, Pakistan, India, Indonesia, South Africa, Sri Lanka, Vietnam, Philippines and Jamaica. In cultivation, it grows rapidly, fruiting within 3 years. It is, however, highly frost sensitive.\n\nGuava fruit.\n\nAmaranth grain; other species present on other continents.\n\nFruit.\n\nFruit, other parts of plants have noted uses.\n\nUnder cultivation in Mexico and Peru for thousands of years, also source of essential oils.\n\nEdible starchy root also known as manioc; also used to make tapioca.\n\nAlso known as tree spinach.\n\nCultivated from Arizona to Costa Rica. Also gathered from the wild in hot, arid, and semi-arid climates\n\nnote: the tobacco plant was cultivated throughout Central America, the Caribbean and North America.\n"}
{"id": "26876591", "url": "https://en.wikipedia.org/wiki?curid=26876591", "title": "Electricity sector in Venezuela", "text": "Electricity sector in Venezuela\n\nThe electricity sector in Venezuela is one of the few in the world to rely primarily on hydroelectricity, which accounted for 64% in 2015 (71% in 2004). \n\nIn 2015, the total of electricity production reaches 117 TWh, of which 64% comes from hydro, 19% from gas and 17% from oil. Losses however are uncommonly high, reaching 34% of production.\n\nIn 2015, Venezuela produced 75 TWh of hydropower, which accounts 1.9% of world's total, a small increase over the production of 2004 of 70 TWh . The installed capacity had however in 2012 reached 26 GW from a total of 13.76 GW at the end of 2002, where 4.5 GW were under construction and 7.4 GW planned. The World Energy Council energy resource report of 2010 estimates the gross theoretical hydropower production could reach 731 TWh per annum, of which 100 TWh are economically exploitable, an increase over the 320 TWh estimates of 2004. \n\nHydroelectricity production is concentrated on the Caroní River in Guayana Region. Today it has 4 different dams. The largest hydroplant is the Guri dam with 10,200 MW of installed capacity, which makes it the third-largest hydroelectric plant in the world. Other hydroelectric projects on the Caroní are Caruachi Dam, Macagua I, Macagua II and Macagua III, with a total of 15.910 MW of installed capacity in 2003. A new dams, Tocoma (2 160 MW) and Tayucay (2 450 MW), was under construction between Guri and Caruachi in 2003. With a projected installed capacity for the whole Hydroelectric Complex (upstream Caroni River and downstream Caroni River), between 17.250 and 20.000 MW were planned for 2010.\n\nThe largest power companies are state-owned CVG Electrificación del Caroní (EDELCA), a subsidiary of the mining company Corporación Venezolana de Guayana (CVG), and Compania Anonima de Administracion y Fomento Electrico (CADAFE) accounting respectively for approximately 63% and 18% of generating capacities. Other state-owned power companies are ENELBAR and ENELVEN-ENELCO (approximately 8% of capacities). In 2007, PDVSA bought 82.14% percent of Electricidad de Caracas (EDC) from AES Corporation as part of a renationalization program. Subsequently, the ownership share rose to 93.62% (December 2008). EDC has 11% of Venezuelan capacity, and owns the majority of conventional thermal power plants. The rest of the power production is owned by private companies.\n\nThe national transmission system (Sistema Inrterconectado Nacional- SIN) is composed by four interconnected regional transmission systems operated by EDELCA, CADAFE, EDC and ENELVEN-ENELCO. Oficina de Operacion de Sistema Interconectados (OPSIS), jointly owned by the four vertical integrated electric companies, operate the SIN under an RTPA regime.\n"}
{"id": "6110610", "url": "https://en.wikipedia.org/wiki?curid=6110610", "title": "Ethylene propylene rubber", "text": "Ethylene propylene rubber\n\nEthylene propylene rubber (EPR, sometimes called EPM referring to an ASTM standard) is a type of synthetic elastomer that is closely related to EPDM rubber. Since introduction in the 1960s, annual production has increased to 870,000 metric tons. \n\nEPM is considered a valuable elastomer due to its useful chemical and physical properties; it is resistant to heat, oxidation, ozone and the weather (owing to its stable, saturated backbone) and it is also not susceptible to color loss. As a non-polar compound, EPM is an electrical resistor and it is insoluble in many polar solvents, both protic and aprotic. Amorphous forms of EPM are flexible at low temperatures (with glass transition points around -60 °C). Via selection of certain sulfur compounds EPM can remain heat resistant up to 130 °C and up to 160 °C with peroxide curing. These two tables contain some of the main properties of EPM.\n\nEPM has a large number of uses due to the many ways in which the polymer can be designed, for example; it is used in automotive weather-stripping and seals, self-amalgamating tape, glass run channels, radiators, garden and appliance hoses, tubing, belts, roofing membranes, expansion joints, rubber mechanical goods, plastic impact modification, thermoplastic vulcanisates and motor oil additive applications. EPM is even more prevalent as an insulator for high-voltage cables since it has improved insulative characteristics over more traditional cables, such as cross-linked polyethylene, enabling a smaller cross sectional area for the same load carrying capacity. The cable is flexible and suited to applications where regular cable movement is required such as in the mining industry.\n\nMajor producers and suppliers of EPM include Crompton Corporation, Exxon-Mobil, Dupont, Herdillia, Kumho Polychem, LANXESS, Mitsui Chemicals, ENI Versalis and Sumitomo Chemical.\n\nEPM manufacture uses the same monomers as polyethene and polypropene, the ethylene and propylene monomers are randomly combined to yield a rubbery, stable polymer. By varying the monomer ratios and method by which the monomers are combined different forms of EPM can be formed (with a wide range of Mooney viscosities); ranging from amorphous to semi-crystalline. A third, non-conjugated diene monomer can be terpolymerized in a controlled manner to maintain a saturated backbone ready for vulcanization or polymer modification.\n\nFrancis P. Baldwin received the 1979 Charles Goodyear Medal for the many patents he held for these developments.\n"}
{"id": "20273696", "url": "https://en.wikipedia.org/wiki?curid=20273696", "title": "Frank Beebe", "text": "Frank Beebe\n\nFrank Lyman Beebe (1914 – 15 November 2008) was a prominent falconer, writer and wildlife illustrator from Canada. \n\n"}
{"id": "3102356", "url": "https://en.wikipedia.org/wiki?curid=3102356", "title": "Global politics", "text": "Global politics\n\nGlobal politics, also known as world politics, names both the discipline that studies the political and economical patterns of the world and the field that is being studied. At the centre of that field are the different processes of political globalization in relation to questions of social power.\n\nThe discipline studies the relationships between cities, nation-states, shell-states, multinational corporations, non-governmental organizations and international organizations. Current areas of discussion include national and ethnic conflict regulation, democracy and the politics of national self-determination, globalization and its relationship to democracy, conflict and peace studies, comparative politics, political economy, and the international political economy of the environment. One important area of global politics is contestation in the global political sphere over legitimacy.\n\nGlobal politics is said by some to be distinct from the field of international politics (commonly seen as a branch of international relations), as it \"does not stress the primacy of intergovernmental relations and transactions\". This distinction however has not always been held among authors and political scientists, who often use the term \"international politics\" to mean global politics.\n\nBeginning in the late nineteenth century, several groups extended the definition of the political community beyond nation-states to include much, if not all, of humanity. These \"internationalists\" include Marxists, human rights advocates, environmentalists, peace activists, feminists, and dalits. This was the general direction of thinking on global politics, though the term was not used as such.\n\nThe modern \"world politics perspective\" is often identified with the works of Joseph Nye and Robert Keohane, in particular their 1972 work \"Transnational Relations and World Politics\". Here, the authors argued that state-centric views of international relations were inadequate frameworks to utilize in political science or international relations studies due to the increased globalization. Today, the practices of global politics are defined by values: norms of human rights, ideas of human development, and beliefs such as Internationalism or cosmopolitanism about how we should relate to each. Over the last couple of decades cosmopolitanism has become one of the key contested ideologies of global politics:\n\nThe intensification of globalization led some writers to suggest that states were no longer relevant to global politics. This view has been subject to debate:\n\n\n"}
{"id": "58494448", "url": "https://en.wikipedia.org/wiki?curid=58494448", "title": "Great Basin water resource region", "text": "Great Basin water resource region\n\nThe Great Basin water resource region is one of 21 major geographic areas, or regions, in the first level of classification used by the United States Geological Survey to divide and sub-divide the United States into successively smaller hydrologic units. These geographic areas contain either the drainage area of a major river, or the combined drainage areas of a series of rivers.\n\nThe Great Basin region, which is listed with a 2-digit HUC code of 16, has an approximate size of , and consists of 6 subregions, which are listed with the 4-digit HUC codes of 1601 through 1606.\n\nThis region includes the drainage of the Great Basin that discharges into the states of Utah and Nevada. Includes parts of California, Idaho, Nevada, Oregon, Utah, and Wyoming.\n\n"}
{"id": "2839595", "url": "https://en.wikipedia.org/wiki?curid=2839595", "title": "Guyu", "text": "Guyu\n\nThe traditional East Asian calendars divide a year into 24 solar terms. Gǔyǔ, \"Kokuu\", \"Gogu\", or \"Cốc vũ\" is the 6th solar term. It begins when the Sun reaches the celestial longitude of 30° and ends when it reaches the longitude of 45°. It more often refers in particular to the day when the Sun is exactly at the celestial longitude of 30°. In the Gregorian calendar, it usually begins around April 20 and ends around May 5.\n\nEach solar term can be divided into 3 pentads (候). They are: first pentad (初候), second pentad (次候) and last pentad (末候). Pentads in Guyu include:\n\n\n"}
{"id": "24769094", "url": "https://en.wikipedia.org/wiki?curid=24769094", "title": "Hicham Aâboubou", "text": "Hicham Aâboubou\n\nHicham Aâboubou (born May 19, 1978) is a Moroccan footballer who last played for Montreal Impact in the North American Soccer League.\n\nAâboubou started his soccer career with Khenifra playing with the junior team from 1992 to 1994. In 1994, he joined Kawkab Marrakech, initially playing with the junior team, before eventually graduating through the senior A team, where he would play until 2001. He played for Mouloudia Oujda, in 2001-2002, before being transferred back to Kawkab Marrakech. From 2002 to 2006 he played with KACM A Team, scoring two goals in 48 games.\n\nIn 2006 Aâboubou joined Canadian team Laval Dynamites, and scored two goals in 13 games in his debut season with the club. On July 31 he joined Montreal Impact, but did not play a single game in his first year. He made his debut for the team on April 21, 2007 against the Atlanta Silverbacks, but was released by Montreal in February 2008. During the 2007 season he was loaned to the Impact's farm team Trois-Rivières Attak of the Canadian Soccer League. He managed to win some silverware with the Attak by winning the Open Canada Cup, where he featured in the finals match against Columbus Clan F.C. which resulted in a 3-0 victory.\n\nIn the summer of 2008 Aâboubou announced he would take a sabbatical from professional soccer, intending to return to study at Arts of Science at Université de Montréal while playing college soccer for the university's Montréal Carabins.\n\nOn July 28, 2009 the Montreal Impact signed Aâboubou to a two-year contract. He made his first appearance of the season on August 1 against Miami FC. During the 2009 season Aâboubou helped the Impact clinch a playoff spot by finishing fifth in the standings. In the playoffs, he helped the Impact reach the USL finals against the Vancouver Whitecaps FC, the match was noted for the first time in USL history where the final match would consist of two Canadian clubs. The Impact would eventually defeat the Whitecaps on 6-3 aggregate on goals, and therefore claim their third USL Championship.\n\n\n"}
{"id": "1420450", "url": "https://en.wikipedia.org/wiki?curid=1420450", "title": "Indian Ocean Tsunami Warning System", "text": "Indian Ocean Tsunami Warning System\n\nThe Indian Ocean Tsunami Warning System is a tsunami warning system set up to provide warning to inhabitants of nations bordering the Indian Ocean of approaching tsunamis.\n\nA warning system for the Indian Ocean was prompted by the 2004 Indian Ocean earthquake and resulting tsunami, which left approximately 230,000 people dead or missing. Many analysts claimed that the disaster would have been mitigated if there had been an effective warning system in place, citing the well-established Hawaii-based Pacific Tsunami Warning Center, which operates in the Pacific Ocean.\n\nPeople in some areas would have had more than adequate time to seek safety if they were aware of the impending catastrophe. The only way to effectively mitigate the impact of a tsunami is through an early warning system. Other methods such as sea walls only work for a percentage of waves, but a warning system is effective for all waves originating outside a minimum distance from the coastline.\n\nThe Indian Ocean Tsunami Warning System was agreed to in a United Nations conference held in January 2005 in Kobe, Japan as an initial step towards an International Early Warning Programme. Nanometrics (Ottawa, Canada) and RESULTS Marine Private Limited, India, delivered and successfully installed 17 Seismic VSAT stations with 2 Central Recording Station to provide the seismic event alert to the scientists through SMS and E-mail automatically within 2 min.\n\nThe system became active in late June 2006 following the leadership of UNESCO. It consists of 25 seismographic stations relaying information to 26 national tsunami information centers, as well as 6 Deep-ocean Assessment and Reporting of Tsunami (DART) buoys. However, UNESCO warned that further coordination between governments and methods of relaying information from the centers to the civilians at risk are required to make the system effective.\n\nSensor data is processed by the U.S. Pacific Tsunami Warning Center in Hawaii and the Japan Meteorological Agency, and alerts are forwarded to threatened countries and also made available to the general public. National governments warn citizens through a variety of means, including SMS messages, radio and television broadcasts, sirens from dedicated platforms and mosque loudspeakers, and police vehicles with loudspeakers.\n\nThe system was not yet operational during the 2006 Pangandaran earthquake and tsunami. The Indonesian government did receive tsunami warnings from the warning centers but did not have a system to relay the alert to its citizens. At least 23,000 people did evacuate the coast after the quake, either fearing a tsunami or because their homes had been destroyed. Waves as high as still resulted in about 700 fatalities and 9,000 injuries.\n\nIn the 2012 Banda Aceh earthquake of 8.4 magnitude, the system alerted the Indian islands on Andaman and Nicobar within 8 minutes. Some tsunami warning sirens in Aceh were delayed by about 20 minutes due to failure of the electrical grid caused by the proximity of the earthquake, and evacuation routes in Banda Aceh were jammed with traffic.\n\nOf the 28 countries that ring the Indian Ocean, now Australia, Indonesia and India are responsible for spearheading tsunami warnings in the area. \n\n"}
{"id": "48456197", "url": "https://en.wikipedia.org/wiki?curid=48456197", "title": "Jan Thornhill", "text": "Jan Thornhill\n\nJan Thornhill (born 1955 in Sudbury, Ontario) is a Canadian writer and illustrator of educational books on science and nature for children. She was the 2015 winner of the Vicky Metcalf Award for Literature for Young People, a lifetime achievement award presented by the Writers' Trust of Canada, and won the Norma Fleck Award in 2007 for her book \"I Found a Dead Bird: The Kids’ Guide to the Cycle of Life & Death\".\n\nA graduate of the Ontario College of Art, Thornhill has illustrated many but not all of her own works. She won UNICEF's Ezra Jack Yeats International Award for illustration in 1990 for \"The Wildlife 123\", and has been a three-time nominee for the Governor General's Award for English-language children's illustration at the 1988 Governor General's Awards for \"The Wildlife ABC\", the 1989 Governor General's Awards for \"The Wildlife 123\" and the 2017 Governor General's Awards for \"The Tragic Tale of the Great Auk\".\n\nShe has also published the adult short story collection \"Drought\", which was a shortlisted nominee for the ReLit Awards in 2001, and has drawn illustrations for general interest magazines including \"The Idler\".\n\n"}
{"id": "37068014", "url": "https://en.wikipedia.org/wiki?curid=37068014", "title": "Laratinga Wetlands", "text": "Laratinga Wetlands\n\nThe Laratinga Wetlands, constructed in 1999, is a wetland located in Mount Barker, South Australia. It is named after the Aboriginal Peramangk peoples' name for the Mount Barker Creek, \"Laratinga\". \n\nThe wetlands occupy an area of , of which is underwater. \n"}
{"id": "24695131", "url": "https://en.wikipedia.org/wiki?curid=24695131", "title": "Levoglucosan", "text": "Levoglucosan\n\nLevoglucosan (CHO) is an organic compound with a six-carbon ring structure formed from the pyrolysis of carbohydrates, such as starch and cellulose. As a result, levoglucosan is often used as a chemical tracer for biomass burning in atmospheric chemistry studies, particularly with respect to airborne particulate matter. \nAlong with other tracers such as potassium, oxalate, and gaseous acetonitrile, levoglucosan has been shown to be highly correlated with regional fires. This is because the gas emitted by the pyrolysis of wood (biomass) contains significant amounts of levoglucosan.\n\nLevoglucosan has been described as \"an unequivocal biomass burning tracer\" in the context of forest and brush fires. But the anhydrosugar has only been found detectable in low temperature samples (150-350 °C), meaning that its value as an indicator for smoke from controlled biomass combustion in, for instance, modern domestic wood stoves which operate at temperatures above 500 °C, is \"very doubtful\". Levoglucosan is a marker for coal combustion as well as wood.\n\nThe hydrolysis of levoglucosan generates the fermentable sugar glucose. Levoglucosan can be utilized in the synthesis of chiral polymers such as unhydrolysable glucose polymers.\n"}
{"id": "38136839", "url": "https://en.wikipedia.org/wiki?curid=38136839", "title": "List of Euphorbia species (A–F)", "text": "List of Euphorbia species (A–F)\n\n\"Euphorbia\" is a highly diverse plant genus, comprising some 5,000 currently accepted taxa.\n\nThis is an alphabetical list of the \"Euphorbia\" species and notable intraspecific taxa.\n\nThe list includes the former (and never generally accepted) genus \"Chamaesyce\", as well as the related genera \"Elaeophorbia\", \"Endadenium\", \"Monadenium\", \"Synadenium\" and \"Pedilanthus\" which according to recent DNA sequence-based phylogenetic studies are all nested within \"Euphorbia\"\n\nNoticeably succulent plants are marked by (s).\n"}
{"id": "29466677", "url": "https://en.wikipedia.org/wiki?curid=29466677", "title": "List of Narcissus species", "text": "List of Narcissus species\n\nThis list of \"Narcissus\" species shows the accepted species names within the genus \"Narcissus\" (), which are predominantly spring perennial plants in the Amaryllidaceae (amaryllis) family. Various common names including daffodil, narcissus, and jonquil are used to describe all or some members of the genus. The list of species is arranged by subgenus and section. Estimates of the number of species in \"Narcissus\" have varied widely, from anywhere between 16 and nearly 160, even in the modern era. Linnaeus originally included six species in 1753.\n\nMuch of the variation lies in the definition of species, and whether closely related taxa are considered separate species or subspecies. Thus, a very wide view of each species, such as Webb's results in few species, while a very narrow view such as that of Fernandes results in a larger number. Another factor is the status of hybrids, given natural hybridisation. There is a distinction between what are referred to as 'ancient hybrids' which are found occurring over a relatively large area, and 'recent hybrids' with a more restricted range and found growing as solitary plants amongst their parents. The former are more often considered as separate species.\n\nFernandes (1951) accepted 22 species, on which were based the 27 species listed by Webb in the 1980 Flora Europaea. By 1968, Fernandes had accepted 63 species, and by 1990 Blanchard listed 65 species, and Erhardt 66 in 1993. In 2006 the International Daffodil Register listed 87 species. In contrast, the genetic study by Zonneveld (2008) resulted in only 36 species (for list and comparison with Webb, see Zonneveld Table 4).\n\n, the World Checklist of Selected Plant Families accepts 52 species, along with at least 60 hybrids. Another important source is the Royal Horticultural Society's Botanical Classification and list of botanical names (October 2014) which is the basis of their \"International Daffodil Register\". This is a searchable list and had 81 accepted names in its October 2014 release.\n\nOver 300 synonymous species names are listed, reflecting wide variations in how the genus is divided into species. These have been arranged into Sections. These should not be confused with the horticultural classification of cultivars into divisions by the Royal Horticultural Society \n\nSections (with type species) shown are according to Zonneveld (2008). In addition Mathew (2002) further divides the sections into subsections.\n\n\nFor a list of species by Section according to the Royal Horticultural Society, see the RHS Botanical Classification (updated September 2013), which is the basis of their \"International Daffodil Register\".\n\n\n\n\n\n"}
{"id": "43204753", "url": "https://en.wikipedia.org/wiki?curid=43204753", "title": "List of Ramsar sites in Ghana", "text": "List of Ramsar sites in Ghana\n\nGhana has wetlands and some Ramsar sites. A Ramsar site is a wetland site designated of international importance for migratory animal life, especially birds, under the Ramsar Convention. These are mainly along the coastal regions with just one in the interior region.\n\n\n\n\n\n"}
{"id": "5893494", "url": "https://en.wikipedia.org/wiki?curid=5893494", "title": "List of Sites of Special Scientific Interest in the West Midlands", "text": "List of Sites of Special Scientific Interest in the West Midlands\n\nThere are twenty-three Sites of Special Scientific Interest (SSSIs) in the county of the West Midlands, England. , of the twenty-three designated sites, eleven have been designated due to their biological interest, nine due to their geological interest and three for both. In England, the body responsible for designating SSSIs is Natural England, which chooses sites because of their flora, fauna, geological or physiographical features. Natural England took over the role of designating and managing SSSIs from English Nature in October 2006 when it was formed from the amalgamation of English Nature, parts of the Countryside Agency and the Rural Development Service. Natural England, like its predecessor, uses the 1974-1996 county system with each area being called an Area of Search. In the West Midlands case, the Area of Search matches the county boundary.\n\nThe West Midlands is the second largest conurbation, after Greater London, with a population of over 2.6 million inhabitants. Consisting of three large cities: Wolverhampton, Birmingham and Coventry, the county of the West Midlands is also one of the most densely populated areas in the United Kingdom. The result of this large human population is that every part of the area has been influenced by humans—often negatively—for example, the clearance of woodland to make room for agriculture. The West Midlands is an area of relatively high ground, ranging from around above sea level, forming the \"Birmingham Plateau\". It is crossed by Britain's main north-south watershed between the basins of the Rivers Severn and Trent. The main habitat types in the area are heathland, woodland and grassland, all of which are found in both urban and rural contexts.\n\nBetween the West Midlands conurbation and Coventry is a stretch of green belt land roughly across known as the \"Meriden Gap\", which retains a strongly rural character, and is the site of a number of SSSIs including Berkswell Marsh. A smaller green belt is located between Birmingham, Walsall and West Bromwich which includes Sutton Park in Sutton Coldfield. Sutton Park, an SSSI and national nature reserve, has an area of making it one of the largest urban parks in Europe and the largest European park outside a capital city.\n\n"}
{"id": "25495977", "url": "https://en.wikipedia.org/wiki?curid=25495977", "title": "List of Superfund sites in Iowa", "text": "List of Superfund sites in Iowa\n\nThis is a list of Superfund sites in Iowa designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). The NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. \n\n, there were eleven Superfund sites on the National Priorities List in Iowa. One more site has been proposed for entry on the list and ten others have been cleaned up and removed from it.\n\n\n"}
{"id": "25334029", "url": "https://en.wikipedia.org/wiki?curid=25334029", "title": "List of Superfund sites in Maryland", "text": "List of Superfund sites in Maryland\n\nThis is a list of Superfund sites in Maryland designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of March 10, 2011, there were 19 Superfund sites on the National Priorities List in Maryland. Two additional sites are currently proposed for entry on the list. Four sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "14447908", "url": "https://en.wikipedia.org/wiki?curid=14447908", "title": "List of X-ray pulsars", "text": "List of X-ray pulsars\n\nThis is a partial list of known accretion-powered pulsars, as of 1997.\n\n"}
{"id": "28127220", "url": "https://en.wikipedia.org/wiki?curid=28127220", "title": "List of earthquakes in Azerbaijan", "text": "List of earthquakes in Azerbaijan\n\nThis list of earthquakes in Azerbaijan, is a list of notable earthquakes that have affected areas within the current boundaries of Azerbaijan.\n"}
{"id": "7065702", "url": "https://en.wikipedia.org/wiki?curid=7065702", "title": "List of hyperaccumulators", "text": "List of hyperaccumulators\n\nThis article covers known hyperaccumulators, accumulators or species tolerant to the following: Aluminium (Al), Silver (Ag), Arsenic (As), Beryllium (Be), Chromium (Cr), Copper (Cu), Manganese (Mn), Mercury (Hg), Molybdenum (Mo), Naphthalene, Lead (Pb), Selenium (Se) and Zinc (Zn).\n\nSee also:\n\nCs-137 activity was much smaller in leaves of larch and sycamore maple than of spruce: spruce > larch > sycamore maple.\n"}
{"id": "20533535", "url": "https://en.wikipedia.org/wiki?curid=20533535", "title": "List of rivers of Saint Vincent and the Grenadines", "text": "List of rivers of Saint Vincent and the Grenadines\n\nThis is a list of rivers of Saint Vincent and the Grenadines. Rivers are listed in clockwise order, starting at the north end of the island.\n\n\n"}
{"id": "36608592", "url": "https://en.wikipedia.org/wiki?curid=36608592", "title": "List of superlative trees", "text": "List of superlative trees\n\nThe world's superlative trees can be ranked by any factor. Records have been kept for trees with superlative height, trunk diameter or girth, canopy coverage, airspace volume, wood volume, estimated mass, and age.\n\nThe heights of the tallest trees in the world have been the subject of considerable dispute and much exaggeration. Modern verified measurements with laser rangefinders or with tape drop measurements made by tree climbers (such as those carried out by canopy researchers), have shown that some older tree height measurement methods are often unreliable, sometimes producing exaggerations of 5% to 15% or more above the real height. Historical claims of trees growing to , and even , are now largely disregarded as unreliable, and attributed to human error.\n\nThe following are the tallest reliably measured specimens from the top species. This table shows only currently standing specimens:\nThe largest trees are defined as having the highest wood volume in a single-stem. These trees are both tall and large in diameter and, in particular, hold a large diameter high up the trunk. Measurement is very complex, particularly if branch volume is to be included as well as the trunk volume, so measurements have only been made for a small number of trees, and generally only for the trunk. Few attempts have ever been made to include root or leaf volume.\n\nThe girth of a tree is usually much easier to measure than the height, as it is a simple matter of stretching a tape round the trunk, and pulling it taut to find the circumference. Despite this, UK tree author Alan Mitchell made the following comment about measurements of yew trees:\n\nAs a general standard, tree girth is taken at \"breast height\". This is converted to and cited as dbh (diameter at breast height) in tree and forestry literature. Breast height is defined differently in different situations, with most forestry measurements taking girth at 1.3 m above ground, while those who measure ornamental trees usually measure at 1.5 m above ground; in most cases this makes little difference to the measured girth. On sloping ground, the \"above ground\" reference point is usually taken as the highest point on the ground touching the trunk, but in North America a point, that is the average of the highest point and the lowest point the tree trunk appears to contact the soil, is usually used. Some of the inflated old measurements may have been taken at ground level. Some past exaggerated measurements also result from measuring the complete next-to-bark measurement, pushing the tape in and out over every crevice and buttress. The measurements could also be influenced by deviation of the tape measure from a horizontal plane (which might seem called for if the trunk does not grow straight up), and the presence of features such as branches, spikes, \"etc\".\n\nModern trends are to cite the tree's diameter rather than the circumference. The diameter of the tree is calculated by finding the mean diameter of the trunk, in most cases obtained by dividing the measured circumference by π; this assumes the trunk is mostly circular in cross-section (an oval or irregular cross-section would result in a mean diameter slightly greater than the assumed circle). Accurately measuring circumference or diameter is difficult in species with the large buttresses that are characteristic of many species of rainforest trees. Simple measurement of circumference of such trees can be misleading when the circumference includes much empty space between buttresses. See also Tree girth measurement\n\nBaobabs (genus \"Adansonia\") store large amounts of water in the very soft wood in their trunks. This leads to marked variation in their girth over the year (though not more than about 2.5%), reaching maximum at the end of the rainy season, and minimum at the end of the dry season.\nMeasurements become ambiguous when multiple trunks (whether from an individual tree or multiple trees) grow together.\nThe Sacred Fig grows adventitious roots from its branches, which become new trunks when the root reaches the ground and thickens; a single sacred fig tree can have hundreds of such trunks. The multi-stemmed Hundred Horse Chestnut was known to have a circumference of when it was measured in 1780.\n\nThere are known more than 50 species of trees exceeding the diameter of 4.45 m or circumference of 14 m.\n\nThe trees with the broadest crowns have the widest spread of limbs from a single trunk.\n\nThe oldest trees are determined by growth rings, which can be seen if the tree is cut down, or in cores taken from the bark to the center of the tree. Accurate determination is only possible for trees that produce growth rings, generally those in seasonal climates. Trees in uniform non-seasonal tropical climates grow continuously and do not have distinct growth rings. It is also only possible for trees that are solid to the center. Many very old trees become hollow as the dead heartwood decays. For some of these species, age estimates have been made on the basis of extrapolating current growth rates, but the results are usually largely speculation. White (1998) proposes a method of estimating the age of large and veteran trees in the United Kingdom through the correlation of a tree's age with its diameter and growth character.\n\nThe verified oldest measured ages are:\nOther species suspected of reaching exceptional age include European Yew (\"Taxus baccata\") (probably over 2,000 years), Sugi (\"Cryptomeria japonica\") (3,000 years or more), and Western Redcedar (\"Thuja plicata\"). The oldest known European Yew may be the Llangernyw Yew in the Churchyard of Llangernyw village in North Wales, or the Fortingall Yew in Perthshire, Scotland. These yews may be from 1,500 to 3,000 years old.\n\nThe olive tree also can live for centuries. The oldest verified age is 900 years at Gethsemane (Mount of Olives, as mentioned in the Bible), while several other olive trees are suspected of being 2,000 to 3,000 years old.\n\nThe pond cypress, \"Taxodium ascendens\", has been known to live more than 1,000 years. One specimen in particular, named \"The Senator\", was estimated to be more than 3,400 years old at the time of its demise in early 2012.\n\nA wild fig tree growing in Echo Caves near Ohrigstad, South Africa has roots going deep, giving it the deepest roots known of any tree. El Drago Milenario, a tree of species \"Dracaena draco\" on Tenerife, Canary Islands, is reported to have aerial roots.\n\nThis list is limited to horizontal or nearly horizontal limbs, in which the governing growth factor is phototropism. Vertical or near vertical limbs, in which the governing growth factor is negative geotropism, are called \"reiterations\" and are really divisions of the trunk, which by definition must be less than the trunk as a whole and therefore less remarkable. The thickest trunks have already been dealt with under \"stoutest\".\n\n"}
{"id": "35616657", "url": "https://en.wikipedia.org/wiki?curid=35616657", "title": "List of wadis of Djibouti", "text": "List of wadis of Djibouti\n\nThis is a list of wadis in Djibouti. Wadis are either permanently or intermittently dry riverbeds, of which Djibouti has several. However, it does not have any permanent rivers.\n\nThis list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n\n\n"}
{"id": "9159177", "url": "https://en.wikipedia.org/wiki?curid=9159177", "title": "Mascall Formation", "text": "Mascall Formation\n\nThe Mascall Formation is a Miocene geologic formation found along the John Day River Valley of Oregon, in the Western United States.\n\nThe formation is described in \"Geologic Formations of Eastern Oregon\" (1972) as follows: \n\nThe ignimbrite was radiometrically dated at 13 million years. Parts of the Mascall are interfingered with the Columbia River Basalt Group.\n\nBarstovian vertebrates have been recovered from the Mascall.\n"}
{"id": "41710718", "url": "https://en.wikipedia.org/wiki?curid=41710718", "title": "Mega Borg Oil Spill", "text": "Mega Borg Oil Spill\n\nThe \"Mega Borg\" Oil Spill occurred in the Gulf of Mexico on June 8, 1990, roughly 50 miles off the coast of Texas, when the oil tanker \"Mega Borg\" caught on fire and exploded. The cleanup was one of the first practical uses of bioremediation.\n\nAt 11:30 PM on the evening of Saturday June 8, 1990, an explosion in the cargo room of the Norwegian oil tanker the \"Mega Borg\" “ruptured the bulkhead between the pump room and the engine room”, causing the ship to catch fire and begin to leak oil. The 853-foot-long, 15-year-old vessel was about 50 miles off the coast of Galveston, Texas when the explosion occurred. The weather at the time was calm and the tanker had easily passed Coast Guard safety inspections in April earlier that year. While the direct cause of the engine room explosion remains unknown, the initial blast occurred during a lightening process in which the \"Mega Borg\" was transferring oil onto a smaller Italian tanker, the \"Fraqmura\", in order to then transport the oil to Houston. This transfer was necessary, as the \"Mega Borg\" was too large to dock at the Texas port. Three million gallons of the total 38 million gallons of light Angolan Palanca crude oil on board the tanker were able to be transferred to the Fraqmura before the blast.\nTwo days after the initial blast, there were five successive explosions in a ten-minute window. These explosions greatly increased the rate of the spill from the tanker into the water. By the end of that day (June 11) the tanker stern had dropped 58 feet and had stabilized five feet above the water line. This was either due to shifting cargo or the tanker taking on water, which would be an indication of the vessel’s imminent sinking.\n\nThe light crude oil spilled in the \"Mega Borg\" incident was brown and evaporated much quicker than the heavy crude oil in spills such as the Exxon Valdez. This means that the oil is less likely to heavily coat nearby beaches, flora and fauna, however the tanker was carrying more oil than the Exxon Valdez incident spilled in total, so there was a lot of concern about the oil not being able to evaporate if the slick became too thick.\n\nThe fire caused by the initial explosion took eight days to burn out, making it hard for firefighters to board the tanker and stop the leakage of oil. However, the fire was helpful in the fact that it functioned as a natural in situ burning – out of the over 4.6 million spilled gallons, only 12,000 to 40,000 were left after the fire had burned out both on the water and on the tanker.\n\nTwo days after the spill, on Monday June 11, eight ships with skimmers, booms, and fire equipment encircled the \"Mega Borg\", with 12 more ships on the way. The Elf Acquitane Petroleum Company of Houston (owners of part of the \"Mega Borg\"’s cargo) used planes to spread dispersants within five miles of the site of the tanker’s explosion. By June 13, the previously 30 mile by 10 mile oil sheen had diminished to 13 miles long by 5 miles wide.\n\nAfter an eight-day-long fire, from June 8 to June 16, the Coast Guard was able to contain and put out the flames. This then allowed for workers to stabilize the engine room and begin pumping out and unloading any remaining oil. Once the fire was stopped, “high seas barrier booms were brought in to contain the rest of the leaked oil \nDuring the eight days the fire lasted, firefighters were continuously cooling the ship with both water and, once the fire had sufficiently died down, with foam cannons.\nOne, however experimental, part of the cleanup process was the release of about 100 pounds of bacteria over an acre of the slick. Known as bioremediation, this process has been used in labs, but never before on the open ocean. The microbes work by breaking down oil by “eating” it and turning the “hydrocarbons into more benign byproducts.” The only drawback to this method is that scientists are unsure about whether or not the bacteria cause more toxins to be released into the water. .\nA “slick-sucking vessel” was sent by the Coast Guard and was waiting on standby in case the \"Mega Borg\" sank and released its entire load of oil into the ocean.\n\nThe nearest fire control equipment of the type necessary for a fire of this size and type was in Louisiana, so it had to be shipped in and was not available as quickly as some people believe it should have been. Additional equipment was shipped in from the Netherlands, adding to the belief that it could have been found closer to the spill site and thus the oil could have been contained earlier. Local officials in Galveston argued that the decision to use foreign equipment delayed cleanup by two days, but Tony Redding, a spokesman for the Dutch company that lead the salvage (Smit Tak B.V.), said that all of the necessary materials were assembled “less than a day after the initial explosion” and that \"Absolutely no delay was encountered as a result of the airlifting of equipment from Europe.” \n\nSome local officials also asked why foam was not used until five days after the explosion. Redding answered this by explaining that “A foam attack without extensive cooling has no chance of killing a fire of this type.”\n\nThe explosion’s initial damage was inflicted on the 41 crew members aboard – two of them died, two disappeared and are presumed dead, and 17 were injured.\n\nThe Gulf of Mexico is one of the United States’ richest fishing grounds, and the \"Mega Borg\" spill added to an abundance of oil spills that have happened in the area. This effects not only marine microorganisms in deep waters, but larger deep-water fish and thus both recreational and commercial fishermen. Additionally, the National Wildlife Refuges near Galveston’s shores, nearby salt marches, and oyster reefs were all in potential danger depending on the extent of the slick’s spread.\n\nOn June 29, 1990, it was reported that tar balls from the \"Mega Borg\" spill were appearing as far away as Louisiana beaches.\n\nThe \"Mega Borg\" spill brought attention to the 984 protocols, legislation that has been held up in Senate since 1985. The Protocols passed the House, but have not been ratified due to arguments over international liabilities and whether or not the US should join international funding groups for oil spills. If ratified, the protocols would do a number of things, including creating multiple new federal response teams, creating a new fleet of special containment booms and skimmers.\n\n"}
{"id": "7209327", "url": "https://en.wikipedia.org/wiki?curid=7209327", "title": "NORMOB", "text": "NORMOB\n\nNORMOB (Nordic Broadcasters Mobile Alliance) is an alliance between the Norwegian Broadcasting Corporation (NRK), Danish Radio (DR), Swedish TV (SVT), Swedish Radio (SR), the Swedish Educational Broadcasting Company (UR) and Finnish Broadcasting Corporation (YLE). NORMOB was established in 2005 in order to promote cooperation between the broadcasters on mobile services. The body meets 2-3 times a year.\n"}
{"id": "840656", "url": "https://en.wikipedia.org/wiki?curid=840656", "title": "Pherusa", "text": "Pherusa\n\nPherusa or Pherousa is the name of two different figures in Greek mythology.\n\nPherusa was a Nereid, one of the fifty daughters of Nereus and Doris. Her name, a participle, means \"she who carries.\" She, along with her sister Dynamene, were associated with the power of great ocean swells. She is mentioned in Hesiod's \"Theogony\".\n\nPherusa is counted by some authors as one of the Horae, goddess of substance and farm estates.\n"}
{"id": "39071843", "url": "https://en.wikipedia.org/wiki?curid=39071843", "title": "Power-to-gas", "text": "Power-to-gas\n\nPower-to-gas (often abbreviated P2G) is a technology that converts electrical power to a gas fuel. When using surplus power from wind generation, the concept is sometimes called windgas. There are currently three methods in use; all use electricity to split water into hydrogen and oxygen by means of electrolysis.\n\nIn the first method, the resulting hydrogen is injected into the natural gas grid or is used in transport or industry. The second method is to combine the hydrogen with carbon dioxide and convert the two gases to methane (see natural gas) using a methanation reaction such as the Sabatier reaction, or biological methanation resulting in an extra energy conversion loss of 8%. The methane /SNG may then be fed into the natural gas grid or further converted in to LPG by synthesising SNG with partial reverse hydrogenation at high pressure and low temperature. LPG in turn can be converted into alkylate which is a premium gasoline blending stock because it has exceptional antiknock properties and gives clean burning. The third method uses the output gas of a wood gas generator or a biogas plant, after the biogas upgrader is mixed with the produced hydrogen from the electrolyzer, to upgrade the quality of the biogas.\n\nImpurities, such as carbon dioxide, water, hydrogen sulfide, and particulates, must be removed from the biogas if the gas is used for pipeline storage to prevent damage.\n\nPower-to-gas systems may be deployed as adjuncts to wind parks or solar-electric generation. The excess power or off-peak power generated by wind generators or solar arrays may then be used at a later time for load balancing in the energy grid. Before switching to natural gas, the German gas networks were operated using towngas, which for 50–60 % consisted of hydrogen. The storage capacity of the German natural gas network is more than 200,000 GWh which is enough for several months of energy requirement. By comparison, the capacity of all German pumped storage power plants amounts to only about 40 GWh. The storage requirement in Germany is estimated at 16GW in 2023, 80GW in 2033 and 130GW in 2050. The transport of energy through a gas network is done with much less loss (<0.1%) than in a power network (8%). The storage costs per kilowatt hour are estimated at €0.10 for hydrogen and €0.15 for methane. The use of the existing natural gas pipelines for hydrogen was studied by the EU NaturalHy project and US DOE. The blending technology is also used in HCNG.\n\nIn 2013 the round-trip efficiency of power-to-gas-storage was well below 50%, with the hydrogen path being able to reach a maximum efficiency of ~ 43% and methane of ~ 39% by using combined-cycle powerplants. If cogeneration plants are used that produce both electricity and heat, efficiency can be above 60%, but is still less than pumped hydro or battery storage. However, there is potential to increase efficiency of power-to-gas storage. In 2015 a study published in Energy and Environmental Science found that by using reversible solid oxide electrochemical cells and recycling waste heat in the storage process a round-trip efficiency electricity to electricity of more than 70% can be reached at low cost. A 2019 study (first published in 2018) also using pressurized reversible solid oxide fuel cells and a similar methodology found that even roundtrip efficiencies (power-to-power) up to 80 % might be feasible.\n\n\nIn this method, electricity is used to split water into hydrogen and oxygen by means of electrolysis. The resulting hydrogen is injected into the natural gas grid or is used in transport or industry.\n\nITM Power won a tender in March 2013 for a Thüga Group project, to supply a 360 kW self-pressurising high pressure electrolysis rapid response PEM electrolyser \"Rapid Response Electrolysis\" Power-to-Gas energy storage plant. The unit produces 125 kg/day of hydrogen gas and incorporates AEG power electronics. It will be situated at a Mainova AG site in the Schielestraße, Frankfurt in the state of Hessen. The operational data will be shared by the whole Thüga group – the largest network of energy companies in Germany with around 100 municipal utility members. The project partners include: badenova AG & Co. kg, Erdgas Mittelsachsen GmbH, Energieversorgung Mittelrhein GmbH, erdgas schwaben GmbH, Gasversorgung Westerwald GmbH, Mainova Aktiengesellschaft, Stadtwerke Ansbach GmbH, Stadtwerke Bad Hersfeld GmbH, Thüga Energienetze GmbH, WEMAG AG, e-rp GmbH, ESWE Versorgungs AG with Thüga Aktiengesellschaft as project coordinator. Scientific partners will participate in the operational phase. It can produce 60 cubic metres of hydrogen per hour and feed 3,000 cubic metres of natural gas enriched with hydrogen into the grid per hour. An expansion of the pilot plant is planned from 2016, facilitating the full conversion of the hydrogen produced into methane to be directly injected into the natural gas grid.\n\nIn December 2013, ITM Power, Mainova, and NRM Netzdienste Rhein-Main GmbH began injecting hydrogen into the German gas distribution network using ITM Power HGas, which is a rapid response proton exchange membrane electrolyser plant. The power consumption of the electrolyser is 315 kilowatts. It produces about 60 cubic meters per hour of hydrogen and thus in one hour can feed 3,000 cubic meters of hydrogen-enriched natural gas into the network.\n\nOn August 28, 2013, E.ON Hanse, Solvicore, and Swissgas inaugurated a commercial power-to-gas unit in Falkenhagen, Germany. The unit, which has a capacity of two megawatts, can produce 360 cubic meters of hydrogen per hour. The plant uses wind power and Hydrogenics electrolysis equipment to transform water into hydrogen, which is then injected into the existing regional natural gas transmission system. Swissgas, which represents over 100 local natural gas utilities, is a partner in the project with a 20 percent capital stake and an agreement to purchase a portion of the gas produced. A second 800 kW power-to-gas project has been started in Hamburg/Reitbrook district and is expected to open in 2015.\n\nIn August 2013, a 140 MW wind park in Grapzow, Mecklenburg-Vorpommern owned by E.ON received an electrolyser. The hydrogen produced can be used in an internal combustion engine or can be injected into the local gas grid. The hydrogen compression and storage system stores up to 27 MWh of energy and increases the overall efficiency of the wind park by tapping into wind energy that otherwise would be wasted. The electrolyser produces 210 Nm/h of hydrogen and is operated by RH2-WKA.\n\nIn January 2011, Secure Supplies Hydrogen, an internationally certified engineering and warrantied power to gas vendor of quality Australian and USA made Hydrogen Fueling equipment, Hydrogen Fueled Engines and Hydrogen Home /Business Solar fuel cell Kits entered the market. Project's deploying (2015-2020) are focused on renewable power investments. Value adding Sites to produce a Hydrogen fuel and transfer storage gas. Secure Supplies value adds all solar wind, geo thermal and hydropower projects. Enabling owners to achieve a higher ROI, to produce hydrogen gas grids to fuel industry and a variety of applications emission free. \nEnd users include Port Operators, Farms, Green Community residential development or and Renewable power operators. Gas is sold to gas grid or transported and used to fuel fixed engines or fuel cell that run 24r to pump water or make power for business and communities projects.\nHydrogen Equipment along with Engineering and Service provision makes Secure Supplies is well positioned to supply key markets globally.\n\nThe INGRID project started in 2013 in Apulia, Italy. It is a four-year project with 39 MWh storage and a 1.2 MW electrolyser for smart grid monitoring and control. The hydrogen is used for grid balancing, transport, industry, and injection into the gas network.\n\nThe surplus energy from the 12 MW Prenzlau Windpark in Brandenburg, Germany will be injected into the gas grid from 2014 on.\n\nThe 6 MW Energiepark Mainz from Stadtwerke Mainz, RheinMain University of Applied Sciences, Linde and Siemens in Mainz (Germany) will open in 2015.\n\nPower to gas and other energy storage schemes to store and utilize renewable energy are part of Germany's Energiewende (energy transition program).\n\nIn France, the MINERVE demonstrator of AFUL Chantrerie (Federation of Local Utilities Association) aims to promote the development of energy solutions for the future with elected representatives, companies and more generally civil society. It also aims to experiment other reactors, catalysts ...\nThe synthetic methane produced by the MINERVE demonstrator (0.6 Nm3 / h of CH4) is recovered as CNG fuel and as fuel in the boilers of the AFUL Chantrerie boiler plant. The installation was designed and built by the French SME Top Industrie, with the support of Leaf. It was received in November 2017 in the expected performances: 93.3% of CH4. This project was supported by the ADEME and the ERDF-Pays de la Loire Region, as well as by several other partners: Conseil départemental de Loire -Atlantic, Engie-Cofely, GRDF, GRTGaz, Nantes-Metropolis, Sydela and Sydev.\n\nThe core of the system is a proton exchange membrane (PEM) electrolyser. The electrolyser converts electrical energy into chemical energy, which in turn facilitates the storage of electricity. A gas mixing plant ensures that the proportion of hydrogen in the natural gas stream does not exceed two per cent by volume, the technically permissible maximum value when a natural gas filling station is situated in the local distribution network. The electrolyser supplies the hydrogen-methane mixture at the same pressure as the gas distribution network, namely 3.5 bar.\n\nThe power to methane method is to combine hydrogen from an electrolyzer with carbon dioxide and convert the two gases to methane (see natural gas) using a methanation reaction such as the Sabatier reaction or biological methanation resulting in an extra energy conversion loss of 8%, the methane may then be fed into the natural gas grid if the purity requirement is reached.\n\nZSW (Center for Solar Energy and Hydrogen Research) and SolarFuel GmbH (now ETOGAS GmbH) realized a demonstration project with 250 kW electrical input power in Stuttgart, Germany. The plant was put into operation on October 30, 2012.\n\nThe first industry-scale Power-to-Methane plant was realized by ETOGAS for Audi AG in Werlte, Germany. The plant with 6 MW electrical input power is using CO from a waste-biogas plant and intermittent renewable power to produce synthetic natural gas (SNG) which is directly fed into the local gas grid (which is operated by EWE). The plant is part of the Audi e-fuels program. The produced synthetic natural gas, named Audi e-gas, enables CO-neutral mobility with standard CNG vehicles. Currently it is available to customers of Audi's first CNG car, the Audi A3 g-tron.\nIn April 2014 the European Union’s co-financed and from the KIT coordinated HELMETH (Integrated High-Temperature ELectrolysis and METHanation for Effective Power to Gas Conversion) research project started. The objective of the project is the proof of concept of a highly efficient Power-to-Gas technology by thermally integrating high temperature electrolysis (SOEC technology) with CO-methanation. \nThrough the thermal integration of exothermal methanation and steam generation for the high temperature steam electrolysis conversion efficiency > 85% (higher heating value of produced methane per used electrical energy) are theoretically possible. The process consists of a pressurized high-temperature steam electrolysis and a pressurized CO-methanation module. \nThe project was completed in 2017 and achieved an efficiency of 76 % for the prototype with an indicated growth potential of 80 % for industrial scale plants. The operating conditions of the CO-methanation are a gas pressure of 10 - 30 bar, a SNG production of 1 - 5.4 m/h (NTP) and a reactant conversion that produces SNG with H < 2 vol.-% resp. CH > 97 vol.-%. Thus, the generated substitute natural gas can be injected in the entire German natural gas network without limitations. As a cooling medium for the exothermic reaction boiling water is used at up to 300 °C, which corresponds to a water vapour pressure of about 87 bar. The SOEC works with a pressure of up to 15 bar, steam conversions of up to 90% and generates one standard cubic meter of hydrogen from 3.37 kWh of electricity as feed for the methanation.\n\nEuropean demonstration project STORE&GO.\nThe technological maturity of Power to Gas is evaluated in the European 27 partner project STORE&GO, which has started in March 2016 with a runtime of four years. Three different technological concepts are demonstrated in three different European countries (Falkenhagen/Germany, Solothurn/Switzerland, Troia/Italy). The technologies involved include biological and chemical methanation, direct capture of CO from atmosphere, liquefaction of the synthesized methane to bio-LNG, and direct injection into the gas grid. The overall goal of the project is to assess those technologies and various usage paths under technical, economic,\nand legal aspects to identify business cases on the short and on the long term. The project is co-funded by the European Union’s Horizon 2020 research and innovation programme (18 million euro) and the Swiss government (6 million euro), with another 4 million euro coming from participating industrial partners. The coordinator of the overall project is the research center of the \nDVGW located at the KIT.\n\nThe biological methanation combines both processes, the electrolysis of water to form hydrogen and the subsequent CO reduction to methane using this hydrogen. During this process, methane forming microorganisms (methanogenic archaea or methanogens) release enzymes that reduce the overpotential of a non-catalytic electrode (the cathode) so that it can produce hydrogen. This microbial power-to-gas reaction occurs at ambient conditions, i.e. room temperature and pH 7, at efficiencies that routinely reach 80-100%. However, methane is formed more slowly than in the Sabatier reaction due to the lower temperatures. A direct conversion of CO to methane has also been postulated, circumventing the need for hydrogen production.\nMicroorganisms involved in the microbial power-to-gas reaction are typically members of the order \"Methanobacteriales\". Genera that were shown to catalyze this reaction are \"Methanobacterium\", \"Methanobrevibacter\", and \"Methanothermobacter\" (thermophile).\n\nThe synthetic methane generated from electricity can also be used for generating protein rich feed for cattle, poultry and fish economically by cultivating \"Methylococcus capsulatus\" bacteria culture with tiny land and water foot print. The carbon dioxide gas produced as by product from these plants can be recycled in the generation of synthetic methane (SNG). Similarly, oxygen gas produced as by product from the electrolysis of water and the methanation process can be consumed in the cultivation of bacteria culture. With these integrated plants, the abundant renewable solar / wind power potential can be converted in to high value food products with out any water pollution or green house gas (GHG) emissions.\n\nIn the third method the carbon dioxide in the output of a wood gas generator or a biogas plant after the biogas upgrader is mixed with the produced hydrogen from the electrolyzer to produce methane. The free heat coming from the electrolyzer is used to cut heating costs in the biogas plant. The impurities carbon dioxide, water, hydrogen sulfide, and particulates must be removed from the biogas if the gas is used for pipeline storage to prevent damage.\n\n2014-Avedøre wastewater Services in Avedøre, Kopenhagen (Denmark) is adding a 1 MW electrolyzer plant to upgrade the anaerobic digestion biogas from sewage sludge. The produced hydrogen is used with the carbon dioxide from the biogas in a Sabatier reaction to produce methane. Electrochaea is testing another project outside P2G BioCat with biocatalytic methanation. The company uses an adapted strain of the thermophilic methanogen Methanothermobacter thermautotrophicus and has demonstrated its technology at laboratory-scale in an industrial environment. A pre-commercial demonstration project with a 10,000-liter reactor vessel was executed between January and November 2013 in Foulum, Denmark.\n\nIn 2016 Torrgas, Siemens, Stedin, Gasunie, A.Hak, Hanzehogeschool/EnTranCe and Energy Valley intend to open a 12 MW Power to Gas facility in Delfzijl (The Netherlands) where biogas from Torrgas (biocoal) will be upgraded with hydrogen from electrolysis and delivered to nearby industrial consumers.\n\n\nOther initiatives to create syngas from carbon dioxide and water may use different water splitting methods.\n\nThe US Naval Research Laboratory (NRL) is designing a power-to-liquids system using the Fischer-Tropsch Process to create fuel on board a ship at sea, with the base products carbon dioxide (CO) and water (HO) being derived from sea water via \"An Electrochemical Module Configuration For The Continuous Acidification Of Alkaline Water Sources And Recovery Of CO With Continuous Hydrogen Gas Production\".\n\n\n\n"}
{"id": "30767204", "url": "https://en.wikipedia.org/wiki?curid=30767204", "title": "Priatelia Zeme Slovensko", "text": "Priatelia Zeme Slovensko\n\nPriatelia Zeme Slovensko (Slovak, translated to English as \"Friends of the Earth Slovakia\") became a Friends of the Earth International member in 1997. Since then, they have been actively participating in activities coordinated by Friends of the Earth International and its regional association, Friends of the Earth Europe.\n\nFriends of the Earth Slovakia consists of three organisations at present: Friends of the Earth-CEPA, Friends of the Earth - SPZ and WOLF Forest Protection Movement.\n\nEnsure environmental, social and economical justice as well as protection of nature, living environment and the quality of life.\n\n"}
{"id": "1227284", "url": "https://en.wikipedia.org/wiki?curid=1227284", "title": "Pulsar", "text": "Pulsar\n\nA pulsar (from \"pulse\" and \"-ar\" as in quasar) is a highly magnetized rotating neutron star that emits a beam of electromagnetic radiation. This radiation can be observed only when the beam of emission is pointing toward Earth (much like the way a lighthouse can be seen only when the light is pointed in the direction of an observer), and is responsible for the pulsed appearance of emission. Neutron stars are very dense, and have short, regular rotational periods. This produces a very precise interval between pulses that ranges from milliseconds to seconds for an individual pulsar. Pulsars are believed to be one of the candidates for the source of ultra-high-energy cosmic rays (see also centrifugal mechanism of acceleration).\n\nThe periods of pulsars make them very useful tools. Observations of a pulsar in a binary neutron star system were used to indirectly confirm the existence of gravitational radiation. The first extrasolar planets were discovered around a pulsar, PSR B1257+12. Certain types of pulsars rival atomic clocks in their accuracy in keeping time.\n\nThe first pulsar was observed on November 28, 1967, by Jocelyn Bell Burnell and Antony Hewish. They observed pulses separated by 1.33 seconds that originated from the same location in the sky, and kept to sidereal time. In looking for explanations for the pulses, the short period of the pulses eliminated most astrophysical sources of radiation, such as stars, and since the pulses followed sidereal time, it could not be man-made radio frequency interference. When observations with another telescope confirmed the emission, it eliminated any sort of instrumental effects. At this point, Bell Burnell said of herself and Hewish that \"we did not really believe that we had picked up signals from another civilization, but obviously the idea had crossed our minds and we had no proof that it was an entirely natural radio emission. It is an interesting problem—if one thinks one may have detected life elsewhere in the universe, how does one announce the results responsibly?\" Even so, they nicknamed the signal LGM-1, for \"little green men\" (a playful name for intelligent beings of extraterrestrial origin). It was not until a second pulsating source was discovered in a different part of the sky that the \"LGM hypothesis\" was entirely abandoned. Their pulsar was later dubbed CP 1919, and is now known by a number of designators including PSR 1919+21 and PSR J1921+2153. Although CP 1919 emits in radio wavelengths, pulsars have subsequently been found to emit in visible light, X-ray, and gamma ray wavelengths.\n\nThe word \"pulsar\" is a portmanteau of 'pulsating' and 'quasar', and first appeared in print in 1968:\n\nThe existence of neutron stars was first proposed by Walter Baade and Fritz Zwicky in 1934, when they argued that a small, dense star consisting primarily of neutrons would result from a supernova. Based on the idea of magnetic flux conservation from magnetic main sequence stars, Lodewijk Woltjer proposed in 1964 that such neutron stars might contain magnetic fields as large as 10^14 to 10^16 G. In 1967, shortly before the discovery of pulsars, Franco Pacini suggested that a rotating neutron star with a magnetic field would emit radiation, and even noted that such energy could be pumped into a supernova remnant around a neutron star, such as the Crab Nebula. After the discovery of the first pulsar, Thomas Gold independently suggested a rotating neutron star model similar to that of Pacini, and explicitly argued that this model could explain the pulsed radiation observed by Bell Burnell and Hewish. The discovery of the Crab pulsar later in 1968 seemed to provide confirmation of the rotating neutron star model of pulsars. The Crab pulsar has a 33-millisecond pulse period, which was too short to be consistent with other proposed models for pulsar emission. Moreover, the Crab pulsar is so named because it is located at the center of the Crab Nebula, consistent with the 1933 prediction of Baade and Zwicky.\n\nIn 1974, Antony Hewish and Martin Ryle became the first astronomers to be awarded the Nobel Prize in Physics, with the Royal Swedish Academy of Sciences noting that Hewish played a \"decisive role in the discovery of pulsars\". Considerable controversy is associated with the fact that Hewish was awarded the prize while Bell, who made the initial discovery while she was his Ph.D student, was not. Bell claims no bitterness upon this point, supporting the decision of the Nobel prize committee.\n\nIn 1974, Joseph Hooton Taylor, Jr. and Russell Hulse discovered for the first time a pulsar in a binary system, PSR B1913+16. This pulsar orbits another neutron star with an orbital period of just eight hours. Einstein's theory of general relativity predicts that this system should emit strong gravitational radiation, causing the orbit to continually contract as it loses orbital energy. Observations of the pulsar soon confirmed this prediction, providing the first ever evidence of the existence of gravitational waves. As of 2010, observations of this pulsar continue to agree with general relativity. In 1993, the Nobel Prize in Physics was awarded to Taylor and Hulse for the discovery of this pulsar.\n\nIn 1982, Don Backer led a group which discovered PSR B1937+21, a pulsar with a rotation period of just 1.6 milliseconds (38,500 rpm). Observations soon revealed that its magnetic field was much weaker than ordinary pulsars, while further discoveries cemented the idea that a new class of object, the \"millisecond pulsars\" (MSPs) had been found. MSPs are believed to be the end product of X-ray binaries. Owing to their extraordinarily rapid and stable rotation, MSPs can be used by astronomers as clocks rivaling the stability of the best atomic clocks on Earth. Factors affecting the arrival time of pulses at Earth by more than a few hundred nanoseconds can be easily detected and used to make precise measurements. Physical parameters accessible through pulsar timing include the 3D position of the pulsar, its proper motion, the electron content of the interstellar medium along the propagation path, the orbital parameters of any binary companion, the pulsar rotation period and its evolution with time. (These are computed from the raw timing data by Tempo, a computer program specialized for this task.) After these factors have been taken into account, deviations between the observed arrival times and predictions made using these parameters can be found and attributed to one of three possibilities: intrinsic variations in the spin period of the pulsar, errors in the realization of Terrestrial Time against which arrival times were measured, or the presence of background gravitational waves. Scientists are currently attempting to resolve these possibilities by comparing the deviations seen between several different pulsars, forming what is known as a pulsar timing array. The goal of these efforts is to develop a pulsar-based time standard precise enough to make the first ever direct detection of gravitational waves.\nIn June 2006, the astronomer John Middleditch and his team at LANL announced the first prediction of pulsar glitches with observational data from the Rossi X-ray Timing Explorer. They used observations of the pulsar PSR J0537-6910.\n\nIn 1992, Aleksander Wolszczan discovered the first extrasolar planets around PSR B1257+12. This discovery presented important evidence concerning the widespread existence of planets outside the Solar System, although it is very unlikely that any life form could survive in the environment of intense radiation near a pulsar.\n\nIn 2016, AR Scorpii was identified as the first pulsar in which the compact object is a white dwarf instead of a neutron star. Because its moment of inertia is much higher than that of a neutron star, the white dwarf in this system rotates once every 1.97 minutes, far slower than neutron-star pulsars. The system displays strong pulsations from ultraviolet to radio wavelengths, powered by the spin-down of the strongly magnetized white dwarf.\n\nInitially pulsars were named with letters of the discovering observatory followed by their right ascension (e.g. CP 1919). As more pulsars were discovered, the letter code became unwieldy, and so the convention then arose of using the letters PSR (Pulsating Source of Radio) followed by the pulsar's right ascension and degrees of declination (e.g. PSR 0531+21) and sometimes declination to a tenth of a degree (e.g. PSR 1913+16.7). Pulsars appearing very close together sometimes have letters appended (e.g. PSR 0021-72C and PSR 0021-72D).\n\nThe modern convention prefixes the older numbers with a B (e.g. PSR B1919+21), with the B meaning the coordinates are for the 1950.0 epoch. All new pulsars have a J indicating 2000.0 coordinates and also have declination including minutes (e.g. PSR J1921+2153). Pulsars that were discovered before 1993 tend to retain their B names rather than use their J names (e.g. PSR J1921+2153 is more commonly known as PSR B1919+21). Recently discovered pulsars only have a J name (e.g. PSR J0437-4715). All pulsars have a J name that provides more precise coordinates of its location in the sky.\n\nThe events leading to the formation of a pulsar begin when the core of a massive star is compressed during a supernova, which collapses into a neutron star. The neutron star retains most of its angular momentum, and since it has only a tiny fraction of its progenitor's radius (and therefore its moment of inertia is sharply reduced), it is formed with very high rotation speed. A beam of radiation is emitted along the magnetic axis of the pulsar, which spins along with the rotation of the neutron star. The magnetic axis of the pulsar determines the direction of the electromagnetic beam, with the magnetic axis not necessarily being the same as its rotational axis. This misalignment causes the beam to be seen once for every rotation of the neutron star, which leads to the \"pulsed\" nature of its appearance.\n\nIn rotation-powered pulsars, the beam originates from the rotational energy of the neutron star, which generates an electrical field from the movement of the very strong magnetic field, resulting in the acceleration of protons and electrons on the star surface and the creation of an electromagnetic beam emanating from the poles of the magnetic field. This rotation slows down over time as electromagnetic power is emitted. When a pulsar's spin period slows down sufficiently, the radio pulsar mechanism is believed to turn off (the so-called \"death line\"). This turn-off seems to take place after about 10–100 million years, which means of all the neutron stars born in the 13.6 billion year age of the universe, around 99% no longer pulsate.\n\nThough the general picture of pulsars as rapidly rotating neutron stars is widely accepted, Werner Becker of the Max Planck Institute for Extraterrestrial Physics said in 2006, \"The theory of how pulsars emit their radiation is still in its infancy, even after nearly forty years of work.\"\n\nThree distinct classes of pulsars are currently known to astronomers, according to the source of the power of the electromagnetic radiation:\n\n\nAlthough all three classes of objects are neutron stars, their observable behavior and the underlying physics are quite different. There are, however, connections. For example, X-ray pulsars are probably old rotationally-powered pulsars that have already lost most of their power, and have only become visible again after their binary companions had expanded and began transferring matter on to the neutron star. The process of accretion can in turn transfer enough angular momentum to the neutron star to \"recycle\" it as a rotation-powered millisecond pulsar. As this matter lands on the neutron star, it is thought to \"bury\" the magnetic field of the neutron star (although the details are unclear), leaving millisecond pulsars with magnetic fields 1000-10,000 times weaker than average pulsars. This low magnetic field is less effective at slowing the pulsar's rotation, so millisecond pulsars live for billions of years, making them the oldest known pulsars. Millisecond pulsars are seen in globular clusters, which stopped forming neutron stars billions of years ago.\n\nOf interest to the study of the state of the matter in a neutron\nstar are the \"glitches\" observed in the rotation velocity\nof the neutron star. This velocity is decreasing slowly but steadily, except by sudden variations. One model put forward to explain these glitches is that they are the result of \"starquakes\" that adjust the crust of the neutron star. Models where the glitch is due to a decoupling of the possibly superconducting interior of the star have also been advanced. In both cases, the star's moment of inertia changes, but its angular momentum does not, resulting in a change in rotation rate.\n\nWhen two massive stars are born close together from the same cloud of gas, they can form a binary system and orbit each other from birth. If those two stars are at least a few times as massive as our sun, their lives will both end in supernova explosions. The more massive star explodes first, leaving behind a neutron star. If the explosion does not kick the second star away, the binary system survives. The neutron star can now be visible as a radio pulsar, and it slowly loses energy and spins down. Later, the second star can swell up, allowing the neutron star to suck up its matter. The matter falling onto the neutron star spins it up and reduces its magnetic field. This is called \"recycling\" because it returns the neutron star to a quickly-spinning state. Finally, the second star also explodes in a supernova, producing another neutron star. If this second explosion also fails to disrupt the binary, a double neutron star binary is formed. Otherwise, the spun-up neutron star is left with no companion and becomes a \"disrupted recycled pulsar\", spinning between a few and 50 times per second.\n\nThe discovery of pulsars allowed astronomers to study an object never observed before, the neutron star. This kind of object is the only place where the behavior of matter at nuclear density can be observed (though not directly). Also, millisecond pulsars have allowed a test of general relativity in conditions of an intense gravitational field.\n\nPulsar maps have been included on the two Pioneer Plaques as well as the Voyager Golden Record. They show the position of the Sun, relative to 14 pulsars, which are identified by the unique timing of their electromagnetic pulses, so that our position both in space and in time can be calculated by potential extraterrestrial intelligences. Because pulsars are emitting very regular pulses of radio waves, its radio transmissions do not require daily corrections. Moreover, pulsar positioning could create a spacecraft navigation system independently, or be used in conjunction with satellite navigation.\n\nGenerally, the regularity of pulsar emission does not rival the stability of atomic clocks. However, for some millisecond pulsars, the regularity of pulsation is even more precise than an atomic clock.\nThis stability allows millisecond pulsars to be used in establishing ephemeris time\nor in building pulsar clocks.\n\n\"Timing noise\" is the name for rotational irregularities observed in all pulsars. This timing noise is observable as random wandering in the pulse frequency or phase. It is unknown whether timing noise is related to pulsar glitches.\n\nThe radiation from pulsars passes through the interstellar medium (ISM) before reaching Earth. Free electrons in the warm (8000 K), ionized component of the ISM and H II regions affect the radiation in two primary ways. The resulting changes to the pulsar's radiation provide an important probe of the ISM itself.\n\nBecause of the dispersive nature of the interstellar plasma, lower-frequency radio waves travel through the medium slower than higher-frequency radio waves. The resulting delay in the arrival of pulses at a range of frequencies is directly measurable as the \"dispersion measure\" of the pulsar. The dispersion measure is the total column density of free electrons between the observer and the pulsar,\n\nwhere formula_2 is the distance from the pulsar to the observer and formula_3 is the electron density of the ISM. The dispersion measure is used to construct models of the free electron distribution in the Milky Way.\n\nAdditionally, turbulence in the interstellar gas causes density inhomogeneities in the ISM which cause scattering of the radio waves from the pulsar. The resulting scintillation of the radio waves—the same effect as the twinkling of a star in visible light due to density variations in the Earth's atmosphere—can be used to reconstruct information about the small scale variations in the ISM. Due to the high velocity (up to several hundred km/s) of many pulsars, a single pulsar scans the ISM rapidly, which results in changing scintillation patterns over timescales of a few minutes.\n\nPulsars orbiting within the curved space-time around Sgr A*, the supermassive black hole at the center of the Milky Way, could serve as probes of gravity in the strong-field regime. Arrival times of the pulses would be affected by special- and general-relativistic Doppler shifts and by the complicated paths that the radio waves would travel through the strongly curved space-time around the black hole. In order for the effects of general relativity to be measurable with current instruments, pulsars with orbital periods less than about 10 years would need to be discovered; such pulsars would orbit at distances inside 0.01 pc from Sgr A*. Searches are currently underway; at present, five pulsars are known to lie within 100 pc from Sgr A*.\n\nThere are 3 consortia around the world which use pulsars to search for gravitational waves. In Europe, there is the European Pulsar Timing Array (EPTA); there is the Parkes Pulsar Timing Array (PPTA) in Australia; and there is the North American Nanohertz Observatory for Gravitational Waves (NANOGrav) in Canada and the US. Together, the consortia form the International Pulsar Timing Array (IPTA). The pulses from Millisecond Pulsars (MSPs) are used as a system of Galactic clocks. Disturbances in the clocks will be measurable at Earth. A disturbance from a passing gravitational wave will have a particular signature across the ensemble of pulsars, and will be thus detected.\n\nThe pulsars listed here were either the first discovered of its type, or represent an extreme of some type among the known pulsar population, such as having the shortest measured period.\n\n\n"}
{"id": "2703758", "url": "https://en.wikipedia.org/wiki?curid=2703758", "title": "Rosewood", "text": "Rosewood\n\nRosewood refers to any of a number of richly hued timbers, often brownish with darker veining, but found in many different hues.\n\nAll genuine rosewoods belong to the genus \"Dalbergia\". The pre-eminent rosewood appreciated in the Western world is the wood of \"Dalbergia nigra\". It is best known as \"Brazilian rosewood\", but also as \"Bahia rosewood\". This wood has a strong, sweet smell, which persists for many years, explaining the name rosewood.\n\nAnother classic rosewood comes from \"Dalbergia latifolia\" known as (East) Indian rosewood or \"sonokeling\" (Indonesia). It is native to India and is also grown in plantations elsewhere in Pakistan (Chiniot).\n\nMadagascar rosewood (\"Dalbergia maritima\"), known as \"bois de rose\", is highly prized for its red color. It is overexploited in the wild, despite a 2010 moratorium on trade and illegal logging, which continues on a large scale.\n\nThroughout southeast Asia, \"Dalbergia oliveri\" is harvested for use in woodworking. It has a very fragrant and dense grain near the core, but the outer sapwood is soft and porous. \"Dalbergia cultrata\", variegated burgundy to light brown in color, is a blackwood timber sold as Burmese rosewood. Products built with rosewood-based engineered woods are sold as Malaysian rosewood or as \"D. oliveri\".\n\nSome rosewood comes from \"Dalbergia retusa\", also known as the Nicaraguan rosewood or as\" cocobolo\". Several species are known as Guatemalan rosewood or Panama rosewood: \"D. tucerencis\", \"D. tucarensis\", and \"D. cubiquitzensis\". Honduran rosewood:\"D. stevensonii\" is used for marimba keys, guitar parts, clarinets and other musical and ornamental applications.\n\nNot all species in the large genus \"Dalbergia\" yield rosewoods; only about a dozen species do. The woods of some other species in the genus \"Dalbergia\" are notable—even famous—woods in their own right: African blackwood, \"cocobolo\", kingwood, and tulipwood.\n\nThe timber trade will sell many timbers under the name rosewood (usually with an adjective) due to some (outward) similarities. A fair number of these timbers come from other legume genera; one such species that is often mentioned is Bolivian \"Machaerium scleroxylon\" sold as Bolivian rosewood. Another that may be found in market from Southeast Asia is \"Pterocarpus indicus\", sold as New Guinea rosewood (and related species). \"Dalbergia sissoo\" is timber from rosewood species from India and Bangladesh, usually known as \"sheesham\" or North-Indian rosewood. It is extremely dense and has mild rot resistance, but it is porous and its exterior is soft and susceptible to wood-boring insects. It is used for making cabinets and flooring, and for carving. It is exported as quality veneers. Due to its after-work quality when sealed and dyed, it is often sold as genuine rosewood or as teak. It has no discernible qualities of a genuine rosewood. It has comparable strength with teak, but lower quality and price than teak or \"Dalbergia latifolia\".\n\nAlthough its wood bears no resemblance whatsoever to the true rosewoods, the Australian rose mahogany (\"Dysoxylum fraserianum\", family Meliaceae) and \nAustralian blackwood, (\"Acacia melanoxylon\") are also sold as rosewood. Australian rose mahogany due to the strong smell of roses from freshly cut bark is more mistakenly called as a \"rosewood\".\n\nAll rosewoods are strong and heavy, taking an excellent polish, being suitable for guitars (the fretboards on electric and acoustic guitars often being made of rosewood), marimbas, recorders, turnery (billiard cues, fountain pens, black pieces in chess sets, etc.), handles, furniture, and luxury flooring, etc. Rosewood oil, used in perfume, is extracted from the wood of \"Aniba rosaeodora\", which is not related to the rosewoods used for lumber.\n\nThe dust created from sanding rosewood is considered a sensitizing irritant and can trigger asthma and other respiratory ailments. Often, the more people are exposed to rosewood dust, the more sensitive they can become to exposure.\n\nIn general, world stocks are poor through overexploitation. Some species become canopy trees (up to 30 m high), and large pieces can occasionally be found in the trade. Rosewood is now protected worldwide. At a summit of the international wildlife trade in South Africa, the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) moved to protect the world’s most trafficked wild product by placing all 300 species of the rosewood tree under trade restrictions.\n\n\nFrom \"Dalbergia\" species\n\nOther than \"Dalbergia\" species\n"}
{"id": "50362416", "url": "https://en.wikipedia.org/wiki?curid=50362416", "title": "Sacred Jackfruit Tree", "text": "Sacred Jackfruit Tree\n\nThe Sacred Jackfruit Tree is a historical site in the Indian state of Manipur where a jackfruit tree (\"Artocarpus heterophyllus\") growing on the small hill of Kaina was used to carve images of Hindu god Krishna. Rajarshi Bhagya Chandra, earlier known as Shree Jai Singh Maharaja, the King of Manipur in the 18th century, had a dream in which he received instructions from Krishna to carve His images from this tree. Accordingly, seven images of Krishna were carved from the jackfruit tree and installed in various temples in Manipur and in the neighboring state of Assam. One such temple is the Shree Govindajee Temple at Imphal.\n\nThe Sacred Jackfruit Tree site is on the Kaina Hill, a small hillock in the southern part of a mountain range called \"Langmaijing\", in the Thoubal district to the east of the Manipur valley. In the past, Kaina Hill was known as \"Lakhai Phandong Ching\". It was a royal orchard of the palace. It is from Imphal on the Imphal-Yariripok road. Kaina Mountain rises to a height of about . The location of the \"Theibong\" Jackfruit tree (Jackfruit in Meitei language means \"Theibong\") is a religious and historical site for the Meitei Hindus who have named it as \"Bhashmukh Parbat\". The site measures and is called Kaina temple complex, which is being developed as one of the three major tourist complexes in Manipur along with the Kheba hillock and Marjing temple complex on the Heingang hill in eastern Imphal.\n\nAccording to mythological lore related to Bhagya Chandra, the King of Manipur who ruled in 1759, the Hindu god Krishna appeared to him in a dream and directed him to carve images of His from a particular jackfruit tree in Kaina.\n\nIn 1765 Chandra was defeated by King Alaungpaya of Konbaung Burma (now Myanmar), and as a result he escaped to Cachar in Assam and took asylum with King Swagadeva Rajeshwer Singh of Tekhau. However, Chandra's uncle, who had plotted with the Burmese king to oust him from Manipur, complained to Rajeshwer Singh that Chandra was an \"impostor\". The Assamese king became suspicious and ordered that Chandra should fight a rogue elephant to prove his bravery and innocence.\n\nAs King Chandra was a highly religious person, dedicated to the worship of Krishna, he appealed for help. Acceding to Chandra's prayers, Krishna appeared to him in a vision the night before the fight with the elephant. He told the king to face the elephant in front of a jackfruit tree in which he would be present, holding a rosary of Tulsi. Krishna commanded that after the fight he should carve images of His out of that particular jackfruit tree, and to deify and worship them in temples built for the purpose.\n\nThe next day when the encounter took place in front of the jackfruit tree in Kaina, the wild elephant, instead of attacking Bhagya Chandra, bowed before him with reverence. After this, the king of Assam decided to help Bhagya Chandra. With his help, Chandra led an army to Manipur and won back his kingdom.\n\nOnce back home, Chandra forgot about his promise to Krishna. One day Krishna appeared in the garb of a boy before a woman at Kaina, and asked her to give Him food. He told her about the commitment made to Him by King Chandra. The boy then disappeared. When this incident was conveyed to the King, he realized his lapse and immediately came to Kaina to find the jackfruit tree to make the images of Krishna. However, with the first strike of an axe the tree started bleeding, forcing him to stop cutting. He appeased the tree by offering worship, following which he uprooted the tree without trouble and had it carried away to make the images. In another version it is said that in 1778, after the tree was cut it was moved along the Iril River but after a short distance of travel the tree got bogged down in water; this place is known as \"Urup\". Even with the help of elephants it could not be pulled out of water. Then the king himself started helping people in pulling it when it surfaced out of water. Then an aarti was performed in reverence of Govindajee; this place is known as \"Arapti\".\n\nThe uprooted jackfruit tree from Kaina was transported by floating it along a river to Langthaband (Iril River is also mentioned). Here the tree, with its roots, was dried and the images were carved. The wood carving was done by Sapam Laxman under the guidance of Wanghei Pandit Angom Gopiram, the Pacha Hanchapa. Laxman took three years to carve the images. In all, seven images were carved from the tree. The first image of Krishna was installed at the Shri Govindajee Temple () in the palace at Imphal. The second image was consecrated at Bihaynath Govinda temple at Sagalband; the third image was installed at the Shree Gopinath Temple at Ningthoukmbam (); the fourth at Nityananda Temple at Khwai Lamabam Leikai in Imphal; and the fifth at Shree Madanmohan at Oinam Thingel, Imphal. The sixth image was fixed at Anuprabhu at Nabadwip at Nadia of West Bengal. The seventh image, carved from the roots of the tree, was installed at Lamangdong and came to be known as \"Advaita Prabhu\" or \"Lamangdong Advaita\".\n\nThe first image made by Laxman, which was installed at the Shree Govindajee Temple in Imphal, is tall. The carving of the image was started on Friday the 12th of Hiyaang-gei (November as per Meitei calendar) 1776 and the completely carved image was inaugurated three years later on Friday the 11th of Hiyaang-gei 1779.\n\nWhen the first temple was built at Imphal, Krishna again appeared to the king and in a vision revealed his mystic Ras dance. Then the king, who himself was a poet and connoisseur of the arts, directed Guru Swarupanand to seek the collaboration of all the famous exponents of dance in Manipur and adapt the \"Jagai-Nin-Thaag Purang\" to the Ras Lila of Lord Krishna.\n\nThe original location of the jackfruit tree has been declared an historical archaeological site by the Archaeological Department of Manipur.\n\n\n"}
{"id": "48516352", "url": "https://en.wikipedia.org/wiki?curid=48516352", "title": "Smart villages in Asia", "text": "Smart villages in Asia\n\nConcept of Smart villages is a global modern approach for off-grid communities. Vision behind this concept is to assist the policy makers, donors and socio-economic planner for rural electrification worldwide, with special focus on Asian and African countries. Smart villages concept is engaged in efforts to combat the real barriers to energy access in villages, particularly in developing countries with technological, financial and educational methodology. Since 20th Century Electricity has become a vital part of our lives, though we can survive without electricity but cannot progress and enjoy the benefits of science. World’s large oil companies have predicted that by 2050, one third of the energy will need to come from Solar, Wind and other renewable resources, therefore adoption of renewable resource in place of fossil fuel is the best approach that can be developed through off-grid systems or communities.\n\nThe term “Off-grid” itself is very broad and simply refers to \"not using or depending on electricity provided through main or national grids and generated by main power infrastructures. The term is also used to describe a particular lifestyle which is embodied by autonomous structures. Off-grid systems have a semi or autonomous capability to satisfy electricity demand through local power generation. The term off-grid systems cover both mini-grids for serving multiple users and stand-alone systems for individual appliance or users. In spite witnessing use of fossil fuel for power generation by mini or individual off-grid system, it is broadly defined that off-grid systems are actually based on renewable energy resources. The terms \"micro-grid, nano-grid and pico-grid are used to differentiate different kinds of mini-grids with size thresholds under off-grid approach.\n\nAccess to un-interrupted and low cost electricity for socio-economic development is an important requirement. There is a universal demand of grid-based and off-grid solutions to ensure access to electricity all over the world, without off-grid approach increasing demand and decreasing supply cannot be stabilized for the mankind on this planet.\n\nAbout 80% of world's population live in rural areas and majority of these people do not have access to electricity. Due to lack of employment people from rural areas migrate to urban areas where they find employment opportunities much easily because of industrial infrastructure established primarily on availability of electricity. International Renewable Energy Agency (IRENA) power generation projects based on renewable energy technology at low cost are the attractive option for off-grid electrification in most of the rural areas of Asian countries. Its work will satisfy the rural electricity demand and provide employment opportunities to minimize the rapid urbanization.\n\nAccording to a publication written for the International Finance Corporation (IFC) in 2012, Asia has the largest off-grid population in the world, with 55% of the global off-grid population, and 798 million people having no access to electricity. As per estimates about 700 million or 90% were located in rural Asia. However, research studies reveal that South Asian and Sub-Saharan African countries have been unable to expand their electrification rate. Whereas electrification progress in regions such as Latin America and East Asia (China) indicates a rapid growth. Central Asian countries are blessed with sufficient resources and export their extra electricity to neighboring countries.\n\nElectrification is highly desired by all rural communities. Different international, national and local organizations use different indicators for measuring and reporting mini-grids or stand-alone systems. South Asian countries have been focusing on off-grid electrification of current trend for Rural Electrification (RE) at regional level. India, Bangladesh, Sri Lanka and Nepal have shown good results for RE through off-grid communities.\n\nAbout 38% of the population of Asia and 22% in the world, live in East Asia.\n\nGeographical marking in the Western Asia consists of 19 countries/territorial states. 5 countries of Asia from this region hold strong financial stability and resources for social development. In this region three countries, According to population demography Turkey, Iraq and Yemen stand at 10th, 13th and 20th position respectively\n\n Covers largest part of Asia with a 17,098,242 km area in the Northern sub-region of Asia. Russia is the world's fourth largest electricity producer after the United States, China, and Japan. Russia exports electricity to countries e.g. Latvia, Lithuania, Poland etc. However, import and export reversal has also been reported due to cost of production.\n\n With its insufficient power supply infrastructure covers its electricity demand through import from electricity-exporting countries i.e. Uzbekistan, Tajikistan, Turkmenistan and Iran, these countries mostly sell their surplus electricity to Afghanistan. Above 4 billion US dollars have so far been disbursed to build power supply infrastructure in Afghanistan but deficiencies not only to its rural/remotes areas but country’s capital needs more considerable help from developed countries for supply of electrification to whole Afghanistan One of the largest solar power project funded at a cost $18 by the government of New Zealand has started functioning for supply of energy to 2,500 households, businesses and government buildings in central Bamyan Province of Afghanistan.\n\nIndia's first smart village has been developed by Eco Needs Foundation http://www.econeeds.org/ at Dhanora village of Rajasthan. The concept is prepared by Prof. Priyanand Agale, Dr. Satyapal Singh Meena an officer of Indian Revenue Service(IRS) http://epaper.bhaskar.com/detail/1202310/52621824785/0/map/tabs-1/05-26-2017/14/6/image/ and Mr. Attdeep Agale. This concept consists of five elements Retrofitting, Redevelopment, Greenfield, e-Pan and Livelihood. Under the project of smart village the Foundation is adopting villages and putting efforts for sustainable development by providing basic amenities like sanitation, safe drinking water, internal road, tree plantation, water conservation. The Foundation is also working for inculcating moral values in the society and for improving the standard of living of the villagers. The Foundation has developed Village Dhanora, Teh. Bari, District Dholpur, one of the remotely situated village of Rajasthan as India’s First Smart Village. The village is situated 30 km away from Dholpur district head quarter and 248 Km from Jaipur city, Capital of Rajasthan State of India. The population of the village was nearly about 2000 having no sanitation facility, potable water facility, which were adversely causing the health of the villagers. The internal roads are also not there and it causes great hardship to the people especially in rainy season. Owing to unawareness and non-availability of sanitation facility and toilets the people of the village use to go open for defecation. There are other problems also which villagers were facing such as Fluoride concentration in drinking water, No water conservation System, Encroachment on the roads, Electrical power fluctuation No outcome base education, Unemployment and poverty. ProF. Priyanand Agale Founder, president of Foundation and Dr. Satyapal Singh Meena officer of the Indian Revenue service has converted this village as India's first Smart Village and now Dhanora has become a role model of Rural Development. Following are the major success achieved within a short span of two years of the project and project is still underway:\n1. Construction of 822 toilets in the Panchayat area with the help of district administration and public participation accordingly, the Dhanora Gram Panchayat has been declared as the first “Open Defecation Free” (ODF) Panchayat by District Administration.\n2. Village Dhanora become India’s first village having sewerage line with treatment plant. The Foundation has laid down nearly 2 Km long sewerage line of diameter 450 mm in the village. Each of the toilets of Dhanora village have been connected to sewerage line with inspection chambers.\n3. Construction of nearly 2 Km long cement concrete internal roads constructed with 3.5 m to 4.5 m width with high quality.\n4. Construction of eight Percolation tank connecting with nearly 2.5 Km artificial channel of 10 feet in width and 10 feet in depth for water conservation and ground water recharge with public participation and with the help of government having groundwater recharge capacity of 97.49 Million liters in one time recharge, which will provide irrigation facility to farms of the village and nearby villages resulting into economic growth of farmers.\n5. The work of the removal of encroachments and road widening has been completed without using any police force. Now the whole village is having motorable road in the village.\n6. Construction of nearly 2 km approach road at Dhodekapura village of the Dhanora Panchayat, which was not done in last 65 years.\n7. The police Administration is going to declare the village as “APRADH MUKKTA GAON” (Crime Free Village), no case or FIR in Police Station.\n8. village Dhanora has been converted into an Art gallery. The paintings in the village are spreading social awareness among villagers \n9. The foundation stone for community centre and information centre has been laid down, work under progress. Work of solar street light, skill development centre, library, meditation centre, sport complex, Wi-Fi facility, and community toilet will be taken up in due course of time and as per availability of funds.\n\nAccess to reliable and uninterrupted electricity is a chronic demand in villages all over the world. The best solution for overcoming this problem is utilization of alternative energy with modern advancement with implementation of off-grid system.\n\nIn India competition for all enthusiastic entrepreneurs, individuals and organizations running energy access programmes and businesses in rural villages in India has also been launched and is about to close in November-2015. The participants were asked to highlight close sustainable examples where off-grid system is being practiced providing a platform for \"energy entrepreneurs\" to discuss the ways for achieving off-grid systems. This competition has also good rewards for successful winners i.e. Cash Prize of I million Indian Rupees, a trip to world Sustainable Development Forum to showcase their business on the main stage, etc.\n\nIn Pakistan the Agha Khan University Examination Board in October-2015 launched a \"Poster Competition\" with the title \"when ideas flow villages grow\" as an initiative to introduce the idea of Smart Villages among young students and to evaluate best measures for its implementation.The most outstanding poster presentation from across the country will get a chance to visit the University of Cambridge, UK, besides other good prizes.\n\nA female student of Aga Khan Higher Secondary School, one of the participant of the competition conducted the survey from her home place to villagers in remote areas by making connections with them through social media and cellular phones. According to her survey 50% of the people were found not satisfied with the rural electrification rate of PEPCO and other power distribution companies. They also believe that off grid system is now a need for the villages of Pakistan. Majority of the people were in favor of installing solar panels and wind turbines for energy generation in remote areas of Pakistan to boost up the development in energy sector of the country. Pie chart of survey depicts the result of question and their answers.\n"}
{"id": "79454", "url": "https://en.wikipedia.org/wiki?curid=79454", "title": "Submarine cable", "text": "Submarine cable\n\nSubmarine cable refers to any kind of electrical cable that is laid on the seabed, although the term is often extended to encompass cables laid on the bottom of large freshwater bodies of water.\n\nExamples include:\n\n"}
{"id": "12280369", "url": "https://en.wikipedia.org/wiki?curid=12280369", "title": "Subsurface currents", "text": "Subsurface currents\n\nA subsurface current is an oceanic current that runs beneath surface currents. Examples include the Equatorial Undercurrents of the Pacific, Atlantic, and Indian Oceans, the California Undercurrent, and the Agulhas Undercurrent, the deep thermohaline circulation in the Atlantic, and bottom gravity currents near Antarctica. The forcing mechanisms vary for these different types of subsurface currents.\n\nThe most common of these is the density current, epitomized by the Thermohaline current. The density current works on a basic principle: the denser water sinks to the bottom, separating from the less dense water, and causing an opposite reaction from it. There are numerous factors controlling density.\n\nOne is the salinity of water, a prime example of this being the Mediterranean/Atlantic exchange. The saltier waters of the Mediterranean sink to the bottom and flow along there, until they reach the ledge between the two bodies of water. At this point, they rush over the ledge into the Atlantic, pushing the less saline surface water into the Mediterranean.\n\nAnother factor of density is temperature. Thermohaline (literally meaning heat-salty) currents are very influenced by heat. Cold water from glaciers, icebergs, etc. descends to join the ultra-deep, cold section of the worldwide Thermohaline current. After spending an exceptionally long time in the depths, it eventually heats up, rising to join the higher Thermohaline current section. Because of the temperature and expansiveness of the Thermohaline current, it is substantially slower, taking nearly 1000 years to run its worldwide circuit.\n\nOne factor of density is so unique that it warrants its own current type. This is the turbidity current. Turbidity current is caused when the density of water is increased by sediment. This current is the underwater equivalent of a landslide. When sediment increases the density of the water, it falls to the bottom, and then follows the form of the land. In doing so, the sediment inside the current gathers more from the ocean bed, which in turn gathers more, and so on. As a limited amount of sediment can be carried by a certain amount of water, more water must become laded with sediment, until a huge, destructive current is washing down some marine hillside. It is theorized that submarine depths, such as the Marianas Trench have been caused in part by this action. There is one additional effect of turbidity currents: upwelling. All of the water rushing into ocean valleys displaces a significant amount of water. This water literally has nowhere to go but up. The upwelling current goes almost straight up. This spreads the nutrient rich ocean life to the surface, feeding some of the world’s largest fisheries. This current also helps Thermohaline currents return to the surface.\n\nAn entirely different class of subsurface current is caused by friction with surface currents and objects. When the wind or some other surface force compels surface currents into motion, some of this is translated into subsurface motion. The Ekman Spiral, named after Vagn Walfrid Ekman, is the standard for this transfer of energy. The Ekman Spiral works as follows: when the surface moves, the subsurface inherits some -but not all- of this motion. Due to the Coriolis Effect, however, the current moves at a 45˚ angle to the right of the first (left in the Southern Hemisphere). The current below is slower yet, and moves at a 45˚ angle to the right. This process continues in the same manner, until, at about 100 meters below the surface, the current is moving in the opposite direction of the surface current.\n\nThe final type of subsurface current is subsidence, caused when forces push water against some obstacle (like a rock), causing it to pile up there. The water at the bottom of the pileup flows away from it, causing a subsidence current.\n\nVarious subsurface currents conflict at times, causing bizarre wave patterns. One of the most noticeable of these is the Maelstrom. The word is derived from Nordic words meaning to grind and stream. Essentially, the maelstrom is a large, very powerful whirlpool, a large swirling body of water being drawn down and inward toward its center. This is usually the result of tidal currents.\n\nSubsurface currents have a large effect on life on earth. They flow beneath the surface of the water, allowing them to be relatively free of external influence. Thus, they function like clockwork, providing nutrient transportation, water transfer, etc., as well as affecting the ocean floor and submarine processes.\n\n"}
{"id": "7929246", "url": "https://en.wikipedia.org/wiki?curid=7929246", "title": "The Meme Machine", "text": "The Meme Machine\n\nThe Meme Machine (2000) is a popular science book by psychologist Susan Blackmore on the subject of memes. Blackmore attempts to constitute memetics as a science by discussing its empirical and analytic potential, as well as some important problems with memetics. The first half of the book tries to create greater clarity about the definition of the meme as she sees it. The last half of the book consists of a number of possible memetic explanations for such different problems as the origin of language, the origin of the human brain, sexual phenomena, the internet and the notion of the self. These explanations, in her view, give simpler and clearer explanations than trying to create genetic explanations in these fields.\n\nThe idea of memes, and the word itself, were originally speculated by Richard Dawkins in his book \"The Selfish Gene\" although similar, or analogous, concepts had been in currency for a while before its publishing. Richard Dawkins wrote a foreword to \"The Meme Machine\".\n\nIn the book, Blackmore examines the difficulties associated with the meme including its definition and how to spot one as well as the difficulties which arise from seeing it as being like the gene. She sees the meme in terms of being a universal replicator, of which the gene is but an example, rather than being like the gene itself. Universal replicators possess three key characteristics: high fidelity replication, high levels of fecundity (and so lots of copies) and longevity. She believes that these are earlier days for memes than genes, and that while memes have attained/evolved a sufficiently high level of these characteristics to qualify as replicators, they are not as effective replicators as genes by these key characteristics.\n\nWhile others have accepted the possible existence of memes, they are sometimes seen as subordinate to genes. The author suggests that this is not the case now and that memes are independent replicators. Indeed, she suggests that memes may now in some cases be driving genetic evolution and be the cause of the abnormally large brain in \"Homo sapiens\". Blackmore notes that human brains began expanding in size at about the same time that we started using tools and suggests that once individuals began to imitate each other, selection pressure favored those who could make good choices on what to imitate, and could imitate intelligently.\n\nIn the book, Blackmore tries to create a consistent vocabulary, since memetics has had a wide range of different terminologies and therefore, in Blackmore's opinion, a lot of misleading concepts. Some of the terms that are central in her book include:\n\nWriting for \"The New York Times\", Robert Wright gave his insight regarding the textbook.\n"}
{"id": "58229069", "url": "https://en.wikipedia.org/wiki?curid=58229069", "title": "United Nations Framework Classification for Resources", "text": "United Nations Framework Classification for Resources\n\nUnited Nations Framework Classification for Resources (UNFC) is an international scheme for the classification, management and reporting of energy, mineral and raw material resources.United Nations Economic Commission for Europe's (UNECE) Expert Group on Resource Classification (EGRC) is responsible for the development promotion and further development of UNFC.\n\nClassification and management of natural resources such as minerals and petroleum are classified using differing schemes. In 1997, UNECE published the United Nations Framework Classification for Reserves and Resources of Solid Fuels and Mineral Commodities (UNFC-1997) as a unifying international system for classifying solid minerals and fuels. In 2004, the Classification was revised to include petroleum (oil and natural gas) and uranium and renamed the UNFC for Fossil Energy and Mineral Resources 2004 (UNFC-2004). In 2009, a simplified United Nations Framework Classification for Fossil Energy and Mineral Reserves and Resources 2009 (UNFC-2009) was published. In response to the application of UNFC being extended to renewable energy, injection projects for geological storage and anthropogenic resources, the name was changed in 2017 to the United Nations Framework Classification for Resources (UNFC). \n\nThe UNFC system is used for: \n\n\nUNFC currently applies to minerals, petroleum, renewable energy,nuclear fuel resources, injection projects for geological storage, and anthropogenic resources. Application of UNFC to groundwater resources is being evaluated. \n\nUNFC has been adopted as the basis of national resource classification in many countries including China, India, Mexico , Poland and Ukraine. African Union Commission has decided to develop a UNFC-based African Mineral and Energy Resources Classification and Management System (UNFC-AMREC) as a unifying system for Africa. Development of AMREC includes preparing a Pan-African Resources Reporting Code (PARC). European Commission is assessing the use of UNFC to classify and report raw material resources of Europe.\n\n"}
{"id": "29975836", "url": "https://en.wikipedia.org/wiki?curid=29975836", "title": "Wave Motion (journal)", "text": "Wave Motion (journal)\n\nWave Motion is a peer-reviewed scientific journal publishing papers on the physics of waves – with emphasis on the areas of acoustics, optics, geophysics, seismology, electromagnetic theory, solid and fluid mechanics. Original research articles on analytical, numerical and experimental aspects of wave motion are covered.\n\nThe journal was established in 1979 by editor in chief Jan D. Achenbach. In 2011, Andrew N. Norris joined as co-editor in chief, and became sole editor in chief in 2012. The role of editor in chief passed to William J. Parnell in 2017 and K.W. Chow became deputy editor in chief at this time. The journal is published by Elsevier.\n\nThe journal is abstracted and indexed in Applied Mechanics Reviews, Current Contents/Engineering, Computing & Technology, Current Contents/Physics, Chemical, & Earth Sciences, Compendex, Inspec, Mathematical Reviews, Scopus, and Zentralblatt MATH. According to the Journal Citation Reports, \"Wave Motion\" has a 2016 5 year impact factor of 1.704 and an impact factor of 1.575, ranking it 15th out of 31 journals in the category \"acoustics\", 71st out of 133 in the category \"mechanics\", and 32nd out of 79 in the category \"physics, multidisciplinary\".\n\n"}
{"id": "1945275", "url": "https://en.wikipedia.org/wiki?curid=1945275", "title": "Wigner effect", "text": "Wigner effect\n\nThe Wigner effect (named for its discoverer, Eugene Wigner), also known as the discomposition effect or Wigner's Disease, is the dislocation of atoms in a solid caused by neutron radiation. \n\nAny solid can display the Wigner effect. The effect is of most concern in neutron moderators, such as graphite, intended to reduce the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235.\n\nTo create the Wigner effect, neutrons that collide with the atoms in a crystal structure must have enough energy to displace them from the lattice. This amount (threshold displacement energy) is approximately 25 eV. A neutron's energy can vary widely, but it is not uncommon to have energies up to and exceeding 10 MeV (10,000,000 eV) in the centre of a nuclear reactor. A neutron with a significant amount of energy will create a displacement cascade in a matrix via elastic collisions. For example, a 1 MeV neutron striking graphite will create 900 displacements; not all displacements will create defects, because some of the struck atoms will find and fill the vacancies that were either small pre-existing voids or vacancies newly formed by the other struck atoms.\n\nThe atoms that do not find a vacancy come to rest in non-ideal locations; that is, not along the symmetrical lines of the lattice. These atoms are referred to as interstitial atoms, or simply interstitials. An interstitial atom and its associated vacancy are known as a Frenkel defect. Because these atoms are not in the ideal location, they have an energy associated with them, much as a ball at the top of a hill has gravitational potential energy. This energy is referred to as Wigner energy. When a large number of interstitials have accumulated, they pose a risk of releasing all of their energy suddenly, creating a rapid, very great increase in temperature. Sudden, unplanned increases in temperature can present a large risk for certain types of nuclear reactors with low operating temperatures; one such was the indirect cause of the Windscale fire. Accumulation of energy in irradiated graphite has been recorded as high as 2.7 kJ/g, but is typically much lower than this. Graphite, having a heat capacity of 0.720 J/g°C, could see a sudden increase in temperature of about 3750 °C (6780 °F).\n\nDespite some reports, Wigner energy buildup had nothing to do with the cause of the Chernobyl disaster: this reactor, like all contemporary power reactors, operated at a high enough temperature to allow the displaced graphite structure to realign itself before any potential energy could be stored. Wigner energy may have played some part following the prompt critical neutron spike, when the accident entered the graphite fire phase of events.\n\nA buildup of Wigner energy can be relieved by heating the material. This process is known as annealing. In graphite this occurs at 250 °C.\n\nIn 2003, it was postulated that Wigner energy can be stored by the formation of metastable defect structures in graphite. Notably, the large energy release observed at 200–250 °C has been described in terms of a metastable interstitial-vacancy pair. The interstitial atom becomes trapped on the lip of the vacancy, and there is a barrier for it to recombine to give perfect graphite.\n\n"}
{"id": "32711985", "url": "https://en.wikipedia.org/wiki?curid=32711985", "title": "William Bayly (astronomer)", "text": "William Bayly (astronomer)\n\nWilliam Bayly (1737–1810) was an English astronomer.\n\nBayly was born at Bishops Cannings, or Carions, in Wiltshire. His father was a small farmer, and Bayly's boyhood was spent at the plough. In spite of the constant manual work he had to do, he took advantage of the kindness of an exciseman living in a neighbouring village, who offered to give him some lessons. From him he learned the elements of arithmetic. A gentleman of Bath, named Kingston, heard of the boy's taste for mathematics, and gave him some help. He became usher in a school at Stoke, near Bristol, and after a while took a similar situation in another school in the neighbourhood. While thus employed, he took every opportunity of increasing his mathematical knowledge. \n\nNevil Maskelyne, the astronomer-royal, happened to hear of his talents, and engaged him as an assistant at the Royal Observatory. On his recommendation Bayly, in 1769, was sent out by the Royal Society to North Cape, Norway to observe the transit of Venus that occurred in that year, and his observations were printed in the \"Philosophical Transactions\" of the society. In 1772 he accompanied William Wales as an astronomer on Cook's second voyage of discovery to the southern hemisphere. The two ships employed in the expedition, the \"Resolution\" and the \"Adventure\", sailed on 13 June. He also sailed in Cook's third and last voyage made with the \"Resolution\" and the \"Discovery\", which cleared the channel on 14 July 1776. This voyage, in which Cook was killed, came to an end in 1780.\n\nIn 1785 Bayly was made head-master of the Royal Academy at Portsmouth, an office he continued to hold until the establishment of the Royal Naval College in 1807, when he retired on a sufficient pension. The organ in the parish church of his native village is his gift. He died at Portsea, Portsmouth towards the end of 1810.\n\n"}
