{"id": "44533399", "url": "https://en.wikipedia.org/wiki?curid=44533399", "title": "1992 Colombian energy crisis", "text": "1992 Colombian energy crisis\n\nThe 1992 Colombian energy crisis was a crisis of the Colombian energy sector during the presidency of César Gaviria, from May 2, 1992, to February 7, 1993, caused by El Niño. El Niño caused droughts across much of the country, which lowered reservoir levels at many of its principal hydroelectric dams and a further crisis within the state public services company, Interconexión Eléctrica, S.A. (ISA).\n\nEl Niño occurs when warm waters originating off the coast of Australia reach South American shores. This causes significant variation in marine climates and droughts and floods. The 1992 El Niño, which produced droughts in Colombia, coincided with infrastructure problems within the nation's hydroelectric power facilities.\n\nThe government under president César Gaviria opted to take action, introducing a rationing program. On March 2, 1992, the government announced scheduled power cuts, of up to nine hours in cities such as Bogotá and up to 18 hours on the islands of San Andrés y Providencia. Public awareness campaigns were launched under the name \"Cierre la llave\" (\"Turn Off the Tap\"), hoping to measure how much water was being wasted. In the city of Cali, a decree was passed setting six days in prison as a penalty for heavy water users.\n\nAs a further measure, Gaviria changed Colombia's time zone from UTC-5 to UTC-4, which was then the time zone of neighboring Venezuela, beginning on Saturday, May 2, 1992, at 1:00 am. The measure came to be known as \"Hora Gaviria\" (\"Gaviria Time\"). Of Colombia's 1,024 municipalities, 1,000 declined to change time zones and took months to come into effect. The time zone change was ended on February 7, 1993.\n\nOne of two most popular telenovelas of 1992 in Colombia, \"En cuerpo ajeno\", was moved from 8pm to 10pm on Cadena Uno, then a highly unusual timeslot for flagship novelas on Colombian television, so that more viewers could see it when they were not impacted by energy rationing (the blackouts in Bogotá ended at 9 pm). The other popular telenovela, \"Sangre de Lobos\", was already being broadcast at 10pm on Canal A. Caracol Radio created a program, \"La Luciérnaga\" (\"The Firefly\"), for the early evening hours when power cuts were in effect in many areas; as of 2015, the program remains on the air.\n"}
{"id": "23737622", "url": "https://en.wikipedia.org/wiki?curid=23737622", "title": "Alternative stable state", "text": "Alternative stable state\n\nIn ecology, the theory of alternative stable states (sometimes termed alternate stable states or alternative stable equilibria) predicts that ecosystems can exist under multiple \"states\" (sets of unique biotic and abiotic conditions). These alternative states are non-transitory and therefore considered stable over ecologically-relevant timescales. Ecosystems may transition from one stable state to another, in what is known as a state shift (sometimes termed a phase shift or regime shift), when perturbed. Due to ecological feedbacks, ecosystems display resistance to state shifts and therefore tend to remain in one state unless perturbations are large enough. Multiple states may persist under equal environmental conditions, a phenomenon known as hysteresis. Alternative stable state theory suggests that discrete states are separated by ecological thresholds, in contrast to ecosystems which change smoothly and continuously along an environmental gradient.\n\nAlternative stable state theory was first proposed by Richard Lewontin (1969), but other early key authors include Holling (1973), Sutherland (1974), May (1977), and Scheffer et al. (2001). In the broadest sense, alternative stable state theory proposes that a change in ecosystem conditions can result in an abrupt shift in the state of the ecosystem, such as a change in population (Barange, M. et al. 2008) or community composition. Ecosystems can persist in states that are considered stable (i.e., can exist for relatively long periods of time). Intermediate states are considered unstable and are, therefore, transitory. Because ecosystems are resistant to state shifts, significant perturbations are usually required to overcome ecological thresholds and cause shifts from one stable state to another. The resistance to state shifts is known as \"resilience\" (Holling 1973).\n\nState shifts are often illustrated heuristically by the ball-in-cup model (Holling, C.S. et al., 1995) Biodiversity in the functioning of ecosystems: an ecological synthesis. In Biodiversity Loss, Ecological and Economical Issues (Perrings, C.A. et al., eds), pp. 44–83, Cambridge University Press). A ball, representing the ecosystem, exists on a surface where any point along the surface represents a possible state. In the simplest model, the landscape consists of two valleys separated by a hill. When the ball is in a valley, or a \"domain of attraction\", it exists in a stable state and must be perturbed to move from this state. In the absence of perturbations, the ball will always roll downhill and therefore will tend to stay in the valley (or stable state). State shifts can be viewed from two different viewpoints, the \"community perspective\" and the \"ecosystem perspective\". The ball can only move between stable states in two ways: (1) moving the ball or (2) altering the landscape. The community perspective is analogous to moving the ball, while the ecosystem perspective is analogous to altering the landscape.\n\nThese two viewpoints consider the same phenomenon with different mechanisms. The community perspective considers ecosystem variables (which change relatively quickly and are subject to feedbacks from the system), whereas the ecosystem perspective considers ecosystem parameters (which change relatively slowly and operate independently of the system). The community context considers a relatively constant environment in which multiple stable states are accessible to populations or communities. This definition is an extension of stability analysis of populations (e.g., Lewontin 1969; Sutherland 1973) and communities (e.g., Drake 1991; Law and Morton 1993). The ecosystem context focuses on the effect of exogenic \"drivers\" on communities or ecosystems (e.g., May 1977; Scheffer et al. 2001; Dent et al. 2002). Both definitions are explored within this article.\n\nEcosystems can shift from one state to another via a significant perturbation directly to state variables. State variables are quantities that change quickly (in ecologically-relevant time scales) in response to feedbacks from the system (i.e., they are dependent on system feedbacks), such as population densities. This perspective requires that different states can exist simultaneously under equal environmental conditions, since the ball moves only in response to a state variable change.\n\nFor example, consider a very simple system with three microbial species. It may be possible for the system to exist under different community structure regimes depending on initial conditions (e.g., population densities or spatial arrangement of individuals) (Kerr et al. 2002). Perhaps under certain initial densities or spatial configurations, one species dominates over all others, while under different initial conditions all species can mutually coexist. Because the different species interact, changes in populations affect one another synergistically to determine community structure. Under both states the environmental conditions are identical. Because the states have resilience, following small perturbations (e.g., changes to population size) the community returns to the same configuration while large perturbations may induce a shift to another configuration.\n\nThe community perspective requires the existence of alternative stable states (i.e., more than one valley) before the perturbation, since the landscape is not changing. Because communities have some level of resistance to change, they will stay in their domain of attraction (or stable state) until the perturbation is large enough to force the system into another state. In the ball-and-cup model, this would be the energy required to push the ball up and over a hill, where it would fall downhill into a different valley.\n\nIt is also possible to cause state shifts in another context, by indirectly affecting state variables. This is known as the ecosystem perspective. This perspective requires a change in environmental parameters that affect the behavior of state variables. For example, birth rate, death rate, migration, and density-dependent predation indirectly alter the ecosystem state by changing population density (a state variable). Ecosystem parameters are quantities that are unresponsive (or respond very slowly) to feedbacks from the system (i.e., they are independent of system feedbacks). The stable state landscape is changed by environmental drivers, which may result in a change in the quantity of stable states and the relationship between states.\n\nBy the ecosystem perspective, the landscape of the ecological states is changed, which forces a change in the ecosystem state. Changing the landscape can modify the number, location, and resilience of stable states, as well as the unstable intermediate states. By this view, the topography in the ball-and-cup model is not static, as it is in the community perspective. This is a fundamental difference between the two perspectives.\n\nAlthough the mechanisms of community and ecosystem perspectives are different, the empirical evidence required for documentation of alternative stable states is the same. In addition, state shifts are often a combination of internal processes and external forces (Scheffer et al. 2001). For example, consider a stream-fed lake in which the pristine state is dominated by benthic vegetation. When upstream construction releases soils into the stream, the system becomes turbid. As a result, benthic vegetation cannot receive light and decline, increasing nutrient availability and allowing phytoplankton to dominate. In this state shift scenario the state variables changing are the populations of benthic vegetation and phytoplankton, and the ecosystem parameters are turbidity and nutrient levels. So, whether identifying mechanisms of variables or parameters is a matter of formulation (Beisner et al. 2003).\n\nHysteresis is an important concept in alternative stable state theory. In this ecological context, hysteresis refers to the existence of different stable states under the same variables or parameters. Hysteresis can be explained by \"path-dependency\", in which the equilibrium point for the trajectory of \"A → B\" is different from for \"B → A\". In other words, it matters which way the ball is moving across the landscape. Some ecologists (e.g., Scheffer et al. 2001) argue that hysteresis is a prerequisite for the existence of alternative stable states. Others (e.g., Beisner et al. 2003) claim that this is not so; although shifts often involve hysteresis, a system can show alternative stable states yet have equal paths for \"A → B\" and \"B → A\".\n\nHysteresis can occur via changes to variables or parameters. When variables are changed the ball is pushed from one domain of attraction to another, yet the same push from the other direction cannot return the ball to the original domain of attraction. When parameters are changed a modification to the landscape results in a state shift, but reversing the modification does not result in a reciprocal shift.\n\nA real-world example of hysteresis is helpful to illustrate the concept. Coral reef systems can dramatically shift from pristine coral-dominated systems to degraded algae-dominated systems when populations grazing on algae decline. The 1983 crash of sea urchin populations in Caribbean reef systems released algae from top-down (herbivory) control, allowing them to overgrow corals and resulting in a shift to a degraded state. When urchins rebounded, the high (pre-crash) coral cover levels did not return, indicating hysteresis (Mumby et al. 2007).\n\nIn some cases, state shifts under hysteresis may be irreversible. For example, tropical cloud forests require high moisture levels, provided by clouds that are intercepted by the canopy (via condensation). When deforested, moisture delivery ceases. Therefore, reforestation is often unsuccessful because conditions are too dry to allow the trees to grow (Wilson and Agnew 1992). Even in cases where there is no obvious barrier to recovery, alternative states can be remarkably persistent: an experimental grassland heavily fertilized for 10 years lost much of its biodiversity, and was still in this state 20 years later.\n\nBy their very nature, basins of attraction display resilience. Ecosystems are resistant to state shifts – they will only undergo shifts under substantial perturbations – but some states are more resilient than others. In the ball-and-cup model, a valley with steep sides has greater resilience than a shallow valley, since it would take more force to push the ball up the hill and out of the valley.\n\nResilience can change in stable states when environmental parameters are shifted. Often, humans influence stable states by reducing the resilience of basins of attraction. There are at least three ways in which anthropogenic forces reduce resilience (Folke et al. 2004): (1) Decreasing diversity and functional groups, often by top-down effects (e.g., overfishing); (2) altering the physico-chemical environment (e.g., climate change, pollution, fertilization); or (3) modifying disturbance regimes to which organisms are adapted (e.g., bottom trawling, coral mining, etc.). When the resilience is decreased, ecosystems can be pushed into alternative, and often less-desirable, stable states with only minor perturbations. When hysteresis effects are present, the return to a more-desirable state is sometimes impossible or impractical (given management constraints). Shifts to less-desirable states often entail a loss of ecosystem service and function, and have been documented in an array of terrestrial, marine, and freshwater environments (reviewed in Folke et al. 2004).\n\nMost work on alternative stable states has been theoretical, using mathematical models and simulations to test ecological hypotheses. Other work has been conducted using empirical evidence from surveying, historical records, or comparisons across spatial scales. There has been a lack of direct, manipulative experimental tests for alternative stable states. This is especially true for studies outside of controlled laboratory conditions, where state shifts have been documented for cultures of microorganisms.\n\nVerifying the existence of alternative stable states carries profound implications for ecosystem management. If stable states exist, gradual changes in environmental factors may have little effect on a system until a threshold is reached, at which point a catastrophic state shift may occur. Understanding the nature of these thresholds will help inform the design of monitoring programs, ecosystem restoration, and other management decisions. Managers are particularly interested in the potential of hysteresis, since it may be difficult to recover from a state shift (Beisner et al. 2003). The mechanisms of feedback loops that maintain stable states are important to understand if we hope to effectively manage an ecosystem with alternative stable states.\n\nEmpirical evidence for the existence of alternative stable states is vital to advancing the idea beyond theory. Schröder et al. (2005) reviewed the current ecological literature for alternative stable states and found 35 direct experiments, of which only 21 were deemed valid. Of these, 62% (14) showed evidence for and 38% (8) showed no evidence for alternative stable states. However, the Schröder et al. (2005) analysis required evidence of hysteresis, which is not necessarily a prerequisite for alternative stable states. Other authors (e.g., Scheffer et al. 2001; Folke et al. 2004) have had less-stringent requirements for the documentation of alternative stable states.\n\nState shifts via the community perspective have been induced experimentally by the addition or removal of predators, such as in Paine's (1966) work on keystone predators (i.e., predators with disproportionate influence on community structure) in the intertidal zone (although this claim is refuted by Schröder et al. 2005). Also, Beisner et al. (2003) suggest that commercially exploited fish populations can be forced between alternative stable states by fishing pressure due to Allee effect that work at very low population sizes. Once a fish population falls below a certain threshold, it will inevitably go extinct when low population densities make replacement of adults impossible due to, for example, the inability to find mates or density-dependent mortality. Since populations cannot return from extinction, this is an example of an irreversible state shift.\n\nAlthough alternative stable state theory is still in its infancy, empirical evidence has been collected from a variety of biomes:\n\n\n"}
{"id": "40051084", "url": "https://en.wikipedia.org/wiki?curid=40051084", "title": "American Environmental Assessment and Solutions Inc.", "text": "American Environmental Assessment and Solutions Inc.\n\nAmerican Environmental Assessment and Solutions Inc., commonly referred to as American Environmental, is a Minority, Woman Owned Business based in New York offering environmental services, environmental site assessment, site remediation, soil testing, subsurface analysis, and bio-remediation services.\nAmerican Environmental provides online training for environmentally related classes such as safety training for DOT, EPA, and OSHA, and is the only WOSB certified Minority and Woman - Owned Environmental Company in Brooklyn, New York. American Environmental services Long Island, New York City, and Upstate New York. Clients of American Environmental include government agencies, real-estate owners and developers, legal firms, engineers and architects, and financial and manufacturing industries. American Environmental also has approval with the following regulatory agencies: NYCHA, NYCEDC, DASNY, NYCSCA and NYCOER.\n\n\n\n"}
{"id": "16873422", "url": "https://en.wikipedia.org/wiki?curid=16873422", "title": "American Gas Association", "text": "American Gas Association\n\nThe American Gas Association (AGA), founded in 1918, is an American trade organization representing over 200 natural gas supply companies and others with an interest in the manufacturing of gas appliances as well as the production of gas. About 92% of the 70 million natural gas customers in the US receive their gas from AGA members.\n\nThe association contains two divisions, operations and engineering, and public affairs. These two divisions work together to handle finance, corporate affairs, communications, and membership departments to deliver timely and relevant information to association members. The AGA also works with lawmakers, regulatory bodies, environmental and consumer affairs organizations, and the public at large, informing them of the natural gas utility industry in the United States.\n\nThe American Gas Association formed in June 1918 after the merger of the American Gas Institute and the National Commercial Gas Association. Those organizations served the interests of companies that dealt in manufactured, as opposed to natural, gas. Manufactured gas was the dominant fuel in the early United States, but during the 19th century, natural gas supplanted it.\n\nIn January 1919, the AGA launched a publication for the natural gas industry providing information on trends, activities, and strategies on how to improve gas companies.\n\nIn 1925, the association formed laboratories in Cleveland, and five years later expanded to Los Angeles. These labs developed technology to improve gas appliances and equipment, making them more energy efficient and consumer friendly. The labs also did testing to ensure gas equipment conformed to national standards for safety, durability, and performance. The AGA ended its laboratory activities in 1997 and the new CSA International took its place. CSA today still runs a U.S. certification-type program from the AGA's original Cleveland laboratory.\n\nIn 1927, the AGA merged with the National Gas Association to help the AGA's member companies which depended on manufactured gas to make a smooth transition to natural gas.\n\nIn 1935, Congress passed the Public Utility Act and broke up the holding company that dominated much of the country's utility industry; this law continued to be enforced until 2005.\n\nDuring the 1930s, the AGA formed the National Advertising Committee to oversee a nationwide advertising program promoting gas for cooking, water heating, refrigeration, and house heating.\n\nIn 2016, the AGA installed a new chairman of the board, Pierce Norton.\n\n\"The American Gas Association has worked with the (FERC) Federal Energy Regulatory Commission to improve market transparency reporting. In 2004, the FERC issued a policy statement on price reporting and price index publishing. The following year, the Energy Policy Act of 2005 included market behavior and provisions and penalties, and FERC issued a rule prohibiting market manipulation.\n\nAccording to Jay Copan, AGA's senior vice president of corporate affairs, AGA members deliver natural gas to 52 million homes, businesses and industrial facilities. Membership also features the unique profile of the millions of individuals who own stock in energy utilities. According to Copan, AGA's research shows that individuals, rather than institutions, own more than 50 percent of utility shares. 70 percent of utility stockholders are the age of 65 and older, according to data provided by Edward Jones a leading brokerage firm, and by AGA member companies.\n\nThe American Gas Association has faced financial issues regarding the taxation of dividends. In 2002, The American Gas Association's main priority was to eliminate double taxation on dividends. AGA analyst Charlie Fritts voiced his opinions about the opposition of double taxation due to the possible effects it can have on gas companies. The United States tax code has also been seen as problematic by the American Gas Association, mostly because it creates an uneven playing field for companies that pay dividends and gas companies that maintain most of their earnings. Fritts had also viewed double taxation as problematic because the gas utility industry is expected to raise approximately $100 billion in capital in the following 20 years after 2002, which can be hampered if profits (dividends) from investments are highly taxed. The main goal was to develop a natural gas infrastructure in the United States. Fritts also stated that \"natural gas demand is expected to grow 50 percent between 2003 until 2020; utilities must raise substantial capital to build 255,000 miles of natural gas distribution pipe to meet that demand.\"\n\nThe United States has enough domestic natural gas to meet demands for several industries for several decades into the future, according to the Potential Gas Committee (PGC). The committee is a nonprofit industry-funded organization. \"Specifically, the United States possesses a technically recoverable natural gas resource base of 2,817 trillion cubic feet (Tcf) yet to be discovered, said Alexei V. Milkov, a professor and director of the PGC at the Colorado School of Mines, which provides guidance and technical assistance to the PGC.\" Milkov Later clarified that \"that’s jargon for 'the U.S. has abundant resources of natural gas.’\"\n\nIn September 2011, Dave McCurdy the President and CEO of the American Gas Association has expressed optimism towards the growth of energy sources in the near future. AGA's President argues how the United States current energy policy is divorced from the economic reality that natural gas has a good chance of competing in the marketplace if the United States is willing to navigate away from dependence on foreign oil. McCurdy has remained optimistic by speculating that consumer demand will increase during the next few years.\n\nMcCurdy suggests that one of the main economic issues in the United States concerns its addiction to foreign oil. The President of the American Gas Association has developed a few proposals to reduce carbon emissions that contribute to global warming. McCurdy mentions how the discovery of natural gas in the Marcellus State region from upstate New York to Kentucky can transform into a future foundation site for American energy. The AGA President presented the idea that switching millions of tractor trailers to natural gas would have played a substantial role not only in reducing the amount of foreign oil consumed by America but would ultimately reduce the production carbon emissions which contributes to global warming.\n\nMcCurdy also argues drilling in some Marcellus states have had revitalized dying towns while keeping the unemployment rate below the national average. McCurdy has been criticized for his proposal requiring the acquisition of natural gas because of the possibility of hydraulic fracturing leading to polluted water supplies. Despite, the long-term issues of McCurdy's proposals, at the state level the short-term benefits are replaced by the realization that natural gas is not only an alternative solution to foreign oil but can also contribute to the creation of thousands of jobs. The AGA President also explains methods on how to meet the Obama administration's goal of the development electric vehicles by 2025 by suggesting that the U.S. capital on natural gas and the development of infrastructure investments.While the American Gas Association believes that natural gas is considered a necessity in the United States they also believe that the fuel-economy requirements of the country cannot be meet without the production of natural gas.\n\n"}
{"id": "213072", "url": "https://en.wikipedia.org/wiki?curid=213072", "title": "Amory Lovins", "text": "Amory Lovins\n\nAmory Bloch Lovins (born November 13, 1947) is an American physicist, environmental scientist, writer, and Chairman/Chief Scientist of the Rocky Mountain Institute. He has worked in the field of energy policy and related areas for four decades. He was named by \"Time\" magazine one of the World's 100 most influential people in 2009.\n\nLovins worked professionally as an environmentalist in the 1970s and since then as an analyst of a \"soft energy path\" for the United States and other nations. He has promoted energy efficiency, the use of renewable energy sources, and the generation of energy at or near the site where the energy is actually used. Lovins has also advocated a \"negawatt revolution\" arguing that utility customers don't want kilowatt-hours of electricity; they want energy services. In the 1990s, his work with Rocky Mountain Institute included the design of an ultra-efficient automobile, the Hypercar.\n\nLovins does not see his energy ideas as green or left-wing, and he is an advocate of private enterprise and free market economics. He notes that Rupert Murdoch has made News Corporation carbon-neutral, with savings of millions of dollars. But, says Lovins, large institutions are becoming more \"gridlocked and moribund\", and he supports the rise of \"citizen organizations\" around the world.\n\nLovins has received ten honorary doctorates and won many awards. He has provided expert testimony in eight countries, briefed 19 heads of state, and published 31 books. These books include \"Reinventing Fire\", \"Winning the Oil Endgame\", \"Small is Profitable\", \"Brittle Power\", and \"Natural Capitalism\".\n\nBorn in Washington, DC, Lovins spent much of his youth in Silver Spring, Maryland, in Amherst, Massachusetts, and in Montclair, New Jersey. In 1964, Lovins entered Harvard College. After two years there, he transferred in 1967 to Magdalen College, Oxford, where he studied physics and other subjects. In 1969 he became a Junior Research Fellow at Merton College, Oxford, where he had a temporary Oxford master of arts status as a result of becoming a university don. He did not graduate, because the University would not allow him to pursue a doctorate in energy, as it was two years before the 1973 oil embargo and energy was not yet considered an academic subject. Lovins resigned his Fellowship and moved to London to pursue his energy work. He moved back to the U.S. in 1981 and settled in western Colorado in 1982.\n\nEach summer from about 1965 to 1981, Lovins guided mountaineering trips and photographed the White Mountains of New Hampshire, contributing photographs to \"At Home in the Wild: New England's White Mountains\". In 1971 he wrote about the endangered Snowdonia National Park in the book, \"Eryri, the Mountains of Longing\", commissioned by David Brower, president of Friends of the Earth. Lovins spent about a decade as British Representative for Friends of the Earth.\n\nDuring the early seventies, Lovins became interested in the area of resource policy, especially energy policy. The 1973 energy crisis helped create an audience for his writing and an essay originally penned as a U.N. paper grew into his first book concerned with energy, \"World Energy Strategies\" (1973). His next book was \"\" (1975), co-authored with John H. Price. Lovins published a 10,000-word essay \"Energy Strategy: The Road Not Taken?\" in \"Foreign Affairs\", in October 1976. Its contents were the subject of many seminars at government departments, universities, energy agencies, and nuclear energy research centers, during 1975–1977. The article was expanded and republished as \"Soft Energy Paths: Toward a Durable Peace\" in 1977.\n\nBy 1978 Lovins had published six books, consulted widely, and was active in energy affairs in some 15 countries. In 1982, he and Hunter Lovins founded Rocky Mountain Institute, based in Snowmass, Colorado. Together with a group of colleagues, the Lovinses fostered efficient resource use and sustainable development.\n\nLovins has briefed 19 heads of state, provided expert testimony in eight countries, and published 29 books and several hundred papers. His clients have included many Fortune 500 companies, major real-estate developers, and utilities. Public-sector clients have included the OECD, UN, Resources for the Future, many national governments, and 13 US states. Lovins served in 1980–81 on the U.S. Department of Energy's Energy Research Advisory Board, and in 1999–2001 and 2006–08 on Defense Science Board task forces on military energy efficiency and strategy. His visiting academic chairs most recently included a visiting professorship in Stanford University's School of Engineering.\n\nSince 1982, RMI has grown into a broad-based \"think-and-do tank\" with more than 85 staff and an annual budget of some $13 million. RMI has spun off five for-profit companies.\n\nAmory Lovins came to prominence in 1976 when he published an article in \"Foreign Affairs\" called \"Energy Strategy: The Road Not Taken?\" Lovins argued that the United States had arrived at an important crossroads and could take one of two paths. The first, supported by U.S. policy, promised a future of steadily increasing reliance on fossil fuels and nuclear fission, and had serious environmental risks. The alternative, which Lovins called \"the soft path,\" favored \"benign\" sources of renewable energy like wind power and solar power, along with a heightened commitment to energy conservation and energy efficiency. In October 1977, \"The Atlantic\" ran a cover story on Lovins' ideas.\n\nAmory Lovins advocates \"soft energy paths\" involving efficient energy use, diverse and renewable energy sources, and special reliance on \"soft energy technologies\". Soft energy technologies are those based on solar, wind, biofuels, geothermal, etc. which are matched in scale and quality to their task. Residential solar energy technologies are prime examples of soft energy technologies and rapid deployment of simple, energy conserving, residential solar energy technologies is fundamental to a soft energy strategy.\n\nLovins has described the \"hard energy path\" as involving inefficient energy use and centralized, non-renewable energy sources such as fossil fuels. He believes soft path impacts are more \"gentle, pleasant and manageable\" than hard path impacts. These impacts range from the individual and household level to those affecting the very fabric of society at the national and international level.\n\n\"Lovins on the Soft Path\" is an award-winning documentary film made by Amory and Hunter Lovins. It received many prizes: \"Best Science and Technology Film, San Francisco International Film Festival, 1983; Blue Ribbon, American Film Festival, 1982; Best of the Festival, Environmental Education Film Festival, 1982; Best Energy Film, International Environmental Film Festival, 1982; and Chris Bronze Plaque, Columbus International Film Festival, 1982.\"\n\nLovins says that nuclear power plants are intermittent in that they will sometimes fail unexpectedly, often for long periods of time. For example, in the United States, 132 nuclear plants were built, and 21% were permanently and prematurely closed due to reliability or cost problems, while another 27% have at least once completely failed for a year or more. The remaining U.S. nuclear plants produce approximately 90% of their full-time full-load potential, but even they must shut down (on average) for 39 days every 17 months for scheduled refueling and maintenance. To cope with such intermittence by nuclear (and centralized fossil-fuelled) power plants, utilities install a \"reserve margin\" of roughly 15% extra capacity spinning ready for instant use.\n\nNuclear plants have an additional disadvantage; for safety, they must instantly shut down in a power failure, but for nuclear-physics reasons, they can't be restarted quickly. For example, during the Northeast Blackout of 2003, nine operating U.S. nuclear units had to shut down and were later restarted. During the first three days, while they were most needed, their output was less than 3% of normal. After twelve days of restart, their average capacity loss had exceeded 50 percent.\n\nLovins general assessment of nuclear power is that \"Nuclear power is the only energy source where mishap or malice can kill so many people so far away; the only one whose ingredients can help make and hide nuclear bombs; the only climate solution that substitutes proliferation, accident, and high-level radioactive waste dangers. Indeed, nuclear plants are so slow and costly to build that they reduce and retard climate protection\". With respect to the 2011 Japanese nuclear accidents, Lovins has said: \"An earthquake-and-tsunami zone crowded with 127 million people is an unwise place for 54 reactors\".\n\nIn terms of the UK, Amory Lovins commented in 2014 that:\n\nBritain's plan for a fleet of new nuclear power stations is … unbelievable … It is economically daft. The guaranteed price [being offered to French state company EDF] is over seven times the unsubsidised price of new wind in the US, four or five times the unsubsidised price of new solar power in the US. Nuclear prices only go up. Renewable energy prices come down. There is absolutely no business case for nuclear. The British policy has nothing to do with economic or any other rational base for decision making.\n\nA negawatt is a unit in watts of energy saved. It is basically the opposite of a watt. Amory Lovins has advocated a \"negawatt revolution\", arguing that utility customers don't want kilowatt-hours of electricity; they want energy services such as hot showers, cold beer, lit rooms, and spinning shafts, which can come more cheaply if electricity is used more efficiently.\n\nAccording to Lovins, energy efficiency represents a profitable global market and American companies have at their disposal the technical innovations to lead the way. Not only should they \"upgrade their plants and office buildings, but they should encourage the formation of negawatt markets\". Lovins sees negawatt markets as a win-win solution to many environmental problems. Because it is \"now generally cheaper to save fuel than to burn it, global warming, acid rain, and urban smog can be reduced not at a cost but at a profit\".\n\nLovins explains that many companies are already enjoying the financial and other rewards that come from saving electricity. Yet progress in converting to electricity saving technologies has been slowed by the indifference or outright opposition of some utilities. A second obstacle to efficiency is that many electricity-using devices are purchased by people who won't be paying their running costs and thus have little incentive to consider efficiency. Lovins also believes that many customers \"don't know what the best efficiency buys are, where to get them, or how to shop for them\".\n\nIn 1994, Amory Lovins developed the design concept of the Hypercar. This vehicle would have ultra-light construction with an aerodynamic body using advanced composite materials, low-drag design, and hybrid drive. Designers of the Hypercar claim that it would achieve a three- to fivefold improvement in fuel economy, equal or better performance, safety, amenity, and affordability, compared with today's cars.\n\nIn 1999, RMI took this process a step further by launching a for-profit venture, Hypercar Inc. This independent company, in which RMI has a minority interest, is now taking the lead in advancing key areas of Hypercar research and development. In 2004, Hypercar Inc. changed its name to Fiberforge to better reflect the company's new goal of lowering the cost of high-volume advanced-composite structures by leveraging the patents of David F. Taggart, one of the founders of Hypercar, Inc.\n\nLovins says the commercialisation of the Hypercar began in 2014, with the production of the all-carbon electric BMW i3 family and the 313 miles per gallon Volkswagen XL1.\n\nLovins does not see his energy ideas as green or left-wing, and he is an advocate of private enterprise and free market economics. He notes that Rupert Murdoch has made News Corporation carbon-neutral, with savings of millions of dollars. But, says Lovins, large institutions are becoming more \"gridlocked and moribund\", and he supports the rise of \"citizen organizations\" around the world.\n\nPaul Hawken's \"Blessed Unrest\" chronicles the rise of millions of non-profit citizen organizations around the world — the greatest social movement in history. As central institutions become more gridlocked and moribund, a new vitality is beginning to spread renewal through the stem to the flower.\n\nInstitutions and energy specialists have criticized various positions taken by Amory Lovins. One of the main points of contention is the assumption by the RMI of a linear relation between improvements in energy efficiency and reductions in aggregate energy consumption. The Jevons Paradox suggests that improvements in energy efficiency actually lead to an increase in energy use, as a result of decreasing cost. This \"rebound effect\" is downplayed in the analyses performed by Lovins. \nOther assumptions made by Lovins have also received criticism. For example, in Lovins' book, Reinventing Fire, it is assumed that 50% of all electricity in the US could come from wind in 2050. Other authors find that this is capped probably around 30%.\nSimilar overestimated are identified in PV (solar), where estimates are made for about 30%; this is seen as implausible. Moreover, according to the authors, no analyses are given about the need for huge volumes of electricity storage, which would be needed when the sun doesn't shine and the wind doesn't blow. \nBut these are always deliberate deceptions by those who don't want the fact know that the same powerlines dispatching/distributing electricity can shunt the same from interment areas it is being produced in surplus to those areas It isn't being produced in at any given time. For instance-Solar being shifted across time zones- primary East to West,and wind power from turbines in windy areas to those who have idle wind turbines at that time.\nThus does Distributed generation tend too win out,once minimum feeder areas are given surplus generation capacity-Particularly from diverse renewable generating sources.\nThis was modelled in the case of Wind Power by Buckminister Fuller in the early 1970's. See:His intro to- Earth Energy and Everyone M. Gabel editor.\nOther essential loads that need to be \"stored' can be dealt with inherently and passively such as heat(and cooling) loads by Passive Annual Heat Storage(see-J. Hait) or not so passively by distributed facilities over a large scale range. Not requiring massive capital expenditure and monstrous monopolies with public charter and subsidy. These also will Not be brought up by proliferationphile Perception Management Popes.\n\nAmory Lovins has received ten honorary doctorates and was elected a Fellow of the American Association for the Advancement of Science in 1984, of the World Academy of Art and Science in 1988, and of the World Business Academy in 2001. He has received the Right Livelihood Award, the Blue Planet Prize, Volvo Environment Prize, the 4th Annual Heinz Award in the Environment in 1998, and the National Design (Design Mind), Jean Meyer, and Lindbergh Awards.\n\nLovins is also the recipient of the \"Time\" Hero for the Planet awards, the Benjamin Franklin and Happold Medals, and the Shingo, Nissan, Mitchell, and Onassis Prizes. He has also received a MacArthur Fellowship and is an honorary member of the American Institute of Architects (AIA), a Foreign Member of the Royal Swedish Academy of Engineering Sciences, and an Honorary Senior Fellow of the Design Futures Council. Furthermore, he is on the Advisory Board of the Holcim Foundation.\n\nIn 2009, \"Time\" magazine named Lovins as one of the world's 100 most influential people.\n\nOn 17 March 2016, Lovins received the Bundesverdienstkreuz 1. Klasse (Officer's Cross of the Order of Merit) from the Federal Republic of Germany for intellectually underpinning Germany's Energiewende, most notably with his concept of \"soft energy\" and how that promotes peace and prosperity.\n\nIn 1979 Amory Lovins married L. Hunter Sheldon, a lawyer, forester, and social scientist. Hunter received her undergraduate degree in sociology and political studies from Pitzer College, and her J.D. from Loyola University's School of Law. They separated in 1989 and divorced in 1999. In 2007, he married Judy Hill Lovins, a fine-art landscape photographer.\n\nLovins is the brother of Julie Beth Lovins, a computational linguist who wrote the first stemming algorithm for word matching.\n\nThis is a list of books which are authored or co-authored by Amory B. Lovins, or which include a foreword by him:\n\nNon-English\n\n"}
{"id": "46587302", "url": "https://en.wikipedia.org/wiki?curid=46587302", "title": "Angove Conservation Park", "text": "Angove Conservation Park\n\nAngove Conservation Park is a protected area located about north-east of the Adelaide city centre within the local government area of the City of Tea Tree Gully. The conservation park was proclaimed under the \"National Parks and Wildlife Act 1972\" in 1994 in order to protect a parcel of undeveloped land which contained remnant native vegetation. The conservation park is classified as an IUCN Category III protected area.\n\n\n"}
{"id": "38065067", "url": "https://en.wikipedia.org/wiki?curid=38065067", "title": "Awful Splendour: A Fire History of Canada", "text": "Awful Splendour: A Fire History of Canada\n\nAwful Splendour: A Fire History of Canada is a 2007 non-fiction book by American environmental historian Stephen J. Pyne. It examines the natural, social and political history of forest fires in Canada.\n\nAuthor Pyne has a long relationship with forest fires; the Arizona State University professor was a wildland firefighter on the North Rim of the Grand Canyon for fifteen years, as well as a member of the United Nations Wildfire Advisory Group. The book is part of a series on forest fires by Pyne. In his \"Circle of Fire\" series, he has covered forest fires in the U.S., Australia and on a global scale. The Canadian Forest Service encouraged Pyne to tackle the topic of Canadian forest fires. The phrase \"awful splendour\" was taken from a quote from early Canadian naturalist Henry Youle Hind, referring to the destructive beauty of prairie fires.\n\nThe book is divided into three sections, titled \"Torch\", \"Axe\", and \"Engine\", roughly corresponding to the pre-contact, exploration, and industrial periods of Canadian history. In its frequent mentions of American experiences with fire, the book engages in some comparative history. Pyne defines several geographical \"rings\" of fire in Canada, including the boreal forests, the coastal forests of the Pacific and Atlantic, the mountain forests of British Columbia and Alberta, and the mixed wood forests of the Prairies, Ontario, and Quebec.\n\n\"Canadian Literature\" noted that the book \"filled a gaping hole\" in Canadian scholarly writing on forest fires, and credited Pyne for accessing \"grey literature\" in hard to find locations. Reviewer David Brownstein called the book a \"marvellously encyclopedic synthesis of a vast secondary literature on a complex topic.\" Reviewed in \"BC Studies\", Philip Van Huizen praised the \"elegant and evocative\" writing of the author, as well as his use of narrative. The reviewer critiqued some of Pyne's organisational choices; by looking at fire management province-by-province, the third section of the book has some redundancies and can be \"a chore to read (at least in places).\" In all, Van Huizen called the work a \"formidable and impressive book\".\n"}
{"id": "4687796", "url": "https://en.wikipedia.org/wiki?curid=4687796", "title": "Baseflow", "text": "Baseflow\n\nBaseflow is the portion of the streamflow that is sustained between precipitation events, fed to streams by delayed pathways. Baseflow (also called drought flow, groundwater recession flow, low flow, low-water flow, low-water discharge and sustained or fair-weather runoff) is the portion of streamflow delayed shallow subsurface flow\". It should not be confused with groundwater flow. Fair weather flow is called as Base flow.\n\nBaseflow is important for sustaining human centers of population and ecosystems. This is especially true for watersheds that do not rely on snowmelt. Different ecological processes will occur at different parts of the hydrograph. During the baseflow ascending limb there is frequently more stream area and habitat available for water dependent species, spawning salmon for example. During the recession limb which in California is from May to October there is increasingly less stream area, indigenous species are more adapt at surviving in low flow conditions than introduced species.\n\nBaseflow is derived from bedrock water storage near surface valley soils and riparian zones. Water percolates to groundwater and then flows to a body of water Baseflow depletion curve is the declining of baseflow/groundwater and soil reserves. The volume and rate of water moving as baseflow can be affected by macropores, micropores, and other fractured conditions in the soil and shallow geomorphic features. Infiltration to recharge subsurface storage increases baseflow. Evapotranspiration reduces baseflow because trees absorb water from the ground. In the fall baseflow can increase before it starts to rain because the trees drop their leaves and stop drinking as much water. River incision can decrease the baseflow by lowering the water table and aquifer.\n\nGood baseflow is connected to surface water that is located in permeable, soluble, or highly fractured bedrock. Bad baseflow is in crystalline or massive bedrock with minor fracturing and doesn’t store water. Losing reaches is when the water flow decreases as it travels downstream and is fracturing deeper than surface water or in karst geology because limestone and dolomite high storage. Gaining reaches is when flow increases as it travels downstream. Gaining reaches are common in humid mountainous regions where the water table is above the surface water and the water flows from high head to low head following Darcy’s Law.\n\nMethods for identifying baseflow sources and residence/transit time include using solutes and tracers. Solutes that originate in distinct areas of the watershed can be used to source baseflow-geochemical signatures. Tracers may be inserted into different parts of the watershed to identify flow paths and transit times.\n\nMethods for summarizing baseflow from an existing streamflow record include event based low flow statistics, flow duration curve, metrics that explain proportioning of baseflow to total flow, and the baseflow recession curve which can be used on ungauged streams based on empirical relationship between watershed characteristics and baseflow at gauged sites.\n\nCertain parameters of baseflow, such as the mean residence time and the baseflow recession curve, can be useful in describing the mixing of waters (such as from precipitation and groundwater) and the level of groundwater contribution to streamflow in catchments.\n\nBaseflow separation is often used to determine what portion of a streamflow hydrograph occurs from baseflow, and what portion occurs from overland flow. Common methods include using isotope tracing and the software program HYSEP, among others.\n\nAnthropogenic effects to baseflow include forestry, urbanization, and agriculture. Forest cover has high infiltration and recharge because of tree roots. Removal of forest cover can have short term increase in mean flow and baseflow because less interception and evapotranspiration. Urbanization includes a re-organization of surface and subsurface pathways so that water is flushed through catchments because of reduced hydraulic resistance, Manning’s n, channels and impervious surfaces which decreases infiltration. In urban areas water is often imported from outside the watershed from deep wells and reservoirs. The pipes that transport the water often leak 20-25% to the subsurface which can actually increase baseflow. Agriculture c≈an lower baseflow if water diverted from stream for irrigation, or can raise baseflow if water is used from a different watershed. Pastures can increase compaction and reduce organic matter with reduces infiltration and baseflow.\n\n"}
{"id": "24007866", "url": "https://en.wikipedia.org/wiki?curid=24007866", "title": "Chlormayenite", "text": "Chlormayenite\n\nChlormayenite (after Mayen, Germany), CaAlO[☐Cl], is a rare calcium aluminium oxide mineral of cubic symmetry. \n\nIt was originally reported from Eifel volcanic complex (Germany) in 1964. It is also found at pyrometamorphic sites such as in the Hatrurim Formation of Israel and in some burned coal dumps.\n\nIt occurs in thermally altered limestone xenoliths within basalts in Mayen, Germany and Klöch, Styria, Austria. In the Hatrurim of Israel it occurs in thermally altered limestones. It occurs with calcite, ettringite, wollastonite, larnite, brownmillerite, gehlenite, diopside, pyrrhotite, grossular, spinel, afwillite, jennite, portlandite, jasmundite, melilite, kalsilite and corundum in the limestone xenoliths. In the Hatrurim it occurs with spurrite, larnite, grossite and brownmillerite.\n\nSynthetic CaAlO and CaAlO(OH) are known, they are stabilized by moisture instead of chlorine. The formula can be written as [CaAlO]O, which refers to the unique feature: anion diffusion process.\n\nChlormayenite is also found as calcium aluminate in cement where its formula is also written as 11CaO·7 AlO·CaCl, or CACaCl in the cement chemist notation.\n\n"}
{"id": "7541598", "url": "https://en.wikipedia.org/wiki?curid=7541598", "title": "Cryoseism", "text": "Cryoseism\n\nA cryoseism, also known as an ice quake or a frost quake, is a seismic event that may be caused by a sudden cracking action in frozen soil or rock saturated with water or ice. As water drains into the ground, it may eventually freeze and expand under colder temperatures, putting stress on its surroundings. This stress builds up until relieved explosively in the form of a cryoseism.\n\nAnother type of cryoseism is a non-tectonic seismic event caused by sudden glacial movements. This movement has been attributed to a veneer of water which may pool underneath a glacier sourced from surface ice melt. Hydraulic pressure of the liquid can act as a lubricant, allowing the glacier to suddenly shift position. This type of cryoseism can be very brief, or may last for several minutes.\n\nThe requirements for a cryoseism to occur are numerous; therefore, accurate predictions are not entirely possible and may constitute a factor in structural design and engineering when constructing in an area historically known for such events. Speculation has been made between global warming and the frequency of cryoseisms.\n\nCryoseisms are often mistaken for minor intraplate earthquakes. Initial indications may appear similar to those of an earthquake with tremors, vibrations, ground cracking and related noises, such as thundering or booming sounds. Cryoseisms can, however, be distinguished from earthquakes through meteorological and geological conditions. Cryoseisms can have an intensity of up to VI on the Modified Mercalli Scale. Furthermore, cryoseisms often exhibit high intensity in a very localized area, in the immediate proximity of the epicenter, as compared to the widespread effects of an earthquake. Due to lower-frequency vibrations of cryoseisms, some seismic monitoring stations may not record their occurrence. Although cryoseisms release less energy than most tectonic events, they can still cause damage or significant changes to an affected area.\n\nSome reports have indicated the presence of \"distant flashing lights\" before or during a cryoseism, possibly because of electrical changes when rocks are compressed. Cracks and fissures may also appear as surface areas contract and split apart from the cold. The sometime superficial to moderate occurrences may range from a few centimeters to several kilometers long, with either singular or multiple linear fracturing and vertical or lateral displacement possible.\n\nGeocryological processes were identified as a possible cause of tremors as early as 1818. In the United States, such events have been reported throughout the Midwestern, Northern and Northeastern United States.\n\nCryoseisms also occur in Canada, especially along the Great Lakes/St. Lawrence corridor, where winter temperatures can shift very rapidly. They have surfaced in Ontario, Quebec and the Maritime Provinces. They are also observed in Calgary.\n\nGlacier-related cryoseism phenomena have been reported in Alaska, Greenland, Iceland (Grímsvötn)., Ross Island, and the Antarctic Prince Charles Mountains.\n\nSouthern Ontario experienced numerous, albeit unverified, frost quakes on December 24, 2013, and more severe, waking a great many from sleep, on December 30, 2013 at 3:00 AM and later at 7:00 AM, as a result of the December 2013 North American storm complex. The towns of Cornwall, Middlebury, Ripton, and Weybridge, Vermont experienced the same phenomenon after the same weather conditions with the loudest occurring around 4:50am on December 25. Essex County in New York experienced cryoseisms as late as December 31, 2013. Numerous frost quakes again occurred in the Greater Toronto Area around 2:00 AM January 3, 2014, and January 7, 2014 overnight. Many cryoseisms were also reported near Lac-Mégantic, in the Eastern Townships (Québec, Canada), in the last days of December 2013. Overall, there were over 2000 reports in Canada and over 260 in the United States of possible frostquakes from January to February 2014. Most of these reports came from Greater Toronto Area and around Green Lake, Wisconsin area.\n\nOn March 4, 2014 the city of Calgary experienced a loud sound at 5:00 PM and many people are attributing this to cryoseism.\n\nThe West Island of Montreal experienced an ice quake at approximately 2:30 AM EST on January 6, 2015.\n\nWichita, Kansas reportedly experienced a loud boom on Saturday, January 17, 2016. It was the first recorded ice quake in Kansas.\n\nDuring a January 2016 record cold spell several unusual loud bangs and tremors were reported around Finland, which has a very low baseline of seismological activity. One of the events was registered seismologically and deemed to be a frost quake.\n\nFebruary 9, 2017 – Barrie to Penetanguishene, ON area reporting this phenomenon.\n\nJanuary 13-15, 2018: Numerous reports in central Indiana due to sub-zero temperatures immediately following rain\n\nThere are four main precursors for a frost quake cryoseism event to occur: (1) a region must be susceptible to cold air masses, (2) the ground must undergo saturation from thaw or liquid precipitation prior to an intruding cold air mass, (3) most frost quakes are associated with minor snow cover on the ground without a significant amount of snow to insulate the ground (i.e., less than 6 inches), and (4) a rapid temperature drop from approximately freezing to near or below zero degrees Fahrenheit, which ordinarily occurred on a timescale of 16 to 48 hours.\n\nCryoseisms typically occur when temperatures rapidly decrease from above freezing to subzero, and are more than likely to occur between midnight and dawn (during the coldest parts of night). [However, due to the permanent nature of glacial ice, glacier-related cryoseisms may also occur in the warmer months of summer.] In general, cryoseisms may occur 3 to 4 hours after significant changes in temperature. Perennial or seasonal frost conditions involved with cryoseisms limit these events to temperate climates that experience seasonal variation with subzero winters. Additionally, the ground must be saturated with water, which can be caused by snowmelt, rain, sleet or flooding. Geologically, areas of permeable materials like sand or gravel, which are susceptible to frost action, are likelier candidates for cryoseisms. Following large cryoseisms, little to no seismic activity will be detected for several hours, indicating that accumulated stress has been relieved.\n\n\n"}
{"id": "4415356", "url": "https://en.wikipedia.org/wiki?curid=4415356", "title": "Debris flow", "text": "Debris flow\n\nDebris flows are geological phenomena in which water-laden masses of soil and fragmented rock rush down mountainsides, funnel into stream channels, entrain objects in their paths, and form thick, muddy deposits on valley floors. They generally have bulk densities comparable to those of rock avalanches and other types of landslides (roughly 2000 kilograms per cubic meter), but owing to widespread sediment liquefaction caused by high pore-fluid pressures, they can flow almost as fluidly as water. Debris flows descending steep channels commonly attain speeds that surpass 10 m/s (36 km/h), although some large flows can reach speeds that are much greater. Debris flows with volumes ranging up to about 100,000 cubic meters occur frequently in mountainous regions worldwide. The largest prehistoric flows have had volumes exceeding 1 billion cubic meters (i.e., 1 cubic kilometer). As a result of their high sediment concentrations and mobility, debris flows can be very destructive.\n\nNotable debris-flow disasters of the twentieth century involved more than 20,000 fatalities in Armero, Colombia in 1985 and tens of thousands in Vargas State, Venezuela in 1999.\n\nDebris flows have volumetric sediment concentrations exceeding about 40 to 50%, and the remainder of a flow's volume consists of water. By definition, “debris” includes sediment grains with diverse shapes and sizes, commonly ranging from microscopic clay particles to great boulders. Media reports often use the term mudflow to describe debris flows, but true mudflows are composed mostly of grains smaller than sand. On Earth's land surface, mudflows are far less common than debris flows. However, underwater mudflows are prevalent on submarine continental margins, where they may spawn turbidity currents. Debris flows in forested regions can contain large quantities of woody debris such as logs and tree stumps. Sediment-rich water floods with solid concentrations ranging from about 10 to 40% behave somewhat differently from debris flows and are known as hyperconcentrated floods. Normal stream flows contain even lower concentrations of sediment.\n\nDebris flows can be triggered by intense rainfall or snowmelt, by dam-break or glacial outburst floods, or by landsliding that may or may not be associated with intense rain. In all cases the chief conditions required for debris flow initiation include the presence of slopes steeper than about 25 degrees, the availability of abundant loose sediment, soil, or weathered rock, and sufficient water to bring this loose material to a state of almost complete saturation. Debris flows can be more frequent following forest and brush fires, as experience in southern California demonstrates. They pose a significant hazard in many steep, mountainous areas, and have received particular attention in Japan, China, Taiwan, USA, Canada, New Zealand, the Philippines, the European Alps, Russia, and Kazakhstan. In Japan a large debris flow or landslide is called \"yamatsunami\" (), literally \"mountain tsunami\".\nDebris flows are accelerated downhill by gravity and tend to follow steep mountain channels that debouche onto alluvial fans or floodplains. The front, or 'head' of a debris-flow surge often contains an abundance of coarse material such as boulders and logs that impart a great deal of friction. Trailing behind the high-friction flow head is a lower-friction, mostly liquefied flow body that contains a higher percentage of sand, silt and clay. These fine sediments help retain high pore-fluid pressures that enhance debris-flow mobility. In some cases the flow body is followed by a more watery tail that transitions into a hyperconcentrated stream flow. Debris flows tend to move in a series of pulses, or discrete surges, wherein each pulse or surge has a distinctive head, body and tail. \n\nDebris-flow deposits are readily recognizable in the field. They make up significant percentages of many alluvial fans and debris cones along steep mountain fronts. Fully exposed deposits commonly have lobate forms with boulder-rich snouts, and the lateral margins of debris-flow deposits and paths are commonly marked by the presence of boulder-rich lateral levees. These natural levees form when relatively mobile, liquefied, fine-grained debris in the body of debris flows shoulders aside coarse, high-friction debris that collects in debris-flow heads as a consequence of grain-size segregation (a familiar phenomenon in granular mechanics). Lateral levees can confine the paths of ensuing debris flows, and the presence of older levees provides some idea of the magnitudes of previous debris flows in a particular area. Through dating of trees growing on such deposits, the approximate frequency of destructive debris flows can be estimated. This is important information for land development in areas where debris flows are common. Ancient debris-flow deposits that are exposed only in outcrops are more difficult to recognize, but are commonly typified by juxtaposition of grains with greatly differing shapes and sizes. This poor sorting of sediment grains distinguishes debris-flow deposits from most water-laid sediments.\n\nOther geological flows that can be described as debris flows are typically given more specific names. These include:\n\nA lahar is a debris flow related in some way to volcanic activity, either directly as a result of an eruption, or indirectly by the collapse of loose material on the flanks of a volcano. A variety of phenomena may trigger a lahar, including melting of glacial ice, intense rainfall on loose pyroclastic material, or the outburst of a lake that was previously dammed by pyroclastic or glacial sediments. The word lahar is of Indonesian origin, but is now routinely used by geologists worldwide to describe volcanogenic debris flows. Nearly all of Earth's largest, most destructive debris flows are lahars that originate on volcanoes. An example is the lahar that inundated the city of Armero, Colombia.\n\nA jökulhlaup is a glacial outburst flood. Jökulhlaup is an Icelandic word, and in Iceland many glacial outburst floods are triggered by sub-glacial volcanic eruptions. (Iceland sits atop the Mid-Atlantic Ridge, which is formed by a chain of mostly submarine volcanoes). Elsewhere, a more common cause of jökulhlaups is the breaching of ice-dammed or moraine-dammed lakes. Such breaching events are often caused by the sudden calving of glacier ice into a lake, which then causes a displacement wave to breach a moraine or ice dam. Downvalley of the breach point, a jökulhlaup may increase greatly in size through entrainment of loose sediment from the valley through which it travels. Ample entrainment can enable the flood to transform to a debris flow. Travel distances may exceed 100 km.\n\nNumerous different approaches have been used to model debris-flow properties, kinematics, and dynamics. Some are listed here.\n\n\nThe mixture theory, originally proposed by Iverson and later adopted and modified by others, treats debris flows as two-phase solid-fluid mixtures.\n\nIn real two-phase (debris) mass flows there exists a strong coupling between the solid and the fluid momentum transfer, where the solid's normal stress is reduced by buoyancy, which in turn diminishes the frictional resistance, enhances the pressure gradient, and reduces the drag on the solid component. Buoyancy is an important aspect of two-phase debris flow, because it enhances flow mobility (longer travel distances) by reducing the frictional resistance in the mixture. Buoyancy is present as long as there is fluid in the mixture. It reduces the solid normal stress, solid lateral normal stresses, and the basal shear stress (thus, frictional resistance) by a factor (formula_1), where formula_2 is the density ratio between the fluid and the solid phases. The effect is substantial when the density ratio (formula_2) is large (e.g., in the natural debris flow).\n\nIf the flow is neutrally buoyant, i.e., formula_4, (see, e.g., Bagnold, 1954) the debris mass is fluidized and moves longer travel distances. This can happen in highly viscous natural debris flows. For neutrally buoyant flows, Coulomb friction disappears, the lateral solid pressure gradient vanishes, the drag coefficient is zero, and the basal slope effect on the solid phase also vanishes. In this limiting case, the only remaining solid force is due to gravity, and thus the force associated with buoyancy.\nUnder these conditions of hydrodynamic support of the particles by the fluid, the debris mass is fully fluidized (or lubricated) and moves very economically, promoting long travel distances. Compared to buoyant flow, the neutrally buoyant flow shows completely different behaviour. For the latter case, the solid and fluid phases move together, the debris bulk mass is fluidized, the front moves substantially farther, the tail lags behind, and the overall flow height is also reduced. When formula_5, the flow does not experience any buoyancy effect. Then the effective frictional shear stress for the solid phase is that of pure granular flow. In this case the force due to the pressure gradient is altered, the drag is high and the effect of the virtual mass disappears in the solid momentum. All this leads to slowing down the motion.\n\nIn order to prevent debris flows reaching property and people, a debris basin may be constructed. Debris basins are designed to protect soil and water resources or to prevent downstream damage. Such constructions are considered to be a last resort because they are expensive to construct and require commitment to annual maintenance.\n\nIn 1989, as part of his large-scale piece \"David Gordon's United States\", and later, in 1999, as part of \"Autobiography of a Liar\", choreographer David Gordon brought together the music of Harry Partch and the words of John McPhee from \"The Control of Nature\", read by Norma Fire, in a dance titled \"Debris Flow\", a \"harrowing taped narrative of a family's ordeal in a massive L.A. mudslide...\"\n\n\nNotes\nFurther reading\n\n"}
{"id": "1587573", "url": "https://en.wikipedia.org/wiki?curid=1587573", "title": "Desizing", "text": "Desizing\n\nDesizing is the process of removing the size material from warp yarns after a textile fabric is woven.\n\nSizing agents are selected on the basis of type of fabric, environmental friendliness, ease of removal, cost considerations, effluent treatment, etc.\n\nNatural sizing agents are based on natural substances and their derivatives:\n\n\n\nDesizing, irrespective of what the desizing agent is, involves impregnation of the fabric with the desizing agent, allowing the desizing agent to degrade or solubilise the size material, and finally to wash out the degradation products. The major desizing processes are:\n\n\nEnzymatic desizing is the classical desizing process of degrading starch size on cotton fabrics using enzymes. Enzymes are complex organic, soluble bio-catalysts, formed by living organisms, that catalyze chemical reaction in biological processes. Enzymes are quite specific in their action on a particular substance. A small quantity of enzyme is able to decompose a large quantity of the substance it acts upon. Enzymes are usually named by the kind of substance degraded in the reaction it catalyzes.\n\nAmylases are the enzymes that hydrolyses and reduce the molecular weight of amylose and amylopectin molecules in starch, rendering it water-soluble enough to be washed off the fabric.\n\nEffective enzymatic desizing require strict control of pH, temperature, water hardness, electrolyte addition and choice of surfactant.\n\nIn oxidative desizing, the risk of damage to the cellulose fiber is very high, and its use for desizing is increasingly rare. Oxidative desizing uses potassium or sodium persulfate or sodium bromite as an oxidizing agent.\n\nCold solutions of dilute sulphuric or hydrochloric acids are used to hydrolyze the starch, however, this has the disadvantage of also affecting the cellulose fiber in cotton fabrics.\n\nFabrics containing water-soluble sizes can be desized by washing using hot water, perhaps containing wetting agents (surfactants) and a mild alkali. The water replaces the size on the outer surface of the fiber, and absorbs within the fiber to remove any fabric residue.\n\nFermentative desizing is defined as a fermentation process and involves the Generally Regarded as Safe (GRAS) microorganisms that have a high potential to produce enzymes; it is carried out via impregnation/padding methods, which provide online monitoring and accurate control. The method allows an economical process with low resource consumption and emission compared to the enzymatic method, it is considerably cheaper.\n\nDesizing 2000 is a reliable and simple combined process of desizing and demineralization for cotton. Desizing 2000 is a novel desizing technique.\n\n"}
{"id": "21091534", "url": "https://en.wikipedia.org/wiki?curid=21091534", "title": "ECHAM", "text": "ECHAM\n\nECHAM is a general circulation model (GCM) developed by the Max Planck Institute for Meteorology, one of the research organisations of the Max Planck Society. It was created by modifying global forecast models developed by ECMWF to be used for climate research. The model was given its name as a combination of its origin (the 'EC' being short for 'ECMWF') and the place of development of its parameterisation package, Hamburg. The default configuration of the model resolves the atmosphere up to 10 hPa (primarily used to study the lower atmosphere), but it can be reconfigured to 0.01 hPa for use in studying the stratosphere and lower mesosphere.\n\nDifferent versions of ECHAM, primarily different configurations of ECHAM5, have been the basis of many publications, listed on the ECHAM5 website .\n\nCompared to its predecessor, ECHAM4, it is more portable and flexible (it is now written in the programming language Fortran 95), and because of both major and minor changes to the different parts of code that it uses, it produces a significantly different simulated climate.\n\nMPI-ECHAM5 was recently used in the IPCC Fourth Assessment Report, alongside many other GCMs from different countries. In the data of this report, it is referred to with the abbreviation MPEH5.\n\nIt appears to be one of the more accurate GCMs.\n\nECHAM6 is currently the most advanced version of the ECHAM models. ECHAM6 is an atmospheric general circulation model, and as such focuses on the coupling between diabatic processes and large-scale circulations, both of which are ultimately driven by radiative forcing. It consists of a dry spectral-transform dynamical core, a transport model for scalar quantities other than temperature and surface pressure, a suite of physical parameterizations for the representation of diabatic processes, as well as boundary data sets for externalized parameters, such as trace gas and aerosol distributions, tabulations of gas absorption optical properties, temporal variations in spectral solar irradiance, land-surface properties, etc.\n\nThe major changes relative to ECHAM5 include: An improved representation of radiative transfer in the shortwave (or solar) part of the spectrum; a completely new description of aerosols; an improved representation of surface albedo, including the treatment of melt-ponds on sea ice; and a greatly improved representation of the middle-atmosphere as part of the standard model. In addition, minor changes have been made in the representation of convective processes, and through the choice of a slightly different vertical discretization within the troposphere, as well as changed model parameters.\n"}
{"id": "5076347", "url": "https://en.wikipedia.org/wiki?curid=5076347", "title": "Energy and Environmental Security Initiative", "text": "Energy and Environmental Security Initiative\n\nEstablished in 2003, the Energy and Environmental Security Initiative (EESI) is an interdisciplinary Research & Policy Institute located at the University of Colorado Law School. The fundamental mission of EESI is to serve as an interdisciplinary research and policy center concerning the development and crafting of State policies, U.S. energy policies, and global responses to the world's energy crisis; and to facilitate the attainment of a global sustainable energy future through the innovative use of laws, policies and technology solutions. In pursuit of this mission, EESI's primary operational function is that of an enabling environment for teaching, research and policy analysis vis-à-vis the impact of laws and policies on the scientific, technological, sociopolitical, commercial, and environmental dimensions of sustainable energy.\n\nISEA is a comprehensive global database of international treaties dealing with, or relevant to, the following energy categories, \"inter alia\": conventional sources of energy such as oil, natural gas, and coal; renewable energy, such as wind, biomass, solar, geothermal, and hydro; energy efficiency and energy conservation; nuclear power; carbon capture and sequestration; and transportation. ISEA\n\nIPECC is designed to improve and enhance the efforts of governments, non-governmental actors—such as corporations, non-governmental organizations, trade unions, churches—and key decision-makers throughout the world in two ways: First by evaluating the extent to which their existing commitments and pledges are actually working; and second, by facilitating new and better clean and affordable energy solutions. IPECC will provide the information needed to improve the effectiveness of existing commitments and encourage new commitments where necessary. It is designed to track and monitor the implementation of sustainable energy commitments undertaken by governments, corporations and other entities, and to provide detailed information on the extent to which they are being complied with. In doing so it will serve as a watchdog over what is and should be happening with respect to these instruments and the commitments they embody. IPECC\n\nCAD contains U.S. policy proposals directed at climate change. CAD Primary document types contained in CAD include:\n\nExcluded from CAD proposals are documents that: (1) are older than two years old (defined as generated prior to January 1, 2005), with certain exceptions; (2) are directed at international activities or policies, unless such activities or policies are to be implemented by the President or executive administrative entities; (3) are directed at a U.S. state other than California, or a regional collaboration other than RGGI; (4) that deal with management of the federal transportation fleet or federal buildings. CAD is not, at present, a living database—meaning that it is not intended to offer an up-to-the moment repository for U.S. federal climate proposals. The material contained in CAD is generally current through February 2007.\n\nEESI is located within the Wolf Law Building, which houses the University of Colorado Law School. Situated on the southern end of the University of Colorado at Boulder campus, the Wolf Law Building was completed in August 2006 and dedicated on September 8. Approximately 60 percent of the Wolf Law Building was financed by University of Colorado at Boulder students. Under the Leadership in Energy and Environmental Design (LEED) Green Building Rating System, the Wolf Law Building has received Gold certification. It is the first publicly supported law school in the country to obtain Gold.\n\nAs the parent organization of EESI, the University of Colorado Law School is well known for its strength in the area of environmental law. The U.S. News & World Report's 2008 edition of America's Best Graduate Schools (reporting on 2005-6 academic-year data) ranks the law school's environmental law program as 4th in the United States.\n\n\n\n"}
{"id": "30803281", "url": "https://en.wikipedia.org/wiki?curid=30803281", "title": "Energy in Belize", "text": "Energy in Belize\n\nEnergy in Belize is based on four main sources: imported fossil fuels, biomass, hydro, and imported electricity.\n\nBelize currently imports 100% of its fossil fuel use (Launchpad Consulting, 2003). Under the San Jose Pact, which Belize signed onto in 1988, Mexico and Venezuela signed a treaty obligating them to offer a concessionary credit from 20-25% of the purchase price of their oil exports. In 1991, both countries increased the oil supply offered under this agreement (Belize – Mining and Energy Industry, 1992). However, fuel prices had jumped to a record high in 2008, over $10 per gallon (Armageddon, 2008). \n\nSince 1981, extensive drilling has occurred throughout Belize to find oil deposits. Near Spanish Lookout and northwest of Belmopan, two oil fields were located. Drilling in these two fields began in 2005 and was predicted to reach a peak by 2010 (Belize, 2010). \n\nSustainable energy is the main goal for Belize. In 2003, the Public Utilities Commission implemented a one-year project entitled Formulation for a National Energy Plan for Belize. The project, funded by the United Nations Development Fund, developed a comprehensive National Energy Policy to promote environmentally sound, safe, reliable, affordable energy (National Energy Plan, 2001). \nIn 2011 this plan was updated with the Framework for the National Energy Policy. The Ministry of Energy, Science & Technology, and Public Utilities was founded following recommendations from the framework. \n\nBelize has also taken a role in reducing greenhouse gas emissions. The Belize and Nicaragua Logs Recovery project aims to reduce greenhouse gas emissions and prevent deforestation by salvaging mahogany and other logs in the Belize and Nicaragua Rivers (Legace and Legault International Inc., 2007). \n\nAlso, in compliance with the United Nations’ program REDD, Reduction of Emissions from Deforestation and forest Degradation, a national workshop performing with the Ministry of Natural Resources and Environment and the Forest Department of Belize are coordinating efforts to forest management and reduction of deforestation (Protecting Belize and other Central American Countries – Reducing Emissions: REDD, 2010). Along with signing the San Jose Pact, Belize has also been a participant to the Kyoto Protocol.\n\n"}
{"id": "9250002", "url": "https://en.wikipedia.org/wiki?curid=9250002", "title": "Energy planning", "text": "Energy planning\n\nEnergy planning has a number of different meanings. However, one common meaning of the term is the process of developing long-range policies to help guide the future of a local, national, regional or even the global energy system. Energy planning is often conducted within Governmental organizations but may also be carried out by large energy companies such as electric utilities or oil and gas producers. Energy planning may be carried out with input from different stakeholders drawn from government agencies, local utilities, academia and other interest groups.\n\nEnergy planning is often conducted using integrated approaches that consider both the provision of energy supplies and the role of energy efficiency in reducing demands (Integrated Resource Planning). Energy planning should always reflect the outcomes of population growth and economic development.\n\nEnergy planning has traditionally played a strong role in setting the framework for regulations in the energy sector (for example, influencing what type of power plants might be built or what prices were charged for fuels). But in the past two decades many countries have deregulated their energy systems so that the role of energy planning has been reduced, and decisions have increasingly been left to the market. This has arguably led to increased competition in the energy sector, although there is little evidence that this has translated into lower energy prices for consumers. Indeed, in some cases, deregulation has led to significant concentrations of \"market power\" with large very profitable companies having a large influence as price setters.\n\nApproaches to energy planning depends on the planning agent and the scope of the exercise. Several catch-phrases are associated with energy planning. Basic to all is resource planning, i.e. a view of the possible sources of energy in the future. A forking in methods is whether the planner considers the possibility of influencing the consumption (demand) for energy. The 1970s energy crisis ended a period of relatively stable energy prices and stable supply-demand relation. Concepts of Demand Side Management, Least Cost Planning and Integrated Resource Planning (IRP) emerged with new emphasis on the need to reduce energy demand by new technologies or simple energy saving.\n\nIn the United States the Public Utility Regulatory Policies Act of 1978 PURPA and more comprehensively the Energy Policy Act of 1992 introduced these concepts into the legal system, to be further detailed by individual states.\nFurther global integration of energy supply systems and local and global environmental limits amplifies the scope of planning both in subject and time perspective. Sustainable Energy Planning should consider environmental impacts of energy consumption and production, particularly in light of the threat of global climate change, which is caused largely by emissions of greenhouse gases from the world's energy systems, which is a long-term process.\n\nMany OECD countries and some U.S. states are now moving to more closely regulate their energy systems. For example, many countries and states have been adopting targets for emissions of CO and other greenhouse gases. In light of these developments, broad scope integrated energy planning could become increasingly important \n\nSustainable Energy Planning takes a more holistic approach to the problem of planning for future energy needs. It is based on a structured decision making process based on six key steps, namely:\n\n1. Exploration of the context of the current and future situation\n\n2. Formulation of particular problems and opportunities which need to be addressed as part of the Sustainable Energy Planning process. This could include such issues as \"Peak Oil\" or \"Economic Recession/Depression\", as well as the development of energy demand technologies.\n\n3. Create a range of models to predict the likely impact of different scenarios. This traditionally would consist of mathematical modelling but is evolving to include \"Soft System Methodologies\" such as focus groups, peer ethnographic research, \"what if\" logical scenarios etc.\n\n4. Based on the output from a wide range of modelling exercises and literature reviews, open forum discussion etc., the results are analysed and structured in an easily interpreted format.\n\n5. The results are then interpreted in order to determine the scope, scale and likely implementation methodologies which would be required to ensure successful implementation.\n\n6. This stage is a quality assurance process which actively interrogates each stage of the Sustainable Energy Planning process and checks if it has been carried out rigorously, without any bias and that it furthers the aims of sustainable development and does not act against them.\n\n7. The last stage of the process is to take action. This may consist of the development, publication and implementation of a range of policies, regulations, procedures or tasks which together will help to achieve the goals of the Sustainable Energy Plan.\n\nDesigning for implementation is often carried out using \"Logical Framework Analysis\" which interrogates a proposed project and checks that it is completely logical, that it has no fatal errors and that appropriate contingency arrangements have been put in place to ensure that the complete project will not fail if a particular strand of the project fails.\n\nSustainable energy planning is particularly appropriate for communities who want to develop their own energy security, while employing best available practice in their planning processes.\n\n"}
{"id": "13314634", "url": "https://en.wikipedia.org/wiki?curid=13314634", "title": "Fern flower", "text": "Fern flower\n\nThe fern flower is a magic flower in Baltic mythology (, ), in Estonian mythology () and in Slavic mythology (, , , ).\n\nAccording to the myth, this flower blooms for a very short time on the eve of the summer solstice (celebrated on June 21 or sometimes July 7) The flower brings fortune to the person who finds it. In various versions of the tale, the fern flower brings luck, wealth, or the ability to understand animal speech. However, the flower is closely guarded by evil spirits and anyone who finds the flower will have access to earthly riches, which have never benefited anyone, so the decision to pick the flower or leave it alone is left up to the individual.\n\nIn the Estonian, Lithuanian and Latvian tradition, the fern flower is supposed to appear only on the night of 23 to 24 June during the celebration of the summer solstice which is called Jāņi in Latvia, Joninės or Rasos in Lithuania, Jaaniõhtu or Jaaniöö in Estonia and juhannus in Finland. The celebration has pre-Christian origins. In addition to the idea that the finder of the fern flower will become rich or happy, here, the fern flower is sometimes perceived a symbol of fertility. During this supposedly magical night, young couples go into the woods \"seeking the fern flower\", which is most commonly read as a euphemism for sex. Sex can lead to pregnancy; the child could be thought of as the fern flower.\n\nReferring to this tradition, Papardes zieds (\"fern flower\" in Latvian) is the name of an NGO in Latvia that promotes education about matters pertaining to sexuality, fertility, and relationships.\n\nSimilar beliefs are attested in Sweden, where the fern flower was said to be found only at midnight on Midsummer's Eve, and even then was protected by magic and thus hard to obtain. This also applied to horsetail and daphne flowers; of daphne, a flowering plant, it was said: \"The flowers are a rarity if picked on nights when they are \"believed\" to bloom. The naturally occurring flowers no one believes to be daphne flowers.\"\n\nIn Ukraine, Belarus and Poland, the holiday is practiced on the eve of Ivan Kupala Day. Young girls wear wreaths in their hair and couples go into the woods searching for the fern flower. When they come out of the woods, if the male is wearing the girl's wreath, it means the couple is engaged to be married.\n\nAccording to folklore, the flower is Chervona Ruta. The flower is yellow, but according to legend, it turns red on the eve of Ivan Kupala Day.\n\nIn fact, ferns are not flowering plants. However some experts think that the flowering fern myth has roots in reality. In the past, the grouping of plants was not as exact as modern taxonomic ones. Numerous flowering plants resemble ferns, or have fern-like foliage, and some of them indeed open flowers during night time. Also, certain true ferns, e.g., \"Osmunda regalis\" have sporangia in tight clusters (termed \"fertile fronds\"), which may appear in flower-like clusters, and as a result, they are commonly known as \"flowering ferns\".\n\n"}
{"id": "2043467", "url": "https://en.wikipedia.org/wiki?curid=2043467", "title": "Heat recovery steam generator", "text": "Heat recovery steam generator\n\nA heat recovery steam generator (HRSG) is an energy recovery heat exchanger that recovers heat from a hot gas stream. It produces steam that can be used in a process (cogeneration) or used to drive a steam turbine (combined cycle).\n\nHRSGs consist of four major components: the economizer, evaporator, superheater and water preheater . The different components are put together to meet the operating requirements of the unit. See the attached illustration of a Modular HRSG General Arrangement.\n\nModular HRSGs can be categorized by a number of ways such as direction of exhaust gases flow or number of pressure levels. Based on the flow of exhaust gases, HRSGs are categorized into vertical and horizontal types. In horizontal type HRSGs, exhaust gas flows horizontally over vertical tubes whereas in vertical type HRSGs, exhaust gas flow vertically over horizontal tubes. Based on pressure levels, HRSGs can be categorized into single pressure and multi pressure. Single pressure HRSGs have only one steam drum and steam is generated at single pressure level whereas multi pressure HRSGs employ two (double pressure) or three (triple pressure) steam drums. As such triple pressure HRSGs consist of three sections: an LP (low pressure) section, a reheat/IP (intermediate pressure) section, and an HP (high pressure) section. Each section has a steam drum and an evaporator section where water is converted to steam. This steam then passes through superheaters to raise the temperature beyond the saturation point.\n\nPackaged HRSGs are designed to be shipped as a fully assembled unit from the factory. They can be used in waste heat or turbine (usually under 20 MW) applications. The packaged HRSG can have a water-cooled furnace, which allows for higher supplemental firing and better overall efficiency.\n\nSome HRSGs include supplemental, or duct firing. These additional burners provide additional energy to the HRSG, which produces more steam and hence increases the output of the steam turbine. Generally, duct firing provides electrical output at lower capital cost. It is therefore often utilized for peaking operations.\n\nHRSGs can also have diverter valves to regulate the inlet flow into the HRSG. This allows the gas turbine to continue to operate when there is no steam demand or if the HRSG needs to be taken offline.\n\nEmissions controls may also be located in the HRSG. Some may contain a Selective Catalytic Reduction system to reduce nitrogen oxides (a large contributor to the formation of smog and acid rain) and/or a catalyst to remove carbon monoxide. The inclusion of an SCR dramatically affects the layout of the HRSG. NOx catalyst performs best in temperatures between 650 °F (340 °C) and 750 °F (400 °C). This usually means that the evaporator section of the HRSG will have to be split and the SCR placed in between the two sections. Some low temperature NOx catalysts have recently come to market that allows for the SCR to be placed between the Evaporator and Economizer sections (350 °F - 500 °F (175 °C - 260 °C)).\n\nA specialized type of HRSG without boiler drums is the once-through steam generator. In this design, the inlet feedwater follows a continuous path without segmented sections for economizers, evaporators, and superheaters. This provides a high degree of flexibility as the sections are allowed to grow or contract based on the heat load being received from the gas turbine. The absence of drums allows for quick changes in steam production and fewer variables to control, and is ideal for cycling and base load operation. With proper material selection, an OTSG can be run dry, meaning the hot exhaust gases can pass over the tubes with no water flowing inside the tubes. This eliminates the need for a bypass stack and exhaust gas diverter system which is required to operate a combustion turbine with a drum-type HRSG out of service.\n\n\n\n"}
{"id": "20556903", "url": "https://en.wikipedia.org/wiki?curid=20556903", "title": "Higgs boson", "text": "Higgs boson\n\nThe Higgs boson is an elementary particle in the Standard Model of particle physics, produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. It is named after physicist Peter Higgs, who in 1964, along with five other scientists, proposed the mechanism, which suggested the existence of such a particle. Its existence was confirmed by the ATLAS and CMS collaborations based on collisions in the LHC at CERN.\n\nOn December 10, 2013, two of the physicists, Peter Higgs and François Englert, were awarded the Nobel Prize in Physics for their theoretical predictions. Although Higgs's name has come to be associated with this theory (the Higgs mechanism), several researchers between about 1960 and 1972 independently developed different parts of it.\n\nIn mainstream media the Higgs boson has often been called the \"God particle\", from , although the nickname is strongly disliked by many physicists, including Higgs himself, who regard it as sensationalistic.\n\nPhysicists explain the properties of and forces between elementary particles in terms of the Standard Model – a widely accepted framework for understanding almost everything in the known universe, other than gravity. (A separate theory, General Relativity, is used for gravity.) In this model, the fundamental forces in nature arise from properties of our universe called gauge invariance and symmetries. The forces are transmitted by particles known as gauge bosons.\n\nIn the Standard Model, the Higgs particle is a boson with spin zero, no electric charge and no colour charge. It is also very unstable, decaying into other particles almost immediately. The Higgs field is a scalar field, with two neutral and two electrically charged components that form a complex doublet of the weak isospin SU(2) symmetry. The Higgs field has a \"Mexican hat-shaped\" potential. In its ground state, this causes the field to have a nonzero value everywhere (including otherwise empty space), and as a result, below a very high energy it breaks the weak isospin symmetry of the electroweak interaction. (Technically the non-zero expectation value converts the Lagrangian's Yukawa coupling terms into mass terms.) When this happens, three components of the Higgs field are \"absorbed\" by the SU(2) and U(1) gauge bosons (the \"Higgs mechanism\") to become the longitudinal components of the now-massive W and Z bosons of the weak force. The remaining electrically neutral component either manifests as a Higgs particle, or may couple separately to other particles known as fermions (via Yukawa couplings), causing these to acquire mass as well.\n\nField theories had been used with great success in understanding the electromagnetic field and the strong force, but by around 1960 all attempts to create a \"gauge invariant\" theory for the weak force (and its combination with fundamental force electromagnetism, the electroweak interaction) had consistently failed, with gauge theories thereby starting to fall into disrepute as a result. The problem was that the symmetry requirements in gauge theory predicted that both electromagnetism's gauge boson (the photon) and the weak force's gauge bosons (W and Z) should have zero mass. Although the photon is indeed massless, experiments show that the weak force's bosons have mass. This meant that either gauge invariance was an incorrect approach, or something else – unknown – was giving these particles their mass, but all attempts to suggest a theory able to solve this problem just seemed to create new theoretical issues.\n\nIn the late 1950s, physicists had \"no idea\" how to resolve these issues, which were significant obstacles to developing a full-fledged theory for particle physics.\n\nBy the early 1960s, physicists had realised that a given symmetry law might not always be followed under certain conditions, at least in some areas of physics. This is called symmetry breaking and was recognised in the late 1950s by Yoichiro Nambu. Symmetry breaking can lead to surprising and unexpected results. In 1962 physicist Philip Anderson – an expert in superconductivity – wrote a paper that considered symmetry breaking in particle physics, and suggested that perhaps symmetry breaking might be the missing piece needed to solve the problems of gauge invariance in particle physics. If electroweak symmetry was somehow being broken, it might explain why electromagnetism's boson is massless, yet the weak force bosons have mass, and solve the problems. Shortly afterwards, in 1963, this was shown to be theoretically possible, at least for some limited cases.\n\nFollowing the 1962 and 1963 papers, three groups of researchers independently published the 1964 PRL symmetry breaking papers with similar conclusions: that the conditions for electroweak symmetry would be \"broken\" if an unusual type of field existed throughout the universe, and indeed, some fundamental particles would acquire mass. The field required for this to happen (which was purely hypothetical at the time) became known as the \"Higgs field\" (after Peter Higgs, one of the researchers) and the mechanism by which it led to symmetry breaking, known as the \"Higgs mechanism\". A key feature of the necessary field is that it would take \"less\" energy for the field to have a non-zero value than a zero value, unlike all other known fields, therefore, the Higgs field has a non-zero value (or \"vacuum expectation\") \"everywhere\". It was the first proposal capable of showing how the weak force gauge bosons could have mass despite their governing symmetry, within a gauge invariant theory.\n\nAlthough these ideas did not gain much initial support or attention, by 1972 they had been developed into a comprehensive theory and proved capable of giving \"sensible\" results that accurately described particles known at the time, and which, with exceptional accuracy, predicted several other particles discovered during the following years. During the 1970s these theories rapidly became the Standard Model of particle physics. There was not yet any direct evidence that the Higgs field existed, but even without proof of the field, the accuracy of its predictions led scientists to believe the theory might be true. By the 1980s the question of whether or not the Higgs field existed, and therefore whether or not the entire Standard Model was correct, had come to be regarded as one of the most important unanswered questions in particle physics.\n\nAccording to the Standard Model, a field of the necessary kind (the \"Higgs field\") exists throughout space and breaks certain symmetry laws of the electroweak interaction. Via the Higgs mechanism, this field causes the gauge bosons of the weak force to be massive at all temperatures below an extreme high value. When the weak force bosons acquire mass, this affects their range, which becomes very small. Furthermore, it was later realised that the same field would also explain, in a different way, why other fundamental constituents of matter (including electrons and quarks) have mass.\n\nFor many decades, scientists had no way to determine whether or not the Higgs field existed, because the technology needed for its detection did not exist at that time. If the Higgs field did exist, then it would be unlike any other known fundamental field, but it also was possible that these key ideas, or even the entire Standard Model, were somehow incorrect. Only discovering that the Higgs boson and therefore the Higgs field existed solved the problem.\n\nUnlike other known fields such as the electromagnetic field, the Higgs field is scalar and has a non-zero constant value in vacuum. The existence of the Higgs field became the last unverified part of the Standard Model of particle physics, and for several decades, was considered \"the central problem in particle physics\".\n\nThe presence of the field, now confirmed by experimental investigation, explains why some fundamental particles have mass, despite the symmetries controlling their interactions implying that they should be massless. It also resolves several other long-standing puzzles, such as the reason for the extremely short range of the weak force.\n\nAlthough the Higgs field is non-zero everywhere and its effects are ubiquitous, proving its existence was far from easy. In principle, it can be proved to exist by detecting its excitations, which manifest as Higgs particles (the \"Higgs boson\"), but these are extremely difficult to produce and detect. The importance of this fundamental question led to a 40-year search, and the construction of one of the world's most expensive and complex experimental facilities to date, CERN's Large Hadron Collider, in an attempt to create Higgs bosons and other particles for observation and study. On 4 July 2012, the discovery of a new particle with a mass between 125 and was announced; physicists suspected that it was the Higgs boson. Since then, the particle has been shown to behave, interact, and decay in many of the ways predicted for Higgs particles by the Standard Model, as well as having even parity and zero spin, two fundamental attributes of a Higgs boson. This also means it is the first elementary scalar particle discovered in nature. As of 2018, in-depth research shows the particle continuing to behave in line with predictions for the Standard Model Higgs boson. More studies are needed to verify with higher precision that the discovered particle has all of the properties predicted, or whether, as described by some theories, multiple Higgs bosons exist.\n\nThe hypothesised Higgs mechanism made several accurate predictions, however to confirm its existence there was an extensive search for a matching particle associated with it — the \"Higgs boson\". Detecting Higgs bosons was difficult due to the energy required to produce them and their very rare production even if the energy is sufficient. It was therefore several decades before the first evidence of the Higgs boson was found. Particle colliders, detectors, and computers capable of looking for Higgs bosons took more than 30 years to develop.\n\nBy March 2013, the existence of the Higgs boson was confirmed, and therefore, the concept of some type of Higgs field throughout space is strongly supported. The nature and properties of this field are now being investigated further, using more data collected at the LHC.\n\nVarious analogies have been used to describe the Higgs field and boson, including analogies with well-known symmetry-breaking effects such as the rainbow and prism, electric fields, ripples, and resistance of macro objects moving through media (such as people moving through crowds or some objects moving through syrup or molasses). However, analogies based on simple resistance to motion are inaccurate, as the Higgs field does not work by resisting motion.\n\nEvidence of the Higgs field and its properties has been extremely significant for many reasons. The importance of the Higgs boson is largely that it is able to be examined using existing knowledge and experimental technology, as a way to confirm and study the entire Higgs field theory. Conversely, proof that the Higgs field and boson do \"not\" exist would have also been significant.\n\nThe Higgs boson validates the Standard Model through the mechanism of mass generation. As more precise measurements of its properties are made, more advanced extensions may be suggested or excluded. As experimental means to measure the field's behaviours and interactions are developed, this fundamental field may be better understood. If the Higgs field had not been discovered, the Standard Model would have needed to be modified or superseded.\n\nRelated to this, a belief generally exists among physicists that there is likely to be \"new\" physics beyond the Standard Model, and the Standard Model will at some point be extended or superseded. The Higgs discovery, as well as the many measured collisions occurring at the LHC, provide physicists a sensitive tool to parse data for where the Standard Model fails, and could provide considerable evidence guiding researchers into future theoretical developments.\n\nBelow an extremely high temperature, electroweak symmetry breaking causes the electroweak interaction to manifest in part as the short-ranged weak force, which is carried by massive gauge bosons. This symmetry breaking is required for atoms and other structures to form, as well as for nuclear reactions in stars, such as our Sun. The Higgs field is responsible for this symmetry breaking.\n\nThe Higgs field is pivotal in generating the masses of quarks and charged leptons (through Yukawa coupling) and the W and Z gauge bosons (through the Higgs mechanism).\n\nIt is worth noting that the Higgs field does not \"create\" mass out of nothing (which would violate the law of conservation of energy), nor is the Higgs field responsible for the mass of all particles. For example, approximately 99% of the mass of baryons (composite particles such as the proton and neutron), is due instead to quantum chromodynamics binding energy, which is the sum of the kinetic energies of quarks and the energies of the massless gluons mediating the strong interaction inside the baryons. In Higgs-based theories, the property of \"mass\" is a manifestation of potential energy transferred to fundamental particles when they interact (\"couple\") with the Higgs field, which had contained that mass in the form of energy.\n\nThe Higgs field is the only scalar (spin 0) field to be detected; all the other fields in the Standard Model are spin ½ fermions or spin 1 bosons. According to Rolf-Dieter Heuer, director general of CERN when the Higgs boson was discovered, this existence proof of a scalar field is almost as important as the Higgs's role in determining the mass of other particles. It suggests that other hypothetical scalar fields suggested by other theories, from the inflaton to quintessence, could perhaps exist as well.\n\nThere has been considerable scientific research on possible links between the Higgs field and the inflaton – a hypothetical field suggested as the explanation for the expansion of space during the first fraction of a second of the universe (known as the \"inflationary epoch\"). Some theories suggest that a fundamental scalar field might be responsible for this phenomenon; the Higgs field is such a field, and its existence has led to papers analysing whether it could also be the \"inflaton\" responsible for this exponential expansion of the universe during the Big Bang. Such theories are highly tentative and face significant problems related to unitarity, but may be viable if combined with additional features such as large non-minimal coupling, a Brans–Dicke scalar, or other \"new\" physics, and they have received treatments suggesting that Higgs inflation models are still of interest theoretically.\n\nIn the Standard Model, there exists the possibility that the underlying state of our universe -known as the \"vacuum\" - is long-lived, but not completely stable. In this scenario, the universe as we know it could effectively be destroyed by collapsing into a more stable vacuum state. This was sometimes misreported as the Higgs boson \"ending\" the universe. If the masses of the Higgs boson and top quark are known more precisely, and the Standard Model provides an accurate description of particle physics up to extreme energies of the Planck scale, then it is possible to calculate whether the vacuum is stable or merely long-lived. A Higgs mass seems to be extremely close to the boundary for stability, but a definitive answer requires much more precise measurements of the pole mass of the top quark. New physics can change this picture.\n\nIf measurements of the Higgs boson suggest that our universe lies within a false vacuum of this kind, then it would imply – more than likely in many billions of years – that the universe's forces, particles, and structures could cease to exist as we know them (and be replaced by different ones), if a true vacuum happened to nucleate. It also suggests that the Higgs self-coupling \"λ\" and its \"β\" function could be very close to zero at the Planck scale, with \"intriguing\" implications, including theories of gravity and Higgs-based inflation. A future electron–positron collider would be able to provide the precise measurements of the top quark needed for such calculations.\n\nMore speculatively, the Higgs field has also been proposed as the energy of the vacuum, which at the extreme energies of the first moments of the Big Bang caused the universe to be a kind of featureless symmetry of undifferentiated, extremely high energy. In this kind of speculation, the single unified field of a Grand Unified Theory is identified as (or modelled upon) the Higgs field, and it is through successive symmetry breakings of the Higgs field, or some similar field, at phase transitions that the presently known forces and fields of the universe arise.\n\nThe relationship (if any) between the Higgs field and the presently observed vacuum energy density of the universe has also come under scientific study. As observed, the present vacuum energy density is extremely close to zero, but the energy density expected from the Higgs field, supersymmetry, and other current theories are typically many orders of magnitude larger. It is unclear how these should be reconciled. This cosmological constant problem remains a further major unanswered problem in physics.\n\nAs yet, there are no known immediate technological benefits of finding the Higgs particle. However, a common pattern for fundamental discoveries is for practical applications to follow later, and once the discovery has been explored further, perhaps becoming the basis for new technologies of importance to society.\n\nThe challenges in particle physics have furthered major technological progress of widespread importance. For example, the World Wide Web began as a project to improve CERN's communication system. CERN's requirement to process massive amounts of data produced by the Large Hadron Collider also led to contributions to the fields of distributed and cloud computing.\n\nParticle physicists study matter made from fundamental particles whose interactions are mediated by exchange particles – gauge bosons – acting as force carriers. At the beginning of the 1960s a number of these particles had been discovered or proposed, along with theories suggesting how they relate to each other, some of which had already been reformulated as field theories in which the objects of study are not particles and forces, but quantum fields and their symmetries. However, attempts to produce quantum field models for two of the four known fundamental forces – the electromagnetic force and the weak nuclear force – and then to unify these interactions, were still unsuccessful.\n\nOne known problem was that gauge invariant approaches, including non-abelian models such as Yang–Mills theory (1954), which held great promise for unified theories, also seemed to predict known massive particles as massless. Goldstone's theorem, relating to continuous symmetries within some theories, also appeared to rule out many obvious solutions, since it appeared to show that zero-mass particles also would have to exist that simply were \"not seen\". According to Guralnik, physicists had \"no understanding\" how these problems could be overcome.\n\nParticle physicist and mathematician Peter Woit summarised the state of research at the time:\n\nThe Higgs mechanism is a process by which vector bosons can acquire rest mass \"without\" explicitly breaking gauge invariance, as a byproduct of spontaneous symmetry breaking. Initially, the mathematical theory behind spontaneous symmetry breaking was conceived and published within particle physics by Yoichiro Nambu in 1960, and the concept that such a mechanism could offer a possible solution for the \"mass problem\" was originally suggested in 1962 by Philip Anderson (who had previously written papers on broken symmetry and its outcomes in superconductivity. Anderson concluded in his 1963 paper on the Yang-Mills theory, that \"considering the superconducting analog... [t]hese two types of bosons seem capable of canceling each other out... leaving finite mass bosons\"), and in March 1964, Abraham Klein and Benjamin Lee showed that Goldstone's theorem could be avoided this way in at least some non-relativistic cases, and speculated it might be possible in truly relativistic cases.\n\nThese approaches were quickly developed into a full relativistic model, independently and almost simultaneously, by three groups of physicists: by François Englert and Robert Brout in August 1964; by Peter Higgs in October 1964; and by Gerald Guralnik, Carl Hagen, and Tom Kibble (GHK) in November 1964. Higgs also wrote a short, but important, response published in September 1964 to an objection by Gilbert, which showed that if calculating within the radiation gauge, Goldstone's theorem and Gilbert's objection would become inapplicable. (Higgs later described Gilbert's objection as prompting his own paper.) Properties of the model were further considered by Guralnik in 1965, by Higgs in 1966, by Kibble in 1967, and further by GHK in 1967. The original three 1964 papers demonstrated that when a gauge theory is combined with an additional field that spontaneously breaks the symmetry, the gauge bosons may consistently acquire a finite mass. In 1967, Steven Weinberg and Abdus Salam independently showed how a Higgs mechanism could be used to break the electroweak symmetry of Sheldon Glashow's unified model for the weak and electromagnetic interactions, (itself an extension of work by Schwinger), forming what became the Standard Model of particle physics. Weinberg was the first to observe that this would also provide mass terms for the fermions.\n\nAt first, these seminal papers on spontaneous breaking of gauge symmetries were largely ignored, because it was widely believed that the (non-Abelian gauge) theories in question were a dead-end, and in particular that they could not be renormalised. In 1971–72, Martinus Veltman and Gerard 't Hooft proved renormalisation of Yang–Mills was possible in two papers covering massless, and then massive, fields. Their contribution, and the work of others on the renormalisation group – including \"substantial\" theoretical work by Russian physicists Ludvig Faddeev, Andrei Slavnov, Efim Fradkin, and Igor Tyutin – was eventually \"enormously profound and influential\", but even with all key elements of the eventual theory published there was still almost no wider interest. For example, Coleman found in a study that \"essentially no-one paid any attention\" to Weinberg's paper prior to 1971 and discussed by David Politzer in his 2004 Nobel speech. – now the most cited in particle physics – and even in 1970 according to Politzer, Glashow's teaching of the weak interaction contained no mention of Weinberg's, Salam's, or Glashow's own work. In practice, Politzer states, almost everyone learned of the theory due to physicist Benjamin Lee, who combined the work of Veltman and 't Hooft with insights by others, and popularised the completed theory. In this way, from 1971, interest and acceptance \"exploded\" and the ideas were quickly absorbed in the mainstream.\n\nThe resulting electroweak theory and Standard Model have accurately predicted (among other things) weak neutral currents, three bosons, the top and charm quarks, and with great precision, the mass and other properties of some of these. Many of those involved eventually won Nobel Prizes or other renowned awards. A 1974 paper and comprehensive review in \"Reviews of Modern Physics\" commented that \"while no one doubted the [mathematical] correctness of these arguments, no one quite believed that nature was diabolically clever enough to take advantage of them\", adding that the theory had so far produced accurate answers that accorded with experiment, but it was unknown whether the theory was fundamentally correct. By 1986 and again in the 1990s it became possible to write that understanding and proving the Higgs sector of the Standard Model was \"the central problem today in particle physics\".\n\nThe three papers written in 1964 were each recognised as milestone papers during \"Physical Review Letters\" 50th anniversary celebration. Their six authors were also awarded the 2010 J. J. Sakurai Prize for Theoretical Particle Physics for this work. (A controversy also arose the same year, because in the event of a Nobel Prize only up to three scientists could be recognised, with six being credited for the papers.) Two of the three PRL papers (by Higgs and by GHK) contained equations for the hypothetical field that eventually would become known as the Higgs field and its hypothetical quantum, the Higgs boson. Higgs' subsequent 1966 paper showed the decay mechanism of the boson; only a massive boson can decay and the decays can prove the mechanism.\n\nIn the paper by Higgs the boson is massive, and in a closing sentence Higgs writes that \"an essential feature\" of the theory \"is the prediction of incomplete multiplets of scalar and vector bosons\". (Frank Close comments that 1960s gauge theorists were focused on the problem of massless \"vector\" bosons, and the implied existence of a massive \"scalar\" boson was not seen as important; only Higgs directly addressed it.) In the paper by GHK the boson is massless and decoupled from the massive states. In reviews dated 2009 and 2011, Guralnik states that in the GHK model the boson is massless only in a lowest-order approximation, but it is not subject to any constraint and acquires mass at higher orders, and adds that the GHK paper was the only one to show that there are no massless Goldstone bosons in the model and to give a complete analysis of the general Higgs mechanism. All three reached similar conclusions, despite their very different approaches: Higgs' paper essentially used classical techniques, Englert and Brout's involved calculating vacuum polarisation in perturbation theory around an assumed symmetry-breaking vacuum state, and GHK used operator formalism and conservation laws to explore in depth the ways in which Goldstone's theorem may be worked around. Some versions of the theory predicted more than one kind of Higgs fields and bosons, and alternative \"Higgsless\" models were considered until the discovery of the Higgs boson.\n\nTo produce Higgs bosons, two beams of particles are accelerated to very high energies and allowed to collide within a particle detector. Occasionally, although rarely, a Higgs boson will be created fleetingly as part of the collision byproducts. Because the Higgs boson decays very quickly, particle detectors cannot detect it directly. Instead the detectors register all the decay products (the \"decay signature\") and from the data the decay process is reconstructed. If the observed decay products match a possible decay process (known as a \"decay channel\") of a Higgs boson, this indicates that a Higgs boson may have been created. In practice, many processes may produce similar decay signatures. Fortunately, the Standard Model precisely predicts the likelihood of each of these, and each known process, occurring. So, if the detector detects more decay signatures consistently matching a Higgs boson than would otherwise be expected if Higgs bosons did not exist, then this would be strong evidence that the Higgs boson exists.\n\nBecause Higgs boson production in a particle collision is likely to be very rare (1 in 10 billion at the LHC), and many other possible collision events can have similar decay signatures, the data of hundreds of trillions of collisions needs to be analysed and must \"show the same picture\" before a conclusion about the existence of the Higgs boson can be reached. To conclude that a new particle has been found, particle physicists require that the statistical analysis of two independent particle detectors each indicate that there is lesser than a one-in-a-million chance that the observed decay signatures are due to just background random Standard Model events—i.e., that the observed number of events is more than 5 standard deviations (sigma) different from that expected if there was no new particle. More collision data allows better confirmation of the physical properties of any new particle observed, and allows physicists to decide whether it is indeed a Higgs boson as described by the Standard Model or some other hypothetical new particle.\n\nTo find the Higgs boson, a powerful particle accelerator was needed, because Higgs bosons might not be seen in lower-energy experiments. The collider needed to have a high luminosity in order to ensure enough collisions were seen for conclusions to be drawn. Finally, advanced computing facilities were needed to process the vast amount of data (25 petabytes per year as of 2012) produced by the collisions. For the announcement of 4 July 2012, a new collider known as the Large Hadron Collider was constructed at CERN with a planned eventual collision energy of 14 TeV—over seven times any previous collider—and over 300 trillion (3×10) LHC proton–proton collisions were analysed by the LHC Computing Grid, the world's largest computing grid (as of 2012), comprising over 170 computing facilities in a worldwide network across 36 countries.\n\nThe first extensive search for the Higgs boson was conducted at the Large Electron–Positron Collider (LEP) at CERN in the 1990s. At the end of its service in 2000, LEP had found no conclusive evidence for the Higgs. This implied that if the Higgs boson were to exist it would have to be heavier than .\n\nThe search continued at Fermilab in the United States, where the Tevatron—the collider that discovered the top quark in 1995—had been upgraded for this purpose. There was no guarantee that the Tevatron would be able to find the Higgs, but it was the only supercollider that was operational since the Large Hadron Collider (LHC) was still under construction and the planned Superconducting Super Collider had been cancelled in 1993 and never completed. The Tevatron was only able to exclude further ranges for the Higgs mass, and was shut down on 30 September 2011 because it no longer could keep up with the LHC. The final analysis of the data excluded the possibility of a Higgs boson with a mass between and . In addition, there was a small (but not significant) excess of events possibly indicating a Higgs boson with a mass between and .\n\nThe Large Hadron Collider at CERN in Switzerland, was designed specifically to be able to either confirm or exclude the existence of the Higgs boson. Built in a 27 km tunnel under the ground near Geneva originally inhabited by LEP, it was designed to collide two beams of protons, initially at energies of per beam (7 TeV total), or almost 3.6 times that of the Tevatron, and upgradeable to (14 TeV total) in future. Theory suggested if the Higgs boson existed, collisions at these energy levels should be able to reveal it. As one of the most complicated scientific instruments ever built, its operational readiness was delayed for 14 months by a magnet quench event nine days after its inaugural tests, caused by a faulty electrical connection that damaged over 50 superconducting magnets and contaminated the vacuum system.\n\nData collection at the LHC finally commenced in March 2010. By December 2011 the two main particle detectors at the LHC, ATLAS and CMS, had narrowed down the mass range where the Higgs could exist to around 116-130 GeV (ATLAS) and 115-127 GeV (CMS). There had also already been a number of promising event excesses that had \"evaporated\" and proven to be nothing but random fluctuations. However, from around May 2011, both experiments had seen among their results, the slow emergence of a small yet consistent excess of gamma and 4-lepton decay signatures and several other particle decays, all hinting at a new particle at a mass around . By around November 2011, the anomalous data at 125 GeV was becoming \"too large to ignore\" (although still far from conclusive), and the team leaders at both ATLAS and CMS each privately suspected they might have found the Higgs. On November 28, 2011, at an internal meeting of the two team leaders and the director general of CERN, the latest analyses were discussed outside their teams for the first time, suggesting both ATLAS and CMS might be converging on a possible shared result at 125 GeV, and initial preparations commenced in case of a successful finding. While this information was not known publicly at the time, the narrowing of the possible Higgs range to around 115–130 GeV and the repeated observation of small but consistent event excesses across multiple channels at both ATLAS and CMS in the 124-126 GeV region (described as \"tantalising hints\" of around 2-3 sigma) were public knowledge with \"a lot of interest\". It was therefore widely anticipated around the end of 2011, that the LHC would provide sufficient data to either exclude or confirm the finding of a Higgs boson by the end of 2012, when their 2012 collision data (with slightly higher 8 TeV collision energy) had been examined.\n\nOn 22 June 2012 CERN announced an upcoming seminar covering tentative findings for 2012, and shortly afterwards (from around 1 July 2012 according to an analysis of the spreading rumour in social media) rumours began to spread in the media that this would include a major announcement, but it was unclear whether this would be a stronger signal or a formal discovery. Speculation escalated to a \"fevered\" pitch when reports emerged that Peter Higgs, who proposed the particle, was to be attending the seminar, and that \"five leading physicists\" had been invited – generally believed to signify the five living 1964 authors – with Higgs, Englert, Guralnik, Hagen attending and Kibble confirming his invitation (Brout having died in 2011).\n\nOn 4 July 2012 both of the CERN experiments announced they had independently made the same discovery: CMS of a previously unknown boson with mass 125.3 ± 0.6 GeV/\"c\" and ATLAS of a boson with mass 126.0 ± 0.6 GeV/\"c\". Using the combined analysis of two interaction types (known as 'channels'), both experiments independently reached a local significance of 5 sigma — implying that the probability of getting at least as strong a result by chance alone is less than 1 in 3 million. When additional channels were taken into account, the CMS significance was reduced to 4.9 sigma.\n\nThe two teams had been working 'blinded' from each other from around late 2011 or early 2012, meaning they did not discuss their results with each other, providing additional certainty that any common finding was genuine validation of a particle. This level of evidence, confirmed independently by two separate teams and experiments, meets the formal level of proof required to announce a confirmed discovery.\n\nOn 31 July 2012, the ATLAS collaboration presented additional data analysis on the \"observation of a new particle\", including data from a third channel, which improved the significance to 5.9 sigma (1 in 588 million chance of obtaining at least as strong evidence by random background effects alone) and mass , and CMS improved the significance to 5-sigma and mass .\n\nFollowing the 2012 discovery, it was still unconfirmed whether or not the 125 GeV/\"c\" particle was a Higgs boson. On one hand, observations remained consistent with the observed particle being the Standard Model Higgs boson, and the particle decayed into at least some of the predicted channels. Moreover, the production rates and branching ratios for the observed channels broadly matched the predictions by the Standard Model within the experimental uncertainties. However, the experimental uncertainties currently still left room for alternative explanations, meaning an announcement of the discovery of a Higgs boson would have been premature. To allow more opportunity for data collection, the LHC's proposed 2012 shutdown and 2013–14 upgrade were postponed by 7 weeks into 2013.\n\nIn November 2012, in a conference in Kyoto researchers said evidence gathered since July was falling into line with the basic Standard Model more than its alternatives, with a range of results for several interactions matching that theory's predictions. Physicist Matt Strassler highlighted \"considerable\" evidence that the new particle is not a pseudoscalar negative parity particle (consistent with this required finding for a Higgs boson), \"evaporation\" or lack of increased significance for previous hints of non-Standard Model findings, expected Standard Model interactions with W and Z bosons, absence of \"significant new implications\" for or against supersymmetry, and in general no significant deviations to date from the results expected of a Standard Model Higgs boson. However some kinds of extensions to the Standard Model would also show very similar results; so commentators noted that based on other particles that are still being understood long after their discovery, it may take years to be sure, and decades to fully understand the particle that has been found.\n\nThese findings meant that as of January 2013, scientists were very sure they had found an unknown particle of mass ~ 125 GeV/\"c\", and had not been misled by experimental error or a chance result. They were also sure, from initial observations, that the new particle was some kind of boson. The behaviours and properties of the particle, so far as examined since July 2012, also seemed quite close to the behaviours expected of a Higgs boson. Even so, it could still have been a Higgs boson or some other unknown boson, since future tests could show behaviours that do not match a Higgs boson, so as of December 2012 CERN still only stated that the new particle was \"consistent with\" the Higgs boson, and scientists did not yet positively say it was the Higgs boson. Despite this, in late 2012, widespread media reports announced (incorrectly) that a Higgs boson had been confirmed during the year.\n\nIn January 2013, CERN director-general Rolf-Dieter Heuer stated that based on data analysis to date, an answer could be possible 'towards' mid-2013, and the deputy chair of physics at Brookhaven National Laboratory stated in February 2013 that a \"definitive\" answer might require \"another few years\" after the collider's 2015 restart. In early March 2013, CERN Research Director Sergio Bertolucci stated that confirming spin-0 was the major remaining requirement to determine whether the particle is at least some kind of Higgs boson.\n\nOn 14 March 2013 CERN confirmed that:\nThis also makes the particle the first elementary scalar particle to be discovered in nature.\n\nExamples of tests used to validate that the discovered particle is the Higgs boson:\nIn July 2017, CERN confirmed that all measurements still agree with the predictions of the Standard Model, and called the discovered particle simply \"the Higgs boson\". As of April 2018, the Large Hadron Collider has continued to produce findings that confirm the 2013 understanding of the Higgs field and particle.\nThe LHC's experimental work since restarting in 2015 has included probing the Higgs field and boson to a greater level of detail, and confirming whether or not less common predictions were correct. In particular, exploration since 2015 has provided strong evidence of the predicted direct decay into fermions such as pairs of bottom quarks (3.6 σ)—described as an \"important milestone\" in understanding its short lifetime and other rare decays—and also to confirm decay into pairs of tau leptons (5.9 σ). This was described by CERN as being \"of paramount importance to establishing the coupling of the Higgs boson to leptons and represents an important step towards measuring its couplings to third generation fermions, the very heavy copies of the electrons and quarks, whose role in nature is a profound mystery\". Published results as of 19 Mar 2018 at 13 TeV for ATLAS and CMS had their measurements of the Higgs mass at and respectively.\n\nIn July 2018, the ATLAS and CMS experiments reported observing the Higgs boson decay into a pair of bottom quarks, which makes up approximately 60% of all of its decays.\n\nGauge invariance is an important property of modern particle theories such as the Standard Model, partly due to its success in other areas of fundamental physics such as electromagnetism and the strong interaction (quantum chromodynamics). However, there were great difficulties in developing gauge theories for the weak nuclear force or a possible unified electroweak interaction. Fermions with a mass term would violate gauge symmetry and therefore cannot be gauge invariant. (This can be seen by examining the Dirac Lagrangian for a fermion in terms of left and right handed components; we find none of the spin-half particles could ever flip helicity as required for mass, so they must be massless.) W and Z bosons are observed to have mass, but a boson mass term contains terms which clearly depend on the choice of gauge, and therefore these masses too cannot be gauge invariant. Therefore, it seems that \"none\" of the standard model fermions \"or\" bosons could \"begin\" with mass as an inbuilt property except by abandoning gauge invariance. If gauge invariance were to be retained, then these particles had to be acquiring their mass by some other mechanism or interaction. Additionally, whatever was giving these particles their mass had to not \"break\" gauge invariance as the basis for other parts of the theories where it worked well, \"and\" had to not require or predict unexpected massless particles or long-range forces (seemingly an inevitable consequence of Goldstone's theorem) which did not actually seem to exist in nature.\n\nA solution to all of these overlapping problems came from the discovery of a previously unnoticed borderline case hidden in the mathematics of Goldstone's theorem, that under certain conditions it \"might\" theoretically be possible for a symmetry to be broken \"without\" disrupting gauge invariance and \"without\" any new massless particles or forces, and having \"sensible\" (renormalisable) results mathematically. This became known as the Higgs mechanism.\nThe Standard Model hypothesises a field which is responsible for this effect, called the Higgs field (symbol: formula_1), which has the unusual property of a non-zero amplitude in its ground state; i.e., a non-zero vacuum expectation value. It can have this effect because of its unusual \"Mexican hat\" shaped potential whose lowest \"point\" is not at its \"centre\". In simple terms, unlike all other known fields, the Higgs field requires \"less\" energy to have a non-zero value than a zero value, so it ends up having a non-zero value \"everywhere\". Below a certain extremely high energy level the existence of this non-zero vacuum expectation spontaneously breaks electroweak gauge symmetry which in turn gives rise to the Higgs mechanism and triggers the acquisition of mass by those particles interacting with the field. This effect occurs because scalar field components of the Higgs field are \"absorbed\" by the massive bosons as degrees of freedom, and couple to the fermions via Yukawa coupling, thereby producing the expected mass terms. When symmetry breaks under these conditions, the Goldstone bosons that arise \"interact\" with the Higgs field (and with other particles capable of interacting with the Higgs field) instead of becoming new massless particles. The intractable problems of both underlying theories \"neutralise\" each other, and the residual outcome is that elementary particles acquire a consistent mass based on how strongly they interact with the Higgs field. It is the simplest known process capable of giving mass to the gauge bosons while remaining compatible with gauge theories. Its quantum would be a scalar boson, known as the Higgs boson.\n\nIn the Standard Model, the Higgs field is a scalar tachyonic field – \"scalar\" meaning it does not transform under Lorentz transformations, and \"tachyonic\" meaning the field (but not the particle) has imaginary mass, and in certain configurations must undergo symmetry breaking. It consists of four components: two neutral ones and two charged component fields. Both of the charged components and one of the neutral fields are Goldstone bosons, which act as the longitudinal third-polarisation components of the massive W, W, and Z bosons. The quantum of the remaining neutral component corresponds to (and is theoretically realised as) the massive Higgs boson, this component can also interact with fermions via Yukawa coupling to give them mass, as well.\n\nMathematically, the Higgs field has imaginary mass and is therefore a tachyonic field. While tachyons (particles that move faster than light) are a purely hypothetical concept, fields with imaginary mass have come to play an important role in modern physics. Under no circumstances do any excitations ever propagate faster than light in such theories — the presence or absence of a tachyonic mass has no effect whatsoever on the maximum velocity of signals (there is no violation of causality). Instead of faster-than-light particles, the imaginary mass creates an instability: Any configuration in which one or more field excitations are tachyonic must spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known as tachyon condensation, and is now believed to be the explanation for how the Higgs mechanism itself arises in nature, and therefore the reason behind electroweak symmetry breaking.\n\nAlthough the notion of imaginary mass might seem troubling, it is only the field, and not the mass itself, that is quantised. Therefore, the field operators at spacelike separated points still commute (or anticommute), and information and particles still do not propagate faster than light. Tachyon condensation drives a physical system that has reached a local limit – and might naively be expected to produce physical tachyons – to an alternate stable state where no physical tachyons exist. Once a tachyonic field such as the Higgs field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles such as the Higgs boson.\n\nSince the Higgs field is scalar, the Higgs boson has no spin. The Higgs boson is also its own antiparticle and is CP-even, and has zero electric and colour charge.\n\nThe Standard Model does not predict the mass of the Higgs boson. If that mass is between 115 and (consistent with empirical observations of ), then the Standard Model can be valid at energy scales all the way up to the Planck scale (10 GeV). Many theorists expect new physics beyond the Standard Model to emerge at the TeV-scale, based on unsatisfactory properties of the Standard Model. The highest possible mass scale allowed for the Higgs boson (or some other electroweak symmetry breaking mechanism) is 1.4 TeV; beyond this point, the Standard Model becomes inconsistent without such a mechanism, because unitarity is violated in certain scattering processes.\n\nIt is also possible, although experimentally difficult, to estimate the mass of the Higgs boson indirectly. In the Standard Model, the Higgs boson has a number of indirect effects; most notably, Higgs loops result in tiny corrections to masses of W and Z bosons. Precision measurements of electroweak parameters, such as the Fermi constant and masses of W/Z bosons, can be used to calculate constraints on the mass of the Higgs. As of July 2011, the precision electroweak measurements tell us that the mass of the Higgs boson is likely to be less than about at 95% confidence level (this upper limit would increase to if the lower bound of from the LEP-2 direct search is allowed for). These indirect constraints rely on the assumption that the Standard Model is correct. It may still be possible to discover a Higgs boson above these masses if it is accompanied by other particles beyond those predicted by the Standard Model.\n\nIf Higgs particle theories are valid, then a Higgs particle can be produced much like other particles that are studied, in a particle collider. This involves accelerating a large number of particles to extremely high energies and extremely close to the speed of light, then allowing them to smash together. Protons and lead ions (the bare nuclei of lead atoms) are used at the LHC. In the extreme energies of these collisions, the desired esoteric particles will occasionally be produced and this can be detected and studied; any absence or difference from theoretical expectations can also be used to improve the theory. The relevant particle theory (in this case the Standard Model) will determine the necessary kinds of collisions and detectors. The Standard Model predicts that Higgs bosons could be formed in a number of ways, although the probability of producing a Higgs boson in any collision is always expected to be very small—for example, only 1 Higgs boson per 10 billion collisions in the Large Hadron Collider. The most common expected processes for Higgs boson production are:\n\nQuantum mechanics predicts that if it is possible for a particle to decay into a set of lighter particles, then it will eventually do so. This is also true for the Higgs boson. The likelihood with which this happens depends on a variety of factors including: the difference in mass, the strength of the interactions, etc. Most of these factors are fixed by the Standard Model, except for the mass of the Higgs boson itself. For a Higgs boson with a mass of the SM predicts a mean life time of about .\nSince it interacts with all the massive elementary particles of the SM, the Higgs boson has many different processes through which it can decay. Each of these possible processes has its own probability, expressed as the \"branching ratio\"; the fraction of the total number decays that follows that process. The SM predicts these branching ratios as a function of the Higgs mass (see plot).\n\nOne way that the Higgs can decay is by splitting into a fermion–antifermion pair. As general rule, the Higgs is more likely to decay into heavy fermions than light fermions, because the mass of a fermion is proportional to the strength of its interaction with the Higgs. By this logic the most common decay should be into a top–antitop quark pair. However, such a decay would only be possible if the Higgs were heavier than ~, twice the mass of the top quark. For a Higgs mass of the SM predicts that the most common decay is into a bottom–antibottom quark pair, which happens 57.7% of the time. The second most common fermion decay at that mass is a tau–antitau pair, which happens only about 6.3% of the time.\n\nAnother possibility is for the Higgs to split into a pair of massive gauge bosons. The most likely possibility is for the Higgs to decay into a pair of W bosons (the light blue line in the plot), which happens about 21.5% of the time for a Higgs boson with a mass of . The W bosons can subsequently decay either into a quark and an antiquark or into a charged lepton and a neutrino. The decays of W bosons into quarks are difficult to distinguish from the background, and the decays into leptons cannot be fully reconstructed (because neutrinos are impossible to detect in particle collision experiments). A cleaner signal is given by decay into a pair of Z-bosons (which happens about 2.6% of the time for a Higgs with a mass of ), if each of the bosons subsequently decays into a pair of easy-to-detect charged leptons (electrons or muons).\n\nDecay into massless gauge bosons (i.e., gluons or photons) is also possible, but requires intermediate loop of virtual heavy quarks (top or bottom) or massive gauge bosons. The most common such process is the decay into a pair of gluons through a loop of virtual heavy quarks. This process, which is the reverse of the gluon fusion process mentioned above, happens approximately 8.6% of the time for a Higgs boson with a mass of . Much rarer is the decay into a pair of photons mediated by a loop of W bosons or heavy quarks, which happens only twice for every thousand decays. However, this process is very relevant for experimental searches for the Higgs boson, because the energy and momentum of the photons can be measured very precisely, giving an accurate reconstruction of the mass of the decaying particle.\n\nThe Minimal Standard Model as described above is the simplest known model for the Higgs mechanism with just one Higgs field. However, an extended Higgs sector with additional Higgs particle doublets or triplets is also possible, and many extensions of the Standard Model have this feature. The non-minimal Higgs sector favoured by theory are the two-Higgs-doublet models (2HDM), which predict the existence of a quintet of scalar particles: two CP-even neutral Higgs bosons h and H, a CP-odd neutral Higgs boson A, and two charged Higgs particles H. Supersymmetry (\"SUSY\") also predicts relations between the Higgs-boson masses and the masses of the gauge bosons, and could accommodate a neutral Higgs boson.\n\nThe key method to distinguish between these different models involves study of the particles' interactions (\"coupling\") and exact decay processes (\"branching ratios\"), which can be measured and tested experimentally in particle collisions. In the Type-I 2HDM model one Higgs doublet couples to up and down quarks, while the second doublet does not couple to quarks. This model has two interesting limits, in which the lightest Higgs couples to just fermions (\"gauge-phobic\") or just gauge bosons (\"fermiophobic\"), but not both. In the Type-II 2HDM model, one Higgs doublet only couples to up-type quarks, the other only couples to down-type quarks. The heavily researched Minimal Supersymmetric Standard Model (MSSM) includes a Type-II 2HDM Higgs sector, so it could be disproven by evidence of a Type-I 2HDM Higgs.\n\nIn other models the Higgs scalar is a composite particle. For example, in technicolor the role of the Higgs field is played by strongly bound pairs of fermions called techniquarks. Other models, feature pairs of top quarks (see top quark condensate). In yet other models, there is no Higgs field at all and the electroweak symmetry is broken using extra dimensions.\n\nThe Standard Model leaves the mass of the Higgs boson as a parameter to be measured, rather than a value to be calculated. This is seen as theoretically unsatisfactory, particularly as quantum corrections (related to interactions with virtual particles) should apparently cause the Higgs particle to have a mass immensely higher than that observed, but at the same time the Standard Model requires a mass of the order of 100 to 1000 GeV to ensure unitarity (in this case, to unitarise longitudinal vector boson scattering). Reconciling these points appears to require explaining why there is an almost-perfect cancellation resulting in the visible mass of ~ 125 GeV, and it is not clear how to do this. Because the weak force is about 10 times stronger than gravity, and (linked to this) the Higgs boson's mass is so much less than the Planck mass or the grand unification energy, it appears that either there is some underlying connection or reason for these observations which is unknown and not described by the Standard Model, or some unexplained and extremely precise fine-tuning of parameters – however at present neither of these explanations is proven. This is known as a hierarchy problem. More broadly, the hierarchy problem amounts to the worry that a future theory of fundamental particles and interactions should not have excessive fine-tunings or unduly delicate cancellations, and should allow masses of particles such as the Higgs boson to be calculable. The problem is in some ways unique to spin-0 particles (such as the Higgs boson), which can give rise to issues related to quantum corrections that do not affect particles with spin. A number of solutions have been proposed, including supersymmetry, conformal solutions and solutions via extra dimensions such as braneworld models.\n\nThere are also issues of quantum triviality, which suggests that it may not be possible to create a consistent quantum field theory involving elementary scalar particles. However, if quantum triviality is avoided, triviality constraints may set bounds on the Higgs Boson mass.\n\nThe name most strongly associated with the particle and field is the Higgs boson and Higgs field. For some time the particle was known by a combination of its PRL author names (including at times Anderson), for example the Brout–Englert–Higgs particle, the Anderson-Higgs particle, or the Englert–Brout–Higgs–Guralnik–Hagen–Kibble mechanism, and these are still used at times. Fuelled in part by the issue of recognition and a potential shared Nobel Prize, the most appropriate name was still occasionally a topic of debate until 2013. Higgs himself prefers to call the particle either by an acronym of all those involved, or \"the scalar boson\", or \"the so-called Higgs particle\".\n\nA considerable amount has been written on how Higgs' name came to be exclusively used. Two main explanations are offered. The first is that Higgs undertook a step which was either unique, clearer or more explicit in his paper in formally predicting and examining the particle. Of the PRL papers' authors, only the paper by Higgs \"explicitly\" offered as a prediction that a massive particle would exist and calculated some of its properties; he was therefore \"the first to postulate the existence of a massive particle\" according to \"Nature\". Physicist and author Frank Close and physicist-blogger Peter Woit both comment that the paper by GHK was also completed after Higgs and Brout–Englert were submitted to Physical Review Letters. and that Higgs alone had drawn attention to a predicted massive \"scalar\" boson, while all others had focused on the massive \"vector\" bosons; In this way, Higgs' contribution also provided experimentalists with a crucial \"concrete target\" needed to test the theory. However, in Higgs' view, Brout and Englert did not explicitly mention the boson since its existence is plainly obvious in their work, while according to Guralnik the GHK paper was a complete analysis of the entire symmetry breaking mechanism whose mathematical rigour is absent from the other two papers, and a massive particle may exist in some solutions. Higgs' paper also provided an \"especially sharp\" statement of the challenge and its solution according to science historian David Kaiser.\n\nThe alternative explanation is that the name was popularised in the 1970s due to its use as a convenient shorthand or because of a mistake in citing. Many accounts (including Higgs' own) credit the \"Higgs\" name to physicist Benjamin Lee (in ). Lee was a significant populist for the theory in its early stages, and habitually attached the name \"Higgs\" as a \"convenient shorthand\" for its components from 1972 and in at least one instance from as early as 1966. Although Lee clarified in his footnotes that \"'Higgs' is an abbreviation for Higgs, Kibble, Guralnik, Hagen, Brout, Englert\", his use of the term (and perhaps also Steven Weinberg's mistaken cite of Higgs' paper as the first in his seminal 1967 paper) meant that by around 1975–76 others had also begun to use the name 'Higgs' exclusively as a shorthand.\n\nThe Higgs boson is often referred to as the \"God particle\" in popular media outside the scientific community. The nickname comes from the title of the 1993 book on the Higgs boson and particle physics, \"\" by Physics Nobel Prize winner and Fermilab director Leon Lederman. Lederman wrote it in the context of failing US government support for the Superconducting Super Collider, a part-constructed titanic competitor to the Large Hadron Collider with planned collision energies of that was championed by Lederman since its 1983 inception and shut down in 1993. The book sought in part to promote awareness of the significance and need for such a project in the face of its possible loss of funding. Lederman, a leading researcher in the field, writes that he wanted to title his book \"The Goddamn Particle: If the Universe is the Answer, What is the Question?\" Lederman's editor decided that the title was too controversial and convinced him to change the title to \"The God Particle: If the Universe is the Answer, What is the Question?\"\n\nWhile media use of this term may have contributed to wider awareness and interest, many scientists feel the name is inappropriate since it is sensational hyperbole and misleads readers; the particle also has nothing to do with God, leaves open numerous questions in fundamental physics, and does not explain the ultimate origin of the universe. Higgs, an atheist, was reported to be displeased and stated in a 2008 interview that he found it \"embarrassing\" because it was \"the kind of misuse... which I think might offend some people\". The nickname has been satirised in mainstream media as well. Science writer Ian Sample stated in his 2010 book on the search that the nickname is \"universally hate[d]\" by physicists and perhaps the \"worst derided\" in the history of physics, but that (according to Lederman) the publisher rejected all titles mentioning \"Higgs\" as unimaginative and too unknown.\n\nLederman begins with a review of the long human search for knowledge, and explains that his tongue-in-cheek title draws an analogy between the impact of the Higgs field on the fundamental symmetries at the Big Bang, and the apparent chaos of structures, particles, forces and interactions that resulted and shaped our present universe, with the biblical story of Babel in which the primordial single language of early Genesis was fragmented into many disparate languages and cultures.\n\nLederman asks whether the Higgs boson was added just to perplex and confound those seeking knowledge of the universe, and whether physicists will be confounded by it as recounted in that story, or ultimately surmount the challenge and understand \"how beautiful is the universe [God has] made\".\n\nA renaming competition by British newspaper \"The Guardian\" in 2009 resulted in their science correspondent choosing the name \"the champagne bottle boson\" as the best submission: \"The bottom of a champagne bottle is in the shape of the Higgs potential and is often used as an illustration in physics lectures. So it's not an embarrassingly grandiose name, it is memorable, and [it] has some physics connection too.\"\nThe name \"Higgson\" was suggested as well, in an opinion piece in the Institute of Physics' online publication \"physicsworld.com\".\n\nThere has been considerable public discussion of analogies and explanations for the Higgs particle and how the field creates mass, including coverage of explanatory attempts in their own right and a competition in 1993 for the best popular explanation by then-UK Minister for Science Sir William Waldegrave and articles in newspapers worldwide.\nAn educational collaboration involving an LHC physicist and a High School Teachers at CERN educator suggests that dispersion of light – responsible for the rainbow and dispersive prism – is a useful analogy for the Higgs field's symmetry breaking and mass-causing effect.\n\nMatt Strassler uses electric fields as an analogy:\nA similar explanation was offered by \"The Guardian\":\nThe Higgs field's effect on particles was famously described by physicist David Miller as akin to a room full of political party workers spread evenly throughout a room: the crowd gravitates to and slows down famous people but does not slow down others. He also drew attention to well-known effects in solid state physics where an electron's effective mass can be much greater than usual in the presence of a crystal lattice.\n\nAnalogies based on drag effects, including analogies of \"syrup\" or \"molasses\" are also well known, but can be somewhat misleading since they may be understood (incorrectly) as saying that the Higgs field simply resists some particles' motion but not others' – a simple resistive effect could also conflict with Newton's third law.\n\nThere was considerable discussion prior to late 2013 of how to allocate the credit if the Higgs boson is proven, made more pointed as a Nobel prize had been expected, and the very wide basis of people entitled to consideration. These include a range of theoreticians who made the Higgs mechanism theory possible, the theoreticians of the 1964 PRL papers (including Higgs himself), the theoreticians who derived from these a working electroweak theory and the Standard Model itself, and also the experimentalists at CERN and other institutions who made possible the proof of the Higgs field and boson in reality. The Nobel prize has a limit of 3 persons to share an award, and some possible winners are already prize holders for other work, or are deceased (the prize is only awarded to persons in their lifetime). Existing prizes for works relating to the Higgs field, boson, or mechanism include:\n\nAdditionally Physical Review Letters' 50-year review (2008) recognised the 1964 PRL symmetry breaking papers and Weinberg's 1967 paper \"A model of Leptons\" (the most cited paper in particle physics, as of 2012) \"milestone Letters\".\n\nFollowing reported observation of the Higgs-like particle in July 2012, several Indian media outlets reported on the supposed neglect of credit to Indian physicist Satyendra Nath Bose after whose work in the 1920s the class of particles \"bosons\" is named (although physicists have described Bose's connection to the discovery as tenuous).\n\nIn the Standard Model, the Higgs field is a four-component scalar field that forms a complex doublet of the weak isospin SU(2) symmetry:\n\\left(\n\\phi^1 + i\\phi^2 \\\\ \\phi^0+i\\phi^3\n\\right)\\;,\nwhile the field has charge +½ under the weak hypercharge U(1) symmetry.\n\nThe Higgs part of the Lagrangian is\n\nwhere formula_2 and formula_3 are the gauge bosons of the SU(2) and U(1) symmetries, formula_4 and formula_5 their respective coupling constants, formula_6 (where formula_7 are the Pauli matrices) a complete set generators of the SU(2) symmetry, and formula_8 and formula_9, so that the ground state breaks the SU(2) symmetry (see figure). The ground state of the Higgs field (the bottom of the potential) is degenerate with different ground states related to each other by a SU(2) gauge transformation. It is always possible to pick a gauge such that in the ground state formula_10. The expectation value of formula_11 in the ground state (the vacuum expectation value or VEV) is then formula_12, where formula_13. The measured value of this parameter is ~. It has units of mass, and is the only free parameter of the Standard Model that is not a dimensionless number. Quadratic terms in formula_14 and formula_15 arise, which give masses to the \"W\" and \"Z\" bosons:\nwith their ratio determining the Weinberg angle, formula_16, and leave a massless U(1) photon, formula_17. The mass of the Higgs boson itself is given by\n\nThe quarks and the leptons interact with the Higgs field through Yukawa interaction terms:\n+\\lambda_u^{ij}\\frac{\\phi^1-i\\phi^2}{\\sqrt{2}}\\overline d_L^i u_R^j\\\\\n&-\\lambda_d^{ij}\\frac{\\phi^0+i\\phi^3}{\\sqrt{2}}\\overline d_L^i d_R^j\n-\\lambda_d^{ij}\\frac{\\phi^1+i\\phi^2}{\\sqrt{2}}\\overline u_L^i d_R^j\\\\\n&-\\lambda_e^{ij}\\frac{\\phi^0+i\\phi^3}{\\sqrt{2}}\\overline e_L^i e_R^j\n-\\lambda_e^{ij}\\frac{\\phi^1+i\\phi^2}{\\sqrt{2}}\\overline \\nu_L^i e_R^j\nwhere formula_18 are left-handed and right-handed quarks and leptons of the \"i\"th generation, formula_19 are matrices of Yukawa couplings where h.c. denotes the hermitian conjugate of all the preceding terms. In the symmetry breaking ground state, only the terms containing formula_11 remain, giving rise to mass terms for the fermions. Rotating the quark and lepton fields to the basis where the matrices of Yukawa couplings are diagonal, one gets\nwhere the masses of the fermions are formula_21, and formula_22 denote the eigenvalues of the Yukawa matrices.\n\n\n\n\n\n\n\n"}
{"id": "474404", "url": "https://en.wikipedia.org/wiki?curid=474404", "title": "High-pressure area", "text": "High-pressure area\n\nA high-pressure area, high or anticyclone is a region where the atmospheric pressure at the surface of the planet is greater than its surrounding environment.\n\nWinds within high-pressure areas flow outward from the higher pressure areas near their centers towards the lower pressure areas further from their centers. Gravity adds to the forces causing this general movement, because the higher pressure compresses the column of air near the center of the area into greater density – and so greater weight compared to lower pressure, lower density, and lower weight of the air outside the center.\n\nHowever, because the planet is rotating underneath the atmosphere, and frictional forces arise as the planetary surface drags some atmosphere with it, the air flow from center to periphery is not direct, but is twisted due to the Coriolis effect, or the merely apparent force that arise when the observer is in a rotating frame of reference. Viewed from above this twist in wind direction is in the same direction as the rotation of the planet.\n\nThe strongest high-pressure areas are associated with cold air masses which push away out of polar regions during the winter when there is less sun to warm neighboring regions. These Highs change character and weaken once they move further over relatively warmer water bodies.\n\nSomewhat weaker but more common are high-pressure areas caused by atmospheric subsidence, that is, areas where large masses of cooler drier air descend from an elevation of 8 to 15 km after the lower temperatures have precipitated out the water vapor.\n\nMany of the features of Highs may be understood in context of middle- or meso-scale and relatively enduring dynamics of a planet's atmospheric circulation. For example, massive atmospheric subsidences occur as part of the descending branches of Ferrel cells and Hadley cells. Hadley cells help form the subtropical ridge, steer tropical waves and tropical cyclones across the ocean and is strongest during the summer. The subtropical ridge also helps form most of the world's deserts.\n\nOn English-language weather maps, high-pressure centers are identified by the letter H. Weather maps in other languages may use different letters or symbols.\n\nThe direction of wind flow around an atmospheric high-pressure area and a low-pressure area, as seen from above, depends on the hemisphere. High-pressure systems rotate clockwise in the northern Hemisphere; low-pressure systems rotate clockwise in the southern hemisphere.\n\nThe scientific terms in English used to describe the weather systems generated by highs and lows were introduced in the mid-1800s, mostly by the British. The scientific theories which explain the general phenomena originated about two centuries earlier.\n\nThe term cyclone was coined by Henry Piddington of the British East India Company to describe the devastating storm of December 1789 in Coringa, India. A cyclone forms around a low-pressure area. Anticyclone, the term for the kind of weather around a high-pressure area, was coined in 1877 by Francis Galton to indicate an area whose winds revolved in the opposite direction of a cyclone. In British English, the opposite direction of clockwise is referred to as anticlockwise, making the label \"anticyclones\" a logical extension.\n\nA simple rule is that for high-pressure areas, where generally air flows from the center outward, the coriolis force given by the earth's rotation to the air circulation is in the opposite direction of earth's apparent rotation if viewed from above the hemisphere's pole. So, both the earth and winds around a low-pressure area rotate counter-clockwise in the northern hemisphere, and clockwise in the southern. The opposite to these two cases occurs in the case of a high. These results derive from the Coriolis effect; that article explains in detail the physics, and provides an animation of a model to aid understanding.\n\nHigh-pressure areas form due to downward motion through the troposphere, the atmospheric layer where weather occurs. Preferred areas within a synoptic flow pattern in higher levels of the troposphere are beneath the western side of troughs.\n\nOn weather maps, these areas show converging winds (isotachs), also known as convergence, near or above the level of non-divergence, which is near the 500 hPa pressure surface about midway up through the troposphere, and about half the atmospheric pressure at the surface.\n\nHigh-pressure systems are alternatively referred to as anticyclones. On English-language weather maps, high-pressure centers are identified by the letter H in English, within the isobar with the highest pressure value. On constant pressure upper level charts, it is located within the highest height line contour.\n\n Highs are frequently associated with light winds at the surface and subsidence through the lower portion of the troposphere. In general, subsidence will dry out an air mass by adiabatic, or compressional, heating. Thus, high pressure typically brings clear skies. During the day, since no clouds are present to reflect sunlight, there is more incoming shortwave solar radiation and temperatures rise. At night, the absence of clouds means that outgoing longwave radiation (i.e. heat energy from the surface) is not absorbed, giving cooler diurnal low temperatures in all seasons. When surface winds become light, the subsidence produced directly under a high-pressure system can lead to a buildup of particulates in urban areas under the ridge, leading to widespread haze. If the low level relative humidity rises towards 100 percent overnight, fog can form.\n\nStrong, vertically shallow high-pressure systems moving from higher latitudes to lower latitudes in the northern hemisphere are associated with continental arctic air masses. Once arctic air moves over an unfrozen ocean, the air mass modifies greatly over the warmer water and takes on the character of a maritime air mass, which reduces the strength of the high-pressure system. When extremely cold air moves over relatively warm oceans, polar lows can develop. However, warm and moist (or maritime tropical) air masses that move poleward from tropical sources are slower to modify than arctic air masses.\n\nIn terms of climatology, high pressure forms at the horse latitudes, or torrid zone, between the latitudes of 20 and 40 degrees from the equator, as a result of air that has been uplifted at the equator. As the hot air rises it cools, losing moisture; it is then transported poleward where it descends, creating the high-pressure area. This is part of the Hadley cell circulation and is known as the subtropical ridge or subtropical high, and is strongest in the summer. The subtropical ridge is a warm core high-pressure system, meaning it strengthens with height. Many of the world's deserts are caused by these climatological high-pressure systems.\n\nSome climatological high-pressure areas acquire regionally based names. The land-based Siberian High often remains quasi-stationary for more than a month during the most frigid time of the year, making it unique in that regard. It is also a bit larger and more persistent than its counterpart in North America. Surface winds accelerating down valleys down the western Pacific ocean coastline, causing the winter monsoon. Arctic high-pressure systems such as the Siberian High are cold core, meaning that they weaken with height. The influence of the Azores High, also known as the Bermuda High, brings fair weather over much of the North Atlantic Ocean and mid to late summer heat waves in western Europe. Along its southerly periphery, the clockwise circulation often impels easterly waves, and tropical cyclones that develop from them, across the ocean towards landmasses in the western portion of ocean basins during the hurricane season. The highest barometric pressure ever recorded on Earth was measured in Tonsontsengel, Mongolia on 19 December 2001.\n\nWind flows from areas of high pressure to areas of low pressure. This is due to density differences between the two air masses. Since stronger high-pressure systems contain cooler or drier air, the air mass is more dense and flows towards areas that are warm or moist, which are in the vicinity of low pressure areas in advance of their associated cold fronts. The stronger the pressure difference, or pressure gradient, between a high-pressure system and a low-pressure system, the stronger the wind. The coriolis force caused by the Earth's rotation is what gives winds within high-pressure systems their clockwise circulation in the northern hemisphere (as the wind moves outward and is deflected right from the center of high pressure) and counterclockwise circulation in the southern hemisphere (as the wind moves outward and is deflected left from the center of high pressure). Friction with land slows down the wind flowing out of high-pressure systems and causes wind to flow more outward than would be the case in the absence of friction. This is known as a geostrophic wind.\n\n"}
{"id": "11708657", "url": "https://en.wikipedia.org/wiki?curid=11708657", "title": "Isobel Bennett", "text": "Isobel Bennett\n\nIsobel Ida Bennett AO 1984 (9 July 1909 – 12 January 2008) was one of Australia's best-known marine biologists. She (with Elizabeth Pope) assisted William John Dakin with research for his final book, \"Australian Seashores\", regarded by many as \"the definitive guide on the intertidal zone, and a recommended source of information to divers\". Following Dakin's death in 1950, she saw the book through to publication in 1952, and she continued to revise and reprint it with a complete revision in 1980 until 1992. In later editions, she was listed as a co-author, then first author. She also wrote nine other books, and was one of the first women (along with Susan Ingham, Mary Gillham & Hope Macpherson) to go south with the Australian National Antarctic Research Expeditions (ANARE).\n\nIsobel Ida Bennett was born in Brisbane in 1909 and educated at Somerville House leaving at the age of 16 when her family moved to Sydney. She attended business college and gained employment in a patent office and for four years at the Associated Board of the Royal Schools of Music in Sydney, she joined the Zoology Department of the University of Sydney in 1933. From that time until 1948, she worked as secretary, librarian, demonstrator and research assistant to Professor W.J. Dakin, and then as research assistant to Professor P.D.F. Murray.\n\nFrom 1950 she regularly led students to the Heron Island and Lizard Island Research Stations on the Great Barrier Reef and did field work on the Victorian and Tasmanian coasts. In 1959 she made her first visit to Macquarie Island with the ANARE relief ship, returning in 1960, 1965 and 1968. From 1959 to 1971, she was a Professional Officer at the University of Sydney, and received the first Honorary Master of Science from the University of Sydney in 1962. She was a temporary Associate Professor at Stanford University in 1963 and a delegate to the 11th Pacific Science Congress in Tokyo in 1966.\n\nShe retired in 1971, but remained an active author and researcher. From 1974 to 1979 she worked with the New South Wales Fisheries Department, and during that time, carried out fieldwork and surveys at the coastal rock platforms at Jervis Bay and Ulladulla, and on the coasts of Lord Howe Island, Norfolk Island and Flinders Island.\n\nBennett died in Sydney at the age of 98. Her papers and a collection of around 500 colour slides covering the last edition of Australian Seashores have been donated to the National Library of Australia and around 400 remaining slides.\n\nIn addition to the editions of \"Australian Seashore\", Bennett wrote the following books;\n\n"}
{"id": "13830040", "url": "https://en.wikipedia.org/wiki?curid=13830040", "title": "Joseph Allen (Doctor of Medicine)", "text": "Joseph Allen (Doctor of Medicine)\n\nJoseph Allen M.D. (ca. 1714 – 10 January 1796) was a prominent eighteenth century physician, surgeon on Lord Anson's circumnavigation of the world, and Master of the College of God's Gift in Dulwich (then colloquially called Dulwich College).\n\nJoseph Allen was born in Ireland in around 1714. He was bred a surgeon and in this capacity accompanied Commodore George Anson on his celebrated circumnavigation of the globe from 1740 to 1743.\n\nOn his return from his voyage with Lord Anson, Joseph was elected as Warden of the College of God's Gift in Dulwich on 25 February 1744. He was not in the post for much beyond two years before the Master of the College, James Allen died on the 28 October 1746. Joseph, by virtue of his position as Warden, immediately became Master.\n\nDuring his time as Master Joseph continued to pursue a medical career and obtained the degree of doctor of medicine from the University of St. Andrew's on 23 April 1754. He was admitted a Licentiate of the College of Physicians on 30 September 1765.\n\nIn his capacity as Master of the College, he did little to further Edward Alleyn's (the founder) directions to provide every poor scholar with adequate preparation for the world.\n\nHe vacated the office of Master on 21 June 1775 in order to marry a widow, Elizabeth Plaw (marriage at that time being prohibited for the Master of the College, in adherence with Alleyn's statutes).\n\nOn his retirement, his portrait was painted by George Romney, and this is still in the possession of Dulwich College. After his wife's death in 1781 he lived a further 15 years, dying on 10 January 1796 following a few days' illness. He was 82 and was the last known survivor of those who had accompanied Lord Anson. In his will he left £500 to the Asylum, £500 to the Lying-In Hospital and £200 to the Parish of Camberwell to provide coals for the poor in the Hamlet of Dulwich.\n"}
{"id": "27136597", "url": "https://en.wikipedia.org/wiki?curid=27136597", "title": "Lesser Antilles subduction zone", "text": "Lesser Antilles subduction zone\n\nThe Lesser Antilles subduction zone is a convergent plate boundary on the seafloor along the eastern margin of the Lesser Antilles island arc. In this subduction zone, oceanic crust of the South American Plate is being subducted under the Caribbean Plate.\n\n"}
{"id": "56194753", "url": "https://en.wikipedia.org/wiki?curid=56194753", "title": "List of Local Nature Reserves in Kent", "text": "List of Local Nature Reserves in Kent\n\nKent is a county in the south-eastern corner of England. It is bounded to the north by Greater London and the Thames Estuary, to the west by Sussex and Surrey, and to the south and east by the English channel and the North Sea. The county town is Maidstone. It is governed by Kent County Council, with twelve district councils, Ashford, Canterbury, Dartford, Dover, Folkestone and Hythe, Gravesham, Maidstone, Thanet, Tonbridge and Malling and Tunbridge Wells. Medway is a separate unitary authority. The chalk hills of the North Downs run from east to west through the county, with the wooded Weald to the south. The coastline is alternately flat and cliff-lined.\n\nLocal Nature Reserves are designated by local authorities under the National Parks and Access to the Countryside Act 1949. The local authority must have a legal control over the site, by owning or leasing it or having an agreement with the owner. Local Nature Reserves are sites which have a special local interest either biologically or geologically. Local authorities have a duty to care for them, and can apply local bye-laws to manage and protect them.\n\nAs of May 2018, there are 42 Local Nature Reserves in the county. Thirteen are Sites of Special Scientific Interest, five are Ramsar internationally important wetland sites, three are Nature Conservation Review sites, five are Special Protection Areas under the European Union Directive on the Conservation of Wild Birds, two are Special Areas of Conservation, one is a Geological Conservation Review site, one includes a Scheduled Monument, one is a National Nature Reserve, five are managed by the Kent Wildlife Trust and one is owned by Plantlife.\n\n\n"}
{"id": "2063037", "url": "https://en.wikipedia.org/wiki?curid=2063037", "title": "List of Minnesota aquatic plants", "text": "List of Minnesota aquatic plants\n\nAquatic plants native to Minnesota:\n\n\n\n\n\n"}
{"id": "53022798", "url": "https://en.wikipedia.org/wiki?curid=53022798", "title": "List of Ramsar sites in Turkey", "text": "List of Ramsar sites in Turkey\n\nThis is a list of Ramsar sites in Turkey. As of April 2013, there are 14 Ramsar sites, which were designated between 1994 and 2013.\n\n"}
{"id": "35231363", "url": "https://en.wikipedia.org/wiki?curid=35231363", "title": "List of Storm Prediction Center extremely critical days", "text": "List of Storm Prediction Center extremely critical days\n\nAn extremely critical fire weather event is the greatest threat level issued by the NWS Storm Prediction Center (SPC) for wildfire events in the United States. Extremely critical areas are issued only several times a year when forecasters at the SPC are confident of extremely dangerous wildfire conditions on a given day. These are typically reserved for the most extreme events. \n\nFire weather products have been issued by the Storm Prediction Center since 1998; however, there is very little online documentation for days before 2002 due to the lack of SPC archives. Therefore, it is likely that there were additional extremely critical days with no online documentation. \n\n"}
{"id": "21966911", "url": "https://en.wikipedia.org/wiki?curid=21966911", "title": "List of animals referred to as girdled", "text": "List of animals referred to as girdled\n\nGirdled identifies, various animal species:\nLizards from the genus \"Cordylus\".\n\n\n\n"}
{"id": "59018", "url": "https://en.wikipedia.org/wiki?curid=59018", "title": "List of biologists", "text": "List of biologists\n\nThis is a list of notable biologists with a biography in Wikipedia. It includes zoologists, botanists, ornithologists, malacologists, naturalists and other specialities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22463899", "url": "https://en.wikipedia.org/wiki?curid=22463899", "title": "List of climate engineering topics", "text": "List of climate engineering topics\n\nThis article is about climate engineering geoengineering topics, related to greenhouse gas remediation\n\n\n\n\n"}
{"id": "51793642", "url": "https://en.wikipedia.org/wiki?curid=51793642", "title": "List of countries and territories bordering the Atlantic Ocean", "text": "List of countries and territories bordering the Atlantic Ocean\n\nThis is a list of states and territories (in italics) with a coastline on the Atlantic Ocean (including the Baltic and Mediterranean Seas) are:\n\n\n\n\n\n"}
{"id": "13415592", "url": "https://en.wikipedia.org/wiki?curid=13415592", "title": "List of environmental engineers", "text": "List of environmental engineers\n\nEnvironmental engineers conduct hazardous-waste management studies to evaluate the significance of such hazards, advise on treatment and containment, and develop regulations to prevent mishaps. Environmental engineers also design municipal water supply and industrial wastewater treatment systems as well as address local and worldwide environmental issues such as the effects of acid rain, global warming, ozone depletion, water pollution and air pollution from automobile exhausts and industrial sources.\n\n"}
{"id": "52931254", "url": "https://en.wikipedia.org/wiki?curid=52931254", "title": "List of longest prison sentences served", "text": "List of longest prison sentences served\n\nThis is a list of longest prison sentences served by a single person, worldwide, without a period of freedom followed by a second conviction. These cases rarely coincide with the longest prison sentences given, because some countries have laws that don't allow sentences without parole or for convicts to remain in prison beyond a given number of years (regardless of their original conviction).\n\nThe sentence duration refers to the time spent in solitary confinement, regardless of time spent in normal prison before or after. Death row prisoners, who are usually also held in isolation, are not included.\n\nThese prisoners were sentenced to death rather than prison, but their execution was stalled for a prolongued time due to different reasons.\n\n\n"}
{"id": "3725555", "url": "https://en.wikipedia.org/wiki?curid=3725555", "title": "List of mountains and hills of Japan by height", "text": "List of mountains and hills of Japan by height\n\nThe following is a list of the mountains and hills of Japan, ordered by height.\n\n\"As the generally accepted definition of a mountain (versus a hill) is 1000 m of height and 500 m of prominence, the following list is provided for convenience only.\"\n\n"}
{"id": "39283575", "url": "https://en.wikipedia.org/wiki?curid=39283575", "title": "List of mountains of Switzerland above 3000 m", "text": "List of mountains of Switzerland above 3000 m\n\nThis is a list of mountains of Switzerland above . This height, in the Alps, approximately corresponds to the level of the climatic snow line. For a list of mountains (major summits only) including lower summits, see List of mountains of Switzerland. \n\nThis list only includes significant summits with a topographic prominence of at least . There are 437 such summits exceeding 3,000 m in Switzerland. They are found in the cantons of Valais, Bern, Graubünden, Uri, Glarus, Ticino, St. Gallen, Obwalden and Vaud. All mountain heights and prominences on the list are from the largest-scale maps available.\n\n"}
{"id": "56609375", "url": "https://en.wikipedia.org/wiki?curid=56609375", "title": "List of power stations in East Timor", "text": "List of power stations in East Timor\n\nThe following lists power stations in East Timor.\n\nThe Hera power station was built to supply to the North coast of the country, while the Betano power station supplies electricity to the South coast.\n"}
{"id": "18025118", "url": "https://en.wikipedia.org/wiki?curid=18025118", "title": "List of rivers of Guatemala", "text": "List of rivers of Guatemala\n\nThis is a list of rivers in Guatemala.\n\nThis list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n\n\n\n"}
{"id": "146184", "url": "https://en.wikipedia.org/wiki?curid=146184", "title": "List of tectonic plate interactions", "text": "List of tectonic plate interactions\n\nTectonic plate interactions are of three different basic types:\n\n\n\n\n"}
{"id": "8881705", "url": "https://en.wikipedia.org/wiki?curid=8881705", "title": "List of vacuum tubes", "text": "List of vacuum tubes\n\nThis is a list of vacuum tubes or \"thermionic valves\", and low-pressure gas-filled tubes, or \"discharge tubes\". Before the advent of semiconductor devices, thousands of tube types were used in consumer electronics. Many industrial, military or otherwise professional tubes were also produced. Only a few types are still used today, mainly in high-power, high-frequency applications.\n\nReceiving tubes have heaters or filaments intended for direct battery operation, parallel operation off a dedicated winding on a supply transformer, or series string operation on transformer-less sets. High-power RF power tubes are directly heated; the heater voltage must be much smaller than the signal voltage on the grid and is therefore in the 5...25 V range, drawing up to hundreds of amperes from a suitable heater transformer. In some valve part number series, the voltage class of the heater is given in the part number, and a similar valve might be available with several different heater voltage ratings.\n\n\nThe system assigned numbers with the base form \"1A21\", and is therefore also referred to as the \"1A21 system\".\nThe first numeric character indicated the filament/heater power rating, the second alphabetic character was a code for the function, and the last 2 digits were sequentially assigned, beginning with 21\n\nRMA tubes\n\nRETMA is the acronym for the \"Radio Electronic Television Manufacturers Association\" formed in 1953. \n\nOften designations that differed only in their initial numerals would be identical except for heater characteristics.\n\nFor examples see below\n\nA four-digit system was maintained by the EIA for special industrial, military and professional vacuum and gas-filled tubes, and all sorts of other devices requiring to be sealed off against the external atmosphere.\n\nSome manufacterers preceded the EIA number with a manufacterer's code:\nFor examples see below.\n\nEitel/McCullough and other manufacturers of high power RF tubes use the following code:\n\nExamples:\n\nThis system is very descriptive of what type of device (triode, diode, pentode etc.) it is applied to, as well as the heater/filament type and the base type (octal, noval, etc.). Adhering manufacturers include AEG (de), Amperex (us), CdL (1921, \"French Mazda\" brand), CIFTE (fr, \"Mazda-Belvu\" brand), EdiSwan (uk, \"British Mazda\" brand), Radiotechnique (fr, \"Coprim\", \"Miniwatt-Dario\" and \"RTC\" brands), Lorenz (de), MBLE (be, \"Adzam\" brand), Mullard (uk), Philips (nl, \"Miniwatt\" brand), RCA (us), RFT (de), Siemens (de), Telefunken (de), Tesla (cz), Toshiba (ja), Tungsram (hu), Unitra (pl, \"Dolam\", \"Polam\" and \"Telam\" brands) and Valvo (de).\n\nThis part dates back to the joint valve code key () negotiated between Philips and Telefunken in 1933–34. Like the North American system the first symbol describes the heater voltage, in this case, a Roman letter rather than a number. Further Roman letters, up to three, describe the device followed by one to four numerals assigned in a semi-chronological order of type development within number ranges assigned to different base types.\n\nIf two devices share the same type designation other than the first letter (e.g. ECL82, PCL82, UCL82) they will usually be identical except for heater specifications; however there are exceptions, particularly with output types (for example, both the PL84 and UL84 differ significantly from the EL84 in certain major characteristics, although they have the same pinout and similar power rating). However, device numbers do not reveal any similarity between different type families; e.g. the triode section of an ECL82 is not related to either triode of an ECC82, whereas the triode section of an ECL86 does happen to be similar to those of an ECC83.\n\n\"Pro Electron\" maintained a subset of the M-P system after their establishment in 1966, with only the first letters E, P for the heater, only the second letters A, B, C, D, E, F, H, K, L, M, Y, Z for the type, and issuing only three-digit numbers starting with 1, 2, 3, 5, 8, 9 for the base.\n\nNotes: Tungsram preceded the M-P designation with the letter T, as in TAD1 for AD1; \"\" (VATEA Radio Technology and Electric Co. Ltd., Budapest, Hungary) preceded the M-P designation with the letter V, as in VEL5 for EL5.\n\n\n\n\n—\"Special quality\":\n\nFor examples see below\n\nVacuum tubes which had special qualities () of some sort, very often long-life designs, particularly for computer and telecommunications use, had the numeric part of the designation placed immediately after the first letter. They were usually special-quality versions of standard types. Thus the E82CC was a long-life version of the ECC82 intended for computer and general signal use, and the E88CC a high quality version of the ECC88/6DJ8. While the E80F pentode was a high quality development of the EF80, they were not pin-compatible and could not be interchanged without rewiring the socket (the E80F is commonly sought after as a high quality replacement for the similar EF86 type in guitar amplifiers). The letters \"CC\" indicated the two triodes and the \"F\", the single pentode inside these types.\n\nA few special-quality tubes did not have a standard equivalent, e.g. the E55L, a broadband power pentode used as the output stage of oscilloscope amplifiers and the E90CC, a double triode with a common cathode connection and seven pin base for use in cathode-coupled Flip-flops in early computers. The E91H is a special heptode with a passivated third grid designed to reduce secondary emission; this device was used as a \"gate\", allowing or blocking pulses applied to the first, (control) grid by changing the voltage on the third grid, in early computer circuits (similar in function to the U.S. 6AS6).\n\nMany of these types had gold-plated base pins and special heater configurations inside the nickel cathode tube designed to reduce hum pickup from the A.C. heater supply, and also had improved oxide insulation between the heater and cathode so the cathode could be elevated to a greater voltage above the heater supply. (Note that \"elevating\" the cathode voltage \"above\" the average heater voltage, which in well-designed equipment was supplied from a transformer with an earthed center-tapped secondary, was less detrimental to the oxide insulation between heater and cathode than \"lowering\" the cathode voltage \"below\" the heater voltage, helping to prevent pyrometallurgical electrolytic chemical reactions where the oxide touched the nickel cathode that could form conductive aluminium tungstate and which could ultimately develop into a heater-cathode short-circuit.)\n\nBetter, often dual, getters were implemented to maintain a better vacuum, and more-rigid electrode supports introduced to reduce microphonics and improve vibration and shock resistance. The mica spacers used in \"SQ\" and \"PQ\" types did not possess sharp protrusions which could flake off and become loose inside the bulb, possibly lodging between the grids and thus changing the characteristics of the device. Some types, particularly the E80F, E88CC and E90CC, had a constricted section of bulb to firmly hold specially shaped flakeless mica spacers.\n\nFor examples see below, starting at DC\nLater special-quality tubes had not base and function swapped but were assigned a 4-digit number, such as ECC2000 or ED8000, the first digit of which again denoting the base:\n\nFor examples see below, starting at EC\n\"Z\" Cold-cathode SQ tubes had a different function letter scheme:\n\nFor examples, see below under Z\n\nIn use since at least 1961, this system was maintained by \"Pro Electron\" after their establishment in 1966.\n\nBoth letters together indicate the type:\n\nThen follows a 4-digit sequentially assigned number.\n\nOptional suffixes for camera tubes:\n\nVersion letter:\n\nLetter for variants derived by selection:\n\nFor examples see below\n\nThe first letter (or letter pair, in the case of a dual-system device) indicates the general type:\n\nThe following letter indicates the filament or cathode type, or the fill gas or other construction detail. The coding for vacuum devices differs between Philips (and other Continental European manufacturers) on the one hand and its Mullard subsidiary on the other.\n\nThe next letter indicates the cooling method or other significant characteristic:\n\nThe following group of digits indicate:\n\nThe following group of digits indicate the power:\n\nAn optional following letter indicates the base or connection method:\n\nFor examples see below\n\nThe first digit indicates the tube base:\n\nThe second digit is a sequentially assigned number.\n\nThe following letter indicates the photocathode type:\n\nThe following letter indicates the filling:\n\nA following letter P indicates a photomultiplier.\n\nExamples:\n\nThe first number indicates the burning voltage\n\nThe following letter indicates the current range:\n\nThe following digit is a sequentially assigned number.\n\nAn optional, following letter indicates the base:\n\n\nExamples:\n\nThe first (1888) incarnation of \"La Compagnie des Lampes\" produced the TM tube since 1915 and defined one of the first French systems; not to be confused with Compagnie des Lampes (1921, \"French Mazda\", see below).\n\nFirst letter: Heater or filament voltage\n\nSecond letter: Heater or filament current\n\nNext number: Gain\n\nNext number: Internal resistance in kΩ\n\nExamples:\n\nNote: EdiSwan also used the Mullard–Philips scheme.\n\nFirst number: Heater or filament rating\n\nFollowing letter or letter sequence: Type\n\nFinal number: Sequentially assigned number\n\nLetter(s): Type\n\n\nNumber: Sequentially assigned number\n\nExamples:\n\nNote: \"AC/\"-series receiver tubes are listed under \"other letter tubes - AC/\"\n\nThis system consists of one or more letters followed by a sequentially assigned number\n\nExamples:\n\nThe British \"Ericsson Telephones Limited\" (ETL), of Beeston, Nottingham (not to be confused with the Swedish \"TelefonAB Ericsson\"), original holder of the now-generic trademark \"Dekatron\", used the following system:\n\n\nThe British GEC–Marconi–Osram designation from the 1920s uses one or two letter(s) followed by two numerals and sometimes by a second letter identifying different versions of a particular type.\n\nThe letter(s) generally denote the type or use:\n\n\n\n\nThe following numbers are sequentially assigned for each new device.\n\nExamples:\n\nNote: \"Kinkless Tetrode\" beam power tubes are listed under \"other letter tubes - KT\"\nOlder Mullard tubes were mostly designated PM, followed by a number containing the filament voltage.\n\nMany later tubes were designated one to three semi-intuitive letters, followed by a number containing the heater voltage. This was phased out after 1934 when Mullard adopted the Mullard–Philips scheme.\n\nExamples:\n\nThe system consisted of one letter followed by 3 or 4 digits. It was phased out after 1934 when Philips adopted the Mullard–Philips scheme.\n\n1st letter: Heater current\n\n1 or 2 digit(s): Heater voltage\n\nLast 2 digits: Type\n\nExamples:\n\nFirst number: Type\n\nNext letter: Heater rating\n\nNumber: Sequentially assigned number\n\nExamples:\n\nValvo was a major German electronic components manufacturer from 1924 to 1989; a Philips subsidiary since 1927, Valvo was one of the predecessors of NXP Semiconductors.\n\nThe system consisted of one or two letters followed by 3 or 4 digits. It was phased out after 1934 when Valvo adopted the Mullard–Philips scheme.\n\nFirst letter(s): Type\nNumber:\nA following letter D indicates more than one grid, not counting a space charge grid\n\nExamples:\n\nPolish Lamina transmitter tube designations consist of one or two letters, a group of digits and an optional letter and/or two digits preceded by a \"/\" sign.\n\nThe first letter indicates the tube type, two equal letters denoting a dual tube:\n\nA group of digits represents the maximum anode power dissipation in kW\n\nAn optional letter specifies the cooling method:\n\nThe first of the two digits after the \"/\" sign means:\n\nThe second digit after the \"/\" is sequentially assigned.\n\nExamples:\n\nRundfunk- und Fernmelde-Technik was the brand of a group of telecommunications manufacturers in the German Democratic Republic. The designation consists of a group of three letters and a group of three or four digits.\n\nThe first two letters determine the tube type:\n\nThe third letter specifies the cooling method:\n\nThe first digit (or the first two digits in double tubes) indicates the number of electrodes:\n\nThe last two digits are sequentially assigned.\n\nExamples:\n\nNote: RFT used the Mullard–Philips and RETMA schemes for their low-power tubes.\n\nBesides the genuine Mullard–Philips system, Tesla also used an M-P/RETMA hybrid scheme:\n\nFirst number: Heater voltage, as in the RETMA system\n\nNext letter(s): Type, subset of the Mullard–Philips system\n\nNext digit: Base\n\nLast digit: Sequentially assigned number\n\nExamples:\n\nFirst letter:\n\nNext letter(s): Type, subset of the Mullard–Philips scheme\n\nNext number: Anode dissipation in W (if radiation cooled) or kW (otherwise)\n\nThe next letter specifies the cooling method:\n\nExamples:\n\nThe Tungsram system was composed of a maximum of three letters and three or four digits. It was phased out after 1934 when Tungsram adopted the Mullard–Philips scheme, frequently preceding it with the letter T, as in TAD1 for AD1.\n\nLetter: System type:\n\nNumber:\n\nExamples:\n\nVacuum tubes produced in the former Soviet Union and in present-day Russia are designated in Cyrillic. Some confusion has been created in transliterating these designations to Latin.\n\nThe first system was introduced in 1929. It consisted of one or two letters and a sequentially assigned number with up to 3 digits\n\nFirst letter: System type:\n\nSecond letter (optional): Type of cathode:\n\nThen a hyphen (\"-\"), followed by a sequentially assigned number\n\nExamples:\n\nIn 1937, the Soviet Union purchased a tube assembly line from RCA (who at the time had difficulties raising funds for their basic operations), including production licenses and initial staff training, and installed it on the Svetlana/Светлана plant in St. Petersburg, Russia. US-licensed tubes were produced since then under an adapted RETMA scheme.\n\nExamples:\n\nIn the 1950s a 5-element system ( \"State standard\" ГОСТ/GOST 5461-59, later 13393-76) was adopted in the (then) Soviet Union for designating receiver vacuum tubes.\n\nThe 1st element is a number specifying filament voltage in volts (rounded to the nearest whole number; 06 means 0.625 V), or, for cathode-ray tubes, the screen diagonal or diameter in cm (rounded-off to the nearest whole number).\n\nThe 2nd element is a Cyrillic character specifying the type of device:\n\nThe 3rd element is a sequentially assigned number – a series designator that distinguishes between different devices of the same type.\n\nThe 4th element denotes vacuum tube construction (base, envelope):\n\nThe 5th element is optional. It consists of a hyphen (\"-\") followed by a single character or a combination of characters, and denotes special characteristics (if any) of the tube:\n\nThere is another designation system for professional tubes such as transmitter ones.\n\nThe 1st element: function\n\nNext elements:\n\n\n\nA letter: Structure and usage\n\nThen a letter: Base and outline\n\nThen a hyphen (\"-\"), followed by a sequentially assigned number or the designation of the American original\n\nThen an optional hyphen (\"-\"), followed by a letter: Version\n\nExamples:\n\nJIS C 7001 was published in 1951 and modified in 1965 and 1970\n\nA number: Heater voltage range, as in the RETMA scheme\netc.\n\nThen a letter: Base and Outline\n\nThen a hyphen (\"-\"), followed by a letter: Structure and usage\n\nThen a sequentially assigned number\n\nThen an optional letter: Version\n\nExamples:\n\nThis system prefixes a three- or four-digit number with the letters \"CV\", meaning \"civilian valve\" i.e. common to all three armed services. It was introduced during the Second World War to rationalise the previous nomenclatures maintained separately by the War Office/Ministry of Supply, Admiralty and Air Ministry/Ministry of Aircraft Production on behalf of the three armed services (e.g. \"ACR~\", \"AR~\", \"AT~\", etc. for CRTs, receiving and transmitting valves used in army equipments, \"NC~\", \"NR~\" and \"NT~\" similarly for navy equipments and \"VCR~\", \"VR~\" and \"VT~\" etc. for air force equipments), in which three separate designations could in principle apply to the same valve (which often had at least one prototype commercial designation as well). These numbers generally have identical equivalents in both the North American, RETMA, and West European, Mullard–Philips, systems but they bear no resemblance to the assigned \"CV\" number.\n\nExamples:\n\nNote: The 4000 numbers identify special-quality valves though SQ valves CV numbered before that rule came in retain their original CV number.\n\nThe principle behind the CV numbering scheme was also adopted by the US Joint Army-Navy JAN numbering scheme which was later considerably expanded into the US Federal and then NATO Stock Number system used by all NATO countries. This part-identification system ensures that every particular spare part (not merely thermionic valves) receives a unique stock number across the whole of NATO irrespective of the source, and hence is not held inefficiently as separate stores. In the case of CV valves, the stock number is always of the format 5960-99-000-XXXX where XXXX is the CV number (with a leading 0 if the CV number only has 3 digits).\n\nOne system prefixes a three-digit number with the letters \"VT\", presumably meaning \"Vacuum Tube\". Other systems prefix the number with the letters \"JHS\" or \"JAN\". The numbers following these prefixes can be \"special\" four-digit numbers, or domestic two- or three-digit numbers or simply the domestic North American \"RETMA\" numbering system. Like the British military system, these have many direct equivalents in the civilian types.\nConfusingly, the British also had two entirely different \"VT\" nomenclatures, one used by the Royal Air Force (see the preceding section) and the other used by the General Post Office, responsible for post and telecommunications at the time, where it may have stood for \"valve, telephone\"; none of these schemes corresponded in any way with each other.\n\nExamples:\n\nVarious numeral-only systems exist. These tend to be used for devices used in commercial or industrial equipment. The oldest numbering systems date back to the early 1920s, such as a two-digit numbering system, starting with the \"UV-201A\", which was considered as \"type 01\", and extended almost continuously up into the 1980s. Three- and four-digit numeral-only systems were maintained by R.C.A., but also adopted by many other manufacturers, and typically encompassed rectifiers and radio transmitter output devices. Devices in the low 800s tend to be transmitter output types, those in the higher 800s are not vacuum tubes, but gas-filled rectifiers and thyratrons, and those in the 900s tend to be special-purpose and high-frequency devices. Use was not rigorously systematic: the 807 had variants 1624, 1625, and 807W.\n\nThere are quite a number of these systems from different geographical realms, such as those used on devices from contemporary Russian and Chinese production. Other compound numbering systems were used to mark higher-reliability types used in industrial or commercial applications. Computers and telecommunication equipment also required tubes of greater quality and reliability than for domestic and consumer equipment.\n\nSome letter prefixes are manufacterer's codes:\nFor examples, see below\n\nSome designations are derived from the behavior of devices considered to be exceptional.\n\nNote: Typecode explained above. See also RETMA tube designation\n\nFirst character is numeric zero, not letter O.\n\n\nThe following tubes were used in post-World War II walkie-talkies and pocket-sized portable radios. All have 1.25 volt DC filaments and directly heated cathodes. Some specify which end of the filament is to be powered by the positive side of the filament power supply (usually a battery). All have glass bodies that measure from wide, and from in overall length.\n\n\nThese tubes were made for home storage battery receivers manufactured during the early to mid-1930s. The numbers of the following tubes all start with 1, but these tubes all have 2.0 volt DC filaments. This numbering scheme was intended to differentiate these tubes from the tubes with 2.5 volt AC heaters listed below.\n\n\n\n\n\n\n\n\nThese tubes all have 6.3 volt AC/DC heaters.\n\n\nThese tubes all have 12.6 volt AC/DC heaters\n\n\n\n\nAll of the following tubes are designed to operate with their heaters connected directly to the 117 volt (now 120 volt) electrical mains of North America. All of them use indirectly heated cathodes. All of them incorporate at least one rectifier diode.\n\nThe tubes in this list are most commonly used in series-wired circuits.\n\n\nNote: Most of these are special quality versions of the equivalents given. Some manufacterers preceded the EIA number with a manufacterer's code, as explained above.\n\n\n\n\n\n\nMost post-war European thermionic valve (vacuum tube) manufacturers have used the \"Mullard–Philips tube designation\" naming scheme.\n\nSpecial quality variants may have the letters \"SQ\" appended, or the device description letters may be swapped with the numerals (e.g. an E82CC is a special quality version of an ECC82)\n\nNote: Typecode explained above. The part behind a slash (\"/\") is the RMA/RETMA/EIA equivalent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Many \"C\" tubes had 13V/200mA heaters, so apart from 1930s European AC/DC radios, these were also used in 12-Volts car radios\n\n\n\n\n\n\n\n\n\n\n\n\nNote: D-type tubes except some rectifiers are directly heated.\n\n\n\n\n\n\n—\"Special quality\":\n\n\n\n\n\n\n—\"Special quality\":\n\n\n\n\n\n\n\n—\"Special quality\":\n\n\n\n\n\n\n\n\n\n\n—\"Special quality\":\n\n—\"Special quality\":\n\n—\"Special quality\":\n\n—\"Special quality\":\n\n\n\n—\"Special quality\":\n\n\n\n\n\n\n—\"Special quality\":\n\n\n\n\n\n—\"Special quality\":\n\n\n—\"Special quality\":\n\n\n—\"Special quality\":\n\n\n\n\n\n—\"Special quality\":\n\n\n\n\n—\"Special quality\":\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Philips sold a family of 150mA series heater tubes under this letter in South America: OBC3, OBF2, OCH4, OH4, OF1, OF5, OF9 and OM5\n\n\nNote: Philips sold a family of 300mA series heater tubes under this letter in South America: PAB1, PBF2, PF9, PH4 and PM5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Tungsram preceded the M-P designation with the letter T, as in TAD1 for AD1\n\n\nNote: Philips sold a family of 100mA series heater tubes under this letter in South America: UBC1, UBF2, UF8 and UL1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Special-quality cold-cathode \"Z\" tubes had a different function letter scheme.\n\nSee also the professional tubes under Z\n\n\n\n\n\nNote: More Nixie tubes under \"professional - ZM\" and \"other letter - GR\"\n\n\n\n\n\n\nNote: Typecode explained above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: See also standard M-P tubes under Z\n\n\n\n\nNote: More Nixie tubes under \"standard - ZM\" and \"other letter - GR\"\n\n\n\n\n\n\nNote: Typecode explained above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEdison and Swan Electric Light Company (British Mazda/EdiSwan):\n\nMazda/EdiSwan 4-volts AC, indirectly heated receiver tubes:\n\nMarconi-Osram Valve Company:\n\nMullard:\n\nMullard:\n\nIndustrial Electronics Engineers:\n\nBurroughs:\n\nBritish Thomson-Houston (General Electric subsidiary):\n\nTung-Sol:\n\nRaytheon:\n\n\nFerranti:\n\nPhilips:\n\nMullard:\n\nCerberus:\n\nFerranti:\n\nStandard Telephones and Cables/Brimar:\n\nCerberus:\n\nEricsson Telephones Limited:\n\nEricsson Telephones Limited:\n\nEricsson Telephones Limited:\n\nEricsson Telephones Limited:\n\nFerranti:\n\nCerberus:\n\nFerranti:\n\nEricsson Telephones Limited:\n\nNote: More Nixie tubes under \"standard - ZM\" and \"professional - ZM\"\n\nCerberus:\n\nFerranti:\n\nEricsson Telephones Limited:\n\nEricsson Telephones Limited:\n\nEricsson Telephones Limited:\n\nEricsson Telephones Limited:\n\nEdgerton, Germeshausen, and Grier:\n\nMarconi-Osram \"Kinkless Tetrode\" beam power tubes\n\nTung-Sol:\n\nUK Military developed:\n\nPhilips:\n\nEdison and Swan Electric Light Company (British Mazda/EdiSwan):\n\nDale:\n\nEdison and Swan Electric Light Company (British Mazda/EdiSwan):\n\nPhilips:\n\nPhilips:\n\nRaytheon:\n\nMarconi-Osram Valve Company:\n\nRaytheon:\n\nRadio Corporation of America:\n\nCossor:\n\nBritish General Electric Company:\n\nStandard Telephones and Cables/Brimar:\n\nCompagnie Française Thomson-Houston:\n\nE.C.&A. Grammont and Compagnie des Lampes (1888):\n\nBendix:\n\nMarconi-Osram Valve Company:\n\nFerranti:\n\nEricsson Telephones Limited:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsed with AC, DC or home-based storage battery power supplies (1927–31)\n\nUsed in 1920s home radios. Filaments powered by 1.5 volt dry cells, anodes powered by storage batteries.\n\nUsed in 1930s home radios powered by storage batteries.\n\nUsed in 1920s home radios powered by dry cells (filaments) and storage batteries (B-plus voltage).\n\nUsed in 1920s home radios powered by storage batteries.\n\n\n\n\n\nNote: All have 2.5 volt heaters.\n\nNote: All have 6.3 volt heaters except type 43\n\nIn the early 1930s, the Grigsby-Grunow Company – makers of Majestic brand radios – introduced the first American-made tubes to incorporate metal shields. These tubes had metal particles sprayed onto the glass envelope, copying a design common to European tubes of the time. Early types were shielded versions of tube types already in use. (The shield was connected to the cathode.) The Majestic numbers of these tube types, which are usually etched on the tube's base, have a \"G\" prefix (for Grigsby-Grunow) and an \"S\" suffix (for shielded). Later types incorporated an extra pin in the base so that the shield could be connected directly to the chassis.\n\nReplacement versions from other manufacturers, such as Sylvania or General Electric, tend to incorporate the less expensive, form-fitting Goat brand shields that are cemented to the glass envelope.\n\nGrigsby-Grunow did not shield rectifier tubes (except for type 6Y5 listed below) or power output tubes.\n\n\n\n\nNote: Typecode explained above.\n\n\nNote: Typecode explained above.\n\n\n\n\nNot to be confused with Compagnie des Lampes (1888, see above) nor with British Mazda (see above).\n\nThe 1921 incarnation of \"La Compagnie des Lampes\" (since 1953 as \"Lampe Mazda\") made light bulbs and electronic tubes under the French \"Mazda\" brand. Many of their tubes were also available from \"Compagnie Industrielle Française des Tubes Electroniques\" (CIFTE) under their \"Mazda-Belvu\" brand, which otherwise used mostly EIA, RETMA and Mullard–Philips tube designations.\n\nExamples:\n\nBefore 1949:\n\nSince 1949 with a logo:\n\nSince 1953 as \"LAMPE MAZDA\":\n\nSince 1959 with a Faravahar logo related to Ahura Mazda:\n\n\n\n"}
{"id": "9710306", "url": "https://en.wikipedia.org/wiki?curid=9710306", "title": "Lists of abbreviations used on Commonwealth World War I medals", "text": "Lists of abbreviations used on Commonwealth World War I medals\n\nWhen a World War I medal was issued to a member of Commonwealth forces, it was issued with a Service Number, Rank, Name and Regiment. This information should be on every medal that was issued during the First World War.\n\nDuring World War I, there were four main medals issued: the 1914 Star, 1914-15 Star, Victory Medal, and the British War Medal. \nFrom a service number it is sometimes possible to determine in which part of the army a medal holder served. However, this is not always the case.\n\nThere are many abbreviations for the rank and unit. The British National archive gives full listings of both Ranks and Units.\n\n\n\n\nThe following is an estimated list of all the Prefix Letter & Numbers (of what?).\n\n\n\n\n"}
{"id": "1951768", "url": "https://en.wikipedia.org/wiki?curid=1951768", "title": "Matthew 3", "text": "Matthew 3\n\nMatthew 3 is the third chapter of the Gospel of Matthew in the New Testament. It is the first chapter dealing with the ministry of Jesus with events taking place some three decades after the close of the infancy narrative related in the previous two chapters. The focus of this chapter is on the preaching of John the Baptist and the Baptism of Jesus.\n\nFor the first time since there are clear links with the Gospel of Mark. Many scholars are certain a good portion of this chapter is a reworking of Mark 1. The chapter also parallels Luke 3, also believed to be based on Mark 1. A number of passages shared by Luke and Matthew, but not found in Mark, are commonly ascribed to the hypothetical source 'Q'.\n\nThe New King James Version organises this chapter as follows:\n\nThe chapter opens with a portrait of John the Baptist. It describes his preaching, clothing, and diet, presenting him as a preacher in the wilderness prophesizing about the \"wrath to come\". The chapter then moves to a tirade, ascribed to John, against the Pharisees and Sadducees in which he warns them to repent. This includes the famous \"generation of vipers\" line at . Jesus then arrives from Galilee to be baptized. The chapter closes with the Baptism of Jesus, the voice of the Father, and the appearance of the Holy Spirit in the form of a dove.\n\nIn the King James Version this chapter reads:\n\nIn those days came John the Baptist, preaching in the wilderness of Judaea,<br>\nAnd saying, Repent ye: for the kingdom of heaven is at hand.<br>\nthis is he that was spoken of by the prophet Esaias, saying, The voice of one crying in the wilderness, Prepare ye the way of the Lord, make his paths straight.<br>\nAnd the same John had his raiment of camel's hair, and a leathern girdle about his loins; and his meat was locusts and wild honey.<br>\nThen went out to him Jerusalem, and all Judaea, and all the region round about Jordan,<br>\nAnd were baptized of him in Jordan, confessing their sins.<br>\nBut when he saw many of the Pharisees and Sadducees come to his baptism, he said unto them, O generation of vipers, who hath warned you to flee from the wrath to come?<br>\nBring forth therefore fruits meet for repentance:<br>\nAnd think not to say within yourselves, We have Abraham to our father: for I say unto you, that God is able of these stones to raise up children unto Abraham.<br>\nAnd now also the axe is laid unto the root of the trees: therefore every tree which bringeth not forth good fruit is hewn down, and cast into the fire.<br>\nI indeed baptize you with water unto repentance. but he that cometh after me is mightier than I, whose shoes I am not worthy to bear: he shall baptize you with the Holy Ghost, and with fire:<br>\nWhose fan is in his hand, and he will thoroughly purge his floor, and gather his wheat into the garner; but he will burn up the chaff with unquenchable fire.<br>\nThen cometh Jesus from Galilee to Jordan unto John, to be baptized of him.<br>\nBut John forbad him, saying, I have need to be baptized of thee, and comest thou to me?<br>\nAnd Jesus answering said unto him, Suffer it to be so now: for thus it becometh us to fulfil all righteousness. Then he suffered him.<br>\nAnd Jesus, when he was baptized, went up straightway out of the water: and, lo, the heavens were opened unto him, and he saw the Spirit of God descending like a dove, and lighting upon him:<br>\nAnd lo a voice from heaven, saying, This is my beloved Son, in whom I am well pleased.\n\n\n"}
{"id": "166543", "url": "https://en.wikipedia.org/wiki?curid=166543", "title": "Mount Parnassus", "text": "Mount Parnassus\n\nMount Parnassus (; , \"Parnassos\") is a mountain of limestone in central Greece that towers above Delphi, north of the Gulf of Corinth, and offers scenic views of the surrounding olive groves and countryside. According to Greek mythology, this mountain was sacred to Dionysus and the Dionysian mysteries; it was also sacred to Apollo and the Corycian nymphs, and it was the home of the Muses. The mountain was also favored by the Dorians. It is suggested that the name derives from \"parnassas\", the possessive adjective of the Luwian word \"parna\" meaning \"house\", or specifically \"temple\", so the name effectively means the \"mountain of the house of the god\".\n\nParnassus is one of the largest mountainous regions of Mainland Greece and one of the highest Greek mountains. It spreads over three municipalities, namely of Boeotia, Phthiotis and Phocis, where its largest part lies. Its altitude is 2,457 meters and its highest peak is Liakouras. To the Northeast it is connected to Giona and to the south with Kirphe. Its name is due to the homonymous hero of the Greek mythology, son of Cleopompus (or Poseidon) and Cleodora, who had built on the mountain a city which was destroyed in the Deluge of Deukalion. Etymological analysis, however, shows a prehellenic origin of the name, relating it to the Pelasgians, and it appears to be from the Anatolian language Luwian. \nThe mountain is delimited to the east by the valley of the Boeotian Kephissus and to the West by the valley of Amfissa. The geological particularity of Parnassus is its rich deposits of bauxite, which has led to their systematic mining since the end of the 1930s, resulting in ecological damage to part of the mountain.\n\nMount Parnassus is named after Parnassos, the son of the nymph Kleodora and the man Kleopompus. A city, of which Parnassos was leader, was flooded by torrential rains. The citizens ran from the flood, following wolves' howling, up the mountain slope. There the survivors built another city, and called it Lykoreia, which in Greek means \"the howling of the wolves.\" While Orpheus was living with his mother and his eight beautiful aunts on Parnassus, he met Apollo who was courting the laughing muse Thalia. Apollo became fond of Orpheus and gave him a little golden lyre, and taught him to play it. Orpheus's mother taught him to make verses for singing. As the Oracle of Delphi was sacred to the god Apollo, so did the mountain itself become associated with Apollo. According to some traditions, Parnassus was the site of the fountain Castalia and the home of the Muses; according to other traditions, that honor fell to Mount Helicon, another mountain in the same range. As the home of the Muses, Parnassus became known as the home of poetry, music, and learning.\n\nParnassus was also the site of several unrelated minor events in Greek mythology.\n\nParnassus was also the home of Pegasus, the winged horse of Bellerophon.\n\nThis relation of the mountain to the Muses offered an instigation to its more recent \"mystification\", with the poetic-artistic trend of the 19th century called \"Parnassism\". The Parnassic movement was established in France in the decade 1866–1876 as a reaction to Romanticism with a return to some classicistic elements and belief in the doctrine \"Art for the Art\", first expressed by Theophile Gautier. The periodical \"Modern Parnassus\" issued for the first time by Catul Mendes and Xavier Ricard contained direct references to Mt. Parnassus and its mythological feature as habitation of the Muses. The Parnassists, who did not exceed a group of twenty poets, exercised a relatively strong influence on the cultural life of Paris, particularly due to their tenacity on perfection of rhyme and vocabulary. Parnassism influenced several French poets, but it also exercised an influence on Modern Greek poets, particularly Kostis Palamas and Gryparis.\nThe name of the mountain (Mont Parnasse) was also given to a quarter of Paris on the left bank of the Seine, where artists and poets used to gather and recite their poems in public. Montparnasse is nowadays one of the most renowned quarters of the city and in its cemetery many personalities of the arts and culture are buried.\n\nParnassus figures earlier in Jonathan Swift's \"The Battle of the Books\" (1697) as the site of an ideological war between the ancients and the moderns.\n\nThe significant biodiversity, both in flora and in fauna, led the authorities to the establishment of the National Park of Parnassus in 1938, the year when the systematic mining of bauxite started. The Park comprises a landscape of spreading on the mountainous region between Delphi, Arachova and Agoriani. Among the endemic flora species under protection are the Cephalonian fir tree and the Parnassian peony (\"Paeonia parnassica\"). In the Park sojourn prey birds and wolves, boars, badgers and weasels.\nThe slopes of Mount Parnassus are composed of two ski sections, Kellaria and Fterolakka, which together make up the largest ski center in Greece. A smaller ski center (only two drag lifts) called Gerontovrahos is across a ridge from Kellaria. Parnassus is mined for its abundant supply of bauxite which is converted to aluminium oxide and then to aluminium.\n\nThe construction of the ski resort started in 1975 and was completed in 1976, when the first two drag lifts operated in Fterolaka. In 1981 the construction of a new ski area was completed in Kelaria, while in winter season 1987–1988 the chair lift Hermes started operating and connected the two ski areas. Both ski resorts continued expanding and in 1993 the first high-speed quad in Greece was installed, named Hercules. In 2014–2015 two new hybrid lifts were installed along with a new eight-seater, replacing the old infrastructure.\n\nToday the Ski Center operates with 16 lifts, two hybrid ski lifts which combine an eight-seater Cabin and a six-seater chair, an eight-seater Cabin, a 4-seater chair lift, a 2-seater chair lift, 6 drag lifts and four baby lifts. The ski center boasts 25 marked ski runs and about 15 ski routes of total length while the longest run is .\n\n\n"}
{"id": "52285396", "url": "https://en.wikipedia.org/wiki?curid=52285396", "title": "National Space Program (Algeria)", "text": "National Space Program (Algeria)\n\nThe national space program program (PSN) horizons 2020 planned to put in place space infrastructures, space systems and increase the specialized human resources in space technologies.\nAmong the space systems planned in the PSN EO Satellites (Alsat-2A, Alsat -2b, Alsat -3, Alsat -4, African resource management ARM and the communications satellite Alcomsat-1), of which a significant number should be partly or totally integrated in the Algerian center for satellite development \"CDS\". CDS offers the technological environment for national competence to develop the future Algerian satellite systems. Algeria's objectives is to make of space tools a powerful instrument in national prosperity in the fields of earth observation, meteorology and communications.\n\nCDS comprises: \nThe CDS is organized in 6 departments:\nthese departments house mechanical & thermal, electrical, electronic and optics research and development laboratories.\n\nThe satellite integration building consistes of a large clean room of class 100000, with 4 subareas dedicated to:\nthese areas are separated by sliding curtains.\n\nThe satellite environmental tests building is also planned for the future, this building shall consiste of:\n\nMissions: \n\nThe \"Alsat program \" is a constellation of Algerian earth observation satellites operated by the Algerian Space Agency. Algeria has three (3) operational units and one (01) retired mission.\n\nAlsat-1 is the first earth observation satellite of Algeria. Its principal mission is monitoring natural resources. It was a Disaster Monitoring Constellation (DMC) satellite that is developed and coordinated by Surrey Satellite Technology Ltd (SSTL).\n\nThe satellite is a 60-centimeter cube, and weighs about 92 kilograms. The imaging system covers the green, red, and the near infrared, at a resolution of 32 meters.\n\nIt was placed in orbit by a kosmos-3M launch vehicle from the Russian cosmodrome of Plessetsk, on 28 November 2002. Its orbit is heliosynchronous at an altitude of about 700 kilometers with orbital inclination of 98°.\n\nIts first images were received on December 17, 2002 at the reception stations of the Center of space technics (Centre national des techniques spatiales CNTS) at Arzew, Algeria. Alsat-1 completed its mission on 15 August 2010.\n\nOn February 1, 2006, EADS Astrium announced the signature of a contract for the realisation of two (02) satellites (Alsat-2A et Alsat-2B) of the Alsat-2 Program. The Alsat-2 program includes also the establishment of two ground control segments and an image terminal to control and pilot the satellites from Algerian territory.\n\nAlsat-2A is a high resolution earth observation satellite that was integrated and tested in France at the d'EADS Astrium workshops with participation of 29 Algerian engineers. It was launched by a Polar Satellite Launch Vehicle on 12 July 2010. With a resolution of 2.5m in panchromatic mode, and of 10m in multispectral mode, it provides satellite images for multiple applications such as topography, agriculture, cartography, and protection of the environment.\n\nThe satellite is based on the Myriade platform of the Centre national d'études spatiales (CNES) and is placed in a polar heliosynchonous orbit.\n\nIt has the following characteristics:\n\nAlsat-2b is the 2nd high resolution earth observation satellite and the 2nd unit of the Alsat-2 program launched on 26 September 2016 with a PSLV-C35 rocket.\n\nAlsat-2b was made in Algeria by ASAL, weights about 125 kg.\nits characteristics and payload are not yet published by the Algerian space agency. Alsat-2B is operational. Images taken by Alsat 2B in muptispectral mode (visible and near infra-red) and in panchromatic mode will be invested in important thematic and economic fields such as: urban and agricultural planification of the territory and littoral, cartography, fellowing mega-projects etc.\n\nAlsat-1B is a medium resolution earth observation satellite based on the SSTL-100 platform. the satellite weights 110 kg. It was integrated at the center of development of satellites of the Algerian space agency in Oran.\nits images are used for the management of natural disasters and the protection of the Environment.\n\nAlsat-1b was launched on 26 September 2016 by a PSLV-C35 launch vehicle. Alsat-1B is a Disaster Monitoring Constellation (DMC) satellite coordinated by Surrey Satellite Technology Limited (SSTL).\n\nThe satellite carries.:\n\n1- ALITE ( ALgerian Imager TElescope): delivers medium resolution images of 12m in panchromatic mode and 24m in multispectral mode.\n\n2- radiation monitor: permits a better understanding of the environment of radiation that satellites circulating in low earth orbit are exposed to.\n\n\n\nAlsat-1N or Alsat-Nano is a 3U technology demonstration nano-satellite,weight of 3.5 kg.built by an Algerian-British team in application of the cooperation agreement signed between the Algerian Space Agency and the l'UK Space Agency. 18 Algerian engineers used the satellite as a pedagogical tool during their graduation research at Surray University.\n\nthe main mission objectives are test of three (03) innovative components:camera, fin solar films and a measure of radiation. Data obtained by the satellite will be studied by Algerian and British researchers. Alsat-1N was launched on September 26, 2016 by an ISRO PSLV\n\n\n3 Scientific and technology demonstration payloads:\n\n\nIs a telecommunications satellites program.\n\nAlcomsat-1 was launched on Dec. 11, 2017 by LM-3B from Xichang Satellite Center. After several maneuvers, it reached the orbital location of 24.8°W. Following the IOAR which was accomplished after the in-orbit test, ASAL confirmed that the in-orbit delivery occurred on March 1.\n\nThe Alcomsat-1 is the ninth telecommunication satellite delivered to the international client by China Aerospace; it is also the first cooperation with Algeria in aerospace industry.\n\nThe Alcomsat-1 satellite program is the first communications satellite program of Algeria. It covers the Algerian territory and the surrounding area, will be mainly used in the fields of broadcast, emergency communications, remote education, e-government, enterprise communications, satellite broadband, and satellite based augmentation system application, etc..\n\nAlsat-3 and Alsat-4 are also planned to cover national needs \n\nThe Algerian Space Agency in charge of the National Space Program of Algeria is now capable of:\n\n\n\n"}
{"id": "22276151", "url": "https://en.wikipedia.org/wiki?curid=22276151", "title": "SY4307A", "text": "SY4307A\n\nThe EIA type 4307A is a power output pentode possessing a similar power rating, but significantly different characteristics to the far more common type 807 thermionic valve/vacuum tube. The \"SY\" prefix denotes the site of manufacture as being Sydney. The plant, operated by Standard Telephones and Cables Pty. Ltd. was located on Mandible Street in the suburb of Alexandria and existed under that corporate entity from 1937 to 1970.. Although the remaining structure postdates the depicted device, this device was likely assembled on an adjoining property upon which the structures have been demolished and site re-purposed.\n\nElectrically equivalent devices marked \"307A\" were manufactured in North America by Western Electric and these possibly predate the Australian version.\n\nAlthough the 807 is used for both audio amplification, (usually class AB), and radio frequency amplification in the 1-30 MHz range, (usually class C), this type was usually used for the latter.\nThis tube/valve is a pentode with a 'directly heated cathode', meaning that the cathode consists of an emissive oxide coated filament. These features suggest the 4307A could predate and is contemporary with, the 807 which is a beam power tetrode, a more advanced design with a more linear transfer characteristic and a more complex 'indirectly heated cathode'.\n\nIf the pinout of the 4307A is compared to that of the 807 the similarity is striking. All pins are assigned equivalent electrodes with the exception of pin 4, which is the suppressor grid, g3, in this type but in the 807 pin 4 is connected to the equivalent of a suppressor grid, the 'beam plates' and also the separate indirectly heated cathode.\nIt is possible to interchange the two with the relatively minor alteration involving the linking of pins 4 and 5 and ensuring the heater supply is floating (not connected to other heaters). If the heater is supplied from an A.C. source, linking pin 4 to the centre of that source and the cathode bootstrap minimizes hum pickup.\n\nThe STC/Brimar-made SY4307A was used extensively during World War II by the Australian armed forces and the example depicted is marked on the Bakelite collar of its base with the \"D broad arrow D\" commonwealth department of defense mark.\n\nTwo SY4307As were used, wired in parallel, as the output stage, or \"final\", in a transmitter constructed clandestinely by the Australian soldiers of Sparrow Force in Japanese occupied Portuguese Timor in 1942. This transmitter became known affectionately as \"Winnie the War Winner\", named after the British wartime Prime Minister, Winston Churchill.\n\n"}
{"id": "54978671", "url": "https://en.wikipedia.org/wiki?curid=54978671", "title": "Southeast Tibet shrub and meadows", "text": "Southeast Tibet shrub and meadows\n\nThe Southeast Tibet shrub and meadows are a montane grassland ecoregion that cover the southeast and eastern parts of the Tibetan Plateau in China. The meadows in this region of Tibet are in the path of the monsoon rains and are wetter than the other upland areas of the Tibetan Plateau. Chinese provinces covered by the Southeast Tibet shrub and meadows include the alpine parts of eastern Tibet Autonomous Region, the alpine parts of western and northern Sichuan, extreme southern and eastern Qinghai, and the montane areas of southern Gansu. Many mountain ranges support the Southeast Tibet meadows, stretching from the Nyainqêntanglha Mountains in the southwest to the Qilian Mountains in the northeast.\n\nInfluenced by the Asian monsoon, he Southeast Tibet shrub and meadows experience seasonal climatic variation. Dry conditions in the winter lead to primarily brown landscapes, but summer rains revitalize grasses and turn the landscapes green. To the northwest, the meadows transition to the drier Tibetan Plateau alpine shrub and meadows.\n\nHuman activity is fairly high as the Southeast Tibet shrub and meadows provide grazing areas for domestic animals such as yak. While not well suited for most agriculture, pastoralist Tibetan people live across the region. The northeastern part of this ecoregion was traditionally known as Amdo to Tibetans, while the southwestern part made up the higher elevation portions of Kham.\n"}
{"id": "642136", "url": "https://en.wikipedia.org/wiki?curid=642136", "title": "Specularity", "text": "Specularity\n\nSpecularity is the visual appearance of specular reflections. \n\nIn computer graphics, it means the quantity used in three-dimensional (3D) rendering which represents the amount of reflectivity a surface has. It is a key component in determining the brightness of specular highlights, along with shininess to determine the size of the highlights.\n\nIt is frequently used in real-time computer graphics and ray tracing, where the mirror-like specular reflection of light from other surfaces is often ignored (due to the more intensive computations required to calculate it), and the specular reflection of light directly from point light sources is modelled as specular highlights.\n\nA materials system may allow specularity to vary across a surface, controlled by additional layers of texture maps.\n\nEarly shaders included a parameter called \"Specularity\". CG Artists, confused by this term discovered by experimentation that the manipulation of this parameter would cause a reflected highlight from a light source to appear and disappear and therefore misinterpreted \"specularity\" to mean \"light highlights\". In fact \"Specular\" is defined in optics as \"Optics. (of reflected light) directed, as from a smooth, polished surface (opposed to diffuse ).\" A specular surface is a highly smooth surface. When the surface is very smooth, the reflected highlight is easy to see. As the surface becomes rougher, the reflected highlights gets broader and dimmer. This is a more \"diffused\" reflection.\n\nThis misinterpretation has been taught from generation to generation of CG artists creating significant cost problems in visual effects problems as artists and technicians struggle to understand how cryptically and inappropriately named shader parameters are intended to work.\n\nIn the context of seismic migration, specularity is defined as the cosine of the angle made by the surface normal vector and the angle bisector of the angle defined by the directions of the incident and diffracted rays. For a purely specular seismic event the value of specularity should be equal to unity, as the angle between the surface normal vector and the angle bisector should be zero, according to Snell's Law. For a diffractive seismic event, the specularity can be sub-unitary. During the seismic migration, one can filter each seismic event according to the value of specularity, in order to enhance the contribution of diffractions in the seismic image. Alternatively, the events can be separated in different sub-images according to the value of specularity to produce a specularity gather.\n\n"}
{"id": "19458931", "url": "https://en.wikipedia.org/wiki?curid=19458931", "title": "Subglacial mound", "text": "Subglacial mound\n\nA subglacial mound (SUGM) is a type of subglacial volcano. This type of volcano forms when lava erupts beneath a thick glacier or ice sheet. The magma forming these volcanoes was not hot enough to melt a vertical pipe right through the overlying glacial ice, instead forming hyaloclastite and pillow lava deep beneath the glacial ice field. Once the glaciers had retreated, the subglacial volcano would be revealed, with a unique shape as a result of their confinement within glacial ice. They are somewhat rare worldwide, being confined to regions which were formerly covered by continental ice sheets and also had active volcanism during the same period. They are found throughout Iceland, Antarctica and the Canadian province of British Columbia.\n\nSubglacial mounds can be mistaken for cinder cones because they may have a similar shape. An example of this confusion is Pyramid Mountain in the Wells Gray-Clearwater volcanic field of east-central British Columbia, Canada.\n"}
{"id": "1088639", "url": "https://en.wikipedia.org/wiki?curid=1088639", "title": "The Battle of Olympus", "text": "The Battle of Olympus\n\nThe Battle of Olympus is a 1988 action rpg video game for the Nintendo Entertainment System. It was released in North America in 1989, in Europe in 1991. A port for the Game Boy was also released. \n\"The Battle of Olympus\" takes place in an ancient Greece which is being terrorized by Hades, the dark ruler of the underworld.\n\nHelene, the girlfriend of Orpheus, is kidnapped by Hades who is holding her captive. A top-down map of Greece shows various dungeons and ancient Greek city-states for the player to visit on their journey. Swords, shields, and crystals help to provide offensive power and defensive strength for the player. Three fragments of love are there to remind Orpheus of his girlfriend Helene. Hades rules his dominion in Tartarus, where his strongest minions live alongside him.\n\nDuring his adventure, Orpheus needs to meet the Greek gods and gain their favor, starting with Zeus, the leader of the Olympian gods, who encourages the other gods to grant Orpheus powers. These powers are in the form of a weapon, a shield, and other special equipment, among them a harp, which summons Pegasus to carry Orpheus to far locations. As the game progresses, players are exposed to various forms of upgraded weaponry. The player starts off with a basic wooden club. The player later obtains items such as the Staff of Fennel (also known as a thyrsus, which is able to project a fireball), Nymph's Sword, and the Divine Sword (able to project a lightning bolt).\n\nThe game features encounters with mythological creatures such as the Taurus, Lamia, cyclops, centaur, Talos, Minotaur, Medusa, Cerberus, Stymphalian birds, Nemean lion, and also a Siren. Players must fight their way deep into the underworld, fight and defeat Hades, and finally save Helene. Several items depicted in the Greek mythology are acquired, such as the Harp of Apollo, the Sword of Hephaestus, the Staff of Prometheus, and the Sandals of Hermes.\n\nPrimarily a sidescroller with some light platforming, the player is a hero with a sword and shield, while just a handful of secondary items come along later in the game, mostly for the purpose of advancement as the protagonist move to new areas. He can find upgrades maximum health, protection, and speed. The biggest differences in combat from Zelda II are the lack of the downward stab in midair, and the lack of the sword beam at full health. However, \"The Battle of Olympus\" does have a sword later in the game that can shoot beams, but doing so damages the health bar if a certain item is not found first.\n\n\"The Battle of Olympus\" is somewhat non-linear and there are some optional side quests that can make the game easier in the long run, leading to a secret item or power-up from one of the gods.\n\nThe final battle brings to the Temple of Hades in fight with the ruler of the underworld in a two-stage event. The first is a blind fight against a shadow reflection, and then the final form is of Hades himself.\n\nIt was developed by Infinity, and was the first game from the company. The dev team for the game was quite small, with only three members. Yokio Horimoto served as designer and programmer for the game. Kazuo Sawa was the composer, and Reiko Oshida did story and graphics.\n\nDesigner Hoshimoto drew inspiration from \"\" for gameplay. Similarities include a final confrontation with a shadow, and similar skills, and general appearance of the game. It was one of several NES games inspired by \"Zelda II\", which include \"Moon Crystal\", and \"Faxanadu\". \n\nIt was published by Imagineer and released in Japan on March 28, 1988. In North America it was released in December 1989, and published by Brøderbund. It was released in Europe in 1991 and published by Nintendo. \n\nThere's also a Game Boy port of the same game that was published by Imagineer exclusively for the European market. Despite its strong resemblance to the NES original, Infinity had no involvement in that version and was instead ported by the Canadian developer Radical Entertainment.\n\nJapanese game magazine \"Famitsu\" gave it a score of 28 out of 40. Reviewers noted similarities between the game and to the earlier game, \"\".\n"}
{"id": "5190298", "url": "https://en.wikipedia.org/wiki?curid=5190298", "title": "Volcanic block", "text": "Volcanic block\n\nA volcanic block is a fragment of rock that measures more than in diameter and is erupted in a solid condition. \n\nBlocks are formed from material from previous eruptions or from country rock and are therefore mostly accessory or accidental in origin. Blocks also occur due to the impact and breakage of volcanic bombs (a bomb is a block with streamlined appearance, often expelled in a molten state). Bombs can also occur due to the disruption of the crust of a lava dome that has formed up or over a vent during an eruption. \n\nBlocks are nearly always angular to sub-angular and roughly equidimensional. If the parent rock is flow-foliated lava, sedimentary material or schistose metamorphic rocks, the blocks may have a plate-like or slab-like form. In other cases, blocks derived from great depths may resemble polished water-worn pebbles and are cobbled due to fluidisation and upwards transport.\n\nBlocks can be enormous and may be transported great distances from the volcanic vent. The 1924 eruption of Kīlauea, Hawaii, expelled rocks weighing up to 14 tons; and Mount Vesuvius in Italy discharged blocks weighing 2-3 tons over distances of 100-200m.\n"}
{"id": "780590", "url": "https://en.wikipedia.org/wiki?curid=780590", "title": "Vredefort crater", "text": "Vredefort crater\n\nThe Vredefort crater is the largest verified impact crater on Earth, more than across when it was formed. What remains of it is located in the present-day Free State province of South Africa and named after the town of Vredefort, which is situated near its centre. Although the crater itself has long since eroded away, the remaining geological structures at its centre are known as the Vredefort Dome or Vredefort impact structure. The crater is estimated to be 2.023 billion years old (± 4 million years), with impact being in the Paleoproterozoic Era. It is the second-oldest-known crater on Earth.\n\nIn 2005, the Vredefort Dome was added to the list of UNESCO World Heritage sites for its geologic interest.\n\nThe asteroid that hit Vredefort is estimated to have been one of the largest ever to strike Earth (at least since the Hadean Eon some four billion years ago), thought to have been approximately in diameter. The bolide that created the Sudbury Basin could have been even larger.\n\nThe original crater was estimated to have a diameter of roughly , although this has been eroded away. It would have been larger than the Sudbury Basin and the Chicxulub crater. The remaining structure, the \"Vredefort Dome\", consists of a partial ring of hills 70 km in diameter, and are the remains of a dome created by the rebound of rock below the impact site after the collision.\n\nThe crater's age is estimated to be 2.023 billion years (± 4 million years), which places it in the Paleoproterozoic Era. It is the second-oldest-known crater on Earth, a little less than 300 million years younger than the Suavjärvi crater in Russia. In comparison, it is about 10% older than the Sudbury Basin impact (at 1.849 billion years).\nThe dome in the centre of the crater was originally thought to have been formed by a volcanic explosion, but in the mid-1990s, evidence revealed it was the site of a huge bolide impact, as telltale shatter cones were discovered in the bed of the nearby Vaal River.\n\nThe crater site is one of the few multiple-ringed impact craters on Earth, although they are more common elsewhere in the Solar System. Perhaps the best-known example is Valhalla crater on Jupiter's moon Callisto, although Earth's Moon has a number, as well. Geological processes, such as erosion and plate tectonics, have destroyed most multiple-ring craters on Earth.\nThe impact distorted the Witwatersrand Basin which was laid down over a period of 250 million years between 950 and 700 million years before the Vredefort impact. The overlying Ventersdorp lavas and the Transvaal Supergroup which were laid down between 700 and 80 million years before the meteorite strike, were similarly distorted by the formation of the crater. These rocks form partial concentric rings round the crater centre today, with the oldest, the Witwatersrand rocks, forming a semicircle 25 km from the centre. Since the Witwatersrand rocks consist of several layers of very hard, erosion resistant sediments (e.g. quartzites and banded ironstones), they form the prominent arc of hills that can be seen to the NW of the crater centre in the satellite picture above. The Witwatersrand rocks are followed, in succession, by the Ventersdorp lavas at a distance of about 35 km from the centre, and the Transvaal Supergroup, consisting of a narrow band of the Ghaap Dolomite rocks and the Pretoria Subgroup of rocks, which together form a 25- to 30-kilometre-wide band beyond that. \nFrom about halfway through the Pretoria Subgroup of rocks around the crater centre, the order of the rocks is reversed. Moving outwards towards where the crater rim used to be, the Ghaap Dolomite group resurfaces at 60 km from the centre, followed by an arc of Ventersdorp lavas, beyond which, at between 80 and 120 km from the centre, the Witwatersrand rocks re-emerge to form an interrupted arc of outcrops today, of which the Johannesburg group is the most famous, because it was here that gold was discovered in 1886. It is thus possible that if it had not been for the Vredefort impact this gold would never have been discovered.\n\nThe centre of the Vredefort crater consists of a granite dome (where it is not covered by much younger rocks belonging to the Karoo Supergroup) which is an exposed part of the Kaapvaal craton, one of the oldest microcontinents which formed on Earth 3,900 million years ago. This central peak uplift, or dome, is typical of a complex impact crater, where the liquefied rocks splashed up in the wake of the meteor as it penetrated the surface.\n\nThe Vredefort Dome World Heritage Site is currently subject to property development, and local owners have expressed concern regarding sewage dumping into the Vaal River and the crater site. The granting of prospecting rights around the edges of the crater has led environmental interests to express fear of destructive mining.\n\nThe Vredefort Dome in the centre of the crater is home to four towns, namely Parys, Vredefort, Koppies and Venterskroon. Parys is the largest and a tourist hub; both Vredefort and Koppies mainly depend on an agricultural economy.\n\nOn 19 December 2011, a broadcasting license was granted by ICASA to a community radio station to broadcast for the Afrikaans- and English-speaking members of the communities within the crater. The Afrikaans name \"Koepel\" Stereo (Dome Stereo) refers to the dome and announces its broadcast as KSFM. The station broadcasts on 94.9 MHz FM.\n\n\n"}
