{"id": "10680694", "url": "https://en.wikipedia.org/wiki?curid=10680694", "title": "120° parhelion", "text": "120° parhelion\n\nA 120° parhelion (plural: 120° parhelia) is a relatively rare halo, an optical phenomenon occasionally appearing along with very bright sun dogs (also called parhelia) when ice crystal-saturated cirrus clouds fill the atmosphere. The 120° parhelia are named for appearing in pair on the parhelic circle ±120° from the sun.\n\nWhen visible, 120° parhelia appear as white-bluish bright spots on the white parhelic circle and are the product of at least two interior reflections in the hexagonal ice crystals. Their colour together with them being rather obscure can make observing them difficult as they tend to fuse with the clouds in the sky.\n\n\n"}
{"id": "37135322", "url": "https://en.wikipedia.org/wiki?curid=37135322", "title": "2012 UK meteoroid", "text": "2012 UK meteoroid\n\nThe 2012 UK meteoroid was an object that entered the atmosphere above the United Kingdom on Friday, 21 September 2012, around 11pm. Many news agencies across the UK reported this fact.\n\nSeveral theories were made as to the origin of the sightings - from it being a meteoroid to a UFO. Initially, the most prominent theory was that it was an old artificial satellite (i.e. a large piece of space junk) re-entering the atmosphere. However, later analysis showed that it was highly unlikely to be space junk; it travelled too fast, towards the slow end of the range of possible meteor speeds and, in addition, it traversed the sky from east to west while almost all satellites orbit from west to east or north and south.\n\nAccording to Finnish mathematician Esko Lyytinen, the meteor was captured by Earth's gravity and entered the atmosphere once again above USA and Canada 155 minutes later. If confirmed, this would classify it as an Earth-grazing meteor.\n\n\n12. Phil Williams (January 2015) \"The Meteoric Earth-Grazing Fireball of September 2012\" Liverpool Astronomical Society Monthly Newsletter (January 2015, pp. 5–9)\n"}
{"id": "56006471", "url": "https://en.wikipedia.org/wiki?curid=56006471", "title": "2017 VL2", "text": "2017 VL2\n\n is a Apollo asteroid, the largest subgroup of near-Earth objects. It orbits the Sun at a distance of 0.9–1.5 AU once every 16 months (498 days; semi-major axis of 1.23 AU). Its orbit has an eccentricity of 0.23 and an inclination of 12° with respect to the ecliptic. It is, however, not a Mars-crossing asteroid, as its aphelion of 1.51 AU is less than the orbit of the Red Planet at 1.666 AU.\n\nThe object has a minimum orbital intersection distance with Earth of , which corresponds to 0.5 lunar distances. On 9 November 2017, it came within 0.31 lunar distances of the Earth \"(see diagrams)\".\n\n has been estimated to measure between 6 and 32 meters in diameter, comparable to the Chelyabinsk meteor, which was also not observed before it hit the atmosphere over Russia in 2013. For an assumed albedo of 0.20, which is typical for the common S-type asteroids, 's diameter would be likely 18 meters only.\n\nAs of 2018, no rotational lightcurve of this asteroid has been obtained from photometric observations. The object's rotation period, pole and shape remain unknown.\n\nThis minor planet has not yet been numbered by the Minor Planet Center and remains unnamed.\n\n"}
{"id": "25213120", "url": "https://en.wikipedia.org/wiki?curid=25213120", "title": "Analysis of water chemistry", "text": "Analysis of water chemistry\n\nWater chemistry analyses are carried out to identify and quantify the chemical components and properties of water samples. The type and sensitivity of the analysis depends on the purpose of the analysis and the anticipated use of the water.\nChemical water analysis is carried out on water used in industrial processes, on waste-water stream, on rivers and stream, on rainfall and on the sea. In all cases the results of the analysis provides information that can be used to make decisions or to provide re-assurance that conditions are as expected.\nThe analytical parameters selected are chosen to be appropriate for the decision making process or to establish acceptable normality.\nWater chemistry analysis is often the groundwork of studies of water quality, pollution, hydrology and geothermal waters.\nAnalytical methods routinely used can detect and measure all the natural elements and their inorganic compounds and a very wide range of organic chemical species using methods such as gas chromatography and mass spectrometry. In water treatment plants producing drinking water and in some industrial processes using products with distinctive taste and odours, specialised organoleptic methods may be used to detect smells at very low concentrations.\n\nSamples of water from the natural environment are routinely taken and analysed as part of a pre-determined monitoring programme by regulatory authorities to ensure that waters remain unpolluted, or if polluted, that the levels of pollution are not increasing or are falling in line with an agreed remediation plan. An example of such a scheme is the Harmonised monitoring scheme operated on all the major river systems in the UK. The parameters analysed will be highly dependent on nature of the local environment and/or the polluting sources in the area. In many cases the parameters will reflect the national and local water quality standards determined by law or other regulations. Typical parameters for ensuring that unpolluted surface waters remain within acceptable chemical standards include pH, major cations and anions including Ammonia, Nitrate, Nitrite, Phosphate, Conductivity, COD, Phenol and BOD.\n\nSurface or ground water abstracted for the supply of drinking water must be capable of meeting rigorous chemical standards following treatment. This requires a detailed knowledge of the water entering the treatment plant. In addition to the normal suite of environmental chemical parameters, other parameters such as hardness, Phenol, Oil and in some cases a real-time organic profile of the incoming water as in the River Dee regulation scheme.\n\nIn industrial process, the control of the quality of process water can be critical to the quality of the end product. Water is often used as a carrier of reagents and the loss of reagent to product must be continuously monitored to ensure that correct replacement rate. Parameters measured relate specifically to the process in use and to any of the expected contaminants that may arise as by-products. This may include unwanted organic chemicals appearing in an inorganic chemical process through contamination with oils and greases from machinery. Monitoring the quality of the waste water discharged from industrial premises is a key factor in controlling and minimising pollution of the environment. In this application monitoring schemes analyse for all possible contaminants arising within the process and in addition contaminants that may have particularly adverse impacts on the environment such as Cyanide and many organic species such as pesticides. In then nuclear industry analysis focuses on specific isotopes or elements of interest. Where the nuclear industry makes waste water discharges to rivers which have drinking water abstraction on them, radio-isotopes which could potentially be harmful or those with long half-lives such as Tritium will form part of the routine monitoring suite.\n\nMany aspects of academic research and industrial research such as in pharmaceuticals, health products, and many others relies on accurate water analysis to identify substances of potential use, to refine those substances and to ensure that when they are manufactured for sale that the chemical composition remains consistent. The analytical methods used in these area can be very complex and may be specific to the process or area of research being conducted and may involve the use of bespoke analytical equipment.\n\nIn environmental management, water analysis is frequently deployed when pollution is suspected to identify the pollutant in order to take remedial action. The analysis can often enable the polluter to be identified. Such forensic work can examine the ratios of various components and can \"type\" samples of oils or other mixed organic contaminants to directly link the pollutant with the source. \nIn drinking water supplies the cause of unacceptable quality can similarly be determined by carefully targeted chemical analysis of samples taken throughout the distribution system.\nIn manufacturing, off-spec products may be directly tied back to unexpected changes in wet processing stages and analytical chemistry can identify which stages may be at fault and for what reason.\n\nTo ensure consistency and repeatability, the methods use in the chemical analysis of water samples are often agreed and published at a national or state level. By convention these are often referred to as \"Blue book\"s \n\nThe methods defined in the relevant standards can be broadly classified as:\nDepending on the components, different methods are applied to determine the quantities or ratios of the components. While some methods can be performed with standard laboratory equipment, others require advanced devices, such as Inductively coupled plasma mass spectrometry (ICP-MS).\n\n"}
{"id": "23394266", "url": "https://en.wikipedia.org/wiki?curid=23394266", "title": "Atlas of Victorian Birds", "text": "Atlas of Victorian Birds\n\nThe Atlas of Victorian Birds is a bird atlas, published in 1987, covering the distribution of birds in the Australian state of Victoria. It is based largely on 615,000 field records of birds in Victoria from the Atlas of Australian Birds database, gathered by volunteers in the course of the Royal Australasian Ornithologists Union’s atlas project from 1977 to 1981, as well as an additional 65,000 records gathered by officers of the National Parks and Wildlife Service of Victoria from 1973 to 1986.\n\nThe book is a quarto-sized, 272 page paperback, 295 mm in height by 210 mm wide. The first 28 pages contain a list of contents and an introductory section explaining the scope and methodology of the atlas, including sections on Victoria’s environment and physiographic regions. The following 244 pages are largely devoted to the accounts of about 480 species, mostly with maps showing both general and breeding distribution, a graph of the monthly recording index, a table showing breeding by month, and a brief text summary of each species’ biology and ecology in the state.\n\n"}
{"id": "7971112", "url": "https://en.wikipedia.org/wiki?curid=7971112", "title": "Benjamin Rush State Park", "text": "Benjamin Rush State Park\n\nBenjamin Rush State Park is a Pennsylvania state park in Philadelphia, Philadelphia County, Pennsylvania, in the United States. The park is undeveloped and is the site of community gardens, believed to be one of the largest in the world. The park is home to the Northeast Radio Controlled Airplane Club. Benjamin Rush State Park is in Northeast Philadelphia at the intersection of Southampton Road and Roosevelt Boulevard (U.S. Route 1). The northern boundary of the park is formed by Poquessing Creek. There are several acres of woodlands along the creek bank. A proposal map show plans to connect the park with Fairmount Park. Other proposed improvements included hiking trails, parking facilities, and a reforestation project. The community gardens and airfield for the radio-controlled aircraft would remain.\n\n\nThe following state parks are within of Benjamin Rush State Park:\n"}
{"id": "7865593", "url": "https://en.wikipedia.org/wiki?curid=7865593", "title": "Bird's eye figure", "text": "Bird's eye figure\n\nBird's eye is a type of figure that occurs within several kinds of wood, most notably in hard maple. It has a distinctive pattern that resembles tiny, swirling eyes disrupting the smooth lines of grain. It is somewhat reminiscent of a burl, but it is quite different: the small knots that make the burl are missing.\n\nIt is not known what causes the phenomenon. Research into the cultivation of bird's eye maple has so far discounted the theories that it is caused by pecking birds deforming the wood grain or that an infecting fungus makes it twist. However, no one has demonstrated a complete understanding of any combination of climate, soil, tree variety, insects, viruses or genetic mutation that may produce the effect.\n\nBird's eye maple is most often found in \"Acer saccharum\" (sugar maple), but millers also find bird's eye figure in red maple, white ash, Cuban mahogany, American beech, black walnut, and yellow birch. Trees that grow in the Great Lakes region of Canada and the United States yield the greatest supply, along with some varieties in the Rocky Mountains. It is not uncommon in Huon Pine, which grows only in Tasmania. Although there are a few clues in a tree's bark that indicate the lumber might have bird's eye figure, it is usually necessary to fell the tree and cut it apart to know for sure.\n\nIn most characteristics, wood with bird's eye figure is no different from the rest of the wood from the same tree. Depending on the frequency of the birdseye swirls, each ⅛\" to ⅜\" wide (0.3–1 cm), the wood may be extremely valuable. While woodworkers prize the timber primarily for its use in veneers, it also turns well on a lathe, allowing it to be shaped into decorative canes, chair legs, and handles.\n\nBird's eye maple may be expensive, up to several times the cost of ordinary hardwood. It is used in refined specialty products, such as in automobile trim, both in solid form and veneer, boxes and bowls for jewelry, thin veneer, humidors, canes, furniture inlays, handles, guitars, bowed instruments and pool cues are popular uses. Items made with this wood tend to be more expensive not only because the wood is more costly but because it is harder to work. When working with bird's eye wood, it is advisable to take care in what tools are used, so as to prevent grain tearout. Also the more \"eyes\" there are in lumber, the weaker the wood tends to be.\n"}
{"id": "8382727", "url": "https://en.wikipedia.org/wiki?curid=8382727", "title": "Davidite", "text": "Davidite\n\nDavidite is a rare earth oxide mineral with chemical end members La and Ce. It exists in two forms:\n"}
{"id": "30250507", "url": "https://en.wikipedia.org/wiki?curid=30250507", "title": "Discrete logarithm records", "text": "Discrete logarithm records\n\nDiscrete logarithm records are the best results achieved to date in solving the discrete logarithm problem, which is the problem of finding solutions \"x\" to the equation \"g\" = \"h\" given elements \"g\" and \"h\" of a finite cyclic group \"G\". The difficulty of this problem is the basis for the security of several cryptographic systems, including Diffie–Hellman key agreement, ElGamal encryption, the ElGamal signature scheme, the Digital Signature Algorithm, and the elliptic curve cryptography analogs of these. Common choices for \"G\" used in these algorithms include the multiplicative group of integers modulo \"p\", the multiplicative group of a finite field, and the group of points on an elliptic curve over a finite field.\n\nOn 16 June 2016, Thorsten Kleinjung, Claus Diem, Arjen K. Lenstra, Christine Priplata, and Colin Stahlke announced the computation of a discrete logarithm modulo a 232-digit (768-bit) safe prime, using the number field sieve. The computation was started in February 2015 and took approximately 6600 core years scaled to an Intel Xeon E5-2660 at 2.2 GHz.\n\nPrevious records for integers modulo \"p\" include:\n\n\nThe current record () in a finite field of characteristic 2 was announced by Robert Granger, Thorsten Kleinjung, and Jens Zumbrägel on 31 January 2014. This team was able to compute discrete logarithms in GF(2) using about 400,000 core hours. New features of this computation include a modified method for obtaining the logarithms of degree two elements and a systematically optimized descent strategy.\n\nPrevious records in a finite field of characteristic 2 were announced by:\n\n\nThe current record () in a finite field of characteristic 2 of prime degree was announced by Thorsten Kleinjung on 17 October 2014. The calculation was done in a field of 2 elements and followed essentially the path sketched for formula_1 in with two main exceptions in the linear algebra computation and the descent phase. The total running time was less than four core years. The previous record in a finite field of characteristic 2 of prime degree was announced by the CARAMEL group on April 6, 2013. They used the function field sieve to compute a discrete logarithm in a field of 2 elements.\n\nThe current record () for a field of characteristic 3 was announced by \nGora Adj, Isaac Canales-Martinez, Nareli Cruz-Cortés, Alfred Menezes, Thomaz Oliveira, Francisco Rodriguez-Henriquez, and Luis Rivera-Zamarripa on . The calculation was done in the 4841-bit finite field with 3 elements and was performed on several computers at CINVESTAV and\nthe University of Waterloo. In total, about 200 core years of computing time was expended on the computation.\nPrevious records in a finite field of characteristic 3 were announced:\n\n\nOver fields of \"moderate\"-sized characteristic, notable computations as of 2005 included those a field of 65537 elements (401 bits) announced on 24 Oct 2005, and in a field of 370801 elements (556 bits) announced on 9 Nov 2005. The current record (as of 2013) for a finite field of \"moderate\" characteristic was announced on 6 January 2013. The team used a new variation of the function field sieve for the medium prime case to compute a discrete logarithm in a field of 33341353 elements (a 1425-bit finite field). The same technique had been used a few weeks earlier to compute a discrete logarithm in a field of 33553771 elements (an 1175-bit finite field).\n\nOn 25 June 2014, Razvan Barbulescu, Pierrick Gaudry, Aurore Guillevic, and François Morain announced a new computation of a discrete logarithm in a finite field whose order has 160 digits and is a degree 2 extension of a prime field. The algorithm used was the number field sieve (NFS), with various modifications. The total computing time was equivalent to 68 days on one core of CPU (sieving) and 30 hours on a GPU (linear algebra).\n\nCerticom Corp. has issued a series of Elliptic Curve Cryptography challenges. Level I involves fields of 109-bit and 131-bit sizes. Level II includes 163, 191, 239, 359-bit sizes. All Level II challenges are currently believed to be computationally infeasible.\n\nThe Level I challenges which have been met are:\n\n\nNone of the 131-bit (or larger) challenges have been met .\n\nIn July 2009, Joppe W. Bos, Marcelo E. Kaihara, Thorsten Kleinjung, Arjen K. Lenstra and Peter L. Montgomery announced that they had carried out a discrete logarithm computation on an elliptic curve modulo a 112-bit prime. The computation was done on a cluster of over 200 PlayStation 3 game consoles over about 6 months. They used the common parallelized version of Pollard rho method.\n\nIn April 2014, Erich Wenger and Paul Wolfger from Graz University of Technology solved the discrete logarithm of a 113-bit Koblitz curve in extrapolated 24 days using an 18-core Virtex-6 FPGA cluster. In January 2015,the same researchers solved the discrete logarithm of an elliptic curve defined over a 113-bit binary field. The average runtime is around 82 days using a 10-core Kintex-7 FPGA cluster.\n\nOn 2 December 2016, Daniel J. Bernstein, Susanne Engels, Tanja Lange, Ruben Niederhagen, Christof Paar, Peter Schwabe, and Ralf Zimmermann announced the solution of a generic 117.35-bit elliptic curve discrete logarithm problem on a binary curve, using an optimized FPGA implementation of a parallel version of Pollard's rho algorithm. The attack ran for about six months on 64 to 576 FPGAs in parallel.\n\nOn 23 August 2017, Takuya Kusaka, Sho Joichi, Ken Ikuta, Md. Al-Amin Khandaker, Yasuyuki Nogami, Satoshi Uehara, Nariyoshi Yamai, and Sylvain Duquesne announced that they had solved a discrete logarithm problem on a 114-bit \"pairing-friendly\" Barreto-Naehrig (BN) curve, using the special sextic twist property of the BN curve to efficiently carry out the random walk of Pollard’s rho method. The implementation used 2000 CPU cores and took about 6 months to solve the problem.\n\n"}
{"id": "16593384", "url": "https://en.wikipedia.org/wiki?curid=16593384", "title": "Energy in Vietnam", "text": "Energy in Vietnam\n\nIn 2013, Vietnam planned to consume over 133.4 billion kWh of electricity, an increase of 11% from 2012.\nVietnam will import 3.5 billion kWh from China, an increase of 1 billion kWh more than 2012. Hydroelectricity still contributes about 40% of total electricity generation, followed by thermal gas turbine with 33%, coal 22%, and the rest come from petroleum and import. \nThe government is expecting to produce 5% of its energy from renewables by 2020.\n\nIn the late 1970s and early 1980s, fuel production increased at more than 10% annually. Coal output grew from 5.2 million tons in 1975 to 6 million tons in 1978 and fell to 5.3  million tons in 1980. According to official figures, 1985 coal production remained at, or somewhat below, the 1981 level of 6 million tons. Coal accounted for about two-thirds of energy consumption in the 1980s. Coal mining remained hampered by coordination and management problems at mining sites, incomplete rail connections to mines, equipment and materials shortages, and inadequate food and consumer goods for miners.\n\nElectric power production, although handicapped by uncompleted projects and shortages of oil and spare parts, grew at an average of 8% per year in the late 1970s and early 1980s. Vietnamese statistics on the annual output of primary products showed that production of electricity increased by almost 60% to nearly 3.8 TWh from 1976 through 1978, then declined to around 3.7 TWh in 1980. By 1985, however, production of electricity had increased to 5.4 TWh. Energy-producing industries generally remained stagnant, however, which caused tremendous difficulties for the other sectors of the economy. Power output grew very slowly, and power shortages forced many factories to operate at only 45 to 50% of their capacity. The government planned that in the 1980s energy production would be tripled by the completion of three big Soviet-assisted projects: the 500-megawatt thermal plant at Pha Lai (now in Hải Dương Province); the 300-megawatt hydroelectric plant at Trị An, Đồng Nai Province, and the giant, 1,900-megawatt hydroelectric plant at Hòa Bình, Hòa Bình Province, which has been called the \"Asian Aswan Dam.\"\n\nPetroleum is the main source of commercial energy, followed by coal, which contributes about 25 percent of the country’s energy (excluding biomass). Vietnam’s oil reserves are presently estimated at 600 million tonnes. Oil production rose rapidly to in 2004, but output is believed to have peaked and is expected to decline gradually. Vietnam’s anthracite coal reserves are estimated at 3.7 billion tons. Coal production was almost 19 million tons in 2003, compared with 9.6 million tons in 1999. Vietnam’s potential natural gas reserves are 1.3 trillion cubic meters. In 2002 Vietnam brought ashore 2.26 billion cubic meters of natural gas. Hydroelectric power is another source of energy. In 2004 Vietnam began to build a nuclear power plant with Russian assistance.\n\nCrude oil was Vietnam’s leading export until the late 2000s, when high-tech electrical manufactures emerged to become the biggest export market (by 2014, crude oil comprised only 5% of Vietnamese exports, compared to 20% of all exports in 1996). This is because Vietnamese crude oil is estimated to have peaked in 2004, when crude oil represented 22 percent of all export earnings. Petroleum exports are in the form of crude petroleum because Vietnam has a very limited refining capacity. Vietnam’s only operational refinery, a facility at Cát Lái near Ho Chi Minh City, has a capacity of only . Several consortia have abandoned commitments to finance a facility at Dung Quat in central Vietnam. Refined petroleum accounted for 10.2 percent of total imports in 2002.\n\n\n[Vietnam Energy Portal]: http://vietnamep.com\n"}
{"id": "10669204", "url": "https://en.wikipedia.org/wiki?curid=10669204", "title": "Energy independence", "text": "Energy independence\n\nEnergy independence is independence or autarky regarding energy resources, energy supply and/or energy generation by the energy industry.\n\nEnergy dependence in general refers to either mankind's general dependence on primary or secondary energy for energy consumption (fuel, transport, automation, etc.). In a narrower sense, it may describe the dependence of one country on energy resources from another country.\n\nGenerally, a higher level of dependence is associated with a higher energy security risk, because of the possible interference of trade regulations, international armed conflicts, etc..\n\nA crucial contribution on the road to energy independence is energy efficiency because efficient use of energy can build on individual efforts in power saving instead of having to rely on costly large-scale infrastructure.\n\nUsually, a country will rely on local and global energy renewable and non-renewable resources, a mixed-model solution that presumes various energy sources and modes of energy transfer between countries like electric power transmission, oil transport (oil and gas pipelines and tankers), etc. \nThe European dependence on Russian energy is a case in point.\n\nPlanning and coordination in the strive for energy independence is the business of energy policy and energy management.\n\nEnergy dependence is also used as a subindicator for Energy Globalisation Index.\n\n\n"}
{"id": "12451", "url": "https://en.wikipedia.org/wiki?curid=12451", "title": "Global Boundary Stratotype Section and Point", "text": "Global Boundary Stratotype Section and Point\n\nA Global Boundary Stratotype Section and Point, abbreviated GSSP, is an internationally agreed upon reference point on a stratigraphic section which defines the lower boundary of a stage on the geologic time scale. The effort to define GSSPs is conducted by the International Commission on Stratigraphy, a part of the International Union of Geological Sciences. Most, but not all, GSSPs are based on paleontological changes. Hence GSSPs are usually described in terms of transitions between different faunal stages, though far more faunal stages have been described than GSSPs. The GSSP definition effort commenced in 1977. As of 2012, 64 of the 101 stages that need a GSSP have been formally defined.\n\nA geologic section has to fulfill a set of criteria to be adapted as a GSSP by the ICS. The following list summarizes the criteria:\n\n\nThe Precambrian-Cambrian boundary GSSP at Fortune Head, Newfoundland is a typical GSSP. It is accessible by paved road and is set aside as a nature preserve. A continuous section is available from beds that are clearly Precambrian into beds that are clearly Cambrian. The boundary is set at the first appearance of a complex trace fossil \"Treptichnus pedum\" that is found worldwide. The Fortune Head GSSP is unlikely to be washed away or built over. Nonetheless, \"Treptichnus pedum\" is less than ideal as a marker fossil as it is not found in every Cambrian sequence, and it is not assured that it is found at the same level in every exposure. In fact, further eroding its value as a boundary marker, it has since been identified in strata 4m \"below\" the GSSP!\nHowever, no other fossil is known that would be preferable. There is no radiometrically datable bed at the boundary at Fortune Head, but there is one slightly above the boundary in similar beds nearby.\nThese factors have led some geologists to suggest that this GSSP is in need of reassigning.\n\nOnce a GSSP boundary has been agreed upon, a \"golden spike\" is driven into the geologic section to mark the precise boundary for future geologists (though in practice the \"spike\" need neither be golden nor an actual spike). The first stratigraphic boundary was defined in 1977 by identifying the Silurian-Devonian boundary with a bronze plaque at a locality called Klonk, northeast of the village of Suchomasty in the Czech Republic. GSSPs are also sometimes referred to as Golden Spikes.\n\nBecause defining a GSSP depends on finding well-preserved geologic sections and identifying key events, this task becomes more difficult as one goes farther back in time. Before 630 million years ago, boundaries on the geologic timescale are defined simply by reference to fixed dates, known as \"Global Standard Stratigraphic Ages\".\n\n\n"}
{"id": "34138013", "url": "https://en.wikipedia.org/wiki?curid=34138013", "title": "Great Rift Valley, Kenya", "text": "Great Rift Valley, Kenya\n\nThe Great Rift Valley is part of an intra-continental ridge system that runs through Kenya from north to south. It is part of the Gregory Rift, the eastern branch of the East African Rift, which starts in Tanzania to the south and continues northward into Ethiopia. It was formed on the \"Kenyan Dome\" a geographical upwelling created by the interactions of three major tectonics: the Arabian, Nubian, and Somalian plates. In the past, it was seen as part of a \"Great Rift Valley\" that ran from Madagascar to Syria. Most of the valley falls within the former Rift Valley Province.\n\nThe valley contains the Cherangani Hills and a chain of volcanoes, some of which are still active.\nThe climate is mild, with temperatures usually below .\nMost rain falls during the March–June and October–November periods.\nThe Tugen Hills to the west of Lake Baringo contain fossils preserved in lava flows from the period 14 to 4 million years ago. The relics of many hominids, ancestors of humans, were found here.\n\nThe valley is bordered by escarpments to the east and west. The floor is broken by volcanoes, some still active, and contains a series of lakes. Some of the soils are Andisols, fertile soils from relatively recent volcanic activity.\n\nLake Turkana occupies the northern end of the Great Rift Valley in Kenya. There are also volcanoes in Lake Turkana. The Suguta Valley, or Suguta Mud Flats, is an arid part of the Great Rift Valley directly south of Lake Turkana. The shield volcano Emuruangogolak straddles the valley to the south of Suguta, and further south Mount Silali and Paka rise from the valley floor. Paka is a shield volcano, with widespread geothermal activity. South of Paka are Mount Korosi, Lake Baringo and Lake Bogoria. Menengai is a massive shield volcano in the floor of the rift with a caldera that formed about 8,000 years ago. It overlooks Lake Nakuru to the south.\nThis region also includes Lake Elementaita, Mount Kipipiri and Lake Naivasha.\n\nThe Hell's Gate National Park lies south of Lake Naivasha. In the early 1900s, Mount Longonot erupted, and ash can still be felt around Hell's Gate. Mount Longonot is a dormant stratovolcano located southeast of Lake Naivasha. Suswa is a shield volcano located between Narok and Nairobi. Lava flows from the most recent eruptions are still not covered by vegetation, and may be no more than one hundred years old. Lake Magadi is the most southern rift valley lake in Kenya, although the northern end of Lake Natron in Tanzania reaches into Kenya.\n\nThe Elgeyo escarpment forms part of the western wall. The Kerio Valley lies between the Tugen Hills and the Elgeyo escarpment at an elevation of \nThere are large deposits of Fluorite in the Kerio Valley area.\nFurther south the Mau Escarpment is a steep natural cliff approximately 3,000 m (10,000 ft) high, running along the western edge of the Great Rift Valley about Lake Naivasha.\nYet further south the Nguruman Escarpment is around 50 kilometers long and elongated in N-W direction. Its northern edge is about southwest of Nairobi, while the southern edge is near the Tanzanian border, at the northwestern corner of Lake Natron.\nThe Aberdare Range forms a section of the eastern rim of the Great Rift Valley to the north of Nairobi. Mount Satima lies at the northern end of the Aberdares and is their highest point, and Mount Kinangop at the southern end is the second highest. The mountains form a ridge between these two peaks. Ngong Hills are peaks in a ridge along the east of the Great Rift Valley, located southwest near Nairobi.\n\nKenya is home to 64 (9.50%) of the total lakes found within the continent of Africa. Eight of these make up the main lakes in the Kenyan Rift Valley. From north to south, the names of these lakes are Lake Turkana, Lake Logipi, Lake Baringo, Lake Bogoria, Lake Nakuru, Lake Elmenteita, Lake Naivasha, and Lake Magadi Of those eight, only Lakes Baringo and Naivasha are fresh water.\n\nLake Turkana, at the northern end of the rift, is long, between and wide and is at its greatest depth.\nMost of the other lakes are shallow and poorly drained, and therefore have become alkaline. They have waters that are rich in blue-green algae, which feed insect larvae, small crustaceans and lesser flamingos. The larvae and crustaceans are food for fish and greater flamingos.\nMassive flocks of these birds have been found to have an effect on the lakeside sediments also. Their numbers cause trampling of the silts in certain areas, while the feeding grounds are oxygenated due to probing beaks in the mud. Their nest mounds can also be preserved and cemented as the lake's water levels change. These form irregularities in the lakeside topography.\n\nTrona, an evaporative mineral, used for sodium carbonate production, has been mined at Lake Magadi for nearly 100 years. It produces about 250,000 metric tonnes per year. Other precious minerals like rubies and pink sapphires have been found and mined from areas around Lake Baringo. In 2004, over 2 kilograms of Corundum were collected.\nThree shallow alkaline lakes and the surrounding lands make up the Kenya Lake system: Lake Bogoria at , Lake Nakuru at and Lake Elementaita at . This system has one of the most diverse populations of birds in the world, and is the home of thirteen globally threatened species of bird. It is an important nesting and breeding site for great white pelicans, and is the most important feeding area for lesser flamingos in the world. The system is home to globally important populations of black-necked grebe, African spoonbill, pied avocet, little grebe, yellow-billed stork, black-winged stilt, grey-headed gull and gull-billed tern.\n\nThe Kenya Lake system is a key location on the West Asian-East African Flyway, a route followed by huge numbers of birds in their annual migration from breeding grounds in the north to wintering places in Africa. The lands around the lakes include large populations of black rhino, Rothschild's giraffe, greater kudu, lion, cheetah and wild dogs.\nThe Kenya Lake system is surrounded by the steep escarpment of the Rift Valley, which provides a spectacular backdrop.\n\nOther lakes are Lake Chew Bahir, in the northeast extension. This lake lies mainly in Ethiopia but extends into Kenya in the rainy season. Lake Kamnarok is another small lake.\n\n\n"}
{"id": "21131801", "url": "https://en.wikipedia.org/wiki?curid=21131801", "title": "Hamedan Museum of Natural History", "text": "Hamedan Museum of Natural History\n\nThe Hamedan Museum of Natural History is a natural history museum located in the Bu Al Sina University in Hamedan, north-western Iran.\n\nIt is noted for its presentation of large horned Alborz red sheep and a black vulture. It also has a considerable taxidermic collection of animals and insects. The museum has an aquarium and live fish tanks.\n\n"}
{"id": "9876877", "url": "https://en.wikipedia.org/wiki?curid=9876877", "title": "Hirsholmene", "text": "Hirsholmene\n\nHirsholmene is a group of ten small Danish islands in the Kattegat; Hirsholm, Græsholm, Lilleholm, Tyvholm, Kølpen and Deget, located approximately 7 km to the northeast of Frederikshavn. Noted for their impressive bird populations, the islands have been a Danish nature reserve since 1938.\n\nOnly Hirsholm is inhabited, with a small settlement dating from the 16th century, and a lighthouse from 1886.\n\nThe island was the summer retreat for the noted Danish author Dines Skafte Jespersen who wrote the children's book series called \"Troldepus\" in addition to other works. He also wrote three autobiographical works, including one about his year living and working on the island, called \"Himmel, hav og en ø\" (Sky, Sea and an Island).\n\nHirsholmene has been an international Ramsar area since 2 September 1977. Today it has number 147 and encompass 3,714 ha.\n"}
{"id": "18103", "url": "https://en.wikipedia.org/wiki?curid=18103", "title": "Leyden jar", "text": "Leyden jar\n\nA Leyden jar (or Leiden jar) stores a high-voltage electric charge (from an external source) between electrical conductors on the inside and outside of a glass jar. A Leyden jar typically consists of a glass jar with metal foil cemented to the inside and the outside surfaces, and a metal terminal projecting vertically through the jar lid to make contact with the inner foil. It was the original form of the capacitor (also called \"condenser\").\n\nIts invention was a discovery made independently by German cleric Ewald Georg von Kleist on 11 October 1745 and by Dutch scientist Pieter van Musschenbroek of Leiden (Leyden) in 1745–1746. The invention was named after the city.\n\nThe Leyden jar was used to conduct many early experiments in electricity, and its discovery was of fundamental importance in the study of electrostatics. The Leyden jar was the first means of accumulating and preserving electric charge in large quantities that could be discharged at the experimenter's will, thus overcoming a significant limit to early research into electrical conduction. Leyden jars are still used in education to demonstrate the principles of electrostatics.\n\nThe Ancient Greeks already knew that pieces of amber could attract lightweight particles after being rubbed. The amber becomes electrified by triboelectric effect, mechanical separation of charge in a dielectric. The Greek word for amber is ἤλεκτρον (\"ēlektron\") and is the origin of the word \"electricity\".\n\nAround 1650, Otto von Guericke built a crude electrostatic generator: a sulphur ball that rotated on a shaft. When Guericke held his hand against the ball and turned the shaft quickly, a static electric charge built up. This experiment inspired the development of several forms of \"friction machines\", that greatly helped in the study of electricity.\n\nThe Leyden jar was effectively discovered independently by two parties: German deacon Ewald Georg von Kleist, who made the first discovery, and Dutch scientists Pieter van Musschenbroek and Andreas Cunaeus, who figured out how it worked only when held in the hand.\n\nDespite its mundane and safe appearance, the Leyden jar is a high voltage device, and it is estimated that at a maximum the early Leyden jars could collect electrical energy from friction in a range from 20,000 to 60,000 volts. The ball on the tip of the rod is a corona ball to prevent leakage of the energy into the air by point discharge.\n\nEwald Georg von Kleist (aka JG von Kleist) discovered the immense storage capability of the Leyden jar while working under a theory that saw electricity as a fluid, and hoped a glass jar filled with alcohol would \"capture\" this fluid. He was the deacon at the cathedral of Camin in Pomerania.\n\nIn October 1745 von Kleist tried to accumulate electricity in a small medicine bottle filled with alcohol with a nail inserted in the cork. He was following up on an experiment developed by Georg Matthias Bose where electricity had been sent through water to set alcoholic spirits alight. He attempted to charge the bottle from a large prime conductor (invented by Bose) suspended above his friction machine.\n\nKleist was convinced that a substantial electric charge could be collected and held within the glass which he knew would provide an obstacle to the escape of the 'fluid'. He received a significant shock from the device when he accidentally touched the nail through the cork while still cradling the bottle in his other hand. He communicated his results to at least five different electrical experimenters, in several letters from November 1745 to March 1746, but did not receive any confirmation that they had repeated his results, until April 1746. Daniel Gralath learned about Kleist's experiment from seeing the letter to Paul Swietlicki, written in November 1746. After Gralath's failed first attempt to reproduce the experiment in December 1746, he wrote to Kleist for more information (and was told that the experiment would work better if the tube half-filled with alcohol was used). Gralath (in collabration with ) succeeded in getting the intended effect on 5 March 1746, holding a small glass medicine bottle with a nail inside in one hand, moving it close to an electrostatic generator, and then moving the other hand close to the nail. Kleist didn't understand the significance of his conducting hand holding the bottle—and both he and his correspondents were loath to hold the device when told that the shock could throw them across the room. It took some time before Kleist's student associates at Leyden worked out that the hand provided an essential element.\n\nThe Leyden jar's invention was long credited to Pieter van Musschenbroek, the physics professor at University of Leiden, who also ran a family foundry which cast brass cannonettes, and a small business (\"De Oosterse Lamp\" – \"The Eastern Lamp\") which made scientific and medical instruments for the new university courses in physics and for scientific gentlemen keen to establish their own 'cabinets' of curiosities and instruments.\n\nLike Kleist, Musschenbroek was also interested in and attempting to repeat Bose's experiment. During this time, Andreas Cunaeus, a lawyer, came to learn about this experiment from visiting Musschenbroek's laboratory and Cunaeus attempted to duplicate the experiment at home with household items. Using a glass of beer, Cunaeus was unable to make it work. Cunaeus was the first to discover that the experimental setup could deliver a severe shock when he held his jar in his hand while charging it rather than placing it on an insulated stand, not realising that was the standard practice, thus making himself part of the circuit. He reported his procedure and experience to Allamand, Musschenbroek's colleague. Allamand and Musschenbroek also received severe shocks. Musschenbroek communicated the experiment in a letter from 20 January 1746 to René Antoine Ferchault de Réaumur, who was Musschenbroek's appointed correspondent at the Paris Academy. Abbé Nollet read this report, confirmed the experiment, and then read Musschenbroek's letter in a public meeting of the Paris Academy in April 1746 (translating from Latin to French). \nMusschenbroek's outlet in France for the sale of his company's 'cabinet' devices was the Abbé Nollet (who started building and selling duplicate instruments in 1735). Nollet then gave the electrical storage device the name \"Leyden jar\" and promoted it as a special type of flask to his market of wealthy men with scientific curiosity.\nThe \"Kleistian jar\" was therefore promoted as the \"Leyden jar\", and as having been discovered by Pieter van Musschenbroek and his acquaintance Andreas Cunaeus. Musschenbroek, however, never claimed that he had invented it, and some think that Cunaeus was mentioned only to diminish credit to him.\n\nWithin months after Musschenbroek's report about how to reliably create a Leyden jar, other electrical researchers were making and experimenting with their own Leyden jars. One interest was to see if the total possible charge could be increased. Johann Heinrich Winckler, whose first experience with a single Leyden jar was reported in a letter to the Royal Society on 29 May 1746, had connected three Leyden jars together in a kind of electrostatic battery on 28 July 1746. Daniel Gralath reported in 1747 that in 1746 he had conducted experiments with connecting two or three jars, probably in series. In 1748, Benjamin Franklin developed a system involving 11 panes of glass with thin lead plates glued on each side, and then connected together. He used the term \"electrical battery\" to describe his electrostatic battery in a 1749 letter about his electrical research in 1748, It is possible that Franklin's choice of the word \"battery\" was inspired by the humorous wordplay at the conclusion of his letter, where he wrote, among other things, about a salute to electrical researchers from a battery of guns. This is the first recorded use of the term \"electrical battery\". The multiple and rapid developments for connecting Leyden jars during the period 1746–1748 resulted in a variety of divergent accounts in secondary literature about who made the first \"battery\" by connecting Leyden jars, whether they were in series or parallel, and who first used the term \"battery\". The term was later used for combinations of multiple electrochemical cells, the modern meaning of the term \"battery\".\n\nStarting in late 1756, Franz Aepinus, in a complicated interaction of cooperation and independent work with Johan Wilcke, developed an \"air condenser\", a variation on the Leyden jar, by using air rather than glass as the dielectric. This functioning apparatus, without glass, created a problem for Benjamin Franklin's explanation of the Leyden jar, which maintained that the charge was located in the glass.\n\nBy the middle of the 19th century, the Leyden jar had become common enough for writers to assume their readers knew of and understood its basic operation. Around the turn of the century it began to be widely used in spark-gap transmitters and medical electrotherapy equipment. By the early 20th century, improved dielectrics and the need to reduce their size and undesired inductance and resistance for use in the new technology of radio caused the Leyden jar to evolve into the modern compact form of capacitor.\n\nA typical design consists of a glass jar with conducting tin foil coating the inner and outer surfaces. The foil coatings stop short of the mouth of the jar, to prevent the charge from arcing between the foils. A metal rod electrode projects through the stopper at the mouth of the jar, electrically connected by some means (usually a hanging chain) to the inner foil, to allow it to be charged. The jar is charged by an electrostatic generator, or other source of electric charge, connected to the inner electrode while the outer foil is grounded. The inner and outer surfaces of the jar store equal but opposite charges.\n\nThe original form of the device was just a glass bottle partially filled with water, with a metal wire passing through a cork closing it. The role of the outer plate was provided by the hand of the experimenter. Soon John Bevis found (in 1747) that it was possible to coat the exterior of the jar with metal foil, and he also found that he could achieve the same effect by using a plate of glass with metal foil on both sides. These developments inspired William Watson in the same year to have a jar made with a metal foil lining both inside and outside, dropping the use of water.\n\nEarly experimenters (such as Benjamin Wilson in 1746) reported that the thinner the dielectric and the greater the surface, the greater the charge that could be accumulated.\n\nFurther developments in electrostatics revealed that the dielectric material was not essential, but increased the storage capability (capacitance) and prevented arcing between the plates. Two plates separated by a small distance also act as a capacitor, even in a vacuum.\n\nIt was initially believed that the charge was stored in the water in early Leyden jars. In the 1700s American statesman and scientist Benjamin Franklin performed extensive investigations of both water-filled and foil Leyden jars, which led him to conclude that the charge was stored in the glass, not in the water. A popular experiment, due to Franklin, which seems to demonstrate this involves taking a jar apart after it has been charged and showing that little charge can be found on the metal plates, and therefore it must be in the dielectric. The first documented instance of this demonstration is in a 1749 letter by Franklin. Franklin designed a \"dissectible\" Leyden jar \"(right)\", which was widely used in demonstrations. The jar is constructed out of a glass cup nested between two fairly snugly fitting metal cups. When the jar is charged with a high voltage and carefully dismantled, it is discovered that all the parts may be freely handled without discharging the jar. If the pieces are re-assembled, a large spark may still be obtained from it.\n\nThis demonstration appears to suggest that capacitors store their charge inside their dielectric. This theory was taught throughout the 1800s. However, this phenomenon is a special effect caused by the high voltage on the Leyden jar. In the dissectible Leyden jar, charge is transferred to the surface of the glass cup by corona discharge when the jar is disassembled; this is the source of the residual charge after the jar is reassembled. Handling the cup while disassembled does not provide enough contact to remove all the surface charge. Soda glass is hygroscopic and forms a partially conductive coating on its surface, which holds the charge. Addenbrook (1922) found that in a dissectible jar made of paraffin wax, or glass baked to remove moisture, the charge remained on the metal plates. Zeleny (1944) confirmed these results and observed the corona charge transfer.\n\nOriginally, the amount of capacitance was measured in number of 'jars' of a given size, or through the total coated area, assuming reasonably standard thickness and composition of the glass. A typical Leyden jar of one pint size has a capacitance of about 1 nF.\n\nIf a charged Leyden jar is discharged by shorting the inner and outer coatings and left to sit for a few minutes, the jar will recover some of its previous charge, and a second spark can be obtained from it. Often this can be repeated, and a series of 4 or 5 sparks, decreasing in length, can be obtained at intervals. This effect is caused by dielectric absorption.\n\nIn 1747–1748, Benjamin Franklin experimented with charging Leyden jars in series.\n\n\n"}
{"id": "6070336", "url": "https://en.wikipedia.org/wiki?curid=6070336", "title": "List of SSSIs in West Gwynedd", "text": "List of SSSIs in West Gwynedd\n\nSSSIs in the UK are notified using the concept of an Area of Search (AOS), an area of between and in size. The Areas of Search were conceived and developed between 1975 and 1979 by the Nature Conservancy Council (NCC), based on regions created by the Local Government Act 1972. Whereas England had its Areas of Search based on 46 counties, those in Wales were based on a combination of the counties and smaller districts. In 1974, Wales was divided into 8 counties, with 37 districts. The NCC created 12 Welsh Areas of Search; they mostly follow county borders, but the larger counties (Dyfed, Powys and Gwynedd) were divided into multiple Areas using district borders. Mid and South Glamorgan were merged into a single AOS, whilst Llanelli district was included in the West Glamorgan AOS.\n\nDue to subsequent local government reorganisation in the UK since 1972, many counties and districts have been divided, merged or renamed. Using the AOS system alone would make it difficult to search for individual SSSI citations via the Countryside Council for Wales (CCW) database without knowing 1972 region divisions. As a result, the CCW groups Welsh SSSIs using the subdivisions of Wales formed in April 1996 by the Local Government (Wales) Act 1994, resulting in 22 principal areas.\n\nWest Gwynedd AOS lies within the counties of Gwynedd and the Isle of Anglesey.\n\nFor SSSIs elsewhere in the UK, see List of SSSIs by Area of Search.\n\n"}
{"id": "6099455", "url": "https://en.wikipedia.org/wiki?curid=6099455", "title": "List of Sites of Special Scientific Interest in Banff and Buchan", "text": "List of Sites of Special Scientific Interest in Banff and Buchan\n\nThe following is a list of Sites of Special Scientific Interest in the Banff and Buchan Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "22046789", "url": "https://en.wikipedia.org/wiki?curid=22046789", "title": "List of earthquakes in California", "text": "List of earthquakes in California\n\nAlthough the written history of California is not long, records of earthquakes exist that affected the Spanish missions that were constructed beginning in the late 18th century. Those records ceased when the missions were secularized in 1834, and from that point until the California Gold Rush in the 1840s, records were sparse. Other sources for the occurrence of earthquakes usually came from ship captains and other explorers. The earliest known earthquake was documented in 1769 by the Spanish explorers and Catholic missionaries of the Portolá expedition as they traveled northward from San Diego along the Santa Ana River near the present site of Los Angeles. For the period 1850–2004, there was about one potentially damaging event per year on average, though many of these did not cause loss of life or property damage.\n\nThe few damaging earthquakes that occurred in the American Midwest and the East Coast were well known (1755 Cape Ann, 1811–12 New Madrid, 1886 Charleston), and it became apparent to settlers that the earthquake hazard situation was much different in the West. While the 1812 San Juan Capistrano, 1857 Fort Tejon, and 1872 Lone Pine shocks were only moderately destructive in mostly unpopulated areas, the 1868 Hayward event affected the thriving financial hub that is the San Francisco Bay Area, with damage from Santa Rosa in the north to Santa Cruz in the south. By this time, scientists were well aware of the threat, but seismology was still in its infancy. Reactions following destructive events in the late 19th and early 20th centuries included real estate developers, press, and boosters minimizing and downplaying the risk out of fear that the ongoing economic boom would be negatively affected.\n\nAccording to seismologist Charles Richter, the 1906 San Francisco earthquake moved the United States Government into acknowledging the problem. Prior to that, no agency was specifically focused on researching earthquake activity. The United States Weather Bureau did record when they happened and several United States Geological Survey scientists had briefly disengaged from their regular duties of mapping mineral resources to write reports on the New Madrid and Charleston events, but no trained geologists were working on the problem until the Coast and Geodetic Survey was made responsible after 1906. The outlook improved when Professor Andrew Lawson brought the state's first monitoring program online at the University of California, Berkeley in 1910 with seismologist Harry Wood, who was later instrumental in getting the Caltech Seismological Laboratory operational in the 1920s.\n\nEarly developments at the Caltech lab in Pasadena included an earthquake observation network using their own custom built short period seismometers, the Richter magnitude scale, and an updated version of the Mercalli intensity scale. In 1933, the Long Beach earthquake occurred in a populated area and damaged or destroyed a large number of public school buildings in Long Beach and Los Angeles. Some decades later, the San Fernando earthquake affected the San Fernando Valley north of Los Angeles with heavy damage to several hospitals. In both cases, the perception of those involved with policy making in California was changed, and state laws and building codes were modified (but not without much debate) to require commercial and residential properties to be built to withstand earthquakes. Higher standards were established for fire stations, hospitals, and schools and construction of dwellings was also restricted near active faults.\n\nDuring the last 66 million years, nearly the entire west coast of North America was dominated by a subduction zone, with the Farallon Plate subducting beneath the North American Plate. Presently, the Juan de Fuca Plate (with its Explorer and Gorda satellite plates) and the Rivera and Cocos Plates are the only remnants of the once much larger Farallon Plate. The plate margin that remains in California is that of the strike-slip San Andreas Fault (SAF), the diffuse Pacific–North American plate boundary that extends east into the Basin and Range Province of eastern California and western Nevada (a seismically active area called Walker Lane) and southwest into the California Continental Borderland region off the central and southern coasts. This system of faults terminates in the north at the Mendocino Triple Junction, one of the most seismically active regions in the state, where earthquakes are occasionally the result of intraplate deformation within the Gorda Plate. It terminates in the south at the Salton Sea where displacement transitions to a series of spreading centers and transform faults, beginning with the Brawley Seismic Zone in the Imperial Valley.\n\nIn the San Francisco Bay Area, the San Andreas system of faults spans offshore and into the East Bay area, with the bulk of the faults lying to the east of the main SAF. There is a 70% probability that one of these faults will generate a M6.7 or greater earthquake before 2030, including the Hayward Fault Zone, which has gone beyond its average return period of 130 years (150 years as of 2018). While the SAF north of San Francisco is quiet, the central SAF segment near San Juan Bautista is where fault creep was first studied, and to the south is where the recurring Parkfield earthquakes occur. The secondary faults lay to the west of the main SAF at the extreme southern portion, including the active and young San Jacinto Fault Zone, which may be taking over as the primary boundary south of Cajon Pass. A paleoseismic investigation using Lidar revealed that more than of slip has accumulated since the 1857 event on the southern SAF, which borders the Mojave Desert to the north and east of the Greater Los Angeles Area. Near the Transverse Ranges, reverse and thrust faults have produced damaging earthquakes in Santa Barbara and the San Fernando Valley.\n\n\nSources\n"}
{"id": "31444281", "url": "https://en.wikipedia.org/wiki?curid=31444281", "title": "List of ecoregions in China", "text": "List of ecoregions in China\n\nThe following is a list of terrestrial ecoregions of the People's Republic of China and the Republic of China, according to the World Wide Fund for Nature.\n\nThe transition between two of the planet's eight ecozones – the Palearctic, which includes temperate and boreal Eurasia, and Indomalaya, which includes tropical South and Southeast Asia – extends through southern China.\n\n\n\n\n\n\n\n"}
{"id": "12186066", "url": "https://en.wikipedia.org/wiki?curid=12186066", "title": "List of horticultural magazines", "text": "List of horticultural magazines\n\nThis is a list of notable magazines devoted to horticulture and gardening.\n\n"}
{"id": "12755231", "url": "https://en.wikipedia.org/wiki?curid=12755231", "title": "List of largest dams", "text": "List of largest dams\n\nThe following table lists the largest man-made dams by volume of fill/structure. By general definition, a dam is a barrier that impounds water or underground streams, hence tailings dams are relegated to a separate list. Be aware that data on volume of structure is not as easily available or reliable as data on dam height and reservoir volume.\nType: TE - Earth; ER - Rock-fill; PG - Concrete gravity; CFRD - Concrete face rock fill.\n\nType: TE - Earth; ER - Rock-fill; PG - Concrete gravity; CFRD - Concrete face rock fill.\n\n"}
{"id": "92485", "url": "https://en.wikipedia.org/wiki?curid=92485", "title": "List of maria on the Moon", "text": "List of maria on the Moon\n\nThis is a list of \"maria\" (large, dark, basaltic plains) on the Moon. It also includes the one \"oceanus\" and the features known by the names \"lacus\", \"palus\" and \"sinus\". The modern system of lunar nomenclature was introduced in 1651 by Giovanni Battista Riccioli. Riccioli's map of the moon was drawn by Francesco Maria Grimaldi, who has a crater named after him.\n\nThere was also a region on the Lunar farside that was briefly misidentified as a mare and named \"Mare Desiderii\" (Sea of Desire). It is no longer recognized. Other former maria include:\n\n\nA related set of features are the Lunar \"lacus\" (singular \"lacus\", Latin for \"lake\"), which are smaller basaltic plains of similar origin:\n\nA related set of features are the \"sinus\" (singular \"sinus\", Latin for \"bay\") and \"paludes\" (singular \"palus\", Latin for \"marsh\"):\n\nSome sources also list a \"Palus Nebularum\" (\"Marsh of Mists\") at 38.0° N, 1.0° E. However the designation for this feature has not been officially recognized by the IAU.\n\n\n\n"}
{"id": "4918409", "url": "https://en.wikipedia.org/wiki?curid=4918409", "title": "List of national parks in Ecuador", "text": "List of national parks in Ecuador\n\nThis is a \"list of national parks in Ecuador\".\n\nThere are 11 national parks:\n\nThere are 9 national ecological reserves:\n\n\nThere are 4 National biological reserves:\n\n\nThere is one National geobotanical reserve:\n\n\nThere are 10 National wildlife refuges:\n\n\nIn addition to the many National reserves, refuges and parks in Ecuador there are some privately owned and operated reserves and refuges not listed on this page which is exclusively National Parks and other National Ecological Assets.\n\n"}
{"id": "55993560", "url": "https://en.wikipedia.org/wiki?curid=55993560", "title": "List of old waterbodies of the Rhine", "text": "List of old waterbodies of the Rhine\n\nThis list of old waterbodies of the Rhine contains a selection of ox-bow lakes and meanders of the River Rhine which go under various names that mean Old Rhine or Old Arm [of the Rhine]: Alter Rhein, Altrhein, Altrheinarm, Altrheinzug, Altarm, Rheinaltarm or Restrhein. Artificially created waterbodies, such as quarry ponds; e. g. the \"Altrheinsee\", which may be confused with old branches of the Rhine are mentioned in the section Waterbodies linked to the Rhine\".\nAs a result of natural translocation, the ox-bows and meanders of the Rhine riverbed remain as side arms of the New Rhine (\"Neurhein\"); others were cut off as a result of artificial river canalisation, such as in the wake of the canalisation of the Rhine (\"Rheinbegradigung\") from 1817 under the direction of Johann Gottfried Tulla. These old waters, together with the New Rhine, circumscribe islands or peninsulas. The old river courses have mostly lost their water-bearing link to the New Rhine. Many lie within nature reserves, offer breeding sites for water birds or act as washlands during flooding. Sometimes they are used by canoeists or anglers.\n\n\"Locating old Rhine waterbodies in the list:\"\nSome names of old Rhine waterbodies include the name of a nearby town or village; others do not. In order to make it easy to find in every case, the nearby settlement is usually written in alphabetical order and \"italics\" before each waterbody name as a means of locating it. Next, the coordinates of the waterbody and its position on the Rhine (l/r =  left/right) follow, then comes either the waterbody name itself in normal lettering, as well as other names of the waterbody in italics. Or there may just be a reference to the name of another place name behind the town name, under which the river is listed in\" italics\".\n\n\nin the canton of St. Gallen (Switzerland) and in the state of Vorarlberg (Austria) – old arms of the Alpine Rhine:\nin Germany:\n\nnumerous old Rhine waterbodies:\n\n\nSection of the River Rhine called the \"Altrhein\" or \"Restrhein\" parallel to the 52.7-kilometre-long Grand Canal d'Alsace (\"Rheinseitenkanal\"), between the \"Stauwehr Märkt\" (; Weil am Rhein; Lörrach, Baden-Württemberg; Germany) and Village-Neuf (department of Haut-Rhin; France) and the \"Kulturwehr Breisach\" (; Breisgau-Hochschwarzwald, Baden-Württemberg; Germany) and Volgelsheim (department of Haut-Rhin; France) running as a border river;\n\n\n\n\nWaterbodies that are not called e.g. \"Alter Rhein\" or \"Altrhein\", but are nevertheless old arms of the Rhine are:\n\nThese are artificially created waterbodies alongside the Rhine (such as quarry ponds), which are neither old arms nor tributaries of the Rhine, but which could be confused with such and are directly or indirectly linked to the Rhine. :\n\n"}
{"id": "30742031", "url": "https://en.wikipedia.org/wiki?curid=30742031", "title": "List of people associated with renewable energy", "text": "List of people associated with renewable energy\n\nThis is a list of people associated with renewable energy:\n\n"}
{"id": "8591877", "url": "https://en.wikipedia.org/wiki?curid=8591877", "title": "List of stars in Triangulum Australe", "text": "List of stars in Triangulum Australe\n\nThis is the list of notable stars in the constellation Triangulum Australe, sorted by decreasing brightness.\n\n\n"}
{"id": "38971506", "url": "https://en.wikipedia.org/wiki?curid=38971506", "title": "Little River (Oklahoma)", "text": "Little River (Oklahoma)\n\nLittle River may refer to the following rivers in the U.S. state of Oklahoma:\n\n"}
{"id": "39652102", "url": "https://en.wikipedia.org/wiki?curid=39652102", "title": "Luzon rainforest", "text": "Luzon rainforest\n\nThe Luzon rainforest is a moist broadleaf forest that contains the lowlands (below 1000 m) of Luzon and the montane rainforests located on a several volcanic and non-volcanic mountains of the island. The rainforest encompasses about 95,571 square kilometers (36,900 sq mi) out of the 109,965 sq kilometers (42,458 sq mi) of total area of the island of Luzon. Luzon is the largest and most northern island of the Philippines, located in the western Pacific Ocean. Though it is the most extensive rainforest ecoregion of the Philippines, very little of the original rainforest remains. Considered a part of the Luzon rainforest ecoregion are the Batanes and Babuyan Islands to the north, Catanduanes and Polillo Islands to the east, and Marinduque to the southwest. The status of this area is critical/endangered.\n\nLuzon has never been connected to mainland Asia. Even when glacial advances during the Pleistocene caused sea levels to fall over 100 meters worldwide, this only connected Luzon to the modern islands of Polillo, Marinduque, and Catanduanes. At least 15 million years ago, friction between the Australian and Asian tectonic plates and volcanic activity created parts of the Luzon highlands, which over the next 10 million years morphed into their modern form. This long period of isolation and complex internal geography is a primary cause for the great biodiversity and high degree of endemism found on the island of Luzon.\n\nIn 2005, evidence for human occupation in northern Luzon since at least 25,000 years ago, was found in Callao Cave. Evidence included chert flake tools, charred parenchymatous tissues, starch grains, grasses, and Moraceae phytolith. The possibility of hunter-gatherers subsisting in Holocene tropical rain forests without support from agriculturalists was debated, based on the patchy and seasonal resources. Wild forest animals are lean and lacking in calorie-rich fat.\nHowever, hunters and gatherers may have managed by strategizing and moving to correspond to the shifting food resources. Information on skeletal morphology and diet is merely speculative as no human remains were recovered from this time period. This idea is supported by the Sierra Madre Agta of the recent ethnographic past who would plan their movements based on the seasons and available resources; they hunted in the montane forest during the rainy season and in the lowland forest during the summer. Most of their food supply came from fishing, shellfish gathering, wild yams, nuts, and Caryota palms.\n\nIn the same cave two years later, in 2007, the same scientist found a metatarsal bone dated to at least around 67,000 years ago, which is speculated to possibly be of \"Homo sapiens\" origin. The bone (Right MT3 – the small bone from the end of the middle toe of the right foot) has been identified as belonging to a species of the genus \"Homo\". To tell if the bone belongs to an ancient anatomically modern human, a skull or mandible from the specimen is needed. This fossilized remain from Callao Cave is referred to as Callao Man.\n\nDuring World War II, the Japanese invaded the Philippines, and a small band of the Communist Party of the Philippines (CPP) activists used the dense mountain jungles and vast swamps of the Luzon rainforest for protection. The communist activists established a base of operations in the nearby Mt. Arayat and the Candaba Swamp. These activists launched small yet annoying attacks against the Japanese. On December 10, 1941, CPP leaders issued a manifesto vowing their support for the anti-Japanese efforts of the Commonwealth and the United States, and urging the peasants to support this united anti-Japanese front. Resulting was the organization of the Hukbalahap, an acronym for the Hukbong Bayan Laban sa Hapon (the Anti-Japanese Army), in a small lowland forest clearing near Mt. Arayat on March 29, 1942 by the merging of the CPP with the remaining socialist and peasant organizations of Luzon.\n\nThe rainforest of the lowlands encompass all areas below in altitude. Much of the lowland rainforest has been destroyed by human deforestation activities. \n\nDipterocarp trees with wide buttresses dominate this area. These trees are massive, growing up to tall with diameters between . \"Vatica pachyphylla\" is a critically endangered tree species endemic to the ecoregion. \n\nThe mature lowland forests tend to have an uneven canopy height. Rattans and lianas grow in the understory, receiving the light they need to thrive through areas of disturbance. There is generally a large amount of herbaceous undergrowth, with epiphytic ferns and orchids growing on the thick branches of tall trees.\n\nIn the south of the island, the ecoregion comprises several volcanic as well as non-volcanic mountains that exceed in altitude. The volcanic mountains include Mt. Makiling, Mt. Banahaw, Mt. Isarog, Mayon Volcano, and Bulusan Volcano. Also within the ecoregion are the Northern and Southern Sierra Madre, Mt. Sapacoy, Mt. Magnas, and Mt. Agnamala in the northern Cordillera Central highlands and the Zambales Mountains in the west.\nIn some areas, annual rainfall can be about quadruple what the lowland rainforests receive (as high as 10,000 mm). The Sierra Madres have very mild seasons, with a slight dry period between December and April. The Zambales Mountains and northern Central Cordillera highlands are more strongly seasonal with a longer dry period and slightly less rainfall generally.\nThe dipterocarp trees of the lowlands are gradually replaced by oak and laurel forest species with increasing altitude. The forests generally have less undergrowth and become shorter in stature as altitude increases. With the decreasing temperature from increasing altitude, decomposition is slowed and results in a forest floor thick with humus. \n\nIn the montane forests, epiphytes, vines, and moss-covered branches are very common. The highest altitudes of montane forests are caller upper montane forest, or elfin forest, and are more extreme: trees are shorter in stature, and tree branches are so thick with moss and organic material that they can sustain aerial plants that are not typically epiphytes. Many endemic animal species reside in the thick, matty soil of the upper montane forests.\n\nIn fact, species richness is greatest along the highest elevations of the montane rainforests of Luzon. Areas with the greatest levels of endemism are reported to be the Cordillera Central highlands, the Sierra Madre, the Zambales Mountains, and highlands on the Bicol Peninsula.\n\nThere are at least 31 endemic species of mammals on the island of Luzon. Sixty-eight percent of all known native non-flying mammals are endemic to the area (23 of 34).\n\nThe Philippine eagle (Pithecophaga jefferyi) is the second-largest eagle in the world found primarily in the Sierra Madre mountain range of Luzon. Primary lowland rainforests of the Philippines have been heavily deforested, and the Philippine eagle needs this area to breed, as well as nesting in large trees and hunting within the trees. The eagle is restricted to the islands of Luzon, Samar, Leyte, and Mindanao. Attempts for captive breeding have been unsuccessful and it is estimated that less than 700 individuals remain.\n\nOften called Myer’s snake in honor of Dr. George S. Myers, the genus \"Myersophis\" represented only by the species \"alpestris\" is a snake found only in the northern highlands of Luzon.\n\nAbout sixty-eight percent of all native reptiles are endemic to the area (about 160 of 235). The Philippine crocodile, Crocodylus mindorensis, is a freshwater crocodile that is considered the most threatened crocodile in the world and is endemic to the area; it is only found in Mindanao, Negros, and Luzon. Wild populations in 1982 totaled somewhere between 500 and 1000 individuals. In 1995, this number decreased to a mere 100 individuals. The discovery of a population of this crocodile in the Northern Sierra Madre on Luzon gives hope for its conservation. Active in the conservation of \"Crocodylus mindorensis\" is the Crocodile Rehabilitation Observance and Conservation (CROC) Project of the Mabuwaya Foundation.\n\nThe Sierra Madres give hope to many other threatened animals by providing one of the largest areas of intact rainforest in the Philippines thereby maintaining the naturally high level of biodiversity. Many species of threatened birds are found in this location.\n\nRecently, a presidential proclamation established the Quirino Protected Landscape covering 206,875 hectares in northeastern Luzon. There is also the Quezon Protected Landscape, covering about 164 km (102 miles) of lowland rainforest in Southern Sierra Madres. The Luzon rainforest is not a priority preservation area according to the World Wildlife Fund.\n\n\n"}
{"id": "5409432", "url": "https://en.wikipedia.org/wiki?curid=5409432", "title": "Mautam", "text": "Mautam\n\nMautam (Mizo for \"bamboo death\") is a cyclic ecological phenomenon that occurs every 48 - 50 years in the northeastern Indian states of Mizoram and Manipur, which are 30% covered by wild bamboo forests, as well as Chin State in Myanmar, particularly Hakha, Thantlang, Falam, Paletwa, and Matupi Townships. One of its stages involves a rat boom, which in turn creates a widespread famine in those areas.\n\nDuring \"mautam\", \"Melocanna baccifera\", a species of bamboo, flowers at one time across a wide area. This event is followed invariably by a plague of black rats in what is called a rat flood. This occurs as the rats multiply in response to the temporary windfall of seeds, and leave the forests to forage on stored grain when the bamboo seeds are exhausted, which in turn causes devastating famine. Famines thus caused have played a significant part in shaping the region's political history. The most recent spate of flowering, on the bamboo species' genetically linked timetable, began in May 2006, and the state government and the Indian Army attempted to prevent a famine.\n\nAfter flowering, the bamboo dies and regenerates from the seeds. The rodents feast on these seeds which are plentiful. As a consequence, a sudden boom in the rat population occurs. The action of the rats is thought to be an ecological control mechanism. The seeds of any culm of bamboo that might flower off-cycle are all eaten by rodents, thus reinforcing the rhythm of this extreme version of a mast year. Some experts believe that the flower has a positive effect on the fertility of the rats, as well as on increasing the viable size of a rat litter. All available explanations point to the fact that the increase in their numbers during the peak year is a natural after-effect of the flowering of the bamboos. However, once they exhaust this temporarily abundant food supply, the rats turn their attention to cultivated crops.\n\nRecords from the British Raj indicate that Mizoram suffered famine in 1862 and again in 1911, after the region witnessed similar bamboo flowerings. In each case, the records suggest that the flowering of the bamboo leads to a dramatic increase in the local rat population. The increase led to raids on granaries and the destruction of paddy fields, and subsequently to a year-long famine.\n\nThe 1958–59 \"mautam\" resulted in the recorded deaths of at least 100 people, besides heavy loss to human property and crops. Some elderly villagers in the undeveloped, more traditional region, recalling this event, have claimed that their warnings based on folk traditions were dismissed as superstition by the government of Assam, which then ruled what is now the state of Mizoram. An estimated two million rats were killed and collected by the locals, after a bounty of 40 paisa (about US$0.01 according to present-day rates) was placed on each. However, even after the increase in the rat population was noted, preparations by the government to avoid a famine were limited.\n\nThis negligence led to the foundation of the Mizo National Famine Front, set up to provide relief to the far-flung areas. This body later became the Mizo National Front (MNF), which staged a major uprising in 1966. Under its leader Laldenga (who later became the chief minister of Mizoram), MNF fought a bitter separatist struggle for 20 years against the Indian Army until an accord that guaranteed Mizoram's autonomy as a separate state was signed in 1986.\n\nChief Minister Zoramthanga, a former guerrilla leader, made preparations for the predicted 2006 \"mautam\" for two years. In June 2006, the Indian Army was pressed into service as an emergency measure to assist the state administration in reaching remote areas. The state administration arranged for alternate food crops to be grown locally, and also arranged for the army to provide instructions on pest control. Villagers were encouraged to grow turmeric and ginger, partially as an insurance against variations in purchasing power, and also because the aromatic spices ward off rodent raids.\n\nRegular rodent outbreaks associated with bamboo flowering (and subsequent fruiting and seeding) also occur in the nearby Indian states of Arunachal Pradesh, Manipur, and Nagaland, as well as in Laos, Japan, Madagascar and South America. \"Thingtam\", a similar famine, occurs with the flowering of another bamboo, \"Bambusa tulda\".\n\n\n"}
{"id": "59713", "url": "https://en.wikipedia.org/wiki?curid=59713", "title": "Mediterranean sea (oceanography)", "text": "Mediterranean sea (oceanography)\n\nA mediterranean sea is, in oceanography, a mostly enclosed sea that has limited exchange of water with outer oceans and with water circulation dominated by salinity and temperature differences rather than winds. The eponymous Mediterranean Sea, for example, is almost completely enclosed by Europe, Asia, and Africa.\n\n\n\nThere are two types of mediterranean sea\n\n\n\n\n\n"}
{"id": "20589543", "url": "https://en.wikipedia.org/wiki?curid=20589543", "title": "Mixing length model", "text": "Mixing length model\n\nIn fluid dynamics, the mixing length model is a method attempting to describe momentum transfer by turbulence Reynolds stresses within a Newtonian fluid boundary layer by means of an eddy viscosity. The model was developed by Ludwig Prandtl in the early 20th century. Prandtl himself had reservations about the model, describing it as, \"only a rough approximation,\"\nbut it has been used in numerous fields ever since, including atmospheric science, oceanography and stellar structure.\n\nThe mixing length is conceptually analogous to the concept of mean free path in thermodynamics: a fluid parcel will conserve its properties for a characteristic length, formula_1, before mixing with the surrounding fluid. Prandtl described that the mixing length,\nIn the figure above, temperature, formula_2, is conserved for a certain distance as a parcel moves across a temperature gradient. The fluctuation in temperature that the parcel experienced throughout the process is formula_3. So formula_4 can be seen as the temperature deviation from its surrounding environment after it has moved over this mixing length formula_5.\n\nTo begin, we must first be able to express quantities as the sums of their slowly varying components and fluctuating components.\n\nThis process is known as Reynolds decomposition. Temperature can be expressed as:\n\nformula_6,\nwhere formula_7, is the slowly varying component and formula_3 is the fluctuating component.\n\nIn the above picture, formula_3 can be expressed in terms of the mixing length:\n\nformula_10\n\nThe fluctuating components of velocity, formula_11, formula_12, and formula_13, can also be expressed in a similar fashion:\n\nformula_14\n\nalthough the theoretical justification for doing so is weaker, as the pressure gradient force can significantly alter the fluctuating components. Moreover, for the case of vertical velocity, formula_13 must be in a neutrally stratified fluid.\n\nTaking the product of horizontal and vertical fluctuations gives us:\n\nformula_16.\n\nThe eddy viscosity is defined from the equation above as:\n\nformula_17,\n\nso we have the eddy viscosity, formula_18 expressed in terms of the mixing length, formula_5.\n\n\n"}
{"id": "381477", "url": "https://en.wikipedia.org/wiki?curid=381477", "title": "Mount Kailash", "text": "Mount Kailash\n\nMount Kailash (also Kailasa; Kangrinboqê or \"Gang Rinpoche\"; Tibetan: གངས་རིན་པོ་ཆེ; ), is a 6,638 m (21,778 ft) high peak in the Kailash Range (Gangdisê Mountains), which forms part of the Transhimalaya in the Tibet Autonomous Region of China.\n\nThe mountain is located near Lake Manasarovar and Lake Rakshastal, close to the source of some of the longest Asian rivers: the Indus, Sutlej, Brahmaputra, and Karnali also known as Ghaghara (a tributary of the Ganges) in India. Mount Kailash is considered to be sacred in four religions: Bon, Buddhism, Hinduism and Jainism.\n\nThe mountain is known as “'” (; var. ' ) in Sanskrit. The name also could have been derived from the word “\"\"” (), which means \"crystal\".\n\nIn his Tibetan-English dictionary, Chandra (1902: p. 32) identifies the entry for 'kai la sha' () which is a loan word from Sanskrit.\n\nThe Tibetan name for the mountain is Gangs Rin-po-che (Tibetan: གངས་རིན་པོ་ཆེ་; ). \"Gangs\" or \"Kang\" is the Tibetan word for \"snow peak\" analogous to or \"hima\"; \"rinpoche\" is an honorific meaning \"precious one\" so the combined term can be translated \"precious jewel of snows\". Alice Albinia lists some of the names for the mountain, and its religious significance to various faiths:\n\nAnother local name for the mountain is Tisé mountain, which derives from \"ti tse\" in the Zhang-Zhung language, meaning \"water peak\" or \"river peak\", connoting the mountain's status as the source of the mythical Lion, Horse, Peacock and Elephant Rivers, and in fact the Indus, Yarlung Tsangpo/Dihang/Brahmaputra, Karnali and Sutlej all begin in the Kailash-Lake Manasarovara region.\n\nAccording to Hinduism, Shiva resided at the summit of a mountain named \"Kailāsh\", where he sat in a state of meditation along with his wife Pārvatī. \n\nAccording to Charles Allen, one description in the Vishnu Purana of the mountain states that its four faces are made of crystal, ruby, gold, and lapis lazuli. It is a pillar of the world and is located at the heart of six mountain ranges symbolizing a lotus.\n\nAccording to Jain scriptures, Ashtapada, the mountain next to Mt. Kailash, is the site where the first Jain Tirthankara, Rishabhadeva, attained moksha (liberation). In Jain tradition, it is believed that after Rishabhdeva attained nirvana, his son emperor Bharata Chakravartin had constructed three stupas and twenty four shrines of the 24 tirthankaras over there with their idols studded with precious stones and named it \"Sinhnishdha\".\n\nIn Jain tradition the 24th and last Tirthankara, Vardhamana Mahavira was taken to the summit of Kailash by Indra shortly after his birth, after putting his mother Queen Trishala into deep slumber. There he was bathed and anointed with precious unctions.\n\nMount Kailash (Kailasa) is known as Mount Meru in Buddhist texts. It is central to its cosmology, and a major pilgrimage site for some Buddhist traditions.\n\nVajrayana Buddhists believe that Mount Kailash is the home of the buddha Cakrasaṃvara (also known as Demchok), who represents supreme bliss.\n\nThere are numerous sites in the region associated with Padmasambhava, whose tantric practices in holy sites around Tibet are credited with finally establishing Buddhism as the main religion of the country in the 7th–8th century AD.\n\nIt is said that Milarepa (c. 1052 – c. 1135), champion of Vajrayana, arrived in Tibet to challenge Naro Bön-chung, champion of the Bön religion of Tibet. The two magicians engaged in a terrifying sorcerers' battle, but neither was able to gain a decisive advantage. Finally, it was agreed that whoever could reach the summit of Kailash most rapidly would be the victor. While Naro Bön-chung sat on a magic drum and soared up the slope, Milarepa's followers were dumbfounded to see him sitting still and meditating. Yet when Naro Bön-chung was nearly at the top, Milarepa suddenly moved into action and overtook him by riding on sunlight, thus winning the contest. He did, however, fling a handful of snow on to the top of a nearby mountain, since known as Bönri, bequeathing it to the Bönpo and thereby ensuring continued Bönpo connections with the region.\n\nBön, a religion native to Tibet, maintain that the entire mystical region and Kailash, which they call the \"nine-story Swastika Mountain\", is the axis mundi, Tagzig Olmo Lung Ring.\n\nEvery year, thousands make a pilgrimage to Kailash, following a tradition going back thousands of years. Pilgrims of several religions believe that circumambulating Mount Kailash on foot is a holy ritual that will bring good fortune. The peregrination is made in a clockwise direction by Hindus and Buddhists, while Jains and Bönpos circumambulate the mountain in a counterclockwise direction.\n\nThe path around Mount Kailash is long. Some pilgrims believe that the entire walk around Kailash should be made in a single day, which is not considered an easy task. A person in good shape walking fast would take perhaps 15 hours to complete the entire trek. Some of the devout do accomplish this feat, little daunted by the uneven terrain, altitude sickness and harsh conditions faced in the process. Indeed, other pilgrims venture a much more demanding regimen, performing body-length prostrations over the entire length of the circumambulation: The pilgrim bends down, kneels, prostrates full-length, makes a mark with his fingers, rises to his knees, prays, and then crawls forward on hands and knees to the mark made by his/her fingers before repeating the process. It requires at least four weeks of physical endurance to perform the circumambulation while following this regimen. The mountain is located in a particularly remote and inhospitable area of the Tibetan Himalayas. A few modern amenities, such as benches, resting places and refreshment kiosks, exist to aid the pilgrims in their devotion. According to all religions that revere the mountain, setting foot on its slopes is a dire sin. It is a popular belief that the stairways on Mount Kailash lead to heaven.\n\nBecause of the Sino-Indian border dispute, pilgrimage to the legendary abode of Shiva was stopped from 1954 to 1978. Thereafter, a limited number of Indian pilgrims have been allowed to visit the place, under the supervision of the Chinese and Indian governments either by a lengthy and hazardous trek over the Himalayan terrain, travel by land from Kathmandu or from Lhasa where flights from Kathmandu are available to Lhasa and thereafter travel over the great Tibetan plateau by car. The journey takes four night stops, finally arriving at Darchen at an elevation of , a small outpost that swells with pilgrims at certain times of the year. Despite its minimal infrastructure, modest guest houses are available for foreign pilgrims, whereas Tibetan pilgrims generally sleep in their own tents. A small regional medical center serving far-western Tibet and funded by the Swiss Ngari Korsum Foundation was built here in 1997.\n\nWalking around the mountain—a part of its official park—has to be done on foot, pony or domestic yak, and takes some three days of trekking starting from a height of around past the Tarboche (flagpole) to cross the Drölma pass , and encamping for two nights en route. First, near the meadow of Dirapuk gompa, some before the pass and second, after crossing the pass and going downhill as far as possible (viewing Gauri Kund in the distance).\n\nThe region around Mount Kailash and the Indus headwaters area is typified by wide scale faulting of metamorphosed late Cretaceous to mid Cenozoic sedimentary rocks which have been intruded by igneous Cenozoic granitic rocks. Mt. Kailash appears to be a metasedimentary roof pendant supported by a massive granite base. The Cenozoic rocks represent offshore marine limestones deposited before subduction of the Tethys oceanic crust. These sediments were deposited on the southern margin of the Asia block during subduction of the Tethys oceanic crust prior to the collision between the Indian and Asian continents.\n\nMount Everest is 8848 metre (29029 ft) in height and its summit has been scaled by over 4,000 people, while Mount Kailash is 6638 metre (21778 ft) and its summit is unclimbed.\n\nIn 1926, Hugh Ruttledge studied the north face, which he estimated was 6,000 ft (1,800 m) high and \"utterly unclimbable\" and thought about an ascent of the northeast ridge, but he ran out of time. Ruttledge had been exploring the area with Colonel R. C. Wilson, who was on the other side of the mountain with his Sherpa named Tseten. According to Wilson, Tseten told Wilson, \"'Sahib, we can climb that!' ... as he too saw that this [the SE ridge] represented a feasible route to the summit.\" Further excerpts from Wilson's article in the \"Alpine Journal\" (vol. 40, 1928) show that he was serious about climbing Kailash, but Colonel Wilson, \"“Just when I discovered an easy walk to the summit of the mountain, heavy snow began to fall, making the ascent impossible.”\". Herbert Tichy was in the area in 1936, attempting to climb Gurla Mandhata. When he asked one of the Garpons of Ngari whether Kailash was climbable, the Garpon replied, \"Only a man entirely free of sin could climb Kailash. And he wouldn't have to actually scale the sheer walls of ice to do it – he'd just turn himself into a bird and fly to the summit.\" Reinhold Messner was given the opportunity by the Chinese government to climb in the mid-1980s but he declined.\n\nIn 2001, reports emerged that the Chinese had given permission for a Spanish team to climb the peak, which caused an international backlash. Chinese authorities disputed the reports, and stated that any climbing activities on Mt Kailash were strictly prohibited. Reinhold Messner, who condemned the reported Spanish plans, said: \n\n\n"}
{"id": "56354259", "url": "https://en.wikipedia.org/wiki?curid=56354259", "title": "National Museum of Natural History at the National Academy of Sciences of Ukraine", "text": "National Museum of Natural History at the National Academy of Sciences of Ukraine\n\nNational Museum of Natural History at the National Academy of Sciences of Ukraine () is a natural history museum in the Ukrainian capital Kyiv. It is one of the biggest scientific research museums of the type in the world\n\nThe museum is housed in a former Olgynska school built in 1914–1927 in neoclassicism style. It was granted the status of the National Museum of December 10, 1996, according to the decision of the President of Ukraine.\n\nIt was established in 1966 as a unified complex comprising five museums: the Geological, Paleontological, Zoological, Botanical and Archaeological. There are approximately 30,000 exhibits in 24 halls with a total area of 8,000 m². The museum is open from Wednesday to Sunday from 10 am to 5 pm.\n\nThe museum publishes two annual journals:\n\n"}
{"id": "8177105", "url": "https://en.wikipedia.org/wiki?curid=8177105", "title": "Natural History Review", "text": "Natural History Review\n\nThe Natural History Review was a short-lived, quarterly journal devoted to natural history. It was published in Dublin and London between 1854 and 1865.\n\nThe \"Natural History Review\" included the transactions of the Belfast Natural History and Philosophical Society, Cork Cuvierian Society, Dublin Natural History Society, Dublin University Zoological Association, and the Literary and Scientific Institution of Kilkenny, as authorised...It was founded by Edward Perceval Wright who was also the editor.\n\nThe parts are:\n\nVols 1-4, 1854–57; title concludes: ...by the Councils of these Societies (Geological Society of Dublin later added to list)\n\nThis was continued as\n\"Natural History Review, and quarterly journal of science\". Edited by Edward Percival Wright, William Henry Harvey, Joseph Reay Greene, Samuel Haughton and Alexander Henry Haliday London, Vols 5-7, 1858-60.\n\nIn turn continued as\n\"Natural History Review\": a quarterly journal of biological science. Edited by George Busk, William Benjamin Carpenter, F.Currey et al., London, Vols 1-5, 1861–65; no more published.\n\n"}
{"id": "453182", "url": "https://en.wikipedia.org/wiki?curid=453182", "title": "Neck (water spirit)", "text": "Neck (water spirit)\n\nThe neck, nicor, nixie or nokken (; , ; Danish: \"nøkke;\" ; ; ; ) are shapeshifting water spirits in Germanic mythology and folklore who usually appeared in forms of other creatures.\n\nUnder a variety of names, they were common to the stories of all Germanic peoples, although they are perhaps best known from Scandinavian folklore. The related English \"knucker\" was generally depicted as a wyrm or dragon, although more recent versions depict the spirits in other forms. Their sex, bynames, and various transformations vary geographically. The German and his Scandinavian counterparts were males. The German was a female river mermaid.\n\nThe names are held to derive from Common Germanic \"*nikwus\" or \"*nikwis(i)\", derived from PIE \"\" (\"to wash\"). They are related to Sanskrit , Greek and , and Irish (all meaning to wash or be washed). The form \"neck\" appears in English and Swedish ( or , meaning \"nude\"). The Swedish form is derived from Old Swedish , which corresponds to Old Icelandic ( ), and in Norwegian Nynorsk. In Finnish, the word is . In Old Danish, the form was and in modern Danish and Norwegian Bokmål it is . The Icelandic and Faroese are horselike creatures. In Middle Low German, it was called and in Middle Dutch (compare also or plus ) . The Old High German form also meant \"crocodile\", while the Old English could mean both a \"water monster\" like those encountered by Beowulf, and a \"hippopotamus\". The Norwegian and Swedish are related figures sometimes seen as by-names for the same creature. The southern Scandinavian version can transform himself into a horse-like \"kelpie\", and is called a (the \"brook horse\"), whilst the Welsh version is called the (the \"water horse\").\n\nEnglish folklore contains many creatures with similar characteristics to the \"Nix\" or . These include Jenny Greenteeth, the Shellycoat, the river-hag Peg Powler, the -like Brag, and the Grindylow.\n\nAt Lyminster near Arundel in the English county of Sussex, there are today said to dwell \"water-wyrms\" called knuckers, in a pool called the \"Knucker-hole\". The great Victorian authority Skeat had plausibly suggested the pool's name of \"knucker\" (a name attested from 1835, Horsfield) was likely derived from the Old English , a creature-name found in \"Beowulf\".. Yet the waters at the pool were badly muddied by a local antiquarian named Samuel Evershed, who from 1866 tried assiduously to connect the pool with dragons and thus with the tale of St. George and the Dragon. Any authentic water-sprite folklore the site may originally have had was thus trampled down by Evershed's enthusiastic inculcation of the local people in ideas about water-dragons.\n\nThe Scandinavian \"näcken\", \"näkki\", \"nøkk\" were male water spirits who played enchanted songs on the violin, luring women and children to drown in lakes or streams. However, not all of these spirits were necessarily malevolent; many stories indicate at the very least that nøkker were entirely harmless to their audience and attracted not only women and children, but men as well with their sweet songs. Stories also exist wherein the Fossegrim agreed to live with a human who had fallen in love with him, but many of these stories ended with the nøkk returning to his home, usually a nearby waterfall or brook. (Compare the legend of Llyn y Fan Fach in Wales.) The nøkker were said to grow despondent unless they had free, regular contact with a water source.\n\nThe Norwegian Fossegrim or \"Grim\", Swedish \"strömkarl\", is a related figure who, if properly approached, will teach a musician to play so adeptly \"that the trees dance and waterfalls stop at his music\".\n\nIt is difficult to describe the appearance of the nix, as one of his central attributes was thought to be shapeshifting. Perhaps he did not have any true shape. He could show himself as a man playing the violin in brooks and waterfalls (though often imagined as fair and naked today, in folklore he was more frequently described as wearing more or less elegant clothing) but also could appear to be treasure or various floating objects, or as an animal—most commonly in the form of a \"brook horse\" (see below). The modern Scandinavian names are derived from Old Norse \"nykr\", meaning \"river horse\". Thus, it is likely that the figure of the brook horse preceded the personification of the nix as the \"man in the rapids\". Fossegrim and derivatives were almost always portrayed as especially beautiful young men, whose clothing (or lack thereof) varied widely from story to story.\n\nThe enthralling music of the nøkk was most dangerous to women and children, especially pregnant women and unbaptised children. He was thought to be most active during Midsummer's Night, on Christmas Eve, and on Thursdays. However, these superstitions do not necessarily relate to all the versions listed here. Many, if not all of them, developed after the Christianizing of the northern countries, as was the case of similar stories of faeries and other entities in other areas.\n\nWhen malicious nøkker attempted to carry off people, they could be defeated by calling their name; this was believed to cause their death.\n\nAnother belief was that if a person bought the nøkk a treat of three drops of blood, a black animal, some \"brännvin\" (Scandinavian vodka) or snus (wet snuff) dropped into the water, he would teach his enchanting form of music.\n\nThe nøkk was also an omen for drowning accidents. He would scream at a particular spot in a lake or river, in a way reminiscent of the loon, and on that spot, a fatality would later take place. He was also said to cause drownings, but swimmers could protect themselves against such a fate by throwing a bit of steel into the water.\n\nIn the later Romantic folklore and folklore-inspired stories of the 19th century, the nøkk sings about his loneliness and his longing for salvation, which he purportedly never shall receive, as he is not \"a child of God\". In a poem by Swedish poet E. J. Stagnelius, a little boy pities the fate of the nøkk, and so saves his own life. In the poem, arguably Stagnelius' most famous, the boy says that the Näck (nøkk) will never be a \"child of God\" which brings \"tears to his face\" as he \"never plays again in the silvery brook\". \n\nOn a similar theme, a 19th-century text called \"Brother Fabian's Manuscript\" by Sebastian Evans has this verse:\n\nIn Scandinavia, water lilies are called \"nix roses\" (\"näckrosor/nøkkeroser\"). A tale from the forest of Tiveden relates that a father promised his daughter to a nøkken who offered him great hauls of fish in a time of need; she refused and stabbed herself to death, staining the water lilies red from that time on:\n\nThe bäckahäst or bækhest (translated as \"the brook horse\") is a mythological horse in Scandinavian folklore. It has a close parallel in the Scottish kelpie, and the Welsh Ceffyl Dŵr.\n\nIt was often described as a majestic white horse that would appear near rivers, particularly during foggy weather. Anyone who climbed onto its back would not be able to get off again. The horse would then jump into the river, drowning the rider. The brook horse could also be harnessed and made to plough, either because it was trying to trick a person or because the person had tricked the horse into it. The following tale is a good illustration of the brook horse:\nThe German \"Nix\" and \"Nixe\" (and \"Nixie\") are types of river merman and mermaid who may lure men to drown, like the Scandinavian type, akin to the Celtic Melusine and similar to the Greek Siren. The German epic \"Nibelungenlied\" mentions the Nix in connection with the Danube, as early as 1180 to 1210.\n\nNixes in folklore became water sprites who try to lure people into the water. The males can assume many different shapes, including that of a human, fish, and snake. The females bear the tail of a fish. When they are in human forms, they can be recognised by the wet hem of their clothes. The Nixes are portrayed as malicious in some stories but harmless and friendly in others.\n\nBy the 19th century Jacob Grimm mentions the Nixie to be among the \"water-sprites\" who love music, song and dancing, and says \"Like the sirens, the Nixie by her song draws listening youth to herself, and then into the deep.\" According to Grimm, they can appear human but have the barest hint of animal features: the nix had \"a slit ear\", and the Nixie \"a wet skirt\". Grimm thinks these could symbolise they are \"higher beings\" who could shapeshift to animal form.\n\nOne famous Nixe of recent German folklore, deriving from 19th-century literature, was Lorelei; according to the legend, she sat on the rock at the Rhine which now bears her name, and lured fishermen and boatmen to the dangers of the reefs with the sound of her voice. In Switzerland there is a legend (myth) of a sea-maid or Nixe that lived in Lake Zug (the lake is in the Canton of Zug).\n\n\"The Yellow Fairy Book\" by Andrew Lang includes a story called \"The Nixie of the Mill-Pond\" in which a malevolent spirit that lives in a mill pond strikes a deal with the miller that she will restore his wealth in exchange for his son. This story is taken from the Tales of Grimm.\n\nThe legend of Heer Halewijn, a dangerous lord who lures women to their deaths with a magic song, may have originated with the Nix.\n\nAlternate names (kennings) for the female German Nixe are Rhine maidens () and Lorelei.\n\nIn a fictional depiction, the Rhine maidens are among the protagonists in the four-part Opera \"Der Ring des Nibelungen\" by the composer Richard Wagner, based loosely on the nix of the \"Nibelungenlied\".\n\nThe Rhine maidens Wellgunde, Woglinde, and Floßhilde (Flosshilde) belong to a group of characters living in a part of nature free from human influence. Erda and the Norns are also considered a part of this 'hidden' world.\n\nThey are first seen in the first work of the Nibelungen cycle, \"Das Rheingold\", as guardians of the \"Rheingold\", a treasure of gold hidden in the Rhein river. The dwarf Alberich, a Nibelung, is eager to win their favour, but they somewhat cruelly dismiss his flattery. They tell him that only one who is unable to love can win the \"Rheingold\". Thus, Alberich curses love and steals the \"Rheingold\". From the stolen gold he forges a ring of power. Further on in the cycle, the Rhine maidens are seen trying to regain the ring and transform it back into the harmless \"Rheingold\". But no one will return the ring to them; not even the supreme god Wotan, who uses the ring to pay the giants Fasolt and Fafner for building Valhalla, nor the hero Siegfried, when the maidens appear to him in the third act of \"Götterdämmerung\". Eventually Brünnhilde returns it to them at the end of the cycle, when the fires of her funeral pyre cleanse the ring of its curse.\n\nDescendants of German immigrants to Pennsylvania sometimes refer to a mischievous child as being \"nixie\".\n\n\n\n"}
{"id": "916971", "url": "https://en.wikipedia.org/wiki?curid=916971", "title": "Nuclear marine propulsion", "text": "Nuclear marine propulsion\n\nNuclear marine propulsion is propulsion of a ship or submarine with heat provided by a nuclear power plant. The power plant heats water to produce steam for a turbine used to turn the ship's propeller through a gearbox or through an electric generator and motor. Naval nuclear propulsion is used specifically within naval warships such as supercarriers (see \"nuclear navy\"). A small number of experimental civil nuclear ships have been built.\n\nCompared to oil or coal fuelled ships, nuclear propulsion offers the advantages of very long intervals of operation before refueling. All the fuel is contained within the nuclear reactor, so no cargo or supplies space is taken up by fuel, nor is space taken up by exhaust stacks or combustion air intakes. However, the low fuel cost is offset by the high operating costs and investment in infrastructure, so nearly all nuclear-powered vessels are military ones.\n\nNaval reactors are of the pressurized water type. A primary water circuit transfers heat generated from nuclear fission in the fuel to a steam generator; this water is kept under pressure so it does not boil. This circuit operates at a temperature of around . Any radioactive contamination in the primary water is confined. Water is circulated by pumps; at lower power levels, reactors designed for submarines may rely on natural circulation of the water to reduce noise generated by the pumps.\n\nThe hot water from the reactor heats a separate water circuit in the steam generator. The water turns to steam and passes through steam driers on its way to the steam turbine. Spent steam at low pressure is run through a condenser cooled by seawater and returns to liquid form. The water is pumped back to the steam generator and continues the cycle. Any water lost in the process can be made up by desalinated sea water added to the steam generator feed water.\n\nIn the turbine, the steam expands and reduces its pressure as it imparts energy to the rotating blades of the turbine. There may be many stages of rotating blades and fixed guide vanes. The output shaft of the turbine may be connected to a gearbox to reduce rotation speed, then a shaft connects to the vessel's propellers. In another form of drive system, the turbine turns an electrical generator, and the electric power produces is fed to one or more drive motors for the vessel's propellers. The Russian, US and British navies rely on direct steam turbine propulsion, while the French and Chinese ships use the turbine to generate electricity for propulsion (turbo-electric transmission).\n\nMost nuclear submarines have a single reactor, but Russian submarines have two, and so had . Most American aircraft carriers are powered by two reactors, but had eight. The majority of marine reactors are of the pressurized water type, although the US and Soviet navies have designed warships powered with liquid metal cooled reactors.\n\nMarine-type reactors differ from land-based commercial electric power reactors in several respects.\n\nWhile land-based reactors in nuclear power plants produce up to around 1600 megawatts of electrical power, a typical marine propulsion reactor produces no more than a few hundred megawatts. Space considerations dictate that a marine reactor must be physically small, so it must generate higher power per unit of space. This means its components are subject to greater stresses than those of a land-based reactor. Its mechanical systems must operate flawlessly under the adverse conditions encountered at sea, including vibration and the pitching and rolling of a ship operating in rough seas. Reactor shutdown mechanisms cannot rely on gravity to drop control rods into place as in a land-based reactor that always remains upright. Salt water corrosion is an additional problem that complicates maintenance.\n\nAs the core of a seagoing reactor is much smaller than a power reactor, the probability of a neutron intersecting with a fissionable nucleus before it escapes into the shielding is much lower. As such, the fuel is typically more highly enriched (i.e., contains a higher concentration of U vs. U) than that used in a land-based nuclear power plant, which increases the probability of fission to the level where a sustained reaction can occur. Some marine reactors run on relatively low-enriched uranium which requires more frequent refueling. Others run on highly enriched uranium, varying from 20% U, to the over 96% U found in U.S. submarines, in which the resulting smaller core is quieter in operation (a big advantage to a submarine). Using more-highly enriched fuel also increases the reactor's power density and extends the usable life of the nuclear fuel load, but is more expensive and a greater risk to nuclear proliferation than less-highly enriched fuel.\n\nA marine nuclear propulsion plant must be designed to be highly reliable and self-sufficient, requiring minimal maintenance and repairs, which might have to be undertaken many thousands of miles from its home port. One of the technical difficulties in designing fuel elements for a seagoing nuclear reactor is the creation of fuel elements which will withstand a large amount of radiation damage. Fuel elements may crack over time and gas bubbles may form. The fuel used in marine reactors is a metal-zirconium alloy rather than the ceramic UO (uranium dioxide) often used in land-based reactors. Marine reactors are designed for long core life, enabled by the relatively high enrichment of the uranium and by incorporating a \"burnable poison\" in the fuel elements, which is slowly depleted as the fuel elements age and become less reactive. The gradual dissipation of the \"nuclear poison\" increases the reactivity of the core to compensate for the lessening reactivity of the aging fuel elements, thereby lengthening the usable life of the fuel. The life of the compact reactor pressure vessel is extended by providing an internal neutron shield, which reduces the damage to the steel from constant bombardment by neutrons.\n\nIn December 2017 the UK government is expected to announce up to £100m funding to support the development of small land based nuclear power plants in effort to make the UK a leader in this technology and provide fresh source of clean power. Rolls Royce is a leading developer of nuclear power plants for submarines with power outputs in the order of 100MWe and above which would make such plants highly effective for decentralised power generation. Since the operational environment of such land based plants would be much simpler and less restricted than in a submarine, it is expected that these units could be constructed and built much cheaper than their marine based equivalent.\n\nDecommissioning nuclear-powered submarines has become a major task for US and Russian navies. After defuelling, U.S. practice is to cut the reactor section from the vessel for disposal in shallow land burial as low-level waste (see the \"ship-submarine recycling program\"). In Russia, whole vessels, or sealed reactor sections, typically remain stored afloat, although a new facility near Sayda Bay is to provide storage in a concrete-floored facility on land for some submarines in the far north.\n\nRussia is well advanced with plans to build a floating nuclear power plant for their far eastern territories. The design has two 35 MWe units based on the KLT-40 reactor used in icebreakers (with refueling every four years). Some Russian naval vessels have been used to supply electricity for domestic and industrial use in remote far eastern and Siberian towns.\n\nLloyd's Register is investigating the possibility of civilian nuclear marine propulsion and rewriting draft rules (see text under \"Merchant Ships\").\n\nInsurance of nuclear vessels is not like the insurance of conventional ships. The consequences of an accident could span national boundaries, and the magnitude of possible damage is beyond the capacity of private insurers. A special international agreement, the \"Brussels Convention on the Liability of Operators of Nuclear Ships\", developed in 1962, would have made signatory national governments liable for accidents caused by nuclear vessels under their flag but was never ratified owing to disagreement on the inclusion of warships under the convention. Nuclear reactors under United States jurisdiction are insured by the provisions of the Price Anderson Act.\n\nBy 1990 there were more nuclear reactors powering ships (mostly military) than there were generating electric power in commercial power plants worldwide.\n\nUnder the direction of U.S. Navy Captain (later Admiral) Hyman G. Rickover, the design, development and production of nuclear marine propulsion plants started in the United States in the 1940s. The first prototype naval reactor was constructed and tested at the Naval Reactor Facility at the National Reactor Testing Station in Idaho (now called the Idaho National Laboratory) in 1953.\n\nThe first nuclear submarine, , put to sea in 1955 (SS was a traditional designation for US submarines, while SSN denoted the first \"nuclear\" submarine).\n\nThe Soviet Union also developed nuclear submarines. The first types developed were the Project 627, NATO designated with two water-cooled reactors, the first of which, K-3 \"Leninskiy Komsomol\", was underway under nuclear power on July 4, 1958.\n\nNuclear power revolutionized the submarine, finally making it a true \"underwater\" vessel, rather than a \"submersible\" craft, which could only stay underwater for limited periods. It gave the submarine the ability to operate submerged at high speeds, comparable to those of surface vessels, for unlimited periods, dependent only on the endurance of its crew. To demonstrate this was the first vessel to execute a submerged circumnavigation of the Earth (Operation Sandblast), doing so in 1960.\n\n\"Nautilus\", with a pressurized water reactor (PWR), led to the parallel development of other submarines like a unique liquid metal cooled (sodium) reactor in , or two reactors in \"Triton\", and then the s, powered by single reactors, and a cruiser, , in 1961, powered by two reactors.\n\nBy 1962 the United States Navy had 26 operational nuclear submarines and another 30 under construction. Nuclear power had revolutionized the Navy. The United States shared its technology with the United Kingdom, while French, Soviet, Indian and Chinese development proceeded separately.\n\nAfter the \"Skate\"-class vessels, US submarines were powered by a series of standardized, single-reactor designs built by Westinghouse and General Electric. Rolls-Royce plc built similar units for Royal Navy submarines, eventually developing a modified version of their own, the PWR-2 (pressurized water reactor).\n\nThe largest nuclear submarines ever built are the 26,500 tonne Russian . The smallest nuclear warships to date are the 2,700 tonne French attack submarines. The US Navy operated an unarmed nuclear submarine, the NR-1 Deep Submergence Craft, between 1969 and 2008, which was not a combat vessel but was the smallest nuclear powered submarine at 400 tons.\n\nUnited States and France have built nuclear aircraft carriers.\n\n\nThe United States Navy has examples of these vessels, utilising the D2G and C1W reactors.\n\nNuclear-powered, civil merchant ships have not developed beyond a few experimental ships. The US-built , completed in 1962, was primarily a demonstration of civil nuclear power and was too small and expensive to operate economically as a merchant ship. The design was too much of a compromise, being neither an efficient freighter nor a viable passenger liner. The German-built , a cargo ship and research facility, sailed some on 126 voyages over 10 years without any technical problems. However, it proved too expensive to operate and was converted to diesel. The Japanese was dogged by technical and political problems. Its reactor had significant radiation leakage and fishermen protested against the vessel's operation. All of these three ships used low-enriched uranium. \"Sevmorput\", a Soviet and later Russian LASH carrier with icebreaking capability, has operated successfully on the Northern Sea Route since it was commissioned in 1988. , it is the only nuclear-powered merchant ship in service.\n\nCivilian nuclear ships suffer from the costs of specialized infrastructure. The Savannah was expensive to operate since it was the only vessel using its specialized nuclear shore staff and servicing facility. A larger fleet could share fixed costs among more operating vessels, reducing operating costs.\n\nRecently there has been renewed interest in nuclear propulsion, and some proposals have been drafted. For example, the cargo coaster is a new design for a nuclear cargo ship.\n\nIn November 2010 British Maritime Technology and Lloyd's Register embarked upon a two-year study with US-based Hyperion Power Generation (now Gen4 Energy), and the Greek ship operator Enterprises Shipping and Trading SA to investigate the practical maritime applications for small modular reactors. The research intended to produce a concept tanker-ship design, based on a 70 MWt reactor such as Hyperion's. In response to its members' interest in nuclear propulsion, Lloyd's Register has also re-written its 'rules' for nuclear ships, which concern the integration of a reactor certified by a land-based regulator with the rest of the ship. The overall rationale of the rule-making process assumes that in contrast to the current marine industry practice where the designer/builder typically demonstrates compliance with regulatory requirements, in the future the nuclear regulators will wish to ensure that it is the operator of the nuclear plant that demonstrates safety in operation, in addition to the safety through design and construction. Nuclear ships are currently the responsibility of their own countries, but none are involved in international trade. As a result of this work in 2014 two papers on commercial nuclear marine propulsion were published by Lloyd's Register and the other members of this consortium. These publications review past and recent work in the area of marine nuclear propulsion and describe a preliminary concept design study for a Suezmax tanker that is based on a conventional hull form with alternative arrangements for accommodating a 70 MWt nuclear propulsion plant delivering up to 23.5 MW shaft power at maximum continuous rating (average: 9.75 MW). The Gen4Energy power module is considered. This is a small fast-neutron reactor using lead-bismuth eutectic cooling and able to operate for ten full-power years before refueling, and in service last for a 25-year operational life of the vessel. They conclude that the concept is feasible, but further maturity of nuclear technology and the development and harmonisation of the regulatory framework would be necessary before the concept would be viable.\n\nNuclear propulsion has proven both technically and economically feasible for nuclear-powered icebreakers in the Soviet Arctic. Nuclear-fuelled ships operate for years without refueling, and the vessels have powerful engines, well-suited to the task of icebreaking.\n\nThe Soviet icebreaker \"Lenin\" was the world's first nuclear-powered surface vessel in 1959 and remained in service for 30 years (new reactors were fitted in 1970). It led to a series of larger icebreakers, the 23,500 ton of six vessels, launched beginning in 1975. These vessels have two reactors and are used in deep Arctic waters. NS \"Arktika\" was the first surface vessel to reach the North Pole.\n\nFor use in shallow waters such as estuaries and rivers, shallow-draft, \"Taymyr\"-class icebreakers are being built in Finland and then fitted with their single-reactor, nuclear propulsion system in Russia. They are built to conform to international safety standards for nuclear vessels.\n\nThe following are ships that are or were in commercial or civilian use and have nuclear marine propulsion.\n\n\nAll nuclear-powered icebreakers have been commissioned by the Soviet Union or Russia.\n\n\n\n"}
{"id": "3088856", "url": "https://en.wikipedia.org/wiki?curid=3088856", "title": "Odei", "text": "Odei\n\nIn Basque mythology, Odei, also known as Hodei is a spirit of thunder and the personification of storm clouds.\n"}
{"id": "11835411", "url": "https://en.wikipedia.org/wiki?curid=11835411", "title": "Ozone Action Day", "text": "Ozone Action Day\n\nAn Ozone Action Day, which can be declared by a local municipality, country or state, is observed at certain times during the summer months, when weather conditions (such as heat, humidity, and air stagnation) run the risk of causing health problems.\n\nOzone Action Days, alternately called an \"Ozone Alert\" or a \"Clean Air Alert\", primarily center in the midwestern portion of the United States; particularly in well-urbanized areas such as Chicago, Cleveland, Detroit, and Indianapolis.\n\nAlthough the ozone found at the Earth's surface is the same chemical species as that found in the ozone layer, they have very different sources, atmospheric chemistry, and affect human health differently as well. The ozone layer protects people from the sun's most damaging ultraviolet rays. Because the ozone layer is located high in the atmosphere, people are not directly exposed to it.\n\nGround-level ozone, however, is a health hazard because people breathe it. It is formed through a complex set of chemical reactions involving hydrocarbons, nitrogen oxides and sunlight on calm summer days where the weather may also be warm and humid. High levels of ground ozone affect the breathing process and aggravate asthma in chronic sufferers. The young, elderly, and those with lung diseases are especially susceptible.\n\nOzone is most likely to exceed safety limits from May through October when seasonal heat and sunlight are at their highest However, similar conditions can occur at other times of the year in specific urbanized areas; namely the Los Angeles area, which is well known for smog formation.\n\nA major cause of the conditions is due to pollutants in the air released by heavy industry (manufacturing plants, refineries, coal-fired power plants). Therefore, Ozone Action Days occur most frequently in the Midwestern United States. In recent years, many sites have taken steps to help reduce the amount of pollutants they discharge.\n\nSecondary sources include automotive emissions (leaky auto exhaust systems, excessive engine idling) and liberal use of household chemicals or sprays.\n\nIn 2008, the EPA created “non-attainment areas” for ozone in which ozone levels shall not exceed the federal standard of 75 parts per billion averaged over the course of three years. The EPA put the revised ozone standard into effect on October 1, 2015. This means that high altitude cities will have a more difficult time meeting the new federal standards. This is due to higher ozone concentrations (Denver-metro area) moving to areas of lower ozone concentrations (rural, mountain areas).\n\nFor example, the high altitude state of Colorado is working on reducing the overall amount of ozone throughout the state. The Denver Metropolitan and North Front Range areas specifically have violated the national ozone standards in the last several years. All other counties in Colorado are in compliance with the 75 ppb standard set by the EPA in 2015. Denver is currently the 8th most polluted area due to ozone in the United States. In 2015, Denver was ranked the 13th most-polluted city in the United States. Experts cite coal mining, population growth, and the oil and gas industries as potential reasons for the Denver metro area becoming more polluted.\n\nIn order to ensure safe ozone levels in an occupational setting, federal regulations are in place in order to enforce workplace exposure limits for all working men and women. These measures are not implemented as a response to Ozone Action Days but rather they are in place in order to reduce the levels of ozone and contribute to the overall reduction of ground ozone in the environment. \n\nCongress created the Occupational Safety and Health Administration (OSHA) in response to the Occupational Safety and Health Act of 1970. To address the issue of ozone in the work environment, OSHA set a legal airborne permissible exposure limit of 0.1 parts per million (ppm) averaged over an 8-hour work shift. \n\nThe National Institute for Occupational Safety and Health (NIOSH) also provides thorough information to ensure a safe working environment by conducting research and making recommendations for the prevention of work-related disease and injury. They recommend an airborne exposure limit for ozone of 0.1ppm, which should not be exceeded at any time.\n\nThe American Conference of Government Industrial Hygienists (ACGIH) is a member-based organization of industrial hygienists and individuals in the occupational and environmental health and safety industry. They recommend an airborne exposure limit of 0.05ppm for heavy work, 0.08ppm for moderate work, and 0.1ppm for light work. If work is done for less than two hours the ACGIH recommends an exposure airborne limit of 0.20pmm averaged over an 8-hour work shift.\nIt is important to note that engineering controls are the most effective way of reducing airborne exposure of ozone (unless the exposure is the result of chemical use, in which exposure can possibly be reduced by substituting a less hazardous chemical). These controls include proper ventilation systems, proper respirators and protective clothing.\n\nMillions suffer from asthma and it is one of the most common long term illnesses of children. It causes wheezing, breathlessness, chest tightness, and coughing. If symptoms get worse, they may end up getting hospitalized. Asthma exacerbations can be triggered by many factors such as tobacco smoke, dust mites, air pollution, pets, and mold. Surface ozone is also one of them. Surface ozone is one of the most common air pollutants and causes airway irritation. It can also reduce lung function. Studies show that areas with higher ozone levels have higher doses of asthma medications and increased emergency room visits, \n\nThere are differences between groups in both the magnitude of exposure to ozone as well as the ultimate health effects from the exposure. A study of 98 communities found that community characteristics changed the health impacts of ozone exposure. A greater effect of ozone was associated with higher unemployment, increased African American populations, increased public transportation use and use of central air conditioning. This could be due to increased exposure in these communities or other underlying health disparities that are exacerbated by the exposure to ozone.\n\nThis differential effect of ozone on health holds true for asthma and ozone exposure as well. In a longitudinal, prospective study, men who were exposed to a 27 parts per billion increase were found to have a 2-fold increase in asthma diagnosis. Although this trend was not found for women, the size of this ozone effect was not diminished when the researchers considered other air pollutants such as (PM10, SO4, NO2, and SO2). Additionally, it was found that after ozone levels equaled or exceeded 0.11 ppm, there was a 37% increase in hospital visits for asthma for African American families in low income areas. These gender, socioeconomic status and race differences need to be investigated further and solidify ozone exposure as a public health problem to be solved in the coming years.\n\nColorado is working on reducing the overall amount of ozone throughout the state. The Denver Metropolitan and North Front Range areas specifically have violated the national ozone standards in the last several years. All other counties in Colorado are in compliance with the 75 ppb standard set by the EPA in 2015. Denver is currently the 8th most polluted area due to ozone in the United States (Finley, 2016). In 2015, Denver was ranked the 13th most-polluted city in the United States. Experts cite coal mining, population growth, and the oil and gas industries as potential reasons for the Denver metro area becoming more polluted.\n\nIn 2008, the EPA created “nonattainment areas” for ozone in which ozone levels shall not exceed the federal standard of 75 parts per billion averaged over the course of three years.\n\nThe EPA put the revised ozone standard into effect on October 1, 2015 (Salley, 2015). This means that high altitude cities will have a more difficult time meeting the new federal standards. This is due to higher ozone concentrations (Denver-metro area) moving to areas of lower ozone concentrations (rural, mountain areas).\n\nState, county, and even local governments can announce Ozone Action Days as much as a day in advance through the monitoring of approaching weather conditions and the Air Quality Index (AQI). The AQI is divided into six levels: the higher the number (on a 0-300 scale), the more severe the ozone threat Air quality index.\n\nAQI of greater than 101 is considered dangerous for people with asthma, and asthma sufferers can take necessary precautions to avoid attacks. An AQI above 150 is considered unhealthy for all populations. They may check the current air quality index at www.airnow.gov for the most up to date information.\n\nHeavy industries make up a high percentage of pollutants causing ground ozone. Without drastically altering or eliminating industrial production in an area altogether, air quality improvements are very slight, though noticeable. Non-industrial pollutants, while not thought of to be a major pollutant group, can be more controlled with more positive change occurring.\n\nBasic steps in limiting ground ozone during Ozone Action Days are:\n\n\nAt home reduction activities include:\n\nSome cities, such as Phoenix, Evansville, Dallas, and most cities in Alaska prohibit outdoor burning during Ozone Action Days. Even if not prohibited, not doing any burning would be heavily advised.\n\nOn days with an AQI greater than 100, you can take several steps to reduce your exposure to ground level ozone.\n\nAvoid prolonged exertion outdoors- any activity that will cause you to be outdoors for several hours where you are breathing harder than normal\n\nAvoid heavy exertion outdoors- any activity that causes you to breathe heavily. For example- go on a walk instead of a run.\n\nAlthough the immediate concern is today’s Ozone Action Day, here are some longer term solutions\n\nUse alternative-fuel vehicles, such as natural gas and hybrid electric\n\nSome communities even expanded its line of buses, garbage trucks and even company vehicles, such as UPS trucks, to use alternative fuels\n\nChoose environmentally friendly products\n\nSuch as reduced volatile organic compounds (VOCs) for paint \n\nLocal groups have chemical round ups. For example, Colorado chemical information is at https://www.colorado.gov/pacific/cdphe/household-hazardous-waste-collection-programs\n\nSome waste companies also have disposal options, such as Waste Management http://www.wm.com/enterprise/municipalities/residential-solutions/household-hazardous-waste.jsp\n\nProper disposal of fridge and air conditioners that contain chemicals. Some energy companies will even pay you to take it! For example, Xcel Energy has $50 rebates https://www.xcelenergy.com/Programs_and_Rebates/Residential_Programs_and_Rebates/Equipment_and_Appliances/\n\n"}
{"id": "266145", "url": "https://en.wikipedia.org/wiki?curid=266145", "title": "Schumann resonances", "text": "Schumann resonances\n\nThe Schumann resonances (SR) are a set of spectrum peaks in the extremely low frequency (ELF) portion of the Earth's electromagnetic field spectrum. Schumann resonances are global electromagnetic resonances, generated and excited by lightning discharges in the cavity formed by the Earth's surface and the ionosphere.\n\nThis global electromagnetic resonance phenomenon is named after physicist Winfried Otto Schumann who predicted it mathematically in 1952. Schumann resonances occur because the space between the surface of the Earth and the conductive ionosphere acts as a closed waveguide. The limited dimensions of the Earth cause this waveguide to act as a resonant cavity for electromagnetic waves in the ELF band. The cavity is naturally excited by electric currents in lightning. Schumann resonances are the principal background in the part of the electromagnetic spectrum from 3 Hz through 60 Hz, and appear as distinct peaks at extremely low frequencies (ELF) around 7.83 Hz (fundamental), 14.3, 20.8, 27.3 and 33.8 Hz.\n\nIn the normal mode descriptions of Schumann resonances, the fundamental mode is a standing wave in the Earth–ionosphere cavity with a wavelength equal to the circumference of the Earth. This lowest-frequency (and highest-intensity) mode of the Schumann resonance occurs at a frequency of approximately 4.11 Hz, but this frequency can vary slightly from a variety of factors, such as solar-induced perturbations to the ionosphere, which compresses the upper wall of the closed cavity. The higher resonance modes are spaced at approximately 6.5 Hz intervals, a characteristic attributed to the atmosphere's spherical geometry. The peaks exhibit a spectral width of approximately 20% on account of the damping of the respective modes in the dissipative cavity. The 8th partial lies at approximately 60 Hz.\n\nObservations of Schumann resonances have been used to track global lightning activity. Owing to the connection between lightning activity and the Earth's climate it has been suggested that they may also be used to monitor global temperature variations and variations of water vapor in the upper troposphere. It has been speculated that extraterrestrial lightning (on other planets) may also be detected and studied by means of their Schumann resonance signatures. Schumann resonances have been used to study the lower ionosphere on Earth and it has been suggested as one way to explore the lower ionosphere on celestial bodies. Effects on Schumann resonances have been reported following geomagnetic and ionospheric disturbances. More recently, discrete Schumann resonance excitations have been linked to transient luminous events – sprites, ELVES, jets, and other upper-atmospheric lightning. A new field of interest using Schumann resonances is related to short-term earthquake prediction. Interest in Schumann resonances was renewed in 1993 when E. R. Williams showed a correlation between the resonance frequency and tropical air temperatures, suggesting the resonance could be used to monitor global warming..In applied geophysics, the resonances of schumann are used in the prospection of offshore hydrocarbon deposits.\n\nIn 1893, George Francis FitzGerald noted that the upper layers of the atmosphere must be fairly good conductors. Assuming that the height of these layers is about 100 km above ground, he estimated that oscillations (in this case the lowest mode of the Schumann resonances) would have a period of 0.1 second. Because of this contribution, it has been suggested to rename these resonances \"Schumann–FitzGerald resonances\". However FitzGerald's findings were not widely known as they were only presented at a meeting of the British Association for the Advancement of Science, followed by a brief mention in a column in \"Nature\".\n\nHence the first suggestion that an ionosphere existed, capable of trapping electromagnetic waves, is attributed to Heaviside and Kennelly (1902). It took another twenty years before Edward Appleton and Barnett in 1925 were able to prove experimentally the existence of the ionosphere.\n\nAlthough some of the most important mathematical tools for dealing with spherical waveguides were developed by G. N. Watson in 1918, it was Winfried Otto Schumann who first studied the theoretical aspects of the global resonances of the earth–ionosphere waveguide system, known today as the Schumann resonances. In 1952–1954 Schumann, together with H. L. König, attempted to measure the resonant frequencies. However, it was not until measurements made by Balser and Wagner in 1960–1963 that adequate analysis techniques were available to extract the resonance information from the background noise. Since then there has been an increasing interest in Schumann resonances in a wide variety of fields.\n\nLightning discharges are considered to be the primary natural source of Schumann resonance excitation; lightning channels behave like huge antennas that radiate electromagnetic energy at frequencies below about 100 kHz. These signals are very weak at large distances from the lightning source, but the Earth–ionosphere waveguide behaves like a resonator at ELF frequencies and amplifies the spectral signals from lightning at the resonance frequencies.\n\nIn an ideal cavity, the resonant frequency of the formula_1-th mode formula_2 is determined by the Earth radius formula_3 and the speed of light formula_4.\n\nThe real Earth–ionosphere waveguide is not a perfect electromagnetic resonant cavity. Losses due to finite ionosphere electrical conductivity lower the propagation speed of electromagnetic signals in the cavity, resulting in a resonance frequency that is lower than would be expected in an ideal case, and the observed peaks are wide. In addition, there are a number of horizontal asymmetries – day-night difference in the height of the ionosphere, latitudinal changes in the Earth's magnetic field, sudden ionospheric disturbances, polar cap absorption, variation in the Earth radius of ± 11 km from equator to geographic poles, etc. that produce other effects in the Schumann resonance power spectra.\n\nToday Schumann resonances are recorded at many separate research stations around the world. The sensors used to measure Schumann resonances typically consist of two horizontal magnetic inductive coils for measuring the north-south and east-west components of the magnetic field, and a vertical electric dipole antenna for measuring the vertical component of the electric field. A typical passband of the instruments is 3–100 Hz. The Schumann resonance electric field amplitude (~300 microvolts per meter) is much smaller than the static fair-weather electric field (~150 V/m) in the atmosphere. Similarly, the amplitude of the Schumann resonance magnetic field (~1 picotesla) is many orders of magnitude smaller than the Earth's magnetic field (~30–50 microteslas). Specialized receivers and antennas are needed to detect and record Schumann resonances. The electric component is commonly measured with a ball antenna, suggested by Ogawa et al., in 1966, connected to a high-impedance amplifier. The magnetic induction coils typically consist of tens- to hundreds-of-thousands of turns of wire wound around a core of very high magnetic permeability.\n\nFrom the very beginning of Schumann resonance studies, it was known that they could be used to monitor global lightning activity. At any given time there are about 2000 thunderstorms around the globe. Producing ~50 lightning events per second, these thunderstorms are directly linked to the background Schumann resonance signal.\n\nDetermining the spatial lightning distribution from Schumann resonance records is a complex problem: in order to estimate the lightning intensity from Schumann resonance records it is necessary to account for both the distance to lightning sources and the wave propagation between the source and the observer. A common approach is to make a preliminary assumption on the spatial lightning distribution, based on the known properties of lightning climatology. An alternative approach is placing the receiver at the North or South Pole, which remain approximately equidistant from the main thunderstorm centers during the day. One method not requiring preliminary assumptions on the lightning distribution is based on the decomposition of the average background Schumann resonance spectra, utilizing ratios between the average electric and magnetic spectra and between their linear combination. This technique assumes the cavity is spherically symmetric and therefore does not include known cavity asymmetries that are believed to affect the resonance and propagation properties of electromagnetic waves in the system.\n\nThe best documented and the most debated features of the Schumann resonance phenomenon are the diurnal variations of the background Schumann resonance power spectrum.\n\nA characteristic Schumann resonance diurnal record reflects the properties of both global lightning activity and the state of the Earth–ionosphere cavity between the source region and the observer. The vertical electric field is independent of the direction of the source relative to the observer, and is therefore a measure of global lightning. The diurnal behavior of the vertical electric field shows three distinct maxima, associated with the three \"hot spots\" of planetary lightning activity: one at 9 UT (Universal Time) linked to the daily peak of thunderstorm activity from Southeast Asia; one at 14 UT linked to the peak of African lightning activity; and one at 20 UT linked to the peak of South American lightning activity. The time and amplitude of the peaks vary throughout the year, linked to seasonal changes in lightning activity.\n\nIn general, the African peak is the strongest, reflecting the major contribution of the African \"chimney\" to global lightning activity. The ranking of the two other peaks—Asian and American—is the subject of a vigorous dispute among Schumann resonance scientists. Schumann resonance observations made from Europe show a greater contribution from Asia than from South America, while observations made from North America indicate the dominant contribution comes from South America.\n\nWilliams and Sátori suggest that in order to obtain \"correct\" Asia-America chimney ranking, it is necessary to remove the influence of the day/night variations in the ionospheric conductivity (day-night asymmetry influence) from the Schumann resonance records. The \"corrected\" records presented in the work by Sátori, et al. show that even after the removal of the day-night asymmetry influence from Schumann resonance records, the Asian contribution remains greater than American.\n\nSimilar results were obtained by Pechony et al. who calculated Schumann resonance fields from satellite lightning data. It was assumed that the distribution of lightning in the satellite maps was a good proxy for Schumann excitations sources, even though satellite observations predominantly measure in-cloud lightning rather than the cloud-to-ground lightning that are the primary exciters of the resonances. Both simulations—those neglecting the day-night asymmetry, and those taking this asymmetry into account—showed the same Asia-America chimney ranking. On the other hand, some optical satellite and climatological lightning data suggest the South American thunderstorm center is stronger than the Asian center.\n\nThe reason for the disparity among rankings of Asian and American chimneys in Schumann resonance records remains unclear, and is the subject of further research.\n\nIn the early literature the observed diurnal variations of Schumann resonance power were explained by the variations in the source-receiver (lightning-observer) geometry. It was concluded that no particular systematic variations of the ionosphere (which serves as the upper waveguide boundary) are needed to explain these variations. Subsequent theoretical studies supported the early estimations of the small influence of the ionosphere day-night asymmetry (difference between day-side and night-side ionosphere conductivity) on the observed variations in Schumann resonance field intensities.\n\nThe interest in the influence of the day-night asymmetry in the ionosphere conductivity on Schumann resonances gained new strength in the 1990s, after publication of a work by Sentman and Fraser. Sentman and Fraser developed a technique to separate the global and the local contributions to the observed field power variations using records obtained simultaneously at two stations that were widely separated in longitude. They interpreted the diurnal variations observed at each station in terms of a combination of a diurnally varying global excitation modulated by the local ionosphere height. Their work, which combined both observations and energy conservation arguments, convinced many scientists of the importance of the ionospheric day-night asymmetry and inspired numerous experimental studies. However, recently it was shown that results obtained by Sentman and Fraser can be approximately simulated with a uniform model (without taking into account ionosphere day-night variation) and therefore cannot be uniquely interpreted solely in terms of ionosphere height variation.\n\nSchumann resonance amplitude records show significant diurnal and seasonal variations which in general coincide in time with the times of the day-night transition (the terminator). This time-matching seems to support the suggestion of a significant influence of the day-night ionosphere asymmetry on Schumann resonance amplitudes. There are records showing almost clock-like accuracy of the diurnal amplitude changes. On the other hand, there are numerous days when Schumann Resonance amplitudes do not increase at sunrise or do not decrease at sunset. There are studies showing that the general behavior of Schumann resonance amplitude records can be recreated from diurnal and seasonal thunderstorm migration, without invoking ionospheric variations. Two recent independent theoretical studies have shown that the variations in Schumann resonance power related to the day-night transition are much smaller than those associated with the peaks of the global lightning activity, and therefore the global lightning activity plays a more important role in the variation of the Schumann resonance power.\n\nIt is generally acknowledged that source-observer effects are the dominant source of the observed diurnal variations, but there remains considerable controversy about the degree to which day-night signatures are present in the data. Part of this controversy stems from the fact that the Schumann resonance parameters extractable from observations provide only a limited amount of information about the coupled lightning source-ionospheric system geometry. The problem of inverting observations to simultaneously infer both the lightning source function and ionospheric structure is therefore extremely underdetermined, leading to the possibility of non-unique interpretations.\n\nOne of the interesting problems in Schumann resonances studies is determining the lightning source characteristics (the \"inverse problem\"). Temporally resolving each individual flash is impossible because the mean rate of excitation by lightning, ~50 lightning events per second globally, mixes up the individual contributions together. However, occasionally extremely large lightning flashes occur which produce distinctive signatures that stand out from the background signals. Called \"Q-bursts\", they are produced by intense lightning strikes that transfer large amounts of charge from clouds to the ground and often carry high peak current. Q-bursts can exceed the amplitude of the background signal level by a factor of 10 or more and appear with intervals of ~10 s, which allows to consider them as isolated events and determine the source lightning location. The source location is determined with either multi-station or single-station techniques and requires assuming a model for the Earth–ionosphere cavity. The multi-station techniques are more accurate, but require more complicated and expensive facilities.\n\nIt is now believed that many of the Schumann resonances transients (Q bursts) are related to the transient luminous events (TLEs). In 1995 Boccippio et al. showed that sprites, the most common TLE, are produced by positive cloud-to-ground lightning occurring in the stratiform region of a thunderstorm system, and are accompanied by Q-burst in the Schumann resonances band. Recent observations reveal that occurrences of sprites and Q bursts are highly correlated and Schumann resonances data can possibly be used to estimate the global occurrence rate of sprites.\n\nWilliams [1992] suggested that global temperature may be monitored with the Schumann resonances. The link between Schumann resonance and temperature is lightning flash rate, which increases nonlinearly with temperature. The nonlinearity of the lightning-to-temperature relation provides a natural amplifier of the temperature changes and makes Schumann resonance a sensitive \"thermometer\". Moreover, the ice particles that are believed to participate in the electrification processes which result in a lightning discharge have an important role in the radiative feedback effects that influence the atmosphere temperature. Schumann resonances may therefore help us to understand these feedback effects. In 2006 a paper was published linking Schumann resonance to global surface temperature followed up with a 2009 study.\n\nTropospheric water vapor is a key element of the Earth’s climate, which has direct effects as a greenhouse gas, as well as indirect effect through interaction with clouds, aerosols and tropospheric chemistry. Upper tropospheric water vapor (UTWV) has a much greater impact on the greenhouse effect than water vapor in the lower atmosphere, but whether this impact is a positive, or a negative feedback is still uncertain. The main challenge in addressing this question is the difficulty in monitoring UTWV globally over long timescales. Continental deep-convective thunderstorms produce most of the lightning discharges on Earth. In addition, they transport large amount of water vapor into the upper troposphere, dominating the variations of global UTWV. Price [2000] suggested that changes in the UTWV can be derived from records of Schumann resonances.\n\nThe existence of Schumann-like resonances is conditioned primarily by two factors:\nWithin the Solar System there are five candidates for Schumann resonance detection besides the Earth: Venus, Mars, Jupiter, Saturn and its biggest moon Titan.\nModeling Schumann resonances on the planets and moons of the Solar System is complicated by the lack of knowledge of the waveguide parameters. No \"in situ\" capability exists today to validate the results.\n\nThe strongest evidence for lightning on Venus comes from the impulsive electromagnetic waves detected by Venera 11 and 12 landers. Theoretical calculations of the Schumann resonances at Venus were reported by Nickolaenko and Rabinowicz [1982] and Pechony and Price [2004]. Both studies yielded very close results, indicating that Schumann resonances should be easily detectable on that planet given a lightning source of excitation and a suitably located sensor.\n\nIn the case of Mars there have been terrestrial observations of radio emission spectra that have been associated with Schumann resonances. The reported radio emissions are not of the primary electromagnetic Schumann modes, but rather of secondary modulations of the nonthermal microwave emissions from the planet at approximately the expected Schumann frequencies, and have not been independently confirmed to be associated with lightning activity on Mars. There is the possibility that future lander missions could carry in situ instrumentation to perform the necessary measurements. Theoretical studies are primarily directed to parameterizing the problem for future planetary explorers.\n\nDetection of lightning activity on Mars has been reported by Ruf et al. [2009]. The evidence is indirect and in the form of modulations of the nonthermal microwave spectrum at approximately the expected Schumann resonance frequencies. It has not been independently confirmed that these are associated with electrical discharges on Mars. In the event confirmation is made by direct, in situ observations, it would verify the suggestion of the possibility of charge separation and lightning strokes in the Martian dust storms made by Eden and Vonnegut [1973] and Renno et al. [2003]. Martian global resonances were modeled by Sukhorukov [1991], Pechony and Price [2004] and Molina-Cuberos et al. [2006]. The results of the three studies are somewhat different, but it seems that at least the first two Schumann resonance modes should be detectable. Evidence of the first three Schumann resonance modes is present in the spectra of radio emission from the lightning detected in Martian dust storms.\n\nIt was long ago suggested that lightning discharges may occur on Titan, but recent data from Cassini–Huygens seems to indicate that there is no lightning activity on this largest satellite of Saturn. Due to the recent interest in Titan, associated with the Cassini–Huygens mission, its ionosphere is perhaps the most thoroughly modeled today. Schumann resonances on Titan have received more attention than on any other celestial body, in works by Besser et al. [2002], Morente et al. [2003], Molina-Cuberos et al. [2004], Nickolaenko et al. [2003] and Pechony and Price [2004]. It appears that only the first Schumann resonance mode might be detectable on Titan.\n\nSince the landing of the Huygens probe on Titan's surface in January 2005, there have been many reports on observations and theory of an atypical Schumann resonance on Titan. After several tens of fly-bys by Cassini, neither lightning nor thunderstorms were detected in Titan's atmosphere. Scientists therefore proposed another source of electrical excitation: induction of ionospheric currents by Saturn's co-rotating magnetosphere. All data and theoretical models comply with a Schumann resonance, the second eigenmode of which was observed by the Huygens probe. The most important result of this is the proof of existence of a buried liquid water-ammonia ocean under few tens of km the icy subsurface crust.\n\nLightning activity has been optically detected on Jupiter. Existence of lightning activity on that planet was predicted by Bar-Nun [1975] and it is now supported by data from Galileo, Voyagers 1 and 2, Pioneers 10 and 11 and Cassini. Saturn is also confirmed to have lightning activity. Though three visiting spacecraft – Pioneer 11 in 1979, Voyager 1 in 1980 and Voyager 2 in 1981, failed to provide any convincing evidence from optical observations, in July 2012 the Cassini spacecraft detected visible lightning flashes, and electromagnetic sensors aboard the spacecraft detected signatures that are characteristic of lightning. Little is known about the electrical parameters of Jupiter and Saturn interior. Even the question of what should serve as the lower waveguide boundary is a non-trivial one in case of the gaseous planets. There seem to be no works dedicated to Schumann resonances on Saturn. To date there has been only one attempt to model Schumann resonances on Jupiter. Here, the electrical conductivity profile within the gaseous atmosphere of Jupiter was calculated using methods similar to those used to model stellar interiors, and it was pointed out that the same methods could be easily extended to the other gas giants Saturn, Uranus and Neptune. Given the intense lightning activity at Jupiter, the Schumann resonances should be easily detectable with a sensor suitably positioned within the planetary-ionospheric cavity.\n\n\n"}
{"id": "31427652", "url": "https://en.wikipedia.org/wiki?curid=31427652", "title": "Sebil (fountain)", "text": "Sebil (fountain)\n\nA sebil or sabil (; Turkish: \"sebil\") is a small kiosk where water is freely dispensed to members of the public. Historically, it is a structure of both civic and religious importance in Muslim cities, most characteristically under the Ottoman Empire, but also in other regions and periods such as Mamluk Cairo. It is sometimes also used to refer to simple fountains for drinking water. Sebils were built at crossroads, in the middle of city squares, and on the outside of mosques and other religious complexes throughout the Ottoman Empire to provide drinking water for travelers and enable ritual purification (ablutions) before prayer.\n\nA typical sebil was built over an underground cistern which supplied the water for distribution. In some cases, the pumped water ran down a decorative carved marble panel called a \"selsebil\", which may also have served the purpose of aerating the water as it came from the cistern. This service was free to members of the public, and was paid for by the revenues or funds of a charitable endowment, an Islamic waqf, provided or set up by the patron who commissioned the building. Endowing money for the construction of sebils was considered an act of piety, and the construction of many sebils was considered the hallmark of a beneficent ruler.\n\nIn 16th century Istanbul, sebils were a symbol of public possession. The attempt to add spigots was opposed because this was perceived as limiting public access to the blessings of nature. Initially, they functioned as kiosks where water was distributed to passersby.\nOften they were decorated in the Ottoman Rococo style and inscribed with Ottoman Turkish verses that formed a chronogram using the Abjad numbers to date the construction.\nUntil the spread of inhouse plumbing by the end of the 20th century, they were essential for the daily life of the inhabitants of Istanbul.\n\n\n"}
{"id": "26134454", "url": "https://en.wikipedia.org/wiki?curid=26134454", "title": "Ship Security Reporting System", "text": "Ship Security Reporting System\n\nThe Ship Security Reporting System (SSRS) is a counter piracy system that has been developed to combat the increasing instances of hijack and ransom on cargo ships predominantly occurring in the Gulf of Aden and around the Horn of Africa.\n\nBefore the development of SSRS, commercial shipping was reliant on the Ship Security Alerting System (SSAS) that contributes to the International Maritime Organization’s initiative to improve maritime security through the Safety of Life at Sea (SOLAS) convention and, more recently, through the International Ship and Port Facility Security Code (ISPS).\n\nThe SSRS builds on the Ship Security Alert System (SSAS) that exists on most cargo and passenger ships over 300 gross tonnes flagged to SOLAS contracting governments by linking the SSAS to naval forces responsible for maritime security in a specific area. The SSRS continually monitors ship security alerts and transmits critical data to participating Naval Operations Centres. The NOC alerts the nearest naval vessel, thus initiating an almost immediate military response.\n\n\n"}
{"id": "40493522", "url": "https://en.wikipedia.org/wiki?curid=40493522", "title": "Su iyesi", "text": "Su iyesi\n\nIn Turkic mythology, Su Iyesi (Tatar: \"Су Иясе\" or \"Su İyäse\"; Chuvash: \"Шыв Ийӗ\"; Sakha: \"Уу Иччи\"; literally \"water master\") is a water spirit. It corresponds to the nymph in Turkic cultures. It is a disembodied, incorporeal, intangible entity, but she can turn into a female creature and daughter of Yer Tanrı.\n\nWhen angered, she breaks dams, washes away water mills, and drowns people and animals. She drags people down to her underwater dwelling to serve her as slaves. She is in Tatar fairy tales the same creature as the Su Anası (\"water mother\"). In Turkic tales, she lives in ponds or rivers. There is no mention of a particular dwelling, and the 'half-sunken log' is unapparent. She rides on a log to travel.\n\nSu Iyesi is sometimes associated with perilous events such as floods, storms, shipwrecks and drownings. In other Turkic folk traditions, she can be benevolent or beneficent.\n\nSu Ana (\"water mother\") is often mentioned as the female form of Su Iyesi. She is said to appear as a naked young woman with a fairy-like face and yellow and long hair, usually covered in black fish scales. She has a fish's tail and eyes that burn like fire. She usually rides along her river on a half-sunken log, making loud splashes. Local drownings are said to be the work of the Su Anası. She is the wife of Su Ata. She likes shores and likes to get out of the water.\n\nIts name in Hungarian culture is \"Víz Anya\" and in Mongolian belief is \"Ус Ээж\" (Buryat: \"Уһан Эхэ\"; Oirat: \"Усн Эк\"). These entities have many similarities, and each name has the same meaning, \"aqua mother\".\n\nSu Ata (\"water father\") is the male form of Su Iyesi. He appears as an old man with a frog-like face, greenish beard, with his body covered in algae and muck. He has webbed paws instead of hands. He usually rides along his river. Consequently, he is often dubbed Vudaş (Chuvash: \"Вутăш, Vutăş\") by the Chuvash people. He is a river and lake god. When someone has drowned, people often say \"Su Ata took him.\" He also reportedly hates people who pollute the waters. The advice on how to please him goes that one should throw a whole bread into the water to make him happy. Also, when a bride must go far away, she has to be introduced to Su Ata.\n\nIts name in Hungarian culture is \"Víz Atya\" or \"Víz Apa\" and in Mongolian belief is \"Ус Эцэг\" (Buryat: \"Уһан Эсэгэ\"; Oirat: \"Усн эцк\"). These entities have many similarities, and each has the same meaning, \"water father\".\n\n\n\n"}
{"id": "18904609", "url": "https://en.wikipedia.org/wiki?curid=18904609", "title": "Subaqueous volcano", "text": "Subaqueous volcano\n\nA subaqueous volcano is a volcano formed beneath freshwater and never builds above lake level. They are commonly in the form of gently sloping tuff cones, although they can sometimes have an , such as White Horse Bluff in the Wells Gray-Clearwater volcanic field of east-central British Columbia, Canada.\n\nSubaqueous volcanoes can be compared to subaerial volcanoes which are formed and erupt on land surface, or under the air. The major differences of volcanic eruptions are due to the effects of pressure, heat capacity or conductivity of water, the presence of steam and water rheology. The thermal conductivity of water is about 20 times that of air and steam has a thermal conductivity nearly 50 times that of water.\nSubaqueous volcanoes are most commonly formed in oceans, but can also form in lakes, rivers and subglacial lakes. In improving our understanding of subaqueous volcanoes, it is important to consider the differences between the characteristics of modern and ancient approaches to the study. Modern studies offer fresh and unaltered observances, can see and map surface features and the water depth is known in areas that allow observation. Ancient studies have had stratigraphic exposure to sections, are easier to work on, have more and better exposures and have an existing relationship to resources.\n\nSome geologists would restrict the term subaqueous pyroclastic flow deposits to volcaniclastic units that show characteristics of emplacement in a hot state deposited underwater—however, this can’t always be done because of the subsequent process of alteration/diagenesis such as active hot springs and associated hydrothermal alteration. Deposits from pyroclastic flows that interact with water and are transformed into water-supported mass flows are called subaqueous pyroclastic debris flow deposits by some geologists. On the other hand, processes that are associated with eruption, transportation and deposition are notably different because of the presence of water. Such differences that the presence of water entails is the ability to vaporize when in contact with water, a high density and resulting confining pressure, high viscosity relative to air and differences in the thermal conductivities/heat capacities in the air relative to water.\n\nSome understanding of subaqueous volcanoes can be inferred from knowledge of volcanic processes based on ancient successions. Subaqueous volcano deposits have been occurring in the south of Honshu, the largest island among Japan’s four principal islands. The four subaqueous volcanic deposits have been documented and are located throughout Japan offer significant evidence to study.\n\nSubaqueous volcanic deposits are associated with subaqueous sedimentary deposits and these deposits range from near shore, off-shore and abyssal mudstone deposits. Unfortunately, paleo-depth constraints for sedimentary strata are poor and subject to contradicting interpretations. However, the depth of emplacement can be conjectured with minor control of water depth. In determining the characteristics of pyroclastic flows in subaerial versus subaqueous deposits, it is commonly believed that water fluidized volcaniclastic flows become normally graded in terms of all components except for large, buoyant pumice blocks which settle to form large pumice layers. However, this phenomenon is usually seen as subaerial ignimbrite (pumice rich pyroclastic flows) deposits. Because of this, the characteristic is not considered clear evidence for the interpretation of the fluidizing agent (hot gas or water) and can therefore only be used in conjunction with other criteria.\n\nCharacteristics can be sorted to infer subaqueous eruption or emplacement of silicic pyroclastic deposits. Larger pumice blocks rise for a more extended period of time (minutes to hours) in comparison to smaller pumice fragments because of gases trapped within vesicles and the very fine ash fragments may become entrained into the rising plume of gas and heated water because of the low density and weight. Therefore, subaqueous silicic pyroclastic eruptions may be diminished in the course size fraction as well as the very fine ash size fraction based on the buoyancy of the material in the water medium. These characteristics may be important in determining the style of subaqueous eruption and emplacement mechanism. The characteristics of texture, such as grain morphology and grain size abundances can also provide knowledge on the process of controlling the eruption style or transport/flow properties, whether turbulent or laminar.\n\nSeafloor exploration has discovered that more volcanic eruptions occur at the bottom of the sea than on land. However, the effects of ambient water and hydrostatic pressure on silicic volcanic eruptions in subaqueous settings are not entirely understood because deep marine eruptions are not directly observed and studied. Because of this, information of recent deep-water volcanic eruptions are still incomplete and limited.\n\nThe conclusions of the studies of subaqueous volcanoes in Japan determine that clear evidence for eruption and/or emplacement of pyroclastic flows continue to be determined from the examination of these deposits although inferential evidence such as grain morphology, sorting and grading can be used to identify and document ancient subaqueous volcanic deposits. \nThe University of California, Santa Barbara will continue to conduct further research which may be able to provide further information on styles of subaqueous volcanic eruptions and/or flow characteristics of volcanic deposits.\n"}
{"id": "12098877", "url": "https://en.wikipedia.org/wiki?curid=12098877", "title": "Tanzania Atomic Energy Commission", "text": "Tanzania Atomic Energy Commission\n\nThe Tanzania Atomic Energy Commission \"(TAEC)\" is a regulatory and service parastatal organization of Tanzania, operating under the mandate of the Ministry of Education, Science and Technology. Offices are located in Njiro Area, Arusha and has zonal offices in Namanga, Dar es Salaam and Zanzibar.\n\nThe TAEC has been mandated to:\n\n\n\n"}
{"id": "1742101", "url": "https://en.wikipedia.org/wiki?curid=1742101", "title": "Titanic (1996 miniseries)", "text": "Titanic (1996 miniseries)\n\nTitanic is a 1996 American two-part television miniseries which premiered on CBS on November 17 and 19, 1996. \"Titanic\" follows several characters on board the RMS \"Titanic\" when she sinks on her maiden voyage in 1912. The miniseries was directed by Robert Lieberman. The original music score was composed by Lennie Niehaus.\n\n\"Titanic\" follows three main story threads.\n\nIsabella Paradine is traveling on the \"Titanic\" to join her husband after attending her aunt's funeral in England. On the \"Titanic\", she meets Wynn Park, her former lover. She falls in love with him again, and after a brief affair, she sends her husband a wireless saying they cannot be together anymore (despite their daughter). When the ship starts sinking, Isabella reluctantly leaves Wynn when he forces her to board a lifeboat. As the boat is lowered, Isabella confesses a long kept secret that her daughter Claire is actually Wynn's. Later on board the RMS \"Carpathia\" she is grief-stricken when she finds Wynn's lifeless body on deck, having died of hypothermia, but luckily, when the \"Carpathia\" reaches New York she is reunited with her family who are blissfully unaware of Isabella's tryst because the telegram was never sent out due to the sinking.\n\nAlso in first class is the Allison family, a family travelling on the Titanic, returning home to Montreal with their two small children and new nurse, Alice Cleaver. They gradually become wary and suspicious of her hysterical and neurotic behavior. Later on, a fellow maid asks her if she'd seen her in Cairo the previous month, but soon realizes that she remembers her from the highly publicized trial where Alice was accused of throwing her baby off a train. When the \"Titanic\" starts sinking, Alice Cleaver panics and quickly boards a lifeboat with Trevor, the Allisons' infant son. The parents with their small daughter are unaware that the baby is safe and refuse to leave the ship without him, which in the end costs them their lives.\n\nIn third class, a young vagrant named Jamie Perse steals a ticket to get on board. He manages to become friends with one of the crewmen, Simon Doonan, who is also a robber, but later is revealed to be a much more violent and callous criminal than Jamie. The young man falls in love with Aase (pronounced \"Osa\") Ludvigsen, a recent Christian convert and missionary. On the night of the sinking, Aase is brutally raped and beaten by Doonan, causing her to lose her faith and will to live, but Jamie manages to get her into Isabella's boat. Unbeknownst to them, Doonan also sneaks aboard that same boat, disguised as an old woman. After the ship sinks, Aase is knocked off the lifeboat by Doonan after she recognizes him, and he attempts to hold the passengers in the boat hostage at gunpoint, but Officer Lowe, who is in charge of the boat, hits Doonan in the head with a paddle, snapping his neck and killing him. Jamie himself manages to survive when he accidentally falls into one of the last lifeboats before the \"Titanic\" sinks. He subsequently atones for his past life after he finds Aase in the makeshift hospital aboard the \"Carpathia\". In the end, upon arriving in New York, the two plan to start a new life together.\n\n\"Titanic\" received mixed to negative reviews from critics. The \"New York Daily News\" commented on the fact that the acting was substandard and the ship's operators and owner are portrayed \"about as sympathetically as those connected with the Exxon Valdez.\" The \"Seattle Post-Intelligencer\" also referenced the \"embarrassingly bad acting\" and out of place scenes.\n\n\"Titanic\" received an Emmy Award for Outstanding Sound Mixing for a Drama Miniseries or a Special. It was also nominated for Outstanding Costume Design for a Miniseries or a Special.\n"}
{"id": "1449821", "url": "https://en.wikipedia.org/wiki?curid=1449821", "title": "Transit Research and Attitude Control", "text": "Transit Research and Attitude Control\n\nThe Transit Research and Attitude Control (TRAAC) satellite was launched by the U. S. Navy from Cape Canaveral along with Transit 4B on November 15, 1961. \n\nThe 109 kg satellite was used to test the feasibility of using gravity-gradient stabilization in Transit navigational satellites. It provided information on the effects of radiation from nuclear explosions in space, as it was one of several satellites whose detectors provided data for the Starfish Prime test; ultimately its solar cells were damaged by the radiation and it ceased operation. (It was among several satellites which were inadvertently damaged or destroyed by the Starfish Prime high-altitude nuclear test on July 9, 1962 and subsequent radiation belt.) It is expected to orbit for 800 years at an altitude of about .\n\nThe first poem to be launched into orbit about the Earth was inscribed on the instrument panel of TRAAC. Entitled \"Space Prober\" and written by Prof. Thomas G. Bergin of Yale University, it reads in part:\n"}
{"id": "32500", "url": "https://en.wikipedia.org/wiki?curid=32500", "title": "Vacuum pump", "text": "Vacuum pump\n\nA vacuum pump is a device that removes gas molecules from a sealed volume in order to leave behind a partial vacuum. The first vacuum pump was invented in 1650 by Otto von Guericke, and was preceded by the suction pump, which dates to antiquity.\n\nThe predecessor to the vacuum pump was the suction pump, which was known to the Romans. Dual-action suction pumps were found in the city of Pompeii. Arabic engineer Al-Jazari also described suction pumps in the 13th century. He said that his model was a larger version of the siphons the Byzantines used to discharge the Greek fire. The suction pump later reappeared in Europe from the 15th century.\nBy the 17th century, water pump designs had improved to the point that they produced measurable vacuums, but this was not immediately understood. What was known was that suction pumps could not pull water beyond a certain height: 18 Florentine yards according to a measurement taken around 1635. (The conversion to metres is uncertain, but it would be about 9 or 10 metres.) This limit was a concern to irrigation projects, mine drainage, and decorative water fountains planned by the Duke of Tuscany, so the duke commissioned Galileo to investigate the problem. Galileo advertised the puzzle to other scientists, including Gasparo Berti who replicated it by building the first water barometer in Rome in 1639. Berti's barometer produced a vacuum above the water column, but he could not explain it. The breakthrough was made by Evangelista Torricelli in 1643. Building upon Galileo's notes, he built the first mercury barometer and wrote a convincing argument that the space at the top was a vacuum. The height of the column was then limited to the maximum weight that atmospheric pressure could support; this is the limiting height of a suction pump.\n\nIn 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that teams of horses could not separate two hemispheres from which the air had been evacuated. Robert Boyle improved Guericke's design and conducted experiments on the properties of vacuum. Robert Hooke also helped Boyle produce an air pump which helped to produce the vacuum. The study of vacuum then lapsed until 1855, when Heinrich Geissler invented the mercury displacement pump and achieved a record vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, and this renewed interest in vacuum. This, in turn, led to the development of the vacuum tube.\n\nIn the 19th century, Nikola Tesla designed an apparatus that contains a Sprengel pump to create a high degree of exhaustion.\n\nPumps can be broadly categorized according to three techniques:\n\nPositive displacement pumps use a mechanism to repeatedly expand a cavity, allow gases to flow in from the chamber, seal off the cavity, and exhaust it to the atmosphere. Momentum transfer pumps, also called molecular pumps, use high speed jets of dense fluid or high speed rotating blades to knock gas molecules out of the chamber. Entrapment pumps capture gases in a solid or adsorbed state. This includes cryopumps, getters, and ion pumps.\n\nPositive displacement pumps are the most effective for low vacuums. Momentum transfer pumps in conjunction with one or two positive displacement pumps are the most common configuration used to achieve high vacuums. In this configuration the positive displacement pump serves two purposes. First it obtains a rough vacuum in the vessel being evacuated before the momentum transfer pump can be used to obtain the high vacuum, as momentum transfer pumps cannot start pumping at atmospheric pressures. Second the positive displacement pump backs up the momentum transfer pump by evacuating to low vacuum the accumulation of displaced molecules in the high vacuum pump. Entrapment pumps can be added to reach ultrahigh vacuums, but they require periodic regeneration of the surfaces that trap air molecules or ions. Due to this requirement their available operational time can be unacceptably short in low and high vacuums, thus limiting their use to ultrahigh vacuums. Pumps also differ in details like manufacturing tolerances, sealing material, pressure, flow, admission or no admission of oil vapor, service intervals, reliability, tolerance to dust, tolerance to chemicals, tolerance to liquids and vibration.\n\nA partial vacuum may be generated by increasing the volume of a container. To continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind a positive displacement pump, for example the manual water pump. Inside the pump, a mechanism expands a small sealed cavity to reduce its pressure below that of the atmosphere. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.\n\nMore sophisticated systems are used for most industrial applications, but the basic principle of cyclic volume removal is the same:\n\nThe base pressure of a rubber- and plastic-sealed piston pump system is typically 1 to 50 kPa, while a scroll pump might reach 10 Pa (when new) and a rotary vane oil pump with a clean and empty metallic chamber can easily achieve 0.1 Pa.\n\nA positive displacement vacuum pump moves the same volume of gas with each cycle, so its pumping speed is constant unless it is overcome by backstreaming.\n\nIn a momentum transfer pump, gas molecules are accelerated from the vacuum side to the exhaust side (which is usually maintained at a reduced pressure by a positive displacement pump). Momentum transfer pumping is only possible below pressures of about 0.1 kPa. Matter flows differently at different pressures based on the laws of fluid dynamics. At atmospheric pressure and mild vacuums, molecules interact with each other and push on their neighboring molecules in what is known as viscous flow. When the distance between the molecules increases, the molecules interact with the walls of the chamber more often than with the other molecules, and molecular pumping becomes more effective than positive displacement pumping. This regime is generally called high vacuum.\n\nMolecular pumps sweep out a larger area than mechanical pumps, and do so more frequently, making them capable of much higher pumping speeds. They do this at the expense of the seal between the vacuum and their exhaust. Since there is no seal, a small pressure at the exhaust can easily cause backstreaming through the pump; this is called stall. In high vacuum, however, pressure gradients have little effect on fluid flows, and molecular pumps can attain their full potential.\n\nThe two main types of molecular pumps are the diffusion pump and the turbomolecular pump. Both types of pumps blow out gas molecules that diffuse into the pump by imparting momentum to the gas molecules. Diffusion pumps blow out gas molecules with jets of oil or mercury, while turbomolecular pumps use high speed fans to push the gas. Both of these pumps will stall and fail to pump if exhausted directly to atmospheric pressure, so they must be exhausted to a lower grade vacuum created by a mechanical pump.\n\nAs with positive displacement pumps, the base pressure will be reached when leakage, outgassing, and backstreaming equal the pump speed, but now minimizing leakage and outgassing to a level comparable to backstreaming becomes much more difficult.\n\nRegenerative pumps utilize vortex behavior of the fluid (air). The construction is based on hybrid concept of centrifugal pump and turbopump. Usually it consists of several sets of perpendicular teeth on the rotor circulating air molecules inside stationary hollow grooves like multistage centrifugal pump. They can reach to 1×10 mbar (0.001 Pa)(when combining with Holweck pump) and directly exhaust to atmospheric pressure. Examples of such pumps are Edwards EPX (technical paper ) and Pfeiffer OnTool™ Booster 150. It is sometimes referred as side channel pump. Due to high pumping rate from atmosphere to high vacuum and less contamination since bearing can be installed at exhaust side, this type of pumps are used in load lock in semiconductor manufacturing processes.\n\nThis type of pump suffers from high power consumption(~1 kW) compare to turbomolecular pump (<100W) at low pressure since most power is consumed to back atmospheric pressure. This can be reduced by nearly 10 times by backing with a small pump.\n\nAn entrapment pump may be a cryopump, which uses cold temperatures to condense gases to a solid or adsorbed state, a chemical pump, which reacts with gases to produce a solid residue, or an ion pump, which uses strong electrical fields to ionize gases and propel the ions into a solid substrate. A cryomodule uses cryopumping. Other types are the sorption pump, non-evaporative getter pump, and titanium sublimation pump (a type of evaporative getter that can be used repeatedly).\n\n\nPumping speed refers to the volume flow rate of a pump at its inlet, often measured in volume per unit of time. Momentum transfer and entrapment pumps are more effective on some gases than others, so the pumping rate can be different for each of the gases being pumped, and the average volume flow rate of the pump will vary depending on the chemical composition of the gases remaining in the chamber.\n\nThroughput refers to the pumping speed multiplied by the gas pressure at the inlet, and is measured in units of pressure·volume/unit time. At a constant temperature, throughput is proportional to the number of molecules being pumped per unit time, and therefore to the mass flow rate of the pump. When discussing a leak in the system or backstreaming through the pump, throughput refers to the volume leak rate multiplied by the pressure at the vacuum side of the leak, so the leak throughput can be compared to the pump throughput.\n\nPositive displacement and momentum transfer pumps have a constant volume flow rate (pumping speed), but as the chamber's pressure drops, this volume contains less and less mass. So although the pumping speed remains constant, the throughput and mass flow rate drop exponentially. Meanwhile, the leakage, evaporation, sublimation and backstreaming rates continue to produce a constant throughput into the system.\n\nVacuum pumps are combined with chambers and operational procedures into a wide variety of vacuum systems. Sometimes more than one pump will be used (in series or in parallel) in a single application. A partial vacuum, or rough vacuum, can be created using a positive displacement pump that transports a gas load from an inlet port to an outlet (exhaust) port. Because of their mechanical limitations, such pumps can only achieve a low vacuum. To achieve a higher vacuum, other techniques must then be used, typically in series (usually following an initial fast pump down with a positive displacement pump). Some examples might be use of an oil sealed rotary vane pump (the most common positive displacement pump) backing a diffusion pump, or a dry scroll pump backing a turbomolecular pump. There are other combinations depending on the level of vacuum being sought.\n\nAchieving high vacuum is difficult because all of the materials exposed to the vacuum must be carefully evaluated for their outgassing and vapor pressure properties. For example, oils, greases, and rubber or plastic gaskets used as seals for the vacuum chamber must not boil off when exposed to the vacuum, or the gases they produce would prevent the creation of the desired degree of vacuum. Often, all of the surfaces exposed to the vacuum must be baked at high temperature to drive off adsorbed gases.\n\nOutgassing can also be reduced simply by desiccation prior to vacuum pumping.\nHigh vacuum systems generally require metal chambers with metal gasket seals such as Klein flanges or ISO flanges, rather than the rubber gaskets more common in low vacuum chamber seals. The system must be clean and free of organic matter to minimize outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure. As a result, many materials that work well in low vacuums, such as epoxy, will become a source of outgassing at higher vacuums.\nWith these standard precautions, vacuums of 1 mPa are easily achieved with an assortment of molecular pumps. With careful design and operation, 1 µPa is possible.\n\nSeveral types of pumps may be used in sequence or in parallel. In a typical pumpdown sequence, a positive displacement pump would be used to remove most of the gas from a chamber, starting from atmosphere (760 Torr, 101 kPa) to 25 Torr (3 kPa). Then a sorption pump would be used to bring the pressure down to 10 Torr (10 mPa). A cryopump or turbomolecular pump would be used to bring the pressure further down to 10 Torr (1 µPa). An additional ion pump can be started below 10 Torr to remove gases which are not adequately handled by a cryopump or turbo pump, such as helium or hydrogen.\n\nUltra high vacuum generally requires custom-built equipment, strict operational procedures, and a fair amount of trial-and-error. Ultra-high vacuum systems are usually made of stainless steel with metal-gasketed vacuum flanges. The system is usually baked, preferably under vacuum, to temporarily raise the vapour pressure of all outgassing materials in the system and boil them off. If necessary, this outgassing of the system can also be performed at room temperature, but this takes much more time. Once the bulk of the outgassing materials are boiled off and evacuated, the system may be cooled to lower vapour pressures to minimize residual outgassing during actual operation. Some systems are cooled well below room temperature by liquid nitrogen to shut down residual outgassing and simultaneously cryopump the system.\n\nIn ultra-high vacuum systems, some very odd leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the absorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The porosity of the metallic vacuum chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.\n\nThe impact of molecular size must be considered. Smaller molecules can leak in more easily and are more easily absorbed by certain materials, and molecular pumps are less effective at pumping gases with lower molecular weights. A system may be able to evacuate nitrogen (the main component of air) to the desired vacuum, but the chamber could still be full of residual atmospheric hydrogen and helium. Vessels lined with a highly gas-permeable material such as palladium (which is a high-capacity hydrogen sponge) create special outgassing problems.\n\nVacuum pumps are used in many industrial and scientific processes including composite plastic moulding processes, production of most types of electric lamps, vacuum tubes, and CRTs where the device is either left evacuated or re-filled with a specific gas or gas mixture, semiconductor processing, notably ion implantation, dry etch and PVD, ALD, PECVD and CVD deposition and so on in photolithography, electron microscopy, medical processes that require suction, uranium enrichment, medical applications such as radiotherapy, radiosurgery and radiopharmacy, analytical instrumentation to analyse gas, liquid, solid, surface and bio materials, mass spectrometers to create a high vacuum between the ion source and the detector, vacuum coating on glass, metal and plastics for decoration, for durability and for energy saving, such as low-emissivity glass, hard coating for engine components (as in Formula One), ophthalmic coating, milking machines and other equipment in dairy sheds, vacuum impregnation of porous products such as wood or electric motor windings, air conditioning service (removing all contaminants from the system before charging with refrigerant), trash compactor, vacuum engineering, sewage systems (see EN1091:1997 standards), freeze drying, and fusion research. In the field of oil regeneration and rerefining, vacuum pumps create a low vacuum for oil dehydration and a high vacuum for oil purification. Especially in the field of transformer maintenance, vacuum pumps play an essential role in transformer oil purification plants which are used to extend the lifetime of transformers in the field.\n\nVacuum may be used to power, or provide assistance to mechanical devices. In hybrid and diesel engine motor vehicles, a pump fitted on the engine (usually on the camshaft) is used to produce vacuum. In petrol engines, instead, vacuum is typically obtained as a side-effect of the operation of the engine and the flow restriction created by the throttle plate, but may be also supplemented by an electrically operated vacuum pump to boost braking assistance or improve fuel consumption. This vacuum may then be used to power the following motor vehicle components: vacuum servo booster for the hydraulic brakes, motors that move dampers in the ventilation system, throttle driver in the cruise control servomechanism, door locks or trunk releases.\n\nIn an aircraft, the vacuum source is often used to power gyroscopes in the various flight instruments. To prevent the complete loss of instrumentation in the event of an electrical failure, the instrument panel is deliberately designed with certain instruments powered by electricity and other instruments powered by the vacuum source.\n\nOld vacuum-pump oils that were produced before circa 1980 often contain a mixture of several different dangerous polychlorinated biphenyls (PCBs), which are highly toxic, carcinogenic, persistent organic pollutants.\n\n"}
{"id": "2625607", "url": "https://en.wikipedia.org/wiki?curid=2625607", "title": "Western Gulf coastal grasslands", "text": "Western Gulf coastal grasslands\n\nThe Western Gulf coastal grasslands are a subtropical grassland ecoregion of the southern United States and northeastern Mexico. It is known in Louisiana as the \"Cajun Prairie\", Texas as \"Coastal Prairie,\" and as the Tamaulipan pastizal in Mexico.\n\nThe ecoregion covers an area of , extending along the shore of the Gulf of Mexico from southeastern Louisiana (west of the Mississippi Delta) through Texas and into the Mexican state of Tamaulipas as far as the Laguna Madre. Specific areas include a number of barrier islands, and the \"resacas\" or natural levees of the Laguna Madre. The coast is vulnerable to tropical storms that can seriously damage habitats.\n\nThis ecosystem is edaphic in origin; the soils in this region are of a heavy clay that contributed to difficulty for woody species to establish, allowing grasses and herbaceous species to be more competitive.The region name, though, is a bit of a misnomer. It is not a wide open, treeless prairie; instead, the grassy areas are broken up by many pockets/groves of forest, usually along water courses, or isolated silt/sand pockets among the substrate (where the soil is more permissive for tree growth).\n\nFrom Louisiana west to the Upper Texas coast, the climate of the region is wet humid subtropical, featuring high annual rainfall. The climate becomes more arid going down the Texas coast, and into Northeastern Mexico, though precipitation totals still remain high enough for humid subtropical classification.\n\nThe natural habitat of the area is a mix of tallgrass prairie similar to those found in inland Texas, with Indiangrass (\"Sorghastrum nutans\"), big bluestem (\"Andropogon gerardi\"), little bluestem (\"Schizachyrium scoparium\"), and switchgrass (\"Panicum virgatum\") the primary tallgrass species that are typical of the coastal prairie, with several other shorter grasses and many herbaceous and woody species. \n\nIn addition, the region has bottomland forests lining the floodplains of the many waterways in the region. In the wetter regions along the upper Texas coast and Louisiana, the tree species include many of those seen in forest of the Southern United States, such as the southern live oak, bald cypress, loblolly pine, post oak, and southern hackberry.\n\nThe southern third of the Texas stretch and all of the Tamaulipan portion contains shrubby areas of honey mesquite (\"Prosopis glandulosa\"), huisache (\"Vachellia farnesiana\" var. \"farnesiana\"), lime prickly-ash (\"Zanthoxylum fagara\"), and Texas persimmon (\"Diospyros texana\").\n\nThis coast is rich in wildlife, and 700 species of birds, animals and reptiles have been counted here, although many are now threatened or endangered. This coast is a critical habitat for the Attwater's prairie chickens (\"Tympanuchus cupido attwateri\"), over one million of which inhabited the prairie in Texas and Louisiana in the 19th century, but extreme reduction of their habitat put them on the U.S. endangered species list in 1967. Another endangered bird of the coast is the whooping crane (\"Grus americana\") while other birds include least grebe, Morelet’s seedeater (\"Sporophila morelleti\"), red-billed pigeon (\"Columba flavirostris\"), brown jay (\"Cyanocorax morio\"), Neotropic cormorant, white-winged dove (\"Leptotila verrequxi\") and Audubon's oriole (\"Icterus graduacauda\").\n\nMammals of the area include ocelot (\"Leopardus pardalis\"), Gulf Coast jaguarundi (\"Puma yagouaroundi cacomitli\"), southern yellow bat (\"Lasiurus ega\"), Mexican spiny pocket mouse (\"Liomys irroratus\"), bobcats, collared peccary and eastern cottontails. Rancho Nuevo beach in Tamaulipas and along the Texas coast are the only two nesting sites in the world for the Kemp's ridley sea turtle (\"Lepidochelys kempii\") while other herpetofauna of the ecoregion include Río Grande chirping frog (\"Eleutherodactylus cystignathoides\") and Mexican white-lipped frog (\"Leptodactylus fragilis\").\n\nLess than 1% of the ecoregion remains in pristine condition, almost entirely in Texas, while most of the coast has been converted to farmland, including rice paddies, grazing land, or urban areas including Houston, Texas. Estuaries and other coastal wetlands are better preserved than the prairie and indeed the protected areas of the coast are mainly sanctuaries for waterbirds.\n\n"}
{"id": "173370", "url": "https://en.wikipedia.org/wiki?curid=173370", "title": "Whistler (radio)", "text": "Whistler (radio)\n\nA whistler is a very low frequency or VLF electromagnetic (radio) wave generated by lightning. Frequencies of terrestrial whistlers are 1 kHz to 30 kHz, with a maximum amplitude usually at 3 kHz to 5 kHz. Although they are electromagnetic waves, they occur at audio frequencies, and can be converted to audio using a suitable receiver. They are produced by lightning strikes (mostly intracloud and return-path) where the impulse travels along the Earth's magnetic field lines from one hemisphere to the other. They undergo dispersion of several kHz due to the slower velocity of the lower frequencies through the plasma environments of the ionosphere and magnetosphere. Thus they are perceived as a descending tone which can last for a few seconds. The study of whistlers categorizes them into Pure Note, Diffuse, 2-Hop, and Echo Train types.\n\nVoyager 1 and 2 spacecraft detected whistler-like activity in the vicinity of Jupiter, implying the presence of lightning there.\n\nThe pulse of electromagnetic energy of a lightning discharge producing whistlers contains a wide range of frequencies below the electron cyclotron frequency. Due to interactions with free electrons in the ionosphere, the waves becomes highly dispersive and like guided waves, follow the lines of geomagnetic field. These lines provide the field with sufficient focusing influence and prevents the scattering of field energy.Their paths reach into the outer space as far as 3 to 4 times the Earth's radius in the plane of equator and bring energy from lightning discharge to the Earth at a point in the opposite hemisphere which is the magnetic conjugate of the position of radio emission for whistlers. From there, the whistler waves are reflected back to the hemisphere from which they started. The energy is almost perfectly reflected from earth surface 4 or 5 times with increase dispersion and diminishing amplitude. Along such long paths the speed of propagation of energy is between c/10 to c/100 and the exact value depends upon frequency.\n\nWhistlers were probably heard as early as 1886 on long telephone lines, but the clearest early description was by Barkhausen in 1919. In 1953, Storey showed that whistlers originate from lightning discharges.\n\nA type of electromagnetic signal propagating in the Earth–ionosphere waveguide, known as a radio atmospheric signal or sferic, may escape the ionosphere and propagate outward into the magnetosphere. The signal is prone to bounce-mode propagation, reflecting back and forth on opposite sides of the planet until totally attenuated. To clarify which part of this hop pattern the signal is in, it is specified by a number, indicating the portion of the bounce path it is currently on. On its first upward path, it is known as a codice_1. After passing the geomagnetic equator, it is referred to as a codice_2. The + or - sign indicates either upward or downward propagation, respectively. The numeral represents the half-bounce currently in progress. The reflected signal is redesignated codice_2, until passing the geomagnetic equator again; then it is called codice_4, and so on.\n\nWhistlers were first detected during World War 1. On the wide-band spectrogram, the observed characteristic of a whistler is that the tone rapidly descends over a few seconds. This is the origin of the name \"whistlers\".\n\nThe first spectrogram through which people heard the whistlers was taken from a 48 second long nightside plasmaspheric pass on March 26, 1996. These whistlers were seen at a frequency below 1.5 kHz.\n\n\n\n"}
