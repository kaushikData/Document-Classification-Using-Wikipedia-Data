{"id": "21138429", "url": "https://en.wikipedia.org/wiki?curid=21138429", "title": "Albeluvisols", "text": "Albeluvisols\n\nAlbeluvisol was a Reference Soil Group of the first edition (1998) and the second edition (2006) of the World Reference Base for Soil Resources (WRB). In the third edition of the WRB (2014), Albeluvisols were replaced by the broader defined Retisols. An Albeluvisol is a soil with a thin, dark surface horizon on a bleached subsurface horizon (an albic horizon) that tongues into a clay illuviation (Bt) horizon. The Bt horizon has an irregular or broken upper boundary resulting from the tonguing of bleached soil material into the illuviation horizon. Albeluvisols correlate with Glossaqualfs, Glossocryalfs and Glossudalfs in the USDA soil taxonomy.\n\nThese soils are formed mostly in unconsolidated glacial till, lacustrine or fluvial materials or aeolian deposits such as loess. They occur on flat to undulating plains under coniferous forest or mixed forest in boreal and temperate climates with cold winters and short cool summers.\n\nThe agricultural suitability of Albeluvisols is limited because of their acidity, low nutrient levels, tillage and drainage problems. In northern regions there is also a short growing season and severe frost during the long winter. The Albeluvisols of the northern taiga zone are almost exclusively under forest with small areas used for pasture or hay fields. In the southern taiga zone, less than 10 percent of the non-forested area is used for livestock farming. In the southern and western parts of the taiga in Russia arable crops, such as cereals, potatoes, sugar beet and forage maize, are found, especially on soils with higher base saturations in the subsoil.\n\nAlbeluvisols cover an estimated 320 million ha in Europe, North Asia, Central Asia and in North America. They are concentrated in two regions:\n\n"}
{"id": "42498195", "url": "https://en.wikipedia.org/wiki?curid=42498195", "title": "Army Energy Initiatives Task Force", "text": "Army Energy Initiatives Task Force\n\nThe Army Energy Initiatives Task Force (EITF) serves as the central management office for partnering with United States Army installations to implement cost-effective, large-scale renewable energy projects, leveraging private sector financing\" part of the Office of the Assistant Secretary of the Army for Installations, Energy and Environment. The Secretary of the Army John M. McHugh established the task force on Sept. 15, 2011.\n\nThe EITF supports the Army’s goal of deploying 1 gigawatt of renewable energy by 2025.\n\nOn 1 October 2014, the Secretary of the Army directed the establishment of the permanent U.S. Army Office of Energy Initiatives (OEI).\n\nOn April 14, 2014, the Army announced the development of a solar array at Fort Huachuca designed to provide about 25 percent of the annual electricity requirement. The project is a joint effort between the U.S. Army Energy Initiatives Task Force, Fort Huachuca, The General Services Administration, Tucson Electric Power and developer E.ON Climate and Renewables.\n\nOn February 19, 2014, ReEnergy Holdings LLC was issued a Notice of Intent (NOI) to Award by Defense Logistics Agency (DLA) for the purchase of up to 28 megawatts (MW) of electricity from a renewable energy biomass facility at Fort Drum, New York. The DLA said that the project is capable of supplying 100% of Fort Drum's electricity requirements. Fort Drum provides planning and support for the mobilization and training of almost 80,000 Army troops annually.\n\nThe EITF in 2013 made 79 contract award announcements for the Multiple Award Task Order Contract (MATOC) for Renewable and Alternative Energy Power Production at Department of Defense Installations. The MATOC is a contract vehicle that aims to establish a pool of qualified firms/contractors for biomass, solar power, geothermal and wind power technologies to compete for individual task order contracts.\n\n\n"}
{"id": "2930244", "url": "https://en.wikipedia.org/wiki?curid=2930244", "title": "Atlantic hurricane season", "text": "Atlantic hurricane season\n\nThe Atlantic hurricane season is the period in a year when hurricanes usually form in the Atlantic Ocean. Tropical cyclones in the North Atlantic are called hurricanes, tropical storms, or tropical depressions. In addition, there have been several storms over the years that have not been fully tropical and are categorized as subtropical depressions and subtropical storms. Even though subtropical storms and subtropical depressions are not technically as strong as tropical cyclones, the damages can still be devastating.\n\nWorldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest. However, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active. In the Northern Atlantic Ocean, a distinct hurricane season occurs from June 1 to November 30, sharply peaking from late August through September; the season's climatological peak of activity occurs around September 10 each season. This is the norm, but in 1938, the Atlantic hurricane season started as early as January 3.\n\nTropical disturbances that reach tropical storm intensity are named from a pre-determined list. On average, 10.1 named storms occur each season, with an average of 5.9 becoming hurricanes and 2.5 becoming major hurricanes (Category 3 or greater). The most active season was 2005, during which 28 tropical cyclones formed, of which a record 15 became hurricanes. The least active season was 1914, with only one known tropical cyclone developing during that year.\nThe Atlantic hurricane season is a time when most tropical cyclones are expected to develop across the northern Atlantic Ocean. It is currently defined as the time frame from June 1 through November 30, though in the past the season was defined as a shorter time frame. During the season, regular tropical weather outlooks are issued by the National Hurricane Center, and coordination between the Weather Prediction Center and National Hurricane Center occurs for systems which have not formed yet, but could develop during the next three to seven days.\n\nThe basic concept of a hurricane season began during 1935, when dedicated wire circuits known as hurricane circuits began to be set up along the Gulf and Atlantic coasts, a process completed by 1955. It was originally the time frame when the tropics were monitored routinely for tropical cyclone activity, and was originally defined as from June 15 through October 31. Over the years, the beginning date was shifted back to June 1, while the end date was shifted to November 15, before settling at November 30 by 1965. This was when hurricane reconnaissance planes were sent out to fly across the Atlantic and Gulf of Mexico on a routine basis to look for potential tropical cyclones, in the years before the continuous weather satellite era. Since regular satellite surveillance began, hurricane hunter aircraft fly only into storm areas which are first spotted by satellite imagery.\n\nDuring the hurricane season, the National Hurricane Center routinely issues their Tropical Weather Outlook product, which identifies areas of concern within the tropics which could develop into tropical cyclones. If systems occur outside the defined hurricane season, special Tropical Weather Outlooks will be issued. Routine coordination occurs at 1700 UTC each day between the Weather Prediction Center and National Hurricane Center to identify systems for the pressure maps three to seven days into the future within the tropics, and points for existing tropical cyclones six to seven days into the future. Possible tropical cyclones are depicted with a closed isobar, while systems with less certainty to develop are depicted as \"spot lows\" with no isobar surrounding them.\n\nThe North Atlantic hurricane database, or HURDAT, is the database for all tropical storms and hurricanes for the Atlantic Ocean, Gulf of Mexico and Caribbean Sea, including those that have made landfall in the United States. The original database of six-hourly positions and intensities were put together in the 1960s in support of the Apollo space program to help provide statistical track forecast guidance. In the intervening years, this database — which is now freely and easily accessible on the Internet from the National Hurricane Center's (NHC) webpage — has been utilized for a wide variety of uses: climatic change studies, seasonal forecasting, risk assessment for county emergency managers, analysis of potential losses for insurance and business interests, intensity forecasting techniques and verification of official and various model predictions of track and intensity.\n\nHURDAT was not designed with all of these uses in mind when it was first put together and not all of them may be appropriate given its original motivation. HURDAT contains numerous systematic as well as some random errors in the database. Additionally, analysis techniques have changed over the years at NHC as their understanding of tropical cyclones has developed, leading to biases in the historical database. Another difficulty in applying the hurricane database to studies concerned with landfalling events is the lack of exact location, time and intensity at hurricane landfall.\n\nHURDAT is regularly updated annually to reflect the previous season's activity. The older portion of the database has been regularly revised since 2001. The first time in 2001 led to the addition of tropical cyclone tracks for the years 1851 to 1885. The second time was August 2002 when Hurricane Andrew was upgraded to a Category 5. Recent efforts into uncovering undocumented historical hurricanes in the late 19th and 20th centuries by various researchers have greatly increased our knowledge of these past events. Possible changes for the years 1951 onward are not yet incorporated into the HURDAT database. Because of all of these issues, a re-analysis of the Atlantic hurricane database is being attempted that will be completed in three years.\n\nIn addition to the groundbreaking work by Partagas, additional analyses, digitization and quality control of the data was carried out by researchers at the NOAA Hurricane Research Division funded by the NOAA Office of Global Programs. This re-analysis will continue to progress through the remainder of the 20th century.\n\nThe National Hurricane Center's Best Track Change Committee has approved changes for a few recent cyclones, such as Hurricane Andrew. Official changes to the Atlantic hurricane database are approved by the National Hurricane Center Best Track Change Committee. Thus research conducted by Chris Landsea and colleagues as part of the Atlantic hurricane database reanalysis project are submitted through this review process. Not all Landsea's recommendations are accepted by the Committee.\n\nNOTE: In the following tables, all estimates of damage costs are expressed in contemporaneous US dollars (USD).\n\nNOTE: In the following tables, all estimates of damage costs are expressed in contemporaneous US dollars (USD).\n\nThis bar chart shows the number of named storms and hurricanes per year from 1851–2018.\n\nA 2011 study analyzing one of the main sources of hurricanes - the African easterly wave (AEW) - found that the change in AEWs is closely linked to increased activity of intense hurricanes in the North Atlantic. The synoptic concurrence of AEWs in driving the dynamics of the Sahel greening also appears to increase tropical cyclogenesis over the North Atlantic.\n"}
{"id": "17602633", "url": "https://en.wikipedia.org/wiki?curid=17602633", "title": "Carbon capture and storage in Australia", "text": "Carbon capture and storage in Australia\n\nCarbon capture and storage (CCS) is an approach to try to mitigate global warming by capturing carbon dioxide from large point sources such as fossil fuel power plants and storing it instead of releasing it into the atmosphere. Carbon capture and storage is also used to sequester filtered out of natural gas from certain from natural gas fields. While typically the has no value after being stored, Enhanced Oil Recovery uses to increase yield from declining oil fields.\n\nThere are no currently-operating large-scale CCS projects in Australia, although the Gorgon gas project will qualify when it is fully operational. Despite multiple CCS demonstration projects at Australian coal-fired power stations, none of Australia's coal plants are currently capturing or have a time frame for doing so. Australian Treasury modelling finds that CCS is not expected to be commercially viable until the 2030s. The Intergovernmental Panel on Climate Change (IPCC) estimates that the economic potential of CCS could be between 10% and 55% of the total carbon mitigation effort until 2100.\n\nIn the 2017 budget, the Turnbull Government announced the cessation of the Low Emissions Technology Demonstration Fund in the 2017 financial year and the cessation of business case funding for the Carbon Capture and Storage Flagships program in financial year 2019. This is on top of the 2015 budget, where the Abbott Government cut $460m from CCS research projects leaving $191.7m to continue existing projects for the next seven years. The program had already been cut by the previous Labor government and much of the funding remained unallocated.\n\n\n\nIn Australia, the major emissions sites are in the Latrobe and Hunter Valleys. The Latrobe Valley has considerable potential storage within a few hundred kilometres in Bass Strait which the CarbonNet Project was investigating (see below). There are no particularly promising large storage prospects near the Hunter Valley. Geologically prospective areas include the North West Shelf (see the Gorgon Project below) and Bass Strait. Australia has very extensive basins with deep saline formations, both onshore and particularly offshore, in which large quantities of carbon dioxide can dissolve. In such formations Australia has a potential carbon dioxide storage resource equivalent to many hundreds of years of emissions at the current rate. Work is now underway to fully assess storage potential.\n\nIn November 2008, the Australian Commonwealth Government passed the Offshore Petroleum Amendment (Greenhouse Gas Storage) Act 2008, which provides a regulatory framework for carbon dioxide storage in federal offshore waters.\n\nThe Victorian Greenhouse Gas Geological Sequestration Act 2008 (No. 61 of 2008) received Royal Assent on 5 November 2008. It provides a dedicated legal framework enabling the onshore injection and permanent storage of greenhouse gas substances. The state government has also developed a regulatory framework for offshore storage sites (i.e. those sites falling within the 3 nautical mile extent of state jurisdiction; the Offshore Petroleum and Greenhouse Gas Storage Act 2010.\n\nBoth Australia's state and federal governments have been major contributors to CCS research and development. Federal government CCS initiatives include the CO2CRC (founded 2003), the Low Emission Technology Demonstration Fund (2004-2017), funding for the Asia-Pacific Partnership on Clean Development and Climate (2006-2011, renewables, CCS and other), National Low Emissions Coal Initiative (founded 2008), Global CCS Institute (founded 2009), Carbon Capture and Storage Flagships (2009-2019), the Carbon Capture and Storage Research Development and Demonstration Fund (2015-2016) and the National CO2 Infrastructure Plan operated by Geoscience Australia (2012-2016).\n\nFederal funding commitments to these initiatives total $3.5-3.6 billion, of which $1.3-$1.6 billion has been committed or is expected to be committed.\n\nIn February 2017, Prime Minister Malcolm Turnbull said:\n\nIn April 2018, a parliamentary inquiry heard from energy researchers that carbon capture and storage requires a price on carbon to be viable.\n\nThere are no large-scale commercial CCS projects within Australia. The Global CCS Institute defines \"large-scale\" as 400,000 tonnes of per annum, or 800,000 tonnes per annum for a coal plant. It previously used a one million tonnes per annum threshold.\n\nDemonstration and proposed projects and projects under construction are listed below with brief descriptions.\n\nThe CO2CRC Otway Project in Western Victoria is a demonstration project which has injected and stored over 65,000 tonnes of carbon dioxide in a depleted natural gas reservoir 2 km below the Earth’s surface. The project was first proposed to the Board of the then Australian Petroleum Cooperative Research Centre (APCRC) in March 1998. There has been no sign of leakage according to a comprehensive monitoring and verification program. A mixture of Carbon dioxide and methane gas is extracted from a well in the Bathurst field, then compressed and transported via dedicated pipeline to the Naylor field two kilometres away. The gases are then injected into the depleted gas reservoir through a dedicated injection well. A nearby well (previously used to produce natural gas) is used to monitor the injected carbon dioxide. A second stage of the project, involving evaluation of carbon dioxide storage in deep saline formations, has been highly successful and provided data on estimating CO2 storage capacity using an innovative single well test. The project is Australia's first demonstration of geosequestration and one of the world's largest geosequestration research projects. This area has active exploration for geothermal and petroleum resources and has been supported by geotechnical work completed by the public sector and the private sector.\n\nThe Latrobe Valley Post Combustion Capture Project was a joint collaboration between Loy Yang Power, International Power Hazelwood, government and researchers from CSIRO’s Energy Transformed Flagship and CO2CRC (including Monash and Melbourne Universities), involving research at both Loy Yang and Hazelwood power stations. The 10.5-metre-high pilot plant at Loy Yang was designed to capture up to 1,000 tonnes of per annum from the power station's exhaust-gas flues. Future trials were expected to involve the use of a range of different -capture liquids. On 9 July 2008, CSIRO Energy Technology Chief Dr David Brockway announced that carbon dioxide () had been captured from power station flue gases in a post-combustion-capture (PCC) pilot plant at Loy Yang Power Station in Victoria’s Latrobe Valley. The purpose of the pilot plant is to conduct research, not to capture all the emissions from the power station.\n\nFurther government projects in this area led to many geo-technical studies that review gas and liquid migration, trapping and leakage. While the Gippsland area has been described as a basin margin, this is somewhat vague. The area defines a major fold belt onshore and offshore. The key risk to injection of in the area is the ability to keep gas in the ground. Multiple regional and local studies over the area have been completed by government and private companies.\n\nIn 2016, the PICA Post-Combustion Project was announced. This is a joint collaboration between CSIRO, IHI Corporation (Japanese technology provider) and AGL. It will use a pilot plant to test -capture liquids over a two-year period.\n\nCO2CRC commissioned three carbon dioxide capture research rigs at HRL’s gasifier research facility at Mulgrave in Melbourne, Victoria. The CO2CRC rigs captured carbon dioxide from syngas, the product of the brown coal gasifier, using solvent, membrane and adsorbent technologies. The capture technologies are equally applicable to syngas from brown and black coal, gas or biomass fuels. During the project, researchers evaluated each technology for efficiency and cost-effectiveness. Advanced gasifier technologies are highly suitable for carbon dioxide capture for CCS as they produce a concentrated stream of carbon dioxide.\n\nThis project led by Chevron will be designed to capture 3.5 Mt of carbon dioxide per annum from Greater Gorgon gas fields and store it in the Dupuy formation beneath the Barrow Island. The project will be the largest carbon dioxide sequestration operation in the world. Chevron is liable for leaks and other damage during the project's lifetime and for 15 years afterwards, but in 2009 the state and federal governments agreed to indemnify Chevron against liability for the project after that time, with the Commonwealth in 2015 confirming it would adopt 80% of the liability and WA the remaining 20%. \n\nWhen construction on the project began in 2009, it was expected to be completed by 2014 - including carbon capture and storage. The project ultimately started extracting gas in February 2017 but carbon capture and storage is now not expected to begin until March 2019, requiring a further five million tonnes of to be released, because: \n\nIn May 2018, the WA Environmental Protection Authority announced an investigation into whether Gorgon could meet its storage commitments given the delays.\n\nWWF claims that the Gorgon geosequestration project is potentially unsafe as the area has over 700 wells drilled in the area, 50 of which reach the area proposed for geosequestration of . Fault lines compound the problems. Barrow Island is also an A class nature reserve of global importance.\n\nIn April 2018, the federal and Victorian governments announced a brown coal-to-hydrogen project that would operate out of AGL Energy's Loy Yang A power station. Construction is expected to begin in 2019, and hydrogen production by 2020 or 2021.\n\nThe pilot program will not include carbon capture and storage, but it is expected if the project is expanded, with proponent Kawasaki Heavy Industries says that coal-to-hydrogen technology is not commercial without it.\n\nCarbonNet was established by the Victorian Government in 2009 to investigate the potential for establishing a world class, large-scale, multi-user carbon capture and storage network in Victoria. In 2012 the Australian Government selected CarbonNet as one of only two CCS flagship projects under its Clean Energy Initiative and, with the State of Victoria, awarded the project a further $100 million in joint funding to undertake feasibility. The Global CCS Institute provided $2.3 million in support.\n\nIn 2016 it was reported that \"When Australia repealed the carbon price the project did not advance\", but as of January 2018 the project was conducting a 17-day seismic survey of former oil wells in the Gippsland Basin.\n\nThe Callide Oxyfuel Project demonstrated carbon capture using oxyfuel combustion, but did not achieve carbon storage. The Oxyfuel boiler operated for two years and nine months, beyond the project's expected duration, and achieved \"partial capture\" of 75 tonnes of per day (27,300 tonnes per annum). The project team assessed eight potential carbon storage sites but were unsuitable because of location, availability and geological profile.\n\nThe project was headed by CS Energy Ltd (which operates the Callide power plants) in conjunction with an international team of partners, including IHI Corporation (Japan), J-Power (Japan), Mitsui & Company (Japan) Schlumberger Oilfields Australia and Xstrata Coal. The Australian Coal Association (through the industry fund Coal21), and the Commonwealth (through the Low Emission Technology Demonstration Fund, Queensland and Japanese governments provided financial support for the project, which was expected to cost A$208 million. It was a project for the Asia-Pacific Partnership on Clean Development and Climate.\n\nIn 2017, chief executive officer of project proponent CS Energy said of the Callide project:\n\nA post-combustion capture plant operated at the International Power GDF Suez Hazelwood Power Station Hazelwood. \n\nWhen announced in 2007, the project was originally planned as a retro-fit of one of Hazelwood's eight generating units, which would have reduced its emissions intensity by 20% (500,000 tonnes per annum). It was offered a $50 million federal government grant from the Low Emissions Technology Development Fund and a $30 million Victorian government grant from the Energy Technology Innovation Strategy.\n\nA pilot program with a more modest capture objective was completed. The solvent capture plant cost $10 million (including grants from state and federal governments) and began operation in 2009, capturing and chemically sequestered at a nominal rate of 10,000 tonnes per annum. \n\nHazelwood Power Station closed in March 2017.\n\nOn 2 December 2008 Shell and Anglo American announced that this possible brown coal project in the Latrobe Valley will not proceed at present. They have described it as a \"long term\" opportunity.\n\nThe planned project was planned to have some CCS, storing the gas captured in depleted off-shore oil fields in the Gippsland Basin in east Bass Strait.\n\nA proposed $2 billion \"hydrogen energy\" coal-to-gas plant will not proceed because the geological formations off Perth, which were intended to sequester the , contain gas \"chimneys\" that \"mean it is next to impossible to establish a seal in the strata that could contain the CO2\".\n\nThe Zerogen powerstation project near Stanwell power station in Queensland is proposed to be a 100 MW \"Integrated Gasification Combined Cycle\" power station with CCS. In late 2010, the Government of Queensland announced it would not fund the Zerogen project because it was not economically viable and that it would be sold off.\n\nThe Fairview Project, near Roma in South West Queensland, was intended to capture 1/3 of the emissions from a 100 MW coal seam methane gas-fired power station. In 2006 it was selected to receive federal government funding, but as of September 2017 it does not appear on the Global CCS Institute's list of projects.\n\n"}
{"id": "996955", "url": "https://en.wikipedia.org/wiki?curid=996955", "title": "Characteristic energy", "text": "Characteristic energy\n\nIn astrodynamics, the characteristic energy (formula_1) is a measure of the excess specific energy over that required to just barely escape from a massive body. The units are length time, i.e. velocity squared or twice the energy per mass.\n\nEvery object in a 2-body ballistic trajectory has a constant specific orbital energy formula_2 equal to the sum of its specific kinetic and specific potential energy:\n\nwhere formula_4 is the standard gravitational parameter of the massive body with mass formula_5, and formula_6 is the radial distance from its center. As an object in an escape trajectory moves outward, its kinetic energy decreases as its potential energy (which is always negative) increases, maintaining a constant sum.\n\nNote that \"C\" is \"twice\" the specific orbital energy formula_2 of the escaping object.\n\nA spacecraft with insufficient energy to escape will remain in a closed orbit (unless it intersects the central body), with\nwhere\n\nIf the orbit is circular, of radius \"r\", then\n\nA spacecraft leaving the central body on a parabolic trajectory has exactly the energy needed to escape and no more:\n\nA spacecraft that is leaving the central body on a hyperbolic trajectory has more than enough energy to escape:\nwhere\n\nAlso,\nwhere formula_17 is the asymptotic velocity at infinite distance. Spacecraft's velocity approaches formula_17 as it is further away from the central object's gravity.\n\nMAVEN, a Mars-bound spacecraft, was launched into a trajectory with a characteristic energy of 12.2 km/s with respect to the Earth. When simplified to a two-body problem, this would mean the MAVEN escaped Earth on a hyperbolic trajectory slowly decreasing its speed towards formula_19. However, since the Sun's gravitational field is much stronger than Earth's, the two-body solution is insufficient. The characteristic energy with respect to Sun was negative, and MAVEN – instead of heading to infinity – entered an elliptical orbit around the Sun. But the maximal velocity on the new orbit could be approximated to 33.5 km/s by assuming that it reached practical \"infinity\" at 3.5 km/s and that such Earth-bound \"infinity\" also moves with Earth's orbital velocity of about 30 km/s.\n\nThe InSight mission to Mars launched with a maximum C of 14.3 km/s. The Parker Solar Probe (via Venus) plans a maximum C of 154 km/s.\n\nC3 (km/s) from earth to get to various planets : Mars 12, Jupiter 80, Saturn or Uranus 147. To Pluto (with its orbital inclination) needs about 160-164 km/s.\n\n"}
{"id": "593281", "url": "https://en.wikipedia.org/wiki?curid=593281", "title": "Charles Pickering (naturalist)", "text": "Charles Pickering (naturalist)\n\nCharles Pickering (November 10, 1805 – March 17, 1878) was an American race scientist and naturalist.\n\nBorn on Starucca Creek, Upper Susquehanna, Pennsylvania, the grandson of Colonel Timothy Pickering, after the death of his father he was raised in the house of his esteemed grandfather in Wenham, Massachusetts. Despite his part in the student rebellion of 1823, he received a medical degree from Harvard University in 1826. A practicing physician in Philadelphia, he became active as librarian and curator at the city's Academy of Natural Sciences.\n\nPickering went with the United States Exploring Expedition of 1838-1842 as one of its naturalists. Charles Wilkes, the expedition's commander, named Pickering Passage in honor of Charles Pickering. His journal of the Expedition was a major influence on Wilkes' \"Narrative of the United States Exploring Expedition\".\n\nFrom 1842-43, Pickering curated the collection from the Wilkes Expedition, which was housed at the Patent Office in Washington DC, these collections were to form the foundation of the Smithsonian Institute.\n\nPickering was a polygenist, he believed that different races had been created separately. In 1843, he traveled to the Middle East, Zanzibar and India to continue his research for his book on the Races of Man. In 1848, Pickering published \"Races of Man and Their Geographical Distribution\", which enumerated eleven races.\n\nHe later moved to Boston, where he resumed his medical practice, and eventually died on March 17, 1878.\n\nHe was married to Sarah Stoddard Pickering, who posthumously published his final work. He was an associate of many important figures in America's intellectual landscape, including Asa Gray, Horatio Hale, James Dwight Dana and Louis Agassiz. \n\nA subspecies of North American garter snake, \"Thamnophis sirtalis pickeringii\", is named in his honor.\n\n\n\n"}
{"id": "37019509", "url": "https://en.wikipedia.org/wiki?curid=37019509", "title": "Delta Chamaeleontis", "text": "Delta Chamaeleontis\n\nThe Bayer designation Delta Chamaeleontis (δ Cha / δ Chamaeleontis) is shared by two star systems, in the constellation Chamaeleon:\n"}
{"id": "49037", "url": "https://en.wikipedia.org/wiki?curid=49037", "title": "Ecological selection", "text": "Ecological selection\n\nEcological selection (or environmental selection or survival selection or individual selection or asexual selection) refers to natural selection without sexual selection, i.e. strictly ecological processes that operate on a species' inherited traits without reference to mating or secondary sex characteristics. The variant names describe varying circumstances where sexual selection is wholly suppressed as a mating factor.\n\nEcologists often study ecological selection when examining the abundance of individuals per population across regions, and what governs such abundances.\n\nEcological selection can be said to be taking place in any circumstance where inheritance of specific traits is determined by ecology alone without direct sexual competition, when e.g. sexual competition is strictly ecological or economic, there is little or no mate choice, females do not resist any male who wishes to mate, all traits will be equally propagated regardless of mating, or the species is hermaphroditic or asexually reproducing, an ecological selection is taking place. For example, environmental pressures are largely responsible for the evolution of different life history strategies between the African honey bee, \"A. m. scutellata\", and the European honey bee.\n\nIn sexually reproducing species, it is applicable mostly to situations where ecological pressures prevent most competitors from reaching maturity, or where crowding or pair-bonding or an extreme suppression of sexual selection factors prevents the normal sexual competition rituals and selection from taking place, but which also prevent artificial selection from operating, e.g. arranged marriages, where parents rather than the young select the mate based on economic or even astrological factors, and where the sexual desires of the mated pair are often subordinated to these factors, are artificial unless \"wholly\" based on an ecological factor such as control of land which is held by their own force.\n\nIn forests, ecological selection can be witnessed involving many factors such as available sunlight, soil quality, and the surrounding biota. During forest growth, tree seedlings in particular, are ecosystem pioneers, and different tree seedlings can often react to a number of members in their ecological community in completely different ways, thus providing a spectrum of ecological occupations. On the other hand, adult trees can heavily impact their ecological communities, reversing the roles of ecological selection. Elements of the soil are an extremely influential selective factor in forest growth. Throughout time, every species of tree has evolved to grow under specific soil conditions, whether it is dependent on the pH levels, the mineral contents, or the drainage levels. Each of these is a vehicle for ecological selection to do its work in the course of evolution. However, ecological selection can be much more specific, not only working within species but within populations, even populations in the same region. For example, scientists in Quebec recently examined how tree seedlings react to different nitrate levels. What they found was that areas with higher nitrate levels contained plants that could much more efficiently metabolize nitrogen. Such plants could perform photosynthesis and respiration at a much faster rate than their nitrogen lacking peers, and also had longer root lengths on average, giving them an evolutionary advantage for their habitat. Nitrogen levels that are unexpectedly too high could harm some tree species, but these particular specimens created a niche for themselves, and could outcompete others around them. A site of tree growth can also be influenced by slope, rockiness, climate, and available sunlight. Space is initially available to everything, but seedlings that can most quickly inhabit the soil and take advantage of the available nutrients are usually most successful. Generally, one of the first factors to control which species grow best in the soil is the amount of sunlight. Soil and water themselves are both very important (For instance, a dry hardwood such as a white oak will not grow in a swamp), but sunlight is the initial decider in forest succession. Shade intolerant trees can immediately grow impressively. They need the sunlight that is offered by an open canopy found in a bare environment. Selection weeds out the seedlings that can not handle full sun, thus tall, straight trees will eventually grow and develop a full, lush canopy. However, these behaviors will soon be reversed. Seedlings that were once removed by ecological selection now become favored, because the shaded forest floor has become ideal for such shade tolerant species. This is a great example of how ecological selection can create niches for different species by performing the same function with different outcomes.\n\nIn cases where ecological and sexual selection factors are strongly at odds, simultaneously encouraging and discouraging the same traits, it may also be important to distinguish them as sub-processes within natural selection.\n\nFor instance, \"Ceratogaulus\", the Oligocene horned gopher, left in the fossil record a series of individuals with successively longer and longer horns, that seemed to be unrelated or maladaptive to its ecological niche. Some modern scientists have theorized that the horns were useful or impressive in mating rituals among males (although other scientists dispute this theory, pointing out that the horns were not sexually dimorphic) and that it was an example of runaway evolution. The species seems to have suddenly died out when horns reached approximately the body length of the animal itself, possibly because it could no longer run or evade predators—thus ecological selection seems to have ultimately trumped sexual. \n\nIt is also important to distinguish ecological selection in cases of extreme ecological abundance, e.g. the human built environment, cities or zoos, where sexual selection must generally predominate, as there is no threat of the species or individuals losing their ecological niche. Even in these situations, however, where survival is not in question, the variety and the quality of food, e.g. as presented by male to female monkeys in exchange for sex in some species, still influences reproduction, however it becomes a sexual selection factor. Similar phenomena can be said to exist in humans e.g. the \"mail order bride\" who primarily mates for economic advantage.\n\nDifferentiating ecological selection from sexual is useful especially in such extreme cases; Above examples demonstrate exceptions rather than a typical selection in the wild. In general, ecological selection is assumed to be the dominant process in natural selection, except in highly cognitive species that do not, or do not always, pair bond, e.g. walrus, gorilla, human. But even in these species, one would distinguish cases where isolated populations had no real choice of mates, or where the vast majority of individuals died before sexual maturity, leaving only the ecologically selected survivor to mate—regardless of its sexual fitness under normal sexual selection processes for that species.\n\nFor example, if only a few closely related males survive a natural disaster, and all are able to mate very widely due to lack of males, sexual selection has been suppressed by an ecological selection (the disaster). Such situations are usually temporary, characteristic of populations under extreme stress, for relatively short terms. However, they can drastically affect populations in that short time, sometimes eliminating all individuals susceptible to a pathogen, and thereby rendering all survivors immune. A few such catastrophic events where ecological selection predominates can lead to a population with specific advantages, e.g. in colonization when invading populations from more crowded disease-prone conditions arrive with antibodies to diseases, and the diseases themselves, which proceed to wipe out natives, clearing the way for the colonists.\n\nIn humans, the intervention of artificial devices such as ships or blankets may be enough to make some consider this an example of artificial selection. However it is clearly observed in other species, it seems unreasonable to differentiate colonization by ship from colonization by walking, and even the word \"colony\" is not specific to humans but refers generically to an intrusion of one species on an ecology to which it has not wholly adapted. So, despite the potential controversy, it may be better to consider all examples of colonist-borne diseases to be ecological selection.\n\nFor another example, in a region devastated by nuclear radiation, such as the Bikini Atoll, capacity to survive gamma rays to sexual maturity and (for the female) to term is a key ecological selection factor, although it is neither \"natural\" nor sexual. Some would call this too artificial selection, not natural or ecological, as the radiation does not enter the ecology as a factor save due to man's effort. Ambiguous artificial-plus-ecological factors may reasonably be called \"environmental\", and the term environmental selection may be preferable in these cases.\n\n"}
{"id": "44764768", "url": "https://en.wikipedia.org/wiki?curid=44764768", "title": "El Cielo Biosphere", "text": "El Cielo Biosphere\n\nThe El Cielo Biosphere (\"Reserva de la Biosfera El Cielo\" in Spanish) is located in the southern part of the Mexican state of Tamaulipas. The biosphere protects the northernmost extension of tropical forest and cloud forest in Mexico. It has an area of (558 square miles) made up mostly of steep mountains rising from about to a maximum altitude of more than .\n\nThe state of Tamaulipas created the biosphere in 1985. In 1987 it was recognized by UNESCO's Man and the Biosphere Programme.\n\nThe El Cielo area attracted little attention until the 1930s. In 1935, A Canadian farmer and horticulturalist named John William Francis (Frank, Francisco, or Pancho) Harrison established a homestead he named Rancho El Cielo at elevation in the cloud forest. Extensive logging and logging roads penetrated the area in the 1950s. Enthusiastic birdwatchers found their way to Harrison's small ranch and in 1965, to protect the ecosystem, Harrison transferred his land to a non-profit corporation in cooperation with Texas Southmost College and the Gorgas Science Foundation. In 1966, Harrison was murdered in a land dispute with local farmers.\n\nHarrison's farm is now the El Cielo Biological Research Center. In 1983, the Gorgas Science Foundation established Rancho El Cielito by purchasing land along the Sabinas River, just outside the Biosphere, to preserve part of a riparian ecosystem.\n\nThe biosphere has two core areas in which most human travel and exploitation are prohibited. One of protects tropical forests; the larger core area of includes a cross section of the altitudes and climates of the biosphere, especially the cloud forest. The remainder of the biosphere is a buffer zone in which human activities, including limited logging, is permitted. Several communities within the biosphere offer facilities for visitors and are reachable by road. An ecological interpretive center is reached by paved road a few miles west of the town of Gomez Farias. The interpretive center, located at an elevation of offers good views of the tropical forest and facilities for visitors.\n\nThe biosphere occupies portions of four Mexican municipalities in the state of Tamaulipas: Jaumave, Llera de Canales. Gómez Farías, and Ocampo. Within the biosphere are 26 ejidos (hamlets with communal land) and about of agricultural land used mostly to cultivate corn, beans, and rice. The principal access is a road, initially paved, from the town of Gomez Farias into the interior and higher elevations of the biosphere. The community of Alta Cima (also known as Altas Cimas), at an elevation of has modest lodging and restaurants for visitors. Camping is allowed.\n\nThe highest point in the biosphere is located at 23 14N, 99 30W. The lowest elevations are about at the eastern, northern, and southern borders of the biosphere. The biosphere is characterized by steep, north-south trending mountain ranges, eastern extensions of the Sierra Madre Oriental, made up of limestone. Typical of karst topography, caves, sinkholes, and rock outcrops are common.\n\nSeveral distinct vegetation types are found in the biosphere. Vegetation in the drier northern and western portions of the biosphere up to an elevation of consists of desert and semi-desert shrublands, the montane Tamaulipan matorral and the lowland Tamaulipan mezquital. Shrubs and small trees generally do not exceed in height except in riparian locations. Annual precipitation in the shrublands is less than .\n\nIn the eastern part of the biosphere, sub-tropical semi-deciduous forests (Veracruz moist forests) are found at elevations of from and above sea level. The closed canopy forests averages about in height. Annual precipitation of this zone is usually from to more than .\n\nThe principal reason for the establishment of the Biosphere was the prevalence of cloud forests, distinguished by heavy precipitation, foggy conditions, and abundant mosses and fungi, at elevations of to . The El Cielo cloud forests receive precipitation of about annually. The closed canopy forests reach a height of about .\n\nOak forests, (Sierra Madre Oriental pine-oak forests), mixed oak-pine forest, and pine forests are found at elevations of to the top of highest summits in the biosphere. These forested highland areas are drier than the cloud forests with an average precipitation of annually.\n\nAll of the vegetation types have a wet season from May to October and a dry season from November to April. More than 1,000 species of vegetation have been recorded from the cloud forests consisting of 56 percent tropical species, 20 percent temperate, 19 percent cosmopolitan, and 5 percent other. Included among the vegetation are species associated with the temperate climate of the eastern United States such as maple (\"Acer skutchii\"), hickory (\"Carya ovata\"), hornbeam (\"Carpinus caroliniana\"), and redbud (\"Cercis canadensis\").\n\nA botanical garden and arboretum is located in Alta Cima at an elevation of .\n\nSix species of cats, none abundant, are found in the biosphere: jaguar, mountain lion, ocelot, margay, jaguarundi, and bobcat. A small population of black bears is present.\n\nAt least 255 species of birds are resident in the biosphere preserve and more than 175 migratory species have been recorded. Both birds and mammals are a mixture of temperate and tropical species.\n\nThe climate of Gomez Farias at the eastern entrance to the park is typical of the lower and wetter elevations of the biosphere. Higher elevations are substantially cooler and precipitation declines rapidly on the western slopes of the mountains. The town of Jaumave, Tamaulipas at the northwestern entrance to the biosphere receives only of precipitation annually and has a semi-arid, near-desert climate. Freezing temperatures are rare at the lower elevations of El Cielo, but common in winter at elevations of more than \n\n</div>\n"}
{"id": "17253697", "url": "https://en.wikipedia.org/wiki?curid=17253697", "title": "Federal Fuel Administration", "text": "Federal Fuel Administration\n\nThe Federal Fuel Administration was a World War I-era agency of the Federal government of the United States established by of August 23, 1917 pursuant to the Food and Fuel Control Act, managed use of coal and oil. To conserve energy, it introduced daylight saving time, shortened work weeks for civilian goods factories, and encouraged Heatless Mondays.\n\nEven prior to a declaration of war by the United States, shortages of coal were experienced in the winter of 1916-17. To address concerns about a steady supply of fuel to support military and industrial operations and for use by consumers, in 1917 the Federal Fuel Administration was established and US President Woodrow Wilson appointed Harry A. Garfield to lead the agency. Garfield in turn selected local administrators for each state. Fuel committees were organized down to the county level.\n\nThe activities of the administration included setting and enforcing the prices of coal. The administration had broad powers to set the price of coal at various points (mine, dock) and the cost of transportation (by rail), and in regard to end use (home, factory, or business, etc.).\n\nDaylight Saving Time was formally adopted in the United States in 1918 by the Fuel Administration. The Standard Time Act of 1918 established both standard time zones and set summer DST to begin on March 31, 1918. The idea was unpopular, however, and Congress abolished DST after the war, overriding President Woodrow Wilson's veto. DST became a local option and was observed in some states until World War II, when President Franklin Roosevelt instituted year-round DST, called \"War Time,\" on February 9, 1942. It lasted until the last Sunday in September 1945. The next year, many states and localities adopted summer DST.\n\nBy mid-1922, the administration's activities were declining and some states were taking a more active role in managing coal production.\n\n"}
{"id": "4853761", "url": "https://en.wikipedia.org/wiki?curid=4853761", "title": "Helianthos", "text": "Helianthos\n\nOn September 8, 2011 Nuon announced the pilot plant would be closed down since no investor for production expansion could be found. However, on May 7, 2012 Nuon announced that Helianthos has been sold to HyET Solar.\n\nThe process uses a temporary substrate on which flexible thin-film solar cells are deposited. The use of the temporary substrate allows relatively high processing temperatures while using (semi-)continuous roll-to-roll (or reel-to-reel) production processes and cost-efficient, abundantly available materials.\n\nThe active layers of such solar cells comprise a transparent conductive oxide layer (TCO), an active absorbent layer (e.g. thin-film silicon), and a back contact layer (e.g. a reflective metal layer).\n\nThe key steps in the process sequence are:\n\nUsing this process, flexible photovoltaic (PV) laminates are fabricated that can substantially reduce the per-kilowatt hour costs of solar electricity. Further, the resulting photovoltaic laminates are lightweight, rugged and offer certain freedoms of design.\n\nPV laminates have the potential to be used for a range of applications, including:\n\n\n\n"}
{"id": "24051690", "url": "https://en.wikipedia.org/wiki?curid=24051690", "title": "History of sustainability", "text": "History of sustainability\n\nThe history of sustainability traces human-dominated ecological systems from the earliest civilizations to the present. This history is characterized by the increased regional success of a particular society, followed by crises that were either resolved, producing sustainability, or not, leading to decline.\n\nIn early human history, the use of fire and desire for specific foods may have altered the natural composition of plant and animal communities. Between 8,000 and 10,000 years ago, agrarian communities emerged which depended largely on their environment and the creation of a \"structure of permanence\".\n\nThe Western industrial revolution of the 18th to 19th centuries tapped into the vast growth potential of the energy in fossil fuels. Coal was used to power ever more efficient engines and later to generate electricity. Modern sanitation systems and advances in medicine protected large populations from disease. In the mid-20th century, a gathering environmental movement pointed out that there were environmental costs associated with the many material benefits that were now being enjoyed. In the late 20th century, environmental problems became global in scale. The 1973 and 1979 energy crises demonstrated the extent to which the global community had become dependent on non-renewable energy resources. \nIn the 21st century, there is increasing global awareness of the threat posed by the human-induced enhanced greenhouse effect, produced largely by forest clearing and the burning of fossil fuels.\n\nIn early human history, although the energy and other resource demands of nomadic hunter-gatherers was small, the use of fire and desire for specific foods may have altered the natural composition of plant and animal communities. Between 8,000 and 10,000 years ago, agriculture emerged in various regions of the world. Agrarian communities depended largely on their environment and the creation of a \"structure of permanence\". Societies outgrowing their local food supply or depleting critical resources either moved on or faced collapse.\nArcheological evidence suggests that the first civilizations arose in Sumer, in southern Mesopotamia (now Iraq) and Egypt, both dating from around 3000 BCE. By 1000 BCE, civilizations were also established in India, China, Mexico, Peru and in parts of Europe. Sumer illustrates issues central to the sustainability of human civilization. Sumerian cities practised intensive, year-round agriculture from  BCE. The surplus of storable food created by this economy allowed the population to settle in one place instead of migrating in search of wild foods and grazing land. It also allowed for a much greater population density. The development of agriculture in Mesopotamia required many labourers to build and maintain its irrigation system. This, in turn, led to political hierarchy, bureaucracy, and religious sanction, along with standing armies to protect the emergent civilization. Intensified agriculture allowed for population increase, but also led to deforestation in upstream areas with resultant flooding and over-irrigation, which raised soil salinity. While there was a shift from the cultivation of wheat to the more salt-tolerant barley, yields still diminished. Eventually, decreasing agricultural production and other factors led to the decline of the civilization. From 2100 BC to 1700 BC, it is estimated that the population was reduced by nearly sixty percent. Civilizations similarly thought to have eventually fallen because of poor management of resources include the Mayans, Anasazi and Easter Islanders, among many others. In contrast, stable communities of shifting cultivators and horticulturists existed in New Guinea and South America, and large agrarian communities in China, India and elsewhere have farmed in the same localities for centuries. Some Polynesian cultures have maintained stable communities for between 1,000 and 3,000 years on small islands with minimal resources using rahui and kaitiakitanga to control human pressure on the environment. In Sri Lanka nature reserves established during the reign of king Devanampiyatissa and dating back to 307 BC were devoted to sustainability and harmonious living with nature.\n\nTechnological advances over several millennia gave humans increasing control over the environment. But it was the Western industrial revolution of the 18th to 19th centuries that tapped into the vast growth potential of the energy in fossil fuels. Coal was used to power ever more efficient engines and later to generate electricity. Modern sanitation systems and advances in medicine protected large populations from disease. Such conditions led to a human population explosion and unprecedented industrial, technological and scientific growth that has continued to this day, marking the commencement of a period of global human influence known as the Anthropocene. From 1650 to 1850 the global population doubled from around 500 million to 1 billion people.\n\nConcerns about the environmental and social impacts of industry were expressed by some Enlightenment political economists and through the Romantic movement of the 1800s. The Reverend Thomas Malthus, devised catastrophic and much-criticized theories of \"overpopulation\", while John Stuart Mill foresaw the desirability of a \"stationary state\" economy, thus anticipating concerns of the modern discipline of ecological economics. In the late 19th century Eugenius Warming was the first botanist to study physiological relations between plants and their environment, heralding the scientific discipline of ecology.\n\nBy the 20th century, the industrial revolution had led to an exponential increase in the human consumption of resources. The increase in health, wealth and population was perceived as a simple path of progress. However, in the 1930s economists began developing models of non-renewable resource management (see Hotelling's rule) and the sustainability of welfare in an economy that uses non-renewable resources (Hartwick's rule).\n\nEcology had now gained general acceptance as a scientific discipline, and many concepts vital to sustainability were being explored. These included: the interconnectedness of all living systems in a single living planetary system, the biosphere; the importance of natural cycles (of water, nutrients and other chemicals, materials, waste); and the passage of energy through trophic levels of living systems.\n\nFollowing the deprivations of the great depression and World War II the developed world entered a new period of escalating growth, a post-1950s \"great acceleration ... a surge in the human enterprise that has emphatically stamped humanity as a global geophysical force.\" A gathering environmental movement pointed out that there were environmental costs associated with the many material benefits that were now being enjoyed. Innovations in technology (including plastics, synthetic chemicals, nuclear energy) and the increasing use of fossil fuels, were transforming society. Modern industrial agriculture—the \"Green Revolution\"—was based on the development of synthetic fertilizers, herbicides and pesticides which had devastating consequences for rural wildlife, as documented by American marine biologist, naturalist and environmentalist Rachel Carson in \"Silent Spring\" (1962).\n\nIn 1956, American geoscientist M. King Hubbert's peak oil theory predicted an inevitable peak of oil production, first in the United States (between 1965 and 1970), then in successive regions of the world—with a global peak expected thereafter. In the 1970s environmentalism's concern with pollution, the population explosion, consumerism and the depletion of finite resources found expression in \"Small Is Beautiful,\" by British economist E. F. Schumacher in 1973, and \"The Limits to Growth\" published by the global think tank, the Club of Rome, in 1975.\n\nEnvironmental problems were now becoming global in scale. The 1973 and 1979 energy crises demonstrated the extent to which the global community had become dependent on a nonrenewable resource; President Carter in his State of the Union Address called on Americans to \"Conserve energy. Eliminate waste. Make 1980 indeed a year of energy conservation.\" While the developed world was considering the problems of unchecked development the developing countries, faced with continued poverty and deprivation, regarded development as essential to raise the living standards of their peoples. In 1980 the International Union for Conservation of Nature had published its influential \"World Conservation Strategy,\" followed in 1982 by its \"World Charter for Nature,\" which drew attention to the decline of the world’s ecosystems.\n\nIn 1987 the United Nation's World Commission on Environment and Development (the Brundtland Commission), in its report \"Our Common Future\" suggested that development was acceptable, but it must be sustainable development that would meet the needs of the poor while not increasing environmental problems. Humanity’s demand on the planet has more than doubled over the past 45 years as a result of population growth and increasing individual consumption. In 1961 almost all countries in the world had more than enough capacity to meet their own demand; by 2005 the situation had changed radically with many countries able to meet their needs only by importing resources from other nations. A move toward sustainable living by increasing public awareness and adoption of recycling, and renewable energies emerged. The development of renewable sources of energy in the 1970s and '80s, primarily in wind turbines and photovoltaics and increased use of hydroelectricity, presented some of the first sustainable alternatives to fossil fuel and nuclear energy generation, the first large-scale solar and wind power plants appearing during the 1980s and '90s. Also at this time many local and state governments in developed countries began to implement small-scale sustainability policies.\n\nThrough the work of climate scientists in the IPCC there is increasing global awareness of the threat posed by the human-induced enhanced greenhouse effect, produced largely by forest clearing and the burning of fossil fuels. In March 2009 the Copenhagen Climate Council, an international team of leading climate scientists, issued a strongly worded statement:\n\"The climate system is already moving beyond the patterns of natural variability within which our society and economy have developed and thrived. These parameters include global mean surface temperature, sea-level rise, ocean and ice sheet dynamics, ocean acidification, and extreme climatic events. There is a significant risk that many of the trends will accelerate, leading to an increasing risk of abrupt or irreversible climatic shifts.\"\n\nEcological economics now seeks to bridge the gap between ecology and traditional neoclassical economics: it provides an inclusive and ethical economic model for society. A plethora of new concepts to help implement and measure sustainability are becoming more widely accepted including the car-free movement, smart growth (more sustainable urban environments), life cycle assessment (the cradle to cradle analysis of resource use and environmental impact over the life cycle of a product or process), ecological footprint analysis, green building, dematerialization (increased recycling of materials), decarbonisation (removing dependence on fossil fuels) and much more.\n\nThe work of Bina Agarwal and Vandana Shiva amongst many others, has brought some of the cultural wisdom of traditional, sustainable agrarian societies into the academic discourse on sustainability, and also blended that with modern scientific principles. In 2009 the Environmental Protection Agency of the United States determined that greenhouse gases \"endanger public health and welfare\" of the American people by contributing to climate change and causing more heat waves, droughts and flooding, and threatening food and water supplies. Rapidly advancing technologies now provide the means to achieve a transition of economies, energy generation, water and waste management, and food production towards sustainable practices using methods of systems ecology and industrial ecology.\n\n\n"}
{"id": "877939", "url": "https://en.wikipedia.org/wiki?curid=877939", "title": "Hyades (mythology)", "text": "Hyades (mythology)\n\nIn Greek mythology, the Hyades (; , popularly \"rain-makers\" or \"the rainy ones\" from ὕω \"hyo\" \"I fall as rain\", but probably from ὗς \"hys\" \"swine\") are a sisterhood of nymphs that bring rain.\n\nThe Hyades were daughters of Atlas (by either Pleione or Aethra, one of the Oceanides) and sisters of Hyas in most tellings, although one version gives their parents as Hyas and Boeotia. The Hyades are sisters to the Pleiades and the Hesperides.\n\nThe main myth concerning them is envisioned to account for their collective name and to provide an etiology for their weepy raininess: Hyas was killed in a hunting accident and the Hyades wept from their grief. They were changed into a cluster of stars, the Hyades, set in the head of Taurus.\n\nTheir number varies from three in the earliest sources to fifteen in the late ones. The names are also variable, according to the mythographer, and include:\nAdditionally, Thyone and Prodice were supposed to be daughters of Hyas by Aethra, and have been added to the group of stars.\n\nThe Greeks believed that the heliacal rising and setting of the Hyades star cluster were always attended with rain, hence the association of the Hyades (sisters of Hyas) and the Hyades (daughters of ocean) with the constellation of the Hyades (rainy ones).\n\nThe Hyades are also thought to have been the tutors of Dionysus, in some tellings of the latter's infancy, and as such are equated with the Nysiads, the nymphs who are also believed to have cared for Dionysus, as well as with other reputed nurses of the god — the Lamides, the Dodonides and the nymphs of Naxos. Some sources relate that they were subject to aging, but Dionysus, to express his gratitude for having raised him, asked Medea to restore their youth.\n\nIn Tennyson's poem, Ulysses recalls his travels of old: \n\n\"I cannot rest from travel: I will drink -\nLife to the lees: All times I have enjoy'd -\nGreatly, have suffer'd greatly, both with those - \nThat loved me, and alone, on shore, and when -\nThro' scudding drifts the rainy Hyades -\nVext the dim sea ...\"\n\n\n"}
{"id": "12830079", "url": "https://en.wikipedia.org/wiki?curid=12830079", "title": "InHour", "text": "InHour\n\nInHour is a unit of reactivity of a nuclear reactor. It stands for the inverse of an hour. It is equal to the inverse of the period in hours. One InHour is the amount of reactivity needed to increase the reaction from critical to where the power will increase by a factor of e in one hour.\n\nThe unit is abbreviated ih or inhr, and is usually measured with a reactimeter.\n\n"}
{"id": "434840", "url": "https://en.wikipedia.org/wiki?curid=434840", "title": "Ironwood", "text": "Ironwood\n\nIronwood is a common name for a large number of woods or plants that have a reputation for hardness of the/their wood, or the wood density is over 1000 kg/m and the wood sinks in water. Usage of the name ironwood in english may (or may not) include the tree that yields this heavy wood that sinks in water.\n\nThe equal german therm „Eisenholz” is given \"only\" to woods of plants with their wood weight of nearly 1000 kg/m or more,\nsee \n\nor may refer to\n\n"}
{"id": "15036150", "url": "https://en.wikipedia.org/wiki?curid=15036150", "title": "John Gore (Royal Navy officer, died 1790)", "text": "John Gore (Royal Navy officer, died 1790)\n\nCaptain John Gore (c. 173010 August 1790) was a British American sailor who circumnavigated the globe four times with the Royal Navy in the 18th century and accompanied Captain James Cook in his discoveries in the Pacific Ocean.\n\nAlthough little is known about John Gore before his service with the Royal Navy, it is believed he was born in the British Colony of Virginia in either 1729 or 1730. He first appears in the record books in 1755, joining HMS \"Windsor\" at Portsmouth as a midshipman.\n\nFive years later Gore took his lieutenant's exam and was appointed master's mate of HMS \"Dolphin\". Aboard the \"Dolphin\" Gore circumnavigated the globe twice—first under John Byron and then Samuel Wallis. His experience in the Pacific Ocean and on extended navy expeditions led to him being called up to join James Cook's mission to record the Transit of Venus in Tahiti and search for \"Terra Australis\" in 1768 aboard HMS \"Endeavour\". On \"Endeavour\", Gore was initially third-in-command (i.e. 3rd Lieutenant) behind Cook (1st Lieutenant) and Zachary Hicks\nGore had previously been part of the Royal Navy crew aboard Wallis's \"Dolphin\" that had \"discovered\" Tahiti and he became valuable to Cook for his knowledge of the island. In 1769 Gore became the first recorded person on the expedition to shoot and kill a person of Māori descent, following an altercation over a piece of cloth as the \"Endeavour\" charted the coast of New Zealand. Later, on 14 July 1770 Gore became famous for being the first person to shoot and kill a kangaroo (for scientific research) as the expedition made its way up the eastern seaboard of Australia.\n\nReturning to England, in 1772 Gore joined the botanist Joseph Banks (who had also been on Cook's first Pacific voyage) in a private scientific expedition to Iceland and the Hebrides. Gore and Banks may have become friends as evidence shows that Banks was the executor of Gore's will. The trip did not return until after Cook had sailed on his second Pacific voyage.\n\nHowever, in 1776 Gore answered the call from Cook and the admiralty once again and joined HMS \"Resolution\" as First Lieutenant for Cook's third voyage. As the \"Resolution\" explored the Pacific in search of the famed Northwest Passage, Gore would sight the American continent of his birth from the west coast. Later, following Cook's death in Hawaii, Charles Clerke, captain of \"Resolution's\" sister ship HMS \"Discovery\" took command. Gore then assumed command of \"Discovery\" in Clerke's place. When Clerke himself died shortly after, Gore took responsibility for the entire expedition (unaware that his place of birth had declared its independence from Great Britain two years previously) and brought the ships home to England on 4 October 1780—more than a year after assuming command. He was formally promoted to the post of captain on 2 October 1780.\n\nIn recognition of his achievements John Webber, who had previously painted Cook, took Gore's portrait as the navy made him an honorary post-captain. Moving further in the footsteps of Cook, he was offered the late Captain's vacant rooms at the Greenwich Hospital. In 1790, having circumnavigated the globe four times, he died on 10 August.\n\nGore was survived by a son, John (born 1774) who was also a Royal Navy Officer, who reached the rank of captain on 19 July 1821, retiring in that rank on 1 October 1846, later promoted to Retired Rear Admiral on 8 March 1852 and dying in 1853. He moved to Australia in 1834 as one of the first free settlers. Little is known of his mother (John Gore senior's wife), Ann Gore, although she is known to have received a Royal Navy widow's pension from 1790. His son (that is, John Gore senior's grandson), Graham Gore, continued the expeditionary heritage, perishing in John Franklin's ill-fated attempt to navigate the Northwest Passage, nearly 70 years after his grandfather had attempted the same.\n\nGore Point and the Gore Peninsula in the Alaskan Kenai fjords were named for John Gore by Captain Nathaniel Portlock, a fellow veteran of Cook's third voyage who explored the Pacific Northwest of America in the late 18th century. There are also several Australian sites named after John Gore.\n\n"}
{"id": "893073", "url": "https://en.wikipedia.org/wiki?curid=893073", "title": "Kermadec Islands", "text": "Kermadec Islands\n\nThe Kermadec Islands (Rangitāhua in Māori) are a subtropical island arc in the South Pacific Ocean northeast of New Zealand's North Island, and a similar distance southwest of Tonga. The islands are part of New Zealand, in total area and nowadays uninhabited, except for the permanently manned Raoul Island Station, the northernmost outpost of New Zealand.\n\nThe islands are listed with the New Zealand Outlying Islands. The islands are an immediate part of New Zealand, but not part of any region or district, but instead \"Area Outside Territorial Authority\", like all the other outlying islands except the Solander Islands.\n\nPolynesian people settled the Kermadec Islands in around the 14th century (and perhaps previously in the 10th century), but the first Europeans to reach the area—the \"Lady Penrhyn\" in May 1788—found no inhabitants. The islands were named after the Breton captain Jean-Michel Huon de Kermadec, who visited the islands as part of the d'Entrecasteaux expedition in the 1790s. European settlers, initially the Bell family, lived on the islands from the early nineteenth century until 1937, as did whalers. One of the Bell daughters, Bessie Dyke, recounted the family's experience there to writer Elsie K. Morton, who published the story in 1957 as \"Crusoes of Sunday Island\".\n\nThe Station consists of a government meteorological and radio station, and a hostel for Department of Conservation officers and volunteers, that has been maintained since 1937. It lies on the northern terraces of Raoul Island, at an elevation of about , above the cliffs of Fleetwood Bluff. It is the northernmost inhabited outpost of New Zealand.\n\nIn 1955 the British Government required a large site remote from population centres to test the new thermonuclear devices it was developing. Various islands in the South Pacific and Southern Oceans were considered, along with Antarctica. The Admiralty suggested the Antipodes Islands.. In May 1955, the Minister for Defence, Selwyn Lloyd, concluded that the Kermadec Islands would be suitable. They were part of New Zealand, so Eden wrote to the Prime Minister of New Zealand, Sidney Holland, to ask for permission to use the islands. Holland refused, fearing an adverse public reaction in the upcoming 1957 general election in New Zealand. Despite reassurances and pressure from the British government, Holland remained firm.\n\nThe islands lie within 29° to 31.5° south latitude and 178° to 179° west longitude, northeast of New Zealand's North Island, and a similar distance southwest of Tonga. The centre of the Kermadec Islands group is located at approximately . The total area of the islands is .\n\nThe climate of the islands is subtropical, with a mean monthly temperature of in February and in August. Rainfall is approximately annually, with lower rainfall from October through January.\n\nThe group includes four main islands (three of them might be considered island groups, because the respective main islands have smaller islands close by) and some isolated rocks, which are, from north to south:\n\nSeamounts north and south of the Kermadec Islands are an extension of the ridge running from Tonga to New Zealand (see Geology).\n\nThe islands are a volcanic island arc, formed at the convergent boundary where the Pacific Plate subducts under the Indo-Australian Plate. The subducting Pacific Plate created the Kermadec Trench, an 8 km deep submarine trench, to the east of the islands. The islands lie along the undersea Kermadec Ridge, which runs southwest from the islands towards the North Island of New Zealand and northeast towards Tonga (Kermadec-Tonga Arc).\n\nThe four main islands are the peaks of volcanoes that rise high enough from the seabed to project above sea level. There are several other volcanoes in the chain that do not reach sea level, but form seamounts with between 65 and 1500 m of water above their peaks. Monowai Seamount, with a depth of 120 m over its peak, is midway between Raoul Island and Tonga. 100 km south of L'Esperance Rock is the little-explored Star of Bengal Bank, probably with submarine volcanoes.\n\nFurther south are the South Kermadec Ridge Seamounts, the southernmost of which, Rumble IV Seamount, is just 150 km North of the North Island of New Zealand. The ridge eventually connects to White Island in New Zealand's Bay of Plenty, at the northern end of the Taupo Volcanic Zone. The islands experience many earthquakes from plate movement and volcanism.\n\nRaoul and Curtis are both active volcanoes. The volcanoes on the other islands are currently inactive, and the smaller islands are the eroded remnants of extinct volcanoes.\n\nFrom 18 to 21 July 2012, Havre Seamount (near Havre Rock) erupted, breaching the ocean surface from a depth of more than 1100 m and producing a large raft of pumice floating northwest of the volcano. The eruption was not directly observed, but it was located using earthquake and remote sensing data after the pumice raft was spotted by aircraft and encountered by HMNZS \"Canterbury\".\n\nThe islands are recognised by ecologists as a distinct ecoregion, the Kermadec Islands subtropical moist forests. They are a tropical and subtropical moist broadleaf forests ecoregion, part of the Oceania ecozone. The forests are dominated by the red-flowering Kermadec pōhutukawa, related to the pōhutukawa of New Zealand. The islands are home to 113 native species of vascular plants, of which 23 are endemic, along with mosses (52 native species), lichens and fungi (89 native species). Most of the plant species are derived from New Zealand, with others from the tropical Pacific. 152 non-native species of plants introduced by humans have become established on the islands.\n\nDense subtropical forests cover most of Raoul, and formerly covered Macauley. \"Metrosideros kermadecensis\" is the dominant forest tree, forming a 10 – 15-meter high canopy. A native nikau palm (\"Rhopalostylis baueri\") is another important canopy tree. The forests had a rich understory of smaller trees, shrubs, ferns, and herbs, including \"Myrsine kermadecensis\"; \"Lobelia anceps\", \"Poa polyphylla\", \"Coprosma acutifolia\", and \"Coriaria arborea\". Two endemic tree ferns, \"Cyathea milnei\" and the rare and endangered \"Cyathea kermadecensis\", are also found in the forests.\n\nAreas near the seashore and exposed to salt spray are covered by a distinct community of shrubs and ferns, notably \"Myoporum obscurum\", \"Coprosma petiolata\", \"Asplenium obtusatum\", \"Cyperus ustulatus\", \"Disphyma australe\", and \"Ficinia nodosa\".\n\nThe islands have no native land mammals. An endemic bird subspecies is the Kermadec red-crowned parakeet. The group has been identified as an Important Bird Area (IBA) by BirdLife International because of its significance as a breeding site for several species of seabirds, including white-necked and black-winged petrels, wedge-tailed and little shearwaters, sooty terns and blue noddies. The area also hosts rich habitats for cetaceans. In recent years, increased presences of humpback whales indicate Kermadec Islands functioning as migratory colliders, and varieties of baleen (not in great numbers) and toothed whales including minke whales, sperm whales, less known beaked whales, killer whales, and dolphins frequent in adjacent waters. Vast numbers of southern right whales were historically seen in southwestern areas although only a handful of recent confirmations exist around Raoul Island. The deep sea hydrothermal vents along the Kermadec ridge support diverse extremophile communities including the New Zealand blind vent crab.\n\nThe introduction of cats, rats, and goats devastated the forests and seabirds. Overgrazing by goats eliminated the forests of Macauley Island, leaving open grasslands, and altered the understory of Raoul Island. Predation by rats and cats reduced the seabird colonies on the main islands from millions of birds to tens of thousands. The New Zealand government has been working for the last few decades to restore the islands. New Zealand declared the islands a nature reserve in 1937, and the sea around them a marine reserve in 1990. Goats were removed from Macauley in 1970 and from Raoul in 1984, and the forests have begun to recover. The islands are still known for their bird life, and seabird colonies presently inhabit offshore islets, which are safe from introduced rats and cats. Efforts are currently underway to remove the rats and cats from the islands, as well as some of the invasive exotic plants.\n\nVisits to the islands are restricted by the Department of Conservation. The Department allows visits to Raoul by volunteers assisting in environmental restoration or monitoring projects, and other visitors engaged in nature study. Visits to the other islands are generally restricted to those engaged in scientific study of the islands.\n\nOn 29 September 2015, New Zealand prime minister John Key announced the creation of the Kermadec Ocean Sanctuary, a protected area in the Kermadec Islands region.\n\n"}
{"id": "3025960", "url": "https://en.wikipedia.org/wiki?curid=3025960", "title": "Kimmeridgian", "text": "Kimmeridgian\n\nIn the geologic timescale, the Kimmeridgian is an age or stage in the Late or Upper Jurassic epoch or series. It spans the time between 157.3 ± 1.0 Ma and 152.1 ± 0.9 Ma (million years ago). The Kimmeridgian follows the Oxfordian and precedes the Tithonian.\n\nThe Kimmeridgian stage takes its name from the village of Kimmeridge on the Dorset coast, England. The name was introduced in literature by Swiss geologist Jules Thurmann in 1832. The Kimmeridge Clay Formation has its name from the same type location. It is the source for about 95% of the petroleum in the North Sea.\n\nHistorically, the term Kimmeridgian has been used in two different ways. The base of the interval is the same but the top was defined by British stratigraphers as the base of the Portlandian (\"sensu anglico\") whereas in France the top was defined as the base of the Tithonian (\"sensu gallico\"). The differences have not yet been fully resolved; Tithonian is the uppermost stage of the Jurassic in the timescale of the ICS.\n\nThe base of the Kimmeridgian is at the first appearance of ammonite species \"Pictonia baylei\" in the stratigraphic column. A global reference profile for the base (the GSSP of the Kimmeridgian stage) had in 2009 not yet been assigned. The top of the Kimmeridgian (the base of the Tithonian) is at the first appearance of ammonite species \"Hybonoticeras hybonotum\". It also coincides with the top of magnetic anomaly M22An.\n\nThe Kimmeridgian is sometimes subdivided into Upper and Lower substages. In the Tethys domain, the Kimmeridgian contains seven ammonite biozones:\n\n\n\n"}
{"id": "27736342", "url": "https://en.wikipedia.org/wiki?curid=27736342", "title": "Lava bench", "text": "Lava bench\n\nA lava bench is a volcanic landform with a horizontal surface raised above the level of the surrounding area. They are created when molten lava travels away from a volcanic vent and expands an old shoreline. This is the process that creates new land in the ocean that life can eventually grow on. The Hawaiian islands are an example of land that was formed this way, and the Big Island is currently still expanding due to lava benches. The Kilauea Volcano releases lava that flows down the slope of the volcano and eventually encounters the ocean; this lava flow hardens when it comes into contact with the significantly cooler water of the ocean and forms an unstable lava bench. Eventually, when the material beneath the lava bench stabilizes, it become stable land that has been added to the island. If a newly formed lava bench rests on sediments, it may pose hazards due to its extremely unstable structure. Oftentimes, these benches are so unstable that they collapse into the sea, exposing the water to the hot lava on the interior of the bench and releasing acres of land into the ocean. When the hot lava hits the water, violent explosions of steam can shoot large rocks and molten lava up to inland. These collapses are extremely dangerous because they can happen without warning, and anyone or anything on the bench will be caught in the collapse. For safety, people are advised not to walk on lava benches because of their unstable nature, and they must maintain a safe distance from the lava bench.\n\n"}
{"id": "6146883", "url": "https://en.wikipedia.org/wiki?curid=6146883", "title": "List of Sites of Special Scientific Interest in Kintyre", "text": "List of Sites of Special Scientific Interest in Kintyre\n\nThe following is a list of Sites of Special Scientific Interest in the Kintyre Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "579472", "url": "https://en.wikipedia.org/wiki?curid=579472", "title": "List of national parks of Sri Lanka", "text": "List of national parks of Sri Lanka\n\nNational parks are a class of protected areas in Sri Lanka and are administered by the Department of Wildlife Conservation. National parks are governed by the \"Fauna and Flora Protection Ordinance (No. 2) of 1937\" and may be created, amended or abolished by ministerial order. All of the land in national parks are state-owned and the entire habitat is protected. Activities prohibited in national parks include hunting, killing or removing any wild animal; destroying eggs/nests of birds and reptiles; disturbing of wild animals; interfering in the breeding of any animal; felling/damaging of any plant; breaking up land for cultivation/mining/other purpose; kindling/carrying of fire; and possessing/using any trap/explosive/poison to damage animal or plant life. Visitors are allowed to enter national parks but only for the purpose of observing flora and fauna and with a permit. There are currently 26 national parks which together cover an area of . \n\n"}
{"id": "38001805", "url": "https://en.wikipedia.org/wiki?curid=38001805", "title": "List of pipeline accidents in the United States before 1900", "text": "List of pipeline accidents in the United States before 1900\n\nThe following is not a complete list of natural gas and petroleum product accidents before 1900, which run into the thousands. The oil and gas industry was as yet unregulated, so leaks and explosions were not tracked in an organized fashion except by city fire departments. Many natural gas accidents were not recorded unless they occurred in population centers with newspapers to report them.\n\nIn the twentieth century, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, would be established to develop and enforce regulations for the safe and environmentally sound operation of the United States' pipelines, and to collect data on pipeline leaks, accidents, and explosions.\n\n\n\n\n"}
{"id": "256679", "url": "https://en.wikipedia.org/wiki?curid=256679", "title": "List of rivers of Albania", "text": "List of rivers of Albania\n\nAlbania has more than 152 rivers and streams, forming 8 large rivers flowing from southeast to northwest, mainly discharging towards the Adriatic coast. The rivers of Albania are characterized by a high flow rate.\n\nA majority of the precipitation that falls on Albania, drains into the rivers and reaches the coast on the west without even leaving the country. In the north, only one small stream escapes Albania. In the south, an even smaller rivulet drains into Greece. Due to the topographical divide is east of the border with the Republic of Macedonia. An extensive portion of the basin of the White Drin, basin is in the Dukagjin region, across the northeastern border with Kosovo. The Lake of Ohrid, Lake of Prespa and the Small Lake of Prespa on the southeast, as well as the streams that flow into them, drain into the Black Drin. The watershed divide in the south also dips nearly into Greece at one point. Several tributaries of the Vjosa River rise in that area. About 65% of their watershed lies within the Albanian territory. The water temperature ranges from 3.5 to 8.9° C in the winter, and from 17.8 to 24.6° C in the summer months.\n\n\n \n"}
{"id": "9125486", "url": "https://en.wikipedia.org/wiki?curid=9125486", "title": "List of rivers of Georgia (country)", "text": "List of rivers of Georgia (country)\n\nThe rivers of Georgia, a country in the Caucasus, include:\n\n"}
{"id": "9017447", "url": "https://en.wikipedia.org/wiki?curid=9017447", "title": "List of tea diseases", "text": "List of tea diseases\n\nMany of the diseases, pathogens and pests that affect the tea plant (\"Camellia sinensis\") may affect other members of the plant genus \"Camellia\".\n\n"}
{"id": "7046215", "url": "https://en.wikipedia.org/wiki?curid=7046215", "title": "Loewe 3NF", "text": "Loewe 3NF\n\nThe Loewe 3NF was an early attempt to combine several functions in one electronic device.\n\nProduced by the German Loewe-Audion GmbH as early as 1926, the device consisted of three triode valves (tubes) in a single glass envelope together with two fixed capacitors and four fixed resistors required to make a complete radio receiver. The resistors and capacitors had to be sealed in their own glass tubes to prevent them from contaminating the vacuum. \n\nThe only other parts required to build a radio receiver were the tuning coil, the tuning capacitor and the loudspeaker. The device was produced not to enter the integrated circuit era several decades early, but to evade German taxes levied on a per valveholder basis. As the Loewe set had only one valveholder, it was able to substantially undercut the competition. The resultant radio receiver required a 90 volt HT plus a 4 volt LT (A and B) battery (the HT battery provided not only 82.5 volts for the HT, but also two grid bias supplies at −1.5 volts and −7.5 volts).\n\nOne major disadvantage of the 3NF was that if one filament failed, the whole device was rendered useless. Loewe countered this by offering a filament repair service.\n\nLoewe were to also offer the 2NF (two tetrodes plus passive components) and the WG38 (two pentodes, a triode and the passive components).\n"}
{"id": "31584", "url": "https://en.wikipedia.org/wiki?curid=31584", "title": "Lord of the Flies", "text": "Lord of the Flies\n\nLord of the Flies is a 1954 novel by Nobel Prize–winning British author William Golding. The book focuses on a group of British boys stranded on an uninhabited island and their disastrous attempt to govern themselves.\n\nThe novel has been generally well received. It was named in the Modern Library 100 Best Novels, reaching number 41 on the editor's list, and 25 on the reader's list. In 2003 it was listed at number 70 on the BBC's The Big Read poll, and in 2005 \"Time\" magazine named it as one of the 100 best English-language novels from 1923 to 2005.\n\nPublished in 1954, \"Lord of the Flies\" was Golding's first novel. Although it did not have great success after being released—selling fewer than three thousand copies in the United States during 1955 before going out of print—it soon went on to become a best-seller. It has been adapted to film twice in English, in 1963 by Peter Brook and 1990 by Harry Hook, and once in Filipino by Lupita A. Concio (1975).\n\nThe book takes place in the midst of an unspecified war. Some of the marooned characters are ordinary students, while others arrive as a musical choir under an established leader. With the exception of Sam and Eric and the choirboys, they appear never to have encountered each other before. The book portrays their descent into savagery; left to themselves on a paradisiacal island, far from modern civilization, the well-educated children regress to a primitive state.\n\nGolding wrote his book as a counterpoint to R.M. Ballantyne's youth novel \"The Coral Island\" (1858), and included specific references to it, such as the rescuing naval officer's description of the children's initial attempts at civilised cooperation as \"a jolly good show, like the Coral Island\". Golding's three central characters—Ralph, Piggy and Jack—have been interpreted as caricatures of Ballantyne's \"Coral Island\" protagonists.\n\nIn the midst of a wartime evacuation, a British aeroplane crashes on or near an isolated island in a remote region of the Pacific Ocean. The only survivors are boys in their middle childhood or preadolescence. Two boys—the fair-haired Ralph and an overweight, bespectacled boy nicknamed \"Piggy\"—find a conch, which Ralph uses as a horn to convene all the survivors to one area. Ralph is optimistic, believing that grown-ups will come to rescue them but Piggy realises the need to organise: (\"put first things first and act proper\"). Because Ralph appears responsible for bringing all the survivors together, he immediately commands some authority over the other boys and is quickly elected their \"chief\". He does not receive the votes of the members of a boys' choir, led by the red-headed Jack Merridew, although he allows the choir boys to form a separate clique of hunters. Ralph establishes three primary policies: to have fun, to survive, and to constantly maintain a smoke signal that could alert passing ships to their presence on the island and thus rescue them. The boys establish a form of democracy by declaring that whoever holds the conch shall also be able to speak at their formal gatherings and receive the attentive silence of the larger group.\n\nJack organises his choir into a hunting party responsible for discovering a food source. Ralph, Jack, and a quiet, dreamy boy named Simon soon form a loose triumvirate of leaders with Ralph as the ultimate authority. Upon inspection of the island, the three determine that it has fruit and wild pigs for food. The boys also use Piggy's glasses to create a fire. Although he is Ralph's only real confidant, Piggy is quickly made into an outcast by his fellow \"biguns\" (older boys) and becomes an unwilling source of laughs for the other children while being hated by Jack. Simon, in addition to supervising the project of constructing shelters, feels an instinctive need to protect the \"littluns\" (younger boys).\n\nThe semblance of order quickly deteriorates as the majority of the boys turn idle; they give little aid in building shelters, spend their time having fun and begin to develop paranoias about the island. The central paranoia refers to a supposed monster they call the \"beast\", which they all slowly begin to believe exists on the island. Ralph insists that no such beast exists, but Jack, who has started a power struggle with Ralph, gains a level of control over the group by boldly promising to kill the creature. At one point, Jack summons all of his hunters to hunt down a wild pig, drawing away those assigned to maintain the signal fire. A ship travels by the island, but without the boys' smoke signal to alert the ship's crew, the vessel continues without stopping. Ralph angrily confronts Jack about his failure to maintain the signal; in frustration Jack assaults Piggy, breaking his glasses. The boys subsequently enjoy their first feast. Angered by the failure of the boys to attract potential rescuers, Ralph considers relinquishing his position as leader, but is persuaded not to do so by Piggy, who both understands Ralph's importance and deeply fears what will become of him should Jack take total control.\n\nOne night, an aerial battle occurs near the island while the boys sleep, during which a fighter pilot ejects from his plane and dies in the descent. His body drifts down to the island in his parachute; both get tangled in a tree near the top of the mountain. Later on, while Jack continues to scheme against Ralph, the twins Sam and Eric, now assigned to the maintenance of the signal fire, see the corpse of the fighter pilot and his parachute in the dark. Mistaking the corpse for the beast, they run to the cluster of shelters that Ralph and Simon have erected to warn the others. This unexpected meeting again raises tensions between Jack and Ralph. Shortly thereafter, Jack decides to lead a party to the other side of the island, where a mountain of stones, later called Castle Rock, forms a place where he claims the beast resides. Only Ralph and a quiet suspicious boy, Roger, Jack's closest supporter, agree to go; Ralph turns back shortly before the other two boys but eventually all three see the parachutist, whose head rises via the wind. They then flee, now believing the beast is truly real. When they arrive at the shelters, Jack calls an assembly and tries to turn the others against Ralph, asking them to remove Ralph from his position. Receiving no support, Jack storms off alone to form his own tribe. Roger immediately sneaks off to join Jack, and slowly an increasing number of older boys abandon Ralph to join Jack's tribe. Jack's tribe continues to lure recruits from the main group by promising feasts of cooked pig. The members begin to paint their faces and enact bizarre rites, including sacrifices to the beast. One night, Ralph and Piggy decide to go to one of Jack's feasts.\n\nSimon, who faints frequently and is probably an epileptic, has a secret hideaway where he goes to be alone. One day while he is there, Jack and his followers erect an offering to the beast nearby: a pig's head, mounted on a sharpened stick and soon swarming with scavenging flies. Simon conducts an imaginary dialogue with the head, which he dubs the \"Lord of the Flies\". The head mocks Simon's notion that the beast is a real entity, \"something you could hunt and kill\", and reveals the truth: they, the boys, are the beast; it is inside them all. The Lord of the Flies also warns Simon that he is in danger, because he represents the soul of man, and predicts that the others will kill him. Simon climbs the mountain alone and discovers that the \"beast\" is the dead parachutist. He rushes down to tell the other boys, who are engaged in a ritual dance. The frenzied boys mistake Simon for the beast, attack him, and beat him to death. Both Ralph and Piggy participate in the melee, and they become deeply disturbed by their actions after returning from Castle Rock.\n\nJack and his rebel band decide that the real symbol of power on the island is not the conch, but Piggy's glasses—the only means the boys have of starting a fire. They raid Ralph's camp, confiscate the glasses, and return to their abode on Castle Rock. Ralph, now deserted by most of his supporters, journeys to Castle Rock to confront Jack and secure the glasses. Taking the conch and accompanied only by Piggy, Sam, and Eric, Ralph finds the tribe and demands that they return the valuable object. Confirming their total rejection of Ralph's authority, the tribe capture and bind the twins under Jack's command. Ralph and Jack engage in a fight which neither wins before Piggy tries once more to address the tribe. Any sense of order or safety is permanently eroded when Roger, now sadistic, deliberately drops a boulder from his vantage point above, killing Piggy and shattering the conch. Ralph manages to escape, but Sam and Eric are tortured by Roger until they agree to join Jack's tribe.\n\nRalph secretly confronts Sam and Eric, who warn him that Jack and Roger hate him and that Roger has sharpened a stick at both ends, implying the tribe intends to hunt him like a pig and behead him. The following morning, Jack orders his tribe to begin a hunt for Ralph. Jack's savages set fire to the forest while Ralph desperately weighs his options for survival. Following a long chase, most of the island is consumed in flames. With the hunters closely behind him, Ralph trips and falls. He looks up at a uniformed adult—a British naval officer whose party has landed from a passing cruiser to investigate the fire. Ralph bursts into tears over the death of Piggy and the \"end of innocence\". Jack and the other children, filthy and unkempt, also revert to their true ages and erupt into sobs. The officer expresses his disappointment at seeing British boys exhibiting such feral, warlike behaviour before turning to stare awkwardly at his own warship.\nAt an allegorical level, the central theme is the conflicting human impulses toward civilisation and social organisation—living by rules, peacefully and in harmony—and toward the will to power. Themes include the tension between groupthink and individuality, between rational and emotional reactions, and between morality and immorality. How these play out, and how different people feel the influences of these form a major subtext of \"Lord of the Flies\". The name \"Lord of the Flies\" is a literal translation of Beelzebub, from .\n\nIn February 1960, Floyd C. Gale of \"Galaxy Science Fiction\" rated \"Lord of the Flies\" five stars out of five, stating that \"Golding paints a truly terrifying picture of the decay of a minuscule society ... Well on its way to becoming a modern classic\".\n\n\nThere have been three film adaptations based on the book:\n\n\nA fourth adaptation, to feature an all-female cast, was announced by Warner Bros. in August 2017. Scott McGehee and David Siegel are slated to write and direct. The film's concept has been negatively received, with some stating that an all-female cast goes against the novel's themes of masculinity and male power.\n\nNigel Williams adapted the text for the stage.\nIt was debuted by the Royal Shakespeare Company in July 1996.\nThe Pilot Theatre Company has toured it extensively in the United Kingdom and elsewhere.\n\nIn October 2014 it was announced that the 2011 production of \"Lord of the Flies\" would return to conclude the 2015 season at the Regent's Park Open Air Theatre ahead of a major UK tour. The production was to be directed by the Artistic Director Timothy Sheader who won the 2014 Whatsonstage.com Awards Best Play Revival for \"To Kill A Mockingbird\".\n\nIn June 2013, BBC Radio 4 Extra broadcast a dramatisation by Judith Adams in four 30-minute episodes directed by Sasha Yevtushenko. The cast included Ruth Wilson as \"The Narrator\", Finn Bennett as \"Ralph\", Richard Linnel as \"Jack\", Caspar Hilton-Hilley as \"Piggy\" and Jack Caine as \"Simon\".\n\n\nMany writers have borrowed plot elements from \"Lord of the Flies\". By the early 1960s, it was required reading in many schools and colleges.\n\nStephen King's fictional town of Castle Rock, inspired by the fictional mountain fort of the same name in \"Lord of the Flies\", in turn inspired the name of Rob Reiner's production company, Castle Rock Entertainment, which produced the film \"Lord of the Flies\" (1990).\n\nStephen King got the name Castle Rock from the fictional mountain fort of the same name in \"Lord of the Flies\" and used the name to refer to a fictional town that has appeared in a number of his novels. The book itself appears prominently in his novels \"Hearts in Atlantis\" (1999), \"Misery\" (1987), and \"Cujo\" (1981).\n\nStephen King wrote an introduction for a new edition of \"Lord of the Flies\" (2011) to mark the centenary of William Golding's birth in 2011.\n\nThe novel \"Garden Lakes\" by Jaime Clarke is an homage to \"Lord of the Flies\".\n\nThe final song on U2's debut album \"Boy\" (1980) takes its title, \"Shadows and Tall Trees\", from Chapter 7 in the book.\n\nIron Maiden wrote a song inspired by the book, included in their 1995 album \"The X Factor\".\n\n\n"}
{"id": "1374344", "url": "https://en.wikipedia.org/wiki?curid=1374344", "title": "Mesothermal", "text": "Mesothermal\n\nIn climatology, the term mesothermal is used to refer to certain forms of climate found typically in the Earth's Temperate Zones. It has a moderate amount of heat, with winters not cold enough to sustain snow cover. Summers are warm within oceanic climate regimes, and hot within continental climate regimes. \n\nThe term is derived from two Greek words meaning \"having a moderate amount of heat.\" This can be misinterpreted, however, since the term is actually intended to describe only the temperature conditions that prevail during the winter months, rather than those for the year as a whole.\n\nUnder the broadest definition, all places with an average temperature in their coldest month that is colder than 18°C, but warmer than −3°C, are said to have a mesothermal climate. In some climate classification schemes, however, this is divided into two segments, with a coldest-month average of 6°C being the line of demarcation between them; then only those locations with a coldest-month temperature of between −3°C and 6°C are reckoned as mesothermal, the label \"subtropical\" being applied to areas where the average temperature in the coldest month ranges from 6°C to 18°C.\n\nObserving the narrower definition articulated above, the mesothermal locations are those where the winters are too cold to allow year-round photosynthesis, but not cold enough to support a fixed period of continuous snow cover every year.\n\nIn the USA, the northern boundary line between mesothermal and microthermal ranges is north of Juneau and Sitka at the Pacific Ocean. It goes sharply south to about 38N latitude in the Rockies, then eastward across the lower Midwest to the East Coast near Boston. The southern boundary line between mesothermal and megathermal (or tropical) is across south Florida just above Palm Beach.\n\nSummers in these places may be hot (that is to say, having an average temperature in their warmest month of 22°C or above) or merely warm (with the warmest month averaging between 10°C and 22°C). The hot-summer, or continental, mesothermal climate is encountered exclusively in the Northern Hemisphere, in the landmass interiors of Asia and North America and along their east coasts, while the most frequently seen example of a warm to cool-summer mesothermal climate is the oceanic climates found along the west coasts of all of the world's continents, roughly equidistant between the geographical tropical and polar zones.\n\nList of mesothermal cities and their summer temperatures: <br>\nHong Kong-hot summer <br>\nMilan-hot summer <br>\nNew York City-hot summer <br>\nTokyo-hot summer <br>\nLondon-warm summer <br>\nMexico City-warm summer <br>\nVancouver-warm summer\n\nIn addition to being subdivisible by summer temperature, mesothermal climates can also be subclassified on the basis of precipitation — into humid, semiarid and arid subtypes within your front door within depth\n\n\n\"Applied Climatology\", John Griffiths (for use of 6°C in the coldest month as poleward limit of subtropical climates)\n"}
{"id": "14810458", "url": "https://en.wikipedia.org/wiki?curid=14810458", "title": "Monte Cavo", "text": "Monte Cavo\n\nMonte Cavo, or less occasionally, \"Monte Albano,\" is the second highest mountain of the complex of the Alban Hills, near Rome, Italy. An old volcano extinguished around 10,000 years ago, it lies about from the sea, in the territory of the \"comune\" of Rocca di Papa. It is the dominant peak of the Alban Hills. The current name comes from \"Cabum\", an Italic settlement existing on this mountain.\n\nVolcanic activity under king Tullus Hostilius on the site was reported by Livy in his book of Roman history: \"...there had been a shower of stones on the Alban Mount...\".\n\nMonte Cavo is the sacred \"Mons Albanus\" of the Italic people of ancient Italy who lived in Alba Longa (the Albani), and other cities, and therefore a sacred mountain to the Romans; there they built the temple of Jove (Jupiter) Latiaris, one of the most important destinations of pilgrimage for all Latin people in the centuries of Roman domination.\nOn the \"Mons Albanus\", between January and March, the \"Latin Festivals\" were held. The newly chosen Consuls had to sacrifice to Jupiter Latiaris and to announce the Latin Holidays. When the Consul obtained a victory in war he also had to celebrate the triumph on the Alban Mount. Here in the Latium temple were celebrated every year the \"Feriae Latinae\" for four days by the representatives of 47 cities (30 Latin and 17 Federate).\n\nIn 531 BC, King Tarquinius Superbus built here a temple shared with the Latins, the Hernici and the Volsci, where every year celebrations in honor of Jupiter Latiaris were held. In return, Jupiter Latiaris conferred upon whoever was elected head of the Latin confederation, the power of \"dictator latinus\". \nA triumphal procession along this sacred way left the Appian Way at Ariccia and climbed up 450 m to the hillside. More than 5 km of this way is well preserved through the woods.\n\nThe history of the Pagan temple of \"Iuppiter Latiaris\" was interrupted in the Early Middle Ages, when a hermitage devoted to Saint Peter which replaced the pagan temple was built by a Dalmatian hermit. It was visited by Pope Pius II in 1463, and subsequently by Pope Alexander VII. After the Dalmatian hermits the Polish religious order of Edmondo of Buisson was established there, then the Trinitarian Spaniards, and finally the Flemish Missionaries.\n\nThen the hermitage was converted to a monastery (1727). The Passionists came in 1758 and restored it in 1783, using the materials of the temple of Jupiter, as found and raised by Henry Benedict Stuart, Duke of York, bishop of Frascati.\n\nDuring this period there were guests in the monastery: the king Francis II of Naples in 1865 and Pope Pius IX in 1867. The \"contemplative-missionaries\" abandoned the monastery in 1889.\n\nIn 1890 the structure was converted to an hotel that entertained national and international personalities, among others: Umberto II of Italy, Massimo d'Azeglio, Luigi Pirandello, Armando Diaz (who sojourned in Rocca di Papa and was remembered with a commemorative headstone mail in the residence on De Rossi palace) and the King Edward VIII with his wife Wallis Simpson.\n\nFrom 1942 the hotel was used as military base for radio communications by the German Wehrmacht. On June 3, 1944, soldiers of 142nd Regiment-36th Infantry Division (United States) (\"Texas\" Division), attacked and captured the military site—with 20 enemy soldiers killed and 30 prisoners taken.\n\nIn the post-war era the structure became a telecommunications station. Access is prohibited to unauthorized persons. A few blocks of the ancient temple are still visible behind the fenced area. The unsightly presence of the antennae has been a cause of some civil complaint. \n\n[*] Students of the Pontifical Scots College, Rome, used to spend the long vacation at the villa in Marino which the College then owned (up until about 1965 or so). Monte Cavo was clearly visible from the villa, and it became a custom for students at least once in their student days to walk to the summit to see the sunrise.To time this right meant rising at about 1.30 a.m. and undertaking a sometimes very rough walk in near total darkness.But catching the first hint of the sun's orb was a breathtaking experience.\"Scots College, Rome,magazine,1961/62\"\n\n"}
{"id": "2574869", "url": "https://en.wikipedia.org/wiki?curid=2574869", "title": "National Museum (Prague)", "text": "National Museum (Prague)\n\nThe National Museum (NM) () is a Czech museum institution intended to systematically establish, prepare and publicly exhibit natural scientific and historical collections. It was founded in 1818 by Kašpar Maria Šternberg. Historian František Palacký was also strongly involved.\n\nAt present the National Museum houses almost 14 million items from the area of natural history, history, arts, music and librarianship, located in dozens of buildings.\n\nThe founding of the National Museum should be seen in the context of the times, where after the French Revolution, royal and private collections of art, science, and culture were being made available to the public. The beginnings of the museum can be seen as far back as 1796, when the private Society of Patriotic Friends of the Arts was founded by Count Casper Sternberk-Manderschied and a group of other prominent nobles. The avowed purpose of the society was \"the renewed promotion of art and taste\", and during the time of Joseph II, it would be adamantly opposed to the King. In 1800 the group founded the Academy of Fine Arts, which would train students in progressive forms of art and history.\n\nThe National Museum in Prague was founded on April 15, 1818, with the first president of the Society of the Patriotic Museum being named Count Sternberk, who would serve as the trustee and operator of the museum. Early on, the focus of the museum was on natural sciences, partially because Count Sternberk was a botanist, mineralogist, and eminent phytopaleontologist, but also because of the natural science slant of the times, as perpetrated by Emperor Joseph II of Austria.\n\nThe museum was originally located in the Sternberg Palace but it was soon apparent that this was too small to hold the museum's collections. The museum relocated to the Nostitz Palace but this was also found to be of insufficient capacity, which led to the decision to construct a new building for the museum in Wenceslas Square.\n\nThe museum did not become interested in the acquisition of historical objects until the 1830s and 40s, when Romanticism became prevalent, and the institution of the museum was increasingly seen as a center for Czech nationalism. Serving as historian and secretary of the National Museum in 1841, Frantisek Palacky would try to balance natural science and history, as he described in his Treatise of 1841. It was a difficult task, however, and it would not be until nearly a century later until the National Museum’s historical treasures equaled its collection of natural science artifacts.\n\nHowever, the importance of the museum was not in its focus, but rather that it signaled, and indeed helped bring about, an intellectual shift in Prague. The Bohemian nobility had, until this time, been prominent, indeed dominant, both politically and fiscally in scholarly and scientific groups. However, the National Museum was created to serve all the inhabitants of the land, lifting the stranglehold the nobility had had on knowledge. This was further accelerated by the historian Frantisek Palacky, who in 1827 suggested that the museum publish separate journals in German and Czech. Previously, the vast majority of scholarly journals were written in German, but within a few years the German journal had ceased publication, while the Czech journal continued for more than a century.\n\nIn 1949, the national government took over the museum, and spelled out its role and leadership in the Museum and Galleries Act of 1959. In May 1964, the Museum was turned into an organization of five professionally autonomous components: the Museum of Natural Science, the Historical Museum, the Naprstek Museum of Asia, African, and American Cultures, the National Museum Library, the Central Office of Museology. A sixth autonomous unit, the Museum of Czech Music, was established in 1976.\n\nAccording to their website, the National Museum at present contains several million items of material in three main parts, the natural Museum, the Historical Museum and the Library.\n\nin 2010 moved their collections to Prague 10, Horní Počernice. It has the departments of mineralogy, paleontology, mycology, botany, entomology, zoology and anthropology, as well as scientific laboratories.\n\n\nImportant parts of the Medieval collection is jewelry, panel painting, wooden sculpture, and weapons (also such as used in the Hussite movement of the 15th century). In addition to their historical value, many of the objects held by this department contain a high artistic value. Examples of precious objects include: a silver tiara of a duke from the twelfth century; Medieval, Renaissance and Baroque jewelry; liturgical objects from the Medieval period, which include several chalices, the reliquary of St Eligius in the shape of mitre; Gothic and Renaissance glazed tiles and paving stones; precious embroidery of Rosenberg antependium dated about 1370. Fine Bohemian porcelain and glass collection represent before all the 18th and 19th centuries, as well as collection of painted portraits and miniature painting.\n\nIt contains the most rare charts and manuscripts of the Czech history from the 11th to the 20th century, the ancient ones have been digitalised. The collection of personal legacy contains written sources of famous personalities of Czech history. Large collection of seals and sealsticks include about 3000 pieces.\n\n\nThe main museum building is located on the upper end of Wenceslas Square and was built by prominent Czech neo-renaissance architect Josef Schulz from 1885 - 1891. Prior to the museum being constructed, there had been several noblemen’s palaces located at this site. With the construction of a permanent building for the museum, a great deal of work which had previously been devoted to ensuring that the collections would remain intact was now put toward collecting new materials.\n\nThe building was damaged during World War II in 1945 by a bomb, but the collections were not damaged due to their removal to other secured storage sites. The museum was reopened after intensive repairs in 1947, and in 1960 exterior night floodlighting was installed, which followed a general repair of the facade that had taken place in previous years.\n\nDuring the 1968 Warsaw Pact intervention the main facade was severely damaged by strong Soviet machine-gun and automatic submachine-gun fire. The shots made numerous holes in sandstone pillars and plaster, destroyed stone statues and reliefs and also caused damage in some of the depositaries. Despite the general facade repair made between 1970 - 1972 the damage still can be seen because the builders used lighter sandstone to repair the bullet holes.\n\nThe main Museum building was also damaged during the construction of the Prague Metro in 1972 and 1978. The opening of the North-South Highway in 1978 on two sides of the building resulted in the museum being cut off from city infrastructure. This also led to the building suffering from an excessive noise level, a dangerously high level of dust and constant vibrations from heavy road traffic.\n\nDue to major renovations the museum is closed since 7 July 2011 and the re-opening is planned for October 2018. Some seven million items had to be removed to the museum’s depositories in what has been dubbed the biggest moving of museum collections in Czech history.\n\n\n\n"}
{"id": "31180472", "url": "https://en.wikipedia.org/wiki?curid=31180472", "title": "New drilling technologies", "text": "New drilling technologies\n\nThe latest study of Massachusetts Institute of Technology \"The Future of Geothermal Energy – Impact of Enhanced Geothermal Systems (EGS) on the United States in the 21st Century\" (2006) points out the essential importance of developing an economical deep geothermal boring technology. With current boring technologies, bore price rises exponentially with depth. Thus, finding a boring technology with which the bore price rise would be approximately linear with increasing bore depth is an important challenge.\n\nThis MIT study characterizes the requirements on new fast and ultra-deep boring technology as follows:\n\n\nThere are more than 20 research efforts solving innovative drilling technology such as: laser, spallation, plasma, electron beam, pallets, enhanced rotary, electric spark and discharge, electric arc, water jet erosion, ultrasonic, chemical, induction, nuclear, forced flame explosive, turbine, high frequency, microwave, heating/cooling stress, electric current and several other. The most promising solutions are mentioned below:\n\n1. Hydrothermal spallation – Thermal spallation drilling uses a large, downhole burner, much like a jet engine, to apply a high heat flux to the rock face. This drilling technology is based on thermal processes of rock spallation and fusion. \n\n2. Chemical plasma – is based on crushing by high-speed combustion, but nitric acid as oxidizing agent instead of oxygen.\n\n3. Erosion - most patents refer to water jet rock cutting. Different modification variants are described, e.g. utilization of cavitation, turbulent processes, combination with mechanical processes, etc.\n\n4. Laser - during the recent decade intense research has been made into utilization of high energy laser beams for rock disintegration. Primarily conversion of military equipment is concerned. Laser energy is used for the process of thermal spallation, melting, or evaporation of rock. \n\n5. Electric discharge - The methods utilizing electric discharge are based on long-term experience gained in other application areas. \n\n6. Electrical plasma - is based on crushing by irradiation of plasma with high temperature up to 20 000°C\n\n7. Direct transfer of heat - This technology is based on electrically melting rock at 1400°C; lava gravel will float to top; bore hole walls are of glass of surrounding rock. Cost decreases with depth, with no limit on depth of bore hole. Bore diameters from 1m to 10m. Recovery of energy used to melt rock.\n\nOne of the most promising approaches in deep drilling field is utilization of electrical plasma. It has lower energy efficiency than some of the other technologies, but it has several other advantages. Producing boreholes with wide range of diameters or drilling in water environment can be mentioned. The research team from Slovakia has developed drilling concept based on utilization of electrical plasma. The core of the research is held in Research Centre for Deep Drilling which was opened in the premises of Slovak Academy of Sciences. Only a very small number of companies have embraced this method, e.g. GA Drilling, headquartered in Bratislava, Slovakia.\n\n\nNo one as of yet has proved the effective use of these techniques in severe conditions. Other complications including on site energy and material transport from 5 to 10 km bores also require refinement to become both technically and economically feasible.\n\n\n"}
{"id": "37392290", "url": "https://en.wikipedia.org/wiki?curid=37392290", "title": "Nudo de los Pastos", "text": "Nudo de los Pastos\n\nNudo de los Pastos, in English meaning \"Knot of the Pastos\" or also known as the \"Massif of Huaca\", is an Andean orographic complex located in the Ecuadorian province of Carchi and the Colombian department of Nariño. It covers the intricate mountain region where the Andes splits into two branches on entering Colombia: the Cordillera Occidental and the Cordillera Central.\n\nAlthough there is no precise definition of the region of \"Nudos de los Pastos\", it is generally considered to begin north of the rivers Chota and Apaqui and east of the rivers Cuasmal and Bishop and extends to the Sickle of Minamá in the Cordillera Occidental and to the Colombian Massif in the Cordillera Central, where this gives rise to the Cordillera Oriental.\n\nIn the Cordillera Occidental the taller heights are the volcanoes Chile (4,718 m), Cumbal (4,764 m) and Azufral (4,070 m). The Minamá Sickle is a deep depression created when the Patia River crosses the Cordillera Occidental on its way to the Pacific coast Nariño.\n\nIn the Cordillera Central there are the highlands Túquerres and Ipiales, Atriz Valley, volcanoes Galeras (4276 m) and Doña Juana (4,250 m), and the town of San Juan de Pasto and Cocha lagoon.\n\n"}
{"id": "66575", "url": "https://en.wikipedia.org/wiki?curid=66575", "title": "Nutrient", "text": "Nutrient\n\nA nutrient is a substance used by an organism to survive, grow, and reproduce. The requirement for dietary nutrient intake applies to animals, plants, fungi, and protists. Nutrients can be incorporated into cells for metabolic purposes or excreted by cells to create non-cellular structures, such as hair, scales, feathers, or exoskeletons. Some nutrients can be metabolically converted to smaller molecules in the process of releasing energy, such as for carbohydrates, lipids, proteins, and fermentation products (ethanol or vinegar), leading to end-products of water and carbon dioxide. All organisms require water. Essential nutrients for animals are the energy sources, some of the amino acids that are combined to create proteins, a subset of fatty acids, vitamins and certain minerals. Plants require more diverse minerals absorbed through roots, plus carbon dioxide and oxygen absorbed through leaves. Fungi live on dead or living organic matter and meet nutrient needs from their host.\n\nDifferent types of organism have different essential nutrients. Ascorbic acid (vitamin C) is essential, meaning it must be consumed in sufficient amounts, to humans and some other animal species, but not to all animals and not to plants, which are able to synthesize it. Nutrients may be organic or inorganic: organic compounds include most compounds containing carbon, while all other chemicals are inorganic. Inorganic nutrients include nutrients such as iron, selenium, and zinc, while organic nutrients include, among many others, energy-providing compounds and vitamins.\n\nA classification used primarily to describe nutrient needs of animals divides nutrients into macronutrients and micronutrients. Consumed in relatively large amounts (grams or ounces), macronutrients (carbohydrates, fats, proteins, water) are used primarily to generate energy or to incorporate into tissues for growth and repair. Micronutrients are needed in smaller amounts (milligrams or micrograms); they have subtle biochemical and physiological roles in cellular processes, like vascular functions or nerve conduction. Inadequate amounts of essential nutrients, or diseases that interfere with absorption, result in a deficiency state that compromises growth, survival and reproduction. Consumer advisories for dietary nutrient intakes, such as the United States Dietary Reference Intake, are based on deficiency outcomes and provide macronutrient and micronutrient guides for both lower and upper limits of intake. In many countries, macronutrients and micronutrients in significant content are required by regulations to be displayed on food product labels. Nutrients in larger quantities than the body needs may have harmful effects. Edible plants also contain thousands of compounds generally called phytochemicals which have unknown effects on disease or health, including a diverse class with non-nutrient status called polyphenols, which remain poorly understood as of 2017.\n\nPlant nutrients consist of more than a dozen minerals absorbed through roots, plus carbon dioxide and oxygen absorbed or released through leaves. All organisms obtain all their nutrients from the surrounding environment.\n\nPlants absorb carbon, hydrogen and oxygen from air. These three, in the form of water and carbon dioxide. Other nutrients are absorbed from soil (exceptions include some parasitic or carnivorous plants). Counting these, there are 17 important nutrients for plants: the macronutrients nitrogen (N), phosphorus (P), potassium (K), calcium (Ca), sulfur (S), magnesium (Mg), carbon (C), oxygen(O) and hydrogen (H), and the micronutrients iron (Fe), boron (B), chlorine (Cl), manganese (Mn), zinc (Zn), copper (Cu), molybdenum (Mo) and nickel (Ni). In addition to carbon, hydrogen and oxygen, nitrogen, phosphorus, and sulfur are also needed in relatively large quantities. Together, the \"Big Six\" are the elemental macronutrients for all organisms.\nThey are sourced from inorganic matter (for example, carbon dioxide, water, nitrates, phosphates, sulfates, and diatomic molecules of nitrogen and, especially, oxygen) and organic matter (carbohydrates, lipids, proteins).\n\nMacronutrients are defined in several ways.\n\nMacronutrients provide energy:\n\n\nFat has an energy content of 9kcal/g (~37.7 kJ/g) and proteins and carbohydrates 4kcal/g (~16.7 kJ/g).\n\nMicronutrients support metabolism.\n\n\nAn essential nutrient is a nutrient required for normal physiological function that cannot be synthesized in the body – either at all or in sufficient quantities – and thus must be obtained from a dietary source. Apart from water, which is universally required for the maintenance of homeostasis in mammals, essential nutrients are indispensable for various cellular metabolic processes and maintaining tissue and organ function. In the case of humans, there are nine amino acids, two fatty acids, thirteen vitamins and fifteen minerals that are considered essential nutrients. In addition, there are several molecules that are considered conditionally essential nutrients since they are indispensable in certain developmental and pathological states.\n\nAn essential amino acid is an amino acid that is required by an organism but cannot be synthesized \"de novo\" by it, and therefore must be supplied in its diet. Out of the twenty standard protein-producing amino acids, nine cannot be endogenously synthesized by humans: phenylalanine, valine, threonine, tryptophan, methionine, leucine, isoleucine, lysine, and histidine.\n\nEssential fatty acids (EFAs) are fatty acids that humans and other animals must ingest because the body requires them for good health but cannot synthesize them. Only two fatty acids are known to be essential for humans: alpha-linolenic acid (an omega-3 fatty acid) and linoleic acid (an omega-6 fatty acid).\n\nVitamins are organic molecules essential for an organism that are not classified as amino acids or fatty acids. They commonly function as enzymatic cofactors, metabolic regulators or antioxidants. Humans require thirteen vitamins in their diet, most of which are actually groups of related molecules (e.g. vitamin E includes tocopherols and tocotrienols): vitamins A, C, D, E, K, thiamine (B), riboflavin (B), niacin (B), pantothenic acid (B), vitamin B (e.g., pyridoxine), biotin (B), folate (B), and cobalamin (B). The requirement for vitamin D is conditional, as people who get sufficient exposure to ultraviolet light, either from the sun or an artificial source, synthesize vitamin D in the skin.\n\nMinerals are the exogenous chemical elements indispensable for life. Although the four elements: carbon, hydrogen, oxygen, and nitrogen, are essential for life, they are so plentiful in food and drink that these are not considered nutrients and there are no recommended intakes for these as minerals. The need for nitrogen is addressed by requirements set for protein, which is composed of nitrogen-containing amino acids. Sulfur is essential, but again does not have a recommended intake. Instead, recommended intakes are identified for the sulfur-containing amino acids methionine and cysteine.\n\nThe essential nutrient elements for humans, listed in order of Recommended Dietary Allowance (expressed as a mass), are potassium, chlorine, sodium, calcium, phosphorus, magnesium, iron, zinc, manganese, copper, iodine, chromium, molybdenum, selenium and cobalt (the last as a component of vitamin B). There are other minerals which are essential for some plants and animals, but may or may not be essential for humans, such as boron and silicon.\n\nConditionally essential nutrients are certain organic molecules that can normally be synthesized by an organism, but under certain conditions in insufficient quantities. In humans, such conditions include premature birth, limited nutrient intake, rapid growth, and certain disease states. Choline, inositol, taurine, arginine, glutamine and nucleotides are classified as conditionally essential and are particularly important in neonatal diet and metabolism.\n\nNon-essential nutrients are substances within foods that can have a significant impact on health; these substances can be beneficial or toxic. For example, dietary fiber is not absorbed in the human digestive tract, but is important in maintaining the bulk of a bowel movement to avoid constipation. A subset of dietary fiber, soluble fiber, can be metabolized by bacteria residing in the large intestine. Soluble fiber is marketed as serving a prebiotic functionpromoting \"healthy\" intestinal bacteria. Bacterial metabolism of soluble fiber also produces short-chain fatty acids like butyric acid, which may be absorbed into intestinal cells as a source of calories.\n\nEthanol (CHOH) supplies calories. For spirits (vodka, gin, rum, etc.) a standard serving in the United States is , which at 40%ethanol (80proof) would be 14 grams and 98 calories. At 50%alcohol, 17.5grams and 122.5calories. Wine and beer contain a similar amount of ethanol in servings of and , respectively, but these beverages also contain non-ethanol calories. A 5-ounce serving of wine contains 100 to 130 calories. A 12-ounce serving of beer contains 95 to 200 calories. According to the U.S. Department of Agriculture, based on NHANES 2013–2014 surveys, women ages 20 and up consume on average 6.8grams of alcohol per day and men consume on average 15.5 grams per day. Ignoring the non-alcohol contribution of those beverages, the average ethanol calorie contributions are 48 and 108 cal/day, respectively. Alcoholic beverages are considered empty calorie foods because, other than calories, they contribute no essential nutrients.\n\nBy definition, phytochemicals include all nutritional and non-nutritional components of edible plants. Included as nutritional constituents are provitamin A carotenoids, whereas those without nutrient status are diverse polyphenols, flavonoids, resveratrol, and lignans – often claimed to have antioxidant effects – that are present in numerous plant foods. A number of phytochemical compounds are under preliminary research for their potential effects on human diseases and health. However, the qualification for nutrient status of compounds with poorly defined properties \"in vivo\" is that they must first be defined with a Dietary Reference Intake level to enable accurate food labeling, a condition not established for most phytochemicals that are claimed to be antioxidant nutrients.\n\n\"See Vitamin, Mineral (nutrient), Protein (nutrient)\"\n\nAn inadequate amount of a nutrient is a deficiency. Deficiencies can be due to a number of causes including an inadequacy in nutrient intake, called a dietary deficiency, or any of several conditions that interfere with the utilization of a nutrient within an organism. Some of the conditions that can interfere with nutrient utilization include problems with nutrient absorption, substances that cause a greater than normal need for a nutrient, conditions that cause nutrient destruction, and conditions that cause greater nutrient excretion. Nutrient toxicity occurs when excess consumption of a nutrient does harm to an organism.\n\nIn the United States and Canada, recommended dietary intake levels of essential nutrients are based on the minimum level that \"will maintain a defined level of nutriture in an individual\", a definition somewhat different from that used by the World Health Organization and Food and Agriculture Organization of a \"basal requirement to indicate the level of intake needed to prevent pathologically relevant and clinically detectable signs of a dietary inadequacy\".\n\nIn setting human nutrient guidelines, government organizations do not necessarily agree on amounts needed to avoid deficiency or maximum amounts to avoid the risk of toxicity. For example, for vitamin C, recommended intakes range from 40 mg/day in India to 155 mg/day for the European Union. The table below shows U.S. Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for vitamins and minerals, PRIs for the European Union (same concept as RDAs), followed by what three government organizations deem to be the safe upper intake. RDAs are set higher than EARs to cover people with higher than average needs. Adequate Intakes (AIs) are set when there is not sufficient information to establish EARs and RDAs. Governments are slow to revise information of this nature. For the U.S. values, with the exception of calcium and vitamin D, all of the data date to 1997-2004.\n\n\nEAR U.S. Estimated Average Requirements.\n\nRDA U.S. Recommended Dietary Allowances; higher for adults than for children, and may be even higher for women who are pregnant or lactating.\n\nAI U.S. Adequate Intake; AIs established when there is not sufficient information to set EARs and RDAs.\n\nPRI Population Reference Intake is European Union equivalent of RDA; higher for adults than for children, and may be even higher for women who are pregnant or lactating. For Thiamin and Niacin the PRIs are expressed as amounts per MJ of calories consumed. MJ = megajoule = 239 food calories.\n\nUpper Limit Tolerable upper intake levels.\n\nND ULs have not been determined.\n\nNE EARs, PRIs or AIs have not yet been established or will not be (EU does not consider chromium an essential nutrient).\n\n"}
{"id": "2124889", "url": "https://en.wikipedia.org/wiki?curid=2124889", "title": "OGS Telescope", "text": "OGS Telescope\n\nThe Optical Ground Station (OGS) telescope (or ESA Space Debris Telescope), installed in the Teide Observatory, has been built by Carl Zeiss. The telescope is ESA's Optical Ground Station forming a part of the Artemis experiment and is operated by the IAC (Instituto de Astrofísica de Canarias) and Ataman Science S.L.U.\n\nThe telescope is a 1 m Ritchey-Chretien / Coudé telescope with field of view of 0.7 degrees, supported by an English-built mount inside a dome 12.5 metre in diameter. Its main purposes are: \n\nIt is equipped with a cryogenically cooled mosaic CCD-Camera of 4k*4k pixels. The detection threshold is between 19th and 21st magnitude, which corresponds to a capability to detect space debris objects as small as 10 cm in the geostationary ring. As a large part of the observation time is dedicated to space debris surveys, in particular the observation of space debris in the geostationary ring and in geostationary transfer orbits, the term ESA Space Debris Telescope became used very frequently. Space debris surveys are carried out every month, centered on New Moon.\n\nSince 2006, the telescope has also been used as a receiver station for quantum communication experiments (such as testing Bell inequalities, quantum cryptography, quantum teleportation), with the sender station being 143 km away in the observatory on La Palma. This is possible because this telescope can be tilted to a near-horizontal position to point it at La Palma, which many large astronomical telescopes are unable to do.\n\nEAS OGS has been credited by the Minor Planet Center with the discovery of 37 minor planets. These are:\n\n\n"}
{"id": "2801560", "url": "https://en.wikipedia.org/wiki?curid=2801560", "title": "Ocean acidification", "text": "Ocean acidification\n\nOcean acidification is the ongoing decrease in the pH of the Earth's oceans, caused by the uptake of carbon dioxide () from the atmosphere. Seawater is slightly basic (meaning pH > 7), and ocean acidification involves a shift towards pH-neutral conditions rather than a transition to acidic conditions (pH < 7). An estimated 30–40% of the carbon dioxide from human activity released into the atmosphere dissolves into oceans, rivers and lakes. To achieve chemical equilibrium, some of it reacts with the water to form carbonic acid. Some of the resulting carbonic acid molecules dissociate into a bicarbonate ion and a hydrogen ion, thus increasing ocean acidity (H ion concentration). Between 1751 and 1996, surface ocean pH is estimated to have decreased from approximately 8.25 to 8.14, representing an increase of almost 30% in H ion concentration in the world's oceans. Earth System Models project that, within the last decade, ocean acidity exceeded historical analogues and, in combination with other ocean biogeochemical changes, could undermine the functioning of marine ecosystems and disrupt the provision of many goods and services associated with the ocean beginning as early as 2100.\n\nIncreasing acidity is thought to have a range of potentially harmful consequences for marine organisms, such as depressing metabolic rates and immune responses in some organisms, and causing coral bleaching. By increasing the presence of free hydrogen ions, the additional carbonic acid that forms in the oceans ultimately results in the conversion of carbonate ions into bicarbonate ions. Ocean alkalinity (roughly equal to [HCO] + 2[CO]) is not changed by the process, or may increase over long time periods due to carbonate dissolution. This net decrease in the amount of carbonate ions available may make it more difficult for marine calcifying organisms, such as coral and some plankton, to form biogenic calcium carbonate, and such structures become vulnerable to dissolution. Ongoing acidification of the oceans may threaten future food chains linked with the oceans. As members of the InterAcademy Panel, 105 science academies have issued a statement on ocean acidification recommending that by 2050, global emissions be reduced by at least 50% compared to the 1990 level.\n\nWhile ongoing ocean acidification is at least partially anthropogenic in origin, it has occurred previously in Earth's history. The most notable example is the Paleocene-Eocene Thermal Maximum (PETM), which occurred approximately 56 million years ago when massive amounts of carbon entered the ocean and atmosphere, and led to the dissolution of carbonate sediments in all ocean basins.\n\nOcean acidification has been compared to anthropogenic climate change and called the \"evil twin of global warming\" and \"the other problem\". Freshwater bodies also appear to be acidifying, although this is a more complex and less obvious phenomenon.\n\nThe carbon cycle describes the fluxes of carbon dioxide () between the oceans, terrestrial biosphere, lithosphere, and the atmosphere. Human activities such as the combustion of fossil fuels and land use changes have led to a new flux of into the atmosphere. About 45% has remained in the atmosphere; most of the rest has been taken up by the oceans, with some taken up by terrestrial plants.\n\nThe carbon cycle involves both organic compounds such as cellulose and inorganic carbon compounds such as carbon dioxide, carbonate ion, and bicarbonate ion. The inorganic compounds are particularly relevant when discussing ocean acidification for they include many forms of dissolved present in the Earth's oceans.\n\nWhen dissolves, it reacts with water to form a balance of ionic and non-ionic chemical species: dissolved free carbon dioxide (), carbonic acid (), bicarbonate () and carbonate (). The ratio of these species depends on factors such as seawater temperature, pressure and salinity (as shown in a Bjerrum plot). These different forms of dissolved inorganic carbon are transferred from an ocean's surface to its interior by the ocean's solubility pump.\n\nThe resistance of an area of ocean to absorbing atmospheric is known as the Revelle factor.\n\nDissolving in seawater increases the hydrogen ion () concentration in the ocean, and thus decreases ocean pH, as follows:\n\nCaldeira and Wickett (2003) placed the rate and magnitude of modern ocean acidification changes in the context of probable historical changes during the last 300 million years.\n\nSince the industrial revolution began, it is estimated that surface ocean pH has dropped by slightly more than 0.1 units on the logarithmic scale of pH, representing about a 29% increase in . It is expected to drop by a further 0.3 to 0.5 pH units (an additional doubling to tripling of today's post-industrial acid concentrations) by 2100 as the oceans absorb more anthropogenic , the impacts being most severe for coral reefs and the Southern Ocean. These changes are predicted to accelerate as more anthropogenic is released to the atmosphere and taken up by the oceans. The degree of change to ocean chemistry, including ocean pH, will depend on the mitigation and emissions pathways taken by society.\n\nAlthough the largest changes are expected in the future, a report from NOAA scientists found large quantities of water undersaturated in aragonite are already upwelling close to the Pacific continental shelf area of North America. Continental shelves play an important role in marine ecosystems since most marine organisms live or are spawned there, and though the study only dealt with the area from Vancouver to Northern California, the authors suggest that other shelf areas may be experiencing similar effects.\n\nOne of the first detailed datasets to examine how pH varied over 8 years at a specific north temperate coastal location found that acidification had strong links to \"in situ\" benthic species dynamics and that the variation in ocean pH may cause calcareous species to perform more poorly than noncalcareous species in years with low pH and predicts consequences for near-shore benthic ecosystems. Thomas Lovejoy, former chief biodiversity advisor to the World Bank, has suggested that \"the acidity of the oceans will more than double in the next 40 years. He says this rate is 100 times faster than any changes in ocean acidity in the last 20 million years, making it unlikely that marine life can somehow adapt to the changes.\" It is predicted that, by the year 2100, If co-occurring biogeochemical changes influence the delivery of ocean goods and services, then they could also have a considerable effect on human welfare for those who rely heavily on the ocean for food, jobs, and revenues.\n\nCurrent rates of ocean acidification have been compared with the greenhouse event at the Paleocene–Eocene boundary (about 55 million years ago) when surface ocean temperatures rose by 5–6 degrees Celsius. No catastrophe was seen in surface ecosystems, yet bottom-dwelling organisms in the deep ocean experienced a major extinction. The current acidification is on a path to reach levels higher than any seen in the last 65 million years, and the rate of increase is about ten times the rate that preceded the Paleocene–Eocene mass extinction. The current and projected acidification has been described as an almost unprecedented geological event. A National Research Council study released in April 2010 likewise concluded that \"the level of acid in the oceans is increasing at an unprecedented rate\". A 2012 paper in the journal \"Science\" examined the geological record in an attempt to find a historical analog for current global conditions as well as those of the future. The researchers determined that the current rate of ocean acidification is faster than at any time in the past 300 million years.\n\nA review by climate scientists at the RealClimate blog, of a 2005 report by the Royal Society of the UK similarly highlighted the centrality of the \"rates\" of change in the present anthropogenic acidification process, writing:\n\"The natural pH of the ocean is determined by a need to balance the deposition and burial of on the sea floor against the influx of and into the ocean from dissolving rocks on land, called weathering. These processes stabilize the pH of the ocean, by a mechanism called compensation...The point of bringing it up again is to note that if the concentration of the atmosphere changes more slowly than this, as it always has throughout the Vostok record, the pH of the ocean will be relatively unaffected because compensation can keep up. The [present] fossil fuel acidification is much faster than natural changes, and so the acid spike will be more intense than the earth has seen in at least 800,000 years.\"\n\nIn the 15-year period 1995–2010 alone, acidity has increased 6 percent in the upper 100 meters of the Pacific Ocean from Hawaii to Alaska. According to a statement in July 2012 by Jane Lubchenco, head of the U.S. National Oceanic and Atmospheric Administration \"surface waters are changing much more rapidly than initial calculations have suggested. It's yet another reason to be very seriously concerned about the amount of carbon dioxide that is in the atmosphere now and the additional amount we continue to put out.\"\n\nA 2013 study claimed acidity was increasing at a rate 10 times faster than in any of the evolutionary crises in Earth's history. In a synthesis report published in \"Science\" in 2015, 22 leading marine scientists stated that from burning fossil fuels is changing the oceans' chemistry more rapidly than at any time since the Great Dying, Earth's most severe known extinction event, emphasizing that the 2 °C maximum temperature increase agreed upon by governments reflects too small a cut in emissions to prevent \"dramatic impacts\" on the world's oceans, with lead author Jean-Pierre Gattuso remarking that \"The ocean has been minimally considered at previous climate negotiations. Our study provides compelling arguments for a radical change at the UN conference (in Paris) on climate change\".\n\nThe rate at which ocean acidification will occur may be influenced by the rate of surface ocean warming, because the chemical equilibria that govern seawater pH are temperature-dependent. Greater seawater warming could lead to a smaller change in pH for a given increase in CO.\n\nChanges in ocean chemistry can have extensive direct and indirect effects on organisms and their habitats. One of the most important repercussions of increasing ocean acidity relates to the production of shells and plates out of calcium carbonate (). This process is called calcification and is important to the biology and survival of a wide range of marine organisms. Calcification involves the precipitation of dissolved ions into solid structures, such as coccoliths. After they are formed, such structures are vulnerable to dissolution unless the surrounding seawater contains saturating concentrations of carbonate ions (CO).\n\nOf the extra carbon dioxide added into the oceans, some remains as dissolved carbon dioxide, while the rest contributes towards making additional bicarbonate (and additional carbonic acid). This also increases the concentration of hydrogen ions, and the percentage increase in hydrogen is larger than the percentage increase in bicarbonate, creating an imbalance in the reaction HCO CO + H. To maintain chemical equilibrium, some of the carbonate ions already in the ocean combine with some of the hydrogen ions to make further bicarbonate. Thus the ocean's concentration of carbonate ions is reduced, creating an imbalance in the reaction Ca + CO CaCO, and making the dissolution of formed structures more likely.\n\nThe increase in concentrations of dissolved carbon dioxide and bicarbonate, and reduction in carbonate, are shown in a Bjerrum plot.\n\nThe saturation state (known as Ω) of seawater for a mineral is a measure of the thermodynamic potential for the mineral to form or to dissolve, and for calcium carbonate is described by the following equation:\n\nHere Ω is the product of the concentrations (or activities) of the reacting ions that form the mineral ( and ), divided by the product of the concentrations of those ions when the mineral is at equilibrium (), that is, when the mineral is neither forming nor dissolving. In seawater, a natural horizontal boundary is formed as a result of temperature, pressure, and depth, and is known as the saturation horizon. Above this saturation horizon, Ω has a value greater than 1, and does not readily dissolve. Most calcifying organisms live in such waters. Below this depth, Ω has a value less than 1, and will dissolve. However, if its production rate is high enough to offset dissolution, can still occur where Ω is less than 1. The carbonate compensation depth occurs at the depth in the ocean where production is exceeded by dissolution.\n\nThe decrease in the concentration of CO decreases Ω, and hence makes dissolution more likely.\n\nCalcium carbonate occurs in two common polymorphs (crystalline forms): aragonite and calcite. Aragonite is much more soluble than calcite, so the aragonite saturation horizon is always nearer to the surface than the calcite saturation horizon. This also means that those organisms that produce aragonite may be more vulnerable to changes in ocean acidity than those that produce calcite. Increasing levels and the resulting lower pH of seawater decreases the saturation state of and raises the saturation horizons of both forms closer to the surface. This decrease in saturation state is believed to be one of the main factors leading to decreased calcification in marine organisms, as the inorganic precipitation of is directly proportional to its saturation state.\n\nIncreasing acidity has possibly harmful consequences, such as depressing metabolic rates in jumbo squid, depressing the immune responses of blue mussels, and coral bleaching. However it may benefit some species, for example increasing the growth rate of the sea star, \"Pisaster ochraceus\", while shelled plankton species may flourish in altered oceans.\n\nThe report \"Ocean Acidification Summary for Policymakers 2013\" describes research findings and possible impacts.\n\nAlthough the natural absorption of by the world's oceans helps mitigate the climatic effects of anthropogenic emissions of , it is believed that the resulting decrease in pH will have negative consequences, primarily for oceanic calcifying organisms. These span the food chain from autotrophs to heterotrophs and include organisms such as coccolithophores, corals, foraminifera, echinoderms, crustaceans and molluscs. As described above, under normal conditions, calcite and aragonite are stable in surface waters since the carbonate ion is at supersaturating concentrations. However, as ocean pH falls, the concentration of carbonate ions required for saturation to occur increases, and when carbonate becomes undersaturated, structures made of calcium carbonate are vulnerable to dissolution. Therefore, even if there is no change in the rate of calcification, the rate of dissolution of calcareous material increases.\n\nCorals, coccolithophore algae, coralline algae, foraminifera, shellfish and pteropods experience reduced calcification or enhanced dissolution when exposed to elevated .\n\nThe Royal Society published a comprehensive overview of ocean acidification, and its potential consequences, in June 2005. However, some studies have found different response to ocean acidification, with coccolithophore calcification and photosynthesis both increasing under elevated atmospheric p, an equal decline in primary production and calcification in response to elevated or the direction of the response varying between species. A study in 2008 examining a sediment core from the North Atlantic found that while the species composition of coccolithophorids has remained unchanged for the industrial period 1780 to 2004, the calcification of coccoliths has increased by up to 40% during the same time. A 2010 study from Stony Brook University suggested that while some areas are overharvested and other fishing grounds are being restored, because of ocean acidification it may be impossible to bring back many previous shellfish populations. While the full ecological consequences of these changes in calcification are still uncertain, it appears likely that many calcifying species will be adversely affected.\n\nWhen exposed in experiments to pH reduced by 0.2 to 0.4, larvae of a temperate brittlestar, a relative of the common sea star, fewer than 0.1 percent survived more than eight days. There is also a suggestion that a decline in the coccolithophores may have secondary effects on climate, contributing to global warming by decreasing the Earth's albedo via their effects on oceanic cloud cover. All marine ecosystems on Earth will be exposed to changes in acidification and several other ocean biogeochemical changes.\n\nThe fluid in the internal compartments where corals grow their exoskeleton is also extremely important for calcification growth. When the saturation rate of aragonite in the external seawater is at ambient levels, the corals will grow their aragonite crystals rapidly in their internal compartments, hence their exoskeleton grows rapidly. If the level of aragonite in the external seawater is lower than the ambient level, the corals have to work harder to maintain the right balance in the internal compartment. When that happens, the process of growing the crystals slows down, and this slows down the rate of how much their exoskeleton is growing. Depending on how much aragonite is in the surrounding water, the corals may even stop growing because the levels of aragonite are too low to pump into the internal compartment. They could even dissolve faster than they can make the crystals to their skeleton, depending on the aragonite levels in the surrounding water. Under the current progression of carbon emissions, around 70% of North Atlantic cold-water corals will be living in corrosive waters by 2050-60.\n\nA study conducted by the Woods Hole Oceanographic Institution in January 2018 showed that the skeletal growth of corals under acidified conditions is primarily affected by a reduced capacity to build dense exoskeletons, rather than affecting the linear extension of the exoskeleton. Using Global Climate Models, they show that the density of some species of corals could be reduced by over 20% by the end of this century.\n\nAn \"in situ\" experiment on a 400 m patch of the Great Barrier Reef to decrease seawater CO level (raise pH) to close to the preindustrial value showed a 7% increase in net calcification.\nA similar experiment to raise \"in situ\" seawater seawater CO level (lower pH) to a level expected soon after the middle of this century found that net calcification decreased 34%.\n\nOcean acidification may force some organisms to reallocate resources away from productive endpoints such as growth in order to maintain calcification.\n\nIn some places carbon dioxide bubbles out from the sea floor, locally changing the pH and other aspects of the chemistry of the seawater. Studies of these carbon dioxide seeps have documented a variety of responses by different organisms. Coral reef communities located near carbon dioxide seeps are of particular interest because of the sensitivity of some corals species to acidification. In Papua New Guinea, declining pH caused by carbon dioxide seeps is associated with declines in coral species diversity. However, in Palau carbon dioxide seeps are not associated with reduced species diversity of corals, although bioerosion of coral skeletons is much higher at low pH sites.\n\nAside from the slowing and/or reversing of calcification, organisms may suffer other adverse effects, either indirectly through negative impacts on food resources, or directly as reproductive or physiological effects. For example, the elevated oceanic levels of may produce -induced acidification of body fluids, known as hypercapnia. Also, increasing ocean acidity is believed to have a range of direct consequences. For example, increasing acidity has been observed to: reduce metabolic rates in jumbo squid; depress the immune responses of blue mussels; and make it harder for juvenile clownfish to tell apart the smells of non-predators and predators, or hear the sounds of their predators. This is possibly because ocean acidification may alter the acoustic properties of seawater, allowing sound to propagate further, and increasing ocean noise. This impacts all animals that use sound for echolocation or communication. Atlantic longfin squid eggs took longer to hatch in acidified water, and the squid's statolith was smaller and malformed in animals placed in sea water with a lower pH. The lower PH was simulated with 20-30 times the normal amount of . However, as with calcification, as yet there is not a full understanding of these processes in marine organisms or ecosystems.\n\nAnother possible effect would be an increase in red tide events, which could contribute to the accumulation of toxins (domoic acid, brevetoxin, saxitoxin) in small organisms such as anchovies and shellfish, in turn increasing occurrences of amnesic shellfish poisoning, neurotoxic shellfish poisoning and paralytic shellfish poisoning.\n\nWhile the full implications of elevated CO on marine ecosystems are still being documented, there is a substantial body of research showing that a combination of ocean acidification and elevated ocean temperature, driven mainly by CO and other greenhouse gas emissions, have a compounded effect on marine life and the ocean environment. This effect far exceeds the individual harmful impact of either. In addition, ocean warming exacerbates ocean deoxygenation, which is an additional stressor on marine organisms, by increasing ocean stratification, through density and solubility effects, thus limiting nutrients, while at the same time increasing metabolic demand.\n\nMeta analyses have quantified the direction and magnitude of the harmful effects of ocean acidification, warming and deoxygenation on the ocean. These meta-analyses have been further tested by mesocosm studies that simulated the interaction of these stressors and found a catastrophic effect on the marine food web, i.e. that the increases in consumption from thermal stress more than negates any primary producer to herbivore increase from elevated CO.\n\nLeaving aside direct biological effects, it is expected that ocean acidification in the future will lead to a significant decrease in the burial of carbonate sediments for several centuries, and even the dissolution of existing carbonate sediments. This will cause an elevation of ocean alkalinity, leading to the enhancement of the ocean as a reservoir for with implications for climate change as more leaves the atmosphere for the ocean.\n\nThe threat of acidification includes a decline in commercial fisheries and in the Arctic tourism industry and economy. Commercial fisheries are threatened because acidification harms calcifying organisms which form the base of the Arctic food webs.\n\nPteropods and brittle stars both form the base of the Arctic food webs and are both seriously damaged from acidification. Pteropods shells dissolve with increasing acidification and the brittle stars lose muscle mass when re-growing appendages. For pteropods to create shells they require aragonite which is produced through carbonate ions and dissolved calcium. Pteropods are severely affected because increasing acidification levels have steadily decreased the amount of water supersaturated with carbonate which is needed for aragonite creation. Arctic waters are changing so rapidly that they will become undersaturated with aragonite as early as 2016. Additionally the brittle star's eggs die within a few days when exposed to expected conditions resulting from Arctic acidification. Acidification threatens to destroy Arctic food webs from the base up. Arctic food webs are considered simple, meaning there are few steps in the food chain from small organisms to larger predators. For example, pteropods are \"a key prey item of a number of higher predators – larger plankton, fish, seabirds, whales\". Both pteropods and sea stars serve as a substantial food source and their removal from the simple food web would pose a serious threat to the whole ecosystem. The effects on the calcifying organisms at the base of the food webs could potentially destroy fisheries. The value of fish caught from US commercial fisheries in 2007 was valued at $3.8 billion and of that 73% was derived from calcifiers and their direct predators. Other organisms are directly harmed as a result of acidification. For example, decrease in the growth of marine calcifiers such as the American lobster, ocean quahog, and scallops means there is less shellfish meat available for sale and consumption. Red king crab fisheries are also at a serious threat because crabs are calcifiers and rely on carbonate ions for shell development. Baby red king crab when exposed to increased acidification levels experienced 100% mortality after 95 days. In 2006, red king crab accounted for 23% of the total guideline harvest levels and a serious decline in red crab population would threaten the crab harvesting industry. Several ocean goods and services are likely to be undermined by future ocean acidification potentially affecting the livelihoods of some 400 to 800 million people depending upon the emission scenario.\n\nAcidification could damage the Arctic tourism economy and affect the way of life of indigenous peoples. A major pillar of Arctic tourism is the sport fishing and hunting industry. The sport fishing industry is threatened by collapsing food webs which provide food for the prized fish. A decline in tourism lowers revenue input in the area, and threatens the economies that are increasingly dependent on tourism. The rapid decrease or disappearance of marine life could also affect the diet of Indigenous peoples.\n\nThe Arctic ocean has experienced drastic change over the years due to global warming. It has been known that the Arctic ocean acidity levels have been increasing and at twice the rate compared to the Pacific and Atlantic oceans. The loss of sea ice has been connected to a decrease in pH levels in the ocean water. Sea ice has experienced an extreme reduction over the past 30 years, forming a minimum area of 2.9×106 km at the end of the boreal summer of 2007, 47%, less than in 1980. Sea ice limits the air-sea gas exchange with carbon dioxide. With less water completely exposed to the atmosphere, the levels of carbon dioxide gas in the water remain low. The Arctic ocean should have low carbon dioxide levels due to intense cooling, run off of fresh water and photosynthesis from marine organisms. However, the decrease of sea ice over the years due to global warming has limited freshwater runoff and has exposed a higher percentage of the ocean surface to the atmosphere. The increase of carbon dioxide in the water decreases the pH of the ocean causing ocean acidification. The decrease in sea ice has also allowed more Pacific water to flow into in the Arctic ocean during the winter, this is called Pacific winter water. The Pacific water flows into the Arctic ocean carrying additional amounts of carbon dioxide by being exposed to the atmosphere and absorbing carbon dioxide from decaying organic matter and from sediments.\n\nThe Arctic ocean pH levels are rapidly decreasing because not only is the ocean water absorbing more carbon dioxide due to increased surface area exposure as a result of a decrease in sea ice. It also has large amounts of carbon dioxide being transferred to the Arctic from the Pacific ocean.\nCold water is able to absorb higher amounts of carbon dioxide compared to warm water. The solubility of gases decreases in relation to increasing temperature. Cold water bodies are absorbing the increasing amount of carbon dioxide in the atmosphere and becoming known as carbon sinks. The increasing amount of carbon dioxide in the water is putting many organisms at risk as they are affected by the increase of acidity in the ocean water.\n\nOrganisms in Arctic waters are already challenged with stressors of living in the arctic ocean, such as dealing with cold temperatures, and it is thought that because of this, additional stressors such as ocean acidification, will cause ocean acidification effects on marine organisms to appear first in the Arctic. There exists a significant variation in the sensitivity of marine organisms to increased ocean acidification. Calcifying organisms generally exhibit larger negative responses from ocean acidification than non‐calcifying organisms across numerous response variables, with the exception of crustaceans, which calcify but were not negatively affected. The acidification of the Arctic ocean will impact these marine calcifiers in several different ways.\n\nThe uptake of CO₂ by seawater increases the concentration of hydrogen ions, which lowers pH and, in changing the chemical equilibrium of the inorganic carbon system, reduces the concentration of carbonate ions (CO₃²⁻). Carbonate ions are required by marine calcifying organisms such as plankton, shellfish, and fish to produce calcium carbonate (CaCO₃) shells and skeletons. For either aragonite or calcite, the two polymorphs of CaCO₃ produced by marine organisms, the saturation state of CaCO₃ in ocean water is expressed by the product of the concentrations of CO₂²⁻ and Ca²⁺ in seawater relative to the stoichiometric solubility product at a given temperature, salinity, and pressure. Waters which are saturated in CaCO₃ are favourable to precipitation and formation of CaCO₃ shells and skeletons, but waters which are undersaturated are corrosive to CaCO₃ shells, and in the absence of protective mechanisms, dissolution of calcium carbonate will occur. Because colder arctic water absorbs more CO₂, the concentration of CO₃²⁻ is reduced, therefore the saturation of calcium carbonate is lower in high-latitude oceans than it is in tropical or temperate oceans. In model simulations of the Arctic Ocean, it is predicted that aragonite saturation will decrease, because of an increased amount of freshwater input from melting sea ice and increased carbon uptake as a result of sea ice retreat. This simulation predicts that Arctic surface waters will become undersaturated with aragonite within a decade. The undersaturation of aragonite will cause the shells of organisms which are constructed from aragonite to dissolve. This would have a profound effect on a large variety of marine organisms and has the potential to do devastating damage to keystone species and to the marine food web in the arctic ocean. Laboratory experiments on various marine biota in an elevated CO₂ environment show that changes in aragonite saturation cause substantial changes in overall calcification rates for many species of marine organisms, including coccolithophore, foraminifera, pteropods, mussels, and clams.\n\nAlthough the undersaturation of arctic water has been proven to have an effect on the ability of organisms to precipitate their shells, recent studies have shown that the calcification rate of calcifiers, such as corals, coccolithophores, foraminiferans and bivalves, decrease with increasing pCO₂, even in seawater supersaturated with respect to CaCO₃. Additionally, increased pCO₂ has been found to have complex effects on the physiology, growth and reproductive success of various marine calcifiers. CO₂ tolerance seems to differ between various marine organisms, as well as differences in CO₂ tolerance at different life cycle stages (e.g. larva and adult). The first stage in the life cycle of marine calcifiers which are at a serious risk by high CO2 content is the planktonic larval stage. The larval development of several marine species, primarily sea urchins and bivalves, are highly affected by elevations of seawater pCO₂. In laboratory tests, numerous sea urchin embryos were reared under different CO₂ concentrations until they developed to the larval stage. It was found that once reaching this stage, larval and arm sizes were significantly smaller, as well as abnormal skeleton morphology was noted with increasing pCO₂.\n\nSimilar findings have been found in CO₂ treated-mussel larvae, which showed a larval size decrease of about 20% and showed morphological abnormalities such as convex hinges, weaker and thinner shells and protrusion of mantle. The larval body size also impacts the encounter and clearance rates of food particles, and if larval shells are smaller or deformed, these larvae are more prone to starvation. In addition, CaCO₃ structures also serve vital functions for calcified larvae, such as defence against predation, as well as roles in feeding, buoyancy control and pH regulation. Another example of a species which may be seriously impacted by ocean acidification is Pteropods, which are shelled pelagic molluscs which play an important role in the food-web of various ecosystems. Since they harbour an aragonitic shell, they could be very sensitive to ocean acidification driven by the increase of anthropogenic CO₂ emissions.Laboratory tests showed that calcification exhibits a 28% decrease at the pH value of the Arctic ocean expected for the year 2100, compared to the present pH value. This 28% decline of calcification in the lower pH condition is within the range reported also for other calcifying organisms such as corals. In contrast with sea urchin and bivalve larvae, corals and marine shrimps are more severely impacted by ocean acidification after settlement, while they developed into the polyp stage. From laboratory tests, the morphology of the CO₂-treated polyp endoskeleton of corals was disturbed and malformed compared to the radial pattern of control polyps.\n\nThis variability in the impact of ocean acidification on different life cycle stages of different organisms can be partially explained by the fact that most echinoderms and mollusks start shell and skeleton synthesis at their larval stage, whereas corals start at the settlement stage. Hence, these stages are highly susceptible to the potential effects of ocean acidification. Most calcifiers, such as corals, echinoderms, bivalves and crustaceans, play important roles in coastal ecosystems as keystone species, bioturbators and ecosystem engineers. The food web in the arctic ocean is somewhat truncated, meaning it is short and simple. Any impacts to key species in the food web can cause exponentially devastating effects on the rest on the food chain as a whole, as they will no longer have a reliable food source. If these larger organisms no longer have any source of nutrients, they too will eventually die off, and the entire Arctic ocean ecosystem will be affected. This would have a huge impact on the arctic people who catch arctic fish for a living, as well as the economic repercussions which would follow such a major shortage of food and living income for these families.\n\nMembers of the InterAcademy Panel recommended that by 2050, global anthropogenic emissions be reduced less than 50% of the 1990 level. The 2009 statement also called on world leaders to:\nStabilizing atmospheric concentrations at 450 ppm would require near-term emissions reductions, with steeper reductions over time.\n\nThe German Advisory Council on Global Change stated:In order to prevent disruption of the calcification of marine organisms and the resultant risk of fundamentally altering marine food webs, the following guard rail should be obeyed: the pH of near surface waters should not drop more than 0.2 units below the pre-industrial average value in any larger ocean region (nor in the global mean).\n\nOne policy target related to ocean acidity is the magnitude of future global warming. Parties to the United Nations Framework Convention on Climate Change (UNFCCC) adopted a target of limiting warming to below 2 °C, relative to the pre-industrial level. Meeting this target would require substantial reductions in anthropogenic emissions.\n\nLimiting global warming to below 2 °C would imply a reduction in surface ocean pH of 0.16 from pre-industrial levels. This would represent a substantial decline in surface ocean pH.\n\nOn September 25, 2015, USEPA denied a June 30, 2015, citizens petition that asked EPA to regulate under TSCA in order to mitigate ocean acidification. In the denial, EPA said that risks from ocean acidification were being \"more efficiently and effectively addressed\" under domestic actions, e.g., under the Presidential Climate Action Plan, and that multiple avenues are being pursued to work with and in other nations to reduce emissions and deforestation and promote clean energy and energy efficiency.\n\nOn March 28, 2017 the US by executive order rescinded the Climate Action Plan. On June 1, 2017 it was announced the US would withdraw from the Paris accords, and on June 12, 2017 that the US would abstain from the G7 Climate Change Pledge, two major international efforts to reduce emissions.\n\nClimate engineering (mitigating temperature or pH effects of emissions) has been proposed as a possible response to ocean acidification. The IAP (2009) statement cautioned against climate engineering as a policy response:Mitigation approaches such as adding chemicals to counter the effects of acidification are likely to be expensive, only partly effective and only at a very local scale, and may pose additional unanticipated risks to the marine environment. There has been very little research on the feasibility and impacts of these approaches. Substantial research is needed before these techniques could be applied.\nReports by the WGBU (2006), the UK's Royal Society (2009), and the US National Research Council (2011) warned of the potential risks and difficulties associated with climate engineering.\n\nIron fertilization of the ocean could stimulate photosynthesis in phytoplankton (see Iron hypothesis). The phytoplankton would convert the ocean's dissolved carbon dioxide into carbohydrate and oxygen gas, some of which would sink into the deeper ocean before oxidizing. More than a dozen open-sea experiments confirmed that adding iron to the ocean increases photosynthesis in phytoplankton by up to 30 times. While this approach has been proposed as a potential solution to the ocean acidification problem, mitigation of surface ocean acidification might increase acidification in the less-inhabited deep ocean.\n\nA report by the UK's Royal Society (2009) reviewed the approach for effectiveness, affordability, timeliness and safety. The rating for affordability was \"medium\", or \"not expected to be very cost-effective\". For the other three criteria, the ratings ranged from \"low\" to \"very low\" (i.e., not good). For example, in regards to safety, the report found a \"[high] potential for undesirable ecological side effects\", and that ocean fertilization \"may increase anoxic regions of ocean ('dead zones')\".\n\n\n\n\n"}
{"id": "3241094", "url": "https://en.wikipedia.org/wiki?curid=3241094", "title": "Orpheus and Eurydice (rock opera)", "text": "Orpheus and Eurydice (rock opera)\n\nOrpheus and Eurydice is a 1975 rock opera album by Russian composer Alexander Zhurbin.\n\nIn 2003, the Opera was included in the Guinness book records as the musical, the maximum number of times played in one team (at the time of registration of the record the play was performed 2350 times).\n"}
{"id": "58494813", "url": "https://en.wikipedia.org/wiki?curid=58494813", "title": "Pacific Northwest water resource region", "text": "Pacific Northwest water resource region\n\nThe Pacific Northwest water resource region is one of 21 major geographic areas, or regions, in the first level of classification used by the United States Geological Survey to divide and sub-divide the United States into successively smaller hydrologic units. These geographic areas contain either the drainage area of a major river, or the combined drainage areas of a series of rivers.\n\nThe Pacific Northwest region, which is listed with a 2-digit hydrologic unit code (HUC) of 17, has an approximate size of , and consists of 12 subregions, which are listed with the 4-digit HUCs 1701 through 1706.\n\nThis region includes the drainage within the United States that ultimately discharges into: (a) the Strait of Georgia and of Strait of Juan de Fuca, and (b) the Pacific Ocean within the states of Oregon and Washington; and that part of the Great Basin whose discharge is into the state of Oregon. Includes all of Washington and parts of California, Idaho, Montana, Nevada, Oregon, Utah, and Wyoming.\n\n"}
{"id": "1231776", "url": "https://en.wikipedia.org/wiki?curid=1231776", "title": "Photoionization", "text": "Photoionization\n\nPhotoionization is the physical process in which an ion is formed from the interaction of a photon with an atom or molecule.\n\nNot every photon which encounters an atom or ion will photoionize it. The probability of photoionization is related to the photoionization cross-section, which depends on the energy of the photon and the target being considered. For photon energies below the ionization threshold, the photoionization cross-section is near zero. But with the development of pulsed lasers it has become possible to create extremely intense, coherent light where multi-photon ionization may occur. At even higher intensities (around of infrared or visible light), non-perturbative phenomena such as \"barrier suppression ionization\" and \"rescattering ionization\" are observed.\n\nSeveral photons of energy below the ionization threshold may actually combine their energies to ionize an atom. This probability decreases rapidly with the number of photons required, but the development of very intense, pulsed lasers still makes it possible. In the perturbative regime (below about 10 W/cm at optical frequencies), the probability of absorbing \"N\" photons depends on the laser-light intensity \"I\" as \"I\". For higher intensities, this dependence becomes invalid due to the then occurring AC Stark effect.\n\nResonance-enhanced multiphoton ionization (REMPI) is a technique applied to the spectroscopy of atoms and small molecules in which a tunable laser can be used to access an excited intermediate state.\n\nAbove threshold ionization (ATI) is an extension of multi-photon ionization where even more photons are absorbed than actually would be necessary to ionize the atom. The excess energy gives the released electron higher kinetic energy than the usual case of just-above threshold ionization. More precisely, The system will have multiple peaks in its photoelectron spectrum which are separated by the photon energies, this indicates that the emitted electron has more kinetic energy than in the normal (lowest possible number of photons) ionization case. The electrons released from the target will have approximately an integer number of photon-energies more kinetic energy. \n\nWhen either the laser intensity is further increased or a longer wavelength is applied as compared with the regime in which multi-photon ionization takes place, a quasi-stationary approach can be used and results in the distortion of the atomic potential in such a way that only a relatively low and narrow barrier between a bound state and the continuum states remains. Then, the electron can tunnel through or for larger distortions even overcome this barrier. These phenomena are called tunnel ionization and over-the-barrier ionization, respectively.\n\n\n"}
{"id": "335223", "url": "https://en.wikipedia.org/wiki?curid=335223", "title": "Plastination", "text": "Plastination\n\nPlastination is a technique or process used in anatomy to preserve bodies or body parts, first developed by Gunther von Hagens in 1977. The water and fat are replaced by certain plastics, yielding specimens that can be touched, do not smell or decay, and even retain most properties of the original sample.\n\nFour steps are used in the standard process of plastination: fixation, dehydration, forced impregnation in a vacuum, and hardening. Water and lipid tissues are replaced by curable polymers, which include silicone, epoxy, and polyester-copolymer.\n\nThe first step of plastination, fixation, frequently uses a formaldehyde-based solution, and serves two functions. Dissecting the specimen to show specific anatomical elements can be time consuming. Formaldehyde or other preserving solutions help prevent decomposition of the tissues. They may also confer a degree of rigidity. This can be beneficial in maintaining the shape or arrangement of a specimen. A stomach might be inflated or a leg bent at the knee, for example.\n\nAfter any necessary dissections have taken place, the specimen is placed in a bath of acetone (freezing point −95° C) at −20° to −30° C. The volume of the bath should be 10 times that of the specimen. The acetone is renewed two times over the course of six weeks. The acetone draws out all the water and replaces it inside the cells.\n\nIn the third step, the specimen is then placed in a bath of liquid polymer, such as silicone rubber, polyester, or epoxy resin. By creating a vacuum, the acetone is made to boil at a low temperature. As the acetone vaporizes and leaves the cells, it draws the liquid polymer in behind it, leaving a cell filled with liquid plastic.\n\nThe plastic must then be cured with gas, heat, or ultraviolet light, to harden it.\n\nA specimen can vary from a full human body to a small piece of an animal organ, and they are known as 'plastinates'. Once plastinated, the specimens and bodies are further manipulated and positioned prior to curing (hardening) of the polymer chains.\n\nIn November 1979, Gunther von Hagens applied for a German patent, proposing the idea of preserving animal and vegetable tissues permanently by synthetic resin impregnation. Since then, von Hagens has applied for further US patents regarding work on preserving biological tissues with polymers.\n\nWith the success of his patents, von Hagens went on to form the Institute for Plastination in Heidelberg, Germany in 1993. The Institute for Plastination, along with von Hagens, made their first showing of plastinated bodies in Japan in 1995, which drew more than three million visitors. The institute maintains three international centres of plastination, in Germany, Kyrgyzstan, and China.\nOther methods have been in place for thousands of years to halt the decomposition of the body. Mummification used by the ancient Egyptians is a widely-known method which involves the removal of body fluid and wrapping the body in linens. Prior to mummification, Egyptians would lay the body in a shallow pit in the desert and allow the sun to dehydrate the body.\n\nFormalin, an important solution to body preservation, was introduced in 1896 to help with body preservation. Soon to follow formalin, color-preserving embalming solutions were developed to preserve life-like color and flexibility to aid in the study of the body.\n\nParaffin impregnation was introduced in 1925, and the embedding of organs in plastic was developed in the 1960s.\n\nBody preservation methods current to the 21st century are cryopreservation, which involves the cooling of the body to very low temperatures to preserve the body tissues, plastination, and embalming.\n\nOther methods used in modern times include the Silicone S 10 Standard Procedure, the Cor-Tech Room temperature procedure, the Epoxy E 12 procedure, and the Polyester P 35 (P 40) procedure. The Silicone S 10 is the procedure most often used in plastination and creates opaque, natural-looking specimen. Dow Corning Corporation's Cor-Tech Room Temperature Procedure is designed to allow plastination of specimen at room temperature to various degrees of flexibility using three combinations of polymer, crosslinker, and catalyst. According to the International Society for Plastination, the Epoxy E 12 procedure is used \"for thin, transparent, and firm body and organ slices\", while the Polyster P 35 (P 40) preserves \"semitransparent and firm brain slices\". Samples are prepared for fixation through the first method by deep freezing, while the second method works best following 4–6 weeks of preparation in a formaldehyde mixture.\n\nPlastination is useful in anatomy, serving as models and teaching tools. It is used at more than 40 medical and dental schools throughout the world as an adjunct to anatomical dissection.\n\nStudents enrolled in introductory animal science courses at many universities learn animal science through collections of multispecies large-animal specimens. Plastination allows students to have hands-on experience in this field, without exposure to chemicals such as formalin. For example, plastinated canine gastrointestinal tracts are used to help in the teaching of endoscopic technique and anatomy. The plastinated specimens retain their dilated conformation by a positive pressure air flow during the curing process, which allows them to be used to teach both endoscopic technique and gastrointestinal anatomy.\n\nWith the use of plastination as a teaching method of animal science, fewer animals have to be killed for research, as the plastination process allows specimens to be studied for a long time.\n\nTTT sheet plastinates for school teaching and lay instruction provide a thorough impression of the complexity of an animal body in just one specimen.\nNorth Carolina State University's College of Veterinary Medicine in Raleigh, North Carolina, uses both plastic coating (PC) and plastination (PN) to investigate and compare the difference in the two methods. The PC method was simple and inexpensive, but the PN specimens were more flexible, durable, and life-like than those preserved by the PC method. The use of plastination allowed the use of many body parts such as muscle, nerves, bones, ligaments, and central nervous system to be preserved.\n\nThe University of Texas Health Science Center at San Antonio was the first school in the United States to use this technique to prepare gross organ specimens for use in teaching. The New York University College of Dentistry., Philadelphia College of Osteopathic Medicine, University of Warwick, and University of Northumbria use collections of plastinates as teaching aids. The University of Vienna has its own plastination laboratory.\n\nConcern over consent of bodies being used in the plastination process has arisen. Over 20 years ago, von Hagens set up a body donation program in Germany and has signed over 9,000 donors into the plastinate program: 531 have already died. The program has reported an average of one body a day being released to the plastination process. About 90% of the donors registered are German. Although von Hagens says he follows strict consent procedures for whole-body specimens, he maintains, \"consent is not important for body parts\". Von Hagens' body donations are now being managed by the Institute for Plastination (IfP) established in 1993.\nA number of religious sects prohibit organ donation. Ultra-Orthodox Jews oppose \"post mortem\" organ donation, and try to pass laws against unclaimed cadavers being used in research.\nA number of religious organizations, including Catholic and Jewish object to the display of plastinated body parts at public exhibitions.\n\nFor the first 20 years, plastination was used to preserve small specimens for medical study. In the early 1990s, the equipment was developed to make plastinating whole body specimens possible, each specimen taking up to 1,500 man-hours to prepare. The first exhibition of whole bodies was displayed by von Hagens in Japan in 1995.\n\nOver the next two years, Von Hagens developed the \"Körperwelten\" (Body Worlds) public exhibitions, showing whole bodies plastinated in life-like poses and dissected to show various structures and systems of human anatomy. The earliest exhibitions were presented in the Far East and in Germany, and Gunther von Hagens' exhibitions have subsequently been hosted by museums and venues in more than 50 cities worldwide, attracting more than 29 million visitors..\n\nGunther von Hagens' Body Worlds exhibitions are the original, precedent-setting public anatomical exhibitions of real human bodies, and the only anatomical exhibits that use donated bodies, willed by donors to the Institute for Plastination for the express purpose of serving the Body Worlds mission to educate the public about health and anatomy. To date, more than 10,000 people have agreed to donate their bodies to Institute for Plastination.\n\nIn 2004, Premier Exhibitions began their \"Bodies Revealed\" exhibition in Blackpool, England, which ran from August through October 2004. In 2005 and 2006, the company opened their \"Bodies Revealed\" and \"Bodies...The Exhibition\" in Seoul, Tampa, and New York City. The West Coast exhibition site opened on 22 June 2006 at the Tropicana Resort and Casino Las Vegas. , BODIES... The Exhibition is showing at the Ambassador Theatre (Dublin) in Dublin, Ireland. The exhibition was in Istanbul, Turkey, until the end of March 2011.\n\nPlastination galleries are offered in a few college medical schools, including University of Michigan (said to be the nation's largest such lab) and the Vienna University. Jss Medical College, Mysuru, India. Gunther von Hagens maintains a permanent exhibition of plastinates and plastination at the Plastinarium in Guben, Germany.\n\n\n\n"}
{"id": "7624870", "url": "https://en.wikipedia.org/wiki?curid=7624870", "title": "Platform (geology)", "text": "Platform (geology)\n\nIn geology, a platform is a continental area covered by relatively flat or gently tilted, mainly sedimentary strata, which overlie a basement of consolidated igneous or metamorphic rocks of an earlier deformation. Platforms, shields and the basement rocks together constitute cratons. Platform sediments can be classified into the following groups: a \"protoplatform\" of metamorphosed sediments at the bottom, a \"quasiplatform\" of slightly deformed sediments, a \"cataplatform\", and a \"orthoplatform\" at the top. The Mesoproterozoic Jotnian sediments of the Baltic area are examples of a \"quasiplatform\". The post-Ordovician rocks of the South American Platform are examples of an orthoplatform.\n\n"}
{"id": "35574369", "url": "https://en.wikipedia.org/wiki?curid=35574369", "title": "Products of conception", "text": "Products of conception\n\nProducts of conception, abbreviated POC, is a medical term used for the tissue derived from the union of an egg and a sperm. It encompasses anembryonic gestation (blighted ovum) which does not have a viable embryo.\n\nIn the context of tissue from a dilation and curettage, the presence of POC essentially excludes an ectopic pregnancy.\n\n\"Retained products of conception\" is where products of conception remain in the uterus after childbirth, medical abortion or spontaneous abortion (colloquially known as miscarriage). Miscarriage with retained products of conception is termed \"delayed\" when no or very little products of conception have been passed, and \"incomplete\" when some products have been passed but some still remain \"in utero\".\n\nThe diagnosis is based on clinical presentation, quantitative HCG, ultrasound, and pathologic evaluation. A solid, heterogeneous, echogenic mass has a positive predictive value of 80%, but is present in only a minority of cases. A thickened endometrium of > 10 mm is usually considered abnormal, though no consensus exists on the appropriate cutoff. A cut-off of 8 mm or more has 34% positive rate, while a cut off of 14 mm or more has 85% sensitivity, 64% specificity for the diagnosis. Color Doppler flow in the endometrial canal can increased confidence in the diagnosis, though its absence does not exclude it, as 40% of cases of retained products have little or no flow. The differential in suspected cases includes uterine atony, blood clot, gestational trophoblastic disease, and normal post partum appearance of the uterus. Post partum blood clot is more common, reported in up to 24% of postpartum patients, and tends to be more hypoechoic than retained products with absent color flow on Doppler, and resolving spontaneously on follow up scans. The presence of gas raises the possibility of post partum endometritis, though this can also be seen in up to 21% of normal post pregnancy states. The normal post partum uterus is usually less than 2 cm in thickness, and continues to involute on follow up scans to 7 mm or less over time. Retained products are not uncommon, occurring in approximately 1% of all pregnancies, though it more common following abortions, either elective or spontaneous. There is significant overlap between appearance of a normal post partum uterus and retained products. If there is no endometrial canal mass or fluid, and endometrial thickness is less than 10 mm and without increased flow, retained products are statistically unlikely.\n\nAccording to the 2006 WHO \"Frequently asked clinical questions about medical abortion\", the presence of remaining products of conception in the uterus (as detected by obstetric ultrasonography) after a medical abortion is \"not\" an indication for surgical intervention (that is, vacuum aspiration or dilation and curettage). Remaining products of conception will be expelled during subsequent vaginal bleeding. Still, surgical intervention may be carried out on the woman's request, if the bleeding is heavy or prolonged, or causes anemia, or if there is evidence of endometritis.\n\nIn delayed miscarriage (also called missed abortion), the Royal Women's Hospital recommendations of management depend on the findings in ultrasonography:\n\nIn incomplete miscarriage, the Royal Women's Hospital recommendations of management depend on the findings in ultrasonography:\n\n"}
{"id": "11456251", "url": "https://en.wikipedia.org/wiki?curid=11456251", "title": "Renewable energy in the United States", "text": "Renewable energy in the United States\n\nRenewable energy accounted for 12.2 % of total primary energy consumption and 14.94 % of the domestically produced electricity in the United States in 2016.\nHydroelectric power is currently the largest producer of renewable electricity in the country, generating around 6.5% of the nation's total electricity in 2016 as well as 45.71% of the total renewable electricity generation. \nThe United States is the fourth largest producer of hydroelectricity in the world after China, Canada and Brazil.\n\nThe next largest share of renewable power was provided by wind power at 5.55% of total power production, amounting to 226.5 terawatt-hours during 2016. By January 2017, the United States nameplate generating capacity for wind power was 82,183 megawatts (MW). Texas remained firmly established as the leader in wind power deployment, followed by Iowa and Oklahoma as of year end 2016. \n\nSolar power provides a growing share of electricity in the country, with over 50 GW of installed capacity generating about 1.3% of the country's total electricity supply in 2017, up from 0.9% the previous year.\nAs of 2016, more than 260,000 people worked in the solar industry and 43 states deployed net metering, where energy utilities bought back excess power generated by solar arrays.\nLarge photovoltaic power plants in the United States include Solar Star (579 MW), near Rosamond, California, the Desert Sunlight Solar Farm, a 550 MW solar power plant in Riverside County, California and the Topaz Solar Farm, a 550 MW photovoltaic power plant, in San Luis Obispo County, California. \nSince the United States pioneered solar thermal power technology in the 1980s with Solar One, several more such power stations have been built. \nThe largest of these solar thermal power stations are the Ivanpah Solar Power Facility (392 MW), southwest of Las Vegas, and the SEGS group of plants in the Mojave Desert, with a total generating capacity of 354 MW. \n\nOther renewable energy sources include geothermal, with The Geysers in Northern California the largest geothermal complex in the world. \n\nThe development of renewable energy and energy efficiency marked \"a new era of energy exploration\" in the United States, according to the former President Barack Obama. In a joint address to the Congress on February 24, 2009, President Obama called for doubling renewable energy within the following three years. Renewable energy reached a major milestone in the first quarter of 2011, when it contributed 11.7 % of total national energy production (2.245 quadrillion BTU of energy), surpassing energy production from nuclear power (2.125 quadrillion BTU) for the first time since 1997.\nIn his 2012 State of the Union address, President Barack Obama restated his commitment to renewable energy and mentioned the long-standing Interior Department commitment to permit 10,000 MW of renewable energy projects on public land in 2012.\n\nRenewable energy technologies encompass a broad, diverse array of technologies, including solar photovoltaics, solar thermal power plants and heating/cooling systems, wind farms, hydroelectricity, geothermal power plants, and ocean power systems and the use of biomass.\n\nThe report \"Outlook On Renewable Energy In America\" explains that America needs renewable energy, for many reasons:\n\nAmerica needs energy that is secure, reliable, improves public health, protects the environment, addresses climate change, creates jobs, and provides technological leadership. America needs renewable energy. If renewable energy is to be developed to its full potential, America will need coordinated, sustained federal and state policies that expand renewable energy markets; promote and deploy new technology; and provide appropriate opportunities to encourage renewable energy use in all critical energy market sectors: wholesale and distributed electricity generation, thermal energy applications, and transportation. \nAnother benefit of some renewable energy technologies, like wind and solar photovoltaics (PV) is that they require little or no water to generate electricity whereas thermoelectric (fossil fuel based) power plants require vast amounts of water for operation.\n\nIn 2009, President Barack Obama in the inaugural address called for the expanded use of renewable energy to meet the twin challenges of energy security and climate change. Those were the first references ever to the nation's energy use, to renewable resources, and to climate change in an inauguration speech of a United States president. President Obama looked to the near future, saying that as a nation, the United States will \"harness the sun and the winds and the soil to fuel our cars and run our factories.\"\n\nThe president's \"New Energy For America\" plan calls for a federal investment of $150 billion over the next decade to catalyze private efforts to build a clean energy future. Specifically, the plan calls for renewable energy to supply 10% of the nation's electricity by 2012, rising to 25% by 2025.\n\nIn his joint address to Congress in 2009, Obama stated that: \"We know the country that harnesses the power of clean, renewable energy will lead the 21st. century...Thanks to our recovery plan, we will double this nation’s supply of renewable energy in the next three years... It is time for America to lead again\".\n\nAccording to Clean Edge, there's little doubt that the future of energy will be cleaner. The transition from carbon-intensive energy sources like wood, coal, and oil to natural gas and renewables, is well underway. For much of the developed world, and for developing nations, the \"future looks increasingly like it will be built off of a mix of energy efficiency, renewables, the electrification of transport, and lower carbon fuels like natural gas\".\n\nAs of 2011, new evidence has emerged that there are considerable risks associated with traditional energy sources, and that major changes to the mix of energy technologies is needed: \n\nSeveral mining tragedies globally have underscored the human toll of the coal supply chain. New EPA initiatives targeting air toxics, coal ash, and effluent releases highlight the environmental impacts of coal and the cost of addressing them with control technologies. The use of fracking in natural gas exploration is coming under scrutiny, with evidence of groundwater contamination and greenhouse gas emissions. Concerns are increasing about the vast amounts of water used at coal-fired and nuclear power plants, particularly in regions of the country facing water shortages. Events at the Fukushima nuclear plant have renewed doubts about the ability to operate large numbers of nuclear plants safely over the long term. Further, cost estimates for “next generation” nuclear units continue to climb, and lenders are unwilling to finance these plants without taxpayer guarantees.\n\nRenewable energy accounted for 14.94% of the domestically produced electricity in 2016 in the United States. \nThis proportion has grown from just 7.7% in 2001, although the trend is sometimes obscured by large yearly variations in hydroelectric power generation. \nMost of the growth since 2001 can be seen in the expansion of wind generated power, and more recently, in the growth in solar generated power.\nCalifornia is a leading state with around 29% of electricity coming from RPS-eligible renewable sources (including hydropower).\n\nThe United States has some of the best renewable energy resources in the world, with the potential to meet a rising and significant share of the nation's energy demand. \nA quarter of the country's land area has winds strong enough to generate electricity at the same price as natural gas and coal.\n\nMany of the new technologies that harness renewables — including wind, solar, geothermal, and biofuels — are, or soon will be, economically competitive with the fossil fuels that meet 85% of United States energy needs. \nDynamic growth rates are driving down costs and spurring rapid advances in technologies. \nWind power and solar power are becoming increasingly important relative to the older and more established hydroelectric power source. \nBy 2016 wind power covered 37.23% of total renewable electricity production against 43.62 % for hydroelectric power. \nThe remaining share of power was generated by biomass at 10.27%, solar power at 6.03% and geothermal with 2.86 % of total renewable generation.\n\nIn 2015, Georgetown, Texas became one of the first American cities to be powered entirely by renewable energy, choosing to do so for financial stability reasons.\n\nThe United States consumed about 4,000 TWh of electricity in 2012, and about 98 quadrillion BTU (30,000 TWh) of primary energy. \nEfficiency improvements are expected to reduce usage to 15,000 TWh by 2050. \n\nUsing data from Electric Power Annual 2014 the expected changes in generating capabilities for different fuel sources is shown in the chart-2015-2019 Electric Power Annual Capacity Projections. Looking only at the renewable fuel sources, a total of 206.2 Gigawatts of renewable would be available by 2019. This is up 36 Gigawatts (+21.1%) from 2014. Using this generating capability and the capacity factors from 2014 data will result in a total of 627.7 terawatt-hours (TWh) of renewable electric energy in 2019. This would be up 89.4 TWh (+16.7%) from 2014.\n\nHydroelectric power is currently the largest producer of renewable power in the United States. \nIt produced around 6.52% of the nation's total electricity in 2016 which was 43.62% of the total renewable power in the country. \nThe United States is the fourth largest producer of hydroelectricity in the world after China, Canada and Brazil. \nThe Grand Coulee Dam is the 5th largest hydroelectric power station in the world and another six U.S. hydroelectric plants are among the 50 largest in the world. \nThe amount of hydroelectric power generated is strongly affected by changes in precipitation and surface runoff.\nHydroelectricity projects such as Hoover Dam, Grand Coulee Dam, and the Tennessee Valley Authority have become iconic large construction projects.\n\nWind power capacity in the United States tripled from 2008 to 2016, and supplied over 5% of the country's total electricity generation in 2016. \nWind and solar accounted for two-thirds of new energy installations in the United States in 2015.\nUnited States wind power installed capacity exceeds 81 GW as of 2017. \nThis capacity is exceeded only by China.\nThe 1,320MW Alta Wind Energy Center is the largest wind farm in the world. \nShepherds Flat Wind Farm in Oregon is the second largest wind farm in the world, completed in 2012, with the nameplate capacity of 845 MW.\n\nThere were 90,000 wind operations jobs in the United States in 2015. \nThe wind industry in the United States generates tens of thousands of jobs and billions of dollars of economic activity. \nWind projects boost local tax bases, and revitalize the economy of rural communities by providing a steady income stream to farmers with wind turbines on their land. \nGE Energy is the largest domestic wind turbine manufacturer. \n\nIn 2013 wind power received $5.936 billion in federal funding, which is 37% of all federal funding for electrity generation.\n\nThe United States has the potential of installing 10 terawatt (TW) of onshore wind power and 4 TW of offshore wind. \nThe U.S. Department of Energy’s report \"20% Wind Energy by 2030\" envisioned that wind power could supply 20% of all the country's electricity, which included a contribution of 4% from offshore wind power. \nAdditional transmission lines will need to be added, to bring power from windy states to the rest of the country. \nIn August 2011, a coalition of 24 governors asked the Obama administration to provide a more favorable business climate for the development of wind power.\n\nThe United States is one of the world's largest producers of solar power.\nThe country pioneered solar farms and many key developments in concentrated solar and photovoltaics came out of national research.\n\nIn 2016, utility scale solar contributed 36.76 TWh to the grid, with 33.367 TWh from photovoltaics and 3.39 TWh from thermal systems. \nIn 2014, 2015, and 2016, EIA estimated that distributed solar generated 11.233 TWh, 14.139 TWh and 19.467 TWh respectively. \nWhile utility-grade systems have well documented generation, distributed systems contributions to user electric power needs are not measured or controlled. \nTherefore, quantitative evaluation of distributed solar to the country's electric power sector has been lacking. \nRecently, the Energy Information Administration has begun estimating that contribution. \nBefore 2008, most solar-generated electric energy was from thermal systems, however by 2011 photovoltaics had overtaken thermal. \n\nAt the end of 2016, the United States had 19.77 gigawatts (GW) of installed utility-scale photovoltaic capacity.\nThe United States has some of the largest solar farms in the world. \nSolar Star is a 579 megawatt (MW) farm near Rosamond, California.\nCompleted in June 2015, it uses 1.7 million solar panels, spread over . \nThe Desert Sunlight Solar Farm is a 550 MW solar power plant in Riverside County, California, that uses thin-film solar photovoltaic modules made by First Solar. The Topaz Solar Farm is a 550 MW photovoltaic power plant, in San Luis Obispo County, California. The Blythe Solar Power Project is a 485 MW photovoltaic station planned for Riverside County, California.\n\nMany schools and businesses have building-integrated photovoltaic solar panels on their roof. \nMost of these are grid connected and use net metering laws to allow use of electricity in the evening that was generated during the daytime. \nNew Jersey leads the nation with the least restrictive net metering law, while California leads in total number of homes which have solar panels installed. Many were installed because of the million solar roof initiative. California decided that it is not moving forward fast enough on photovoltaic generation and in 2008 enacted a feed-in tariff. Washington state has a feed-in tariff of 15 ¢/kWh which increases to 54 ¢/kWh if components are manufactured in the state. By 2015, California, Hawaii, Arizona and some other states were lowering payments to distributed solar owners and instituting new fees for grid usage. Tesla and a handful of other companies were promoting household grid-tied batteries while some electric companies were investing in utility-scale grid energy storage including very large batteries.\n\nBeginning with the 2014 data year, Energy Information Administration has estimated distributed solar photovoltaic generation and distributed solar photovoltaic capacity. \nThese non-utility scale estimates that the United States, generated the following additional electric energy from such distributed solar PV systems.\n\nAt the end of 2016 there were 1.76 GW total installed capacity of solar thermal power across the United States.\nSolar thermal power is generally utility-scale.\nPrior to 2012, in six southwestern states (Arizona, California, Colorado, Nevada, New Mexico, and Utah) the US Bureau of Land Management owned nearly (an area larger than the state of Montana) that was open to proposals for solar power installations. \nTo streamline consideration of applications, the BLM produced a Programmatic Environmental Impact Statement (PEIS). \nBy the subsequent Record of Decision in October 2012, the BLM withdrew 78% of its land from possible solar development, leaving still open to applications for solar installations, an area nearly as large as South Carolina. Of the area left open to solar proposals, the BLM has identified 285 thousand acres in 17 highly favorable areas it calls Solar Energy Zones.\n\nSolar thermal power plants designed for solar-only generation are well matched to summer noon peak loads in prosperous areas with significant cooling demands, such as the south-western United States. \nUsing thermal energy storage systems, solar thermal operating periods can even be extended to meet base-load needs.\n\nA 2013 study by the US National Renewable Energy Laboratory concluded that utility-scale solar power plants directly disturb an average of 2.7 to 2.9 acres per gigawatt-hour/year, and use from 3.5 to 3.8 acres per gW-hr/year for the entire sites. \nAccording to a 2009 study, this intensity of land use is less than that of the country's average power plant using surface-mined coal. Some of the land in the eastern portion of the Mojave Desert is to be preserved, but the solar industry is more interested in areas of the western desert, \"where the sun burns hotter and there is easier access to transmission lines\".\n\nSome of the largest solar thermal power plants in the United States are in the south-west of the country, especially in the Mojave Desert.\nSolar Energy Generating Systems (SEGS) is the name given to nine solar power plants in the Mojave Desert commissioned between 1984 and 1991.\nThe installation uses parabolic trough solar thermal technology along with natural gas to generate electricity. \nThe facility has a total of 400,000 mirrors and covers 1,000 acres (4 km²).\nThe plants have a total generating capacity of 354 MW.\n\nNevada Solar One generates 64MW of power and in Boulder City, Nevada, and was built by the U.S. Department of Energy (DOE), National Renewable Energy Laboratory (NREL), and Solargenix Energy. Nevada Solar One started producing electricity in June 2007.\nNevada Solar One uses parabolic troughs as thermal solar concentrators, heating tubes of liquid which act as solar receivers. These solar receivers are specially coated tubes made of glass and steel. About 19,300 of these 4 metre long tubes are used in the newly built power plant. Nevada Solar One also uses a technology that collects extra heat by putting it into phase-changing molten salts. This energy can then be drawn on at night.\nThe Ivanpah Solar Power Facility is a 392 megawatt (MW) solar power facility which is located in south-eastern California. The facility formally opened on February 13, 2014.\nThe Solana Generating Station is a 280 MW solar power plant which is near Gila Bend, Arizona, about southwest of Phoenix. The 250MW Mojave Solar Project is located near Barstow, California. The Crescent Dunes Solar Energy Project is a 110 megawatt (MW) solar thermal power project near Tonopah, about northwest of Las Vegas.\n\nThe United States is the world leader in online capacity and the generation of electricity from geothermal energy. \nAccording to 2014 state energy data, geothermal energy provided approximately 16 terawatt-hours (TWh) of electricity, or 0.31% of the total electricity consumed in the country. \nAs of May 2007, geothermal electric power was generated in five states: Alaska, California, Hawaii, Nevada, and Utah. \nAccording to the Geothermal Energy Association's recent report, there were 75 new geothermal power projects underway in 12 states as of May 2007.\nThis is an increase of 14 projects in an additional three states compared to a survey completed in November 2006.\n\nThe most significant catalyst behind new industry activity is the \"Energy Policy Act of 2005\". \nThis Act made new geothermal plants eligible for the full federal production tax credit, previously available only to wind power projects. \nIt also authorized and directed increased funding for research by the Department of Energy, and gave the Bureau of Land Management new legal guidance and secure funding to address its backlog of geothermal leases and permits.\n\nThe contribution over the last thirteen years of geothermal power to the renewable power generation and to the total US power generation is shown below along with the yearly profile of the geothermal power generation for 2016 where 2.51 GW of capacity produced 17.42 TWh of Energy. \n\nIn 2015, biomass generated 63.63 terawatt-hours (TWh) of electricity, or 1.56% of the country's total electricity production. \nBiomass was the largest source of renewable primary energy in the US, and the third-largest renewable source of electrical power in the US, after hydropower and wind.\n\nBiomass electric generation data combines two basic categories:\n\nThe contribution from these two categories over the last thirteen years of biomass electric power to the renewable power generation and to the total US power generation is shown below along with the yearly profile of the electric power generation for 2016. \nThis shows the typical variations over the months of the year due to fuel availability and needs.\n\nWave power in the United States is under development in several locations off the east and west coasts as well as Hawaii. It has moved beyond the research phase and is producing reliable energy. Its use to-date has been for situations where other forms of energy production are not economically viable and as such, the power output is currently modest. But major installations are planned to come on-line within the next few years.\n\nThe U.S. Department of Energy stated (in 2006) that more than 1.5 million homes and businesses were currently using solar water heating in the United States, representing a capacity of over 1,000 megawatts (MW) of thermal energy generation. It predicted that another 400 MW was likely to be installed over the next 3–5 years.\n\nAssuming that 40 percent of existing homes in the United States have adequate access to sunlight, 29 million solar water heaters could be installed.\n\nSolar water heaters can operate in any climate. Performance varies depending on how much solar energy is available at the site, as well as how cold the water coming into the system is. The colder the water, the more efficiently the system operates. \nSolar water heaters reduce the need for conventional water heating by about two-thirds and pay for their installation within 4 to 8 years with electricity or natural gas savings. Compared to those with electric water heaters, Florida homeowners with solar water heaters save 50 to 85 percent on their water heating bills, according to the Florida Solar Energy Center.\n\nMany cars sold in the U.S. since 2001 are able to run on blends of up to 15% ethanol.\nOlder cars in the United States can run on blends of up to 10% ethanol. Motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, DaimlerChrysler, and GM are among the automobile companies that sell “flexible-fuel” cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol (E85). By mid-2006, there were approximately 6 million E85-compatible vehicles on the road.\n\nNinety-five percent of gasoline sold in the U.S.(2016) is blended with 10% ethanol. There are challenges in moving to higher blends, however. Flex-fuel vehicles are assisting in this transition because they allow drivers to choose different fuels based on price and availability. The \"Energy Independence and Security Act of 2007\", which calls for of biofuels to be used annually by 2012, will also help to expand the market. The USDA in 2015 began offering grants to help gasoline retailers install blender pumps for dispensing mid-level ethanol blends.\n\nThe expanding ethanol and biodiesel industries are providing jobs in plant construction, operations, and maintenance, mostly in rural communities. According to the Renewable Fuels Association, the ethanol industry created almost 154,000 jobs in 2005 alone, boosting household income by $5.7 billion. It also contributed about $3.5 billion in tax revenues at the local, state, and federal levels. On the other hand, in 2010, the biofuel industry received $6.64 billion in federal government support.\n\nThere are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields. \nMultiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.\n\nBoth Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), have heavily funded solar research programs. British Petroleum was also heavily invested in solar research programs until 2008 when the company began scaling back its solar operations. The company finally shut down its forty-year-old solar business after executives decided solar power production is not economically competitive. The NREL solar program has a budget of around $75 million and develops research projects in the areas of photovoltaic (PV) technology, solar thermal energy, and solar radiation. The budget for Sandia’s solar division is unknown, however it accounts for a significant percentage of the laboratory’s $2.4 billion budget. Several academic programs have focused on solar research in recent years. The Solar Energy Research Center (SERC) at University of North Carolina (UNC) has the sole purpose of developing cost effective solar technology. In 2008, researchers at Massachusetts Institute of Technology (MIT) developed a method to store solar energy by using it to produce hydrogen fuel from water. Such research is targeted at addressing the obstacle that solar development faces of storing energy for use during nighttime hours when the sun is not shining. \nIn February 2012, North Carolina-based Semprius Inc., a solar development company backed by German corporation Siemens, announced that they had developed the world’s most efficient solar panel. The company claims that the prototype converts 33.9% of the sunlight that hits it to electricity, more than double the previous high-end conversion rate.\n\nWind energy research dates back several decades to the 1970s when NASA developed an analytical model to predict wind turbine power generation during high winds. Today, both Sandia National Laboratories and National Renewable Energy Laboratory have programs dedicated to wind research. Sandia’s laboratory focuses on the advancement of materials, aerodynamics, and sensors. The NREL wind projects are centered on improving wind plant power production, reducing their capital costs, and making wind energy more cost effective overall.\nThe Field Laboratory for Optimized Wind Energy (FLOWE) at Caltech was established to research renewable approaches to wind energy farming technology practices that have the potential to reduce the cost, size, and environmental impact of wind energy production.\n\nAs the primary source of biofuels in North America, many organizations are conducting research in the area of ethanol production. On the Federal level, the USDA conducts a large amount of research regarding ethanol production in the United States. Much of this research is targeted toward the effect of ethanol production on domestic food markets.\nThe National Renewable Energy Laboratory has conducted various ethanol research projects, mainly in the area of cellulosic ethanol. Cellulosic ethanol has many benefits over traditional corn based-ethanol. It does not take away or directly conflict with the food supply because it is produced from wood, grasses, or non-edible parts of plants. Moreover, some studies have shown cellulosic ethanol to be more cost effective and economically sustainable than corn-based ethanol. Sandia National Laboratories conducts in-house cellulosic ethanol research and is also a member of the Joint BioEnergy Institute (JBEI), a research institute founded by the United States Department of Energy with the goal of developing cellulosic biofuels.\n\nOver $1 billion of federal money has been spent on the research and development of hydrogen fuel in the United States. Both the National Renewable Energy Laboratory and Sandia National Laboratories have departments dedicated to hydrogen research.\n\nA 2010 survey conducted by Applied Materials shows that two-thirds of Americans believe solar technology should play a greater role in meeting the country's energy needs. In addition, \"three-quarters of Americans feel that increasing renewable energy and decreasing U.S. dependence on foreign oil are the country's top energy priorities\". According to the survey, \"67 percent of Americans would be willing to pay more for their monthly utility bill if their utility company increased its use of renewable energy\".\n\nIn a 2010 Chicago Council on Global Affairs public opinion survey, an overwhelming 91 percent believed \"investing in renewable energy\" is important for the United States to remain economically competitive with other countries, with 62 percent considering this very important. The same poll found strong support for tax incentives to encourage development of renewable energy sources specifically as a way to reduce foreign energy imports. Eight in ten (80 percent) favored tax incentives, 47 percent strongly, and only 17 percent were opposed.\n\nThe Chicago Council on Global Affairs ( 2010 ) in the public opinion survey also found that approximately 65% of the Americans supported increased fuel efficiencies of the products of the auto makers , irrespective of the price escalation that would be associated with the implementation .\n\nPublic Surveys conducted by WPO in 2008 also revealed that 79% of the Americans were of the notion that a major shift to alternative sources of energy is economically beneficial in the long run .\n\nEnergy technologies receive government subsidies. \nIn 2013, federal government energy-specific subsidies and supports for renewables, fossil fuels, and nuclear power were $15.043 billion, $3.431 billion and $1.66 billion respectively. \nThe subsidies and supports specific to electricity production amount to $11.678 billion, $1.591 billion and $1.66 billion respectively.\nAll but a few U.S. states now have incentives in place to promote renewable energy, while more than a dozen have enacted new renewable energy laws in recent years. \nRenewable energy suffered a political setback in the United States in September 2011 with the bankruptcy of Solyndra, a company that had received a $535 million federal loan guarantee.\n\nThe Energy Policy Act of 2005 requires all public electric utilities to facilitate net metering. This allows homes and businesses performing distributed generation to pay only the net cost of electricity from the grid: electricity used minus electricity produced locally and sent back into the grid. For intermittent renewable energy sources this effectively uses the grid as a battery to smooth over lulls and fill in production gaps.\n\nSome jurisdictions go one step further and have instituted feed-in tariff, which allows any power customer to actually make money by producing more renewable energy than is consumed locally.\n\nFrom 2006-14, US households received more than $18 billion in federal income tax credits for weatherizing their homes, installing solar panels, buying hybrid and electric vehicles, and other \"clean energy\" investments. These tax expenditures went predominantly to higher-income Americans. The bottom three income quintiles received about 10% of all credits, while the top quintile received about 60%. The most extreme is the program aimed at electric vehicles, where the top income quintile received about 90% of all credits. Market mechanisms have less skewed distributional effects.\n\nThe American Recovery and Reinvestment Act of 2009 included more than $70 billion in direct spending and tax credits for clean energy and associated transportation programs. This policy-stimulus combination represents the largest federal commitment in United States history for renewable energy, advanced transportation, and energy conservation initiatives. These new initiatives were expected to encourage many more utilities to strengthen their clean energy programs. While the Department of Energy has come under criticism for providing loan guarantees to Solyndra, its SunShot initiative has funded successful companies such as EnergySage and Zep Solar.\n\nIn his January 24, 2012, State of the Union address, President Barack Obama restated his commitment to renewable energy, stating that he “will not walk away from the promise of clean energy.” Obama called for a commitment by the Defense Department to purchase 1,000 MW of renewable energy. He also mentioned the long-standing Interior Department commitment to permit 10,000 MW of renewable energy projects on public land in 2012.\n\nSome Environmental Protection Agency facilities in the United States use renewable energy for all or part of their supply at the following facilities.\n\nIn February 2011, the U.S. Department of Energy (DOE) launched its SunShot initiative, a collaborative national effort to cut the total cost of photovoltaic solar energy systems by 75% by 2020. Reaching this goal would make unsubsidized solar energy cost-competitive with other forms of electricity and get grid parity . The SunShot initiative included a crowdsourced innovation program run in partnership with Topcoder, during which 17 different solar energy application solutions were developed in 60 days. In 2011, the price was $4/W, and the SunShot goal of $1/W by 2020 was reached in 2017.\n\nWind Powering America (WPA) is another DOE initiative that seeks to increase the use of wind energy. WPA collaborates with state and regional stakeholders, including farmers, ranchers, Native Americans, rural electric cooperatives, consumer-owned utilities and schools.\n\nWPA has focused on states with strong potential for wind energy generation but with few operational projects. WPA provides information about the challenges, benefits, and impacts of wind technology implementation.\n\nThe Solar America Initiative (SAI) is a part of the Federal Advanced Energy Initiative to accelerate the development of advanced photovoltaic materials with the goal of making it cost-competitive with other forms of renewable electricity by 2015.\n\nThe DOE Solar Energy Technology Program (SETP) intended to achieve the goals of the SAI through partnerships and strategic alliances by focusing primarily on four areas:\n\n\nAs part of former Governor Arnold Schwarzenegger's Million Solar Roofs Program, California set a goal to create 3,000 megawatts of new, solar-produced electricity by 2017, with funding of $2.8 billion.\n\nThe California Solar Initiative offers cash incentives on solar PV systems of up to $2.50 a watt. These incentives, combined with federal tax incentives, can cover up to 50% of the total cost of a solar panel system. Financial incentives to support renewable energy are available in some other US states.\n\nThe EPA named the top 20 partners in its Green Power Partnership that are generating their own renewable energy on-site. Combined, they generate more than 736 million kilowatt-hours of renewable energy on-site each year, enough to power more than 61,000 average U.S. homes.\n\nA Renewable Portfolio Standard refers to legislation that creates a market in tradeable renewable or green electricity certificates. Electricity distributors or wholesaler purchasers of electricity are required to source a specified percentage of their electricity (portfolio) from renewable generation sources. Liable entities that fall short of their quota can purchase certificates from accredited suppliers who have generated renewable electricity and obtained and registered certificates to sell on that market.\n\nThe American Council on Renewable Energy (ACORE), is a non-profit organization with headquarters in Washington DC. It was founded in 2001 as a unifying forum for bringing renewable energy into the mainstream of American’s economy and lifestyle. In 2010 ACORE had over 700 member organizations. In 2007, ACORE published Outlook On Renewable Energy In America, a two volume report about the future of renewable energy in the United States. It has been said that this report exposes a \"new reality for renewable energy in America.\"\n\nThe Environmental and Energy Study Institute (EESI) is a non-profit organization which promotes environmentally sustainable societies. Founded in 1984 by a group of Congressional Members, EESI seeks to be a catalyst that moves society away from environmentally damaging fossil fuels and toward a clean energy future. EESI presents policy solutions that will result in decreased global warming and air pollution; improvements in public health, energy security and rural economic development opportunities; increased use of renewable energy sources and improved energy efficiency.\n\nAn important part of the mission of the National Renewable Energy Laboratory (NREL) is the transfer of NREL-developed technologies to renewable energy markets. NREL's Technology Transfer Office supports laboratory scientists and engineers in the successful and practical application of their expertise and the technologies they develop. R&D staff and facilities are recognized and valued by industry, as demonstrated through many collaborative research projects and licensed technologies with public and private partners. NREL's innovative technologies have also been recognized with 39 R&D 100 Awards.\n\nThe Rocky Mountain Institute (RMI) is an organization dedicated to research, publication, consulting, and lecturing in the general field of sustainability, with a special focus on profitable innovations for energy and resource efficiency. RMI is headquartered in Snowmass, Colorado, and also maintains offices in Boulder, Colorado. RMI is the publisher of the book \"Winning the Oil Endgame\".\n\nThe United States has the potential of installing 11 terawatt (TW) of onshore wind power and 4 TW of offshore wind power, capable of generating over 47,000 TWh. \nThe potential for concentrated solar power in the southwest is estimated at 10 to 20 TW, capable of generating over 10,000 TWh.\n\nA 2012 report by the National Renewable Energy Laboratory evaluates the potential energy resources for each state of the United States.\n\nIn 2010, the United States used 3,754 TWh of electricity. Total energy used in 2010 was 98.16 quadrillion BTU (28,800 TWh, but over 30% is thermal losses).\nNote: Total use is inflated to create an oil equivalence.\n\n\n\n"}
{"id": "32554813", "url": "https://en.wikipedia.org/wiki?curid=32554813", "title": "Rutilated quartz", "text": "Rutilated quartz\n\nRutilated quartz is a variety of quartz which contains acicular (needle-like) inclusions of rutile. It is used for gemstones. These inclusions mostly look golden, but they also can look silver, copper red or deep black. They can be distributed randomly or in bundles, which sometimes are arranged star-like, and they can be sparse or dense enough to make the quartz body nearly opaque. While otherwise inclusions often reduce the value of a crystal, rutilated quartz in fact is valued for the quality and beauty of these inclusions.\n"}
{"id": "20709007", "url": "https://en.wikipedia.org/wiki?curid=20709007", "title": "Solid surface", "text": "Solid surface\n\nSolid surface is a man-made material usually composed of a combination of alumina trihydrate (ATH), acrylic, epoxy or polyester resins and pigments. It is most frequently used for seamless countertop installations.\n\nSolid surface was first introduced by DuPont in 1967 under the name of Corian. Since the expiration of their patent, other manufacturers have also introduced their brands as well. \n\nSolid surface is a non-porous low-maintenance material used for surfaces such as countertops. It can mimic the appearance of granite, marble, stone, and other naturally occurring materials, and can be joined nearly invisibly by a trained craftsman. Typically manufactured in sheet form for fabrication into finished countertops, solid surface can also be cast into a variety of shapes, including sinks, shower pans and bathtubs. Sheet goods can also be heated and bent into three-dimensional shapes using a process called thermoforming, which adds to the versatility of the product. Color and design flexibility are key factors when choosing engineered composites over natural stone.\n\nIn a residential setting, solid surface can be used as kitchen countertops, bathroom vanity tops, and shower and tub surrounds. Countertop fabricators typically join solid surface sheets into desired shapes using a two-part adhesive, after which the cured joint is machined flat. The same method is used to build up edge thickness, which can be profiled using tools and techniques similar to those used to work hardwoods.\n\nA major appeal of solid surface is its impermeability. Solid surface sinks can be joined to the countertop surface with no gaps, which eliminates areas for water to collect and bacteria to grow. Integral backsplashes can also be created that follow the contours of the wall “seamlessly” and without gaps.\n\nShould the material become scratched or broken, solid surface can, in most cases, be repaired quickly by a certified solid surface repair company or trained fabricator. Cracked sinks can be replaced as well. Because the surface is solid throughout, a countertop that has undergone years of wear and tear can be refinished. The installed product is available in a variety of finishes, ranging from matte to high-gloss, although most manufacturers recommend a matte or satin finish for ease of maintenance.\n\nSolid surface is available in hundreds of colors and visual textures, ranging from solid colors to sparkling, marbleized, or granite effect. Dozens of multinational companies manufacture solid surface sheet goods and sinks for the world market; and hundreds of smaller, regional manufacturers produce for local markets. \n\nAlthough solid surface faces competition from natural stone and engineered stone/quartz surfacing, the material is popular in Asia, particularly in residential new construction. An emerging market for solid surface is in commercial and industrial settings, where its non-porous characteristics, combined with durability, renewability and formability make it the material of choice for many designers and architects. Also appealing, is the wide range of colors and textures, as well as the ability to custom-color the material for large projects. Hospitals, in particular, employ large amounts of solid surface for sinks, showers, nursing stations and wall cladding, because its seamless properties eliminate gaps and crevices that harbor harmful bacteria.\n\nSolid surface is manufactured by mixing acrylic, epoxide or polyester resin with powdered bauxite filler and pigments. The material chemically cures and is heated to 60°C or more. Cured material is then cut in sheets or shapes and sanded on one or both sides depending the brand used. Sometimes the material is brought to 160°C and cooled, to improve its heat resistance. \n\nAcrylic-modified polyester solid surface can be mixed one sheet at a time using different formulations, effects and appearances. Acrylic-modified polyester solid surface can be injected into molds to produce various solid decorative design figures.\n\nSolid-surface manufacturing companies may be members of the trade organizations American Composites Manufacturers Association, ISFA (International Surface Fabricators Association, and ICPA (International Cast Polymer Association). These organizations identify the product under the \"brand\" of MasterCast Engineered Composites.\n\nSolid surface can also be moulded using dedicated moulds. For example free standing baths are manufactured using a 2-part mould tool which creates a cavity for the material to be poured into. Once the mould has been prepared for casting, the material is mixed and degassed in an automated casting machine. It is then injected into the cavity of the mould. Many finishes are available including, gel coated, matte, gloss, metallic and textured.\n\nA \"surface fabricator\" shapes and installs functional and decorative surfaces, usually in buildings. Surface fabrication, as a distinct professional enterprise, usually involves complex tasks of surface shaping. A leading case is countertop fabrication.\n\nFor interior spaces, surfaces include solid coverings or decking for floors or walls, and countertops for kitchens, bathrooms, and workspaces. Natural surfaces are composed of naturally occurring materials like wood, stone, mineral, clay, gum and resin. Synthetic surfaces contain, in addition to any natural materials, man-made substances such as metal alloys or polymers. Solid surfaces are synthetic surfaces manufactured from polymeric materials.\n\nTools of surface fabrication include routers, drills, templates, clamps, ovens, suction cup grips, and table saws. Larger surface fabricators may also use CNC routers. For all but the simplest jobs, surface fabrication usually takes place in the workshop, after which the finished product is assembled and installed at the work site. Installation of multi-part surfaces, such as \"L\" and \"U\" shaped countertops, often involves complex work with specialized tools, to create a seamless result.\n\nSolid surfaces is used in the following environment :\n\nSolid surface is priced to compete with natural stone or quartz surfacing. It is available in many colors and patterns. The material can be formed to shape, which makes it appealing to commercial and industrial designers. Specialist fabricators possess the skills and knowledge necessary to provide advice and customer service. \n\nCertain high-heat applications (anything above 400 F /200 C) are discouraged, due to the danger of cracking. Because scratches in solid surface appear white to the human eye, most dark colors are not recommended for high use areas.\n\n\n"}
{"id": "27980", "url": "https://en.wikipedia.org/wiki?curid=27980", "title": "Stellar evolution", "text": "Stellar evolution\n\nStellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are born from collapsing clouds of gas and dust, often called nebulae or molecular clouds. Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.\n\nNuclear fusion powers a star for most of its life. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their lives, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.\n\nStellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.\n\nStellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly across and contain up to . As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating sphere of superhot gas known as a protostar.\n\nA protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: means 1 solar mass.\n\nProtostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.\nObservations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous Galactic protostars and their parent star clusters.\n\nProtostars with masses less than roughly never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (), 2.5 × 10 kg, or ). Objects smaller than are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and die away slowly, cooling gradually over hundreds of millions of years.\n\nFor a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over , the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star's matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.\n\nA new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.\n\nEventually the core exhausts its supply of hydrogen and the star begins to evolve off of the main sequence, without the outward pressure generated by the fusion of hydrogen to counteract the force of gravity the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star's mass.\n\nWhat happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.\n\nRecent astrophysical models suggest that red dwarfs of may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion more to collapse, slowly, into a white dwarf. Such stars will not become red giants as they are fully convective and will not develop a degenerate helium core with a shell burning hydrogen. Instead, hydrogen fusion will proceed until almost the whole star is helium.\n\nSlightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red giant branch. When hydrogen shell burning finishes, these stars move directly off the red giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf. A star with an initial mass above about will be able to reach temperatures high enough to fuse helium, and these \"mid-sized\" stars go on to further stages of evolution beyond the red giant branch.\n\nStars of roughly become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.\n\nMid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells. Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core. Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.\n\nWhen a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core. The core increases in mass as the shell produces more helium. Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state. Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars. Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red giant branch.\n\nThe expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star. For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower C/C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.\n\nThe helium core continues to grow on the red giant branch. It is no longer in thermal equilibrium, either degenerate or above the Schoenberg-Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase. The star increases in luminosity towards the tip of the red-giant branch. Red giant branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.\n\nIn the helium cores of stars in the 0.8 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 10 times the luminosity of the Sun for a few days and 10 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.\n\nCore helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning. These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.\n\nAfter a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red giant evolution, but with even faster energy generation (which lasts for a shorter time). Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star. Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically. This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.\n\nThere is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface. This is known as the second dredge up, and in some stars there may even be a third dredge up. In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra. A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.\n\nAnother well known class of asymptotic-giant-branch stars are the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths. These stars can be observed as OH/IR stars, pulsating in the infra-red and showing OH maser activity. These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.\n\nThese mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.\n\nIt is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.\n\nIn massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as much as lower-mass stars; however, they were much brighter than lower-mass stars to begin with, and are thus still brighter than the red giants formed from less-massive stars. These stars are unlikely to survive as red supergiants; instead they will destroy themselves as type II supernovas.\n\nExtremely massive stars (more than approximately ), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.\n\nThe core grows hotter and denser as it gains material from fusion of hydrogen at the base of the envelope. In all massive stars, electron degeneracy pressure is insufficient to halt collapse by itself, so as each major element is consumed in the center, progressively heavier elements ignite, temporarily halting collapse. If the core of the star is not too massive (less than approximately , taking into account mass loss that has occurred by this time), it may then form a white dwarf (possibly surrounded by a planetary nebula) as described above for less-massive stars, with the difference that the white dwarf is composed chiefly of oxygen, neon, and magnesium.\n\nAbove a certain mass (estimated at approximately and whose star's progenitor was around ), the core will reach the temperature (approximately 1.1 gigakelvins) at which neon partially breaks down to form oxygen and helium, the latter of which immediately fuses with some of the remaining neon to form magnesium; then oxygen fuses to form sulfur, silicon, and smaller amounts of other elements. Finally, the temperature gets high enough that any nucleus can be partially broken down, most commonly releasing an alpha particle (helium nucleus) which immediately fuses with another nucleus, so that several nuclei are effectively rearranged into a smaller number of heavier nuclei, with net release of energy because the addition of fragments to nuclei exceeds the energy required to break them off the parent nuclei.\n\nA star with a core mass too great to form a white dwarf but insufficient to achieve sustained conversion of neon to oxygen and magnesium, will undergo core collapse (due to electron capture) before achieving fusion of the heavier elements. Both heating and cooling caused by electron capture onto minor constituent elements (such as aluminum and sodium) prior to collapse may have a significant impact on total energy generation within the star shortly before collapse. This may produce a noticeable effect on the abundance of elements and isotopes ejected in the subsequent supernova.\n\nOnce the nucleosynthesis process arrives at iron-56, the continuation of this process consumes energy (the addition of fragments to nuclei releases less energy than required to break them off the parent nuclei). If the mass of the core exceeds the Chandrasekhar limit, electron degeneracy pressure will be unable to support its weight against the force of gravity, and the core will undergo sudden, catastrophic collapse to form a neutron star or (in the case of cores that exceed the Tolman-Oppenheimer-Volkoff limit), a black hole. Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.\n\nThe energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Note that current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.\n\nSome evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.\n\nThe most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.\nAfter a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.\n\nFor a star of , the resulting white dwarf is of about , compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.\n\nA white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence, but will have lost most of its energy after a billion years.\n\nThe chemical composition of the white dwarf depends upon its mass. A star of a few solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.\n\nIn the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.\n\nIf the white dwarf's mass increases above the Chandrasekhar limit, which is for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.\n\nIf a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.\n\nOrdinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli Exclusion Principle, in a way analogous to electron degeneracy pressure, but stronger.\n\nThese stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.\n\nIf the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and .\n\nBlack holes are predicted by the theory of general relativity. According to classical general relativity, no matter or information can flow from the interior of a black hole to an outside observer, although quantum effects may allow deviations from this strict rule. The existence of black holes in the universe is well supported, both theoretically and by astronomical observation.\n\nBecause the core-collapse mechanism of a supernova is, at present, only partially understood, it is still not known whether it is possible for a star to collapse directly to a black hole without producing a visible supernova, or whether some supernovae initially form unstable neutron stars which then collapse into black holes; the exact relation between the initial mass of the star and the final remnant is also not completely certain. Resolution of these uncertainties requires the analysis of more supernovae and supernova remnants.\n\nA stellar evolutionary model is a mathematical model that can be used to compute the evolutionary phases of a star from its formation until it becomes a remnant. The mass and chemical composition of the star are used as the inputs, and the luminosity and surface temperature are the only constraints. The model formulae are based upon the physical understanding of the star, usually under the assumption of hydrostatic equilibrium. Extensive computer calculations are then run to determine the changing state of the star over time, yielding a table of data that can be used to determine the evolutionary track of the star across the Hertzsprung–Russell diagram, along with other evolving properties. Accurate models can be used to estimate the current age of a star by comparing its physical properties with those of stars along a matching evolutionary track.\n\n\n\n"}
{"id": "39672605", "url": "https://en.wikipedia.org/wiki?curid=39672605", "title": "Sunshine duration", "text": "Sunshine duration\n\nSunshine duration or sunshine hours is a climatological indicator, measuring duration of sunshine in given period (usually, a day or a year) for a given location on Earth, typically expressed as an averaged value over several years. It is a general indicator of cloudiness of a location, and thus differs from insolation, which measures the total energy delivered by sunlight over a given period.\n\nSunshine duration is usually expressed in hours per year, or in (average) hours per day. The first measure indicates the general sunniness of a location compared with other places, while the latter allows for comparison of sunshine in various seasons in the same location. Another often-used measure is percentage ratio of recorded bright sunshine duration and daylight duration in the observed period.\n\nAn important use of sunshine duration data is to characterize the climate of sites, especially of health resorts. This also takes into account the psychological effect of strong solar light on human well-being. It is often used to promote tourist destinations.\n\nIf the Sun were to be above the horizon 50% of the time for a standard year consisting of 8,760 hours, apparent maximal daytime duration would be 4,380 hours for any point on Earth. However, there are physical and astronomical effects that change that picture. Namely, atmospheric refraction allows the Sun to be still visible even when it physically sets below the horizon. For that reason, average daytime (disregarding cloud effects) is longest in polar areas, where the apparent Sun spends the most time around the horizon. Places on the Arctic Circle have the longest total annual daytime, 4,647 hours, while the North Pole receives 4,575. Because of elliptic nature of the Earth's orbit, the Southern Hemisphere is not symmetrical: the Antarctic Circle, with 4,530 hours of daylight, receives five days less of sunshine than its antipodes. The Equator has a total daytime of 4,422 hours per year.\n\nGiven the theoretical maximum of daytime duration for a given location, there is also a practical consideration at which point the amount of daylight is sufficient to be treated as a \"sunshine hour\". \"Bright\" sunshine hours represent the total hours when the sunlight is stronger than a specified threshold, as opposed to just \"visible\" hours. \"Visible\" sunshine, for example, occurs around sunrise and sunset, but is not strong enough to excite the sensor. Measurement is performed by instruments called sunshine recorders. For the specific purpose of sunshine duration recording, Campbell–Stokes recorders are used, which use a spherical glass lens to focus the sun rays on a specially designed tape. When the intensity exceeds a pre-determined threshold, the tape burns. The total length of the burn trace is proportional to the number of bright hours. Another type of recorder is the Jordan sunshine recorder. Newer, electronic recorders have more stable sensitivity than that of the paper tape.\n\nIn order to harmonize the data measured worldwide, in 1962 the World Meteorological Organization (WMO) defined a standardized design of the Campbell–Stokes recorder, called an Interim Reference Sunshine Recorder (IRSR). In 2003, the sunshine duration was finally defined as the period during which direct solar irradiance exceeds a threshold value of 120 W/m².\n\nSunshine duration follows a general geographic pattern: subtropical latitudes (about 25° to 40° north/south) have the highest sunshine values, because these are the locations of the eastern sides of the subtropical high pressure systems, associated with the large-scale descent of air from the upper-level tropopause. Many of the world's driest climates are found adjacent to the eastern sides of the subtropical highs, which create stable atmospheric conditions, little convective overturning, and little moisture and cloud cover. Desert regions, with nearly constant high pressure aloft and rare condensation—like North Africa, the Southwestern United States, Western Australia, and the Middle East—are examples of hot, sunny, dry climates where sunshine duration values are very high.\n\nThe two major areas with the highest sunshine duration, measured as annual average, are the central and the eastern Sahara Desert—covering vast, mainly desert countries such as Egypt, Sudan, Libya, Chad, and Niger—and the Southwestern United States (Arizona, California, Nevada). The city claiming the official title of the sunniest in the world is Yuma, Arizona, with over 4,000 hours (about 91% of daylight time) of bright sunshine annually, but many climatological books suggest there may be sunnier areas in North Africa. In the belt encompassing northern Chad and the Tibesti Mountains, northern Sudan, southern Libya, and Upper Egypt, annual sunshine duration is estimated at over 4,000 hours. There is also a smaller, isolated area of sunshine maximum in the heart of the western section of the Sahara Desert around the Eglab Massif and the Erg Chech, along the borders of Algeria, Mauritania, and Mali where the 4,000-hour mark is exceeded, too. Some places in the interior of the Arabian Peninsula receive 3,600–3,800 hours of bright sunshine annually. The largest sun-baked region in the world (over 3,000 hours of yearly sunshine) is North Africa. The sunniest month in the world is December in Eastern Antarctica, with almost 23 hours of bright sun daily.\n\nConversely, higher latitudes (above 50° north/south) lying in stormy westerlies have much cloudier and more unstable and rainy weather, and often have the lowest values of sunshine duration annually. Temperate oceanic climates like those in northwestern Europe, the western coast of Canada, and areas of New Zealand's South Island are examples of cool, cloudy, wet, humid climates where cloudless sunshine duration values are very low. The areas with the lowest sunshine duration annually lie mostly over the polar oceans, as well as parts of northern Europe, southern Alaska, northern Russia, and areas near the Sea of Okhotsk. The cloudiest place in the United States is Cold Bay, Alaska, with an average of 304 days of heavy overcast (covering over 3/4 of the sky). In addition to these polar oceanic climates, certain low-latitude basins enclosed by mountains, like the Sichuan and Taipei Basins, can have sunshine duration as low as 1,000 hours per year, as cool air consistently sinks to form fogs that winds cannot dissipate. Tórshavn in the Faroe Islands is among the cloudiest places in the world with yearly only 840 sunshine hours.\n\n\n"}
{"id": "33307786", "url": "https://en.wikipedia.org/wiki?curid=33307786", "title": "Supercapacitor", "text": "Supercapacitor\n\nA supercapacitor (SC) (also called a supercap, ultracapacitor or Goldcap) is a high-capacity capacitor with capacitance values much higher than other capacitors (but lower voltage limits) that bridge the gap between electrolytic capacitors and rechargeable batteries. They typically store 10 to 100 times more energy per unit volume or mass than electrolytic capacitors, can accept and deliver charge much faster than batteries, and tolerate many more charge and discharge cycles than rechargeable batteries.\n\nSupercapacitors are used in applications requiring many rapid charge/discharge cycles rather than long term compact energy storage: within cars, buses, trains, cranes and elevators, where they are used for regenerative braking, short-term energy storage or burst-mode power delivery. Smaller units are used as memory backup for static random-access memory (SRAM).\n\nUnlike ordinary capacitors, supercapacitors do not use the conventional solid dielectric, but rather, they use electrostatic double-layer capacitance and electrochemical pseudocapacitance, both of which contribute to the total capacitance of the capacitor, with a few differences:\n\n\nThe electrolyte forms an ionic conductive connection between the two electrodes which distinguishes them from conventional electrolytic capacitors where a dielectric layer always exists, and the so-called electrolyte (e.g., MnO or conducting polymer) is in fact part of the second electrode (the cathode, or more correctly the positive electrode). Supercapacitors are polarized by design with asymmetric electrodes, or, for symmetric electrodes, by a potential applied during manufacture.\n\nDevelopment of the double layer and pseudocapacitance models (see Double layer (interfacial)).\n\nIn the early 1950s, General Electric engineers began experimenting with porous carbon electrodes, in the design of capacitors, from the design of fuel cells and rechargeable batteries. Activated charcoal is an electrical conductor that is an extremely porous \"spongy\" form of carbon with a high specific surface area. In 1957 H. Becker developed a \"Low voltage electrolytic capacitor with porous carbon electrodes\". He believed that the energy was stored as a charge in the carbon pores as in the pores of the etched foils of electrolytic capacitors. Because the double layer mechanism was not known by him at the time, he wrote in the patent: \"It is not known exactly what is taking place in the component if it is used for energy storage, but it leads to an extremely high capacity.\"\n\nGeneral Electric did not immediately pursue this work. In 1966 researchers at Standard Oil of Ohio (SOHIO) developed another version of the component as \"electrical energy storage apparatus\", while working on experimental fuel cell designs. The nature of electrochemical energy storage was not described in this patent. Even in 1970, the electrochemical capacitor patented by Donald L. Boos was registered as an electrolytic capacitor with activated carbon electrodes.\n\nEarly electrochemical capacitors used two aluminum foils covered with activated carbon—the electrodes—which were soaked in an electrolyte and separated by a thin porous insulator. This design gave a capacitor with a capacitance on the order of one farad, significantly higher than electrolytic capacitors of the same dimensions. This basic mechanical design remains the basis of most electrochemical capacitors.\n\nSOHIO did not commercialize their invention, licensing the technology to NEC, who finally marketed the results as \"supercapacitors\" in 1971, to provide backup power for computer memory.\nBetween 1975 and 1980 Brian Evans Conway conducted extensive fundamental and development work on ruthenium oxide electrochemical capacitors. In 1991 he described the difference between \"Supercapacitor\" and \"Battery\" behavior in electrochemical energy storage. In 1999 he coined the term supercapacitor to explain the increased capacitance by surface redox reactions with faradaic charge transfer between electrodes and ions. His \"supercapacitor\" stored electrical charge partially in the Helmholtz double-layer and partially as result of faradaic reactions with \"pseudocapacitance\" charge transfer of electrons and protons between electrode and electrolyte. The working mechanisms of pseudocapacitors are redox reactions, intercalation and electrosorption (adsorption onto a surface). With his research, Conway greatly expanded the knowledge of electrochemical capacitors.\n\nThe market expanded slowly. That changed around 1978 as Panasonic marketed its Goldcaps brand. This product became a successful energy source for memory backup applications. Competition started only years later. In 1987 ELNA \"Dynacap\"s entered the market. First generation EDLC's had relatively high internal resistance that limited the discharge current. They were used for low current applications such as powering SRAM chips or for data backup.\n\nAt the end of the 1980s, improved electrode materials increased capacitance values. At the same time, the development of electrolytes with better conductivity lowered the equivalent series resistance (ESR) increasing charge/discharge currents. The first supercapacitor with low internal resistance was developed in 1982 for military applications through the Pinnacle Research Institute (PRI), and were marketed under the brand name \"PRI Ultracapacitor\". In 1992, Maxwell Laboratories (later Maxwell Technologies) took over this development. Maxwell adopted the term Ultracapacitor from PRI and called them \"Boost Caps\" to underline their use for power applications.\n\nSince capacitors' energy content increases with the square of the voltage, researchers were looking for a way to increase the electrolyte's breakdown voltage. In 1994 using the anode of a 200V high voltage tantalum electrolytic capacitor, David A. Evans developed an \"Electrolytic-Hybrid Electrochemical Capacitor\". These capacitors combine features of electrolytic and electrochemical capacitors. They combine the high dielectric strength of an anode from an electrolytic capacitor with the high capacitance of a pseudocapacitive metal oxide (ruthenium (IV) oxide) cathode from an electrochemical capacitor, yielding a hybrid electrochemical capacitor. Evans' capacitors, coined Capattery, had an energy content about a factor of 5 higher than a comparable tantalum electrolytic capacitor of the same size. Their high costs limited them to specific military applications.\n\nRecent developments include lithium-ion capacitors. These hybrid capacitors were pioneered by FDK in 2007. They combine an electrostatic carbon electrode with a pre-doped lithium-ion electrochemical electrode. This combination increases the capacitance value. Additionally, the pre-doping process lowers the anode potential and results in a high cell output voltage, further increasing specific energy.\n\nResearch departments active in many companies and universities are working to improve characteristics such as specific energy, specific power, and cycle stability and to reduce production costs.\n\nElectrochemical capacitors (supercapacitors) consist of two electrodes separated by an ion-permeable membrane (separator), and an electrolyte ionically connecting both electrodes. When the electrodes are polarized by an applied voltage, ions in the electrolyte form electric double layers of opposite polarity to the electrode's polarity. For example, positively polarized electrodes will have a layer of negative ions at the electrode/electrolyte interface along with a charge-balancing layer of positive ions adsorbing onto the negative layer. The opposite is true for the negatively polarized electrode.\n\nAdditionally, depending on electrode material and surface shape, some ions may permeate the double layer becoming specifically adsorbed ions and contribute with pseudocapacitance to the total capacitance of the supercapacitor.\n\nThe two electrodes form a series circuit of two individual capacitors \"C\" and \"C\". The total capacitance \"C\" is given by the formula\n\nSupercapacitors may have either symmetric or asymmetric electrodes. Symmetry implies that both electrodes have the same capacitance value, yielding a total capacitance of half the value of each single electrode (if \"C\" = \"C\", then \"C\" = ½ \"C\"). For asymmetric capacitors, the total capacitance can be taken as that of the electrode with the smaller capacitance (if \"C\" <nowiki»></nowiki> \"C\", then \"C\" ≈ \"C\").\n\nElectrochemical capacitors use the double-layer effect to store electric energy; however, this double-layer has no conventional solid dielectric to separate the charges. There are two storage principles in the electric double-layer of the electrodes that contribute to the total capacitance of an electrochemical capacitor:\n\n\nBoth capacitances are only separable by measurement techniques. The amount of charge stored per unit voltage in an electrochemical capacitor is primarily a function of the electrode size, although the amount of capacitance of each storage principle can vary extremely.\n\nPractically, these storage principles yield a capacitor with \n\nEvery electrochemical capacitor has two electrodes, mechanically separated by a separator, which are ionically connected to each other via the electrolyte. The electrolyte is a mixture of positive and negative ions dissolved in a solvent such as water. At each of the two electrode surfaces originates an area in which the liquid electrolyte contacts the conductive metallic surface of the electrode. This interface forms a common boundary among two different phases of matter, such as an insoluble solid electrode surface and an adjacent liquid electrolyte. In this interface occurs a very special phenomenon of the double layer effect.\n\nApplying a voltage to an electrochemical capacitor causes both electrodes in the capacitor to generate electrical double-layers. These double-layers consist of two layers of charges: one electronic layer is in the surface lattice structure of the electrode, and the other, with opposite polarity, emerges from dissolved and solvated ions in the electrolyte. The two layers are separated by a monolayer of solvent molecules, e. g. for water as solvent by water molecules, called inner Helmholtz plane (IHP). Solvent molecules adhere by physical adsorption on the surface of the electrode and separate the oppositely polarized ions from each other, and can be idealised as a molecular dielectric. In the process, there is no transfer of charge between electrode and electrolyte, so the forces that cause the adhesion are not chemical bonds but physical forces (e.g. electrostatic forces). The adsorbed molecules are polarized but, due to the lack of transfer of charge between electrolyte and electrode, suffered no chemical changes.\n\nThe amount of charge in the electrode is matched by the magnitude of counter-charges in outer Helmholtz plane (OHP). This double-layer phenomena stores electrical charges as in a conventional capacitor. The double-layer charge forms a static electric field in the molecular layer of the solvent molecules in the IHP that corresponds to the strength of the applied voltage.\n\nThe double-layer serves approximately as the dielectric layer in a conventional capacitor, albeit with the thickness of a single molecule. Thus, the standard formula for conventional plate capacitors can be used to calculate their capacitance:\n\nAccordingly, capacitance \"C\" is greatest in capacitors made from materials with a high permittivity \"ε\", large electrode plate surface areas \"A\" and small distance between plates \"d\". As a result, double-layer capacitors have much higher capacitance values than conventional capacitors, arising from the extremely large surface area of activated carbon electrodes and the extremely thin double-layer distance on the order of a few ångströms (0.3-0.8 nm), of order of the Debye length.\n\nThe main drawback of carbon electrodes of double-layer SCs is small values of quantum capacitance which act in series with capacitance of ionic space charge. Therefore, further increase of density of capacitance in SCs can be connected with increasing of quantum capacitance of carbon electrode nanostructures.\n\nThe amount of charge stored per unit voltage in an electrochemical capacitor is primarily a function of the electrode size. The electrostatic storage of energy in the double-layers is linear with respect to the stored charge, and correspond to the concentration of the adsorbed ions. Also, while charge in conventional capacitors is transferred via electrons, capacitance in double-layer capacitors is related to the limited moving speed of ions in the electrolyte and the resistive porous structure of the electrodes. Since no chemical changes take place within the electrode or electrolyte, charging and discharging electric double-layers in principle is unlimited. Real supercapacitors lifetimes are only limited by electrolyte evaporation effects.\n\nApplying a voltage at the electrochemical capacitor terminals moves electrolyte ions to the opposite polarized electrode and forms a double-layer in which a single layer of solvent molecules acts as separator. Pseudocapacitance can originate when specifically adsorbed ions out of the electrolyte pervade the double-layer. This pseudocapacitance stores electrical energy by means of reversible faradaic redox reactions on the surface of suitable electrodes in an electrochemical capacitor with an electric double-layer. Pseudocapacitance is accompanied with an electron charge-transfer between electrolyte and electrode coming from a de-solvated and adsorbed ion whereby only one electron per charge unit is participating. This faradaic charge transfer originates by a very fast sequence of reversible redox, intercalation or electrosorption processes. The adsorbed ion has no chemical reaction with the atoms of the electrode (no chemical bonds arise) since only a charge-transfer take place.\nThe electrons involved in the faradaic processes are transferred to or from valence electron states (orbitals) of the redox electrode reagent. They enter the negative electrode and flow through the external circuit to the positive electrode where a second double-layer with an equal number of anions has formed. The electrons reaching the positive electrode are not transferred to the anions forming the double-layer, instead they remain in the strongly ionized and \"electron hungry\" transition-metal ions of the electrode's surface. As such, the storage capacity of faradaic pseudocapacitance is limited by the finite quantity of reagent in the available surface.\n\nA faradaic pseudocapacitance only occurs together with a static double-layer capacitance, and its magnitude may exceed the value of double-layer capacitance for the same surface area by factor 100, depending on the nature and the structure of the electrode because all the pseudocapacitance reactions take place only with de-solvated ions, which are much smaller than solvated ion with their solvating shell. The amount of pseudocapacitance has a linear function within narrow limits determined by the potential-dependent degree of surface coverage of the adsorbed anions.\n\nThe ability of electrodes to accomplish pseudocapacitance effects by redox reactions, intercalation or electrosorption strongly depends on the chemical affinity of electrode materials to the ions adsorbed on the electrode surface as well as on the structure and dimension of the electrode pores. Materials exhibiting redox behavior for use as electrodes in pseudocapacitors are transition-metal oxides like RuO, IrO, or MnO inserted by doping in the conductive electrode material such as active carbon, as well as conducting polymers such as polyaniline or derivatives of polythiophene covering the electrode material.\n\nThe amount of electric charge stored in a pseudocapacitance is linearly proportional to the applied voltage. The unit of pseudocapacitance is farad.\n\nConventional capacitors (also known as electrostatic capacitors), such as ceramic capacitors and film capacitors, consist of two electrodes which are separated by a dielectric material. When charged, the energy is stored in a static electric field that permeates the dielectric between the electrodes. The total energy increases with the amount of stored charge, which in turn correlates linearly with the potential (voltage) between the plates. The maximum potential difference between the plates (the maximal voltage) is limited by the dielectric's breakdown field strength. The same static storage also applies for electrolytic capacitors in which most of the potential decreases over the anode's thin oxide layer. The somewhat resistive liquid electrolyte (cathode) accounts for a small decrease of potential for \"wet\" electrolytic capacitors, while electrolytic capacitors with solid conductive polymer electrolyte this voltage drop is negligible.\n\nIn contrast, electrochemical capacitors (supercapacitors) consists of two electrodes separated by an ion-permeable membrane (separator) and electrically connected via an electrolyte. Energy storage occurs within the double-layers of both electrodes as a mixture of a double-layer capacitance and pseudocapacitance. When both electrodes have approximately the same resistance (internal resistance), the potential of the capacitor decreases symmetrically over both double-layers, whereby a voltage drop across the equivalent series resistance (ESR) of the electrolyte is achieved. For asymmetrical supercapacitors like hybrid capacitors the voltage drop between the electrodes could be asymmetrical. The maximum potential across the capacitor (the maximal voltage) is limited by the electrolyte decomposition voltage.\n\nBoth electrostatic and electrochemical energy storage in supercapacitors are linear with respect to the stored charge, just as in conventional capacitors. The voltage between the capacitor terminals is linear with respect to the amount of stored energy. Such linear voltage gradient differs from rechargeable electrochemical batteries, in which the voltage between the terminals remains independent of the amount of stored energy, providing a relatively constant voltage.\n\nSupercapacitors compete with electrolytic capacitors and rechargeable batteries especially lithium-ion batteries. The following table compares the major parameters of the three main supercapacitor families with electrolytic capacitors and batteries.\n\nElectrolytic capacitors feature unlimited charge/discharge cycles, high dielectric strength (up to 550 V) and good frequency response as AC resistance in the lower frequency range. Supercapacitors can store 10 to 100 times more energy than electrolytic capacitors but they do not support AC applications.\n\nWith regards to rechargeable batteries supercapacitors feature higher peak currents, low cost per cycle, no danger of overcharging, good reversibility, non-corrosive electrolyte and low material toxicity, while batteries offer, lower purchase cost, stable voltage under discharge, but they require complex electronic control and switching equipment, with consequent energy loss and spark hazard given a short.\n\nSupercapacitors are made in different styles such as flat with a single pair of electrodes, wound in a cylindrical case or stacked in a rectangular case. Because they cover a broad range of capacitance values the size of the cases can vary.\nSupercapacitors are constructed with two metal foils (current collectors), each coated with an electrode material such as activated carbon, which serve as the power connection between the electrode material and the external terminals of the capacitor. Specifically to the electrode material is a very large surface area. In this example the activated carbon is electrochemically etched, so that the surface of the material is about a factor 100,000 larger than the smooth surface. The electrodes are kept apart by an ion-permeable membrane (separator) used as an insulator to protect the electrodes against short circuits. This construction is subsequently rolled or folded into a cylindrical or rectangular shape and can be stacked in an aluminum can or an adaptable rectangular housing. Then the cell is impregnated with a liquid or viscous electrolyte of organic or aqueous type. The electrolyte, an ionic conductor, enters the pores of the electrodes and serves as the conductive connection between the electrodes across the separator. Finally the housing is hermetically sealed to ensure stable behavior over the specified lifetime.\n\nElectrical energy is stored in supercapacitors via two storage principles: static double-layer capacitance and electrochemical pseudocapacitance; and the distribution of the two types of capacitance depends on the material and structure of the electrodes. There are three types of supercapacitors based on storage principle:\n\nBecause double-layer capacitance and pseudocapacitance both contribute inseparably to the total capacitance value of an electrochemical capacitor, a correct description of these capacitors only can be given under the generic term. The concepts of supercapattery and supercabattery have been recently proposed to better represent those hybrid devices that behave more like the supercapacitor and the rechargeable battery, respectively.\n\nThe capacitance value of a supercapacitor is determined by two storage principles:\n\n\nDouble-layer capacitance and pseudocapacitance both contribute inseparably to the total capacitance value of a supercapacitor. However, the ratio of the two can vary greatly, depending on the design of the electrodes and the composition of the electrolyte. Pseudocapacitance can increase the capacitance value by as much as a factor of ten over that of the double-layer by itself.\n\nElectric double-layer capacitors (EDLC) are electrochemical capacitors in which energy storage predominantly is achieved by double-layer capacitance. In the past, all electrochemical capacitors were called \"double-layer capacitors\". Contemporary usage sees double-layer capacitors, together with pseudocapacitors, as part of a larger family of electrochemical capacitors called supercapacitors. They are also known as ultracapacitors. \nThe properties of supercapacitors come from the interaction of their internal materials. Especially, the combination of electrode material and type of electrolyte determine the functionality and thermal and electrical characteristics of the capacitors.\n\nSupercapacitor electrodes are generally thin coatings applied and electrically connected to a conductive, metallic current collector. Electrodes must have good conductivity, high temperature stability, long-term chemical stability (inertness), high corrosion resistance and high surface areas per unit volume and mass. Other requirements include environmental friendliness and low cost.\n\nThe amount of double-layer as well as pseudocapacitance stored per unit voltage in a supercapacitor is predominantly a function of the electrode surface area. Therefore, supercapacitor electrodes are typically made of porous, spongy material with an extraordinarily high specific surface area, such as activated carbon. Additionally, the ability of the electrode material to perform faradaic charge transfers enhances the total capacitance.\n\nGenerally the smaller the electrode's pores, the greater the capacitance and specific energy. However, smaller pores increase equivalent series resistance (ESR) and decrease specific power. Applications with high peak currents require larger pores and low internal losses, while applications requiring high specific energy need small pores.\n\nThe most commonly used electrode material for supercapacitors is carbon in various manifestations such as activated carbon (AC), carbon fibre-cloth (AFC), carbide-derived carbon (CDC), carbon aerogel, graphite (graphene), graphane and carbon nanotubes (CNTs).\n\nCarbon-based electrodes exhibit predominantly static double-layer capacitance, even though a small amount of pseudocapacitance may also be present depending on the pore size distribution. Pore sizes in carbons typically range from micropores (less than 2 nm) to mesopores (2-50 nm), but only micropores (<2 nm) contribute to pseudocapacitance. As pore size approaches the solvation shell size, solvent molecules are excluded and only unsolvated ions fill the pores (even for large ions), increasing ionic packing density and storage capability by faradaic intercalation.\n\nActivated carbon (AC) was the first material chosen for EDLC electrodes. Even though its electrical conductivity is approximately 0.003% that of metals (1,250 to 2,000 S/m), it is sufficient for supercapacitors.\n\nActivated carbon is an extremely porous form of carbon with a high specific surface area — a common approximation is that 1 gram (0.035 oz) (a pencil-eraser-sized amount) has a surface area of roughly — about the size of 4 to 12 tennis courts. The bulk form used in electrodes is low-density with many pores, giving high double-layer capacitance.\n\nSolid activated carbon, also termed \"consolidated amorphous carbon\" (CAC) is the most used electrode material for supercapacitors and may be cheaper than other carbon derivatives. It is produced from activated carbon powder pressed into the desired shape, forming a block with a wide distribution of pore sizes. An electrode with a surface area of about 1000 m/g results in a typical double-layer capacitance of about 10 μF/cm and a specific capacitance of 100 F/g.\n\nActivated carbon fibres (ACF) are produced from activated carbon and have a typical diameter of 10 µm. They can have micropores with a very narrow pore-size distribution that can be readily controlled. The surface area of ACF woven into a textile is about . Advantages of ACF electrodes include low electrical resistance along the fibre axis and good contact to the collector.\n\nAs for activated carbon, ACF electrodes exhibit predominantly double-layer capacitance with a small amount of pseudocapacitance due to their micropores.\n\nCarbon aerogel is a highly porous, synthetic, ultralight material derived from an organic gel in which the liquid component of the gel has been replaced with a gas.\n\nAerogel electrodes are made via pyrolysis of resorcinol-formaldehyde aerogels and are more conductive than most activated carbons. They enable thin and mechanically stable electrodes with a thickness in the range of several hundred micrometres (µm) and with uniform pore size. Aerogel electrodes also provide mechanical and vibration stability for supercapacitors used in high-vibration environments.\n\nResearchers have created a carbon aerogel electrode with gravimetric densities of about 400–1200 m/g and volumetric capacitance of 104 F/cm, yielding a specific energy of () and specific power of .\n\nStandard aerogel electrodes exhibit predominantly double-layer capacitance. Aerogel electrodes that incorporate composite material can add a high amount of pseudocapacitance.\nCarbide-derived carbon (CDC), also known as tunable nanoporous carbon, is a family of carbon materials derived from carbide precursors, such as binary silicon carbide and titanium carbide, that are transformed into pure carbon via physical (e.g., thermal decomposition) or chemical (e.g., halogenation) processes.\n\nCarbide-derived carbons can exhibit high surface area and tunable pore diameters (from micropores to mesopores) to maximize ion confinement, increasing pseudocapacitance by faradaic adsorption treatment. CDC electrodes with tailored pore design offer as much as 75% greater specific energy than conventional activated carbons.\n\n, a CDC supercapacitor offered a specific energy of 10.1 Wh/kg, 3,500 F capacitance and over one million charge-discharge cycles.\n\nGraphene is a one-atom thick sheet of graphite, with atoms arranged in a regular hexagonal pattern, also called \"nanocomposite paper\".\n\nGraphene has a theoretical specific surface area of 2630 m/g which can theoretically lead to a capacitance of 550 F/g. In addition, an advantage of graphene over activated carbon is its higher electrical conductivity. a new development used graphene sheets directly as electrodes without collectors for portable applications.\n\nIn one embodiment, a graphene-based supercapacitor uses curved graphene sheets that do not stack face-to-face, forming mesopores that are accessible to and wettable by ionic electrolytes at voltages up to 4 V. A specific energy of () is obtained at room temperature equaling that of a conventional nickel metal hydride battery, but with 100-1000 times greater specific power.\n\nThe two-dimensional structure of graphene improves charging and discharging. Charge carriers in vertically oriented sheets can quickly migrate into or out of the deeper structures of the electrode, thus increasing currents. Such capacitors may be suitable for 100/120 Hz filter applications, which are unreachable for supercapacitors using other carbon materials.\n\nCarbon nanotubes (CNTs), also called buckytubes, are carbon molecules with a cylindrical nanostructure. They have a hollow structure with walls formed by one-atom-thick sheets of graphite. These sheets are rolled at specific and discrete (\"chiral\") angles, and the combination of chiral angle and radius controls properties such as electrical conductivity, electrolyte wettability and ion access. Nanotubes are categorized as single-walled nanotubes (SWNTs) or multi-walled nanotubes (MWNTs). The latter have one or more outer tubes successively enveloping a SWNT, much like the Russian matryoshka dolls. SWNTs have diameters ranging between 1 and 3 nm. MWNTs have thicker coaxial walls, separated by spacing (0.34 nm) that is close to graphene's interlayer distance.\n\nNanotubes can grow vertically on the collector substrate, such as a silicon wafer. Typical lengths are 20 to 100 µm.\n\nCarbon nanotubes can greatly improve capacitor performance, due to the highly wettable surface area and high conductivity.\n\nA SWNT-based supercapacitor with aqueous electrolyte was systematically studied at University of Delaware in Prof. Bingqing Wei's group. Li et al., for the first time, discovered that the ion-size effect and the electrode-electrolyte wettability are the dominant factors affecting the electrochemical behavior of flexible SWCNTs-supercapacitors in different 1 molar aqueous electrolytes with different anions and cations. The experimental results also showed for flexible supercapacitor that it is suggested to put enough pressure between the two electrodes to improve the aqueous electrolyte CNT supercapacitor.\n\nCNTs can store about the same charge as activated carbon per unit surface area, but nanotubes' surface is arranged in a regular pattern, providing greater wettability. SWNTs have a high theoretical specific surface area of 1315 m/g, while that for MWNTs is lower and is determined by the diameter of the tubes and degree of nesting, compared with a surface area of about 3000 m/g of activated carbons. Nevertheless, CNTs have higher capacitance than activated carbon electrodes, e.g., 102 F/g for MWNTs and 180 F/g for SWNTs.\n\nMWNTs have mesopores that allow for easy access of ions at the electrode–electrolyte interface. As the pore size approaches the size of the ion solvation shell, the solvent molecules are partially stripped, resulting in larger ionic packing density and increased faradaic storage capability. However, the considerable volume change during repeated intercalation and depletion decreases their mechanical stability. To this end, research to increase surface area, mechanical strength, electrical conductivity and chemical stability is ongoing.\n\nMnO and RuO are typical materials used as electrodes for pseudocapacitors, since they have the electrochemical signature of a capacitive electrode (linear dependence on current versus voltage curve) as well as exhibiting faradaic behavior. Additionally, the charge storage originates from electron-transfer mechanisms rather than accumulation of ions in the electrochemical double layer. Pseudocapacitors were created through faradaic redox reactions that occur within the active electrode materials. More research was focused on transition-metal oxides such as MnO since transition-metal oxides have a lower cost compared to noble metal oxides such as RuO. Moreover, the charge storage mechanisms of transition-metal oxides are based predominantly on pseudocapacitance. Two mechanisms of MnO charge storage behavior were introduced. The first mechanism implies the intercalation of protons (H) or alkali metal cations (C) in the bulk of the material upon reduction followed by deintercalation upon oxidation.\n\nThe second mechanism is based on the surface adsorption of electrolyte cations on MnO.\n\nNot every material that exhibits faradaic behavior can be used as an electrode for pseudocapacitors, such as Ni(OH) since it is a battery type electrode (non-linear dependence on current versus voltage curve).\n\nBrian Evans Conway's research described electrodes of transition metal oxides that exhibited high amounts of pseudocapacitance. Oxides of transition metals including ruthenium (), iridium (), iron (), manganese () or sulfides such as titanium sulfide () alone or in combination generate strong faradaic electron–transferring reactions combined with low resistance. Ruthenium dioxide in combination with electrolyte provides specific capacitance of 720 F/g and a high specific energy of 26.7 Wh/kg ().\n\nCharge/discharge takes place over a window of about 1.2 V per electrode. This pseudocapacitance of about 720 F/g is roughly 100 times higher than for double-layer capacitance using activated carbon electrodes. These transition metal electrodes offer excellent reversibility, with several hundred-thousand cycles. However, ruthenium is expensive and the 2.4 V voltage window for this capacitor limits their applications to military and space applications.\nDas et al. reported highest capacitance value (1715 F/g) for ruthenium oxide based supercapacitor with electrodeposited ruthenium oxide onto porous single wall carbon nanotube film electrode. A high specific capacitance of 1715 F/g has been reported which closely approaches the predicted theoretical maximum capacitance of 2000 F/g.\n\nIn 2014 a supercapacitor anchored on a graphene foam electrode delivered specific capacitance of 502.78 F/g and areal capacitance of 1.11 F/cm) leading to a specific energy of 39.28 Wh/kg and specific power of 128.01 kW/kg over 8,000 cycles with constant performance. The device was a three-dimensional (3D) sub-5 nm hydrous ruthenium-anchored graphene and carbon nanotube (CNT) hybrid foam (RGM) architecture. The graphene foam was conformally covered with hybrid networks of nanoparticles and anchored CNTs.\n\nLess expensive oxides of iron, vanadium, nickel and cobalt have been tested in aqueous electrolytes, but none has been investigated as much as manganese dioxide (). However, none of these oxides are in commercial use.\n\nAnother approach uses electron-conducting polymers as pseudocapacitive material. Although mechanically weak, conductive polymers have high conductivity, resulting in a low ESR and a relatively high capacitance. Such conducting polymers include polyaniline, polythiophene, polypyrrole and polyacetylene. Such electrodes also employ electrochemical doping or dedoping of the polymers with anions and cations. Electrodes made from or coated with conductive polymers have costs comparable to carbon electrodes.\n\nConducting polymer electrodes generally suffer from limited cycling stability. However, polyacene electrodes provide up to 10,000 cycles, much better than batteries.\n\nAll commercial hybrid supercapacitors are asymmetric. They combine an electrode with high amount of pseudocapacitance with an electrode with a high amount of double-layer capacitance. In such systems the faradaic pseudocapacitance electrode with their higher capacitance provides high specific energy while the non-faradaic EDLC electrode enables high specific power. An advantage of the hybrid-type supercapacitors compared with symmetrical EDLC's is their higher specific capacitance value as well as their higher rated voltage and correspondingly their higher specific energy.\n\nComposite electrodes for hybrid-type supercapacitors are constructed from carbon-based material with incorporated or deposited pseudocapacitive active materials like metal oxides and conducting polymers. most research for supercapacitors explores composite electrodes.\n\nCNTs give a backbone for a homogeneous distribution of metal oxide or electrically conducting polymers (ECPs), producing good pseudocapacitance and good double-layer capacitance. These electrodes achieve higher capacitances than either pure carbon or pure metal oxide or polymer-based electrodes. This is attributed to the accessibility of the nanotubes' tangled mat structure, which allows a uniform coating of pseudocapacitive materials and three-dimensional charge distribution. The process to anchor pseudocapacitve materials usually uses a hydrothermal process. However, a recent researcher, Li et al., from the University of Delaware found a facile and scalable approach to precipitate MnO2 on a SWNT film to make an organic-electrolyte based supercapacitor.\n\nAnother way to enhance CNT electrodes is by doping with a pseudocapacitive dopant as in lithium-ion capacitors. In this case the relatively small lithium atoms intercalate between the layers of carbon. The anode is made of lithium-doped carbon, which enables lower negative potential with a cathode made of activated carbon. This results in a larger voltage of 3.8-4 V that prevents electrolyte oxidation. As of 2007 they had achieved capacitance of 550 F/g. and reach a specific energy up to 14 Wh/kg ().\n\nRechargeable battery electrodes influenced the development of electrodes for new hybrid-type supercapacitor electrodes as for lithium-ion capacitors. Together with a carbon EDLC electrode in an asymmetric construction offers this configuration higher specific energy than typical supercapacitors with higher specific power, longer cycle life and faster charging and recharging times than batteries.\n\nRecently some asymmetric hybrid supercapacitors were developed in which the positive electrode were based on a real pseudocapacitive metal oxide electrode (not a composite electrode), and the negative electrode on an EDLC activated carbon electrode.\n\nAn advantage of this type of supercapacitors is their higher voltage and correspondingly their higher specific energy (up to 10-20 Wh/kg (36-72 kJ/kg)).\n\nAs far as known no commercial offered supercapacitors with such kind of asymmetric electrodes are on the market.\n\nElectrolytes consist of a solvent and dissolved chemicals that dissociate into positive cations and negative anions, making the electrolyte electrically conductive. The more ions the electrolyte contains, the better its conductivity. In supercapacitors electrolytes are the electrically conductive connection between the two electrodes. Additionally, in supercapacitors the electrolyte provides the molecules for the separating monolayer in the Helmholtz double-layer and delivers the ions for pseudocapacitance.\n\nThe electrolyte determines the capacitor's characteristics: its operating voltage, temperature range, ESR and capacitance. With the same activated carbon electrode an aqueous electrolyte achieves capacitance values of 160 F/g, while an organic electrolyte achieves only 100 F/g.\n\nThe electrolyte must be chemically inert and not chemically attack the other materials in the capacitor to ensure long time stable behavior of the capacitor's electrical parameters. The electrolyte's viscosity must be low enough to wet the porous, sponge-like structure of the electrodes. An ideal electrolyte does not exist, forcing a compromise between performance and other requirements.\n\nWater is a relatively good solvent for inorganic chemicals. Treated with acids such as sulfuric acid (), alkalis such as potassium hydroxide (KOH), or salts such as quaternary phosphonium salts, sodium perchlorate (), lithium perchlorate () or lithium hexafluoride arsenate (), water offers relatively high conductivity values of about 100 to 1000 mS/cm. Aqueous electrolytes have a dissociation voltage of 1.15 V per electrode (2.3 V capacitor voltage) and a relatively low operating temperature range. They are used in supercapacitors with low specific energy and high specific power.\n\nElectrolytes with organic solvents such as acetonitrile, propylene carbonate, tetrahydrofuran, diethyl carbonate, γ-butyrolactone and solutions with quaternary ammonium salts or alkyl ammonium salts such as tetraethylammonium tetrafluoroborate () or triethyl (metyl) tetrafluoroborate () are more expensive than aqueous electrolytes, but they have a higher dissociation voltage of typically 1.35 V per electrode (2.7 V capacitor voltage), and a higher temperature range. The lower electrical conductivity of organic solvents (10 to 60 mS/cm) leads to a lower specific power, but since the specific energy increases with the square of the voltage, a higher specific energy.\n\nSeparators have to physically separate the two electrodes to prevent a short circuit by direct contact. It can be very thin (a few hundredths of a millimeter) and must be very porous to the conducting ions to minimize ESR. Furthermore, separators must be chemically inert to protect the electrolyte's stability and conductivity. Inexpensive components use open capacitor papers. More sophisticated designs use nonwoven porous polymeric films like polyacrylonitrile or Kapton, woven glass fibers or porous woven ceramic fibres.\n\nCurrent collectors connect the electrodes to the capacitor's terminals. The collector is either sprayed onto the electrode or is a metal foil. They must be able to distribute peak currents of up to 100 A.\n\nIf the housing is made out of a metal (typically aluminum) the collectors should be made from the same material to avoid forming a corrosive galvanic cell.\n\nCapacitance values for commercial capacitors are specified as \"rated capacitance C\". This is the value for which the capacitor has been designed. The value for an actual component must be within the limits given by the specified tolerance. Typical values are in the range of farads (F), three to six orders of magnitude larger than those of electrolytic capacitors.\n\nThe capacitance value results from the energy formula_3 (expressed in Joule) of a loaded capacitor loaded via a DC voltage V.\n\nThis value is also called the \"DC capacitance\".\n\nConventional capacitors are normally measured with a small AC voltage (0.5 V) and a frequency of 100 Hz or 1 kHz depending on the capacitor type. The AC capacitance measurement offers fast results, important for industrial production lines. The capacitance value of a supercapacitor depends strongly on the measurement frequency, which is related to the porous electrode structure and the limited electrolyte's ion mobility. Even at a low frequency of 10 Hz, the measured capacitance value drops from 100 to 20 percent of the DC capacitance value.\n\nThis extraordinary strong frequency dependence can be explained by the different distances the ions have to move in the electrode's pores. The area at the beginning of the pores can easily be accessed by the ions. The short distance is accompanied by low electrical resistance. The greater the distance the ions have to cover, the higher the resistance. This phenomenon can be described with a series circuit of cascaded RC (resistor/capacitor) elements with serial RC time constants. These result in delayed current flow, reducing the total electrode surface area that can be covered with ions if polarity changes – capacitance decreases with increasing AC frequency. Thus, the total capacitance is only achieved after longer measuring times.\n\nOut of the reason of the very strong frequency dependence of the capacitance this electrical parameter has to be measured with a special constant current charge and discharge measurement, defined in IEC standards 62391-1 and -2.\n\nMeasurement starts with charging the capacitor. The voltage has to be applied and after the constant current/constant voltage power supply has achieved the rated voltage, the capacitor has to be charged for 30 minutes. Next, the capacitor has to be discharged with a constant discharge current I. Then the time t and t, for the voltage to drop from 80% (V) to 40% (V) of the rated voltage is measured. The capacitance value is calculated as:\n\nThe value of the discharge current is determined by the application. The IEC standard defines four classes:\n\nThe measurement methods employed by individual manufacturers are mainly comparable to the standardized methods.\n\nThe standardized measuring method is too time consuming for manufacturers to use during production for each individual component. For industrial produced capacitors the capacitance value is instead measured with a faster low frequency AC voltage and a correlation factor is used to compute the rated capacitance.\n\nThis frequency dependence affects capacitor operation. Rapid charge and discharge cycles mean that neither the rated capacitance value nor specific energy are available. In this case the rated capacitance value is recalculated for each application condition.\n\nSupercapacitors are low voltage components. Safe operation requires that the voltage remain within specified limits. The rated voltage U is the maximum DC voltage or peak pulse voltage that may be applied continuously and remain within the specified temperature range. Capacitors should never be subjected to voltages continuously in excess of the rated voltage.\n\nThe rated voltage includes a safety margin against the electrolyte's breakdown voltage at which the electrolyte decomposes. The breakdown voltage decomposes the separating solvent molecules in the Helmholtz double-layer, f. e. water splits into hydrogen and oxide. The solvent molecules then cannot separate the electrical charges from each other. Higher voltages than rated voltage cause hydrogen gas formation or a short circuit.\n\nStandard supercapacitors with aqueous electrolyte normally are specified with a rated voltage of 2.1 to 2.3 V and capacitors with organic solvents with 2.5 to 2.7 V. Lithium-ion capacitors with doped electrodes may reach a rated voltage of 3.8 to 4 V, but have a lower voltage limit of about 2.2 V.\n\nOperating supercapacitors below the rated voltage improves the long-time behavior of the electrical parameters. Capacitance values and internal resistance during cycling are more stable and lifetime and charge/discharge cycles may be extended.\n\nHigher application voltages require connecting cells in series. Since each component has a slight difference in capacitance value and ESR, it is necessary to actively or passively balance them to stabilize the applied voltage. Passive balancing employs resistors in parallel with the supercapacitors. Active balancing may include electronic voltage management above a threshold that varies the current.\n\nCharging/discharging a supercapacitor is connected to the movement of charge carriers (ions) in the electrolyte across the separator to the electrodes and into their porous structure. Losses occur during this movement that can be measured as the internal DC resistance.\n\nWith the electrical model of cascaded, series-connected RC (resistor/capacitor) elements in the electrode pores, the internal resistance increases with the increasing penetration depth of the charge carriers into the pores. The internal DC resistance is time dependent and increases during charge/discharge. In applications often only the switch-on and switch-off range is interesting. The internal resistance R can be calculated from the voltage drop ΔV at the time of discharge, starting with a constant discharge current I. It is obtained from the intersection of the auxiliary line extended from the straight part and the time base at the time of discharge start (see picture right). Resistance can be calculated by:\nThe discharge current I for the measurement of internal resistance can be taken from the classification according to IEC 62391-1.\n\nThis internal DC resistance R should not be confused with the internal AC resistance called equivalent series resistance (ESR) normally specified for capacitors. It is measured at 1 kHz. ESR is much smaller than DC resistance. ESR is not relevant for calculating superconductor inrush currents or other peak currents.\n\nR determines several supercapacitor properties. It limits the charge and discharge peak currents as well as charge/discharge times. R and the capacitance C results in the time constant formula_7\nThis time constant determines the charge/discharge time. A 100 F capacitor with an internal resistance of 30 mΩ for example, has a time constant of 0.03 • 100 = 3 s. After 3 seconds charging with a current limited only by internal resistance, the capacitor has 63.2% of full charge (or is discharged to 36.8% of full charge).\n\nStandard capacitors with constant internal resistance fully charge during about 5 τ. Since internal resistance increases with charge/discharge, actual times cannot be calculated with this formula. Thus, charge/discharge time depends on specific individual construction details.\n\nBecause supercapacitors operate without forming chemical bonds, current loads, including charge, discharge and peak currents are not limited by reaction constraints. Current load and cycle stability can be much higher than for rechargeable batteries. Current loads are limited only by internal resistance, which may be substantially lower than for batteries.\n\nInternal resistance \"R\" and charge/discharge currents or peak currents \"I\" generate internal heat losses \"P\" according to:\n\nThis heat must be released and distributed to the ambient environment to maintain operating temperatures below the specified maximum temperature.\n\nHeat generally defines capacitor lifetime because of electrolyte diffusion. The heat generation coming from current loads should be smaller than 5 to 10 K at maximum ambient temperature (which has only minor influence on expected lifetime). For that reason the specified charge and discharge currents for frequent cycling are determined by internal resistance.\n\nThe specified cycle parameters under maximal conditions include charge and discharge current, pulse duration and frequency. They are specified for a defined temperature range and over the full voltage range for a defined lifetime. They can differ enormously depending on the combination of electrode porosity, pore size and electrolyte. Generally a lower current load increases capacitor life and increases the number of cycles. This can be achieved either by a lower voltage range or slower charging and discharging.\n\nSupercapacitors (except those with polymer electrodes) can potentially support more than one million charge/discharge cycles without substantial capacity drops or internal resistance increases. Beneath the higher current load is this the second great advantage of supercapacitors over batteries. The stability results from the dual electrostatic and electrochemical storage principles.\n\nThe specified charge and discharge currents can be significantly exceeded by lowering the frequency or by single pulses. Heat generated by a single pulse may be spread over the time until the next pulse occurs to ensure a relatively small average heat increase. Such a \"peak power current\" for power applications for supercapacitors of more than 1000 F can provide a maximum peak current of about 1000 A. Such high currents generate high thermal stress and high electromagnetic forces that can damage the electrode-collector connection requiring robust design and construction of the capacitors.\n\nDevice parameters such as capacitance initial resistance and steady state resistance are not constant but are variable and dependent on the device's operating voltage. Device capacitance will have a measurable increase as the operating voltage increases. For example: a 100F device can be seen to vary 26% from its maximum capacitance over its entire operational voltage range. Similar dependence on operating voltage is seen in steady state resistance (R) and initial resistance (R). \n\nDevice properties can also be seen to be dependent on device temperature. As the temperature of the device changes either through operation of varying ambient temperature, the internal properties such as capacitance and resistance will vary as well. Device capacitance is seen to increase as the operating temperature increases.\n\nSupercapacitors occupy the gap between high power/low energy electrolytic capacitors and low power/high energy rechargeable batteries. The energy W (expressed in Joule) that can be stored in a capacitor is given by the formula\n\nThis formula describes the amount of energy stored and is often used to describe new research successes. However, only part of the stored energy is available to applications, because the voltage drop and the time constant over the internal resistance mean that some of the stored charge is inaccessible. The effective realized amount of energy W is reduced by the used voltage difference between V and V and can be represented as:\n\nThis formula also represents the energy asymmetric voltage components such as lithium ion capacitors.\n\nThe amount of energy that can be stored in a capacitor per mass of that capacitor is called its specific energy. Specific energy is measured gravimetrically (per unit of mass) in watt-hours per kilogram (Wh/kg).\n\nThe amount of energy can be stored in a capacitor per volume of that capacitor is called its energy density. Energy density is measured volumetrically (per unit of volume) in watt-hours per litre (Wh/l).\n\nCommercial energy density (also called volumetric specific energy in some literature) varies widely but in general range from around 5 to . Units of liters and dm can be used interchangeably. In comparison, petrol fuel has an energy density of 32.4 MJ/l or .\n\nAlthough the specific energy of supercapacitors is insufficient compared with batteries, capacitors have the important advantage of the specific power. Specific power describes the speed at which energy can be delivered to/absorbed from the load. The maximum power is given by the formula:\n\nwith V = voltage applied and R, the internal DC resistance of the capacitor.\n\nSpecific power is measured either gravimetrically in kilowatts per kilogram (kW/kg, specific power) or volumetrically in kilowatts per litre (kW/l, power density).\n\nThe described maximum power P specifies the power of a theoretical rectangular single maximum current peak of a given voltage. In real circuits the current peak is not rectangular and the voltage is smaller, caused by the voltage drop. IEC 62391–2 established a more realistic effective power P for supercapacitors for power applications:\n\nSupercapacitor specific power is typically 10 to 100 times greater than for batteries and can reach values up to 15 kW/kg.\n\nRagone charts relate energy to power and are a valuable tool for characterizing and visualizing energy storage components. With such a diagram, the position of specific power and specific energy of different storage technologies is easily to compare, see diagram.\n\nSince supercapacitors do not rely on chemical changes in the electrodes (except for those with polymer electrodes), lifetimes depend mostly on the rate of evaporation of the liquid electrolyte. This evaporation in general is a function of temperature, of current load, current cycle frequency and voltage. Current load and cycle frequency generate internal heat, so that the evaporation-determining temperature is the sum of ambient and internal heat. This temperature is measurable as core temperature in the center of a capacitor body. The higher the core temperature the faster the evaporation and the shorter the lifetime.\n\nEvaporation generally results in decreasing capacitance and increasing internal resistance. According to IEC/EN 62391-2 capacitance reductions of over 30% or internal resistance exceeding four times its data sheet specifications are considered \"wear-out failures\", implying that the component has reached end-of-life. The capacitors are operable, but with reduced capabilities. Whether the aberration of the parameters have any influence on the proper functionality or not depends on the application of the capacitors.\n\nSuch large changes of electrical parameters specified in IEC/EN 62391-2 are usually unacceptable for high current load applications. Components that support high current loads use much smaller limits, e.g., 20% loss of capacitance or double the internal resistance. The narrower definition is important for such applications, since heat increases linearly with increasing internal resistance and the maximum temperature should not be exceeded. Temperatures higher than specified can destroy the capacitor.\n\nThe real application lifetime of supercapacitors, also called \"service life\", \"life expectancy\" or \"load life\", can reach 10 to 15 years or more at room temperature. Such long periods cannot be tested by manufacturers. Hence, they specify the expected capacitor lifetime at the maximum temperature and voltage conditions. The results are specified in datasheets using the notation \"tested time (hours)/max. temperature (°C)\", such as \"5000 h/65 °C\". With this value and expressions derived from historical data, lifetimes can be estimated for lower temperature conditions.\n\nDatasheet lifetime specification is tested by the manufactures using an accelerated aging test called \"endurance test\" with maximum temperature and voltage over a specified time. For a \"zero defect\" product policy during this test no wear out or total failure may occur.\n\nThe lifetime specification from datasheets can be used to estimate the expected lifetime for a given design. The \"10-degrees-rule\" used for electrolytic capacitors with non-solid electrolyte is used in those estimations and can be used for supercapacitors. This rule employs the Arrhenius equation, a simple formula for the temperature dependence of reaction rates. For every 10 °C reduction in operating temperature, the estimated life doubles.\n\nWith\n\nCalculated with this formula, capacitors specified with 5000 h at 65 °C, have an estimated lifetime of 20,000 h at 45 °C.\n\nLifetimes are also dependent on the operating voltage, because the development of gas in the liquid electrolyte depends on the voltage. The lower the voltage the smaller the gas development and the longer the lifetime. No general formula relates voltage to lifetime. The voltage dependent curves shown from the picture are an empirical result from one manufacturer.\n\nLife expectancy for power applications may be also limited by current load or number of cycles. This limitation has to be specified by the relevant manufacturer and is strongly type dependent.\n\nStoring electrical energy in the double-layer separates the charge carriers within the pores by distances in the range of molecules. Over this short distance irregularities can occur, leading to a small exchange of charge carriers and gradual discharge. This self-discharge is called leakage current. Leakage depends on capacitance, voltage, temperature and the chemical stability of the electrode/electrolyte combination. At room temperature leakage is so low that it is specified as time to self-discharge. Supercapacitor self-discharge time is specified in hours, days or weeks. As an example, a 5.5 V/F Panasonic \"Goldcapacitor\" specifies a voltage drop at 20 °C from 5.5 V to 3 V in 600 hours (25 days or 3.6 weeks) for a double cell capacitor.\n\nIt has been noticed that after the EDLC experiences a charge or discharge, the voltage will drift over time, relaxing toward its previous voltage level. The observed relaxation can occur over several hours and is likely due to long diffusion time constants of the porous electrodes within the EDLC. \nSince the positive and negative electrodes (or simply positrode and negatrode, respectively) of symmetric supercapacitors consist of the same material, theoretically supercapacitors have no true polarity and catastrophic failure does not normally occur. However reverse-charging a supercapacitor lowers its capacity, so it is recommended practice to maintain the polarity resulting from the formation of the electrodes during production. Asymmetric supercapacitors are inherently polar.\n\nPseudocapacitor and hybrid supercapacitors which have electrochemical charge properties may not be operated with reverse polarity, precluding their use in AC operation. However, this limitation does not apply to EDLC supercapacitors\n\nA bar in the insulating sleeve identifies the negative terminal in a polarized component.\n\nIn some literature, the terms \"anode\" and \"cathode\" are used in place of negative electrode and positive electrode. Using anode and cathode to describe the electrodes in supercapacitors (and also rechargeable batteries including lithium ion batteries) can lead to confusion, because the polarity changes depending on whether a component is considered as a generator or as a consumer of current. In electrochemistry, cathode and anode are related to reduction and oxidation reactions, respectively. However, in supercapacitors based on electric double layer capacitance, there is no oxidation and/or reduction reactions on any of the two electrodes. Therefore, the concepts of cathode and anode do not apply.\n\nThe range of electrodes and electrolytes available yields a variety of components suitable for diverse applications. The development of low-ohmic electrolyte systems, in combination with electrodes with high pseudocapacitance, enable many more technical solutions.\n\nThe following table shows differences among capacitors of various manufacturers in capacitance range, cell voltage, internal resistance (ESR, DC or AC value) and volumetric and gravimetric specific energy.\n\nIn the table, ESR refers to the component with the largest capacitance value of the respective manufacturer. Roughly, they divide supercapacitors into two groups. The first group offers greater ESR values of about 20 milliohms and relatively small capacitance of 0.1 to 470 F. These are \"double-layer capacitors\" for memory back-up or similar applications. The second group offers 100 to 10,000 F with a significantly lower ESR value under 1 milliohm. These components are suitable for power applications. A correlation of some supercapacitor series of different manufacturers to the various construction features is provided in Pandolfo and Hollenkamp.\n\nIn commercial double-layer capacitors, or, more specifically, EDLCs in which energy storage is predominantly achieved by double-layer capacitance, energy is stored by forming an electrical double layer of electrolyte ions on the surface of conductive electrodes. Since EDLCs are not limited by the electrochemical charge transfer kinetics of batteries, they can charge and discharge at a much higher rate, with lifetimes of more than 1 million cycles. The EDLC energy density is determined by operating voltage and the specific capacitance (farad/gram or farad/cm) of the electrode/electrolyte system. The specific capacitance is related to the Specific Surface Area (SSA) accessible by the electrolyte, its interfacial double-layer capacitance, and the electrode material density.\n\nCommercial EDLCs are based on two symmetric electrodes impregnated with electrolytes comprising tetraethylammonium tetrafluoroborate salts in organic solvents. Current EDLCs containing organic electrolytes operate at 2.7 V and reach energy densities around 5-8 Wh/kg and 7 to 10 Wh/l. The specific capacitance is related to the specific surface area (SSA) accessible by the electrolyte, its interfacial double-layer capacitance, and the electrode material density. Graphene-based platelets with mesoporous spacer material is a promising structure for increasing the SSA of the electrolyte.\n\nSupercapacitors vary sufficiently that they are rarely interchangeable, especially those with higher specific energy. Applications range from low to high peak currents, requiring standardized test protocols.\n\nTest specifications and parameter requirements are specified in the generic specification\n\nThe standard defines four application classes, according to discharge current levels:\n\nThree further standards describe special applications:\n\nSupercapacitors do not support AC applications.\n\nSupercapacitors have advantages in applications where a large amount of power is needed for a relatively short time, where a very high number of charge/discharge cycles or a longer lifetime is required. Typical applications range from milliamp currents or milliwatts of power for up to a few minutes to several amps current or several hundred kilowatts power for much shorter periods.\n\nThe time t a supercapacitor can deliver a constant current I can be calculated as:\n\nas the capacitor voltage decreases from U down to U.\n\nIf the application needs a constant power P for a certain time t this can be calculated as:\n\nwherein also the capacitor voltage decreases from U down to U.\n\nIn applications with fluctuating loads, such as laptop computers, PDA's, GPS, portable media players, hand-held devices, and photovoltaic systems, supercapacitors can stabilize the power supply.\n\nSupercapacitors deliver power for photographic flashes in digital cameras and for LED life flashlights that can be charged in, e.g., 90 seconds.\n\n, portable speakers powered by supercapacitors were offered to the market.\n\nA cordless electric screwdriver with supercapacitors for energy storage has about half the run time of a comparable battery model, but can be fully charged in 90 seconds. It retains 85% of its charge after three months left idle.\n\nA group of EVs and HEVs during their charging process draw very high current for a short duration of time which creates power pulsation on the grid. Power pulsation not only reduces the efficiency of the grid and cause voltage drop in the common coupling bus, but it can cause considerable frequency fluctuation in the entire system. To overcome this problem, supercapacitors can be implemented as an interface between the charging station and the grid to buffer the grid from the high pulse power drawn from the charging station.\n\nSupercapacitors provide backup or emergency shutdown power to low-power equipment such as RAM, SRAM, micro-controllers and PC Cards. They are the sole power source for low energy applications such as automated meter reading (AMR) equipment or for event notification in industrial electronics.\n\nSupercapacitors buffer power to and from rechargeable batteries, mitigating the effects of short power interruptions and high current peaks. Batteries kick in only during extended interruptions, e.g., if the mains power or a fuel cell fails, which lengthens battery life.\n\nUninterruptible power supplies (UPS), where supercapacitors have replaced much larger banks of electrolytic capacitors. This combination reduces the cost per cycle, saves on replacement and maintenance costs, enables the battery to be downsized and extends battery life. A disadvantage is the need for a special circuit to reconcile the differing behaviors.\n\nSupercapacitors provide backup power for actuators in wind turbine pitch systems, so that blade pitch can be adjusted even if the main supply fails.\n\nSupercapacitors can stabilize voltage for powerlines. Wind and photovoltaic systems exhibit fluctuating supply evoked by gusting or clouds that supercapacitors can buffer within milliseconds. This helps stabilize grid voltage and frequency, balance supply and demand of power and manage real or reactive power.\n\nSupercapacitors are suitable temporary energy storage devices for energy harvesting systems. In energy harvesting systems, the energy is collected from the ambient or renewable sources, e.g. mechanical movement, light or electromagnetic fields, and converted to electrical energy in an energy storage device. For example, it was demonstrated that energy collected from RF (radio frequency) fields (using an RF antenna as an appropriate rectifier circuit) can be stored to a printed supercapacitor. The harvested energy was then used to power an application-specific integrated circuit (ASIC) circuit for over 10 hours.\n\nThe UltraBattery is a hybrid rechargeable lead-acid battery and a supercapacitor invented by Australia's national science organisation CSIRO. Its cell construction contains a standard lead-acid battery positive electrode, standard sulphuric acid electrolyte and a specially prepared negative carbon-based electrode that store electrical energy with double-layer capacitance. The presence of the supercapacitor electrode alters the chemistry of the battery and affords it significant protection from sulfation in high rate partial state if charge use, which is the typical failure mode of valve regulated lead-acid cells used this way. The resulting cell performs with characteristics beyond either a lead-acid cell or a supercapacitor, with charge and discharge rates, cycle life, efficiency and performance all enhanced. UltraBattery has been installed in kW and MW scale applications in Australia, Japan and the U.S.A. in frequency regulation, solar smoothing and shifting, wind smoothing and other applications.\n\nSado City, in Japan's Niigata Prefecture, has street lights that combine a stand-alone power source with solar cells and LEDs. Supercapacitors store the solar energy and supply 2 LED lamps, providing 15 W power consumption overnight. The supercapacitors can last more than 10 years and offer stable performance under various weather conditions, including temperatures from +40 to below -20 °C.\n\nSupercapacitors are used in defibrillators where they can deliver 500 joules to shock the heart back into sinus rhythm.\n\nIn 2005, aerospace systems and controls company Diehl Luftfahrt Elektronik GmbH chose supercapacitors to power emergency actuators for doors and evacuation slides used in airliners, including the Airbus 380.\n\nSupercapacitors' low internal resistance supports applications that require short-term high currents. Among the earliest uses were motor startup (cold engine starts, particularly with diesels) for large engines in tanks and submarines. Supercapacitors buffer the battery, handling short current peaks, reducing cycling and extending battery life.\n\nFurther military applications that require high specific power are phased array radar antennae, laser power supplies, military radio communications, avionics displays and instrumentation, backup power for airbag deployment and GPS-guided missiles and projectiles.\n\nToyota's Yaris Hybrid-R concept car uses a supercapacitor to provide bursts of power. PSA Peugeot Citroën has started using supercapacitors as part of its stop-start fuel-saving system, which permits faster initial acceleration. Mazda's i-ELOOP system stores energy in a supercapacitor during deceleration and uses it to power on-board electrical systems while the engine is stopped by the stop-start system.\n\nMaxwell Technologies, an American supercapacitor-maker, claimed that more than 20,000 hybrid buses use the devices to increase acceleration, particularly in China. Guangzhou, In 2014 China began using trams powered with supercapacitors that are recharged in 30 seconds by a device positioned between the rails, storing power to run the tram for up to 4 km — more than enough to reach the next stop, where the cycle can be repeated.\n\nA primary challenge of all transport is reducing energy consumption and reducing emissions. Recovery of braking energy (recuperation or regeneration) helps with both. This requires components that can quickly store and release energy over long times with a high cycle rate. Supercapacitors fulfill these requirements and are therefore used in a lot of applications in all kinds of transportation.\n\nSupercapacitors can be used to supplement batteries in starter systems in diesel railroad locomotives with diesel-electric transmission. The capacitors capture the braking energy of a full stop and deliver the peak current for starting the diesel engine and acceleration of the train and ensures the stabilization of catenary voltage. Depending on the driving mode up to 30% energy saving is possible by recovery of braking energy. Low maintenance and environmentally friendly materials encouraged the choice of supercapacitors.\n\nMobile hybrid diesel-electric rubber tyred gantry cranes move and stack containers within a terminal. Lifting the boxes requires large amounts of energy. Some of the energy could be recaptured while lowering the load resulting in improved efficiency.\n\nA triple hybrid forklift truck uses fuel cells and batteries as primary energy storage and supercapacitors to buffer power peaks by storing braking energy. They provide the fork lift with peak power over 30 kW. The triple-hybrid system offers over 50% energy savings compared with diesel or fuel-cell systems.\n\nSupercapacitor-powered terminal tractors transport containers to warehouses. They provide an economical, quiet and pollution-free alternative to diesel terminal tractors.\n\nSupercapacitors make it possible not only to reduce energy but to replace overhead lines in historical city areas, so preserving the city's architectural heritage. This approach may allow many new LRV city lines to replace overhead wires that are too expensive to fully route.\n\nIn 2003 Mannheim adopted a prototype light-rail vehicle (LRV) using the MITRAC Energy Saver system from Bombardier Transportation to store mechanical braking energy with a roof-mounted supercapacitor unit. It contains several units each made of 192 capacitors with 2700 F /2.7 V interconnected in three parallel lines. This circuit results in a 518 V system with an energy content of 1.5 kWh. For acceleration when starting this \"on-board-system\" can provide the LRV with 600 kW and can drive the vehicle up to 1 km without catenary supply integrating the LRV into the urban environment by driving without catenary lines. Compared to conventional LRVs or Metro vehicles that return energy into the grid, onboard energy storage saves up to 30% and reduces peak grid demand by up to 50%.\n\nIn 2009 supercapacitors enabled LRV's to operate in the historical city area of Heidelberg without catenary overhead wires preserving the city's architectural heritage. The SC equipment cost an additional €270,000 per vehicle, which was expected to be recovered over the first 15 years of operation. The supercapacitors are charged at stop-over stations when the vehicle is at a scheduled stop. This approach may allow many LRV city lines to serve catenary overhead wires that are too expensive to fully route installation. In April 2011 German regional transport operator Rhein-Neckar, responsible for Heidelberg, ordered a further 11 units.\n\nIn 2009, Alstom and RATP equipped a Citadis tram with an experimental energy recovery system called \"STEEM\". The system is fitted with 48 roof-mounted supercapacitors to store braking energy provides tramways with a high level of energy autonomy by enabling them to run without catenary power on parts of its route, recharging while traveling on powered stop-over stations. During the tests, which took place between the Porte d’Italie and Porte de Choisy stops on line T3 of the tramway network in Paris, the tramset used an average of approximately 16% less energy.\n\nIn 2012 tram operator Geneva Public Transport began tests of an LRV equipped with a prototype roof-mounted supercapacitor unit to recover braking energy.\n\nSiemens is delivering supercapacitor-enhanced light-rail transport systems that include mobile storage.\n\nHong Kong's South Island metro line is to be equipped with two 2 MW energy storage units that are expected to reduce energy consumption by 10%.\n\nIn August 2012 the CSR Zhuzhou Electric Locomotive corporation of China presented a prototype two-car light metro train equipped with a roof-mounted supercapacitor unit. The train can travel up 2 km without wires, recharging in 30 seconds at stations via a ground mounted pickup. The supplier claimed the trains could be used in 100 small and medium-sized Chinese cities. Seven trams (street cars) powered by supercapacitors were scheduled to go into operation in 2014 in Guangzhou, China. The supercapacitors are recharged in 30 seconds by a device positioned between the rails. That powers the tram for up to .\nAs of 2017, Zhuzhou's supercapacitor vehicles are also used on the new Nanjing streetcar system, and are undergoing trials in Wuhan.\n\nIn 2012, in Lyon (France), the (Lyon public transportation administration) started experiments of a \"way side regeneration\" system built by Adetel Group which has developed its own energy saver named ″NeoGreen″ for LRV, LRT and metros.\n\nIn 2015, Alstom announced SRS, an energy storage system that charges supercapacitors on board a tram by means of ground-level conductor rails located at tram stops. This allows trams to operate without overhead lines for short distances. The system has been touted as an alternative to the company's ground-level power supply (APS) system, or can be used in conjunction with it, as in the case of the VLT network in Rio de Janeiro, Brazil, which opened in 2016.\n\nThe first hybrid bus with supercapacitors in Europe came in 2001 in Nuremberg, Germany. It was MAN's so-called \"Ultracapbus\", and was tested in real operation in 2001/2002. The test vehicle was equipped with a diesel-electric drive in combination with supercapacitors. The system was supplied with 8 Ultracap modules of 80 V, each containing 36 components. The system worked with 640 V and could be charged/discharged at 400 A. Its energy content was 0.4 kWh with a weight of 400 kg.\n\nThe supercapacitors recaptured braking energy and delivered starting energy. Fuel consumption was reduced by 10 to 15% compared to conventional diesel vehicles. Other advantages included reduction of emissions, quiet and emissions-free engine starts, lower vibration and reduced maintenance costs.\n\nIn early 2005 Shanghai tested a new form of electric bus called capabus that runs without powerlines (catenary free operation) using large onboard supercapacitors that partially recharge whenever the bus is at a stop (under so-called electric umbrellas), and fully charge in the terminus. In 2006, two commercial bus routes began to use the capabuses; one of them is route 11 in Shanghai. It was estimated that the supercapacitor bus was cheaper than a lithium-ion battery bus, and one of its buses had one-tenth the energy cost of a diesel bus with lifetime fuel savings of $200,000.\n\nA hybrid electric bus called tribrid was unveiled in 2008 by the University of Glamorgan, Wales, for use as student transport. It is powered by hydrogen fuel or solar cells, batteries and ultracapacitors.\n\nThe FIA, a governing body for motor racing events, proposed in the \"Power-Train Regulation Framework for Formula 1\" version 1.3 of 23 May 2007 that a new set of power train regulations be issued that includes a hybrid drive of up to 200 kW input and output power using \"superbatteries\" made with batteries and supercapacitors connected in parallel (KERS). About 20% tank-to-wheel efficiency could be reached using the KERS system.\n\nThe Toyota TS030 Hybrid LMP1 car, a racing car developed under Le Mans Prototype rules, uses a hybrid drivetrain with supercapacitors. In the 2012 24 Hours of Le Mans race a TS030 qualified with a fastest lap only 1.055 seconds slower (3:24.842 versus 3:23.787) than the fastest car, an Audi R18 e-tron quattro with flywheel energy storage. The supercapacitor and flywheel components, whose rapid charge-discharge capabilities help in both braking and acceleration, made the Audi and Toyota hybrids the fastest cars in the race. In the 2012 Le Mans race the two competing TS030s, one of which was in the lead for part of the race, both retired for reasons unrelated to the supercapacitors. The TS030 won three of the 8 races in the 2012 FIA World Endurance Championship season. In 2014 the Toyota TS040 Hybrid used a supercapacitor to add 480 horsepower from two electric motors.\n\nSupercapacitor/battery combinations in electric vehicles (EV) and hybrid electric vehicles (HEV) are well investigated. A 20 to 60% fuel reduction has been claimed by recovering brake energy in EVs or HEVs. The ability of supercapacitors to charge much faster than batteries, their stable electrical properties, broader temperature range and longer lifetime are suitable, but weight, volume and especially cost mitigate those advantages.\n\nSupercapacitors lower specific energy makes them unsuitable for use as a stand-alone energy source for long distance driving. The fuel economy improvement between a capacitor and a battery solution is about 20% and is available only for shorter trips. For long distance driving the advantage decreases to 6%. Vehicles combining capacitors and batteries run only in experimental vehicles.\n\nRussian Yo-cars Ё-mobile series was a concept and crossover hybrid vehicle working with a gasoline driven rotary vane type and an electric generator for driving the traction motors. A supercapacitor with relatively low capacitance recovers brake energy to power the electric motor when accelerating from a stop.\n\nToyota's Yaris Hybrid-R concept car uses a supercapacitor to provide quick bursts of power.\n\nPSA Peugeot Citroën fit supercapacitors to some of its cars as part of its stop-start fuel-saving system, as this permits faster start-ups when the traffic lights turn green.\n\nIn Zell am See, Austria, an aerial lift connects the city with Schmittenhöhe mountain. The gondolas sometimes run 24 hours per day, using electricity for lights, door opening and communication. The only available time for recharging batteries at the stations is during the brief intervals of guest loading and unloading, which is too short to recharge batteries. Supercapacitors offer a fast charge, higher number of cycles and longer life time than batteries.\n\nEmirates Air Line (cable car), also known as the Thames cable car, is a 1-kilometre (0.62 mi) gondola line that crosses the Thames from the Greenwich Peninsula to the Royal Docks. The cabins are equipped with a modern infotainment system, which is powered by supercapacitors.\n\n commercially available lithium-ion supercapacitors offered the highest gravimetric specific energy to date, reaching 15 Wh/kg (). Research focuses on improving specific energy, reducing internal resistance, expanding temperature range, increasing lifetimes and reducing costs.\nProjects include tailored-pore-size electrodes, pseudocapacitive coating or doping materials and improved electrolytes.\n\n worldwide sales of supercapacitors is about US$400 million.\n\nThe market for batteries (estimated by Frost & Sullivan) grew from US$47.5 billion, (76.4% or US$36.3 billion of which was rechargeable batteries) to US$95 billion. The market for supercapacitors is still a small niche market that is not keeping pace with its larger rival.\n\nIn 2016, IDTechEx forecast sales to grow from $240 million to $2 billion by 2026, an annual increase of about 24%.\n\nSupercapacitor costs in 2006 were US$0.01 per farad or US$2.85 per kilojoule, moving in 2008 below US$0.01 per farad, and were expected to drop further in the medium term.\n\nExceptional for electronic components like capacitors are the manifold different trade or series names used for supercapacitors like: \"APowerCap, BestCap, BoostCap, CAP-XX, C-SECH, DLCAP, EneCapTen, EVerCAP, DynaCap, Faradcap, GreenCap, Goldcap, HY-CAP, Kapton capacitor, Super capacitor, SuperCap, PAS Capacitor, PowerStor, PseudoCap, Ultracapacitor\" making it difficult for users to classify these capacitors. (Compare with #Comparison of technical parameters)\n\n\n"}
{"id": "2007274", "url": "https://en.wikipedia.org/wiki?curid=2007274", "title": "Universe of Energy", "text": "Universe of Energy\n\nThe Universe of Energy was a pavilion located in the eastern half of Future World at Epcot. The pavilion contained one attraction, Ellen's Energy Adventure, starring Ellen DeGeneres and Bill Nye, which was the second version of the show since the pavilion's 1982 opening. The attraction featured a combination of four separate large-format film presentations and a slow-moving dark ride through audio-animatronic filled sets. \n\nThe Universe of Energy pavilion was previously sponsored by ExxonMobil (formerly Exxon) from opening day on October 1, 1982, through 2004. At D23 Expo 2017, it was announced that the Universe of Energy pavilion would close to be replaced with a \"Guardians of the Galaxy\"-themed roller coaster. The attraction closed permanently on August 13, 2017.\n\nThe original \"Universe of Energy\" pavilion itself was an innovation in energy technology, as the entire roof was covered in 80,000 photovoltaic solar cells that partially powered the ride vehicles. Visitors were transported through the pavilion in large battery-powered \"traveling theatre cars\" that followed guide-wires embedded in the floor as opposed to riding along conventional ride tracks. The original attraction featured numerous films that presented information on the subject of energy in a serious fashion as well as a ride through a primeval diorama complete with audio-animatronic dinosaurs.\n\nThe original pre-show featured an eight-minute live-action film presentation about the various forms of energy found in nature and traced the history of how mankind harnessed these different forms of energy for his use. This unique film presentation was known as the \"Kinetic Mosaic\" and was invented by Czech film director Emil Radok. The mosaic screen consisted of 100 rotating prism-shaped flip screens (reminiscent of those on the classic game show \"Concentration\"), arranged in a twenty five wide by four high array. These flip screens rotated under computer control and were synchronized to the motion picture that was projected onto their surface via five synchronized motion picture projectors. Each flip screen contained three sides, with white projection surfaces on two sides and a matte black surface on the third. The combination of the film and the screens' rotation created undulating, sometimes three-dimensional-appearing images. During the conclusion of the pre-show, the song \"Energy (You Make The World Go ‘Round)\" was played.\n\nUpon entering the theatre, guests were seated in one of six sections. The seating area rotated 180 degrees to face three large movie screens for the first film: a four-minute hand-animated film that depicted the beginnings of life on earth and the formation of fossil fuels.\n\nAt the conclusion of the film, the seating area rotated 90 degrees to face a curtain, which then raised to reveal a primeval diorama. The entire seating area moved into the diorama where it then broke apart into six multi-passenger vehicles that took guests on a seven-minute journey through the diorama, which was populated by numerous animatronic dinosaurs including an \"Edaphosaurus\" and two \"Arthropleura\" fighting and a family of \"Brontosaurus\" in a swamp (complete with realistic \"swampy\" smell), a \"Stegosaurus\" fighting an \"Allosaurus\" on an overhead cliff, several \"Trachodon\" bathing beneath a waterfall, a number of \"Ornithomimus\" watching helplessly as one of their own sank into a boiling tar pit, an \"Elasmosaurus\" that lashed out of a tidal pool at guests, and numerous \"Pteranodon\" that were perched around an erupting volcano (complete with flowing lava and realistic \"volcano\" smell).\n\nLeaving the diorama, the vehicles entered the \"EPCOT Energy Information Center\" where they reassembled back into their original theatre seating formation. Here, guests viewed a twelve-minute live-action film on three giant wrap around screens that took them on an in-depth look at various current and future energy resources around the world.\n\nAt the conclusion of the film, the screens raised and the entire seating area traveled beneath them into Theatre I and rotated back into its starting position facing the audience towards a large cylindrical-shaped screen. There, guests viewed a two-minute computer-animated film that was reflected off of mirrored walls within the theatre. The film depicted an ever-evolving landscape of colorful, laser-like imagery of the various ways mankind has benefited from harnessing energy for his use and was accompanied by an upbeat song entitled \"Universe of Energy\".\n\nThe Summer of 1996 saw many changes come to Future World East. World of Motion closed in January of that year to make way for Test Track, and Horizons was not operating consistently due to alleged structural issues with the pavilion. The updated films for the attraction were behind schedule and with that side of Future World set to only have Wonders of Life open for the summer peak season, the decision was made to reopen Universe of Energy after renovations had been made to the pavilion in preparation for the new show.\n\nThis version of the ride featured all of the original films from the 1982 version, but lacked some of the effects. Most notably, the Kinetic Mosaic screen from the original pre-show had been removed resulting in the film being projected onto static screens which resulted in the shape-shifting effect of the film being lost. Also removed were the maps and television monitors on the wall in the \"EPCOT Energy Information Center\" in Theatre II, having already been replaced by the KNRG radio tower backdrop for the new show. For this scene, a new narration played that covered much of the same information as the original narration minus any mention of the maps and monitors. In Theatre I, the mirrors on the walls had already been removed by this point, resulting in a much less dramatic version of the finale film.\n\nDuring this period, some elements for the new show had already been installed in preparation for the new show and had to be hidden. This included the Audio-Animatronic figure of Ellen DeGeneres in the diorama. To solve this, temporary rockwork was placed in front of the figure hiding it from view. However, the Elasmosaurus figure had already been reprogrammed for the new show, leading to the awkward result of having it lunge at rocks instead of the ride vehicles as it had originally done.\n\nThis version of the show only ran during the summer of 1996. The pavilion was closed again soon after peak season to allow for the installation of the new films for the new version of the attraction.\n\nEllen's Energy Adventure starred Ellen DeGeneres, Bill Nye \"The Science Guy\", Jamie Lee Curtis, Alex Trebek, and Johnny Gilbert. It took a light-hearted look at various energy resources, how energy was produced, the history of energy production, and the search for new energy resources. In particular it focused on the origins of fossil fuels such as petroleum, coal and natural gas. It also mentioned renewable sources such as solar and hydroelectric power. In 2011, \"Ellen's Energy Adventure\" surpassed the original \"Universe of Energy\" show as the longest running version of the attraction.\n\nThe second version of the attraction used the same traveling theater system that was used by the original \"Universe of Energy\" show. The primeval diorama used essentially the same sets and Audio-Animatronic dinosaurs as the original show, although during the renovation to the \"Ellen's Energy Adventure\" show, all of the dinosaurs were repainted in much brighter colors, and several audio-animatronic figures were added along with an upbeat musical score to help tie it into the new film footage.\n\nGuests viewed an eight-minute film in which Ellen DeGeneres falls asleep and dreams that she is in an energy-themed version of \"Jeopardy!\", playing against an old college rival Judy Peterson (Curtis) and Albert Einstein (Benny Wasserman). Not knowing anything about energy, Ellen fell way behind Judy. At the end of the first round, Bill Nye the Science Guy stepped in and offered to help teach Ellen about energy during the commercial break.\n\nUpon entering the theatre, guests were seated in one of six sections. The seating area rotated 180 degrees to face three large movie screens for the first film: a four-minute computer animated film in which Bill took Ellen back billions of years in time to witness the Big Bang and the formation of the earth. Finally, they ended up in a prehistoric jungle where he briefly explained how fossil fuels were formed.\n\nAt the conclusion of the film, the seating area rotated 90 degrees to face a curtain, which then rose to reveal a primeval diorama. The entire seating area moved into the diorama where it then broke apart into six multi-passenger vehicles that took guests on a seven-minute journey through the diorama, which was populated by numerous audio-animatronic dinosaurs including an \"Edaphosaurus\", two \"Arthropleura\" fighting and a family of \"Brontosaurus\" in a swamp (one of whom sneezed water onto guests), a \"Stegosaurus\" fighting an \"Allosaurus\" on an overhead cliff, several \"Trachodon\" bathing beneath a waterfall, a number of \"Ornithomimus\" drinking from a pond (one of whom spit water at guests), an Audio-Animatronic Ellen standing near a tidal pool fighting off an \"Elasmosaurus\" with a tree branch, and numerous \"Pteranodon\" perched around an erupting volcano. After November 2014, the Audio-Animatronic figure of Ellen fighting off the \"Elasmosaurus\" with a tree branch stopped working and was removed. It was replaced with a group of smaller \"Pteranodons\".\n\nLeaving the diorama, the vehicles entered a second theatre where they reassembled back into their original theatre seating formation. After listening to a brief prehistoric broadcast from KNRG News Radio (which featured the voices of Willard Scott and Chris Berman), guests viewed a twelve-minute live-action film on three giant wrap around screens in which Bill Nye took Ellen on an in-depth look at various current and future energy resources across the United States. Actor Michael Richards made a brief cameo as a caveman near the beginning of the film.\n\nAt the conclusion of the film, the screens rose and the entire seating area traveled beneath them back into Theatre I and rotated back into its original starting position facing the audience towards a large television-shaped screen. The final two minute film presented the end of Ellen's dream in which she used her new knowledge about energy in Round 2 to unseat Judy as \"Jeopardy!\" champion.\n\n\n\n"}
{"id": "53089755", "url": "https://en.wikipedia.org/wiki?curid=53089755", "title": "Xi Lupi", "text": "Xi Lupi\n\nThe Bayer designation ξ Lupi (Xi Lupi) is shared by two star systems in the constellation Lupus:\n\nListed in the Washington Double Star Catalog as WDS J15569-3358, this double star most likely forms a wide binary star system. As of epoch 2004.38, the pair had an angular separation of along a position angle of . The difference in visual magnitude between the two stars is .\n"}
