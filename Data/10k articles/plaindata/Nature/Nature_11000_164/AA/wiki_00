{"id": "42070410", "url": "https://en.wikipedia.org/wiki?curid=42070410", "title": "2013 in fish paleontology", "text": "2013 in fish paleontology\n\nThis list of fossil fishes described in 2013 is a list of new taxa of placoderms, fossil cartilaginous fishes and bony fishess of every kind that have been described during the year 2013. The list only includes taxa at the level of genus or species.\n\n\n"}
{"id": "13411406", "url": "https://en.wikipedia.org/wiki?curid=13411406", "title": "Aleutian Low", "text": "Aleutian Low\n\nThe Aleutian Low is a semi-permanent low-pressure system located near the Aleutian Islands in the Bering Sea during the Northern Hemisphere winter. It is a climatic feature centered near the Aleutian Islands measured based on mean sea-level pressure. It is one of the largest atmospheric circulation patterns in Northern Hemisphere and represents one of the \"main centers of action in atmospheric circulation.\"\nThe Aleutian Low is characterized by heavily influencing the path and strength of cyclones. Extratropical cyclones which form in the sub-polar latitudes in the North Pacific typically slow down and reach maximum intensity in the area of the Aleutian Low. Tropical cyclones that form in the tropical and equatorial regions of the Pacific can veer northward and get caught in the Aleutian Low. This is usually seen in the later summer seasons. Both the November 2011 Bering Sea cyclone and the November 2014 Bering Sea cyclone were post-tropical cyclones that had dissipated and restrengthened when the systems entered the Aleutian Low region. The storms are remembered and marked as two of the strongest storms to impact the Bering Sea and Aleutian Islands with pressure dropping below 950 mb in each system. The magnitude of the low pressure creates an extreme atmospheric disturbance, which can cause other significant shifts in weather. Following the November 2014 Bering Sea cyclone, a huge cold wave, November 2014 North American cold wave, hit the US bringing record breaking low temperatures to many states.\n\nThe low serves as an atmospheric driver for low-pressure systems, post-tropical cyclones and their remnants and can generate strong storms that impact Alaska and Canada. Intensity of the low is strongest in the winter and almost completely dissipates in the summer. The circulation pattern is measured based on averages of synoptic features help mark the locations of cyclones and their paths over a given time period. However, there is significant variability in these measurements. The circulation pattern shifts during the Northern Hemisphere summer when the North Pacific High takes over and breaks apart the Aleutian Low. This high-pressure circulation pattern strongly influences tropical cyclone paths. The presence of the Eurasian and North American continents prevent a continuous belt of low pressure from developing in the Northern Hemisphere sub-polar latitudes, which would mirror the circumpolar belt of low pressure and frequent storms in the Southern Ocean. However, the presence of the continents disrupts this motion, and the subpolar belt of low pressure is well developed only in the North Pacific (the Aleutian Low) and the North Atlantic (the Icelandic Low, which is located between Greenland and Iceland).\n\n"}
{"id": "2830096", "url": "https://en.wikipedia.org/wiki?curid=2830096", "title": "Andean-Saharan glaciation", "text": "Andean-Saharan glaciation\n\nThe Andean-Saharan glaciation occurred during the Paleozoic from 450 Ma to 420 Ma, during the late Ordovician and the Silurian period. For the Ordovician/Saharan part, see the more extensive article on the Late Ordovician glaciation.\n\nAccording to Eyles and Young, \"A major glacial episode at c. 440 Ma, is recorded in Late Ordovician strata (predominantly Ashgillian) in West Africa (Tamadjert Formation of the Sahara), in Morocco (Tindouf Basin) and in west-central Saudi Arabia, all areas at polar latitudes at the time. From the Late Ordovician to the Early Silurian the centre of glaciation moved from northern Africa to southwestern South America.\"\n\nDuring this period glaciation is known from Arabia, Sahara, West Africa, the south Amazon, and the Andes. The center of glaciation migrated from Sahara in the Ordovician (450–440 Ma) to South America in the Silurian (440–420 Ma). The maximum extent of glaciation developed in Africa and eastern Brazil.\n\nA minor ice age, the Andean-Saharan was preceded by the Cryogenian ice ages (720–630 Ma, the Sturtian and Marinoan glaciations), often referred to as Snowball Earth, and followed by the Karoo Ice Age (350–260 Ma).\n\n"}
{"id": "347245", "url": "https://en.wikipedia.org/wiki?curid=347245", "title": "Antediluvian", "text": "Antediluvian\n\nThe Antediluvian (alternatively Pre-Diluvian or Pre-Flood period) is the time period referred to in the Bible between the fall of humans and the Genesis flood narrative in the biblical cosmology. The narrative takes up chapters 1–6 (excluding the flood narrative) of the Book of Genesis. The term found its way into early geology and science until the late Victorian era. Colloquially, the term is used to refer to any ancient and murky period.\n\nThe Sumerian flood myth is the direct mythological antecessor to the biblical flood myth as well as other Near Eastern flood stories, and reflects a similar religious and cultural relevance to their religion. Much as Jews and Christians, ancient Sumerians divided the world between pre-flood and post-flood eras, the former being a time where the gods walked the earth and humans were immortal. After the flood, humans ceased to be immortal and the gods distanced themselves.\n\nIn the Christian Bible and Hebrew Torah, the Antediluvian period begins with the Fall of the first Man and woman, according to Genesis and ends with the destruction of all life on the earth except those saved with Noah in the ark. (Noah and his wife, his three Sons and their wives) According to Bishop Ussher's 17th-century chronology, the Antediluvian period lasted for 1656 years, from Creation (some say the fall of man) at 4004 BC to the Flood at 2348 BC. \nThe elements of the narrative include some of the best-known stories in the Bible — the creation, Adam and Eve, and Cain and Abel, followed by the genealogies tracing the descendants of Cain and Seth, the third mentioned son of Adam and Eve. (These genealogies provide the framework for the biblical chronology, in the form \"A lived X years and begat B\").\n\nThe Bible speaks of this era as being a time of great wickedness. There were Gibborim (giants) in the earth in those days as well as Nephilim; some translations identify the two as one and the same. The Gibborim were unusually powerful; Genesis calls them \"mighty men which were of old, men of renown\". The antediluvian period ended when God sent the Flood to wipe out all life except Noah, his family, and the animals they took with them. Nevertheless, the Nephilim (literally meaning 'fallen ones', from the Hebrew root n-f-l 'to fall') reappear much later in the biblical narrative, in Numbers (where the spies sent forth by Moses report that there were Nephilim or \"giants\" in the promised land).\n\nEarly scientific attempts at reconstructing the history of the Earth were founded on the biblical narrative and thus used the term \"Antediluvian\" to refer to a period understood to be essentially similar to the biblical one. Early scientific interpretation of the biblical narrative divided the Antediluvian into sub-periods based on the :\n\n\nPrior to the 19th century, rock was classified into three main types: primary or primitive (igneous and metamorphic rock), secondary (sedimentary rock) and tertiary (sediments). The primary rocks (like granite and gneiss) are void of fossils and were thought to be associated with the very creation of the world in the primary Pre-Adamitic period. The secondary rocks, often containing copious fossils, though human remains had not been found, were thought to have been laid down in the secondary Pre-Adamitic period. The Tertiary rocks (sediments) were thought to have been put down after Creation and possibly in connection to a flood event, and were thus associated with the Adamitic period. The Post-Flood period was termed the Quaternary, a name still in use in geology.\n\nAs mapping of the geological strata progressed in the early decades of the 19th century, the estimated lengths of the various sub-periods were greatly increased. The fossil rich Secondary Pre-Adamitic period was divided up into the Coal period, the Lias and the Chalk period, later expanded into the now-familiar geologic time scale of the Phanerozoic. The term \"Antediluvian\" was used in natural science well into the 19th century and lingered in popular imagination despite increasingly detailed stratigraphy mapping the Earth's past, and was often used for the Pleistocene period, where humans existed alongside now extinct megafauna.\n\nWriters such as William Whiston (\"A New Theory of the Earth\" 1696) and Henry Morris (\"\" 1961) who launched the modern Creationist movement described the Antediluvian period as follows:\n\n\nHowever, there has since been debate among Creationists over the authenticity of arguments such as the one that 'there was no rain before the Flood' and previous ideas about what the Antediluvian world was like are constantly changing.\n\nDuring the late 18th and early 19th centuries, the understanding of the nature of early Earth went through a transformation from a biblical or deist interpretation to a naturalistic one. Even back in the early 18th century, Plutonists had argued for an ancient Earth, but the full impact of the depth of time involved in the Pre-Adamitic period was not commonly accepted until uniformitarianism as presented in Charles Lyell's \"Principles of Geology\" of 1830. While vast aeons of time were involved, the narrative of the pre-Adamitic world was still influenced by the biblical story of creation in this transition. A striking example is a description from \"Memoires of Ichtyosauri and Plesiosauri\", 1839:\n\nA modern naturalistic view of the ancient world, along with the abandonment of the term \"Antediluvian\", came about with the works of Darwin and Agassiz in the 1860s.\n\nFrom antiquity, fossils of large animals were often quoted as having lived together with the giants from the Book of Genesis: e.g. the Tannin or \"great sea monsters\" of Gen. 1:21. They are often described in later books of the Bible, especially by God himself in the Book of Job: e.g. Re'em in verse 39:9, Behemoth in chapter 40 and Leviathan in chapter 41. With the advent of geological mapping in the early 19th century, it became increasingly obvious that much of the fossils associated with the \"secondary\" (sedimentary) rock, notably large animals like Ichthyosaurs, Mosasaurs, Pliosaurs and the various giant mammals found when excavating the Catacombs of Paris, were neither those of giant humans nor of any extant animals. The geologists of the day increasingly came to use the term \"Antediluvian\" only for the younger strata containing fossils of animals resembling those alive today.\n\n\n"}
{"id": "23983963", "url": "https://en.wikipedia.org/wiki?curid=23983963", "title": "Battery balancing", "text": "Battery balancing\n\nBattery balancing and battery redistribution refer to techniques that maximize the capacity of a battery pack with multiple cells (usually in series) to make all of the capacity available for use and increase each cell's longevity. A battery balancer or battery regulator is a device in a battery pack that performs battery balancing. Balancers are often found in lithium-ion battery packs for cell phones and laptop computers. They can also be found in battery electric vehicle battery packs.\n\nTypically, the individual cells in a battery have somewhat different capacities and may be at different levels of state of charge (SOC). This is due to manufacturing variances, assembly variances (e.g., cells from one production run mixed with others), different histories experienced amongst the cells in a battery pack (e.g., charging/discharging, heat exposures, etc.) and must be accounted for to maximize life and service of the particular battery pack in use. Each battery pack will be, for these reasons, somewhat different and so every balancing circuit must be able to accommodate those differences. Without effective and appropriate balancing, discharging during use must stop when the cell with the lowest capacity is empty (even though other cells aren't); this limits the energy that can be taken from and returned to the battery.\n\nWithout balancing, the smallest capacity cell is a problem, and potentially a serious one. It can be easily overcharged or over-discharged whilst cells with higher capacities are only partially charged. The balance circuit should arrange for higher capacity cells to fully charge/discharge, while smaller capacity cells are charged/discharged suitably—which will necessarily be rather different. In a properly balanced battery pack, the cell with the largest capacity will be filled without overcharging any other (i.e., weaker, smaller) cell, and it can be discharged in use without over-discharging any other cell. Battery balancing is done by transferring energy from or to individual cells, until the SOC of the cell with the lowest capacity is equal to the battery's SOC.\n\nBattery redistribution is sometimes distinguished from battery balancing by saying the latter stops at matching the cell's state of charge (SOC) only at one point (usually 100% SOC), so that the battery's capacity is only limited by the capacity of its weakest cell.\n\nA full battery management system (BMS) might include active balancing as well as temperature monitoring, charging, and other features to maximize the life of a battery pack.\n\nLithium ion rechargeable battery cells are rather more sensitive to overcharging, overheating, improper charge levels during storage, and other forms of mistreatment, than most commonly used battery chemistries. The reason is that the various lithium battery chemistries are susceptible to chemical damage (e.g., cathode fouling, molecular breakdown, etc.) by only very slight overvoltages (i.e., millivolts) during charging, or more charging current than the internal chemistry can tolerate at this point in its charge/discharge cycle, and so on. Heat accelerates these unwanted, but so far inescapable, chemical reactions and overheating during charging amplifies those effects. Because lithium chemistries often permit flexible membrane structures, lithium cells can be deployed in flexible though sealed bags, which permits higher packing densities within a battery pack. Some of the breakdown products (usually of electrolyte chemicals or additives) outgas when mistreated; such cells will become 'puffy' and are very much on the way to failure. In sealed lithium ion cylinders, the same outgassing has caused rather large pressures (800+ psi has been reported); such cells can explode if not provided with a relief failure mechanism. Compounding the danger is that many lithium cell chemistries include hydrocarbon chemicals (the exact nature of which is typically proprietary) which are flammable. Not only is explosion a possibility with mistreated lithium cells, but even a non-explosive leak can cause a fire.\n\nMost battery chemistries have less dramatic, and less dangerous, failure modes. The chemicals in most batteries are often toxic to some degree, but are rarely explosive or flammable; many are corrosive, which accounts for advice to avoid leaving batteries inside equipment for long periods as the batteries may leak and damage the equipment. Lead acid batteries are an exception, for charging them generates hydrogen gas, which can explode if exposed to an ignition source (e.g., a lit cigarette or some such) and such an explosion will spray sulfuric acid in all directions. Since this is corrosive and potentially blinding, this is a particular danger.\nBalancing can be active or passive. The term \"battery regulator\" typically refers only to devices that perform passive balancing.\n\nIn passive balancing, energy is drawn from the most charged cell and dissipated as heat, usually through resistors.\n\nIn active balancing, energy is drawn from the most charged cell and transferred to the least charged cells, usually through DC-DC converters.\n\nBattery balancing can be performed by DC-DC converters, in one of 3 topologies:\n\nTypically, the power handled by each DC-DC converter is a few orders of magnitude lower than the power handled by the battery pack as a whole.\n\n\n"}
{"id": "10064152", "url": "https://en.wikipedia.org/wiki?curid=10064152", "title": "Continental rise", "text": "Continental rise\n\nThe continental rise is an underwater feature found between the continental slope and the abyssal plain. This feature can be found all around the world, and it represents the final stage in the boundary between continents and the deepest part of the ocean. The environment in the continental rise is quite unique, and many oceanographers study it extensively in the hopes of learning more about the ocean and geologic history.\n\nAt the bottom of the continental slope, one will find the continental rise, an underwater hill composed of tons of accumulated sediments. The general slope of the continental rise is between 0.5 degrees and 1.0 degrees. Deposition of sediments at the mouth of submarine canyons may form enormous fan-shaped accumulations called submarine fans. Submarine fans form part of the continental rise. Beyond the continental rise stretches the abyssal plain, an extremely deep and flat area of the sea floor. The abyssal plain hosts many unique life forms which are uniquely adapted to survival in its cold, high pressure, and dark conditions. The flatness of the abyssal plain is interrupted by massive underwater mountain chains near the tectonic boundaries of the Earth's plates. The sediments are mostly sand and pieces of coral or rock.\n"}
{"id": "12086634", "url": "https://en.wikipedia.org/wiki?curid=12086634", "title": "County flowers of Norway", "text": "County flowers of Norway\n\nThis is a list of country flowers of Norway.\n\n"}
{"id": "1684288", "url": "https://en.wikipedia.org/wiki?curid=1684288", "title": "Dinosaur Provincial Park", "text": "Dinosaur Provincial Park\n\nDinosaur Provincial Park is a UNESCO World Heritage Site located about two-and-a-half hours drive east of Calgary, Alberta, Canada; or , about a half-hour drive northeast of Brooks.\n\nThe park is situated in the valley of the Red Deer River, which is noted for its striking badland topography. The park is well known for being one of the richest dinosaur fossil locales in the world. Fifty-eight dinosaur species have been discovered at the park and more than 500 specimens have been removed and exhibited in museums around the globe. The renowned fossil assemblage of nearly 500 species of life, from microscopic fern spores to large carnivorous dinosaurs, justified its becoming a World Heritage Site in 1979.\n\nThe Dinosaur Provincial Park Visitor Centre features exhibits about dinosaurs, fossils, and the geology and natural history of the park. There is a video theater, fossil prep lab area, and a gift shop. Public programs are offered in the summer.\n\nJohn Ware's Cabin is a restored early 20th century cabin that was used by John Ware, an African-American cowboy and important figure in Alberta's ranching history. The cabin is located near the visitor center and is open on select days in the summer.\n\nEstablished on June 27, 1955 as part of Alberta's 50th Jubilee Year with the goal of protecting the fossil beds, the first warden was Roy Fowler (1902-1975), a farmer and amateur fossil hunter.\n\nThe park was established as a UNESCO World Heritage Site on October 26, 1979 both for its nationally significant badlands and riverside riparian habitats, and for the international importance of the fossils found there.\n\nUntil 1985, discoveries made in the park had to be shipped to museums throughout the world for scientific analysis and display, including the Royal Ontario Museum in Toronto in Ontario, the Canadian Museum of Nature in Ottawa in Ontario, and the American Museum of Natural History in New York City, New York State. This changed with the opening of the Royal Tyrrell Museum of Palaeontology 100 kilometres upstream in Midland Provincial Park adjacent to Drumheller.\n\nThe park protects a very complex ecosystem including three communities: prairie grasslands, badlands, and riverside cottonwoods. Its ecosystem is surrounded by prairies but is unique unto itself. Choruses of coyotes are common at dusk, as are the calls of nighthawks. Cottontail rabbits, mule deer, and pronghorn can all be seen in the park; the prairie rattlesnake, bull snake and the red-sided garter snake are present as well. Curlews and Canada geese are among the 165 bird species that can be seen in the spring and summer. Some of the most northern species of cactus, including \"Opuntia\" (prickly pear) and \"Pediocactus\" (pincushion) can be observed in full bloom during the later half of June.\n\nThe sediments exposed in the badlands at Dinosaur Provincial Park were laid down over a period of about 1.5 million years during the Campanian stage of the Late Cretaceous epoch, and belong to three different geologic formations. The top of the terrestrial Oldman Formation, which outcrops at the base of the sequence, is the oldest. It is overlain by a complete section of the terrestrial Dinosaur Park Formation, which is in turn overlain by the base of the marine Bearpaw Formation. The Dinosaur Park Formation, which contains most of the articulated dinosaur skeletons, was laid down between about 76.5 and 74.8 million years ago. It was deposited in floodplain and coastal plain environments by river systems that flowed eastward and southeastward to the Western Interior Seaway.\n\nDinosaur Provincial Park preserves an extraordinarily diverse group of freshwater vertebrates. Fish include sharks, rays (such as the durophage \"Myledaphus\"), paddlefish, bowfins, gars, and teleosts. Amphibians include frogs, salamanders, and the extinct albanerpetontids. Reptiles include lizards (such as the large monitor \"Palaeosaniwa\"), a wide range of turtles, crocodilians, and the fish-eating \"Champsosaurus\". Mammal fossils from the park are relatively rare and consist of isolated teeth, fragmentary jaws with teeth, and tooth fragments from mouse-sized and shrew-sized animals. They include representatives of placental, marsupial, and multituberculate mammals.\n\nPlant fossils from the park and surrounding area include fern fronds; foliage and wood of taxodiaceous and cupressaceous conifers; and leaves of \"Ginkgo\", \"Cercidiphyllum\", \"Platanus\", a \"Pistia\"-like aquatic plant, and others. A rich assemblage of fossil pollen and spores has also been described.\n\nThe dinosaurs of the park are astonishingly diverse. They include:\n\nCeratopsia\n\nHadrosauridae\n\nAnkylosauria\n\nHypsilophodontidae\n\nPachycephalosauria\n\nTyrannosauridae\n\nOrnithomimidae\n\n\nCaenagnathidae\n\n\nDromaeosauridae\n\nTroodontidae\n\nClassification Uncertain\n\nBirds such as Hesperornithiformes were present, as well as giant Pterosauria related to \"Quetzalcoatlus\". Stagodont marsupials, placentals and multituberculate mammals scurried underfoot.\n\n\n"}
{"id": "28886038", "url": "https://en.wikipedia.org/wiki?curid=28886038", "title": "Dodge Morgan", "text": "Dodge Morgan\n\nDodge David Morgan (January 15, 1932 – September 14, 2010) was an American sailor, businessman, publisher and \"self-proclaimed contrarian.\" He flew fighter jets in the U.S. Air Force in the early 1950s, worked as a newspaper reporter in Alaska, and became a millionaire by operating Controlonics, a company that manufactured Whistler radar detectors from 1971 to 1983. He gained fame in 1986 as the first American to sail solo around the world with no stops; Dodge was the third person to ever accomplish the feat and set eleven world records during his voyage including the fastest ever solo, non-stop circumnavigation. He also set a world record for eastward sailing when he completed his journey in 150 days, cutting the prior record of 292 days nearly in half. He spent his later years living on a 30-acre island that he purchased in 1998 in Maine's Quahog Harbor.\n\nMorgan was born in Malden, Massachusetts, in 1932. His father, Russell Morgan, was a pharmacist who died when Morgan was two years old. His mother, Ruth Dodge Morgan remarried, and Morgan recalled having \"quite a happy life\" with his new family. He described himself as \"a lousy student\" who devoted most of his time to \"sports and other such activities.\" He also worked at his uncle's boatyard on Cape Cod as a teenager.\n\nOn reaching adulthood, Morgan joined the United States Air Force and flew jet fighters. He joined the Air Force, he said, \"so I wouldn't have to tell my mother I got kicked out of college.\" While serving in the Air Force, he once crashed his F-86 Sabre fighter jet. Morgan later recalled that he was trying to land at Presque Isle Air Force Base when his engine flamed out and the plane crashed into the Maine woods. His canopy jammed, and rescuers had to extricate him with an ax. Asked about the threat of fire from the fuel tanks in the jet's wings, Morgan said, \"No problem, the wings were about 300 yards behind me when the fuselage finally stopped.\"\n\nAfter being discharged from the Air Force, Morgan attended Boston University, where he received a degree in journalism. He briefly married Lael Warren (Morgan) while they were both at BU. They moved to Alaska and worked as reporters for the \"Anchorage Daily News\".\n\n'Morgan returned to Massachusetts and headed his own advertising and public relations firm. He saved enough money to buy a 36-foot wooden schooner named \"Coaster.\" He sailed the 'Coaster' from Maine to Alaska with stops at the Virgin Islands, the Panama Canal and Hawaii. Morgan recalled that, after buying the boat, \"I never slept ashore for 2-1/2 years.\" Coaster was sold in Alaska and Dodge returned to Massachusetts.\n\nIn 1971, Morgan formed Controlonics Corporation in Westford, Massachusetts. The company manufactured and marketed many early radio frequency devices, early radar and their most successful Whistler radar detector. He started the company with a $5,000 loan and built it \"from three people in a garage to 300 people.\" Interviewed in 2005, Morgan said that, after his children, the accomplishment that made him proudest was \"the culture of openness that I felt responsible for at my company. It was even more important than our financial success. There was a culture that accepted and celebrated individual eccentricities.\" Morgan sold Controlonics in December 1983 for a sum between $32 million and $35 million.\n\nMorgan made a promise to himself in the early 1960s that he would one day sail around the world. He sold Controlonics to \"follow a dream I had years before on the old schooner, to sail around the world on a boat which was designed for that.\"\n\nIn 1985, at age 53, he embarked his journey around the world on the 60-foot cutter American Promise. The boat was designed by 1974 America's Cup winner Ted Hood. Hood recalled that the boat was designed for safety and redundancy rather than speed: \"Everyone said there's no way that boat is going to get around the world in record speed, but it did.\"\n\nMorgan commissioned The New Film Company, Inc. of Boston, Massachusetts, to produce a film about his journey. Producer Christopher G. Knight placed six film cameras on \"American Promise,\" three above deck and three below. One in each set was programmed to come on twice a day during daylight hours and run for 30 seconds, thus enabling Morgan to film himself. He used the cameras as a film log and shot over 9 hours of film that was ultimately edited into the 57-minute film, 'AROUND ALONE.\n\nMorgan departed from Ordnance Island, in St. George's, Bermuda, on November 12, 1985, and returned there on April 11, 1986, completing the journey in 150 days, 1 hour, and 6 minutes. As the \"American Promise\" sailed into St. George's Harbor, Morgan's project manager, Grant Robinson, noted that the boat looked spit-and-polished. He radioed, \" \"She doesn't look any the worse for wear at all. We see you've still got some paint on her.\" Morgan replied, \"What do you mean 'some paint on her'? She's only been used once.\"\n\nAs Morgan disembarked, his wife, Manny, and his two children, Hoyt David and Kimberley Promise, embraced in a family hug that was cut short when he was handed his favorite food, a cheeseburger. He told the crowd that had gathered to greet him: \"It takes three things to sail around the world alone. A good boat, an iron will and luck. To do so in record time takes a great boat, an iron will and extraordinary luck. And, my friends, here is a great boat.\"\n\nMorgan joked that his most frightening moment came \"when I pulled the next-to-last bottle of beer from the bilge.\" But his journey brought real dangers, including a tropical storm in the South Pacific that battered the boat with 70-mile-per-hour winds for three days.\n\nMorgan was the third person and the first American to sail solo around the globe with no stops. At the time, \"Sports Illustrated\" reported, \"Only [two] men before Morgan -- none of them American -- had sailed around the world alone without stopping. No taking on additional food or water; no assistance accepted from another vessel; no using the motor for propulsion.\" In making the journey in 150 days, Morgan shattered the prior record of 292 days set in 1971 by English sailor Chay Blyth. (Note Blyth's voyage in British Steel was \"Westabout\" against the prevailing winds.)\n\nMorgan wrote about his voyage in a book titled \"The Voyage of American Promise,\" published by Houghton Mifflin in 1989. AROUND ALONE, the 57-minute film produced by The New Film Company, Inc. about his around-the-world journey was the first featured film in the PBS series, \"Adventure.\" It first aired in March 1987.\n\nMorgan's voyage was also the focus of a series of psychological papers, including an entire issue of the Journal of Personality devoted to analyzing Morgan's life, his experience of the voyage, and the ways in which it may have affected his personality development.\n\nIn 1985, Morgan moved to Cape Elizabeth, Maine. He purchased the alternative weekly newspaper the \"Maine Times\" in 1985 and also purchased the \"Casco Bay Weekly\", an alternative newspaper in Portland, Maine, in 1990. In 2002, the \"Casco Bay Weekly\" stopped publishing. Morgan began by laying off much of the staff, noting at the time that the newspaper had been losing money for about a year. When the paper was finally shut down, Morgan said, \"I don't want to get in the Guinness Book of World Records for money buried in a small-market weekly newspaper.\"\n\nMorgan's first marriage was brief and ended in divorce. He married his second wife, Manny, in 1972, and they had two children, Kimberly Promise and Hoyt David. Morgan's second marriage also ended in divorce, but after a rewarding and loving family was built. All four remained very close.\n\nIn 1998, Morgan purchased Snow Island in Harpswell, Maine. He commissioned Portland architect Winton Scott to design a home for him on the island. In a feature story, the \"Boston Globe\" described Morgan's island as \"30 acres of dense cedar forest and fragrant fern, spongy moss and scaly lichen, cattail bog and rockweed shore smack in the middle of Quahog Bay.\" In 1999, the Maine chapter of the American Institute of Architects gave the compound a design award and called it \"a triumph of programmatic virtue in a natural setting that demands nothing less.\" For the next 12 years, Morgan lived on his 30-acre island sanctuary where he moored six boats of various types. In 2005, he told a reporter, \"I live alone here on Snow Island. In one sense, I am quite pleased with my solitary life. I engage with people I love and admire, one-on-one, with an intense joy.\" Most recently, he lived on Snow Island with his fiancée, Mary Beth Teas.\n\nIn September 2010, Morgan died at age 78 from complications from cancer surgery at Brigham and Women's Hospital in Boston, Massachusetts.\n\ndfgvbfcv\n"}
{"id": "9871209", "url": "https://en.wikipedia.org/wiki?curid=9871209", "title": "Driller's depth", "text": "Driller's depth\n\nThe original depth recorded while drilling an oil or gas well is known as the Driller's Depth.\n\nSince there is not a single reference or measurement system for calculating the depth in sub-surface environments, two engineers talking about a single drilling might give different answers when asked to give a measurement of depth.\n\nThe two main depth references used in the \"downhole\" (i.e. sub-surface) environment are Driller's Depths and Logger's Depths (also called Wireline Logger's Depths). These measurement systems are recorded quite differently and generally Logger's Depths are considered the more accurate of the two:\n\nThere are several parts of the drilling site to be considered while measuring:\n\nIn practice, Driller's Depth measurement is a manual operation, not changed significantly over the years and there are many facets of the system with potential to introduce errors and inconsistencies.\n\nThe bulk of the drill string is drill pipe which has a nominal length of 9.6 meter per pipe section, however, in reality, not all pipes are the same length. Weakened or damaged ends of a pipe section will be reworked, resulting in reduced length.\n\nSteel pipe has a \"male\" connection at one end (called \"the pin\") and a \"female\" connection at the other end (called \"the box\") and as each section of pipe is lowered into the hole it is connected to the pipe preceding it by threading together the male and female components. Drill pipe connections (or drill pipe/collar, or collar/bit and any other connection) must have a very good sealing surface because high pressure mud will be traveling through the pipe and any pitted or galled areas could be quickly eroded out. This is usually referred to as a \"wash-out\", or words to that effect, and can occur in any part of the drill string or bottom hole assembly. Because of this pipe is routinely inspected before and after use. Any imperfections are eliminated one of two actions:\n\nTracking and recording of drill pipe at the rig site starts when individual joints are picked up. Joint numbers are manually marked on the side of the pipe. Typically three sections of pipe are joined together into a \"stand\" (of about 27–29 m in length) and stacked in rows of 10, with their base resting on the drill floor. Prior to running in the hole each stand is manually measured with a steel tape measure and the measurement recorded in a computer spreadsheet (previously a pipe tally book was used) alongside the stand number. To confirm at any stage what depth the drill bit is operating at, the driller consults the pipe tally records, and measures the length of the current stand of drill pipe below the rig floor.\n\nAnother potential area for error is the Bottom Hole Assembly (BHA). The BHA consists of the drill bit, drill collars and stabilizers. Additionally, it can also include a downhole motor, MWD (measurement while drilling) and LWD (logging while drilling) tools. Errors come about if the total BHA is not correctly measured or recorded. Often BHA changes may be made during operations and if these changes are not recorded then the depths will be incorrect.\n\nIdeally the BHA is operated to minimize “sagging” within the borehole. Pipe stretch and compression will occur from time to time but are not corrected for during normal operations, even though they can introduce fairly significant cumulative errors on driller's depth, particularly in deep wells or in areas of hard rock.\n\nIf exploration derived prognosed depths are significantly different from the Driller's Depths, for example by 10−30 m, then warning bells come up – it is possible a pipe section or stand has been left off the calculations. If this is suspected, then the drill string should be measured (in tension) when the string is next pulled out of the hole, and the results checked with the tally. Mudloggers should be vigilant, as they provide the opportunity to cross check with the drilling company.\n\nFor some deep wells, e.g. 7000 m or 25000 ft deep, the drill pipe elongation due to its own weight and temperature must be taken into account. This can be on the order of 24 m (80 ft). Wireline does not behave this way: it tends to lengthen under tension but shorten with increasing temperature. One can only assume by how much this net effect varies. Wireline depth correction for temperature and tension has been around since before the days of computer data acquisition, and is generally seen as reliable. Based on experience, the impact on a geological model previously based on wireline depth, when drilling at greater than 7,000 m and using LWD (driller's) depths, can introduce differences in marker depths of up to 25 m (80 ft): the driller's depths are consistently higher than the more reliable wireline depths.\n\nThe driller's survey do not call this elongation an uncertainty but rather call it \"bias\" or \"error\". For the example above, in addition to the 25 m (+80 ft) bias, there would be about ± 3 m/12 ft to 10 m/30 ft residual uncertainty depending on hole inclination. There are a few in the industry who know how to correct for this real time and some service companies have developed conceptual or prototype tools/processes to account for this elongation effect. In future, these corrections should become standard practice for the industry, but they are not . The determination of an accurate depth has not traditionally been a popular area of research. The impact of errors in depth is most critical when integrating data from more than one well, e.g. to build a reservoir model.\n\n\n"}
{"id": "26436980", "url": "https://en.wikipedia.org/wiki?curid=26436980", "title": "Eastern Himalayan subalpine conifer forests", "text": "Eastern Himalayan subalpine conifer forests\n\nThe Eastern Himalayan subalpine conifer forests is a temperate coniferous forests ecoregion which is found in the middle and upper elevations of the eastern Middle Himalayas, in western Nepal, Bhutan and northern Indian states including Arunachal Pradesh.\n\nThe ecoregion forms a belt of coniferous forest covering from elevation extending from the Gandaki River in Nepal east through Bhutan and into Arunachal Pradesh. It is part of a transition zone from Indomalaya ecozone, in the south, to the Palearctic ecozone, in the north and is the last habitat below the treeline of the Himalayas. The Himalayas are lined with belts of habitat from the grassy foothills to the high peaks and are home to a number of birds and animals that migrate seasonally through these zones, including these conifer forests, each of which provides crucial habitat at different times of the year. Furthermore the streams and rivers of the steep mountainsides will flood if not held in place by woodland.\n\nThe Eastern Himalayas are watered by the Bay of Bengal monsoon so are wetter than in the west and have a higher treeline (4,500m compared to 3,000m in the western Himalayas).\n\nThese forests are typically found on steep, rocky, north-facing slopes. The most common trees are \"Abies spectabilis, Larix griffithii, Juniperus recurva, Juniperus indica, Betula utilis\", \"Acer\" spp., and \"Sorbus\" spp. The understory features a rich community of colourful rhododendrons, including \"Rhododendron hodgsonii\", \"Rhododendron barbatum\", \"Rhododendron campylocarpum\", \"Rhododendron campanulatum, Rhododendron fulgens\", and \"Rhododendron thomsonii\". Other shrubs include \"Viburnum grandiflorum\" and \"Lonicera angustifolia\". \n\n\"Tsuga dumosa\" occurs in wetter areas and lower elevations. \"Pinus wallichiana\" occurs in drier areas around Tibet. It is particularly common in the Khumbu region. \"Taxus baccata\" is important but uncommon.\n\nJuniper woodlands grow in flat, inner river valleys, mixed with various species of \"Salix\" and \"Prunus\". Those in the Tsarijathang Valley in Bhutan's Jigme Dorji National Park are an important summer habitat for takin (\"Budorcas taxicolor\").\n\nThis ecoregion is home to some eighty-nine species of mammals originally from both Indomalayan and Palearctic ecozones, including civets, martens, Himalayan tahr and muntjac. The white-bellied musk deer, hunted for its musk-glands, and the endangered red panda are important inhabitants for whom the conifer forests are typical habitat with the red panda living between 3,000m and 4,000m where there is an undercover of bamboo beneath the fir trees. Other endangered species found here are the takin, Himalayan serow (\"Capricornis thar\"), and particolored flying squirrel (\"Hylopetes alboniger\") while Mandelli's mouse-eared bat, the Asiatic wild dog, the Asiatic black bear and the Himalayan tahr are considered vulnerable. There are two near-endemic squirrels, Hodgson's giant flying squirrel (\"Petaurista magnificus\") and the Bhutan giant flying squirrel (\"Petaurista nobilis\") along with a pure endemic rodent, the Himalayan field mouse (\"Apodemus gurkha\").\n\nAbout 200 species of birds have been recorded in this ecoregion of which six are endemic; chestnut-breasted partridge (\"Arborophila mandellii\"), hoary-throated barwing (\"Actinodura nipalensis\"), Ludlow's fulvetta, (\"Alcippe ludlowi\"), Nepal wren-babbler (\"Pnoepyga immaculata\"), buff-throated partridge (\"Tetraophasis szechenyii\"), and Lord Derby's parakeet (\"Psittacula derbiana\"). The last two are limited to an area of conifer forest in Arunachal Pradesh. Threatened or endangered birds of the ecoregion include Tibetan eared pheasant (\"Crossoptilon harmani\") and Sclater's monal (\"Lophophorus sclateri\") while a number of other birds are sensitive to habitat change and therefore potentially vulnerable, these include blood pheasant (\"Ithaginis cruentus\"), Blyth's tragopan (\"Tragopan blythii\"), satyr tragopan (\"Tragopan satyra\"), Ward's trogon (\"Harpactes wardi\") and chestnut-breasted partridge. Indeed this ecoregion forms part of two BirdLife International Endemic Bird Areas because of the number of birds for which the confifers are important for breeding.\n\nThe human population of these heights is very low and most of the natural conifer forest remains with a considerable portion in protected areas. Damage is caused as trees are cut to provide firewood for local inhabitants and for trekking parties or to clear land for grazing. Large protected areas that contain areas of conifer forest include Annapurna Conservation Area, Langtang and Makalu Barun National Parks in Nepal, Singalila National Park in India, and Sakteng Wildlife Sanctuary, Jigme Singye Wangchuck and Jigme Dorji National Parks in Bhutan.\n\n"}
{"id": "17890562", "url": "https://en.wikipedia.org/wiki?curid=17890562", "title": "Elburz Range forest steppe", "text": "Elburz Range forest steppe\n\nThe Elburz Range forest steppe ecoregion is an arid, mountainous 1,000-kilometer arc south of the Caspian Sea, stretching across northern Iran from the Azerbaijan border to near the Turkmenistan border. It covers and encompasses the southern and eastern slopes of the Alborz Mountains as well as their summits. The Caspian Hyrcanian mixed forests ecoregion, with its lush green mountainsides and plains that receive moisture from the Caspian Sea, forms this ecoregion's northern border. The vast Central Persian desert basin ecoregion forms its southern border.\n\nThe Alborz range is composed of a granite core overlain with sedimentary rock including limestones, shales, sandstones, and tuffs. Metamorphic rocks such as schists, marbles, and amphibolite are also widely found. The climate is arid with annual precipitation varying from 150 mm to 500 mm, falling mostly as winter snow. \n\nElevations typically range from , and the highest point in the Middle East, high Mount Damavand, is found here. Mount Damavand is also the tallest volcano in Asia and below its summit crater are found fumaroles and hot springs as well as glaciers.\n\nJuniper (\"Juniperus excelsa\" subsp. polycarpos) is the most common tree in this ecoregion. It formerly covered south-facing slopes, but logging has greatly reduced its range to inaccessible areas and high elevations. Shrubs in the ecoregion are pistachio (\"Pistacia atlantica\"), cotoneaster (\"Cotoneaster racemiflora\"), maple (\"Acer turcomanicum\"), and almond (\"Amygdalus\" spp.). Wormwood is a common herbaceous plant.\n\nThis ecoregion is home to several large mammal species. Syrian brown bear (\"Ursus arctos syriacus\") wander the mountains and hillsides while solitary roe deer (\"Capreolus capreolus\") feed on grass and berries in and around forests. Groups of native wild boar (\"Sus scrofa\") forage at night and beech martens (\"Martes foina\") hunt smaller mammals and search for eggs and worms at dawn and dusk. Red deer (\"Cervus elaphus\") live in single sex groups most of the year, but rut in the fall, sometimes locking antlers. Canids in this ecoregion are Indian wolf (\"Canis lupus pallipes\"), common jackal (\"Canis aureus aureus\"), and red fox (\"Vulpes vulpes\"). Felids are Persian leopard (\"Panthera pardus ciscaucasica\"), jungle cat (\"Felis chaus\"), and Caucasus lynx (\"Lynx lynx dinniki\"). Goitered gazelle (\"Gazella subgutturosa\") walk the plains in the southeast. There are also large populations of the globally endangered argali (\"Ovis ammon\"). \n\nNotable birds in this ecoregion are honey buzzard (\"Pernis apivorus\"), goshawk (\"Accipiter gentilis\"), black vulture (\"Aegypius monachus\"), bimaculated lark (\"Melanocorypha bimaculata\") and Caspian snowcock (\"Tetraogallus caspius\"). Eagles here are the lesser spotted eagle (\"Aquila pomarina\") and the golden eagle (\"Aquila chrysaetos\"). The ecoregion is also a breeding area for the little bustard (\"Tetrax tetrax\") and black woodpecker (\"Dryocopus martius\").\n\nLogging and agriculture have reduced the range of the forests in this ecoregion, dams have disrupted river flows, and overgrazing has degraded habitat. \n\nProtected areas with Elburz Range forest steppe in Iran include Golestan National Park and the Ghorkhod Protected Area, both in Golestan Province and totalling .\n\n"}
{"id": "320217", "url": "https://en.wikipedia.org/wiki?curid=320217", "title": "Electrical substation", "text": "Electrical substation\n\nA substation is a part of an electrical generation, transmission, and distribution system. Substations transform voltage from high to low, or the reverse, or perform any of several other important functions. Between the generating station and consumer, electric power may flow through several substations at different voltage levels. A substation may include transformers to change voltage levels between high transmission voltages and lower distribution voltages, or at the interconnection of two different transmission voltages.\n\nSubstations may be owned and operated by an electrical utility, or may be owned by a large industrial or commercial customer. Generally substations are unattended, relying on SCADA for remote supervision and control.\n\nThe word \"substation\" comes from the days before the distribution system became a grid. As central generation stations became larger, smaller generating plants were converted to distribution stations, receiving their energy supply from a larger plant instead of using their own generators. The first substations were connected to only one power station, where the generators were housed, and were subsidiaries of that power station.\n\nSubstations may be described by their voltage class, their applications within the power system, the method used to insulate most connections, and by the style and materials of the structures used. These categories are not disjointed; for example, to solve a particular problem, a transmission substation may include significant distribution functions.\nA \"transmission substation\" connects two or more transmission lines. The simplest case is where all transmission lines have the same voltage. In such cases, substation contains high-voltage switches that allow lines to be connected or isolated for fault clearance or maintenance. A transmission station may have transformers to convert between two transmission voltages, voltage control/power factor correction devices such as capacitors, reactors or static VAR compensators and equipment such as phase shifting transformers to control power flow between two adjacent power systems.\n\nTransmission substations can range from simple to complex. A small \"switching station\" may be little more than a bus plus some circuit breakers. The largest transmission substations can cover a large area (several acres/hectares) with multiple voltage levels, many circuit breakers, and a large amount of protection and control equipment (voltage and current transformers, relays and SCADA systems). Modern substations may be implemented using international standards such as IEC Standard 61850.\n\nA \"distribution substation\" transfers power from the transmission system to the distribution system of an area. It is uneconomical to directly connect electricity consumers to the main transmission network, unless they use large amounts of power, so the distribution station reduces voltage to a level suitable for local distribution.\n\nThe input for a distribution substation is typically at least two transmission or sub-transmission lines. Input voltage may be, for example, 115 kV, or whatever is common in the area. The output is a number of feeders. Distribution voltages are typically medium voltage, between 2.4 kV and 33 kV, depending on the size of the area served and the practices of the local utility. The feeders run along streets overhead (or underground, in some cases) and power the distribution transformers at or near the customer premises.\n\nIn addition to transforming voltage, distribution substations also isolate faults in either the transmission or distribution systems. Distribution substations are typically the points of voltage regulation, although on long distribution circuits (of several miles/kilometers), voltage regulation equipment may also be installed along the line.\n\nThe downtown areas of large cities feature complicated distribution substations, with high-voltage switching, and switching and backup systems on the low-voltage side. More typical distribution substations have a switch, one transformer, and minimal facilities on the low-voltage side.\n\nIn distributed generation projects such as a wind farm, a collector substation may be required. It resembles a distribution substation although power flow is in the opposite direction, from many wind turbines up into the transmission grid. Usually for economy of construction the collector system operates around 35 kV, and the collector substation steps up voltage to a transmission voltage for the grid. The collector substation can also provide power factor correction if it is needed, metering, and control of the wind farm. In some special cases a collector substation can also contain an HVDC converter station.\n\nCollector substations also exist where multiple thermal or hydroelectric power plants of comparable output power are in proximity. Examples for such substations are Brauweiler in Germany and Hradec in the Czech Republic, where power is collected from nearby lignite-fired power plants. If no transformers are required for increasing the voltage to transmission level, the substation is a switching station.\n\nConverter substations may be associated with HVDC converter plants, traction current, or interconnected non-synchronous networks. These stations contain power electronic devices to change the frequency of current, or else convert from alternating to direct current or the reverse. Formerly rotary converters changed frequency to interconnect two systems; nowadays such substations are rare.\n\nA switching station is a substation without transformers and operating only at a single voltage level. Switching stations are sometimes used as collector and distribution stations. Sometimes they are used for switching the current to back-up lines or for parallelizing circuits in case of failure. An example is the switching stations for the HVDC Inga–Shaba transmission line.\n\nA switching station may also be known as a switchyard, and these are commonly located directly adjacent to or nearby a power station. In this case the generators from the power station supply their power into the yard onto the Generator Bus on one side of the yard, and the transmission lines take their power from a Feeder Bus on the other side of the yard.\n\nAn important function performed by a substation is switching, which is the connecting and disconnecting of transmission lines or other components to and from the system. Switching events may be planned or unplanned. A transmission line or other component may need to be de-energized for maintenance or for new construction, for example, adding or removing a transmission line or a transformer. To maintain reliability of supply, companies aim at keeping the system up and running while performing maintenance. All work to be performed, from routine testing to adding entirely new substations, should be done while keeping the whole system running.\n\nUnplanned switching events are caused by a fault in a transmission line or any other component, for example:\nThe function of the switching station is to isolate the faulty portion of the system in the shortest possible time. De-energizing faulty equipment protects it from further damage, and isolating a fault helps keep the rest of the electrical grid operating with stability.\n\nElectrified railways also use substations, often distribution substations. In some cases a conversion of the current type takes place, commonly with rectifiers for direct current (DC) trains, or rotary converters for trains using alternating current (AC) at frequencies other than that of the public grid. Sometimes they are also transmission substations or collector substations if the railway network also operates its own grid and generators to supply the other stations.\n\nA \"mobile substation\" is a substation on wheels, containing a transformer, breakers and buswork mounted on a self-contained semi-trailer, meant to be pulled by a truck. They are designed to be compact for travel on public roads, and are used for temporary backup in times of natural disaster or war. Mobile substations are usually rated much lower than permanent installations, and may be built in several units to meet road travel limitations.\n\nSubstations generally have switching, protection and control equipment, and transformers. In a large substation, circuit breakers are used to interrupt any short circuits or overload currents that may occur on the network. Smaller distribution stations may use recloser circuit breakers or fuses for protection of distribution circuits. Substations themselves do not usually have generators, although a power plant may have a substation nearby. Other devices such as capacitors and voltage regulators may also be located at a substation.\n\nSubstations may be on the surface in fenced enclosures, underground, or located in special-purpose buildings. High-rise buildings may have several indoor substations. Indoor substations are usually found in urban areas to reduce the noise from the transformers, for reasons of appearance, or to protect switchgear from extreme climate or pollution conditions.\n\nA grounding (earthing) system must be designed. The total ground potential rise, and the gradients in potential during a fault (called \"touch\" and \"step\" potentials), must be calculated to protect passers-by during a short-circuit in the transmission system. Earth faults at a substation can cause a ground potential rise. Currents flowing in the Earth's surface during a fault can cause metal objects to have a significantly different voltage than the ground under a person's feet; this touch potential presents a hazard of electrocution. Where a substation has a metallic fence, it must be properly grounded to protect people from this hazard.\n\nThe main issues facing a power engineer are reliability and cost. A good design attempts to strike a balance between these two, to achieve reliability without excessive cost. The design should also allow expansion of the station, when required.\n\nSelection of the location of a substation must consider many factors. Sufficient land area is required for installation of equipment with necessary clearances for electrical safety, and for access to maintain large apparatus such as transformers.\n\nWhere land is costly, such as in urban areas, gas insulated switchgear may save money overall. Substations located in coastal areas affected by flooding and tropical storms may often require an elevated structure to keep equipment sensitive to surges hardened against these elements. The site must have room for expansion due to load growth or planned transmission additions. Environmental effects of the substation must be considered, such as drainage, noise and road traffic effects.\n\nThe substation site must be reasonably central to the distribution area to be served. The site must be secure from intrusion by passers-by, both to protect people from injury by electric shock or arcs, and to protect the electrical system from misoperation due to vandalism.\n\nThe first step in planning a substation layout is the preparation of a one-line diagram, which shows in simplified form the switching and protection arrangement required, as well as the incoming supply lines and outgoing feeders or transmission lines. It is a usual practice by many electrical utilities to prepare one-line diagrams with principal elements (lines, switches, circuit breakers, transformers) arranged on the page similarly to the way the apparatus would be laid out in the actual station.\n\nIn a common design, incoming lines have a disconnect switch and a circuit breaker. In some cases, the lines will not have both, with either a switch or a circuit breaker being all that is considered necessary. A disconnect switch is used to provide isolation, since it cannot interrupt load current. A circuit breaker is used as a protection device to interrupt fault currents automatically, and may be used to switch loads on and off, or to cut off a line when power is flowing in the 'wrong' direction. When a large fault current flows through the circuit breaker, this is detected through the use of current transformers. The magnitude of the current transformer outputs may be used to trip the circuit breaker resulting in a disconnection of the load supplied by the circuit break from the feeding point. This seeks to isolate the fault point from the rest of the system, and allow the rest of the system to continue operating with minimal impact. Both switches and circuit breakers may be operated locally (within the substation) or remotely from a supervisory control center.\n\nWith overhead transmission lines, the propagation of lightning and switching surges can cause insulation failures into substation equipment. Line entrance surge arrestors are used to protect substation equipment accordingly. Insulation Coordination studies are carried out extensively to ensure equipment failure (and associated outages) is minimal.\n\nOnce past the switching components, the lines of a given voltage connect to one or more buses. These are sets of busbars, usually in multiples of three, since three-phase electrical power distribution is largely universal around the world.\n\nThe arrangement of switches, circuit breakers, and buses used affects the cost and reliability of the substation. For important substations a ring bus, double bus, or so-called \"breaker and a half\" setup can be used, so that the failure of any one circuit breaker does not interrupt power to other circuits, and so that parts of the substation may be de-energized for maintenance and repairs. Substations feeding only a single industrial load may have minimal switching provisions, especially for small installations. \n\nOnce having established buses for the various voltage levels, transformers may be connected between the voltage levels. These will again have a circuit breaker, much like transmission lines, in case a transformer has a \"fault\" (commonly called a \"short circuit\").\n\nAlong with this, a substation always has control circuitry needed to command the various circuit breakers to open in case of the failure of some component.\n\nEarly electrical substations required manual switching or adjustment of equipment, and manual collection of data for load, energy consumption, and abnormal events. As the complexity of distribution networks grew, it became economically necessary to automate supervision and control of substations from a centrally attended point, to allow overall coordination in case of emergencies and to reduce operating costs. Early efforts to remote control substations used dedicated communication wires, often run alongside power circuits. Power-line carrier, microwave radio, fiber optic cables as well as dedicated wired remote control circuits have all been applied to Supervisory Control and Data Acquisition (SCADA) for substations. The development of the microprocessor made for an exponential increase in the number of points that could be economically controlled and monitored. Today, standardized communication protocols such as DNP3, IEC 61850 and Modbus, to list a few, are used to allow multiple intelligent electronic devices to communicate with each other and supervisory control centers. Distributed automatic control at substations is one element of the so-called smart grid.\n\nSwitches, circuit breakers, transformers and other apparatus may be interconnected by air-insulated bare conductors strung on support structures. The air space required increases with system voltage and with the lightning surge voltage rating. For medium-voltage distribution substations, metal-enclosed switch gear may be used and no live conductors exposed at all. For higher voltages, gas-insulated switch gear reduces the space required around live bus. Instead of bare conductors, bus and apparatus are built into pressurized tubular containers filled with sulfur hexafluoride (SF) gas. This gas has a higher insulating value than air, allowing the dimensions of the apparatus to be reduced. In addition to air or SF gas, apparatus will use other insulation materials such as transformer oil, paper, porcelain, and polymer insulators.\n\nOutdoor, above-ground substation structures include wood pole, lattice metal tower, and tubular metal structures, although other variants are available. Where space is plentiful and appearance of the station is not a factor, steel lattice towers provide low-cost supports for transmission lines and apparatus. Low-profile substations may be specified in suburban areas where appearance is more critical. Indoor substations may be gas-insulated switchgear (at high voltages), or metal-enclosed or metal-clad switchgear at lower voltages. Urban and suburban indoor substations may be finished on the outside so as to blend in with other buildings in the area.\n\nA \"compact substation\" is generally an outdoor substation built in a metal enclosure, in which each item of the electrical equipment is located very near to each other to create a relatively smaller footprint size of the substation.\n\n\n"}
{"id": "1765158", "url": "https://en.wikipedia.org/wiki?curid=1765158", "title": "Elevation", "text": "Elevation\n\nThe elevation of a geographic location is its height above or below a fixed reference point, most commonly a reference geoid, a mathematical model of the Earth's sea level as an equipotential gravitational surface (see Geodetic datum § Vertical datum).\nThe term \"elevation\" is mainly used when referring to points on the Earth's surface, while \"altitude\" or \"geopotential height\" is used for points above the surface, such as an aircraft in flight or a spacecraft in orbit, and \"depth\" is used for points below the surface.\nElevation is not to be confused with the distance from the center of the Earth. Due to the equatorial bulge, the summits of Mount Everest and Chimborazo have, respectively, the largest elevation and the largest geocentric distance.\n\nGIS or geographic information system is a computer system that allows for visualizing, manipulating, capturing, and storage of data with associated attributes. GIS offers better understanding of patterns and relationships of the landscape at different scales. Tools inside the GIS allow for manipulation of data for spatial analysis or cartography.\nA topographical map is the main type of map used to depict elevation, often through use of contour lines.\nIn a Geographic Information System (GIS), digital elevation models (DEM) are commonly used to represent the surface (topography) of a place, through a raster (grid) dataset of elevations. Digital terrain models are another way to represent terrain in GIS.\n\nUSGS (United States Geologic Survey) is developing a 3D Elevation Program (3DEP) to keep up with growing needs for high quality topographic data. 3DEP is a collection of enhanced elevation data in the form of high quality LiDAR data over the conterminous United States, Hawaii, and the U.S. territories. There are three bare earth DEM layers in 3DEP which are nationally seamless at the resolution of 1/3, 1, and 2 arcseconds.\n\nThis map is derived from GTOPO30 data that describes the elevation of Earth's terrain at intervals of 30 arcseconds (approximately 1 km). It uses color and shading instead of contour lines to indicate elevation.\n\nHypsography is the study of the distribution of elevations on the surface of the Earth, although the term is sometimes also applied to other rocky planets such as Mars or Venus. The term originates from the Greek word \"hypsos\" meaning height. Most often it is used only in reference to elevation of land but a complete description of Earth's solid surface requires a description of the seafloor as well. Related to the term hypsometry, the measurement of these elevations of a planet's solid surface are taken relative to mean datum, except for Earth which is taken relative to the sea level.\nIn the troposphere, temperatures decrease with altitude. This lapse rate is approximately 6.5 °C/km.\n\n\n"}
{"id": "4024968", "url": "https://en.wikipedia.org/wiki?curid=4024968", "title": "Emagram", "text": "Emagram\n\nAn emagram is one of four thermodynamic diagrams used to display temperature lapse rate and moisture content profiles in the atmosphere. The emagram has axes of temperature (T) and pressure (p). In the emagram, the dry adiabats make an angle of about 45 degrees with the isobars, isotherms are vertical and isopleths of saturation mixing ratio are almost straight and vertical. \nUsually, temperature and dew point data from radiosondes are plotted on these diagrams to allow calculations of convective stability or Convective Available Potential Energy. Wind barbs are often plotted at the side of a tephigram to indicate the winds at different heights.\n\nFirst devised in 1884 by Heinrich Hertz, the emagram is used primarily in European countries. Other countries use similar thermodynamic diagrams for the same purpose. However, the details of their construction vary.\n\nexample: \n\n\nThermodynamic Diagrams\n\n"}
{"id": "446539", "url": "https://en.wikipedia.org/wiki?curid=446539", "title": "Fermionic condensate", "text": "Fermionic condensate\n\nA fermionic condensate is a superfluid phase formed by fermionic particles at low temperatures. It is closely related to the Bose–Einstein condensate, a superfluid phase formed by bosonic atoms under similar conditions. The earliest recognized fermionic condensate described the state of electrons in a superconductor; the physics of other examples including recent work with fermionic atoms is analogous. The first atomic fermionic condensate was created by a team led by Deborah S. Jin in 2003.\n\nFermionic condensates are attained at temperatures lower than Bose–Einstein condensates. Fermionic condensates are a type of superfluid. As the name suggests, a superfluid possesses fluid properties similar to those possessed by ordinary liquids and gases, such as the lack of a definite shape and the ability to flow in response to applied forces. However, superfluids possess some properties that do not appear in ordinary matter. For instance, they can flow at high velocities without dissipating any energy—i.e. zero viscosity. At lower velocities, energy is dissipated by the formation of quantized vortices, which act as \"holes\" in the medium where superfluidity breaks down.\n\nSuperfluidity was originally discovered in solid helium-4, in 1938, by Pyotr Kapitsa, John Allen and Don Misener. Superfluidity in helium-4, which occurs at temperatures below 2.17 kelvins (K), has long been understood to result from Bose condensation, the same mechanism that produces the Bose–Einstein condensates. The primary difference between superfluid helium and a Bose–Einstein condensate is that the former is condensed from a liquid while the latter is condensed from a gas.\n\nIt is far more difficult to produce a fermionic superfluid than a bosonic one, because the Pauli exclusion principle prohibits fermions from occupying the same quantum state. However, there is a well-known mechanism by which a superfluid may be formed from fermions. This is the BCS transition, discovered in 1957 by John Bardeen, Leon Cooper and Robert Schrieffer for describing superconductivity. These authors showed that, below a certain temperature, electrons (which are fermions) can pair up to form bound pairs now known as Cooper pairs. As long as collisions with the ionic lattice of the solid do not supply enough energy to break the Cooper pairs, the electron fluid will be able to flow without dissipation. As a result, it becomes a superfluid, and the material through which it flows a superconductor.\n\nThe BCS theory was phenomenally successful in describing superconductors. Soon after the publication of the BCS paper, several theorists proposed that a similar phenomenon could occur in fluids made up of fermions other than electrons, such as helium-3 atoms. These speculations were confirmed in 1971, when experiments performed by Douglas D. Osheroff showed that helium-3 becomes a superfluid below 0.0025 K. It was soon verified that the superfluidity of helium-3 arises from a BCS-like mechanism. (The theory of superfluid helium-3 is a little more complicated than the BCS theory of superconductivity. These complications arise because helium atoms repel each other much more strongly than electrons, but the basic idea is the same.)\n\nWhen Eric Cornell and Carl Wieman produced a Bose–Einstein condensate from rubidium atoms in 1995, there naturally arose the prospect of creating a similar sort of condensate made from fermionic atoms, which would form a superfluid by the BCS mechanism. However, early calculations indicated that the temperature required for producing Cooper pairing in atoms would be too cold to achieve. In 2001, Murray Holland at JILA suggested a way of bypassing this difficulty. He speculated that fermionic atoms could be coaxed into pairing up by subjecting them to a strong magnetic field.\n\nIn 2003, working on Holland's suggestion, Deborah Jin at JILA, Rudolf Grimm at the University of Innsbruck, and Wolfgang Ketterle at MIT managed to coax fermionic atoms into forming molecular bosons, which then underwent Bose–Einstein condensation. However, this was not a true fermionic condensate. On December 16, 2003, Jin managed to produce a condensate out of fermionic atoms for the first time. The experiment involved 500,000 potassium-40 atoms cooled to a temperature of 5×10 K, subjected to a time-varying magnetic field.\n\nA chiral condensate is an example of a fermionic condensate that appears in theories of massless fermions with chiral symmetry breaking.\n\nThe BCS theory of superconductivity has a fermion condensate. A pair of electrons in a metal with opposite spins can form a scalar bound state called a Cooper pair. Then, the bound states themselves form a condensate. Since the Cooper pair has electric charge, this fermion condensate breaks the electromagnetic gauge symmetry of a superconductor, giving rise to the wonderful electromagnetic properties of such states.\n\nIn quantum chromodynamics (QCD) the chiral condensate is also called the quark condensate. This property of the QCD vacuum is partly responsible for giving masses to hadrons (along with other condensates like the gluon condensate).\n\nIn an approximate version of QCD, which has vanishing quark masses for \"N\" quark flavours, there is an exact chiral symmetry of the theory. The QCD vacuum breaks this symmetry to SU(\"N\") by forming a quark condensate. The existence of such a fermion condensate was first shown explicitly in the lattice formulation of QCD. The quark condensate is therefore an order parameter of transitions between several phases of quark matter in this limit.\n\nThis is very similar to the BCS theory of superconductivity. The Cooper pairs are analogous to the pseudoscalar mesons. However, the vacuum carries no charge. Hence all the gauge symmetries are unbroken. Corrections for the masses of the quarks can be incorporated using chiral perturbation theory.\n\nA helium-3 atom is a fermion and at very low temperatures, they form two-atom Cooper pairs which are bosonic and condense into a superfluid. These Cooper pairs are substantially larger than the interatomic separation.\n\n\n"}
{"id": "11729", "url": "https://en.wikipedia.org/wiki?curid=11729", "title": "Fuel cell", "text": "Fuel cell\n\nA fuel cell is an electrochemical cell that converts the chemical energy from a fuel into electricity through an electrochemical reaction of hydrogen fuel with oxygen or another oxidizing agent. Fuel cells are different from batteries in requiring a continuous source of fuel and oxygen (usually from air) to sustain the chemical reaction, whereas in a battery the chemical energy comes from chemicals already present in the battery. Fuel cells can produce electricity continuously for as long as fuel and oxygen are supplied.\n\nThe first fuel cells were invented in 1838. The first commercial use of fuel cells came more than a century later in NASA space programs to generate power for satellites and space capsules. Since then, fuel cells have been used in many other applications. Fuel cells are used for primary and backup power for commercial, industrial and residential buildings and in remote or inaccessible areas. They are also used to power fuel cell vehicles, including forklifts, automobiles, buses, boats, motorcycles and submarines.\n\nThere are many types of fuel cells, but they all consist of an anode, a cathode, and an electrolyte that allows positively charged hydrogen ions (protons) to move between the two sides of the fuel cell. At the anode a catalyst causes the fuel to undergo oxidation reactions that generate protons (positively charged hydrogen ions) and electrons. The protons flow from the anode to the cathode through the electrolyte after the reaction. At the same time, electrons are drawn from the anode to the cathode through an external circuit, producing direct current electricity. At the cathode, another catalyst causes hydrogen ions, electrons, and oxygen to react, forming water. Fuel cells are classified by the type of electrolyte they use and by the difference in startup time ranging from 1 second for proton exchange membrane fuel cells (PEM fuel cells, or PEMFC) to 10 minutes for solid oxide fuel cells (SOFC). A related technology is flow batteries, in which the fuel can be regenerated by recharging. Individual fuel cells produce relatively small electrical potentials, about 0.7 volts, so cells are \"stacked\", or placed in series, to create sufficient voltage to meet an application's requirements. In addition to electricity, fuel cells produce water, heat and, depending on the fuel source, very small amounts of nitrogen dioxide and other emissions. The energy efficiency of a fuel cell is generally between 40–60%; however, if waste heat is captured in a cogeneration scheme, efficiencies up to 85% can be obtained.\n\nThe fuel cell market is growing, and in 2013 Pike Research estimated that the stationary fuel cell market will reach 50 GW by 2020.\nThe first references to hydrogen fuel cells appeared in 1838. In a letter dated October 1838 but published in the December 1838 edition of \"The London and Edinburgh Philosophical Magazine and Journal of Science\", Welsh physicist and barrister William Grove wrote about the development of his first crude fuel cells. He used a combination of sheet iron, copper and porcelain plates, and a solution of sulphate of copper and dilute acid. In a letter to the same publication written in December 1838 but published in June 1839, German physicist Christian Friedrich Schönbein discussed the first crude fuel cell that he had invented. His letter discussed current generated from hydrogen and oxygen dissolved in water. Grove later sketched his design, in 1842, in the same journal. The fuel cell he made used similar materials to today's phosphoric-acid fuel cell.\n\nIn 1939, British engineer Francis Thomas Bacon successfully developed a 5 kW stationary fuel cell. In 1955, W. Thomas Grubb, a chemist working for the General Electric Company (GE), further modified the original fuel cell design by using a sulphonated polystyrene ion-exchange membrane as the electrolyte. Three years later another GE chemist, Leonard Niedrach, devised a way of depositing platinum onto the membrane, which served as catalyst for the necessary hydrogen oxidation and oxygen reduction reactions. This became known as the \"Grubb-Niedrach fuel cell\". GE went on to develop this technology with NASA and McDonnell Aircraft, leading to its use during Project Gemini. This was the first commercial use of a fuel cell. In 1959, a team led by Harry Ihrig built a 15 kW fuel cell tractor for Allis-Chalmers, which was demonstrated across the U.S. at state fairs. This system used potassium hydroxide as the electrolyte and compressed hydrogen and oxygen as the reactants. Later in 1959, Bacon and his colleagues demonstrated a practical five-kilowatt unit capable of powering a welding machine. In the 1960s, Pratt and Whitney licensed Bacon's U.S. patents for use in the U.S. space program to supply electricity and drinking water (hydrogen and oxygen being readily available from the spacecraft tanks). In 1991, the first hydrogen fuel cell automobile was developed by Roger Billings.\n\nUTC Power was the first company to manufacture and commercialize a large, stationary fuel cell system for use as a co-generation power plant in hospitals, universities and large office buildings.\n\nIn recognition of the fuel cell industry and America’s role in fuel cell development, the US Senate recognized 8 October 2015 as National Hydrogen and Fuel Cell Day, passing S. RES 217. The date was chosen in recognition of the atomic weight of hydrogen (1.008).\n\nFuel cells come in many varieties; however, they all work in the same general manner. They are made up of three adjacent segments: the anode, the electrolyte, and the cathode. Two chemical reactions occur at the interfaces of the three different segments. The net result of the two reactions is that fuel is consumed, water or carbon dioxide is created, and an electric current is created, which can be used to power electrical devices, normally referred to as the load.\n\nAt the anode a catalyst oxidizes the fuel, usually hydrogen, turning the fuel into a positively charged ion and a negatively charged electron. The electrolyte is a substance specifically designed so ions can pass through it, but the electrons cannot. The freed electrons travel through a wire creating the electric current. The ions travel through the electrolyte to the cathode. Once reaching the cathode, the ions are reunited with the electrons and the two react with a third chemical, usually oxygen, to create water or carbon dioxide.\nDesign features in a fuel cell include:\n\nA typical fuel cell produces a voltage from 0.6 V to 0.7 V at full rated load. Voltage decreases as current increases, due to several factors:\n\nTo deliver the desired amount of energy, the fuel cells can be combined in series to yield higher voltage, and in parallel to allow a higher current to be supplied. Such a design is called a \"fuel cell stack\". The cell surface area can also be increased, to allow higher current from each cell. Within the stack, reactant gases must be distributed uniformly over each of the cells to maximize the power output.\n\nIn the archetypical hydrogen–oxide proton exchange membrane fuel cell design, a proton-conducting polymer membrane (typically nafion) contains the electrolyte solution that separates the anode and cathode sides. This was called a \"solid polymer electrolyte fuel cell\" (SPEFC) in the early 1970s, before the proton exchange mechanism was well understood. (Notice that the synonyms \"polymer electrolyte membrane\" and \"proton exchange mechanism\" result in the same acronym.)\n\nOn the anode side, hydrogen diffuses to the anode catalyst where it later dissociates into protons and electrons. These protons often react with oxidants causing them to become what are commonly referred to as multi-facilitated proton membranes. The protons are conducted through the membrane to the cathode, but the electrons are forced to travel in an external circuit (supplying power) because the membrane is electrically insulating. On the cathode catalyst, oxygen molecules react with the electrons (which have traveled through the external circuit) and protons to form water.\n\nIn addition to this pure hydrogen type, there are hydrocarbon fuels for fuel cells, including diesel, methanol (\"see:\" direct-methanol fuel cells and indirect methanol fuel cells) and chemical hydrides. The waste products with these types of fuel are carbon dioxide and water. When hydrogen is used, the CO2 is released when methane from natural gas is combined with steam, in a process called steam methane reforming, to produce the hydrogen. This can take place in a different location to the fuel cell, potentially allowing the hydrogen fuel cell to be used indoors—for example, in fork lifts.\n\nThe different components of a PEMFC are\n\nThe materials used for different parts of the fuel cells differ by type. The bipolar plates may be made of different types of materials, such as, metal, coated metal, graphite, flexible graphite, C–C composite, carbon–polymer composites etc. The membrane electrode assembly (MEA) is referred as the heart of the PEMFC and is usually made of a proton exchange membrane sandwiched between two catalyst-coated carbon papers. Platinum and/or similar type of noble metals are usually used as the catalyst for PEMFC. The electrolyte could be a polymer membrane.\n\n\nPhosphoric acid fuel cells (PAFC) were first designed and introduced in 1961 by G. V. Elmore and H. A. Tanner. In these cells phosphoric acid is used as a non-conductive electrolyte to pass positive hydrogen ions from the anode to the cathode. These cells commonly work in temperatures of 150 to 200 degrees Celsius. This high temperature will cause heat and energy loss if the heat is not removed and used properly. This heat can be used to produce steam for air conditioning systems or any other thermal energy consuming system. Using this heat in cogeneration can enhance the efficiency of phosphoric acid fuel cells from 40–50% to about 80%. Phosphoric acid, the electrolyte used in PAFCs, is a non-conductive liquid acid which forces electrons to travel from anode to cathode through an external electrical circuit. Since the hydrogen ion production rate on the anode is small, platinum is used as catalyst to increase this ionization rate. A key disadvantage of these cells is the use of an acidic electrolyte. This increases the corrosion or oxidation of components exposed to phosphoric acid.\n\nSolid acid fuel cells (SAFCs) are characterized by the use of a solid acid material as the electrolyte. At low temperatures, solid acids have an ordered molecular structure like most salts. At warmer temperatures (between 140 and 150 degrees Celsius for CsHSO), some solid acids undergo a phase transition to become highly disordered \"superprotonic\" structures, which increases conductivity by several orders of magnitude. The first proof-of-concept SAFCs were developed in 2000 using cesium hydrogen sulfate (CsHSO). Current SAFC systems use cesium dihydrogen phosphate (CsHPO) and have demonstrated lifetimes in the thousands of hours.\n\nThe alkaline fuel cell or hydrogen-oxygen fuel cell was designed and first demonstrated publicly by Francis Thomas Bacon in 1959. It was used as a primary source of electrical energy in the Apollo space program. The cell consists of two porous carbon electrodes impregnated with a suitable catalyst such as Pt, Ag, CoO, etc. The space between the two electrodes is filled with a concentrated solution of KOH or NaOH which serves as an electrolyte. H gas and O gas are bubbled into the electrolyte through the porous carbon electrodes. Thus the overall reaction involves the combination of hydrogen gas and oxygen gas to form water. The cell runs continuously until the reactant's supply is exhausted. This type of cell operates efficiently in the temperature range 343 K to 413 K and provides a potential of about 0.9 V. AAEMFC is a type of AFC which employs a solid polymer electrolyte instead of aqueous potassium hydroxide (KOH) and it is superior to aqueous AFC.\n\nSolid oxide fuel cells (SOFCs) use a solid material, most commonly a ceramic material called yttria-stabilized zirconia (YSZ), as the electrolyte. Because SOFCs are made entirely of solid materials, they are not limited to the flat plane configuration of other types of fuel cells and are often designed as rolled tubes. They require high operating temperatures (800–1000 °C) and can be run on a variety of fuels including natural gas.\n\nSOFCs are unique since in those, negatively charged oxygen ions travel from the cathode (positive side of the fuel cell) to the anode (negative side of the fuel cell) instead of positively charged hydrogen ions travelling from the anode to the cathode, as is the case in all other types of fuel cells. Oxygen gas is fed through the cathode, where it absorbs electrons to create oxygen ions. The oxygen ions then travel through the electrolyte to react with hydrogen gas at the anode. The reaction at the anode produces electricity and water as by-products. Carbon dioxide may also be a by-product depending on the fuel, but the carbon emissions from an SOFC system are less than those from a fossil fuel combustion plant. The chemical reactions for the SOFC system can be expressed as follows:\n\nSOFC systems can run on fuels other than pure hydrogen gas. However, since hydrogen is necessary for the reactions listed above, the fuel selected must contain hydrogen atoms. For the fuel cell to operate, the fuel must be converted into pure hydrogen gas. SOFCs are capable of internally reforming light hydrocarbons such as methane (natural gas), propane and butane. These fuel cells are at an early stage of development.\n\nChallenges exist in SOFC systems due to their high operating temperatures. One such challenge is the potential for carbon dust to build up on the anode, which slows down the internal reforming process. Research to address this \"carbon coking\" issue at the University of Pennsylvania has shown that the use of copper-based cermet (heat-resistant materials made of ceramic and metal) can reduce coking and the loss of performance. Another disadvantage of SOFC systems is slow start-up time, making SOFCs less useful for mobile applications. Despite these disadvantages, a high operating temperature provides an advantage by removing the need for a precious metal catalyst like platinum, thereby reducing cost. Additionally, waste heat from SOFC systems may be captured and reused, increasing the theoretical overall efficiency to as high as 80%–85%.\n\nThe high operating temperature is largely due to the physical properties of the YSZ electrolyte. As temperature decreases, so does the ionic conductivity of YSZ. Therefore, to obtain optimum performance of the fuel cell, a high operating temperature is required. According to their website, Ceres Power, a UK SOFC fuel cell manufacturer, has developed a method of reducing the operating temperature of their SOFC system to 500–600 degrees Celsius. They replaced the commonly used YSZ electrolyte with a CGO (cerium gadolinium oxide) electrolyte. The lower operating temperature allows them to use stainless steel instead of ceramic as the cell substrate, which reduces cost and start-up time of the system.\n\nMolten carbonate fuel cells (MCFCs) require a high operating temperature, , similar to SOFCs. MCFCs use lithium potassium carbonate salt as an electrolyte, and this salt liquefies at high temperatures, allowing for the movement of charge within the cell – in this case, negative carbonate ions.\n\nLike SOFCs, MCFCs are capable of converting fossil fuel to a hydrogen-rich gas in the anode, eliminating the need to produce hydrogen externally. The reforming process creates emissions. MCFC-compatible fuels include natural gas, biogas and gas produced from coal. The hydrogen in the gas reacts with carbonate ions from the electrolyte to produce water, carbon dioxide, electrons and small amounts of other chemicals. The electrons travel through an external circuit creating electricity and return to the cathode. There, oxygen from the air and carbon dioxide recycled from the anode react with the electrons to form carbonate ions that replenish the electrolyte, completing the circuit. The chemical reactions for an MCFC system can be expressed as follows:\n\nAs with SOFCs, MCFC disadvantages include slow start-up times because of their high operating temperature. This makes MCFC systems not suitable for mobile applications, and this technology will most likely be used for stationary fuel cell purposes. The main challenge of MCFC technology is the cells' short life span. The high-temperature and carbonate electrolyte lead to corrosion of the anode and cathode. These factors accelerate the degradation of MCFC components, decreasing the durability and cell life. Researchers are addressing this problem by exploring corrosion-resistant materials for components as well as fuel cell designs that may increase cell life without decreasing performance.\n\nMCFCs hold several advantages over other fuel cell technologies, including their resistance to impurities. They are not prone to \"carbon coking\", which refers to carbon build-up on the anode that results in reduced performance by slowing down the internal fuel reforming process. Therefore, carbon-rich fuels like gases made from coal are compatible with the system. The Department of Energy claims that coal, itself, might even be a fuel option in the future, assuming the system can be made resistant to impurities such as sulfur and particulates that result from converting coal into hydrogen. MCFCs also have relatively high efficiencies. They can reach a fuel-to-electricity efficiency of 50%, considerably higher than the 37–42% efficiency of a phosphoric acid fuel cell plant. Efficiencies can be as high as 65% when the fuel cell is paired with a turbine, and 85% if heat is captured and used in a Combined Heat and Power (CHP) system.\n\nFuelCell Energy, a Connecticut-based fuel cell manufacturer, develops and sells MCFC fuel cells. The company says that their MCFC products range from 300 kW to 2.8 MW systems that achieve 47% electrical efficiency and can utilize CHP technology to obtain higher overall efficiencies. One product, the DFC-ERG, is combined with a gas turbine and, according to the company, it achieves an electrical efficiency of 65%.\n\nThe electric storage fuel cell is a conventional battery chargeable by electric power input, using the conventional electro-chemical effect. However, the battery further includes hydrogen (and oxygen) inputs for alternatively charging the battery chemically.\n\n\"Glossary of Terms in table:\"\n\n\"For more information see Glossary of fuel cell terms\"\n\nThe energy efficiency of a system or device that converts energy is measured by the ratio of the amount of useful energy put out by the system (\"output energy\") to the total amount of energy that is put in (\"input energy\") or by useful output energy as a percentage of the total input energy. In the case of fuel cells, useful output energy is measured in electrical energy produced by the system. Input energy is the energy stored in the fuel. According to the U.S. Department of Energy, fuel cells are generally between 40–60% energy efficient. This is higher than some other systems for energy generation. For example, the typical internal combustion engine of a car is about 25% energy efficient. In combined heat and power (CHP) systems, the heat produced by the fuel cell is captured and put to use, increasing the efficiency of the system to up to 85–90%.\n\nThe theoretical maximum efficiency of any type of power generation system is never reached in practice, and it does not consider other steps in power generation, such as production, transportation and storage of fuel and conversion of the electricity into mechanical power. However, this calculation allows the comparison of different types of power generation. The maximum theoretical energy efficiency of a fuel cell is 83%, operating at low power density and using pure hydrogen and oxygen as reactants (assuming no heat recapture) According to the World Energy Council, this compares with a maximum theoretical efficiency of 58% for internal combustion engines.\n\nIn a fuel-cell vehicle the tank-to-wheel efficiency is greater than 45% at low loads and shows average values of about 36% when a driving cycle like the NEDC (New European Driving Cycle) is used as test procedure. The comparable NEDC value for a Diesel vehicle is 22%. In 2008 Honda released a demonstration fuel cell electric vehicle (the Honda FCX Clarity) with fuel stack claiming a 60% tank-to-wheel efficiency.\n\nIt is also important to take losses due to fuel production, transportation, and storage into account. Fuel cell vehicles running on compressed hydrogen may have a power-plant-to-wheel efficiency of 22% if the hydrogen is stored as high-pressure gas, and 17% if it is stored as liquid hydrogen. Fuel cells cannot store energy like a battery, except as hydrogen, but in some applications, such as stand-alone power plants based on discontinuous sources such as solar or wind power, they are combined with electrolyzers and storage systems to form an energy storage system. Most hydrogen is used for oil refining, chemicals and fertilizer production and therefore produced by steam methane reforming, which emits carbon dioxide. The overall efficiency (electricity to hydrogen and back to electricity) of such plants (known as \"round-trip efficiency\"), using pure hydrogen and pure oxygen can be \"from 35 up to 50 percent\", depending on gas density and other conditions.. The electrolyzer/fuel cell system can store indefinite quantities of hydrogen, and is therefore suited for long-term storage.\n\nSolid-oxide fuel cells produce heat from the recombination of the oxygen and hydrogen. The ceramic can run as hot as 800 degrees Celsius. This heat can be captured and used to heat water in a micro combined heat and power (m-CHP) application. When the heat is captured, total efficiency can reach 80–90% at the unit, but does not consider production and distribution losses. CHP units are being developed today for the European home market.\n\nProfessor Jeremy P. Meyers, in the Electrochemical Society journal \"Interface\" in 2008, wrote, \"While fuel cells are efficient relative to combustion engines, they are not as efficient as batteries, due primarily to the inefficiency of the oxygen reduction reaction (and ... the oxygen evolution reaction, should the hydrogen be formed by electrolysis of water)... [T]hey make the most sense for operation disconnected from the grid, or when fuel can be provided continuously. For applications that require frequent and relatively rapid start-ups ... where zero emissions are a requirement, as in enclosed spaces such as warehouses, and where hydrogen is considered an acceptable reactant, a [PEM fuel cell] is becoming an increasingly attractive choice [if exchanging batteries is inconvenient]\". In 2013 military organizations are evaluating fuel cells to significantly reduce the battery weight carried by soldiers.\n\nStationary fuel cells are used for commercial, industrial and residential primary and backup power generation. Fuel cells are very useful as power sources in remote locations, such as spacecraft, remote weather stations, large parks, communications centers, rural locations including research stations, and in certain military applications. A fuel cell system running on hydrogen can be compact and lightweight, and have no major moving parts. Because fuel cells have no moving parts and do not involve combustion, in ideal conditions they can achieve up to 99.9999% reliability. This equates to less than one minute of downtime in a six-year period.\n\nSince fuel cell electrolyzer systems do not store fuel in themselves, but rather rely on external storage units, they can be successfully applied in large-scale energy storage, rural areas being one example. There are many different types of stationary fuel cells so efficiencies vary, but most are between 40% and 60% energy efficient. However, when the fuel cell's waste heat is used to heat a building in a cogeneration system this efficiency can increase to 85%. This is significantly more efficient than traditional coal power plants, which are only about one third energy efficient. Assuming production at scale, fuel cells could save 20–40% on energy costs when used in cogeneration systems. Fuel cells are also much cleaner than traditional power generation; a fuel cell power plant using natural gas as a hydrogen source would create less than one ounce of pollution (other than ) for every 1,000 kW·h produced, compared to 25 pounds of pollutants generated by conventional combustion systems. Fuel Cells also produce 97% less nitrogen oxide emissions than conventional coal-fired power plants.\n\nOne such pilot program is operating on Stuart Island in Washington State. There the Stuart Island Energy Initiative has built a complete, closed-loop system: Solar panels power an electrolyzer, which makes hydrogen. The hydrogen is stored in a tank at , and runs a ReliOn fuel cell to provide full electric back-up to the off-the-grid residence. Another closed system loop was unveiled in late 2011 in Hempstead, NY.\n\nFuel cells can be used with low-quality gas from landfills or waste-water treatment plants to generate power and lower methane emissions. A 2.8 MW fuel cell plant in California is said to be the largest of the type.\n\nCombined heat and power (CHP) fuel cell systems, including Micro combined heat and power (MicroCHP) systems are used to generate both electricity and heat for homes (see home fuel cell), office building and factories. The system generates constant electric power (selling excess power back to the grid when it is not consumed), and at the same time produces hot air and water from the waste heat. As the result CHP systems have the potential to save primary energy as they can make use of waste heat which is generally rejected by thermal energy conversion systems. A typical capacity range of home fuel cell is 1–3 kW / 4–8 kW. CHP systems linked to absorption chillers use their waste heat for refrigeration.\n\nThe waste heat from fuel cells can be diverted during the summer directly into the ground providing further cooling while the waste heat during winter can be pumped directly into the building. The University of Minnesota owns the patent rights to this type of system\n\nCo-generation systems can reach 85% efficiency (40–60% electric + remainder as thermal). Phosphoric-acid fuel cells (PAFC) comprise the largest segment of existing CHP products worldwide and can provide combined efficiencies close to 90%. Molten Carbonate (MCFC) and Solid Oxide Fuel Cells (SOFC) are also used for combined heat and power generation and have electrical energy efficiences around 60%. Disadvantages of co-generation systems include slow ramping up and down rates, high cost and short lifetime. Also their need to have a hot water storage tank to smooth out the thermal heat production was a serious disadvantage in the domestic market place where space in domestic properties is at a great premium.\n\nDelta-ee consultants stated in 2013 that with 64% of global sales the fuel cell micro-combined heat and power passed the conventional systems in sales in 2012. The Japanese ENE FARM project will pass 100,000 FC mCHP systems in 2014, 34.213 PEMFC and 2.224 SOFC were installed in the period 2012-2014, 30,000 units on LNG and 6,000 on LPG.\n\nAs of 2017, about 6500 FCEVs have been leased or sold worldwide. Three fuel cell electric vehicles have been introduced for commercial lease and sale: the Honda Clarity, Toyota Mirai and the Hyundai ix35 FCEV. Additional demonstration models include the Honda FCX Clarity, and Mercedes-Benz F-Cell. As of June 2011 demonstration FCEVs had driven more than , with more than 27,000 refuelings. Fuel cell electric vehicles feature an average range of 314 miles between refuelings. They can be refueled in less than 5 minutes. The U.S. Department of Energy's Fuel Cell Technology Program states that, as of 2011, fuel cells achieved 53–59% efficiency at one-quarter power and 42–53% vehicle efficiency at full power, and a durability of over with less than 10% degradation. In a Well-to-Wheels simulation analysis that \"did not address the economics and market constraints\", General Motors and its partners estimated that per mile traveled, a fuel cell electric vehicle running on compressed gaseous hydrogen produced from natural gas could use about 40% less energy and emit 45% less greenhouse gasses than an internal combustion vehicle. A lead engineer from the Department of Energy whose team is testing fuel cell cars said in 2011 that the potential appeal is that \"these are full-function vehicles with no limitations on range or refueling rate so they are a direct replacement for any vehicle. For instance, if you drive a full sized SUV and pull a boat up into the mountains, you can do that with this technology and you can't with current battery-only vehicles, which are more geared toward city driving.\"\n\nIn 2015, Toyota introduced its first fuel cell vehicle, the Mirai, at a price of $57,000. Hyundai introduced the limited production Hyundai ix35 FCEV under a lease agreement.. In 2016, Honda started leasing the Honda Clarity Fuel Cell.\n\nSome experts believe that hydrogen fuel cell cars will never become economically competitive with other technologies or that it will take decades for them to become profitable. Elon Musk stated in 2015 that fuel cells for use in cars will never be commercially viable because of the inefficiency of producing, transporting and storing hydrogen and the flammability of the gas, among other reasons. Professor Jeremy P. Meyers estimated in 2008 that cost reductions over a production ramp-up period will take about 20 years after fuel-cell cars are introduced before they will be able to compete commercially with current market technologies, including gasoline internal combustion engines. In 2011, the chairman and CEO of General Motors, Daniel Akerson, stated that while the cost of hydrogen fuel cell cars is decreasing: \"The car is still too expensive and probably won't be practical until the 2020-plus period, I don't know.\"\n\nIn 2012, Lux Research, Inc. issued a report that stated: \"The dream of a hydrogen economy ... is no nearer\". It concluded that \"Capital cost ... will limit adoption to a mere 5.9 GW\" by 2030, providing \"a nearly insurmountable barrier to adoption, except in niche applications\". The analysis concluded that, by 2030, PEM stationary market will reach $1 billion, while the vehicle market, including forklifts, will reach a total of $2 billion. Other analyses cite the lack of an extensive hydrogen infrastructure in the U.S. as an ongoing challenge to Fuel Cell Electric Vehicle commercialization. In 2006, a study for the IEEE showed that for hydrogen produced via electrolysis of water: \"Only about 25% of the power generated from wind, water, or sun is converted to practical use.\" The study further noted that \"Electricity obtained from hydrogen fuel cells appears to be four times as expensive as electricity drawn from the electrical transmission grid. ... Because of the high energy losses [hydrogen] cannot compete with electricity.\" Furthermore, the study found: \"Natural gas reforming is not a sustainable solution\". \"The large amount of energy required to isolate hydrogen from natural compounds (water, natural gas, biomass), package the light gas by compression or liquefaction, transfer the energy carrier to the user, plus the energy lost when it is converted to useful electricity with fuel cells, leaves around 25% for practical use.\"\n\nIn 2014, Joseph Romm, the author of \"The Hype About Hydrogen\" (2005), stated that FCVs still had not overcome the high fueling cost, lack of fuel-delivery infrastructure, and pollution caused by producing hydrogen. \"It would take several miracles to overcome all of those problems simultaneously in the coming decades.\" He concluded that renewable energy cannot economically be used to make hydrogen for an FCV fleet \"either now or in the future.\" Greentech Media's analyst reached similar conclusions in 2014. In 2015, \"Clean Technica\" listed some of the disadvantages of hydrogen fuel cell vehicles. So did \"Car Throttle\".\n\n, there were a total of approximately 100 fuel cell buses deployed around the world. Most buses are produced by UTC Power, Toyota, Ballard, Hydrogenics, and Proton Motor. UTC buses had accumulated over of driving by 2011. Fuel cell buses have a 39–141% higher fuel economy than diesel buses and natural gas buses. Fuel cell buses have been deployed around the world including in Whistler, Canada; San Francisco, United States; Hamburg, Germany; Shanghai, China; London, England; and São Paulo, Brazil.\n\nThe Fuel Cell Bus Club is a global cooperative effort in trial fuel cell buses. Notable projects include:\n\nThe first Brazilian hydrogen fuel cell bus prototype in Brazil was deployed in São Paulo. The bus was manufactured in Caxias do Sul and the hydrogen fuel will be produced in São Bernardo do Campo from water through electrolysis. The program, called \"Ônibus Brasileiro a Hidrogênio\" (Brazilian Hydrogen Autobus), includes three additional buses.\n\nA fuel cell forklift (also called a fuel cell lift truck) is a fuel cell-powered industrial forklift truck used to lift and transport materials. In 2013 there were over 4,000 fuel cell forklifts used in material handling in the US, of which only 500 received funding from DOE (2012). The global market is 1 million fork lifts per year. Fuel cell fleets are operated by various companies, including Sysco Foods, FedEx Freight, GENCO (at Wegmans, Coca-Cola, Kimberly Clark, and Whole Foods), and H-E-B Grocers. Europe demonstrated 30 fuel cell forklifts with Hylift and extended it with HyLIFT-EUROPE to 200 units, with other projects in France and Austria. Pike Research stated in 2011 that fuel cell-powered forklifts will be the largest driver of hydrogen fuel demand by 2020.\n\nMost companies in Europe and the US do not use petroleum-powered forklifts, as these vehicles work indoors where emissions must be controlled and instead use electric forklifts. Fuel cell-powered forklifts can provide benefits over battery-powered forklifts as they can work for a full 8-hour shift on a single tank of hydrogen and can be refueled in 3 minutes. Fuel cell-powered forklifts can be used in refrigerated warehouses, as their performance is not degraded by lower temperatures. The FC units are often designed as drop-in replacements.\n\nIn 2005 a British manufacturer of hydrogen-powered fuel cells, Intelligent Energy (IE), produced the first working hydrogen run motorcycle called the ENV (Emission Neutral Vehicle). The motorcycle holds enough fuel to run for four hours, and to travel in an urban area, at a top speed of . In 2004 Honda developed a fuel-cell motorcycle that utilized the Honda FC Stack.\n\nOther examples of motorbikes and bicycles that use hydrogen fuel cells include the Taiwanese company APFCT's scooter using the fueling system from Italy's Acta SpA and the Suzuki Burgman scooter with an IE fuel cell that received EU Whole Vehicle Type Approval in 2011. Suzuki Motor Corp. and IE have announced a joint venture to accelerate the commercialization of zero-emission vehicles.\n\nIn 2003, the world's first propeller-driven airplane to be powered entirely by a fuel cell was flown. The fuel cell was a stack design that allowed the fuel cell to be integrated with the plane's aerodynamic surfaces. Fuel cell-powered unmanned aerial vehicles (UAV) include a Horizon fuel cell UAV that set the record distance flown for a small UAV in 2007. Boeing researchers and industry partners throughout Europe conducted experimental flight tests in February 2008 of a manned airplane powered only by a fuel cell and lightweight batteries. The fuel cell demonstrator airplane, as it was called, used a proton exchange membrane (PEM) fuel cell/lithium-ion battery hybrid system to power an electric motor, which was coupled to a conventional propeller. \n\nIn 2009 the Naval Research Laboratory's (NRL's) Ion Tiger utilized a hydrogen-powered fuel cell and flew for 23 hours and 17 minutes. Fuel cells are also being tested and considered to provide auxiliary power in aircraft, replacing fossil fuel generators that were previously used to start the engines and power on board electrical needs, while reducing carbon emissions.\n\nIn 2016 a Raptor E1 drone made a successful test flight using a fuel cell that was lighter than the lithium-ion battery it replaced. The flight lasted 10 minutes at an altitude of , although the fuel cell reportedly had enough fuel to fly for two hours. The fuel was contained in approximately 100 solid pellets composed of a proprietary chemical within an unpressurized cartridge. The pellets are physically robust and operate at temperatures as warm as . The cell was from Arcola Energy.\n\nThe world's first fuel-cell boat HYDRA used an AFC system with 6.5 kW net output. Iceland has committed to converting its vast fishing fleet to use fuel cells to provide auxiliary power by 2015 and, eventually, to provide primary power in its boats. Amsterdam recently introduced its first fuel cell-powered boat that ferries people around the city's canals.\n\nThe Type 212 submarines of the German and Italian navies use fuel cells to remain submerged for weeks without the need to surface.\n\nThe U212A is a non-nuclear submarine developed by German naval shipyard Howaldtswerke Deutsche Werft. The system consists of nine PEM fuel cells, providing between 30 kW and 50 kW each. The ship is silent, giving it an advantage in the detection of other submarines. A naval paper has theorized about the possibility of a nuclear-fuel cell hybrid whereby the fuel cell is used when silent operations are required and then replenished from the Nuclear reactor (and water).\n\nPortable fuel cell systems are generally classified as weighing under 10 kg and providing power of less than 5 kW. The potential market size for smaller fuel cells is quite large with an up to 40% per annum potential growth rate and a market size of around $10 billion, leading a great deal of research to be devoted to the development of portable power cells. Within this market two groups have been identified. The first is the microfuel cell market, in the 1-50 W range for power smaller electronic devices. The second is the 1-5 kW range of generators for larger scale power generation (e.g. military outposts, remote oil fields).\n\nMicrofuel cells are primarily aimed at penetrating the market for phones and laptops. This can be primarily attributed to the advantageous energy density provided by fuel cells over a lithium-ion battery, for the entire system. For a battery, this system includes the charger as well as the battery itself. For the fuel cell this system would include the cell, the necessary fuel and peripheral attachments. Taking the full system into consideration, fuel cells have been shown to provide 530Wh/kg compared to 44 Wh/kg for lithium ion batteries. However, while the weight of fuel cell systems offer a distinct advantage the current costs are not in their favor. while a battery system will generally cost around $1.20 per Wh, fuel cell systems cost around $5 per Wh, putting them at a significant disadvantage.\n\nAs power demands for cell phones increase, fuel cells could become much more attractive options for larger power generation. The demand for longer on time on phones and computers is something often demanded by consumers so fuel cells could start to make strides into laptop and cell phone markets. The price will continue to go down as developments in fuel cells continues to accelerate. Current strategies for improving micro fuelcells is through the use of carbon nanotubes. It was shown by Girishkumar et al. that depositing nanotubes on electrode surfaces allows for substantially greater surface area increasing the oxygen reduction rate.\n\nFuel cells for use in larger scale operations also show much promise. Portable power systems that use fuel cells can be used in the leisure sector (i.e. RVs, cabins, marine), the industrial sector (i.e. power for remote locations including gas/oil wellsites, communication towers, security, weather stations), and in the military sector. SFC Energy is a German manufacturer of direct methanol fuel cells for a variety of portable power systems. Ensol Systems Inc. is an integrator of portable power systems, using the SFC Energy DMFC. The key advantage of fuel cells in this market is the great power generation per weight. While fuel cells can be expensive, for remote locations that require dependable energy fuel cells hold great power. For a 72-h excursion the comparison in weight is substantial, with a fuel cell only weighing 15 pounds compared to 29 pounds of batteries needed for the same energy.\n\n\nIn 2013, \"The New York Times\" reported that there were \"10 hydrogen stations available to the public in the entire United States: one in Columbia, S.C., eight in Southern California and the one in Emeryville\". , there were 31 publicly accessible hydrogen refueling stations in the US, 28 of which were located in California.\n\nA public hydrogen refueling station in Iceland operated from 2003 to 2007. It served three buses in the public transport net of Reykjavík. The station produced its own hydrogen with an electrolyzing unit. The 14 stations in Germany were planned to be expanded to 50 by 2015 through its public–private partnership Now GMBH.\n\nBy May 2017, there were 91 hydrogen fueling stations in Japan. As of 2016, Norway planned to build a network of hydrogen stations between the major cities, starting in 2017.\n\nIn 2012, fuel cell industry revenues exceeded $1 billion market value worldwide, with Asian pacific countries shipping more than 3/4 of the fuel cell systems worldwide. However, as of January 2014, no public company in the industry had yet become profitable. There were 140,000 fuel cell stacks shipped globally in 2010, up from 11,000 shipments in 2007, and from 2011 to 2012 worldwide fuel cell shipments had an annual growth rate of 85%. Tanaka Kikinzoku expanded its manufacturing facilities in 2011. Approximately 50% of fuel cell shipments in 2010 were stationary fuel cells, up from about a third in 2009, and the four dominant producers in the Fuel Cell Industry were the United States, Germany, Japan and South Korea. The Department of Energy Solid State Energy Conversion Alliance found that, as of January 2011, stationary fuel cells generated power at approximately $724 to $775 per kilowatt installed. In 2011, Bloom Energy, a major fuel cell supplier, said that its fuel cells generated power at 9–11 cents per kilowatt-hour, including the price of fuel, maintenance, and hardware.\n\nIndustry groups predict that there are sufficient platinum resources for future demand, and in 2007, research at Brookhaven National Laboratory suggested that platinum could be replaced by a gold-palladium coating, which may be less susceptible to poisoning and thereby improve fuel cell lifetime. Another method would use iron and sulphur instead of platinum. This would lower the cost of a fuel cell (as the platinum in a regular fuel cell costs around , and the same amount of iron costs only around ). The concept was being developed by a coalition of the John Innes Centre and the University of Milan-Bicocca. PEDOT cathodes are immune to monoxide poisoning.\n\nIn 2016, Samsung \"decided to drop fuel cell-related business projects, as the outlook of the market isn't good\".\n\n\n\n"}
{"id": "52435851", "url": "https://en.wikipedia.org/wiki?curid=52435851", "title": "Gamma Octantis", "text": "Gamma Octantis\n\nThe Bayer designation γ Octantis (gamma Octantis, γ Oct) is shared by three stars, in the constellation Octans, all yellow giants:\n"}
{"id": "11801677", "url": "https://en.wikipedia.org/wiki?curid=11801677", "title": "Geogony", "text": "Geogony\n\nGeogony (or Geogeny) is an antiquated term used to refer to the, at the time, speculative science of the Earth's formation and history. The term was used in \"Elementary Geology\" an early geology book by Edward Hitchcock, eighth edition published in New York City by Mark H. Newman and Company in 1847.\n"}
{"id": "40679614", "url": "https://en.wikipedia.org/wiki?curid=40679614", "title": "Hydroelectricity in Turkey", "text": "Hydroelectricity in Turkey\n\nHydroelectricity is a critical source of energy in Turkey and substantial amounts of it can be generated due to Turkey's mountainous landscape, abundance of rivers, and its position surrounded by three seas. The main river basins are the Euphrates () and the Tigris () rivers. Many dams have been built throughout the country, and a peak of 28GW of power can be generated by hydroelectric plants. There are many projects currently in progress, such as the Southeastern Anatolia Project, which will allow generation of 15GW through hydroelectric energy. There are many policies that support the usage of hydroelectric energy. A number of dams built are subject to controversy as they have had many negative effects on the environment and the wildlife. \n\nThe first hydroelectric plant in Turkey was constructed in Tarsus, Istanbul in 1911 and opened in 1914 as a result of the Balkan Wars. This plant only produced 60kW, but nonetheless it alone was able to provide power to all of Istanbul for almost 40 years.\n\nOver the years, more and more hydroelectric projects were constructed, such as the Seyhan Dam, the Sarıyar Dam, the Hirfanlı Dam, the Kesikköprü Dam, the Demirköprü Dam, and the Kemer Dam.\n\nAfter the State Hydraulic Works (DSI) was established in 1954, projects were better funded and the hydroelectric power produced per year greatly increased.\n\nThe most recent project in Turkey is the GAP (short for Southeastern Anatolia Project, \"Turkish:\" Güneydoğu Anadolu Projesi), the GAP was planned by the DSI, and includes the construction of 22 dams and 19 hydroelectric power plants, the cost of which is 100 billion lira ($32 billion USD, 2017 adjusted price). Most of the project has been completed, with some dams and hydroelectric power plants still under construction. Theoretically, 27 billion kWh/year will be generated from the project.\n\nCurrently 172 dams/hydroelectric power plants are in operation and 148 are under construction.\n\nThere have been both positive and negative environmental effects caused by the dams and hydroelectric power plants. One of the positive effects of hydroelectric power plants has been the decrease of carbon emissions, as the production of hydroelectric energy does not emit any byproduct. Compared to thermal energy, hydroelectric energy is much more environmentally friendly because of emissions. Another positive impact has been the decrease of the importation of energy, since Turkey imports around 90% of its energy.\n\nOn the other hand, the hydroelectric power plants have had a negative impact on the local \"fauna\". A large amount of fish are killed in hydroelectric power plants because there are no fish passages, which can be improved. Nevertheless, some species have already gone extinct.\n\nAnother issue is salinity of the water. The salinity of the water can cause negative effects on water quality, soil erosion, and plant growth. The salinity caused by dams has decreased the agricultural activity near dams and hydroelectric power plants as well. There has also been a great issue with increased bedrock exposure and erosion.\n\nThese projects have also caused an issue of land acquisition and resettlement of people. The Ataturk and Karakaya project of the GAP caused the displacement of 100,000 people. Hundreds of villages have been affected by projects in Turkey to build dams and hydroelectric power plants. Some locals were given land as reparations. On the other hand, some people were admitted back to their own properties after the construction of the dams or power plants, if feasible. However, it cannot be denied that there has been disregard to ancient settlements, such of Assyrian, Greek, Armenian, and many more civilizations, such as the destruction of Hasankeyf.\n\nHydroelectric power is one of the most common sources of renewable energy in the world and it plays a vital role in Turkey’s energy production as well. The theoretical viable hydroelectric potential of the country has been estimated at 433 TWh/year, nearly 1% of the total hydropower potential of the world, and the hydroelectric technical potential of Turkey is 58 TWh/year, also around 1% of the hydroelectric energy produced in the world. This is seen as promising as the estimated demand for electricity in Turkey will be 420 TWh/year by 2020, meaning that if the country utilizes all of its potential hydroelectric power, the supply would surpass demand.\n\nIt might be that in coming years the pumped storage power plant market will also grow or start in Turkey as this could store energy produced by wind and solar power plants.\n\nIn 2015, the total installed capacity of hydropower in Turkey was 25868 MW, and total generated energy in 2015 from hydropower was 66903.2 GWh.\n\n"}
{"id": "50343446", "url": "https://en.wikipedia.org/wiki?curid=50343446", "title": "Hypernuclear Spectroscopy", "text": "Hypernuclear Spectroscopy\n\nHypernuclear spectroscopy is the spectroscopy or measuring the energy spectra of a Hypernucleus, i.e. an atomic nucleus which has a hyperon or a strange quark embedded in it. This is an important field of nuclear physics which helps understand the structure of nuclei and their interactions.\n"}
{"id": "31508335", "url": "https://en.wikipedia.org/wiki?curid=31508335", "title": "In situ chemical reduction", "text": "In situ chemical reduction\n\nIn situ chemical reduction (ISCR) is a new type of environmental remediation technique used for soil and/or groundwater remediation to reduce the concentrations of targeted environmental contaminants to acceptable levels. It is the mirror process of In Situ Chemical Oxidation (ISCO). ISCR is usually applied in the environment by injecting chemically reductive additives in liquid form into the contaminated area or placing a solid medium of chemical reductants in the path of a contaminant plume. It can be used to remediate a variety of organic compounds, including some that are resistant to natural degradation.\n\nThe \"in situ\" in ISCR is just Latin for \"in place\", signifying that ISCR is a chemical reduction reaction that occurs at the site of the contamination. Like ISCO, it is able to decontaminate many compounds, and, in theory, ISCR could be more effective in ground water remediation than ISCO.\n\nChemical reduction is one half of a redox reaction, which results in the gain of electrons. One of the reactants in the reaction becomes oxidized, or loses electrons, while the other reactant becomes reduced, or gains electrons. In ISCR, reducing compounds, compounds that accept electrons given by other compounds in a reaction, are used to change the contaminants into harmless compounds.\n\nISCR is a relatively new type of ground water remediation technology. The most work on this method of remediation has been done in the past 10–15 years, so there are still many gaps in the understanding of the chemistry behind this process. The development of ISCR started out when K.H. Sweeny conducted research with zero-valent copper and iron in the late 1970s. He was able to treat a number of different chlorinated substances such as DDT, endrin, chloroform, and hexachlorocyclopentadiene to name a few. His work has been the basis of ISCR today.\n\nIn the 1990s, Gillham, Tratnyek, Kriegman, Zhang, and Batchelor all made significant contributions in testing different metals and oxides for the use of ISCR. Gillham and Tratnyek in particular applied the reductive chemistry to groundwater treatment with the emplacement of ZVI barriers. Although it has been shown that other metals like aluminum and magnesium can produce the same effect in the laboratory, ground water treatment most generally focuses on the use of iron. Other major contributions in this field includes Zhang, who researched nanoscale iron, and Batchelor, who researched zero-valent iron clay (ZVI Clay). This past decade, more aspects of ISCR have been researched and new methods of implementation, such as ZVI clay and emulsified ZVI (EZVI), have been created. Scientists have also found that certain iron minerals, like green rust, magnetite, and pyrite, also have reductive capabilities although they contain ferrous iron rather than ZVI.\n\nZero Valent Metals are the main reductants used in ISCR. The most common metal used is iron, in the form of ZVI (zero valent iron), and it is also the metal longest in use. However, some studies show that zero valent zinc (ZVZ) could actually be up to ten times more effective at eradicating the contaminants than ZVI. Some applications of ZVMs are to clean up Trichloroethylene (TCE) and Hexavalent chromium (Cr(VI)). ZVMs are usually implemented by a permeable reactive barrier. For example, iron that has been embedded in a swellable, organically modified silica creates a permanent soft barrier underground to capture and reduce small, organic compounds as groundwater passes through it.\n\nThere are also many iron minerals that can actively be used in dechlorination. These minerals use . Particular minerals that can be used include green rust, magnetite, pyrite, and glauconite. The most reactive of the iron minerals are the iron sulfides and oxides. Pyrite, an iron sulfide, is able to dechlorinate carbon tetrachloride in suspension. These substances are very interesting because they are naturally present, and learning about how they produce reductive zones could lead to the development of better reductants for ISCR.\n\nPolysulfides are compounds that have chains of sulfur atoms. This is a relatively new reactant, but it has been tested on the field in treating TCE and in comparison to EHC. The use of Polysulfides is a type of abiotic reduction and works best in anaerobic conditions where iron (III) is available. The benefit of using polysulfides is that they do not produce any biological waste products; however, the reaction rates are slow and they require more time to create the DVI (dual valent iron) minerals that are needed for the reduction to occur.\n\nDithionite () can also be used as a reductant. It is usually used in addition to iron reduce contaminants. A number of reactions take place and eventually the contaminant is removed. In the process, ditionite is consumed and the final product of all the reactions is 2 sulfur dioxide anions. The dithionite is not stable for a long period of time.\n\nBimetallic materials are materials that are made out of two different metals or alloys that are tightly bonded together. A good example of a bimetallic material would be a bimetallic strip which is used in some kinds of thermometers. In ISCR, bimetallic materials are small pieces of metals that are coated lightly with a catalyst such as palladium, silver, or platinum. The catalyst drives a faster reaction and the small size of the particles allows them to effectively move into and remain in the target zone.\n\nOne proprietary material present today for ISCR is the EHC technology created by Adventus. This particular product is actually a mixture of carbon, nutrients, and zero-valent iron. The theory behind this product is that the carbon in the mixture will promote bacterial growth in the subsurface. The growing bacteria consume oxygen, which easily accepts electrons, present in the subsurface which increases reducing potential. The growing bacteria also ferment and produce fatty acids that act as electron donors to other bacteria and substances. Adventus uses this combination of biotic and abiotic processes to implement ISCR. EHC is injected as a \"slurry\" (a mixture that is 15 to 40% solids and weight with the rest being liquid) into the substratum.\n\nAnother material worth mentioning is EZVI (emulsified ZVI) which is a NASA technology. EZVI is used mainly to treat halogenated hydrocarbons and DNAPLs. EZVI is nanoscale iron that is placed into a biodegradable oil emulsion. The emulsion is then injected into the substratum.\n\nIn ISCR, there are many reductive processes that can take place. There are hydrogenolysis, β-elimination, hydrogenation, α-elimination, and electron transfer. The specific combination of reductive processes that actually take place in the subsurface depends on the species of contaminant that is present and also the type of reduction being used. The natural and biological processes that take place in the substratum also affect the kinds of reductive processes that are found.\n\nThe reactions that occur with permeable reactive barriers and ferrous iron are surface based. The surface reactions take three different forms: direct reduction, electron shunting through ferrous iron, and reduction by production and reaction of hydrogen. Pathway A represents direct electron transfer (ET) for Fe to the adsorbed halocarbon (RX) at the metal/water point of contact, resulting in dechlorination and production of Fe. Pathway B shows that Fe (resulting from corrosion of Fe) may also dechlorinate RX, producing Fe. Pathway C shows that H from the anaerobic corrosion of Fe might react with RX if a catalyst is present.\n\nThe reductive processes discussed above can be enhanced in two ways. One is by increasing the amount of usable iron in the subsurface to increase the rate of the reduction by chemical or biological means. The second method is to enhance the reducing ability of the iron by coupling it with other chemical reductants or using biological reduction with it. Using this processes, scientists combined sodium dithionite with iron to treat Chrominum VI and TCE effectively.\n\nCombining bacterial action and biological processes with iron is also known to be effective. The most evident uses of biological processes are with the EZVI technology created by NASA and with the EHC product created by Adventus. Both of these materials have iron within some biological matrix (iron is suspended in vegetable oil in EZVI and in organic carbon in EHC) and use microbial organisms to enhance the reduction zone and to create a more anaerobic environment for the reactions to take place in.\n\nThe most common type of implementation of ISCR is the installation of permeable reactive barriers (PRBs), but there are instances when the reductant can be directly injected into the subsurface to treat source areas.\n\nThese barriers are usually made out of zero-valent iron (ZVI) but can also be made with any other zero-valent metal. The most common way they are made is by filling a trench with ZVI, nanoscale iron, or palladium. Nanoscale iron particles can also be injected directly into the subsurface to treat plumes, and they have large surface areas and, therefore, high reactivities and can be distributed more evenly in the contamination site. Palladium's reaction rates are rapid. The main advantages of PRBs are that it can reduce many a variety of contaminants and it has no above-ground structure. Problems with PRBs include that even with well constructed barriers, there might be the problem of hydraulic short-circuiting.\n\nNanoscale iron can be directly into the subsurface because they are small enough to be distributed thoroughly. Because the particles are so small, they have a comparatively large reactive surface, providing a more effective reaction. As of now, nanoscale iron is the only material that has been used with this injection strategy, and it is probably the only material that is effective in injection.\n\nISCR is a relatively new technology, so there much scope for research and improvement. Right now, although the reactions that make up ISCR have been studied extensively, there is not much background on what factors most contribute to the effectiveness of ISCR. One thing that needs to be done is find out exactly what reactions are taking place in the subsurface. ISCR is fairly more complex than ISCO because there are substances in the subsurface that will naturally reduce contaminants. The pathways that a contaminant can go through are consequently more diverse. Also, questions that need to be kept in mind are:\n\nAdditional information on this topic may be found at the following sites:\n"}
{"id": "29819978", "url": "https://en.wikipedia.org/wiki?curid=29819978", "title": "Jewish Veg", "text": "Jewish Veg\n\nJewish Veg is a Baltimore, Maryland based 501(c)(3) charitable organization whose mission is to encourage and help Jews to embrace plant-based diets as an expression of the Jewish values of compassion for animals, concern for health, and care for the environment. Jewish Veg was formerly called Jewish Vegetarians of North America (JVNA).\n\nThe current executive director of Jewish Veg is Jeffrey Cohan, who has served since 2013. Under Cohan’s leadership, the organization has added professional staff, built a Board of Directors, and assembled Rabbinic and Advisory councils.\n\nJewish Veg’s Website is the most visited Website on the intersection of Judaism and veganism. It features plant-based versions of such traditional Jewish foods as challah, matzah ball soup and kugel. They publish a monthly e-newsletter that can be subscribed to on their Website. In 2015, Jewish Veg created a Veg Pledge campaign to help people adopt plant-based diets. Pledge-takers have the option to be connected with a vegan mentor if they so choose.\n\nJewish Veg has forged partnerships with prominent Jewish organizations, including Hazon and Hillel International. Their highly esteemed speakers bureau gives numerous presentations in Jewish venues around the country. One of their most prominent speakers is Dr. Alex Hershaft who is a holocaust survivor and the founder of the animal advocacy organization Farm Animal Rights Movement (FARM).\n\nJewish Veg produced their first speaking tour with Israeli vegan leader Ori Shavit in the Fall of 2015. Shavit visited 10 college campuses across the country to speak to Jewish students. In partnership with Hillel International, Jewish Veg will host another tour in the Spring of 2016.\n\nJewish Veg currently has local chapters in Houston, Philadelphia, and Washington, D.C. They are all-volunteer groups which are supported by staff at the national organization. The chapters serve to educate the local Jewish population about veganism and provide community for Jewish vegans.\n\nThe organization was originally called The Jewish Vegetarian Society of America and was founded in 1975 by Jonathan Wolf after a World Vegetarian Conference was held at the University of Maine in Orono, Maine. It was affiliated with the Jewish Vegetarians of England. Wolf stated in 1980: \"In a real sense, vegetarianism is the highest form of Judaism... Intrinsic values in Judaism -- compassion for animals, concern about world hunger and ecology -- are exemplified by vegetarianism.\" \n\nWolf became the organization's first president. For many years, Rabbi Noach Valley was president. Richard H. Schwartz became president in 2003. He is currently president emeritus and serves on the organization’s Board of Directors. Schwartz's book, \"Judaism and Vegetarianism,\" was published in 1982. He argues that Jewish mandates to protect human health, treat animals with compassion, preserve the environment, conserve natural resources, help hungry people, and pursue peace point to vegetarianism as the ideal diet for Jews today.\n\n\"A Sacred Duty: Applying Jewish Values to Help Heal the World\", is Jewish Veg’s 2007 documentary on Judaism, vegetarianism, and the environment. It was directed by Lionel Friedberg. The film received numerous favorable reviews. Jewish Veg has made the entire film freely available on YouTube.\n\nIn 2017, Jewish Veg published a statement by 75 rabbis encouraging Jews to move towards a vegan diet.\n\n\n"}
{"id": "1225755", "url": "https://en.wikipedia.org/wiki?curid=1225755", "title": "Limb darkening", "text": "Limb darkening\n\nLimb darkening is an optical effect seen in stars (including the Sun), where the center part of the disk appears brighter than the edge or \"limb\" of the image. Its understanding offered early solar astronomers an opportunity to construct models with such gradients. This encouraged the development of the theory of radiative transfer.\n\nCrucial to understanding limb darkening is the idea of optical depth. A distance equal to one optical depth is the thickness of the absorbing gas from which a fraction of 1/e photons can escape. This is what defines the visible edge of a star, since it is at a few optical depths that the star becomes opaque. The radiation reaching us is closely approximated by the sum of all the emission along the entire line of sight, up to that point where the optical depth is unity. In particular, if the intensity of radiation in the star varies linearly with optical depth, then the radiation reaching us will be of the intensity at an optical depth of unity. When we look near the edge of a star, we cannot \"see\" to the same depth as when we look at the center because the line of sight must travel at an oblique angle through the stellar gas when looking near the limb. In other words, the star radius at which we see the optical depth as being unity increases as we move our line of sight towards the limb.\n\nThe second effect is the fact that the effective temperature of the stellar atmosphere is (usually) decreasing for an increasing distance from the center of the star. The radiation emitted from a gas is a strong function of temperature. For a black body, for example, the spectrally integrated intensity is proportional to the fourth power of the temperature (Stefan–Boltzmann law). Since when we look at a star, at first approximation, the radiation comes from the point at which the optical depth is unity, and that point is deeper in when looking at the center, the temperature will be higher, and the intensity will be greater, than when we look at the limb.\n\nIn fact, the temperature in the atmosphere of a star does not always decrease with increasing height, and for certain spectral lines, the optical depth is unity in a region of increasing temperature. In this case we see the phenomenon of \"limb brightening\"; for the Sun the existence of a temperature minimum region means that limb brightening should start to dominate at far-infrared or radio wavelengths. Outside the lower atmosphere, and well above the temperature-minimum region, we find the million-kelvin solar corona.\nFor most wavelengths this region is optically thin, i.e. has small optical depth, and must therefore be limb-brightened if spherically symmetric.\n\nFurther complication comes from the existence of rough (three-dimensional) structure. The classical analysis of stellar limb darkening, as described below, assumes the existence of a smooth hydrostatic equilibrium, and at some level of precision this assumption must fail (most obviously in sunspots and faculae, but generally everywhere). Instead, the boundary between the chromosphere and the corona consists of a very complicated transition region, best observed at ultraviolet wavelengths only detectable from space.\n\nIn the figure shown here, as long as the observer at point P is outside the stellar atmosphere, the intensity seen in the direction θ will be a function only of the angle of incidence ψ. This is most conveniently approximated as a polynomial in cos ψ:\n\nwhere \"I\"(ψ) is the intensity seen at P along a line of sight forming angle ψ with respect to the stellar radius, and \"I\"(0) is the central intensity. In order that the ratio be unity for ψ = 0, we must have\n\nFor example, for a Lambertian radiator (no limb darkening) we will have all \"a\" = 0 except \"a\" = 1. As another example, for the sun at 550 nm, the limb darkening is well expressed by \"N\" = 2 and\n\n(See Cox, 2000). The equation for limb darkening is sometimes more conveniently written as\n\nwhich now has \"N\" independent coefficients rather than \"N\" + 1 coefficients that must sum to unity. \n\nThe \"a\" constants can be related to the \"A\" constants. For N = 2,\n\nFor the sun at 550 nm, we then have\n\nThis model gives an intensity at the edge of the sun's disk of only 30% of the intensity at the centre of the disk.\n\nWe can convert these formulas to functions of θ by using the substitution\n\nwhere Ω is the angle from the observer to the limb of the star. For small θ we have\n\nWe see that the derivative of cos ψ is infinite at the edge.\n\nThe above approximation can be used to derive an analytic expression for the ratio of the mean intensity to the central intensity. The mean intensity \"I\" is the integral of the intensity over the disk of the star divided by the solid angle subtended by the disk:\n\nwhere dω = sin θ dθ dφ is a solid angle element, and the integrals are over the disk: 0 ≤ φ ≤ 2π and 0 ≤ θ ≤ Ω. We may rewrite this as\n\nAlthough this equation can be solved analytically, it is rather cumbersome. However, for an observer at infinite distance from the star, formula_15 can be replaced by formula_16, so we have\n\nwhich gives\n\nFor the sun at 550 nm, this says that the average intensity is 80.5% of the intensity at the centre.\n\n"}
{"id": "5874379", "url": "https://en.wikipedia.org/wiki?curid=5874379", "title": "List of Oonopidae species", "text": "List of Oonopidae species\n\nThis page lists all described species of the spider family Oonopidae as of Dec. 12, 2009.\n\n\"Anophthalmoonops\" \n\n\"Antoonops\" \n\n\"Aprusia\" \n\n\"Aridella\" \n\n\"Australoonops\" \n\n\"Bannana\" — Yunnan\n\n\"Blanioonops\" \n\n\"Brignolia\" \n\n\"Caecoonops\" \n\n\"Calculus\" \n\n\"Camptoscaphiella\" \n\n\"Cousinea\" \n\n\"Coxapopha\" \n\n\"Dalmasula\" \n\n\"Decuana\" \n\n\"Diblemma\" \n\n\"Dysderina\" \n\n\"Dysderoides\" \n\n\"Epectris\" \n\n\"Escaphiella\" \n\n\"Farqua\" \n\n\"Ferchestina\" \n\n\"Gamasomorpha\" \n\n\"Grymeus\" \n\n\"Heteroonops\" \n\n\"Hypnoonops\" \n\n\"Hytanis\" \n\n\"Ischnothyrella\" \n\n\"Ischnothyreus\" \n\n\"Kapitia\" \n\n\"Khamisia\" \n\n\"Kijabe\" \n\n\"Lionneta\" \n\n\"Lisna\" \n\n\"Lucetia\" \n\n\"Marsupopaea\" \n\n\"Megabulbus\" \n\n\"Megaoonops\" \n\n\"Myrmopopaea\" \n\n\"Neoxyphinus\" \n\n\"Nephrochirus\" \n\n\"Oonopinus\" \n\n\"Oonopoides\" \n\n\"Oonops\" \n\n\"Opopaea\" \n\n\"Orchestina\" \n\n\"Ovobulbus\" \n\n\"Patri\" \n\n\"Pelicinus\" \n\n\"Pescennina\" \n\n\"Plectoptilus\" \n\n\n\"Prida\" \n\n\"Prodysderina\" \n\n\"Pseudoscaphiella\" \n\n\"Pseudotriaeris\" \n\n\"Scaphiella\" \n\n\"Semibulbus\" \n\n\"Silhouettella\" \n\n\"Simonoonops\" \n\n\"Socotroonops\" \n\n\"Spinestis\" \n\n\"Stenoonops\" \n\n\"Sulsula\" \n\n\"Tapinesthis\" \n\n\"Telchius\" \n\n\"Termitoonops\" \n\n\"Triaeris\" \n\n\"Trilacuna\" \n\n\"Unicorn\" \n\n\"Wanops\" \n\n\"Xestaspis\" \n\n\"Xiombarg\" \n\n\"Xyccarph\" \n\n\"Xyphinus\" \n\n\"Yumates\" \n\n\"Zyngoonops\" \n\n"}
{"id": "2440342", "url": "https://en.wikipedia.org/wiki?curid=2440342", "title": "List of Virginia state forests", "text": "List of Virginia state forests\n\nThe Virginia state forest system includes 24 state-managed forests covering a total of . They are managed by the Virginia Department of Forestry.\n\nThe system was created to manage and maintain forests for wildlife, timber production, recreation, water quality, and aesthetics. The system receives no taxpayer funds, and is self-supported by the sale of forest products.\n\nMost Virginia state forests are accessible to the public. Activities such as hiking, biking, horseback riding, hunting, and fishing are permitted in some state forests; permissible uses vary between individual state forests. Some activities require the purchase of a \"State Forest Use Permit\" for individuals ages 16 or older.\n\nThe following table lists Virginia's 24 state forests, as of 2016.\n\n\n"}
{"id": "947102", "url": "https://en.wikipedia.org/wiki?curid=947102", "title": "List of cranesbill species", "text": "List of cranesbill species\n\nThe genus \"Geranium\" contains more than 420 plant species, which are also known as cranesbill or hardy geranium (to distinguish them from \"Pelargonium\" species).\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15566042", "url": "https://en.wikipedia.org/wiki?curid=15566042", "title": "List of ecoregions in Malawi", "text": "List of ecoregions in Malawi\n\nThe following is a list of ecoregions in Malawi, as identified by the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\n\nby bioregion\n\n\n\n\n"}
{"id": "21305180", "url": "https://en.wikipedia.org/wiki?curid=21305180", "title": "List of glaciers in Europe", "text": "List of glaciers in Europe\n\nThis is a list of glaciers in Europe.\n\n\nThe Italian glaciological committee reports more than 700 glaciers in Italy.\n\n\n\nGlaciers in the Sierra Nevada and the Picos de Europa melted by the end of the 19th century. In 2006, ten small glaciers and six glaciers-glacierets remain in the Spanish Pyrenees. The largest are on:\n\nSweden has a total of around 300 glaciers. The largest is Stuorrajekna in Sulitelma with an area of 13 km.\n\n"}
{"id": "9121362", "url": "https://en.wikipedia.org/wiki?curid=9121362", "title": "List of lakes by volume", "text": "List of lakes by volume\n\nThis article lists lakes with a water volume of more than 100 km³, ranked by volume. The volume of a lake is a difficult quantity to measure. Generally, the volume must be inferred from bathymetric data by integration. Lake volumes can also change dramatically over time and during the year, especially for salt lakes in arid climates. For these reasons, and because of changing research, information on lake volumes can vary considerably from source to source. The base data for this article are from \"The Water Encyclopedia\" (1990). Where volume data from more recent surveys or other authoritative sources has been used, it is referenced in each entry.\n\nThe volume of the lakes below varies little by season. This list does not include reservoirs; if it did, six reservoirs would appear on the list: Lake Kariba at 26th, Bratsk Reservoir, Lake Volta, Lake Nasser, Manicouagan Reservoir, and Lake Guri.\n\n\nTwo bodies of water at times considered to be lakes are hydrologically ocean (Maracaibo) or geologically ocean (the Caspian Sea).\n\n\nThe following are geological as well as geographical lakes.\n\nIn 1960, the Aral Sea was the world's twelfth-largest known lake by volume, at . However, by 2007 it had shrunk to 10% of its original volume and was divided into three lakes; none of them is large enough to appear on this list.\n\n\n\n\n"}
{"id": "37914181", "url": "https://en.wikipedia.org/wiki?curid=37914181", "title": "List of nature centers in New Jersey", "text": "List of nature centers in New Jersey\n\nThis is a list of nature centers and environmental education centers in the state of New Jersey.\n\nTo use the sortable tables: click on the icons at the top of each column to sort that column in alphabetical order; click again for reverse alphabetical order.\n\n"}
{"id": "16075299", "url": "https://en.wikipedia.org/wiki?curid=16075299", "title": "List of rivers of Tanzania", "text": "List of rivers of Tanzania\n\nRivers of Tanzania listed by drainage basin:\n\n\n\n\n\n\nKagera River - Kalambo River\n\nMalagarasi River - Manonga River - Mara River\n\nRufiji River - Rurubu River - Ruvuma River - Ruvyironza River\n\nSemu River - Sibiti River\n\nUmba River (Tanzania)\n\nWami River - Wembere River\n"}
{"id": "8591700", "url": "https://en.wikipedia.org/wiki?curid=8591700", "title": "List of stars in Indus", "text": "List of stars in Indus\n\nThis is the list of notable stars in the constellation Indus, sorted by decreasing brightness.\n\n\n"}
{"id": "8591723", "url": "https://en.wikipedia.org/wiki?curid=8591723", "title": "List of stars in Libra", "text": "List of stars in Libra\n\nThis is the list of notable stars in the constellation Libra, sorted by decreasing brightness.\n\n\n"}
{"id": "8591822", "url": "https://en.wikipedia.org/wiki?curid=8591822", "title": "List of stars in Reticulum", "text": "List of stars in Reticulum\n\nThis is the list of notable stars in the constellation Reticulum, sorted by decreasing brightness.\n\n\n"}
{"id": "36990968", "url": "https://en.wikipedia.org/wiki?curid=36990968", "title": "Manyava Skete", "text": "Manyava Skete\n\nManyava Skete of the Exaltation of the Holy Cross, (transliterated often as Maniava or Manjava Skete) - otherwise known as Ukrainian Athos, is Orthodox solitary cell men´s monastery (skete) in the Carpathian mountains of western Ukraine. It is situated on the outskirts of the village of Maniava in Bohorodchany raion of Ivano-Frankivsk region and belongs to the Ukrainian Orthodox Church of Kyiv Patriarchate. Hegumen of the monastery is Ioasaf, metropolitan of Ivano-Frankivsk and Halych. Currently there are 8 monks residing at the monastery.\n\nManyava Skete is famous for the apparition of Virgin Mary who appeared here twice and for its miracle-working icon of Manyava Icon of Mother of God. The icon \"cried\" twice, shedding myrrh (supernatural oil) in 2004 and last in spring of 2012. These events were widely reported by the Ukrainian news and TV media. Thousands of Orthodox pilgrims arrive here during the year.\n\nIt played an imported role as Orthodox spiritual center during the 17th and 18th centuries. In 1628 at the Kyiv church synod Manyava Skete was granted the status of \"prot\", the principal heading monastery over the monasteries in the Ruthenian, Belz and Podolian Voivodeships. Over 500 monasteries in Ukraine, Moldova and Romania were under its control.\n\nThe Orthodox monastery of Manyava was founded and developed in period between 1606 - 1785. The founding father of the monastery is Saint Job of Maniava, who was monk on the Greek Athos for some time before. In 1606, together with Ivan Vyshensky and religious writer Zakhariya Kopystensky of Kyiv Pechersk lavra he organized the first monk community here.\n\nIn 1618 there were 40 monks residing in the skete. In 1621 the monastery was granted stauropegic status and that meant its direct subordination to the Patriarch of Constantinople.\n\nRight after its renovation in 1781, it was unexpectedly closed down by Austrian authorities on 1 July 1785. The monastery in Maniava was reestablished with the collapse of communism in Ukraine in the early 1990s, nearly 200 years later.\n\nThe clairvoyant hegumen Ioann / John (in the world: \"Vasyl Hrynyuk\"), known for his kind-heartedness, was the first head of Manyava Skete after it was reopened. Many Orthodox faithful from Ukraine, Russia and Poland sought his help. Having foreseen his death, he died in 2003 and is buried on the territory of the monastery and his grave is much venerated by the Orthodox pilgrims. Already after his death, he appeared in a dream to one of the monks waking him up with words \"Stand up, you are burning\" when the electrical wiring was on fire in the monastery. This happened to be true and saved the Skete from the fire.\n\nIn 1652 there was the plague epidemic in skete and the neighbouring areas. Many monks were dying from it. The remaining monks were pleading to God for mercy. On December 22, 1652 the Manyava hieromonk Filaret had a dream vision of Blessed Theotokos being dressed in a red hegumen´s cloak, who came in through the monastery gates and entered the Church of Anunciation of Virgin Mary and having stopped there, said. \"The plague will already stop\". In remembrance of this event there is an icon \"Hegumenia of the Manyava monastery\" kept in the skete.\n\nIn Manyva Skete there are three churches. The main one of the Exaltation of the Holy Cross, the second church is of the holy martyrs Borys and Hlib and besides these there is also underground church dedicated to Archangel Michael.\n\nThe second president of Ukraine - Leonid Kuchma visited the place and supported the restoration of the monastery. Other presidents of Ukraine - Viktor Yushchenko, of Georgia - Mikhail Saakashvili and of Russia - Boris Yeltsin all visited the Manyava Skete.\n\nPatriarch Filaret and many of the hierarchs of the Ukrainian Orthodox Church of Kyiv patriarchate visit Maniava Skete on regular basis.\n\nManyava Skete is also famous for its \"Blessed Stone\", regarded as a place of prayer and of the spiritual purification. The stone reminds a giant cave with the space of 10 x 3 meters, a typical dwelling of the solitary monks, a skete. This was a first place, where the founding monks (the first apostles of the subcarpathian highland) settled in the 13th century.\n\nThe spring of healing water emits from underneath the Blessed Stone. It is being said that the water has the same healing properties as the waters of Lourdes. The spring ceased when the monastery was closed and miraculously started flowing again with the reopening of Maniava Skete.\n\n\n"}
{"id": "11074998", "url": "https://en.wikipedia.org/wiki?curid=11074998", "title": "Mooring (oceanography)", "text": "Mooring (oceanography)\n\nA mooring in oceanography is a collection of devices, connected to a wire and anchored on the sea floor. It is the \"Eulerian way\" of measuring ocean currents, since a mooring is stationary at a fixed location. In contrast to that, the \"Lagrangian way\" measures the motion of an oceanographic drifter, see Lagrangian drifter.\n\nThe mooring is held up in the water column with various forms of buoyancy such as glass balls and syntactic foam floats. The attached instrumentation is wide-ranging but often includes CTDs (conductivity, temperature depth sensors), current meters (e.g. acoustic Doppler current profilers or deprecated rotor current meters), biological sensors, and other devices to measure various parameters. Long-term moorings can be deployed for durations of two years or more, powered with alkaline or lithium battery packs.\n\nMoorings often include surface buoys that transmit real time data back to shore. The traditional approach is to use the Argos System. Alternatively, one may use the commercial Iridium satellites which allow higher data rates.\nIn deeper waters, areas covered by sea ice, areas within or near shipping lines or areas that are prone to theft or vandalism, moorings are often submerged with no surface markers. Submerged moorings typically use an acoustic release or a Timed Release that connects the mooring to an anchor weight on the sea floor. The weight is released by sending a coded acoustic command signal and stays on the ground. Deep water anchors are typically made from steel and may be as large as 100 kg. A common deep water anchor consists of a stack of 2-4 railroad wheels. In shallow waters anchors may consist of a cinder block or small portable anchor.\n\nThe buoyancy of the \"floats\", i.e. of the top buoy plus additional packs of glass bulbs of foam, is sufficient to carry the instruments back to the surface. In order to avoid entangled ropes, it has been practical to place additional floats directly above each instrument.\n\nPrawlers (profiling crawlers) are sensor bodies which climb and descend the cable, to observe multiple depths. The energy to move is \"free,\" harnessed by ratcheting upward via wave energy, then returning downward via gravity.\n\nSimilar to a kite in the wind, the mooring line will follow a so-called (half-)catenary.\nThe influence of currents (and wind if the top buoy is above the sea surface) can be modeled and the shape of the mooring line can be determined by software. If the currents are strong (above 0.1 m/s) and the mooring lines are long (more than 1 km), the instrument position may vary up to 50 m.\n\nOther oceanographic instruments include:\n"}
{"id": "21814", "url": "https://en.wikipedia.org/wiki?curid=21814", "title": "Nitrogen Oxide Protocol", "text": "Nitrogen Oxide Protocol\n\nProtocol to the 1979 Convention on Long-Range Transboundary Air Pollution Concerning the Control of Emissions of Nitrogen Oxides or Their Transboundary Fluxes, opened for signature on 31 October 1988 and entered into force on 14 February 1991, was to provide for the control or reduction of nitrogen oxides and their transboundary fluxes. It was concluded in Sofia, Bulgaria.\n\nParties (as of September 2016): (36) Albania, Austria, Belarus, Belgium, Bulgaria, Canada, Croatia, Cyprus, Czech Republic, Denmark, Estonia, European Union, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Liechtenstein, Lithuania, Luxembourg, Republic of Macedonia, Netherlands, Norway, Poland, Russia, Slovakia, Slovenia. Spain, Sweden, Switzerland, Ukraine, United Kingdom, United States.\n\n\n"}
{"id": "38533961", "url": "https://en.wikipedia.org/wiki?curid=38533961", "title": "OSCAR 3", "text": "OSCAR 3\n\nOSCAR III (a.k.a. OSCAR 3) is the third amateur radio satellite launched by Project OSCAR into Low Earth Orbit. OSCAR 3 was launched March 9, 1965 by a Thor-DM21 Agena D launcher from Vandenberg Air Force Base, Lompoc, California. The satellite, weighing 16.3 kg, was launched piggyback with seven United States Air Force satellites. The satellite employed no attitude control system. OSCAR III linear transponder lasted 18 days. More than 1000 amateurs in 22 countries communicated through the linear transponder. The two beacon transmitters continued operating for several months.\n\nProject OSCAR Inc. started in 1960 with the radio amateurs from the TRW Radio Club of Redondo Beach, California, many who worked at TRW and defense industries, to investigate the possibility of putting an amateur satellite in orbit. Project OSCAR was responsible for the construction of the first Amateur Radio Satellite OSCAR-1, that was successfully launched from Vandenberg AFB in California . OSCAR-1 orbited the earth for 22 days, transmitting the “HI” greeting you see in Morse Code above.\n\nProject OSCAR was responsible for launching 3 other amateur radio satellites during the 1960s: OSCAR 1, OSCAR 2, and OSCAR 4\n\nIn 1969, AMSAT-NA was founded by radio amateurs working at NASA's Goddard Space Flight Center and the Baltimore-Washington DC region, to continue the efforts begun by Project OSCAR. Its first project was to coordinate the launch of Australis-OSCAR 5, constructed by students at the University of Melbourne.\n\nToday, over fifty years later, Project OSCAR still exists as part of the San Jose (CA) Amateur Radio Club. Its mission is “To initiate and support activities that promote the Satellite Amateur Radio Hobby”. The primary goal is to reach out and provide logistical support, training and in some cases equipment to amateur radio associations, schools and the public at large.\n\nOscar III was an upgrade from the earlier Oscar I and Oscar II amateur satellites, three years earlier. Improvements included:\n\n\n\n"}
{"id": "25436194", "url": "https://en.wikipedia.org/wiki?curid=25436194", "title": "Outwelling", "text": "Outwelling\n\nOutwelling is a hypothesized process by which coastal salt marshes and mangroves, “hot spots” of production, produce an excess amount of carbon each year and “outwell” these organic nutrients and detritus into the surrounding coastal embayment or ocean, thus increasing the productivity of local fisheries or other coastal plants. Outwelling also nourishes plankton communities and causes a spike in activity.\n\nThe majority of outwelling is dissolved organic carbon (DOC) and some particulate organic carbon (POC) Outwelling expels salt (90 g salt/m2), silicate (1.0 mmol/m2), orthophosphate (0.03 mmol/m2), and nitrate (0.04 mmol/m2) during each tidal cycle.\nOutwelling is affected by a number of different factors. For one, the amount of outwelling is dependent upon the primary production of an estuary, thus, highly productive salt marshes result in increased outwelling. It is also dependent on tidal amplitude and geomorphology of the estuary. Outwelling is not a steady process, and is affected by large rainfalls or inundation events (the larger the inundation, the greater the outwelling).\n\nOutwelling does not occur in every estuary. It is more evident and occurs more in estuaries bordering extensive coastal marshes. For example, a study done in a New England salt marsh found no evidence of outwelling, and in fact found that the salt marshes import carbon; however, another study done in Louisiana near the extensive salt marshes where tidal amplitude is larger found that outwelling contributed a significant amount of organic carbon to the nearby waters.\nOutwelling occurs as pulses that correlate to inundation and precipitation events, productivity and tidal fluctuations. In some cases, it is macrofauna and algae that are pulsed out of the salt marsh into the water column rather than nutrients, but this has a similar effect of attracting small fish and nourishing the marine environment.\n\nBecause of this hypothesis, many states have passed laws protecting estuaries based on the rationale that protecting estuaries will protect the food source of local fish populations.\n\nThe Outwelling hypothesis has been hotly debated for decades. There are many studies examining this hypothesis, but there has not yet been a firm conclusion. It appears that it may occur in some estuaries at some times, but there is a body of evidence contesting the claimed amount of organic carbon export, and even whether marshes export carbon at all.\n"}
{"id": "55926948", "url": "https://en.wikipedia.org/wiki?curid=55926948", "title": "Pesticide standard value", "text": "Pesticide standard value\n\nPesticide standard values are applied worldwide to control pollution, since pesticides are largely applied in numerous agricultural, commercial, residential, and industrial applications. Usually, pesticide standard value is regulated in residential surface soil (i.e., pesticide soil regulatory guidance value, or RGV), drinking water (i.e., pesticide drinking water maximum concentration level, or MCL), foods (i.e., pesticide food maximum residue level, or MRL), and other ecological sections (e.g., air, surface water, groundwater, bed sediment, or aquatic organisms).\n\nPesticide standard values specify the maximum amount of a pollutant that may be present without prompting some form of regulatory response such as human health and ecological effects. Pesticide standard values are often derived from laboratory toxicology data (i.e., animal tests), human or ecological parameters (i.e., body weight, intake rate, lifetime, etc.), and human health risk models such as USEPA and RIVM models. On the other hand, the European Union took a precautionary approach (in accordance with the principles of its environmental policy) before toxicological data was available and provided very strict and protective standards for all pesticides in drinking water.\n\nUp till now (November 2017), less than 30% of the worldwide nations have regulated pesticide standard values in surface residential soil, about 50% of the total nations have provided pesticide standard values in drinking water and agricultural foods. Many nations in Africa, Asia, and South America are lacking pesticide standard values for the major human and ecological exposure pathways such as soil, sediment, and water.\n\nPesticide standard values for many current and historical largely used pesticides such as DDT, aldrin, lindane, glyphosate, MCPA, chlorpyrifos, and 2,4-D often vary over seven, eight, or nine orders of magnitude and are log-normally distributed, which indicates that there is little agreement on the regulation of pesticide standard values among worldwide jurisdictions. Additionally, many worldwide pesticide standard values are not sufficiently low to protect public health based on human health risk uncertainty bounds calculations and maximum legal contribution estimations.\n"}
{"id": "41995484", "url": "https://en.wikipedia.org/wiki?curid=41995484", "title": "Resource and Energy Economics", "text": "Resource and Energy Economics\n\nResource and Energy Economics is a quarterly peer-reviewed academic journal covering energy economics and environmental economics published by Elsevier. It was established in 1978 as \"Resources and Energy\" and obtained its current title in 1993. The editors-in-chief are R.D. Horan (Michigan State University) and D. van Soest (Tilburg University). The journal was founded by University of Chicago economist George S. Tolley, and Tolley continues to serve as an honorary editor.\n\nThe journal is abstracted and indexed in ABI/Inform, Engineering Index, Geosystems, INSPEC, \"Journal of Economic Literature\", RePEc, Scopus, Current Contents/Social & Behavioral Sciences, and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 1.495.\n\n"}
{"id": "2256308", "url": "https://en.wikipedia.org/wiki?curid=2256308", "title": "Songpyeon", "text": "Songpyeon\n\nSongpyeon () is a traditional Korean food made of rice powder. It is a type of \"tteok\", small rice cakes, traditionally eaten during the Korean autumn harvest festival, Chuseok. It became a popular symbol of traditional Korean culture. The earliest records of songpyeon date from the Goryeo period. \n\nSongpyeon are half-moon shaped rice cakes that contain sweet or semi-sweet fillings, such as sesame seeds and honey, sweet red bean paste or chestnut paste, all steamed over a layer of pine needles, which gives them the fragrant smell of fresh pine trees. The name Songpyeon comes from the use of pine needles (\"song\" in \"songpyeon\" means pine tree).\n\nSongpyeon is made by kneading rice powder with hot water and stuffing the dough with beans, sesame, chestnuts and other fillings. Songpyeon can be made into many unique colors and flavors using natural ingredients. The shape, size and ingredients may vary by region. Common in Seoul are smaller bite-sized songpyeon, while in other regions, such as Gangwon Province, potatoes and acorn powder are more common because Gangwon Province is known for growing these two. Other ingredients that can be added to songpyeon are soybeans, cowpeas, chestnuts, bean powder, jujubes, sesame and honey.\n\nSongpyeon is one of the most popular homemade Korean dishes for holidays. Many families make their own songpyeon. Songpyeon is given to family members and close neighbors as a sign of respect. An old Korean anecdote says that the person who makes beautifully-shaped songpyeon will meet a good spouse or give birth to a beautiful baby. Many Korean families say that Chuseok would not be complete without songpyeon.\n\nSongpyeon was used for ancestral celebrations. Songpyeon and other foods were used to show appreciation for the year's harvest. Koreans hoped and prayed to help them avoid misfortune. Songpyeon is mainly eaten at Chuseok, at harvest time, but some Korean families like to eat them during the spring. \n\nMany stories describe why songpyeon is in the shape of a half moon. Most believe it is because Korean ancestors thought the full moon could only wane while a crescent shape/half-moon would fill up.\n\n\n"}
{"id": "30896817", "url": "https://en.wikipedia.org/wiki?curid=30896817", "title": "Targeting (international health)", "text": "Targeting (international health)\n\nTargeting, a commonly used technique in international health and public health, develops a specialized health intervention approach for a specific group of people. This group of people may be identified by various factors, including geography, race/ethnicity, age, health issues, etc. Taking into account characteristics the group members share in common, only one version of program materials is used in a targeted health intervention and the intervention materials show features that the targeted population subgroup members prefer.\n\nThe idea of targeting was adapted from marketing. Target marketing tries to define a subgroup of people with shared characteristics (such as liking of certain product features) as the right group of consumers for a particular product or service. Adopting the same principle, targeted health interventions identify a group of people who are particular vulnerable to certain health risks, intervene to address their health risks and in turn, improve their health. \nFrom early to mid-1980s, most health information was mass-produced for undifferentiated audience in the form of brochures, pamphlets, and booklets. As people are increasingly aware of different characteristics within mass audiences, behavioral scientists began to develop different versions of materials for diverse population subgroups in order for health campaigns to be cost effective while still reaching large number of audiences. Scholars have suggested that compared to generic interventions which do not take into consideration the audience characteristics, targeting strategy can be more effective for health interventions.\n\nTargeting as an intervention strategy is now widely used in the health fields and has been found to help promote behavioral change. The level of audience specificity in targeting can vary significantly, ranging from one single broad factor (e.g. interventions targeting women), to combined factors. For example, Gomez, Tan, Keegan, and Clarke (2007) identified South Asians in the U.S. who were less than age 50 and were unemployed or non-citizens as potential population subgroup for targeted mammogram utilization intervention.\n\nTargeted health interventions have adopted different audience factors, including age (e.g. older smokers), race/ethnicity (e.g. African-Americans), socioeconomic status (e.g. low-income women), professions (e.g. health providers), health status (e.g. pregnant women and women with children, sensation-seeking adolescents).\n\nTargeting has been widely used in international health interventions and was found to be effective in promoting behavioral change.\n\nAhmed, Petzold, Kabir, and Tomson (2006) identified the problem in Bangladesh that there was not enough usage of formal health services among the ultra poor population. In order to promote greater use of these health services, Ahmed and colleagues carried out a health intervention in Bangladesh targeting ultra poor households in rural areas in order to change their health-seeking behavior. The intervention provided ultra poor households with money for household activities (e.g. raising livestock, vegetables farming) to increase their income. In addition, it provided 18 months of supports, such as skill training (e.g. vegetable cultivation) and counseling information on health services, to make sure that the targeted people were able to make full use of the money and had the resources to use the health services if they chose to. This targeted intervention was found to be effective that it decreased the amount of self-care and increased the use of professional health care among the targeted population.\n\nIn another intervention, Lowndes, Alary, Labbe, et al. (2007) targeted male clients of female sex workers in Benin, West Africa, in order to reduce the male clients' risky sexual behavior and HIV/STI rates. During this peer-education intervention, trained health educators approached male clients at prostitution venues at night, discussed with them HIV and STI transmission risks, educated them about proper condom use, and distributed condoms. Although HIV prevalence remained unchanged, the intervention effectively affected knowledge and there was an increase in the condom use rate among the targeted population.\n\nDespite its effectiveness in promoting behavioral change, targeting strategy should be used with caution and with careful considerations of ethics. Kass (2001) suggested that interventions targeting already vulnerable segments of the population may face certain ethical challenges. Potential burden and harm in these health programs may include risks to privacy and confidentiality, risks to liberty and self-determination, and risks to justice. Subgroups who are targeted in interventions take most of the burden from the process (for example, their privacy may be intruded) while the benefits resulted from the health interventions are for the general public (e.g. improvements in mortality). Kass posed the question whether the unequal distribution of burden and benefits is ethical.\nMoreover, the evidence of intervention effectiveness may not be strong enough at times. Thus it may not be known for sure whether the benefits the targeted audience may gain from the intervention will outweigh the harms they will be put in. Hence, health experts face another ethical dilemma.\n\nKass further pointed out that targeting may create stigma that some segments of the population are more vulnerable to certain diseases, which might result in social harms, such as psychological distress and discrimination. For example, although the HIV interventions targeting male clients of female sex workers in Benin contributed to HIV prevention, one possible stigma that it may create is that male clients of female sex workers are high risk population of HIV infection. This stigma may create anxiety and panic among the targeted population. On the other hand, if a subgroup were never targeted in interventions, they may not believe that they are at the risk of diseases. People who were not targeted in HIV interventions, e.g. people with regular sex partners, may perceive themselves as not at the risk of HIV infection and hence continue engaging in risky sexual behaviors.\n\nGiven the above discussed ethical dilemma that targeted health interventions may face, Kass suggested that, if health programs and health resources are going to be unequally allocated (e.g. targeting ethnic minority groups), they should be done with caution and be based on strong intervention effectiveness evidences.\n\nWhile Kreuter and Skinner's (2000) definition of targeting \"the development of a single intervention approach for a defined population subgroup that takes into account characteristics shared by the subgroups' members\" is now widely accepted in international health, targeting was defined in a few different ways.\n\nIt is important to understand the concept of tailoring in order to capture the idea of targeting, as interventions using targeting strategy were mislabeled as tailoring in some health interventions. Pasick (1997)defined tailoring as \"the adaptation of interventions to best fit the relevant needs and characteristics of a specified target population\". Eakin and colleagues described tailoring as \"a process of producing a video whose content and characters are adapted for or designed to appeal to a particular target population\". Both statements would now be considered as a more appropriate definition of \"targeting\" according to Kreuter and Skinner (2000). The distinction between targeting and tailoring is that targeting is to design information for a defined population subgroup while tailoring is to develop interventions for one specific person based on the individual characteristics.\n\nAnother competing definition of targeting was developed by Rimal and Adkin (2003). They stated that both targeting and tailoring should follow the audience segmentation process, which is to divide audience into highly homogeneous clusters. They distinguish targeting and tailoring by suggesting that targeting is the selection of communication channel for the health campaign while tailoring is about message design.\n\nDespite the wide acceptance of the Kreuter and Skinner (2000) definition, Rimer (2000 pointed out that, although some \"tailored\" intervention would now be appropriately labeled as \"targeted\" intervention, these published projects are out there and the article titles remain as \"tailored\", which might create confusion. Rimer suggested that precision of language is important in order to avoid confusion. For people who are trying to understand targeting and tailoring, it is important to keep in mind the inconsistent labeling of targeting and tailoring in earlier published articles and to make critical judgments when reading.\n\n"}
{"id": "23258412", "url": "https://en.wikipedia.org/wiki?curid=23258412", "title": "Transit passage", "text": "Transit passage\n\nTransit passage is a concept of the Law of the Sea, which allows a vessel or aircraft the freedom of navigation or overflight solely for the purpose of continuous and expeditious transit of a strait between one part of the high seas or exclusive economic zone and another. The requirement of continuous and expeditious transit does not preclude passage through the strait for the purpose of entering, leaving or returning from a state bordering the strait, subject to the conditions of entry to that state.\n\nThis navigation rule is codified in Part III of the United Nations Convention on the Law of the Sea. Although not all countries have ratified the convention, most countries, including the US, accept the customary navigation rules as codified in the Convention. This navigation rule took on more importance with UNCLOS III as that convention confirmed the widening of territorial waters from three to twelve nautical miles, causing more straits not to have a navigation passage between the territorial waters of the coastal nations.\n\nTransit passage exists throughout the entire strait, not just the area overlapped by the territorial waters of the coastal nations. The ships and aircraft of all nations, including warships, auxiliaries, and military aircraft, enjoy the right of unimpeded transit passage in such straits and their approaches. Submarines are free to transit international straits submerged since that is their normal mode of operation . The legal regime of transit passage exists in the most important straits for the international trade exchange and security (Strait of Gibraltar, Dover Strait, Strait of Hormuz, Bab-el-Mandeb, Strait of Malacca).\n\nTransit passage rights do not extend to any state's internal waters within a strait.\n\n"}
{"id": "4992420", "url": "https://en.wikipedia.org/wiki?curid=4992420", "title": "UN-Energy", "text": "UN-Energy\n\nUN-Energy is an interagency mechanism within the system of the United Nations related to energy. It was created after the 2002 World Summit on Sustainable Development in Johannesburg, and its purpose is to create a coherent approach towards a sustainable energy system especially in developing countries to meet the Millennium Development Goals. \n\nTo do this, UN-Energy is reviewing energy-related activities within the UN system and trying to mainstream them into a broader approach. At present, UN-Energy remains a very small UN entity since it does not even reach the programme status.\n\nUN-Energy was established as a subsidiary body of CEB in 2004 to help ensure coherence in the United Nations system’s multi-disciplinary response to the World Summit on Sustainable Development (WSSD) and to promote the effective engagement of non-UN stakeholders in implementing WSSD energy-related decisions. Its membership consists of senior officials and experts on energy of the commissions, organizations, funds and programmes listed below. Secretariat work is being done by the Department of Economic and Social Affairs (DESA).\n\nThe United Nations System Chief Executives Board for Coordination (CEB), chaired by the United Nations Secretary-General, endorsed the director general of the United Nations Industrial Development Organization (UNIDO), Kandeh Yumkella, as the Chairman of UN-Energy at its second regular session on 26-27 October 2007 in New York. Yumkella will serve as Chairman of UN- Energy for two years beginning on 1 January 2008. \n\n"}
{"id": "634858", "url": "https://en.wikipedia.org/wiki?curid=634858", "title": "Vacuum chamber", "text": "Vacuum chamber\n\nA vacuum chamber is a rigid enclosure from which air and other gases are removed by a vacuum pump. This results in a low-pressure environment within the chamber, commonly referred to as a vacuum. A vacuum environment allows researchers to conduct physical experiments or to test mechanical devices which must operate in outer space (for example) or for processes such as vacuum drying or vacuum coating. Chambers are typically made of metals which may or may not shield applied external magnetic fields depending on wall thickness, frequency, resistivity, and permeability of the material used. Only some materials are suitable for vacuum use.\n\nChambers often have multiple ports, covered with vacuum flanges, to allow instruments or windows to be installed in the walls of the chamber. In low to medium-vacuum applications, these are sealed with elastomer o-rings. In higher vacuum applications, the flanges have knife edges machined onto them, which cut into a copper gasket when the flange is bolted on.\n\nA type of vacuum chamber frequently used in the field of spacecraft engineering is a thermal vacuum chamber, which provides a thermal environment representing what a spacecraft would experience in space.\n\nVacuum chambers can be constructed of many materials. \"Metals are arguably the most prevalent vacuum chamber materials.\" The strength, pressure, and permeability are considerations for selecting chamber material.\nCommon materials are:\n\n\"Vacuum degassing is the process of using vacuum to remove gases from compounds which become entrapped in the\nmixture when mixing the components.\" To assure a bubble-free mold when mixing resin and silicone rubbers and slower-setting harder resins, a vacuum chamber is required. A small vacuum chamber is needed for de-airing (eliminating air bubbles) for materials prior to their setting. The process is fairly straightforward. The casting or molding material is mixed according to the manufacturers directions.\n\nSince the material may expand 4–5 times under a vacuum, the mixing container must be large enough to hold a volume of four to five times the amount of the original material that is being vacuumed to allow for the expansion; if not, it will spill over the top of the container requiring clean-up that can be avoided. The material container is then placed into the vacuum chamber; a vacuum pump is connected and turned on. Once the vacuum reaches (at sea level) of mercury, the material will begin to rise (resembling foam). When the material falls, it will plateau and stop rising. The vacuuming is continued for another 2 to 3 minutes to make certain all of the air has been removed from the material. Once this interval is reached, the vacuum pump is shut off and the vacuum chamber release valve is opened to equalize air pressure. The vacuum chamber is opened, the material is removed and is ready to pour into the mold.\n\nThough a maximum vacuum one can theoretically achieve at sea level is 29.921 inches of mercury (Hg,) this will vary significantly as altitude increases. For example, for a mold making in \"Denver, Colorado\", which sits at 5280 feet above sea level, one can only achieve a vacuum on the mercury scale of 24.896 Hg in their vacuum chamber. \n\nTo keep the material air-free, it must be slowly poured in a high and narrow stream starting from the corner of the mold box, or mold, letting the material flow freely into the box or mold cavity. Usually, this method will not introduce any new bubbles into the vacuumed material. To ensure that the material is totally devoid of air bubbles, the entire mold/mold box may be placed in the chamber for an additional few minutes; this will assist the material in flowing into difficult areas of the mold/mold box.\n\nWater and other liquids may accumulate on a product during the production process. \"Vacuum is often employed as a process for removing bulk and absorbed water (or other solvents) from a product. Combined\nwith heat, vacuum can be an effective method for drying.\"\n\nNASA's Space Power Facility houses the world's largest vacuum chamber. It was built in 1969 and stands high and in diameter, enclosing a bullet-shaped space. It was originally commissioned for nuclear-electric power studies under vacuum conditions, but was later decommissioned. Recently, it was recommissioned for use in testing spacecraft propulsion systems. Recent uses include testing the airbag landing systems for the Mars Pathfinder and the Mars Exploration Rovers, Spirit and Opportunity, under simulated Mars atmospheric conditions.\n\n"}
{"id": "44065801", "url": "https://en.wikipedia.org/wiki?curid=44065801", "title": "Volcano tectonics", "text": "Volcano tectonics\n\nVolcano tectonics is a scientific field that uses the techniques and methods of structural geology, tectonics, and physics to analyse and interpret physical processes and the associated deformation in volcanic areas, at any scale.\n\nThese processes may be 1) magma-induced or, conversely, 2) control magma propagation and emplacement. \nIn the first case, the process has a local extent, usually within the volcanic area. Typical examples include the development of calderas and resurgences, pit craters, dikes, sills, laccoliths, magma chambers, eruptive fissures, volcanic rift zones and any type of volcano flank dynamics, including sector collapses. \nIn the second case, the process controlling the magma may have a regional extent, also outside the volcanic area. Typical examples include the activity of regional faults and earthquakes along divergent, convergent and transform plate boundaries, as continental, transitional and oceanic rifts, magmatic arcs and back-arcs, as well as of any intraplate structure possibly controlling volcanism. \nThe study of these processes is not restricted to the Earth's crust. In fact, an increasing number of studies has been considering also the Volcano-Tectonic features of extraterrestrial bodies, including Venus, Mars and Jupiter's moon Io.\n\nAs a volcano consists, in the broadest sense, of a volcanic edifice, a plumbing system and a deeper magma reservoir, Volcano-Tectonics is not restricted to the surface processes, but also includes any subsurface process in the host rock related to the shallower and deeper plumbing system of the volcano. The latter may be directly accessible in the eroded portions of active volcanoes or, more commonly, in extinct eroded volcanoes.\n\nThe general aim of Volcano-Tectonics is to capture the shallower and deeper structure of volcanoes, establishing the overall stress-strain relationships between the magma and the host rock, to ultimately understand how volcanoes work in their regional context. This approach allows defining the dynamic behaviour of active volcanoes during unrest periods and eruptions and thus being able to make reliable forecasts as to the likely scenarios.\n\nVolcano-Tectonics merges the knowledge and expertise of a wide range of methodologies. \nThese primarily include structural geology (usually at the outcrop scale), tectonics (usually at the regional scale), geodesy from active volcanoes (GPS, InSAR, levelling, strainmeters, tiltmeters), geophysics (seismicity, gravity, seismic lines), remote sensing (optical and thermal), and modelling (analytical, numerical and analogue models). \nMore volcanological-oriented methodologies are also involved, including stratigraphy, petrology, geochemistry and geochronology.\n\nData, however, are of little use if they cannot be interpreted and understood within the framework of a reasonable model or theory of volcano behaviour. Quantitative and testable models must, in the end, be related to some physical theories and thus to physics. In Volcano-Tectonics, like in solid-earth geophysics in general, the main physical theories used are those that derive from continuum mechanics. For solid-earth sciences, these are mainly solid mechanics, including rock mechanics, fracture mechanics and general tectonophysics, and fluid mechanics, including fluid transport in rock fractures.\n\n\n"}
