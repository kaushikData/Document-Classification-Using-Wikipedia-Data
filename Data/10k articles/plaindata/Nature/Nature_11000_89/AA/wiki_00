{"id": "16035100", "url": "https://en.wikipedia.org/wiki?curid=16035100", "title": "AMSAT-OSCAR 7", "text": "AMSAT-OSCAR 7\n\nAMSAT-OSCAR 7, or AO-7, is the second Phase 2 amateur radio satellite constructed by the Radio Amateur Satellite Corporation or AMSAT. It was launched into Low Earth Orbit on November 15, 1974 and remained operational until a battery failure in 1981. Then after 21 years of apparent silence, the satellite was heard again on June 21, 2002 – 27 years after launch. At that time the public learned that the satellite had remained intermittently functional and used surreptitiously for communication by the anticommunist opposition Fighting Solidarity during the martial law in Poland.\n\nAO-7 is the oldest amateur satellite still in use, and is one of the oldest operational communications satellites. It carries two amateur radio transponders. Its \"Mode A\" transponder has an uplink on the 2-meter band and a downlink on the 10-meter band. The \"Mode B\" transponder has an uplink on the 70-centimeter band and a downlink on the 2-meter band. The satellite also carries four beacons which are designed to operate on the 10-meter, 2-meter, 70-centimeter and 13-centimeter bands. The 13-cm beacon was never activated due to a change in international treaties.\n\nAMSAT reported AO-7 still operational on June 25, 2015, with reliable power only from its solar panels; the report stated the cause of the 21-year outage was a short circuit in the battery and the restoration of service was due to its becoming an open circuit. The satellite eclipses on every orbit during the northern summer and autumn; the rest of the year it is in continuous sunlight and alternates between transmission modes A and B. All transponders and beacons are operational.\n\nAO-7 was the second Phase 2 satellite (Phase II-B). At launch, the satellite had a mass of and it was placed into a 1444 x 1459 km orbit. It is shaped as an octahedron 360 mm high and 424 mm in diameter. It has a circularly-polarized, canted turnstile VHF/UHF antenna system and HF dipole. Four radio masts mounted at 90 degree intervals on the base of the satellite and two experimental repeater systems provided store-and-forward for Morse code and teletype messages (\"codestore\") as it orbited around the world. The Mode-B transponder was designed and build by Karl Meinzer, DJ4ZC and Werner Haas, DJ5KQ. The Mode-B transponder was the first using “HELAPS” (High Efficient Linear Amplification by Parametric Synthesis) technology was developed by Dr. Karl Meinzer as part of his Ph.D. research. AO-7 has redundant command decoders of a design similar to the unit proven highly successful in AMSAT-OSCAR 6. The decoder has provisions for 35 separate functions, and is designed to provide a reliable means of controlling the emissions of the repeaters, beacons and other experiments aboard the spacecraft.\n\nAO-7 demonstrated several uses of new technologies and operations \n\n\nThe uplink frequency predates the WARC 1979 allocation of 435-438 MHz by the ITU for the Amateur Satellite Service which places the uplink in the 70cm weak signal segment. Additionally, the IARU bandplan has the 432.1 MHz range (which is used for mode B uplink) marked for \"weak signal\" in all three Regions. Accessing the Mode B uplink is permitted in the United States under a waiver from the FCC.\n\nIn the summer of 1982 the Fighting Solidarity in Wrocław learned that AO-7 became periodically functional, when its solar panels got enough sunlight to power up the satellite. It was then used to communicate with Solidarity activists in other Polish cities and to send messages to the West. Satellite communication was invaluable at that time, as the regular telephone network was tapped by the government and shut down when martial law was imposed in December 1981. Ham radios were not of much use as they were easy to track. On the other hand, a satellite link required highly directional antennas which were impossible to track by the regime. In 2002 Pat Gowen (G3IOR), inspired by the history of Fighting Solidarity, attempted to communicate with AO-7 and confirmed it to be operational.\n\n, contacts with AO-7 are reported daily.\n"}
{"id": "25275503", "url": "https://en.wikipedia.org/wiki?curid=25275503", "title": "Ashoverite", "text": "Ashoverite\n\nAshoverite is one of three polymorphs of zinc hydroxide, Zn(OH). It is a rare mineral first found in a limestone quarry near Ashover, Derbyshire, England, in 1988. It has also been found in the Harz mountain range in Germany, and in Namibia.\n\nThe mineral was discovered after samples of the polymorph sweetite were sent to labs by S. A. Rust. Some specimens contained what appeared to be baryte but, which on further examination, were found to be a previously undescribed mineral.\n\n"}
{"id": "30580710", "url": "https://en.wikipedia.org/wiki?curid=30580710", "title": "Catfish Creek (Ontario)", "text": "Catfish Creek (Ontario)\n\nThere are three creeks named Catfish Creek in Ontario, Canada:\n\n\n"}
{"id": "11950042", "url": "https://en.wikipedia.org/wiki?curid=11950042", "title": "Center for Climate Systems Research", "text": "Center for Climate Systems Research\n\nThe Center for Climate Systems Research is a key Columbia University Earth Institute center with over 25 scientists and staff researching issues involving the interplay between Earth's climate and society. The center's motto is \"Columbia's Gateway to NASA and Beyond\", as it has a special relationship with the NASA Goddard Institute for Space Studies (GISS). It is a \"Gateway to NASA\", because the center is co-located with GISS in Columbia University's Armstrong Hall above Tom's Restaurant in New York City. In this historic location, Columbia and NASA scientists work together closely to understand the Earth's climate and how changes can impact society both in the United States and around the world. Importantly, CCSR scientists also work with many other partners – governmental, private sector, and non-governmental organizations – to answer these challenging scientific and socioeconomic questions (i.e., \"and Beyond\"). Since late 2017, the director of CCSR has been Dr. Michael J. Puma.\n"}
{"id": "15036532", "url": "https://en.wikipedia.org/wiki?curid=15036532", "title": "Coombabah Lake Conservation Park", "text": "Coombabah Lake Conservation Park\n\nThe Coombabah Lake Conservation Park is a conservation park that is an Important Wetland in Australia, located in the Gold Coast region of South East Queensland, Australia. Part of the Coomera River catchment, Lake Coombabah is a tidal lake at the mouth of Coombabah Creek. The Coombabah wetlands are significant because they are the most southerly lake and coastal swampland representatives in the bioregion, and because the area provides significant wildlife value and refuge habitat. The conservation area includes tidal marshlands and mangroves along part of the lakes edge. The Melaleuca boardwalk allows viewing of the wildlife. The mangroves are home to frogs, crabs and fish that attract native and migratory birds. There are guided bushwalking and canoeing activities as part of community conservation and environmental workshops to promote local conservation.\n\nSituated near the suburb of , from the coast and northwest of , the lake borders the Ivan Gibbs Wetland Reserve, is classified as a Fish Habitat Area within the Moreton Bay Marine Park and serves as an important wildlife corridor between the Nerang State Forest and the coast.\n\nThe lake is fed by Coombabah Creek which rises to the west in the Nerang Forest Reserve. The Ivan Gibbs Wetlands Reserve and Lakeside Country Club golf course are both located to the south of the lake. Houses in Helensvale have been built a short distance from the lake's western shore.\n\n\n"}
{"id": "2119658", "url": "https://en.wikipedia.org/wiki?curid=2119658", "title": "Crystallinity", "text": "Crystallinity\n\nCrystallinity refers to the degree of structural order in a solid. In a crystal, the atoms or molecules are arranged in a regular, periodic manner. The degree of crystallinity has a big influence on hardness, density, transparency and diffusion. In a gas, the relative positions of the atoms or molecules are completely random. Amorphous materials, such as liquids and glasses, represent an intermediate case, having order over short distances (a few atomic or molecular spacings) but not over longer distances.\n\nMany materials, such as glass-ceramics and some polymers, can be prepared in such a way as to produce a mixture of crystalline and amorphous regions. In such cases, crystallinity is usually specified as a percentage of the volume of the material that is crystalline. Even within materials that are completely crystalline, however, the degree of structural perfection can vary. For instance, most metallic alloys are crystalline, but they usually comprise many independent crystalline regions (grains or crystallites) in various orientations separated by grain boundaries; furthermore, they contain other crystallographic defects (notably dislocations) that reduce the degree of structural perfection. The most highly perfect crystals are silicon boules produced for semiconductor electronics; these are large single crystals (so they have no grain boundaries), are nearly free of dislocations, and have precisely controlled concentrations of defect atoms.\n\nCrystallinity can be measured using x-ray crystallography, but calorimetric techniques are also commonly used.\n\nGeologists describe four qualitative levels of crystallinity:\n\nOxford dictionary of science, 1999, .\n"}
{"id": "313739", "url": "https://en.wikipedia.org/wiki?curid=313739", "title": "Current solar income", "text": "Current solar income\n\nThe current solar income of the Earth, or an ecozone or ecoregion or any area, is the amount of solar energy that falls on it as sunlight. This is thought important in some branches of green economics, as the ultimate measure of renewable energy. \n\nBuckminster Fuller first described the concept in his 1970 paper \"Cosmic Costing\", contrasting the photosynthesis on which natural capital and sustainable infrastructural capital depend, with the chemosynthesis of extracting and using fossil fuels.\n\nPaul Hawken is a more recent advocate of the concept, and views it as central to his notion of a restorative economy. It remains a popular notion among those who believe that toxic waste and maintenance problems of direct solar energy devices can ultimately be overcome, or that yields of passive or biological means of gathering and using this energy as biofuels can be made to approximate those of fossil fuels.\n\n"}
{"id": "33895300", "url": "https://en.wikipedia.org/wiki?curid=33895300", "title": "Energy in Africa", "text": "Energy in Africa\n\nEnergy in Africa describes energy production, consumption and import in Africa.\n\nEnergy use and development varies widely across the continent, with some African countries exporting energy to neighbors or the global market while others lack even basic infrastructures, or systems to acquire energy. The World Bank has declared 32 of the 48 nations on the continent to be in an energy crisis. Energy development has not kept pace with rising demand in developing regions, placing a large strain on the continent's existing resources over the first decade of the new century. From 2001 to 2005, GDP for over half of the countries in Sub Saharan Africa rose by over 4.5% annually, while generation capacity grew at a rate of 1.2%.\n\nAccording to the World Development Report published by the World Bank in 2012, Africa’s economy is about the size of the Netherlands' economy, which is equivalent to only approximately six percent of the U.S. economy. Akin Iwayemi, a professor at University of Ibadan in Nigeria, suggests that there is potentially a “strong feedback relationship between the energy sector and the national economy.” Determining socio-economic indicators in Africa include population, population density, land area, proportion of citizens living in an urban setting, and Gross Domestic Product (GNP) per capita.\n\nOverall, the African continent is a net energy exporter. In 2009 the net energy export was 40% of the energy production 13,177 TWh. The world share of energy production in Africa was 12% of oil and 7% of gas in 2009.\n\nEnergy in Africa is a scarcer commodity than in the developed world – annual consumption is 518 KWh in Sub-Saharan Africa, the same amount of electricity used by an individual in an Organization for Economic Cooperation and Development (OECD – example is the U.S.) country in 25 days. More than 500 million people live without electricity. Across the continent only 10% of individuals have access to the electrical grid, and of those, 75% come from the richest two quintiles in overall income. Less than 2% of the rural populations of Malawi, Ethiopia, Niger, and Chad have access to electrical power. Electrical provisioning in Africa has generally only reached wealthy, urban middle class, and commercial sectors, bypassing the region’s large rural populations and urban poor. According to the forum of Energy Ministers of Africa, most agriculture still relies primarily on humans and animals for energy input. The electrical industry in Africa faces the economic paradox that raising prices will prohibit access to their services, but that they cannot afford to roll out additional infrastructure to drive prices down and increase access without additional capital.\n\nOverall rates of access to energy in Africa have held constant since the 1980s, while the rest of the developing world has seen electrical grid distribution increase by 20%. Sub-Saharan Africa is the only region in the world where per-capita access rates are falling. According to recent trends, over 60% of Sub-Saharan Africans will still lack access to electricity by 2020.\n\nMoreover, Africa has an average electrification rate of 24%, while the rate in the rest of the developing world lies closer to 40%. Even in the areas covered by the electrical grid, power is often unreliable: the manufacturing sector loses power on average 56 days out of the year. In Senegal power is out 25 days a year, in Tanzania 63 days, and in Burundi 144 days. Frequent power outages cause damage to sales, equipment, and discourage international investment. According to the periodical African Business, “Poor transport links and irregular power supplies have stunted the growth of domestic companies and discouraged foreign firms from setting up manufacturing plants in the continent.\"\n\nDespite its unreliability, electrical service in Sub-Saharan Africa also costs more than in other parts of the world. The protective tariff required in Sub-Saharan Africa is $0.13 USD, compared to $0.04–0.08 USD in the rest of the developing world. Additionally, one of the greatest challenges in an effort to create sustainable development in Africa is that many countries with exportable resources are land-locked without a system of transportation.\n\nAlthough Africa lacks a sufficient transport system, new developments in industry and manufacturing have resulted in tremendous population growth, increased urbanization, high energy consumption, over-cultivation of lands, and significant industrial advancements engendered by globalization. Professor Iwayemi of University of Ibadan in Nigeria states that the “fundamental energy question facing Africa [is]…providing and maintaining widespread access of the population to reliable and affordable supplies of environmentally cleaner energy to meet the requirements of rapid economic growth and improved living standards.” In correspondence to The Africa Society, the population growth of sub-Saharan Africa is 2.2% annually; therefore, by 2025, it is estimated that Africa will consist of over a billion people. If this mathematical model is correct environmental problems could double or even triple by 2025.\n\nPerhaps a confounding variable of these trends is that less than 1% of the electricity generated in the Africa originates from renewable resources, as the \"White Paper on Energy Policy\" proclaims. The main objectives of the paper were to “increase access to affordable energy services, improve energy governance, stimulate economic growth, manage energy-related environmental impacts, and ensure security of supply through diversification.”\n\nThe African continent features many sustainable energy resources, of which only a small percentage have been harnessed. 5–7% of the continent’s hydroelectric potential has been tapped, and only 0.6% of its geothermal. The publication \"Energy Economics\" estimates that replacing South African coal power with hydroelectric imported from the Democratic Republic of the Congo could save 40 million tons of carbon dioxide emissions annually. 2011 estimates place African geothermal capacity at 14,000 MW, of which only 60 MW has been tapped. The African Energy Policy Research Network calculates that biomass from agricultural waste alone could meet the present electrical needs of 16 south eastern countries with bagasse-based cogeneration. The sugar industry in Mauritius already provides 25% of the country’s energy from byproduct cogeneration, with the potential for up to 13 times that amount with a widespread rollout cogeneration technology and process optimization.\n\nAccording to Stephen Karekezi, Director of African Energy Policy Research Network and co-worker Waeni Kithyoma, Africa is third largest in crude oil reserves (behind the Middle East and Latin America), third largest in natural gas resources (behind the Middle East and Europe), second greatest for uranium (behind Australia), and is plentiful in hydro energy potentials and other renewable energy, such as bio-energy and solar energy. Professor Iwayemi states that there are conventional energy sources in Africa: hydroelectric and wood fuels, coal lignite, crude oil, natural gas and nuclear fuels, and there are unconventional energy sources, such as solar, geothermal, biomass, oil and tar sands, wind energy and tidal energy from the influence of the sea.\n\nIn addition, South Africa alone obtains the sixth largest coal reserves on the planet, after China, the US, India, Russia and Australia. Specific renewable resources in South Africa include solar, wind, hydropower, wave energy, and bio-energy.\n\nProfessor Iwayemi suggests that “commercial energy use remains a key factor in human development.” Commercial energy can include solar powered systems and the like.\n\nIn addition, The Africa Society admits that much of Africa’s apparent facilitation of poverty is the result of degradation of agriculture and arable lands, as well as, the mismanagement of water resources. A large contributing factor to these events and others, such as famine, is deforestation. Clean energy potentiality in Africa could therefore reduce environmental degradation, and consequently, poverty.\n\nTo provide an example, implementation of biodiesel technology has potential for the creation of jobs, as well as consequent economic development in disadvantaged rural communities. This form of clean energy also enables energy security for many nations throughout the globe including those in Africa, and reduces greenhouse gas emissions rather significantly.\n\nSen, an Indian economist, has assimilated a concept referred to as \"The Capabilities Approach,\" in which he suggests that “poverty can be sensibly identified in terms of capability deprivation.” Further, he states that “relative deprivation in terms of incomes can yield absolute deprivation in terms of capabilities.” He believes that the freedom to achieve well-being is of great importance, and can lead to increased capabilities. Energy could facilitate a great deal of freedom, for individuals could gain access to a wide variety of resources.\n\nThe high upfront capital cost of many resources, particularly renewable resources, is one of the most critical barriers to the development of an energy market. Other challenges include the lack of food security and limited water resources, for these factors are necessary for life and therefore take priority over energy initiation.\n\nNorth Africa is dominant in oil and in gas, for Libya obtains approximately 50% of the oil reserves available in Africa. Libya designated USD $5 billion to assert programs and regulations that will reduce carbon emissions. Resources, such as oil and gas, are also prevalent in Algeria, in addition to natural gas. According to the Renewable Energy Sector in N. Africa, solar capacity is also extremely relevant in North Africa. The total power installed in North Africa region was roughly 61.6 GW in 2012. This is mostly made up of hydro accounting for nearly 10%.\n\nSouthern Africa has 91 percent of all of Africa’s coal reserves and 70% of the nuclear/uranium resources in Africa, according to Professor Iwayemi. Southern Africa follows Central Africa closely in hydro resources; hydroelectric potential can particularly be found in the Congo DRC, Mozambique, Zambia, Cameroon, Ethiopia, Sudan, and Nigeria. Mozambique in particular has joined an international initiative to develop an energy action plan, to contribute to \"Sustainable Energy for All\".\n\nIn accordance with The Africa Society, USAID’s Living in the Finite Environment program has helped form 15 protected areas in Southern Africa, encompassing nearly 40,000 community members, known as conservancies in Namibia.\n\nThe country of South Africa alone obtains the sixth largest coal reserves on the planet, after China, the US, India, Russia and Australia. Specific renewable resources in South Africa include solar, wind, hydro power, wave energy, and bio-energy.\n\n, Zambia is in a massive power crisis that began in June. In Lusaka the eight-hour blackouts cause families to cook on a simple charcoal fire. The reason for this is poor rainfall, causing less hydroelectric output.\n\nThe Africa Society portrays that promotion of sustainable use of natural resources is occurring in Kenya and in Uganda; Kenya and Uganda are “improving community-based wildlife management, strengthening forestry and environmental management, and enhancing integrated coastal zone management…[this] reduces conflicts between communities and protected areas by promoting access rights, revenue sharing…,” etc. Kenya also organized an instrumental energy plan to support development and economic growth.\n\nIn Tanzania, the goal is to conserve biodiversity. “The USAID supports local actions in the Pangani, Bagamoyo, and Mkuranga districts that promote sustainable coastal and marine resources management through co-management for near-shore fishery areas, small-scale enterprise development, marine culture, and coastal tourism.” There is also an essential push for geothermal power in East Africa, given the arid climate.\n\nNorway also supports the replacement of kerosene lamps with alternatives facilitated from solar power in Kenya, access to energy in Ethiopia’s rural areas for job growth and a better standard of living, and Liberia’s implementation of a climate plan.\n\nElectricity access in Ghana increased 500% between 1991 and 2000, but per capita consumption actually fell over the same period, suggesting electricity usage was unaffordable. Ghana was also one of the primary countries to develop an energy action plan, in response to the initiative for Sustainable Energy for All.\n\nNigeria is currently a dumping ground for electronic products, which leach toxic metals and substances such as lead, mercury, cadmium, arsenic, antimony, and trioxide into water sources. When burned, carcinogenic dioxins and polyaromatic hydrocarbons are emitted and toxic chemicals like barium are transmitted into the soil. The 1989 Basel Convention established an international treaty designed to regulate hazardous waste from being dumped into the developing world.\n\nIn reflection of statements made in Prof. Iwayemi’s essay, West Africa does have some coal reserves – approximately 10 percent of coal in Africa, particularly Nigeria. West Africa also exhibits some nuclear resources. In addition to coal reserves, Nigeria contains natural gas and oil resources.\n\n“In Guinea, West Africa, the US is making significant input in the area of environmental protection.” These progressive steps will improve agricultural production technologies and exchange trade opportunities. “In Guinea 115,000 hectares of forests and tree plantations have been placed under sustainable management…USAID has assisted more than 37,000 farmers to improve agricultural production through sustainable management practices, and has helped establish over 2,800 new businesses.”\n\nCentral Africa has abundant hydro-electric resources due to the multiple rivers that run through this region. The publication Energy Economics estimates that replacing South African coal power with hydroelectric imported from the Democratic Republic of the Congo could save 40 million tons of carbon dioxide emissions annually.\n\nAfrica is protecting forest resources. “USAID will contribute approximately $48 million to partnership through its successful Central African Regional Program for the Environment (CARPE)…goal is to improve forest governance, develop sustainable means of livelihood for 60 million people who live in the Basin, reduce the rate of forest degradation and loss of biodiversity through protected area management, improve logging policies, and achieve sustainable forest use by local inhabitants.”\n\nThe utilization of solar water heaters and biodiesel resources in South Africa within recent years reveals that renewable energy can significantly diminish poverty, for the implementation of clean energy systems has led to improvement in health, and general welfare of the people. The assimilation of these programs also generates employment, and develops empowerment of the people due to a localized level of energy operation.\n\n\"Since the turn of the century, the Shri Kshetra Dharmasthala Rural Development Project (SKDRDP) has extended micro-credits for renewable energy projects for a total of $3.2 million to poor farmers in the South Indian state of Karnataka. The credits paid for the installation of almost 20,000 biogas plants, solar home systems, improved cooking stoves and family-size pico-hydropower plants\".\n\nA major concern in Southern Africa is addressing the inequalities associated with the aftermath of the apartheid era. There are also several other factors or occurrences in Africa that lead to inequality, such as one's location of residence (urban vs. rural), one's access to food, water, and energy, and one's freedom to achieve well-being. The Human Development Report of 2013 suggests that the regions with the largest gender inequality index values are West and Central Africa; Liberia has the highest at an index of 143, followed by Central African Republic (142), Mauritania (139), Côte d'Ivoire (138), and Cameroon (137).\n\nEnergy can facilitate the development of schools, and help teachers gain access to a wide variety of teaching mechanisms, such as potentially computers. Energy can contribute to the allowance for freedom of education.\n\nAs a whole, foreign direct investment into Africa has been low. According to the Forum of Energy Ministers of Africa, Africa as a whole receives less than 2% of foreign direct investment across the world. A survey of 20 Poverty Reduction Strategy Papers prepared by countries across Africa found that most neglect to consider energy or individual energy access as an integral part of their development strategy. Trans-national initiatives play an important role in development for the entire region too. One example of international cooperation for energy development is the Chad-Cameroon pipeline.\n\nAdditionally, “The United States announced USD $2 billion in grants, loans and loan guarantees across U.S. government agencies and departments for capacity-building projects, policy and regulatory development, public-private partnerships, and loan guarantees to leverage private investment in clean energy technologies.”\n\nEskom and Duke Energy currently support an initiative to facilitate an electrical roadmap in Southern Africa. “The goal is to connect 500 million people to modern energy service by 2025\".\n\nEconomic reasoning predicts increased efficiency and lower government overhead from a private contractor. Privatized Northern Electricity in Namibia implemented improved billing and reduced losses to lower the required energy tariff and lower energy prices. Private companies can also work closely with government to provide the social benefits of a state utility in the short term and the competition of a private market for the long term. South Africa commercialized the formerly public utility Eskom, but worked with them to continue grid expansion. The South African government helps fund new connections and subsidizes the first 100kWh per month for poor households, up from a previous 50kWh per month. By 2005 South Africa's electrification rate had increased to about 70% (from 30% in 1990).\n\nPrivatization can lead to price increases if the market is not otherwise profitable. An unregulated or lightly regulated market could tend towards proven profitable customers too, ignoring riskier opportunities to expand service to rural or poor customers. Extending the electrical grid becomes difficult because of the high upfront investments required to serve a low population density. According to the Forum of Energy Ministers of Africa, most rural customers can't even afford the install costs of the most basic single phase circuit with an electrical socket. Energy subsidies are one possible solution, but they can disproportionately effect demographics who already have access to electricity, missing the most poor.\n\nMost development initiatives work at the individual project level, leading to fragmentation and inconsistency between projects that discourage long term investors. Institutional reform is vital to improving the operating efficiency of the electrical sector as a whole. The current hybrid public/private model lacks a clear leading organization with one clear vision of the system's future. Attempts to negotiate management contracts over utility hardware have generally failed, leaving the public utility still burdened with day-to-day hardware support as well as growth, planning, and development. Of 17 high profile African energy management contracts, four were cancelled before they even reached full term, five were not renewed after only one cycle, and five more were dropped in later years. Only three remain in place today.\n\nSmart utility management must meet increased investment in the middle to smartly allocate resources and develop a sustainable grid model. Of the current utilities, \"On average, Africa's state-owned power utilities embody only 40% of good governance practices for such enterprise\".\n\nNevertheless, federal support for energy is gaining momentum, especially in Southern Africa. South Africa’s government has established a Joint Implementation Committee to progress the biodiesel industry. This committee encompasses a variety of sub-committees, like “South Renewable Energy Technologies for Poverty Alleviation South Africa: Solar Water Heaters and Biodiesel,” and the “African Petroleum Industry Association. Plans for the promotion of harvest to create bioethanol are underway, the South African Bureau of Standards is developing pricing models to enable economic growth.\n\n“The World Bank and the International Finance Corporation will expand existing programs such as Lighting Africa, which develops off-grid lighting markets, to provide affordable lighting to 70 million low-income households by 2020, as well as undertake new initiatives with the Energy Sector Management Assistance Program, such as mapping of renewable energy resources”.\n\nThe Global Ministerial Environment Forum in Nairobi, Kenya was broadcast throughout Africa, and comprised a panel of energy experts who discussed the successes achieved in energy in Africa so far, lessons learned from implementations, and future projections for energy.\n\nMoreover, the United Nations Development Program and UN Capital Development Fund recently initiated a global Clean Start program, which will enable millions of impoverished people both in Africa and in Asia to shift out of energy poverty by creating microfinance opportunities to encourage poorer individuals to purchase and utilize electricity. Twenty-five countries in Africa have joined this global task: Botswana, Burundi, Burkina Faso, Cape Verde, Côte d'Ivoire, Democratic Republic of Congo, Ethiopia, Gambia, Ghana, Guinea, Kenya, Lesotho, Liberia, Malawi, Mozambique, Namibia, Nigeria, São Tomé and Principe, Senegal, Sierra Leone, Tanzania, Togo, Uganda, Zambia, and Zimbabwe.\n\nThe World Bank operates a portfolio of 48 Sub-Saharan Africa energy projects with over $3 billion USD in investment. Individual governments as well as private entities also contribute to overall energy projects. China and India have recently emerged as large players in the space, committing $2 billion USD annually to new development projects. China focused specifically on 10 large hydropower projects, which combined are expected to produce another 6,000 MW of electrical energy. This is estimated to increase the hydroelectric capabilities of Sub-Saharan Africa by 30%. Another project currently undergoing feasibility exploration would install hydroelectric facilities on the Zambezi river, potentially generating 2,000–2,500 MW. Smaller scale projects also receive funding, such as efforts to distribute safe cookstoves and efficient kilns to lower the effects of biomass, initiatives to improve lighting efficiency, or smaller scale microgrid electrical distributions.\n\nOne characterizing feature of the electrical grid in Africa is its isolation. The formation of regional energy trading pools would help stabilize energy markets, but would require building out a transmission line infrastructure between countries. Installing those resources would be expected to require ~$19 billion USD in investment. Regional energy trade would save an estimated $5 billion USD annually in emergency generation costs, yielding a 22% rate of return even at 5% deflation. Energy economist Orvika Rosnes estimates that fair regional pooling in the least developed countries could actually generate money in less than one year, with a 168% annual return on investment.\n\nCreating an effective and far reaching modern electrical system in Africa will take significant investment. The African Development Bank has estimated that a universal access system for all 53 countries in Africa would cost $547 billion USD total to implement by 2030, which averages to $27 billion USD per year. Total investment has not come close to this mark, instead hovering until recently between $1–2 billion USD annually. Recent participation from China and India on the order of $2 Billion USD annually brings the investment total up to ~$4 billion USD. The power sector still faces a finance gap on the order of $23 billion USD per year though, severely constraining its development options. Operating at 1/4 of the necessary budget to grow and expand, current networks must mark most funds for maintenance of aging existing systems.\n\nAccess to modern forms of Energy can impact socio-economic parameters like income, education, and life expectancy. Energy can act as a multiplier of the Millennium Development Goals through its ability to stimulate economic growth to generate employment, improve educational opportunities, and improve general health compared to existing energy sources. Research of past successful development suggests that energy, especially from transportation and industry, helped drive growth and modernization\n"}
{"id": "52058583", "url": "https://en.wikipedia.org/wiki?curid=52058583", "title": "Energy system", "text": "Energy system\n\nAn energy system is a system primarily designed to supply energy-services to end-users. Taking a structural viewpoint, the IPCC Fifth Assessment Report defines an energy system as \"all components related to the production, conversion, delivery, and use of energy\". The field of energy economics includes energy markets and treats an energy system as the technical and economic systems that satisfy consumer demand for energy in the forms of heat, fuels, and electricity.\n\nThe first two definitions allow for demand-side measures, including daylighting, retrofitted building insulation, and passive solar building design, as well as socio-economic factors, such as aspects of energy demand management and even telecommuting, while the third does not. Neither does the third account for the informal economy in traditional biomass that is significant in many developing countries.\n\nThe analysis of energy systems thus spans the disciplines of engineering and economics. Merging ideas from both areas to form a coherent description, particularly where macroeconomic dynamics are involved, is challenging.\n\nThe concept of an energy system is evolving as new regulations, technologies, and practices enter into service – for example, emissions trading, the development of smart grids, and the greater use of energy demand management, respectively.\n\nFrom a structural perspective, an energy system is like any general system and is made up of a set of interacting component parts, located within an environment. These components derive from ideas found in engineering and economics. Taking a process view, an energy system \"consists of an integrated set of technical and economic activities operating within a complex societal framework\". The identification of the components and behaviors of an energy system depends on the circumstances, the purpose of the analysis, and the questions under investigation. The concept of an energy system is therefore an abstraction which usually precedes some form of computer-based investigation, such as the construction and use of a suitable energy model.\n\nViewed in engineering terms, an energy system lends itself to representation as a flow network: the vertices map to engineering components like power stations and pipelines and the edges map to the interfaces between these components. This approach allows collections of similar or adjacent components to be aggregated and treated as one to simplify the model. Once described thus, flow network algorithms, such as minimum cost flow, may be applied. The components themselves can be treated as simple dynamical systems in their own right.\n\nConversely, relatively pure economic modeling may adopt a sectorial approach with only limited engineering detail present. The sector and sub-sector categories published by the International Energy Agency are often used as a basis for this analysis. A 2009 study of the UK residential energy sector contrasts the use of the technology-rich Markal model with several UK sectoral housing stock models.\n\nInternational energy statistics are typically broken down by carrier, sector and sub-sector, and country. Energy carriers ( energy products) are further classified as primary energy and secondary (or intermediate) energy and sometimes final (or end-use) energy. Published energy datasets are normally adjusted so that they are internally consistent, meaning that all energy stocks and flows must balance. The IEA regularly publishes energy statistics and energy balances with varying levels of detail and cost and also offers mid-term projections based on this data. The notion of an energy carrier, as used in energy economics, is distinct and different from the definition of energy used in physics.\n\nEnergy systems can range in scope, from local, municipal, national, and regional, to global, depending on issues under investigation. Researchers may or may not include demand side measures within their definition of an energy system. The IPCC does so, for instance, but covers these measures in separate chapters on transport, buildings, industry, and agriculture.\n\nHousehold consumption and investment decisions may also be included within the ambit of an energy system. Such considerations are not common because consumer behavior is difficult to characterize, but the trend is to include human factors in models. Household decision-taking may be represented using techniques from bounded rationality and agent-based behavior. The American Association for the Advancement of Science (AAAS) specifically advocates that \"more attention should be paid to incorporating behavioral considerations other than price- and income-driven behavior into economic models [of the energy system]\".\n\nThe concept of an energy-service is central, particularly when defining the purpose of an energy system:\n\nEnergy-services can be defined as amenities that are either furnished through energy consumption or could have been thus supplied. More explicitly:\n\nA consideration of energy-services per capita and how such services contribute to human welfare and individual quality of life is paramount to the debate on sustainable energy. People living in poor regions with low levels of energy-services consumption would clearly benefit from greater consumption, but the same is not generally true for those with high levels of consumption.\n\nThe notion of energy-services has given rise to energy-service companies (ESCo) who contract to provide energy-services to a client for an extended period. The ESCo is then free to choose the best means to do so, including investments in the thermal performance and HVAC equipment of the buildings in question.\n\nISO13600, ISO13601, and ISO13602 form a set of international standards covering technical energy systems (TES). Although withdrawn prior to 2016, these documents provide useful definitions and a framework for formalizing such systems. The standards depict an energy system broken down into supply and demand sectors, linked by the flow of tradable energy commodities (or energywares). Each sector has a set of inputs and outputs, some intentional and some harmful byproducts. Sectors may be further divided into subsectors, each fulfilling a dedicated purpose. The demand sector is ultimately present to supply energyware-based services to consumers (see energy-services).\n\n"}
{"id": "41521903", "url": "https://en.wikipedia.org/wiki?curid=41521903", "title": "Francis Fletcher (priest)", "text": "Francis Fletcher (priest)\n\nFrancis Fletcher ( – ) was a priest of the Church of England who accompanied Sir Francis Drake on his circumnavigation of the world from 1577 to 1580 and kept a written account of it.\n\nMuch is known about Fletcher's three years of voyaging around the world with Drake, but there is little certain information about the rest of his life. John Venn identified Fletcher with a man of this name who entered Pembroke College, Cambridge, in 1574, but did not take a degree. He was briefly Rector of St Mary Magdalen, Milk Street, a parish of the City of London, resigning in July 1576 to join Drake in his preparation of a fleet for purposes which are still disputed. He acted as Drake's chaplain during the three-year voyage which ensued, keeping a journal of their adventures which he handed to Drake on the expedition's return to England in 1580.\n\nIn September 1578, Drake's own ship, the \"Golden Hind\", passed the Strait of Magellan amid storms, and Fletcher recorded that the ship was driven to the \"utmost island of Terra Incognita\". He made a map of \"Elizabeth Iland\" \"(pictured)\", which Fletcher and Drake claimed for England, naming it Elizabeth Island.\nFletcher was sometimes at odds with Drake. In a sermon he preached to the expedition in January 1580, Fletcher suggested that their ships' recent woes had resulted from the unjust death of Thomas Doughty, whom Drake had ordered to be beheaded on 2 July 1578. After the sermon, Drake had Fletcher chained to a hatch cover, then \"solemnly excommunicated him\".\n\nVenn states that Fletcher was Rector of Bradenham in Buckinghamshire from 1579 to 1592, but a later writer, David B. Quinn, points out that Fletcher was still overseas in 1579 and believes Venn has confused him with a man named Richard Fletcher. In 1593 Fletcher became Vicar of Tickhill, Yorkshire, and in 1605 he married Margaret Gallard, a widow. He may have died in 1619, when another man was appointed to his church benefice.\n\nFletcher's log of Drake's voyage formed the basis of \"The World Encompassed by Sir Francis Drake\", an account of the voyage published in London in 1628 at the instigation of Drake's nephew, another Francis Drake. A copy of the first part of Fletcher's journal was made by a man named John Conyers, described as \"Citizen and Apothecary of London\", about 1677, and this survives in the British Library, catalogued as \"Sloane MS 61, \"Francis Fletcher's Log\"\".\n\nFletcher is portrayed by Roger Adamson in the film \"Drake's Venture\" (1980).\n"}
{"id": "44489906", "url": "https://en.wikipedia.org/wiki?curid=44489906", "title": "Giovanni Jacopo Spada", "text": "Giovanni Jacopo Spada\n\nGiovanni Jacopo Spada (1680 - 1774) was an Italian naturalist and pioneering geologist, born in Verona, who served for many years as archipresbyter at Graz in Styria. He is known for the revised and expanded catalogue of his collection that he published in 1744, \"Corporum lapidefactorum agri veronensis catalogus quæ apud Joan. Jacobum Spadam Gretianae Archipresbyterum Aasservantur\" A first version, now very rare, had been published in 1739. His dissertation of 1737 at the University of Verona asserted that fossils, then known as \"petrifications\", of recognizably marine species, to be found in the mountains near Verona, were neither \"sports of Nature\" nor the products of the biblical Great Deluge, but \"antediluvian\". He was listed among the subscribers to the corpus of Latin inscriptions assembled at Verona (and other north Italian locations), the \"Museum Veronense hoc est antiquarum inscriptionum atque anaglyphorum...\" (Verona 1749).\n\nThough Spada is barely remembered today, he was commemorated in the snail \"Candidula spadae\", described by Pietro Calcara in 1845.\n"}
{"id": "13858146", "url": "https://en.wikipedia.org/wiki?curid=13858146", "title": "Glacier foreland", "text": "Glacier foreland\n\nThe region between the current leading edge of the glacier and the moraines of latest maximum is called glacier foreland or glacier forefield. In the Alps this maximum was in 1850 and since then the region has become ice free due to deglaciation. Because of this relative recent development of vegetation and morphodynamic the glacier foreland differs considerably from the surrounding landscape.\n\n"}
{"id": "4563675", "url": "https://en.wikipedia.org/wiki?curid=4563675", "title": "Glacier ice accumulation", "text": "Glacier ice accumulation\n\nGlacier ice accumulation occurs through accumulation of snow and other frozen precipitation, as well as through other means including rime ice (freezing of water vapor on the glacier surface), avalanching from hanging glaciers on cliffs and mountainsides above, and re-freezing of glacier meltwater as superimposed ice. Accumulation is one element in the glacier mass balance formula, with ablation counteracting. With successive years in which accumulation exceeds ablation, then a glacier will experience positive mass balance, and its terminus will advance.\n\nGlaciologists subdivide glaciers into glacier accumulation zones, based on the melting and refreezing occurring. These zones include the dry snow zone, in which the ice entirely retains subfreezing temperatures and no melting occurs. Dry snow zones only occur within the interior regions of the Greenland and Antarctica ice sheets. Below the dry snow zone is the percolation zone, where some meltwater penetrates down into the glacier where it refreezes. In the wet snow zone, all the seasonal snow melts. The meltwater either percolates into the depths of the glacier or flows down-glacier where it might refreeze as superimposed ice. A glacier's equilibrium line is located at the lower limit of the wet snow zone.\n\n"}
{"id": "4186046", "url": "https://en.wikipedia.org/wiki?curid=4186046", "title": "Glacier mass balance", "text": "Glacier mass balance\n\nCrucial to the survival of a glacier is its mass balance or surface mass balance (SMB), the difference between accumulation and ablation (sublimation and melting). Climate change may cause variations in both temperature and snowfall, causing changes in the surface mass balance. Changes in mass balance control a glacier's long-term behavior and are the most sensitive climate indicators on a glacier. From 1980–2012 the mean cumulative mass loss of glaciers reporting mass balance to the World Glacier Monitoring Service is −16 m. This includes 23 consecutive years of negative mass balances.\nA glacier with a sustained negative balance is out of equilibrium and will retreat, while one with a sustained positive balance is out of equilibrium and will advance. Glacier retreat results in the loss of the low elevation region of the glacier. Since higher elevations are cooler than lower ones, the disappearance of the lowest portion of the glacier reduces overall ablation, thereby increasing mass balance and potentially reestablishing equilibrium. However, if the mass balance of a significant portion of the accumulation zone of the glacier is negative, it is in disequilibrium with the local climate. Such a glacier will melt away with a continuation of this local climate.\nThe key symptom of a glacier in disequilibrium is thinning along the entire length of the glacier. For example, Easton Glacier (pictured below) will likely shrink to half its size, but at a slowing rate of reduction, and stabilize at that size, despite the warmer temperature, over a few decades. However, the Grinnell Glacier (pictured below) will shrink at an increasing rate until it disappears. The difference is that the upper section of Easton Glacier remains healthy and snow-covered, while even the upper section of the Grinnell Glacier is bare, melting and has thinned. Small glaciers with shallow slopes such as Grinnell Glacier are most likely to fall into disequilibrium if there is a change in the local climate.\n\nIn the case of positive mass balance, the glacier will continue to advance expanding its low elevation area, resulting in more melting. If this still does not create an equilibrium balance the glacier will continue to advance. If a glacier is near a large body of water, especially an ocean, the glacier may advance until iceberg calving losses bring about equilibrium.\n\nThe different processes by which a glacier can gain mass are collectively known as accumulation. Snowfall is the most obvious form of accumulation. Avalanches, particularly in steep mountain environments, can also add mass to a glacier. Other methods include deposition of wind-blown snow; the freezing of liquid water, including rainwater and meltwater; deposition of frost in various forms; and the expansion of a floating area of ice by the freezing of additional ice to it. Snowfall is the predominant form of accumulation overall, but in specific situations other processes may be more important; for example, avalanches can be much more important than snowfall in small cirque basins.\n\nAccumulation can be measured at a single point on the glacier, or for any area of the glacier. The units of accumulation are meters: 1 meter accumulation means that the additional mass of ice for that area, if turned to water, would increase the depth of the glacier by 1 meter.\n\nAblation is the reverse of accumulation: it includes all the processes by which a glacier can lose mass. The main ablation process for most glaciers that are entirely land-based is melting; the heat that causes melting can come from sunlight, or ambient air, or from rain falling on the glacier, or from geothermal heat below the glacier bed. Sublimation of ice to vapor is an important ablation mechanism for glaciers in arid environments, high altitudes, and very cold environments, and can account for all the surface ice loss in some cases, such as the Taylor Glacier in the Transantarctic Mountains. Sublimation consumes a great deal of energy, compared to melting, so high levels of sublimation have the effect of reducing overall ablation.\n\nSnow can also be eroded from glaciers by wind, and avalanches can remove snow and ice; these can be important in some glaciers. Calving, in which ice detaches from the snout of a glacier that terminates in water, forming icebergs, is a significant form of ablation for many glaciers.\n\nAs with accumulation, ablation can be measured at a single point on the glacier, or for any area of the glacier, and the units are meters.\n\nGlaciers typically accumulate mass during part of the year, and lose mass the rest of the year; these are the \"accumulation season\" and \"ablation season\" respectively. This definition means that the accumulation rate is greater than the ablation rate during the accumulation season, and during the ablation season the reverse is true. A \"balance year\" is defined as the time between two consecutive minima in the glaciers mass—that is, from the start of one accumulation season through to the start of the next. The snow surface at these minima, where snow begins to accumulate again at the start of each accumulation season, is identifiable in the stratigraphy of the snow, so using balance years to measure glacier mass balance is known as the stratigraphic method. The alternative is to use a fixed calendar date, but this requires a field visit to the glacier each year on that date, and so it is not always possible to strictly adhere to the exact dates for the fixed year method.\n\nThe mass balance of a glacier is the net change in its mass over a balance year or fixed year. If accumulation exceeds ablation for a given year, the mass balance is positive; if the reverse is true, the mass balance is negative. These terms can be applied to a particular point on the glacier to give the \"specific mass balance\" for that point; or to the entire glacier or any smaller area.\n\nFor many glaciers, accumulation is concentrated in winter, and ablation in the summer; these are referred to as \"winter-accumulation\" glaciers. For some glaciers, the local climate leads to accumulation and ablation both occurring in the same season. These are known as \"summer-accumulation\" glaciers; examples are found in the Himalayas and Tibet. The layers that make winter-accumulation glaciers easy to monitor via the stratigraphic method are not usable, so fixed date monitoring is preferable.\n\nFor winter-accumulation glaciers, the specific mass balance is usually positive for the upper part of the glacier—in other words, the accumulation area of the glacier is the upper part of its surface. The line dividing the accumulation area from the ablation area—the lower part of the glacier—is called the equilibrium line; it is the line at which the specific net balance is zero. The altitude of the equilibrium line, abbreviated as ELA, is a key indicator of the health of the glacier; and since the ELA is usually easier to measure than the overall mass balance of the glacier it is often taken as a proxy for the mass balance.\n\nThe most frequently used standard variables in mass-balance research are:\n\n\nBy default, a term in lower case refers to the value at a specific point on the glacier's surface; a term in upper case refers to the value across the entire glacier.\n\nTo determine mass balance in the accumulation zone, snowpack depth is measured using probing, snowpits or crevasse stratigraphy. Crevasse stratigraphy makes use of annual layers revealed on the wall of a crevasse. Akin to tree rings, these layers are due to summer dust deposition and other seasonal effects. The advantage of crevasse stratigraphy is that it provides a two-dimensional measurement of the snowpack layer, not a point measurement. It is also usable in depths where probing or snowpits are not feasible. In temperate glaciers, the insertion resistance of a probe increases abruptly when its tip reaches ice that was formed the previous year. The probe depth is a measure of the net accumulation above that layer. Snowpits dug through the past winters residual snowpack are used to determine the snowpack depth and density. The snowpack's mass balance is the product of density and depth. Regardless of depth measurement technique the observed depth is multiplied by the snowpack density to determine the accumulation in water equivalent. It is necessary to measure the density in the spring as snowpack density varies. Measurement of snowpack density completed at the end of the ablation season yield consistent values for a particular area on temperate alpine glaciers and need not be measured every year. In the ablation zone, ablation measurements are made using stakes inserted vertically into the glacier either at the end of the previous melt season or the beginning of the current one. The length of stake exposed by melting ice is measured at the end of the melt (ablation) season. Most stakes must be replaced each year or even midway through the summer.\n\nNet balance is the mass balance determined between successive mass balance minimums. This is the stratigraphic method focusing on the minima representing a stratigraphic horizon. In the northern mid-latitudes, a glacier's year follows the hydrologic year, starting and ending near the beginning of October. The mass balance minimum is the end of the melt season. The net balance is then the sum of the observed winter balance (bw) normally measured in April or May and summer balance (bs) measured in September or early October.\n\nAnnual balance is the mass balance measured between specific dates. The mass balance is measured on the fixed date each year, again sometime near the start of October in the mid northern latitudes.\n\nGeodetic methods are an indirect method for the determination of mass balance of glacier. Maps of a glacier made at two different points in time can be compared and the difference in glacier thickness observed used to determine the mass balance over a span of years. This is best accomplished today using Differential Global Positioning System. Sometimes the earliest data for the glacier surface profiles is from images that are used to make topographical maps and digital elevation models. Aerial mapping or photogrammetry is now used to cover larger glaciers and icecaps such found in Antarctica and Greenland, however, because of the problems of establishing accurate ground control points in mountainous terrain, and correlating features in snow and where shading is common, elevation errors are typically not less than 10 m (32 ft). Laser altimetry provides a measurement of the elevation of a glacier along a specific path, e.g., the glacier centerline. The difference of two such measurements is the change in thickness, which provides mass balance over the time interval between the measurements. Again a good method over a span of time but not for annual change detection. The value of geodetic programs is providing an independent check of traditional mass balance work, by comparing the cumulative changes over ten or more years.\n\nMass balance studies have been carried out in various countries worldwide, but have mostly conducted in the Northern Hemisphere due to there being more mid-latitude glaciers in that hemisphere. The World Glacier Monitoring Service annually compiles the mass balance measurements from around the world. From 2002–2006, continuous data is available for only 7 glaciers in the southern hemisphere and 76 glaciers in the Northern Hemisphere. The mean balance of these glaciers was its most negative in any year for 2005/06. The similarity of response of glaciers in western North America indicates the large scale nature of the driving climate change.\n\nThe Taku Glacier near Juneau, Alaska has been studied by the Juneau Icefield Research Program since 1946, and is the longest continuous mass balance study of any glacier in North America. Taku is the world's thickest known temperate alpine glacier, and experienced positive mass balance between the years 1946 and 1988, resulting in a huge advance. The glacier has since been in a negative mass balance state, which may result in a retreat if the current trends continue. The Juneau Icefield Research Program also has studied the mass balance of the Lemon Creek Glacier since 1953. The glacier has had an average annual balance of −0.44 m per year from 1953–2006, resulting in a mean loss of over 27 m of ice thickness. This loss has been confirmed by laser altimetry.\n\nThe mass balance of Hintereisferner and Kesselwandferner glaciers in Austria have been continuously monitored since 1952 and 1965 respectively. Having been continuously measured for 55 years, Hintereisferner has one of the longest periods of continuous study of any glacier in the world, based on measured data and a consistent method of evaluation. Currently this measurement network comprises about 10 snow pits and about 50 ablation stakes distributed across the glacier. In terms of the cumulative specific balances, Hintereisferner experienced a net loss of mass between 1952 and 1964, followed by a period of recovery to 1968. Hintereisferner reached an intermittent minimum in 1976, briefly recovered in 1977 and 1978 and has continuously lost mass in the 30 years since then. Total mass loss has been 26 m since 1952 Sonnblickkees Glacier has been measured since 1957 and the glacier has lost 12 m of mass, an average annual loss of −0.23 m per year.\n\nGlacier mass balance studies have been ongoing in New Zealand since 1957. Tasman Glacier has been studied since then by the New Zealand Geological Survey and later by the Ministry of Works, measuring the ice stratigraphy and overall movement. However, even earlier fluctuation patterns were documented on the Franz Josef and Fox Glaciers in 1950. Other glaciers on the South Island studied include Ivory Glacier since 1968, while on the North Island, glacier retreat and mass balance research has been conducted on the glaciers on Mount Ruapehu since 1955. On Mount Ruapehu, permanent photographic stations allow repeat photography to be used to provide photographic evidence of changes to the glaciers on the mountain over time.\n\nAn aerial photographic survey of 50 glaciers in the South Island has been carried out for most years since 1977. The data was used to show that between 1976 and 2005 there was a 10% loss in glacier volume.\n\nThe North Cascade Glacier Climate Project measures the annual balance of 10 glaciers, more than any other program in North America, to monitor an entire glaciated mountain range, which was listed as a high priority of the National Academy of Sciences in 1983. These records extend from 1984–2008 and represent the only set of records documenting the mass balance changes of an entire glacier clad range. North Cascade glaciers annual balance has averaged −0.48 m/a from 1984–2008, a cumulative thickness loss of over 13 m or 20–40% of their total volume since 1984 due to negative mass balances. The trend in mass balance is becoming more negative which is fueling more glacier retreat and thinning.\n\nNorway maintains the most extensive mass balance program in the world and is largely funded by the hydropower industry. Mass balance measurements are currently (2012) performed on fifteen glaciers in Norway. In southern Norway six of the glaciers have been measured continuously since 1963 or earlier, and they constitute a west-east profile reaching from the maritime Ålfotbreen Glacier, close to the western coast, to the continental Gråsubreen Glacier, in the eastern part of Jotunheimen. Storbreen Glacier in Jotunheimen has been measured for a longer period of time than any other glacier in Norway, starting in 1949, while Engabreen Glacier at Svartisen has the longest series in northern Norway (starting in 1970). The Norwegian program is where the traditional methods of mass balance measurement were largely derived.\n\nThe Tarfala research station in the Kebnekaise region of northern Sweden is operated by Stockholm University. It was here that the first mass balance program was initiated immediately after World War II, and continues to the present day. This survey was the initiation of the mass balance record of Storglaciären Glacier, and constitutes the longest continuous study of this type in the world. Storglaciären has had a cumulative negative mass balance from 1946–2006 of −17 m. The program began monitoring the Rabots Glaciär in 1982, Riukojietna in 1985, and Mårmaglaciären in 1988. All three of these glaciers have had a strong negative mass balance since initiation.\n\nGlacier mass balance is measured once or twice annually on numerous stakes on the several ice caps in Iceland by the National Energy Authority. Regular pit and stake mass-balance measurements have been carried out on the northern side of Hofsjökull since 1988 and likewise on the Þrándarjökull since 1991. Profiles of mass balance (pit and stake) have been established on the eastern and south-western side of Hofsjökull since 1989. Similar profiles have been assessed on the Tungnaárjökull, Dyngjujökull, Köldukvíslarjökull and Brúarjökull outlet glaciers of Vatnajökull since 1992 and the Eyjabakkajökull outlet glacier since 1991.\n\nTemporal changes in the spatial distribution of the mass balance result primarily from changes in accumulation and melt along the surface. As a consequence, variations in the mass of glaciers reflect changes in climate and the energy fluxes at the Earth's surface. The Swiss glaciers Gries in the central Alps and Silvretta in the eastern Alps, have been measured for many years. The distribution of seasonal accumulation and ablation rates are measured in-situ. Traditional field methods are combined with remote sensing techniques to track changes in mass, geometry and the flow behaviour of the two glaciers. These investigations contribute to the Swiss Glacier Monitoring Network and the International network of the World Glacier Monitoring Service (WGMS).\n\nThe USGS operates a long-term \"benchmark\" glacier monitoring program which is used to examine climate change, glacier mass balance, glacier motion, and stream runoff. This program has been ongoing since 1965 and has been examining three glaciers in particular. Gulkana Glacier in the Alaska Range and Wolverine Glacier in the Coast Ranges of Alaska have both been monitored since 1965, while the South Cascade Glacier in Washington State has been continuously monitored since the International Geophysical Year of 1957. This program monitors one glacier in each of these mountain ranges, collecting detailed data to understand glacier hydrology and glacier climate interactions.\n\nThe GSC operates Canada's Glacier-Climate Observing System as part of its Climate Change Geoscience Program. With its University partners, it conducts monitoring and research on glacier-climate changes, water resources and sea level change using a network of reference observing sites located in the Cordillera and the Canadian Arctic Archipelago. This network is augmented with remote sensing assessments of regional glacier changes. Sites in the Cordillera include the Helm, Place, Andrei, Kaskakwulsh, Haig, Peyto, Ram River, Castle Creek, Kwadacha and Bologna Creek Glaciers; in the Arctic Archipelago include the White, Baby and Grise Glaciers and the Devon, Meighen, Melville and Agassiz Ice Caps. GSC reference sites are monitored using the standard stake based glaciological method (stratigraphic) and periodic geodetic assessments using airborne lidar. Detailed information, contact information and database available here: Helm Glacier (−33 m) and Place Glacier (−27 m) have lost more than 20% of their entire volume, since 1980, Peyto Glacier (−20 m) is close to this amount. The Canadian Arctic White Glacier has not been as negative at (−6 m) since 1980.\n\nThe glacier monitoring network in Bolivia, a branch of the glacio-hydrological system of observation installed throughout the tropical Andes mountains by IRD and partners since 1991, has monitored mass balance on Zongo (6000 m asl), Chacaltaya (5400 m asl) and Charquini glaciers (5380 m asl). A system of stakes has been used, with frequent field observations, as often as monthly. These measurements have been made in concert with energy balance to identify the cause of the rapid retreat and mass balance loss of these tropical glaciers.\n\nNowadays, glaciological stations exist in Russia and Kazakhstan. In Russia there are 2 stations: Glacier Djankuat in Caucasus,is located near the mountain Elbrus, and Glacier Aktru in Altai Mountains. In Kazakhstan there is glaciological station in Glacier Tuyuk-Su, in Tian Shan, is located near the city of Almaty\n\nA recently developed glacier balance model based on Monte Carlo principals is a promising supplement to both manual field measurements and geodetic methods of measuring mass balance using satellite images. The PTAA (precipitation-temperature-area-altitude) model requires only daily observations of precipitation and temperature collected at usually low-altitude weather stations, and the area-altitude distribution of the glacier. Output are daily snow accumulation (Bc) and ablation (Ba) for each altitude interval, which is converted to mass balance by Bn = Bc – Ba. Snow Accumulation (Bc) is calculated for each area-altitude interval based on observed precipitation at one or more lower altitude weather stations located in the same region as the glacier and three coefficients that convert precipitation to snow accumulation. It is necessary to use established weather stations that have a long unbroken records so that annual means and other statistics can be determined. Ablation (Ba) is determined from temperature observed at weather stations near the glacier. Daily maximum and minimum temperatures are converted to glacier ablation using twelve coefficients.\n\nThe fifteen independent coefficients that are used to convert observed temperature and precipitation to ablation and snow accumulation apply a simplex optimizing procedure. The simplex automatically and simultaneously calculates values for each coefficient using Monte Carlo principals that rely on random sampling to obtain numerical results. Similarly, the PTAA model makes repeated calculations of mass balance, minutely re-adjusting the balance for each iteration.\n\nThe PTAA model has been tested for eight glaciers in Alaska, Washington, Austria and Nepal. Calculated annual balances are compared with measured balances for approximately 60 years for each of five glaciers. The Wolverine and Gulkana in Alaska, Hintereisferner, Kesselwandferner and Vernagtferner in Austria. It has also been applied to the Langtang Glacier in Nepal. Results for these tests are shown on the GMB (glacier mass balance) website at ptaagmb.com. Linear regressions of model versus manual balance measurements are based on a split-sample approach so that the calculated mass balances are independent of the temperature and precipitation used to calculate the mass balance.\nRegression of model versus measured annual balances yield R values of 0.50 to 0.60. Application of the model to Bering Glacier in Alaska demonstrated a close agreement with ice volume loss for the 1972–2003 period measured with the geodetic method. Determining the mass balance and runoff of the partially debris-covered Langtang Glacier in Nepal demonstrates an application of this model to a glacier in the Himalayan Range.\n\nCorrelation between ablation of glaciers in the Wrangell Range in Alaska and global temperatures observed at 7000 weather stations in the Northern Hemisphere indicates that glaciers are more sensitive to the global climate than are individual temperature stations, which do not show similar correlations.\n\nValidation of the model to demonstrate the response of glaciers in Northwestern United States to future climate change is shown in a hierarchical modeling approach. Climate downscaling to estimate glacier mass using the PTAA model is applied to determine the balance of the Bering and Hubbard Glaciers and is also validated for the Gulkana, a USGS benchmark glacier.\n\n\n"}
{"id": "20188768", "url": "https://en.wikipedia.org/wiki?curid=20188768", "title": "Global administrative law", "text": "Global administrative law\n\nGlobal administrative law is an emerging field that is based upon a dual insight: that much of what is usually termed “global governance” can be accurately characterized as administrative action; and that increasingly such action is itself being regulated by administrative law-type principles, rules and mechanisms – in particular those relating to participation, transparency, accountability and review. GAL, then, refers to the structures, procedures and normative standards for regulatory decision-making including transparency, participation, and review, and the rule-governed mechanisms for implementing these standards, that are applicable to formal intergovernmental regulatory bodies; to informal intergovernmental regulatory networks; to regulatory decisions of national governments where these are part of or constrained by an international intergovernmental regime; and to hybrid public-private or private transnational bodies. The focus of this field is not the specific content of substantive rules, but rather the operation of existing or possible principles, procedural rules and reviewing and other mechanisms relating to accountability, transparency, participation, and assurance of legality in global governance.\n\nToday almost all human activity is subject to some form of global regulation. Goods and activities that are beyond the effective control of any one State are regulated at the global level. Global regulatory regimes cover a vast array of different subject-areas, including forest preservation, the control of fishing, water regulation, environmental protection, arms control, food safety and standardization, financial and accounting standards, internet governance, pharmaceuticals regulation, intellectual property protection, refugee protection, coffee and cocoa standards, labour standards, antitrust regulation, to name but a very few.\nThis increase in the number and scope of regulatory regimes has been matched by the huge growth of international organizations: nowadays over 2,000 intergovernmental organizations (IGO) and around 40,000 Non-governmental organizations (NGO) are operating worldwide.\n\nThere are, of course, great differences among the various different types of regulatory regimes. Some merely provide a framework for State action, whereas others establish guidelines addressed to domestic administrative agencies, and others still impact directly upon national civil society actors. Some regulatory regimes create their own implementation mechanisms, while others rely on national or regional authorities for this task. To settle disputes, some regulatory regimes have established judicial (or quasi-judicial) bodies, or refer to those of different regimes; while others resort to “softer” forms, such as negotiation. Within this framework, the traditional mechanisms based on State consent as expressed through treaties or custom are simply no longer capable of accounting for all global activities.\n\nA new regulatory space is emerging, distinct from that of inter-State relations, transcending the sphere of influence of both international law and domestic administrative law: this can be defined as the global administrative space. IOs have become much more than instruments of the governments of their Member States; rather, they set their own norms and regulate their field of activity; they generate and follow their own, particular legal proceedings; and they can grant participatory rights to subjects, both public and private, affected by their activities. Ultimately, they have emerged as genuine global public administrations.\nIn other words, the structures, procedures and normative standards for regulatory decision-making applicable to global institutions (including transparency, participation, and review), and the rule-governed mechanisms for implementing these standards are coming to form a specific field of legal theory and practice: that of global administrative law. The main focus of this emerging field is not the particular content of substantive rules generated by global regulatory institutions, but rather the actual or potential application of principles, procedural rules and reviewing and other mechanisms relating to accountability, transparency, participation, and the rule of law in global governance.\n\n\n\n\n\n"}
{"id": "2656049", "url": "https://en.wikipedia.org/wiki?curid=2656049", "title": "Grenen", "text": "Grenen\n\nGrenen is a long sandbar spit at Skagen Odde (the headland of Jutland), north of the town of Skagen. \n\n\"Grenen\" (The Branch) was named for its shape like a tree-branch, reaching out from the mainland. The beach of Grenen appears in many of the works of the Skagen Painters, a community that gathered there every summer between 1875 and the end of the 19th century. The area is also home to the Skagen Odde Nature Centre, designed by Jørn Utzon.\n\nNear the tip of the spit are two small museums: Skagen Bunker Museum and Grenens Kunstmuseum. Danish national road 40 passes through Grenen, and it is one of the most popular tourist destinations in the country, with approximately 2 million visitors each year.\n\nGrenen marks the junction between the strait of Skagerrak (part of the North Sea) and the Kattegat sea, and the turbulent colliding seas have created a 4-km long curved sandbar above and below the waves stretching east. The reef is still active and has grown about 1 km northeast towards Sweden over the last century, resulting in a mean annual growth rate of about 10 m. Because of the very strong currents, swimming there can be fatal and is prohibited in the waters around Grenen. \n\nThe area surrounding Grenen is the place with the greatest number of observed bird species in all of Denmark. Birdwatchers regard it as the best spot in Northern Europe, for observing birds of prey during their spring migrations. Birds often gather here before crossing the seas to Bohuslän in Sweden. There are more migratory birds near Grenen when the wind is from the south-east. If the wind is from the south-west, many birds choose a route across Funen and Zealand instead. The annual Skagen Birding Festival has been celebrated here since 2005, attracting more than a thousand visitors and participants.\n\nGrenen is also one of the best places in Denmark to observe sea mammals. Porpoises and common seals are very common here, and grey seals can be spotted here year round as well. As the area attracts many birdwatchers with binoculars, Grenen has also offered many whale sightings. The species most often reported are dolphins (especially white-beaked dolphins), northern minke whale and orcas. There have been isolated reports from Grenen of more exotic animals like walrus, hooded seal, etc.. \n\nScientists view Grenen as a laboratory on both land formation and botany, as new land is continuously being formed and shaped here, soon to be colonized by pioneering flora.\n\nSailors long feared Grenen, as many ships have run aground on the shallow reef through history. The first light signals was erected in 1561 on orders from King Frederik II after international pressure. It was not very effective and was not regularly attended, so from 1627 Skagen's Vippefyr, a coal- and wood-fired tipping lantern, replaced it until 1747, when Skagen's White Lighthouse was built. The 44-metre high Skagen's Grey Lighthouse has done the job since 1858, and in 1956 the 26-metre tall Skagen's West Lighthouse was added, making the heavy traffic in and out of Kattegat adequately safe.\n\n\n"}
{"id": "32993914", "url": "https://en.wikipedia.org/wiki?curid=32993914", "title": "International Cyanide Management Code", "text": "International Cyanide Management Code\n\nThe International Cyanide Management Code For The Manufacture, Transport and Use of Cyanide In The Production of Gold, commonly referred to as the Cyanide Code is a voluntary program designed to assist the global gold mining industry and the producers and transporters of cyanide used in gold mining in improving cyanide management practices, and to publicly demonstrate their compliance with the Cyanide Code through an independent and transparent process. The Cyanide Code is intended to reduce the potential exposure of workers and communities to harmful concentrations of cyanide‚ to limit releases of cyanide to the environment‚ and to enhance response actions in the event of an exposure or release.\n\nThe Cyanide Code was one of the earliest standards and certification programs developed for the minerals sector. Today, it is amongst the most established certification programs in the mining industry.\n\nThe program’s audit process and the transparency of audit results set it apart from other voluntary industry programs.\n\nCyanide is a general term for a group of chemicals containing carbon and nitrogen. Cyanide compounds include both naturally occurring and human-made chemicals. \n\nCyanide should be strictly controlled on mine sites, and proper management requires that certain precautions be taken to limit worker exposure and to prevent chemical solutions containing cyanide from entering the environment. \nThe most-used process of removing gold from ore is through leaching. In the leaching process, sodium cyanide is dissolved in water where, under mildly oxidizing conditions, it dissolves the gold contained in the crushed gold ore. The resultant gold-bearing solution is called 'pregnant solution.' Either zinc metal or activated carbon is then added to the pregnant solution to recover the gold by removing it from the solution. \nThere are two main leaching methods for gold extraction using cyanide. The first is 'heap' leaching in which the dilute cyanide solution is sprayed on large piles or heaps of coarse gold ore. The solution percolates through the pile dissolving the gold and the pregnant solution is then collected. This method is mainly used for ore with lower concentrations of gold. The other method is 'vat' leaching in which the process is similar but the gold ore is finely ground and leaching takes place in a tank or vat. Vat leaching is used mainly for ores with higher concentrations of gold due to the cost of milling the ore to a very small particle size. \nAlternative lixiviant (leaching) chemicals to cyanide (for example thiourea, sodium bromide) have been investigated for many years, but they are generally less effective and/or economical than cyanide, and they also present environmental risks that can be greater than cyanide. Because cyanide is toxic if not properly handled, its use is highly regulated in most countries.\n\nDespite regulatory and voluntary safeguards, however, some jurisdictions have banned its use in gold mining. These include Slovakia, the Czech Republic, Germany, and Hungary. Several provinces in Argentina also prohibit the use of cyanide in mining. In the U.S., the state of Montana has taken action to ban cyanide in gold production.\n\nCyanide, a highly toxic chemical, has been the most widely used reagent for extracting gold from ore for the past century. The Cyanide Code grew out of the first-of-its-kind workshop of multi-stakeholders held in Paris in May, 2000 convened to consider developing a code of best practice for the use of cyanide in gold mining in response to a tailings spill from the Aural Mine at Baia Mare in Romania in January 2000. The meeting was co-hosted by The United Nations Environment Programme (UNEP) and the International Council on Metals and the Environment (now the International Council on Mining and Metals). Workshop participants were almost 40 representatives of such diverse organizations as the Worldwide Fund for Nature, the Mineral Policy Center (now Earthworks), the Sierra Club, and the World Gold Council, along with representation from the U.S. Environmental Protection Agency, the governments of Australia, France, Hungary, Romania, and the world’s leading gold producers and cyanide producers.\n\nAs a result of the workshop, a multi-stakeholder steering committee was established to oversee development of a code of best practice for the management of cyanide used for gold recovery. The committee met five times over a 13-month period starting in late 2000, and each successive draft of the best practice document it produced was made available to the public on the UNEP web site with an open invitation for comments. The committee also solicited comments directly from 140 groups and individuals, including governments, NGOs, academics, consultants, industry, and financial institutions, and received 68 written responses and 15 stakeholder presentations at its meetings. In early 2002, the committee completed the \"International Cyanide Management Code for The Manufacture, Transport and Use of Cyanide In The Production of Gold\".\n\nAs conceived by the steering committee, the Cyanide Code was more than a guide to best management practices for the use of cyanide in the production of gold. The Cyanide Code also addressed the safe manufacture and transport of the cyanide used in the gold industry. Further, although the program is voluntary, the Cyanide Code includes a process by which its implementation at gold mines and other facilities is verified by independent third-party professional auditors and audit results are made available to the public. The International Cyanide Management Institute (ICMI) was established in 2003 to oversee the Cyanide Code’s implementation and verification, and by 2005, the administrative procedures, audit protocols and guidance documents necessary for full program implementation had been developed.\n\n2015 marked the tenth year of implementation of the Cyanide Code globally.\n\nIn 2017, the International Cyanide Management Institute undertook a public consultation process to determine if the Cyanide Code should be extended to include primary silver mines, which is defined as an operation where silver is the main commodity produced. About 30% of the world’s silver production comes from primary silver mines; the balance is produced as a co-product from polymetallic, base metal, or gold mines. The Institute solicited input from stakeholders on the proposal, including its advisability, policy or technical issues, and any other related matters. Following receipt of stakeholder comments, all of which were supportive of the change, ICMI’s Board of Directors approved the program’s expansion, effective January 1, 2017.\n\nThe Cyanide Code is a resource for any gold mine, cyanide producer or cyanide transporter regarding best practices for cyanide management. A company that becomes a signatory to the Cyanide Code commits to implement its Principles and Standards of Practice at its operations and to demonstrate compliance by having their facilities audited against the Cyanide Code’s Verification Protocols.\n\nThe first 14 Cyanide Code signatory companies were announced in November 2005. Over the past eight years, the number of companies participating in the program has substantially increased. As of January 1, 2018, the Cyanide Code had 195 signatory companies, with operations in 54 countries. These include 47 gold mining companies, 23 cyanide producers, and 125 cyanide transporters\nGold mines, cyanide production facilities, and cyanide transport operations owned by Cyanide Code signatory companies are certified through a transparent process using independent third-party professional auditors and technical experts meeting requirements established by ICMI for experience and expertise. The initial audit must be conducted within three years of the facility’s owner becoming a signatory, and audits to evaluate continuing compliance are conducted on a three-year interval. Operations are certified in compliance with the Cyanide Code based on the auditors’ findings, and a summary of the audit results, as well as the credentials of the auditors, are made available to the public on the Cyanide Code web site.\nThe program’s transparency gives stakeholders the ability to judge the rigor of the audit process and the audit findings. \n\nThe Cyanide Code is being widely implemented in the gold mining sector. As of January 1, 2018, 279 operations had been certified in compliance with the Cyanide Code, including 99 gold mines, 30 cyanide production facilities and 150 cyanide transporters. One hundred thirty-one had been audited two or more times and found to have maintained compliance.\n\nThe Cyanide Code has been recognized by the Group of Eight (\"G8\") as one of several certification systems that are suitable instruments for \"increasing transparency and good governance in the extraction and processing of mineral raw materials.\" The G8 is an international forum for the governments of Canada, France, Germany, Italy, Japan, Russia, the United Kingdom and the United States. Article 85 of the G8’s Summit Declaration, issued during its 2007 annual meeting, states the following:\n\n\"Certification systems can be a suitable instrument in appropriate cases for increasing transparency and good governance in the extraction and processing of mineral raw materials and to reduce environmental impacts, support the compliance with minimum social standards and resolutely counter illegal resource extraction. Therefore, we reaffirm our support for existing initiatives such as … the International Cyanide Management Code, and encourage the adaptation of the respective principles of corporate social responsibility by those involved in the extraction and processing of mineral resources,\"\n\nThe International Finance Corporation (IFC), a part of the World Bank that provides funding for mining projects, applies the Cyanide Code in lieu of its own requirements in its \"Environmental, Health and Safety (EHS) Guidelines for Mining\". As a condition of its loans, IFC EHS Guidelines call on mines to use cyanide in a manner \"consistent with the principles and standards of practice of the International Cyanide Management Code.\" The European Bank for Reconstruction and Development requires Cyanide Code compliance in their loan agreements to gold mines or otherwise encourage projects they fund that use cyanide to comply with the Cyanide Code.\n\nEnvironment Canada’s \"Environmental Code of Practice for Metal Mines\" cites the Cyanide Code as the guide for the environmentally responsible management of cyanide used in the production of both gold and base metals, and recommends that cyanide management planning and the transportation, storage, use and disposal of cyanide and cyanide-related materials be done \"in a manner consistent with practices described in the International Cyanide Management Code.\"\n\nThe Australian National Industrial Chemicals’ Notification and Assessment Scheme’s 2010 evaluation of the risks posed by sodium cyanide characterizes the Cyanide Code as \"an excellent initiative to lift international standards and demonstrate the environmental commitment of an operator.\"\n\nIn a 2015 report commissioned by the Dutch Ministry of Foreign Affairs, the research firm Profundo noted that \"The International Cyanide Management Code, to which all respectable mining companies subscribe, therefore governs not only the uses and storage of the chemical, but also its transport to a mine.\"\n\nAccording to \"Benchmark Study of Environmental and Social Standards in Industrialised Precious Metals Mining\" produced by Solidaridad, an international network organization with more than 20 years of experience in creating fair and sustainable supply chains from producer to consumer the Cyanide Code offers \"a good level of transparency as all of the Code’s implementing documents are available to the public on the ICMI website.\" The Study also noted the Cyanide Code offers \"rigorous and transparent verification of compliance, that there is \"respect for the Code by operational staff and excellent emergency procedures.\"\n\nIn a May 2013 report titled \"More Substance Than Shine\", several NGOs including Earthworks, MiningWatch Canada, and United Steelworkers noted that the International Cyanide Management Code is an example of a certification or monitoring system that\nprovides audit information to the public. \"For example, the following documents are available online: Auditor summary reports; Auditor Credential forms; Corrective Action Completion forms showing that those that did not obtain full compliance carried out the corrective actions necessary to obtain certification. Auditor summary reports also provide the basis for the findings or deficiencies identified during the audit.\"\n\nIn commenting on the stringent auditing process, a report titled \"Global Best Practices in Cyanide Management: The International Cyanide Management Code (ICMC) and Turkish Experience\" noted: \"In summary, the ICMC is one of the most rigorous voluntary auditing benchmarks ever to be applied in any international industry. ICMC auditing protocols are exacting, detailed, and transparent, and third party auditors are held to a very high standard with respect to their independence, auditing capabilities, and technical experience.\"\n\nThe Mining Certification Evaluation Project's final report concluded that \"third party certification schemes offer a credible means with which to assess and verify performance against agreed standards\", and noted that \"the Cyanide Code is a prominent example of an existing third-party certification scheme for the mining industry.\"\n\nIn an article in the Canadian Institute of \"Mining, Metallurgy and Petroleum\" May, 2015 magazine Kinross Gold Corporation’s Vice President of Environmental Affairs, Dean Williams, concurred saying that the Cyanide Code has become the new industry standard. “Any responsible gold miner of any size that elects not to become certified is actually making a statement that is contrary to most of the industry.\"\n\nAustralian regulators have credited reductions by the Australian gold mining industry in the incidence of environmental impacts, regulatory non-compliance and community resistance by complying with the Cyanide Code. It has been observed that globally there have been no major environmental incidents at a gold mining operation certified as compliant with the Cyanide Code.\n\nThe Responsible Jewellery Council, an international standards setting and certification organization for the jewelry supply chain, requires its gold mining members to have applicable sites certified in compliance to the Cyanide Code.\n\nThe Initiative for Responsible Mining Assurance (IRMA) is developing a system of best practice standards aimed at improving mining operations' social and environmental performance, with a goal of certifying mine sites in 2015. In developing its system, IRMA is building on the work of other initiatives including the Cyanide Code.\n\nBy adopting standards and practices that exceed state-imposed regulations and laws, the mining industry and financing institutions, provide an additional source of protection for indigenous and local communities. As a result, their institutional policies may strengthen the ability of developing nations to demand higher standards which would benefit these communities. Support for industry codes, such as the International Cyanide Management Code, could relieve some pressure on developing nations to maintain lower standards to attract foreign investment, according to Christine R. Thompson, writing in the \"Suffolk Transnational Law Review.\" \"The Code's Principles and Standards of Practice would apply regardless of the state's regulatory environment. The Code's third-party audits guarantee uniform compliance, an aspect lacking in other voluntary measures such as the U.N. Global Compact. Wide adoption of voluntary standards might make them binding and applicable to all, regardless of whether state regulations are less stringent or protective.\"\n\nThe Cyanide Code also has been endorsed by the Chief Inspector of Mines in South Africa.\n\nThe government of Zimbabwe, like other countries, recommends that mines using cyanide in their operation do so consistent with the Cyanide Code’s requirements.\n\nThe United National Environment Programme (UNEP) has described ICMI as a partner in its efforts to promote emergency preparedness. In UNEP's 2012 publication \"Commemorating 25 Years of Awareness and Preparedness for Emergencies at Local Level (APELL),\" it stated that the Cyanide Code is \"consistent with the APELL framework and its ten-step process.\"\n\nThe China Chamber of Commerce of Metals, Minerals & Chemicals Importers & Exporters,’ a national industry association officially affiliated with China’s Ministry of Commerce, released their first industry specific guidance on social responsibility for China’s mining industry. Their “Guidelines for Social Responsibility in Outbound Mining Investment” requires Chinese mining companies to include social and environmental factors into their management plans for overseas operations, and specifically encourages that mining operations using cyanide be certified in compliance with the Cyanide Code.\nAccording to a leading expert on wildlife interaction with gold mine tailings, “Since the inception of the International Cyanide Management Code … the issue of wildlife deaths at operations compliant with the Code has been largely resolved; however, according to available information the wildlife death rate remains unabated at operations that are not signatories to the Code.” \n\n"}
{"id": "204842", "url": "https://en.wikipedia.org/wiki?curid=204842", "title": "Kishar", "text": "Kishar\n\nIn the Akkadian epic Enuma Elish, Kishar is the daughter of Lahmu and Lahamu, the first children of Tiamat and Abzu. She is the female principle, sister and wife of Anshar, the male principle, and the mother of Anu. Kishar may represent the earth as a counterpart to Anshar, the sky, and can be seen as an earth mother goddess. Her name also means \"Whole Earth\". \n\nKishar appears only once in Enuma Elish, in the opening lines of the epic, and then disappears from the remainder of the story. She appears only occasionally in other first millennium BCE texts, where she can be equated with the goddess Antu.\n\n"}
{"id": "20303829", "url": "https://en.wikipedia.org/wiki?curid=20303829", "title": "Korea Aerospace University", "text": "Korea Aerospace University\n\nKorea Aerospace University (한국항공대학교 (韓國航空大學校) [Han’guk Hang-gong Dae-hak-gyo]) is a private university in Goyang, Gyeonggi, South Korea. Established in 1952 as a national university, it was taken over by Jungseok Foundation established by Hanjin Group and transferred to a private university. The university — which encompasses most of the aerospace fields including Aerospace & Mechanical Engineering, Electronics, Telecommunications, Computer Engineering, Air Transportation and Logistics, Aeronautical Science & Flight Operation, and Air and Space Law — has been designated to take several national undertakings and collaborative research projects with prominent global corporations including GE, Airbus, PLANSEE since 2009.\n\nMajor research results have been observed in the realm of unmanned aerial vehicle (UAV), since the first autonomous formation flight of UAV and the first flight of solar powered UAV for 12 consecutive hours in Korea. ‘The 1st Hannuri’, the micro satellite (CubeSat), was developed and launched for the first time among Korean universities in 2006.\n\nKorea Aerospace University was established as a national school in June 16, 1952, when the Korean War was raged, under the Charter for Transport School which was granted by the Ministry of Transportation (which is now Ministry of Land, Transport and Maritime Affairs) to develop a civil aviation industry. Although the university primarily started as a two-year course school solely with three departments — Department of Flight Operation, Department of Aircraft Power, Department of Telecommunication Engineering — its status had been elevated by 1953.\n\nAfter the War, the campus moved to Seoul in 1962, then to Goyang City, Gyeonggi-do in 1963, where it is today. School buildings were built (the Hangar, the Flight Training Center, and the Electronics & Telecommunication Building) and several institutes (the Central Library, the Maintenance Factory, the Wireless Lab, the Aviation Research Institute, the Training School for Aviation Tech) were opened.\n\nIn 1979, the university was taken over by Jungseok Foundation, established by Hanjin Group, and it was transferred to a private university. Ever since the transformation, the university started to strengthen its inner and outer sides.\n\nThrough the modifications on quota and name, establishment of schools, departments, and graduate schools were finalized. Auxiliary organizations and institutes were reorganized (see “Centers and institutes”) to vitalize its inner strength.\n\nFor the latter, the Liberal Arts Building, Central Library, the Aviation Control Center, the Mechanical Engineering Building, and the Flight Operation Building were opened in 1970s; the Student’s Hall, the Science Building, the Mechanical Engineering Building, the Electronic Engineering Building were constructed in 1990s. Most of the structures were completed through this period.\n\nPassing through the strengthening period, the university started to focus on ensuring its internal stability. From the early 2000s, structures were added and expanded. The Central Library, the Center for Technical Assiatance to Small and Medium-sized Industries, the KAU Aerospace Center/Museum, and the New Administrative Building were built. The Library and the Student’s Hall had a level extension.\n\nThe university changed its name from Hankuk Aviation University to Korea Aerospace University in 2007. It started to develop its competence by expanding international networks. After obtaining AABI (Aviation Accreditation Board International) qualification, for flight education, aviation management, air traffic management, and air transportation systems in 2007, MOUs (memoranda of understanding) and agreements with well-known organizations were concluded: MOUs with University of Southern California, Drexel University, Oregon State University; agreement for cooperation with Embry-Riddle Aeronautical University; joint development agreement with General Electric.\n\nKorea Aerospace University is the one and only university in Korea specialized in aviation and aerospace.\n\n\n\n\n\n\nKorea Aerospace University established the KAU Aerospace Museum in pursuance of elevating the understanding on aviation and aerospace for the public on August 2004. With the area of 780m2, the museum exhibits more than 800 aviation and aerospace parts. It is structured with ‘Aerospace Zone’ introducing the history of aerospace, ‘Flight Simulator’, ‘Virtual Experience’, ‘Experience Zone’ displaying various plastic airplane, ‘Future of Aerospace Zone’ exhibiting the history and principle of rocket and satellite. KAU Aerospace Museum also earned the first accreditation from the FAI(Fédération Aéronautique Internationale) in Korea and was selected for receiving a benefit from ‘Boeing GCC Fund’.\nFlight Training Center (FTC) at Korea Aerospace University, one of the subsidiaries of GATI, was founded to produce global-standard pilots for airlines and the military. The FTC provides a civil aviation pilot training program entrusted to KAU by Korean Air. For the development of general aviation, the FTC also has a flight training course for members of the public who want to learn how to fly.\n\nOn the KAU campus, FTC-Susaek operates a private pilot course and instrument rating course, both certified by Korea’s Ministry of Land, Transport and Maritime Affairs. FTC-Susaek operates an academic training course for APP ab-initio cadets and Korean Air (KAL) pilots, as well as a flight training course for the public.\n\nFTC-Jungseok operates a jet transition course for Korean Air (KAL) pilots. FTC-Jungseok operates four CE-560s and two CE-525 aircraft, as well as CE-525 and CTN-II simulators for the course.\n\nAt Uljin Airport on South Korean’s eastern coast, FTC-Uljin operates a pilot training course certified by Korea’s Ministry of Land, Transport and Maritime Affairs. The airline pilot training program adheres to stringent global standards. FTC-Uljin operates seven brand new single engine aircraft with glass cockpits, as well as a multi-engine aircraft and two FTDs used for flight training.\n\nThe Korean Language Education Center (KLEC), established in July 2006, founded the Korean Language Program to meet the growing demand from foreign students for qualified, innovative, and progressive Korean language training. The program implements a proven program that has been nationally recognized as the most effective method of language acquisition to date. The curriculum is supported with supplemental materials such as exercise books.\n\nKorea Aerospace University provides study abroad programs for students to gain overseas experiences. The programs are NASA and Boeing Fieldtrip, Research Internship at University of Southern California (USC), Work & Travel Program at University of North Dakota (UND), visiting overseas firms by self-designed plan, and others.\n\n\n\n"}
{"id": "53145642", "url": "https://en.wikipedia.org/wiki?curid=53145642", "title": "Lakawood", "text": "Lakawood\n\nLakawood, or laka wood (), is a reddish aromatic heartwood used as incense in China, India and South East Asia. It also had a number of other uses in the past, for example as a dye and for medicinal purposes. The name lakawood has been used to refer to the wood of different plants, such as \"Acronychia pedunculata\", \"A. Laurifolia\", and in particular, \"Dalbergia parviflora\" found in South East Asia. Historically it was one of the most commonly-traded commodities of South East Asia in the trade between China and South East Asia from the Song dynasty onwards, possibly earlier.\n\nThe lakawood of \"Dalbergia parviflora\" is a product of the Malay peninsula and archipelago, and its native name in Malay is \"kayu laka\" (literally \"laka wood\"), from which the words cayolaque and lakawood are derived. In old Javanese literature, the word \"laka\" was also used to denote a shade of red on cloth, and the word \"manglaka\" meant \"processor of laka-wood dye\", although the tree from which the dye was derived from is \"Emblica officinalis\".\n\nIn Chinese, lakawood may be called \"jiangzhenxiang\" (降真香) or \"zitengxiang\" (紫藤香). The two names referred to different types of fragrant wood in the early period, but by the early 13th century, the two names were regarded as referring to the same product. The older term \"ziteng\" (紫藤, literally \"purple vine\") has been identified as a plant grown in Southern China \"Acronychia pedunculata\" and \"A. Laurifolia\", but is distinct from wisteria which also has the same Chinese name. The fragrance and appearance of the heartwood and root wood from \"Dalbergia parviflora\" of South East Asia, known to have been imported into China in the 10th century, is similar to the earlier Chinese incense wood, it therefore became a substitute for the Chinese product. Its fragrance was particularly appreciated by Taoists, and it therefore gained the name \"jiangzhenxiang\" meaning \"the incense that summons the Perfected Ones to descend among us\". Historical records however used two similar terms, \"jiangzhenxiang\" and \"jiangzhen\", which may have been two different products. \nLakawood was also once referred to as \"Tanarius major\" in some English sources.\n\nThe wood has been used as incense in China from an early period, and it was said to be particularly favoured by the Taoists. It is powdered and mixed with other substances to make incense, commonly in the form of joss sticks. It was first mentioned in 304 AD as a preservative in wine and an incense wood for the summoning of spirit. During the Tang dynasty, it was used for magical and medicinal purposes, burnt in home to rid of all that's \"weird and strange\", and pieces of the wood were attached to children to ward off \"evil vapours\". According to 16th century herbologist and doctor Li Shizhen, it was also used \"as an astringent, as a wash to cleanse sores and to excite granulations, and as a deodorizing and disinfecting agent.\" The sap of \"Emblica officinalis\", also called laka, was used as a red dye by people of Java and the Malacca Strait area. According to \"Zhu Fan Zhi\", the red-coloured sap of lakawood was also once used as an ingredient in a product called \"imitation dragon's blood\".\n\n\"Ziteng\" was first described in 304 AD in a book on plants, \"Nanfang Caomu Zhuang\" (\"Plants of the Southern Regions\") written by Ji Han, as having long and slender leaves, white flower and black seed. Its wood was chopped up and used as incense. The 9th century Tang poet Cao Tang (曹唐) wrote a poem on a Taoist theme that refers to the lakawood \"jiangzhenxiang\": \"Reddish dew gives me an image of upturning \"the wine which extends life\", Whitish smoke puts me in mind of burning \"jiangzhenxiang\" (\"the aromatic which brings down the True Ones\")\".\n\nLakawood from South East Asia was first noted in 982 (early Song dynasty) as one of the 37 foreign products that can be freely traded in China, and descriptions of the trade and product are given in accounts from the Sung and Yuan dynasty, \"Zhu Fan Zhi\" and \"Daoyi Zhilüe\". These texts indicate that Lakawood was a product of various states in the Malay Peninsula, Singapore, Sumatra, Java, as well as Borneo. They suggest a significant trade in lakawood, but it was also regarded as a cheap import during the Song dynasty, such that people of Quanzhou be they rich or poor can afford to buy the incense to burn at the end of the year as a sacrifice to Heaven. Lambri in Sumatra was mentioned as producing the best quality lakawood. The value of lakawood however increased during the Ming dynasty. The product was mentioned in accounts of Zheng He's voyages such \"Yingya Shenglan\" by Ma Huan during the Ming dynasty, and its value was considered high enough to be presented to the imperial court as tributes by various ports of Sumatra as well as Siam.\n"}
{"id": "2271535", "url": "https://en.wikipedia.org/wiki?curid=2271535", "title": "List of Dacian plant names", "text": "List of Dacian plant names\n\nThis is a list of plant names in Dacian, surviving from ancient botanical works such as Dioscorides' \"De Materia Medica\" (abb. MM) and Pseudo-Apuleius' \"Herbarius\" (abb. Herb.). Dacian plant names are one of the primary sources left to us for studying the Dacian language, an ancient language of South Eastern Europe. This list also includes a Bessian plant name and a Moesian plant name, both neighboring Daco-Thracian tribes.\n\nA separate list exists containing Romanian words of possible Dacian origin that form the Eastern Romance substratum.\n\n\n"}
{"id": "30313405", "url": "https://en.wikipedia.org/wiki?curid=30313405", "title": "List of Entoloma species", "text": "List of Entoloma species\n\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\nThis is an incomplete list of species in the genus \"Entoloma\". According to a standard reference book, the genus contains about 1000 species. Many species formerly classified in the genera \"Rhodocybe\", \"Clitopilus\", \"Richoniella\", and \"Rhodogaster\" were formally transferred to \"Entoloma\" as a result of molecular analysis published in 2009.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3499552", "url": "https://en.wikipedia.org/wiki?curid=3499552", "title": "List of Phacelia species", "text": "List of Phacelia species\n\nList of \"Phacelia\" species. This plant genus is in the family Boraginaceae, basal in one of the 2 main euasterid lineages, as per Angiosperm Phylogeny Group. It is usually placed in the Hydrophylloideae subfamily.\n\nThis is a list of binomial names, including both accepted species and synonyms.\n\nSource: Index Kewensis (at IPNI) and ITIS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "9132973", "url": "https://en.wikipedia.org/wiki?curid=9132973", "title": "List of foliage plant diseases (Palmae)", "text": "List of foliage plant diseases (Palmae)\n\nThis is a list of diseases of foliage plants belonging to the family Palmae.\n\n"}
{"id": "58516310", "url": "https://en.wikipedia.org/wiki?curid=58516310", "title": "List of pipeline accidents in the United States in 2005", "text": "List of pipeline accidents in the United States in 2005\n\nThe following is a list of pipeline accidents in the United States in 2005. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n"}
{"id": "322340", "url": "https://en.wikipedia.org/wiki?curid=322340", "title": "List of rivers of Oceania", "text": "List of rivers of Oceania\n\nThis is a list of rivers in Oceania.\n\n\n\n\n\n\n\n\n\n \n"}
{"id": "415366", "url": "https://en.wikipedia.org/wiki?curid=415366", "title": "Lists of tornadoes and tornado outbreaks", "text": "Lists of tornadoes and tornado outbreaks\n\nThese are some notable tornadoes, tornado outbreaks, and tornado outbreak sequences that have occurred around the globe.\n\n\nCurrently 1970 - Present\n\n\n\n\n\n\n"}
{"id": "18336807", "url": "https://en.wikipedia.org/wiki?curid=18336807", "title": "Magsat", "text": "Magsat\n\nMagsat (Magnetic Field Satellite, Explorer 61, Applications Explorer Mission-3 or AEM-3) spacecraft was launched in the fall of 1979 and ended in the spring of 1980. The mission was to map the Earth's magnetic field, the satellite had two magnetometers. The scalar (Cesium vapor) and vector (fluxgate) magnetometers gave Magsat a capability beyond that of any previous spacecraft. Extended by a telescoping boom, the magnetometers were distanced from the magnetic field created by the satellite and its electronics. The satellite carried two magnetometers, a three-axis fluxgate magnetometer for determining the strength and direction of magnetic fields, and an ion-vapor/vector magnetometer for determining the magnetic field caused by the vector magnetometer itself. \nMAGSAT is considered to be one of the more important Science/Earth orbiting satellites launched; the data it accumulated is still being used, particularly in linking new satellite data to past observations.\n\nAfter launch the payload was brought to an orbit of 96.8° facing the Sun as the Earth rotated underneath. It was kept in a close Earth orbit, with vector magnetometers capable of sensing magnetic fields closer to Earth's surface. The data collected by this satellite allowed a 3D-mapping of the Earth's magnetic interior as never seen before. In combination with a later satellite, Ørsted, it has been an essential component for explaining the current declining state of the Earth's magnetic field.\n\nOn October 30, 1979 Magsat was launched from pad SLC-5 at Vandenberg AFB in California on a Scout II (101) rocket bearing 97° in a dusk to dawn orbit. The spacecraft was placed in an orbit with a perigee of and an apogee of . After reaching orbit, its telescoping boom was extended outward by . Two star cameras were used to define the position of the spacecraft relative to Earth. The orbit allowed the satellite to map a majority of the Earth's surfaces except the geographic poles. The satellite decayed from orbit on June 11, 1980.\n\nAccording to a Johns Hopkins University / Applied Physics Laboratory (JHU/APL) report, and archival NASA source documentation (Johns Hopkins APL Technical Digest, July–September 1980, Vol. 1, No. 3), the MAGSAT spacecraft utilized two RCA 1802 microprocessors running at a 2 MHz clock speed in a redundant setup. A stored memory of 2.8 kilobytes in PROMs with 1 K bytes of random access memory (RAM) provided the program and working space for the microprocessor. Other integrated circuits chips of the CDP 1800 family of circuits were also used, including the CDP 1852 interface circuit and the CDP 1822 1K x 1 RAM, as well as Harris CMOS 6611A PROMS.\n\nThree families of circuits were considered for the computer system design: two NMOS families (the Motorola 6800 and Intel 8080 microprocessors) and the RCA CDP1802 CMOS microprocessor. The 1802 was chosen based on various criteria, including the 1802 CMOS technology being power efficient by two orders of magnitude compared to the NMOS microprocessors, compatibility with the existing power supply of the satellite and the low-power requirements of CMOS, the radiation hardening of the 1802 and lack thereof in the 6800 and 8080, and other 1802-based functioning and features.\n\nSoftware for the project was developed with an in-house APL-generated 1802 cross-assembler running on IBM 360/370 mainframe computers.\n\nMagsat was not without problems. One of the biggest is that the motion of a metallic object tends to create a magnetic field. One study after the mission found a nonlinear fluxgate response when exposed to fields greater than 5000 mT. The applied field had to be transverse to the axis of the magnetometer. The design was improved by creating a feedback relay over a spherical design. This was the design used on later spacecraft [See:Ørsted (satellite)].\n\nThis configuration magnetometer was also later used on the Magnetometer of the Jupiter orbiter Juno, which arrived at the planet Jupiter in the 2010s.\n"}
{"id": "32479073", "url": "https://en.wikipedia.org/wiki?curid=32479073", "title": "Marcahuasi", "text": "Marcahuasi\n\nMarcahuasi () is a plateau in the Andes Mountains, located 60 km east of Lima, on the mountain range that rises to the right bank of the Rímac River. This mountain range dominates the landscape at 4,000 m above sea level and the place is known for curious shapes of human faces and animals visible in granite rock.\n\nThe place was first investigated by Daniel Ruzo during the 1950s and in an area of about 3 km, several hundred curious shapes are found, which can be presumed as natural formations. The place is located at a height of about 12500 feet in the Andes Mountain.\n\nThe plateau originated from a volcanic reaction. It is about 4 square kilometers in area, and is located almost 4,000 meters high in Huarochirí Province, east of Lima, Peru. Marcahuasi is home to a unique set of huge granite rocks with curious shapes resembling human faces, animals, and religious symbols. There are many theories as to their origins, including the assertion that their unusual shapes formed naturally through erosion. Some argued that they are sculptures shaped by ancient people but archaeologists clearly state that the shapes are the result of erosion over centuries. There are some small pre-Columbian structures, which are tombs of ancient people and some of which are robbed and vandalised.\n\nThere is also a collection of ruins on the north side of the plateau. Over 50 structures still stand in varying states of decay. The doorways are very small, some three feet high. Most of the structures are narrow since the use of arches was not known, therefore spans had to be covered with suitable rock that may have been quarried locally. There are also what appear to be burial tombs on the outskirts of the settlement.\n\n\n"}
{"id": "13087180", "url": "https://en.wikipedia.org/wiki?curid=13087180", "title": "Mass (mass spectrometry)", "text": "Mass (mass spectrometry)\n\nThe mass recorded by a mass spectrometer can refer to different physical quantities depending on the characteristics of the instrument and the manner in which the mass spectrum is displayed.\n\nThe unified atomic mass unit (symbol: u) is the standard unit that is used for indicating mass on an atomic or molecular scale (atomic mass). The dalton (symbol: Da) is equivalent to the unified atomic mass unit. One unified atomic mass unit is approximately the mass of one a single proton or neutron. The unified atomic mass unit has a value of . The \"amu\" without the \"unified\" prefix is an obsolete unit based on oxygen, which was replaced in 1961.\n\nThe molecular mass (abbreviated M) of a substance, formerly also called molecular weight and abbreviated as MW, is the mass of one molecule of that substance, relative to the unified atomic mass unit u (equal to 1/12 the mass of one atom of C). Due to this relativity, the molecular mass of a substance is commonly referred to as the relative molecular mass, and abbreviated to M.\n\nThe average mass of a molecule is obtained by summing the average atomic masses of the constituent elements. For example, the average mass of natural water with formula HO is 1.00794 + 1.00794 + 15.9994 = 18.01528.\n\nThe mass number, also called the nucleon number, is the number of protons and neutrons in an atomic nucleus. The mass number is unique for each isotope of an element and is written either after the element name or as a superscript to the left of an element's symbol. For example, carbon-12 (C) has 6 protons and 6 neutrons.\n\nThe nominal mass for an element is the mass number of its most abundant naturally occurring stable isotope, and for an ion or molecule, the nominal mass is the sum of the nominal masses of the constituent atoms. Isotope abundances are tabulated by IUPAC: for example carbon has two stable isotopes C at 98.9% natural abundance and C at 1.1% natural abundance, thus the nominal mass of carbon is 12. The nominal mass is not always the lowest mass number, for example iron has isotopes Fe, Fe, Fe, and Fe with abundances 6%, 92%, 10%, and 2%, respectively, and a nominal mass of 56. For a molecule, the nominal mass is obtained by summing the nominal masses of the constituent elements, for example water has two hydrogen atoms with nominal mass 1 and one oxygen atom with nominal mass 16, therefore the nominal mass of HO is 18.\n\nIn mass spectrometry, the difference between the nominal mass and the monoisotopic mass is the mass defect. This differs from the definition of mass defect used in physics which is the difference between the mass of a composite particle and the sum of the masses of its constituent parts.\n\nThe accurate mass (more appropriately, the measured accurate mass) is an experimentally determined mass that allows the elemental composition to be determined. For molecules with mass below 200 u, 5 ppm accuracy is often sufficient to uniquely determine the elemental composition.\n\nThe exact mass of an isotopic species (more appropriately, the calculated exact mass) is obtained by summing the masses of the individual isotopes of the molecule. For example, the exact mass of water containing two hydrogen-1 (H) and one oxygen-16 (O) is 1.0078 + 1.0078 + 15.9949 = 18.0105. The exact mass of heavy water, containing two hydrogen-2 (deuterium or H) and one oxygen-16 (O) is 2.0141 + 2.0141 + 15.9949 = 20.0229.\n\nWhen an exact mass value is given without specifying an isotopic species, it normally refers to the most abundant isotopic species.\n\nThe monoisotopic mass is the sum of the masses of the atoms in a molecule using the unbound, ground-state, rest mass of the principal (most abundant) isotope for each element. The monoisotopic mass of a molecule or ion is the exact mass obtained using the principal isotopes. Monoisotopic mass is typically expressed in unified atomic mass units.\n\nFor typical organic compounds, where the monoisotopic mass is most commonly used, this also results in the lightest isotope being selected. For some heavier atoms such as iron and argon the principal isotope is not the lightest isotope. The mass spectrum peak corresponding to the monoisotopic mass is often not observed for large molecules, but can be determined from the isotopic distribution.\n\nThis refers to the mass of the molecule with the most highly represented isotope distribution, based on the natural abundance of the isotopes.\n\nIsotopomers (isotopic isomers) are isomers having the same number of each isotopic atom, but differing in the positions of the isotopic atoms. For example, CHCHDCH and CHCHCHD are a pair of structural isotopomers.\n\nIsotopomers should not be confused with isotopologues, which are chemical species that differ in the isotopic composition of their molecules or ions. For example, three isotopologues of the water molecule with different isotopic composition of hydrogen are: HOH, HOD and DOD, where D stands for deuterium (H).\n\nThe Kendrick mass is a mass obtained by multiplying the measured mass by a numeric factor. The Kendrick mass is used to aid in the identification of molecules of similar chemical structure from peaks in mass spectra. The method of stating mass was suggested in 1963 by the chemist Edward Kendrick.\n\nAccording to the procedure outlined by Kendrick, the mass of CH is defined as 14.000 Da, instead of 14.01565 Da.\n\nThe Kendrick mass for a family of compounds F is given by\n\nFor hydrocarbon analysis, F=CH.\n\nThe mass defect used in nuclear physics is different from its use in mass spectrometry. In nuclear physics, the mass defect is the difference in the mass of a composite particle and the sum of the masses of its component parts. In mass spectrometry the mass defect is defined as the difference between the exact mass and the nearest integer mass.\n\nThe Kendrick mass defect is the exact Kendrick mass subtracted from the nearest integer Kendrick mass.\n\nMass defect filtering can be used to selectively detect compounds with a mass spectrometer based on their chemical composition.\n\nThe term packing fraction was defined by Aston as the difference of the measured mass \"M\" and the nearest integer mass \"I\" (based on the oxygen-16 mass scale) divided by the quantity comprising the mass number multiplied by ten thousand:\n\nAston's early model of nuclear structure (prior to the discovery of the neutron) postulated that the electromagnetic fields of closely packed protons and electrons in the nucleus would interfere and a fraction of the mass would be destroyed. A low packing fraction is indicative of a stable nucleus.\n\nThe nitrogen rule states that organic compounds containing exclusively hydrogen, carbon, nitrogen, oxygen, silicon, phosphorus, sulfur, and the halogens either have an odd nominal mass that indicates an odd number of nitrogen atoms are present or an even nominal mass that indicates an even number of nitrogen atoms are present in the molecular ion.\n\nThe whole number rule states that the masses of the isotopes are integer multiples of the mass of the hydrogen atom. The rule is a modified version of Prout's hypothesis proposed in 1815, to the effect that atomic weights are multiples of the weight of the hydrogen atom.\n\n\n"}
{"id": "15683187", "url": "https://en.wikipedia.org/wiki?curid=15683187", "title": "Mediterranean conifer and mixed forests", "text": "Mediterranean conifer and mixed forests\n\nMediterranean conifer and mixed forests is an ecoregion, in the temperate coniferous forest biome, which occupies the high mountain ranges of North Africa and southern Spain. The term is also a botanically recognized plant association in the African and Mediterranean literature.\n\nThe Mediterranean conifer and mixed forests ecoregion consists of a series of enclaves in the coastal Rif Mountains and interior Middle Atlas and High Atlas of Morocco, the eastern Tell Atlas and eastern Saharan Atlas of Algeria, and the Kroumerie and Mogod ranges of Tunisia. \n\nThe Mediterranean woodlands and forests ecoregion surrounds the Mediterranean conifer and mixed forests at lower elevations. \n\nIn the High Atlas, the Mediterranean conifer and mixed forests yield to the Mediterranean High Atlas juniper steppe at the highest elevations.\n\nThe predominant canopy tree in the forests is Atlas cedar (\"Cedrus atlantica\"). Other conifer trees that grow in this area may include pines such as Aleppo pine and Maritime pine. Firs such as the Spanish fir and the Algerian fir. Junipers such as Juniperus oxycedrus and Juniperus thurifera And Yews such as Taxus baccata.\n\nOther non-coniferous trees can be found in scattered area throughout this Eco-region which includes cork oak, White willow and any other oaks and trees. \n\"Quercus afares\", a deciduous oak, is endemic to the ecoregion.\n\nEndangered mammals in the ecoregion include the Barbary macaque (\"Macaca sylvanus\") at locations such as the Djebel Babor Mountains; other species in the ecoregion include Atlas deer (\"Cervus elaphus barbarus\"), and the African leopard (\"Panthera pardus pardus\").\n\nDeforestation due to overuse by the local population is a major threat as are the effects of climate change.\n\n"}
{"id": "28668879", "url": "https://en.wikipedia.org/wiki?curid=28668879", "title": "Mid-Continental Canadian forests", "text": "Mid-Continental Canadian forests\n\nThe Mid-Continental Canadian forests are a taiga ecoregion of northern Canada.\n\nThis ecoregion extends from south of the Great Slave Lake in the Northwest Territories through most of northeastern Alberta, central Saskatchewan and parts of west-central Manitoba and consists of three main areas: the Slave River basin in northeastern Alberta, the lowlands of the northern Manitoba plain, and the uplands south of the Canadian Shield from north-central Alberta to southwestern Manitoba. This is a mixed area of lowlands and mountains up to 800m high, including areas of wetland and peat bog and mountain lakes and ponds. The area has a subhumid mid-boreal ecoclimate with short summers (average temperature 14°C) and long, cold winters (ave. -15°C) and patches of permafrost in the lowlands.\n\nThese forests, like so much of Canada at this latitude, are a mixture of conifers and deciduous trees including quaking aspen (\"Populus tremuloides\"), balsam poplar (\"Populus balsamifera\"), white spruce (\"Picea glauca\"), black spruce (\"Picea mariana\") and balsam fir (\"Abies balsamea\").\n\nWildlife of the area includes moose (\"Alces alces\"), American black bear (\"Ursus americanus\"), wolf (\"Canis lupus\"), Canada lynx (\"Lynx canadensis\"), white-tailed deer (\"Odocoileus virginianus\"), elk (\"Cervus canadensis\"), North American beaver (\"Castor canadensis\"), muskrat (\"Ondatra zibethicus\"), snowshoe hare (\"Lepus americanus\"). The plain to the south of the lake is home to moose, coyote (\"Canis latrans\"), and eastern cottontail (\"Sylvilagus floridanus\") as well. Wood Buffalo National Park on the Slave River is the largest national park in Canada and home to the world's largest herd of American bison (\"Bison bison\").\n\nBirds include ducks, geese, American white pelican (\"Pelecanus erythrorhynchos\"), sandhill crane (\"Grus canadensis\"), ruffed grouse (\"Bonasa umbellus\") and common loon (\"Gavia immer\") The wetlands of the region, such as Cumberland Lake, are an important refuge for migratory birds and include the most important breeding populations of the endangered whooping crane in North America.\n\nHalf of the natural forest remains intact, the other half having been removed by extensive logging, oil and gas exploration and mining. Blocks of intact forest include Wood Buffalo Park, the areas around Cold Lake/Primrose Lake and Doré Lake, Prince Albert National Park, to the north of Cumberland Lake, Riding Mountain National Park, Porcupine Hills, Duck Mountain Provincial Park (Manitoba), Duck Mountain Provincial Park (Saskatchewan) (near the town of Kamsack) and the Peace–Athabasca Delta on the Slave River. Other protected areas include Clearwater River (Saskatchewan), Meadow Lake Provincial Park (near Goodsoil, Saskatchewan) and Narrow Hills Provincial Park.\n"}
{"id": "40874485", "url": "https://en.wikipedia.org/wiki?curid=40874485", "title": "Molecules in stars", "text": "Molecules in stars\n\nStellar molecules are molecules that exist or form in stars. Such formations can take place when the temperature is low enough for molecules to form – typically around 6000 K or cooler. Otherwise the stellar matter is restricted to atoms (chemical elements) in the forms of gas or – at very high temperatures – plasma.\n\nMatter is made up by atoms (formed by protons and other subatomic particles). When the environment is right, atoms can join together and form molecules, which give rise to most materials studied in materials science. But certain environments, such as high temperatures, don't allow atoms to form molecules. Stars have very high temperatures, primarily in their interior, and therefore there are few molecules formed in stars. For this reason, a chemist (who studies atoms and molecules) would not have much to study in a star, so stars are better explained by physicists. However, low abundance of molecules in stars is not equated with no molecules at all.\n\nAlthough the Sun is a star, its photosphere has a low enough temperature of 6,000 kelvins, and therefore molecules can form. Water has been found on the Sun, and there is evidence of H in white dwarf stellar atmospheres.\n\nCooler stars include absorption band spectra that are characteristic of molecules. Similar absorption bands are found in sun spots which are cooler areas on the Sun. Molecules found in the Sun include MgH, CaH, FeH, CrH, NaH, OH, SiH, VO, and TiO. Others include CN CH, MgF, NH, C, SrF, zirconium monoxide, YO, ScO, BH.\n\nStars of most types can contain molecules, even the Ap category of A class stars. Only the hottest O, B and A class stars have no detectable molecules. Also carbon rich white dwarfs, even though very hot, have spectral lines of C and CH.\n\nMeasurements of simple molecules that may be found in stars are performed in laboratories to determine the wavelengths of the spectra lines. Also it is important to measure the dissociation energy and oscillator strengths (how strongly the molecule interacts with electromagnetic radiation). These measurements are inserted into formula that can calculate the spectrum under different conditions of pressure and temperature. However man-made conditions are often different to stars, because it is hard to achieve the temperatures, and also local thermal equilibrium, as found in stars, is unlikely. Accuracy of oscillator strengths and actual measurement of dissociation energy is usually only approximate.\n\nA numerical model of a star's atmosphere will calculate pressures and temperatures at different depths, and can predict the spectrum for different elemental concentrations.\n\nThe molecules in stars can be used to determine some characteristics of the star. The isotopic composition can be determined if the lines in the molecular spectrum are observed. The different masses of different isotopes cause vibration and rotation frequencies to significantly vary. Secondly the temperature can be determined, as the temperature will change the numbers of molecules in the different vibrational and rotational states. Some molecules are sensitive to the ratio of elements, and so indicate elemental composition of the star. Different molecules are characteristic of different kinds of stars, and are used to classify them. Because there can be numerous spectral lines of different strength, conditions at different depths in the star can be determined. These conditions include temperature and speed towards or away from the observer. \n\nThe spectrum of molecules has advantages over atomic spectral lines, as atomic lines are often very strong, and therefore only come from high in the atmosphere. Also the profile of the atomic spectral line can be distorted due to isotopes or overlaying of other spectral lines. The molecular spectrum is much more sensitive to temperature than atomic lines.\n"}
{"id": "7125481", "url": "https://en.wikipedia.org/wiki?curid=7125481", "title": "Monsoon trough", "text": "Monsoon trough\n\nThe monsoon trough is a portion of the Intertropical Convergence Zone in the Western Pacific, as depicted by a line on a weather map showing the locations of minimum sea level pressure, and as such, is a convergence zone between the wind patterns of the southern and northern hemispheres.\n\nWesterly monsoon winds lie in its equatorward portion while easterly trade winds exist poleward of the trough. Right along its axis, heavy rains can be found which usher in the peak of a location's respective rainy season. As it passes poleward of a location, hot and dry conditions develop. The monsoon trough plays a role in creating many of the world's rainforests.\n\nThe term \"monsoon trough\" is most commonly used in monsoonal regions of the Western Pacific such as Asia and Australia. The migration of the ITCZ/monsoon trough into a landmass heralds the beginning of the annual rainy season during summer months. Depressions and tropical cyclones often form in the vicinity of the monsoon trough, with each capable of producing a year's worth of rainfall in a matter of days.\n\nMonsoon troughing in the western Pacific reaches its zenith in latitude during the late summer when the wintertime surface ridge in the opposite hemisphere is the strongest. It can reach as far as the 40th parallel in East Asia during August and the 20th parallel in Australia during February. Its poleward progression is accelerated by the onset of the summer monsoon which is characterized by the development of lower air pressure over the warmest part of the various continents. In the Southern Hemisphere, the monsoon trough associated with the Australian monsoon reaches its most southerly latitude in February, oriented along a west-northwest/east-southeast axis.\n\nIncreases in the relative vorticity, or spin, with the monsoon trough are normally a product of increased wind convergence within the convergence zone of the monsoon trough. Wind surges can lead to this increase in convergence. A strengthening or equatorward movement in the subtropical ridge can cause a strengthening of a monsoon trough as a wind surge moves towards the location of the monsoon trough. As fronts move through the subtropics and tropics of one hemisphere during their winter, normally as shear lines when their temperature gradient becomes minimal, wind surges can cross the equator in oceanic regions and enhance a monsoon trough in the other hemisphere's summer. A key way of detecting whether a wind surge has reached a monsoon trough is the formation of a burst of thunderstorms within the monsoon trough.\n\nIf a circulation forms within the monsoon trough, it is able to compete with the neighboring thermal low over the continent, and a wind surge will occur at its periphery. Such a circulation which is broad in nature within a monsoon trough is known as a monsoon depression. In the Northern Hemisphere, monsoon depressions are generally asymmetric, and tend to have their strongest winds on their eastern periphery. Light and variable winds cover a large area near their center, while bands of showers and thunderstorms develop within their area of circulation.\n\nThe presence of an upper level jet stream poleward and west of the system can enhance its development by leading to increased diverging air aloft over the monsoon depression, which leads to a corresponding drop in surface pressure. Even though these systems can develop over land, the outer portions of monsoon depressions are similar to tropical cyclones. In India, for example, 6 to 7 monsoon depressions move across the country yearly, and their numbers within the Bay of Bengal increase during July and August of El Niño events. Monsoon depressions are efficient rainfall producers, and can generate a year's worth of rainfall when they move through drier areas, such as the outback of Australia.\n\nSince the monsoon trough is an area of convergence in the wind pattern, and an elongated area of low pressure at the surface, the trough focuses low level moisture and is defined by one or more elongated bands of thunderstorms when viewing satellite imagery. Its abrupt movement to the north between May and June is coincident with the beginning of the monsoon regime and rainy seasons across South and East Asia. This convergence zone has been linked to prolonged heavy rain events in the Yangtze river as well as northern China. Its presence has also been linked to the peak of the rainy season in locations within Australia. As it progresses poleward of a particular location, clear, hot, and dry conditions develop as winds become westerly. Many of the world's rainforests are associated with these climatological low-pressure systems.\n\nA monsoon trough is a significant genesis region for tropical cyclones. Vorticity-rich low level environments, with significant low level spin, lead to a better than average chance of tropical cyclone formation due to their inherent rotation. This is because a pre-existing near-surface disturbance with sufficient spin and convergence is one of the six requirements for tropical cyclogenesis. There appears to be a 15- to 25-day cycle in thunderstorm activity associated with the monsoon trough, which is roughly half the wavelength of the Madden–Julian oscillation, or MJO. This mirrors tropical cyclone genesis near these features, as genesis clusters in 2–3 weeks of activity followed by 2–3 weeks of inactivity. Tropical cyclones can form in outbreaks around these features under special circumstances, tending to follow the next cyclone to its poleward and west.\n\nWhenever the monsoon trough on the eastern side of the summertime Asian monsoon is in its normal orientation (oriented east-southeast to west-northwest), tropical cyclones along its periphery will move with a westward motion. If it is reverse oriented, or oriented southwest to northeast, tropical cyclones will move more poleward. Tropical cyclone tracks with S shapes tend to be associated with reverse-oriented monsoon troughs. The South Pacific convergence zone and South American convergence zones are generally reverse oriented. The failure of the monsoon trough, or the ITCZ, to move south of the equator in the eastern Pacific Ocean and Atlantic Ocean, during the southern hemisphere summer, is considered one of the reasons that tropical cyclones normally do not form in those regions. It has also been noted that when the monsoon trough lies near 20 degrees north latitude in the Pacific, the frequency of tropical cyclones is 2 to 3 times greater than when it lies closer to 10 degrees north.\n"}
{"id": "42796964", "url": "https://en.wikipedia.org/wiki?curid=42796964", "title": "Naturalism (philosophy)", "text": "Naturalism (philosophy)\n\nIn philosophy, naturalism is the \"idea or belief that only natural (as opposed to supernatural or spiritual) laws and forces operate in the world.\" Adherents of naturalism (\"i.e.\", naturalists) assert that natural laws are the rules that govern the structure and behavior of the natural universe, that the changing universe at every stage is a product of these laws.\n\n\"Naturalism can intuitively be separated into an ontological and a methodological component,\" argues David Papineau. \"Ontological\" refers to the philosophical study of the nature of reality. Some philosophers equate naturalism with materialism. For example, philosopher Paul Kurtz argues that nature is best accounted for by reference to material principles. These principles include mass, energy, and other physical and chemical properties accepted by the scientific community. Further, this sense of naturalism holds that spirits, deities, and ghosts are not real and that there is no \"purpose\" in nature. Such an absolute belief in naturalism is commonly referred to as \"metaphysical naturalism\".\n\nAssuming naturalism in working methods as the current paradigm, without the further consideration of naturalism as an absolute truth with philosophical entailment, is called \"methodological naturalism\". The subject matter here is a philosophy of acquiring knowledge based on an assumed paradigm.\n\nWith the exception of pantheists—who believe that Nature is identical with divinity while not recognizing a distinct personal anthropomorphic god—theists challenge the idea that nature contains all of reality. According to some theists, natural laws may be viewed as so-called secondary causes of God(s).\n\nIn the 20th century, Willard Van Orman Quine, George Santayana, and other philosophers argued that the success of naturalism in science meant that scientific methods should also be used in philosophy. Science and philosophy are said to form a continuum, according to this view.\n\nThe current usage of the term naturalism \"derives from debates in America in the first half of the last century. The self-proclaimed 'naturalists' from that period included John Dewey, Ernest Nagel, Sidney Hook and Roy Wood Sellars.\"\n\nCurrently, metaphysical naturalism is more widely embraced than in previous centuries, especially but not exclusively in the natural sciences and the Anglo-American, analytic philosophical communities. While the vast majority of the population of the world remains firmly committed to non-naturalistic worldviews, prominent contemporary defenders of naturalism and/or naturalistic theses and doctrines today include J. J. C. Smart, David Malet Armstrong, David Papineau, Paul Kurtz, Brian Leiter, Daniel Dennett, Michael Devitt, Fred Dretske, Paul and Patricia Churchland, Mario Bunge, Jonathan Schaffer, Hilary Kornblith, Quentin Smith, Paul Draper and Michael Martin, among many other academic philosophers.\n\nAccording to David Papineau, contemporary naturalism is a consequence of the build-up of scientific evidence during the twentieth century for the \"causal closure of the physical\", the doctrine that all physical effects can be accounted for by physical causes.\n\nThe term \"methodological naturalism\" is much more recent though. According to Ronald Numbers, it was coined in 1983 by Paul de Vries, a Wheaton College philosopher. De Vries distinguished between what he called \"methodological naturalism,\" a disciplinary method that says nothing about God's existence, and \"metaphysical naturalism,\" which \"denies the existence of a transcendent God.\" The term \"methodological naturalism\" had been used in 1937 by Edgar S. Brightman in an article in \"The Philosophical Review\" as a contrast to \"naturalism\" in general, but there the idea was not really developed to its more recent distinctions.\n\nAccording to Steven Schafersman, naturalism is a philosophy that maintains that; \n\nOr, as Carl Sagan succinctly put it: \"\"The Cosmos is all that is or ever was or ever will be\".\"\n\nIn addition Arthur C. Danto states that Naturalism, in recent usage, is a species of philosophical monism according to which whatever exists or happens is \"natural\" in the sense of being susceptible to explanation through methods which, although paradigmatically exemplified in the natural sciences, are continuous from domain to domain of objects and events. Hence, naturalism is polemically defined as repudiating the view that there exists or could exist any entities which lie, in principle, beyond the scope of scientific explanation.\n\nAccording to Kuhn, all science is based on an approved agenda of unprovable assumptions about the character of the universe, rather than merely on empirical facts. These assumptions—a paradigm—comprise a collection of beliefs, values and techniques that are held by a given scientific community, which legitimize their systems and set the limitations to their investigation. Alfred North Whitehead wrote, \"All science must start with some assumptions as to the ultimate analysis of the facts with which it deals. These assumptions are justified partly by their adherence to the types of occurrence of which we are directly conscious, and partly by their success in representing the observed facts with a certain generality, devoid of \"ad hoc\" suppositions.\" Priddy notes that all scientific study inescapably builds on at least some essential assumptions that are untested by scientific processes. For naturalists, nature is the only reality. There is no such thing as 'supernatural'. The scientific method is to be used to investigate all reality, including the human spirit: \"The great majority of contemporary philosophers would happily... reject 'supernatural' entities, and allow that science is a possible route (if not necessarily the only one) to important truths about the 'human spirit'.\"\n\nNaturalism is the implicit philosophy of working scientists, that the following basic assumptions are needed to justify the scientific method:\n\n\nMetaphysical naturalism, also called \"ontological naturalism\" and \"philosophical naturalism\", is a philosophical worldview and belief system that holds that there is nothing but natural elements, principles, and relations of the kind studied by the natural sciences, i.e., those required to understand our physical environment by mathematical modeling. Methodological naturalism, on the other hand, refers exclusively to the methodology of science, for which metaphysical naturalism provides only one possible ontological foundation.\n\nMetaphysical naturalism holds that all properties related to consciousness and the mind are reducible to, or supervene upon, nature. Broadly, the corresponding theological perspective is religious naturalism or spiritual naturalism. More specifically, metaphysical naturalism rejects the supernatural concepts and explanations that are part of many religions.\n\nMethodological naturalism concerns itself with methods of learning what nature is. These methods are useful in the evaluation of claims about existence and knowledge and in identifying causal mechanisms responsible for the emergence of physical phenomena. It attempts to explain and test scientific endeavors, hypotheses, and events with reference to natural causes and events. This second sense of the term \"naturalism\" seeks to provide a framework within which to conduct the scientific study of the laws of nature. Methodological naturalism is a way of acquiring knowledge. It is a distinct system of thought concerned with a cognitive approach to reality, and is thus a philosophy of knowledge. Studies by sociologist Elaine Ecklund suggest that religious scientists in practice apply methodological naturalism. They report that their religious beliefs affect the way they think about the implications - often moral - of their work, but not the way they practice science.\n\nIn a series of articles and books from 1996 onward, Robert T. Pennock wrote using the term \"methodological naturalism\" to clarify that the scientific method confines itself to natural explanations without assuming the existence or non-existence of the supernatural, and is not based on dogmatic metaphysical naturalism (as claimed by creationists and proponents of intelligent design, in particular by Phillip E. Johnson). Pennock's testimony as an expert witness at the Kitzmiller v. Dover Area School District trial was cited by the Judge in his \"Memorandum Opinion\" concluding that \"Methodological naturalism is a 'ground rule' of science today\":\nExpert testimony reveals that since the scientific revolution of the 16th and 17th centuries, science has been limited to the search for natural causes to explain natural phenomena... While supernatural explanations may be important and have merit, they are not part of science.\" Methodological naturalism is thus \"a paradigm of science.\" It is a \"ground rule\" that \"requires scientists to seek explanations in the world around us based upon what we can observe, test, replicate, and verify.\n\nAlvin Plantinga, Professor Emeritus of Philosophy at Notre Dame, and a Christian, has become a well-known critic of naturalism. He suggests, in his evolutionary argument against naturalism, that the probability that evolution has produced humans with reliable true beliefs, is low or inscrutable, unless the evolution of humans was guided (for example, by God). According to David Kahan of the University of Glasgow, in order to understand how beliefs are warranted, a justification must be found in the context of supernatural theism, as in Plantinga's epistemology. \"(See also supernormal stimuli).\"\n\nPlantinga argues that together, naturalism and evolution provide an insurmountable \"\"defeater\" for the belief that our cognitive faculties are reliable\", i.e., a skeptical argument along the lines of Descartes' Evil demon or Brain in a vat.\n\nRobert T. Pennock contends that as supernatural agents and powers \"are above and beyond the natural world and its agents and powers\" and \"are not constrained by natural laws\", only logical impossibilities constrain what a supernatural agent could not do. He states: \"If we could apply natural knowledge to understand supernatural powers, then, by definition, they would not be supernatural\". As the supernatural is necessarily a mystery to us, it can provide no grounds on which to judge scientific models. \"Experimentation requires observation and control of the variables... But by definition we have no control over supernatural entities or forces.\" Science does not deal with meanings; the closed system of scientific reasoning cannot be used to define itself. Allowing science to appeal to untestable supernatural powers would make the scientist's task meaningless, undermine the discipline that allows science to make progress, and \"would be as profoundly unsatisfying as the ancient Greek playwright's reliance upon the \"deus ex machina\" to extract his hero from a difficult predicament.\"\n\nNaturalism of this sort says nothing about the existence or nonexistence of the supernatural, which by this definition is beyond natural testing. As a practical consideration, the rejection of supernatural explanations would merely be pragmatic, thus it would nonetheless be possible, for an ontological supernaturalist to espouse and practice methodological naturalism. For example, scientists may believe in God while practicing methodological naturalism in their scientific work. This position does not preclude knowledge that is somehow connected to the supernatural. Generally however, anything that can be scientifically examined and explained would not be supernatural, simply by definition.\n\nW. V. O. Quine describes naturalism as the position that there is no higher tribunal for truth than natural science itself. In his view, there is no better method than the scientific method for judging the claims of science, and there is neither any need nor any place for a \"first philosophy\", such as (abstract) metaphysics or epistemology, that could stand behind and justify science or the scientific method.\n\nTherefore, philosophy should feel free to make use of the findings of scientists in its own pursuit, while also feeling free to offer criticism when those claims are ungrounded, confused, or inconsistent. In Quine's view, philosophy is \"continuous with\" science and \"both\" are empirical. Naturalism is not a dogmatic belief that the modern view of science is entirely correct. Instead, it simply holds that science is the best way to explore the processes of the universe and that those processes are what modern science is striving to understand. However, this Quinean Replacement Naturalism finds relatively few supporters among philosophers.\n\nKarl Popper equated naturalism with inductive theory of science. He rejected it based on his general critique of induction (see problem of induction), yet acknowledged its utility as means for inventing conjectures.\n\nPopper instead proposed that science should adopt a methodology based on falsifiability for demarcation, because no number of experiments can ever prove a theory, but a single experiment can contradict one. Popper holds that scientific theories are characterized by falsifiability.\n\n\n\n"}
{"id": "473319", "url": "https://en.wikipedia.org/wiki?curid=473319", "title": "Opinion leadership", "text": "Opinion leadership\n\nOpinion leadership is leadership by an active media user who interprets the meaning of media messages or content for lower-end media users. Typically the opinion leader is held in high esteem by those who accept their opinions. Opinion leadership comes from the theory of two-step flow of communication propounded by Paul Lazarsfeld and Elihu Katz. Significant developers of the theory have been Robert K. Merton, C. Wright Mills and Bernard Berelson. This theory is one of several models that try to explain the diffusion of innovations, ideas, or commercial products.\n\nMerton distinguishes two types of opinion leadership: monomorphic and polymorphic. Typically, opinion leadership is viewed as a monomorphic, domain-specific measure of individual differences, that is, a person that is an opinion leader in one field may be a follower in another field. An example of a monomorphic opinion leader in the field of computer technology, might be a neighborhood computer service technician. The technician has access to far more information on this topic than the average consumer and has the requisite background to understand the information, though the same person might be a follower at another field (for example sports) and ask others for advice. In contrast, polymorphic opinion leaders are able to influence others in a broad range of domains. Variants of polymorphic opinion leadership include market mavenism, personality strength and generalized opinion leadership. So far, there is little consensus as to the degree these concepts operationalize the same or simply related constructs.\n\nIn his article \"The Two Step Flow of Communication\", Elihu Katz, found opinion leaders to have more influence on people's opinions, actions, and behaviors than the media. Opinion leaders are seen to have more influence than the media for a number of reasons. Opinion leaders are seen as trustworthy and non-purposive. People do not feel they are being tricked into thinking a certain way about something if they get information from someone they know. However, the media can be seen as forcing a concept on the public and therefore will be less influential. While the media can act as a reinforcing agent, opinion leaders have a more changing or determining role in an individual's opinion or action.\n\nIn his article, Elihu Katz answers the question, \"Who is an opinion leader?\" One or more of these factors make noteworthy opinion leaders:\n\nOpinion leaders are individuals who obtain more media coverage than others and are especially educated on a certain issue. They seek the acceptance of others and are especially motivated to enhance their social status. In the jargon of public relations, they are called thought leaders. Research has also found that opinion leaders tend to be boundary spanners.\n\nIn a strategic attempt to engage the public in environmental issues and his nonprofit, The Climate Project, Al Gore used the concept of opinion leaders. Gore found opinion leaders by recruiting individuals who were educated on environmental issues and saw themselves as influential in their community and amongst their friends and family. From there, he trained the opinion leaders on the information he wanted them to spread and enabled them to influence their communities. By using opinion leaders, Gore was able to educate and influence many Americans to take notice of climate change and change their actions.\n\nMatthew Nisbet describes the use of opinion leaders as intermediaries between scientists and the public as a way to reach the public via trained individuals who are more closely engaged with their communities, such as \"teachers, business leaders, attorneys, policymakers, neighborhood leaders, students, and media professionals.\" Examples of initiatives that take this approach include Science & Engineering Ambassadors, sponsored by the National Academy of Sciences, and Science Booster Clubs, coordinated by the National Center for Science Education.\n\n\n\n"}
{"id": "35177618", "url": "https://en.wikipedia.org/wiki?curid=35177618", "title": "Philip Crosbie Morrison", "text": "Philip Crosbie Morrison\n\nPhilip Crosbie Morrison (19 December 1900 – 1 March 1958), generally known as \"Crosbie Morrison\", was an Australian naturalist, educator, journalist, broadcaster and conservationist.\n\nMorrison was born in Hawthorn, Victoria. He attended Auburn State School and University High School. In 1918 he became a teacher at Wesley College. He entered Melbourne University in 1921 where he studied zoology, obtaining a BSc in 1924 and a MSc in 1926.\n\nIn 1926 Morrison joined the staff of the Melbourne Argus as a journalist. In 1938 he was persuaded by Keith Murdoch to accept the position of founding editor of a new monthly magazine, \"Wild Life\". In order to promote the magazine he also began a series of weekly radio broadcasts on the ABC (through 3AR), 3UZ and 3DB. His \"Wild Life\" series on 3DB ran for over 20 years and made him an admired radio personality. It was syndicated throughout Australia on the Major Broadcasting Network. He was also a prominent panelist on 3DB's popular Information Please, which was also heard Australia-wide through the Major network.\n\nDuring the Second World War he also served for a while as the Victorian state publicity censor, and later with the broadcasting division of the federal Department of Information, until policy disagreements forced his departure. He also worked as a lecturer in natural history with the Victorian Council of Adult Education and, from 1942, with the Australian Army Education Service.\n\nMorrison had long promoted the protection of wildlife and the need for proper management of national parks in his radio broadcasts and in \"Wild Life\" magazine. In 1952 he became the inaugural chairman of the newly formed Victorian National Parks Association. In 1957 he was appoionted the first director of the Victorian National Parks Authority.\n\nMorrison married Lucy Frances Washington on 8 March 1930. He died on 1 March 1958 of a cerebral haemorrhage at his home in Brighton, survived by his wife and two sons.\n\n\nApart from numerous articles, papers and reports published in \"Wild Life\" and elsewhere, books authored by Morrison include two posthumous compilations of material from his radio broadcasts:\n"}
{"id": "344899", "url": "https://en.wikipedia.org/wiki?curid=344899", "title": "Quercus suber", "text": "Quercus suber\n\nQuercus suber, commonly called the cork oak, is a medium-sized, evergreen oak tree in the section \"Quercus\" sect. \"Cerris\". It is the primary source of cork for wine bottle stoppers and other uses, such as cork flooring and as the cores of cricket balls. It is native to southwest Europe and northwest Africa. In the Mediterranean basin the tree is an ancient species with fossil remnants dating back to the Tertiary period.\n\nIt grows to up to , although it is typically more stunted in its native environment. The leaves are long, weakly lobed or coarsely toothed, dark green above, paler beneath, with the leaf margins often downcurved. The acorns are long, in a deep cup fringed with elongated scales.\n\nIn the Portuguese town of Águas de Moura is located the Sobreiro Monumental (Monumental Cork Oak), a tree 234 years old, tall and with a trunk that requires at least five people to embrace it. It has been considered a National Monument since 1988, and the Guinness Book of Records states it as the largest and oldest in the world.\n\nThe cork oak forest is one of the major plant communities of the Mediterranean woodlands and forests ecoregion.\n\nNatural stands of cork oak can support diverse ecosystems. For example, in parts of northwestern North Africa, some cork oak forests are habitat to the endangered Barbary macaque, \"Macaca sylvanus\", a species whose habitat is fragmented and whose range was prehistorically much wider. In Western Europe, particularly in Portugal and Spain, the cork oak forests are home to endangered species such as the Iberian lynx, the most critically threatened feline in the world.\n\nAs a pyrophyte, this tree has a thick, insulating bark that makes it well adapted to forest fires. After a fire, many tree species regenerate from seeds (as, for example, the maritime pine) or resprout from the base of the tree (as, for example, the holm oak). The bark of the cork oak allows it to survive fires and then simply regrow branches to fill out the canopy. The quick regeneration of this oak makes it successful in the fire-adapted ecosystems of the Mediterranean biome.\n\nThe tree forms a thick, rugged bark containing high levels of suberin. Over time the cork cambium layer of bark can develop considerable thickness and can be harvested every 7 to 10 years to produce cork. The harvesting of cork does not harm the tree (though such activity tends to reduce its life expectancy), in fact, no trees are cut down during the harvesting process. Only the bark is extracted, and a new layer of cork regrows, making it a renewable resource. The tree is cultivated in Spain, Portugal, Algeria, Morocco, France, Italy and Tunisia. Cork oaks are considered to be soil builders and their fruits have been shown to have useful insecticidal properties. Cork oak forests cover approximately 25,000 square kilometres in those countries (equivalent to ). Portugal accounts for around 50% of the world cork harvest. Cork oaks cannot legally be cut down in Portugal, except for forest management felling of old, unproductive trees, and, even in those cases, farmers need special permission from the Ministry of Agriculture.\n\n\"Q. suber\" is commonly grown in agroforestry systems, known as \"montado\" in Portugal and \"dehesa\" in Spain. These are open woods with low tree density (50–300 trees/ha). In these systems, forage species are commonly grown under the trees and grazed by cattle during the summer.\n\nCork oaks commonly live more than 200 years. Virgin cork (or 'male' cork) is the first cork cut from generally 25-year-old trees. Another 9 to 12 years is required for the second harvest, and a tree can be harvested about twelve times in its lifetime. Cork harvesting is done entirely without machinery, being dependent solely on human labor. Usually five people are required to harvest the tree's bark, using a small axe. The process requires training due to the skill required to harvest bark without harming the tree. The European cork industry produces 300,000 tonnes of cork a year, with a value of €1.5 billion and employing 30,000 people. Wine corks represent 15% of cork usage by weight but 66% of revenues.\n\nThe cork left after stoppers have been made is used to make a wide range of products, including insulation panels, floor and wall tiles and sound-proofing in the car industry, as well as for handicrafts and artistic uses. This include \"cork paper\", used in printing, book covering, clothing manufacture, cork \"maroquinerie\" and other products. Cork is also used in making cricket balls, Hurling Ball (Sliothars), badminton shuttlecocks, handles of fishing rods and special devices for the space industry.\n\nCork oaks are sometimes planted as individual trees, providing a minor income to their owners. The tree is also sometimes cultivated for ornament. Hybrids with turkey oak (\"Quercus cerris\") are regular, both in the wild in southwest Europe and in cultivation; the hybrid is known as Lucombe oak \"Quercus\" × \"hispanica\". Some cork is also produced in eastern Asia from the related Chinese cork oak (\"Quercus variabilis\")\n\nCork oak is relatively resistant to pathogens, but some diseases occur in the species. Leaf spot can be caused by the fungus \"Apiognomonia errabunda\". Other fungi can cause leaf scorching, powdery mildew, rust, and cankers.\n\nThe most virulent cork oak pathogen may be \"Diplodia corticola\", a sac fungus which causes sap-bleeding sunken canker wounds in the wood, withering of the leaves, and lesions on the acorns. The fungus \"Biscogniauxia mediterranea\" is becoming more common in cork oak forests. Its fruiting bodies appear as charcoal-black cankers. Both of these fungi are transmitted by the oak pinhole borer (\"Platypus cylindrus\"), a species of weevil.\n\nThe common water mould \"Phytophthora cinnamomi\" grows in the roots of the tree and has been known to devastate cork oak woodlands.\n\n\n"}
{"id": "20826256", "url": "https://en.wikipedia.org/wiki?curid=20826256", "title": "Shiny Brite", "text": "Shiny Brite\n\nThe Shiny Brite company produced the most popular Christmas tree ornaments in the United States throughout the 1940s and 1950s.\n\nIn 1937, Max Eckardt established Shiny Brite ornaments, working with the Corning Glass company to mass-produce glass Christmas ornaments. Eckardt had been importing hand-blown glass balls from Germany since around 1907, but had the foresight to anticipate a disruption in his supply from the upcoming war. Corning adapted their process for making light bulbs to making clear glass ornaments, which were then shipped to Eckardt's factories to be decorated by hand. The fact that Shiny Brite ornaments were an American-made product was stressed as a selling point during World War II.\n\nDating of the ornaments is often facilitated by studying the hook. The first Shiny Brite ornaments had the traditional metal cap and loop, with the hook attached to the loop, from which the ornament was hung from the tree.\n\nWartime production necessitated the replacement of the metal cap with a cardboard tab, from which the owner would use yarn or string to hang the ornament. These hangers firmly place the date of manufacture of the ornament to the early 1940s.\n\nFollowing the war, Shiny Brite introduced a line of ornaments with a newly designed metal hook that provided the user with two lengths of hanger. The long hook traveled through the center of the ornament and exited the bottom, where it attached to the foot of the ornament. This provided the \"short\" hanger. Unlatched from the bottom, the entire length of the hook was available, allowing the ornament to dangle at a greater distance from the tree limb to which it was attached. This arrangement was designed to allow the ornament to fill sparsely limbed areas of a natural tree.\n\nThe increasing popularity of the aluminum artificial Christmas tree, first manufactured in 1958, made this device far less attractive to the consumer, as an artificial tree had no gaps to be filled. The added expense of the lengthy hanging wire coupled with the diminishing need caused this feature to be discontinued in 1960.\n\nDuring its peak, Shiny Brite had four factories in New Jersey, located in the cities of Hoboken, Irvington, North Bergen, and West New York. The company's main office and showroom were located at 45 East 17th Street in New York city.\n\nShiny Brite's most popular ornaments have been reissued under the same trademark by Christopher Radko since 2001.\n\n\n"}
{"id": "10366081", "url": "https://en.wikipedia.org/wiki?curid=10366081", "title": "Shirshov Institute of Oceanology", "text": "Shirshov Institute of Oceanology\n\nThe Shirshov Institute of Oceanology (P.P. Shirshov Institute of Oceanology (IO) RAN, ) is the premier research institution for ocean, climate, and earth science in Russia. It was established in 1946 and is part of the Russian Academy of Sciences. It is headquartered in Moscow.\n\nThe institute is named after Pyotr Shirshov, who founded it in 1946. Amongst others, Andrei Monin served as director.\n\nMathematicians Grigory Barenblatt and Andrei Monin, physical oceanographers Vladimir Shtokman and Leonid Brekhovskikh, and biologist Igor Akimushkin have been or are researchers at IORAN.\n\nExplorer and pilot of the MIR submersible to the seabed under the North Pole (a.k.a. the Arktika 2007 project) Anatoly Sagalevich and renowned Russian poet and geologist Alexander Gorodnitsky are also current researchers.\n\n\n\n"}
{"id": "261273", "url": "https://en.wikipedia.org/wiki?curid=261273", "title": "Southern Hemisphere", "text": "Southern Hemisphere\n\nThe Southern Hemisphere is the half of Earth that is south of the Equator. It contains all or parts of five continents (Antarctica, Australia, about 90% of South America, the southern third of Africa, and several southern islands off the continental mainland of Asia), four oceans (Indian, South Atlantic, Southern, and South Pacific) and most of the Pacific Islands in Oceania. Its surface is 80.9% water, compared with 60.7% water in the case of the Northern Hemisphere, and it contains 32.7% of Earth's land.\n\nOwing to the tilt of Earth's rotation relative to the Sun and the ecliptic plane, summer is from December to March and winter is from June to September. September 22 or 23 is the vernal equinox and March 20 or 21 is the autumnal equinox. The South Pole is in the center of the southern hemispherical region.\n\nSouthern Hemisphere climates tend to be slightly milder than those at similar latitudes in the Northern Hemisphere, except in the Antarctic which is colder than the Arctic. This is because the Southern Hemisphere has significantly more ocean and much less land; water heats up and cools down more slowly than land. The differences are also attributed to oceanic heat transfer (OHT) and differing extents of greenhouse trapping.\n\nIn the Southern Hemisphere the sun passes from east to west through the north, although north of the Tropic of Capricorn the mean sun can be directly overhead or due south at midday. The Sun rotating through the north causes an apparent right-left trajectory through the sky unlike the left-right motion of the Sun when seen from the Northern Hemisphere as it passes through the southern sky. Sun-cast shadows turn anticlockwise throughout the day and sundials have the hours increasing in the anticlockwise direction. During solar eclipses viewed from a point to the south of the Tropic of Capricorn, the Moon moves from left to right on the disc of the Sun (see, for example, photos with timings of the solar eclipse of November 13, 2012), while viewed from a point to the north of the Tropic of Cancer (i.e., in the Northern Hemisphere), the Moon moves from right to left during solar eclipses.\n\nCyclones and tropical storms spin clockwise in the Southern Hemisphere (as opposed to anticlockwise in the Northern Hemisphere) due to the Coriolis effect.\n\nThe southern temperate zone, a subsection of the Southern Hemisphere, is nearly all oceanic. This zone includes the southern tip of Uruguay and South Africa; the southern half of Chile and Argentina; parts of Australia, going south from Adelaide, and all of New Zealand.\n\nThe Sagittarius constellation that includes the galactic centre is a southern constellation and this, combined with clearer skies, makes for excellent viewing of the night sky from the Southern Hemisphere with brighter and more numerous stars.\n\nForests in the Southern Hemisphere have special features which set them apart from those in the Northern Hemisphere. Both Chile and Australia share, for example, unique beech species or \"Nothofagus\", and New Zealand has members of the closely related genera Lophozonia and Fuscospora. The eucalyptus is native to Australia but is now also planted in Southern Africa and Latin America for pulp production and, increasingly, biofuel uses.\n\nApproximately 800 million humans live in the Southern Hemisphere, representing only 10–12% of the total global human population of 7.3 billion. Of those 800 million people, 200 million live in Brazil, the largest country by land area in the Southern Hemisphere, while 141 million live on the island of Java, the most populous island in the world. The most populous nation in the Southern Hemisphere is Indonesia, with 261 million people (roughly 30 million of whom live north of the equator on the northern portions of the islands of Sumatra, Borneo and Sulawesi, while the rest of the population lives in the Southern Hemisphere). Portuguese is the most spoken language in the Southern Hemisphere, followed by Spanish and Javanese. \n\nThe largest metropolitan areas in the Southern Hemisphere are São Paulo (21 million people), Jakarta (18 million people), Buenos Aires (12 million people), Rio de Janeiro (11 million people), Kinshasa (11 million people) and Sydney (6 million) . The most important financial and commercial centers in the Southern Hemisphere are São Paulo, where the Bovespa Index is headquartered, along with Sydney, home to the Australian Securities Exchange, Johannesburg, home to the Johannesburg Stock Exchange and Buenos Aires, headquarters of the Buenos Aires Stock Exchange, the oldest stock market in the Southern Hemisphere.\n\nAmong the most developed nations in the Southern Hemisphere are Australia, with a nominal GDP per capita of US$51,850 and a Human Development Index of 0.939, the second highest in the world as of 2016. New Zealand is also well developed, with a nominal GDP per capita of US$38,385 and a Human Development Index of 0.915, putting it at #13 in the world in 2016. The least developed nations in the Southern Hemisphere cluster in Africa and Oceania, with Burundi and Mozambique at the lowest ends of the Human Development Index, at 0.404 (#184 in the world) and 0.418 (#181 in the world) respectively. The nominal GDP per capitas of these two countries don't go above US$550 per capita, a tiny fraction of the incomes enjoyed by Australians and New Zealanders.\n\nThe most widespread religions in the Southern Hemisphere are Christianity in South America, southern Africa and Australia/New Zealand, followed by Islam in most of the islands of Indonesia and in parts of southeastern Africa, and Hinduism, which is mostly concentrated on the island of Bali and neighboring islands. \n\nThe oldest continuously inhabited city in the Southern Hemisphere is Bogor, in western Java, which was founded in 669 CE. Ancient texts from the Hindu kingdoms prevalent in the area definitively record 669 CE as the year when Bogor was founded. However, there is some evidence that Zanzibar, an ancient port with around 200,000 inhabitants on the coast of Tanzania, may be older than Bogor. A Greco-Roman text written between 1 CE and 100 CE, the \"Periplus of the Erythraean Sea\", mentioned the island of \"Menuthias\" (Ancient Greek: Μενουθιάς) as a trading port on the east African coast, which is probably the small island of Unguja on which Zanzibar is located. The oldest proven archaeological site in the Southern Hemisphere is Sechin Bajo, located on the coast of northern Peru, and dates back to 3600 BCE. Sechin Bajo may also be the oldest site for monumental architecture in the Americas.\n\n"}
{"id": "6906718", "url": "https://en.wikipedia.org/wiki?curid=6906718", "title": "Subsun", "text": "Subsun\n\nA subsun or \"sub-sun\" is a glowing spot that can be seen within clouds or haze when observed from above. The subsun appears directly below the sun, and is caused by its light reflecting off of numerous tiny ice crystals suspended in the atmosphere. As such the effect belongs to the family of halos. The region of ice crystals acts as a large mirror, creating a virtual image of the sun which appears below the horizon, analogous to the sun's reflection in a body of water. \n\nThe ice crystals responsible for a subsun are typically in the shape of flat hexagonal plates. As they fall through the air, their aerodynamic properties cause them to orient themselves horizontally, i.e., with their hexagonal surfaces parallel to the Earth's surface. When they are disturbed by turbulence, however, the plates start to \"wobble\", causing their surfaces to deviate some degrees from the ideal horizontal orientation, and causing the reflection (i.e., the subsun) to become elongated vertically. When the deviation is sufficiently large, the subsun is stretched into a vertical column known as a lower sun pillar.\n\n"}
{"id": "20631045", "url": "https://en.wikipedia.org/wiki?curid=20631045", "title": "Sverdrup Basin Magmatic Province", "text": "Sverdrup Basin Magmatic Province\n\nThe Sverdrup Basin Magmatic Province is a large igneous province located on Axel Heiberg Island and Ellesmere Island, Nunavut, Canada near the rifted margin of the Arctic Ocean at the end of Alpha Ridge. \n\nWith an area of 550,110 km, the Sverdrup Basin Magmatic Province forms part of the larger High Arctic Large Igneous Province and consists of flood basalts, dikes and sills which form two volcanic formations called the Ellesmere Island Volcanics and Strand Fiord Formation.\n\nThe flood basalt lava flows are similar to those of the Columbia River Basalt Group in the U.S. states of Washington, Oregon and Idaho.\n\n\n"}
{"id": "15339373", "url": "https://en.wikipedia.org/wiki?curid=15339373", "title": "The Lion Children", "text": "The Lion Children\n\nThe Lion Children is a story about a group of children who are taken to Botswana in 1995 by their mother Kate Nicholls to study the behavior of lions. \n\nThe book was praised by British biologist Richard Dawkins who wrote a foreword and said, \"This is an astonishing book, by an even more astonishing group of children.\"\n"}
{"id": "22214392", "url": "https://en.wikipedia.org/wiki?curid=22214392", "title": "The Survival Handbook", "text": "The Survival Handbook\n\nThe Survival Handbook: A Practical Guide to Woodcraft and Woodlore is a book written by author, television presenter and outdoorsman Ray Mears. It was first published on 1 March 1990 by The Oxford Illustrated Press and then re-printed by The Promotional Reprint Co Ltd in 1994. It is a guidebook to outdoor life, survival and camping. The difference between the two versions being that the colour photographs were printed on glossy paper in the First Edition. It contains sections on the basics of outdoor skill, making fire by friction, obtaining food, and working with stone, flint and bone as well as working animal hide.\n\n\n"}
{"id": "534079", "url": "https://en.wikipedia.org/wiki?curid=534079", "title": "Timeline of discovery of Solar System planets and their moons", "text": "Timeline of discovery of Solar System planets and their moons\n\n<onlyinclude>\nThe timeline of discovery of Solar System planets and their natural satellites charts the progress of the discovery of new bodies over history. Each object is listed in chronological order of its discovery (multiple dates occur when the moments of imaging, observation, and publication differ), identified through its various designations (including temporary and permanent schemes), and the discoverer(s) listed.\n\nHistorically the naming of moons did not always match the times of their discovery. Traditionally, the discoverer enjoys the privilege of naming the new object; however, some neglected to do so (E. E. Barnard stated he would \"defer any suggestions as to a name\" [for Amalthea] \"until a later paper\" but never got around to picking one from the numerous suggestions he received) or actively declined (S. B. Nicholson stated \"Many have asked what the new satellites\" [Lysithea and Carme] \"are to be named. They will be known only by the numbers X and XI, written in Roman numerals, and usually prefixed by the letter J to identify them with Jupiter.\"). The issue arose nearly as soon as planetary satellites were discovered: Galileo referred to the four main satellites of Jupiter using numbers while the names suggested by his rival Simon Marius gradually gained universal acceptance. The International Astronomical Union (IAU) eventually started officially approving names in the late 1970s.</onlyinclude>\n\nIn the following tables, planetary satellites are indicated in bold type (e.g. Moon) while planets and dwarf planets, which directly circle the Sun, are in italic type (e.g. \"Earth\"). The Sun itself is indicated in roman type. The tables are sorted by publication/announcement date. Dates are annotated with the following symbols:\nIn a few cases, the date is uncertain and is then marked \"(?)\".\n\n\"* Note: Moons marked by an asterisk (*) had complicated discoveries. Some took years to be confirmed, and in several cases were actually lost and rediscovered. Others were found in Voyager photographs years after they were taken.\"\n\nThe planets and their natural satellites are marked in the following colors:\n\n\n\n<onlyinclude>\n</includeonly></onlyinclude>\n\n\n"}
{"id": "2151659", "url": "https://en.wikipedia.org/wiki?curid=2151659", "title": "West Antarctica", "text": "West Antarctica\n\nWest Antarctica, or Lesser Antarctica, one of the two major regions of Antarctica, is the part of that continent that lies within the Western Hemisphere, and includes the Antarctic Peninsula. It is separated from East Antarctica by the Transantarctic Mountains and is covered by the West Antarctic Ice Sheet. It lies between the Ross Sea (partly covered by the Ross Ice Shelf), and the Weddell Sea (largely covered by the Filchner-Ronne Ice Shelf). It may be considered a giant peninsula stretching from the South Pole towards the tip of South America.\n\nWest Antarctica is largely covered by the Antarctic ice sheet, but there have been signs that climate change is having some effect and that this ice sheet may have started to shrink slightly. The coasts of the Antarctic Peninsula are the only parts of West Antarctica that become (in summer) ice-free. These constitute the Marielandia Antarctic tundra and have the warmest climate in Antarctica. The rocks are clad in mosses and lichens that can cope with the intense cold of winter and the short growing-season.\n\nLying on the Pacific Ocean side of the Transantarctic Mountains, West Antarctica comprises the Antarctic Peninsula (with Graham Land and Palmer Land) and Ellsworth Land, Marie Byrd Land and King Edward VII Land, offshore islands such as Adelaide Island, and ice shelves, notably the Filchner-Ronne Ice Shelf on the Weddell Sea, and the Ross Ice Shelf on the Ross Sea. The Ross Sea and the Weddell Sea separate West Antarctica from the main land-mass of the continent: one can think of West Antarctica as a giant peninsula that stretches roughly from the South Pole towards the southern tip of South America.\n\nWest Antarctica was named in the early 20th century (Balch, 1902; Nordenskiöld, 1905); that usage became standard following the International Geophysical Year (1957–1958) and explorations which disclosed that the Transantarctic Mountains provide a useful regional border between West Antarctica and East Antarctica. Advisory Committee on Antarctic Names (US-ACAN) approved the name in 1962.\n\nWest Antarctica is mostly covered by a massive ice sheet referred to as the West Antarctic Ice Sheet (WAIS). In recent decades this ice sheet has shown signs of decreasing mass.\n\nUnlike East Antarctica, West Antarctica has shown some effects of global warming.\n\nThe parts of West Antarctica not covered with ice (Antarctic oasis), which are the coasts of the Antarctic Peninsula, constitute a biodiversity region known as Marielandia Antarctic tundra (after Marie Byrd Land). This area has the warmest climate in Antarctica and the moss and lichen-covered rocks are free of snow during the summer months, although the weather is still intensely cold and the growing season very short.\n\n"}
{"id": "42500373", "url": "https://en.wikipedia.org/wiki?curid=42500373", "title": "Wind power in Thailand", "text": "Wind power in Thailand\n\nWind power in Thailand amounted to an installed production capacity of 224.5 MW as of the end of 2014. Installed capacity was 112 MW at the end of 2012, with 111 MW added in 2013, and a minor amount added in 2014. This ranked Thailand 46th in the world by installed capacity as of 2015.\n\nThailand's natural gas reserves are projected to run out in 2021, and Thailand began importing expensive liquefied natural gas in 2011. These factors have led to increased demand for renewable energy, and Thailand's Alternative Energy Development Plan (AEDP) in 2011 called for 25 percent of its energy to come from renewable sources by 2036. By June 2012, projects totalling over 1,600 MW had been proposed.\n\nWith increasing demand for energy, Thailand found itself dependant on energy imported from other countries, mainly oil and natural gas. This, along with repeated occurrence of oil crises, raised awareness of renewable energy since The Fifth National Economic and Social Development Plan (1983-1987). The support for renewable energy became clear when the National Energy Policy Council Act was declared in 1992. The act started the energy conservation plan, which aimed to decrease the amount of imported energy by developing renewable energy sources in Thailand, including wind power.\n\nThe first study of Thai wind energy potential was conducted in 1975 by the Department of Energy Development and Promotion, which is now called the Department of Alternative Energy Development and Efficiency (DEDE). Data of mean wind speed from the Thai Meteorological Department (TMD) was used to produce a wind map of areas with medium to high potential. Later, in 1981, King Mongkut's Institute of Technology Thonburi (KMITT) and King Mongkut's Institute of Technology North Bangkok (KMITNB), in cooperation with Electricity Generating Authority of Thailand (EGAT), produced another wind resource map with 13 year-long data (1966-1978) from 53 wind speed measurement stations of the TMD, using power law to standardize all wind speed to 10 m. In 1984, another similar attempt was carried out by KMITT, using 17 year-long (1966-1982) data from 62 measurement stations, and with financial support from USAID.\n\nAttempts at producing wind resource maps of Thailand all faced the problem of shortage in wind speed data, especially offshore and at high elevations. This resulted in maps with low coverage. In 2001, another set of wind resource maps was produced by DEDE using data from over 150 measurement stations including offshore stations and high elevation stations. The data was processed using computer simulation and mathematical modelling, and the map and data were also published in electronic form. In the same year, the World Bank offered another set of wind resource maps for four countries: Cambodia, Laos, Vietnam, and Thailand, which was calculated from global wind data and each country's geographical data using computer simulations.\n\nIn 2008, wind speed data in southern Thailand was further calibrated with the setup of more stations in six provinces of southern Thailand. Southern Thailand was shown in previous studies to have the highest wind energy potential in Thailand. The stations were installed to measure wind speeds at 80, 90, and 100 m heights. Also, from 2007 to 2009 wind speed and direction were measured and recorded from several stations in the northern Thailand.\n\nIn 2011, research to improve the wind map was conducted by the Department of Physics of Silpakorn University. The research produced mesoscale wind maps with resolution of 3x3 km cells using atmospheric model and computer simulation software, and also experimented with the making of microscale wind maps, which could be the next step in the study of Thai wind energy potential.\n\nIn 1983, Thailand had its first set of electricity generating wind turbines, consisting of six turbines, installed at Laem Phromthep in Phuket Province by EGAT as a pilot project. The generated electricity was used to power nearby research stations. The outcome was satisfying, so in 1988 EGAT planned to connect the turbines to the power grid of the Provincial Electricity Authority (PEA), and in 1990 they started operating. This was the first time Thailand had electricity generated by wind power supplying the power grid. Later, in 1992, two more turbines with 10 kW of capacity were installed and connected to the grid.\n\nLater, both government and private organizations, especially educational institutes, had more interest in the potential in wind power in Thailand. In 1996, KMUTT was the first organization to install 2.5 kW and 10 kW wind turbines at Phu Kradueng National Park in Loei Province, and Tarutao National Marine Park in Satun Province. Later in the same year, the Recycle Engineering Company Limited installed one 150 kW wind turbine at its facility at Ko Chang District in Chonburi Province, becoming Thailand's first private wind turbine.\n\nOn the government side, in 2007, DEDE installed wind turbine of 250 kW capacity at Hua Sai District in Nakhon Si Thammarat Province, and later in 2009, another of 1.5 MW capacity. Also in 2009, two wind turbines of 1.25 MW capacity each were installed by EGAT at the upper reservoir of Lam Takhong Cholapawattana power plant, a hydroelectric power plant in Sikhio District, Nakhon Ratchasima Province, becoming Thailand’s first large electricity-generating wind power plant.\n\nThailand has relatively low average wind speeds with most areas being of class 1-1.4 wind speed, or about 2.8–4 m/s measured at 10 m. This is because Thailand is near the equator which has generally low wind speed. In general, Thailand's inland winds are sub-par, but there are areas with topography such as mountain ranges, canyons, and slopes that help increase wind speeds. \n\nA study conducted in 2001 for the World Bank found limited potential for large-scale wind power in Thailand. On a land area basis, only 761 km or 0.2 percent of Thailand's land mass was found to have \"good to excellent\" winds. Prospects for small-scale village wind power were found to be more promising. Seventy-three percent of the rural population lived in areas with \"fair to good\" wind resources. \n\nNevertheless, Thailand still has some areas with utilizable wind speeds of no less than class 3, that is, with no lower than 6.4 m/s annual average wind speed. This is caused by the two monsoons that affect Thailand annually, the northeast monsoon and the southwest monsoon. The northeast monsoon comes from the South China Sea during the period between November and March, producing strong wind in the Gulf of Thailand and the coastal areas of southern Thailand. The southwest monsoon comes from the Indian Sea between May and October, producing strong wind at the peaks of mountain ranges in the west part of upper southern and lower northern Thailand.\n\nFor offshore wind, there are some areas with high wind speeds in Bandon Bay in Surat Thani Province, Pattani Gulf in Songkhla, and Pattani Province, and Songkhla Lake (actually a lagoon) in Songkhla Province.\n\nAs of the end of 2014, Thailand's wind power capacity stood at 224.5 MW, generating 305 GWh of energy throughout the year. This ranks Thailand 46th in the world by wind power capacity.\n\nThe spikes in increase in capacity in 2012 and 2013 were caused by the construction of \"First Korat Wind\" wind farm and \"K.R. Two\" wind farm by Wind Energy Holding Co., Ltd., founded by Nopporn Suppipat, in each year, respectively. Each wind farm is composed of 45 wind turbines, each with 2.3 MW capacity, resulting in wind farms with highest capacities in Southeast Asia.\n\nThe latest plan to develop alternative energy in Thailand is the Alternative Energy Development Plan (AEDP). The plan aims to increase the share of renewable energy in the total energy production to 25 percent. The share of renewable energy stands at 11.91 percent as of 2014.\n\nFormerly, the AEDP was planned to span 2012 to 2021 and to employ an Adder system: subsidies for independent sales of wind power from small and very small power producers, aiming to promote electricity generation from wind power. However, under the lead of Prime Minister General Prayut Chan-o-cha, the plan was revised to span 2015 to 2036. FiT (Feed-in-tariff) system is used instead of Adder, which calculates sales price of energy from real development cost of the energy producer, but with premium extra rate of 0.50 baht/kWh for the southern border provinces. This prevents the price of energy from increasing too much.\n\nAlso, Energy Absolute Public Company Limited, headed by Somphote Ahunai, is constructing three wind farm projects in Nakhon Si Thammarat Province with combined installed capacity of 126 MW. It also plans to start construction of five more projects in 2017 in Chaiyaphum Province, northeast Thailand, which will produce 260 MW of electricity. As of 2012, total capacity of proposed big scale wind farm projects that are accepted by the government amounts to 787.37 MW, with 1674.20 MW more yet to be confirmed.\n\nAccording to a research study by King Mongkut's University of Technology Thonburi, the cost of wind energy production in Thailand ranges from about 2-6 baht/kWh, but in some unsuitable areas the cost can be as high as 11 baht/kWh. When compared to the cost of production in Denmark at about 45 øre (about 2.36 baht) /kWh, Thailand's cost is relatively higher. But with PEA buying electricity with FiT system at the typical rate of about six baht/kWh, production by very small power producers (VSPP) is possible.\n\nAs a result of the government's continuous supporting policy, renewable energy consumption has grown dramatically. This leads to an increase in investments from private organizations in renewable energy industry. Most recently in 2014, combined net value of investment in renewable energy both from the government and private organizations amounted to 84,588 million baht. Of this amount, the wind energy industry received the highest proportion of the investment, amounting to 25,720 million baht or 30.4 percent of total investment.\n\n\n"}
{"id": "14542902", "url": "https://en.wikipedia.org/wiki?curid=14542902", "title": "Wrangellia Terrane", "text": "Wrangellia Terrane\n\nThe Wrangellia Terrane (named for the Wrangell Mountains, Alaska) is a terrane extending from the south-central part of Alaska through southwestern Yukon and along the Coast of British Columbia in Canada. Some workers contend that Wrangellia extends southward to Oregon, although this is not generally accepted.\n\nThe term Wrangellia is confusingly applied to all of: the Wrangell(ia) Terrane alone; a composite terrane (CT) consisting of the Wrangell Terrane, Peninsular Terrane, and other rock units that were not originally part of the North American craton; and a composite terrane which also includes the Alexander Terrane. Earlier workers sometimes used the term, \"Talkeetna Superterrane,\" to describe Wrangellia.\n\nThere are two conflicting hypotheses whether the Wrangellia Superterrane originated at polar or equatorial latitudes. One hypothesis proposes that Wrangellia accreted at a northerly latitude near it current location (when North America, or Laurentia, was located farther east as part of Pangaea), the other that it originated south of its current location, approximately where Baja California is now located. The former hypothesis is favoured in most plate tectonic reconstructions since the latter introduces rapid, implausible displacements of Wrangellia across the Panthalassic Ocean.\n\nRocks of Wrangellia (the individual terrane, not the CT) were originally created in the Pennsylvanian to the Jurassic somewhere, but probably near the equator, in the Panthalassic Ocean off of the west coast of the North American craton as island arcs, oceanic plateaus, and rock assemblages of the associated tectonic settings. Although composed of many different rocks types, of various composition, age, and tectonic affinity, it is the Late Triassic flood basalts that are the defining unit of Wrangellia. These basalts, extruded onto land over 5-million years about 230 million years ago, on top of an extinct Pennsylvanian and Permian island arc, constitute a large igneous province, currently exposed in a long belt. \n\nWrangellia collided and amalgamated with the Alexander Terrane by Pennsylvanian time. By the end of the Triassic, the Peninsular Terrane had also joined the Wrangellia CT. A subduction zone existed on the west side of Wrangellia. Seafloor rocks too light to be subducted were instead compressed against the western edge of Wrangellia; these rocks are now known as the Chugach Terrane. A complex fault system, known as the Border Ranges Fault, is the modern expression of the suture zone between Wrangellia and Chugach Terranes. Over time, plate tectonics moved this amalgamation of crust generally northeastward into contact with the North American continental margin. The Wrangellia CT collided with and docked to North America by Cretaceous time. Strike-slip displacement, with Wrangellia travelling northward, continued after docking, although the amount of post-accretion displacement is controversial.\n\n\n"}
