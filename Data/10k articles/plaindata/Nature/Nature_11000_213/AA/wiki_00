{"id": "5493846", "url": "https://en.wikipedia.org/wiki?curid=5493846", "title": "Aganju", "text": "Aganju\n\nAganju (known as Agayú or Aganyú in Latin America) is an Orisha. He is syncretized with Saint Christopher. Aganju is strongly associated with Shango, being either Shango's father or his brother; both Orishas being members of the deified royal family of Oyo. \n\nIn the Yoruba areas of Nigeria and Benin Republic, Aganju is known as a deified warrior king from the town of Shaki in the present-day Oyo State of Nigeria. He was said to walk with a sword. Shaki is in the savannah area of northern Yorubaland that has monoliths and boulder outcroppings.\n\nIn Cuba, Aganju is a volcano deity for the practitioners of Santeria/Lucumi religion. But there are no volcanoes in Yorubaland, nor is Aganju associated with volcanoes among the Yoruba people.\n\nIn the Afro-Brazilian tradition of Candomblé, Aganjú is worshiped as a manifestation or quality of the Orisha Shango, often called \"Xango Aganjú\". Aganjú represents all that is explosive and lacking control. He is also nicknamed \"Xangô menino\" among Candomblé practitioners.\n\n"}
{"id": "323413", "url": "https://en.wikipedia.org/wiki?curid=323413", "title": "Alpenglow", "text": "Alpenglow\n\nAlpenglow (from , Italian: \"Enrosadira\") is an optical phenomenon that appears as a horizontal reddish glow near the horizon opposite of the Sun when the solar disk is just below the horizon. This effect is easily visible when mountains are illuminated, but can also be seen when clouds are lit through backscatter.\n\nSince the sun is below the horizon, there is no direct path for the sunlight to reach the mountain. Unlike sunrise or sunset, the light that causes alpenglow is reflected off airborne precipitation, ice crystals, or particulates in the lower atmosphere. These conditions differentiate between a normal sunrise or sunset, and alpenglow.\n\nThe term is generally confused to be any sunrise or sunset light reflected off the mountains or clouds, but true alpenglow is not direct sunlight and is only visible after sunset or before sunrise.\n\nAfter sunset, if mountains are absent, the aerosols in the eastern part of the sky can be lit in the same way by the remaining scattered red light straddling the fringe of Earth's shadow (the terminator). This backscattered light projects a pinkish band opposite of the sun.\n\n"}
{"id": "50221117", "url": "https://en.wikipedia.org/wiki?curid=50221117", "title": "Altınbeşik Cave National Park", "text": "Altınbeşik Cave National Park\n\nAltınbeşik Cave National Park (), established on August 31, 1994, is a national park in southern Turkey. It is located in the İbradı district of Antalya Province.\n"}
{"id": "16899513", "url": "https://en.wikipedia.org/wiki?curid=16899513", "title": "Atomic fountain", "text": "Atomic fountain\n\nAn atomic fountain is a cloud of atoms that is tossed upwards in the Earth's gravitational field by lasers. If it were visible, it would resemble the water in a fountain. While weightless in the toss, the atoms are measured to set the frequency of an atomic clock.\n\nThe primary motivation behind the development of the atomic fountain derives from the Ramsey method of measuring the frequency of atomic transitions. In broad strokes, the Ramsey method involves exposing a cloud of atoms to a brief radiofrequency (rf) electromagnetic field; waiting a time \"T\"; briefly exposing the cloud to the rf field again; and then measuring what fraction of the atoms in the cloud have transitioned. If the frequency of the rf field is identical to the atomic transition frequency, 100% of the atoms will have transitioned; if the frequency of the field differs slightly from the transition frequency, some of the atoms will not have transitioned. By repeatedly sending clouds of atoms through such an apparatus, the frequency of the field can be adjusted to match the atomic transition frequency.\n\nThe precision of the Ramsey method can be increased by increasing the wait time \"T\" of the cloud. The use of an atomic fountain with a cooled atomic cloud allows for wait times on the order of one second, which is vastly greater than what can be achieved by performing the Ramsey method on a hot atomic beam. This is one reason why NIST-F1, a cesium fountain clock, can keep time more precisely than NIST-7, a cesium beam clock.\n\nThe idea of the atomic fountain was first proposed in the 1950s by Jerrold Zacharias. Zacharias attempted to implement an atomic fountain using a thermal beam of atoms, under the assumption that the atoms at the low-velocity end of the Maxwell–Boltzmann distribution would be of sufficiently low energy to execute a reasonably sized parabolic trajectory. However, the attempt was not successful because fast atoms in a thermal beam strike the low-velocity atoms and scatter them.\n"}
{"id": "8670379", "url": "https://en.wikipedia.org/wiki?curid=8670379", "title": "Autobiogeography", "text": "Autobiogeography\n\nAutobiogeography is a self-referential map or other geographic document created by the subject. It is a convergence of autobiography and geography that indicates geolocation of personal experiences such as travel, personal migration or important experiences. The first use of autobiogeography documented online was in the Summer 2002 Reconstruction.org academic peer review journal. The technique was popularized in 2005 by internet trends encouraging social mapping and personal mapping.\n\nAn autobiogeography can take many forms, from formal anthropological study to loose, intimate autobiogeographies found on social mapping websites. Unlike autobiographies, broad celebrity is less important than relevance to a small social circle or to oneself. For this reason, an interesting subject for an autobiogeography may be someone relatively unknown.\n\n"}
{"id": "4462637", "url": "https://en.wikipedia.org/wiki?curid=4462637", "title": "Azhar Mansor", "text": "Azhar Mansor\n\nDato' Azhar Mansor is the first Malaysian to sail solo around the world. He made his trip in 1999, sailing the ship \"Jalur Gemilang\". His round the world trip, with stops, took 190 days, 6 hours 57 minutes and 2 seconds\n\nDatuk Azhar has managed to set a new world record via an East about route, attempted by no-one and has been verified by the WSSRC (World Speed Sailing Record Council) as an official record.\n\nAzhar is currently based in Langkawi managing Telaga Harbour Park, one of Malaysia's finest marinas.\n\n"}
{"id": "36880490", "url": "https://en.wikipedia.org/wiki?curid=36880490", "title": "BWF World Ranking", "text": "BWF World Ranking\n\nThe BWF World Ranking is the official ranking of the Badminton World Federation for badminton players who participate in tournaments sanctioned by Badminton World Federation. It is used to determine the qualification for the World Championships and Summer Olympic Games, as well as BWF World Tour tournaments. Seedings of draws at all BWF-sanctioned tournaments are conducted using the BWF World Ranking.\nPlayers under 19 years of age are eligible to rank in the BWF World Junior Ranking, which were introduced in January 2011. The following lists are the rankings:\n\nThe ranking points are awarded based on the level and progress of the tournament from each player/pair. Ranking points calculated are based on the tournaments each players/pairs participate in from the last 52 weeks. If a player or pair has participated in ten or fewer World Ranking tournaments, then the ranking is worked out by adding together the points won at tournaments in the last 52 weeks. If a player or pair has participated in 11 or more World Ranking tournaments, only the 10 highest points scored in the tournaments during the 52-week period count towards their ranking. The highest possible ranking points are 116,000.\n\nPoints were awarded according to the following table from 2007 to 2017:\n\nSince 2018, BWF has started a new system for counting points:\n\nThe following is a list of players who have achieved the number one position since 1 January 2009 (active players in purple, and current number 1 players are marked in bold):\n\n\"Last update: 29 November 2018\"\n\nThe following is a list of players who were ranked world no. 5 or higher but not no. 1 in the period since the introduction of the BWF computer rankings (active players in purple):\n\n\"Last update: 29 November 2018\"\n"}
{"id": "26961795", "url": "https://en.wikipedia.org/wiki?curid=26961795", "title": "Black Virgin Mountain", "text": "Black Virgin Mountain\n\nBlack Virgin Mountain ( meaning \"Black Lady Mountain\", ) is a mountain in the Tây Ninh Province of Vietnam. To the Vietnamese the mountain is the center of a myth about \"Bà Đen\", a local deity of Khmer origin. During the Vietnam War the area around the mountain was very active as the Ho Chi Minh Trail ended a few kilometers west across the Cambodian border. As such there were many battles and American and Vietnamese soldiers based in the region remember the prominent landmark. After the war the mountain turned from a battleground to being famous for its beautiful temples and theme park.\n\nAt , the extinct volcano rises from the flat Mekong Delta jungle and farmland. The mountain is almost a perfect cinder cone with a saddle and a slight bulge on her northwest side. The mountain is honeycombed with caves and is covered in many large basalt boulders. The mountain is located approximately 10 km northeast of Tây Ninh and 96 km northwest of Ho Chi Minh City.\n\nA species of gecko, \"Gekko badenii\", is named for the mountain and is endemic to the mountain.\n\nVariations of the legend of \"Núi Bà Đen\" exist. The oldest Khmer myth involves a female deity, \"Neang Khmau\" who left her footprints on the mountain rocks. The Vietnamese myth centers around a woman, \"Bà Đen\", falling in love with a soldier and then through betrayal or suicide \"Bà Đen\" dies on the mountain. It has special significance to the Vietnamese Buddhist population and has a famous shrine about two thirds of the way up the mountain. Also, to the Cao Dai sect the mountain has special religious significance and its temple, the Tay Ninh Holy See, is close to the mountain.\n\nDuring World War II the mountain was occupied by the Japanese and it was occupied by the Viet Minh, the French and the Vietcong.\n\nThe Mekong Delta is generally a flat region with the exception of the Black Virgin Mountain. The mountain commands everything in its sight and was therefore a strategic location for both sides during the war. In May 1964 the mountain top was assaulted by the Special Forces 3rd MIKE Force and the peak was held by American forces as a radio relay station. Supplied by helicopter for much of the war the Americans controlled the top and the Vietcong controlled the bottom and surrounding foothills.\n\nThe base was occupied by over 140 Americans when on the night of 13 May 1968 the base was attacked and overrun by the Vietcong. By 02:30 on 14 May the Vietcong had been driven off by gunship and artillery fire. The results of the attack were 24 U.S. killed, 2 U.S. MIA and 25 Vietcong killed.\n\nIn January 1969 the mountain was extensively searched by 1st Brigade elements of the 3rd Battalion, 22nd Infantry, Ranger team 1-3 from the 75th Ranger Division, 4th Battalion, 23rd Infantry, and tanks from the 2nd Battalion, 34th Armor. In the tunnels that honeycombed the mountain they found arms caches, and engaged Vietcong units stationed on the mountain. Throughout the war the Vietcong returned to the mountain and its cave bases.\n\nColonel Donald Cook was the first Marine captured in the Vietnam war. For a time he was held near Black Virgin mountain. In 1973 the mountain was closed as an American base. During the closing days of the war when the mountain was abandoned by Army of the Republic of Vietnam troops, the local population left the region afraid of the approaching communist soldiers.\n\nThe mountain is famed for its beautiful views. Visitors may hike up trails but many people take the Núi Bà Đen gondola lift to the top of the temple complex. Many of the trails up the mountain are dangerous when wet.\n\nCommon fruit orchards on the mountain and in the neighborhood grow the custard apple, which is called the mountain custard apple by the local citizens, bananas, or cashews.\n\n\n"}
{"id": "2266146", "url": "https://en.wikipedia.org/wiki?curid=2266146", "title": "Cape Doctor", "text": "Cape Doctor\n\n\"Cape Doctor\" is the local name for the strong, often persistent and dry south-easterly wind that blows on the South African coast from spring to late summer (September to March in the southern hemisphere). It is known as the Cape Doctor because of a local belief that it clears Cape Town of pollution and 'pestilence'.\n\nAlthough the wind blows over a wide area of the Western Cape Province, it is notorious especially in and around the Cape Peninsula, where it can be unpleasantly strong and irritating. Capetonians also call it \"the South-Easter\". \n\nThe South Easter is usually accompanied by fair weather. However, if the South-Easter is accompanied by a cut-off low as occasionally happens in the spring and autumn months, this can cause heavy rains to fall over the Western Cape. This phenomenon is popularly known as a Black South-Easter. The Laingsburg flood of January 1981 was caused by heavy rains as part of a Black South Easter.\n\nIt is ironic that the meteorological records for Cape Town show that the north-westerly winds of winter can be far stronger than the South-Easter, while these winds are not given such a positive name. This could be because the north-westerly winds are usually accompanied by rain, which can fall for days and even weeks.\n\n\n"}
{"id": "21161280", "url": "https://en.wikipedia.org/wiki?curid=21161280", "title": "Carbon grid", "text": "Carbon grid\n\nCarbon Grid Fibre-reinforced plastic (FRP) structures are used for reinforcing concrete.\n\nCarbon fiber FRP Grids are structural reinforcement materials that improve the performance of concrete structures such as insulated wall panels, architectural panels, double tee parking garage tee beams, concrete countertops and other products. The carbon grids can be used in place of welded wire mesh in most structures and is imbedded the same way as a welded wire mesh. Carbon fiber's high strength, high modulus, resistance to creep and excellent fatigue properties allow the carbon grid to provide excellent crack and structural reinforcement. Ideally the carbon tows (yarns) are constructed on top of each other in a superimposed design those aides with fiber alignment and toughness of the finished grid.\n\nConcrete is inherently strong in compression and weak in tension. To address this issue, concrete is often reinforced with steel in the form of rebar, welded wire mesh or stressing strands (Prestressed or post-tensioned). In theory, the concrete has to crack when loaded in tension for the steel to begin to share the load in the steel reinforced concrete composite. The concrete helps protect the steel by providing an alkaline environment (ph=13 in many cases) to retard corrosion of the steel. To properly protect the steel the concrete needs to cover the steel by more than the minimum thickness, not have large cracks and not have its chemistry altered by environmental factors like chloride attack from deicing salts, carbonation, etc.. ACI and PCI (American Concrete Institute, Precast/Prestressed Concrete Institute) codes specify minimum cover thicknesses depending on the application of the structure. In practice, steel often corrodes due to improper placement of steel reinforcement, moisture drive through cracks, poor workmanship, environmental effects such as deicing salts or coastal environments and a myriad of other causes. Due to the inherent properties of steel reinforced concrete, many structures are expensive, heavy and costly to maintain. The repair of these structures is a multibillion-dollar business worldwide according to industry sources (ICRI International Concrete Repair Institute).\n\nMulti-layer concrete structures (like insulated walls panels) use a sandwich technology that use insulation layers placed in between two or more layers of concrete. Often these concrete layers are tied together by sections of solid concrete, metal tie structures or FRP pin structures. Often, the design of the wall panel is limited by lack of effective insulation in the tie areas or poor structural performance of the tie design or solid section. These limitations cause structures to be weaker, heavier and more costly both in manufacturing and ownership.\n\nCarbon grids provide a corrosion resistant reinforcement that has a higher modulus of elasticity than steel that is imbedded inside concrete (similar to conventional steel mesh). This allows for designs that don't require as much concrete cover, so concrete structures can be lighter and much more durable than before. For instance, using carbon fiber grid in insulated wall panels can provide a stronger, lighter, more thermally efficient panel than is practical with conventional designs. Due to the lighter weight less energy is required to provide the necessary concrete and ship products adding an environmental incentive to using this technology.\n\nFRP fabrics that are used as external reinforcements are impregnated with polymer on the job site and have to be bonded to a surface of a concrete structure. In contrast, carbon grids provide an internal reinforcement and have fibers that are impregnated with a polymer (typically epoxy) in a factory rather than on the job site, saving time and ensuring consistent quality. Also, the open spaces of the grid provide a mechanical bond to transfer load within the concrete structure.\n\nAdditional benefits of the carbon grid structure are that it is light, easy to handle, non-magnetic and easier to cut than steel. These properties allow for use where steel is impractical or cumbersome.\n\nCarbon grids have been manufactured by Chomarat North America (formerly TechFab LLC) since 1998 and have found commercial uses in concrete countertops, shotcrete, ornamental concrete, caststone products, precast insulated wall panels, double tee beams and a myriad of other concrete products. These materials have also been used in numerous repair projects including the Naumburg Band shell in New York City cited on the website www.carbongrid.com. A variety of product strengths and apertures are offered depending on the application.\n\nThe main limitation of the carbon grid is the additional cost of carbon as compared to mild carbon steel (with rising energy costs this gap has narrowed). In addition, since carbon is a linear elastic material (without a yield point), reinforced concrete structures have to be designed to account for this. Carbon fiber grids can be used in design by following the American Concrete Institute - ACI 440 methodology similar to FRPrebar. Typically, the structure will also use steel in an appropriate location or be over designed so that the structure fails by crushing the concrete in tension providing ductility.\n\n\n"}
{"id": "144428", "url": "https://en.wikipedia.org/wiki?curid=144428", "title": "Degenerate matter", "text": "Degenerate matter\n\nDegenerate matter is a highly dense state of fermionic matter in which particles must occupy high states of kinetic energy in order to satisfy the Pauli exclusion principle. The description applies to matter composed of electrons, protons, neutrons or other fermions. The term is mainly used in astrophysics to refer to dense stellar objects where gravitational pressure is so extreme that quantum mechanical effects are significant. This type of matter is naturally found in stars in their final evolutionary states, like white dwarfs and neutron stars, where thermal pressure alone is not enough to avoid gravitational collapse.\n\nDegenerate matter is usually modelled as an ideal Fermi gas, an ensemble of non-interacting fermions. In a quantum mechanical description, particles limited to a finite volume may take only a discrete set of energies, called quantum states. The Pauli exclusion principle prevents identical fermions from occupying the same quantum state. At lowest total energy (when the thermal energy of the particles is negligible), all the lowest energy quantum states are filled. This state is referred to as full degeneracy. This degeneracy pressure remains non-zero even at absolute zero temperature. Adding particles or reducing the volume forces the particles into higher-energy quantum states. In this situation, a compression force is required, and is made manifest as a resisting pressure. The key feature is that this degeneracy pressure does not depend on the temperature but only on the density of the fermions. Degeneracy pressure keeps dense stars in equilibrium, independent of the thermal structure of the star.\n\nA degenerate mass whose fermions have velocities close to the speed of light (particle energy larger than its rest mass energy) is called relativistic degenerate matter.\n\nThe concept of degenerate stars, stellar objects composed of degenerate matter, was originally developed in a joint effort between Arthur Eddington, Ralph Fowler and Arthur Milne. Eddington had suggested that the atoms in Sirius B were almost completely ionised and closely packed. Fowler described white dwarfs as composed of a gas of particles that became degenerate at low temperature. Milne proposed that degenerate matter is found in most of the nucleus of stars, not only in compact stars.\n\nIf a plasma is cooled and under increasing pressure, it will eventually not be possible to compress the plasma any further. This constraint is due to the Pauli exclusion principle, which states that two fermions cannot share the same quantum state. When in this highly compressed state, since there is no extra space for any particles, a particle's location is extremely defined. Since the locations of the particles of a highly compressed plasma have very low uncertainty, their momentum is extremely uncertain. The Heisenberg uncertainty principle states \n\nwhere Δ\"p\" is the uncertainty in the particle's momentum and Δ\"x\" is the uncertainty in position. Therefore, even though the plasma is cold, such particles must on average be moving very fast. Large kinetic energies lead to the conclusion that, in order to compress an object into a very small space, tremendous force is required to control its particles' momentum.\n\nUnlike a classical ideal gas, whose pressure is proportional to its temperature\nwhere \"P\" is pressure, \"V\" is the volume, \"N\" is the number of particles—typically atoms or molecules—\"k\" is Boltzmann's constant, and \"T\" is temperature), the pressure exerted by degenerate matter depends only weakly on its temperature. In particular, the pressure remains nonzero even at absolute zero temperature. At relatively low densities, the pressure of a fully degenerate gas can be derived by treating the system as an ideal Fermi gas, in this way\nwhere \"K\" depends on the properties of the particles making up the gas. At very high densities, where most of the particles are forced into quantum states with relativistic energies, the pressure is given by\nwhere \"K\" again depends on the properties of the particles making up the gas.\n\nAll matter experiences both normal thermal pressure and degeneracy pressure, but in commonly encountered gases, thermal pressure dominates so much that degeneracy pressure can be ignored. Likewise, degenerate matter still has normal thermal pressure, but at extremely high densities, the degeneracy pressure usually dominates.\n\nExotic examples of degenerate matter include neutron degenerate matter, strange matter, metallic hydrogen and white dwarf matter. Degeneracy pressure contributes to the pressure of conventional solids, but these are not usually considered to be degenerate matter because a significant contribution to their pressure is provided by electrical repulsion of atomic nuclei and the screening of nuclei from each other by electrons. In metals it is useful to treat the conduction electrons alone as a degenerate free electron gas, while the majority of the electrons are regarded as occupying bound quantum states. This solid state contrasts with degenerate matter that forms the body of a white dwarf, where most of the electrons would be treated as occupying free particle momentum states.\n\nDegenerate gases are gases composed of fermions such as electrons, protons, and neutrons rather than molecules of ordinary matter. The electron gas in ordinary metals and in the interior of white dwarfs are two examples. Following the Pauli exclusion principle, there can be only one fermion occupying each quantum state. In a degenerate gas, all quantum states are filled up to the Fermi energy. Most stars are supported against their own gravitation by normal thermal gas pressure, while in white dwarf stars the supporting force comes from the degeneracy pressure of the electron gas in their interior. In neutron stars, the degenerate particles are neutrons.\n\nA fermion gas in which all quantum states below a given energy level are filled is called a fully degenerate fermion gas. The difference between this energy level and the lowest energy level is known as the Fermi energy.\n\nIn an ordinary fermion gas in which thermal effects dominate, most of the available electron energy levels are unfilled and the electrons are free to move to these states. As particle density is increased, electrons progressively fill the lower energy states and additional electrons are forced to occupy states of higher energy even at low temperatures. Degenerate gases strongly resist further compression because the electrons cannot move to already filled lower energy levels due to the Pauli exclusion principle. Since electrons cannot give up energy by moving to lower energy states, no thermal energy can be extracted. The momentum of the fermions in the fermion gas nevertheless generates pressure, termed 'degeneracy pressure'.\n\nUnder high densities the matter becomes a degenerate gas when the electrons are all stripped from their parent atoms. In the core of a star, once hydrogen burning in nuclear fusion reactions stops, it becomes a collection of positively charged ions, largely helium and carbon nuclei, floating in a sea of electrons, which have been stripped from the nuclei. Degenerate gas is an almost perfect conductor of heat and does not obey the ordinary gas laws. White dwarfs are luminous not because they are generating any energy but rather because they have trapped a large amount of heat which is gradually radiated away. Normal gas exerts higher pressure when it is heated and expands, but the pressure in a degenerate gas does not depend on the temperature. When gas becomes super-compressed, particles position right up against each other to produce degenerate gas that behaves more like a solid. In degenerate gases the kinetic energies of electrons are quite high and the rate of collision between electrons and other particles is quite low, therefore degenerate electrons can travel great distances at velocities that approach the speed of light. Instead of temperature, the pressure in a degenerate gas depends only on the speed of the degenerate particles; however, adding heat does not increase the speed of most of the electrons, because they are stuck in fully occupied quantum states. Pressure is increased only by the mass of the particles, which increases the gravitational force pulling the particles closer together. Therefore, the phenomenon is the opposite of that normally found in matter where if the mass of the matter is increased, the object becomes bigger. In degenerate gas, when the mass is increased, the pressure is increased, and the particles become spaced closer together, so the object becomes smaller. Degenerate gas can be compressed to very high densities, typical values being in the range of 10,000 kilograms per cubic centimeter.\n\nThere is an upper limit to the mass of an electron-degenerate object, the Chandrasekhar limit, beyond which electron degeneracy pressure cannot support the object against collapse. The limit is approximately 1.44 solar masses for objects with typical compositions expected for white dwarf stars (carbon and oxygen with 2 baryons per electron). This mass cutoff is appropriate only for a star supported by ideal electron degeneracy pressure under Newtonian gravity; in general relativity and with realistic Coulomb corrections, the corresponding mass limit is around 1.38 solar masses. The limit may also change with the chemical composition of the object, as it affects the ratio of mass to number of electrons present. Celestial objects below this limit are white dwarf stars, formed by the gradual shrinking of the cores of stars that run out of fuel. During this shrinking, an electron-degenerate gas forms in the core, providing sufficient degeneracy pressure as it is compressed to resist further collapse. Above this mass limit, a neutron star (partially supported by neutron degeneracy pressure) or a black hole may be formed instead.\n\nNeutron degeneracy is analogous to electron degeneracy and is demonstrated in neutron stars, which are partially supported by the pressure from a degenerate neutron gas. The collapse may happen when the core of a white dwarf is above the vicinity of 1.4 solar masses, which is the Chandrasekhar limit, and the collapse is not halted by the pressure of degenerate electrons. As the star collapses, the Fermi energy of the electrons increases to the point where it is energetically favorable for them to combine with protons to produce neutrons (via inverse beta decay, also termed electron capture and \"neutronization\"). The result is an extremely compact star composed of nuclear matter, which is predominantly a degenerate neutron gas, sometimes called neutronium, with a small admixture of degenerate proton and electron gases (and at higher densities, muons).\n\nNeutrons in a degenerate neutron gas are spaced much more closely than electrons in an electron-degenerate gas because the more massive neutron has a much shorter wavelength at a given energy. Typical separations are comparable with the size of the neutron and the range of the strong nuclear force, and it is actually the repulsive nature of the latter at small separations that primarily supports neutron stars more massive than 0.7 solar masses (which includes all measured neutron stars). In the case of neutron stars and white dwarfs, this phenomenon is compounded by the fact that the pressures within neutron stars are much higher than those in white dwarfs. The pressure increase is caused by the fact that the compactness of a neutron star causes gravitational forces to be much higher than in a less compact body with similar mass. The result is a star with a diameter on the order of a thousandth that of a white dwarf.\n\nThere is an upper limit to the mass of a neutron-degenerate object, the Tolman–Oppenheimer–Volkoff limit, which is analogous to the Chandrasekhar limit for electron-degenerate objects. The limit for objects supported by ideal neutron degeneracy pressure is only 0.75 solar masses. For more realistic models including baryon interaction, the precise limit is unknown, as it depends on the equations of state of nuclear matter, for which a highly accurate model is not yet available. Above this limit, a neutron star may collapse into a black hole or into other, denser forms of degenerate matter (such as quark matter) if these forms exist and have suitable properties (mainly related to degree of compressibility, or \"stiffness\", described by the equations of state).\n\nSufficiently dense matter containing protons experiences proton degeneracy pressure, in a manner similar to the electron degeneracy pressure in electron-degenerate matter: protons confined to a sufficiently small volume have a large uncertainty in their momentum due to the Heisenberg uncertainty principle. However, because protons are much more massive than electrons, the same momentum represents a much smaller velocity for protons than for electrons. As a result, in matter with approximately equal numbers of protons and electrons, proton degeneracy pressure is much smaller than electron degeneracy pressure, and proton degeneracy is usually modelled as a correction to the equations of state of electron-degenerate matter.\n\nAt densities greater than those supported by neutron degeneracy, quark matter is expected to occur. Several variations of this hypothesis have been proposed that represent quark-degenerate states. Strange matter is a degenerate gas of quarks that is often assumed to contain strange quarks in addition to the usual up and down quarks. Color superconductor materials are degenerate gases of quarks in which quarks pair up in a manner similar to Cooper pairing in electrical superconductors. The equations of state for the various proposed forms of quark-degenerate matter vary widely, and are usually also poorly defined, due to the difficulty of modeling strong force interactions.\n\nQuark-degenerate matter may occur in the cores of neutron stars, depending on the equations of state of neutron-degenerate matter. It may also occur in hypothetical quark stars, formed by the collapse of objects above the Tolman–Oppenheimer–Volkoff mass limit for neutron-degenerate objects. Whether quark-degenerate matter forms at all in these situations depends on the equations of state of both neutron-degenerate matter and quark-degenerate matter, both of which are poorly known. Quark stars are considered to be an intermediate category among neutron stars and black holes. Few scientists claim that quark stars and black holes are one and the same. Not enough data exist to support any hypothesis but neutron stars with awkward spectrums have been used in arguments.\n\nAt densities greater than those supported by any degeneracy, gravity overwhelms all other forces. The stellar body collapses to form a black hole, though this is not well modeled by quantum mechanics. At the same time, the material must be converted from fermions, subject to degeneracy pressure, to bosons, which are not. Current hypothesis suggest gluons as the most likely boson thought possible.\n\nIn the frame of reference that is co-moving with the collapsing matter, general relativity models without quantum mechanics have all the matter ending up in an infinitely dense singularity at the center of the event horizon. (If one uses the UFT Einstein–Maxwell–Dirac system or its generalizations, then the singularity is avoided and one ends up with a quark star, possibly surrounded by an event horizon.) It is a general result of quantum mechanics that no fermion can be confined in a space smaller than its own wavelength, making such a singularity impossible, unless only bosons are present, but there is no widely accepted theory that combines general relativity and quantum mechanics sufficiently to tell us what the structure inside a black hole might be. If bosons can be conclusively ruled out, one possible theory is that constituent particles decompose into strings, forming a structure called a fuzzball.\n\n\n"}
{"id": "16695869", "url": "https://en.wikipedia.org/wiki?curid=16695869", "title": "Delamination (geology)", "text": "Delamination (geology)\n\nIn geophysics, delamination refers to the loss and sinking (foundering) of the portion of the lowermost lithosphere from the tectonic plate to which it was attached.\n\nThe outer portion of the Earth is divided into an upper, lithosphere layer and a lower, asthenosphere layer. The lithosphere layer is composed of two parts, an upper, the crustal lithosphere and lower, the mantle lithosphere. The crustal lithosphere is in unstable mechanical equilibrium because the underlying mantle lithosphere has a greater density than the asthenosphere below. The difference in densities can be explained by thermal expansion/contraction, composition, and phase changes. Negative buoyancy of the lower continental crust and mantle lithosphere drive delamination.\n\nDelamination occurs when the lower continental crust and mantle lithosphere break away from the upper continental crust. There are two conditions that need to be met in order for delamination to proceed:\n\n\nThe metamorphic transition from mafic granulite facies to the denser eclogite facies in the lower portion of the crust is the main mechanism responsible for creating negative buoyancy of the lower lithosphere. The lower crust undergoes a density inversion, causing it to break off of the upper crust and sink into the mantle. Density inversions are more likely to occur where there are high mantle temperatures. This limits this phenomenon to arc environments, volcanic rifted margins and continental areas undergoing extension.\n\nThe asthenosphere rises until it comes into contact with the base of the lower crust, causing the lower crust and lithospheric mantle to start to peel away. Slumping, cracking, or plume erosion facilitates the intrusion of underlying asthenosphere. Potential energy that drives the delamination is released as the low density, hot asthenosphere rises and replaces the higher density, cold lithosphere. Separation of lowermost crust and lithospheric mantle is controlled by the effective viscosity of the upper continental crust. These processes often occur in environments of rifting, plume erosion, continental collision or where there is convective instability.\n\nConvective instabilities facilitate delamination. The convection can simply peel away the lower crust or, in a different scenario, a Rayleigh-Taylor instability is created. Due to the instability in a local area, the base of the lithosphere breaks up into descending blobs fed by an enlarging region of thinning lithosphere. The space left behind by departing lithosphere is filled by upwelling asthenosphere.\n\nAs delamination continues, more asthenosphere rises to replace the lower lithosphere as it sinks. This process causes three different changes to occur which can have an effect on the delamination process.\n\n\nIf the freezing of the asthenosphere dominates (2) the system is stable, however if subsidence, and therefore separation of the lower lithosphere dominates (3) the system is unstable. Processes (2) and (3) compete with each other.\n\nDelamination of the lithosphere has two major geologic effects. First, because a large portion of dense material is removed, the remaining portion of the crust and lithosphere undergo rapid uplift to form mountain ranges. Second, flow of hot mantle material encounters the base of the thin lithosphere and often results in melting and a new phase of volcanism. Delamination may thus account for some volcanic regions that have been attributed to mantle plumes in the past.\n\nDelamination is seen in convergence zones, especially where continental-continental collisions occur. For example, delamination is seen in the Tibetan Plateau, which has formed from the collision of India with Asia. Observations which support delamination include sudden mafic volcanism and acceleration of uplift, occurring 14 to 11 Ma.\n\nAreas of extension are also associated with delamination. Negative buoyancy of the lower lithosphere drives delamination in both environments of collision and extension. During the collapse of a mountain belt, the thick crustal roots beneath what used to be a mountain disappear. The processes behind this disappearance are not clear. Granitic plutons formed by strong heat pulses have been associated with the disappearance of thick crustal roots. Delamination is a likely source for the heat pulses.\n\nThe tectonic development of collapsed mountain belts is heavily debated. Some argue that delamination causes a second uplift along with crustal thickening, heating and volcanism. Others argue that delamination causes collapse and thinning of the crust. Some researchers postulate that the Sierra Nevada, Basin and Range Province and Colorado Plateau in the western USA exemplify this.\n\nOne example of the effects of lithosphere delamination is seen in the Sierra Nevada, Basin and Range Province and Colorado Plateau in the western USA. During crustal extension in the Basin and Range Province 10 million years ago, the upwelling of asthenosphere thinned the lithosphere. Heating caused by the rise of warmer asthenosphere created a crustal low-viscosity zone and delamination occurred on the flanks of the Basin and Range. Uplift of the Sierra Nevada mountain range and the Colorado Plateau has occurred on the flanks as a result of the loss of high density lower lithosphere. Eclogite xenoliths found within the crust in the region support the metamorphic phase change associated with the density inversion in the lower crust. It is possible that the Sierra Nevada is the only place on Earth where dense material is currently being removed from the crust.\n\n"}
{"id": "6183688", "url": "https://en.wikipedia.org/wiki?curid=6183688", "title": "Demand destruction", "text": "Demand destruction\n\nDemand destruction is a permanent downward shift on the demand curve in the direction of lower demand of a commodity, such as energy products, induced by a prolonged period of high prices or constrained supply. In the context of the oil industry, \"demand\" generally refers to the quantity consumed (see for example the output of any major industry organization such as the International Energy Agency), rather than any measure of a demand curve as used in mainstream economics.\n\nThe term has come to some prominence lately as a result of the growing interest in the peak oil theory, where demand destruction is the reduction of demand for oil and oil-derived products. The term is used by Matthew Simmons, Mike Ruppert and other prominent proponents of the theory. It is also used in other resource industries, such as mining.\n\nA familiar illustration of demand destruction is the effect of high gasoline prices on automobile sales. It has been widely observed that when gasoline prices are high enough, consumers tend to begin buying smaller and more efficient cars, gradually reducing per-capita demand for gasoline. If the price rise were caused by a temporary lack of supply, and the price then subsequently goes back down as supply returns to normal, the quantity of gas consumed in this case does not immediately go back to its previous level, since the smaller cars that had been sold remain in the fleet for some time. Demand thereby has been \"destroyed\", shifting the demand curve.\nThe expectation of future prices and their long-term maintenance at non-economic levels for a certain quantity of consumption also affects vehicle decisions. If the price of fuel is so high that marginal consumers cannot afford the same mileage without switching to a more efficient car, then they are forced to sell the less efficient one. An increase of the quantity of such vehicles causes the used market value to fall, which then increases the depreciation expected of a new vehicle, which increases the total cost of ownership of such vehicles, making them less popular.\n\nThe coal reserves in some regions are regarded as a stranded asset that may be permanently left in the ground. Competition from low priced natural gas, reduced demand for coal due to emission restrictions and uneconomic export situations each play a part. Environmental legislation that prevents fracking strands potential natural gas reserves.\n\n"}
{"id": "55377", "url": "https://en.wikipedia.org/wiki?curid=55377", "title": "ELDIS", "text": "ELDIS\n\nEldis is a database and email service of information sources on international development. It aims to share the best knowledge on development, policy, practice and research.\n\n\"Eldis\" was originally an acronym for \"Electronic Development and Environment Information System\". It is one of a family of knowledge services produced at the Institute of Development Studies, Sussex, England.\n\nEldis is funded by the UK Department for International Development (DFID), Swedish International Development Cooperation Agency (Sida), the Norwegian Agency for Development Cooperation (Norad) and the Swiss Agency for Development and Cooperation (SDC).\n\nThe information in Eldis is organised into subject-focused \"resource guides\" and regional and country \"profiles.\"\n\n\n"}
{"id": "11789061", "url": "https://en.wikipedia.org/wiki?curid=11789061", "title": "Earth goddess", "text": "Earth goddess\n\nAn Earth goddess is a deification of the Earth. Earth goddesses are often associated with the \"chthonic\" deities of the underworld.\n\nKi and Ninhursag are Mesopotamian earth goddesses. In Greek mythology, the Earth is personified as Gaia, corresponding to Roman Terra, Indic Prithvi/Bhūmi, etc. traced to an \"Earth Mother\" complementary to the \"Sky Father\" in Proto-Indo-European religion. Egyptian mythology exceptionally has a sky goddess and an Earth god.\n\nOther Earth goddesses include:\n\n"}
{"id": "12893837", "url": "https://en.wikipedia.org/wiki?curid=12893837", "title": "Energetic material", "text": "Energetic material\n\nEnergetic materials are a class of material with high amount of stored chemical energy that can be released.\n\nTypical classes of energetic materials are e.g. explosives, pyrotechnic compositions, propellants (e.g. smokeless gunpowders and rocket fuels), and fuels (e.g. diesel fuel and gasoline).\n"}
{"id": "8997074", "url": "https://en.wikipedia.org/wiki?curid=8997074", "title": "Energy Independence and Security Act of 2007", "text": "Energy Independence and Security Act of 2007\n\nThe Energy Independence and Security Act of 2007 (Pub.L. 110-140 originally named the Clean Energy Act of 2007) is an Act of Congress concerning the energy policy of the United States. As part of the Democratic Party's 100-Hour Plan during the 110th Congress, it was introduced in the United States House of Representatives by Representative Nick Rahall of West Virginia, along with 198 cosponsors. Despite Rahall becoming 1 of only 4 Democrats to oppose the final bill, it passed in the House without amendment in January 2007. When the Act was introduced in the Senate in June 2007, it was combined with Senate Bill S. 1419: \"Renewable Fuels, Consumer Protection, and Energy Efficiency Act of 2007\". This amended version passed the Senate on June 21, 2007. After further amendments and negotiation between the House and Senate, a revised bill passed both houses on December 18, 2007 and President Bush, a Republican, signed it into law on December 19, 2007, in response to his \"Twenty in Ten\" challenge to reduce gasoline consumption by 20% in 10 years.\n\nThe stated purpose of the act is “to move the United States toward greater energy independence and security, to increase the production of clean renewable fuels, to protect consumers, to increase the efficiency of products, buildings, and vehicles, to promote research on and deploy greenhouse gas capture and storage options, and to improve the energy performance of the Federal Government, and for other purposes.”. House Speaker Nancy Pelosi promoted the Act as a way of lowering energy costs to consumers. The bill followed another major piece of energy legislation, the Energy Policy Act of 2005.\n\nThe bill originally sought to cut subsidies to the petroleum industry in order to promote petroleum independence and different forms of alternative energy. These tax changes were ultimately dropped after opposition in the Senate, and the final bill focused on automobile fuel economy, development of biofuels, and energy efficiency in public buildings and lighting.\n\nThe bill signed into law in December 2007 was an 822-page document changing U.S. energy policy in many areas.\n\nTitle I contains the first increase in fuel economy standards for passenger cars since 1975, and the establishment of the first efficiency standard for medium-duty and heavy-duty commercial vehicles. By the year 2020, it is estimated to save Americans a total of $22 billion and have a significant reduction in emissions equivalent to removing 28 million cars from the road. Title I is responsible for 60% of the estimated energy savings of the bill.\n\n\n\n\nTitle II contains the first legislation that specifically requires the creation of biomass-based diesel fuel, which is the addition of renewable biofuels to diesel fuel. To be labeled as Biomass-based Diesel, fuel must be able to reduce emissions by 50 percent when compared to petroleum diesel. As of now, Biodiesel is the only commercial fuel that meets this requirement .\n\n\n\nTitle III contains standards for ten appliances and equipment: residential boilers, clothes dryers, room air conditioners, clothes washers, residential water heaters, dishwashers, kitchen stoves ovens, microwave ovens, and dehumidifiers. Previous national efficiency standards for covered products were made in 1987, 1988, 1992 and 2005.\n\n\n\nIndustrial and commercial buildings are responsible for using almost half of the nation’s energy and greenhouse gas emissions, costing over $200 billion each year. Title IV aims to reduce the energy used of Federal buildings by 30 percent by the year 2015.\n\n\n\n\n\n\n\n\n\nThe House passed versions of the bill which contained two controversial provisions: a renewable portfolio standard which required that utilities to produce 15% of their power from renewable energy and a tax package which would fund renewable energy through the repeal of $21 billion in oil and gas tax breaks; the Senate failed to pass these provisions in two cloture votes. On June 21, 2007, an attempt by the Senate Democrats to raise taxes on oil & gas by $32 billion was reportedly blocked by the Republicans.\n\nTitle I of the original bill, the “Ending Subsidies for Big Oil Act of 2007,” denied certain tax deductions to producers of oil, natural gas, or primary products of oil or natural gas, and increased from five to seven years the period during which five major integrated oil companies must write off their expenditures on geological and geophysical studies related to oil exploration.\n\nTitle II, the “Royalty Relief for American Consumers Act of 2007,” addressed an oversight that occurred when the Interior Department issued oil and gas leases for off-shore drilling in the Gulf of Mexico in 1998 and 1999. The leases didn’t include price thresholds that require companies to pay royalties to the Federal Government when the price of oil and gas exceeds a certain level. These companies would be required to renegotiate their leases to include price thresholds that are equal or less than thresholds described in the Outer Continental Shelf Lands Act. Companies who failed to renegotiate their leases or pay the fees would not be allowed to obtain any oil or gas leases in the Gulf of Mexico.\n\nTitle II also repealed several provisions of the Energy Policy Act of 2005. One provision suspended royalty fees on oil and gas production in certain waters of the Gulf of Mexico. A provision of the Energy Policy Act that protects drilling permit applicants from additional fees to recover the cost of processing paperwork would also be repealed, and special policies for leases in the National Petroleum Reserve–Alaska and royalty relief for specific offshore drilling in Alaska would be discontinued.\n\nTitle III of the bill created a Strategic Energy Efficiency and Renewables Reserve, an account to hold additional money received by the Federal Government as a result of the enactment of the act, and to offset the cost of subsequent legislation.\n\nOpponents argued that the act would \"increase Americans reliance on foreign sources of energy by making new domestic exploration and production more costly\" and stated that markets should drive U.S. energy policy. They were concerned that the Strategic Energy Efficiency and Renewables Reserve would be used for \"politically connected pet projects,\" citing a similar fund created by the Carter administration that went bankrupt after only a few years.\n\nThe U.S. Chamber of Commerce said that the bill would punish an industry that has made many Americans wealthy for generations, adding that \"Congress and various Administrations have perhaps imposed more regulations on the oil and gas industry than any other industry in the United States.\" The Chamber said it supported the rapid development of alternative fuels but that the new technologies are not developed enough, and are insufficient to make any real difference. It believed more regulation on oil and gas producers is not the answer to the energy problem.\n\nGrover Norquist, Conservative activist and president of Americans for Tax Reform, characterized the bill’s provisions regarding renegotiation of leases as a violation of binding contracts, calling the bill “a violation of the Taxpayer Protection Pledge” since it wouldn’t create tax cuts to offset the additional revenue it would raise.\n\nRepresentative Ted Poe said the bill “will decrease U.S. exploration and will increase our dependence on foreign oil,” and, “by raising taxes and fees on oil and gas companies that choose to manufacture in America, the U.S. will become a less attractive place to produce oil and natural gas. This essentially creates incentives for foreign importation and could kill manufacturing jobs in an industry that employs nearly 1.8 million Americans.”\n\nCongressmen representing automobile manufacturing regions objected to the proposed increase in fuel economy standards. They said the measure would sharply increase the cost of new cars, lowering demand and further burdening the struggling automotive industry. Representative John Dingell of Michigan advocated instead an increase in the federal gasoline tax, which he said would have more immediate effects on oil consumption by influencing consumer behavior (i.e. car purchase decisions and total miles driven).\n\nCompact fluorescent lamps were an existing technology that exceeded the initial EISA 2007 requirements for lumens per watt. LED bulbs were not in widespread use or affordable in 2007. Energy efficient halogen bulbs also met the minimum EISA requirements (see below).\n\nThe majority of the supporters for the original bill were Representatives from the Democratic party. Speaker of the House Nancy Pelosi described the vote as \"the first step toward a future of energy independence.\" Moira Chapin, Environment California Federal Field Organizer, said \"the 110th Congress made a down payment on a new energy future,\" referring to its investment in renewable energy resources from solar and wind power generation facilities.\n\nProponents believed that investing the new tax revenue in renewable energy resources would foster a new industry, creating more jobs and helping to reduce American dependency on oil imports. They claimed that as many as 3.3 million new jobs would be created, cutting unemployment, adding $1.4 trillion to the gross national product in the economy, and paying for itself within ten years. Air quality would be improved by reducing the amount of emissions released by using a cleaner energy source other than oil.\n\nAnother supporter of the bill, Representative Steve Rothman of New Jersey, said that if the proposed bill passed, \"the U.S. can improve air quality, create jobs, and corner a new business market.\"\n\nUnder the law, incandescent bulbs that produced 310–2600 lumens of light were effectively phased out between 2012 and 2014 unless they could meet the increasing energy efficiency standards mandated by the bill. Bulbs outside this range (roughly, light bulbs currently less than 40 watts or more than 150 watts) were exempt from the ban. Also exempt were several classes of speciality lights, including appliance lamps, \"rough service\" bulbs, 3-way, colored lamps, and plant lights.\n\nIn 2013, Advanced Lighting Technologies, an Ohio company that develops and manufactures lighting products, announced the release of an incandescent bulb that it claimed significantly exceeds the efficiency requirements of the law \n\nBy 2020, a second tier of restrictions is set to take effect, requiring all general-purpose bulbs to produce at least 45 lumens per watt (similar to CFLs in 2007 but far less efficient than that the LED bulbs that have since become available and affordable). Exemptions from the Act included reflector floodlights, 3-way bulbs, candelabra, colored bulbs, and other specialty bulbs.\n\nThe phase-out of inefficient incandescent light bulbs was supported by the Alliance to Save Energy, a coalition of light bulb manufacturers, electric utilities and conservation groups. The group estimated that lighting accounts for 22% of total U.S. electricity usage, and that eliminating incandescent bulbs completely would save $18 billion per year (equivalent to the output of 80 coal plants). Light bulb manufacturers also hoped a single national standard would prevent the enactment of conflicting bans and efficiency standards by state governments.\n\nThe initial version of H.R. 6 passed the House of Representatives on January 18, 2007, by a vote of 264 to 163. The Senate version passed 65-27 on June 21, but bore almost no resemblance to the original bill. Speaker Pelosi indicated on Oct 10 that instead of sending the bill to a conference committee, the House would negotiate informally with the Senate to resolve their differences.\n\nThe House took up the energy bill again in December, passing a new version on December 6. This version, renamed the \"Energy Independence and Security Act of 2007\", restored the oil industry tax increases of the original bill. It also added a requirement that U.S. electric utilities must obtain 15 percent of their power from renewable sources by 2020.\n\nWhen this bill was introduced to the Senate, the new provisions became the focus of debate. The White House and Sen. Domenici warned that Bush would veto the bill because of the tax portion. Senate Minority Leader Mitch McConnell (R-Ky.) said Democrats had \"shown how to snatch defeat from the jaws of victory\" by \"inserting an enormous tax hike, a tax hike they knew would doom this legislation.\" Reid said Congress should not be intimidated by a veto threat, \"We are the Congress of the United States. We can write things even though the president may not like them.\" Democrats said that the tax measure was modest and only took back tax breaks the oil companies received in 2004 and that they did not need them with oil prices at about $90 a barrel.\n\nThe House version of the bill (with $13 billion raised from the oil industry, a mandate that utilities rely on renewable energy for at least 15 percent of their power generation, and a $21.8 billion 10-year tax package) failed by a one-vote margin. A final attempt to end debate and make way for a vote failed by 59 - 40 despite the return of four Democratic presidential candidates, Hillary Clinton (NY), Barack Obama (Ill.), Christopher Dodd (Conn.), and Joseph Biden (Del.). Nine Republicans voted in favor of ending debate while one Democrat, Sen. Mary Landrieu (D-La.) voted against it. Sen. John McCain was not present.\n\nThe revised Senate bill passed 86-8 on December 13. The House approved this final version 314-100 on December 18, and President Bush signed it the following day.\n\nA 2011 report from the United States National Research Council investigated the potential economic and environmental effects of reaching the Renewable Fuel Standard, which mandates that 35 billion gallons of ethanol-equivalent biofuels and 1 billion gallons of biomass-based diesel be consumed in the United States by 2022.\n\nThe study found that the United States already has the capacity to produce 14 billion gallons of corn-grain ethanol (an amount close to the consumption mandate for conventional biofuels in 2022) and has infrastructure for producing 2.7 billion gallons of biomass-based diesel, Some of the key factors that influence environmental effects of biofuels are site specific and depend on the type of feedstocks produced, the management practices used to produce them, prior land use, and any land-use changes that their production might incur. In addition to greenhouse-gas emissions, production and use of biofuels affect air quality, water quality, water use, and biodiversity.\n\nA 2018 US EPA report to the Congress confirmed conclusions of the previous report and found that the expected transition to the second generation biofuels has not been realized; biofuels are still mainly produced from corn grain and soy beans. The impacts on environment is predominantly negative and include land use shift from conservation to agriculture, increase in water abstraction for irrigation, reduction in air and water quality, and reduced biodiversity. \n\nIn December 2011, the U.S. Congress defunded enforcement of EISA light-bulb performance requirements as part of the Consolidated Appropriations Act in the 2012 federal budget. However, a representative of the American lighting industry said that \"the industry has moved on\" and that American manufacturers have already retooled production lines to make other bulbs.\n\nIn the wake of the 2012 North American drought, which had a devastating effect on the US corn crop, there were calls for the quota imposed by the Renewable Fuel Standard to be suspended. In August 2012, 25 senators and over 100 house members echoed the demand. The head of the Food and Agriculture Organization, José Graziano da Silva, called on the United States to suspend the quota. Writing in a letter to the Financial Times he argued that the reduced harvest and the large demand for corn created by the quota has caused an unaffordable price rise in a key food crop. At this point the quota meant that biofuel production accounted for 40% of the entire US corn crop.\n\nAccording to research sponsored by the United States government, the World Bank, and other organizations, there is no clear link between the RFS and higher food prices. Ethanol critics contend that RFS requirements crowd out production that would go to feed livestock.\n\n\n"}
{"id": "3646203", "url": "https://en.wikipedia.org/wiki?curid=3646203", "title": "Expanding Earth", "text": "Expanding Earth\n\nThe expanding Earth or growing Earth hypothesis asserts that the position and relative movement of continents is at least partially due to the volume of Earth increasing. Conversely, geophysical global cooling was the hypothesis that various features could be explained by Earth contracting.\n\nAlthough it was suggested historically, since the recognition of plate tectonics in the 1970s, scientific consensus has rejected any significant expansion or contraction of Earth.\n\nThere are three forms of the expanding earth hypothesis:\n\nIn 1834, during the second voyage of HMS \"Beagle\", Charles Darwin investigated stepped plains featuring raised beaches in Patagonia which indicated to him that a huge area of South America had been \"uplifted to its present height by a succession of elevations which acted over the whole of this space with nearly an equal force.\" While his mentor Charles Lyell had suggested forces acting near the crust on smaller areas, Darwin hypothesized that uplift at this continental scale required \"the gradual expansion of some central mass\" [of the earth] \"acting by intervals on the outer crust\" with the \"elevations being concentric with form of globe (or certainly nearly so)\". In 1835 he extended this concept to include the Andes as part of a curved enlargement of the earth's crust due to \"the action of one connected force\". Not long afterwards, he moved on from this idea and proposed that as mountains rose, the ocean floor subsided, explaining the formation of coral reefs.\n\nIn 1889 and 1909 Roberto Mantovani published a hypothesis of Earth expansion and continental drift. He assumed that a closed continent covered the entire surface of a smaller Earth. Thermal expansion led to volcanic activity, which broke the land mass into smaller continents. These continents drifted away from each other because of further expansion at the rip-zones, where oceans currently lie. Although Alfred Wegener noticed some similarities to his own hypothesis of continental drift, he did not mention Earth expansion as the cause of drift in Mantovani's hypothesis.\n\nA compromise between Earth-expansion and Earth-contraction is the \"theory of thermal cycles\" by Irish physicist John Joly. He assumed that heat flow from radioactive decay inside Earth surpasses the cooling of Earth's exterior. Together with British geologist Arthur Holmes, Joly proposed a hypothesis in which Earth loses its heat by cyclic periods of expansion. In their hypothesis, expansion led to cracks and joints in Earth's interior, that could fill with magma. This was followed by a cooling phase, where the magma would freeze and become solid rock again, causing Earth to shrink.\n\nIn 1888 Ivan Osipovich Yarkovsky suggested that some sort of aether is absorbed within Earth and transformed into new chemical elements, forcing the celestial bodies to expand. This was connected with his mechanical explanation of gravitation. Also the theses of Ott Christoph Hilgenberg (1933, 1974) and Nikola Tesla (1935) were based on absorption and transformation of aether-energy into normal matter.\n\nAfter initially supporting continental drift, the late Australian geologist S. Warren Carey advocated expansion from the 1950s (before the development of plate tectonics provided the generally accepted explanation of the movement of continents) to his death, demonstrating that subduction and other events could not balance the sea-floor spreading at oceanic ridges, and piling yet unresolved paradoxes that continue to plague plate tectonics. Starting in 1956, he proposed some sort of mass increase in the planets and said that a final solution to the problem is only possible in a cosmological perspective in connection with the expansion of the universe.\n\nBruce Heezen initially interpreted his work on the mid-Atlantic ridge as supporting S. Warren Carey's Expanding Earth Theory, but later withdrew his support, finally convinced by the data and analysis of his assistant, Marie Tharp. The remaining proponents after the 1970s, like the Australian geologist James Maxlow, are mainly inspired by Carey's ideas.\n\nIn the last few decades, no credible mechanism of action has been proposed for this addition of new mass, and there is no credible evidence for new mass having been added in the past. The increased gravity of Earth would have altered the orbits of the celestial objects in the Solar System, including Moon's orbit and Earth's own orbit; proponents have no adequate explanation to address this problem. This is a big obstacle for acceptance of the theory by other geologists.\n\nIt is a well known fact that the earth is constantly acquiring mass through accumulation of rocks and dust from space, as are all other planetary bodies in our system. According to NASA, \"Every day about 100 tons of meteoroids -- fragments of dust and gravel and sometimes even big rocks – enter the Earth's atmosphere.\" The majority of this debris burns up in the atmosphere and lands as dust. Such accretion, however, is only a minuscule fraction of the mass increase required by the expanding Earth hypothesis.\n\nPaul Dirac suggested in 1938 that the universal gravitational constant had \"decreased\" in the billions of years of its existence. This led German physicist Pascual Jordan to a modification of general relativity and to propose in 1964 that all planets slowly expand. Contrary to most of the other explanations this one was at least within the framework of physics considered as a viable hypothesis.\n\nMeasurements of a possible variation of the gravitational constant showed an upper limit for a relative change of 5•10 per year, excluding Jordan's idea.\n\nThe hypothesis had never developed a plausible and verifiable mechanism of action. During the 1960s, the theory of plate tectonics—initially based on the assumption that Earth's size remains constant, and relating the subduction zones to burying of lithosphere at a scale comparable to seafloor spreading—became the accepted explanation in the Earth Sciences.\n\nThe scientific community finds that significant evidence contradicts the Expanding Earth theory, and that evidence used in support of it is better explained by plate tectonics: \n\n\n\n\n"}
{"id": "52809973", "url": "https://en.wikipedia.org/wiki?curid=52809973", "title": "F Velorum", "text": "F Velorum\n\nThe Bayer designations f Velorum and F Velorum are distinct. Due to , both designations link here. For the star\n"}
{"id": "12830424", "url": "https://en.wikipedia.org/wiki?curid=12830424", "title": "Field aligned irregularities", "text": "Field aligned irregularities\n\nField Aligned Irregularities (FAIs) are anisotropic (different values when measured in different directions) perturbations of plasma density associated with magnetic fields. FAIs are often thought of in the context of the Earth's ionosphere where several natural processes generate FAIs in the E-region and F-region. They occur at 50 and 140 MHz, where electrons associated with the event become vertically aligned with Earth's magnetic field. FAI may occur for several hours after it starts. Optimum times for observance appear to be between 8 PM and midnight.\n"}
{"id": "11147575", "url": "https://en.wikipedia.org/wiki?curid=11147575", "title": "Guild (ecology)", "text": "Guild (ecology)\n\nA guild (or ecological guild) is any group of species that exploit the same resources, or that exploit different resources in related ways. It is not necessary that the species within a guild occupy the same, or even similar, ecological niches. An ecological niche is defined as the role an organism plays in its community, i.e. decomposer, primary producer, etc. Guilds are defined according to the locations, attributes, or activities of their component species. For example, the mode of acquiring nutrients, the mobility, and the habitat zones that the species occupy or exploit can be used to define a guild. The number of guilds occupying an ecosystem is termed its \"disparity\". Members of a guild within a given ecosystem could be competing for resources, such as space or light, while cooperating in resisting wind stresses, attracting pollinators, or detecting predators, such as happens among savannah-dwelling antelope and zebra.\n\nA guild does not typically have strict, or even clearly defined boundaries, nor does it need to be taxonomically cohesive. A broadly defined guild will almost always have constituent guilds; for example, grazing guilds will have some species that concentrate on coarse, plentiful forage, while others concentrate on low-growing, finer plants. Each of those two sub-guilds may be regarded as guilds in appropriate contexts, and they might, in turn, have sub-guilds in more closely selective contexts. Some authorities even speak of guilds in terms of a fractal resource model. This concept arises in several related contexts, such as the metabolic theory of ecology, the scaling pattern of occupancy, and spatial analysis in ecology, all of which are fundamental concepts in defining guilds.\n\nAn ecological guild is not to be confused with a taxocene, a group of phylogenetically related organisms in a community that do not necessarily share the same or similar niches (for example, \"the insect community\"). Nor is a guild the same as a trophic species, organisms of the same species that have mutual predators and prey.\n\n"}
{"id": "40249948", "url": "https://en.wikipedia.org/wiki?curid=40249948", "title": "Heah Joo Seang", "text": "Heah Joo Seang\n\nHeah Joo Seang () was an Malayan politician, business leader, rubber magnate, philanthropist and especially a supporter of education. Malaysia, as it has since become, only existed after Joo Seang's death. His contributions span three distinct periods in the country's history: the British Malaya period, the Malayan Union period, and the Federation of Malaya period.\n\nThe son of Heah Ah-Kia (or Heah Ah Kiah), Joo Seang was born in Sungei Nibong Kechil, a small village in Province Wellesley, Penang on 2 November 1899.\n\nHeah Joo Seang began his education at a Chinese school in his hometown. Later on, he was sent to live in Georgetown, Penang. His uncle, Heah Lye Hiang, registered him to study in St. Xavier's Institution, a Christian missionary school, founded in 1852. He sat for, and passed the Cambridge School Certificate when he was only fourteen years old. Later on in life, when Joo Seang became successful in business, and gain prominence in politics, SXI (as the school is popularly known to the local people) benefited from his generosity. His legacy and association with the school was forever engraved in its history, when the school auditorium (fully funded by him) was named after him. For him to have contributed so willingly, he must have felt a deep sense of gratitude towards his alma mater.\n\nHis working life began as a clerk in his uncle Heah Lye Hiang's tapioca mill, earning $40 a month.\n\nOver the span of 15 years he had led three political parties – the Independence of Malaya Party, Party Negara and the Penang MCA as well as serving as national treasurer for the parent body. His last public appearance was during the official visit of MCA national President, Tan Siew-Sin, in March. Although he had lost his voice – he had already been ill when he travelled through Borneo on party business in February – he insisted on addressing the Alliance rally in Siew-Sin's honour. Soon after he flew to England for a throat operation. He died in Brompton Hospital on 14 May. He died in London on 14 May 1962, leaving behind a wife, Lim Soh-Liang (or Lim Su-Lian), eight sons (Heah Hock-Khoon, Heah Hock-Soon, Heah Hock-Lye, Heah Hock-Meng, Heah Hock-Thye, Heah Hock-Aun, Heah Hock-Hin, Heah Hock-Heng), three daughters (Beng-Hong, Phee-Hong, Kwee-Hong), one son-in-law, six daughters-in-law and 15 grandchildren to mourn his loss. Unknown to many, Joo Seang has a second wife. Her name is Eu Phooi Chen. It was quite common at that time for wealthy men to have more than one wife. It was during the period of the Japanese occupation of Malaya when Phooi Chen had borne him two sons namely Paul Heah Hock Teik (born 1943) and Jeffrey Heah Hock Hai (born 1944).\n\nAbout a thousand people gathered at the Penang Bayan Lepas airport to receive his body which was flown back from London.\n\nHis family had decided to follow the old custom concerning death that happens outside of a person's home – when a person dies outside of his home, he cannot be brought back into the house. Accordingly, when he was welcomed at the airport, they did so as if he were still alive. There was no weeping and no black clothes or other signs of mourning until his body was back at his mansion, \"Goodwood,\" in Macalister Road, and his death then announced by a representative of the family. His body was borne in a coffin draped in red and was accompanied by a motorcade nearly five (5) miles long. The announcement of his death was made by Rev. Soo Hneah Hee Seong, a \"kee-tong\" (medium), after which the mourning officially began. his body lay in state until Sunday, 21 May 1962 where it was removed for burial at the Kwangtung and Tengchow Cemetery at Mount Erskine.\n\nMalayan Chinese Association (MCA) national President, Tan Siew Sin said, the death of Heah Joo Seang left a gap that would be difficult to fill. \"He was a tower of strength nor only to the MCA's central working committee but also the Penang organisation,\" the President wrote in a letter to Joo Seang's eldest son, Heah Hock-Khoon. \"He infused new life into the Penang MCA and their loss is indeed a very grievous one... ...Apart from the MCA, the country will always remember him for his munificent gifts to charities and deserving institutions... ...I think it is only right that I should put these sentiments on record.\"\n\nLa Sallean Brother Visitor T. Michael said the Christian Brothers had reason to grieve over the loss of a \"great old boy and a very generous benefactor.\" He said that Joo Seang was unsparing in his attachment to his old school and worked towards its progress and improvement with genuine affection and loyalty. In referring to two new schools completed at Jalan Brother James and Kampong Bharu, he said, \"These schools will stand as monuments to a public-spirited citizen whose departure leaves a gap which it will be very difficult to fill.\"\n\nReporting on his funeral, the Straits Times noted, \"Crowds swelling to well over 50,000 lined an eight-mile route today for the funeral of the late Mr. Heah Joo Seang, millionaire rubber magnate and Penang MCA President, whose body was flown home last week from London where he died on May 14. The two-mile procession in which 15 associations and 12 schools took part, was the longest seen in Penang for many years. 5,000 people. Three Cabinet Ministers together with the Governor of Penang, Raja Tun Uda Al-Haj, and the Chief Minister, Inche Aziz Ibrahim, were among the 5,000 people who followed the hearse on its last journey. Early arrivals were Tun Leong Yew Koh, Minister of Justice, Mr. Tan Siew Sin, Minister of Finance and National President of the MCA, Dato Ong Yoke Lin, Minister of Health and Social Welfare, and Mr. Cheah Theam Swee, assistant Minister of Commerce and Industry. The Prime Minister, Tengku Abdul Rahman, the Deputy Prime Minister, Tun Abdul Razak, and the Governor, Raja Tun Uda Al-Haj, were among those who sent wreaths. Crowds started to gather from an early hour. Before the procession lined up, the coffin was taken out of the Heah mansion, Goodwood, in Macalister Road, and laid on the lawn where representatives of various schools and organisations paid their last respects to the late Mr. Heah. Mr. Yeo Hui Tung, a leader of the Teochew community, then read out an eulogy. Although timed to start at 11.30 a.m., the procession could not get moving until 12.15 p.m. Led by Senator Cheah Seng Khim and other members of the funeral committee, it proceeded along Macalister Road to the Church of Seven Sorrows where the first stage of the procession broke up. 'Last look.' It re-assembled later in front of the Teochew Association at Chulia Street and made its way to Beach Street to give the late Mr. Heah a \"last look\" at his office, Hock Lye Co., Ltd. The hearse, followed by 200 cars, then proceeded to Mount Erskine Cemetery where the burial took place at 5. p.m. after full Buddhist rites. Three school bands—St. Xavier's corps of drums and the Han Chiang and Jit Sin High School bands—marched with the procession. Among the 15 political parties, local guilds and associations which took part were UMNO and MCA youths, MCA women, UMNO and MIC members, the Teochew Hoay Kuan, the Kwangtung and Tengchow Associations, the Chinese Chamber of Commerce and the Penang Rubber Trade Association. Flags of various clubs and associations with which the late Mr. Heah was actively connected flew their flags at half-mast today.\"\n\nA Heah Joo Seang Scholarship Fund was set up in his memory, his family contributing $10,000 to get the fund going.\n\nHe was Managing Partner of Hin Giap, the largest rubber exporter in Penang, and one of the biggest employers of labour of Malaya in 1931; Chairman of Amalgamated Amusements (1932–1937); Chairman of Lam Seng Cheong (Ipoh); Chairman of Synn Cheong and Company (Taiping); Director of Khiam Seng Trading Company (Ipoh); Director of Kim Hin Company; and owner of Heah Joo Seang Rubber Estates. He was also the Proprietor of the Wembley Dance Hall, Penang.\n\nFollowing a notice in the Straits Times in August, Joo-Seang had bought from the Federation's Custodian of Property, about 26,000 acres of rubber estate land in Johore, at $6,000,000, by the start of October 1951. The transaction was the biggest of its kind up to that time involved property in the Kota Tinggi area and included Pengerang (4,632 acres), Senti (7,711), Belungkor (813), Telok Sengat (8,173), Sungei Ambok (843), Nanyo (3,089), Asahi (1,079), Sungei Seluyut (574), Tanjong Buai (132), and Buntu (425). This, on top of everything else he had done, pushed him to the forefront of the rubber business. He also actively led the industry to resolve issues. In February 1951 he convened and presided over a meeting of Penang merchants, both European and Asian, at the Penang Rubber Exchange to address the breakdown in discussions between the Penang Employers' Association and the owners of local lighters, the latter of whom were demanding a 25 percent increase in rates. The breakdown between the parties had negatively affected the trade of Penang. The matter, now being brought back to the table was resolved, and trading resumed.\n\nHe advocated finding new uses for natural rubber including laying rubber roads and rubber flooring in offices.\n\nAt a dinner hosted by the Penang Rubber Trade Association on 18 March 1961 to celebrate his Johan Mangku Negara award from the King, Joo Seang, then still the President of the association, said he would still be engaged in the rubber trade if he had to live his life over. \"It is a fascinating game in which one has to pit one's brains against the world.\" He had been connected to the organisation for a long time, having first served that body as Honorary Secretary, forty (40) years earlier.\n\nWho's Who in Malaya 1939: A Biographical Record of Prominent Members of Malaya's Community in Official, Professional and Commercial Circles, Singapore: Fishers Ltd (1939) provides a summary of the varied roles that Joo Seang played in Malaya:\nHe was also\n\nHe was also President, Li Tek Siah; President, Chinese Benevolent Association; President, Rubber Trade Association; President, Federation of Malaya Rubber Trade Association, Member of the Board of Managers of St. Xavier's Institution, Chairman, Han Chiang School and bore office among very many other organisations.\n\nOn August 1956, Heah Joo Seang hosted a dinner at his residence (the Goodwood mansion) in MacAlister road, Georgetown, Penang, in honour of the visit of Lord William Reid. Lord Reid was the Chairman of the Commission which was set up in England for the purpose of drafting and defining the Constitution of the Federation of Malaya. During his dinner speech, Lord Reid said, \"We intend to do our utmost to leave some trace of your devotion to this Country in the shape of the Constitution\", in reference to Joo Seang's championing of the rights of the straits born Chinese.\n\nThe drafting of the Constitution of the Federation of Malaya began the process toward the establishment of a new Government, after England agreed to concede independence to Malaya. The Commission was set up in March 1956 to seek \"the views of political parties, non-government organization and individuals on the form of government and racial structure appropriate to this Country\".\n\nJoo Seang was a strong supporter of Malayan politician Sir Onn Jaafar, founder of the United Malays National Organisation and Chief Minister of Johore. When the latter was facing political attacks, shortly after the Onn retired as Member for Home Affairs and Chairman of the Rural and Industrial Development Authority, Joo Seang, said in his defence, \"Sir Onn has been falsely accused of being anti-Chinese. He has been severely criticised by some so-called Malay leaders who only want to gratify their own selfish ends.\"\n\nJoo Seang was passionate about sports and education and these, more than anything attracted his interest. While not a comprehensive list, some of his gifts included:\n\nIn January 1961 he was awarded the Johan Mangku Negara (JMN) at the King's installation honours that year.\n\n"}
{"id": "24763942", "url": "https://en.wikipedia.org/wiki?curid=24763942", "title": "Hybrid solar lighting", "text": "Hybrid solar lighting\n\nHybrid solar lighting (HSL) or hybrid lighting systems combine the use of solar with artificial light for interior illumination by channelling sunlight through fiber optic cable bundles to provide solar light into rooms without windows or skylights, and by supplementing this natural light with artificial light—typically LED—as required.\nThe bundles are led from exterior/rooftop optical light collectors through small openings or cable ducts and carry the light to where it is needed. The optical fibers end in hybrid luminaires where the sunlight is joined with electric light, either on demand or to automatically maintain a constant light level even as the available sunlight decreases.\n\nSolar lighting systems capture light from the sun and conduct it towards a room using optical fibers. They use rooftop collectors, large mirrored dishes, that track the sun. The collectors adjust to aim the sunlight onto 127 optical fibers which are conducted into a single chord. The optical fibers are flexible and can be connected into hybrid light fixtures that are joined to diffusing rods that disperse the light. A single collector can power up to eight hybrid light fixtures covering .\n\nThe hybrid lights also use artificial lighting which is mixed with the natural sunlight beamed in down the fiber optic chord. Photosensors focus on how much light needs to be generated to add to the natural light in order to keep a room illuminated at a constant brightness. When the sun is blocked by clouds around five percent of its sunlight requirement will need to be added. Hybrid solar lighting systems should be used in rooms with direct roof access.\n\nThe price of each hybrid solar lighting system requires to be installed with each watt of light bulb used. It is about $5–$8 per watt.\n\n\n"}
{"id": "23466802", "url": "https://en.wikipedia.org/wiki?curid=23466802", "title": "Keilir", "text": "Keilir\n\nKeilir – Atlantic Centre of Excellence (Icelandic: \"Keilir, miðstöð vísinda, fræða og atvinnulífs\") is a private, non-profit, international educational institution located in Ásbrú next to Keflavik International Airport in the city of Reykjanesbaer in Iceland. The school was established in 2007.\n\nThe school is owned by the University of Iceland and Icelandic companies. Among the shareholders are Keflavik Savings Bank, Icelandair Group, HS Geothermal Power, Reykjavik Energy, Icelandic Geosurvey, Geysir Green Energy.\n\nKeilir is divided into four schools: School of Energy and Technology, Aviation Academy, School of Health and a University Bridge. \n\nKeilir graduates its university level students through its affiliation with the University of Iceland which is accredited higher education institution in Iceland under the recent Higher Education Act in Iceland, No. 63/2006, for each of its academic subject areas.\n\nKeilir's School of Energy and Technology offers two multidisciplinary B.Sc. programs in co-operation with the University of Iceland 220 ECTS units. One in energy technology and the other in mechatronics.\n\nThe program in energy technology has a focus on geothermal energy technology, capitalizing on the expertise of Icelandic energy companies in this field. The program also provides a background in other forms of renewable energy technology.\n\nThe program in mechatronics revolves around the integration of mechanical, electronic and software components into monitoring, control, and regulation systems. The two programs are interconnected, as such systems lie at the heart of geothermal and other renewable energy plants and play a role in the efficient use and conservation of energy.\n\nSchool places emphasis on project-based learning and collaboration with both the energy industry and the University of Iceland. The school is a part of Asbru's Energy cluster. The cluster also operates a research center in energy sciences. An energy resource park that is under development will provide additional facilities for collaboration between the energy industries (geothermal and other) and the School of Energy and Technology. \n\nKeilir Aviation Academy offers professional flight training, headed by a staff of airline pilots and instructors along with housing and Diamond DA20, DA40 and DA42 aircraft. As a former NATO base in Iceland, Keflavik International Airport (BIKF) has two 3000m runways and is the Icelandic gateway between Europe and the United States.\n\nThe Diamond aircraft are made of carbon fiber materials. The engines on the DA-40 are Centurion Engines 2.0 liter diesel engines with FADEC (Full Authority Digital Engine Control). In the same manner, the flight instruments are electronic as well, the Garmin G1000 Integrated Flight Deck is used to display the necessary information.\n\nTeaching staff of Keilir are researchers and professors from Iceland as well as energy experts from engineering and energy consulting firms.\n\n"}
{"id": "32401636", "url": "https://en.wikipedia.org/wiki?curid=32401636", "title": "Kenyte", "text": "Kenyte\n\nKenyte is a variety of porphyritic phonolite or trachyte with rhomb shaped phenocrysts of anorthoclase with variable olivine and augite in a glassy matrix. It was originally described and named by J. W. Gregory in 1900 for the occurrence on Mount Kenya. Kenyte has also been reported from Mount Kilimanjaro and Mount Erebus in Antarctica.\n\nKenyte has a glassy texture and may be composed of devitrified glass, which contains large phenocrysts of anorthoclase feldspar.\n"}
{"id": "5967486", "url": "https://en.wikipedia.org/wiki?curid=5967486", "title": "List of Sites of Special Scientific Interest in Gwent", "text": "List of Sites of Special Scientific Interest in Gwent\n\nDue to subsequent local government reorganisation in the UK since 1972, many counties and districts have been divided, merged or renamed. Using the AOS system alone would make it difficult to search for individual SSSI citations via the Countryside Council for Wales (CCW) database without knowing 1972 region divisions. As a result, the CCW groups Welsh SSSIs using the subdivisions of Wales formed in April 1996 by the Local Government (Wales) Act 1994, resulting in 22 principal areas.\n\nGwent AOS lies within the counties of Blaenau Gwent, Caerphilly, Monmouthshire, Newport, and Torfaen.\n\nFor SSSIs elsewhere in the UK, see List of SSSIs by Area of Search.\n\n"}
{"id": "6113041", "url": "https://en.wikipedia.org/wiki?curid=6113041", "title": "List of Sites of Special Scientific Interest in North East Fife", "text": "List of Sites of Special Scientific Interest in North East Fife\n\nThe following is a list of Sites of Special Scientific Interest in the North East Fife Area of Search, in Scotland. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "10697942", "url": "https://en.wikipedia.org/wiki?curid=10697942", "title": "List of Superfund sites", "text": "List of Superfund sites\n\nThis is a list of Superfund sites in the United States, designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) of 1980. Superfund sites are polluted locations requiring a long-term response to clean up hazardous material contaminations. CERCLA authorized the United States Environmental Protection Agency (EPA) to create a list of such locations, which are placed on the National Priorities List (NPL).\n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. , there were 1322 Superfund sites on the National Priorities List in the United States. Fifty-three additional sites have been proposed for entry on the list. , 375 sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "12698762", "url": "https://en.wikipedia.org/wiki?curid=12698762", "title": "List of fountains in Rome", "text": "List of fountains in Rome\n\nRome, is Italy's capital the City has a tremendous amount of art, architecture and culture nearly 3,000 years worth. The Colosseum, the Vatican City and The Trevi Fountain are very well known and are popular tourists destinations. \nThis is a list of the notable fountains in Rome, Italy. Rome has fifty monumental fountains and hundreds of smaller fountains, over 2000 fountains in all, more than any other city in the world. \n\nFor more than two thousand years fountains have provided drinking water and decorated the piazzas of Rome. During the Roman Empire, in 98 AD, according to Sextus Julius Frontinus, the Roman consul who was named \"curator aquarum\" or guardian of the water of the city, Rome had nine aqueducts which fed 39 monumental fountains and 591 public basins, not counting the water supplied to the Imperial household, baths and owners of private villas. Each of the major fountains was connected to two different aqueducts, in case one was shut down for service.\n\nAfter the fall of the Western Roman Empire, the aqueducts were wrecked or fell into disrepair, and the fountains stopped working. In the 14th century, Pope Nicholas V (1397–1455), a scholar who commissioned hundreds of translations of ancient Greek classics into Latin, decided to embellish the city and make it a worthy capital of the Christian world. In 1453 he began to rebuild the Acqua Vergine, the ruined Roman aqueduct which had brought clean drinking water to the city from eight miles (13 km) away. He also decided to revive the Roman custom of marking the arrival point of an aqueduct with a \"mostra\", a grand commemorative fountain. He commissioned the architect Leon Battista Alberti to build a wall fountain where the Trevi Fountain is now located. Alberti restored, modified, and expanded the aqueduct that supplied both the Trevi Fountain as well as the famous baroque fountains in the Piazza del Popolo and Piazza Navona.\n\nOne of the first new fountains to be built in Rome during the Renaissance was the Fountain in Piazza Santa Maria in Trastevere (1499), which was placed on the site of an earlier Roman fountain. Its design, based on an earlier Roman model, with a circular vasque on a pedestal pouring water into a basin below, became the model for many other fountains in Rome, and eventually for fountains in other cities, from Paris to London.\n\nDuring the 17th and 18th century the Roman popes reconstructed other ruined Roman aqueducts and built new display fountains to mark their termini, launching the golden age of the Roman fountain. The fountains of Rome, like the paintings of Rubens, were expressions of the new style of Baroque art. They were crowded with allegorical figures, and filled with emotion and movement. In these fountains, sculpture became the principal element, and the water was used simply to animate and decorate the sculptures. They, like baroque gardens, were \"a visual representation of confidence and power.\" \n\nThe most famous Roman fountains of this period include:\n\n\nThe fountains of Rome all operated purely by gravity- the source of water had to be higher than the fountain itself, and the difference in elevation and distance between the source and the fountain determined how high the fountain could shoot water. The fountain in St. Peter's Square was fed by the Paola aqueduct, restored in 1612, whose source was above sea level, which meant it could shoot water twenty feet up from the fountain. The Triton fountain benefited from its location in a valley, and the fact that it was fed by the Aqua Felice aqueduct, restored in 1587, which arrived in Rome at an elevation of above sea level (fasi), a difference of in elevation between the source and the fountain, which meant that the water from this fountain jetted sixteen feet straight up into the air from the conch shell of the Triton.\n\nThe fountains of Piazza Navona, on the other hand, took their water from the Acqua Vergine, which had only a drop from the source to the fountains, which meant the water could only fall or trickle downwards, not jet very high upwards. For the Trevi Fountain, the architect Nicola Salvi compensated for this problem by sinking the fountain down into the ground, and by carefully designing the cascade so that the water churned and tumbled, to add movement and drama.\n\nToday all of the fountains have been rebuilt, and the Roman water system uses both gravity and mechanical pumps. Water is recycled and water from different aqueducts is sometimes mixed before it reaches the fountains and performs for the spectators.\n\nThis (incomplete) list contains important fountains in the city:\n\nThese fountains were built at the termini of the restored aqueducts of Rome, to supply water to the population and to glorify the Popes who built them.\n\n\nThese fountains were linked to the restored aqueducts, decorated the piazzi, or squares, of Rome, and provided drinking water to the population around the squares.\n\n\n\n \n"}
{"id": "1685988", "url": "https://en.wikipedia.org/wiki?curid=1685988", "title": "List of herbivorous animals", "text": "List of herbivorous animals\n\nThis is a list of herbivorous animals. Herbivores are animals that eat plants. Herbivory is a form of consumption in which a heterotrophic organism consumes other organisms, principally autotrophs such as plants, algae and photosynthesizing bacteria. More generally, organisms that feed on autotrophs in general are known as 1st level consumers.\n\nMammals (formally Mammalia) are a class of vertebrate, air-breathing animals whose females are characterized by the possession of mammary glands while both males and females are characterized by hair and/or fur, three middle ear bones used in hearing, and a neocortex region in the brain. Herbivorous mammals include:\n\n\n\n\n\n\n\n\n\n\n\n\nBirds (class: Aves) are winged, bipedal, endothermic (warm-blooded), egg-laying, vertebrate animals. There are around 10,000 living species, making them the most varied of tetrapod vertebrates. Some birds are herbivorous, and some are not.\n\n\nSome extant Lissamphibians display semi-herbivorous habits:\n\n\n\n\n\n"}
{"id": "53200194", "url": "https://en.wikipedia.org/wiki?curid=53200194", "title": "List of highest mountains of the Czech Republic", "text": "List of highest mountains of the Czech Republic\n\nThis is the List of highest mountains of the Czech Republic.\n\nMost of them are located at Krkonoše, Hrubý Jeseník and Šumava () mountain ranges . Other mountain ranges with mountains over 1000 m that are not included in this general list are shown in additional tables.\n\nAll Czech mountain ranges are located along the borders to neighbouring Austria, Slovakia, Poland and Germany.\n\nThe information was obtained from \"Mountains in Czech Republic over one thousand meters above sea“ and \"Peakclimber“.\n"}
{"id": "2029556", "url": "https://en.wikipedia.org/wiki?curid=2029556", "title": "List of historical tropical cyclone names", "text": "List of historical tropical cyclone names\n\nTropical cyclones are named for historical reasons and so as to avoid confusion when communicating with the public as more than one tropical cyclone can exist at a time. Names are drawn in order from predetermined lists and are usually assigned to tropical cyclones with one-, three- or ten-minute windspeeds of at least 65 km/h (40 mph). However, standards vary from basin to basin with some tropical depressions named in the Western Pacific whilst tropical cyclones have to have gale force winds occurring more than halfway around the center within the Australian and Southern Pacific regions.\n\nThe official practice of naming tropical cyclones started in 1945 within the Western Pacific. Naming continued through the next few years before in 1950, names also started to be assigned to tropical storms forming in the North Atlantic ocean. In the Atlantic, names were originally taken from the World War Two version of the Phonetic Alphabet but this was changed in 1953 to use lists of women names which were created yearly. Around this time naming of tropical cyclones also began within the Southern and Central parts of the Pacific. However naming didn't begin in the Eastern Pacific until 1960 with the original naming lists designed to be used year after year in sequence. In 1960, naming also began in the Southwest Indian Ocean and in 1963 the Philippine Meteorological Service started assigning names to tropical cyclones that moved into or formed in their area of responsibility. Later in 1963 warning centers within the Australian region also commenced naming tropical cyclones. In 2011, the Brazilian Navy Hydrographic Center started using naming list to name tropical cyclones over the South Atlantic basin.\n\nBy 1950, tropical cyclones that were judged by the US Weather Bureau to have intensified into a tropical storm started to be assigned names. Storms were originally named in alphabetical order using the World War Two version of the Phonetic Alphabet. By 1952 a new phonetic alphabet had been developed and this led to confusion as some parties wanted to use the newer phonetic alphabet. In 1953, to alleviate any confusion, forecasters decided to use a set of 23 feminine names. After the 1953 Atlantic hurricane season, public reception to the idea seemed favorable, so the same list was adopted for the next year with one change; Gilda for Gail. However, after storms like Carol and Hazel got a lot of publicity during the 1953 season, forecasters agreed to develop a new set of names for 1955. However, before this could happen, a tropical storm was declared significant on January 2, 1955 and was named as Alice. The new set of names were developed and used in 1955 beginning with Brenda continuing through the alphabet to Zelda. For each season before 1960, a new set of names were developed. In 1960 forecasters decided to begin rotating names in a regular sequence and thus four alphabetical lists were established to be repeated every four years. The sets followed the example of the western Pacific typhoon naming lists and excluded names beginning with the letters Q, U, X, Y and Z. These four lists were used until 1972 when the National Oceanic and Atmospheric Administration (NOAA), replaced them with 9 lists designed to be used from 1972. In 1977, NOAA made the decision to relinquish control over the name selection by allowing a regional committee of the World Meteorological Organization to select the new sets of names which would contain male names and some Spanish and French names in order to reflect all the cultures and languages within the Atlantic Ocean. The World Meteorological Organization decided that the new lists of hurricane name would start to be used in 1979. Since 1979 the same lists have been used, with names of significant tropical cyclones removed from the lists and replaced with new names. In 2002 Subtropical Cyclones started to be assigned names from the main list of names set up for that year. In 2005 as all the names preselected for the season were exhausted, the contingency plan of using Greek letters for names had to be used. Since then there have been a few attempts to get rid of the Greek names, as they are seen to be inconsistent with the standard naming convention used for tropical cyclones and are considered generally unknown and confusing to the public. However the lists of preselected names for the year, are not expected to be used up frequently enough to warrant any change in the existing naming procedure and thus the Greek Alphabet will be used if the list of pre selected names should ever be used up again.\n\nWithin the Eastern Pacific basin between the western coasts of the Americas and 140°W the naming of tropical cyclones started in 1960, with four lists of female names initially designed to be used consecutively before being repeated. In 1965 after two lists of names had been used, it was decided to return to the top of the second list and to start recycling the sets of names on an annual basis.\n\nIn 1977, after protests by various women's rights groups, NOAA made the decision to relinquish control over the name selection by allowing a regional committee of the World Meteorological Organization (WMO) to select new sets of names. The WMO selected six lists of names which contained male names and rotated every six years. They also decided that the new lists of hurricane name would start to be used in 1978 which was a year earlier than the Atlantic. Since 1978 the same lists of names have been used, with names of significant tropical cyclones removed from the lists and replaced with new names. As in the Atlantic basin should the names preselected for the season be exhausted, the contingency plan of using Greek letters for names would be used. However unlike in the Atlantic basin the contingency plan has never had to be used, although in 1985 to avoid using the contingency plan, the letters X, Y, and Z were added to the lists. Since the contingency plan had to be used in the North Atlantic during 2005 there have been a few attempts to get rid of the Greek names as they are seen to be inconsistent with the standard naming convention used for tropical cyclones and are generally unknown and confusing to the public. However none of the attempts have succeeded and thus the Greek letters will be used should the lists be used up.\n\nIn 1950 a tropical cyclone that affected Hawaii was named Able, after a tropical cyclone had not affected Hawaii for a number of years. The system was also named Salome by the Air Weather Service Office in Guam, before it became widely known as Hurricane Hiki, since Hiki is Hawaiian for Able. Typhoon Olive of 1952 developed within the Central Pacific, but was not named until it had crossed the International Dateline and moved into the Western Pacific basin. During 1957, three other tropical cyclones developed in the Central Pacific and were named Kanoa, Della and Nina, by the Hawaiian military meteorological offices. It was subsequently decided that future tropical cyclones, would be named by borrowing names from the Western Pacific naming lists. Hawaiian names were reinstated for the lists during 1979, with 5 sets of names drafted using only the 12 letters of the Hawaiian alphabet, with the intent being to use the sets of names on an annual rotation basis. However, after no storms had developed in this region between 1979 and 1981, the annual lists were scrapped and replaced with four sets of names and designed to be used consecutively. Ahead of the 2007 hurricane season, the Central Pacific Hurricane Center introduced a revised set of Hawaiian names for the Central Pacific, after they had worked with the University of Hawaii Hawaiian Studies Department to ensure the correct meaning and appropriate historical and cultural use of the names.\n\nIn the Western North Pacific ocean, there are two sets of names generally used. The first are the international names assigned to a tropical cyclone by the Japan Meteorological Agency (JMA) or the Joint Typhoon Warning Center (JTWC). The second set of names are local names assigned to a tropical cyclone by the Philippine Atmospheric, Geophysical and Astronomical Services Administration. This system often ends up with a tropical cyclone being assigned two names, should a tropical storm threaten the Philippines.\n\nOn January 1, 2000, the Japan Meteorological Agency, as the official Regional Specialized Meteorological Center, took over the naming of tropical cyclones in this basin. The names selected by the World Meteorological Organization's Typhoon Committee were from a pool of names submitted by the various countries that make up the Typhoon Committee.\n\nSince 1963, the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA), have assigned their own names to typhoons that pass through its area of responsibility. Unlike the World Meteorological Organization's standard of assigning names to tropical cyclones when they reach wind-speeds of 65 km/h (40 mph), PAGASA assigns a name to a tropical depression when they either form or move into their area of responsibility. Four sets of tropical cyclone names are rotated annually with typhoon names stricken from the list should they do more than  pesos worth of damage to the Philippines and/or cause 300 or more deaths. Should the list of names for a given year prove insufficient, names are taken from an auxiliary list.\n\nDuring its annual session in 2000 the WMO/ESCAP Panel on North Indian tropical cyclones, agreed in principle to start assigning names to Cyclonic Storms that developed within the North Indian Ocean. As a result, the panel requested that each member country submit a list of ten names to a rapporteur by the end of the year 2000. At the 2001 session of the Panel, the rapporteur reported that seven of the eight countries had submitted their names. However, India had refused to submit a list of names, as it had some reservations about assigning names to tropical cyclones, due to the regional, cultural and linguistic diversity of the panel members. The panel subsequently studied the names and felt that some of the names would not be appealing to the public or the media, and requested that members submit new lists of names. At the following years session the rapporteur reported that there had been a poor response by member countries in resubmitting their lists of names. In response the panel felt that it was important that the work continued and urged the members to copperate and submit their names to the rapporteur. The names were subsequently submitted in time for the 2004 session, however, India had still not submitted their names, despite promising to do so. The rapporteur presented the 4 lists of names that would be used with a gap left for India's names and recommended that the India Meteorological Department's Regional Specialised Meteorological Centre in New Delhi name the systems. The rapporteur also recommended that the naming lists were used on an experimental basis during the season, starting in May or June 2004 and that the lists should only be used until 2009 when a new list would be drawn up for the following ten years. The naming lists were then completed in May 2004, after India submitted their names, however the lists were not used until September 2004 when the first tropical cyclone was named Onil by India Meteorological Department.\n\nIn January 1960, a formal naming scheme was introduced for the South-West Indian Ocean between Africa and 80°E, by the Mauritius and Madagascan Weather Services with the first cyclone being named Alix. Over the next few years the names were selected in various ways including by the meteorological services of the region for several years at a time, before it was turned over to the WMO's South West Indian Ocean Tropical Cyclone Committee at the start of the 2000-01 season.\n\nTropical cyclones in the Australian region are named by one of five tropical cyclone warning centers that each have the right to name tropical cyclones. The naming of cyclones officially started during the 1963–64 tropical cyclone season, with the first name being assigned on January 6, 1964 to Cyclone Bessie. Female names were used exclusively until the current convention of alternating male and female names commenced in 1975. Names that cause significant damage within the Australian region are retired by the Bureau of Meteorology (Australia) with new names selected at the bi-annual meeting of the World Meteorological Organization's RA V Tropical Cyclone Committee. In 2008, the lists used by the three TCWC centres were combined to form a single list of names.\n\nTropical Cyclones started to be named within the South Pacific, by the New Caledonia Meteorological Office during the 1958–59 season. The Fiji Office of the New Zealand Meteorological Service subsequently started to also name cyclones during the 1969–70 season with Alice being the first name to be used.\n\nDuring March 2004, a rare tropical cyclone developed within the Southern Atlantic, about to the east-southeast of Florianópolis in southern Brazil. As the system was threatening the Brazilian state of Santa Catarina, a newspaper used the headline \"Furacão Catarina,\" which was presumed to mean \"furacão (hurricane) threatening (Santa) Catarina (the state)\". However, when the international press started monitoring the system, it was assumed that \"Furacão Catarina\" meant \"Cyclone Catarina\" and that it had been formally named in the usual way. During March 12, 2010, public and private weather services in Southern Brazil, decided to name a tropical storm Anita in order to avoid confusion in future references. A naming list was subsequently set up by the Brazilian Navy Hydrographic Center with the names Arani, Bapo, Cari, Deni, Eçaí and Guará subsequently taken from that list during 2011, 2015, 2016 and 2017, respectively.\n\n\n"}
{"id": "8986578", "url": "https://en.wikipedia.org/wiki?curid=8986578", "title": "List of rye diseases", "text": "List of rye diseases\n\nThis article is a list of diseases of rye (\"Secale cereale\").\n\n"}
{"id": "13659870", "url": "https://en.wikipedia.org/wiki?curid=13659870", "title": "Lópezite", "text": "Lópezite\n\nLopezite is a rare red chromate mineral with chemical formula: KCrO. It crystallizes in the triclinic crystal system.\n\nIt occurs as rare vug fillings in nitrate ores in association with tarapacáite (KCrO), dietzeite and ulexite in the Chilean Atacama and is reported from the Bushveld igneous complex of South Africa. Lopezite was first described in 1937 for an occurrence in Iquique Province, Chile and named after Chilean mining engineer Emiliano López Saa (1871–1959).\n\nMost lopezite offered for sale to collectors is artificially produced. Synthetic varieties also exhibit monoclinic crystals.\n"}
{"id": "14504482", "url": "https://en.wikipedia.org/wiki?curid=14504482", "title": "Micro power source", "text": "Micro power source\n\n[1] La O` G.J., In H.J., Crumlin E., Barbastathis G., Shao-Horn Y. Resent advances in microdevices for electrochemical energy conversion and storage // Int. J. Energy Res. 2007. V.31. P.548-575.\n\n[2] Curtright A.E., Bouwman P.J., Wartena R.C., Swider-Lyons K.E. Power sources for nanotechnology // International Journal of Nanotechnology. 2004. V.1. Nos.1/2. P.226-239\n"}
{"id": "5540651", "url": "https://en.wikipedia.org/wiki?curid=5540651", "title": "Microwave transmission", "text": "Microwave transmission\n\nMicrowave transmission is the transmission of information or energy by microwave radio waves. Although an experimental microwave telecommunication link across the English Channel was demonstrated in 1931, the development of radar in World War II provided the technology for practical exploitation of microwave communication. In the 1950s, large transcontinental \"microwave relay\" networks, consisting of chains of repeater stations linked by line-of-sight beams of microwaves were built in Europe and America to relay long distance telephone traffic and television programs between cities. Communication satellites which transferred data between ground stations by microwaves took over much long distance traffic in the 1960s. In recent years, there has been an explosive increase in use of the microwave spectrum by new telecommunication technologies such as wireless networks, and direct-broadcast satellites which broadcast television and radio directly into consumers' homes.\n\nMicrowaves are widely used for point-to-point communications because their small wavelength allows conveniently-sized antennas to direct them in narrow beams, which can be pointed directly at the receiving antenna. This allows nearby microwave equipment to use the same frequencies without interfering with each other, as lower frequency radio waves do. Another advantage is that the high frequency of microwaves gives the microwave band a very large information-carrying capacity; the microwave band has a bandwidth 30 times that of all the rest of the radio spectrum below it. A disadvantage is that microwaves are limited to line of sight propagation; they cannot pass around hills or mountains as lower frequency radio waves can.\nMicrowave radio transmission is commonly used in point-to-point communication systems on the surface of the Earth, in satellite communications, and in deep space radio communications. Other parts of the microwave radio band are used for radars, radio navigation systems, sensor systems, and radio astronomy.\n\nThe next higher part of the radio electromagnetic spectrum, where the frequencies are above 30 GHz and below 100 GHz, are called \"millimeter waves\" because their wavelengths are conveniently measured in millimeters, and their wavelengths range from 10 mm down to 3.0 mm (Higher frequency waves are smaller in wavelength). Radio waves in this band are usually strongly attenuated by the Earthly atmosphere and particles contained in it, especially during wet weather. Also, in wide band of frequencies around 60 GHz, the radio waves are strongly attenuated by molecular oxygen in the atmosphere. The electronic technologies needed in the millimeter wave band are also much more difficult to utilize than those of the microwave band.\n\n\n\n\n\nMicrowave radio relay is a technology widely used in the 1950s and 1960s for transmitting signals, such as long-distance telephone calls and television programs between two terrestrial points on a narrow beam of microwaves. In microwave radio relay, microwaves are transmitted on a line of sight path between relay stations using directional antennas, forming a fixed radio connection between the two points. The requirement of a line of sight limits the separation between stations to the visual horizon, about 30 to 50 miles. Before the widespread use of communications satellites, chains of microwave relay stations were used to transmit telecommunication signals over transcontinental distances. \n\nBeginning in the 1950s, networks of microwave relay links, such as the AT&T Long Lines system in the U.S., carried long distance telephone calls and television programs between cities. The first system, dubbed TD-2 and built by AT&T, connected New York and Boston in 1947 with a series of eight radio relay stations. These included long daisy-chained series of such links that traversed mountain ranges and spanned continents. Much of the transcontinental traffic is now carried by cheaper optical fibers and communication satellites, but microwave relay remains important for shorter distances.\n\nBecause the radio waves travel in narrow beams confined to a line-of-sight path from one antenna to the other, they don't interfere with other microwave equipment, so nearby microwave links can use the same frequencies (see Frequency reuse). Antennas must be highly directional (high gain); these antennas are installed in elevated locations such as large radio towers in order to be able to transmit across long distances. Typical types of antenna used in radio relay link installations are parabolic antennas, dielectric lens, and horn-reflector antennas, which have a diameter of up to 4 meters. Highly directive antennas permit an economical use of the available frequency spectrum, despite long transmission distances.\n\nBecause of the high frequencies used, a line-of-sight path between the stations is required. Additionally, in order to avoid attenuation of the beam, an area around the beam called the first Fresnel zone must be free from obstacles. Obstacles in the signal field cause unwanted attenuation. High mountain peak or ridge positions are often ideal.\n\nObstacles, the curvature of the Earth, the geography of the area and reception issues arising from the use of nearby land (such as in manufacturing and forestry) are important issues to consider when planning radio links. In the planning process, it is essential that \"path profiles\" are produced, which provide information about the terrain and Fresnel zones affecting the transmission path. The presence of a water surface, such as a lake or river, along the path also must be taken into consideration since it can reflect the beam, and the direct and reflected beam can interfere at the receiving antenna, causing multipath fading. Multipath fades are usually deep only in a small spot and a narrow frequency band, so space and/or frequency diversity schemes can be applied to mitigate these effects.\n\nThe effects of atmospheric stratification cause the radio path to bend downward in a typical situation so a major distance is possible as the earth equivalent curvature increases from 6370 km to about 8500 km (a 4/3 equivalent radius effect). Rare events of temperature, humidity and pressure profile versus height, may produce large deviations and distortion of the propagation and affect transmission quality. High-intensity rain and snow making rain fade must also be considered as an impairment factor, especially at frequencies above 10 GHz. All previous factors, collectively known as path loss, make it necessary to compute suitable power margins, in order to maintain the link operative for a high percentage of time, like the standard 99.99% or 99.999% used in 'carrier class' services of most telecommunication operators.\n\nThe longest microwave radio relay known up to date crosses the Red Sea with 360 km (200 mi) hop between Jebel Erba (2170m a.s.l., , Sudan) and Jebel Dakka (2572m a.s.l., , Saudi Arabia). The link was built in 1979 by to transmit 300 telephone channels and 1 TV signal, in the 2 GHz frequency band. (Hop distance is the distance between two microwave stations)\n\nPrevious considerations represent typical problems characterizing terrestrial radio links using microwaves for the so-called backbone networks: hop lengths of few tens of kilometers (typically 10 to 60 km) were largely used until the 1990s. Frequency bands below 10 GHz, and above all, the information to be transmitted, were a stream containing a fixed capacity block. The target was to supply the requested availability for the whole block (Plesiochronous digital hierarchy, PDH, or Synchronous Digital Hierarchy, SDH). Fading and/or multipath affecting the link for short time period during the day had to be counteracted by the diversity architecture. During 1990s microwave radio links begun widely to be used for urban links in cellular network. Requirements regarding link distance changed to shorter hops (less than 10 km, typically 3 to 5 km), and frequency increased to bands between 11 and 43 GHz and more recently, up to 86 GHz (E-band). Furthermore, link planning deals more with intense rainfall and less with multipath, so diversity schemes became less used. Another big change that occurred during the last decade was an evolution toward packet radio transmission. Therefore, new countermeasures, such as adaptive modulation, have been adopted.\n\nThe emitted power is regulated by norms (EIRP) both for cellular system and microwave. These microwave transmissions use emitted power typically from 30 mW to 0.3 W, radiated by the parabolic antenna on a beam wide round few degrees (1 to 3-4). The microwave channel arrangement is regulated by International Telecommunication Union (ITU-R) or local regulations (ETSI, FCC). In the last decade the dedicated spectrum for each microwave band reaches an extreme overcrowding, forcing efforts towards techniques for increasing the transmission capacity (frequency reuse, Polarization-division multiplexing, XPIC, MIMO).\n\nThe history of radio relay communication began in 1898 from the publication by Johann Mattausch in Austrian journal, Zeitschrift für Electrotechnik. But his proposal was primitive and not suitable for practical use. The first experiments with radio repeater stations to relay radio signals were done in 1899 by Emile Guarini-Foresio. However the low frequency and medium frequency radio waves used during the first 40 years of radio proved to be able to travel long distances by ground wave and skywave propagation. The need for radio relay did not really begin until the 1940s exploitation of microwaves, which traveled by line of sight and so were limited to a propagation distance of about by the visual horizon. \nIn 1931 an Anglo-French consortium headed by Andre C. Clavier demonstrated an experimental microwave relay link across the English Channel using dishes. Telephony, telegraph, and facsimile data was transmitted over the bidirectional 1.7 GHz beams between Dover, UK, and Calais, France. The radiated power, produced by a miniature Barkhausen-Kurz tube located at the dish's focus, was one-half watt. A 1933 military microwave link between airports at St. Inglevert, France, and Lympne, UK, a distance of , was followed in 1935 by a 300 MHz telecommunication link, the first commercial microwave relay system.\n\nThe development of radar during World War II provided much of the microwave technology which made practical microwave communication links possible, particularly the klystron oscillator and techniques of designing parabolic antennas. Though not commonly known, the US military used both portable and fixed-station microwave communications in the European Theater during World War II.\n\nAfter the war telephone companies used this technology to build large microwave radio relay networks to carry long distance telephone calls. During the 1950s a unit of the US telephone carrier, AT&T Long Lines, built a transcontinental system of microwave relay links across the US that grew to carry the majority of US long distance telephone traffic, as well as television network signals. The main motivation in 1946 to use microwave radio instead of cable was that a large capacity could be installed quickly and at less cost. It was expected at that time that the annual operating costs for microwave radio would be greater than for cable. There were two main reasons that a large capacity had to be introduced suddenly: Pent up demand for long distance telephone service, because of the hiatus during the war years, and the new medium of television, which needed more bandwidth than radio. The prototype was called TDX and was tested with a connection between New York City and Murray Hill, the location of Bell Laboratories in 1946. The TDX system was set up between New York and Boston in 1947. The TDX was upgraded to the TD2 system, which used [the Morton tube, 416B and later 416C, manufactured by Western Electric] in the transmitters, and then later to TD3 that used solid state electronics.\n\nMilitary microwave relay systems continued to be used into the 1960s, when many of these systems were supplanted with tropospheric scatter or communication satellite systems. When the NATO military arm was formed, much of this existing equipment was transferred to communications groups. The typical communications systems used by NATO during that time period consisted of the technologies which had been developed for use by the telephone carrier entities in host countries. One example from the USA is the RCA CW-20A 1–2 GHz microwave relay system which utilized flexible UHF cable rather than the rigid waveguide required by higher frequency systems, making it ideal for tactical applications. The typical microwave relay installation or portable van had two radio systems (plus backup) connecting two line of sight sites. These radios would often carry 24 telephone channels frequency division multiplexed on the microwave carrier (i.e. Lenkurt 33C FDM). Any channel could be designated to carry up to 18 teletype communications instead. Similar systems from Germany and other member nations were also in use.\n\nLong-distance microwave relay networks were built in many countries until the 1980s, when the technology lost its share of fixed operation to newer technologies such as fiber-optic cable and communication satellites, which offer a lower cost per bit.\nDuring the Cold War, the US intelligence agencies, such as the National Security Agency (NSA), were reportedly able to intercept Soviet microwave traffic using satellites such as Rhyolite. Much of the beam of a microwave link passes the receiving antenna and radiates toward the horizon, into space. By positioning a geosynchronous satellite in the path of the beam, the microwave beam can be received.\n\nAt the turn of the century, microwave radio relay systems are being used increasingly in portable radio applications. The technology is particularly suited to this application because of lower operating costs, a more efficient infrastructure, and provision of direct hardware access to the portable radio operator.\n\nA microwave link is a communications system that uses a beam of radio waves in the microwave frequency range to transmit video, audio, or data between two locations, which can be from just a few feet or meters to several miles or kilometers apart. Microwave links are commonly used by television broadcasters to transmit programmes across a country, for instance, or from an outside broadcast back to a studio.\n\nMobile units can be camera mounted, allowing cameras the freedom to move around without trailing cables. These are often seen on the touchlines of sports fields on Steadicam systems.\n\n\n\nTerrestrial microwave relay links are limited in distance to the visual horizon, a few tens of miles or kilometers depending on tower height. Tropospheric scatter (\"troposcatter\" or \"scatter\") was a technology developed in the 1950s allow microwave communication links beyond the horizon, to a range of several hundred kilometers. The transmitter radiates a beam of microwaves into the sky, at a shallow angle above the horizon toward the receiver. As the beam passes through the troposphere a small fraction of the microwave energy is scattered back toward the ground by water vapor and dust in the air. A sensitive receiver beyond the horizon picks up this reflected signal. Signal clarity obtained by this method depends on the weather and other factors, and as a result a high level of technical difficulty is involved in the creation of a reliable over horizon radio relay link. Troposcatter links are therefore only used in special circumstances where satellites and other long distance communication channels cannot be relied on, such as in military communications.\n\n\n\n"}
{"id": "27663682", "url": "https://en.wikipedia.org/wiki?curid=27663682", "title": "Offshore oil spill prevention and response", "text": "Offshore oil spill prevention and response\n\nOffshore oil spill prevention and response is the study and practice of reducing the number of offshore incidents that release oil or hazardous substances into the environment and limiting the amount released during those incidents.\n\nImportant aspects of prevention include technological assessment of equipment and procedures, and protocols for training, inspection, and contingency plans for the avoidance, control, and shutdown of offshore operations. Response includes technological assessment of equipment and procedures for cleaning up oil spills, and protocols for the detection, monitoring, containment, and removal of oil spills, and the restoration of affected wildlife and habitat.\n\nIn the United States, offshore oil spill prevention contingency plans and emergency response plans are federally mandated requirements for all offshore oil facilities in U.S. Federal waters. Currently administered by the Minerals Management Service (MMS), these regulatory functions were ordered on May 19, 2010 to be transferred to the United States Department of the Interior's newly created Bureau of Safety and Environmental Enforcement. Oil spills in inland waters are the responsibility of the Environmental Protection Agency (EPA), while oil spills in coastal waters and deepwater ports are the responsibility of the U.S. Coast Guard.\n\nUnlike the Best Available Technology (BAT) criteria stipulated by the Clean Air Act and the Clean Water Act, the Outer Continental Shelf Lands Act amendments of 1978 stipulated that offshore drilling and oil spill response practices incorporate the use of Best Available and Safest Technologies (BAST).\n\nLink\n\nBecause of treatment and disposal requirements for drilling and production, wastes are likely to become ever more stringent. Bans on land disposal will pose even greater challenges, especially for remote oil and gas operations. The significant costs to oil and gas producers complying with this new wave of regulation will be outweighed only by the even more significant costs of non-compliance. The federal Environmental Protection Agency (EPA) in the United States and similar bodies globally as well as many state and local agencies have greatly increased both their enforcement capabilities and activities. Most environmental laws carry criminal charges. Because of this many operations personnel and members of senior management of large companies have found themselves on the wrong side of environmental enforcement actions through ignorance to the increasingly complex requirements and the severe consequences of violating environmental laws. \n\nInternational treaties, like the International Convention for the Prevention of Pollution from Ships (MARPOL), administered by the International Maritime Organization and implemented in many countries as legislation (such as the US Oil Pollution Act of 1973) place mandatory restrictions, recording, and penalties for the spilling of oil from ships.\n\nHydrocarbon producing wells are designed and managed on the basis of the 'barriers' in place to maintain containment. A 'dual barrier' philosophy is typically used whereby two independently verified barriers to the hydrocarbon reservoir and the environment are required at all times. The failure of a single barrier would not lead to a hydrocarbon release. During the different phases of drilling, production, workover and abandonments, many different pieces of equipment will be used to maintain control of the well fluids and pressures.\n\nThe primary safety control devices for well drilling are blowout preventers (BOPs), which have been used for nearly a century in control of oil well drilling on land. The BOP equipment technology has been adapted and used in offshore wells since the 1960s. The inspection and repair of subsea BOPs are much more costly, and the consequences of failure potentially much worse. There are two variations of offshore BOP in use; the sub-sea blowout preventer which sits on the ocean floor, and the surface blowout preventer which sits between the riser pipe and the drilling platform. The surface unit is smaller, lighter, less costly, and more easily accessed for routine tests and maintenance. However, it does not prevent blowouts involving a broken riser pipe.\n\nBlowout Preventers often contain a stack of independently-operated cutoff mechanisms, so there is redundancy in case of failure, and the ability to work in all normal circumstances with the drill pipe in or out of the well bore. The BOP used in the Deepwater Horizon, for example, had five \"rams\" and two \"annular\" blowout preventers. The rams were of two types: \"pipe rams\" and \"shear rams\". If the drill pipe is in the well, the pipe rams slide perpendicular to the pipe, closing around it to form a tight seal. The annular preventers also close around the pipe, but have more of a vertical motion, so they loosen slightly if the drill pipe is being pushed downward, as might be necessary in a \"snubbing\" or \"well kill\" operation. Shear rams may be used as a last resort to cut through the drill pipe and shut off everything, including whatever might be coming up inside the drill pipe.\n\nStudies done for the Minerals Management Service have questioned the reliability of shear rams in deep-water drilling. Figure 1 shows the result of a 2002 study on offshore oil rigs. This study was designed to answer the question “Can a given rig’s BOP equipment shear the pipe to be used in a given drilling program at the most demanding condition to be expected?” Seven of the fourteen cases in this study opted not to test, another had insufficient data to draw a definitive conclusion, and three failed to shear the pipe under realistic conditions of expected well bore and seawater pressure. In each case of failure, increasing the pressure on the rams above its design value, successfully sheared the pipe. A follow-up study in 2004 confirmed these results with a much larger sample of drill pipes and typical blowout preventers from three different manufacturers.\n\nIn addition to insufficient ram pressure, a \"New York Times\" investigation\nof the Deepwater Horizon oil spill listed other problem areas for deepwater blowout preventers. If one of the threaded joints between pipe sections is positioned within a shear ram, the ram would probably not cut through it, because the joints are \"nearly indestructable\". Requiring two shear rams in every blowout preventer may help to avoid this problem and to avoid some types of \"single-point failure\". Other technologies that might improve the reliability of BOPs include backup systems for sending commands to the BOP and more powerful submersibles that connect to the BOP's hydraulics system.\n\nCasing of offshore oil wells is done with a set of nested steel pipes, cemented to the rock walls of the borehole as in Figure 4. Each section is suspended by a threaded adapter inside the bottom end of the section above. Failure of either the casings or the cement can lead to injection of oil into groundwater layers, flow to the surface far from the well, or a blowout at the wellhead.\n\nIn addition to casings, oil wells usually contain a \"production liner\" or \"production tubing\", which is another set of steel pipes suspended inside the casing. The \"annulus\" between the casing and the production liner is filled with \"mud\" of a specific density to \"balance\" the pressure inside the casing with the \"pore pressure\" of fluids in the surrounding rock \"formations\".\n\nTo ensure that the cement forms a strong, continuous, 360-degree seal between the casing and the borehole, \"centralizers\" are placed around the casing sections before they are lowered into the borehole. Cement is then injected in the space between the bottom of the new casing section and the bottom of the borehole. The cement flows up around the outside of the casing, replacing the mud in that space with pure, uncontaminated cement. Then the cement is held perfectly still for several hours while it solidifies.\n\nWithout centralizers, there is a high risk that a channel of drilling mud or contaminated cement will be left where the casing contacts the borehole. These channels can provide a path for a later blowout. Even a thin crack can be pushed open by the enormous pressure of oil from below. Then erosion of the cement can occur from high-velocity sand particles in the oil. A hairline crack can thus become a wide-open gushing channel.\n\nAnother cause of cement failure is not waiting long enough for the cement to solidify. This can be the result of a rushed drilling schedule, or it could happen if there is a leak causing the cement to creep during the time it is supposed to be setting. A \"cement evaluation log\" can be run after each cement job to provide a detailed, 360-degree check of the integrity of the entire seal. Sometimes these logs are skipped due to schedule pressures.\n\nCement is also used to form permanent barriers in the annulus outside the production liner, and temporary barriers inside the liner. The temporary barriers are used to \"shut in\" the well after drilling and before the start of production. Figure 4 shows a barrier being tested by replacing the heavy mud above it with lighter seawater. If the cement plug is able to contain the pressure from the mud below, there will be no upward flow of seawater, and it can be replaced with mud for the final shut in.\n\nThere are no cement barriers in the annulus in Figure 4. While there is no requirement for such barriers, adding them can minimize the risk of a blowout through a direct wide-open channel from the reservoir to the surface.\n\n\n"}
{"id": "177602", "url": "https://en.wikipedia.org/wiki?curid=177602", "title": "Outer space", "text": "Outer space\n\nOuter space, or just space, is the expanse that exists beyond the Earth and between celestial bodies. Outer space is not completely empty—it is a hard vacuum containing a low density of particles, predominantly a plasma of hydrogen and helium as well as electromagnetic radiation, magnetic fields, neutrinos, dust, and cosmic rays. The baseline temperature, as set by the background radiation from the Big Bang, is . The plasma between galaxies accounts for about half of the baryonic (ordinary) matter in the universe; it has a number density of less than one hydrogen atom per cubic metre and a temperature of millions of kelvins; local concentrations of this plasma have condensed into stars and galaxies. Studies indicate that 90% of the mass in most galaxies is in an unknown form, called dark matter, which interacts with other matter through gravitational but not electromagnetic forces. Observations suggest that the majority of the mass-energy in the observable universe is a poorly understood vacuum energy of space, which astronomers label \"dark energy\". Intergalactic space takes up most of the volume of the Universe, but even galaxies and star systems consist almost entirely of empty space.\n\nOuter space does not begin at a definite altitude above the Earth's surface. However, the Kármán line, an altitude of above sea level, is conventionally used as the start of outer space in space treaties and for aerospace records keeping. The framework for international space law was established by the Outer Space Treaty, which entered into force on 10 October 1967. This treaty precludes any claims of national sovereignty and permits all states to freely explore outer space. Despite the drafting of UN resolutions for the peaceful uses of outer space, anti-satellite weapons have been tested in Earth orbit.\n\nHumans began the physical exploration of space during the 20th century with the advent of high-altitude balloon flights, followed by manned rocket launches. Earth orbit was first achieved by Yuri Gagarin of the Soviet Union in 1961, and unmanned spacecraft have since reached all of the known planets in the Solar System. Due to the high cost of getting into space, manned spaceflight has been limited to low Earth orbit and the Moon.\n\nOuter space represents a challenging environment for human exploration because of the hazards of vacuum and radiation. Microgravity also has a negative effect on human physiology that causes both muscle atrophy and bone loss. In addition to these health and environmental issues, the economic cost of putting objects, including humans, into space is very high.\n\nIn 350 BCE, Greek philosopher Aristotle suggested that \"nature abhors a vacuum\", a principle that became known as the \"horror vacui\". This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.\n\nIn ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, \"empty and void of substance\". Likewise, the \"sun, moon, and the company of stars float in the empty space, moving or standing still\".\n\nThe Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. However, it would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.\n\nIn 1650, German scientist Otto von Guericke constructed the first vacuum pump: a device that would further refute the principle of \"horror vacui\". He correctly noted that the atmosphere of the Earth surrounds the planet like a shell, with the density gradually declining with altitude. He concluded that there must be a vacuum between the Earth and the Moon.\n\nBack in the 15th century, German theologian Nicolaus Cusanus speculated that the Universe lacked a center and a circumference. He believed that the Universe, while not infinite, could not be held as finite as it lacked any bounds within which it could be contained. These ideas led to speculations as to the infinite dimension of space by the Italian philosopher Giordano Bruno in the 16th century. He extended the Copernican heliocentric cosmology to the concept of an infinite Universe filled with a substance he called aether, which did not resist the motion of heavenly bodies. English philosopher William Gilbert arrived at a similar conclusion, arguing that the stars are visible to us only because they are surrounded by a thin aether or a void. This concept of an aether originated with ancient Greek philosophers, including Aristotle, who conceived of it as the medium through which the heavenly bodies move.\n\nThe concept of a Universe filled with a luminiferous aether remained in vogue among some scientists until the early 20th century. This form of aether was viewed as the medium through which light could propagate. In 1887, the Michelson–Morley experiment tried to detect the Earth's motion through this medium by looking for changes in the speed of light depending on the direction of the planet's motion. However, the null result indicated something was wrong with the concept. The idea of the luminiferous aether was then abandoned. It was replaced by Albert Einstein's theory of special relativity, which holds that the speed of light in a vacuum is a fixed constant, independent of the observer's motion or frame of reference.\n\nThe first professional astronomer to support the concept of an infinite Universe was the Englishman Thomas Digges in 1576. But the scale of the Universe remained unknown until the first successful measurement of the distance to a nearby star in 1838 by the German astronomer Friedrich Bessel. He showed that the star 61 Cygni had a parallax of just 0.31 arcseconds (compared to the modern value of 0.287″). This corresponds to a distance of over 10 light years. In 1917, Heber Curtis noted that novae in spiral nebulae were, on average, 10 magnitudes fainter than galactic novae, suggesting that the former are 100 times further away. The distance to the Andromeda Galaxy was determined in 1923 by American astronomer Edwin Hubble by measuring the brightness of cepheid variables in that galaxy, a new technique discovered by Henrietta Leavitt. This established that the Andromeda galaxy, and by extension all galaxies, lay well outside the Milky Way.\n\nThe earliest known estimate of the temperature of outer space was by the Swiss physicist Charles É. Guillaume in 1896. Using the estimated radiation of the background stars, he concluded that space must be heated to a temperature of 5–6 K. British physicist Arthur Eddington made a similar calculation to derive a temperature of 3.18 K in 1926. German physicist Erich Regener used the total measured energy of cosmic rays to estimate an intergalactic temperature of 2.8 K in 1933.\n\nThe modern concept of outer space is based on the \"Big Bang\" cosmology, first proposed in 1931 by the Belgian physicist Georges Lemaître. This theory holds that the universe originated from a very dense form that has since undergone continuous expansion. The background energy released during the initial expansion has steadily decreased in density, leading to a 1948 prediction by American physicists Ralph Alpher and Robert Herman of a temperature of 5 K for the temperature of space.\n\nThe term \"outward space\" was used in 1842 by the English poet Lady Emmeline Stuart-Wortley in her poem \"The Maiden of Moscow\". The expression \"outer space\" was used as an astronomical term by Alexander von Humboldt in 1845. It was later popularized in the writings of H. G. Wells in 1901. The shorter term \"space\" is older, first used to mean the region beyond Earth's sky in John Milton's \"Paradise Lost\" in 1667.\n\nAccording to the Big Bang theory, the very early Universe was an extremely hot and dense state about 13.8 billion years ago which rapidly expanded. About 380,000 years later the Universe had cooled sufficiently to allow protons and electrons to combine and form hydrogen—the so-called recombination epoch. When this happened, matter and energy became decoupled, allowing photons to travel freely through the continually expanding space. Matter that remained following the initial expansion has since undergone gravitational collapse to create stars, galaxies and other astronomical objects, leaving behind a deep vacuum that forms what is now called outer space. As light has a finite velocity, this theory also constrains the size of the directly observable universe. This leaves open the question as to whether the Universe is finite or infinite.\n\nThe present day shape of the universe has been determined from measurements of the cosmic microwave background using satellites like the Wilkinson Microwave Anisotropy Probe. These observations indicate that the spatial geometry of the observable universe is \"flat\", meaning that photons on parallel paths at one point remain parallel as they travel through space to the limit of the observable universe, except for local gravity. The flat Universe, combined with the measured mass density of the Universe and the accelerating expansion of the Universe, indicates that space has a non-zero vacuum energy, which is called dark energy.\n\nEstimates put the average energy density of the present day Universe at the equivalent of 5.9 protons per cubic meter, including dark energy, dark matter, and baryonic matter (ordinary matter composed of atoms). The atoms account for only 4.6% of the total energy density, or a density of one proton per four cubic meters. The density of the Universe, however, is clearly not uniform; it ranges from relatively high density in galaxies—including very high density in structures within galaxies, such as planets, stars, and black holes—to conditions in vast voids that have much lower density, at least in terms of visible matter. Unlike matter and dark matter, dark energy seems not to be concentrated in galaxies: although dark energy may account for a majority of the mass-energy in the Universe, dark energy's influence is 5 orders of magnitude smaller than the influence of gravity from matter and dark matter within the Milky Way.\n\nOuter space is the closest known approximation to a perfect vacuum. It has effectively no friction, allowing stars, planets, and moons to move freely along their ideal orbits, following the initial formation stage. However, even the deep vacuum of intergalactic space is not devoid of matter, as it contains a few hydrogen atoms per cubic meter. By comparison, the air humans breathe contains about 10 molecules per cubic meter. The low density of matter in outer space means that electromagnetic radiation can travel great distances without being scattered: the mean free path of a photon in intergalactic space is about 10 km, or 10 billion light years. In spite of this, extinction, which is the absorption and scattering of photons by dust and gas, is an important factor in galactic and intergalactic astronomy.\n\nStars, planets, and moons retain their atmospheres by gravitational attraction. Atmospheres have no clearly delineated upper boundary: the density of atmospheric gas gradually decreases with distance from the object until it becomes indistinguishable from outer space. The Earth's atmospheric pressure drops to about Pa at of altitude, compared to 100,000 Pa for the International Union of Pure and Applied Chemistry (IUPAC) definition of standard pressure. Above this altitude, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar wind. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather.\n\nThe temperature of outer space is measured in terms of the kinetic activity of the gas, as it is on Earth. However, the radiation of outer space has a different temperature than the kinetic temperature of the gas, meaning that the gas and radiation are not in thermodynamic equilibrium. All of the observable universe is filled with photons that were created during the Big Bang, which is known as the cosmic microwave background radiation (CMB). (There is quite likely a correspondingly large number of neutrinos called the cosmic neutrino background.) The current black body temperature of the background radiation is about . The gas temperatures in outer space are always at least the temperature of the CMB but can be much higher. For example, the corona of the Sun reaches temperatures over 1.2–2.6 million K.\n\nMagnetic fields have been detected in the space around just about every class of celestial object. Star formation in spiral galaxies can generate small-scale dynamos, creating turbulent magnetic field strengths of around 5–10 μG. The Davis–Greenstein effect causes elongated dust grains to align themselves with a galaxy's magnetic field, resulting in weak optical polarization. This has been used to show ordered magnetic fields exist in several nearby galaxies. Magneto-hydrodynamic processes in active elliptical galaxies produce their characteristic jets and radio lobes. Non-thermal radio sources have been detected even among the most distant, high-z sources, indicating the presence of magnetic fields.\n\nOutside a protective atmosphere and magnetic field, there are few obstacles to the passage through space of energetic subatomic particles known as cosmic rays. These particles have energies ranging from about 10 eV up to an extreme 10 eV of ultra-high-energy cosmic rays. The peak flux of cosmic rays occurs at energies of about 10 eV, with approximately 87% protons, 12% helium nuclei and 1% heavier nuclei. In the high energy range, the flux of electrons is only about 1% of that of protons. Cosmic rays can damage electronic components and pose a health threat to space travelers. According to astronauts, like Don Pettit, space has a burned/metallic odor that clings to their suits and equipment, similar to the scent of an arc welding torch.\n\nDespite the harsh environment, several life forms have been found that can withstand extreme space conditions for extended periods. Species of lichen carried on the ESA BIOPAN facility survived exposure for ten days in 2007. Seeds of \"Arabidopsis thaliana\" and \"Nicotiana tabacum\" germinated after being exposed to space for 1.5 years. A strain of \"bacillus subtilis\" has survived 559 days when exposed to low-Earth orbit or a simulated martian environment. The lithopanspermia hypothesis suggests that rocks ejected into outer space from life-harboring planets may successfully transport life forms to another habitable world. A conjecture is that just such a scenario occurred early in the history of the Solar System, with potentially microorganism-bearing rocks being exchanged between Venus, Earth, and Mars.\n\nEven at relatively low altitudes in the Earth's atmosphere, conditions are hostile to the human body. The altitude where atmospheric pressure matches the vapor pressure of water at the temperature of the human body is called the Armstrong line, named after American physician Harry G. Armstrong. It is located at an altitude of around . At or above the Armstrong line, fluids in the throat and lungs boil away. More specifically, exposed bodily liquids such as saliva, tears, and liquids in the lungs boil away. Hence, at this altitude, human survival requires a pressure suit, or a pressurized capsule.\n\nOnce in space, sudden exposure of unprotected humans to very low pressure, such as during a rapid decompression, can cause pulmonary barotrauma—a rupture of the lungs, due to the large pressure differential between inside and outside the chest. Even if the subject's airway is fully open, the flow of air through the windpipe may be too slow to prevent the rupture. Rapid decompression can rupture eardrums and sinuses, bruising and blood seep can occur in soft tissues, and shock can cause an increase in oxygen consumption that leads to hypoxia.\n\nAs a consequence of rapid decompression, oxygen dissolved in the blood empties into the lungs to try to equalize the partial pressure gradient. Once the deoxygenated blood arrives at the brain, humans lose consciousness after a few seconds and die of hypoxia within minutes. Blood and other body fluids boil when the pressure drops below 6.3 kPa, and this condition is called ebullism. The steam may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Ebullism is slowed by the pressure containment of blood vessels, so some blood remains liquid. Swelling and ebullism can be reduced by containment in a pressure suit. The Crew Altitude Protection Suit (CAPS), a fitted elastic garment designed in the 1960s for astronauts, prevents ebullism at pressures as low as 2 kPa. Supplemental oxygen is needed at to provide enough oxygen for breathing and to prevent water loss, while above pressure suits are essential to prevent ebullism. Most space suits use around 30–39 kPa of pure oxygen, about the same as on the Earth's surface. This pressure is high enough to prevent ebullism, but evaporation of nitrogen dissolved in the blood could still cause decompression sickness and gas embolisms if not managed.\n\nHumans evolved for life in Earth gravity, and exposure to weightlessness has been shown to have deleterious effects on human health. Initially, more than 50% of astronauts experience space motion sickness. This can cause nausea and vomiting, vertigo, headaches, lethargy, and overall malaise. The duration of space sickness varies, but it typically lasts for 1–3 days, after which the body adjusts to the new environment. Longer-term exposure to weightlessness results in muscle atrophy and deterioration of the skeleton, or spaceflight osteopenia. These effects can be minimized through a regimen of exercise. Other effects include fluid redistribution, slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, nasal congestion, sleep disturbance, and puffiness of the face.\n\nFor long-duration space travel, radiation can pose an acute health hazard.\nExposure to high-energy, ionizing cosmic rays can result in fatigue, nausea, vomiting, as well as damage to the immune system and changes to the white blood cell count. Over longer durations, symptoms include an increased risk of cancer, plus damage to the eyes, nervous system, lungs and the gastrointestinal tract. On a round-trip Mars mission lasting three years, a large fraction of the cells in an astronaut's body would be traversed and potentially damaged by high energy nuclei. The energy of such particles is significantly diminished by the shielding provided by the walls of a spacecraft and can be further diminished by water containers and other barriers. However, the impact of the cosmic rays upon the shielding produces additional radiation that can affect the crew. Further research is needed to assess the radiation hazards and determine suitable countermeasures.\n\nThere is no clear boundary between Earth's atmosphere and space, as the density of the atmosphere gradually decreases as the altitude increases. There are several standard boundary designations, namely:\n\nIn 2009, scientists reported detailed measurements with a Supra-Thermal Ion Imager (an instrument that measures the direction and speed of ions), which allowed them to establish a boundary at above Earth. The boundary represents the midpoint of a gradual transition over tens of kilometers from the relatively gentle winds of the Earth's atmosphere to the more violent flows of charged particles in space, which can reach speeds well over .\n\nThe Outer Space Treaty provides the basic framework for international space law. It covers the legal use of outer space by nation states, and includes in its definition of \"outer space\" the Moon and other celestial bodies. The treaty states that outer space is free for all nation states to explore and is not subject to claims of national sovereignty. It also prohibits the deployment of nuclear weapons in outer space. The treaty was passed by the United Nations General Assembly in 1963 and signed in 1967 by the USSR, the United States of America and the United Kingdom. As of 2017, 105 state parties have either ratified or acceded to the treaty. An additional 25 states signed the treaty, without ratifying it..\n\nSince 1958, outer space has been the subject of multiple United Nations resolutions. Of these, more than 50 have been concerning the international co-operation in the peaceful uses of outer space and preventing an arms race in space. Four additional space law treaties have been negotiated and drafted by the UN's Committee on the Peaceful Uses of Outer Space. Still, there remains no legal prohibition against deploying conventional weapons in space, and anti-satellite weapons have been successfully tested by the US, USSR and China. The 1979 Moon Treaty turned the jurisdiction of all heavenly bodies (including the orbits around such bodies) over to the international community. However, this treaty has not been ratified by any nation that currently practices manned spaceflight.\n\nIn 1976, eight equatorial states (Ecuador, Colombia, Brazil, Congo, Zaire, Uganda, Kenya, and Indonesia) met in Bogotá, Colombia. With their \"Declaration of the First Meeting of Equatorial Countries\", or \"the Bogotá Declaration\", they claimed control of the segment of the geosynchronous orbital path corresponding to each country. These claims are not internationally accepted.\n\nA spacecraft enters orbit when its centripetal acceleration due to gravity is less than or equal to the centrifugal acceleration due to the horizontal component of its velocity. For a low Earth orbit, this velocity is about ; by contrast, the fastest manned airplane speed ever achieved (excluding speeds achieved by deorbiting spacecraft) was in 1967 by the North American X-15.\n\nTo achieve an orbit, a spacecraft must travel faster than a sub-orbital spaceflight. The energy required to reach Earth orbital velocity at an altitude of is about 36 MJ/kg, which is six times the energy needed merely to climb to the corresponding altitude. Spacecraft with a perigee below about are subject to drag from the Earth's atmosphere, which decreases the orbital altitude. The rate of orbital decay depends on the satellite's cross-sectional area and mass, as well as variations in the air density of the upper atmosphere. Below about , decay becomes more rapid with lifetimes measured in days. Once a satellite descends to , it has only hours before it vaporizes in the atmosphere. The escape velocity required to pull free of Earth's gravitational field altogether and move into interplanetary space is about .\n\nSpace is a partial vacuum: its different regions are defined by the various atmospheres and \"winds\" that dominate within them, and extend to the point at which those winds give way to those beyond. Geospace extends from Earth's atmosphere to the outer reaches of Earth's magnetic field, whereupon it gives way to the solar wind of interplanetary space. Interplanetary space extends to the heliopause, whereupon the solar wind gives way to the winds of the interstellar medium. Interstellar space then continues to the edges of the galaxy, where it fades into the intergalactic void.\n\nGeospace is the region of outer space near Earth, including the upper atmosphere and magnetosphere. The Van Allen radiation belts lie within the geospace. The outer boundary of geospace is the magnetopause, which forms an interface between the Earth's magnetosphere and the solar wind. The inner boundary is the ionosphere. The variable space-weather conditions of geospace are affected by the behavior of the Sun and the solar wind; the subject of geospace is interlinked with heliophysics -- the study of the Sun and its impact on the planets of the Solar System.\n\nThe day-side magnetopause is compressed by solar-wind pressure -- the subsolar distance from the center of the Earth is typically 10 Earth radii. On the night side, the solar wind stretches the magnetosphere to form a magnetotail that sometimes extends out to more than 100–200 Earth radii. For roughly four days of each month, the lunar surface is shielded from the solar wind as the Moon passes through the magnetotail.\n\nGeospace is populated by electrically charged particles at very low densities, the motions of which are controlled by the Earth's magnetic field. These plasmas form a medium from which storm-like disturbances powered by the solar wind can drive electrical currents into the Earth's upper atmosphere. Geomagnetic storms can disturb two regions of geospace, the radiation belts and the ionosphere. These storms increase fluxes of energetic electrons that can permanently damage satellite electronics, interfering with shortwave radio communication and GPS location and timing. Magnetic storms can also be a hazard to astronauts, even in low Earth orbit. They also create aurorae seen at high latitudes in an oval surrounding the geomagnetic poles.\n\nAlthough it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. This region contains material left over from previous manned and unmanned launches that are a potential hazard to spacecraft. Some of this debris re-enters Earth's atmosphere periodically.\n\nEarth's gravity keeps the Moon in orbit at an average distance of . The region outside Earth's atmosphere and extending out to just beyond the Moon's orbit, including the Lagrangian points, is sometimes referred to as cislunar space.\n\nThe region of space where Earth's gravity remains dominant against gravitational perturbations from the Sun is called the Hill sphere. This extends well out into translunar space to a distance of roughly 1% of the mean distance from Earth to the Sun, or .\n\nDeep space has different definitions as to where it starts. It has been defined by the United States government and others as any region beyond cislunar space. The International Telecommunication Union responsible for radio communication (including satellites) defines the beginning of deep space at about 5 times that distance ().\n\nInterplanetary space is defined by the solar wind, a continuous stream of charged particles emanating from the Sun that creates a very tenuous atmosphere (the heliosphere) for billions of kilometers into space. This wind has a particle density of 5–10 protons/cm and is moving at a velocity of . Interplanetary space extends out to the heliopause where the influence of the galactic environment starts to dominate over the magnetic field and particle flux from the Sun. The distance and strength of the heliopause varies depending on the activity level of the solar wind.\n\nThe volume of interplanetary space is a nearly total vacuum, with a mean free path of about one astronomical unit at the orbital distance of the Earth. However, this space is not completely empty, and is sparsely filled with cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also gas, plasma and dust, small meteors, and several dozen types of organic molecules discovered to date by microwave spectroscopy. A cloud of interplanetary dust is visible at night as a faint band called the zodiacal light.\n\nInterplanetary space contains the magnetic field generated by the Sun. There are also magnetospheres generated by planets such as Jupiter, Saturn, Mercury and the Earth that have their own magnetic fields. These are shaped by the influence of the solar wind into the approximation of a teardrop shape, with the long tail extending outward behind the planet. These magnetic fields can trap particles from the solar wind and other sources, creating belts of charged particles such as the Van Allen radiation belts. Planets without magnetic fields, such as Mars, have their atmospheres gradually eroded by the solar wind.\n\nInterstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 10 particles per m, but cold molecular clouds can hold 10–10 per m.\n\nA number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.\n\nThe local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 10–10 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.\n\nWhen stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).\n\nIntergalactic space is the physical space between galaxies. Studies of the large scale distribution of galaxies show that the Universe has a foam-like structure, with clusters and groups of galaxies lying along filaments that occupy about a tenth of the total space. The remainder forms huge voids that are mostly empty of galaxies. Typically, a void spans a distance of (10–40) \"h\" Mpc, where \"h\" is the Hubble constant in units of .\n\nSurrounding and stretching between galaxies, there is a rarefied plasma that is organized in a galactic filamentary structure. This material is called the intergalactic medium (IGM). The density of the IGM is 5–200 times the average density of the Universe. It consists mostly of ionized hydrogen; i.e. a plasma consisting of equal numbers of electrons and protons. As gas falls into the intergalactic medium from the voids, it heats up to temperatures of 10 K to 10 K, which is high enough so that collisions between atoms have enough energy to cause the bound electrons to escape from the hydrogen nuclei; this is why the IGM is ionized. At these temperatures, it is called the warm–hot intergalactic medium (WHIM). (Although the plasma is very hot by terrestrial standards, 10 K is often called \"warm\" in astrophysics.) Computer simulations and observations indicate that up to half of the atomic matter in the Universe might exist in this warm–hot, rarefied state. When gas falls from the filamentary structures of the WHIM into the galaxy clusters at the intersections of the cosmic filaments, it can heat up even more, reaching temperatures of 10 K and above in the so-called intracluster medium.\n\nFor the majority of human history, space was explored by observations made from the Earth's surface—initially with the unaided eye and then with the telescope. Prior to the advent of reliable rocket technology, the closest that humans had come to reaching outer space was through the use of balloon flights. In 1935, the U.S. \"Explorer II\" manned balloon flight had reached an altitude of . This was greatly exceeded in 1942 when the third launch of the German A-4 rocket climbed to an altitude of about . In 1957, the unmanned satellite \"Sputnik 1\" was launched by a Russian R-7 rocket, achieving Earth orbit at an altitude of . This was followed by the first human spaceflight in 1961, when Yuri Gagarin was sent into orbit on Vostok 1. The first humans to escape low-Earth orbit were Frank Borman, Jim Lovell and William Anders in 1968 on board the U.S. Apollo 8, which achieved lunar orbit and reached a maximum distance of from the Earth.\n\nThe first spacecraft to reach escape velocity was the Soviet Luna 1, which performed a fly-by of the Moon in 1959. In 1961, Venera 1 became the first planetary probe. It revealed the presence of the solar wind and performed the first fly-by of Venus, although contact was lost before reaching Venus. The first successful planetary mission was the 1962 fly-by of Venus by Mariner 2. The first fly-by of Mars was by Mariner 4 in 1964. Since that time, unmanned spacecraft have successfully examined each of the Solar System's planets, as well their moons and many minor planets and comets. They remain a fundamental tool for the exploration of outer space, as well as observation of the Earth. In August 2012, Voyager 1 became the first man-made object to leave the Solar System and enter interstellar space.\n\nThe absence of air makes outer space an ideal location for astronomy at all wavelengths of the electromagnetic spectrum. This is evidenced by the spectacular pictures sent back by the Hubble Space Telescope, allowing light from more than 13 billion years ago—almost to the time of the Big Bang—to be observed. However, not every location in space is ideal for a telescope. The interplanetary zodiacal dust emits a diffuse near-infrared radiation that can mask the emission of faint sources such as extrasolar planets. Moving an infrared telescope out past the dust increases its effectiveness. Likewise, a site like the Daedalus crater on the far side of the Moon could shield a radio telescope from the radio frequency interference that hampers Earth-based observations.\n\nUnmanned spacecraft in Earth orbit are an essential technology of modern civilization. They allow direct monitoring of weather conditions, relay long-range communications like television, provide a means of precise navigation, and allow remote sensing of the Earth. The latter role serves a wide variety of purposes, including tracking soil moisture for agriculture, prediction of water outflow from seasonal snow packs, detection of diseases in plants and trees, and surveillance of military activities.\n\nThe deep vacuum of space could make it an attractive environment for certain industrial processes, such as those requiring ultraclean surfaces. However, like asteroid mining, space manufacturing requires significant investment with little prospect of immediate return. An important factor in the total expense is the high cost of placing mass into Earth orbit: $– per kg in inflation-adjusted dollars, according to a 2006 estimate. Proposed concepts for addressing this issue include non-rocket spacelaunch, momentum exchange tethers, and space elevators.\n\nInterstellar travel for a human crew remains at present only a theoretical possibility. The distances to the nearest stars will require new technological developments and the ability to safely sustain crews for journeys lasting several decades. For example, the Daedalus Project study, which proposed a spacecraft powered by the fusion of Deuterium and He, would require 36 years to reach the nearby Alpha Centauri system. Other proposed interstellar propulsion systems include light sails, ramjets, and beam-powered propulsion. More advanced propulsion systems could use antimatter as a fuel, potentially reaching relativistic velocities.\n\n\n"}
{"id": "23603203", "url": "https://en.wikipedia.org/wiki?curid=23603203", "title": "Pacific white line", "text": "Pacific white line\n\nThe Pacific White Line is a periodic but random natural feature in the Pacific Ocean. It is a huge collection of fish, foam and algae that usually occurs between January and August. When conditions are right it can be seen from space.\n\nThe line can clearly be seen from space when at its greatest, being around wide and up to several hundred kilometres long. Although only covering 0.1% of the surface, it provides 50% of all the fish caught in the Pacific Ocean.\n\nThe white line is formed because the currents bring fresh, cool and nutritious water loaded with minerals from the depths of the ocean to the surface. When this occurs, it moves west along the surface, with a 70-metre zone of cool water and a 40-metre zone of warm water, that has been subducted under the cold water leading to a lot of turbulence quite often. When these conditions are present tiny algae called Rhizosolenia, part of the diatom family, begin to flourish. No thicker than two or three times the width of a human hair, they pile up ahead of the line as it moves west. They breed once a day to form baby diatomettes and this creates a very localized food source, that all the fish are attracted to. The white color of the line is caused by the huge collection of diatoms, the cooler water and the whitecaps, formed by the turbulence in the water.\n\nEven since 1926, records of the white line and associated features have been taking place. In that year an unnamed ocean explorer described the white line and its collection of diatoms as having \"the consistency of soup\" due to the abundance of ocean fauna. In these early days scientists were able to discover the time at which the line formed (between January and August, as mentioned earlier).\n\nDuring a 10-year international study called the Joint Global Ocean Flux Study the white line was photographed for the first time, as the scientists were studying how currents, chemicals, ocean color, and temperature affected the overall ocean and it was during this study that the line was photographed and the scale of the event was discovered, by the Space Shuttle.\n\nAlso, scientists have measured the concentrations of these diatoms by lasers in low flying aircraft, during normal conditions and during the time in which the white line is present, and it has been found that the concentrations of these algae is up to 100 times greater in the white line period.\n\nThese conditions are only rarely present and once one of the needed conditions fails, the Pacific returns to its normal state. Often, the line collapses because of the currents returning to normal and the cool waters return to the depths, causing the diatoms to die out (they need the difference in ocean temperature to flourish), the turbulence to cease and the fish to move elsewhere.\n"}
{"id": "39780828", "url": "https://en.wikipedia.org/wiki?curid=39780828", "title": "Presidential Climate Action Plan", "text": "Presidential Climate Action Plan\n\nPresident Barack Obama’s Climate Action Plan proposed a reduction in carbon dioxide emissions. It included preserving forests, encouraging the use of alternate fuels, and increased study of climate change. The plan was first prepared in 2008 and had then been updated every two years.\n\nPresident Obama's last Climate Action Plan, issued in June 2013, included regulations to industry with the ultimate goal of cutting domestic carbon emission, preparing the U.S. for impending effects of climate change, and working internationally to address climate change. Among the regulations outlined in the plan were initiatives to increase natural disaster preparedness, create and improve existing hospitals, and modernize infrastructure to better withstand extreme weather. \n\nThe plan would have support conservation of land and water resources and developing actionable climate science, and encouraged other countries to take action to address climate change, including reducing deforestation and lowering subsidies that increase use of fossil fuels. The plan specifically mentioned methane, building efficiency, wind, solar and hydroelectricity.\n\nWhite House staff members who were directly tasked with implementation of the plan included Heather Zichal and Michelle Patron.\n\nOn the first day of the presidency of Donald Trump, the White House website announced that Obama's Climate Action Plan would be eliminated, stating it is 'harmful and unnecessary'. In March 2017, Trump signed an executive order to officially nullify Obama's Clean Power Plan in an effort, it said, of reviving the coal industry.\n\n\n\n"}
{"id": "307155", "url": "https://en.wikipedia.org/wiki?curid=307155", "title": "Primitive equations", "text": "Primitive equations\n\nThe primitive equations are a set of nonlinear differential equations that are used to approximate global atmospheric flow and are used in most atmospheric models. They consist of three main sets of balance equations:\n\n\nThe primitive equations may be linearized to yield Laplace's tidal equations, an eigenvalue problem from which the analytical solution to the latitudinal structure of the flow may be determined.\n\nIn general, nearly all forms of the primitive equations relate the five variables \"u\", \"v\", ω, \"T\", \"W\", and their evolution over space and time.\n\nThe equations were first written down by Vilhelm Bjerknes.\n\n\nForces that cause atmospheric motion include the pressure gradient force, gravity, and viscous friction. Together, they create the forces that accelerate our atmosphere.\n\nThe pressure gradient force causes an acceleration forcing air from regions of high pressure to regions of low pressure. Mathematically, this can be written as:\n\nThe gravitational force accelerates objects at approximately 9.81 m/s directly towards the center of the Earth.\n\nThe force due to viscous friction can be approximated as:\n\nUsing Newton's second law, these forces (referenced in the equations above as the accelerations due to these forces) may be summed to produce an equation of motion that describes this system. This equation can be written in the form:\n\nTherefore, to complete the system of equations and obtain 6 equations and 6 variables:\n\nThe precise form of the primitive equations depends on the vertical coordinate system chosen, such as pressure coordinates, log pressure coordinates, or sigma coordinates. Furthermore, the velocity, temperature, and geopotential variables may be decomposed into mean and perturbation components using Reynolds decomposition.\n\nIn this form pressure is selected as the vertical coordinate and the horizontal coordinates are written for the Cartesian tangential plane (i.e. a plane tangent to some point on the surface of the Earth). This form does not take the curvature of the Earth into account, but is useful for visualizing some of the physical processes involved in formulating the equations due to its relative simplicity.\n\nNote that the capital derivatives are the material derivatives.\n\n\n\n\n\nWhen a statement of the conservation of water vapor substance is included, these six equations form the basis for any numerical weather prediction scheme.\n\nAccording to the \"National Weather Service Handbook No. 1 – Facsimile Products\", the primitive equations can be simplified into the following equations:\n\n\n\n\nThe first term is equal to the change in temperature due to incoming solar radiation and outgoing longwave radiation, which changes with time throughout the day. The second, third, and fourth terms are due to advection. Additionally, the variable \"T\" with subscript is the change in temperature on that plane. Each \"T\" is actually different and related to its respective plane. This is divided by the distance between grid points to get the change in temperature with the change in distance. When multiplied by the wind velocity on that plane, the units kelvins per meter and meters per second give kelvins per second. The sum of all the changes in temperature due to motions in the \"x\", \"y\", and \"z\" directions give the total change in temperature with time.\n\n\nThis equation and notation works in much the same way as the temperature equation. This equation describes the motion of water from one place to another at a point without taking into account water that changes form. Inside a given system, the total change in water with time is zero. However, concentrations are allowed to move with the wind.\n\n\nThese simplifications make it much easier to understand what is happening in the model. Things like the temperature (potential temperature), precipitable water, and to an extent the pressure thickness simply move from one spot on the grid to another with the wind. The wind is forecast slightly differently. It uses geopotential, specific heat, the exner function \"π\", and change in sigma coordinate.\n\nThe analytic solution to the linearized primitive equations involves a sinusoidal oscillation in time and longitude, modulated by coefficients related to height and latitude.\n\nwhere \"s\" and formula_38 are the zonal wavenumber and angular frequency, respectively. The solution represents atmospheric waves and tides.\n\nWhen the coefficients are separated into their height and latitude components, the height dependence takes the form of propagating or evanescent waves (depending on conditions), while the latitude dependence is given by the Hough functions.\n\nThis analytic solution is only possible when the primitive equations are linearized and simplified. Unfortunately many of these simplifications (i.e. no dissipation, isothermal atmosphere) do not correspond to conditions in the actual atmosphere. As a result, a numerical solution which takes these factors into account is often calculated using general circulation models and climate models.\n\n\n\nNational Weather Service – NCSU \nCollaborative Research and Training Site, Review of the Primitive Equations.\n"}
{"id": "13676801", "url": "https://en.wikipedia.org/wiki?curid=13676801", "title": "Regional Forest Agreement", "text": "Regional Forest Agreement\n\nThe Regional Forest Agreements are 20 year plans for the conservation and sustainable management of Australia's native forests, and are intended to provide certainty to commercial forestry operations while protecting environmental values. The 10 RFA's were progressively signed between 1997 and 2001. The RFA process grew out of the 1992 National Forest Policy Statement.\n\nThe Agreements relies on a mix of community and industry consultation combined with scientific research. While the Agreements are supported by forestry industry, they are widely criticized by environmentalist groups.\n\nIn Tasmania, a forestry operation that is undertaken in accordance with an RFA is not required obtain environmental approvals otherwise required by the Environment Protection and Biodiversity Conservation Act, nor to protect rare or threatened species listed in the CAR Reserve System. A conclusive presumption was added to the agreement in 2007 which states \"The Parties agree that the CAR Reserve System, established in accordance with this Agreement, and the application of management strategies and management prescriptions developed under Tasmania's Forest Management Systems, protect rare and threatened fauna and flora species and Forest Communities\". That is, the threatened species have simply been declared to be protected without regard for actual circumstances.\n\nThere are currently 10 regions covered by RFAs:\n\n\n"}
{"id": "26686432", "url": "https://en.wikipedia.org/wiki?curid=26686432", "title": "Substorm", "text": "Substorm\n\nA substorm, sometimes referred to as a magnetospheric substorm or an auroral substorm, is a brief disturbance in the Earth's magnetosphere that causes energy to be released from the \"tail\" of the magnetosphere and injected into the high latitude ionosphere. Visually, a substorm is seen as a sudden brightening and increased movement of auroral arcs. Substorms were first described in qualitative terms by Kristian Birkeland which he called polar elementary storms. Sydney Chapman used the term substorm about 1960 which is now the standard term. The morphology of aurora during a substorm was first described by Syun-Ichi Akasofu in 1964 using data collected during the International Geophysical Year.\n\nSubstorms are distinct from geomagnetic storms in that the latter take place over a period of several days, are observable from anywhere on Earth, inject a large number of ions into the outer radiation belt, and occur once or twice a month during the maximum of the solar cycle and a few times a year during solar minimum. Substorms, on the other hand, take place over a period of a few hours, are observable primarily at the polar regions, do not inject many particles into the radiation belt, and are relatively frequent — often occurring only a few hours apart from each other. Substorms can be more intense and occur more frequently during a geomagnetic storm when one substorm may start before the previous one has completed. The source of the magnetic disturbances observed at the Earth's surface during geomagnetic storms is the ring current, whereas the sources of magnetic disturbances observed on the ground during substorms are electric currents in the ionosphere at high latitudes.\n\nSubstorms can cause magnetic field disturbances in the auroral zones up to a magnitude of 1000 nT, roughly 2% of the total magnetic field strength in that region. The disturbance is much greater in space, as some geosynchronous satellites have registered the magnetic field dropping to half of its normal strength during a substorm. The most visible indication of a substorm is an increase in the intensity and size of polar auroras. Substorms can be divided into three phases: the growth phase, the expansion phase, and the recovery phase.\n\nIn 2012, the THEMIS satellite mission observed the dynamics of rapidly developing substorms, confirming the existence of giant magnetic ropes and witnessed small explosions in the outskirts of Earth's magnetic field.\n"}
{"id": "27680", "url": "https://en.wikipedia.org/wiki?curid=27680", "title": "Supernova", "text": "Supernova\n\nA supernova ( plural: supernovae or supernovas, abbreviations: SN and SNe) is an event that occurs upon the death of certain types of stars.\n\nSupernovae are more energetic than novae. In Latin, \"nova\" means \"new\", referring astronomically to what appears to be a temporary new bright star. Adding the prefix \"super-\" distinguishes supernovae from ordinary novae, which are far less luminous. The word \"supernova\" was coined by Walter Baade and Fritz Zwicky in 1931.\n\nOnly three, Milky Way naked-eye supernova events have been observed during the last thousand years, though many have been seen in other galaxies using telescopes. The most recent directly observed supernova in the Milky Way was Kepler's Supernova in 1604, but two more recent supernova remnants have also been found. Statistical observations of supernovae in other galaxies suggest they occur on average about three times every century in the Milky Way, and that any galactic supernova would almost certainly be observable with modern astronomical telescopes.\n\nSupernovae may expel much, if not all, of the material away from a star at velocities up to or 10% of the speed of light. This drives an expanding and fast-moving shock wave into the surrounding interstellar medium, and in turn, sweeping up an expanding shell of gas and dust, which is observed as a supernova remnant. Supernovae create, fuse and eject the bulk of the chemical elements produced by nucleosynthesis. Supernovae play a significant role in enriching the interstellar medium with the heavier atomic mass chemical elements. Furthermore, the expanding shock waves from supernovae can trigger the formation of new stars. Supernova remnants are expected to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production was found only in a few of them so far. They are also potentially strong galactic sources of gravitational waves.\n\nTheoretical studies indicate that most supernovae are triggered by one of two basic mechanisms: the sudden re-ignition of nuclear fusion in a degenerate star or the sudden gravitational collapse of a massive star's core. In the first instance, a degenerate white dwarf may accumulate sufficient material from a binary companion, either through accretion or via a merger, to raise its core temperature enough to trigger runaway nuclear fusion, completely disrupting the star. In the second case, the core of a massive star may undergo sudden gravitational collapse, releasing gravitational potential energy as a supernova. While some observed supernovae are more complex than these two simplified theories, the astrophysical collapse mechanics have been established and accepted by most astronomers for some time.\n\nDue to the wide range of astrophysical consequences of these events, astronomers now deem supernova research, across the fields of stellar and galactic evolution, as an especially important area for investigation.\n\nThe earliest recorded supernova, SN 185, was viewed by Chinese astronomers in 185 AD. The brightest recorded supernova was SN 1006, which occurred in 1006 AD and was described by observers across China, Japan, Iraq, Egypt, and Europe. The widely observed supernova SN 1054 produced the Crab Nebula. Supernovae SN 1572 and SN 1604, the latest to be observed with the naked eye in the Milky Way galaxy, had notable effects on the development of astronomy in Europe because they were used to argue against the Aristotelian idea that the universe beyond the Moon and planets was static and unchanging. Johannes Kepler began observing SN 1604 at its peak on October 17, 1604, and continued to make estimates of its brightness until it faded from naked eye view a year later. It was the second supernova to be observed in a generation (after SN 1572 seen by Tycho Brahe in Cassiopeia).\n\nThere is some evidence that the youngest galactic supernova, G1.9+0.3, occurred in the late 19th century, considerably more recently than Cassiopeia A from around 1680. Neither supernova was noted at the time. In the case of G1.9+0.3, high extinction along the plane of the galaxy could have dimmed the event sufficiently to go unnoticed. The situation for Cassiopeia A is less clear. Infrared light echos have been detected showing that it was a type IIb supernova and was not in a region of especially high extinction.\n\nBefore the development of the telescope, only five supernovae were seen in the last millennium. Compared to a star's entire history, the visual appearance of a galactic supernova is very brief, perhaps spanning several months, so that the chances of observing one is roughly once in a lifetime. Only a tiny fraction of the 100 billion stars in a typical galaxy have the capacity to become a supernova, restricted to either those having large mass or extraordinarily rare kinds of binary stars containing white dwarfs.\n\nHowever, observation and discovery of extragalactic supernovae are now far more common. The first such observation was of SN 1885A in the Andromeda galaxy. Today, amateur and professional astronomers are finding several hundred every year, some when near maximum brightness, others on old astronomical photographs or plates. American astronomers Rudolph Minkowski and Fritz Zwicky developed the modern supernova classification scheme beginning in 1941. During the 1960s, astronomers found that the maximum intensities of supernovae could be used as standard candles, hence indicators of astronomical distances. Some of the most distant supernovae observed in 2003, appeared dimmer than expected. This supports the view that the expansion of the universe is accelerating. Techniques were developed for reconstructing supernovae events that have no written records of being observed. The date of the Cassiopeia A supernova event was determined from light echoes off nebulae, while the age of supernova remnant RX J0852.0-4622 was estimated from temperature measurements and the gamma ray emissions from the radioactive decay of titanium-44.\n\nThe most luminous supernova ever recorded is ASASSN-15lh. It was first detected in June 2015 and peaked at , which is twice the bolometric luminosity of any other known supernova. However, the nature of this supernova continues to be debated and several alternative explanations have been suggested, e.g. tidal disruption of a star by a black hole.\n\nAmong the earliest detected since time of detonation, and for which the earliest spectra have been obtained (beginning at 6 hours after the actual explosion), is the Type II SN 2013fs (iPTF13dqy) which was recorded 3 hours after the supernova event on 6 October 2013 by the Intermediate Palomar Transient Factory (iPTF). The star is located in a spiral galaxy named NGC 7610, 160 million light years away in the constellation of Pegasus.\n\nOn 20 September 2016, amateur astronomer Victor Buso from Rosario, Argentina was testing out his new 16 inch telescope. When taking several twenty second exposures of galaxy NGC 613, Buso chanced upon a supernova that had just become visible on earth. After examining the images he contacted the Instituto de Astrofísica de La Plata. \"It was the first time anyone had ever captured the initial moments of the “shock breakout” from an optical supernova, one not associated with a gamma-ray or X-ray burst.\" The odds of capturing such an event were put between one in ten million to one in a hundred million, according to astronomer Melina Bersten from the Instituto de Astrofísica.\nThe supernova Buso observed was a Type IIb made by a star twenty times the mass of the sun. Astronomer Alex Filippenko from the University of California remarked that professional astronomers had been searching for such an event for a long time. He stated: \"Observations of stars in the first moments they begin exploding provide information that cannot be directly obtained in any other way.\"\n\nEarly work on what was originally believed to be simply a new category of novae was performed during the 1930s by two astronomers named Walter Baade and Fritz Zwicky at Mount Wilson Observatory. The name \"super-novae\" was first used during 1931 lectures held at Caltech by Baade and Zwicky, then used publicly in 1933 at a meeting of the American Physical Society. By 1938, the hyphen had been lost and the modern name was in use. Because supernovae are relatively rare events within a galaxy, occurring about three times a century in the Milky Way, obtaining a good sample of supernovae to study requires regular monitoring of many galaxies.\n\nSupernovae in other galaxies cannot be predicted with any meaningful accuracy. Normally, when they are discovered, they are already in progress. Most scientific interest in supernovae—as standard candles for measuring distance, for example—require an observation of their peak luminosity. It is therefore important to discover them well before they reach their maximum. Amateur astronomers, who greatly outnumber professional astronomers, have played an important role in finding supernovae, typically by looking at some of the closer galaxies through an optical telescope and comparing them to earlier photographs.\n\nToward the end of the 20th century astronomers increasingly turned to computer-controlled telescopes and CCDs for hunting supernovae. While such systems are popular with amateurs, there are also professional installations such as the Katzman Automatic Imaging Telescope. The Supernova Early Warning System (SNEWS) project uses a network of neutrino detectors to give early warning of a supernova in the Milky Way galaxy. Neutrinos are particles that are produced in great quantities by a supernova, and they are not significantly absorbed by the interstellar gas and dust of the galactic disk.\n\nSupernova searches fall into two classes: those focused on relatively nearby events and those looking farther away. Because of the expansion of the universe, the distance to a remote object with a known emission spectrum can be estimated by measuring its Doppler shift (or redshift); on average, more-distant objects recede with greater velocity than those nearby, and so have a higher redshift. Thus the search is split between high redshift and low redshift, with the boundary falling around a redshift range of \"z\"=0.1–0.3—where \"z\" is a dimensionless measure of the spectrum's frequency shift.\n\nHigh redshift searches for supernovae usually involve the observation of supernova light curves. These are useful for standard or calibrated candles to generate Hubble diagrams and make cosmological predictions. Supernova spectroscopy, used to study the physics and environments of supernovae, is more practical at low than at high redshift. Low redshift observations also anchor the low-distance end of the Hubble curve, which is a plot of distance versus redshift for visible galaxies. (See also Hubble's law).\n\nSupernova discoveries are reported to the International Astronomical Union's Central Bureau for Astronomical Telegrams, which sends out a circular with the name it assigns to that supernova. The name is the marker \"SN\" followed by the year of discovery, suffixed with a one or two-letter designation. The first 26 supernovae of the year are designated with a capital letter from \"A\" to \"Z\". Afterward pairs of lower-case letters are used: \"aa\", \"ab\", and so on. Hence, for example, \"SN 2003C\" designates the third supernova reported in the year 2003. The last supernova of 2005 was SN 2005nc, indicating that it was the 367th supernova found in 2005. Since 2000, professional and amateur astronomers have been finding several hundreds of supernovae each year (572 in 2007, 261 in 2008, 390 in 2009; 231 in 2013).\n\nHistorical supernovae are known simply by the year they occurred: SN 185, SN 1006, SN 1054, SN 1572 (called \"Tycho's Nova\") and SN 1604 (\"Kepler's Star\"). Since 1885 the additional letter notation has been used, even if there was only one supernova discovered that year (e.g. SN 1885A, SN 1907A, etc.) — this last happened with SN 1947A. \"SN\", for SuperNova, is a standard prefix. Until 1987, two-letter designations were rarely needed; since 1988, however, they have been needed every year.\n\nAs part of the attempt to understand supernovae, astronomers have classified them according to their light curves and the absorption lines of different chemical elements that appear in their spectra. The first element for division is the presence or absence of a line caused by hydrogen. If a supernova's spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified \"Type II\"; otherwise it is \"Type I\". In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova's apparent magnitude as a function of time).\n\nType I supernovae are subdivided on the basis of their spectra, with Type Ia showing a strong ionised silicon absorption line. Type I supernovae without this strong line are classified as Type Ib and Ic, with Type Ib showing strong neutral helium lines and Type Ic lacking them. The light curves are all similar, although Type Ia are generally brighter at peak luminosity, but the light curve is not important for classification of Type I supernovae.\n\nA small number of Type Ia supernovae exhibit unusual features such as non-standard luminosity or broadened light curves, and these are typically classified by referring to the earliest example showing similar features. For example, the sub-luminous SN 2008ha is often referred to as SN 2002cx-like or class Ia-2002cx.\n\nA small proportion of type Ic supernovae show highly broadened and blended emission lines which are taken to indicate very high expansion velocities for the ejecta. These have been classified as type Ic-BL or Ic-bl.\n\nThe supernovae of Type II can also be sub-divided based on their spectra. While most Type II supernovae show very broad emission lines which indicate expansion velocities of many thousands of kilometres per second, some, such as SN 2005gl, have relatively narrow features in their spectra. These are called Type IIn, where the 'n' stands for 'narrow'.\n\nA few supernovae, such as SN 1987K and SN 1993J, appear to change types: they show lines of hydrogen at early times, but, over a period of weeks to months, become dominated by lines of helium. The term \"Type IIb\" is used to describe the combination of features normally associated with Types II and Ib.\n\nType II supernovae with normal spectra dominated by broad hydrogen lines that remain for the life of the decline are classified on the basis of their light curves. The most common type shows a distinctive \"plateau\" in the light curve shortly after peak brightness where the visual luminosity stays relatively constant for several months before the decline resumes. These are called Type II-P referring to the plateau. Less common are Type II-L supernovae that lack a distinct plateau. The \"L\" signifies \"linear\" although the light curve is not actually a straight line.\n\nSupernovae that do not fit into the normal classifications are designated peculiar, or 'pec'.\n\nFritz Zwicky defined additional supernovae types, although based on a very few examples that did not cleanly fit the parameters for a Type I or Type II supernova. SN 1961i in NGC 4303 was the prototype and only member of the Type III supernova class, noted for its broad light curve maximum and broad hydrogen Balmer lines that were slow to develop in the spectrum. SN 1961f in NGC 3003 was the prototype and only member of the Type IV class, with a light curve similar to a Type II-P supernova, with hydrogen absorption lines but weak hydrogen emission lines. The Type V class was coined for SN 1961V in NGC 1058, an unusual faint supernova or supernova impostor with a slow rise to brightness, a maximum lasting many months, and an unusual emission spectrum. The similarity of SN 1961V to the Eta Carinae Great Outburst was noted. Supernovae in M101 (1909) and M83 (1923 and 1957) were also suggested as possible Type IV or Type V supernovae.\n\nThese types would now all be treated as peculiar Type II supernovae, of which many more examples have been discovered, although it is still debated whether SN 1961V was a true supernova following an LBV outburst or an impostor.\n\nThe type codes, described above given to supernovae, are \"taxonomic\" in nature: the type number describes the light observed from the supernova, not necessarily its cause. For example, Type Ia supernovae are produced by runaway fusion ignited on degenerate white dwarf progenitors while the spectrally similar Type Ib/c are produced from massive Wolf–Rayet progenitors by core collapse. The following summarizes what is currently believed to be the most plausible explanations for supernovae.\n\nA white dwarf star may accumulate sufficient material from a stellar companion to raise its core temperature enough to ignite carbon fusion, at which point it undergoes runaway nuclear fusion, completely disrupting it. There are three avenues by which this detonation is theorized to happen: stable accretion of material from a companion, the collision of two white dwarfs, or accretion that causes ignition in a shell that then ignites. The dominant mechanism by which Type Ia supernovae are produced remains unclear. Despite this uncertainty in how Type Ia supernovae are produced, Type Ia supernovae have very uniform properties, and are useful standard candles over intergalactic distances. Some calibrations are required to compensate for the gradual change in properties or different frequencies of abnormal luminosity supernovae at high red shift, and for small variations in brightness identified by light curve shape or spectrum.\n\nThere are several means by which a supernova of this type can form, but they share a common underlying mechanism. If a carbon-oxygen white dwarf accreted enough matter to reach the Chandrasekhar limit of about 1.44 solar masses () (for a non-rotating star), it would no longer be able to support the bulk of its mass through electron degeneracy pressure and would begin to collapse. However, the current view is that this limit is not normally attained; increasing temperature and density inside the core ignite carbon fusion as the star approaches the limit (to within about 1%), before collapse is initiated.\n\nWithin a few seconds, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1–) to unbind the star in a supernova. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000–20,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of −19.3 (or 5 billion times brighter than the Sun), with little variation.\n\nThe model for the formation of this category of supernova is a closed binary star system. The larger of the two stars is the first to evolve off the main sequence, and it expands to form a red giant. The two stars now share a common envelope, causing their mutual orbit to shrink. The giant star then sheds most of its envelope, losing mass until it can no longer continue nuclear fusion. At this point it becomes a white dwarf star, composed primarily of carbon and oxygen. Eventually the secondary star also evolves off the main sequence to form a red giant. Matter from the giant is accreted by the white dwarf, causing the latter to increase in mass. Despite widespread acceptance of the basic model, the exact details of initiation and of the heavy elements produced in the catastrophic event are still unclear.\n\nType Ia supernovae follow a characteristic light curve—the graph of luminosity as a function of time—after the event. This luminosity is generated by the radioactive decay of nickel-56 through cobalt-56 to iron-56. The peak luminosity of the light curve is extremely consistent across normal Type Ia supernovae, having a maximum absolute magnitude of about −19.3. This allows them to be used as a secondary standard candle to measure the distance to their host galaxies.\n\nAnother model for the formation of Type Ia supernovae involves the merger of two white dwarf stars, with the combined mass momentarily exceeding the Chandrasekhar limit. There is much variation in this type of event, and in many cases there may be no supernova at all, but it is expected that they will have a broader and less luminous light curve than the more normal SN Type Ia.\n\nAbnormally bright Type Ia supernovae are expected when the white dwarf already has a mass higher than the Chandrasekhar limit, possibly enhanced further by asymmetry, but the ejected material will have less than normal kinetic energy.\n\nThere is no formal sub-classification for the non-standard Type Ia supernovae. It has been proposed that a group of sub-luminous supernovae that occur when helium accretes onto a white dwarf should be classified as Type Iax. This type of supernova may not always completely destroy the white dwarf progenitor and could leave behind a zombie star.\n\nOne specific type of non-standard Type Ia supernova develops hydrogen, and other, emission lines and gives the appearance of mixture between a normal Type Ia and a Type IIn supernova. Examples are SN 2002ic and SN 2005gj. These supernova have been dubbed Type Ia/IIn, Type Ian, Type IIa and Type IIan.\n\nVery massive stars can undergo core collapse when nuclear fusion becomes unable to sustain the core against its own gravity; passing this threshold is the cause of all types of supernova except Type Ia. The collapse may cause violent expulsion of the outer layers of the star resulting in a supernova, or the release of gravitational potential energy may be insufficient and the star may collapse into a black hole or neutron star with little radiated energy.\n\nCore collapse can be caused by several different mechanisms: electron capture; exceeding the Chandrasekhar limit; pair-instability; or photodisintegration. When a massive star develops an iron core larger than the Chandrasekhar mass it will no longer be able to support itself by electron degeneracy pressure and will collapse further to a neutron star or black hole. Electron capture by magnesium in a degenerate O/Ne/Mg core causes gravitational collapse followed by explosive oxygen fusion, with very similar results. Electron-positron pair production in a large post-helium burning core removes thermodynamic support and causes initial collapse followed by runaway fusion, resulting in a pair-instability supernova. A sufficiently large and hot stellar core may generate gamma-rays energetic enough to initiate photodisintegration directly, which will cause a complete collapse of the core.\n\nThe table below lists the known reasons for core collapse in massive stars, the types of star that they occur in, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun's mass, although the mass at the time of the supernova may be much lower.\n\nType IIn supernovae are not listed in the table. They can potentially be produced by various types of core collapse in different progenitor stars, possibly even by Type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed Type IIn supernovae are actually supernova impostors, massive eruptions of LBV-like stars similar to the Great Eruption of Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.\n\nWhen a stellar core is no longer supported against gravity, it collapses in on itself with velocities reaching 70,000 km/s (0.23\"c\"), resulting in a rapid increase in temperature and density. What follows next depends on the mass and structure of the collapsing core, with low mass degenerate cores forming neutron stars, higher mass degenerate cores mostly collapsing completely to black holes, and non-degenerate cores undergoing runaway fusion.\n\nThe initial collapse of degenerate cores is accelerated by beta decay, photodisintegration and electron capture, which causes a burst of electron neutrinos. As the density increases, neutrino emission is cut off as they become trapped in the core. The inner core eventually reaches typically 30 km diameter and a density comparable to that of an atomic nucleus, and neutron degeneracy pressure tries to halt the collapse. If the core mass is more than about then neutron degeneracy is insufficient to stop the collapse and a black hole forms directly with no supernova.\n\nIn lower mass cores the collapse is stopped and the newly formed neutron core has an initial temperature of about 100 billion kelvin, 6000 times the temperature of the sun's core. At this temperature, neutrino-antineutrino pairs of all flavors are efficiently formed by thermal emission. These thermal neutrinos are several times more abundant than the electron-capture neutrinos. About 10 joules, approximately 10% of the star's rest mass, is converted into a ten-second burst of neutrinos which is the main output of the event. The suddenly halted core collapse rebounds and produces a shock wave that stalls within milliseconds in the outer core as energy is lost through the dissociation of heavy elements. A process that is is necessary to allow the outer layers of the core to reabsorb around 10 joules (1 foe) from the neutrino pulse, producing the visible brightness, although there are also other theories on how to power the explosion.\n\nSome material from the outer envelope falls back onto the neutron star, and for cores beyond about there is sufficient fallback to form a black hole. This fallback will reduce the kinetic energy created and the mass of expelled radioactive material, but in some situations it may also generate relativistic jets that result in a gamma-ray burst or an exceptionally luminous supernova.\n\nCollapse of massive non-degenerate cores will ignite further fusion. When the core collapse is initiated by pair instability, oxygen fusion begins and the collapse may be halted. For core masses of , the collapse halts and the star remains intact, but core collapse will occur again when a larger core has formed. For cores of around , the fusion of oxygen and heavier elements is so energetic that the entire star is disrupted, causing a supernova. At the upper end of the mass range, the supernova is unusually luminous and extremely long-lived due to many solar masses of ejected Ni. For even larger core masses, the core temperature becomes high enough to allow photodisintegration and the core collapses completely into a black hole.\n\nStars with initial masses less than about eight times the sun never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least (possibly as much as ) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.\n\nIf core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a Type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a Type II supernova. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.\n\nStars with an initial mass up to about 90 times the sun, or a little less at high metallicity, are expected to result in a Type II-P supernova which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a Type II-L supernova. At very low metallicity, stars of around will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with Type II characteristics but a very large mass of ejected Ni and high luminosity.\n\nThese supernovae, like those of Type II, are massive stars that undergo core collapse. However the stars which become Types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf–Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass loss rates. Observations of Type Ib/c supernova do not match the observed or expected occurrence of Wolf–Rayet stars and alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed. Since a supernova can occur whenever the mass of the star at the time of core collapse is low enough not to cause complete fallback to a black hole, any massive star may result in a supernova if it loses enough mass before core collapse occurs.\n\nType Ib supernovae are the more common and result from Wolf–Rayet stars of Type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining and these are the progenitors of Type Ic supernovae.\n\nA few percent of the Type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped Type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.\n\nUltra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. ). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be an observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core.\n\nThe core collapse of some massive stars may not result in a visible supernova. The main model for this is a sufficiently massive core that the kinetic energy is insufficient to reverse the infall of the outer layers onto a black hole. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star's location.\n\nA historic puzzle concerned the source of energy that can maintain the optical supernova glow for months. Although the energy that disrupts each type of supernovae is delivered promptly, the light curves are mostly dominated by subsequent radioactive heating of the rapidly expanding ejecta. Some have considered rotational energy from the central pulsar. The ejecta gases would dim quickly without some energy input to keep it hot. The intensely radioactive nature of the ejecta gases, which is now known to be correct for most supernovae, was first calculated on sound nucleosynthesis grounds in the late 1960s. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.\n\nIt is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a Type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of Ni through its daughters Co to Fe produces gamma-ray photons, primarily of 847keV and 1238keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of Ni to Co (half life 6 days) while energy for the later light curve in particular fit very closely with the 77.3 day half-life of Co decaying to Fe. Later measurements by space gamma-ray telescopes of the small fraction of the Co and Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\n\nThe visual light curves of the different supernova types all depend at late times on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.\n\nThe light curves for Type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half life 6 days), which then decays to radioactive cobalt-56 (half life 77 days). These radioisotopes excite the surrounding material to incandescence. Studies of cosmology today rely on Ni radioactivity providing the energy for the optical brightness of supernovae of Type Ia, which are the \"standard candles\" of cosmology but whose diagnostic 847keV and 1238keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission becomes dominant from the remaining cobalt-56, although this portion of the light curve has been little-studied.\n\nType Ib and Ic light curves are basically similar to Type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional Type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous Type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\n\nThe light curves for Type II supernovae are characterised by a much slower decline than Type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of Type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in Type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\n\nIn Type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In Type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a Type I supernova and the hydrogen even disappears from the spectrum after several weeks.\n\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\n\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\n\nNotes:\n\nA long-standing puzzle surrounding Type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object a puzzle. Proposed explanations for this kick include convection in the collapsing star and jet production during neutron star formation.\n\nOne possible explanation for this asymmetry is a large-scale convection above the core. The convection can create variations in the local abundances of elements, resulting in uneven nuclear burning during the collapse, bounce and resulting expansion.\n\nAnother possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is now favored for explaining long gamma-ray bursts.)\n\nInitial asymmetries have also been confirmed in Type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarization of the emitted light.\n\nAlthough we are used to thinking of supernovae primarily as luminous visible events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\n\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In Type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\n\nType Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the end result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is Ni generated from silicon burning. Ni is radioactive and decays into Co by beta plus decay (with a half life of six days) and gamma rays. Co itself decays by the beta plus (positron) path with a half life of 77 days into stable Fe. These two processes are responsible for the electromagnetic radiation from Type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\n\nCore collapse supernovae are on average visually fainter than Type Ia supernovae, but the total energy released is far higher. In these type of supernovae, the gravitational potential energy is converted into kinetic energy that compresses and collapses the core, initially producing electron neutrinos from disintegrating nucleons, followed by all flavours of thermal neutrinos from the super-heated neutron star core. Around 1% of these neutrinos are thought to deposit sufficient energy into the outer layers of the star to drive the resulting catastrophe, but again the details cannot be reproduced exactly in current models. Kinetic energies and nickel yields are somewhat lower than Type Ia supernovae, hence the lower peak visual luminosity of Type II supernovae, but energy from the de-ionisation of the many solar masses of remaining hydrogen can contribute to a much slower decline in luminosity and produce the plateau phase seen in the majority of core collapse supernovae.\n\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high luminosity supernovae and is thought to be the cause of Type Ic hypernovae and long duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\n\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause Type IIn hypernovae.\n\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to Type II-P, the nature after core collapse is more like that of a giant Type Ia with runaway fusion of carbon, oxygen, and silicon. The total energy released by the highest mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\n\nThe supernova classification type is closely tied to the type of star at the time of the collapse. The occurrence of each type of supernova depends dramatically on the metallicity, and hence the age of the host galaxy.\n\nType Ia supernovae are produced from white dwarf stars in binary systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in Type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\n\nType Ib/c and II-L, and possibly most Type IIn, supernovae are only thought to be produced from stars having near-solar metallicity levels that result in high mass loss from massive stars, hence they are less common in older, more-distant galaxies. The table shows the expected progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the expected progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about and respectively. Most progenitors of Type II supernovae are not detected and must be considerably fainter, and presumably less massive. It is now proposed that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of Type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for Type IIb supernovae, and almost all Type IIb supernovae near enough to observe have shown such progenitors.\n\nUntil just a few decades ago, hot supergiants were not considered likely to explode, but observations have shown otherwise. Blue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a Type IIn supernova. Several examples of hot luminous progenitors of Type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\n\nThe progenitors of Type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed Type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal Type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly-rotating massive stars, likely corresponding to the highly-energetic Type Ic-BL events that are associated with long-duration gamma-ray bursts.\n\nSupernovae are the major source of elements heavier than nitrogen. These elements are produced by nuclear fusion for nuclei up to S, by silicon photodisintegration rearrangement and quasiequilibrium (see Supernova nucleosynthesis) during silicon burning for nuclei between Ar and Ni, and by rapid captures of neutrons during the supernova's collapse for elements heavier than iron. Nucleosynthesis during silicon burning yields nuclei roughly 1000-100,000 times more abundant than the r-process isotopes heavier than iron. Supernovae are the most likely, although not undisputed, candidate sites for the r-process, which is the rapid capture of neutrons that occurs at high temperature and high density of neutrons. Those reactions produce highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. The r-process produces about half of all the heavier isotopes of the elements beyond iron, including plutonium and uranium. The only other major competing process for producing elements heavier than iron is the s-process in large, old, red-giant AGB stars, which produces these elements slowly over longer epochs and which cannot produce elements heavier than lead.\n\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up the surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium, and traces of lithium, while all heavier elements are synthesized in stars and supernovae. Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as \"metals\".\n\nThese injected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life, and may decisively influence the possibility of having planets orbiting it.\n\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\n\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system. Supernova production of heavy elements over astronomic periods of time ultimately made the chemistry of life on Earth possible.\n\nA near-Earth supernova is a supernova close enough to the Earth to have noticeable effects on its biosphere. Depending upon the type and energy of the supernova, it could be as far as 3000 light-years away. Gamma rays from a supernova would induce a chemical reaction in the upper atmosphere converting molecular nitrogen into nitrogen oxides, depleting the ozone layer enough to expose the surface to harmful ultraviolet solar radiation. This has been proposed as the cause of the Ordovician–Silurian extinction, which resulted in the death of nearly 60% of the oceanic life on Earth.\nIn 1996 it was theorized that traces of past supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Iron-60 enrichment was later reported in deep-sea rock of the Pacific Ocean. In 2009, elevated levels of nitrate ions were found in Antarctic ice, which coincided with the 1006 and 1054 supernovae. Gamma rays from these supernovae could have boosted levels of nitrogen oxides, which became trapped in the ice.\n\nType Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because these supernovae arise from dim, common white dwarf stars in binary systems, it is likely that a supernova that can affect the Earth will occur unpredictably and in a star system that is not well studied. The closest known candidate is IK Pegasi (see below). Recent estimates predict that a Type II supernova would have to be closer than eight parsecs (26 light-years) to destroy half of the Earth's ozone layer, and there are no such candidates closer than about 500 light years.\n\nThe next supernova in the Milky Way will likely be detectable even if it occurs on the far side of the galaxy. It is likely to be produced by the collapse of an unremarkable red supergiant and it is very probable that it will already have been catalogued in infrared surveys such as 2MASS. There is a smaller chance that the next core collapse supernova will be produced by a different type of massive star such as a yellow hypergiant, luminous blue variable, or Wolf–Rayet. The chances of the next supernova being a Type Ia produced by a white dwarf are calculated to be about a third of those for a core collapse supernova. Again it should be observable wherever it occurs, but it is less likely that the progenitor will ever have been observed. It isn't even known exactly what a Type Ia progenitor system looks like, and it is difficult to detect them beyond a few parsecs. The total supernova rate in our galaxy is estimated to be between 2 and 12 per century, although we haven't actually observed one for several centuries.\nStatistically, the next supernova is likely to be produced from an otherwise unremarkable red supergiant, but it is difficult to identify which of those supergiants are in the final stages of heavy element fusion in their cores and which have millions of years left. The most-massive red supergiants are expected to shed their atmospheres and evolve to Wolf–Rayet stars before their cores collapse. All Wolf–Rayet stars are expected to end their lives from the Wolf–Rayet phase within a million years or so, but again it is difficult to identify those that are closest to core collapse. One class that is expected to have no more than a few thousand years before exploding are the WO Wolf–Rayet stars, which are known to have exhausted their core helium. Only eight of them are known, and only four of those are in the Milky Way.\n\nA number of close or well known stars have been identified as possible core collapse supernova candidates: the red supergiants Antares and Betelgeuse; the yellow hypergiant Rho Cassiopeiae; the luminous blue variable Eta Carinae that has already produced a supernova impostor; and the brightest component, a Wolf–Rayet star, in the Regor or Gamma Velorum system, Others have gained notoriety as possible, although not very likely, progenitors for a gamma-ray burst; for example WR 104.\n\nIdentification of candidates for a Type Ia supernova is much more speculative. Any binary with an accreting white dwarf might produce a supernova although the exact mechanism and timescale is still debated. These systems are faint and difficult to identify, but the novae and recurrent novae are such systems that conveniently advertise themselves. One example is U Scorpii. The nearest known Type Ia supernova candidate is IK Pegasi (HR 8210), located at a distance of 150 light-years, but observations suggest it will be several million years before the white dwarf can accrete the critical mass required to become a Type Ia supernova.\n\n\n"}
{"id": "42474831", "url": "https://en.wikipedia.org/wiki?curid=42474831", "title": "The Harlem Hellfighters", "text": "The Harlem Hellfighters\n\nThe Harlem Hellfighters is a graphic novel written by author Max Brooks with illustrations done by Caanan White. It is a fictionalized account of the experiences of the largely African American 369th Infantry Regiment, nicknamed the \"Hell-fighters\" by German soldiers, during the First World War.\n\nThe novel is narrated by Mark, a veteran of the 369th's tour in Europe. It begins upon the inception of U.S. involvement in World War I, with recruitment for the 15th New York National Guard Regiment being held in Harlem, New York. The soldiers, though all black, come from a diverse array of backgrounds, classes, and cultures, leading to some initial internal strain. The 15th is headed by Lieutenant Adams and Sergeant Mandla.\n\nIn July 1917, the regiment commences basic training at Camp Whitman, where they are strictly disciplined and educated by a benevolent Adams. Concurrently, members of the regimental band are subjected to the precise training of renowned bandleader Lt. James Europe. It's here that the African-American soldiers begin to first feel the effects of segregation with the United States Army; they are given uniforms later than their white counterparts and train with broomsticks in the place of rifles.\n\nIn October, the regiment is reassigned to Spartanburg, South Carolina to finish their training. Fears run high within their ranks given that the town is located in the Jim Crow South; only weeks earlier tensions between racist locals and African-American trainees had led to a riot in Houston. In order to preserve the well-being of his men, Lt. Adams orders them to completely ignore any verbal or physical provocations made by the local populace. Mark and several of his fellow infantrymen are subject to a vicious beating at the hands of townsmen that is only broken up when white soldiers from the 7th Infantry Regiment intervene.\n\nAfter a handful of weeks in Spartanburg, the 15th is deployed to France. When they reach the Western Front in January 1918, the regiment's populace is bitterly assigned to laborious roles, unloading ships and performing janitorial tasks for the American Expeditionary Force.\n\nAfter months more of \"pick and shovel work,\" the 15th is reorganized into the 369th Infantry Regiment and transferred under the jurisdiction of the French Fourth Army. The French soldiers show a foreign level of acceptance and tolerance of the newly instilled black presence. Mark's friend David Scott is killed by a sniper on their first day in the trenches whilst naively peeking into no man's land.\n\nThe 369th is soon taken aback by the brutal realities of war; the rat-infested, muddy, lice-encompassed trenches become a subject of much complaining. Nonetheless, the soldiers soon prove themselves in combat. Private Henry Johnson becomes the first American to receive the Croix de Guerre after dispatching a German \"raider party\" armed solely with a bolo knife and rifle, saving the lives of multiple comrades. This results in the 369th's rise to prominence on both sides of the war, with the Germans nicknaming them the Harlem Hellfighters. As the war progresses, bombings and gas attacks become part of the 369th's daily routine.\n\nThe African American soldiers soon face intense discrimination from their white counterparts; they are eventually barred altogether from social interactions with the friendlier French. Mark is soon arrested by military police after defending himself against a provocation by white soldiers. A frustrated Mark, insisting that he is fighting a \"white man's war\", transfers to a labor unit.\n\nAnticipating the Second Battle of the Marne, the Expeditionary Force formulates a plan to retreat from the would-be German assaulted forward trenches whilst bombarding German supply roads. However, in order for this to be executed, the Americans must enforce the illusion that the trenches are still stocked with men by placing a \"volunteer unit\" left behind. Lt. Adams subsequently volunteers the 369th for this duty. Shortly before combat commences, Mark returns to his regiment, who rides out German bombardment of the forward trenches completely unscathed.\n\nImmediately afterwards the 369th participates in the perilous Allied counterattack, with Sgt. Mandla being killed in the process.\n\nThe novel ends with a series of conclusory panels, dictating that the Harlem Hellfighters spent 191 days in combat, were the first Allied unit to reach the Rhine, and that they received a victory parade upon their return home to New York.\n\nBrooks' interest in the 369th Infantry Division sparked as a 10-year-old, when a tutor told him their story. As an adult, Brooks attempted to have their story turned into a film, pitching his script across the movie industry to no avail. The various studios Brooks sought out believed that World War I had been rendered obscure to the general American public; they believed a film dealing with the subject would not be profitable. Finally, after collaborating with White on a to his novel The Zombie Survival Guide in 2006, Brooks realized the story could be told within the comics medium sans the financial concerns of film.\n\nBrooks often employed the use of amalgamation in creating the characters of \"The Harlem Hellfighters\", taking inspiration from real life soldiers and transmitting it onto the pages in the form of a single individual. For example, Lt. Adams was a combination of several black officers at the time. The role of a French officer derived inspiration from an excerpt of \"From Harlem to the Rhine\". Concurrently, real-life members of the 369th were utilized in the novel's fictionalized retelling, among them James Europe, Henry Lincoln Johnson, Arthur Little, and Henri Gouraud. \n\nAnother difference between the book and reality, is that while the Hellfighters did receive a parade in Paris and were adored and considered heroes by the French, they were not given many accolades when they returned home to the United States, aside from a parade in New York City on February 17, 1919. In fact, a number of the 369th soldiers elected to stay in France after the war was over, rather than come back to a segregated America, where they would be expected to resume their place at the bottom of the social ladder. \n\nReaction to the graphic novel was universally positive. Harvard professor Henry Louis Gates, Jr hailed \"The Harlem Hellfighters\" as \"a major contribution to our understanding of Black History.\" \"The Washington Post\" went on to praise the illustrations, arguing that \"White’s grid work is forever shifting, as his overlapping panels shuffle like snapshots fallen from a scrapbook, and his full “splash” pages are so visually engaging that the greedy reader wishes he’d provided yet more of them.\"\n\nSony Pictures has purchased the rights to create a film version of the novel, with Caleeb Pinkett and James Lassiter producing on behalf of Overbrook Entertainment under the leadership of Will Smith.\n"}
{"id": "4889401", "url": "https://en.wikipedia.org/wiki?curid=4889401", "title": "Tiddalik", "text": "Tiddalik\n\nThe tale of Tiddalik the Frog is a legend from Australian Aboriginal mythology.\n\nIn the telling of the myth, Tiddalik awoke one morning with an unquenchable thirst, and began to drink until all the fresh water was greedily consumed. Creatures and plant life everywhere began to die due to lack of moisture. Other animals conspired against Tiddalik and devised a plan for him to release all of the water he had consumed. This was successfully coordinated by a wise old Owl, when Nabunum the eel made Tiddalik laugh when he tied himself in comical shapes. As Tiddalik laughed, the water rushed out of him to replenish the lakes, swamps and rivers. The legend of Tiddalik is not only an important story of the Dreamtime, but has been the subject of popular modern children's books. In some Aboriginal cultures, Tiddalik is known as \"Molok\".\n\nThe story has been said to describe the Water-holding Frog (\"Litoria platycephala\"), from central Australia. The frogs burrow under ground during dry periods, and emerge during the rain to absorb large amounts of water, breed and feed. This allows it to avoid desiccation during drought, a trait not exhibited by most frogs. They were used by Indigenous Australians during times of drought as a source of water.\n\nThis story originated with the Aboriginal people of South Gippsland, Victoria but has spread worldwide since first being published. Tiddalik is commemorated in a statue in Warwick, Queensland. Various versions of the story were recorded by amateur ethnographers in the late nineteenth century; originating with the Gunai people near Port Albert, approximately south-east of Melbourne. In the original story, Tiddalik formed the area's bays, estuaries, inlets and islands. The substance of the story has changed over time, with different animals being able to make Tiddalik laugh, and many of the modern versions being dissimilar to those of the nineteenth century.\n\nThe Water-holding Frog (\"Litoria platycephala\") ascribed in modern times to Tiddalik is not found in the area of the legend's origin. It is likely that Tiddalik either refers to a different frog, or is a mythical memory of a time, 10,000 to 12,000 years ago, when the landscape was sufficiently different for the frog's range to extend to the South Gippsland. While the modern story has a happy ending, with water returned for all to use, the original ends in environmental disaster. The flood caused many to drown and others to be stranded on islands. Those stranded were rescued by \"Borun\" the pelican, with the end of the tale explaining how the pelican's feathers subsequently changed from all-black to a mixture of black and white.\n\n\n"}
{"id": "45447250", "url": "https://en.wikipedia.org/wiki?curid=45447250", "title": "Wind Energy (journal)", "text": "Wind Energy (journal)\n\nWind Energy is a monthly peer-reviewed scientific journal covering research on wind power published by John Wiley & Sons. The editor-in-chief is Simon Watson (Delft University of Technology). According to the \"Journal Citation Reports\", the journal has a 2013 impact factor of 2.556, ranking it 35th out of 83 journals in \"Energy & Fuels\" and 12th out of 128 journals in \"Engineering Mechanical\".\n\n"}
{"id": "1866629", "url": "https://en.wikipedia.org/wiki?curid=1866629", "title": "World Glacier Monitoring Service", "text": "World Glacier Monitoring Service\n\nThe World Glacier Monitoring Service (WGMS) was started in 1986, combining the two former services PSFG (Permanent Service on Fluctuations of Glaciers) and TTS/WGI (Temporal Technical Secretary/World Glacier Inventory). It is a service of the International Association of the Cryospheric Sciences of the International Union of Geodesy and Geophysics (IACS, IUGG) as well as of the World Data System of the International Council for Science (WDS, ICSU) and works under the auspices of the United Nations Environment Programme (UNEP), the United Nations Educational, Scientific and Cultural Organisation (UNESCO), and the World Meteorological Organization (WMO)\n\nThe WGMS is based at a centre at the University of Zurich in Switzerland, and the Director of the Service is Michael Zemp. It is supported by the United Nations Environment Programme.\n\nWGMS \"collects standardised observations on changes in mass, volume, area and length of glaciers with time (glacier fluctuations), as well as statistical information on the distribution of perennial surface ice in space (glacier inventories). Such glacier fluctuation and inventory data are high priority key variables in climate system monitoring; they form a basis for hydrological modelling with respect to possible effects of atmospheric warming, and provide fundamental information in glaciology, glacial geomorphology and quaternary geology. Such glacier fluctuation and inventory data are high priority key variables in climate system monitoring; they form a basis for hydrological modelling with respect to possible effects of atmospheric warming, and provide fundamental information in glaciology, glacial geomorphology and quaternary geology. The highest information density is found for the Alps and Scandinavia, where long and uninterrupted records are available\"\n\n\"In close collaboration with the U.S. National Snow and Ice Data Center (NSIDC) and the Global Land Ice Measurements from Space (GLIMS) initiative, the WGMS is in charge of the Global Terrestrial Network for Glaciers (GTN-G) within GTOS/GCOS. GTN-G aims at combining (a) in-situ observations with remotely sensed data, (b) process understanding with global coverage and (c) traditional measurements with new technologies by using an integrated and multi-level strategy\" \n\n"}
{"id": "53126139", "url": "https://en.wikipedia.org/wiki?curid=53126139", "title": "Yellow pine", "text": "Yellow pine\n\nIn ecology and forestry, yellow pine refers to a number of conifer species which tend to grow in similar plant communities and yield similar strong wood. In the Western United States, yellow pine refers to Jeffrey pine or ponderosa pine. In the Southern United States, yellow pine refers to longleaf pine, shortleaf pine, slash pine, or loblolly pine. In the United Kingdom, yellow pine refers to Eastern white pine or Scots pine.\n\nThe Jeffrey pine and the ponderosa pine are pines that are common in drier montane areas of the Sierra Nevada. They are often confused by casual observers. Across the remainder of the American West, Jeffrey pine is absent, with ponderosa pine being the sole yellow pine.\n\nJeffrey pine is more stress tolerant than ponderosa pine in the Sierra Nevada. At higher elevations, on poorer soils, in colder climates, and in dryer climates, Jeffrey pine replaces ponderosa as the dominant tree (\"Jeffrey pine forest\"). Ponderosa pine-dominated forests (\"Ponderosa pine forest\") occur at elevations from about . Jeffrey pine-dominated forests occur mostly in California, from in the north, and in the south. The highest elevations are typically on the east side of the Sierra Nevada. \"Eastside pine forest\" refers to areas of Lassen National Forest, Plumas National Forest, and Tahoe National Forest, all on the east of the Sierra Nevada crest, where ponderosa and Jeffrey pine codominate.\n\nPonderosa pine forests occurs on the Colorado Plateau and in the Sierra Nevada of the western United States, as well as other parts of North America.\n\nOne way to distinguish between them is by their cones. Each has barbs at the end of the scales. The sharp Jeffrey pine cone scale barbs point inward, so the cone feels smooth to the palm of one's hand when rubbed down the cone. Ponderosa pine cone scale barbs point outward, so feel sharp and prickly to the palm of one's hands. Another distinguishing characteristic is that the needles of Jeffrey pine are glaucous, less bright green than those of ponderosa pine, and by the stouter, heavier cones with larger seeds and inward-pointing barbs.\n\nJeffrey pine wood and ponderosa pine wood are sold together as yellow pine. Both kinds of wood are hard (with a Janka hardness of 550 pounds), but the western yellow pine wood is less dense than southern yellow pine wood (28 pounds / cubic foot versus 35 pounds / cubic foot for shortleaf pine).\n\nIn the South, yellow pines grow very well in the acidic red clay soil found in most of the region. The wood from the southern yellow pines typically has a density value between 50–55 lbs/cubic foot when pressure treated. Yellow pine grows across the South and Mid-Atlantic regions, from Texas to New Jersey.\n\nDimensional lumber and plywood products manufactured from southern yellow pine are used extensively in home construction in the United States. They are also used for wooden roller coasters and are most used for utility poles throughout the United States.\n\n\"Halocarpus biformis\", a species of podocarp, is known as yellow pine in New Zealand. It is a tight-grained sweet-smelling and extremely durable wood.\n"}
