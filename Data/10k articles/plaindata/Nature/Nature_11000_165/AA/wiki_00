{"id": "52286443", "url": "https://en.wikipedia.org/wiki?curid=52286443", "title": "Air quality and EU legislation", "text": "Air quality and EU legislation\n\nSince the late 1970s, the European Union's (EU) policy has been to develop and drive appropriate measures to improve air quality throughout the EU. The control of emissions from mobile sources, improving fuel quality and promoting and integrating environmental protection requirements into the transport and energy sector are part of these aims.\n\nThe main advising agency of the EU is the European Environment Agency (EEA). It came into force in late 1993, after the decision to locate the EEA in Copenhagen. Work started in earnest in 1994. The EEA's mandate is to help the community and member countries make informed decisions about improving the environment and integrating environmental considerations into economic policies, and to coordinate the European environment information and observation network (Eionet). Eionet is a partnership network across member states involving approximately 1000 experts and more than 350 national institutions. The network supports the collection and organisation of data and the development and dissemination of information concerning Europe's environment.\n\nThe first serious attempt at an environmental harm treaty was the 1992 Framework Convention on Climate Change. Earlier attempts, such as the 1979 Convention on Long-range Transboundary Air Pollution and 1985 Convention for the Protection of the Ozone Layer, turned out to be largely futile. (Birnie et al., 2009).\n\nThe genesis of air quality within the EU is the Council Directive 96/62/EC on ambient air quality assessment and management which is commonly referred to as the Air Quality Framework Directive. It described the approach as to how air quality would be assessed and managed in the EU member states. It lists the pollutants for which air quality standards and objectives would be developed and specified in legislation (EU, 1996). There were four ‘daughter’ directives for specified pollutants; however as shown in the Cleaner Air For Europe Directive (CAFE), 2008/50/EC below, Daughter Directives 1, 2 and 3 were subsumed into CAFE, with daughter directive 4 to be subsumed at a later stage (EPA, 2016).\n\nSubsequent to the Gothenburg Protocol United Nations Economic Commission for Europe (UNECE) agreed in December 1999, which focused on reducing, by 2010, the air concentrations of four air pollutants, namely sulfur dioxide (to be reduced by 63%), nitrogen oxides (41%), volatile organic compounds (40%) and ammonia (17%), leading the EU to introduce the 2001/81/EC Directive on national emissions ceilings. This became law in Ireland by way of the European Communities (National Emissions Ceilings) Regulations 2004. The Environmental Protection Agency was assigned the status of competent authority to assess ambient air quality, approving air measuring devices, etc. (Menzies, 2016).\n\nThe Cleaner Air For Europe Directive, 2008/50/EC or CAFE as it is known, sets out the need to ascertain the ambient air qualities, assessing for comparison and informing the public where need be. In order to do this the legislation sets out the need for member states to break their countries into zones and agglomerations (collections of people). Ireland is broken into 4 zones, A, B, C and D representing Dublin, Cork, small towns/cities and rural areas respectively (Menzies, 2016). CAFE, like that of the 2001/81/EC Directive, determines that additional pollutants will be measured and reduced, namely:\n\n\nThe CAFE Directive was transposed into Irish legislation by the Air Quality Standards Regulations 2011 (S.I. No. 180 of 2011) which in turn replaced the Air Quality Standards Regulations 2002 (S.I. No. 271 of 2002), the Ozone in Ambient Air Regulations 2004 (S.I. No. 53 of 2004) and S.I. No. 33 of 1999 - Environmental Protection Agency Act, 1992 (Ambient Air Quality Assessment and Management) Regulations. (EPA, 2016).\n\nThe Environmental Protection Agency is the designated competent authority for the implementation of all Irish and EU ambient air quality legislation. The EPA manages the national ambient air quality monitoring network - assisted in its role of implementing this legislation by the local authorities, many of whom carry out ambient air quality monitoring - while also measuring the levels of a number of atmospheric pollutants (see bullet points above). For example, the EPA utilises several state and semi-state buildings, such as fire stations and garda stations, around the Greater Dublin area to gather data in real time and on a collected basis.\n\nDomestic solid fuel use such as coal and turf is the other main source of air pollution in Ireland (outside Greater Dublin) and particularly impacts air quality in areas where the sale of coal is permitted. When the Air Pollution (Marketing, Sale and Distribution of Fuel) Regulations were enacted in 1990, this banned the sale of bituminous fuels in the Greater Dublin area. This led to an immediate reduction in smoke levels, thus offering the potential for better health. According to the Dublin Regional Air Quality Management Plan 2009 – 2012 (Dublin City Council, 2009): \"these levels subsequently reduced from being close to or exceeding legal limit values to a point that is almost one tenth of the legal standard of 250 μg/m3.\"\n\nAs a result, air pollution from the burning of solid fuel can be of a greater concern in smaller towns in Ireland (Dublin City Council, 2009).\n\nAmong the many reasons that something had to be done was the importance of reducing the greenhouse gases that contribute to global warming. Before the industrial revolution era of the late 1700s, carbon dioxide could be measured in the atmosphere at around 280 parts per million (ppm), whereas in 2005 it was found to be 379 ppm. Methane, similarly in the same period, was 715ppm reaching up to 1774 ppm. This is significant because methane has a potential for global warming 70% more than CO2 (Birnie et al., 2009, Bodansky et al., 2007).\n\nThe Treaty of Amsterdam which came into force in 1999 set out new objectives such as environmental protection in its own right as opposed to being a subset of economic development. Article 6 ensures that \"environmental protection requirements must be integrated into the definition and implementation of Community policies\" (Asser Institute, 2016) which in effect meant that it went from \"semi-obscurity\" in previous environmental provisions to an Article in the core principles of the Treaty, the only constitutional document in the World to do so (Macrory, 2010). In fact the 2000 EU Charter of Rights alludes to the concept in Article 37: \"[a] high level of environmental protection and the improvement of the quality of the environment must...\" This was copper fastened in the 2007 Lisbon (EU Reform) Treaty, Article 7 by virtue of Article 6 of the same Treaty which affords the integration concept with the same value/status as the Treaties themselves (Macrory, 2010).\n\nSection 4 of the Air Pollution Act, 1987 defines air pollution as :\n\"a condition of the atmosphere in which a pollutant is present in such a quantity as to be liable to: \n\nThe legislation applies in cases where there are emissions of smoke, particulate matter, fumes or where there are odours in areas specifically defined in the regulations.\n\nThe Air Pollution Act (Marketing, Sale, Distribution and Burning of Specified Fuels) Regulations, 2012 (S.I. No. 326 of 2012), consolidated all previous applicable/associated regulations made under the Air Pollution Act, e.g. Air Pollution (Marketing, Sale and Distribution of Fuel) Regulations 1990.\nUnder Section 6 it is an offence in these regulations for the home owner/occupier of any private dwelling to burn bituminous (smoky) coal. Not only that but is also an offence to market, sell or distribute any fuels specified in the regulations within defined areas of Ireland.\n\nThe Air Pollution Act (Marketing, Sale, Distribution and Burning of Specified Fuels) Regulations, 2012 enable local authorities to take enforcement action against anyone suspected of non-compliance by initiating prosecutions at District Court level.\n\nSection 20 of the Environment (Miscellaneous Provisions) Act, 2015 increased the enforcement options for local authorities by amending Section 12A of the Air Pollution Act, 1987 to enable the issuing of fixed penalty notices i.e. on the spot fines. Following the signing into law of the Air Pollution (Fixed Payment Notice) Regulations, 2015 in December 2015, by Minister Alan Kelly, local authorities now have the option of issuing fixed penalty notices for alleged breaches of the 2012 Regulations. (Limerick City Council, 2016)\n\nPenalties range from a ceiling of €1000 (marketing, selling or distributing) for on the spot fines or up to €5000 plus costs if court proceedings are instigated.\n\nBecause of the failure to meet the objectives of WHO, i.e. clean air is considered to be a basic requirement of human health and the fact that within the EU, “poor air quality is worse than for road traffic accidents, making it the number one environmental cause of premature death in the EU.” (EU, 2015a) that in December 2013, the EU announced a Clean Air Policy Package (after a comprehensive review which began in 2011) to reduce the effect on human health. This package has at its core the following;:\n\n\nGiven the fact that in Ireland in 2012 approximately 1200 deaths were attributed to air pollution, as such it is still an issue that requires further measures (EEA, 2014).\n\nThe EU estimate that the benefits to health, by 2020 and certainly by 2030, will save society in the region of at least €40 billion with even more if the higher productivity brought about by healthier workers (fewer lost working days) was to materialise. Other benefits would be reduced healthcare costs such as lower levels of asthmatics for example. The reduction of acid rain, the scourge of many cities because of damaged buildings, would be another benefit\n\n\nAvailable at: http://europa.eu/rapid/press-release_IP-13-1274_en.htm (Accessed 14 November 2016). \n"}
{"id": "46736055", "url": "https://en.wikipedia.org/wiki?curid=46736055", "title": "Atmospheric lidar", "text": "Atmospheric lidar\n\nAtmospheric lidar is a class of instruments that uses laser light to study atmospheric properties from the ground up to the top of the atmosphere. Such instruments have been used to study, among other, atmospheric gases, aerosols, clouds, and temperature.\n\nThe basic concepts to study the atmosphere using light were developed before World War II. In 1930, E.H. Synge proposed to study the density of the upper atmosphere using a searchlight beam \n. In the following years, searchlight beams were used to study cloud altitude using both scanning and pulsed light. Advanced techniques to study cloud properties using scattered light with different wavelengths were also proposed. With the first experiments, light scattering patterns were observed in the troposphere that were not compatible with a pure molecular atmosphere. This incompatibility was attributed to suspended haze particles.\nSimilar techniques were also developed in the U.S.S.R. The searchlight beam technique continued to improve after the end of the War, with more precise instruments and new atmospheric parameters, like temperature At the same time, pulsed light was used to construct a rangefinder to measure the distance of objects, but remained only an experimental design.\n\nIn 1960, T. Maiman demonstrated the first functional laser at Hughes Research Laboratories. The demonstration was a pivotal moment for lidar development. Soon afterwards, engineers at Hughes Aircraft Company developed a laser rangefinder using ruby laser light.\nThe new device, named colidar (coherent light detection and ranging), gained widespread publicity\n. In 1962, L. Smullin and G. Fiocco used a ruby laser to detect echoes from the Moon. During their experiments they observed light scattered in the upper atmosphere that they attributed to dust particles. Soon, several research groups constructed similar devices to observe the atmosphere. By 1969, “over 20 lasers were in use by meteorologists in the United States on at least a semi-routine basis” for various applications including aerosol measurements, sub-visible cirrus and noctilucent clouds observations, and visibility measurement\n\nA simplified representation of a lidar set-up is demonstrated in Figure 1. The transmission unit consists of a laser source, followed by a series of mirrors, and a beam expander which sends the collimated light beam vertically up to the open atmosphere. Part of the transmitted radiation is scattered by atmospheric components (i.e., gases, molecules, aerosols, clouds) backward to the lidar, where it is collected by a telescope. The backscattered light is driven to an optical analyzer where the optical signal is first spectrally separated, amplified and transformed to an electrical signal. Finally, the signal is digitized and stored in a computer unit.\n\nLidars have been proven useful for classification of cloud types (i.e., cumuli versus cirrus). Cloud boundaries can be retrieved from a ground-based lidar operating at a visible and/or near-infrared band. Cloud-base height can be identified by the time difference between the transmittance of the laser pulse to the sky and the detection of the backscattered light by the telescope. The laser beam is always attenuated when it penetrates through the clouds. However, when a powerful laser (e.g., Nd:YAG laser with high energy per pulse) is used, cloud tops can be retrieved too. Another physical parameter that can be retrieved is the cloud phase. By using a linear polarized laser beam, a linear particle depolarization ratio (δ) can be defined as the ratio of measured perpendicular backscatter intensity over parallel backscatter intensity with respect to the transmitter polarization axis:\nformula_1\n\nWhen this parameter is zero (the backscattered signal is linearly polarized), the cloud contains liquid spherical droplets. However, when the cloud contains ice crystals, backscattered light arrives at the receiver unit with a cross-polarized component, and δ has a higher value (0 < δ < 1). Liquid droplets tend to behave as symmetrical scattering elements, while ice crystals are asymmetrical.\n\nThe use of the polarization ratio generally includes an implicit assumption that the particles in the volume are randomly oriented. The polarization properties of oriented particles cannot be properly represented by the depolarization ratio. Ice crystals are known to horizontally orient when they are large enough that drag forces overcome the randomizing effects of Brownian motion. Rain is also generally oriented, where drag forces flatten the drops along the fall direction. In such cases, the measured depolarization ratio may depend on the particular polarization state used by the lidar system. Some polarization lidar systems can measure the entire backscatter phase matrix, thereby avoiding the ambiguity of the depolarization ratio when oriented particles are present.\n\nOne of the biggest uncertainties for climate change is the importance of aerosol direct and indirect effects. The uncertainties were stressed in the 4th Assessment Report by the Intergovernmental Panel for Climate Change (IPCC). The large diversity of aerosol optical properties, including their sources and the meteorological processes they are subjected to, requires vertically resolved measurements, which can only be performed with routine lidar observations. Networks of aerosol lidars such as the European Aerosol Research Lidar Network (EARLINET) were established to investigate aerosol properties, along with transport and modification phenomena, in a coherent way on a regional to continental scale. As of 2015, EARLINET consists of 27 lidar stations hosting more than 44000 profiles.\nElastic-backscatter lidars (EBL) have been used extensively to investigate clouds and aerosol layers since the 1960s. EBLs detect the total backscattered signal (particle and molecular contributions). Profiles of the extinction coefficient have to be estimated using the molecular signal and the assumption of a conditionally “constant” (roughly speaking) aerosol extinction to backscatter ratio, called the lidar ratio. The main equation involved, known as the lidar equation is:\nwhere \"P(r)\" is the power of the backscattered radiation received by the lidar telescope in distance \"r\", \"E\" is transmitted laser-pulse energy, \"L\" is the lidar constant summarizing its optical and detection characteristics, \"O(r)\" is the overlap function, and formula_2 and formula_3 are the aerosol/molecular backscatter- and extinction coefficient respectively. Molecular backscatter and extinction can be derived by meteorological data, therefore the only unknowns in the lidar equation are formula_2 and formula_3. However the lidar ratio, as an intensive aerosol property, strongly depends on the size, morphology and chemical composition of the particles and is highly variable with respect to height, which often risks the extinction profile credibility. The process for calculating backscatter- and extinction coefficient profiles from EBL returns is widely known as the Klett method and was originally formalised by Hitschfeld and Bordan in 1954. \nThe aforementioned defect on estimating extinction profiles is overcome by Raman (inelastic) backscatter lidar and high spectral resolution lidar (HSRL). Raman lidar works by additionally measuring the inelastic backscatter by nitrogen and/or oxygen molecules. HSRL uses a processing approach but obtains the additional measure of molecular only backscatter at the transmitted wavelength by blocking the spectrally narrow aerosol returns and passing the spectrally broad molecular returns. These techniques provide a direct calculation of the extinction coefficient, eliminating the need for a lidar ratio assumption since any additional terms involved (e.g. the molecular extinction coefficient) are handled by meteorological (e.g. radiosoundings) and standard-atmosphere data. After some mathematical manipulations of the lidar equation the extinction-related equation reads:\n(r,{\\lambda _{inc}}) = \\frac{\\frac{\\mathrm{d}}}\\ln \\frac{N_\\mathrm{sca}}} - {\\alpha _\\mathrm{mol}}(r,{\\lambda _\\mathrm{inc}}) - {\\alpha _\\mathrm{aer}}(r,{\\lambda _\\mathrm{sca}})} {1 + {\\left( {\\frac}}} \\right)}^{\\AA (r)}},\nwhere the subscripts “inc” and “sca” refer to the incident laser light and the shifted backscattered light respectively (in HSRL these terms are the same thus further simplifying the equation, but the distinction is needed in the case of Raman lidar), N is the nitrogen / oxygen molecule number density and formula_6 is the Ångström exponent. A drawback of this method is the presence of a derivative in the resulting extinction coefficient formula () which results in potential numerical instability, introducing an active field of research.\n\nExtracting the microphysical properties of particles is motivated by the need for a deeper understanding of the effect of aerosols on climate by investigating their spatial and temporal variability. A key parameter is the distribution of the number of particles with respect to their size. Other microphysical parameters involving the characterization of aerosols are the mean (effective) radius, the total volume and surface-area concentration, the complex refractive index and the single-scattering albedo (climate forcing). While knowing the aerosol properties (forward problem) and predicting the lidar signal is a straightforward calculation, the inverse process is mathematically ill-posed (i.e., non-unique and incomplete solution space), showing a strong sensitivity on input uncertainties.\nOptical parameters can be obtained from measurements using multi-wavelength elastic-Raman lidar systems. The parameters are used as inputs to the inversion algorithms. \nThe extinction (formula_7) and backscatter (formula_8) coefficients measured by a multi-wavelength (formula_9 ) lidar is related to the number size distribution via the Fredholm integral equation of the first kind:\n(\\lambda ) = \\int\\limits_}^} (r,\\lambda ;m)n(r)dr} ,\nwhere r is particle radius, m is the complex refractive index, and ? are the kernel functions which summarize the size, shape and composition of particles. The non-linear dependence on the refractive index is usually tackled by assuming a grid of viable options. The solution space is built and further restricted by physical and/or mathematical constraints and the particle size bounds formula_10 are also pre-determined. The model Eq. () further assumes a wavelength-independent refractive index.\nThe wavelength is restricted to several discrete values depending on current technology and availability of the lidar system. The minimum optical data setup consists of 5 values, where formula_11 nm, formula_12.\nEq. () has to be discretized as it cannot be solved analytically. The theory of inverse ill-posed problems demonstrates that potential noisy components in the lidar data will cause the solution to blow up, regardless of the error level magnitude. Regularization methods are used to counteract the inherent instability of the inversion. The goal of these methods is to filter out the noisy components of the solutions, keeping at the same time as much of the solution content as possible. The ideal compromise between noise and regularity is expressed by the so-called parameter choice rules. Commonly used regularization methods are the Truncated Singular Value Decomposition, Tikhonov regularization combined with the Discrepancy Principle, the L-curve method or the Generalized Cross Validation method as a parameter choice rule. \nWhile the model Eq. () offers a reasonable approximation for almost-spherical particles (e.g. biomass burning aerosols), it no longer provides a viable description for the non-spherical case. Particle shape is known to have substantial effects for the scattering in side- and backward direction. Recent studies show that the spheroidal particle approximation is able to reproduce the optical data much more accurately than spheres.\n\nLidar systems can be used to measure concentration profiles of atmospheric gases (i.e., water vapor, ozone), and industrial emissions (i.e., SO, NO, HCl). Such measurements are performed using two basic types of lidar; Raman lidar and Differential Absorption lidars (DIAL). In the first type, the Raman lidar detects the scattering of the laser beam due to Raman scattering. The frequency shift induced by such scattering is unique for each molecule, and acts as a “signature” to detect its specific contribution. The second type, DIAL systems, emit two beams with two distinct frequencies. One beam is tuned exactly on a molecular absorption line and the other beam is tuned in a nearby wavelength without molecular absorption. By examining the intensity difference of the scattered light at the two frequencies, DIAL systems can separate the contribution of the specific molecule in the atmosphere.\n\nLidar systems can measure atmospheric temperature from the ground up to approximately 120 km using a variety of techniques, each adapted for a specific altitude range \nMeasuring temperature in the lower part of the atmosphere is typically done by taking advantage of temperature-dependent changes in molecular scattering properties. Rotational Raman systems can take advantage of the temperature-dependent intensity of the rotational Raman band of laser light scattered from reference gases like nitrogen and oxygen\n. By precisely measuring only this scattered light, such systems can determine the temperature profile up to 40 km during night and up to 12 km during day. In rare cases, DIAL systems are also used to retrieve temperature profiles, which take advantage of the temperature-dependent shape of specific molecular absorption lines.\n\nEBLs are used to derive temperature profiles from the upper atmosphere (~ 30 km to ~ 100 km). Without the presence of clouds or aerosol, the backscattered laser light from these altitudes is only due to molecular scattering. The received signal is proportional to molecular numerical density, which is in turn connected to temperature based on the ideal gas law. Temperature profiles at higher altitudes, up to 120 km, can be derived by measuring the broadening of absorption spectra of atoms of metals such as Na, Ca, K, Fe. State-of-the-art lidar systems can combine several of these techniques in one system and deliver temperature profiles from the ground up to approximately 90 km altitude \n\nLidars are capable of retrieving the complete wind vector based on the optical Doppler effect. The so-called Doppler lidars can capture the movement of molecules and particles by detecting the frequency shift of the backscattered light. In particular, supposing that the emitting radiation is at a frequency f=c/λ, where λ is the wavelength of the laser beam, for a moving target (i.e., aerosol particle or molecule) with a relative line-of-sight velocity v, the backscattered light detected by the lidar receiver has a frequency shift equal to Δf=2v/c. The particle velocity is defined where a positive line-of-sight velocity means that a target is moving towards the lidar and leads to a positive frequency shift. In literature regarding lidar applications, the line-of-sight velocity is always referred as radial velocity. The magnitude of the shift can be detected by several methods, the major being coherent and direct-detection detection techniques \nWhen aerosols are used as tracers, the strength of the return signal depends upon the aerosol load in the atmosphere and this is known to be dependent upon geographic location, the condition of the atmosphere, and the synoptic situation. The operational wavelength can be any wavelength sensitive to the underlying particle sizes. In general, aerosol return improves at lower wavelengths in the UV band. Nevertheless, the lidar signal gets more sensitive to air molecules in the UV band, and an expected aerosol-to-molecule backscatter ratio is harder to be met. Doppler lidars are usually pointed to zenith and provide vertically-resolved profiles of the vertical wind component. Scanning techniques are applied to retrieve the horizontal wind component.\nSeveral such systems are operated from the ground for applications related to e.g. airports, wind-farms, study of the Planetary Boundary Layer turbulence etc. The ADM-Aeolus satellite mission of the European Space Agency, will be the first wind lidar to operate from space.\n\nJAXA and Mitsubishi Electric are developing the SafeAvio airborne lidar to halve accidents due to clear-air turbulence.\nThe 1.9 kW, 148-kg (325-lb.) prototype has a spatial resolution of 300 m (980 ft.) and a 1-30-km (0.5-16-nmi) remote sensing range reduced to 9 km at 40,000 ft.\nIt will alert crews to tell passengers to fasten seatbelts, before developing automatic attitude control to minimize shaking.\nThe prototype was flight-tested in Boeing’s 777F EcoDemonstrator in March 2018, goals and requirements should be determined by March 2019, and a feasibility report should be completed by March 2020 before a decision to develop the system.\n\nLidars take advantage of resonance scattering in the upper atmosphere to detect metallic atoms. In such systems, the emitted laser light has to be precisely tuned in the resonance frequency of the studied species\nThe first such measurements were the detection of atomic layers of metallic Sodium (Na) in the mesopause. The same technique is now applied to detect metallic Potassium (K), Lithium (Li), Calcium (Ca), and Calcium ion (Ca ion), and Iron (Fe). These measurements provide important information in an under-studied region of the atmosphere and have helped increase the knowledge on species concentration, origin, and the complex atmospheric dynamics at these altitudes.\n\nThe planetary boundary layer (PBL) is the part of the troposphere that is directly influenced by the presence of the earth’s surface, and responds to surface forcings with a timescale of about an hour or less\n. Convective turbulent mixing processes are dominant in the mixed layer (ML) of the PBL and have a major influence on the growth and transport of atmospheric pollutants. Meteorological variables (i.e. temperature, humidity, wind) in the PBL are critically important as inputs for reliable simulations in air quality models. One of the key parameters which determine the vertical extent of the ML is the PBL height.\n\nFrom an observational perspective, PBL height has historically been measured with radiosondes but in recent years remote sensing instruments such as lidar have been utilized. Since it is well known that PBL height varies greatly in both time and space, on the order of a few meters and several minutes, radiosoundings aren’t the optimal choice for observations of PBL height. The concept of using lidar to detect PBL height relies on the assumption that there is a strong gradient in the concentration of aerosols in the ML versus the free atmosphere. An advantage of using remote sensing instruments over radiosondes for detection of the PBL height is the possibility of nearly continuous monitoring versus typical observations of twice per day from radiosondes. Continuous monitoring of PBL height will allow for a better understanding of the depth of convective turbulent processes in the ML which are a primary driver of air pollutants.\n\nThe depth of the PBL is defined as the height of the inversion level separating the free troposphere (FT) from the boundary layer. Normally at the top of the PBL, buoyancy flux reaches a minimum and large gradients of potential temperature, water vapor, and aerosols are observed. Identifying an accurate position of the depth of the PBL is essential for reliable representation of parameters in meteorological and air quality models as the PBL is the region of maximum turbulence.\nIt is well known that convective mixing processes are predominant in the PBL which in result influences the structure and composition of aerosols. Knowing the vertical extent of the convective mixing will allow a more accurate depiction of the atmosphere in the boundary layer. \nIn recent years, remote sensing instruments such as lidar have been employed to identify and observe the PBL height. An advantage to using lidar is its high-resolution temporal and vertical spatial coverage which can possibly be operated continuously and in a nearly automated status. Thus, an instantaneous PBL height can be recorded which allows more in-depth analysis such as diurnal evolution and long-term climate studies.\n\nSeveral methods have been applied to determine the PBL height from lidar observations. They are both objective and subjective methods. Objective methods consist of various forms of derivative methods, wavelet analysis methods, the variance method, and the ideal profile fitting method. Visual inspection methods are infrequently used as a subjective approach but they are not the best approach.\n\nCeilometers are a ground based Lidar optimised for measurement of cloud on the approach path of aircraft, they can also be used for PBL studies.\n\n"}
{"id": "2577287", "url": "https://en.wikipedia.org/wiki?curid=2577287", "title": "Berriasian", "text": "Berriasian\n\nIn the geological timescale, the Berriasian is an age or stage of the Early Cretaceous. It is the oldest, or lowest, subdivision in the entire Cretaceous. It spanned the time between 145.0 ± 4.0 Ma and 139.8 ± 3.0 Ma (million years ago). The Berriasian succeeds the Tithonian (part of the Jurassic) and precedes the Valanginian.\n\nThe Berriasian Stage was introduced in scientific literature by Henri Coquand in 1869. It is named after the village of Berrias in the Ardèche department of France. The largely non-marine English Purbeck Formation is in part of Berriasian age. In fact, the first rocks to be described of this age were the beds of the English Purbeck Formation, named as the Purbeckian by Alexandre Brongniart in 1829 following description by Henry De la Beche, William Buckland, Thomas Webster and William Henry Fitton.\n\nThe base of the Berriasian, which is also the base of the Cretaceous system, has traditionally been placed at the first appearance of fossils of the ammonite species \"Berriasella jacobi\". But this is a species that has a stratigraphically problematic and geographically limited distribution. A global reference profile (a GSSP) for the Berriasian has been under active consideration by the International Subcommission on Cretaceous Stratigraphy (ISCS) of IUGS. A range of contender GSSP localities has been studied in detail by the ISCS's Berriasian Working Group including localities as far apart as Mexico, Ukraine, Tunisia, Iraq and the Russian Far East. Several markers have been employed to refine correlations and to work towards defining a base for the Berriasian Stage. These include calcareous nannofossils, such as \"Nannoconus\", calpionellids, ammonites, palynological data and magnetostratigraphy, notably magnetozone M19n. The calibration of these markers, especially \"Nannoconus steinmannii minor\", \"N. kamptneri minor\", and \"Calpionella alpina\", within precisely fixed magnetozones give greater precision in trying to identify the best position for a boundary. In June 2016, the Berriasian Working Group voted to adopt \"Calpionella alpina\" as the primary marker for the base of the Berriasian Stage.\n\nIn the western part of the ocean of Tethys, the Berriasian consists of four ammonite biozones, from top to bottom (latest to earliest):\n\nThe top of the Berriasian stage is defined by the base of the Valanginian, which is fixed at the first appearance of calpionellid species \"Calpionellites darderi\". This is just a little below the first appearance of the ammonite species \"Thurmanniceras pertransiens\".\n\n\n"}
{"id": "20250538", "url": "https://en.wikipedia.org/wiki?curid=20250538", "title": "Big Coal: The Dirty Secret Behind America's Energy Future", "text": "Big Coal: The Dirty Secret Behind America's Energy Future\n\nBig Coal: The Dirty Secret Behind America's Energy Future is a book by Jeff Goodell which claims that coal mining is one of America's largest and most influential industries. Goodell suggests that coal mining is deadly and environmentally destructive.\n\n\n"}
{"id": "31908510", "url": "https://en.wikipedia.org/wiki?curid=31908510", "title": "Bootleg ground", "text": "Bootleg ground\n\nIn United States building wiring installed with separate neutral and protective ground bonding conductors ( a TN-S network), a bootleg ground (or a false ground) is a connection between the neutral side of a receptacle or light fixture and the ground lug or enclosure of the wiring device. This connects the neutral side of the receptacle to the casing of an appliance or lamp. It can be a hazard because the neutral wire is a current-carrying conductor, which means the casing can become energized. In addition, a fault condition to a bootleg ground will not trip a GFCI breaker or a receptacle that is wired from the load side of a GFCI receptacle.\n\nBefore 1996, in the United States it was common to ground the frames of 120/240-volt permanently connected appliances (such as a clothes dryer or oven) to neutral conductors. This has been prohibited in new installations since the 1996 National Electrical Code upon local adoption by legislation or regulation. Existing installations are permitted to continue in accordance with NEC 250.140 Exception.\n\nA safer alternative, allowed in recent editions of the National Electrical Code [NEC Sec. 406.4(D)(2)(b)] if a grounding connection is not practicable (where a local electrical code allows it) is to install a GFCI and leave the grounding terminal screw unconnected, then place a label that says \"No Equipment Ground\" on the GFCI and a marking that states “GFCI Protected” and “No Equipment Ground” on all downstream receptacles.\n\nIn the least-dangerous instance of a bootleg ground, a short wire jumper is connected between the bonding screw terminal (usually colored green) on a NEMA 5-15R or 5-20R outlet to the neutral (a.k.a. non-grounded conductor, colored white according to code) or directly to the white neutral wire via a pigtail. This practice is a NEC code violation, but a standard 3-lamp receptacle tester will report the outlet as correctly wired.\n\nIn the very-dangerous instance of a bootleg ground, the hot and neutral wires have been connected to the opposite terminals, and a jumper or pigtail connection is made between the green bonding screw terminal and what is believed to be the neutral circuit. But because the wiring has been crossed at some point, the hot 120 Volt wire is now connected directly to the ground on the receptacle, placing live voltage on all grounded parts of all equipment plugged into that outlet, thus allowing people to come into contact with a deadly voltage that can travel back to the source (the power transformer) through a path that does not trip either a normal circuit breaker, a GFCI, nor an AFCI soon enough to prevent electrocution.\n\nWest Germany banned bootleg grounding in 1973, although it was common practice before and can still be found in older installations.\n\n"}
{"id": "1097663", "url": "https://en.wikipedia.org/wiki?curid=1097663", "title": "Brazilian Highlands", "text": "Brazilian Highlands\n\nThe Brazilian Highlands or Brazilian Plateau () are an extensive geographical region, covering most of the eastern, southern and central portions of Brazil, in all approximately half of the country's land area, or some 4,500,000 km² (1,930,511 sq mi). In addition, the vast majority of Brazil's population (190,755,799; \"2010 census\") lives in the highlands or on the narrow coastal region immediately adjacent to it.\n\nAncient basaltic lava flows gave birth to much of the region. However, the time of dramatic geophysical activity is long past, as there is now no seismic or volcanic activity. Erosion has also played a large part in shaping the Highlands, forming extensive sedimentary deposits and wearing down the mountains.\n\nThe Brazilian Highlands are recognized for the great diversity to be found there: within the region there are several different biomes, vastly different climatic conditions, many types of soil, and thousands of animal and plant species.\n\nDue to their size and diversity, the Brazilian Highlands are usually divided into three main areas:\n\n\nIn addition to the plateau regions, several adjoining or enclosed mountain ranges are considered to be part of the Brazilian Highlands. Some of the most important are (from north to south):\n\nThe highest point of the Brazilian Highlands is the Pico da Bandeira in the Serra do Caparaó, 2,891 meters (9,485 ft).\n\n"}
{"id": "4979784", "url": "https://en.wikipedia.org/wiki?curid=4979784", "title": "CLIWOC", "text": "CLIWOC\n\nThe Climatological database for the world's oceans (CLIWOC) was a research project to convert ships' logbooks into a computerised database. It was funded by the European Union, and the bulk of the work was done between 2001 and 2003. The database draws on British, Dutch, French and Spanish ships' logbook records for the immediate pre-instrumental period, 1750 to 1850.\n\nLogbooks from the eighteenth and early nineteenth century had previously been used in case studies of individual events of historic or climatic interest.\n\nCLIWOC established early ships' logbooks as another source for those seeking to understand climate change, to be used alongside proxy and instrument data. The observations were made at local noon every single day, and cover most of the world's oceans - only the Pacific Ocean lacks detailed coverage. This volume of data was not available by any other means.\n\nIn researching the data, CLIWOC staff found that the data need to be treated with caution, and subjected to careful scrutiny. The range of information - wind force terms and directions, and general weather descriptions - is consistent between the different national sources. The data was primarily based on observations made by experienced officers.\n\nThough each book used consistent terms to refer to wind speeds, these values were not always consistent between logbooks. The researchers chose to standardise the terms into their Beaufort scale equivalents. The vocabulary used also differed between the national sources - British mariners used a relatively narrow range of terms, while sailors from the Netherlands, Spain, and France used a wider set of descriptions. Researchers found that the majority of wind force entries were accounted for by twelve or so terms, allowing the group to prepare a dictionary defining most wind force terms in use. This multi-lingual dictionary has also been published.\n\nIn order to establish the reliability of logbook records, the researchers looked at readings taken where ships were in convoy or travelling in close company. These voyages often lasted several weeks, giving large samples. The research showed that there was a consistently high degree of correlation in recorded wind forces and recorded wind directions.\n\nOn a number of occasions, the records showed small but persistent differences between absolute wind force records prepared on ships of different sizes. This was adjudged not to materially influence the scientific outcome of the project, but remains a matter for further investigation.\n\nIt was also necessary to correct the data to modern norms, both for wind speed and for wind direction - some books recorded data by reference to magnetic north, rather than true north. Precise navigational methods were not widely used until late in the study period, so it was necessary to correct latitude and longitude using specifically designed software.\n\nAn initial version of the database was released in late 2003, as a CD-ROM and via the CLIWOC website. The data structure is based on the International Maritime Meteorological Archive (IMMA) format. \n\nThe database includes information on date and time of each observation, the latitude and longitude of the recording vessel, its country of origin. wind direction and wind force. Where available - usually only towards the end of the survey period - instrumental observations are also included.\n\nAfter the CLIWOC project ended in 2003, the researchers claimed a number of successes. According to the project's website, these included:\n\n\nThe CLIWOC database is also being used as an extension of the instrument-based records contained in the I-COADS dataset.\n\nThe database was used to feed wind force and direction into statistical models, which in turn produced monthly pressure field reconstructions for the Indian and the South and North Atlantic Oceans. These models provide information for researchers into climate change over the oceans in the century from 1750. It also allows estimates of such measures as the North Atlantic Oscillation and Southern Oscillation indices for this period.\n\nBy the time the project closed in 2003, the Spanish logbooks have been almost completely studied. Over 50 per cent of Dutch and over 90 per cent of British and French logbooks remained unexamined - each of these sources contains around 100,000 observations. All sources include many observations taken at times other than local noon; these observations have not been studied in any depth.\n\nParticipating institutions included:\n\n\nCLIWOC's efforts culminated in the creation of this historical document, namely \"A Dictionary of Nautical Meteorological Terms: CLIWOC Multilingual Dictionary of Meteorological Terms; An English/Spanish/French/Dutch Dictionary of Windforce Terms Used by Mariners from 1750 to 1850\".\n\n\n"}
{"id": "6117505", "url": "https://en.wikipedia.org/wiki?curid=6117505", "title": "Calipuy National Sanctuary", "text": "Calipuy National Sanctuary\n\nThe Calipuy National Sanctuary is a Peruvian wildlife refuge created on January 8, 1981. The sanctuary, adjacent to the larger Calipuy National Reserve, is located in the Santiago de Chuco Province of the La Libertad region in northwestern Peru.\n\nThe Calipuy National Sanctuary, located high in the Andes Mountains, encompasses a variety of habitats. Prominent endangered and threatened species protected by the sanctuary include the puya and the Guanaco. \n\nPrior to the creation of the sanctuary, the land was a private plantation which was reassigned to the Agrarian Society of Social Interest, SAIS Libertad N°18 in October 1972 and the designated 3,000 hectares of land became a protected area. However, due to terrorism in the late 1980's to early 1990's, the area could no longer be protected. The area was later recovered and is now managed by National Institute of Natural Resources (INRENA) which is part of the Ministry of Agriculture.\n\nThe Calipuy National Sanctuary covers 4,500 hectares of land and is located adjacent to the Calipuy National Reserve in the Andes Mountains of northern Peru. The topography of the land varies from plains to slopes. \n\nDuring winter, temperatures here are cold with low precipitation, ranging from 280-500 mm. In summer, precipitation can reach up to 1,200 mm in the [highland|highlands]]. Rainfall varies during the rest of the year, however, the lack of climate stations near the sanctuary limits access to accurate climate measurements. Access to water from the reserve is seasonal but there are some old irrigation canals that provide water to the area.\n\nFlora: Most plants are small to medium in size and grow in rocky areas. The density of vegetation is highest along the hillsides of the sanctuary, with the main species being \"Puya raimondii\". There are 3,000 to 4,000 of these plants in the Calipuy National Sanctuary, reaching up to 6 meters tall.\n\nOther species in the sanctuary include:\n\n\nFauna: One of the purposes of creating the Calipuy National Sanctuary and national reserve was for the protection of the guanaco (\"Lama guanicoe\"). The population of this species has declined to 400-500 individuals from the 1,000 individuals that were estimated to be present in the area in 1965.\n\nOther species that are present in the sanctuary and reserve include:\n\nMammals:\n\nBirds:\nReptiles:\n\n"}
{"id": "9696639", "url": "https://en.wikipedia.org/wiki?curid=9696639", "title": "Cameroonian Highlands forests", "text": "Cameroonian Highlands forests\n\nThe Cameroonian Highlands forests are a montane tropical moist broadleaf forest ecoregion located on the range of mountains that runs inland from the Gulf of Guinea and forms the border between Cameroon and Nigeria. This is an area of forest and grassland which is becoming increasingly more populous as more and more land is cleared for agriculture.\n\nThe Cameroonian Highlands forests extend across the Cameroon Highlands, a chain of extinct volcanoes, covering an area of in eastern Nigeria and western Cameroon. The ecoregion lies above 900 meters elevation, and is surrounded at lower elevations by the Cross-Sanaga-Bioko coastal forests at the southern end of the range, and by forest-savanna mosaic along the central and northern ends of the range; the Cameroon Highlands form the boundary between the Guinean and Northern Congolian forest-savanna mosaic ecoregions. The highest mountain in the chain, Mount Cameroon is considered a separate ecoregion, while the highest peak within the region is Mount Oku (). As with similar highland areas in Angola and in East Africa the climate is cooler here than is typical of tropical Africa. The highlands are an important source of water for both Nigeria and Cameroon.\n\nThe vegetation varies with elevation. Submontane forests extend from to meters elevation. Above elevation are distinct montane forests and patches of montane grassland, bamboo forest, and subalpine grasslands and shrublands. The ecoregion is characterized by the presence of afromontane species, which have an archipelago-like distribution across the highlands of Africa. Typical afromontane species are \"Nuxia congesta, Podocarpus latifolius, Prunus africana, Rapanea melanophloeos,\" and \"Syzygium guineense bamendae\".\n\nThe ecoregion is home to a number of endemic species, along with several more that are also found in the nearby Mount Cameroon and Bioko montane forests ecoregion.\n\nSeven species of birds are strictly endemic: the Bamenda apalis (\"Apalis bamendae\"), Bangwa forest warbler (\"Bradypterus bangwaensis\"), white-throated mountain-babbler (\"Kupeornis gilberti\"), banded wattle-eye (\"Platysteira laticincta\"), Bannerman's weaver (\"Ploceus bannermani\"), Mount Kupe bush-shrike (\"Telophorus kupeensis\") and Bannerman's turaco (\"Tauraco bannermani),\" which is a cultural icon for the Kom people who live in the area. Fourteen species are endemic to the Cameroon Highlands forests and Mt. Cameroon: \"Andropadus montanus, Phyllastrephus poliocephalus, Laniarius atroflavus, Malaconotus gladiator, Cossypha isabellae\" and the subspecies \"Cisticola chubbi discolor\" (sometimes considered a separate species \"C. discolor\"). Nine more montane endemic species are shared with Mt. Cameroon and Bioko: \"Psalidoprocne fuliginosa, Andropadus tephrolaemus, Phyllastrephus poensis, Phylloscopus herberti, Urolais epichlora, Poliolais lopezi, Nectarinia oritis, Nectarinia ursulae,\" and \"Nesocharis shelleyi\".\n\nEleven small mammal species are endemic to the ecoregion: Eisentraut's striped mouse (\"Hybomys eisentrauti\"), the Mount Oku hylomyscus (\"Hylomyscus grandis\"), Mount Oku rat (\"Lamottemys okuensis\"), Mittendorf's striped grass mouse (\"Lemniscomys mittendorfi\"), Dieterlen's brush-furred mouse (\"Lophuromys dieterleni\") and Eisentraut's brush-furred rat (\"L. eisentrauti\"), Oku mouse shrew (\"Myosorex okuensis,\") Rumpi mouse shrew (\"M. rumpii\"), western vlei rat (\"Otomys occidentalis\"), Hartwig's soft-furred mouse (\"Praomys hartwigi\"), and Bioko forest shrew (\"Sylvisorex isabellae\").\n\nThe ecoregion is home to several endangered primates, including the Cross River gorilla (\"Gorilla gorilla diehli\"), an endemic subspecies of western gorilla, mainland drill (\"Mandrillus leucophaeus leucophaeus\"), Preuss's red colobus (\"Pilocolobus preussi\"), common chimpanzee (\"Pan troglodytes\") and several species of guenon including Preuss's monkey (\"Cercopithecus preussi\").\n\nForty species of amphibians are endemic to the ecoregion: \"Petropedetes parkeri\", \"Petropedetes perreti\", \"Phrynobatrachus cricogaster\", \"Phrynobatrachus steindachneri\", \"Phrynobatrachus werneri\", \"Phrynobatrachus\" species, \"Phrynodon\" species, \"Cardioglossa melanogaster\", \"Cardioglossa oreas\", \"Cardioglossa pulchra\", \"Cardioglossa schioetzi\", \"Cardioglossa trifasciata\", \"Cardioglossa venusta\", \"Astylosternus nganhanus\", \"Astylosternus perreti\", \"Astylosternus montanus\", \"Astylosternus rheophilus\", \"Leptodactylodon axillaris\", \"Leptodactylodon bicolor\", \"Leptodactylodon boulengeri\", \"Leptodactylodon erythrogaster\", \"Leptodactylodon mertensi\", \"Leptodactylodon polyacanthus\", \"Leptodactylodon perreti\", \"Afrixalus lacteus\", \"Hyperolius ademetzi\", \"Hyperolius riggenbachi\", \"Leptopelis nordequatorialis\", \"Xenopus amieti\", \"Xenopus\" species, \"Bufo villiersi\", \"Werneria bambutensis\", \"Werneria tandyi\", \"Wolterstorffina mirei\".\n\nThe following reptiles are also considered more or less endemic: \"Atractaspis coalescens\", \"Chamaeleo eisentrauti\", Pfeffer's chameleon (\"C. pfefferi\"), four-horned chameleon (\"Trioceros quadricornis\"), \"Leptosiaphos ianthinoxantha\", and Angel's five-toed skink (\"L. lepesmei\").\n\nThe forest is continually being cleared for firewood, timber and to create farmland, and many of the mountains have lost significant amounts of forest cover. There is very little formal environmental protection.\n\nIn Cameroon the mountains are quite heavily populated and used for farming and grazing; much of this ecoregion lies in the Northwest and Adamawa Regions. Towns include Bamenda, capital of the Northwest and base for visiting the mountains including Oku, the Kilum-Ijim Forest and Lake Nyos. In Nigeria the ecoregion is located mainly on the Mambila Plateau, an area of agricultural and grazing land in Taraba State.\n\n"}
{"id": "40565660", "url": "https://en.wikipedia.org/wiki?curid=40565660", "title": "Checkerboard score", "text": "Checkerboard score\n\nIn biodiversity studies, the checkerboard score or C-score is a statistic which determines the randomness of the distribution of two or more species through a collection of biomes. The statistic, first published by Stone and Roberts in 1990, expands on the earlier work of Diamond that defined a notion of \"checkerboard distributions\" as an indicator of species competition.\n\nA low c-score indicates a higher randomness, i.e. a greater likelihood that the distribution of one species has not been directly affected by the presence of other species.\n\nGiven two species \"sp\", \"sp\" and \"n\" islands, an incident matrix is built.\nIn the 2xn incident matrix, each row represents one of the two species and each column represents a different island.\nThe matrix is then filled with each cell being set to either 0 or 1. Cell with the value of 0 means that a given species doesn't exist in the given island whilst the value of 1 means that the species do exist in the given island.\n\nThe calculation of the co-occurrence of two species \"sp\", \"sp\" in the given set of islands is done as follows:\n\nThe checkerboard score (c-score) for the colonisation pattern is then calculated as the mean number of checkerboard units per species-pair in the community:\n\nFor M species, there are species-pairs, so C-score is calculated:\n\nThe C-score is sensitive to the proportion of islands that are occupied, thereby confounding comparisons between matrices or sets of species pairs within them. An extension of the C-score therefore standardizes by the number of islands each species-pair occupies using:\n"}
{"id": "2849003", "url": "https://en.wikipedia.org/wiki?curid=2849003", "title": "Chica (dye)", "text": "Chica (dye)\n\nChica is an orange-red dye obtained from boiling the leaves of the \"Arrabidaea chica\" (\"Bignonia chica\") plant. It is used by some native South American peoples to stain the skin. \n\n"}
{"id": "37724748", "url": "https://en.wikipedia.org/wiki?curid=37724748", "title": "Concealing-Coloration in the Animal Kingdom", "text": "Concealing-Coloration in the Animal Kingdom\n\n\"Concealing-Coloration in the Animal Kingdom: An Exposition of the Laws of Disguise Through Color and Pattern; Being a Summary of Abbott H. Thayer’s Discoveries\" is a book published ostensibly by Gerald H. Thayer in 1909, and revised in 1918, but in fact a collaboration with and completion of his father Abbott Handerson Thayer's major work.\n\nThe book, illustrated artistically by Abbott Thayer, sets out the controversial thesis that all animal coloration has the evolutionary purpose of camouflage. Thayer rejected Charles Darwin's theory of sexual selection, arguing in words and paintings that even such conspicuous animal features as the peacock's tail or the brilliant pink of flamingoes or roseate spoonbills were effective as camouflage in the right light.\n\nThe book introduced the concepts of disruptive coloration to break up an object's outlines, of masquerade, as when a butterfly mimics a leaf, and especially of countershading, where an animal's tones make it appear flat by concealing its self-shadowing.\n\nThe book was criticised by big game hunter and politician Theodore Roosevelt for its central assertion that every aspect of animal coloration is effective as camouflage. Roosevelt's detailed reply attacked the biased choice of examples to suit Abbott Thayer's thesis and the book's reliance on unsubstantiated claims in place of evidence. The book was more evenly criticised by zoologist and camouflage researcher Hugh Cott, who valued Thayer's work on countershading but regretted his overenthusiastic attempts to explain all animal coloration as camouflage. Thayer was mocked to a greater or lesser extent by other scientific reviewers.\n\nAbbott Thayer (1849–1921) was an American artist, known for his figure paintings, often of \"virginal, spiritual beauty\", which were sometimes, as in his most famous painting, \"Angel\", modeled on his children. He had studied at an art school in Paris, but unlike James McNeill Whistler he returned to the United States. Along with seeking timeless beauty, Thayer also became obsessed with nature, which he felt contained the pure beauty that he was seeking to capture in his paintings.\n\nThayer's close observation led him to notice what scientists such as Edward Bagnall Poulton were just beginning to describe. This was that many animals were \"painted\" the opposite way to how painters create the appearance of solidity in figures. A canvas is flat, and areas of uniform color painted on a canvas also appear flat. To make a body appear to have depth and solidity, the artist paints in shadows on the body itself. The top of an animal's back, facing the sky, remains bright, while it must become darker towards its underside. Thayer was excited to realize that by reversing such shading, nature could and did make animals appear flat. He was so passionate about this \"concealing coloration\" theory that he called it his \"second child\". Poulton had noticed countershading in certain caterpillars, but he had not realized that the phenomenon was widespread, and he championed Thayer's theory in a 1902 article in \"Nature\".\n\nHowever, Thayer was not a scientist, and he lacked a scientist's inclination to attempt to test and disprove every aspect of a new theory. Instead, Thayer came to believe that the theory belonged to artists, with their trained perception: \"The whole basis of picture making consists in contrasting against its background every object in the picture\", he argued.\n\nThe obsession led him to deny that animals could be colored for other reasons: for protection by mimicry, as the naturalist Henry Walter Bates had proposed, supported by many examples of butterflies from South America; through sexual selection, as Charles Darwin had argued, again supported by many observations. The unbalanced treatment of animal coloration in \"Concealing-Coloration in the Animal Kingdom\" encapsulates Thayer's partial understanding and his rejection of other theories.\n\nThe same obsession led him, later, to attempt to persuade the military to adopt camouflage based on his ideas, traveling to London in 1915, and writing \"passionate letters\" to the Assistant Secretary to the US Navy, Franklin Delano Roosevelt, in 1917.\n\nGerald Thayer describes the book as having two main purposes: to present Abbott Thayer's research to naturalists; and to make the subject available to a wider readership.\n\nThe book's list of contents reveals Thayer's heavy reliance on bird examples, filling 16 of the 27 chapters. Other vertebrates occupy 5 chapters. Insects receive 3 chapters, of which two are dedicated to lepidoptera - one to caterpillars, one to adult butterflies and moths; the remaining one devotes 14 pages to all other insects, starting with orthoptera including the leaf-mimic grasshoppers.\n\nThe book has 16 colored plates of paintings by Abbott Thayer and Richard S. Meryman, including the well known frontispiece \"Peacock amid foliage\", and the heavily criticised images of wood ducks, blue jays against snow, roseate spoonbills and flamingoes \"at dawn or sunset, and the skies they picture\". The last 4 colored plates are of caterpillars. Gerald Thayer claims that \"The illustrations are of particular importance, inasmuch as they include what we believe to be the first scientific paintings ever published of animals lighted as they actually are in nature\".\n\nThere are 140 black and white figures, mainly photographs with a few diagrams and drawings. Half the photographs are of birds. The photographs are from various sources, \"gleaned from periodicals, or secured by special advertising.\"\n\n\nChapter 1 sets out the \"long-ignored laws\" of \"protective coloration\", an act which \"has waited for an artist\" to perceive. Thayer explains the principle of countershading with a diagram, arguing that a naive view of being \"colored like their surroundings\" does not explain how animal camouflage works. He acknowledges the prior work of Edward Bagnall Poulton (\"The Colours of Animals\", 1890) in identifying countershading in caterpillars, quoting some passages where Poulton describes how larvae and pupae can appear flat. Countershading is named as \"the law which underlies protective coloration\", rather than as one of several principles.\n\nChapter 2 defines the book's terms, equating \"mimicry\" with \"protective resemblance\", so that it becomes a form of \"protective or disguising coloration\". Thayer distinguishes \"concealing-colors\" (mainly countershading for \"invisibility\") from the \"other\" branch of protective coloration, which includes most kinds of mimicry, for \"deceptive visibility\". The two branches are then named \"obliterative coloration\" and \"mimicry\". Mimicry is dismissed as playing \"a very insignificant part\" in the \"higher orders\", i.e. it is limited mainly to invertebrates. A fine photograph of a \"white fowl, lacking counter-shading, against a flat white cloth\" demonstrates that camouflage is more than color matching. Thayer then gives several examples of what he considers countershaded animals.\n\nChapter 3 describes the combination of markings with countershading, with photographs of a model bird and of a woodcock, showing how in the correct position these are well camouflaged with \"wonderful obliterative picture-patterns\", but wrongly positioned or upside down (with a photograph of a dead woodcock) they are easily visible.\n\nChapters 4 and 5 illustrate more \"picture-patterns\" in well camouflaged birds including Wilson's snipe and whip-poor-will (nighthawks and goatsuckers, Caprimulgidae). Thayer describes these as showing \"obliteration, or \"merging with the background\"\" but that their patterning is close to mimicry as they \"perfectly\" resemble objects such as \"a stone or mossy log\".\n\nChapter 6 argues that some birds such as the ruffed grouse have patterns designed as camouflage against distant backgrounds, with a painting of a bird against a forest background as evidence. \"The bird is in plain sight, but invisible\". For the great horned owl, a piece of the wing is \"super-imposed\" on a photograph of a wood, \"to show how closely the owl's patterns reproduce such a forest interior.\" The text describes the owl as having \"a highly developed forest-vista pattern\". Chapter 7 similarly argues for grass and heather patterns on \"terrestrial\" (as opposed to arboreal) birds. The disruptively patterned white-tailed ptarmigan is shown in \"a very remarkable photograph\" by Evan Lewis. Thayer attempts to classify the camouflage types, for example writing\n\nChapter 8 continues the theme with \"scansorial\" or tree climbing birds. Chapter 9 claims that \"obliterative shading, pure and simple, is the rule among the Shore Birds\" such as sandpipers and curlew. Chapter 10 describes the \"background-picturing\" of bitterns, birds which live in reedbeds, where\n\nChapter 11 argues (in a way that was heavily criticised when the book appeared, see below) that water birds, some of them highly conspicuous like the jacana and notoriously the male wood duck, are colored for camouflage: \"The beautifully contrasted black-and-white bars on the flanks of the Wood Duck (\"Aix sponsa\") are \"ripple pictures\", and as potent [as camouflage], in their place, as the most elaborate markings of land birds\". Chapter 12 argues that the \"pure white\" of ocean birds such as gulls and terns equally functions as camouflage. Thayer admits that these often appear conspicuous, but argues that against varied backgrounds, white offers \"the \"greatest average inconspicuousness\" against the ocean\" (his italics) or against the bright sky when seen from below.\n\nChapter 13 analyses \"markings and patterns in detail, starting with a color plate that shows the effect of disruptive patterning, which Thayer calls \"strong 'secant' and 'ruptive' patterns\". Using a photograph of an oystercatcher at its nest by Cherry and Richard Kearton, Thayer argues that the boldly marked bird (mainly black above, white below, with red beak) is both countershaded and \"ruptively\" patterned. Chapter 14 discusses the barred markings of hawks and owls, with further fine plates of photographs by the Keartons of disruptively patterned waders and their cryptic chicks. The ringed plover is described as having \"eye-masking and 'obliterative' shadow-and-hole-picturing pattern\".\n\nChapter 15 describes the leg feather patterns of hawks, asserting that these \"pantaloons\" mask these \"dangerous talons\" to facilitate attack, just as their beaks, like the beaks of wading birds, are masked paradoxically with \"gaudy colors\". Chapter 16 controversially claims that the iridescent colours of, for example, the speculum wing patch of the mallard and other ducks is \"obliterative\", the \"brightly changeable plumage\" serving to camouflage the wearer in varying conditions. Thayer asserts that such brightly colored species as the European kingfisher and the purple gallinule are camouflaged:\n\nChapter 17 argues that bird plumage has \"many devices\" to conceal the animals' outlines. Even the \"enormously developed feather-appendages\" of the birds of paradise are argued to provide camouflage in this way. Sexual display is mentioned but dismissed as not being the sole reason for the colours, outlines and patterns of the male birds. Chapter 18 briefly discusses mimicry, before returning to \"the evident paramount importance of the \"obliterative\" function\", this time of the \"brilliant, flowerlike\" heads of hummingbirds. The one case that Thayer admits is mimetic is the goatsucker of Trinidad, a plant mimic that perches \"by day and night\" on a tree stump or branch, where the purpose of the mimicry is crypsis. Chapter 19 concludes the description of bird plumage, claiming that birds from the tropical forests to the \"snowy north\", including woodpeckers and the blue jay are all \"colored for inconspicuousness\".\n\nChapters 20, 21, and 22 discuss the \"disguising-coloration\" of mammals, including the whales which \"are equipped with a full obliterative shading of surface-colors\". The bats are admitted to have very little in the way of countershading, unlike all other families in the order. Thayer notes that a few species with strong defences such as hedgehogs, porcupines, echidnas, pangolins and \"some armadillos\" are exceptions, along with some beasts which \"enjoy a like security by virtue of their gigantic bigness\", including the elephants, rhinoceroses, and hippopotami. The domestic hare is shown to be strongly countershaded with a pair of photographs \"from life\", one sitting and one \"laid on its back, outdoors, so that the obliterative shading is reversed\". Chapter 21 asserts that zebras \"must be extraordinarily inconspicuous\" against vegetation, a claim derided by Theodore Roosevelt (see below). Chapter 22 addresses the problem of the \"few [beasts] whose bold, clear patterns seem to defy that foremost obliterative law.\" These include the skunks, the African zoril (striped polecat) and the teledu (stink badger) of Java, which all have dark underparts and white upperparts. Thayer dismisses the aposematism of these species, instead asserting the effectiveness of their camouflage:\n\nSeveral photographs using stuffed skins of skunks attempt to prove the point. The chapter goes on to claim that roseate spoonbills, flamingoes, and prongbuck are all obliteratively colored. The raccoon's head resembles \"the end of a hollow stump or log\", while its tail is said to be \"distractive\", the strong banding serving like an eyespot to divert the attention of a predator to the tail rather than the head while the animal dives down a hole. But Thayer is unable to resist arguing that when \"quiet, their tail-bands act \"obliteratively\"\".\n\nChapter 23 looks at fish, admitting frankly that the authors \"know next to nothing about fishes from the standpoint of systematic science\", but saying that they have gathered a \"trustworthy general estimate\" of their \"disguising coloration\" from market stalls, museums and books. Many fish are countershaded. The bioluminescence of some deep sea fish and other animals is seen as a problem as it is not \"obliterative\"; the possibility of counterillumination camouflage is not considered.\n\nChapter 24 considers the reptiles and amphibians. These are noted to be predominantly green, often with \"ruptive\" patterns. Plate 11 treats a \"Copperhead snake on dead leaves\", the caption explaining that \"This is a bona-fide study of a Copperhead Snake among dead leaves—its normal situation.\" There is a full-page sheet of card, cut out in the shape of the snake lying on a bed of leaves. When this is folded back, a painting by Rockwell Kent and Abbott Thayer \"(Also G.H. Thayer and E.B. Thayer)\" is revealed, showing the snake's outline powerfully disrupted by its zigzag pattern among the light and shade of the leaf litter.\n\nChapter 24 mentions that some terrestrial salamanders \"are rather brightly pied with black and whitish, or yellow\", while other amphibians \"are extremely gaudy—wearing much bright blue, green, purple and sometimes red.\" It suggests that some of these markings are \"baits or targets\", again to distract predators from striking at the head, while the salamander markings are left as a problem as the authors \"know too little about the habits\" of these species. It is admitted that \"the disguising coloration of many of them is very obscure.\"\n\nThe final chapters 25, 26 and 27 turn to the insects. Chapter 25 looks at caterpillars, with, as Poulton had earlier noted, convincing examples of countershading. Plate 13 shows caterpillars including the \"larger-spotted beech-leaf-edge caterpillar\" both in position \"passing for a part of the leaf on which it is feeding\", strongly cryptic and flattened like a slightly browning leaf, and inverted, when its countershading makes it appear conspicuously solid. Chapter 26 looks at other insects and spiders, noting the \"famous leaf-mimicking \"Kallima inachus\"\" butterfly of India, but again claiming that even conspicuous butterflies are in fact \"obliterative\". Eye-spots are mentioned, but instead of noting that these might be distractive, they are asserted to be \"dazzling\", appearing as holes, and thus functioning as disruptive camouflage.\n\nThe text ends with a paragraph that asks if it is \"any wonder that artists should feel keen delight in looking at the disguising-patterns worn by animals?\" These are \"triumphs of art\", where the student can find \"in epitome, painted and perfected by Nature herself\", the typical color and pattern scheme of each kind of landscape.\n\nAn appendix provides extracts from a \"very remarkable addition to our subject\", Poulton's 1907 observations of color change in chameleons.\n\nThe Thayers' views were vigorously criticised in 1911 by Theodore Roosevelt, an experienced big game hunter and naturalist familiar with animal camouflage as well as a politician, in a lengthy article in the \"Bulletin of the American Museum of Natural History\".\n\nRoosevelt begins by writing that the Thayers expounded the \"doctrine\" of concealing coloration \"in its extreme form\", which he thought had been \"pushed to such a fantastic extreme and to include such wild absurdities as to call for the application of common sense thereto.\" Then, \"to show the sweeping claims made\", Roosevelt quotes verbatim eight passages from the book, one after the other, 500 words in all, the last one being \"'All patterns and colors whatsoever of all animals that ever prey or are preyed upon are under certain normal circumstances obliterative.'\"\n\nHe then observes that the Thayers' claims, both in \"pictures\" and in writing, are not so much arguments as plain \"misstatements of facts, or wild guesses put forward as facts.\" He puts these down to enthusiasm rather than dishonesty, and as an example critiques the picture (the book's frontispiece) of the peacock in a tree\n\nThis, Roosevelt writes, would be an extremely rare sight in nature. Worse, the female (the peahen) would, he argues, be conspicuous in those conditions. The Thayers have chosen a blue sky to argue that the peacock is camouflaged; but then they choose a \"white\" sky to allow the prongbuck's white rump to fade into that background. This, Roosevelt argues, is so dishonest that an engineer who constructed a report in that way would at once be dismissed, and the directors of a corporation who \"tried to float shares on the strength of such a report\" would be liable to \"prosecution for fraud\".\n\nRoosevelt had recently returned from his African safari, having seen, admired and shot large numbers of animals. He was scornful of Thayer's theories, which he described as \"phantasmagoria\", and the writer as \"a well meaning and ill-balanced enthusiast\". Thayer's suggestion that the white markings on the body of the harnessed bush buck are meant to resemble \"flecks of water shine\" is dismissed as wild, with the observation from personal experience that bush buck spend little time in watery places, while the \"situtunga or lechwe, which lack the spots\" spend more. Roosevelt does not refrain from harshness: he describes the camouflaged flamingo theory as \"probably the wildest\" of \"all the wild absurdities to which Mr. Thayer has committed himself\".\n\nThayer was also roundly criticised in 1911 by herpetologist Thomas Barbour and conservation pioneer John C. Phillips in \"The Auk\", where they wrote that\n\nBarbour and Phillips warmly welcome Thayer's work on countershading \"which he has so excellently demonstrated\"; they \"protest gently\" against his \"slightly patronizing\" treatment of the camouflage of birds like woodcock and grouse \"which has been known and recognized since ornithology began\"; and go on to the attack on his claims for the flamingo:\n\nThey are equally critical of his roseate spoonbill, observing that the painting looks nothing like \"actual skins of the species\". As for the wood duck, they point out its [sexual] \"dimorphism of plumage\", and that the male spends the summer in eclipse plumage, while he is\n\nBarbour and Phillips note that Thayer \"in his enthusiasm, has ignored or glossed over [sexual dimorphism] with an artistic haze.\" They also question whether every animal needs protection. \"By skilful jugglings we are shown how anything and everything may be rendered inconspicuous,\" citing the skunk among other boldly black and white animals with both the skunk coloration and the \"well-known skunk smell\". They conclude by writing that they have \"purposely omitted calling special attention to the strong features of the book\" and that they have no axe to grind.\n\nThe English ornithologists Douglas Dewar and Frank Finn write in their 1909 book \"The Making of Species\" that Thayer \"seems to be of opinion that \"all\" animals are cryptically or, as he calls it, concealingly or obliteratively coloured\". They note that Edward Bagnall Poulton had written approvingly of Thayer, and that Thayer had asserted that almost all animals were countershaded. They agree that countershading exists, but to his suggestion that it is universal \"we feel sorely tempted to poke fun at him\", and promptly ask any reader who agrees with Thayer that every animal is countershaded to look at a flock of rooks at sunset. They admit that camouflage is in general advantageous, but point out that the different plumages of seasonally and sexually dimorphic birds cannot all be explained as camouflage, considering the conspicuous colours of the male birds:\n\nThey counter the further argument that hens may be in more danger than cocks, through sitting on nests, by observing that in many dimorphic species, the showy cock shares the work of incubating the eggs.\n\nThe zoologist and camouflage expert Hugh Cott, in his 1940 book \"Adaptive Coloration in Animals\", writes that\n\nCott attacks Thayer's comprehensive assertion that \"all patterns and colors whatsoever...are obliterative\", and continues more specifically with a detailed rebuttal of both the text and Thayer's contrived paintings:\n\nCott then gives the examples of the peacock in the woods with the blue sky behind the neck; the \"flock of red Flamingoes matching a red sunset sky\", and the roseate spoonbill \"whose pink plumage matches a pink cloud scheme\". He then lists the cases of the white flamingo, the skunk and the white rump of the prongbuck, quoting Roosevelt (\"The raven's coloration is of course concealing if it is put into a coal scuttle\"), notes \"How unreasonable are extreme views like that adopted by Thayer\", and admits that criticisms of \"certain of Thayer's conclusions\" are justified, before returning to the attack on those critics, robustly defending the \"theory of protective and aggressive resemblance\".\n\nMore favourably, Cott explicitly recognises Thayer's work on countershading, though granting Edward Bagnall Poulton's partial anticipation with his work on the chrysalis of the purple emperor butterfly. Further, Cott quotes Thayer's description of countershading, and Cott's Figure 1, of countershaded fish, is captioned \"Diagrams illustrating Thayer's principle of obliterative shading\". Implicitly, also, Cott follows Thayer in his Figure 3 \"Larva of Eyed Hawk-moth\" in both \"natural (e.g. 'up-side-down')\" and \"unnatural\" positions; in his Figure 5 drawing of the disruptive effect of the stripes and bold markings of woodcock chicks (like Thayer's Figure 81); in his Plate 7, with (just like Thayer's Figure 7) a photograph of a white cock against a white background; in his Figure 18 and front cover drawings of a copperhead snake lying on a bed of leaves, with and without its disruptive pattern (like Thayer's Plate 11) and so on.\n\nThe evolutionary biologist John Endler, reviewing the topic of camouflage in \"Proceedings of the Royal Society B\" in 2006, cites Thayer's 1909 book three times: for disruption, with \"conspicuous elements [which] distract the predator's attention and break up the body outline, making detection of the prey difficult\"; for \"masquerade, [where] the prey is detected as distinct from the visual background but not recognized as edible.., for example by resembling a leaf\"; and for countershading, where \"False gradients are common in animal colour patterns, leading to misleading appearance of shape, even when they do not disrupt the body outline\". Thayer is by far the earliest source used by Endler; the only other early source he cites (for disruption) is Hugh Cott's 1940 \"Adaptive Coloration in Animals\".\n\nThe art and science writer Peter Forbes notes that Thayer became obsessed by the \"flattening effect\" of countershading, and that far from being a scientist, he was \"an artist whose idealist fervour, edged by deep insecurity, led him to regard his findings less as discovery than as revelation.\" Describing \"Concealing-Coloration\" as a \"magnum opus\", Forbes writes that by 1909 \"Thayer's prophetic intolerance was in full flood\", that he was overcompensating for his need for approval of his artwork, and that he failed to see that acceptance of ideas in science does not depend on \"the vehemence with which they are expressed\". In Forbes's view, Thayer was battling for the rights of artists over scientists, citing Thayer (\"it properly belongs to the realm of pictorial art\") in evidence. Apart from Thayer's \"bizarre\" flamingos, Forbes calls Thayer's opposition to Batesian mimicry \"extreme\". For Forbes, \"Reading Thayer's book today is a strange experience. He sets out with the idea that \"every single creature\" is perfectly camouflaged\", and then \"tries to bludgeon his readers\" into agreeing. Forbes is critical of Thayer's rejection of warning coloration, quoting Thayer's daughter Gladys as writing \"My father's special mission was \"tasting\" butterflies\"; Thayer apparently wanted to prove that mimicry was the wrong explanation as both model and mimic tasted the same. Forbes observes that natural selection did not have to contend with human reactions to the taste of butterflies.\n\nThe philosopher and jazz musician David Rothenberg, in his 2012 book \"Survival of the Beautiful\" on the relationship between aesthetics and evolution, argues that while the Thayers' book set out the principles of camouflage: \"From observation of nature ... art contributed to the military needs of society\", Thayer, following Charles Darwin, was \"swept up in the idea that every animal had evolved to perfectly live in its surroundings\", but was emotionally unable to accept the other \"half\" of Darwin's view of animal coloration:\n\nRothenberg then discusses the Thayers' account of the wood duck, which Rothenberg calls \"our most garishly colored duck\". He explains that the Thayers believed they, \"trained as artists\", had seen what earlier observers had missed:\n\nThe Smithsonian American Art Museum's website, describing the Thayers' book as \"controversial\", writes sceptically that\n\n\n"}
{"id": "33974936", "url": "https://en.wikipedia.org/wiki?curid=33974936", "title": "Damped sine wave", "text": "Damped sine wave\n\nA damped sine wave is a sinusoidal function whose amplitude approaches zero as time increases.\n\nDamped sine waves are commonly seen in science and engineering, wherever a harmonic oscillator is losing energy faster than it is being supplied.\n\nSine waves describe many oscillating phenomena. When the wave is damped, each successive peak decreases as time goes on.\n\nA true sine wave starting at time = 0 begins at the origin (amplitude = 0). A cosine wave begins at its maximum value due to its phase difference from the sinewave. In practice a given waveform may be of intermediate phase, having both sine and cosine components. The term \"damped sine wave\" describes all such damped waveforms, whatever their initial phase value.\n\nThe most common form of damping, and that usually assumed, is exponential damping, in which the outer envelope of the successive peaks is an exponential decay curve.\n\nThe general equation for an exponentially damped sinusoid may be represented as:\n\nwhere:\n\nwhich can be simplified to\n\nWhere:\n\nOther important parameters include:\n"}
{"id": "5380129", "url": "https://en.wikipedia.org/wiki?curid=5380129", "title": "Derveni papyrus", "text": "Derveni papyrus\n\nThe Derveni papyrus is an ancient Macedonian papyrus roll that was found in 1962. It is a philosophical treatise that is an allegorical commentary on an Orphic poem, a theogony concerning the birth of the gods, produced in the circle of the philosopher Anaxagoras. It was composed near the end of the 5th century BC, and \"in the fields of Greek religion, the sophistic movement, early philosophy, and the origins of literary criticism it is unquestionably the most important textual discovery of the 20th century.\" The roll itself dates to around 340 BC, during the reign of Philip II of Macedon, making it Europe's oldest surviving manuscript. While interim editions and translations were published over the subsequent years, the manuscript as a whole was finally published in 2006.\n\nThe roll was found on 15 January 1962 at a site in Derveni, Macedonia, northern Greece, on the road from Thessaloniki to Kavala. The site is a nobleman's grave in a necropolis that was part of a rich cemetery belonging to the ancient city of Lete. It is the oldest surviving manuscript in the Western tradition and one of the very few surviving papyri, and probably the oldest literary papyrus, found in Greece. The archaeologists Petros Themelis and Maria Siganidou recovered the top parts of the charred papyrus scroll and fragments from ashes atop the slabs of the tomb; the bottom parts had burned away in the funeral pyre. The scroll was carefully unrolled and the fragments joined together, thus forming 26 columns of text. It survived in the humid Greek soil, which is unfavorable to the conservation of papyri, because it was carbonized (hence dried) in the nobleman’s funeral pyre. However, this has made it extremely difficult to read, since the ink is black and the background is black too; in addition, it survives in the form of 266 fragments, which are conserved under glass in descending order of size, and has had to be painstakingly reconstructed. Many smaller fragments are still not placed. The papyrus is kept in the Archaeological Museum of Thessaloniki.\n\nThe main part of the text is a commentary on a hexameter poem ascribed to Orpheus, which was used in the mystery cult of Dionysus by the 'Orphic initiators'. Fragments of the poem are quoted, followed by interpretations by the main author of the text, who tries to show that the poem does not mean what it literally says. The poem begins with the words \"Close the doors, you uninitiated\", a famous admonition to secrecy, also quoted by Plato. The interpreter claims that this shows that Orpheus wrote his poem as an allegory. The theogony described in the poem has Nyx (Night) give birth to Heaven (Uranus), who becomes the first king. Cronus follows and takes the kingship from Uranus, but he is likewise succeeded by Zeus, whose power over the whole universe is celebrated. Zeus gains his power by hearing oracles from the sanctuary of Night, who tells him \"all the oracles which afterwards he was to put into effect.\" At the end of the text Zeus rapes his mother Rhea, which, in the Orphic theogony, will lead to the birth of Demeter. Zeus would then have raped Demeter, who would then have given birth of Persephone, who then married Dionysus. However, this part of the story must have continued in a second roll which is now lost.\n\nThe interpreter of the poem argues that Orpheus did not intend any of these stories in a literal sense, but they are allegorical in nature.\nThe first surviving columns of the text are less well preserved, but talk about occult ritual practices, including sacrifices to the Erinyes (Furies), how to remove daimones that become a problem, and the beliefs of the magi. They include a quotation of the philosopher Heraclitus. Their reconstruction is extremely controversial, since even the order of fragments is disputed. Two different reconstructions have recently been offered, that by Valeria Piano and that by Richard Janko, who says that he has found that these columns also include a quotation of the philosopher Parmenides.\n\nThe text was not officially published for forty-four years after its discovery (though three partial editions were published). A team of experts was assembled in autumn 2005 led by A. L. Pierris of the Institute for Philosophical studies and Dirk Obbink, director of the Oxyrhynchus Papyri project at the University of Oxford, with the help of modern multispectral imaging techniques by Roger Macfarlane and Gene Ware of Brigham Young University to attempt a better approach to the edition of a difficult text. However, nothing appears to have been published as a result of that initiative, and the photographs are not available to scholars or the Museum. Meanwhile, the papyrus was finally published by a team of scholars from Thessaloniki (Tsantsanoglou et al., below), which provides a complete text of the papyrus based on autopsy of the fragments, with photographs and translation. More work clearly remained to be done (see Janko 2006, below). Subsequent progress has been made in reading the papyrus by Valeria Piano and Richard Janko, who has developed a new method for taking digital microphotographs of the papyrus, which permits some of its most difficult passages to be read for the first time. Examples of these images are now published. A version of Janko's new text is available in the recent edition by Mirjam Kotwick, and a new edition in English is in preparation. A complete digital edition of the papyrus using the new technique is a major \"desideratum\".\n\nThe text of the papyrus contains a mix of dialects. It is mainly a mixture of Attic and Ionic Greek; however it contains a few Doric forms. Sometimes the same word appears in different dialectal forms e.g. cμικρό-, μικρό; ὄντα, ἐόντα; νιν for μιν etc.\n\nOn 12 December 2015, the Archaeological Museum of Thessaloniki held the official event to celebrate the registration of the Derveni Papyrus in the UNESCO Memory of the World Register.\n\nAccording to UNESCO \n\n\"The Derveni Papyrus is of immense importance not only for the study of Greek religion and philosophy, which is the basis for the western philosophical thought, but also because it serves as a proof of the early dating of the Orphic poems offering a distinctive version of Presocratic philosophers. The text of the Papyrus, which is the first book of western tradition, has a global significance, since it reflects universal human values: the need to explain the world, the desire to belong to a human society with known rules and the agony to confront the end of life.\"\n\n\n"}
{"id": "2205759", "url": "https://en.wikipedia.org/wiki?curid=2205759", "title": "Drosera (naiad)", "text": "Drosera (naiad)\n\nIn Greek mythology, Drosera (Ancient Greek: Δροσερῇ) was a naiad. She was one of the three ancestors of the Tyrians, along with Abarbarea and Callirrhoe. These nymphs were joined to sons of the soil (autochthonous) by the god Eros who was angered by their chastity.\n\nIn Nonnus' \"Dionysiaca\". Abarbarea was mentioned in the following text:\"'There, Lord Dionysos, I have told you of the soilbred race of the Earthborn, self born, Olympian, that you might know how the Tyrian breed of your ancestors sprang out of the earth. Now I will speak of the fountains. In the olden days they were chaste maidens primeval, but hot Eros was angered against their maiden girdles, and drawing a shaft of love he spoke thus to the marriage-hating nymphs: ' Naiad Abarbarea, so fond of your maidenhood, you too receive this shaft, which all nature has felt. Here I will build Callirhoe's bridechamber, here I will sing Drosera's wedding hymn ... and from his [i.e. Eros] backbent bow let fly three shots. Then in that watery bower he joined in love sons of the soil to the Naiads, and sowed the divine race of your family.'\"\n"}
{"id": "11994934", "url": "https://en.wikipedia.org/wiki?curid=11994934", "title": "Energy switching services in the UK", "text": "Energy switching services in the UK\n\nEnergy switching services are companies that have come to exist since the EU began deregulating the gas and electricity markets, to open them to competition, in 1996. Progress has been uneven across member countries, but in the UK there is now open competition among suppliers. Pricing structures and special offers are often complicated enough that it's not obvious which supplier and tariff will be best value for a consumer. This has provided an opportunity for specialist price comparison services. These are chiefly offered by companies who will manage a change to a different supplier and tariff, as well as advising on the best one. These companies primarily operate over the Web, although some also offer a telephone service.\n\nEnergy switching companies usually operate on a commission model, where they are paid a flat fee by a supplier for each customer that they persuade to switch. This has been the cause of some controversy. In order to ensure that advice remains impartial, energywatch, the UK gas and electricity watchdog, operates a voluntary code of conduct. To be accredited under the code, switching companies must satisfy Energywatch that:\n\n\n\n"}
{"id": "3518342", "url": "https://en.wikipedia.org/wiki?curid=3518342", "title": "Global Sea Level Observing System", "text": "Global Sea Level Observing System\n\nThe Global Sea Level Observing System (GLOSS) is an Intergovernmental Oceanographic Commission program whose purpose is to measure sea level globally for long-term climate change studies. The program's purpose has changed since the 2004 Indian Ocean earthquake and the program now collects realtime measurements of sea level. The project is currently upgrading the over 290 stations it currently runs, so that they can send realtime data via satellite to newly set up national tsunami centres. They are also fitting the stations with solar panels so they can continue to operate even if the mains power supply is interrupted by severe weather. The Global Sea Level Observing System does not compete with Deep-ocean Assessment and Reporting of Tsunamis as most GLOSS transducers are located close to land masses while DART's transducers are far out in the ocean.\n\n"}
{"id": "50503576", "url": "https://en.wikipedia.org/wiki?curid=50503576", "title": "Indiana University School of Public Health-Bloomington", "text": "Indiana University School of Public Health-Bloomington\n\nThe IU School of Public Health-Bloomington is an undergraduate and graduate school at Indiana University Bloomington. Until 2012, it was the School of Health, Physical Education, and Recreation (HPER). Now, the School of Public Health on IU's Bloomington campus is the largest school of public health in the Big 10. With 2,790 undergraduate and graduate students, it offers 34 different degrees. It has five academic departments.\n\nThe mission of the Indiana University School of Public Health-Bloomington is to promote health among individuals and communities in Indiana, the nation, and the world through integrated multidisciplinary approaches to research and creative activities, teaching, and community engagement.\n\nThe precursor to the School of Public Health-Bloomington, the School of Health, Physical Education, and Recreation, was founded in 1946. In 2012, the school became the School of Public Health-Bloomington. The school earned accreditation from the Council on Education for Public Health in 2015. It has the oldest Master of Public Health program, established in 1969, in the state of Indiana.\n\nA history book, titled \"A Legacy Transformed\", about the school's origins and transformation into the IU School of Public Health-Bloomington, was published in 2016.\n\nThe school offers a variety of undergraduate and graduate degrees, all with various concentrations:\n\n\nThe Master of Public Health Program (MPH) offers eight degree fields of study:\n\n\n\n\n\n"}
{"id": "39909316", "url": "https://en.wikipedia.org/wiki?curid=39909316", "title": "Keweenawite", "text": "Keweenawite\n\nKeweenawite is a discredited mineral species. It was described as an arsenide of copper, nickel, and cobalt containing 39% to 54% copper, 9.7% to 20% nickel, and 0.9% cobalt. Keweenawite was discovered in July 1901, in the Mohawk Mine, Keweenaw County, Michigan. George A Koenig analyzed and named the copper ore.\n\nKeweenawite was first discovered, in July 1901, located on the fifth level of the Mohawk Mine between Shaft No. 1 and Shaft No. 2. Fred Smith, mine superintendent, sent specimens to George A Koenig for analysis. Dr. Koenig deemed it to be a new mineral species and named it Keweenawite, after its discovery locality, Keweenaw County. \n\nHowever, a re-analysis of the material in 1971 found the keweenite to be a mixture of the copper and nickel arsenates: α-domeykite, niccolite and rammelsbergite.\n\n"}
{"id": "40255219", "url": "https://en.wikipedia.org/wiki?curid=40255219", "title": "List of Award of Garden Merit dahlias", "text": "List of Award of Garden Merit dahlias\n\nThe following is a list of dahlia cultivars which have gained the Royal Horticultural Society's Award of Garden Merit. They are tuberous perennials, originally from South America, with showy daisy-like composite flowerheads in all shades and combinations of white, yellow, orange, pink and red, flowering in late summer and autumn (fall). Much work has been done on the development of a range of flower shapes and sizes. They may be sold as dry tubers in Spring, and started off in heat before being planted out after all danger of frost has passed. Alternatively they can be purchased later in the season, in pots ready to flower. Dwarf bedding types are usually cultivated as annuals and discarded after flowering. In mild areas without penetrating frosts, mature plants can be overwintered in the garden; otherwise, they are lifted and stored in a frost free place. They are easily propagated from cuttings in Spring.\n\nFlower shapes can be divided into 14 main groups:-\n\nAs of 2015, 124 cultivars are listed. Maximum dimensions of plants are shown in centimetres.\n"}
{"id": "33077228", "url": "https://en.wikipedia.org/wiki?curid=33077228", "title": "List of Doctor Who universe creatures and aliens", "text": "List of Doctor Who universe creatures and aliens\n\nThis is a list of fictional creatures and aliens from the universe of the long-running BBC science fiction television series \"Doctor Who\", including \"Torchwood\", \"The Sarah Jane Adventures\", \"Class\", \"K-9\" and \"K-9 and Company\". It covers alien races and other fictional creatures, but not specific characters. Individual characters are listed in separate articles.\n\n\n\n"}
{"id": "5893821", "url": "https://en.wikipedia.org/wiki?curid=5893821", "title": "List of Microstigmatidae species", "text": "List of Microstigmatidae species\n\nThis page lists all described species of the spider family Microstigmatidae as of Nov. 19, 2011.\n\n\"Envia\" \n\n\"Micromygale\" \n\n\"Microstigmata\" \n\n\"Ministigmata\" \n\n\"Pseudonemesia\" \n\n\"Spelocteniza\" \n\n\"Xenonemesia\" \n\n"}
{"id": "16053511", "url": "https://en.wikipedia.org/wiki?curid=16053511", "title": "List of North Carolina weather records", "text": "List of North Carolina weather records\n\nThe following is a list of North Carolina weather records. North Carolina is located on the Atlantic Coast in the Southern United States. Since it has the Appalachian Mountains on the western part of the state, and the ocean on the east, North Carolina has experienced many different weather conditions.\n\n\n"}
{"id": "48970539", "url": "https://en.wikipedia.org/wiki?curid=48970539", "title": "List of Pholiota species", "text": "List of Pholiota species\n\nThis is a list of species in the fungal genus \"Pholiota\". , Index Fungorum accepts 370 species in \"Pholiota\".\n\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24790248", "url": "https://en.wikipedia.org/wiki?curid=24790248", "title": "List of Roman dams and reservoirs", "text": "List of Roman dams and reservoirs\n\nThis is a list of Roman dams and reservoirs. The study of Roman dam-building has received little scholarly attention in comparison to their other civil engineering activities, even though their contributions in this field have been ranked alongside their expertise in constructing the well-known Roman aqueducts, bridges, and roads.\n\nRoman dam construction began in earnest in the early imperial period. For the most part, it concentrated on the semi-arid fringe of the empire, namely the provinces of North Africa, the Near East, and Hispania. The relative abundance of Spanish dams below is due partly to more intensive field work there; for Italy only the Subiaco Dams, created by emperor Nero (54–68 AD) for recreational purposes, are attested. These dams are noteworthy, though, for their extraordinary height, which remained unsurpassed anywhere in the world until the Late Middle Ages.\n\nThe most frequent dam types were earth- or rock-filled embankment dams and masonry gravity dams. These served a wide array of purposes, such as irrigation, flood control, river diversion, soil-retention, or a combination of these functions. In this, Roman engineering did not differ fundamentally from the practices of older hydraulic societies. \n\n\"The Romans' ability to plan and organise engineering construction on a grand scale\" gave their dam construction special distinction. Their engineering prowess, therefore, facilitated the construction of large and novel reservoir dams, which secured a permanent water supply for urban settlements even during the dry season, a common concept today, but little-understood and -employed in ancient times. \n\nThe impermeability of Roman dams was increased by the introduction of waterproof hydraulic mortar and especially \"opus caementicium\" in the Concrete Revolution. These materials also allowed for bigger structures to be built, like the Lake Homs Dam, possibly the largest water barrier to date, and the sturdy Harbaqa Dam, both of which consist of a concrete core.\n\nOn the whole, Roman dam engineering displayed a high degree of completeness and innovativeness. While hitherto dams relied solely on their heavy weight to resist the thrust of water, Roman builders were the first to realize the stabilizing effect of arches and buttresses, which they integrated into their dam designs. Previously unknown dam types introduced by the Romans include:\n\nThe origin of the so-called weir bridges, which were to become a popular design in Iran thereafter, can also be traced to the forced labour of Roman prisoners of war (see Band-e Kaisar). \n\nThis list is sorted by maximum height. All measurements are in m; in case of differing values, more recent respectively more detailed studies were given preference. In earth dams, thickness refers to the masonry wall. \n\n\n\n\n\n \n"}
{"id": "23447603", "url": "https://en.wikipedia.org/wiki?curid=23447603", "title": "List of Sphagnum species", "text": "List of Sphagnum species\n\nAround 380 species are currently recognised in the peat-moss genus \"Sphagnum\":\n"}
{"id": "56350369", "url": "https://en.wikipedia.org/wiki?curid=56350369", "title": "List of asteroid close approaches to Earth in 2018", "text": "List of asteroid close approaches to Earth in 2018\n\nBelow is the list of asteroid close approaches to Earth in 2018.\n\nA list of known near-Earth asteroid close approaches less than 1 lunar distance () from Earth in 2018.\n\nFor reference, the radius of Earth is approximately or 0.0166 lunar distances.Geosynchronous satellites have an orbit with semi-major axis length of or 0.110 lunar distances. Seven known asteroids (2018 BD, , 2018 UA, , A107j4p, A106fgF, and ) are confirmed to have passed within this distance, and poorly-observed asteroids ZB0A262, , ZGBE54F, A1080DC, and ZW900BE may have also passed within this distance.\nWhile most asteroids on this list are confirmed, well-observed unconfirmed objects with a 50% or greater chance of passing within 1 LD of the Earth are included as well.\n\nThe number of asteroids listed here are significantly less than those of asteroids that approach Earth for several reasons. Asteroids that approach Earth not only move faster, but are brighter and are easier to detect with modern surveys because:\n\nThese factors severely limit the amount of Moon-approaching asteroids, to a level many times lower than the asteroids detected passing as close to Earth.\nAn example list of near-Earth asteroids that passed or will pass more than 1 lunar distance (384,400 km or 0.00256 AU) from Earth in 2018.\n\nList of asteroids that are listed on the Sentry Risk Table because they have short observation arcs with poorly constrained orbits and have a chance of impacting Earth in 2018. Given a short observation arc, many different orbits fit the observed data. These objects could be millions if not billions of kilometers from Earth on the date of a low probability virtual impactor. For example, is expected to be from Earth in December 2018 around the time of the 3 virtual impactors. 2010 GZ60 was removed from the sentry table in February 2018 after further observations were found by NEOWISE, ruling out any possible impacts.\n\nCumulatively among the asteroids listed below, there is a roughly 1 in 98,700 chance that any of the asteroids will impact Earth in 2018. Most of this comes from asteroid 2008 US which is only ~2 meters in diameter and had a 1 in 240,000 chance of impact on 18 April 2018.\n\n\n"}
{"id": "903139", "url": "https://en.wikipedia.org/wiki?curid=903139", "title": "List of national parks in Malaysia", "text": "List of national parks in Malaysia\n\nMalaysia has a number of national parks but most of them are \"de facto\" state parks.\n\nThis page provides the list of protected areas and pictures associated with the facilities and activities available in each area.\n\nAll parks and reserves in Peninsular Malaysia are under the jurisdiction of the Department of Wildlife and National Parks of Malaysia.\n\n\n\n\n\n\n\n\nNational Parks and other conservation areas in Sarawak are under the governance of the Sarawak Forestry Corporation \n\nNational or state parks in Sabah are managed by Sabah Parks. Other reserves or protected areas are under the governance of the Sabah Forestry Department and Sabah Foundation.\n\n\n"}
{"id": "36927324", "url": "https://en.wikipedia.org/wiki?curid=36927324", "title": "List of nature centers in Illinois", "text": "List of nature centers in Illinois\n\nThis is a list of nature centers in the U.S. state of Illinois. \n\n\n\n"}
{"id": "9441569", "url": "https://en.wikipedia.org/wiki?curid=9441569", "title": "List of rare flora of the Warren region", "text": "List of rare flora of the Warren region\n\nThis is a list of endangered flora of the Warren region, a biogeographic region in southern Western Australia. It includes all taxa that occur in the region, and that have been classified as \"R: Declared Rare Flora - Extant Taxa\" or \"X: Declared Rare Flora - Presumed Extinct Taxa\" under the Department of Environment and Conservation's Declared Rare and Priority Flora List, and are hence gazetted as endangered extant flora under the Wildlife Conservation Act 1950.\n\nThere are 28 endangered taxa. \"Leucopogon cryptanthus\" is presumed extinct. The other 27 are believed extant:\n"}
{"id": "6600588", "url": "https://en.wikipedia.org/wiki?curid=6600588", "title": "List of stars in Antlia", "text": "List of stars in Antlia\n\nThis is the list of notable stars in the constellation Antlia, sorted by decreasing brightness.\n\n"}
{"id": "10932926", "url": "https://en.wikipedia.org/wiki?curid=10932926", "title": "Marine conservation activism", "text": "Marine conservation activism\n\nMarine Conservation Activism refers to the efforts of non-governmental organizations and individuals to bring about social and political change in the area of marine conservation. Marine conservation is properly conceived as a set of management strategies for the protection and preservation of ecosystems in oceans and seas. Activists raise public awareness and support for conservation, while pushing governments and corporations to practice sound ocean management, create conservation policy, and enforce existing laws and policy through effective regulation. There are many different kinds of organizations and agencies that work toward these common goals. They all are a part of the growing movement that is ocean conservation. These organizations fight for many causes including stopping pollution, overfishing, whaling, by-catching, and Marine Protected Areas.\n\nThough the environmental movement began in the United States during the 1960s, the idea of marine conservation really did not take off in the country until the 1972 Marine Protection, Research, and Sanctuaries Act (MPRSA) passed, beginning the movement. The act allowed the regulation by the United States Environmental Protection Agency (EPA) over dumping in the seas. Though the act was later amended, it was one of several key events to bring marine issues towards the front of environmental issues in the United States.\n\n\nMarine Debris is defined as “any persistent solid material that is manufactured or processed and directly or indirectly, intentionally or unintentionally, disposed of or abandoned into the marine environment or the Great Lakes”. This debris can injure or even kill marine organisms; it can also interfere with navigation safety and could pose a threat to human health. Marine debris can range from soda cans to plastic bags and can even include abandoned vessels or neglected fishing gear.\n\nOcean Conservancy is a non-profit environmental group that fights for the improvement and conservation of marine ecosystems and marine life. They work to find science-based solutions to protect the world's oceans from the global challenges that they face today.\n\nOne of the many issues that they work closely to stop is the flow of trash that enters the ocean. The International Coastal Cleanup (ICC) is one of the methods Ocean Conservancy uses to prevent marine debris. The ICC is the largest volunteer effort to clean up the world's oceans and other waterways; over the past 25 years the ICC has cleaned up approximately 144,606,491 pounds of trash from beaches all over the world.\n\nWhaling is the hunting of free roaming whales; many whaling practices have led to drastic population loss in many whale populations around the world. In 1986, The International Whaling Commission (IWC) was founded to put a ban on commercial whaling. The commission recognizes three different types of whaling: aboriginal subsistence, commercial, and special permit (or scientific) whaling.\n\nThis form of whaling supports indigenous communities where whale products play an important role in cultural and nutritional life. The IWC sets catch limits for aboriginal subsistence whaling every six years.\n\nThis form of whaling is highly regulated by the IWC and is currently on a moratorium. There are a few countries that oppose the moratorium and continue to hunt for whales; these countries share catch data with the Commission but are not regulated by it.\n\nSince the moratorium was put in place in 1986, more than 50,000 whales have been hunted and killed; there are three nations that are still able to hunt whales because of loopholes in the ban. Norway is able to hunt because of an “objection” to the ban; Iceland is able to hunt because of a “reservation” and Japan is able to hunt because they claim it is for “research purposes”. If combined these nations kill around 2,000 whales each year; these whales include humpback, minke, sperm, fin, Bryde’s, and sei. The IWC ban does allow for some Aboriginal Subsistence Whaling (ASW) in certain countries.\n\nThis category of whaling is separated from IWC-regulated whaling by international law. Special permit research proposals are to be submitted by countries to the IWC for scientific scrutiny. The role of the IWC is advisory only.\n\nGreenpeace, an international environmental organization founded in 1971 in British Columbia, fights against whaling. Their campaigns are nonviolent and many times involve one or more of the five Greenpeace ships which first made the organization famous in the 1970s. In late December 2005, Japanese whaling fleets experienced heavy opposition from Greenpeace, who protested that the Japanese were continuing their commercial whaling under the guise of research, and even worse, they were doing so in the Southern Ocean Whale Sanctuary. They sent volunteer workers in inflatable boats to get in the line of fire in order to stop the whaling.\n\nSea Shepherd Conservation Society is a non-profit, marine wildlife conservation organization that works internationally on numerous campaigns to protect the world’s oceans. Their mission is to conserve and protect the world’s ecosystems and species; they work to end the destruction of habitat and slaughter of the ocean’s wildlife. Unlike many other non-profit environmental groups, Sea Shepherd uses direct-action tactics to expose and challenge illegal activities at sea; they strive to ensure that the ocean can survive for future generations. In doing so, they refer to the United Nations World Charter for Nature that calls on individuals to \"safeguard and conserve nature in areas beyond national jurisdiction\".\n\nSea Shepherd was founded in 1977 by Captain Paul Watson in Vancouver BC, Canada; it wasn’t until 1981 that it was formally incorporated in the United States. Throughout the years their campaigns have ranged from stopping the annual killing of baby harp seals in Eastern Canada to preventing Japanese whalers from killing endangered whale species. They only work to uphold international conservation law and to protect the endangered ocean habitats and species; they do this without prejudice against race, nationality, color, or religious belief. Their crews are made up of volunteers from all over the world, some of which are from countries that Sea Shepherd has campaigns against; they describe themselves as “pro-ocean” instead of “anti-any nationality or culture”.\n\nShark finning is a worldwide issue that involves cutting off the fins of sharks. This is done while the shark is still alive followed by the rest of the body being thrown back into the ocean, leaving it to die days after. Used in countries like China and Japan, shark fins are a key ingredient in the world-renowned meal, shark fin soup. The high demand for this particular type of soup has skyrocketed in the last few decades and sells for around $100 on average and is often catered at special occasions such as weddings and banquets. Due to the increased want for these shark fins, traders seek out the fins in order to make a profit. However, the fins are the only part of the shark that fishermen seek out to retrieve due to the low economical value of the actual shark meet. This recently exposed issue along with other overfishing issues has brought upon roughly 80 percent of the shark population decline. It has become prominent concern in marine conservation activism for millions of sharks are killed yearly at an often-unregulated expense.\n\nCurrent campaign known as Project AWARE is working globally to advocate solutions for long-term protection for these animals. Created initially as an environmental initiative project, this campaign was developed by the Professional Associations of Diving Instructors (PADI) in 1989. Used to educate divers about environmental problems this program eventually grew to become a registered non-profit organization in the US in 1992 and eventually became recognized in the UK and Australia in 1999 and 2002 respectively. In spite of the arising issues with marine challenges, Project AWARE has continued to grow towards meeting the needs of the marine ecosystem as they see fit. Marine debris and shark and ray conservation activism are the two most prevalent issues that are being further worked toward improving since 2011.\n\nAnother huge campaign working to ensure the protection of these marine species is a group called Shark Savers that is sponsored by the group called WildAid. Through the use of community motivation, the project encourages the public to stop eating sharks and shark fin soup. By also working to improve global regulations and creating sanctuaries for sharks, the project aims to take action and get results. Similarly to Project Aware, the Shark Savers program was founded by a group of divers that wanted to help the marine system in 2007. Through the recent creation of shark sanctuaries, the program focuses on sustainability when thinking about the economical and environmental benefits. These created sanctuaries provide a protected area for the sharks and also promote change in nearby communities.\n\nBite-Back is another organization that is active in the community and aims to stop the sale of shark fins for the making of shark fin soup in Great Britain. By exposing the UK and their acts toward profiting from shark products, they aim to put an end to their ways of over fishing and exploitation. The organizations main goal is to allow marine life a chance to thrive while they are busy doing the dirty work of lowering consumer stipulation.\n\nPart of a worldwide alliance called The Global Shark and Ray Initiative (GSRI), the Shark Trust is working in efforts to better the ocean for marine animals such as shark and rays. The Initiative created a plan for changing the status of the shark population that would span over 10 years starting on February 15, 2016. Teaming up with other large conservation organization such as Shark Advocates International and World Wide Fund for Nature (WWF), the team aims to ultimately give these vulnerable animals the safety and security that the ought to have in their natural environment.\n\nOverfishing occurs when fish stocks are over-exploited to below acceptable levels; eventually the fish populations will no longer be able to sustain themselves. This can lead to resource depletion, reduced biological growth, and low biomass levels.\n\nIn September 2016, a partnership of Google and Oceana and Skytruth introduced Global Fishing Watch, a website designed to assist citizens of the globe in monitoring fishing activities.\n\nEven though the idea of Marine Protected Areas is an internationally known concept, there is no one term used internationally. Rather, each country has its own name for the areas. Marine Reserves, Specially Protected Areas, Marine Park all relate to this concept, though differ slightly. Some of the most famous Marine Protected Areas are the Ligurian Sea Cetacean Sanctuary along the coasts of Spain, Monaco, and Italy, and Australia's Great Barrier Reef. The largest sanctuary in the world is the Northwestern Hawaiian Islands National Monument. The purpose of these sanctuaries is to provide protection for the living and non- living resources of the oceans and seas. They are created to save species, nursing resources and to help sustain the fish population.\n\nThe activists at the Ocean Conservancy fight for this cause. They believe that the United States should put forth a consistent and firm commitment in using Marine Protected Areas as a management strategy. Currently, the argument in the United States is whether or not they are necessary, when it should be how can they work the most efficiently. Activists at the Ocean Conservancy have been working on a campaign called the \"Save Our Ocean Legacy,\" a campaign lasting several years trying to establish Marine Protected Areas' off of the California coasts. 29 Marine Protected Areas were planned to be established when the legislation bill passed in 1999. The hope is that the plan will be finalized in 2007.\n\nSome fishers do not accept that Marine Protected Areas (MPA) benefit fish stocks and provide insurance against stock collapse. MPAs can cause a short-term loss in fisheries production. However, the concept of spillover, where fish within a Marine Protected Areas (MPAs) move into fished areas, thus benefiting fisheries, has been misunderstood by some fishers. The term is a simplification of numerous ecological benefits that are derived from removing fishing from nursery, breeding grounds and essential fish habitats.\n"}
{"id": "1739569", "url": "https://en.wikipedia.org/wiki?curid=1739569", "title": "Mass-independent fractionation", "text": "Mass-independent fractionation\n\nMass-independent isotope fractionation or Non-mass-dependent fractionation (NMD), refers to any chemical or physical process that acts to separate isotopes, where the amount of separation does not scale in proportion with the difference in the masses of the isotopes. Most isotopic fractionations (including typical kinetic fractionations and equilibrium fractionations) are caused by the effects of the mass of an isotope on atomic or molecular velocities, diffusivities or bond strengths. Mass-independent fractionation processes are less common, occurring mainly in photochemical and spin-forbidden reactions. Observation of mass-independently fractionated materials can therefore be used to trace these types of reactions in nature and in laboratory experiments.\n\nThe most notable examples of mass-independent fractionation in nature are found in the isotopes of oxygen and sulfur. The first example was discovered by Robert N. Clayton, Toshiko Mayeda, and Lawrence Grossman in 1973, in the oxygen isotopic composition of refractory calcium-aluminium-rich inclusions in the Allende meteorite. The inclusions, thought to be among the oldest solid materials in the Solar System, show a pattern of low O/O and O/O relative to samples from the Earth and Moon. Both ratios vary by the same amount in the inclusions, although the mass difference between O and O is almost twice as large as the difference between O and O. Originally this was interpreted as evidence of incomplete mixing of O-rich material (created and distributed by a large star in a supernova) into the Solar nebula. However, recent measurement of the oxygen-isotope composition of the Solar wind, using samples collected by the Genesis spacecraft, shows that the most O-rich inclusions are close to the bulk composition of the solar system. This implies that Earth, the Moon, Mars and asteroids all formed from O- and O-enriched material. Photochemical dissociation of carbon monoxide in the Solar nebula has been proposed to explain this isotope fractionation.\n\nMass-independent fractionation also has been observed in ozone. Large, 1:1 enrichments of O/O and O/O in ozone were discovered in laboratory synthesis experiments by John Heidenreich and Mark Thiemens in 1983, and later found in stratospheric air samples measured by Konrad Mauersberger. These enrichments were eventually traced to the three-body ozone formation reaction.\n\nTheoretical calculations by Rudolph Marcus and others suggest that the enrichments are the result of a combination of mass-dependent and mass-independent kinetic isotope effects (KIE) involving the excited state O* intermediate related to some unusual symmetry properties. For formation of ozone substituted with a heavy oxygen atom at the terminal position of the molecule, a highly zero-point energy difference sensitive KIE creates large enrichments for the differences in mass between O, O, and O. For formation of ozone substituted with a heavy oxygen atom at the central position of the molecule (or unsubstituted), the relatively short lifetime the O* intermediate does not allow a statistical distribution of energy throughout all the degrees of freedom, resulting in a mass-independent distribution of isotopes.\n\nMass-independent fractionation of sulfur can be observed in ancient sediments, where it preserves a signal of the prevailing environmental conditions. The creation and transfer of the mass-independent signature into minerals would be unlikely in an atmosphere containing abundant oxygen, constraining the Great Oxygenation Event to some time after . Prior to this time, the MIS record implies that sulfate-reducing bacteria did not play a significant role in the global sulfur cycle, and that the MIS signal is due primarily to changes in volcanic activity.\n\n"}
{"id": "21479121", "url": "https://en.wikipedia.org/wiki?curid=21479121", "title": "National Council for Energy Policy", "text": "National Council for Energy Policy\n\nThe Brazilian National Council of Energy Policy (CNPE, ) was created by the law no. 9.478/1997, also known as Petroleum Law. The council is the governmental organization of Brazil responsible for advising the Presidency of the Republic and has the objective of elaborating policies for the electric sector of the country.\n\nCNPE is formed by state government representatives, experts in energy, non-governmental organizations and seven ministers.\n"}
{"id": "55629898", "url": "https://en.wikipedia.org/wiki?curid=55629898", "title": "Neeliyar Kottam", "text": "Neeliyar Kottam\n\nNeeliyar Kottam is a sacred grove in Kannur district, Kerala, India, situated at Mangattuparamba near Dharmasala. At present, this 20.18 Acre sacred grove is controlled by members of Cheriya Veedu family belonging to the Kulala community.\n\nNeeliyamma or Kottathamma (Mother of the grove) is the deity here. According to the lore, a wise and beautiful woman, Neeli, turned in to a bloodthirsty demon after a local king betrayed and murdered her. But ever since a man took her for a compassionate mother, she turned into a motherly form. The Goddess came along with a priest of Kalikattu illam when he was returning from Kottiyoor. Goddess then asked to be installed in this jungle location where the leopards and cattle were seen living together in peace\n\nNeeliyar Kottam still preserves the jungle surroundings. Its sanctum is not roofed. When the ritualistic Theyyam performance is seasonal elsewhere, the belief is that the Goddess graces the Neeliyar Kottam grooves whenever a devotee makes an offering. The Theyyam costume consists of large blood-red clothes, with a 20 ft bamboo framed headgear and traditional Theyyam ornaments. Artists of Vannan community performs the Theyyam of Neeliyar Bhagavathi or the Ottathira\n\nThe sacred grove is situated at a small hillock. It is a midland sacred grove. Variety of species of plants including Angiosperms, Gymnosperms, Pteridophytes, Bryophytes and Lichens are present here. Memecylone species is the main tree present here. There are also trees like \"Madhuca longifolia\", ,\"Elaeocarpus serratus\". In the upper part of the grove where there is frequent human intervention, \"Syzygium caryophyllatum\" and \"Acacia auriculiformis\" plants are seen growing. Some rare medicinal plants and orchids like \"Rhynchostylis retusa\" and Kingidium deliciosum(\"Phalaenopsis deliciosa\") are present. There is a small stream originating from the groove during monsoon season. is seen in the grove, the fruits of which feed birds and small animals of the grove.\n\n"}
{"id": "24610803", "url": "https://en.wikipedia.org/wiki?curid=24610803", "title": "Nickel–lithium battery", "text": "Nickel–lithium battery\n\nThe nickel–lithium battery, also known as Ni–Li, is an experimental battery using a nickel hydroxide cathode and lithium anode. The two metals cannot normally be used together in a battery, as there are no electrolytes compatible with both. The LISICON design uses a layer of porous glass to separate two electrolytes in contact with each metal. The battery is predicted to hold more than three and a half times as much energy per pound as lithium-ion batteries, and to be safer. However, the battery will be complex to manufacture and durability issues have yet to be resolved.\n"}
{"id": "42780", "url": "https://en.wikipedia.org/wiki?curid=42780", "title": "Nordic Council", "text": "Nordic Council\n\nThe Nordic Council is the official body for formal inter-parliamentary co-operation among the Nordic countries. Formed in 1952, it has 87 representatives from Denmark, Finland, Iceland, Norway, and Sweden as well as from the autonomous areas of the Faroe Islands, Greenland, and the Åland Islands. The representatives are members of parliament in their respective countries or areas and are elected by those parliaments. The Council holds ordinary sessions each year in October/November and usually one extra session per year with a specific theme.\n\nIn 1971, the Nordic Council of Ministers, an intergovernmental forum, was established to complement the Council. The official and working languages of both the Nordic Council and the Nordic Council of Ministers are Danish, Norwegian, and Swedish, which comprise the first language of around 80% of the region's population and learned as a foreign language by the remaining 20%.\n\nThe Nordic Council and the Nordic Council of Ministers are involved in various forms of cooperation with neighbouring areas, amongst them being the Baltic Assembly and the Benelux, as well as Russia and Schleswig-Holstein.\n\nDuring World War II, Denmark and Norway were occupied by Germany; Finland was under assault by the Soviet Union; while Sweden, though neutral, still felt the war's effects. Following the war, the Nordic countries pursued the idea of a Scandinavian defence union to ensure their mutual defence. However, Finland, due to its Paasikivi-Kekkonen policy of neutrality and FCMA treaty with the USSR, could not participate.\n\nIt was proposed that the Nordic countries would unify their foreign policy and defence, remain neutral in the event of a conflict and not ally with NATO, which some were planning at the time. The United States, keen on getting access to bases in Scandinavia and believing the Nordic countries incapable of defending themselves, stated it would not ensure military support for Scandinavia if they did not join NATO. As Denmark and Norway sought US aid for their post-war reconstruction, the project collapsed, with Denmark, Norway and Iceland joining NATO.\n\nFurther Nordic co-operation, such as an economic customs union, also failed. This led Danish Prime Minister Hans Hedtoft to propose, in 1951, a consultative inter-parliamentary body. This proposal was agreed by Denmark, Iceland, Norway, and Sweden in 1952. The Council's first session was held in the Danish Parliament on 13 February 1953 and it elected Hans Hedtoft as its president. When Finnish-Soviet relations thawed following the death of Joseph Stalin, Finland joined the council in 1955.\n\nOn 2 July 1954, the Nordic labour market was created and in 1958, building upon a 1952 passport-free travel area, the Nordic Passport Union was created. These two measures helped ensure Nordic citizens' free movement around the area. A Nordic Convention on Social Security was implemented in 1955. There were also plans for a single market but they were abandoned in 1959 shortly before Denmark, Norway, and Sweden joined the European Free Trade Area (EFTA). Finland became an associated member of EFTA in 1961 and Denmark and Norway applied to join the European Economic Community (EEC).\n\nThis move towards the EEC led to desire for a formal Nordic treaty. The Treaty of Helsinki outlined the workings of the Council and came into force on 24 March 1962. Further advancements on Nordic cooperation were made in the following years: a Nordic School of Public Health, a Nordic Cultural Fund, and Nordic House in Reykjavík were created. Danish Prime Minister Hilmar Baunsgaard proposed full economic cooperation (\"Nordek\") in 1968. Nordek was agreed in 1970, but Finland then backtracked, stating that its ties with the Soviet Union meant it could not form close economic ties with potential members of the EEC (Denmark and Norway). Nordek was then abandoned.\n\nAs a consequence, Denmark and Norway applied to join the EEC and the Nordic Council of Ministers was set up in 1971 to ensure continued Nordic cooperation. In 1970 representatives of the Faroe Islands and Åland were allowed to take part in the Nordic Council as part of the Danish and Finnish delegations. Norway turned down EEC membership in 1972 while Denmark acted as a bridge builder between the EEC and the Nordics. Also in 1973, although it did not opt for full membership of the EEC, Finland negotiated a free trade treaty with the EEC that in practice removed customs duties from 1977 on, although there were transition periods up to 1985 for some products. Sweden did not apply due to its non-alliance policy, which was aimed at preserving neutrality. Greenland subsequently left the EEC and has since sought a more active role in circumpolar affairs.\n\nIn the 1970s, the Nordic Council founded the Nordic Industrial Fund, Nordtest and the Nordic Investment Bank. The Council's remit was also expanded to include environmental protection and, in order to clean up the pollution in the Baltic Sea and the North Atlantic, a joint energy network was established. The Nordic Science Policy Council was set up in 1983 and, in 1984, representatives from Greenland were allowed to join the Danish delegation.\n\nFollowing the collapse of the Soviet Union in 1991, the Nordic Council began to cooperate more with the Baltic states and new Baltic Sea organisations. Sweden and Finland joined the European Union (EU), the EEC's successor, in 1995. Norway had also applied, but once again voted against membership. However, Norway and Iceland did join the European Economic Area (EEA) which integrated them economically with the EU. The Nordic Passport Union was also subsumed into the EU's Schengen Area in 1996.\n\nThe Nordic Council became more outward-looking, to the Arctic, Baltic, Europe, and Canada. The Øresund Bridge linking Sweden and Denmark led to a large amount of cross-border travel, which in turn led to further efforts to reduce barriers. However, the initially envisioned tasks and functions of the Nordic Council have become partially dormant due to the significant overlap with the EU and EEA. In 2008 Iceland began EU membership talks, but decided to annul these in 2015.\n\nThe Nordic Council and the Nordic Council of Ministers have a particular focus on strengthening the Nordic language community; the main focus of their work to promote language understanding in the Nordic countries is on children and young people's understanding of written and oral Danish, Norwegian, and Swedish, the three mutually intelligible Scandinavian languages. Representatives of the Council have the ability to issue proposals in their own languages, and official documents are translated to cater to all five of the major Nordic languages. Discussions for bringing Finnish and Icelandic into equal footing with the three other languages has been proposed.\n\nThe Nordic Council consists of 87 representatives, elected from its members' parliaments and reflecting the relative representation of the political parties in those parliaments. It holds its main session in the autumn, while a so-called \"theme session\" is arranged in the spring. Each of the national delegations has its own secretariat in the national parliament. The autonomous territoriesGreenland, the Faroe Islands and Ålandalso have Nordic secretariats.\n\nThe Nordic Council uses the three Continental Scandinavian languages (Danish, Norwegian, and Swedish) as its official working languages, but also publishes material in Finnish, Icelandic, and English for information purposes. The council refers to Danish, Norwegian, and Swedish collectively as Scandinavian and considers them to be different forms of the same language forming a common language community. Since 1987, under the Nordic Language Convention, citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable to any interpretation or translation costs. The Convention covers visits to hospitals, job centres, the police and social security offices. The languages included are Swedish, Danish, Norwegian, Finnish, and Icelandic.\n\nThe Council does not have any formal power on its own, but each government has to implement any decisions through its national legislature. With Denmark, Norway, and Iceland being members of NATO and Finland and Sweden being neutral, the Nordic Council has not been involved in any military cooperation.\n\nThe original Nordic Council concentrates on inter-parliamentary cooperation. The \"Nordic Council of Ministers\", founded in 1971, is responsible for inter-governmental cooperation. Prime Ministers have ultimate responsibility but this is usually delegated to the Minister for Nordic Cooperation and the Nordic Committee for Co-operation, which co-ordinates the day-to-day work. The autonomous territories have the same representation as states.\n\n\nThe Nordic Council and the Council of Ministers have their headquarters in Copenhagen and various installations in each separate country, as well as many offices in neighbouring countries. The headquarters are located at Ved Stranden No. 18, close to Slotsholmen.\n\nThe Nordic Council has 8 members, 5 sovereign states and 3 self-governing regions.\n\nIn accordance with § 13 of the Rules of Procedure for the Nordic Council the Sámi Parliamentary Council is the only institution with observer status with the Nordic Council. In accordance with § 14, the Nordic Youth Council has the status of \"guest\" on a permanent basis, and the Presidium \"may invite representatives of popularly elected bodies and other persons to a session and grant them speaking rights\" as guests. According to the council, \"within the last couple of years, guests from other international and Nordic organisations have been able to take part in the debates at the Sessions. Visitors from the Baltic States and Northwest Russia are those who mostly take up this opportunity. Guests who have a connection to the theme under discussion are invited to the Theme Session.\"\n\nThe Nordic Council of Ministers has established four \"Offices outside the Nordic Region\", namely in all the Baltic states – Estonia, Latvia and Lithuania – and the German state of Schleswig-Holstein. The offices form part of the secretariat of the Nordic Council of Ministers; according to the Council of Ministers their primary mission is to promote cooperation between the Nordic countries and the Baltic states and to promote the Nordic countries in cooperation with their embassies within the Baltic states.\n\nThe Nordic Council and the Council of Ministers define Estonia, Latvia, Lithuania and Russia as \"Adjacent Areas\" and has formal cooperation with them under the Adjacent Areas policies framework; in recent years the cooperation has focused increasingly on Russia.\n\nThe Nordic Council had historically been a strong supporter of Baltic independence from the Soviet Union. During the move towards independence in the Baltic States in 1991, Denmark and Iceland pressed for the Observer Status in the Nordic Council for the then-nonsovereign Estonia, Latvia and Lithuania. The move in 1991 was opposed by Norway and Finland. The move was heavily opposed by the Soviet Union, accusing the Nordic Council of getting involved in its internal affairs. In the same year, the Nordic Council refused to give observer status for the three, at the time nonsovereign, Baltic states.\n\nWhile the Nordic Council rejected the Baltic states' application for formal observer status, the council nevertheless has extensive cooperation on different levels with all neighbouring countries, including the Baltic states and Germany, especially the state of Schleswig-Holstein. Representatives of Schleswig-Holstein were present as informal guests during a session for the first time in 2016. The state has historical ties to Denmark and cross-border cooperation with Denmark and has a Danish minority population. As parliamentary representatives from Schleswig-Holstein, a member of the South Schleswig Voter Federation and a member of the Social Democrats with ties to the Danish minority were elected. \n\nThe Sámi political structures long desired formal representation in the Nordic Council's structures, and were eventually granted observer status through the Sámi Parliamentary Council. In addition, representatives of the Sámi people are de facto included in activities touching upon their interests. In addition, the Faroe Islands have expressed their wishes for full membership in the Nordic Council instead of the current associate membership.\n\nRecently, three of the members of the Nordic Council (Sweden, Denmark and Finland, all EU-member states), the Baltic Assembly and the Benelux sought intensifying cooperation in the Digital Single Market, as well as discussing social matters, the Economic and Monetary Union of the European Union, the European migrant crisis and defense cooperation. Relations with Russia, Turkey and the United Kingdom was also on the agenda.\n\nSome desire the Nordic Council's promotion of Nordic cooperation to go much further than at present. If the states of Iceland, Sweden, Norway, Denmark and Finland were to merge in such an integration as some desire, it would command a gross domestic product of US$1.60 trillion, making it the twelfth largest economy in the world, larger than that of Australia, Spain, Mexico or South Korea. Gunnar Wetterberg, a Swedish historian and economist, wrote a book entered into the Nordic Council's year book that proposes the creation of a Nordic Federation from the Council in a few decades.\n\n\n"}
{"id": "208932", "url": "https://en.wikipedia.org/wiki?curid=208932", "title": "Outline of sustainable agriculture", "text": "Outline of sustainable agriculture\n\nThe following outline is provided as an overview of and topical guide to sustainable agriculture:\n\nSustainable agriculture – applied science that integrates three main goals, environmental health, economic profitability, and social and economic equity. These goals have been defined by a variety of philosophies, policies and practices, from the vision of farmers and consumers. Perspectives and approaches are very diverse, the following topics intend to help understanding what sustainable agriculture is.\n\n\n\nSustainable farming\n\n\nSustainable forest management\n\nSustainable landscaping\n\nHydroculture\n\n\nPermaculture\n\n\n\n\n\n\n\n\n\n\n\nSustainable agricultural practices\n\n\n\n\n\n\n\n\n\n\ny protocol - Montreal 2000\n\n\n\n"}
{"id": "14322415", "url": "https://en.wikipedia.org/wiki?curid=14322415", "title": "Parasitic load", "text": "Parasitic load\n\nParasitic load is a term used with regard to electrical appliances and railway locomotives. With regard to electrical appliances, it represents the power consumed even when the appliance is shut off, that is standby power. With regard to railway locomotives, it is any of the loads or devices powered by the prime mover not contributing to tractive effort, such as an air compressor, traction motor blower, or radiator fans.\n\nWith regard to electricity production, parasitic loss it is any of the loads or devices powered by the wind generator, not contributing to net electric yield found by subtracting productive yield from gross yield or:\n\nThe term \"parasitic loss\" is often applied to devices that take energy from the engine in order to enhance the engine's ability to create more energy. In the internal combustion engine, almost everything, including the drive line, causes parasitic loss. \n\nBearings, oil pumps, piston rings, valve springs, flywheels, transmissions, driveshafts, and differentials also rob the system of power. An oil pump, being used to lubricate the engine, is a necessary parasite that consumes power from the engine (its host).\n\nAnother example is a supercharger, which derives its power from the engine and creates more power for the engine. The power that the supercharger consumes is parasitic loss and is usually expressed in horsepower (HP). While the HP that the supercharger consumes in comparison to what it generates is small, it is still measurable or calculable. One of the desirable features of a turbocharger over a supercharger is the lower parasitic loss of the former. \n\nAnother common use of the term \"parasitic loss\" is where a new or different design reduces parasitic loss, such as the use of a dry sump over a wet sump. The reason may be less friction or many other variables that cause the design to be more efficient.\n\n"}
{"id": "24647715", "url": "https://en.wikipedia.org/wiki?curid=24647715", "title": "Phosphate rich organic manure", "text": "Phosphate rich organic manure\n\nPhosphate rich organic manure is a type of fertilizer used as an alternative to diammonium phosphate and single super phosphate.\n\nPhosphorus is required by all plants but is limited in soil, creating a problem in agriculture In many areas phosphorus must be added to soil for the extensive plant growth that is desired for crop production. Phosphorus was first added as a fertilizer in the form of single super phosphate (SSP) in the mid-nineteenth century, following research at Rothamsted Experimental Station in England. SSP is non-nitrogen fertiliser containing Phosphate in the form of monocalcium phosphate and Gypsum which is best suited for Alkali soils to supplement Phosphate and reduce soil alkalinity.\n\nThe world consumes around 140 million tons of high grade rock phosphate mineral today, 90% of which goes into the production of diammonium phosphate (DAP). Excess application of chemical fertilizers in fact reduces the agricultural production as chemicals destroy natural soil flora and fauna. When DAP or SSP is applied to the soil only about 30% of the phosphorus is used by the plants, while the rest is converted to forms which cannot be used by the crops a phenomenon which is known as phosphate problem to soil scientists.\nPhosphate Rich Organic Manure is produced by co-composting high-grade (32% P2O5 +/- 2%) rock phosphate in very fine size (say 80% finer than 54 microns). Needless to say, the finer the rock phosphate the better is the agronomic efficiency of PROM. Research indicates that this substance may be a more efficient way of adding phosphorus to soil than applying chemical fertilizers. Other benefits of PROM are that it supplies phosphorus to the second crop planted in a treated area as efficiently as the first, and that it can be produced using acidic waste solids recovered from the discharge of biogas plants.\n\nPhosphorus in rock phosphate mineral is mostly in the form of tricalcium phosphate, which is water-insoluble. Phosphorus dissolution in the soil is most favorable at a pH between 5.5 and 7. Ions of aluminum, iron, and manganese prevent phosphorus dissolution by keeping local pH below 5.5, and magnesium and calcium ions prevent the pH from dropping below 7, preventing the release of phosphorus from its stable molecule. Microorganisms produce organic acids, which cause the slow dissolution of phosphorus from rock phosphate dust added to the soil, allowing more phosphorus uptake by the plant roots. Organic manure can prevent ions of other elements from locking phosphorus into insoluble forms. The phosphorus in phosphate enhanced organic manure is water-insoluble, so it does not leach into ground water or enter runoff \n\nMost phosphate rocks can be used for phosphate rich organic manure. It was previously thought that only those rocks which have citric acid soluble phosphate and those of sedimentary origin could be used. Rocks of volcanic origin can be used as long as they are ground to very fine size.\n\nOrganic manure should be properly prepared for use in agriculture, reducing the C:N ratio to 30:1 or lower. Alkaline and acidic soils require different ratios of phosphorus.\n\nPROM is known as a green chemistry phosphatic fertilizer. Addition of natural minerals or synthetic oxides in water-insoluble forms that contain micronutrients such as copper, zinc, and cobalt may improve the efficiency of PROM. Using natural sources of nitrogen, such as \"Azolla\", may be more environmentally sound.\n\nMinistry of Agriculture and Cooperation, Government of India has now approved Phosphate Rich Organic Manure (PROM) and included the same under Fertilizer Control Order (FCO). The approved specifications may be seen from Gazette Notification from the web site of PROM Society here: http://www.promsociety.net/\n\nhttps://www.researchgate.net/publication/235918492_Principles_of_Phosphate_Fertilization_and_PROM__Progress_Review_2012?ev=prf_pub\n\n"}
{"id": "23310", "url": "https://en.wikipedia.org/wiki?curid=23310", "title": "Pleistocene", "text": "Pleistocene\n\nThe Pleistocene (, often colloquially referred to as the Ice Age) is the geological epoch which lasted from about 2,588,000 to 11,700 years ago, spanning the world's most recent period of repeated glaciations. The end of the Pleistocene corresponds with the end of the last glacial period and also with the end of the Paleolithic age used in archaeology.\n\nThe Pleistocene is the first epoch of the Quaternary Period or sixth epoch of the Cenozoic Era. In the ICS timescale, the Pleistocene is divided into four stages or ages, the Gelasian, Calabrian, Middle Pleistocene (unofficially the 'Chibanian') and Upper Pleistocene (unofficially the 'Tarantian'). In addition to this international subdivision, various regional subdivisions are often used.\n\nBefore a change finally confirmed in 2009 by the International Union of Geological Sciences, the time boundary between the Pleistocene and the preceding Pliocene was regarded as being at 1.806 million years Before Present (BP), as opposed to the currently accepted 2.588 million years BP: publications from the preceding years may use either definition of the period.\n\nCharles Lyell introduced the term \"pleistocene\" in 1839 to describe strata in Sicily that had at least 70% of their molluscan fauna still living today. This distinguished it from the older Pliocene epoch, which Lyell had originally thought to be the youngest fossil rock layer. He constructed the name \"Pleistocene\" (\"Most New\" or \"Newest\") from the Greek πλεῖστος, \"pleīstos\", \"most\", and καινός, \"kainós\" (latinized as \"cænus\"), \"new\"; this contrasting with the immediately preceding Pliocene (\"More New\" or \"Newer\", from πλείων, \"pleíōn\", \"more\", and \"kainós\"; usual spelling: Pliocene), and the immediately subsequent Holocene (\"wholly new\" or \"entirely new\", from ὅλος, \"hólos\", \"whole\", and \"kainós\") epoch, which extends to the present time.\n\nThe Pleistocene has been dated from 2.588 million (±0.005) to 11,700 years BP with the end date expressed in radiocarbon years as 10,000 carbon-14 years BP. It covers most of the latest period of repeated glaciation, up to and including the Younger Dryas cold spell. The end of the Younger Dryas has been dated to about 9640 BC (11,654 calendar years BP). The end of the Younger Dryas is the official start of the current Holocene Epoch. However, there is no scientific evidence that the Holocene should be a separate epoch. At this time, the Holocene appears to be just one of a number of interglacial periods within the Pleistocene.\n\nIt was not until after the development of radiocarbon dating, however, that Pleistocene archaeological excavations shifted to stratified caves and rock-shelters as opposed to open-air river-terrace sites.\n\nIn 2009 the International Union of Geological Sciences (IUGS) confirmed a change in time period for the Pleistocene, changing the start date from 1.806 to 2.588 million years BP, and accepted the base of the Gelasian as the base of the Pleistocene, namely the base of the Monte San Nicola GSSP. The IUGS has yet to approve a type section, Global Boundary Stratotype Section and Point (GSSP), for the upper Pleistocene/Holocene boundary (\"i.e.\" the upper boundary). The proposed section is the \"North Greenland Ice Core Project\" ice core 75° 06' N 42° 18' W. The lower boundary of the Pleistocene Series is formally defined magnetostratigraphically as the base of the Matuyama (C2r) chronozone, isotopic stage 103. Above this point there are notable extinctions of the calcareous nanofossils: \"Discoaster pentaradiatus\" and \"Discoaster surculus\".\n\nThe Pleistocene covers the recent period of repeated glaciations. The name Plio-Pleistocene has, in the past, been used to mean the last ice age. The revised definition of the Quaternary, by pushing back the start date of the Pleistocene to 2.58 Ma, results in the inclusion of all the recent repeated glaciations within the Pleistocene.\n\nThe modern continents were essentially at their present positions during the Pleistocene, the plates upon which they sit probably having moved no more than 100 km relative to each other since the beginning of the period.\n\nAccording to Mark Lynas (through collected data), the Pleistocene's overall climate could be characterized as a continuous El Niño with trade winds in the south Pacific weakening or heading east, warm air rising near Peru, warm water spreading from the west Pacific and the Indian Ocean to the east Pacific, and other El Niño markers.\n\nPleistocene climate was marked by repeated glacial cycles in which continental glaciers pushed to the 40th parallel in some places. It is estimated that, at maximum glacial extent, 30% of the Earth's surface was covered by ice. In addition, a zone of permafrost stretched southward from the edge of the glacial sheet, a few hundred kilometres in North America, and several hundred in Eurasia. The mean annual temperature at the edge of the ice was ; at the edge of the permafrost, .\n\nEach glacial advance tied up huge volumes of water in continental ice sheets thick, resulting in temporary sea-level drops of or more over the entire surface of the Earth. During interglacial times, such as at present, drowned coastlines were common, mitigated by isostatic or other emergent motion of some regions.\n\nThe effects of glaciation were global. Antarctica was ice-bound throughout the Pleistocene as well as the preceding Pliocene. The Andes were covered in the south by the Patagonian ice cap. There were glaciers in New Zealand and Tasmania. The current decaying glaciers of Mount Kenya, Mount Kilimanjaro, and the Ruwenzori Range in east and central Africa were larger. Glaciers existed in the mountains of Ethiopia and to the west in the Atlas mountains.\n\nIn the northern hemisphere, many glaciers fused into one. The Cordilleran ice sheet covered the North American northwest; the east was covered by the Laurentide. The Fenno-Scandian ice sheet rested on northern Europe, including much of Great Britain; the Alpine ice sheet on the Alps. Scattered domes stretched across Siberia and the Arctic shelf. The northern seas were ice-covered.\n\nSouth of the ice sheets large lakes accumulated because outlets were blocked and the cooler air slowed evaporation. When the Laurentide ice sheet retreated, north-central North America was totally covered by Lake Agassiz. Over a hundred basins, now dry or nearly so, were overflowing in the North American west. Lake Bonneville, for example, stood where Great Salt Lake now does. In Eurasia, large lakes developed as a result of the runoff from the glaciers. Rivers were larger, had a more copious flow, and were braided. African lakes were fuller, apparently from decreased evaporation. Deserts, on the other hand, were drier and more extensive. Rainfall was lower because of the decreases in oceanic and other evaporation.\n\nIt has been estimated that during the Pleistocene, the East Antarctic Ice Sheet thinned by at least 500 meters, and that thinning since the Last Glacial Maximum is less than 50 meters and probably started after ca 14 ka.\n\nOver 11 major glacial events have been identified, as well as many minor glacial events. A major glacial event is a general glacial excursion, termed a \"glacial.\" Glacials are separated by \"interglacials\". During a glacial, the glacier experiences minor advances and retreats. The minor excursion is a \"stadial\"; times between stadials are \"interstadials\".\n\nThese events are defined differently in different regions of the glacial range, which have their own glacial history depending on latitude, terrain and climate. There is a general correspondence between glacials in different regions. Investigators often interchange the names if the glacial geology of a region is in the process of being defined. However, it is generally incorrect to apply the name of a glacial in one region to another.\n\nFor most of the 20th century only a few regions had been studied and the names were relatively few. Today the geologists of different nations are taking more of an interest in Pleistocene glaciology. As a consequence, the number of names is expanding rapidly and will continue to expand. Many of the advances and stadials remain unnamed. Also, the terrestrial evidence for some of them has been erased or obscured by larger ones, but evidence remains from the study of cyclical climate changes.\n\nThe glacials in the following tables show \"historical\" usages, are a simplification of a much more complex cycle of variation in climate and terrain, and are generally no longer used. These names have been abandoned in favor of numeric data because many of the correlations were found to be either inexact or incorrect and more than four major glacials have been recognized since the historical terminology was established.\n\nCorresponding to the terms glacial and interglacial, the terms pluvial and interpluvial are in use (Latin: \"pluvia\", rain). A pluvial is a warmer period of increased rainfall; an interpluvial, of decreased rainfall. Formerly a pluvial was thought to correspond to a glacial in regions not iced, and in some cases it does. Rainfall is cyclical also. Pluvials and interpluvials are widespread.\n\nThere is no systematic correspondence of pluvials to glacials, however. Moreover, regional pluvials do not correspond to each other globally. For example, some have used the term \"Riss pluvial\" in Egyptian contexts. Any coincidence is an accident of regional factors. Only a few of the names for pluvials in restricted regions have been stratigraphically defined.\n\nThe sum of transient factors acting at the Earth's surface is cyclical: climate, ocean currents and other movements, wind currents, temperature, etc. The waveform response comes from the underlying cyclical motions of the planet, which eventually drag all the transients into harmony with them. The repeated glaciations of the Pleistocene were caused by the same factors.\n\nGlaciation in the Pleistocene was a series of glacials and interglacials, stadials and interstadials, mirroring periodic changes in climate. The main factor at work in climate cycling is now believed to be Milankovitch cycles. These are periodic variations in regional and planetary solar radiation reaching the Earth caused by several repeating changes in the Earth's motion.\n\nMilankovitch cycles cannot be the sole factor responsible for the variations in climate since they explain neither the long term cooling trend over the Plio-Pleistocene, nor the millennial variations in the Greenland Ice Cores. Milankovitch pacing seems to best explain glaciation events with periodicity of 100,000, 40,000, and 20,000 years. Such a pattern seems to fit the information on climate change found in oxygen isotope cores. \n\nIn oxygen isotope ratio analysis, variations in the ratio of to (two isotopes of oxygen) by mass (measured by a mass spectrometer) present in the calcite of oceanic core samples is used as a diagnostic of ancient ocean temperature change and therefore of climate change. Cold oceans are richer in , which is included in the tests of the microorganisms (foraminifera) contributing the calcite.\n\nA more recent version of the sampling process makes use of modern glacial ice cores. Although less rich in than sea water, the snow that fell on the glacier year by year nevertheless contained and in a ratio that depended on the mean annual temperature.\n\nTemperature and climate change are cyclical when plotted on a graph of temperature versus time. Temperature coordinates are given in the form of a deviation from today's annual mean temperature, taken as zero. This sort of graph is based on another of isotope ratio versus time. Ratios are converted to a percentage difference from the ratio found in standard mean ocean water (SMOW).\n\nThe graph in either form appears as a waveform with overtones. One half of a period is a Marine isotopic stage (MIS). It indicates a glacial (below zero) or an interglacial (above zero). Overtones are stadials or interstadials.\n\nAccording to this evidence, Earth experienced 102 MIS stages beginning at about 2.588 Ma BP in the Early Pleistocene Gelasian. Early Pleistocene stages were shallow and frequent. The latest were the most intense and most widely spaced.\n\nBy convention, stages are numbered from the Holocene, which is MIS1. Glacials receive an even number; interglacials, odd. The first major glacial was MIS2-4 at about 85–11 ka BP. The largest glacials were 2, 6, 12, and 16; the warmest interglacials, 1, 5, 9 and 11. For matching of MIS numbers to named stages, see under the articles for those names.\n\nBoth marine and continental faunas were essentially modern but with many more large land mammals such as Mammoths, Mastodons, \"Diprotodon\", \"Smilodon\", tiger, lion, Aurochs, Short-faced bear, giant sloths, \"Gigantopithecus\" and others. Isolated places such as Australia, Madagascar, New Zealand and islands in the Pacific saw the evolution of large birds and even reptiles such as the Elephant bird, moa, Haast's eagle, \"Quinkana\", \"Megalania\" and \"Meiolania\".\nThe severe climatic changes during the ice age had major impacts on the fauna and flora. With each advance of the ice, large areas of the continents became totally depopulated, and plants and animals retreating southwards in front of the advancing glacier faced tremendous stress. The most severe stress resulted from drastic climatic changes, reduced living space, and curtailed food supply. A major extinction event of large mammals (megafauna), which included mammoths, mastodons, saber-toothed cats, \"glyptodons\", the woolly rhinoceros, various giraffids, such as the Sivatherium; ground sloths, Irish elk, cave bears, Gomphothere, dire wolves, and short-faced bears, began late in the Pleistocene and continued into the Holocene. Neanderthals also became extinct during this period. At the end of the last ice age, cold-blooded animals, smaller mammals like wood mice, migratory birds, and swifter animals like whitetail deer had replaced the megafauna and migrated north.\n\nThe extinctions hardly affected Africa but were especially severe in North America where native horses and camels were wiped out.\n\nIn July 2018, a team of Russian scientists in collaboration with Princeton University announced that they had brought two female nematodes frozen in permafrost, from around 42,000 years ago, back to life. The two nematodes, at the time, were the oldest confirmed living animals on the planet.\n\nThe evolution of anatomically modern humans took place during the Pleistocene. In the beginning of the Pleistocene \"Paranthropus\" species are still present, as well as early human ancestors, but during the lower Palaeolithic they disappeared, and the only hominin species found in fossilic records is \"Homo erectus\" for much of the Pleistocene. Acheulean lithics appear along with \"Homo erectus\", some 1.8 million years ago, replacing the more primitive Oldowan industry used by \"A. garhi\" and by the earliest species of \"Homo\".\nThe Middle Paleolithic saw more varied speciation within \"Homo\", including the appearance of \"Homo sapiens\" about 200,000 years ago.\n\nAccording to mitochondrial timing techniques, modern humans migrated from Africa after the Riss glaciation in the Middle Palaeolithic during the Eemian Stage, spreading all over the ice-free world during the late Pleistocene. A 2005 study posits that humans in this migration interbred with archaic human forms already outside of Africa by the late Pleistocene, incorporating archaic human genetic material into the modern human gene pool.\n\nPleistocene non-marine sediments are found primarily in fluvial deposits, lakebeds, slope and loess deposits as well as in the large amounts of material moved about by glaciers. Less common are cave deposits, travertines and volcanic deposits (lavas, ashes). Pleistocene marine deposits are found primarily in shallow marine basins mostly (but with important exceptions) in areas within a few tens of kilometers of the modern shoreline. In a few geologically active areas such as the Southern California coast, Pleistocene marine deposits may be found at elevations of several hundred meters.\n\n\n"}
{"id": "392185", "url": "https://en.wikipedia.org/wiki?curid=392185", "title": "Relay program", "text": "Relay program\n\nThe Relay program consisted of Relay 1 and Relay 2, two early American satellites in elliptical Low Earth orbit. Both were primarily experimental communications satellites funded by NASA and developed by RCA. As of December 2, 2016, both satellites were still in orbit. Relay 1 provided the first American television transmissions across the Pacific Ocean.\n\nRelay 1 was launched atop a Delta B rocket on December 13, 1962 from LC-17A at Cape Canaveral Air Force Station. Its payload included radiation experiments designed to map the Earth's radiation belts. Apogee was 7500 km; perigee 1300. The spin-stabilized satellite had an initial spin rate of 167.3 rpm and an initial spin axis orientation with a declination of -68.3 deg and a right ascension of -56 deg. Its orbital period was 185.09 minutes. Shortly after launch, two basic problems evolved. One was the satellite's response to spurious commands, and the other was the leakage of a high-power regulator. This leakage caused the first two weeks of satellite operation to be useless. After this period, satellite operation returned to normal. The satellite carried one transmitter for tracking and one for telemetry. The telemetry system was PCM at 1152 bit/s. Each 128 words per telemetry frame (of one s duration) used 113 words for the particle experiment. The leakage problem caused the spacecraft to revert to a low voltage state early in 1965. Sporadic transmission occurred until February 10, 1965, after which no usable scientific data was obtained.\n\nRelay 1 was the first satellite to broadcast television from the United States to Japan. The first broadcast during orbit 2677 (1963-11-22, 2027:42-2048 (GMT), or 1:27 pm Dallas time) was to be a prerecorded address from the president of the United States to the Japanese people, but was instead the announcement of the John F. Kennedy assassination. On orbit 2678, this satellite carried a broadcast titled \"Record, Life of the Late John F. Kennedy\", the first television program broadcast simultaneously in the U.S. and Japan. In later orbits, NBC transmitted coverage of the funeral procession from the White House to the cathedral. In the three days following the Kennedy assassination, Relay 1 handled a total of 11 spot broadcasts; eight to Europe and three to Japan. All the useful passes of the satellite were made available to permit immediate coverage of the tragic events.\n\nIn August 1964, this satellite was used as the United States-Europe link for the broadcast of the 1964 Summer Olympics from Tokyo, after the signal was relayed to the United States via Syncom 3. This marked the first time that two satellites were used in tandem for a television broadcast.\n\nCOSPAR satellite ID: Relay 1 1962-Beta-Upsilon 1 (62BU1)\n\nRelay 2 was launched atop a Delta B rocket on January 21, 1964 from LC-17B at Cape Canaveral Air Force Station. Apogee 7600 km; perigee 1870 km. It was physically similar to Relay 1. Design changes in this satellite improved its performance so response to spurious commands was essentially eliminated. One of the two onboard transponders operated normally until November 20, 1966. From that time until its failure on January 20, 1967, it required a longer time than normal to come on. The other transponder continued to operate until June 9, 1967, when it too failed to operate normally.\n\nCOSPAR satellite ID: Relay 2 1964-003A\n\n\n"}
{"id": "10109430", "url": "https://en.wikipedia.org/wiki?curid=10109430", "title": "Reynolds analogy", "text": "Reynolds analogy\n\nReynolds analogy is popularly known to relate laminar momentum and heat transfer. The main assumption is that heat flux q/A in a turbulent system is analogous to momentum flux τ, which suggests that the ratio τ/(q/A) must be constant for all radial positions.\n\nThe complete Reynolds analogy* is:\n\nformula_1\n\nExperimental data for gas streams agree approximately with above equation if the Schmidt and Prandtl numbers are near 1.0 and only skin friction is present in flow past a flat plate or inside a pipe. When liquids are present and/or form drag is present, the analogy is conventionally known to be invalid.\n\nIn 2008, the qualitative form of validity of Reynolds' analogy was re-visited for laminar flow of incompressible fluid with variable dynamic viscosity (μ). It was shown that the inverse dependence of Reynolds number (\"Re\") and skin friction coefficient(\"c\") is the basis for validity of the Reynolds’ analogy, in laminar convective flows with constant & variable μ. For μ = const. it reduces to the popular form of Stanton number (\"St\") increasing with increasing \"Re\", whereas for variable μ it reduces to \"St\" increasing with decreasing \"Re\". Consequently, the Chilton-Colburn analogy of \"St\"•\"Pr\" increasing with increasing \"c\" is qualitatively valid whenever the\nReynolds’ analogy is valid. Further, the validity of the Reynolds’ analogy is linked to the applicability of Prigogine's Theorem of Minimum Entropy Production. Thus, Reynolds' analogy is valid for flows that are close to developed, for whom, changes in the gradients of field variables (velocity & temperature) along the flow are small.\n\n"}
{"id": "401062", "url": "https://en.wikipedia.org/wiki?curid=401062", "title": "Shower-curtain effect", "text": "Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of how a shower curtain gets blown inward with a running shower. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\n\nThe shower-curtain effect may also be used to describe the observation how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n\nAlso called Chimney effect or Stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air. By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this cannot be the only mechanism at work.\n\nSee also Cooling tower.\n\nThe most popular explanation given for the shower-curtain effect is Bernoulli's principle. Bernoulli's principle states that an increase in velocity results in a decrease in pressure. This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water. This movement would be parallel to the plane of the shower curtain. If air is moving across the inside surface of the shower curtain, Bernoulli's principle says the air pressure there will drop. This would result in a pressure differential between the inside and outside, causing the curtain to move inward. It would be strongest when the gap between the bather and the curtain is smallest - resulting in the curtain attaching to the bather.\n\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\n\nDavid Schmidt of University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results. Professor Schmidt is adamant that this was done \"for fun\" in his own free time without the use of grants.\n\nThe Coandă effect, also known as \"boundary layer attachment\", is the tendency of a moving fluid to adhere to an adjacent wall.\n\nA hot shower will produce steam that condenses on the shower side of the curtain; lowering the pressure there. In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\n\nIt's possible to use a telescopic shower curtain rod to block the curtain on its lower part and to prevent it from sucking inside.\n\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A (convex) curved shower rod can also be used to hold the curtain against the inside wall of a tub.\n\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\n\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n\n"}
{"id": "51099085", "url": "https://en.wikipedia.org/wiki?curid=51099085", "title": "To Save Humanity", "text": "To Save Humanity\n\nTo Save Humanity is a 2015 collection of 96 essays on global health from a collection of authors who range from heads of states, movie stars, scientists at leading universities, activists, and Nobel Prize winners. Each contributor was asked the same question: \"What is the single most important thing for the future of global health over the next fifty years?\" The collection was edited by Julio Frenk and Steven J. Hoffman.\n\nThe Global Strategy Lab called the collection \"unparalleled\" and \"a primer on the major issues of our time and a blueprint for post-2015 health and development,\" and featured it in their annual conference.\n\nThe Health Impact Fund also featured the collection at their conference.\n\nThe Lancet described the book as \"testimony to the complexity of global health politics,\" and called it \"a reminder that the breadth of individual and institutional engagement with global health cannot be fully captured by one set of global goals.\"\n\nVox has republished several of the articles for free online as part of a series entitled \"One Change to Save the World.\"\n"}
{"id": "3294173", "url": "https://en.wikipedia.org/wiki?curid=3294173", "title": "Vactrain", "text": "Vactrain\n\nA vactrain (or vacuum tube train) is a proposed design for very-high-speed rail transportation. It is a maglev (magnetic levitation) line using partly evacuated tubes or tunnels. Reduced air resistance could permit vactrains to travel at very high speeds with relatively little power—up to . This is 5–6 times the speed of sound in Earth's atmosphere at sea level. Vactrains might use gravity to assist their acceleration. If these trains achieve the predicted speeds, they could surpass aircraft as the world's fastest mode of public transportation. \n\nHowever, without major advances in tunneling and other technology, vactrains would be prohibitively expensive.\n\nIn 1799, George Medhurst of London conceived of and patented an atmospheric railway that could convey people or cargo through pressurized or evacuated tubes. The early atmospheric railways and pneumatic tube transport systems (such as the Dalkey Atmospheric Railway) relied on steam power for propulsion.\n\nIn 1888, Michel Verne, son of Jules Verne, imagined a submarine pneumatic tube transport system that could propel a passenger capsule at speeds up to 1800 km/h under the Atlantic Ocean in a short story called \"An Express of the Future\".\n\nThe vactrain proper was invented by Robert Goddard as a freshman at Worcester Polytechnic Institute in the United States in 1904. Goddard subsequently refined the idea in a 1906 short story called \"The High-Speed Bet\" which was summarized and published in a Scientific American editorial in 1909 called \"The Limit of Rapid Transit\". Esther, his wife, was granted a US patent for the vactrain in 1950, five years after his death.\n\nIn 1909, Russian professor built the world's first model of his proposed version of the vactrain at Tomsk Polytechnic University. He later published a vactrain concept in 1914 in the book \"Motion without friction (airless electric way)\".\n\nIn 1955, Polish science-fiction writer Stanisław Lem in a novel \"The Magellan Nebula\" wrote about intercontinental vactrain called \"organowiec\", which moved in a transparent tube at a speed higher than 1,666 km/h.\nLater in April 1962, the vactrain appears in the story \"Mercenary\" by Mack Reynolds, where he mentions Vacuum Tube Transport in passing.\n\nDuring the 1970s, a leading vactrain advocate, Robert M. Salter of RAND, published a series of elaborate engineering articles.\n\nAn interview with Robert Salter appeared in the \"Los Angeles Times\" (June 11, 1972). He discussed, in detail, the relative ease with which the U.S. government could build a tube shuttle system using technologies available at that time. Maglev being poorly developed at the time, he proposed steel wheels. The chamber's door to the tube would be opened, and enough air admitted behind to accelerate the train into the tube. Gravity would further accelerate the departing train down to cruise level. Rising from cruise level, the arriving train would decelerate by compressing the rarefied air ahead of it, which would be vented. Pumps at the stations would make up for losses due to friction or air escaping around the edges of the train, the train itself requiring no motor. This combination of modified (shallow) gravity train and atmospheric railway propulsion would consume little energy but limit the system to subsonic speeds, hence initial routes of tens or hundreds of miles or kilometers rather than transcontinental distances were proposed.\n\nTrains were to require no couplers, each car being directly welded, bolted, or otherwise firmly connected to the next, the route calling for no more bending than the flexibility of steel could easily handle. At the end of the line the train would be moved sideways into the end chamber of the return tube. The railway would have both an inner evacuated tube and an outer tunnel. At cruise depth, the space between would have enough water to float the vacuum tube, softening the ride.\n\nA route through the Northeast Megalopolis was laid out, with nine stations, one each in the District of Columbia, Maryland, Delaware, Pennsylvania, New York, Rhode Island, Massachusetts, and two in Connecticut. Commuter rail systems were mapped for the San Francisco and New York areas, the commuter version having longer, heavier trains, to be propelled less by air and more by gravity than the intercity version. The New York system was to have three lines, terminating in Babylon, Paterson, Huntington, Elizabeth, White Plains, and St. George.\n\nSalter pointed out how such a system would help reduce the environmental damage being done to the atmosphere by aviation and surface transportation. He called underground Very High Speed Transportation (tube shuttles) his nation's \"logical next step\". The plans were never taken to the next stage.\n\nAt the time these reports were published, national prestige was an issue as Japan had been operating its showcase bullet train for several years and maglev train research was hot technology. The American \"Planetran\" would establish transcontinental subway service in the United States and provide a commute from Los Angeles to New York City in one hour. The tunnel would be buried to a depth of several hundred feet in solid rock formations. Construction would make use of lasers to ensure alignment and use tungsten probes to melt through igneous rock formations. The tunnel would maintain a partial vacuum to minimize drag. A trip would average and subject passengers to accelerations up to 1.4 times that of gravity, requiring the use of gimballed compartments. Enormous construction costs (estimated as high as US$1 trillion) were the primary reason why Salter's proposal was never built.\nStarting in the late 1970s and early 1980s, the Swissmetro was proposed to leverage the invention of the experimental German Transrapid maglev train, and operate in large underground tunnels reduced to the pressure altitude of at which the Concorde SST was certified to fly.\n\nIn the 1980s, Frank P. Davidson, a founder and chairman of the Channel Tunnel project, and Japanese engineer tackled the transoceanic problems with a proposal to float a tube above the ocean floor, anchored with cables (a submerged floating tunnel). The transit tube would remain at least below the ocean surface to avoid water turbulence.\n\nOn November 18, 1991, Gerard K. O'Neill filed a patent application for a vactrain system. He called the company he wanted to form \"VSE International\", for velocity, silence, and efficiency. However, the concept itself he called \"Magnetic Flight\". The vehicles, instead of running on a pair of tracks, would be elevated using electromagnetic force by a single track within a tube (permanent magnets in the track, with variable magnets on the vehicle), and propelled by electromagnetic forces through tunnels. He estimated the trains could reach speeds of up to – about five times faster than a jet airliner – if the air was evacuated from the tunnels. To obtain such speeds, the vehicle would accelerate for the first half of the trip, and then decelerate for the second half of the trip. The acceleration was planned to be a maximum of about one-half of the force of gravity. O'Neill planned to build a network of stations connected by these tunnels, but he died two years before his first patent on it was granted.\n\nJames R. Powell, former co-inventor of superconducting maglev in the 1960s, has since 2001 led investigation of a concept for using a maglev vactrain for space launch (theoretically two orders of magnitude less marginal cost than present rockets), where the StarTram proposal would have vehicles reach up to within an acceleration tunnel (lengthy to limit g-forces), considering boring through the ice sheet in Antarctica for lower anticipated expense than in rock.\n\nET3 claim to have achieved some work that resulted in a patent on \"evacuated tube transport technology\" which was granted in 2009. They presented their idea 2013 on public stage.\n\nElon Musk, CEO of Tesla and SpaceX, champions the vactrain concept under the term hyperloop. An agreement was signed in 2017 to co-develop a hyperloop line between Seoul and Busan in South Korea.\n\nTransPod is a Canadian company designing and manufacturing ultra-high-speed tube transportation technology and vehicles. TransPod vehicles are being designed to travel at over 1000 km/h between cities using fully electric propulsion and zero need for fossil fuels. Unlike the hyperloop vactrain concept, the TransPod system uses moving electromagnetic fields to propel the vehicles with stable levitation off the bottom surface, rather than compressed air. In September 2017, TransPod released a scientific peer-reviewed publication in the journal Procedia Engineering. The paper was premiered at the EASD EURODYN 2017 conference, and presents the physics of the TransPod system.\n\n\n"}
{"id": "29229299", "url": "https://en.wikipedia.org/wiki?curid=29229299", "title": "Vallonia (mythology)", "text": "Vallonia (mythology)\n\nIn Roman mythology, Vallonia was the goddess of valleys (cf. Latin \"\" \"valley\"). Her name is known from St. Augustine's work \"The City of God\", and is not attested otherwise.\n"}
{"id": "1926211", "url": "https://en.wikipedia.org/wiki?curid=1926211", "title": "William L. Hudson", "text": "William L. Hudson\n\nCaptain William Levereth Hudson, USN (11 May 1794 – 15 October 1862) was a United States Navy officer in the first half of the 19th century.\n\nHudson was born 11 May 1794 in Brooklyn. His first service afloat was in the Mediterranean Squadron under Commodore William Bainbridge in the schooner and sloop-of-war from 1815 to 1817.\n\nHudson was appointed midshipman 1 January 1816. In 1821–1823, he served in on the Pacific coast of South America, and in for a Mediterranean cruise 1826–1829. In 1830–1831, Hudson accompanied Lieutenant Ramsey on a tour to Russia, and then assumed duty at the New York Navy Yard.\n\nIn June 1838 he was ordered to command , attached to Commander Charles Wilkes's exploring expedition, second in command overall. After strenuous service in the Antarctic, the South Seas, and along the coast of North America, \"Peacock\" was wrecked 18 July 1841 while attempting to cross the bar and enter the Columbia River on Wilkes' orders. Commander Hudson made every effort to free his ship but was forced to leave her, saving all his men and the scientific papers.\n\nIn September 1849, after shore and lighthouse duty, he was ordered to command \"Vincennes\", cruising the Pacific until 1852. In March 1857 Hudson, appointed captain 8 October 1855, assumed command of . That August, in conjunction with British ships, he made the first attempt at laying a transatlantic cable. This try was unsuccessful, but a second attempt met with success 10 August 1858. After commanding the Boston Navy Yard in 1858–1862, Captain Hudson was made Inspector of the 3d Light House District. He died 15 October 1862 in Brooklyn, aged 68.\n\nThree ships have been named USS \"Hudson\" in his honor.\n\n\n"}
{"id": "481484", "url": "https://en.wikipedia.org/wiki?curid=481484", "title": "Wood preservation", "text": "Wood preservation\n\nAll measures that are taken to ensure a long life of wood fall under the definition wood preservation (timber treatment).\n\nApart from structural wood preservation measures, there are a number of different (chemical) preservatives and processes (also known as timber treatment, lumber treatment or pressure treatment) that can extend the life of wood, timber, wood structures or engineered wood. These generally increase the durability and resistance from being destroyed by insects or fungus.\n\nAs proposed by Richardson, treatment of wood has been practiced for almost as long as the use of wood itself. There are records of wood preservation reaching back to ancient Greece during Alexander the Great's rule, where bridge wood was soaked in olive oil. The Romans protected their ship hulls by brushing the wood with tar. During the Industrial Revolution wood preservation became a cornerstone of the wood processing industry. Inventors and scientists such as Bethell, Boucherie, Burnett and Kyan made historic developments in wood preservation, with the preservative solutions and processes. Commercial pressure treatment began in the latter half of the 19th century with the protection of railroad cross-ties using creosote. Treated wood was used primarily for industrial, agricultural, and utility applications, where it is still used, until its use grew considerably (at least in the United States) in the 1970s as homeowners began building decks and backyard projects. Innovation in treated timber products continues to this day, with consumers becoming more interested in less toxic materials.\n\nWood that has been industrially pressure-treated with approved preservative products poses a limited risk to the public, and should be disposed of properly. On December 31, 2003, the U.S. wood treatment industry stopped treating residential lumber with arsenic and chromium (chromated copper arsenate, or CCA). This was a voluntary agreement with the United States Environmental Protection Agency. CCA was replaced by copper-based pesticides, with exceptions for certain industrial uses. CCA may still be used for outdoor products like utility trailer beds and non-residential construction like piers, docks, and agricultural buildings. Industrial wood preservation chemicals are generally not available directly to the public and may require special approval to import or purchase depending on the product and the jurisdiction where being used. In most countries, industrial wood preservation operations are notifiable industrial activities that require licensing from relevant regulatory authorities such as EPA or equivalent. Reporting and licensing conditions vary widely depending on the particular chemicals used and the country of use.\n\nAlthough pesticides are used to treat lumber, preserving lumber protects natural resources (in the short term) by enabling wood products to last longer. Previous poor practices in industry have left legacies of contaminated ground and water around wood treatment sites in some cases. However, under currently approved industry practices and regulatory controls such as implemented in Europe, North America, Australia, New Zealand, Japan and elsewhere, environmental impact of these operations should be minimal.\n\nWood treated with modern preservatives is generally safe to handle given appropriate handling precautions and personal protection measures. However, treated wood may present certain hazards in some circumstances such as during combustion or where loose wood dust particles or other fine toxic residues are generated or where treated wood comes into direct contact with food and agriculture.\n\nPreservatives containing copper in the form of microscopic particles have recently been introduced to the market, usually with \"micronized\" or \"micro\" trade names and designations such as MCQ or MCA. The manufacturers represent that these products are safe and EPA has registered these products.\n\nThe American Wood Protection Association (AWPA) recommends that all treated wood be accompanied by a Consumer Information Sheet (CIS), to communicate safe handling and disposal instructions as well as potential health and environmental hazards of treated wood. Many producers have opted to provide Material Safety Data Sheets (MSDS) instead. Although the practice of distributing MSDS instead of CIS is widespread there is an ongoing debate regarding the practice and how best to best communicate potential hazards and hazard mitigation to the end-user. Neither MSDS or the newly adopted International Safety Data Sheets (SDS) are required for treated lumber under current U.S. Federal law.\n\nFounded in 1904, the American Wood Protection Association (AWPA), formerly American Wood-Preservers' Association, is a non-profit organization which is the standard setting body for wood preservation standards (including ANSI). AWPA Standards are developed by its technical committees in an open, consensus-based process that involves individuals from all facets of wood preservation: Producers of preservatives and preservative components; producers of treated and untreated wood products; end users of treated wood; engineers, architects and building code officials; government entities, academia, and other groups with a general interest in wood preservation. AWPA's Standards are universally specified for wood preservation in the US, and are recognized worldwide.\n\nAWPA standards help ensure that treated wood products perform satisfactorily for their intended use. They are recognized and used by most, if not all, specifiers of treated wood including electrical utility, marine, road and building construction as well as by local, state and federal governments. \"AWPA\", \"American Wood Protection Association\", identifiers of AWPA Standards (e.g., U1, T1, M4, etc.), and Use Category designations (e.g., UC1, UC3B, UC4A, etc.) are AWPA trademarks and the intellectual property of AWPA and its Technical Committees.\n\nWood preservative systems produced under the AWPA standards system for the residential market are required to be inspected under the stringent American Lumber Standards Committee (ALSC) third party inspection system in order to assure compliance with AWPA standards.\n\nWhile many wood preservative systems are produced under the AWPA standards system, there are wood preservative products in the market that have not earned AWPA standard status and are not subject to the ALSC inspection system. Compliance with AWPA and ASLC will be noted by the AWPA logo on the product end tags.\n\nIn general, marketers of wood preservation systems favor certain terminology. For example, the term 'preservative' is used in preference to words such as: chemical, pesticide, fungicide or biocide. And with newer preservatives, the term 'micronized' is favored over the term nanoparticle or nanotechnology, which may raise public safety concerns.\n\nChemical preservatives can be classified into three broad categories: water-borne preservatives, oil-borne preservatives, and light organic solvent preservatives (LOSPs). These are discussed in more detail below.\n\nParticulate (micronised or dispersed) copper preservative technology has recently been introduced in the US and Europe. In these systems, the copper is ground to micro sized particles and suspended in water rather than being dissolved in a chemical reaction as is the case with other copper products such as ACQ and Copper Azole. There are currently two particulate copper systems in production. One system uses a quat biocide system (known as MCQ) and is a take-off of ACQ. The other uses an azole biocide (known as MCA or μCA-C) and is a take-off of Copper Azole.\n\nProponents of the particulate copper systems make the case that the particulate copper system perform as well or better than the dissolved copper systems as a wood preservative, but other industry researchers disagree. None of the particulate copper systems have been submitted to the American Wood Protection Association (AWPA) for evaluation, thus the particulate systems should not be used in applications where AWPA standards are required. However, all of the particulate copper systems have been tested and approved for building code requirements by the International Code Council (ICC). The particulate copper systems provide a lighter color than dissolved copper systems such as ACQ or copper azole.\n\nProponents of the micronized copper systems claim that the systems are subject to third party inspection under a quality monitor program. However, the monitoring program is not subject to oversight by the American Lumber Standards Committee (ALSC) as is required for the AWPA standard systems.\n\nTwo particulate copper systems, one marketed as MicroPro and the other as Wolmanized using μCA-C formulation, have achieved Environmentally Preferable Product (EPP) certification. The \"EPP\" certification was issued by Scientific Certifications Systems (SCS), and is based on a comparative life-cycle impact assessments with an industry standard.\n\nThe copper particle size used in the \"micronized\" copper beads ranges from 1 to 700 nm with an average under 300 nm. Larger particles (such as actual micron-scale particles) of copper do not adequately penetrate the wood cell walls. These micronized preservatives use nano particles of copper oxide or copper carbonate, for which there are alleged safety concerns. An environmental group has recently petitioned EPA to revoke the registration of the micronized copper products citing safety issues.\n\nAlkaline copper quaternary (ACQ) is a preservative made of copper, a fungicide, and a quaternary ammonium compound (quat) like didecyl dimethyl ammonium chloride, an insecticide which also augments the fungicidal treatment. ACQ has come into wide use in the US, Europe, Japan and Australia following restrictions on CCA. Its use is governed by national and international standards, which determine the volume of preservative uptake required for a specific timber end use.\n\nSince it contains high levels of copper, ACQ-treated timber is five times more corrosive to common steel. It is necessary to use fasteners meeting or exceeding requirements for ASTM A 153 Class D, such as ceramic-coated, as mere galvanized and even common grades of stainless steel corrode. The U.S. began mandating the use of non-arsenic containing wood preservatives for virtually all residential use timber in 2004.\n\nThe American Wood Protection Association (AWPA) standards for ACQ require a retention of 0.15 lb/ft (PCF) for above ground use and 0.40 lb/ft for ground contact.\n\nChemical Specialties, Inc (CSI, now Viance) received U.S. Environmental Protection Agency's Presidential Green Chemistry Challenge Award in 2002 for commercial introduction of ACQ. Its widespread use has eliminated major quantities of arsenic and chromium previously contained in CCA.\n\nCopper azole preservative (denoted as CA-B and CA-C under American Wood Protection Association/AWPA standards) is a major copper based wood preservative that has come into wide use in Canada, the US, Europe, Japan and Australia following restrictions on CCA. Its use is governed by national and international standards, which determine the volume of preservative uptake required for a specific timber end use.\n\nCopper azole is similar to ACQ with the difference being that the dissolved copper preservative is augmented by an azole co-biocide like organic triazoles such as tebuconazole or propiconazole, which are also used to protect food crops, instead of the quat biocide used in ACQ. The azole co-biocide yields a copper azole product that is effective at lower retentions than required for equivalent ACQ performance. The general appearance of wood treated with copper azole preservative is similar to CCA with a green colouration.\n\nCopper azole treated wood is marketed widely under the \"Preserve CA\" and \"Wolmanized\" brands in North America, and the \"Tanalith\" brand across Europe and other international markets.\n\nThe AWPA standard retention for CA-B is 0.10 lb/ft for above ground applications and 0.21 lb/ft for ground contact applications. Type C copper azole, denoted as CA-C, has been introduced under the Wolmanized and Preserve brands. The AWPA standard retention for CA-C is 0.06 lb/ft for above ground applications and 0.15 lb/ft for ground contact applications.\n\nCopper naphthenate, invented in Denmark in 1911, has been used effectively for many applications including: fence posts, canvas, nets, greenhouses, utility poles, railroad ties, beehives, and wooden structures in ground contact. Copper naphthenate is registered with the EPA as a non-restricted use pesticide, so there is no federal applicators licensing requirements for its use as a wood preservative. Copper Naphthenate can be applied by brush, dip, or pressure treatment.\n\nThe University of Hawaii has found that copper naphthenate in wood at loadings of 1.5 lbs per cubic foot is resistant to Formosan termite attack. On February 19, 1981 the Federal Register outlined the EPA's position regarding the health risks associated with various wood preservatives. As a result, the National Park Service recommended the use of copper naphthenate in its facilities as an approved substitute for pentachlorophenol, creosote, and inorganic arsenicals. A 50-year study presented to AWPA in 2005 by Mike Freeman and Douglas Crawford says, \"This study reassessed the condition of the treated wood posts in southern Mississippi, and statistically calculated the new expected post life span. It was determined that commercial wood preservatives, like pentachlorophenol in oil, creosote, and copper naphthenate in oil, provided excellent protection for posts, with life spans now calculated to exceed 60 years. Surprisingly, creosote and penta treated posts at 75% of the recommended AWPA retention, and copper naphthenate at 50% of the required AWPA retention, gave excellent performance in this AWPA Hazard Zone 5 site. Untreated southern pine posts lasted 2 years in this test site.\" \n\nThe AWPA M4 Standard for the care of preservative-treated wood products, reads, \"The appropriateness of the preservation system for field treatment shall be determined by the type of preservative originally used to protect the product and the availability of a field treatment preservative. Because many preservative products are not packaged and labeled for use by the general public, a system different from the original treatment may need to be utilized for field treatment. Users shall carefully read and follow the instructions and precautions listed on the product label when using these materials. Copper naphthenate preservatives containing a minimum of 2.0% copper metal are recommended for material originally treated with copper naphthenate, pentachlorophenol, creosote, creosote solution or waterborne preservatives.\" The M4 Standard has been adopted by the International Code Council's (ICC) 2015 International Building Code (IBC) section 2303.1.9 Preservative-treated Wood, and 2015 International Residential Code (IRC) R317.1.1 Field Treatment. The American Association of State Highway and Transportation Officials AASHTO has also adopted the AWPA M4 Standard.\n\nA waterborne copper naphthenate is sold to consumers under the tradename QNAP 5W. Oilborne copper napthenates with 1% copper as metal solutions are sold to consumers under the tradenames Copper Green, and Wolmanized Copper Coat, a 2% copper as metal solution is sold under the tradename Tenino.\n\nIn CCA treatment, copper is the primary fungicide, arsenic is a secondary fungicide and an insecticide, and chromium is a fixative which also provides ultraviolet (UV) light resistance. Recognized for the greenish tint it imparts to timber, CCA is a preservative that was extremely common for many decades.\n\nIn the pressure treatment process, an aqueous solution of CCA is applied using a vacuum and pressure cycle, and the treated wood is then stacked to dry. During the process, the mixture of oxides reacts to form insoluble compounds, helping with leaching problems.\n\nThe process can apply varying amounts of preservative at varying levels of pressure to protect the wood against increasing levels of attack. Increasing protection can be applied (in increasing order of attack and treatment) for: exposure to the atmosphere, implantation within soil, or insertion into a marine environment.\n\nIn the last decade concerns were raised that the chemicals may leach from the wood into surrounding soil, resulting in concentrations higher than naturally occurring background levels. A study cited in \"Forest Products Journal\" found 12–13% of the chromated copper arsenate leached from treated wood buried in compost during a 12-month period. Once these chemicals have leached from the wood, they are likely to bind to soil particles, especially in soils with clay or soils that are more alkaline than neutral. In the United States the US Consumer Product Safety Commission issued a report in 2002 stating that exposure to arsenic from direct human contact with CCA treated wood may be higher than was previously thought. On 1 January 2004, the Environmental Protection Agency (EPA) in a voluntary agreement with industry began restricting the use of CCA in treated timber in residential and commercial construction, with the exception of shakes and shingles, permanent wood foundations, and certain commercial applications. This was in an effort to reduce the use of arsenic and improve environmental safety, although the EPA were careful to point out that they had not concluded that CCA treated wood structures in service posed an unacceptable risk to the community. The EPA did not call for the removal or dismantling of existing CCA treated wood structures.\n\nIn Australia, the Australian Pesticides and Veterinary Medicines Authority (APVMA) restricted the use of CCA preservative for treatment of timber used in certain applications from March 2006. CCA may no longer be used to treat wood used in 'intimate human contact' applications such as children's play equipment, furniture, residential decking and handrailing. Use for low contact residential, commercial and industrial applications remains unrestricted, as does its use in all other situations. The APVMA decision to restrict the use of CCA in Australia was a precautionary measure, even though the report found no evidence that demonstrated CCA treated timber posed unreasonable risks to humans in normal use. Similarly to the US EPA, the APVMA did not recommend dismantling or removal of existing CCA treated wood structures.\n\nIn Europe, Directive 2003/2/EC restricts the marketing and use of arsenic, including CCA wood treatment. CCA treated wood is not permitted to be used in residential or domestic constructions. It is permitted for use in various industrial and public works, such as bridges, highway safety fencing, electric power transmission and telecommunications poles.\nIn the United Kingdom waste timber treated with CCA was classified in July 2012 as hazardous waste by the Department for the Environment, Food and Rural Affairs.\n\nThese include copper HDO (Bis-(N-cyclohexyldiazeniumdioxy)-copper or CuHDO), copper chromate, copper citrate, acid copper chromate, and ammoniacal copper zinc arsenate (ACZA). The CuHDO treatment is an alternative to CCA, ACQ and CA used in Europe and in approval stages for United States and Canada. ACZA is generally used for marine applications.\n\nBoric acid, oxides and salts (borates) are effective wood preservatives and are supplied under numerous brand names throughout the world. One of the most common compounds used is disodium octaborate tetrahydrate (commonly abbreviated DOT). Borate treated wood is of low toxicity to humans, and does not contain copper or other heavy metals. However, unlike most other preservatives, borate compounds do not become fixed in the wood and can be partially leached out if exposed repeatedly to water that flows away rather than evaporating (evaporation leaves the borate behind so is not a problem). Even though leaching will not normally reduce boron concentrations below effective levels for preventing fungal growth, borates should not be used where they will be exposed to repeated rain, water or ground contact unless the exposed surfaces are treated to repel water. Zinc-borate compounds are less suspectible to leaching than sodium-borate compounds, but are still not recommended for below-ground use unless the timber is first sealed. Recent interest in low toxicity timber for residential use, along with new regulations restricting some wood preservation agents, has resulted in a resurgence of the use of borate treated wood for floor beams and internal structural members. Researchers at CSIRO in Australia have developed organoborates which are much more resistant to leaching, while still providing timber with good protection from termite and fungal attack. The cost of the production of these modified borates will limit their widespread take-up but they are likely to be suitable for certain niche applications, especially where low mammalian toxicity is of paramount importance.\n\nRecent concerns about the health and environmental impacts of metallic wood preservatives have created a market interest in non-metallic wood preservatives such as Propiconazole-Tebuconazole-Imidacloprid better known as PTI. The American Wood Protection Association (AWPA) standards for PTI require a retention of 0.018 lb/ft3 (PCF) for above ground use and 0.013 lb/ft3 when applied in combination with a wax stabilizer. The AWPA has not developed a standard for a PTI ground contact preservative, so PTI is currently limited to above ground applications such as decks. All three of the PTI components are also used in food crop applications. The very low required retentions for PTI pressure treated wood further limits impacts plus substantinally decreases the freight costs and associated environmental impacts for shipping preservative components to the pressure treating plants.\n\nThe PTI preservative imparts very little color to the wood. Producers generally add a color agent or a trace amount of copper solution so as to identify the wood as pressure treated and to better match the color of other pressure treated wood products. The PTI wood products are very well adapted for paint and stain applications with no bleed-through. The addition of the wax stabilizer allows a lower preservative retention plus substantially reduces the tendency of wood to warp and split as it dries. In combination with normal deck maintenance and sealer applications, the stabilizer helps maintain appearance and performance over time. PTI pressure treated wood products are no more corrosive than untreated wood and are approved for all types of metal contact, including aluminum.\n\nPTI pressure treated wood products are relatively new to the market place and are not yet widely available in building supply stores. However, there are some suppliers selling PTI products for delivery anywhere in the US on a job lot order basis.\n\nSodium silicate is produced by fusing sodium carbonate with sand or heating both ingredients under pressure. It has been in use since the 19th century. It can be a deterrent against insect attack and possesses minor flame-resistant properties; however, it is easily washed out of wood by moisture, forming a flake-like layer on top of the wood.\n\nTimber Treatment Technology, LLC, markets TimberSIL®, a sodium silicate wood preservative. The TimberSIL® proprietary process surrounds the wood fibers with a protective, non-toxic, amorphous glass matrix. The result is a product the company calls \"Glass Wood,\" which they claim is Class A fire-retardant, chemically inert, rot and decay resistant, and superior in strength to untreated wood. Timbersil is currently involved in litigation over its claims.\n\nThere are a number of European natural paint fabricants that have developed potassium silicate (potassium waterglass) based preservatives. They frequently include boron compounds, cellulose, lignin and other plant extracts. They are a surface application with a minimal impregnation for internal use.\n\nIn Australia, a water-based bifenthrin preservative has been developed to improve the insect resistance of timber. As this preservative is applied by spray, it only penetrates the outer 2 mm of the timber cross-section. Concerns have been raised as to whether this thin-envelope system will provide protection against insects in the longer term, particularly when exposed to sunlight for extended periods.\n\nThis treated wood utilizes a fire retardant chemical that remains stable in high temperature environments. The fire retardant is applied under pressure at a wood treating plant like the preservatives described above, or applied as a surface coating.\n\nIn both cases, treatment provides a physical barrier to flame spread. The treated wood chars but does not oxidize. Effectively this creates a convective layer that transfers flame heat to the wood in a uniform way which significantly slows the progress of fire to the material. There are several commercially available wood-based construction materials using pressure-treatment (such as those marketed in the United States and elsewhere under the trade names of \"FirePro\", \"Burnblock\" 'Woodsafe, Dricon', 'D-Blaze,' and 'Pyro-Guard'), as well as factory-applied coatings under the trade names of 'PinkWood' and 'NexGen'. Some site-applied coatings as well as brominated fire retardants have lost favor due to safety concerns as well as concerns surrounding the consistency of application. Specialized treatments also exist for wood used in weather-exposed applications.\n\nThe only impregnation-applied fire retardant commercially available in Australia is 'NexGen'. 'Guardian', which used calcium formate as a 'powerful wood modifying agent', was removed from sale in early 2010 for unspecified reasons.\n\nThese include pentachlorophenol (\"penta\") and creosote. They emit a strong petrochemical odor and are generally not used in consumer products. Both of these pressure treatments routinely protect wood for 40 years in most applications.\n\nCreosote was the first wood preservative to gain industrial importance more than 150 years ago and it is still widely used today for protection of industrial timber components where long service life is essential.\nCreosote is a tar-based preservative that is commonly used for utility poles and railroad ties (UK: railway sleepers). Creosote is one of the oldest wood preservatives, and was originally derived from a wood distillate, but now, virtually all creosote is manufactured from the distillation of coal tar. Creosote is regulated as a pesticide, and is not usually sold to the general public.\n\nIn recent years in Australia and New Zealand, linseed oil has been incorporated in preservative formulations as a solvent and water repellent to \"envelope treat\" timber. This involves just treating the outer 5 mm of the cross-section of a timber member with preservative (e.g., permethrin 25:75), leaving the core untreated. While not as effective as CCA or LOSP methods, envelope treatments are significantly cheaper, as they use far less preservative. Major preservative manufacturers add a blue (or red) dye to envelope treatments. Blue colored timber is for use south of the Tropic of Capricorn and red for elsewhere. The colored dye also indicates that the timber is treated for resistance to termites/white ants. There is an ongoing promotional campaign in Australia for this type of treatment.\n\nThis class of timber treatments use white spirit, or light oils such as kerosene, as the solvent carrier to deliver preservative compounds into timber. Synthetic pyrethroids are typically used as an insecticide, such as permethrin, bifenthrin or deltamethrin. In Australia and New Zealand, the most common formulations use Permethrin as an insecticide, and Propaconazole and Tebuconazole as fungicides. While still using a chemical preservative, this formulation contains no heavy-metal compounds.\n\nWith the introduction of strict volatile organic compound (VOC) laws in the European Union, LOSPs have disadvantages due to the high cost and long process times associated with vapour-recovery systems. LOSPs have been emulsified into water-based solvents. While this does significantly reduce VOC emissions, the timber swells during treatment, removing many of the advantages of LOSP formulations.\n\nVarious Epoxy resins usually thinned with a solvent like acetone or MEK can be used to both preserve and seal wood.\n\nBiological modified timber is treated with biopolymers from agricultural waste. After drying and curing, the soft timber becomes durable and strong. With this process fast growing pinewood reaches similar properties of tropical hardwood. Production facilities for this process are in The Netherlands and is known under the trade name “NobelWood”.\n\nFrom agricultural waste, like sugarcane bagasse, furfuryl alcohol is manufactured. Theoretically this alcohol can be from any bio-mass waste fermented and therefore can be called a green chemical. After condensation reactions pre-polymers are formed from furfuryl alcohol. \nFast growing softwood is impregnated with the water-soluble bio-polymer. After impregnation the wood is dried and heated up which initiates a polymerisation reaction between the bio-polymer and the wood cell. This process results in wood cells which are resistant against micro-organisms.\nAt the moment the only timber specie which is being used for this process is Pinus radiata. This is the fastest growing tree specie on Earth that has a porous structure, which is particularly suitable for impregnation processes.\n\nThe technique is applied on timber mainly for the building industry as a cladding material. The technique is being further developed in order to reach similar physical and biological properties of other polyfurfuryl impregnated wood species. \nBesides the impregnation with the biopolymers the timber can also be impregnated with a fire retardant resins. This combination creates a timber with durability class I and with a fire safety certification of Euro class B.\n\nChemical modification of wood at the molecular level has been used to improve its performance properties. Many chemical reaction systems for the modification of wood, especially those using various types of anhydrides, have been published; however, the reaction of wood with acetic anhydride has been the most studied.\n\nThe physical properties of any material are determined by its chemical structure. Wood contains an abundance of chemical groups called \"free hydroxyls\". Free hydroxyl groups readily absorb and release water according to changes in the climatic conditions to which they are exposed. This is the main reason why wood's dimensional stability is impacted by swelling and shrinking. It is also believed that the digestion of wood by enzymes initiates at the free hydroxyl sites, which is one of the principal reasons why wood is prone to decay.\n\nAcetylation effectively changes the free hydroxyls within wood into acetyl groups. This is done by reacting the wood with acetic anhydride, which comes from acetic acid. When free hydroxyl groups are transformed to acetyl groups, the ability of the wood to absorb water is greatly reduced, rendering the wood more dimensionally stable and, because it is no longer digestible, extremely durable. In general, softwoods naturally have an acetyl content from 0.5 to 1.5% and more durable hardwoods from 2 to 4.5%. Acetylation takes wood well beyond these levels with corresponding benefits. These include an extended coatings life due to acetylated wood acting as a more stable substrate for paints and translucent coatings. Acetylated wood is non-toxic and does not have the environmental issues associated with traditional preservation techniques.\n\nThe acetylation of wood was first done in Germany in 1928 by Fuchs. In 1946, Tarkow, Stamm and Erickson first described the use of wood acetylation to stabilize wood from swelling in water. Since the 1940s, many laboratories around the world have looked at acetylation of many different types of woods and agricultural resources.\n\nIn spite of the vast amount of research on chemical modification of wood, and, more specifically, on the acetylation of wood, commercialization did not come easily. The first patent on the acetylation of wood was filed by Suida in Austria in 1930. Later, in 1947, Stamm and Tarkow filed a patent on the acetylation of wood and boards using pyridine as a catalyst. In 1961, the Koppers Company published a technical bulletin on the acetylation of wood using no catalysis, but with an organic cosolvent In 1977, in Russia, Otlesnov and Nikitina came close to commercialization, but the process was discontinued, presumably because cost-effectiveness could not be achieved. In 2007, Titan Wood, a London-based company, with production facilities in The Netherlands, achieved cost-effective commercialization and began large-scale production of acetylated wood under the trade name \"Accoya\".\n\nCopper plating or Copper sheathing is the practice of covering wood most usually wooden hulls of ships with copper metal. As metallic copper is both repellent and toxic to fungus, insects such as termites, and marine bi-valves this would preserve the wood and also act as an anti-fouling measure to prevent aquatic life from attaching to the ship's hull and reducing a ship's speed and maneuverability.\n\nThese species are resistant to decay in their natural state, due to high levels of organic chemicals called \"extractives\", mainly polyphenols. Extractives are chemicals that are deposited in the heartwood of certain tree species as they convert sapwood to heartwood. Huon pine (\"Lagarostrobos franklinii\"), merbau (\"Intsia bijuga\"), ironbark (\"Eucalyptus\" spp.), tōtara (\"Podocarpus totara\"), puriri (\"Vitex lucens\"), kauri (\"Agathis australis\"), and many cypresses, such as coast redwood (\"Sequoia sempervirens\") and western red cedar (\"Thuja plicata\"), fall in this category. However, many of these species tend to be prohibitively expensive for general construction applications.\n\nHuon pine was used for ship hulls in the 19th century, but over-harvesting and Huon pine's extremely slow growth rate makes this now a speciality timber. Huon pine is so rot resistant, that fallen trees from many years ago are still commercially valuable.\n\nMerbau is still a popular decking timber and has a long life in above ground applications, but it is logged in an unsustainable manner and is too hard and brittle for general use.\n\nIronbark is a good choice where available. It is harvested from both old-growth and plantation in Australia and is highly resistant to rot and termites. It is most commonly used for fence posts and house stumps.\n\nEastern red cedar (\"Juniperus virginiana\") and black locust (\"Robinia pseudoacacia\") have long been used for rot-resistant fence posts and rails in eastern United States, with the black locust also planted in modern times in Europe. Coast redwood is commonly used for similar applications in the western United States.\n\nTōtara and puriri were used extensively in New Zealand during the European colonial era when native forests were \"mined\", even as fence posts of which many are still operating. Tōtara was used by the Māori to build large \"waka\" (canoes). Today, they are specialty timbers as a result of their scarcity, although lower grade stocks are sold for landscaping use.\n\nKauri is a superb timber for building the hulls and decks of boats. It too is now a specialty timber and ancient logs (in excess of 3,000 years) that have been mined from swamps are used by wood turners and furniture makers.\n\nThe natural durability or rot and insect resistance of wood species is always based on the heartwood (or \"truewood\"). The sapwood of all timber species should be considered to be non-durable without preservative treatment.\n\nTung oil has been used for hundreds of years in China, where it was used as a preservative for wood ships. The oil penetrates the wood, and then hardens to form an impermeable hydrophobic layer up to 5 mm into the wood. As a preservative it is effective for exterior work above and below ground, but the thin layer makes it less useful in practice. It is not available as a pressure treatment.\n\nBy going beyond kiln drying wood, heat treatment may make timber more durable. By heating timber to a certain temperature, it may be possible to make the wood fibre less appetizing to insects.\n\nHeat treatment can also improve the properties of the wood with respect to water, with lower equilibrium moisture, less moisture deformation, and weather resistance. It is weather-resistant enough to be used unprotected, in facades or in kitchen tables, where wetting is expected.\n\nThere are four similar heat treatments — Westwood, developed in the United States; Retiwood, developed in France; Thermowood, developed in Finland by VTT; and Platowood, developed in The Netherlands. These processes autoclave the treated wood, subjecting it to pressure and heat, along with nitrogen or water vapour to control drying in a staged treatment process ranging from 24 to 48 hours at temperatures of 180 °C to 230 °C depending on timber species. These processes increase the durability, dimensional stability and hardness of the treated wood by at least one class; however, the treated wood is darkened in colour, and there are changes in certain mechanical characteristics: Specifically, the modulus of elasticity is increased to 10%, and the modulus of rupture is diminished by 5% to 20%; thus, the treated wood requires drilling for nailing to avoid splitting the wood. Certain of these processes cause less impact than others in their mechanical effects upon the treated wood. Wood treated with this process is often used for cladding or siding, flooring, furniture and windows.\n\nFor the control of pests that may be harbored in wood packaging material (i.e. crates and pallets), the ISPM 15 requires heat treatment of wood to 56 °C for 30 minutes to receive the HT stamp. This is typically required to ensure the killing of the pine wilt nematode and other kinds of wood pests that could be transported internationally.\n\nWood and bamboo can be buried in mud to help protect them from insects and decay. This practice is used widely in Vietnam to build farm houses consisting of a wooden structural frame, a bamboo roof frame and bamboo with mud mixed with rice hay for the walls. While wood in contact with soil will generally decompose more quickly than wood not in contact with it, it is possible that the predominantly clay soils prevalent in Vietnam provide a degree of mechanical protection against insect attack, which compensates for the accelerated rate of decay.\n\nAlso, since wood is only subject to bacterial decay under specific temperature and moisture content ranges, submerging it in water-saturated mud can retard decay, by saturating the wood's internal cells beyond their moisture decay range.\n\nProbably the first attempts made to protect wood from decay and insect attack consisted of brushing or rubbing preservatives onto the surfaces of the treated wood. Through trial and error the most effective preservatives and application processes were slowly determined. In the Industrial Revolution, demands for such things as telegraph poles and railroad ties (UK: railway sleepers) helped to fuel an explosion of new techniques that emerged in the early 19th century. The sharpest rise in inventions took place between 1830 and 1840, when Bethell, Boucherie, Burnett and Kyan were making wood-preserving history. Since then, numerous processes have been introduced or existing processes improved. The goal of modern-day wood preservation is to ensure a deep, uniform penetration with reasonable cost, without endangering the environment. The most widespread application processes today are those using artificial pressure through which many woods are being effectively treated, but several species (such as spruce, Douglas-fir, larch, hemlock and fir) are very resistant to impregnation. With the use of incising, the treatment of these woods has been somewhat successful but with a higher cost and not always satisfactory results. One can divide the wood-preserving methods roughly into either non-pressure processes or pressure processes.\n\nThere are numerous non-pressure processes of treating wood which vary primarily in their procedure. The most common of these treatments involve the application of the preservative by means of brushing or spraying, dipping, soaking, steeping or by means of hot and cold bath. There is also a variety of additional methods involving charring, applying preservatives in bored holes, diffusion processes and sap displacement.\n\nBrushing preservatives is a long-practised method and often used in today's carpentry workshops. Technological developments mean it is also possible to spray preservative over the surface of the timber. Some of the liquid is drawn into the wood as the result of capillary action before the spray runs off or evaporates, but unless puddling occurs penetration is limited and may not be suitable for long-term weathering. By using the spray method, coal-tar creosote, oil-borne solutions and water-borne salts (to some extent) can also be applied. A thorough brush or spray treatment with coal-tar creosote can add 1 to 3 years to the lifespan of poles or posts. Two or more coats provide better protection than one, but the successive coats should not be applied until the prior coat has dried or soaked into the wood. The wood should be seasoned before treatment.\n\nDipping consists of simply immersing the wood in a bath of creosote or other preservative for a few seconds or minutes. Similar penetrations to that of brushing and spraying processes are achieved. It has the advantage of minimizing hand labor. It requires more equipment and larger quantities of preservative and is not adequate for treating small lots of timber. Usually the dipping process is useful in the treatment of window sashes and doors. Except for copper naphthenate, treatment with copper salt preservative is no longer allowed with this method.\n\nIn this process the wood is submerged in a tank of water-preservative mix, and allowed to soak for a longer period of time (several days to weeks). This process was developed in the 19th century by John Kyan. The depth and retention achieved depends on factors such as species, wood moisture, preservative and soak duration. The majority of the absorption takes place during the first two or three days, but will continue at a slower pace for an indefinite period. As a result, the longer the wood can be left in the solution, the better treatment it will receive. When treating seasoned timber, both the water and the preservative salt soak into the wood, making it necessary to season the wood a second time. Posts and poles can be treated directly on endangered areas, but should be treated at least above the future ground level.\n\nThe depth obtained during regular steeping periods varies from up to by sap pine. Due to the low absorption, solution strength should be somewhat stronger than that in pressure processes, around 5% for seasoned timber and 10% for green timber (because the concentration slowly decreases as the chemicals diffuse into the wood). The solution strength should be controlled continually and, if necessary, be corrected with the salt additive. After the timber is removed from the treatment tank, the chemical will continue to spread within the wood if it has sufficient moisture content. The wood should be weighed down and piled so that the solution can reach all surfaces. (Sawed materials stickers should be placed between every board layer.) This process finds minimal use despite its former popularity in continental Europe and Great Britain.\n\nNamed after John Howard Kyan, who patented this process in England in 1833, Kyanizing consists of steeping wood in a 0.67% mercuric chloride preservative solution. It is no longer used.\n\nPatented by Charles A. Seely, this process achieves treatment by immersing seasoned wood in successive baths of hot and cold preservatives. During the hot baths, the air expands in the timbers. When the timbers are changed to the cold bath (the preservative can also be changed) a partial vacuum is created within the lumen of the cells, causing the preservative to be drawn into the wood. Some penetration occurs during the hot baths, but most of it takes place during the cold baths. This cycle is repeated with a significant time reduction compared to other steeping processes. Each bath may last 4 to 8 hours or in some cases longer. The temperature of the preservative in the hot bath should be between and in the cold bath (depending on preservative and tree species). The average penetration depths achieved with this process ranges from . Both preservative oils and water-soluble salts can be used with this treatment. Due to the longer treatment periods, this method finds little use in the commercial wood preservation industry today.\n\nAs explained in Uhlig's Corrosion Handbook, this process involves two or more chemical baths that undergo a reaction with the cells of the wood, and result in the precipitation of preservative into the wood cells. Two chemicals commonly employed in this process are Copper Ethanolamine, and Sodium Dimethyldithiocarbamate, which reacts to precipitate Copper Dimetyldithiocarbamate. The precipitated preservative is very resistant to leeching. Since its use in the mid 1990s, it has been discontinued in the United States of America, however it never saw commercialization in Canada.\n\nPressure processes are the most permanent method around today in preserving timber life. Pressure processes are those in which the treatment is carried out in closed cylinders with applied pressure or vacuum. These processes have a number of advantages over the non-pressure methods. In most cases, a deeper and more uniform penetration and a higher absorption of preservative is achieved. Another advantage is that the treating conditions can be controlled so that retention and penetration can be varied. These pressure processes can be adapted to large-scale production. The high initial costs for equipment and the energy costs are the biggest disadvantages. These treatment methods are used to protect ties, poles and structural timbers and find use throughout the world today. The various pressure processes that are used today differ in details, but the general method is in all cases the same. The treatment is carried out in cylinders. The timbers are loaded onto special tram cars, so called \"buggies\" or \"bogies\", and into the cylinder. These cylinders are then set under pressure often with the addition of higher temperature. As final treatment, a vacuum is frequently used to extract excess preservatives. These cycles can be repeated to achieve better penetration.\n\nLOSP treatments often use a vacuum impregnation process. This is possible because of the lower viscosity of the white-spirit carrier used.\n\nIn the full-cell process, the intent is to keep as much of the liquid absorbed into the wood during the pressure period as possible, thus leaving the maximum concentration of preservatives in the treated area. Usually, water solutions of preservative salts are employed with this process, but it is also possible to impregnate wood with oil. The desired retention is achieved by changing the strength of the solution. William Burnett patented this development in 1838 of full-cell impregnation with water solutions. The patent covered the use of zinc chloride on water basis, also known as \"Burnettizing\". A full-cell process with oil was patented in 1838 by John Bethell. His patent described the injection of tar and oils into wood by applying pressure in closed cylinders. This process is still used today with some improvements.\n\nContrary to the static full-cell and empty-cell processes, the fluctuation process is a dynamic process. By this process the pressure inside the impregnation cylinder changes between pressure and vacuum within a few seconds. There have been inconsistent claims that through this process it is possible to reverse the pit closure by spruce. However, the best results that have been achieved with this process by spruce do not exceed a penetration deeper than . Specialized equipment is necessary and therefore higher investment costs are incurred.\n\nDeveloped by Dr. Boucherie of France in 1838, this approach consisted of attaching a bag or container of preservative solution to a standing or a freshly cut tree with bark, branches, and leaves still attached, thereby injecting the liquid into the sap stream. Through transpiration of moisture from the leaves the preservative is drawn upward through the sapwood of the tree trunk.\n\nThe modified Boucherie process consists of placing freshly cut, unpeeled timbers onto declining skids, with the stump slightly elevated, then fastening watertight covering caps or boring a number of holes into the ends, and inserting a solution of copper sulfate or other waterborne preservative into the caps or holes from an elevated container. Preservative oils tend to not penetrate satisfactorily by this method. The hydrostatic pressure of the liquid forces the preservative lengthwise into and through the sapwood, thus pushing the sap out of the other end of the timber. After a few days, the sapwood is completely impregnated; unfortunately little or no penetration takes place in the heartwood. Only green wood can be treated in this manner. This process has found considerable usage to impregnate poles and also larger trees in Europe and North America, and has experienced a revival of usage to impregnate bamboo in countries such as Costa Rica, Bangladesh, India and the state of Hawaii.\n\nDeveloped in the Philippines, this method (abbreviated HPSD) consists of a cylinder pressure cap made from a 3 mm thick mild steel plate secured with 8 sets of bolts, a 2-HP diesel engine, and a pressure regulator with 1.4–14 kg/m capacity. The cap is placed over the stump of a pole, tree or bamboo and the preservative is forced into the wood with pressure from the engine.\n\nFirst tested and patented by Kolossvary, Haltenberger, and Berdenich of Austria in 1911 and 1912 (U.S. patents. 1,012,207 and 1,018,624) with several improvements from O. P. M. Goss, D. W. Edwards and J. H. Mansfield among others, this process consists of making shallow, slit-like holes in the surfaces of material to be treated, so that deeper and more uniform penetration of preventative may be obtained. The term \"incising\" or perforating comes from the Latin \"incidere\", a compound of \"in\" and \"caedere\" (to cut). Incisions made in sawed material usually are parallel with the grain of the wood. This process is common in North America (since the 1950s), where Douglas-fir products and pole butts of various species are prepared before treatment. It is most useful for woods that are resistant to side penetration, but allow preservative transport along the grain. In the region in which it is produced, it is common practice to incise all sawed Douglas-fir or more in thickness before treatment.\n\nUnfortunately, the impregnation of spruce, the most important structural timber in large areas in Europe, has shown that unsatisfactory treatment depths have been achieved with impregnation. The maximum penetration of is not sufficient to protect wood in weathered positions. The present-day incising machines consist essentially of four revolving drums fitted with teeth or needles or with lasers that burn the incisions into the wood. Preservatives can be spread along the grain up to in radial and up to in tangential and radial direction.\n\nIn North America, where smaller timber dimensions are common, incision depths of have become standard. In Europe, where larger dimensions are widespread, incision depths of are necessary. The incisions are visible and often considered to be wood error. Incisions by laser are significantly smaller than those of spokes or needles. The costs for each process type are approximately for spoke/conventional all-round incising €0.50/m, by laser incising €3.60/m and by needle incision €1.00/m. (Figures originate from the year 1998 and may vary from present day prices.)\n\nAn alternative increases the permeability of timber using microwave technology. There is some concern that this method may adversely affect the structural performance of the material. Research in this area has been conducted by the Cooperative Research Centre at the University of Melbourne, Australia.\n\nCharring of timber results in surfaces which are fire-resistant, insect-resistant and proof against weathering. Wood surfaces are ignited using a hand-held burner or moved slowly across a fire. The charred surface is then cleaned using a steel brush to remove loose bits and to expose the grain. Oil or varnish may be applied if required. In Japan this traditional technique is called \"yakisugi\" or \"shō sugi ban\".\n\n\n\n\n\n\n"}
