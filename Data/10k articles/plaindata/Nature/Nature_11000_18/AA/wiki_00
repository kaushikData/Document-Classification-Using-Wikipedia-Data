{"id": "43838810", "url": "https://en.wikipedia.org/wiki?curid=43838810", "title": "Autumn in New England", "text": "Autumn in New England\n\nThe autumn in New England marks the transition from summer to winter in New England, United States. The autumn color of trees in New England has been said to be some of the most brilliant color in the United States. Travelers from many areas of the USA and even Europe flock to Vermont, New Hampshire, Maine, and parts of Massachusetts to see the colors each fall. Hiking has become popular among the tourists, and several areas offer guided tours.\n\nThe combination of the natural rugged landscape with the rural and small town villages has made several areas in New England iconic in terms of fall color photography. The numerous barns, church buildings, farmhouses, and villages combined with the bright oranges, reds, and yellows makes for iconic foliage photographs. \n\nAcross most of New England, by mid September the days are still warm, but the nights have become cool, and this is the process that begins the colorful change in trees that occur each fall. Trees sense the lowering of the solar angle and reduced hours of daylight, this starts the process of the chlorophyll breaking down, the green color disappears, and the yellow to orange colors become visible. This change is most acute in sugar maple trees. This change in color starts in far Northern New England in northern Maine and the higher elevations of Vermont and New Hampshire in mid September, reaching central New England areas of southern Vermont, southern New Hampshire, and Massachusetts by early October. By mid October the color peak reaches northern Rhode Island and northern Connecticut. \n\nAs one reaches southern Connecticut southward toward into New Jersey and points south, the number of sugar maple trees declines as the climate changes to a more temperate zone and oaks become more dominant, thus there are less bright colors.\n\nAutumn in New England has also become a popular theme in American popular culture, with many fictional stories being set in autumntime New England.\n"}
{"id": "54838", "url": "https://en.wikipedia.org/wiki?curid=54838", "title": "Biogas", "text": "Biogas\n\nBiogas refers to a mixture of different gases produced by the breakdown of organic matter in the absence of oxygen. Biogas can be produced from raw materials such as agricultural waste, manure, municipal waste, plant material, sewage, green waste or food waste. Biogas is a renewable energy source.\n\nBiogas is produced by anaerobic digestion with methanogen or anaerobic organisms, which digest material inside a closed system, or fermentation of biodegradable materials. This closed system is called an anaerobic digester, biodigester or a bioreactor.\n\nBiogas is primarily methane () and carbon dioxide () and may have small amounts of hydrogen sulphide (), moisture and siloxanes. The gases methane, hydrogen, and carbon monoxide () can be combusted or oxidized with oxygen. This energy release allows biogas to be used as a fuel; it can be used for any heating purpose, such as cooking. It can also be used in a gas engine to convert the energy in the gas into electricity and heat.\n\nBiogas can be compressed, the same way as natural gas is compressed to CNG, and used to power motor vehicles. In the United Kingdom, for example, biogas is estimated to have the potential to replace around 17% of vehicle fuel. It qualifies for renewable energy subsidies in some parts of the world. Biogas can be cleaned and upgraded to natural gas standards, when it becomes bio-methane. Biogas is considered to be a renewable resource because its production-and-use cycle is continuous, and it generates no net carbon dioxide. As the organic material grows, it is converted and used. It then regrows in a continually repeating cycle. From a carbon perspective, as much carbon dioxide is absorbed from the atmosphere in the growth of the primary bio-resource as is released, when the material is ultimately converted to energy.\n\nThe biogas is a renewable energy that can be used for heating, electricity, and many other operations that use a reciprocating internal combustion engine, such as GE Jenbacher or Caterpillar gas engines. To provide these internal combustion engines with biogas having ample gas pressure to optimize combustion, within the European Union ATEX centrifugal fan units built in accordance with the European directive 2014/34/EU (previously 94/9/EG) are obligatory. These centrifugal fan units, for example Combimac, Meidinger AG or Witt & Sohn AG are suitable for use in Zone 1 and 2 .\n\nOther internal combustion engines such as gas turbines are suitable for the conversion of biogas into both electricity and heat. The digestate is the remaining inorganic matter that was not transformed into biogas. It can be used as an agricultural fertiliser.\n\nBiogas is produced either; \n\nProjects such as NANOCLEAN are nowadays developing new ways to produce biogas more efficiently, using iron oxide nanoparticles in the processes of organic waste treatment. This process can triple the production of biogas.\n\nA \"biogas plant\" is the name often given to an anaerobic digester that treats farm wastes or energy crops. It can be produced using anaerobic digesters (air-tight tanks with different configurations). These plants can be fed with energy crops such as maize silage or biodegradable wastes including sewage sludge and food waste. During the process, the micro-organisms transform biomass waste into biogas (mainly methane and carbon dioxide) and digestate.\n\nThere are two key processes: mesophilic and thermophilic digestion which is dependent on temperature. In experimental work at University of Alaska Fairbanks, a 1000-litre digester using psychrophiles harvested from \"mud from a frozen lake in Alaska\" has produced 200–300 liters of methane per day, about 20%–30% of the output from digesters in warmer climates.\n\nThe air pollution produced by biogas is similar to that of natural gas. The content of toxic hydrogen sulfide presents additional risks and has been responsible for serious accidents . Leaks of unburned methane are an additional risk, because methane is a potent greenhouse gas.\n\nBiogas can be explosive when mixed in the ratio of one part biogas to 8–20 parts air. Special safety precautions have to be taken for entering an empty biogas digester for maintenance work. It is important that a biogas system never has negative pressure as this could cause an explosion. Negative gas pressure can occur if too much gas is removed or leaked; Because of this biogas should not be used at pressures below one column inch of water, measured by a pressure gauge.\n\nFrequent smell checks must be performed on a biogas system. If biogas is smelled anywhere windows and doors should be opened immediately. If there is a fire the gas should be shut off at the gate valve of the biogas system.\n\nLandfill gas is produced by wet organic waste decomposing under anaerobic conditions in a biogas.\n\nThe waste is covered and mechanically compressed by the weight of the material that is deposited above. This material prevents oxygen exposure thus allowing anaerobic microbes to thrive. Biogas builds up and is slowly released into the atmosphere if the site has not been engineered to capture the gas. Landfill gas released in an uncontrolled way can be hazardous since it can become explosive when it escapes from the landfill and mixes with oxygen. The lower explosive limit is 5% methane and the upper is 15% methane.\n\nThe methane in biogas is 28 times more potent a greenhouse gas than carbon dioxide. Therefore, uncontained landfill gas, which escapes into the atmosphere may significantly contribute to the effects of global warming. In addition, volatile organic compounds (VOCs) in landfill gas contribute to the formation of photochemical smog.\n\nBiochemical oxygen demand (BOD) is a measure of the amount of oxygen required by aerobic micro-organisms to decompose the organic matter in a sample of material being used in the biodigester as well as the BOD for the liquid discharge allows for the calculation of the daily energy output from a biodigester.\n\nAnother term related to biodigesters is effluent dirtiness, which tells how much organic material there is per unit of biogas source. Typical units for this measure are in mg BOD/litre. As an example, effluent dirtiness can range between 800–1200 mg BOD/litre in Panama.\n\nFrom 1 kg of decommissioned kitchen bio-waste, 0.45 m³ of biogas can be obtained. The price for collecting biological waste from households is approximately €70 per ton.\n\nThe composition of biogas varies depending upon the substrate composition, as well as the conditions within the anaerobic reactor (temperature, pH, and substrate concentration). Landfill gas typically has methane concentrations around 50%. Advanced waste treatment technologies can produce biogas with 55%–75% methane, which for reactors with free liquids can be increased to 80%–90% methane using in-situ gas purification techniques. As produced, biogas contains water vapor. The fractional volume of water vapor is a function of biogas temperature; correction of measured gas volume for water vapour content and thermal expansion is easily done via simple mathematics which yields the standardized volume of dry biogas.\n\nIn some cases, biogas contains siloxanes. They are formed from the anaerobic decomposition of materials commonly found in soaps and detergents. During combustion of biogas containing siloxanes, silicon is released and can combine with free oxygen or other elements in the combustion gas. Deposits are formed containing mostly silica () or silicates () and can contain calcium, sulfur, zinc, phosphorus. Such \"white mineral\" deposits accumulate to a surface thickness of several millimeters and must be removed by chemical or mechanical means.\n\nPractical and cost-effective technologies to remove siloxanes and other biogas contaminants are available.\n\nFor 1000 kg (wet weight) of input to a typical biodigester, total solids may be 30% of the wet weight while volatile suspended solids may be 90% of the total solids. Protein would be 20% of the volatile solids, carbohydrates would be 70% of the volatile solids, and finally fats would be 10% of the volatile solids.\n\nHigh levels of methane are produced when manure is stored under anaerobic conditions. During storage and when manure has been applied to the land, nitrous oxide is also produced as a byproduct of the denitrification process. Nitrous oxide () is 320 times more aggressive as a greenhouse gas than carbon dioxide and methane 25 times more than carbon dioxide.\n\nBy converting cow manure into methane biogas via anaerobic digestion, the millions of cattle in the United States would be able to produce 100 billion kiloWatt hours of electricity, enough to power millions of homes across the United States. In fact, one cow can produce enough manure in one day to generate 3 kiloWatt hours of electricity; only 2.4 kiloWatt hours of electricity are needed to power a single 100-Watt light bulb for one day. Furthermore, by converting cattle manure into methane biogas instead of letting it decompose, global warming gases could be reduced by 99 million metric tons or 4%.\n\nBiogas can be used for electricity production on sewage works, in a CHP gas engine, where the waste heat from the engine is conveniently used for heating the digester; cooking; space heating; water heating; and process heating. If compressed, it can replace compressed natural gas for use in vehicles, where it can fuel an internal combustion engine or fuel cells and is a much more effective displacer of carbon dioxide than the normal use in on-site CHP plants.\n\nRaw biogas produced from digestion is roughly 60% methane and 29% with trace elements of : inadequate for use in machinery. The corrosive nature of alone is enough to destroy the mechanisms.\n\nMethane in biogas can be concentrated via a biogas upgrader to the same standards as fossil natural gas, which itself has to go through a cleaning process, and becomes \"biomethane\". If the local gas network allows, the producer of the biogas may use their distribution networks. Gas must be very clean to reach pipeline quality and must be of the correct composition for the distribution network to accept. Carbon dioxide, water, hydrogen sulfide, and particulates must be removed if present.\n\nThere are four main methods of upgrading: water washing, pressure swing absorption, selexol absorption, and amine gas treating. In addition to these, the use of membrane separation technology for biogas upgrading is increasing, and there are already several plants operating in Europe and USA.\n\nThe most prevalent method is water washing where high pressure gas flows into a column where the carbon dioxide and other trace elements are scrubbed by cascading water running counter-flow to the gas. This arrangement could deliver 98% methane with manufacturers guaranteeing maximum 2% methane loss in the system. It takes roughly between 3% and 6% of the total energy output in gas to run a biogas upgrading system.\n\nGas-grid injection is the injection of biogas into the methane grid (natural gas grid). Until the breakthrough of micro combined heat and power two-thirds of all the energy produced by biogas power plants was lost (as heat). Using the grid to transport the gas to customers, the energy can be used for on-site generation, resulting in a reduction of losses in the transportation of energy. Typical energy losses in natural gas transmission systems range from 1% to 2%; in electricity transmission they range from 5% to 8%.\n\nBefore being injected in the gas grid, biogas passes a cleaning process, during which it is upgraded to natural gas quality. During the cleaning process trace components harmful to the gas grid and the final users are removed.\n\nIf concentrated and compressed, it can be used in vehicle transportation. Compressed biogas is becoming widely used in Sweden, Switzerland, and Germany. A biogas-powered train, named Biogaståget Amanda (The Biogas Train Amanda), has been in service in Sweden since 2005. Biogas powers automobiles. In 1974, a British documentary film titled \"Sweet as a Nut\" detailed the biogas production process from pig manure and showed how it fueled a custom-adapted combustion engine. In 2007, an estimated 12,000 vehicles were being fueled with upgraded biogas worldwide, mostly in Europe.\n\nBiogas is part of the wet gas and condensing gas (or air) category that includes mist or fog in the gas stream. The mist or fog is predominately water vapor that condenses on the sides of pipes or stacks throughout the gas flow. Biogas environments include wastewater digesters, landfills, and animal feeding operations (covered livestock lagoons).\n\nUltrasonic flow meters are one of the few devices capable of measuring in a biogas atmosphere. Most of thermal flow meters are unable to provide reliable data because the moisture causes steady high flow readings and continuous flow spiking, although there are single-point insertion thermal mass flow meters capable of accurately monitoring biogas flows with minimal pressure drop. They can handle moisture variations that occur in the flow stream because of daily and seasonal temperature fluctuations, and account for the moisture in the flow stream to produce a dry gas value.\n\nThe European Union has legislation regarding waste management and landfill sites called the Landfill Directive.\n\nCountries such as the United Kingdom and Germany now have legislation in force that provides farmers with long-term revenue and energy security.\n\nThe United States legislates against landfill gas as it contains VOCs. The United States Clean Air Act and Title 40 of the Code of Federal Regulations (CFR) requires landfill owners to estimate the quantity of non-methane organic compounds (NMOCs) emitted. If the estimated NMOC emissions exceeds 50 tonnes per year, the landfill owner is required to collect the gas and treat it to remove the entrained NMOCs. Treatment of the landfill gas is usually by combustion. Because of the remoteness of landfill sites, it is sometimes not economically feasible to produce electricity from the gas.\n\nWith the many benefits of biogas, it is starting to become a popular source of energy and is starting to be used in the United States more. In 2003, the United States consumed 147 trillion BTU of energy from \"landfill gas\", about 0.6% of the total U.S. natural gas consumption. Methane biogas derived from cow manure is being tested in the U.S. According to a 2008 study, collected by the \"Science and Children\" magazine, methane biogas from cow manure would be sufficient to produce 100 billion kilowatt hours enough to power millions of homes across America. Furthermore, methane biogas has been tested to prove that it can reduce 99 million metric tons of greenhouse gas emissions or about 4% of the greenhouse gases produced by the United States.\n\nIn Vermont, for example, biogas generated on dairy farms was included in the CVPS Cow Power program. The program was originally offered by Central Vermont Public Service Corporation as a voluntary tariff and now with a recent merger with Green Mountain Power is now the GMP Cow Power Program. Customers can elect to pay a premium on their electric bill, and that premium is passed directly to the farms in the program. In Sheldon, Vermont, Green Mountain Dairy has provided renewable energy as part of the Cow Power program. It started when the brothers who own the farm, Bill and Brian Rowell, wanted to address some of the manure management challenges faced by dairy farms, including manure odor, and nutrient availability for the crops they need to grow to feed the animals. They installed an anaerobic digester to process the cow and milking center waste from their 950 cows to produce renewable energy, a bedding to replace sawdust, and a plant-friendly fertilizer. The energy and environmental attributes are sold to the GMP Cow Power program. On average, the system run by the Rowells produces enough electricity to power 300 to 350 other homes. The generator capacity is about 300 kilowatts.\n\nIn Hereford, Texas, cow manure is being used to power an ethanol power plant. By switching to methane biogas, the ethanol power plant has saved 1000 barrels of oil a day. Over all, the power plant has reduced transportation costs and will be opening many more jobs for future power plants that will rely on biogas.\n\nIn Oakley, Kansas, an ethanol plant considered to be one of the largest biogas facilities in North America is using Integrated Manure Utilization System \"IMUS\" to produce heat for its boilers by utilizing feedlot manure, municipal organics and ethanol plant waste. At full capacity the plant is expected to replace 90% of the fossil fuel used in the manufacturing process of ethanol and methanol.\n\nThe level of development varies greatly in Europe. While countries such as Germany, Austria and Sweden are fairly advanced in their use of biogas, there is a vast potential for this renewable energy source in the rest of the continent, especially in Eastern Europe. Different legal frameworks, education schemes and the availability of technology are among the prime reasons behind this untapped potential. Another challenge for the further progression of biogas has been negative public perception.\n\nIn February 2009, the European Biogas Association (EBA) was founded in Brussels as a non-profit organisation to promote the deployment of sustainable biogas production and use in Europe. EBA's strategy defines three priorities: establish biogas as an important part of Europe’s energy mix, promote source separation of household waste to increase the gas potential, and support the production of biomethane as vehicle fuel. In July 2013, it had 60 members from 24 countries across Europe.\n\n, there are about 130 non-sewage biogas plants in the UK. Most are on-farm, and some larger facilities exist off-farm, which are taking food and consumer wastes.\n\nOn 5 October 2010, biogas was injected into the UK gas grid for the first time. Sewage from over 30,000 Oxfordshire homes is sent to Didcot sewage treatment works, where it is treated in an anaerobic digestor to produce biogas, which is then cleaned to provide gas for approximately 200 homes.\n\nIn 2015 the Green-Energy company Ecotricity announced their plans to build three grid-injecting digesters.\n\n, in Italy there are more than 200 biogas plants with a production of about 1.2GW \n\nGermany is Europe's biggest biogas producer and the market leader in biogas technology. In 2010 there were 5,905 biogas plants operating throughout the country: Lower Saxony, Bavaria, and the eastern federal states are the main regions. Most of these plants are employed as power plants. Usually the biogas plants are directly connected with a CHP which produces electric power by burning the bio methane. The electrical power is then fed into the public power grid. In 2010, the total installed electrical capacity of these power plants was 2,291 MW. The electricity supply was approximately 12.8 TWh, which is 12.6% of the total generated renewable electricity.\n\nBiogas in Germany is primarily extracted by the co-fermentation of energy crops (called 'NawaRo', an abbreviation of \"nachwachsende Rohstoffe\", German for renewable resources) mixed with manure. The main crop used is corn. Organic waste and industrial and agricultural residues such as waste from the food industry are also used for biogas generation. In this respect, biogas production in Germany differs significantly from the UK, where biogas generated from landfill sites is most common.\n\nBiogas production in Germany has developed rapidly over the last 20 years. The main reason is the legally created frameworks. Government support of renewable energy started in 1991 with the Electricity Feed-in Act (\"StrEG\"). This law guaranteed the producers of energy from renewable sources the feed into the public power grid, thus the power companies were forced to take all produced energy from independent private producers of green energy. In 2000 the Electricity Feed-in Act was replaced by the Renewable Energy Sources Act (\"EEG\"). This law even guaranteed a fixed compensation for the produced electric power over 20 years. The amount of around 8¢/kWh gave farmers the opportunity to become energy suppliers and gain a further source of income.\n\nThe German agricultural biogas production was given a further push in 2004 by implementing the so-called NawaRo-Bonus. This is a special payment given for the use of renewable resources, that is, energy crops. In 2007 the German government stressed its intention to invest further effort and support in improving the renewable energy supply to provide an answer on growing climate challenges and increasing oil prices by the ‘Integrated Climate and Energy Programme’.\n\nThis continual trend of renewable energy promotion induces a number of challenges facing the management and organisation of renewable energy supply that has also several impacts on the biogas production. The first challenge to be noticed is the high area-consuming of the biogas electric power supply. In 2011 energy crops for biogas production consumed an area of circa 800,000 ha in Germany. This high demand of agricultural areas generates new competitions with the food industries that did not exist hitherto. Moreover, new industries and markets were created in predominately rural regions entailing different new players with an economic, political and civil background. Their influence and acting has to be governed to gain all advantages this new source of energy is offering. Finally biogas will furthermore play an important role in the German renewable energy supply if good governance is focused.\n\nBiogas in India has been traditionally based on dairy manure as feed stock and these \"gobar\" gas plants have been in operation for a long period of time, especially in rural India. In the last 2–3 decades, research organisations with a focus on rural energy security have enhanced the design of the systems resulting in newer efficient low cost designs such as the Deenabandhu model.\n\nThe Deenabandhu Model is a new biogas-production model popular in India. (\"Deenabandhu\" means \"friend of the helpless.\") The unit usually has a capacity of 2 to 3 cubic metres. It is constructed using bricks or by a ferrocement mixture. In India, the brick model costs slightly more than the ferrocement model; however, India's Ministry of New and Renewable Energy offers some subsidy per model constructed.\n\nBiogas which is mainly methane/natural gas can also be used for generating protein rich cattle, poultry and fish feed in villages economically by cultivating \"Methylococcus capsulatus\" bacteria culture with tiny land and water foot print. The carbon dioxide gas produced as by product from these plants can be put to use in cheaper production of algae oil or spirulina from algaculture particularly in tropical countries like India which can displace the prime position of crude oil in near future. Union government of India is implementing many schemes to utilise productively the agro waste or biomass in rural areas to uplift rural economy and job potential. With these plants, the non edible biomass or waste of edible biomass is converted in to high value products without any water pollution or green house gas (GHG) emissions.\n\nLPG (Liquefied Petroleum Gas) is a key source of cooking fuel in urban India and its prices have been increasing along with the global fuel prices. Also the heavy subsidies provided by the successive governments in promoting LPG as a domestic cooking fuel has become a financial burden renewing the focus on biogas as a cooking fuel alternative in urban establishments. This has led to the development of prefabricated digester for modular deployments as compared to RCC and cement structures which take a longer duration to construct. Renewed focus on process technology like the Biourja process model has enhanced the stature of medium and large scale anaerobic digester in India as a potential alternative to LPG as primary cooking fuel.\n\nIn India, Nepal, Pakistan and Bangladesh biogas produced from the anaerobic digestion of manure in small-scale digestion facilities is called gobar gas; it is estimated that such facilities exist in over 2 million households in India, 50,000 in Bangladesh and thousands in Pakistan, particularly North Punjab, due to the thriving population of livestock. The digester is an airtight circular pit made of concrete with a pipe connection. The manure is directed to the pit, usually straight from the cattle shed. The pit is filled with a required quantity of wastewater. The gas pipe is connected to the kitchen fireplace through control valves. The combustion of this biogas has very little odour or smoke. Owing to simplicity in implementation and use of cheap raw materials in villages, it is one of the most environmentally sound energy sources for rural needs. One type of these system is the Sintex Digester. Some designs use vermiculture to further enhance the slurry produced by the biogas plant for use as compost.\n\nIn Pakistan, the Rural Support Programmes Network is running the Pakistan Domestic Biogas Programme which has installed 5,360 biogas plants and has trained in excess of 200 masons on the technology and aims to develop the Biogas Sector in Pakistan.\n\nIn Nepal, the government provides subsidies to build biogas plant at home.\n\nThe Chinese have experimented with the applications of biogas since 1958. Around 1970, China had installed 6,000,000 digesters in an effort to make agriculture more efficient. During the last years the technology has met high growth rates. This seems to be the earliest developments in generating biogas from agricultural waste.\n\nDomestic biogas plants convert livestock manure and night soil into biogas and slurry, the fermented manure. This technology is feasible for small-holders with livestock producing 50 kg manure per day, an equivalent of about 6 pigs or 3 cows. This manure has to be collectable to mix it with water and feed it into the plant. Toilets can be connected. Another precondition is the temperature that affects the fermentation process. With an optimum at 36 C° the technology especially applies for those living in a (sub) tropical climate. This makes the technology for small holders in developing countries often suitable.\nDepending on size and location, a typical brick made fixed dome biogas plant can be installed at the yard of a rural household with the investment between US$300 to $500 in Asian countries and up to $1400 in the African context. A high quality biogas plant needs minimum maintenance costs and can produce gas for at least 15–20 years without major problems and re-investments. For the user, biogas provides clean cooking energy, reduces indoor air pollution, and reduces the time needed for traditional biomass collection, especially for women and children. The slurry is a clean organic fertilizer that potentially increases agricultural productivity.\n\nDomestic biogas technology is a proven and established technology in many parts of the world, especially Asia. Several countries in this region have embarked on large-scale programmes on domestic biogas, such as China and India.\n\nThe Netherlands Development Organisation, SNV, supports national programmes on domestic biogas that aim to establish commercial-viable domestic biogas sectors in which local companies market, install and service biogas plants for households. In Asia, SNV is working in Nepal, Vietnam, Bangladesh, Bhutan, Cambodia, Lao PDR, Pakistan and Indonesia, and in Africa; Rwanda, Senegal, Burkina Faso, Ethiopia, Tanzania, Uganda, Kenya, Benin and Cameroon.\n\nIn South Africa a prebuilt Biogas system is manufactured and sold. One key feature is that installation requires less skill and is quicker to install as the digester tank is premade plastic.\n\nIn the 1985 Australian film \"Mad Max Beyond Thunderdome\" the post-apocalyptic settlement Barter town is powered by a central biogas system based upon a piggery. As well as providing electricity, methane is used to power Barter's vehicles.\n\n\"Cow Town\", written in the early 1940s, discuss the travails of a city vastly built on cow manure and the hardships brought upon by the resulting methane biogas. Carter McCormick, an engineer from a town outside the city, is sent in to figure out a way to utilize this gas to help power, rather than suffocate, the city.\n\nThe Biogas production is providing nowadays new opportunities for skilled employment, drawing on the development of new technologies.\n\n\n"}
{"id": "46435788", "url": "https://en.wikipedia.org/wiki?curid=46435788", "title": "CMAQ", "text": "CMAQ\n\nCMAQ is an acronym for the Community Multi-scale Air Quality Model, a sophisticated atmospheric dispersion model developed by the US EPA to address regional air pollution problems. An example of a regional air pollution problem is a multi-state area where ozone or fine particulate levels exceed the US health standards. In addition to simulating the emission, advection, diffusion, and deposition of air pollutants, CMAQ treats a wide array of chemical reactions that occur throughout the lower atmosphere. For example, ozone forms in the atmosphere when nitrogen oxides interact with volatile organic compounds in the presence of sunlight. Ammonium sulfate is formed in fine particulate matter when sulfuric acid (formed largely in cloud water) interacts with gas-phase ammonia. Meteorological conditions such as subsidence inversions, decrease the amount of fresh air available for dilution of air emissions, and increase the rate of production of secondary air pollutants. CMAQ has the capability to accurately predict air pollution concentrations resulting from secondary formation. Like any air dispersion model, CMAQ inputs air pollutant emissions and meteorological data and outputs air pollutant concentrations and deposited totals. Its particular strength is in assessing the efficacy of regional emissions control strategies in reducing regional air pollution levels.\n\nCMAQ may also refer to the Congestion Mitigation and Air Quality Improvement Program, a program of the United States Department of Transportation.\n"}
{"id": "2316067", "url": "https://en.wikipedia.org/wiki?curid=2316067", "title": "Cape Horn Current", "text": "Cape Horn Current\n\nThe Cape Horn Current is a cold water current that flows west-to-east around Cape Horn. This current is caused by the intensification of the West Wind Drift as it rounds the cape.\n\n"}
{"id": "5779365", "url": "https://en.wikipedia.org/wiki?curid=5779365", "title": "Central dense overcast", "text": "Central dense overcast\n\nThe central dense overcast, or CDO, of a tropical cyclone or strong subtropical cyclone is the large central area of thunderstorms surrounding its circulation center, caused by the formation of its eyewall. It can be round, angular, oval, or irregular in shape. This feature shows up in tropical cyclones of tropical storm or hurricane strength. How far the center is embedded within the CDO, and the temperature difference between the cloud tops within the CDO and the cyclone's eye, can help determine a tropical cyclone's intensity. Locating the center within the CDO can be a problem for strong tropical storms and with systems of minimal hurricane strength as its location can be obscured by the CDO's high cloud canopy. This center location problem can be resolved through the use of microwave satellite imagery.\n\nAfter a cyclone reaches hurricane intensity, an eye appears at the center of the CDO, defining its center of low pressure and its cyclonic wind field. Tropical cyclones with changing intensity have more lightning within their CDO than steady state storms. Tracking cloud features within the CDO, using frequently updated satellite imagery, can also be used to determine its intensity. The highest maximum sustained winds within a tropical cyclone, as well as its heaviest rainfall, are usually located under the coldest cloud tops in the CDO.\n\nIt is a large region of thunderstorms surrounding the center of stronger tropical and subtropical cyclones which shows up brightly (with cold cloud tops) on satellite imagery. The CDO forms due to the development of an eyewall within a tropical cyclone. Its shape can be round, oval, angular, or irregular. Its development can be preceded by a narrow, dense, C-shaped convective band. Early in its development, the CDO is often angular or oval in shape, which rounds out, increases in size, and appears more smooth as a tropical cyclone intensifies. Rounder CDO shapes occur in environments with low levels of vertical wind shear.\n\nThe strongest winds within tropical cyclones tend to be located under the deepest convection within the CDO, which is seen on satellite imagery as the coldest cloud tops. The radius of maximum wind is usually collocated with the coldest cloud tops within the CDO, which is also the area where a tropical cyclone's rainfall reaches its maximum intensity. For mature tropical cyclones that are steady state, the CDO contains nearly no lightning activity, though lightning is more common within weaker tropical cyclones and for systems fluctuating in intensity.\n\nThe eye is a region of mostly calm weather at the center of the CDO of strong tropical cyclones. The eye of a storm is a roughly circular area, typically 30–65 km (20–40 miles) in diameter. It is surrounded by the eyewall, a ring of towering thunderstorms surrounding its center of circulation. The cyclone's lowest barometric pressure occurs in the eye, and can be as much as 15% lower than the atmospheric pressure outside the storm. In weaker tropical cyclones, the eye is less well-defined, and can be covered by high cloudiness caused by cirrus cloud outflow from the surrounding central dense overcast.\nWithin the Dvorak satellite strength estimate for tropical cyclones, there are several visual patterns that a cyclone may take on which define the upper and lower bounds on its intensity. The central dense overcast (CDO) pattern is one of those patterns. The central dense overcast utilizes the size of the CDO. The CDO pattern intensities start at T2.5, equivalent to minimal tropical storm intensity, . The shape of the central dense overcast is also considered. The farther the center is tucked into the CDO, the stronger it is deemed. Banding features can be utilized to objectively determine the tropical cyclone's center, using a ten degree logarithmic spiral. Using the 85–92 GHz channels of polar-orbiting microwave satellite imagery can definitively locate the center within the CDO.\n\nTropical cyclones with maximum sustained winds between and can have their center of circulations obscured by cloudiness within visible and infrared satellite imagery, which makes diagnosis of their intensity a challenge. Winds within tropical cyclones can also be estimated by tracking features within the CDO using rapid scan geostationary satellite imagery, whose pictures are taken minutes apart rather than every half-hour.\n"}
{"id": "29648687", "url": "https://en.wikipedia.org/wiki?curid=29648687", "title": "Compact toroid", "text": "Compact toroid\n\nCompact toroids are a class of toroidal plasma configurations that are self-stable, and whose configuration does not require magnet coils running through the center of the toroid. They are studied primarily in the field of fusion energy, where the lack of complex magnets and a simple geometry may allow the construction of dramatically simpler and less expensive fusion reactors. The two best studied compact toroids are the spheromak and field-reversed configuration (FRC). A third configuration, the particle ring, does not appear to have attractive performance.\n\nThe spheromak configuration is similar in arrangement to a smoke ring. The FRC is also toroidal, but extended into a tubular shape or hollow cylinder. The main difference between the two is that the spheromak contains poloidal (vertical rings) and toroidal (horizontal) magnetic fields, while the FRC has only the poloidal fields and requires an external magnet for confinement. In both cases the combination of electrical currents and their associated magnetic fields result in a series of closed magnetic lines that maintains the ring shape, without the need for magnets in the center of the plasma (unlike a tokamak).\n\nOf the two, the FRC naturally has a higher beta, a measure of fusion economics. However, the spheromak had generated better confinement times and temperatures, and recent work suggests that great advances in performance can be made.\n\nCompact toroids are also similar to the spherical tokamak, and many spherical tokamak machines were converted from earlier spheromak reactors.\n\n"}
{"id": "35830148", "url": "https://en.wikipedia.org/wiki?curid=35830148", "title": "Condensation particle counter", "text": "Condensation particle counter\n\nA condensation particle counter or CPC is a particle counter that detects and counts aerosol particles by first enlarging them by using the particles as nucleation centers to create droplets in a supersaturated gas. \n\nThree techniques have been used to produce nucleation:\n\n\nThe most usually used (also the most efficient) method is cooling by thermal diffusion. Most abundantly used working fluid is n-butanol; during last years water is also encountered in this use.\n\nCondensation particle counters are able to detect particles with dimensions from 2 nm and larger. This is of special importance because particles sized down from 50 nm are generally undetectable with conventional optical techniques. Usually the supersaturation is ca. 100…200 % in condensation chamber, despite the fact that heterogeneous nucleation (droplet growth on surface of a suspended solid particle) can occur at supersaturation as small as 1 %. The greater vapour content is needed because, according to surface science laws, the vapour pressure over a convex surface is less than over a plane, thus greater content of vapor in air is required to meet \"actual\" supersaturation criteria. This amount grows (vapor pressure decreases) along with decrease in particle size, the critical diameter for which condensation can occur at the present saturation level is called \"Kelvin diameter\". The supersaturation level must, however, be small enough to prevent homogeneous nucleation (when liquid molecules collide so often that they form clusters – stable enough to ensure further growth is possible), which will produce false counts. This usually starts at ca. 300 % supersaturation. \n\nOperation of a diffusional thermal cooling CPC is depicted on the right. Air passes through a hollow block of porous material in contact with the working liquid, the block being heated to ensure high vapour content. Then the humified air enters the cooler where nucleation occur. Temperature difference between the heater and the cooler determines the supersaturation, which in its turn determines the minimal size of particles that will be detected (the greater the difference, the smaller particles get counted). As proper nucleation conditions occur in the center of the flow, sometimes incoming flow is divided: most of it undergoes filtering and forms the sheath flow, which the rest of flow, still containing particles, is inserted into via a capillary. The more uniform is obtained supersaturation, the sharper is particle minimal size cutoff. During the heterogeneous nucleation process in the nucleation chamber, particles grow up to 10…12 μm large and so are conveniently detected by usual techniques, such as laser nefelometry (measurement of light pulses scattered by the grown-up particles).\n"}
{"id": "39475804", "url": "https://en.wikipedia.org/wiki?curid=39475804", "title": "Continuous gusts", "text": "Continuous gusts\n\nContinuous gusts or stochastic gusts are winds that vary randomly in space and time. Models of continuous gusts are used to represent atmospheric turbulence, especially clear air turbulence and turbulent winds in storms. The Federal Aviation Administration (FAA) and the United States Department of Defense provide requirements for the models of continuous gusts used in design and simulation of aircraft.\n\nA variety of models exist for gusts but only two, the Dryden and von Kármán models, are generally used for continuous gusts in flight dynamics applications. Both of these models define gusts in terms of power spectral densities for the linear and angular velocity components parameterized by turbulence length scales and intensities. The velocity components of these continuous gust models can be incorporated into airplane equations of motion as a wind disturbance. While these models of continuous gusts are not white noise, filters can be designed that take a white noise input and output a random process with the Dryden or von Kármán models.\n\nThe models accepted by the FAA and Department of Defense represent continuous gusts as a wind linear and angular velocity field that is a random process and make a number of simplifying assumptions in order to describe them mathematically. In particular, continuous gusts are assumed to be:\nThese assumptions, while unrealistic, yield acceptable models for flight dynamics applications. The last assumption of a velocity field that does not vary with time is especially unrealistic, since measurements of atmospheric turbulence at one point in space always vary with time. These models rely on the airplane's motion through the gusts to generate temporal variations in wind velocity, making them inappropriate for use as inputs to models of hovering, wind turbines, or other applications that are fixed in space.\n\nThe models also make assumptions about how continuous gusts vary with altitude. The Dryden and von Kármán models specified by the Department of Defense define three different altitude ranges: low, 10 ft to 1000 ft AGL; medium/high, 2000 ft AGL and above; and in between. The turbulence intensities, turbulence scale lengths, and turbulence axes depend on the altitude. The Department of Defense also provides models for the gust angular velocity but gives criteria based on airplane stability derivatives for when they can be omitted.\n\nThe Dryden model is one of the most commonly used models of continuous gusts. It was first published in 1952. The power spectral density of the longitudinal linear velocity component is\n\nformula_1\n\nwhere \"u\" is the gusts' longitudinal linear velocity component, \"σ\" is the turbulence intensity, \"L\" is the turbulence scale length, and \"Ω\" is a spatial frequency.\n\nThe Dryden model has rational power spectral densities for each velocity component. This means that an exact filter can be formed that takes white noise as an input and outputs a random process with the Dryden model's power spectral densities.\n\nThe von Kármán model is the preferred model of continuous gusts for the Department of Defense and the FAA. The model first appeared in a 1957 NACA report based on earlier work by Theodore von Kármán. In this model, the power spectral density of the longitudinal linear velocity component is\n\nformula_2\n\nwhere \"u\" is the longitudinal linear velocity component, \"σ\" is the turbulence intensity, \"L\" is the turbulence scale length, and \"Ω\" is a spatial frequency.\n\nThe von Kármán model has irrational power spectral densities. So, a filter with a white noise input that outputs a random process with the von Kármán model's power spectral densities can only be approximated.\n\nBoth the Dryden and von Kármán models are parameterized by a length scale and turbulence intensity. The combination of these two parameters determine the shape of the power spectral densities and therefore the quality of the models' fit to spectra of observed turbulence. Many combinations of length scale and turbulence intensity give realistic power spectral densities in the desired frequency ranges. The Department of Defense specifications include choices for both parameters, including their dependence on altitude, which are summarized below.\n\nLow altitude is defined as altitudes between 10 ft AGL and 1000 ft AGL.\n\nAt low altitude, the scale lengths are functions of altitude,\n\nformula_3\n\nformula_4\n\nwhere \"h\" is the altitude AGL. At 1000 ft AGL, \"L\" = 2\"L\" = 2\"L\" = 1000 ft.\n\nAt low altitude, the turbulence intensities are parameterized by \"W\", the wind speed at 20 ft.\n\nformula_5\n\nformula_6\n\nAt 1000 ft AGL,\n\nformula_7\n\nMedium/high altitude is defined as 2000 ft AGL and above.\n\nFor the Dryden model,\n\nformula_8\n\nFor the von Kármán model,\n\nformula_9\n\nAt high altitude,\n\nformula_10\n\nThey are parameterized by the probability of exceedance or the turbulence severity. A plot of turbulence intensity versus altitude showing lines of constant probability of exceedance and ranges corresponding to different turbulence severities is provided in the military specifications.\n\nFrom 1000 ft AGL to 2000 ft AGL, both the length scale and turbulence intensity are determined by linear interpolation between the low altitude value at 1000 ft and the medium/high altitude value at 2000 ft.\n\nAbove 1750 ft, the axes of the turbulence coincide with the wind frame axes. Below 1750 ft, the vertical turbulence axis is aligned with the Earth frame \"z\"-axis, the longitudinal turbulence axis is aligned with the projection of the mean wind vector onto the Earth frame's horizontal plane, and the lateral turbulence axis is determined by the right hand rule.\n\n\n"}
{"id": "35150994", "url": "https://en.wikipedia.org/wiki?curid=35150994", "title": "Cover-abundance", "text": "Cover-abundance\n\nCover-abundance is a measure of plant cover, used in phytosociology (or vegetation science). It is based on percentages at the top end, but uses abundance estimates for species with a low cover plant cover. Several scales of cover-abundance are used, e.g. the original 5-point cover scale of Braun-Blanquet or the Domin scale, with finer subdivisions (from simple presence through 10 grades of linked cover-abundance).\n\n"}
{"id": "47713", "url": "https://en.wikipedia.org/wiki?curid=47713", "title": "Direct current", "text": "Direct current\n\nDirect current (DC) is the unidirectional flow of electric charge. A battery is a good example of a DC power supply. Direct current may flow in a conductor such as a wire, but can also flow through semiconductors, insulators, or even through a vacuum as in electron or ion beams. The electric current flows in a constant direction, distinguishing it from alternating current (AC). A term formerly used for this type of current was galvanic current.\n\nThe abbreviations \"AC\" and \"DC\" are often used to mean simply \"alternating\" and \"direct\", as when they modify \"current\" or \"voltage\".\n\nDirect current may be obtained from an alternating current supply by use of a rectifier, which contains electronic elements (usually) or electromechanical elements (historically) that allow current to flow only in one direction. Direct current may be converted into alternating current with an inverter or a motor-generator set.\n\nDirect current is used to charge batteries and as a power supply for electronic systems. Very large quantities of direct-current power are used in production of aluminum and other electrochemical processes. It is also used for some railways, especially in urban areas. High-voltage direct current is used to transmit large amounts of power from remote generation sites or to interconnect alternating current power grids.\n\nDirect current was produced in 1800 by Italian physicist Alessandro Volta's battery, his Voltaic pile. The nature of how current flowed was not understood. French physicist André-Marie Ampère conjectured that current travelled in one direction from positive to negative. When French instrument maker Hippolyte Pixii built the first dynamo electric generator in 1832, he found that as the magnet used passed the loops of wire each half turn, it caused the flow of electricity to reverse, generating an alternating current. At Ampère's suggestion, Pixii later added a commutator, a type of \"switch\" where contacts on the shaft work with \"brush\" contacts to produce direct current.\n\nThe late 1870s and early 1880s saw electricity starting to be generated at power stations. These were initially set up to power arc lighting (a popular type of street lighting) running on very high voltage (usually higher than 3000 volt) direct current or alternating current. This was followed by the wide spread use of low voltage direct current for indoor electric lighting in business and homes after inventor Thomas Edison launched his incandescent bulb based electric \"utility\" in 1882. Because of the significant advantages of alternating current over direct current in using transformers to raise and lower voltages to allow much longer transmission distances, direct current was replaced over the next few decades by alternating current in power delivery. In the mid-1950s, high-voltage direct current transmission was developed, and is now an option instead of long-distance high voltage alternating current systems. For long distance underseas cables (e.g. between countries, such as NorNed), this DC option is the only technically feasible option. For applications requiring direct current, such as third rail power systems, alternating current is distributed to a substation, which utilizes a rectifier to convert the power to direct current.\n\n The term \"DC\" is used to refer to power systems that use only one polarity of voltage or current, and to refer to the constant, zero-frequency, or slowly varying local mean value of a voltage or current. For example, the voltage across a DC voltage source is constant as is the current through a DC current source. The DC solution of an electric circuit is the solution where all voltages and currents are constant. It can be shown that any stationary voltage or current waveform can be decomposed into a sum of a DC component and a zero-mean time-varying component; the DC component is defined to be the expected value, or the average value of the voltage or current over all time.\n\nAlthough DC stands for \"direct current\", DC often refers to \"constant polarity\". Under this definition, DC voltages can vary in time, as seen in the raw output of a rectifier or the fluctuating voice signal on a telephone line.\n\nSome forms of DC (such as that produced by a voltage regulator) have almost no variations in voltage, but may still have variations in output power and current.\n\nA direct current circuit is an electrical circuit that consists of any combination of constant voltage sources, constant current sources, and resistors. In this case, the circuit voltages and currents are independent of time. A particular circuit voltage or current does not depend on the past value of any circuit voltage or current. This implies that the system of equations that represent a DC circuit do not involve integrals or derivatives with respect to time.\n\nIf a capacitor or inductor is added to a DC circuit, the resulting circuit is not, strictly speaking, a DC circuit. However, most such circuits have a DC solution. This solution gives the circuit voltages and currents when the circuit is in DC steady state. Such a circuit is represented by a system of differential equations. The solution to these equations usually contain a time varying or transient part as well as constant or steady state part. It is this steady state part that is the DC solution. There are some circuits that do not have a DC solution. Two simple examples are a constant current source connected to a capacitor and a constant voltage source connected to an inductor.\n\nIn electronics, it is common to refer to a circuit that is powered by a DC voltage source such as a battery or the output of a DC power supply as a DC circuit even though what is meant is that the circuit is DC powered.\n\nDC is commonly found in many extra-low voltage applications and some low-voltage applications, especially where these are powered by batteries or solar power systems (since both can produce only DC).\n\nMost electronic circuits require a DC power supply.\n\nDomestic DC installations usually have different types of sockets, connectors, switches, and fixtures from those suitable for alternating current. This is mostly due to the lower voltages used, resulting in higher currents to produce the same amount of power.\n\nIt is usually important with a DC appliance to observe polarity, unless the device has a diode bridge to correct for this.\n\nMost automotive applications use DC. An automotive battery provides power for engine starting, lighting, and ignition system. The alternator is an AC device which uses a rectifier to produce DC for battery charging. Most highway passenger vehicles use nominally 12 V systems. Many heavy trucks, farm equipment, or earthmoving equipment with Diesel engines use 24 volt systems. In some older vehicles, 6 V was used, such as in the original classic VW Beetle. At one point a 42 V electrical system was considered for automobiles, but this found little use. To save weight and wire, often the metal frame of the vehicle is connected to one pole of the battery and used as the return conductor in a circuit. Often the negative pole is the chassis \"ground\" connection, but positive ground may be used in some wheeled or marine vehicles. \n\nTelephone exchange communication equipment uses standard −48 V DC power supply. The negative polarity is achieved by grounding the positive terminal of power supply system and the battery bank. This is done to prevent electrolysis depositions. Telephone installations have a battery system to ensure power is maintained for subscriber lines during power interruptions. \n\nOther devices may be powered from the telecommunications DC system using a DC-DC converter to provide any convenient voltage. \n\nMany telephones connect to a twisted pair of wires, and use a bias tee to internally separate the AC component of the voltage between the two wires (the audio signal) from the DC component of the voltage between the two wires (used to power the phone).\n\nHigh-voltage direct current (HVDC) electric power transmission systems use DC for the bulk transmission of electrical power, in contrast with the more common alternating current systems. For long-distance transmission, HVDC systems may be less expensive and suffer lower electrical losses.\n\nApplications using fuel cells (mixing hydrogen and oxygen together with a catalyst to produce electricity and water as byproducts) also produce only DC.\n\nLight aircraft electrical systems are typically 12 V or 24 V DC similar to automobiles. \n\n\n"}
{"id": "58797946", "url": "https://en.wikipedia.org/wiki?curid=58797946", "title": "Earth systems model of intermediate complexity", "text": "Earth systems model of intermediate complexity\n\nEarth systems Models of Intermediate Complexity (EMICs) form an important class of climate models, primarily used to investigate the earth's systems on long timescales or at reduced computational cost. This is mostly achieved through operation at lower temporal and spatial resolution than more comprehensive general circulation models (GCMs). Due to the non-linear relationship between spatial resolution and model run-speed, modest reductions in resolution can lead to large improvements in model run-speed. This has historically allowed the inclusion of previously unincorporated earth-systems such as ice sheets and carbon cycle feedbacks. These benefits are conventionally understood to come at the cost of some model accuracy. However, the degree to which higher resolution models improve accuracy rather than simply precision is contested..\n\nComputing power had become sufficiently powerful by the middle of the 20th century to allow mass and energy flow models on a vertical and horizontally resolved grid . By 1955 these advances had produced what is recognisable now as a primitive GCM (Phillips prototype ). Even at this early stage, a lack of computing power formed a significant barrier to entry and limitation on model-time. \n\nThe next half century saw rapid improvement and exponentially increasing computational demands . Modelling on ever smaller length scales required smaller time steps due to the Courant–Friedrichs–Lewy condition . For example, doubling the spatial resolution increases the computational cost by a factor of 16 (factors of 2 for each spatial dimension and time) . As well as working on smaller scales, GCMs began to solve more accurate versions of the Navier-Stokes equations . GCMs also began to incorporate more earth systems and feedback mechanisms, transforming themselves into coupled Earth Systems Models. The inclusion of elements from the cryosphere, carbon cycle and cloud feedbacks was both facilitated and constrained by growth in computing power .\n\nThe powerful computers and high cost required to run these “comprehensive” models limited accessibility to many university research groups. This helped drive the development of EMICs. Through judicious parametrisation of key variables, researchers could run climate simulations on less powerful computers, or alternatively much faster on comparable computers. A modern example of this difference in speed can be seen between the EMIC JUMP-LCM and the GCM MIROC4h; the former runs 63,000 times faster than the latter . The decrease in required computing power allowed EMICs to run over longer model times, and thus include earth systems occupying the “slow domain”.\n\nPetoukhov’s 1980 statistical dynamical model has been cited as the first modern EMIC , but despite development throughout the 1980s, their specific value only achieved wider recognition in the late 1990s with inclusion in IPCC AR2 under the moniker of “Simple Climate Models”. It was shortly afterwards at the IGBP congress in Shonnan Village, Japan, in May 1999, where the acronym “EMICs” was publicly coined by Claussen. The first simplified model to adopt the nomenclature of “intermediate complexity” is now one of the best known: CLIMBER 2. The Potsdam conference under the guidance of Claussen identified 10 EMICs, a list updated to 13 in 2005 . Eight models contributed to IPCC AR4, and 15 to AR4 .\n\nAs well as “complexity”, climate models have been classified by their resolution, parametrisation and “integration”. Integration expresses the level of interaction of different components of the earth system. This is influenced by the number of different links in the web (interactivity of coordinates), as well as the frequency of interaction. Because of their speed, EMICs offer the opportunity for highly integrated simulations when compared with more comprehensive ESMs. Four EMIC categorisations have been suggested based on the mode of atmospheric simplification : Statistical-Dynamical Models, Energy Moisture Balance Models, Quasi Geostrophic Models, and Primitive Equation Models. Of the 15 models in the community contribution to the IPCC’s fifth assessment report, four were statistical-dynamic, seven energy moisture balance, two quasi-geostrophic and two primitive equations models . To illustrate these categories, a case study for each is given.\n\nCLIMBER-2 and CLIMBER-3α are successive generations of 2.5 and 3 dimensional statistical dynamical models . Rather than continuous evolution of solutions to the Navier Stokes or Primitive Equations, atmospheric dynamics are handled through statistical knowledge of the system (an approach not new to CLIMBER ). This approach expresses the dynamics of the atmosphere as large-scale, long term fields of velocity and temperature. Climber-3α’s horizontal atmospheric resolution is substantially courser than a typical atmospheric GCM at 7.5°x 22.5°.\n\nWith a characteristic spatial scale of 1000km, this simplification prohibits resolution of synoptic level features. Climber-3α incorporates comprehensive ocean, sea ice and biogeochemistry models. Despite these full descriptions, simplification of the atmosphere allows it to operate two orders of magnitude faster than comparable GCMs . Both CLIMBER models offer performances comparable to that of contemporary GCMs in simulating present climates. This is clearly of interest due to the significantly lower computational costs. Both models have been principally used to investigate paleoclimates, particularly ice sheet nucleation \n\nThe thermodynamic approach of the UVic model involves simplification of mass transport (with Fickian diffusion) and precipitation conditions. This model can be seen as a direct descendant of earlier energy balance models . These reductions reduce the atmosphere to three state variables, surface air temperature, sea surface temperature and specific humidity. By parametrising heat and moisture transport with diffusion, timescales are limited to greater than annual and length scales to greater than 1000km. A key result of the thermodynamic rather than fluid dynamic approach is that the simulated climate exhibits no internal variability. Like CLIMBER-3α, it is coupled to a state of the art, 3D ocean model and includes other cutting edge models for sea-ice and land-ice. Unlike CLIMBER, the UVic model does not have significantly coarser resolution than contemporary AOGCMs (3.6°x 1.8°). As such, all computational advantage is from the simplification of atmospheric dynamics.\n\nThe quasi-geostrophic equations are a reduction of the Primitive Equations first written down by Charney . These equations are valid in the case of low Rossby number, signifying only a small contribution from inertial forces. Assumed dominance of the Coriolis and pressure-gradient forces facilitates the reduction of the primitive equations to a single equation for potential vorticity in five variables . LOVECLIM features a horizontal resolution of 5.6° and uses the quasi geostrophic atmosphere model ECBilt. It includes a vegetation feedback module by Brovkin et al. (1997). The model exhibits some significant limitations that are fundamentally linked to its design. The model predicts an Equilibrium Climate Sensitivity of 1.9°C, at the lower end of the range of GCM predictions. The model’s surface temperature distribution is overly-symmetric, and does not represent the northern bias in location of the Intertropical Convergence Zone. The model generally shows lower skill at low latitudes. Other examples of quasi-geostrophic models are PUMA and SPEEDY.\n\nThe UK Met-Office’s FAMOUS blurs the line between more coarsely resolved comprehensive models and EMICs. Designed to run paleoclimate simulations of the Pleistocene, it has been tuned to reproduce the climate of its parent, HADCM3, by solving the Primitive Equations written down by Charney. These are of higher complexity than the quasi-geostrophic equations. Originally named ADTAN, preliminary runs had significant biases involving sea ice and the AMOC, which were later corrected through tuning of sea-ice parameters. The model runs at half the horizontal resolution of HADCM3. Atmospheric resolution is 7.5°x5°, and oceanic is 3.75°x 2.5°. Atmosphere-Ocean coupling is done once daily.\n\nSystematic intercomparison of EMICs has been undertaken since 2000, most recently with a community contribution to the IPCC’s fifth assessment report . The equilibrium and transient climate sensitivity of EMICs broadly fell within the range of contemporary GCMs with a range of 1.9 - 4.0°C (compared to 2.1° - 4.7°C, CMIP5). Tested over the last millennium, the average response of the models was close to the real trend, however this conceals much wider variation between individual models. Models generally overestimate ocean heat uptake over the last millennium and indicate a moderate slowing. No relationship was observed in EMICs between levels of polar amplification, climate sensitivity, and initial state . The above comparisons to the performance of GCMs and comprehensive ESMs do not reveal the full value of EMICs. Their ability to run as “fast ESMs” allows them to simulate much longer periods, up to many millennia. As well as running on time-scales far greater than available to GCMs, they provide fertile ground for development and integration of systems that will later join GCMs.\n\nPossible future directions for EMICs are likely to be in assessment of uncertainties and as a vanguard for incorporation of new earth systems . By virtue of speed they also lend themselves to the creation of ensembles with which to constrain parameters and assess earth systems . EMICs have also recently led in the field of climate stabilisation research . McGuffie and Henderson-Sellers argued in 2001 that in the future, EMICs would be “as important” as GCMs to the climate modelling field - while this has perhaps not been true in the time since that statement, their role has not diminished. Finally, as climate science has come under increasing levels of scrutiny , the ability of models not just to project but to explain has become important. The transparency of EMICs is attractive in this domain, as causal chains are easier to identify and communicate (as opposed to emergent properties generated by comprehensive models).\n\n"}
{"id": "24793719", "url": "https://en.wikipedia.org/wiki?curid=24793719", "title": "Energetic neutral atom", "text": "Energetic neutral atom\n\nEnergetic neutral atom (ENA) imaging, often described as \"seeing with atoms\", is a technology used to create global images of otherwise invisible phenomena in the magnetospheres of planets and throughout the heliosphere, even to its outer boundary.\nThis constitutes the far-flung edge of the solar system.\n\nThe solar wind consists of ripped-apart atoms (called plasma) flying out of the Sun. This is mostly hydrogen, that is, bare electrons and protons, with a little bit of other kinds of nuclei, mostly helium. The space between solar systems is similar, but they come from other stars in our galaxy. These charged particles can be redirected by magnetic fields; for instance, Earth's magnetic field shields us from these particles. Every so often, a few of them steal electrons from neutral atoms they run into, making them neutral and not subject to large-scale electromagnetic fields. Still moving very fast, they tend to travel mostly in a straight line, subject to gravity. These are called Energetic Neutral Atoms. ENA images are constructed from the detection of these energetic neutral atoms.\n\nEarth's magnetosphere preserves Earth's atmosphere and protects us from cell-damaging radiation. This region of \"space weather\" is the site of geomagnetic storms that disrupt communications systems and pose radiation hazards to humans traveling at high polar altitudes or in orbiting spacecraft. A deeper understanding of this region is vitally important. Geomagnetic weather systems have been late to benefit from the satellite imagery taken for granted in weather forecasting, and space physics because their origins in magnetospheric plasmas present the added problem of invisibility.\n\nThe heliosphere protects the entire Solar System from the majority of cosmic rays but is so remote that only an imaging technique such as ENA imaging will reveal its properties. The heliosphere's structure is due to the invisible interaction between the solar wind and cold gas from the local interstellar medium.\n\nThe creation of ENAs by space plasmas was predicted but their discovery was both deliberate and serendipitous. While some early efforts were made at detection, their signatures also explained inconsistent findings by ion detectors in regions of expected low ion populations. Ion detectors were co-opted for further ENA detection experiments in other low-ion regions. However, the development of dedicated ENA detectors entailed overcoming significant obstacles in both skepticism and technology.\n\nAlthough ENAs were observed in space from the 1960s through 1980s, the first dedicated ENA camera was not flown until 1995 on the Swedish Astrid-1 satellite, to study Earth's magnetosphere.\n\nToday, dedicated ENA instruments have provided detailed magnetospheric images from Venus, Mars, Jupiter, and Saturn. Cassini's ENA images of Saturn revealed a unique magnetosphere with complex interactions that have yet to be fully explained. The IMAGE mission's three dedicated ENA cameras observed Earth's magnetosphere from 2000–2005 while the TWINS Mission, launched in 2008, provides stereo ENA imaging of Earth's magnetosphere using simultaneous imaging from two satellites.\n\nThe first ever images of the heliospheric boundary, published in October 2009, were made by the ENA instruments aboard the IBEX and Cassini spacecraft. These images are very exciting because they challenge existing theories about the region.\n\nThe most abundant ion in space plasmas is the hydrogen ion—a bare proton with no excitable electrons to emit visible photons. The occasional visibility of other plasma ions is not sufficient for imaging purposes. ENAs are created in charge-exchange collisions between hot solar plasma ions and a cold neutral background gas. These charge-exchange processes occur with high frequency in planetary magnetospheres and at the edge of the heliosphere.\n\nIn a charge-exchange collision between a high energy plasma ion and a cold neutral atom, the ion 'steals' electrons from the neutral atom, producing a cold ion and an energetic neutral atom(ENA).\n\nI + A → A + I\n\nwhere\n\nSpecies 1 and 2 may be the same or different and an exchange of two electrons is possible, e.g.\n\nH + H → H + H\nProton–hydrogen charge-exchange\nor\nHe + He → He + He\nalpha-helium charge-exchange.\n\nDue to its charge neutrality, the resulting ENA is subject to gravitational forces only. Because gravitation influences can normally be ignored, it is safe to assume that the ENA preserves the vector momentum of the original pre-interaction plasma ion.\n\nSome ENAs are lost in further charge-exchange, electron collisions and photoionization, but a great many travel very long distances in space completely undisturbed.\n\nAlthough plasma recombination and neutral atom acceleration by the solar gravitation may also contribute to an ENA population under certain conditions, the main exception to this creation scenario is the flux of interstellar gas, where neutral particles from the local interstellar medium penetrate the heliosphere with considerable velocity, which classifies them as ENAs as well.\n\nProton–hydrogen charge-exchange collisions are often the most important process in space plasma because Hydrogen is the most abundant constituent of both plasmas and background gases and hydrogen charge-exchange occurs at very high velocities involving little exchange of momentum.\n\nIn general, only a few species are important for ENA formation, namely hydrogen, helium, oxygen and sulfur:\nThe corresponding neutral gases are:\n\nENAs are found everywhere in space and are directly observable at energies from 10eV to more than 1 MeV. Their energies are described more with reference to the instruments used for their detection than to their origins.\n\nNo single particle analyzer can cover the entire energy interval from 10 eV to beyond 1 MeV. ENA instruments are roughly divided into low, medium and high overlapping groups that can be arbitrary and vary from author to author. The low, medium and high energy range from one author is shown in the graph along with the energy ranges for the three instruments aboard the IMAGE satellite:\n\nAtoms are usually considered ENAs if they have kinetic energies clearly higher than can be reached by typical thermodynamic planetary atmospheres which is usually in excess of 1 eV. This classification is somewhat arbitrary, being driven by the lower limits of ENA measurement instrumentation. The high end limitations are imposed by both measurement techniques and for scientific reasons.\n\nMagnetospheres are formed by the solar wind plasma flow around planets with an intrinsic magnetic field (Mercury, Earth, Jupiter, Saturn, Uranus, and Neptune), although planets and moons lacking magnetic fields may sometimes form magnetosphere-like plasma structures. The ionospheres of weakly magnetized planets such as Venus and Mars set up currents that partially deflect the solar wind flow around the planet.\n\nAlthough magnetospheric plasmas have very low densities; e.g. near Jupiter's moon Europa, plasma pressures are about 10 bar, compared to 1 bar at Earth's surface, and are responsible for magnetospheric dynamics and emissions. For example, geomagnetic storms create serious disturbances in Earth's cable communications systems, navigational systems and power distribution systems.\n\nThe strength and orientation of the magnetic field with respect to solar wind flow determines the shape of the magnetosphere. It is usually compressed on the day side and elongated at the night side.\n\nEarth's magnetic field dominates the terrestrial magnetosphere and prevents the solar wind from hitting us head on. Lacking a large protective magnetosphere, Mars is thought to have lost much of its former oceans and atmosphere to space in part due to the direct impact of the solar wind. Venus with its thick atmosphere is thought to have lost most of its water to space in large part owing to solar wind ablation.\n\nUnderstanding the magnetosphere increased in importance with the realization of the detrimental impact of geomagnetic storms, caused by solar coronal mass ejections, particularly in years of high solar activity. In addition to long known effects on Earth's cable communication systems, communications, broadcasting, navigation and security applications are increasingly dependent on satellites. Most of these satellites are well within the protective magnetosphere but are vulnerable to space weather systems that affect them adversely. There are also radiation hazards for humans traveling at high polar altitudes or in orbiting spacecraft Many countries, including the U.S., provide a Space Weather Service reporting existing or predicted Geomagnetic Storms, Solar Radiation Storms and Radio Blackouts.\n\nThe first dedicated ENA instrument was launched on a Nike–Tomahawk sounding rocket from Fort Churchill, Manitoba, Canada. This experiment was followed by the launch of a similar instrument on a Javelin sounding rocket in 1970 to an altitude of 840 km at Wallops Island off the coast of Virginia. In 1972 and 1973, the presence of ENA signatures explained inconsistencies in measurements by the IMP-7 and 8 satellites.\n\nENA data from the NASA/ESA ISEE 1 satellite enabled the construction of the first global image of the storm time ring current in 1982. This was a breakthrough that paved the way for the use of ENAs as a powerful imaging technique. ENAs were also detected during the 1982 magnetic storm by SEEP instrument on the NASA S81-1 spacecraft. In 1989, the exospheric hydrogen atom population around Earth was extensively studied by the NASA Dynamic Explorer (DE-1) satellite.\nAn instrument with a dedicated high-energy ENA detection channel was flown on the 1991 NASA CRRES satellite. A more sophisticated high energy particle Instrument was launched on the 1992 NASA/ISAS GEOTAIL spacecraft dedicated to observing Earth's magnetosphere. Precipitating ENAs can be studied from a low Earth orbit and were measured \"looking out\" by CRRES and the 1995 Swedish ASTRID satellites.\n\nThe new millennium saw ENA Imaging coming into its own. Extensive and detailed observations of the Earth's magnetosphere were made with three ENA instruments aboard the NASA IMAGE Mission from 2000 - 2005. In July 2000, a set of ENA images of the Earth's ring current were made during a geomagnetic storm. (See image at the top of the page.) The storm was triggered by a fast coronal mass ejection that erupted from the Sun on July 14, 2000 and arrived at Earth the next day.\n\nLaunched in 2008, the NASA TWINS Mission (two wide-angle Imaging Neutral-atom Spectrometers) provides the capability for stereoscopically imaging the magnetosphere. By imaging ENAs over a broad energy range (~1-100 keV) using identical instruments on two widely spaced high-altitude, high-inclination spacecraft, TWINS enables 3-dimensional visualization and the resolution of large scale structures and dynamics within the magnetosphere.\n\nMagnetospheres of other planets have been studied by flyby spacecraft, by orbiters, landers and by Earth-based observations.\n\nIn February 2009, the ESA SARA LENA instrument aboard India's Chandrayaan-1 detected hydrogen ENAs sputtered from the lunar surface by solar wind protons. Predictions had been that all impacting protons would be absorbed by the lunar regolith but for an as yet unknown reason, 20% of them are bounced back as low energy hydrogen ENAs. It is hypothesized that the absorbed protons may produce water and hydroxyls in interactions with the regolith. The Moon has no magnetosphere.\n\nThe proposed 2014 ESA BepiColombo mission includes ENA instruments to further its objective to study the origin, structure and dynamics of Mercury's magnetic field. The LENA instrument will resemble the SARA instrument sent to Earth's Moon. In addition to magnetospheric ENAs, sputtering from Mercury's surface is also expected.\n\nLaunched in 2005, the ESA VEX (Venus Express) mission's ASPERA (Energetic Neutral Atoms Analyser) consists of two dedicated ENA detectors. In 2006 ENA images were obtained of the interaction between the solar wind and the Venusian upper atmosphere, showing massive escape of planetary oxygen ions.\n\nLaunched in 2003, the ESA MEX (Mars Express) mission's ASPERA instrument has obtained images of the solar wind interacting with the upper Martian atmosphere. The 2004 observations show solar wind plasma and accelerated ions very deep in the ionosphere, down to 270 km. above the dayside planetary surface—evidence for solar wind atmospheric erosion.\n\nThe GAS instrument on the ESA/NASA Ulysses, launched in 1990, produced unique data on interstellar helium characteristics and ENAs emitted from Jupiter's Io torus.\nOn its Jupiter flyby in 2000, the NASA/ESA/ASI Cassini's INCA instrument confirmed a neutral gas torus associated with Europa. Cassini's ENA images also showed Jupiter's magnetosphere to be dominated by hydrogen atoms ranging from a few to 100 keV. The atoms are emitted from the planet's atmosphere and from neutral gas tori near the inner Galilean moons. A population of heavier ions was also detected, indicating a significant emission of oxygen and/or sulfur from Jupiter's magnetosphere.\n\nThe first dedicated ENA camera was flown on the NASA/ESA/ASI Cassini mission, launched in 1997 to study Saturn's magnetosphere.\n\nSaturn's main radiation belt was measured beginning at an altitude 70,000 km from its surface and reaching out to 783,000 km. Cassini also detected a previously unknown inner belt nearer its surface that is about 6,000 km thick.\n\nThe dynamics of Saturn's magnetosphere are very different from Earth's. Plasma co-rotates with Saturn in its magnetosphere. Saturn's strong magnetic field and rapid rotation create a strong co-rotational electric field that accelerates plasma in its magnetosphere until it reaches rotation speeds near that of the planet. Because Saturn's moons are essentially 'sitting still' in this very high speed flow, a complex interaction between this plasma and the atmosphere of the moon Titan was observed.\n\nCassini's MIMI-INCA ENA instrument has observed Titan on many occasions revealing the structure of the magnetospheric interaction with Titan's dense atmosphere. Several studies have been performed on Titan's ENA emissions.\n\nNASA's Voyager 2 took advantage of its orbit to explore Uranus and Neptune, the only spacecraft to ever have done so. In 1986 spacecraft found a Uranian magnetic field that is both large and unusual. More detailed investigations have yet to be carried out.\n\nThe heliosphere is a cavity built up by the solar wind as it presses outward against the pressure of the local interstellar medium (LISM). As the solar wind is a plasma, it is charged and so carries with it the Sun's magnetic field. So the heliosphere can be conceptualized as the Solar System's magnetosphere. The edge of the heliosphere is found far beyond the orbit of Pluto where diminishing solar wind pressure is stopped by the pressure from the LISM.\nThe background neutral gas for ENA production at the heliospheric boundary comes predominantly from interstellar gas penetrating the heliosphere. A tiny amount comes from solar wind neutralization of interplanetary dust near the sun. The heliospheric boundaries are invisible and fluctuating. Although the densities are low, the enormous thickness of the heliosheath make it a dominant source of ENAs, aside from planetary magnetospheres. Because of the strong dependence of ENA characteristics on heliospheric properties, remote ENA imaging techniques will provide a global view of the structure and dynamics of the heliosphere unattainable by any other means.\n\nThe first glimpse of this view was announced in October, 2009, when the NASA IBEX Mission, returned its first image of the unexpected ENA ribbon at the edge of the heliosphere. Results revealed a previously unpredicted \"very narrow ribbon that is two to three times brighter than anything else in the sky\" at the edge of the heliosphere that was not detected by Voyager 1 and Voyager 2 in the region. These results are truly exciting as they do not match any existing theoretical models of this region.\n\nCassini also ENA-imaged the heliosphere and its results complement and extend the IBEX findings, making it possible for scientists to construct the first comprehensive sky map of the heliosphere. Preliminary Cassini data suggest the heliosphere may not have the comet-like shape predicted by existing models but that its shape may be more like a large, round bubble.\n\nEstimates for size of the heliosphere vary between 150 – 200 AU. It is believed that Voyager 1 passed the heliosphere's termination shock in 2002 at approx. 85 – 87 AU while Voyager 2 passed the termination shock in 2007 at about 85 AU. Others place the termination shock at a mean distance of ≈100 AU. Because the solar wind varies by a factor of 2 during the 11 year solar cycle, there will be variations in the size and shape of the heliosphere, known as heliosphere \"breathing.\"\n\nThe huge distances involved mean we will never accumulate a large number of \"in situ\" measurements of the various layers of the heliosphere. Voyager 1 and 2 took 27 yrs. and 30 yrs. respectively to arrive at the termination shock. It is worth noting that for large distances to the object, high energy (velocity) and slower ENAs emitted simultaneously would be detected at different times. This time difference varies from 1 - 15 minutes for observing Earth's magnetosphere from a high altitude spacecraft to more than a year for imaging the heliospheric boundary from an Earth orbit.\n\nIn a surprising development, a wholly different kind of ENA source appeared in 2006.\nThe STEREO spacecraft detected neutral hydrogen atoms with energies in the 2-5 MeV range from the flare/CME SOL2006-12-05.\nThese particles were not detected with an instrument designed to see ENAs, but there was sufficient ancillary data to make the observation quite unambiguous. \nAccelerating ENAs without ionizing them would be difficult, so the reasonable interpretation here is that SEP protons from the flare/CME were able to find singly-charged He and He-like atoms in the solar wind, and thence to convert and continue without magnetic effects.\nThe particles thus arrived prior to the SEP protons themselves, constrained to follow the Parker spiral.\nAlthough no other event has been detected this way, probably many could, and in principle could provide substantial information about the processes involved in SEP acceleration and propagation.\n\nAlthough the study of ENAs promised improvements in the understanding of global magnetospheric and heliospheric processes, its progress was hindered due to initially enormous experimental difficulties.\n\nIn the late 1960s, the first direct ENA measurement attempts revealed the difficulties involved. ENA fluxes are very weak, sometimes less than 1 particle per cm per second and are typically detected by secondary electron emission upon contact with a solid surface. They exist in regions containing ultraviolet (UV) and extreme ultraviolet (EUV) radiation at fluxes 100 times greater than produce similar emissions.\n\nAn ENA instrument ideally would also specifically:\nThe challenge to remote sensing via ENAs lies in combining mass spectrometry with the imaging of weak particle fluxes within the stringent limitations imposed by an application on a spacecraft.\n\nIt became clear very early that to succeed, instruments would have to specialize in specific ENA energies. The following describes, in very simplified terms, a typical instrument function for high (HENA) or medium (MENA) energy instrument, with differences noted. The accompanying illustration is of the HENA camera flown on the NASA IMAGE mission and the description that follows most closely resembles IMAGE mission instruments.\n\nA set of electrostatic plates deflect charged particles away from the instrument and collimates the beam of incoming neutral atoms to a few degrees.\nHENA: TOF is determined by a coincidence detection requirement that turns out to be efficient at eliminating photon background noise as well. An ENA passes through a thin film to a particle energy detector with its energy nearly completely preserved. At the same time, electrons forward scattered from the film are electrostatically deflected to a detector to create a start pulse. The ENA arriving at its solid state detector (SSD) creates the end pulse and its impact position yields its trajectory and therefore path length. The start and stop signals enable TOF to be determined.\n\nIf the electrons are scattered by incoming photons, no ENA will be detected to create the stop pulse. If no stop pulse is sensed within an established time appropriate to the energy of the expected particles, the start pulse is discarded.\n\nMENA: Medium energy ENAs would lose too much energy penetrating the film used in the HENA instrument. The thinner film required would be vulnerable to damage by incident UV and EUV. Therefore, photons are prevented from entering the instrument by using a gold diffraction grating. An ultra thin carbon film is mounted on the back of the grating. ENAs pass through the grating and the film to impact a solid state detector (SSD), scattering electrons and allowing path length and TOF determinations as for the HENA above.\n\nKnowing path length and TOF enables velocity to be determined.\n\nThe solid state detector (SSD) impacted by the ENA after it passes through the foil registers its energy. The small energy loss due to passing through the foil is handled by instrument calibration.\n\nKnowing the energy and velocity, the mass of the particle can be calculated from energy = mv/2. Alternatively, the number of scattered electrons detected can also serve to measure the mass of the ENA.\n\nMass resolution requirements are normally modest, requiring at most distinguishing among hydrogen (1 amu), helium (4 amu), and oxygen (16 amu) atoms with sulfur (32 amu) also expected in Jupiter's magnetosphere.\n\nUsually, obtaining images from a spinning spacecraft provides the second dimension of direction identification. By combining synchronized observations from two different satellites, stereo imaging becomes possible. Results from the TWINS Mission are eagerly awaited, as two viewing points will provide substantially more information about the 3-D nature of Earth's magnetosphere.\n\nWhile the collimator is similar, low-energy instruments such as the NASA GSFC LENA use a foil-stripping technique. Incident ENAs interact with a surface such as tungsten to generate ions that are then analyzed by an ion spectrometer.\n\nBecause of the need to detect atoms sputtered from the lunar surface as well lighter ENAs, the ESA LENA on the Chandrayaan-1 incorporated a mass spectrometer designed to resolve heavier masses including sodium, potassium, and iron.\n\nAs of 2005, a total of only six dedicated ENA detectors had been flown. The launch of instruments aboard in the TWINS and IBEX missions brings the total to nine in 2009 - a 50% increase in only 4 years. Space plasma observation using ENA imaging is an emerging technology that is finally coming into its own.\n\nSeveral improvements are still needed to perfect the technique. Although the angular resolution has now decreased to a few degrees and different species can be separated, one challenge is to expand the energy range upwards to about 500 keV. This high energy range covers most of the plasma pressure of Earth's inner magnetosphere as well as some of the higher-energy radiation belts so is desirable for terrestrial ENA imaging.\n\nFor lower energy ENAs, below 1 keV, the imaging techniques are completely different and rely on the spectroscopic analysis of ions stripped from a surface by the impinging ENA. Improvements in sub-keV measurements will be needed to image Mercury's magnetosphere due to the consequences of its smaller magnetic field and it smaller geometry.\n\nIn addition to the obvious intellectual benefits brought by increased understanding of our space environment, there are many practical motivations for enhancing our knowledge of space plasmas.\n\nThe heliosphere is a protective cocoon for the Solar System, just as the Earth's magnetosphere is a protective cocoon for the Earth. The insight provided by ENAs into the behaviour of space plasmas improves our understanding of these protective mechanisms.\n\nWithout the magnetosphere, Earth would be subject to direct bombardment by the solar wind and may be unable to retain an atmosphere. This, plus increased exposure to solar radiation means that life on Earth as we know it would not be possible without the magnetosphere. Similarly, the heliosphere protects the Solar System from the majority of otherwise damaging cosmic rays, with the remainder being deflected by the Earth's magnetosphere.\n\nAlthough most orbiting satellites are protected by the magnetosphere, geomagnetic storms induce currents in conductors that disrupt communications both in space and in cables on the ground. Better understanding of the magnetosphere and the ring current and its interaction with the solar wind during high solar activity will allow us to better protect these assets.\n\nAstronauts on deep space missions will not have Earth's protections so understanding the factors that may affect their exposure to cosmic rays and the solar wind is critical to manned space exploration.\n\nAstronomers measure distances within the Solar System in astronomical units (AU). One AU equals the average distance between the centers of Earth and the Sun, or 149,598,000 km. Pluto is about 38 AU from the Sun and Jupiter is about 5.2 AU from the Sun. One light-year is 63,240 AU.\n\n\n"}
{"id": "46676004", "url": "https://en.wikipedia.org/wiki?curid=46676004", "title": "Energy in Guam", "text": "Energy in Guam\n\nThe pattern of energy production and use in Guam is shaped by its location, a remote island. Almost all energy is reliant on imports of petroleum products for use in transport and electricity. Guam has no domestic production of conventional fuels such as oil, natural gas or coal. Its economy is dependent on the import of gasoline and jet fuel for transport and residual fuel oil for electricity. One third of electricity produced is used in commercial settings including the leading industry of tourism. Despite making up about one-tenth of the islands population, the U.S. military uses one-fifth of the island's energy.\n\nRising fuel costs and environmental concerns have led to major plans to alter the electrical industry in Guam. Renewables and cleaner burning natural gas and diesel power are planned.\n\nGuam has a rated generating capacity of 560 MW, more than twice its historical highest load. This is supplied by several plants burning residual fuel oil operated for the Guam Power Authority by independent power providers. In 2015 electricity cost 2.5 times as much on Guam as on the U.S. mainland. A new plant was proposed in 2014 which would replace all of these generators and run on either diesel or on liquified natural gas. This new plant would comply with U.S. EPA Clean Air Act requirements.\n\nUntil 2015, only a few off-grid photovoltaic systems (PV) and some distributed generation PV and small wind turbines are in use on the island. Plans for several large solar farms have been announced. Guam has adopted a renewables policy that requires the reduction of fossil fuel consumption by 2020 to 20% less than the rate in 2010. Another requirement is for 5% of electricity in 2015 to be from renewables, increasing to 25% by 2035. A net metering program began in 2009.\n\nA 25.6MW Dan Dan solar farm at Inarajan came online in late 2015. It would reduce fuel oil consumption by almost 2 million barrels a year. The U.S. military has proposed to lease land for a 44 MW solar farm. Proposals for a second solar farm of up to 40MW were requested in 2015 to include details on incorporating battery storage.\n\nA single 0.275MW wind turbine is expected to be online in 2015 as part of a pilot program in wind energy.\n\nA 10MW waste to energy plant has been proposed but, in 2015, has met resistance due to cost and air pollution concerns.\n"}
{"id": "5658096", "url": "https://en.wikipedia.org/wiki?curid=5658096", "title": "Exotic material", "text": "Exotic material\n\nExotic Materials can include plastics, superalloys, semiconductors, superconductors, and ceramics.\n\nExamples of metals and alloys that can be exotic:\n"}
{"id": "11200543", "url": "https://en.wikipedia.org/wiki?curid=11200543", "title": "Fatal Frame III: The Tormented", "text": "Fatal Frame III: The Tormented\n\nFatal Frame III: The Tormented, known in Japan as and in Europe as Project Zero 3: The Tormented, is the third installment in the \"Fatal Frame\" video game series. The PlayStation 2 game takes place two years after \"Fatal Frame\" and about two months after \"\", introducing old and new characters, as well as a new haunted location.\n\nIn \"Fatal Frame III\", the player uses the vintage \"Camera Obscura\" to photograph and dispel ghosts. The player explores in third-person in either the Manor of Sleep or the real world. On the main screen is a \"Ghost Filament\", which indicates a ghost's presence; red signals a hostile ghost, while blue indicates a neutral ghost. At any point in the game, the player can enter first-person mode via the viewfinder, used for photographing ghosts. The Camera Obscura has a limited amount of film for ammunition, and the player must search areas thoroughly to obtain more. The player gains points through defeating ghosts or by taking pictures of neutral ghosts or seals. These points can be used to upgrade different aspects of the camera, such as range and sensitivity.\n\n\"Fatal Frame III\"s story is split into chapters called \"Hours\", which are further split into \"day-and-night\" sequences. While awake, Rei moves about her home, interacting with other characters and developing photographs taken from the Manor of Sleep. While sleeping, Rei enters the Manor of Sleep in her dreams. In certain Hours, the player controls either Miku or Kei, and their dreams include sections from previous games (e.g. the Himuro Mansion). Each character has different special abilities, making each of their playing styles unique. Rei can use the camera's flash to scare off some spirits, but can only use it a limited number of times. Miku has a special \"Sacred Stone\" charm that decreases the speed of spirits when used, and a \"Double\" ability that allows her to charge the camera up twice. Kei, with his greater physical strength, can move bookshelves or jump from one roof of the Manor to another. However, since Kei's camera ability is much weaker than that of either Rei or Miku, the player must use his \"Hide\" ability to avoid spirits' detection. \n\n23-year-old freelance photographer Rei Kurosawa has been mourning the death of her fiancé, Yuu Asou, after a car accident caused by Rei's reckless driving. About two months after his death, while on assignment for the derelict Kuze mansion, Rei discovers his image on one of the photos she had taken. Shortly after, she begins having recurring nightmares of an old Japanese manor house—the —during a heavy snowfall, where she is touched by a tattooed ghost. Upon waking, Rei notices a mysterious bruise on her body, where the ghost had touched her; this mark spreads each time she dreams about the Manor.\n\nAs Rei continues to explore the Manor of Sleep, she begins to receive investigation notes from Kei Amakura, a close friend of Yuu who writes in his letters that he and his niece Mio both have similar symptoms. Rei learns that her photography assistant, Miku Hinasaki, is also under the same curse. The lines between dream and reality blur when ghosts began appearing in Rei's house, and the bruise on her body continues to expand and form a snake-and-holly tattoo similar to the tattooed ghost who touched her.\n\nRei learns through uncovering diaries and research that the dreams are caused by Reika Kuze, the last Tattooed Priestess tasked with the duty of containing others' emotional pain within the tattoos on her body. Although meant to rest for eternity, Reika had unintentionally released \"the Rift\", a form of darkness which destroyed the village, after her childhood lover Kaname was murdered in front of her eyes. Now corrupted and unable to rest, she haunts people with a strong attachment to dead loved ones. Within time, the tattoos Reika had cursed her victims with will completely cover their bodies and cause them to disappear, leaving behind black soot, although following the dead would eventually lead to death as well. Because of Rei's attachment to Yuu; Miku's to her brother Mafuyu; and Kei's strong connection to Mio, , all three are trapped in the Manor of Sleep.\n\nWhen Miku enters a coma, Rei loses all hope of ending the curse until Kei suggests that by staking down Reika's body in the Chamber of Thorns, she can rest peacefully and no longer endanger them. However, he becomes trapped in the chamber himself upon making a horrific discovery: Reika's body has already been pinned to the ground. With only Rei remaining, she confronts Reika and exorcises her spirit. Following the lyrics of a song in the ritual, Rei puts the bodies of Reika and Kaname in a small boat and pushes it across an underground sea behind the stone ritual chamber - the Abyss of the Horizon, the passage to the spirit world. As the boat begins to vanish into the distance, other dead spirits begin to cross the sea, one of them being Yuu. The tattoo which now covers Rei's body transfers to Yuu, who insists that she must go on living and keep his memory alive. Once Rei and Miku wake up, they resume their normal lives.\n\nTwo \"Endings\" appear in the game. The \"Normal Ending\" shows that Rei and Miku have survived, while Kei has been spirited away; Mio's fate is unknown (presumed spirited away by Reika as well). The \"Photograph Ending\" is achieved by finishing an optional sidequest only available on the second playthrough of the game. This ending shows that both Kei and Mio survive as well, and that Mio is introduced to Rei and Miku after the game. In both endings, both Rei and Miku resolve to live on, as their deceased loved ones would have wanted.\n\n\"Fatal Frame III\" was released for the PlayStation 3 as a PlayStation 2 Classic on October 1, 2013 in North America only.\n\n\"Fatal Frame III\" has received positive reviews from critics. Aggregating review websites GameRankings and Metacritic gave the game a 79.31% and 78/100. \n\n"}
{"id": "1195115", "url": "https://en.wikipedia.org/wiki?curid=1195115", "title": "Frazil ice", "text": "Frazil ice\n\nFrazil ice is a collection of loose, randomly oriented, plate or discoid ice crystals formed in supercooled turbulent water . Frazil ice formation is common during the winter in rivers and lakes located in northern latitudes, and usually forms in open-water reaches of rivers where and when the heat exchange between the air and the water is such that the water temperature can drop below its freezing point (typically not lower than -0.1 °C in rivers). As a rule of thumb, such conditions may happen on cold and clear nights, when the air temperature is lower than . Frazil ice also forms in oceans, where it is often referred to as grease ice when floating on the surface.\n\nFrazil ice is notorious for blocking water intakes as crystals accumulate and build up on the intake trash rack. Such blockages negatively impact water supply facilities, hydropower plants, nuclear power facilities, and vessels navigating in cold waters, and can lead to unexpected shut downs of the facility or even collapse of the trash rack.\n\nWhen the water surface begins to lose heat rapidly, the water becomes supercooled. Turbulence, caused by strong winds or flow from a river, will mix the supercooled water throughout its entire depth. The supercooled water will already be encouraging the formation of small ice crystals (frazil ice) and the crystals get taken to the bottom of the water body. Ice generally floats, but due to frazil ice's small size relative to current speeds, it has an ineffective buoyancy and can be carried to the bottom very easily.\n\nThrough a process called secondary nucleation, the crystals quickly increase in number, and because of its supercooled surrounding, the crystals will continue to grow. Sometimes, the concentration is estimated to reach one million ice crystals per cubic meter.\n\nAs the crystals grow in number and size, the frazil ice will begin to adhere to objects in the water, especially if the objects themselves are at a temperature below water’s freezing point. The accumulation of frazil ice often causes flooding or damage to objects such as trash racks. Since frazil ice is found below the surface of water, it is difficult to detect its formation.\n\nUsually what happens is the frazil ice accumulates on the upstream side of objects and sticks to them. The frazil ice accumulates as more gets deposited. The growth will extend upstream and increase in width until the point where the frazil ice accumulations bridge together and block the water. As more and more water flows against this block, the pressure on the upstream side increases and causes a differential pressure (difference in pressure from the upstream side and the downstream side). This will cause the growth of the bridge to extend downstream. Once this happens, flooding and damage is likely unless otherwise prevented.\n\nFrazil ice has also been demonstrated to form beneath temperate (or \"warm-based\") glaciers as water flows quickly downhill and supercools due to a rapid loss of pressure. This \"glaciohydraulic supercooling\" process forms an open network of platy ice crystals that can effectively trap silt from the sediment-laden water that flows beneath glaciers and ice sheets. Subsequent freezing and recrystallization can result in a layer of sediment-rich ice at the base of the glacier which, upon melting at the terminus, can result in significant accumulation of sediment in moraines. This phenomenon has been verified by elevated concentrations of tritium — produced by nuclear weapons testing and therefore almost entirely absent in ice frozen before 1945 — in the basal ice of several glaciers (signifying young ice) and the observation of rapid growth of ice crystals around water discharge vents at glacier termini.\n\nThere are several ways to control frazil ice build up. They include suppression, mechanical control, thermal control, vibration, materials selection and damage mitigation.\n\nFrazil ice forms in supercooled water which occurs because the surface water loses heat to cooler air above. Suppression is the idea of ‘insulating’ the surface water with an intact, stable ice cover. The ice cover will prevent heat loss and warm the supercooled water that might have already formed. Sufficient area needs to be covered in order for this method to work, but it is still unknown what is meant by \"sufficient\".\n\nThese methods include stabilizing freeze without restricting water flow, such as implementing weirs and ice booms, installing water jets to break up any accumulation that might occur, and using manual labour to rake away the accumulation. This final method is often not preferred because of high labour costs, cold, wet and late night working conditions. Back flushing is another technology that uses the idea of cancelling out the differential pressure caused by the frazil ice accumulation. This technology creates a high pressure on the downstream side of objects to reverse the differential pressure.\n\nThese methods either heat the structures in the water to prevent frazil ice adhesion or heat the water to prevent frazil ice from forming in the first place. When heating the structure, it must be heated to a temperature above freezing. Electrical resistance heaters have been found to work well, but these have potential safety problems. Installing hollow tubes in the structures through which steam or warm water is pumped also works, but this method has been seen to cause operational problems. Other active methods are also available. Sometimes warm water byproducts are released from nearby water facilities, and the warm water is wasted. Redirecting this water to potential frazil ice accumulation regions could raise the water temperature by , often enough to prevent supercooled water from developing.\n\nAlthough still in the experimental stages, blasting with dynamite is one form of vibrational control that will break loose any frazil ice accumulation. The charge must be precise such that the ice breaks, but surrounding structures and environment are not harmed. Safety of the blasting also is important and nearby residents might complain about sound pollution. For all these reasons, this method is not often used, except as an emergency last resort.\n\nMan-made structures are often the objects to which frazil ice adheres. As such, the choice of materials for these structures should include consideration of ice adhesion. Steel structures, for example will rust, and rust-to-ice adhesion is very strong. Choosing a material with lower adhesion such as plastic, fiberglass, graphite or even an epoxy paint coating on the steel will reduce the adhesion probability. Although adhesion will still occur, using such materials makes other methods, such as raking, easier.\n\nDamage could be reduced by protecting designated flood regions with mechanical structures.\n\n\n\n"}
{"id": "1430271", "url": "https://en.wikipedia.org/wiki?curid=1430271", "title": "Garland", "text": "Garland\n\nA garland is a decorative wreath or cord (typically used at festive occasions) which can be hung round a person's neck or on inanimate objects like Christmas trees. Originally garlands were made of flowers or leaves.\n\nFrom the French \"guirlde\", itself from the Italian \"ghirlanda,\" a braid.\n\n\nA garland created from the daisy flower (generally as a children's game) is called a daisy chain. One method of creating a daisy chain is to pick daisies and create a hole towards the base of the stem (such as with fingernails or tying a knot). The stem of the next flower can be threaded through until stopped by the head of the flower. By repeating this with many daisies, it is possible to build up long chains and to form them into simple bracelets and necklaces. Another popular method involves pressing the flower heads against each other to create a look similar to a caterpillar.\n\nThe terms \"daisy chain\" or \"daisy chaining\" can also refer to various technical and social \"chains.\"\n\nIn \"Alice's Adventures in Wonderland\" by Lewis Carroll, before Alice's adventures begin, she is sitting outside with her sister considering whether to make a daisy chain before being interrupted by a White Rabbit.\n\n\nIn India, where flower garlands have an important and traditional role in every festival, Hindu deities are decorated with garlands made from different fragrant flowers (often jasmine) and leaves. Both fragrant and non-fragrant flowers and religiously-significant leaves are used to make garlands to worship Hindu deities. Some of those flowers are as follows: jasmine, champaka, lotus, lilies, ashoka, nerium/oleander, chrysanthemum, roses, hibiscus, pinwheel flowers, manoranjini etc.\n\nApart from these, leaves and grasses like arugampul, maruvakam, davanam, maachi, paneer leaves, lavancha are also used for making garlands. Also fruit, vegetables and sometimes even currency notes are used for garlands, given as thanksgiving. Also in wedding the couple wears a wedding garland. In other occasions, it is used to show respect to an individual person or statue (murti). In Tamil Nadu marigold, nitya kalyani flower garlands are used only for dead bodies or burial rituals.In functions, garlands are used to denote the main person(host).\n\nA Gajra is a flower garland which women in India and Bangladesh wear in their hair during traditional festivals. It is made usually of jasmine. It can be worn both around a bun and in braids. Women usually wear these when they wear sarees. Sometimes, they are pinned in hair with other flowers, like roses.\n\nIn Tamil Nadu temples (ancient) kings appointed people for making garlands daily for a particular deity. They were not allowed to sell that garland. Each Hindu temple in southern India has nandavanam (flower garden) where floral plants, trees for garlands are grown. Huge Shiva temples like Thillai Nataraja Temple, Chidambaram, Thyagaraja Temple, Tiruvarur and Arunachaleswara Temple, Thiruvannamalai still preserves such Nandavanams for supply of flowers for everyday rituals. Stone Inscriptions of Raja Raja Chola I at Thanjavur gives details of patronage of royals to the conservation of Nadavanams that belongs to The Big Temple.\n\nIn Srirangam Ranganathar temple only garlands made by temple sattharars (people who make garlands and never marry - surrendering their life to the lord's service) are used to adorn Lord Ranganatha. No other garlands, flowers are used there. Sattarars have traditional rules for everything - from plucking the flower to making garlands. Some of them are as follows:\n\nWhile making garlands they keep flowers and other materials on a table because the garland for God should not touch the feet. It is always kept above hip level.\n\nDepending upon the pattern and materials used the south Indian garlands are of different types. Some of them are as follows:\n\nThe maala for Gods has 2 free lower ends with kunjam (bunch of flowers), i.e. only the upper two ends are joined and the lower ends should not be not joined. It has two kunjams. Whereas garlands for humans have both lower ends joined together (only one kunjam).\n\nEach Hindu deity has a unique garland: Goddess Lalitha wears hibiscus garland, Lord Vishnu wears tulasi leaves garland, Lord Shiva wears Bilvga leaves garland, Lord Subrahmanya wears jasmine garland, Mahalakshmi wears red lotus, Saraswathi devi wears a white lotus garland.\nDurga Devi wears nerium oleander garland, Vinayaka wears garland made of Durva Grass.\n\nThe tradition of garlanding statues as a sign of respect also extends to respected non-deities including ancient King Perumbidugu Mutharaiyar II and even innovative colonial administrator Mark Cubbon.\n\n\n"}
{"id": "45598981", "url": "https://en.wikipedia.org/wiki?curid=45598981", "title": "Gaseous signaling molecules", "text": "Gaseous signaling molecules\n\nGaseous signaling molecules are gaseous molecules that are either synthesised internally (endogenously) in the organism, tissue or cell or are received by the organism, tissue or cell from outside (say, from the atmosphere or hydrosphere, as in the case of oxygen) and that are used to transmit chemical signals which induce certain physiological or biochemical changes in the organism, tissue or cell. The term is applied to, for example, oxygen, carbon dioxide, nitric oxide, carbon monoxide, hydrogen sulfide, sulfur dioxide, nitrous oxide, hydrogen cyanide, ammonia, methane, hydrogen, ethylene, etc.\n\nMany, but not all, gaseous signaling molecules are called gasotransmitters.\n\nThe biological roles of each of the gaseous signaling molecules are in short outlined below.\n\nGasotransmitters is a subfamily of endogenous molecules of gases or gaseous signaling molecules, including NO, CO, . These particular gases share many common features in their production and function but carry on their tasks in unique ways, which differ from classical signaling molecules, in the human body. In 1981, it was first suggested from clinical work with nitrous oxide that a gas had a direct action at pharmacological receptors and thereby acted as a neurotransmitter. In vitro experiments confirmed these observations which were replicated at NIDA later.\nThe terminology and characterization criteria of “gasotransmitter” were firstly introduced in 2002. For one gas molecule to be categorized as a gasotransmitters, all of the following criteria should be met.\n\nIn 2011, a European Network on Gasotransmitters (ENOG) was formed. The aim of the network is to promote research on NO, CO and in order to better understand the biology of gasotransmitters and to unravel the role of each mediator in health and disease. Moreover, the network aims to contribute to the translation of basic science knowledge in this area of research into therapeutic or diagnostic tools.\n\nCarbon dioxide is one of the mediators of local autoregulation of blood supply. If its levels are high, the capillaries expand to allow a greater blood flow to that tissue.\n\nBicarbonate ions are crucial for regulating blood pH. A person's breathing rate influences the level of CO in their blood. Breathing that is too slow or shallow causes respiratory acidosis, while breathing that is too rapid leads to hyperventilation, which can cause respiratory alkalosis.\n\nAlthough the body requires oxygen for metabolism, low oxygen levels normally do not stimulate breathing. Rather, breathing is stimulated by higher carbon dioxide levels.\n\nThe respiratory centers try to maintain an arterial CO pressure of 40 mm Hg. With intentional hyperventilation, the CO content of arterial blood may be lowered to 10–20 mm Hg (the oxygen content of the blood is little affected), and the respiratory drive is diminished. This is why one can hold one's breath longer after hyperventilating than without hyperventilating. This carries the risk that unconsciousness may result before the need to breathe becomes overwhelming, which is why hyperventilation is particularly dangerous before free diving.\n\nNO is one of the few gaseous signalling molecules known and is additionally exceptional due to the fact that it is a radical gas. It is a key vertebrate biological messenger, playing a role in a variety of biological processes. It is a known bioproduct in almost all types of organisms, ranging from bacteria to plants, fungi, and animal cells.\n\nNitric oxide, known as the 'endothelium-derived relaxing factor', or 'EDRF', is biosynthesized endogenously from L-arginine, oxygen, and NADPH by various nitric oxide synthase (NOS) enzymes. Reduction of inorganic nitrate may also serve to make nitric oxide. The endothelium (inner lining) of blood vessels uses nitric oxide to signal the surrounding smooth muscle to relax, thus resulting in vasodilation and increasing blood flow. Nitric oxide is highly reactive (having a lifetime of a few seconds), yet diffuses freely across membranes. These attributes make nitric oxide ideal for a transient paracrine (between adjacent cells) and autocrine (within a single cell) signaling molecule.\n\nIndependent of nitric oxide synthase, an alternative pathway, coined the nitrate-nitrite-nitric oxide pathway, elevates nitric oxide through the sequential reduction of dietary nitrate derived from plant-based foods. Nitrate-rich vegetables, in particular leafy greens, such as spinach and arugula, and beetroot, have been shown to increase cardioprotective levels of nitric oxide with a corresponding reduction in blood pressure in pre-hypertensive persons. For the body to generate nitric oxide through the nitrate-nitrite-nitric oxide pathway, the reduction of nitrate to nitrite occurs in the mouth, by commensal bacteria, an obligatory and necessary step. Monitoring nitric oxide status by saliva testing detects the bioconversion of plant-derived nitrate into nitric oxide. A rise in salivary levels is indicative of diets rich in leafy vegetables which are often abundant in anti-hypertensive diets such as the DASH diet.\n\nThe production of nitric oxide is elevated in populations living at high altitudes, which helps these people avoid hypoxia by aiding in pulmonary vasculature vasodilation. Effects include vasodilatation, neurotransmission, modulation of the hair cycle, production of reactive nitrogen intermediates and penile erections (through its ability to vasodilate). Nitroglycerin and amyl nitrite serve as vasodilators because they are converted to nitric oxide in the body. The vasodilating antihypertensive drug minoxidil contains an NO moiety and may act as an NO agonist. Likewise, Sildenafil citrate, popularly known by the trade name \"Viagra\", stimulates erections primarily by enhancing signaling through the nitric oxide pathway in the penis.\n\nNitric oxide (NO) contributes to vessel homeostasis by inhibiting vascular smooth muscle contraction and growth, platelet aggregation, and leukocyte adhesion to the endothelium. Humans with atherosclerosis, diabetes, or hypertension often show impaired NO pathways. A high salt intake was demonstrated to attenuate NO production in patients with essential hypertension, although bioavailability remains unregulated.\n\nNitric oxide is also generated by phagocytes (monocytes, macrophages, and neutrophils) as part of the human immune response. Phagocytes are armed with inducible nitric oxide synthase (iNOS), which is activated by interferon-gamma (IFN-γ) as a single signal or by tumor necrosis factor (TNF) along with a second signal. On the other hand, transforming growth factor-beta (TGF-β) provides a strong inhibitory signal to iNOS, whereas interleukin-4 (IL-4) and IL-10 provide weak inhibitory signals. In this way, the immune system may regulate the resources of phagocytes that play a role in inflammation and immune responses. Nitric oxide is secreted as free radicals in an immune response and is toxic to bacteria and intracellular parasites, including \"Leishmania\" and malaria; the mechanism for this includes DNA damage and degradation of iron sulfur centers into iron ions and iron-nitrosyl compounds.\n\nIn response, many bacterial pathogens have evolved mechanisms for nitric oxide resistance. Because nitric oxide might have proinflammatory actions in conditions like asthma, there has been increasing interest in the use of exhaled nitric oxide as a breath test in diseases with airway inflammation. Reduced levels of exhaled NO have been associated with exposure to air pollution in cyclists and smokers, but, in general, increased levels of exhaled NO are associated with exposure to air pollution.\n\nNitric oxide can contribute to reperfusion injury when an excessive amount produced during reperfusion (following a period of ischemia) reacts with superoxide to produce the damaging oxidant peroxynitrite. In contrast, inhaled nitric oxide has been shown to help survival and recovery from paraquat poisoning, which produces lung tissue-damaging superoxide and hinders NOS metabolism.\n\nIn plants, nitric oxide can be produced by any of four routes: (i) L-arginine-dependent nitric oxide synthase, (although the existence of animal NOS homologs in plants is debated), (ii) plasma membrane-bound nitrate reductase, (iii) mitochondrial electron transport chain, or (iv) non-enzymatic reactions. It is a signaling molecule, acts mainly against oxidative stress and also plays a role in plant pathogen interactions. Treating cut flowers and other plants with nitric oxide has been shown to lengthen the time before wilting.\n\nTwo important biological reaction mechanisms of nitric oxide are S-nitrosation of thiols, and nitrosylation of transition metal ions. S-nitrosation involves the (reversible) conversion of thiol groups, including cysteine residues in proteins, to form S-nitrosothiols (RSNOs). S-Nitrosation is a mechanism for dynamic, post-translational regulation of most or all major classes of protein. The second mechanism, nitrosylation, involves the binding of NO to a transition metal ion like iron or copper. In this function, NO is referred to as a nitrosyl ligand. Typical cases involve the nitrosylation of heme proteins like cytochromes, thereby disabling the normal enzymatic activity of the enzyme. Nitrosylated ferrous iron is particularly stable, as the binding of the nitrosyl ligand to ferrous iron (Fe(II)) is very strong. Hemoglobin is a prominent example of a heme protein that may be modified by NO by both pathways: NO may attach directly to the heme in the nitrosylation reaction, and independently form S-nitrosothiols by S-nitrosation of the thiol moieties.\n\nThere are several mechanisms by which NO has been demonstrated to affect the biology of living cells. These include oxidation of iron-containing proteins such as ribonucleotide reductase and aconitase, activation of the soluble guanylate cyclase, ADP ribosylation of proteins, protein sulfhydryl group nitrosylation, and iron regulatory factor activation. NO has been demonstrated to activate NF-κB in peripheral blood mononuclear cells, an important transcription factor in iNOS gene expression in response to inflammation.\n\nIt was found that NO acts through the stimulation of the soluble guanylate cyclase, which is a heterodimeric enzyme with subsequent formation of cyclic-GMP. Cyclic-GMP activates protein kinase G, which causes reuptake of Ca and the opening of calcium-activated potassium channels. The fall in concentration of Ca ensures that the myosin light-chain kinase (MLCK) can no longer phosphorylate the myosin molecule, thereby stopping the crossbridge cycle and leading to relaxation of the smooth muscle cell.\n\nNitrous oxide in biological systems can be formed by an enzymatic or non-enzymatic reduction of nitric oxide. In vitro studies have shown that endogenous nitrous oxide can be formed by the reaction between nitric oxide and thiol. Some authors have shown that this process of NO reduction to NO takes place in hepatocytes, specifically in their cytoplasm and mitochondria, and suggested that the NO can possibly be produced in mammalian cells. It is well known that NO is produced by some bacteria during process called denitrification.\n\nApart from its direct and indirect actions at opioid receptors, it was also shown that NO inhibits NMDA receptor-mediated activity and ionic currents and diminishes NMDA receptor-mediated excitotoxicity and neurodegeneration. Nitrous oxide also inhibits methionine synthase and slows the conversion of homocysteine to methionine, increases homocysteine concentration and decreases methionine concentration. This effect was shown in lymphocyte cell cultures and in human liver biopsy samples.\n\nNitrous oxide does not bind as a ligand to the heme and does not react with thiol-containing proteins. Nevertheless, studies have shown that nitrous oxide can reversibly and non-covalently \"insert\" itself into the inner structures of some heme-containing proteins such as hemoglobin, myoglobin, cytochrome oxidase and alter their structure and function. The ability of nitrous oxide to alter the structure and function of these proteins was demonstrated by shifts in infrared spectra of cysteine thiols of hemoglobin and by partial and reversible inhibition of cytochrome oxidase.\n\nEndogenous nitrous oxide can possibly play a role in modulating endogenous opioid and NMDA systems.\n\nCarbon monoxide is produced naturally by the human body as a signaling molecule. Thus, carbon monoxide may have a physiological role in the body, such as a neurotransmitter or a blood vessel relaxant. Because of carbon monoxide's role in the body, abnormalities in its metabolism have been linked to a variety of diseases, including neurodegenerations, hypertension, heart failure, and inflammation.\n\nSummary of function: \n\nIn mammals, carbon monoxide is naturally produced by the action of heme oxygenase 1 and 2 on the heme from hemoglobin breakdown. This process produces a certain amount of carboxyhemoglobin in normal persons, even if they do not breathe any carbon monoxide.\n\nFollowing the first report that carbon monoxide is a normal neurotransmitter in 1993, as well as one of three gases that naturally modulate inflammatory responses in the body (the other two being nitric oxide and hydrogen sulfide), carbon monoxide has received a great deal of clinical attention as a biological regulator. In many tissues, all three gases are known to act as anti-inflammatories, vasodilators, and encouragers of neovascular growth. However, the issues are complex, as neovascular growth is not always beneficial, since it plays a role in tumor growth, and also the damage from \"wet\" macular degeneration, a disease for which smoking (a major source of carbon monoxide in the blood, several times more than natural production) increases the risk from 4 to 6 times.\n\nThere is a theory that, in some nerve cell synapses, when long-term memories are being laid down, the receiving cell makes carbon monoxide, which back-transmits to the transmitting cell, telling it to transmit more readily in future. Some such nerve cells have been shown to contain guanylate cyclase, an enzyme that is activated by carbon monoxide.\n\nStudies involving carbon monoxide have been conducted in many laboratories throughout the world for its anti-inflammatory and cytoprotective properties. These properties have potential to be used to prevent the development of a series of pathological conditions including ischemia reperfusion injury, transplant rejection, atherosclerosis, severe sepsis, severe malaria, or autoimmunity. Clinical tests involving humans have been performed, however the results have not yet been released.\n\nCarbon suboxide, CO, can be produced in small amounts in any biochemical process that normally produces carbon monoxide, CO, for example, during heme oxidation by heme oxygenase-1. It can also be formed from malonic acid. It has been shown that carbon suboxide in an organism can quickly polymerize into macrocyclic polycarbon structures with the common formula (CO) (mostly (CO) and (CO)), and that those macrocyclic compounds are potent inhibitors of Na/K-ATP-ase and Ca-dependent ATP-ase, and have digoxin-like physiological properties and natriuretic and antihypertensive actions. Those macrocyclic carbon suboxide polymer compounds are thought to be endogenous digoxin-like regulators of Na/K-ATP-ases and Ca-dependent ATP-ases, and endogenous natriuretics and antihypertensives. Other than that, some authors think also that those macrocyclic compounds of carbon suboxide can possibly diminish free radical formation and oxidative stress and play a role in endogenous anticancer protective mechanisms, for example in the retina.\n\nHydrogen sulfide is produced in small amounts by some cells of the mammalian body and has a number of biological signaling functions. (Only two other such gases are currently known: nitric oxide (NO) and carbon monoxide (CO).)\n\nThe gas is produced from cysteine by the enzymes cystathionine beta-synthase and cystathionine gamma-lyase. It acts as a relaxant of smooth muscle and as a vasodilator and is also active in the brain, where it increases the response of the NMDA receptor and facilitates long term potentiation, which is involved in the formation of memory.\n\nEventually the gas is converted to sulfite in the mitochondria by thiosulfate reductase, and the sulfite is further oxidized to thiosulfate and sulfate by sulfite oxidase. The sulfates are excreted in the urine.\n\nDue to its effects similar to nitric oxide (without its potential to form peroxides by interacting with superoxide), hydrogen sulfide is now recognized as potentially protecting against cardiovascular disease. The cardioprotective role effect of garlic is caused by catabolism of the polysulfide group in allicin to , a reaction that could depend on reduction mediated by glutathione.\n\nThough both nitric oxide (NO) and hydrogen sulfide have been shown to relax blood vessels, their mechanisms of action are different: while NO activates the enzyme guanylyl cyclase, activates ATP-sensitive potassium channels in smooth muscle cells. Researchers are not clear how the vessel-relaxing responsibilities are shared between nitric oxide and hydrogen sulfide. However, there exists some evidence to suggest that nitric oxide does most of the vessel-relaxing work in large vessels and hydrogen sulfide is responsible for similar action in smaller blood vessels.\n\nRecent findings suggest strong cellular crosstalk of NO and , demonstrating that the vasodilatatory effects of these two gases are mutually dependent. Additionally, reacts with intracellular S-nitrosothiols to form the smallest S-nitrosothiol (HSNO), and a role of hydrogen sulfide in controlling the intracellular S-nitrosothiol pool has been suggested.\n\nLike nitric oxide, hydrogen sulfide is involved in the relaxation of smooth muscle that causes erection of the penis, presenting possible new therapy opportunities for erectile dysfunction.\n\nHydrogen sulfide (H2S) deficiency can be detrimental to the vascular function after an acute myocardial infarction (AMI). AMIs can lead to cardiac dysfunction through two distinct changes; increased oxidative stress via free radical accumulation and decreased NO bioavailability. Free radical accumulation occurs due to increased electron transport uncoupling at the active site of endothelial nitric oxide synthase (eNOS), an enzyme involved in converting L-arginine to NO. During an AMI, oxidative degradation of tetrahydrobiopterin (BH4), a cofactor in NO production, limits BH4 availability and limits NO productionby eNOS. Instead, eNOS reacts with oxygen, another cosubstrates involved in NO production. The products of eNOS are reduced to superoxides, increasing free radical production and oxidative stress within the cells. A H2S deficiency impairs eNOS activity by limiting Akt activation and inhibiting Akt phosphorylation of the eNOSS1177 activation site. Instead, Akt activity is increased to phosphorylate the eNOST495 inhibition site, downregulating eNOS production of NO.\n\nH2S therapy uses a H2S donor, such as diallyl trisulfide (DATS), to increase the supply of H2S to an AMI patient. H2S donors reduce myocardial injury and reperfusion complications. Increased H2S levels within the body will react with oxygen to produce sulfane sulfur, a storage intermediate for H2S. H2S pools in the body attracts oxygen to react with excess H2S and eNOS to increase NO production. With increased use of oxygen to produce more NO, less oxygen is available to react with eNOS to produce superoxides during an AMI, ultimately lowering the accumulation of reactive oxygen species (ROS). Furthermore, decreased accumulation of ROS lowers oxidative stress in vascular smooth muscle cells, decreasing oxidative degeneration of BH4. Increased BH4 cofactor contributes to increased production of NO within the body. Higher concentrations of H2S directly increase eNOS activity through Akt activation to increase phosphorylation of the eNOSS1177 activation site, and decrease phosphorylation of the eNOST495 inhibition site. This phosphorylation process upregulates eNOS activity, catalyzing more conversion of L-arginine to NO. Increased NO production enables soluble guanylyl cyclase (sGC) activity, leading to an increased conversion of guanosine triphosphate (GTP) to 3’,5’-cyclic guanosine monophosphate (cGMP). In H2S therapy immediately following an AMI, increased cGMP triggers an increase in protein kinase G (PKG) activity. PKG reduces intracellular Ca2+ in vascular smooth muscle to increase smooth muscle relaxation and promote blood flow. PKG also limits smooth muscle cell proliferation, reducing intima thickening following AMI injury, ultimately decreasing myocardial infarct size.\n\nIn Alzheimer's disease the brain's hydrogen sulfide concentration is severely decreased. In a certain rat model of Parkinson's disease, the brain's hydrogen sulfide concentration was found to be reduced, and administering hydrogen sulfide alleviated the condition. In trisomy 21 (Down syndrome) the body produces an excess of hydrogen sulfide. Hydrogen sulfide is also involved in the disease process of type 1 diabetes. The beta cells of the pancreas in type 1 diabetes produce an excess of the gas, leading to the death of these cells and to a reduced production of insulin by those that remain.\n\nIn 2005, it was shown that mice can be put into a state of suspended animation-like hypothermia by applying a low dosage of hydrogen sulfide (81 ppm ) in the air. The breathing rate of the animals sank from 120 to 10 breaths per minute and their temperature fell from 37 °C to just 2 °C above ambient temperature (in effect, they had become cold-blooded). The mice survived this procedure for 6 hours and afterwards showed no negative health consequences. In 2006 it was shown that the blood pressure of mice treated in this fashion with hydrogen sulfide did not significantly decrease.\n\nA similar process known as hibernation occurs naturally in many mammals and also in toads, but not in mice. (Mice can fall into a state called clinical torpor when food shortage occurs). If the -induced hibernation can be made to work in humans, it could be useful in the emergency management of severely injured patients, and in the conservation of donated organs. In 2008, hypothermia induced by hydrogen sulfide for 48 hours was shown to reduce the extent of brain damage caused by experimental stroke in rats.\n\nAs mentioned above, hydrogen sulfide binds to cytochrome oxidase and thereby prevents oxygen from binding, which leads to the dramatic slowdown of metabolism. Animals and humans naturally produce some hydrogen sulfide in their body; researchers have proposed that the gas is used to regulate metabolic activity and body temperature, which would explain the above findings.\n\nTwo recent studies cast doubt that the effect can be achieved in larger mammals. A 2008 study failed to reproduce the effect in pigs, concluding that the effects seen in mice were not present in larger mammals. Likewise a paper by Haouzi et al. noted that there is no induction of hypometabolism in sheep, either.\n\nAt the February 2010 TED conference, Mark Roth announced that hydrogen sulfide induced hypothermia \"in humans\" had completed Phase I clinical trials. The clinical trials commissioned by the company he helped found, Ikaria, were however withdrawn or terminated by August 2011.\n\nThe role of sulfur dioxide in mammalian biology is not yet well understood. Sulfur dioxide blocks nerve signals from the pulmonary stretch receptors and abolishes the Hering–Breuer inflation reflex.\n\nIt was shown that endogenous sulfur dioxide plays a role in diminishing an experimental lung damage caused by oleic acid. Endogenous sulfur dioxide lowered lipid peroxidation, free radical formation, oxidative stress and inflammation during an experimental lung damage. Conversely, a successful lung damage caused a significant lowering of endogenous sulfur dioxide production, and an increase in lipid peroxidation, free radical formation, oxidative stress and inflammation. Moreover, blockade of an enzyme that produces endogenous SO significantly increased the amount of lung tissue damage in the experiment. Conversely, adding acetylcysteine or glutathione to the rat diet increased the amount of endogenous SO produced and decreased the lung damage, the free radical formation, oxidative stress, inflammation and apoptosis.\n\nIt is considered that endogenous sulfur dioxide plays a significant physiological role in regulating cardiac and blood vessel function, and aberrant or deficient sulfur dioxide metabolism can contribute to several different cardiovascular diseases, such as arterial hypertension, atherosclerosis, pulmonary arterial hypertension, stenocardia.\n\nIt was shown that in children with pulmonary arterial hypertension due to congenital heart diseases the level of homocysteine is higher and the level of endogenous sulfur dioxide is lower than in normal control children. Moreover, these biochemical parameters strongly correlated to the severity of pulmonary arterial hypertension. Authors considered homocysteine to be one of useful biochemical markers of disease severity and sulfur dioxide metabolism to be one of potential therapeutic targets in those patients.\n\nEndogenous sulfur dioxide also has been shown to lower the proliferation rate of endothelial smooth muscle cells in blood vessels, via lowering the MAPK activity and activating adenylyl cyclase and protein kinase A. Smooth muscle cell proliferation is one of important mechanisms of hypertensive remodeling of blood vessels and their stenosis, so it is an important pathogenetic mechanism in arterial hypertension and atherosclerosis.\n\nEndogenous sulfur dioxide in low concentrations causes endothelium-dependent vasodilation. In higher concentrations it causes endothelium-independent vasodilation and has a negative inotropic effect on cardiac output function, thus effectively lowering blood pressure and myocardial oxygen consumption. The vasodilating effects of sulfur dioxide are mediated via ATP-dependent calcium channels and L-type (\"dihydropyridine\") calcium channels. Endogenous sulfur dioxide is also a potent antiinflammatory, antioxidant and cytoprotective agent. It lowers blood pressure and slows hypertensive remodeling of blood vessels, especially thickening of their intima. It also regulates lipid metabolism.\n\nEndogenous sulfur dioxide also diminishes myocardial damage, caused by isoproterenol adrenergic hyperstimulation, and strengthens the myocardial antioxidant defense reserve.\n\nSulfur dioxide expelled from the gastrointestinal tract (through the process of flatulence) is thought to be one of the main constituents to the unpleasant smell colloquially referred to as a \"fart\".\n\nSome authors have shown that neurons can produce hydrogen cyanide upon activation of their opioid receptors by endogenous or exogenous opioids. They have also shown that neuronal production of HCN activates NMDA receptors and plays a role in signal transduction between neuronal cells (neurotransmission). Moreover, increased endogenous neuronal HCN production under opioids was seemingly needed for adequate opioid analgesia, as analgesic action of opioids was attenuated by HCN scavengers. They considered endogenous HCN to be a neuromodulator.\n\nIt was also shown that, while stimulating muscarinic cholinergic receptors in cultured pheochromocytoma cells \"increases\" HCN production, in a living organism (\"in vivo\") muscarinic cholinergic stimulation actually \"decreases\" HCN production.\n\nLeukocytes generate HCN during phagocytosis.\n\nThe vasodilatation, caused by sodium nitroprusside, has been shown to be mediated not only by NO generation, but also by endogenous cyanide generation, which adds not only toxicity, but also some additional antihypertensive efficacy compared to nitroglycerine and other non-cyanogenic nitrates which do not cause blood cyanide levels to rise.\n\nAmmonia also plays a role in both normal and abnormal animal physiology. It is biosynthesised through normal amino acid metabolism and is toxic in high concentrations. The liver converts ammonia to urea through a series of reactions known as the urea cycle. Liver dysfunction, such as that seen in cirrhosis, may lead to elevated amounts of ammonia in the blood (hyperammonemia). Likewise, defects in the enzymes responsible for the urea cycle, such as ornithine transcarbamylase, lead to hyperammonemia. Hyperammonemia contributes to the confusion and coma of hepatic encephalopathy, as well as the neurologic disease common in people with urea cycle defects and organic acidurias.\n\nAmmonia is important for normal animal acid/base balance. After formation of ammonium from glutamine, α-ketoglutarate may be degraded to produce two molecules of bicarbonate, which are then available as buffers for dietary acids. Ammonium is excreted in the urine, resulting in net acid loss. Ammonia may itself diffuse across the renal tubules, combine with a hydrogen ion, and thus allow for further acid excretion.\n\nSome authors have shown that endogenous methane is produced not only by the intestinal flora and then absorbed into the blood, but also is produced - in small amounts - by eukaryotic cells (during process of lipid peroxidation). And they have also shown that the endogenous methane production rises during an experimental mitochondrial hypoxia, for example, sodium azide intoxication. They thought that methane could be one of intercellular signals of hypoxia and stress.\n\nOther authors have shown that cellular methane production also rises during sepsis or bacterial endotoxemia, including an experimental imitation of endotoxemia by lipopolysaccharide (LPS) administration.\n\nSome other researchers have shown that methane, produced by the intestinal flora, is not fully \"biologically neutral\" to the intestine, and it participates in the normal physiologic regulation of peristalsis. And its excess causes not only belching, flatulence and belly pain, but also functional constipation.\n\nEthylene serves as a hormone in plants. It acts at trace levels throughout the life of the plant by stimulating or regulating the ripening of fruit, the opening of flowers, and the abscission (or shedding) of leaves.\nCommercial ripening rooms use \"catalytic generators\" to make ethylene gas from a liquid supply of ethanol. Typically, a gassing level of 500 to 2,000 ppm is used, for 24 to 48 hours. Care must be taken to control carbon dioxide levels in ripening rooms when gassing, as high temperature ripening () has been seen to produce CO levels of 10% in 24 hours.\n\nEthylene has been used since the ancient Egyptians, who would gash figs in order to stimulate ripening (wounding stimulates ethylene production by plant tissues). The ancient Chinese would burn incense in closed rooms to enhance the ripening of pears. In 1864, it was discovered that gas leaks from street lights led to stunting of growth, twisting of plants, and abnormal thickening of stems. In 1901, a Russian scientist named Dimitry Neljubow showed that the active component was ethylene. Sarah Doubt discovered that ethylene stimulated abscission in 1917. It wasn't until 1934 that Gane reported that plants synthesize ethylene. In 1935, Crocker proposed that ethylene was the plant hormone responsible for fruit ripening as well as senescence of vegetative tissues.\nEthylene is produced from essentially all parts of higher plants, including leaves, stems, roots, flowers, fruits, tubers, and seeds.\nEthylene production is regulated by a variety of developmental and environmental factors. During the life of the plant, ethylene production is induced during certain stages of growth such as germination, ripening of fruits, abscission of leaves, and senescence of flowers. Ethylene production can also be induced by a variety of external aspects such as mechanical wounding, environmental stresses, and certain chemicals including auxin and other regulators.\n\nEthylene is biosynthesized from the amino acid methionine to \"S\"-adenosyl--methionine (SAM, also called Adomet) by the enzyme Met Adenosyltransferase. SAM is then converted to 1-aminocyclopropane-1-carboxylic acid (ACC) by the enzyme ACC synthase (ACS). The activity of ACS determines the rate of ethylene production, therefore regulation of this enzyme is key for the ethylene biosynthesis. The final step requires oxygen and involves the action of the enzyme ACC-oxidase (ACO), formerly known as the ethylene forming enzyme (EFE). Ethylene biosynthesis can be induced by endogenous or exogenous ethylene. ACC synthesis increases with high levels of auxins, especially indole acetic acid (IAA) and cytokinins.\n\nEthylene is perceived by a family of five transmembrane protein dimers such as the ETR protein in \"Arabidopsis\". The gene encoding an ethylene receptor has been cloned in \"Arabidopsis thaliana\" and then in tomato. Ethylene receptors are encoded by multiple genes in the \"Arabidopsis\" and tomato genomes. Mutations in any of the gene family, which comprises five receptors in \"Arabidopsis\" and at least six in tomato, can lead to insensitivity to ethylene. DNA sequences for ethylene receptors have also been identified in many other plant species and an ethylene binding protein has even been identified in Cyanobacteria.\n\nEnvironmental cues such as flooding, drought, chilling, wounding, and pathogen attack can induce ethylene formation in plants. In flooding, roots suffer from lack of oxygen, or anoxia, which leads to the synthesis of 1-aminocyclopropane-1-carboxylic acid (ACC). ACC is transported upwards in the plant and then oxidized in leaves. The ethylene produced causes nastic movements (epinasty) of the leaves, perhaps helping the plant to lose water.\n\nEthylene in plant induces such responses:\n\nSmall amounts of endogenous ethylene are also produced in mammals, including humans, due to lipid peroxidation. Some of endogenous ethylene is then oxidized to ethylene oxide, which is able to alkylate DNA and proteins, including hemoglobin (forming a specific adduct with its N-terminal valine, N-hydroxyethyl-valine). Endogenous ethylene oxide, just as like environmental (exogenous) one, can alkylate guanine in DNA, forming an adduct 7-(2-hydroxyethyl)-guanine, and this poses an intrinsic carcinogenic risk. It is also mutagenic.\n"}
{"id": "345035", "url": "https://en.wikipedia.org/wiki?curid=345035", "title": "Halo (optical phenomenon)", "text": "Halo (optical phenomenon)\n\nHalo (from Greek , \"halōs\") is the name for a family of optical phenomena produced by sunlight interacting with ice crystals suspended in the atmosphere. Halos can have many forms, ranging from colored or white rings to arcs and spots in the sky. Many of these appear near the Sun or Moon, but others occur elsewhere or even in the opposite part of the sky. Among the best known halo types are the circular halo (properly called the 22° halo), light pillars, and sun dogs, but many others occur; some are fairly common while others are (extremely) rare.\n\nThe ice crystals responsible for halos are typically suspended in cirrus or cirrostratus clouds in the upper troposphere (), but in cold weather they can also float near the ground, in which case they are referred to as diamond dust. The particular shape and orientation of the crystals are responsible for the type of halo observed. Light is reflected and refracted by the ice crystals and may split into colors because of dispersion. The crystals behave like prisms and mirrors, refracting and reflecting light between their faces, sending shafts of light in particular directions.\n\nAtmospheric optical phenomena like halos were used as part of weather lore, which was an empirical means of weather forecasting before meteorology was developed. They often do indicate that rain will fall within the next 24 hours, since the cirrostratus clouds that cause them can signify an approaching frontal system.\n\nOther common types of optical phenomena involving water droplets rather than ice crystals include the glory and rainbow.\n\nWhile Aristotle had mentioned halos and parhelia, in antiquity, the first European descriptions of complex displays were those of Christoph Scheiner in Rome (circa 1630), Hevelius in Danzig (1661), and Tobias Lowitz in St Petersburg (c. 1794).\nChinese observers had recorded these for centuries, the first reference being a section of the \"Official History of the Chin Dynasty\" (\"Chin Shu\") in 637, on the \"Ten Haloes\", giving technical terms for 26 solar halo phenomena.\n\nA light pillar, or sun pillar, appears as a vertical pillar or column of light rising from the sun near sunset or sunrise, though it can appear below the sun, particularly if the observer is at a high elevation or altitude. Hexagonal plate- and column-shaped ice crystals cause the phenomenon. Plate crystals generally cause pillars only when the sun is within 6 degrees of the horizon; column crystals can cause a pillar when the sun is as high as 20 degrees above the horizon. The crystals tend to orient themselves near-horizontally as they fall or float through the air, and the width and visibility of a sun pillar depend on crystal alignment.\n\nLight pillars can also form around the moon, and around street lights or other bright lights. Pillars forming from ground-based light sources may appear much taller than those associated with the sun or moon. Since the observer is closer to the light source, crystal orientation matters less in the formation of these pillars.\n\nAmong the best-known halos is the 22° halo, often just called \"halo\", which appears as a large ring around the Sun or Moon with a radius of about 22° (roughly the width of an outstretched hand at arm's length). The ice crystals that cause the 22° halo are oriented semi-randomly in the atmosphere, in contrast to the horizontal orientation required for some other halos such as sun dogs and light pillars. As a result of the optical properties of the ice crystals involved, no light is reflected towards the inside of the ring, leaving the sky noticeably darker than the sky around it, and giving it the impression of a \"hole in the sky\". The 22° halo is not to be confused with the corona, which is a different optical phenomenon caused by water droplets rather than ice crystals, and which has the appearance of a multicolored disk rather than a ring.\n\nOther haloes can form at 46° to the sun, or at the horizon, or around the zenith, and can appear as full haloes or incomplete arcs.\n\nA \"Bottlinger's ring\" is a rare type of halo that is elliptical instead of circular. It has a small diameter, which makes it very difficult to see in the Sun's glare and more likely to be spotted around the dimmer Subsun, often seen from mountain tops or airplanes. Bottlinger's rings are not well understood yet. It is suggested that they are formed by very flat pyramidal ice crystals with faces at uncommonly low angles, suspended horizontally in the atmosphere. These precise and physically problematic requirements would explain why the halo is very rare.\n\nIn the Anglo-Cornish dialect of English, a halo round the sun or the moon is called a \"cock's eye\" and is a token of bad weather. The term is related to the Breton word \"kog-heol\" (sun cock) which has the same meaning. In Nepal, the halo round the sun is called \"Indrasabha\" with a connotation of the assembly court of Lord Indra – the Hindu god of lightning, thunder and rain.\n\nThe natural phenomena may be reproduced artificially by several means. Firstly, by computer simulations, or secondly by experimental means. Regarding the latter, one may either take a single crystal and rotate it around the appropriate axis/axes, or take a chemical approach. A still further and more indirect experimental approach is to find analogous refraction geometries.\n\n This approach employs the fact that in some cases the average geometry of refraction through an ice crystal may be imitated / mimicked via the refraction through another geometrical object. In this way, the Circumzenithal arc, the Circumhorizontal arc and the suncave Parry arcs may be recreated by refraction through rotationally symmetric (i.e. non-prismatic) static bodies. A particularly simple table-top experiment reproduces artificially the colorful circumzenithal and circumhorizontal arcs using a water glass only. The refraction through the cylinder of water turns out to be (almost) identical to the rotationally averaged refraction through an upright hexagonal ice crystal / plate-oriented crystals, thereby creating vividly colored circumzenithal and the circumhorizontal arcs. In fact, the water glass experiment is often confused as representing a rainbow and has been around at least since 1920.\n\nFollowing Huygens' idea of the (false) mechanism of the 22° parhelia, one may also illuminate (from the side) a water-filled cylindrical glass with an inner central obstruction of half the glasses' diameter to achieve upon projection on a screen an appearance which closely resembles parhelia (cf. footnote [39] in Ref., or see here), i.e. an inner red edge transitioning into a white band at larger angles on both sides of the direct transmission direction. However, while the visual match is close, this particular experiment does not involve a fake caustic mechanism and is thus no real analogue.\n\nThe earliest chemical recipes to generate artificial halos has been put forward by Brewster and studied further by A. Cornu in 1889.\n\nThe earliest experimental studies on halo phenomena have been attributed to Auguste Bravais in 1847. Bravais used an equilateral glass prism which he spun around its vertical axis. When illuminated by parallel white light, this produced an artificial Parhelic circle and many of the embedded parhelia. Similarly, A. Wegener used hexagonal rotating crystals to produce artificial subparhelia. In a more recent version of this experiment, many more embedded parhelia have been found using commercially available hexagonal BK7 glass crystals. Simple experiments like these can be used for educational purposes and demonstration experiments. Unfortunately, using glass crystals one cannot reproduce the circumzenithal arc or the circumhorizontal arc due to total internal reflections preventing the required ray-paths when formula_1.\n\nEven earlier than Bravais, the Italian scientist F. Venturi experimented with pointed water-filled prisms to demonstrate the circumzenithal arc. However, this explanation was replaced later by the CZA's correct explanation by Bravais.\n\nIn order to produce artificial halos such as the tangent arcs or the circumscribed halo one should rotate a single columnar hexagonal crystal about 2 axes. Similarly, the Lowitz arcs can be created by rotating a single plate crystal about two axes. This can be done by engineered halo machines. The first such machine was constructed in 2003; several more followed. Putting such machines inside spherical projection screens, and by the principle of the so-called sky transform, the analogy is nearly perfect. A realization using micro-versions of the aforementioned machines produces authentic distortion-free projections of such complex artificial halos. Finally, superposition of several images and projections produced by such halo machines may be combined to create a single image. The resulting superposition image is then a representation of complex natural halo displays containing many different orientation sets of ice prisms.\n\nThe experimental reproduction of circular halos is the most difficult using a single crystal only, while it is the simplest and typically achieved one using chemical recipes. Using a single crystal, one needs to realize all possible 3D orientations of the crystal. This has recently been achieved by two approaches. The first one using pneumatics and a sophisticated rigging, and a second one using an Arduino-based random walk machine which stochastically reorients a crystal embedded in a transparent thin-walled sphere.\n\n\n\n"}
{"id": "241133", "url": "https://en.wikipedia.org/wiki?curid=241133", "title": "Hill of Tara", "text": "Hill of Tara\n\nThe Hill of Tara (, \"Teamhair\" or \"Teamhair na Rí\"), located near the River Boyne, is an archaeological complex that runs between Navan and Dunshaughlin in County Meath, Ireland. It contains a number of ancient monuments and, according to tradition, was the seat of the High King of Ireland.\n\nAt the summit of the hill, to the north of the ridge, is an oval Iron Age hilltop enclosure, measuring north-south by east-west and enclosed by an internal ditch and external bank, known as \"Ráith na Ríogh\" (the Fort of the Kings, also known as the Royal Enclosure). The most prominent earthworks within are the two linked enclosures, a bivallate (double-ditched) ring fort and a bivallate ring barrow known as \"Teach Chormaic\" (Cormac's House) and the \"Forradh\" or Royal Seat. In the middle of the \"Forradh\" is a standing stone, which is believed to be the \"Lia Fáil\" (Stone of Destiny) at which the High Kings were crowned. According to legend, the stone would scream if a series of challenges were met by the would-be king. At his touch the stone would let out a screech that could be heard all over Ireland. To the north of the ring-forts is a small Neolithic passage tomb known as \"Dumha na nGiall\" (the Mound of the Hostages), which was constructed around 3,400 (cal.) BC. It is the oldest part of the site.\n\nTo the north, just outside the bounds of the \"Ráith na Rí\", is a ringfort with three banks known as \"Ráith na Seanadh\" (the Rath of the Synods). Excavations of this monument have produced Roman artefacts dating from the 1st–3rd centuries.\n\nFarther north is a long, narrow rectangular feature known as the Banqueting Hall (\"Teach Miodhchuarta\"), although it is more likely to have been a ceremonial avenue or cursus monument approaching the site, and three circular earthworks known as the Sloping Trenches and Gráinne's Fort. All three are large ring barrows which may have been built too close to the steep declivity and subsequently slipped.\nTo the south of the Royal Enclosure lies a ring-fort known as \"Ráith Laoghaire\" (Laoghaire's Fort), where the eponymous king is said to have been buried in an upright position. Half a mile south of the Hill of Tara is another hill fort known as Rath Meave, the fort of either the legendary queen Medb, who is more usually associated with Connacht, or the less well known legendary figure of Medb Lethderg, who is associated with Tara.\n\nA church, called Saint Patrick's, is on the eastern side of the hilltop. The \"Rath of the Synods\" has been partly destroyed by its churchyard. The modern church was built in 1822–23 on the site of an earlier one. The earliest evidence of a church at Tara is a charter dating from the 1190s. In 1212, this church was \"among the possessions confirmed to the Knights Hospitallers of Saint John of Kilmainham by Pope Innocent III\". A 1791 illustration shows the church building internally divided into a nave and chancel, with a bell-tower over the western end. A stump of wall marks the site of the old church today, but some of its stonework was re-used in the current church. The building is now used as a visitor centre.\n\nThe Hill of Tara is documented in the 11th-century text The Book of Invasions as the seat of the high-kings of Ireland from the times of the mythological Fir Bolg and Tuatha Dé Danann to the text's composition. However, there is no evidence that the institution of High-kingship of Ireland conferred authority over the whole island on its possessor.\n\nLiathdroim was an ancient name of Tara. The Hill of Tara has been in use by people from the Neolithic era, although it is not known whether Tara was continuously used as a sacred and/or a political centre from the Neolithic period to the 12th century.\n\nThe central part of the site could not have housed a large permanent retinue, implying that it was instead used for occasional meetings. There were no large defensive structures. Earliest extant written records show that high kings were inaugurated there, and the \"Senchas Már\" legal text (written some time after 600AD) specified that the king must drink ale and symbolically marry the goddess Maeve (Medb) in order to qualify for high kingship. The last ever High King to observe the pagan inauguration ritual of marrying Medb, the goddess of the land was Diarmait mac Cerbaill, he’s also seen as the first High King of Ireland in the Christian era.\n\nPrevious scholarly disputes over Tara's initial importance increased when 20th-century archaeologists identified pre-Iron Age monuments and human-built habitable forms from the Neolithic period (roughly 5,000 years ago). One of these forms, the Mound of the Hostages, has a short passage aligned with sunrise on the solar cross-quarter days coinciding with ancient annual festivals celebrated on the midpoints between vernal and autumnal equinox (\"Imbolc\" honoring preparations for planting time or 'pre-spring' on about 4 February) and summer and winter solstice (\"Samhain\" honoring harvest time or 'first of winter' on about 8 November). The mound's passage is shorter than the long entryways of monuments like Newgrange, which makes it less precise in providing alignments with the Sun. Martin Brennan, in \"The Stones of Time\", states that the daily changes in the position of a 13-foot (4-m) long sunbeam are more than adequate to determine specific dates.\n\nWithin 12 miles of the Hill of Tara is another significant site known as Hill of Ward or Tlachtga, named after a powerful druidess. The site dates from 200 AD and is reputed to be the place of the first Celtic Pagan Fire Festival that took place on the eve of Samhain, 31 October. It was a pagan ritual of burning cattle bones, referred to then as bonefires or 'tine cnámh'. Samhain was later incorporated or replaced by All Saints' Day in the Christian calendar.\n\nA theory that may predate the Hill of Tara's splendor before Milesian times is the legendary story naming the Hill of Tara as the capital of the Tuatha Dé Danann, the pre-Milesians dwellers of Ireland. When the Milesians established a seat in the hill, the hill became the place from which the kings of Mide ruled Ireland. There is much debate among historians as to how far the King's influence spread; it may have been as little as the middle of Ireland, or may have been all the northern half. The high kingship of the whole island was only established to an effective degree by Máel Sechnaill mac Máele Ruanaid (Malachy I). Irish pseudohistorians of the Middle Ages made it stretch back into prehistoric times. Atop the hill stands a stone pillar that was the Irish Lia Fáil (Stone of Destiny) on which the High Kings of Ireland were crowned; legends suggest that the stone was required to roar three times if the chosen one was a true king (compare with the Scottish Lia Fail). Both the Hill of Tara as a hill and as a capital seems to have political and religious influence, which diminished since St. Patrick's time.\n\nDuring the rebellion of 1798, United Irishmen formed a camp on the hill but were attacked and defeated by British troops on 26 May 1798 and the Lia Fáil was moved to mark the graves of the 400 rebels who died on the hill that day. In 1843, the Irish Member of Parliament Daniel O'Connell hosted a peaceful political demonstration on Hill of Tara in favour of repeal of the Act of Union which drew over 750,000 people, which indicates the enduring importance of the Hill of Tara.\n\nDuring the turn of the 20th century, the Hill of Tara was vandalized by British Israelists who thought that the Irish were part of the Lost Tribes of Israel and that the hill contained the Ark of the Covenant. A contingent of British Israelites, led by a retired Anglo-Indian judge Edward Wheeler Bird, set about excavating the site having previously paid off the local land owner Gustavus Villiers Briscoe. Numerious Irish cultural nationalists organised a mass protest over the destruction of the National heritage site, including people such as Douglas Hyde, Arthur Griffith, Maud Gonne, George Moore and William Butler Yeats. Hyde tried to interrupt the dig but was ordered back off site by a man wielding a rifle. Maud Gonne made a more flamboyant protest by reigniting an old bonfire that Briscoe had previously lit to celebrate the coronation of Edward VII. She began to sing Thomas Davis's song A Nation Once Again by the fire, much to the consternation of the landlord and the police.\n\nBritish Prime Minister John Russell inherited the Tara estate during the 19th Century. The southern part of the hill was subsequently bought by the Irish state in 1952, and then later in 1972 the northern section was bought by the Irish government.\n\nAccording to legend five ancient Slighe or Roads meet at Tara, they connected the Royal City with the four Provinces of Ireland, at that time Tara was the capital of Ireland. They all intersected onto a triangle at Lismullen, just north of the hill of Tara. Some of the country’s present day road’s and City Street’s in part still utilize the same routes of these ancient slighe. The earliest reference in Irish literature to the five roads of Tara was in the Irish Tale Bruden da Derga.The now extinct Irish hobby, a distant descendant of the Connamara Pony were commonly used on these ancient slighe to draw war chariots and carts.\n\nThe name and routes of the five roads:\n\nSlighe Assail, went west towards Lough Owel in Co. Westmeath, then to Cruachain.\n\nSlige Midluachra, went to Slane, through Moyry Pass north of Dundalk, through Armagh to Eamhain Mhacha, finally ending at Dunseverick in Co. Antrim.\n\nSlighe Cualann, Ran South East through North and South Dublin, crossing the hurdle bridge over the liffey, then onto the old district of Cualann.\n\nSlighe Dala, ran towards and through Ossory in Co. Kilkenny\n\nSlighe Mhor('Great Highway'), joined the Esker Riada. It then more or less followed the Esker Riada to Co. Galway.\n\nThe M3 motorway which opened in June 2010, passes through the Tara-Skryne Valley – as did the existing N3 road. Protesters argue that since the Tara Discovery Programme started in 1992, there is an appreciation that the Hill of Tara is just the central complex of a wider landscape. The distance between the motorway and the exact site of the Hill is – it intersects the old N3 at the Blundelstown interchange between the Hill of Tara and the Hill of Skryne. The presence of this interchange situated in the valley has led to allegations that further development of an energy generator is planned near Tara. An alternative route approximately west of the Hill of Tara was claimed to be a straighter, cheaper and less destructive alternative, but no substantiation of this claim exists.\nOn Sunday 23 September 2007 over 1500 people met on the Hill of Tara to take part in a human sculpture representing a harp and spelling out the words \"SAVE TARA VALLEY\" as a call for the rerouting of the M3 motorway away from Tara valley. Actors Stuart Townsend and Jonathan Rhys Meyers attended this event.\n\nThe Hill of Tara was included in the World Monuments Fund's 2008 Watch List of the 100 Most Endangered Sites in the world. It was included, in 2009, in the 15 must-see endangered cultural treasures in the world by the Smithsonian Institution.\n\nThere was a letter writing campaign being undertaken to preserve the Hill of Tara.\n\n\n\n\n"}
{"id": "612100", "url": "https://en.wikipedia.org/wiki?curid=612100", "title": "Ideosphere", "text": "Ideosphere\n\nThe ideosphere, much like the noosphere, is the realm of memetic evolution, just like the biosphere is the realm of biological evolution. It is the \"place\" where thoughts, theories and ideas are thought to be created, evaluated and evolved. The health of an ideosphere can be measured by its memetic diversity.\n\nThe ideosphere is not considered to be a physical place by most people. It is instead \"inside the minds\" of all the humans in the world. It is also, sometimes, believed that the Internet, books and other media could be considered to be part of the ideosphere. Alas, as such media are not aware, it cannot process the thoughts it contains.\n\nAaron Lynch claims to have co-invented this word with Douglas Hofstadter in the mid-1980s.\n\nAccording to philosopher Yasuhiko Kimura, the ideosphere is in the form of a \"concentric ideosphere\" where ideas are generated by a few people with others merely perceiving and accepting these ideas from these \"external authorities.\" He advocates an \"omnicentric ideosphere\" where all individuals create new ideas and interact as self-authorities. \n\n\n"}
{"id": "63794", "url": "https://en.wikipedia.org/wiki?curid=63794", "title": "Impact event", "text": "Impact event\n\nAn impact event is a collision between astronomical objects causing measurable effects. Impact events have physical consequences and have been found to regularly occur in planetary systems, though the most frequent involve asteroids, comets or meteoroids and have minimal effect. When large objects impact terrestrial planets such as the Earth, there can be significant physical and biospheric consequences, though atmospheres mitigate many surface impacts through atmospheric entry. Impact craters and structures are dominant landforms on many of the Solar System's solid objects and present the strongest empirical evidence for their frequency and scale.\n\nImpact events appear to have played a significant role in the evolution of the Solar System since its formation. Major impact events have significantly shaped Earth's history, have been implicated in the formation of the Earth–Moon system, the evolutionary history of life, the origin of water on Earth and several mass extinctions. Notable impact events include the Chicxulub impact, 66 million years ago, believed to be the cause of the Cretaceous–Paleogene extinction event.\n\nThroughout recorded history, hundreds of Earth impacts (and exploding bolides) have been reported, with some occurrences causing deaths, injuries, property damage, or other significant localised consequences. One of the best-known recorded events in modern times was the Tunguska event, which occurred in Siberia, Russia, in 1908. The 2013 Chelyabinsk meteor event is the only known such incident in modern times to result in a large number of injuries, excluding the 1490 Ch'ing-yang event in China. The Chelyabinsk meteor is the largest recorded object to have encountered the Earth since the Tunguska event. The asteroid impact that caused Mistastin crater generated temperatures exceeding 2,370 °C, the highest known to have occurred on the surface of the Earth.\n\nThe Comet Shoemaker–Levy 9 impact provided the first direct observation of an extraterrestrial collision of Solar System objects, when the comet broke apart and collided with Jupiter in July 1994. An extrasolar impact was observed in 2013, when a massive terrestrial planet impact was detected around the star ID8 in the star cluster NGC 2547 by NASA's Spitzer space telescope and confirmed by ground observations. Impact events have been a plot and background element in science fiction.\n\nIn April 2018, the B612 Foundation reported \"It's a 100 per cent certain we'll be hit [by a devastating asteroid], but we're not 100 per cent sure when.\" Also in 2018, physicist Stephen Hawking, in his final book \"Brief Answers to the Big Questions\", considered an asteroid collision to be the biggest threat to the planet. In June 2018, the US National Science and Technology Council warned that America is unprepared for an asteroid impact event, and has developed and released the \"National Near-Earth Object Preparedness Strategy Action Plan\" to better prepare. According to expert testimony in the United States Congress in 2013, NASA would require at least five years of preparation before a mission to intercept an asteroid could be launched.\n\nMajor impact events have significantly shaped Earth's history, having been implicated in the formation of the Earth–Moon system, the evolutionary history of life, the origin of water on Earth, and several mass extinctions. Impact structures are the result of impact events on solid objects and, as the dominant landforms on many of the System's solid objects, present the most solid evidence of prehistoric events. Notable impact events include the Late Heavy Bombardment, which occurred early in history of the Earth–Moon system, and the Chicxulub impact, 66 million years ago, believed to be the cause of the Cretaceous–Paleogene extinction event.\n\nSmall objects frequently collide with Earth. There is an inverse relationship between the size of the object and the frequency of such events. The lunar cratering record shows that the frequency of impacts decreases as approximately the cube of the resulting crater's diameter, which is on average proportional to the diameter of the impactor. Asteroids with a diameter strike Earth every 500,000 years on average. Large collisions – with objects – happen approximately once every twenty million years. The last known impact of an object of or more in diameter was at the Cretaceous–Paleogene extinction event 66 million years ago.\n\nThe energy released by an impactor depends on diameter, density, velocity, and angle. The diameter of most near-Earth asteroids that have not been studied by radar or infrared can generally only be estimated within about a factor of two based on the asteroid brightness. The density is generally assumed because the diameter and mass are also generally estimates. Due to Earth's escape velocity, the minimum impact velocity is 11 km/s with asteroid impacts averaging around 17 km/s on the Earth. The most probable impact angle is 45 degrees. \n\nImpact conditions such as asteroid size and speed, but also density and impact angle determine the kinetic energy released in an impact event. The more energy is released, the more damage is likely to occur on the ground due to the environmental effects triggered by the impact. Such effects can be shock waves, heat radiation, the formation of craters with associated earthquakes, and tsunamis if water bodies are hit. Human populations are vulnerable to these effects if they live within the affected zone.\n\nStony asteroids with a diameter of enter Earth's atmosphere approximately once per year. Asteroids with a diameter of 7 meters enter the atmosphere about every 5 years with as much kinetic energy as the atomic bomb dropped on Hiroshima (approximately 16 kilotons of TNT), but the air burst is reduced to just 5 kilotons. These ordinarily explode in the upper atmosphere and most or all of the solids are vaporized. However, asteroids with a diameter of , and which strike Earth approximately twice every century, produce more powerful airbursts. The 2013 Chelyabinsk meteor was estimated to be about 20 m in diameter with an airburst of around 500 kilotons, an explosion 30 times the one over Hiroshima. Much larger objects may impact the solid earth and create a crater.\n\nObjects with a diameter less than are called meteoroids and seldom make it to the ground to become meteorites. An estimated 500 meteorites reach the surface each year, but only 5 or 6 of these typically create a weather radar signature with a strewn field large enough to be recovered and be made known to scientists.\n\nThe late Eugene Shoemaker of the U.S. Geological Survey estimated the rate of Earth impacts, concluding that an event about the size of the nuclear weapon that destroyed Hiroshima occurs about once a year. Such events would seem to be spectacularly obvious, but they generally go unnoticed for a number of reasons: the majority of the Earth's surface is covered by water; a good portion of the land surface is uninhabited; and the explosions generally occur at relatively high altitude, resulting in a huge flash and thunderclap but no real damage.\n\nAlthough no human is known to have been killed directly by an impact, over 1000 people were injured by the Chelyabinsk meteor airburst event over Russia in 2013. In 2005 it was estimated that the chance of a single person born today dying due to an impact is around 1 in 200,000. The four-meter-sized asteroids , , 2018 LA, and suspected artificial satellite WT1190F are the only known objects to be detected before impacting the Earth.\n\nImpacts have had, during the history of the Earth, a significant geological and climatic influence.\n\nThe Moon's existence is widely attributed to a huge impact early in Earth's history. Impact events earlier in the history of Earth have been credited with creative as well as destructive events; it has been proposed that impacting comets delivered the Earth's water, and some have suggested that the origins of life may have been influenced by impacting objects by bringing organic chemicals or lifeforms to the Earth's surface, a theory known as exogenesis.\n\nThese modified views of Earth's history did not emerge until relatively recently, chiefly due to a lack of direct observations and the difficulty in recognizing the signs of an Earth impact because of erosion and weathering. Large-scale terrestrial impacts of the sort that produced the Barringer Crater, locally known as Meteor Crater, northeast of Flagstaff, Arizona, are rare. Instead, it was widely thought that cratering was the result of volcanism: the Barringer Crater, for example, was ascribed to a prehistoric volcanic explosion (not an unreasonable hypothesis, given that the volcanic San Francisco Peaks stand only to the west). Similarly, the craters on the surface of the Moon were ascribed to volcanism.\n\nIt was not until 1903–1905 that the Barringer Crater was correctly identified as an impact crater, and it was not until as recently as 1963 that research by Eugene Merle Shoemaker conclusively proved this hypothesis. The findings of late 20th-century space exploration and the work of scientists such as Shoemaker demonstrated that impact cratering was by far the most widespread geological process at work on the Solar System's solid bodies. Every surveyed solid body in the Solar System was found to be cratered, and there was no reason to believe that the Earth had somehow escaped bombardment from space. In the last few decades of the 20th century, a large number of highly modified impact craters began to be identified. The first direct observation of a major impact event occurred in 1994: the collision of the comet Shoemaker-Levy 9 with Jupiter.\n\nBased on crater formation rates determined from the Earth's closest celestial partner, the Moon, astrogeologists have determined that during the last 600 million years, the Earth has been struck by 60 objects of a diameter of or more. The smallest of these impactors would leave a crater almost across. Only three confirmed craters from that time period with that size or greater have been found: Chicxulub, Popigai, and Manicouagan, and all three have been suspected of being linked to extinction events though only Chicxulub, the largest of the three, has been consistently considered.\n\nBesides direct effect of asteroid impacts on a planet's surface topography, global climate and life, recent studies have shown that several consecutive impacts might have an effect on the dynamo mechanism at a planet's core responsible for maintaining the magnetic field of the planet, and might eventually shut down the planet's magnetic field.\n\nWhile numerous impact craters have been confirmed on land or in the shallow seas over continental shelves, no impact craters in the deep ocean have been widely accepted by the scientific community. Impacts of projectiles as large as one km in diameter are generally thought to explode before reaching the sea floor, but it is unknown what would happen if a much larger impactor struck the deep ocean. The lack of a crater, however, does not mean that an ocean impact would not have dangerous implications for humanity. Some scholars have argued that an impact event in an ocean or sea may create a megatsunami (a giant wave), which can cause destruction both at sea and on land along the coast, but this is disputed. An example of an ocean impact is the large but apparently craterless Eltanin impact into the Pacific Ocean in 2.5 Ma and is thought to involve an object about across.\n\nAn impact event may cause a mantle plume (volcanism) at the antipodal point of the impact.\n\nThe effect of impact events on the biosphere has been the subject of scientific debate. Several theories of impact-related mass extinction have been developed. In the past 500 million years there have been five generally accepted major mass extinctions that on average extinguished half of all species. One of the largest mass extinctions to have affected life on Earth was the Permian-Triassic, which ended the Permian period 250 million years ago and killed off 90 percent of all species; life on Earth took 30 million years to recover. The cause of the Permian-Triassic extinction is still a matter of debate; the age and origin of proposed impact craters, i.e. the Bedout High structure, hypothesized to be associated with it are still controversial. The last such mass extinction led to the demise of the dinosaurs and coincided with a large meteorite impact; this is the Cretaceous–Paleogene extinction event (also known as the K–T or K–Pg extinction event), which occurred 66 million years ago. There is no definitive evidence of impacts leading to the three other major mass extinctions.\n\nIn 1980, physicist Luis Alvarez; his son, geologist Walter Alvarez; and nuclear chemists Frank Asaro and Helen V. Michael from the University of California, Berkeley discovered unusually high concentrations of iridium in a specific layer of rock strata in the Earth's crust. Iridium is an element that is rare on Earth but relatively abundant in many meteorites. From the amount and distribution of iridium present in the 65-million-year-old \"iridium layer\", the Alvarez team later estimated that an asteroid of must have collided with the earth. This iridium layer at the Cretaceous–Paleogene boundary has been found worldwide at 100 different sites. Multidirectionally shocked quartz (coesite), which is normally associated with large impact events or atomic bomb explosions, has also been found in the same layer at more than 30 sites. Soot and ash at levels tens of thousands times normal levels were found with the above.\n\nAnomalies in chromium isotopic ratios found within the K-T boundary layer strongly support the impact theory. Chromium isotopic ratios are homogeneous within the earth, and therefore these isotopic anomalies exclude a volcanic origin, which has also been proposed as a cause for the iridium enrichment. Further, the chromium isotopic ratios measured in the K-T boundary are similar to the chromium isotopic ratios found in carbonaceous chondrites. Thus a probable candidate for the impactor is a carbonaceous asteroid, but a comet is also possible because comets are assumed to consist of material similar to carbonaceous chondrites.\n\nProbably the most convincing evidence for a worldwide catastrophe was the discovery of the crater which has since been named Chicxulub Crater. This crater is centered on the Yucatán Peninsula of Mexico and was discovered by Tony Camargo and Glen Pentfield while working as geophysicists for the Mexican oil company PEMEX. What they reported as a circular feature later turned out to be a crater estimated to be in diameter. This convinced the vast majority of scientists that this extinction resulted from a point event that is most probably an extraterrestrial impact and not from increased volcanism and climate change (which would spread its main effect over a much longer time period).\n\nAlthough there is now general agreement that there was a huge impact at the end of the Cretaceous that led to the iridium enrichment of the K-T boundary layer, remnants have been found of other, smaller impacts, some nearing half the size of the Chicxulub crater, which did not result in any mass extinctions, and there is no clear linkage between an impact and any other incident of mass extinction.\n\nPaleontologists David M. Raup and Jack Sepkoski have proposed that an excess of extinction events occurs roughly every 26 million years (though many are relatively minor). This led physicist Richard A. Muller to suggest that these extinctions could be due to a hypothetical companion star to the Sun called Nemesis periodically disrupting the orbits of comets in the Oort cloud, leading to a large increase in the number of comets reaching the inner Solar System where they might hit Earth. Physicist Adrian Melott and paleontologist Richard Bambach have more recently verified the Raup and Sepkoski finding, but argue that it is not consistent with the characteristics expected of a Nemesis-style periodicity.\n\nAn impact event is commonly seen as a scenario that would bring about the end of civilization. In 2000, \"Discover Magazine\" published a list of 20 possible sudden doomsday scenarios with an impact event listed as the most likely to occur.\n\nA joint Pew Research Center/\"Smithsonian\" survey from April 21–26, 2010 found that 31 percent of Americans believed that an asteroid will collide with Earth by 2050. A majority (61 percent) disagreed.\n\nIn the early history of the Earth (about four billion years ago), bolide impacts were almost certainly common since the Solar System contained far more discrete bodies than at present. Such impacts could have included strikes by asteroids hundreds of kilometers in diameter, with explosions so powerful that they vaporized all the Earth's oceans. It was not until this heavy bombardment slackened that life appears to have begun to evolve on Earth.\n\nThe leading theory of the Moon's origin is the giant impact theory, which postulates that Earth was once hit by a planetoid the size of Mars; such a theory is able to explain the size and composition of the Moon, something not done by other theories of lunar formation.\n\nEvidence of a massive impact in South Africa near a geological formation known as the Barberton Greenstone Belt was uncovered by scientists in April 2014. They estimated the impact occurred about 3.26 billion years ago and that the impactor was approximately wide. The crater from this event, if it still exists, has not yet been found.\n\nTwo 10-kilometre sized asteroids are now believed to have struck Australia between 360 and 300 million years ago at the Western Warburton and East Warburton Basins creating a 400-kilometre impact zone. According to evidence found in 2015 it is the largest ever recorded. A third, possible impact was also identified in 2015 to the north, on the upper Diamantina River, also believed to have been caused by an asteroid 10 km across about 300 million years ago, but further studies are needed to establish that this crustal anomaly was indeed the result of an impact event.\n\nArtifacts recovered with tektites from the 803,000-year-old Australasian strewnfield event in Asia link a \"Homo erectus\" population to a significant meteorite impact and its aftermath. Significant examples of Pleistocene impacts include the Lonar crater lake in India, approximately 52,000 years old (though a study published in 2010 gives a much greater age), which now has a flourishing semi-tropical jungle around it.\n\nThe Rio Cuarto craters in Argentina were produced approximately 10,000 years ago, at the beginning of the Holocene. If proved to be impact craters, they would be the first impact of the holocene.\nThe Campo del Cielo (\"Field of Heaven\") refers to an area bordering Argentina's Chaco Province where a group of iron meteorites were found, estimated as dating to 4,000–5,000 years ago. It first came to attention of Spanish authorities in 1576; in 2015, police arrested four alleged smugglers trying to steal more than a ton of protected meteorites. The Henbury craters in Australia (~5,000 years old) and Kaali craters in Estonia (~2,700 years old) were apparently produced by objects that broke up before impact.\n\nWhitecourt crater in Alberta, Canada is estimated to be between 1,080 and 1,130 years old. The crater is approximately 36 m (118 ft) in diameter and 9 m (30 ft) deep, is heavily forested and was discovered in 2007 when a metal detector revealed fragments of meteoric iron scattered around the area.\n\nA Chinese record states that 10,000 people were killed in the 1490 Ch'ing-yang event with the deaths caused by a hail of \"falling stones\"; some astronomers hypothesize that this may describe an actual meteorite fall, although they find the number of deaths implausible.\n\nKamil Crater, discovered from Google Earth image review in Egypt, in diameter and deep, is thought to have been formed less than 3,500 years ago in a then-unpopulated region of western Egypt. It was found February 19, 2009 by V. de Michelle on a Google Earth image of the East Uweinat Desert, Egypt.\n\nOne of the best-known recorded impacts in modern times was the Tunguska event, which occurred in Siberia, Russia, in 1908. This incident involved an explosion that was probably caused by the airburst of an asteroid or comet above the Earth's surface, felling an estimated 80 million trees over .\n\nIn February 1947, another large bolide impacted the Earth in the Sikhote-Alin Mountains, Primorye, Soviet Union. It was during daytime, so was witnessed by many people, which allowed V. G. Fesenkov, then chairman of the meteorite committee of the USSR Academy of Science, to estimate the meteoroid's orbit before it encountered the Earth. Sikhote-Alin is a massive fall with the overall size of the meteoroid estimated at approximately . A more recent estimate by Tsvetkov (and others) puts the mass at around . It was an iron meteorite belonging to the chemical group IIAB and with a coarse octahedrite structure. More than 70 tonnes (metric tons) of material survived the collision.\n\nA case of a human injured by a space rock occurred on November 30, 1954, in Sylacauga, Alabama. There a stone chondrite crashed through a roof and hit Ann Hodges in her living room after it bounced off her radio. She was badly bruised by the fragments. Several persons have since claimed to have been struck by \"meteorites\" but no verifiable meteorites have resulted.\n\nA small number of meteor falls have been observed with automated cameras and recovered following calculation of the impact point. The first of these was the Pribram meteorite, which fell in Czechoslovakia (now the Czech Republic) in 1959. In this case, two cameras used to photograph meteors captured images of the fireball. The images were used both to determine the location of the stones on the ground and, more significantly, to calculate for the first time an accurate orbit for a recovered meteorite.\n\nFollowing the Pribram fall, other nations established automated observing programs aimed at studying infalling meteorites. One of these was the Prairie Network, operated by the Smithsonian Astrophysical Observatory from 1963 to 1975 in the midwestern U.S. This program also observed a meteorite fall, the \"Lost City\" chondrite, allowing its recovery and a calculation of its orbit. Another program in Canada, the Meteorite Observation and Recovery Project, ran from 1971 to 1985. It too recovered a single meteorite, \"Innisfree\", in 1977. Finally, observations by the European Fireball Network, a descendant of the original Czech program that recovered Pribram, led to the discovery and orbit calculations for the Neuschwanstein meteorite in 2002.\n\nOn August 10, 1972, a meteor which became known as the 1972 Great Daylight Fireball was witnessed by many people as it moved north over the Rocky Mountains from the U.S. Southwest to Canada. It was filmed by a tourist at the Grand Teton National Park in Wyoming with an 8-millimeter color movie camera. The object was in the range of size from a car to a house and could have ended its life in a Hiroshima-sized blast, but there was never any explosion. Analysis of the trajectory indicated that it never came much lower than off the ground, and the conclusion was that it had grazed Earth's atmosphere for about 100 seconds, then skipped back out of the atmosphere to return to its orbit around the Sun.\n\nMany impact events occur without being observed by anyone on the ground. Between 1975 and 1992, American missile early warning satellites picked up 136 major explosions in the upper atmosphere. In the November 21, 2002, edition of the journal \"Nature\", Peter Brown of the University of Western Ontario reported on his study of U.S. early warning satellite records for the preceding eight years. He identified 300 flashes caused by meteors in that time period and estimated the rate of Tunguska-sized events as once in 400 years. Eugene Shoemaker estimated that an event of such magnitude occurs about once every 300 years, though more recent analyses have suggested he exaggerated by an order of magnitude.\n\nIn the dark morning hours of January 18, 2000, a fireball exploded over the city of Whitehorse, Yukon Territory at an altitude of about , lighting up the night like day. The meteor that produced the fireball was estimated to be about in diameter, with a weight of 180 tonnes. This blast was also featured on the Science Channel series \"Killer Asteroids\", with several witness reports from residents in Atlin, British Columbia.\n\nOn 7 June 2006, a meteor was observed striking Reisadalen in Nordreisa municipality in Troms County, Norway. Although initial witness reports stated that the resultant fireball was equivalent to the Hiroshima nuclear explosion, scientific analysis places the force of the blast at anywhere from 100–500 tonnes TNT equivalent, around three percent of Hiroshima's yield.\n\nOn 15 September 2007, a chondritic meteor crashed near the village of Carancas in southeastern Peru near Lake Titicaca, leaving a water-filled hole and spewing gases across the surrounding area. Many residents became ill, apparently from the noxious gases shortly after the impact.\n\nOn 7 October 2008, an approximately 4 meter asteroid labeled was tracked for 20 hours as it approached Earth and as it fell through the atmosphere and impacted in Sudan. This was the first time an object was detected before it reached the atmosphere and hundreds of pieces of the meteorite were recovered from the Nubian Desert.\n\nOn 15 February 2013, an asteroid entered Earth's atmosphere over Russia as a fireball and exploded above the city of Chelyabinsk during its passage through the Ural Mountains region at 09:13 YEKT (03:13 UTC). The object's air burst occurred at an altitude between above the ground, and about 1,500 people were injured, mainly by broken window glass shattered by the shock wave. Two were reported in serious condition; however, there were no fatalities. Initially some 3,000 buildings in six cities across the region were reported damaged due to the explosion's shock wave, a figure which rose to over 7,200 in the following weeks. The Chelyabinsk meteor was estimated to have caused over $30 million in damage. It is the largest recorded object to have encountered the Earth since the 1908 Tunguska event. The meteor is estimated to have an initial diameter of 17–20 metres and a mass of roughly 10,000 tonnes. On 16 October 2013, a team from Ural Federal University led by Victor Grokhovsky recovered a large fragment of the meteor from the bottom of Russia's Lake Chebarkul, about 80 km west of the city.\n\nOn 1 January 2014, a ~3 meter (10 foot) asteroid, 2014 AA was discovered by the Mount Lemmon Survey and observed over the next hour, and was soon found to be on a collision course with Earth. The exact location was uncertain, constrained to a line between Panama, the central Atlantic Ocean, The Gambia, and Ethiopia. Around roughly the time expected (2 January 3:06 UTC) an infrasound burst was detected near the center of the impact range, in the middle of the Atlantic Ocean. This marks the second time a natural object was identified prior to impacting earth after 2008 TC3.\n\nNearly two years later, on October 3, WT1190F was detected orbiting Earth on a highly eccentric orbit, taking it from well within the Geocentric satellite ring to nearly twice the orbit of the Moon. It was estimated to be perturbed by the Moon onto a collision course with Earth on November 13. With over a month of observations, as well as precovery observations found dating back to 2009, it was found to be far less dense than a natural asteroid should be, suggesting that it was most likely an unidentified artificial satellite. As predicted, it fell over Sri Lanka at 6:18 UTC (11:48 local time). The sky in the region was very overcast, so only an airborne observation team was able to successfully observe it falling above the clouds. It is now thought to be a remnant of the Lunar Prospector mission in 1998, and is the third time any previously unknown object – natural or artificial – was identified prior to impact.\n\nOn 22 January 2018, an object, A106fgF, was discovered by the Asteroid Terrestrial-impact Last Alert System (ATLAS) and identified as possibly impacting Earth later that day. While a very close approach to Earth was not ruled out entirely, the odds of impact were fairly likely. As it was very dim, and only identified hours before its approach, no more than the initial 4 observations covering a 39-minute period were made of the object. It is unknown if it impacted Earth or not, but no fireball was detected in either infrared or infrasound, so if it did, it would have been very small, and likely near the eastern end of its potential impact area – in the western Pacific Ocean.\n\nOn 2 June 2018, the Mount Lemmon Survey detected (ZLAF9B2), a small 2-5 meter asteroid which further observations soon found had an 85% chance of impacting Earth. Soon after the impact, a fireball report from Botswana arrived to the American Meteor Society. Further observations with ATLAS extended the observation arc from 1 hour to 4 hours and confirmed that the asteroid orbit indeed impacted Earth in southern Africa, fully closing the loop with the fireball report and making this the third natural object confirmed to impact Earth, and the second on land after .\n\nIn the late 20th and early 21st century scientists put in place measures to detect Near earth objects, and predict the dates and times of asteroids impacting earth, along with the locations at which they will impact. The International Astronomical Union Minor Planet Center (MPC) is the global clearing house for information on asteroid orbits. NASA's Sentry System continually scans the MPC catalog of known asteroids, analyzing their orbits for any possible future impacts. Currently none are predicted (the single highest probability impact currently listed is ~7 m asteroid , which is due to pass earth in September 2095 with only a 5% predicted chance of impacting).\n\nSo far only three impact events have been successfully predicted in advance. Currently prediction is mainly based on cataloging asteroids years before they are due to impact. This works well for larger asteroids (> 1 km across) as they are easily seen from a long distance. Over 95% of them are already known, so their orbits can be measured and any future impacts predicted long before they are on their final approach to Earth. Smaller objects are usually too faint to observe very far in advance and so most can only be observed on final approach. Current mechanisms for detecting asteroids on final approach rely on wide-field ground based telescopes, such as the ATLAS system. However current telescopes are not powerful enough to detect the smaller asteroids that commonly impact Earth unless conditions are just right, which is why so few are detected.\n\nImpact craters provide evidence of past impacts on other planets in the Solar System, including possible interplanetary terrestrial impacts. Without carbon dating, other points of reference are used to estimate the timing of these impact events. Mars provides some significant evidence of possible interplanetary collisions. The North Polar Basin on Mars is speculated by some to be evidence for a planet-sized impact on the surface of Mars between 3.8 and 3.9 billion years ago, while Utopia Planitia is the largest confirmed impact and Hellas Planitia is the largest visible crater in the Solar System. The Moon provides similar evidence of massive impacts, with the South Pole–Aitken basin being the biggest. Mercury's Caloris Basin is another example of a crater formed by a massive impact event. Rheasilvia on Vesta is an example of a crater formed by an impact capable of, based on ratio of impact to size, severely deforming a planetary-mass object. Impact craters on the moons of Saturn such as Engelier and Gerin on Iapetus, Mamaldi on Rhea and Odysseus on Tethys and Herschel on Mimas form significant surface features. 2018 models developed to explain the unusual spin of Uranus support a long-held theory that this was caused by an oblique collision with massive object twice the size of Earth.\n\nOn July 1994, Comet Shoemaker–Levy 9 was a comet that broke apart and collided with Jupiter, providing the first direct observation of an extraterrestrial collision of Solar System objects. The event served as a \"wake-up call\", and astronomers responded by starting programs such as Lincoln Near-Earth Asteroid Research (LINEAR), Near-Earth Asteroid Tracking (NEAT), Lowell Observatory Near-Earth Object Search (LONEOS) and several others which have drastically increased the rate of asteroid discovery.\n\nThe 2009 Jupiter impact event happened on July 19 when a new black spot about the size of Earth was discovered in Jupiter's southern hemisphere by amateur astronomer Anthony Wesley. Thermal infrared analysis showed it was warm and spectroscopic methods detected ammonia. JPL scientists confirmed that there was another impact event on Jupiter, probably involving a small undiscovered comet or other icy body. The impactor is estimated to have been about 200–500 meters in diameter.\n\nA 2010 Jupiter impact event occurred on June 3 involving an object estimated at 8–13 meters was recorded and first reported by Anthony Wesley.\n\nOn Sept 10, 2012, amateur astronomer Dan Petersen visually detected a fireball on Jupiter that lasted 1 or 2 seconds using a 12″ LX200. It was estimated that the fireball was created by a meteoroid less than 10 meters in diameter.\n\nOn March 17, 2016, a Jupiter impact event occurred involving an unknown object, possibly a small comet or asteroid initially estimated at 30–90 meters (or a few hundred feet) across. The size estimate was later corrected to 7 and 19 meters. The event was first reported by Austrian amateur astronomer Gerrit Kernbauer, and later confirmed in footage from the telescope of amateur astronomer John McKeon.\n\nOn May 26, 2017, amateur astronomer Sauveur Pedranghelu observed another flash from Corsica (France). The event was announced the next day, and was quickly confirmed by two German observers, Thomas Riessler and André Fleckstein. The impactor had an estimated size of 4 to 10 meters.\n\nIn 1998, two comets were observed plunging toward the Sun in close succession. The first of these was on June 1 and the second the next day. A video of this, followed by a dramatic ejection of solar gas (unrelated to the impacts), can be found at the NASA website. Both of these comets evaporated before coming into contact with the surface of the Sun. According to a theory by NASA Jet Propulsion Laboratory scientist Zdeněk Sekanina, the latest impactor to actually make contact with the Sun was the \"supercomet\" Howard-Koomen-Michels on August 30, 1979. (See also sungrazer.)\n\nIn 2010, between January and May, Hubble's Wide Field Camera 3 took images of an unusual X shape originated in the aftermath of the collision between asteroid P/2010 A2 with a smaller asteroid.\n\nAround March 27, 2012, based on evidence, there were signs of an impact on Mars. Images from the Mars Reconnaissance Orbiter provide compelling evidence of the largest impact observed to date on Mars in the form of fresh craters, the largest measuring 48.5 by 43.5 meters. It is estimated to be caused by an impactor 3 to 5 meters long.\n\nOn March 19, 2013, an impact occurred on the Moon that was visible from Earth, when a boulder-sized 30 cm meteoroid slammed into the lunar surface at 90,000 km/h (56,000 mph) creating a 20-meter crater. NASA has actively monitored lunar impacts since 2005, tracking hundreds of candidate events.\n\nCollisions between galaxies, or galaxy mergers, have been observed directly by space telescopes such as Hubble and Spitzer. However, collisions in planetary systems including stellar collisions, while long speculated, have only recently begun to be observed directly.\n\nIn 2013, an impact between minor planets was detected around the star NGC 2547 ID 8 by Spitzer and confirmed by ground observations. Computer modelling suggests that the impact involved large asteroids or protoplanets similar to the events believed to have led to the formation of terrestrial planets like the Earth.\n\nNumerous science fiction stories and novels center around an impact event. One of the first and more popular is \"Off on a Comet\" () by Jules Verne, published in 1877, and H. G. Wells wrote about such an event in his 1897 short story \"The Star.\" In more modern times, possibly the best-selling was the novel \"Lucifer's Hammer\" by Larry Niven and Jerry Pournelle. Arthur C. Clarke's novel \"Rendezvous with Rama\" opens with a significant asteroid impact in northern Italy in the year 2077 which gives rise to the Spaceguard Project, which later discovers the Rama spacecraft. In 1992 a Congressional study in the U.S. led to NASA being directed to undertake the \"Spaceguard Survey\", with the novel being named as the inspiration for the name to search for Earth-impacting asteroids. This in turn inspired Clarke's 1993 novel \"The Hammer of God\".\n\nA variation on the traditional impact story was provided by Jack McDevitt's 1999 novel \"Moonfall\", in which a very large comet traveling at interstellar velocities collides with and partially destroys the Moon, fragments of which then collide with the Earth. The 1985 Niven and Pournelle novel \"Footfall\" features the examination of the effects of planetary warfare conducted by an alien species that culminates in the use of asteroids to bombard the planet, creating very large craters and the human species' near-extinction. Robert A. Heinlein used the concept of guided meteors in his novel \"The Moon is a Harsh Mistress\", in which Moon rebels use rock-filled shipping containers as a weapon against their Earth oppressors.\n\nSome science fiction has concerned itself not with the specifics of the impact event and/or its prevention or avoidance but its secondary effects on human society. Ben H. Winters' 2012 novel \"The Last Policeman\" is set six months prior to an asteroid collision, following a murder investigation that is complicated by the political and cultural responses to the impending event.\n\nSeveral disaster films center on actual or threatened impact events. Released during the turbulence of World War I, the Danish feature film \"The End of the World\" revolves around the near-miss of a comet which causes fire showers and social unrest in Europe. \"When Worlds Collide\" (1951), based on a 1933 novel by Philip Wylie, deals with two planets on a collision course with Earth—the smaller planet a \"near miss,\" causing extensive damage and destruction, followed by a direct hit from the larger planet.\n\n\"Meteor\" (1979) features small asteroid fragments and a large -wide asteroid heading for Earth after being wrenched out of orbit by a collision with a comet. Orbiting U.S. and Soviet nuclear weapons platforms are turned away from their respective earthbound targets and toward the incoming threat. The concept for this defense mechanism was taken from a 1968 MIT study called \"Project Icarus.\"\n\nIn 1998, two films were released in the United States on the subject of attempting to stop impact events: Paramount/DreamWorks' \"Deep Impact\", about a comet, and Touchstone Pictures' \"Armageddon\", about an asteroid. Both involved using Space Shuttle-derived craft to deliver nuclear weapons to destroy their targets. The 2008 American Broadcasting Company's miniseries \"Impact\" deals with a splinter of a brown dwarf hidden in a meteor shower which strikes the Moon and sends it on a collision course with Earth. The 2011 film \"Melancholia\" uses the motif of an impact event incorporated in the aesthetics of Romanticism.\n\n\n"}
{"id": "4426463", "url": "https://en.wikipedia.org/wiki?curid=4426463", "title": "Indian Century", "text": "Indian Century\n\nThe Indian Century (or India's Century) is the possibility that the 21st century will be dominated by India, similarly to how the 20th century is often called the American Century, and the 19th century as Pax Britannica (British Peace). The phrase is used particularly in the assertion that the economy of India could overtake the economy of the United States and economy of China as the largest national economy in the world, a position it held from 1 to 1500 A.D. and in 1700 A.D.\n\nIndia has created North–South Transport Corridor as an alternative to One Belt, One Road policy initiative of China (PRC), to link in with Iran, Russia, the Caucasus, and Central Asia. In 2017, India and Japan joined together to form Asia-Africa Growth Corridor, to better integrate the economies of South, Southeast, and East Asia with Oceania and Africa.\n\nAccording to the report named \"Indian Century: Defining India's Place in a Rapidly Changing Global Economy\" by IBM Institute for Business Value, India is predicted to be among the world’s highest-growth nations over the coming years.\n\nOne of the key factors includes its populous democracy. As per United Nations report, India will overtake China to become the world's most populous nation by 2022.\n\nEconomists and Researchers at Harvard University have projected India’s 7% projected annual growth rate through 2024 would continue to put it ahead of China, making India the fastest growing economy in the world. In 2017, Center for International Development at Harvard University, published a research study, projecting that India has emerged as the economic pole of global growth by surpassing China and is expected to maintain its lead over the coming decade.\n\nIndia is generally considered an emerging power due to its large and stable population, and its rapidly growing economic and military sectors.\n\n"}
{"id": "3592098", "url": "https://en.wikipedia.org/wiki?curid=3592098", "title": "Industrial melanism", "text": "Industrial melanism\n\nIndustrial melanism is an evolutionary effect prominent in several arthropods, where dark pigmentation (melanism) has evolved in an environment affected by industrial pollution, including sulphur dioxide gas and dark soot deposits. Sulphur dioxide kills lichens, leaving tree bark bare where in clean areas it is boldly patterned, while soot darkens bark and other surfaces. Darker pigmented individuals have a higher fitness in those areas as their camouflage matches the polluted background better; they are thus favoured by natural selection. This change, extensively studied by Bernard Kettlewell, is a popular teaching example in Darwinian evolution, providing evidence for natural selection. Kettlewell's results have been challenged by zoologists, creationists and the journalist Judith Hooper, but later researchers have upheld Kettlewell's findings.\nIndustrial melanism is widespread in the Lepidoptera (butterflies and moths), involving over 70 species such as \"Odontopera bidentata\" (scalloped hazel) and \"Lymantria monacha\" (dark arches), but the most studied is the evolution of the peppered moth, \"Biston betularia\". It is also seen in a beetle, \"Adalia bipunctata\" (two-spot ladybird), where camouflage is not involved as the insect has conspicuous warning coloration, and in the seasnake \"Emydocephalus annulatus\" where the melanism may help in excretion of trace elements through sloughing of the skin. The rapid decline of melanism that has accompanied the reduction of pollution, in effect a natural experiment, makes natural selection for camouflage \"the only credible explanation\".\n\nOther explanations for the observed correlation with industrial pollution have been proposed, including strengthening the immune system in a polluted environment, absorbing heat more rapidly when sunlight is reduced by air pollution, and the ability to excrete trace elements into melanic scales and feathers.\n\nIndustrial melanism was first noticed in 1900 by the geneticist William Bateson; he observed that the colour morphs were inherited, but did not suggest an explanation for the polymorphism.\n\nIn 1906, the geneticist Leonard Doncaster described the increase in frequency of the melanic forms of several moth species from about 1800 to 1850 in the heavily industrialised north-west region of England.\n\nIn 1924, the evolutionary biologist J. B. S. Haldane constructed a mathematical argument showing that the rapid growth in frequency of the \"carbonaria\" form of the peppered moth, \"Biston betularia\", implied selective pressure.\n\nFrom 1955 onwards, the geneticist Bernard Kettlewell conducted a series of experiments exploring the evolution of melanism in the peppered moth. He used a capture-mark-recapture technique to show that dark forms survived better than light ones.\n\nBy 1973, pollution in England had begun to decrease, and the dark \"carbonaria\" form had declined in frequency. This provided convincing evidence, gathered and analysed by Kettlewell and others such as the entomologist and geneticist Michael Majerus and the population geneticist Laurence M. Cook, that its rise and fall had been caused by natural selection in response to the changing pollution of the landscape.\n\nIndustrial melanism is known from over 70 species of moth that Kettlewell found in England, and many others from Europe and North America.\nAmong these, \"Apamea crenata\" (clouded border brindle moth) and \"Acronicta rumicis\" (knot grass moth) are always polymorphic, though the melanic forms are more common in cities and (like those of the peppered moth) are declining in frequency as those cities become less polluted.\n\nAmong other insects, industrial melanism has been observed in a beetle, \"Adalia bipunctata\", the two-spot ladybird.\n\nIn the vertebrates, industrial melanism is known from the turtle-headed seasnake \"Emydocephalus annulatus\", and may be present in urban feral pigeons.\n\nOriginally, peppered moths lived where light-colored lichens covered the trees. For camouflage from predators against that clean background, they had generally light coloration. During the Industrial Revolution in England, sulphur dioxide pollution in the atmosphere reduced the lichen cover, while soot blackened the bark of urban trees, making the light-colored moths more vulnerable to predation. This provided a selective advantage to the gene responsible for melanism, and the darker-colored moths increased in frequency. The melanic phenotype of \"Biston betularia\" has been calculated to give a fitness advantage as great as 30 per cent. By the end of the 19th century it almost completely replaced the original light-coloured type (var. \"typica\"), forming a peak of 98% of the population in 1895.\n\nMelanic \"B. betularia\" have been widely observed in North America. In 1959, 90% of \"B. betularia\" in Michigan and Pennsylvania were melanic. By 2001, melanism dropped to 6% of the population, following clean air legislation. The drop in melanism was correlated with an increase in species diversity of lichens, a decrease in the atmospheric pollutant sulphur dioxide, and an increase in the pale phenotype. The return of lichens is in turn directly correlated with the reduction in atmospheric sulphur dioxide.\n\nKettlewell's experiments were criticised by the zoologist Theodore David Sargent, who failed to reproduce Kettlewell's results between 1965 and 1969, and argued that Kettlewell had specially trained his birds to give the desired results. \nMichael Majerus however found that Kettlewell was basically correct in concluding that differential bird predation in a polluted environment was the primary cause of industrial melanism in the peppered moth. The story was in turn taken up in a 2002 book \"Of Moths and Men\", by the journalist Judith Hooper, asserting that Kettlewell's findings were fraudulent. The story was picked up by creationists who repeated the assertions of fraudulence. Zoologists including L. M. Cook, B. S. Grant, Majerus and David Rudge however all upheld Kettlewell's account, finding that each of Hooper's and the creationists' claims collapsed when the facts were examined. \n\nIt has been suggested that the demonstrated relationship between melanism and pollution can not be fully proven because the exact reason for increase in survivability can not be tracked and pin-pointed. However, as air quality has improved in industrial areas of America and Britain, through improved regulation, offering the conditions for a natural experiment, melanism has sharply declined in moths including \"B. betularia\" and \"Odontopera bidentata\". Cook and J. R. G. Turner have concluded that \"natural selection is the only credible explanation for the overall decline\", and other biologists working in the area concur with this judgement.\n\nIn 1921, the evolutionary biologist Richard Goldschmidt argued that the observed increase in the melanic form of the black arches moth, \"Lymantria monacha\", could not have been caused by mutation pressure alone, but required a selective advantage from an unknown cause: he did not consider camouflage as an explanation.\n\nNearly a century later, it was suggested that the moth's industrial melanism might, in addition (pleiotropy) to providing camouflage with \"the well-known protective dark coloration\", also confer better immunity to toxic chemicals from industrial pollution. The darker forms have a stronger immune response to foreign objects; these are encapsulated by haemocytes (insect blood cells), and the capsule so formed is then hardened with deposits of the dark pigment, melanin.\n\nA non-camouflage mechanism has been suggested for some vertebrates. In tropical ocean regions subject to industrial pollution the turtle-headed seasnake \"Emydocephalus annulatus\" is more likely to be melanic. These snakes shed their skin every two to six weeks. Sloughed skin contains toxic minerals, higher for dark skin, so industrial melanism could be selected for through improved excretion of trace elements. The same may apply in the case of urban feral pigeons, which have the ability to remove trace metals such as zinc to their feathers. However, toxic lead was not found to accumulate in feathers, so the putative mechanism is limited in its range.\n\nMelanic forms of the two-spot ladybird \"Adalia bipunctata\" are very frequent in and near cities, and rare in unpolluted countryside, so they appear to be industrial. Ladybirds are aposematic (with conspicuous warning coloration), so camouflage cannot explain the distribution. A proposed explanation is that the melanic forms have a thermal advantage directly linked to the pollution aspect of industrialization, since smoke and particulates in the air reduce the amount of sunlight that reaches the habitats of these species. Melanic phenotypes should then be favoured by natural selection, as the dark coloration absorbs the limited sunlight better. A possible explanation might be that in colder environments, the thermal advantages of industrial melanism might increase activity and the likelihood to mate. In the Netherlands, melanic \"A. bipunctata\" had a distinct mating advantage over the non-melanic form.\n\nHowever, thermal melanism failed to explain the distribution of the species near Helsinki where the city forms a relatively warm 'heat island', while near the Finnish coast there is more sunlight as well as more melanism, so the selective pressure driving melanism requires a different explanation. A study in Birmingham similarly found no evidence of thermal melanism but a strong correlation with smoke pollution; melanism declined from 1960 to 1978 as the city became cleaner. Further, the same study found that a related species, \"Adalia decempunctata\", experienced no change in frequency of melanism in the same places in that period.\n"}
{"id": "723638", "url": "https://en.wikipedia.org/wiki?curid=723638", "title": "Journey to the Center of the Earth", "text": "Journey to the Center of the Earth\n\nJourney to the Center of the Earth (, also translated under the titles A Journey to the Centre of the Earth and A Journey to the Interior of the Earth) is an 1864 science fiction novel by Jules Verne. The story involves German professor Otto Lidenbrock who believes there are volcanic tubes going toward the centre of the Earth. He, his nephew Axel, and their guide Hans descend into the Icelandic volcano Snæfellsjökull, encountering many adventures, including prehistoric animals and natural hazards, before eventually coming to the surface again in southern Italy, at the Stromboli volcano.\n\nThe genre of subterranean fiction already existed long before Verne. However, Journey considerably added to the genre's popularity and influenced later such writings. For example, Edgar Rice Burroughs explicitly acknowledged Verne's influence on his own \"Pellucidar\" series.\n\nThe story begins in May 1863, in the Lidenbrock house in Hamburg, Germany, with Professor Lidenbrock rushing home to peruse his latest purchase, an original runic manuscript of an Icelandic saga written by Snorri Sturluson (Snorre Tarleson in some versions of the story), \"Heimskringla\"; the chronicle of the Norwegian kings who ruled over Iceland. While looking through the book, Lidenbrock and his nephew Axel find a coded note written in runic script along with the name of a 16th-century Icelandic alchemist, Arne Saknussemm. (This was a first indication of Verne's love for cryptography. Coded, cryptic, or incomplete messages as a plot device would continue to appear in many of his works and in each case Verne would go a long way to explain not only the code used but also the mechanisms used to retrieve the original text.) Lidenbrock and Axel transliterate the runic characters into Latin letters, revealing a message written in a seemingly bizarre code. Lidenbrock attempts a decipherment, deducing the message to be a kind of transposition cipher; but his results are as meaningless as the original.\n\nProfessor Lidenbrock decides to lock everyone in the house and force himself and the others (Axel, and the maid, Martha) to go without food until he cracks the code. Axel discovers the answer when fanning himself with the deciphered text: Lidenbrock's decipherment was correct, and only needs to be read backwards to reveal sentences written in rough Latin. Axel decides to keep the secret hidden from Professor Lidenbrock, afraid of what the Professor might do with the knowledge, but after two days without food he cannot stand the hunger and reveals the secret to his uncle. Lidenbrock translates the note, which is revealed to be a medieval note written by Saknussemm, who claims to have discovered a passage to the centre of the Earth via Snæfell in Iceland. In what Axel calls bad Latin, the deciphered message reads:\n\nIn slightly better Latin, with errors amended:\nwhich, when translated into English, reads:\n\nProfessor Lidenbrock is a man of astonishing impatience, and departs for Iceland immediately, taking his reluctant nephew with him. Axel, who, in comparison, is anti-adventurous, repeatedly tries to reason with him, explaining his fears of descending into a volcano and putting forward various scientific theories as to why the journey is impossible, but Professor Lidenbrock repeatedly keeps himself blinded against Axel's point of view. After a rapid journey via Kiel and Copenhagen, they arrive in Reykjavík, where the two procure the services of Hans Bjelke (a Danish-speaking Icelander eiderdown hunter) as their guide, and travel overland to the base of the volcano.\n\nIn late June, they reach the volcano, which has three craters. According to Saknussemm's message, the passage to the center of the Earth is through the one crater that is touched by the shadow of a nearby mountain peak at noon. However, the text also states that this is only true during the last days of June. During the next few days, with July rapidly approaching, the weather is too cloudy for any shadows. Axel silently rejoices, hoping this will force his unclewho has repeatedly tried to impart courage to him only to succeed in making him even more cowardly stillto give up the project and return home. Alas for Axel, however, on the second to last day, the sun comes out and the mountain peak shows the correct crater to take.\n\nAfter descending into the crater, the three travellers set off into the bowels of the Earth, encountering many strange phenomena and great dangers, including a chamber filled with firedamp, and steep-sided wells around the \"path\". After taking a wrong turn, they run out of water and Axel almost dies, but Hans taps into a neighbouring subterranean river. Lidenbrock and Axel name the resulting stream the \"Hansbach\" in his honour and the three are saved. At another point, Axel becomes separated from the others and is lost several miles from them. Luckily, a strange acoustic phenomenon allows him to communicate with them from some miles away, and they are soon reunited.\n\nAfter descending many miles, following the course of the Hansbach, they reach an unimaginably vast cavern. This underground world is lit by electrically charged gas at the ceiling, and is filled with a very deep subterranean ocean, surrounded by a rocky coastline covered in petrified trees and giant mushrooms. The travelers build a raft out of trees and set sail. The Professor names this sea the \"Lidenbrock Sea\" and the port as \"Port Gräuben\", after the name of his goddaughter. While on the water, they see several prehistoric creatures such as a giant \"Ichthyosaurus\", which fights with a \"Plesiosaurus\" and wins. After the battle between the monsters, the party comes across an island with a huge geyser, which Lidenbrock names \"Axel Island\".\n\nA lightning storm again threatens to destroy the raft and its passengers, but instead throws them onto the coastline. This part of the coast, Axel discovers, is alive with prehistoric plant and animal life forms, including giant insects and a herd of mastodons. On a beach covered with bones, Axel discovers an oversized human skull. Axel and Lidenbrock venture some way into the prehistoric forest, where Professor Lidenbrock points out, in a shaky voice, a prehistoric human, more than twelve feet in height, leaning against a tree and watching a herd of mastodons. Axel cannot be sure if he has really seen the man or not, and he and Professor Lidenbrock debate whether or not a proto-human civilization actually exists so far underground. The three wonder if the creature is a man-like ape, or an ape-like man. The sighting of the creature is considered the most alarming part of the story, and the explorers decide that it is better not to alert it to their presence as they fear it may be hostile.\n\nThe travellers continue to explore the coastline, and find a passageway marked by Saknussemm as the way ahead. However, it is blocked by what appears to be a recent cave-in and two of the three, Hans and the Professor, despair at being unable to hack their way through the granite wall. The adventurers plan to blast the rock with gun cotton and paddle out to sea to escape the blast. Upon executing the plan, however, they discover that behind the rockfall was a seemingly bottomless pit, not a passage to the center of the Earth. The travellers are swept away as the sea rushes into the large open gap in the ground. After spending hours being swept along at lightning speeds by the water, the raft ends up inside a large volcanic chimney filling with water and magma. Terrified, the three are rushed upwards, through stifling heat, and are ejected onto the surface from a side-vent of a stratovolcano. When they regain consciousness, they discover that they have been ejected from Stromboli, a volcanic island located in southern Italy. They return to Hamburg to great acclaimProfessor Lidenbrock is hailed as one of the great scientists of history, Axel marries his sweetheart Gräuben, and Hans eventually returns to his peaceful life in Iceland. The Professor has some regret that their journey was cut short.\n\nAt the very end of the book, Axel and Lidenbrock realize why their compass was behaving strangely after their journey on the raft. They realize that the needle was pointing the wrong way after being struck by an electric fireball which nearly destroyed the wooden raft.\n\nThe book was inspired by Charles Lyell's \"Geological Evidences of the Antiquity of Man\" of 1863 (and probably also influenced by Lyell's earlier ground-breaking work \"Principles of Geology\", published 1830–33). By that time geologists had abandoned a literal biblical account of Earth's development and it was generally thought that the end of the last glacial period marked the first appearance of humanity, but Lyell drew on new findings to put the origin of human beings much further back in the deep geological past. Lyell's book also influenced Louis Figuier's 1867 second edition of \"La Terre avant le déluge\" (\"The Earth before the flood\") which included dramatic illustrations of savage men and women wearing animal skins and wielding stone axes, in place of the Garden of Eden shown in the 1863 edition.\n\nIt is noteworthy that at the time of writing Verne had no hesitation with having sympathetic German protagonists with whom the reader could identify. Verne's attitude to Germans would drastically change in the aftermath of the 1871 Franco-Prussian War. After 1871, the sympathetic if eccentric Professor Otto Lidenbrock would be replaced in Verne's fiction by the utterly evil and demonic Professor Schultze of \"The Begum's Fortune\".\n\n\n\nThe first English edition was published in its entirety by Henry Vickers in 12 installments of a Boys magazine entitled \"The Boys Journal\". The plates are more numerous than the book form which was published with an 1872 title page. If it was released in 1871 as a single volume it was late in the year. This \"True\" first edition also found in an octavo normal book size (not Annual size), has been overlooked by bibliographers. It has a place of pre-eminence up to about a 3rd of the way through the 12 monthly issues and then slides down into the main body of the journal. The Magazine does not seem to have survived in its loose format of 12 individual parts.\n\n\n\n\n\n\n\n\n"}
{"id": "57066869", "url": "https://en.wikipedia.org/wiki?curid=57066869", "title": "List of C4 plants", "text": "List of C4 plants\n\nThe following list presents known lineages by family, based on the overview by Sage (2016). They correspond to single species or clades thought to have acquired the pathway independently. In some lineages that also include and – intermediate species, the pathway may have evolved more than once.\n\nThe large acanthus family Acanthaceae includes one genus with species, found in dry habitats from Africa to Asia.\n\n\nWhile many species in the ice plant family Aizoaceae use crassulacean acid metabolism (CAM), one subfamily with drought-tolerant and halophytic plants includes species:\n\n\nThe amaranth family Amaranthaceae (including the former goosefoot family Chenopodiaceae) contains around 800 known species, which belong to 14 distinct lineages in seven subfamilies. This makes Amaranthaceae the family with most species and lineages among the eudicots. \"Suaeda aralocaspica\" and species of the genus \"Bienertia\" use a particular, single-cell type of carbon fixation.\n\n\nThe composite family Asteraceae contains three lineages, in two different tribes of subfamily Asteroideae. They include the model genus \"Flaveria\" with closely related , , and intermediate species.\n\n\nThe borage family Boraginaceae contains one widespread genus, \"Euploca\", which has also been treated as part of a distinct family Heliotropiaceae.\n\n\nThe Cleomaceae, formerly included in the caper family Capparaceae, contains three species in genus \"Cleome\". These three species independently acquired the pathway; the genus also contains numerous as well as – intermediate species.\n\n\nIn the carnation family Caryophyllaceae, the pathway evolved once, in a clade within the polyphyletic genus \"Polycarpaea\".\n\n\nThe sedge family Cyperaceae is second only to the grasses in number of species. Prominent sedges include culturally important species such as papyrus (\"Cyperus papyrus\") and chufa (\"C. esculentus\") but also purple nutsedge (\"C. rotundus\"), one of the world's major weeds. \"Eleocharis vivipara\" uses carbon fixation in underwater leaves and carbon fixation in aerial leaves.\n\n\nThe spurge family Euphorbiaceae contains the largest single lineage among eudicots. The spurges are diverse and widespread; they range from weedy herbs to the only known trees – four species from Hawaii, including \"Euphorbia olowaluana\" (up to 10 m) and \"E. herbstii\" (up to 8 m).\n\n\nContains a genus with a single species.\n\n\nIncludes the only known aquatic plants.\n\n\nThe two species within the same genus have acquired the pathway independently.\n\n\n\n\nThe single genus of this family forms one lineage. CAM photosynthesis is also known. Common purslane (\"Portulaca oleracea\") is a major weed but also a vegetable.\n\nThe grass family includes most of the known species – around 5000. They are only found in subfamilies of the PACMAD clade. Major crops such as maize, sugarcane, sorghum and pearl millet belong in this family. The only known species with , and intermediate variants, \"Alloteropsis semialata\", is a grass.\n\n\n\n"}
{"id": "8137709", "url": "https://en.wikipedia.org/wiki?curid=8137709", "title": "List of Florida hurricanes (pre-1900)", "text": "List of Florida hurricanes (pre-1900)\n\n<onlyinclude>\nThe list of Florida hurricanes prior to 1900 extends back to 1523 and encompasses 159 North Atlantic hurricanes known to have affected Florida. Since the start of the Atlantic hurricane database in 1851, there were only eight years in which no tropical cyclone affected the state. Collectively, tropical cyclones in Florida resulted in at least 6,504 fatalities and monetary damage of over $90 million (2008 USD). At least 109 boats or ships were either driven ashore, wrecked, or damaged due to the storms.</onlyinclude>\n\nInformation is sparse for earlier years due to limitations in tropical cyclone observation, though as coastlines became more populated, more data became available. The National Hurricane Center recognizes the uncertainty in both the death tolls and the dates of the events.\n<onlyinclude>\n\n\n\n\n\n\n\n\n\nThe following is a list of hurricanes with known deaths in the state. Several other hurricanes killed an unknown number of people in Florida, and multiple others left several missing.\n\n"}
{"id": "33258814", "url": "https://en.wikipedia.org/wiki?curid=33258814", "title": "List of Potentilla species", "text": "List of Potentilla species\n\nThe following species in the genus \"Potentilla\" are recognised by \"The Plant List\":\n\nThe following additional species are accepted by ITIS, although they might be considered synonyms by other sources:\n\nThe following additional species are accepted by the Germplasm Resources Information Network (GRIN), although they might be considered synonyms by other sources, or be erroneous accessions:\n\n"}
{"id": "15976177", "url": "https://en.wikipedia.org/wiki?curid=15976177", "title": "List of Senecio species", "text": "List of Senecio species\n\nSenecio is a very large genus of flowering plants in the sunflower family (Asteraceae). The following species are accepted by \"The Plant List\".\n"}
{"id": "11096996", "url": "https://en.wikipedia.org/wiki?curid=11096996", "title": "List of Superfund sites in Florida", "text": "List of Superfund sites in Florida\n\nThis is a list of Superfund sites in Florida designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of May 3, 2010, there were 52 Superfund sites on the National Priorities List in Florida. Three more sites have been proposed for entry on the list and 23 others have been cleaned up and removed from it. There are also twelve Superfund Alternative sites in the state.\n\n\n"}
{"id": "5874911", "url": "https://en.wikipedia.org/wiki?curid=5874911", "title": "List of Titanoecidae species", "text": "List of Titanoecidae species\n\nThis page lists all described species of the spider family Titanoecidae as of Oct. 14, 2013.\n\n\"Anuvinda\" \n\n\"Goeldia\" \n\n\"Nurscia\" \n\n\"Pandava\" \n\n\"Titanoeca\" \n\n"}
{"id": "56550630", "url": "https://en.wikipedia.org/wiki?curid=56550630", "title": "List of Xenylla species", "text": "List of Xenylla species\n\nThis is a list of 123 species in \"Xenylla\", a genus of springtails in the family Hypogastruridae.\n\n"}
{"id": "9019565", "url": "https://en.wikipedia.org/wiki?curid=9019565", "title": "List of hemp diseases", "text": "List of hemp diseases\n\nThis is a list of diseases of hemp (\"Cannabis sativa\").\n\n"}
{"id": "17964903", "url": "https://en.wikipedia.org/wiki?curid=17964903", "title": "Lists of ecoregions in the United States", "text": "Lists of ecoregions in the United States\n\nLists of ecoregions in the United States — based upon two separate ecoregion classification systems:\n\n\n"}
{"id": "36081308", "url": "https://en.wikipedia.org/wiki?curid=36081308", "title": "Monodominance", "text": "Monodominance\n\nMonodominance is an ecological condition in which more than 60% of the tree canopy comprises a single species of tree. Although monodominance is studied across different regions, most research focuses on the many prominent species in tropical forests. Connel and Lowman, originally called it single-dominance. Conventional explanations of biodiversity in tropical forests in the decades prior to Connel and Lowman's work either ignored monodominance entirely or predicted that it would not exist.\n\nConnel and Lowman hypothesized two contrasting mechanisms by which dominance can be attained. The first is by fast regrowth in unstable habitats with high disturbance rates. The second is through competitive exclusion in stable habitats that have low disturbance rates. Explanations of persistent monodominace include the monodominant species being more resistant than others to seasonal flooding, or that the monodominance is simply a sere. With persistent monodominance, the monodominant species successfully remains so from generation to generation.\n\nA minimum of 22 species from eight different families are known to create monodominant forests. Examples of persistent monodominance are seen in Africa, Central and South America, and Asia. \"Dipterocarpaceae\" is one example of a plant family that is recognized as persistently dominant in Asia. The ectomycorrhizal tree \"Dicymbe corymbosa\", found in central Guyana, creates wide ranges of monodominant forests containing more than 80% of the canopy tree species.\n\nDominant plants in the Neotropics and Africa are usually in the Leguminosae family. The species \"Gilbertiodendron dewevrei\", \"Cynometra alexandri\", and \"Julbernardia seretii\" are pronounced as exclusive dominants in their individual forests in equatorial Africa. \"G. dewevrei\" dominated forests are more widespread on the highlands adjacent to the central basin of the Zaire River. This species in the Ituri forest forms monodominant stands that occupy more than 90% of the canopy trees.\n\nConnel and Lowman originally hypothesized ectomycorrhizal association causing the replacement of other species as one of two mechanisms by which a species becomes persistently monodominant; the other is the simple colonization of large gaps. However, subsequent research over the years has shown that there is not a single, simple mechanism by which monodominance occurs. Monodominant species have been recorded forming at various times after forest clearance, though this has not been shown to be a predictor of monodominant species persistence. Reliance upon ectomycorrhizae and poor soils have not been demonstrated. Instead, multiple traits of adult monodominant species hinder the ability of other species to grow, including a dense canopy, a uniform canopy, deep leaf litter, slow nutrient processing, mast fruiting, and poor dispersal.\n\nSeveral causal mechanisms have been proposed for the formation of monodominant forest in tropical ecosystems, including features of the environment such as low disturbance rates, and intrinsic characteristics of the dominant species: escape from herbivores, high seedling shade-tolerance, and the formation of mycorrhizal networks between individuals of the same species.\n\nThe dense canopy of the adult trees prevents light from getting into the understory. In the Ituri Forest of the Democratic Republic of the Congo a monodominant \"Gilbertiodendron\" forest understory only receives 0.57% full sunlight while a mixed-forest understory received 1.15% full sunlight. This difference may prohibit many plant species from living in that environment due to the low light conditions and their resulting inability to sufficiently and effectively photosynthesize. Even some species that are more shade tolerant cannot survive the severe low light conditions.\n\nA monodominant forest has generally very deep leaf litter because the leaves are not decomposing as quickly as in other forests. In some monodominant forests the decomposition rates can be two to three times slower than mixed forests. Low ammonium and nitrate could be the result of this slow decomposition which in turn, means less nutrients in the soil for other plant species to use.\n\nNutrient processing is somewhat different from one forest to another. In the \"Gilbertiodendron\" forests there is low availability of nitrogen due to the low levels in the leaves that fall to the ground and the slow decomposition. This could prevent other plant species from colonizing because the soil lacks necessary nutrients. In \"Parashorea chinensis\" forests, trees are known to require more fertile soils than in other areas. There is a large amount of manganese though that prevents other plants from taking root. Manganese can poison other trees if the levels are too high and possibly cause leaf chlorosis and necrosis and prevent the nutrient uptake of calcium and magnesium.\n\nMast fruiting is a mass fruiting event that overwhelms the animals that consume fruit and helps the seeds' survival rate. Well-defended leaves also assist in the prevention of predation. In the \"Gilbertiodendron\" forests this mast fruiting does not assist in lesser predation, but in Asia and the Neotropics this does induce fitness benefits and sometimes is actually important to monodominant maintenance.\n\nA monodominant forest has poor dispersal because of the lack of animal dispersers, so many of the seeds are just dropped from the parent tree and fall to the ground where they germinate. This can create a regular and radial path around the parent tree that results in a \"tree-by-tree replacement\" in a mixed forest. In a monodominant forest the dominant species do not need all of the described traits to overwhelm the area. Though many have a combination, all monodominant forests have at least one of these traits to create the monodominant habitat.\n\nMany of the tropical monodominant trees are associated with ectomycorrhizal (ECM) fungi networks. Mycorrhizal fungi are known to effect plant diversity trends in a variety of ecosystems around the world. Ectomycorrhizal relations with trees can increase nutrient supplies through a more effectual use of larger capacities of soils or through the direct decomposition of leaf litter. This has been suggested to provide a competitive advantage to such tree species.\n\nExamples of ectomycorrhizal trees in tropical rainforests can be found in Asia, Africa, and the Neotropics. There is a strong correlation between the ECM association in tropical trees and the occurrence of monodominance.\n\nFungi like mycorrihizae appear not to harm the leaves and even display a symbiotic relationship. ECM fungi are derived from saprotrophs and retain some ability to decompose organic material. Because tropical soils are often nutrient-poor, ECM trees are predicted to have a competitive advantage over neighboring trees because of their ability to attain more nutrients. With time this could lead to dominance in a tropical rainforest.\n\nA study of \"Dicymbe corymbosa\" individuals show that (in terms of total basal area) the adult trees dominate resources and space. Additionally, they form coppices, also known as epicormic shoots, which allow their perseverance over time. Hence, if one stem of the tree dies, it is replaced by another living stem in the canopy. This creates same-species regrowth at stem level. All of this requires high levels of carbohydrates and nutrients that are accumulated from the ECM association.\nThere is evidence that masting tree species rely on ECM associations to accumulate these requisite nutrients for reproduction during inter-mast years. Associations between resource levels stowed in plant tissue, timing of masting, and ECM patterns propose that ECM fungi are essential in the procurement of nutrients required for large masting trees.\n\nSeeds of monodominant trees typically have higher rates of germination and seedling survival when planted in monodominant forests rather than mixed forests. Monodominant seedlings planted in mixed forests have significantly lower levels of ECM colonization of roots. The lower percent of ECM colonization can cause the low survival rates of these seedlings in mixed forest. Another mechanism that can be important for seedling and growth survival is a connection to a common ECM network. By connecting their small root systems to ECM networks that emanate from larger adults, more benefits can be received.\n\nSlower decomposition rates in monodominant forests have been hypothesized to be a result of competition between saprotrophic bacteria and fungi. ECM fungi may be suppressing saprotrophs in the monodominant forest to slow decomposition and return organically bound nutrients back to the tree. This is also called the \"Gadgil\" hypothesis.\n\nAll of the traits that contribute to creating a monodominant forest over time hinder the growth of other plant species and force them to move to a more mixed forest. Even though this is inconvenient for the plant species that were there, there has not been any evidence that suggests that this is a negative effect of monodominance. Monodominant forests are also found to have significantly less nitrogen in their soil than mixed forests. In these monodominant forests there are a lot of dominant tree species from the legume family that have nitrogen fixation. Nitrogen fixation creates compounds that help a plant to grow in otherwise low nutrient conditions.\n"}
{"id": "39867389", "url": "https://en.wikipedia.org/wiki?curid=39867389", "title": "Myristica swamp", "text": "Myristica swamp\n\n\"Myristica\" swamps are a type of freshwater swamp forest predominantly composed of species of \"Myristica\". These are found in two localities in India. \"Myristica\" swamps have adapted to inundation by way of stilt roots and knee roots. \"Myristica\" swamps are found in the Uttara Kannada district of Karnataka State and in the southern parts of Kerala State.\n\n\"Myristica\" swamps were described as a separate evergreen forest type by Krishnamurthy in 1960. These received attention in 1988 when Rodgers and Panwar described it as the most endangered forest ecosystem in India. The swamps in Karnataka has been studied in detail by Chandran, MDS and his colleagues in 1999.\n\nNair, PV and his team has conducted a detailed study on the flora and fauna of the \"Myristica\" swamps of southern Kerala in 2007. In Kerala, the Myristica swamps are found predominantly in Kulathupuzha , Anchal and Shendurney regions of Kollam district. Nair PVand Pandalai carried out successful re-introduction of one of the swamp tree species in 2012.\n\nThe swamps occur on either side of first order streams. The swamp boundary can be seen distinctly in the field due to the stilt roots.\nChandran and his colleagues were able to locate 51 swamps in Uttara Kannada. They were able to show them on maps, and describe location, but exact boundary mapping was not attempted. Mapping by Nairpv and colleagues in 2007 was more detailed and they mapped 60 swamps using GPS technology. The swamps ranged in area from about 0.25 to 10 hectares. The swamps are distributed in two river systems spread over two districts. Boundary mapping has revealed that the total area of Myristica swamps in Kerala is about 1.5 km2 which hardly make up 0.004% of the total land area of Kerala (38,864 km2) and 0.014% of the total forest area of Kerala (11,126 km2). The swamps in Karnataka are located at about 300m altitude, the swamps in Kerala are at an altitude of 200m.\n\nChandran and his colleagues give detailed description of the flora. They found that 63 of 130 flowering plants identified are endemic to Western Ghats of India. Major species of trees were Gymnocranthera canarica, Myristica fatua, Mastixia arborea, Semecarpus travancorica, Hopea whitiana, Lophopetalum whitiana, Holigarna grahami, Sysygium laetum, etc.\nStudies by Nair and colleagues in southern Kerala in 2007 documents eighty two trees and ninety four species of herbs/shrubs. Forty nine lianas have also been recorded. Twelve of these plants species have been redlisted and about 28 species of them are endemic to Western Ghats. Out of the 19 sample plots, Gymnacranthera farquhariana was dominant in 10 plots. Myristica fatua was the dominant tree in 6 swamps. In the remaining plots, Vateria indica was the dominant tree. Holigarna arnottiana and Lophopetalum wightianum dominated in another two plots.\n\nNair and colleagues in southern Kerala in 2007 focussed on fauna also. Faunal biodiversity of the Myristica swamps consisted of Platyhelminthes- (Bipalium-2, tapeworm-1) 3 species, Nemathelminthes – 1 species, Annelida (Oligochaeta -2and Hirudinea-2) 4 species, Mollusca- 10 species, Unidentified Crustacean-1 species, Insecta- 281 species belonging to 83 identified families, Myriapoda- 6 species and Arachnidae 54 species, Pisces 14 species, Amphibia 56 species, Reptilia 55 species, Aves 129 species and Mammalia 27 species. Quantitative analysis of herpetofauna revealed that the differences in the environmental characteristics inside and outside the swamp play an important role in regulating the species diversity and abundance of both amphibians and reptiles. Amphibians were more susceptible to environmental changes. Patterns of diversity and abundance during day and night, across swamps and among months varied. There was no significant difference in patterns of diversity and abundance recorded during the two years. Many of the animals documented belong to red list and endemic categories.\n\nNair and colleagues in southern Kerala highlight the enormous biodiversity of the Myristica swamp forests. The study also indicates that there are gaps in information which can be filled up only with further studies in this region. A challenge is how further studies can be carried out without disturbing the delicate ecosystem of these swamps. A pertinent question is whether all human entry should be banned into the best and least disturbed patches of swamps, leaving only the disturbed patches for human visits (tourism and academic study). Conservation of these small and scattered swamp patches must also address the contiguous areas. Strategies for management and conservation have been suggested in the above context.\n\n"}
{"id": "48760976", "url": "https://en.wikipedia.org/wiki?curid=48760976", "title": "Nakshatravana", "text": "Nakshatravana\n\nNakshatravana, also referred to as Nakshatravanam or Nakshatravan, is a sacred grove consisting of 27 trees that are related to 27 Nakshatras of Indian Astrology. The Nakshatras and the trees are as below:\n\nConsidering the diversity of plants involved, their medicinal value, and association with Nakshatras, many organisations are popularizing the creation of Nakshatravanam.\n"}
{"id": "5230394", "url": "https://en.wikipedia.org/wiki?curid=5230394", "title": "National Register of Big Trees", "text": "National Register of Big Trees\n\nThe National Register of Big Trees is a list of the largest recorded living specimens of each tree variety found in the continental United States. A tree on this list is called a National Champion Tree.\n\nThis list has been maintained since 1940 by American Forests, a nonprofit conservation organization. To be eligible, a species must be recognized as native or naturalized in the continental United States, including Alaska but not Hawaii, as documented in Elbert L. Little Jr.'s \"Checklist of United States Trees (Native and Naturalized)\", published in 1979 as \"Agricultural Handbook 541\" by the United States Department of Agriculture. At present 747 native and 79 naturalized trees are eligible, for a total of 826 eligible species and varieties.\n\nAmerican Forests uses the following formula to calculate a point score for each tree so that they may be compared to others:\n\nThe current list of National Champion Trees is available online. In addition to the national list, several states, counties, and cities maintain their own list of local Champion Trees. Many are on public ground and can be visited without obtaining prior permission. The public may nominate trees as well.\n\n, the largest National Champion Tree is a giant sequoia in California. Known as the General Sherman tree, it is some 83.8 m (274.9 feet) tall, 31.1 m (85.3 feet) in circumference and 32.5 m (106.5 feet) in average crown spread.\n\n\n"}
{"id": "20248259", "url": "https://en.wikipedia.org/wiki?curid=20248259", "title": "Natural and Culturo-Historical Region of Kotor", "text": "Natural and Culturo-Historical Region of Kotor\n\nThe Natural and Culturo-Historical Region of Kotor is a World Heritage Site located in Montenegro that was inscribed in 1979. It encompasses the old town of Kotor (Italian Cattaro), the fortifications of Kotor, and the surrounding region of the inner Bay of Kotor.\n\nThe old town of Kotor is contained within the city walls and a well preserved and restored medieval cityscape with notable buildings including the Cathedral of Saint Tryphon (built in 1166). \nKotor was heavily damaged during the earthquake on April 15, 1979, and this prompted the site to be also listed on the Danger List in 1979 when the site was inscribed. After significant rehabilitation within the town, the site was taken off this list in 2003.\n\nThe fortifications consist of a system of defensive military buildings to protect the medieval town of Kotor. They include the city walls with gates and bastions, ramparts that ascend the mountain of St. John, the castle of St. John (San Giovanni), and supportive structures. While some of the structures date back to Roman and Byzantine times, most of the fortifications were erected during the Venetian rule; later some modifications were made by the Austrians. The fortifications are the most significant aspect of the World Heritage site.\n\nThe region that is included in the heritage is the inner bay of Kotor (past the Verige strait) with its surrounding mountains and towns, notably Risan and Perast in addition to Kotor. Further the islets of St. George (Sveti Đorđe) and Our Lady of the Rocks (Gospa od Škrpijela) are part of the heritage site.\n\nIn 1979, when the site was inscribed it was also placed on the List in Danger due to earthquake damage. Due to concerted international efforts much of this, specifically concerning the city of Kotor, has been mitigated. In 2003 the site could be taken off the danger list.\n\nThe heritage site faces challenges in a number of ways. Natural dangers such as erosion and earthquakes will always remain a threat. More acute, however, is the impact of human activity. Thus some urban development has been noted to be incongruent with the goals of preservation. An issue has been the proposal to bridge the Verige strait: the proposed Verige bridge between Cape St. Nedjelja and Cape Opatovo would facilitate traffic of the Adriatic Highway that currently utilizes a ferry system to cross the bay.\nWhen the site was inscribed in 1979, it was done so based on its cultural values; a 2008 UNESCO Mission Report suggests to also consider its outstanding value as a “cultural landscape” which may lead to a re-submission.\n"}
{"id": "3883838", "url": "https://en.wikipedia.org/wiki?curid=3883838", "title": "Nectar", "text": "Nectar\n\nNectar is a sugar-rich liquid produced by plants in glands called nectaries, either within the flowers with which it attracts pollinating animals, or by extrafloral nectaries, which provide a nutrient source to animal mutualists, which in turn provide antiherbivore protection. Common nectar-consuming pollinators include mosquitoes, hoverflies, wasps, bees, butterflies and moths, hummingbirds, and bats. Nectar plays an important role in the foraging economics and overall evolution of nectar-eating species; for example, nectar and its properties are responsible for the differential evolution of the African honey bee, \"A. m. scutellata\" and the western honey bee.\n\nNectar is an ecologically important item, the sugar source for honey. It is also useful in agriculture and horticulture because the adult stages of some predatory insects feed on nectar. For example, the social wasp species \"Apoica flavissima\" relies on nectar as a primary food source. In turn, these wasps then hunt agricultural pest insects as food for their young. For example, thread-waisted wasps (genus \"Ammophila\") are known for hunting caterpillars that are destructive to crops. Caterpillars however, do eventually become butterflies and moths, which are very important pollinators. \n\nNectar secretion increases as the flower is visited by pollinators. After pollination, the nectar is frequently reabsorbed into the plant.\n\n\"Nectar\" is derived from Greek \"nektar\", the fabled drink of Greek gods. The word is derived as a compound of \"nek\", meaning death, and \"tar\", meaning the ability to overcome. The common use of nectar refers to the \"sweet liquid in flowers\", first recorded in AD 1600.\n\nA nectary is floral tissue found in different locations in the flower. The different types of floral nectaries include septal nectaries found on the sepal, petal nectaries, staminal nectaries found on the stamen, and gynoecial nectaries found on the ovary tissue. The nectaries also may vary in color, number, and symmetry. Nectaries can also be categorized as structural or non-structural. Structural nectaries refer to specific areas of tissue that exude nectar, such as the types of floral nectaries previously listed. Non-structural nectaries secrete nectar infrequently from non-differentiated tissues. The different types of floral nectaries coevolved depending on the pollinator that feeds on the plant's nectar. Nectar is secreted from epidermal cells of the nectaries by means of trichomes or modified stomata. The nectar comes from phloem with additional sugars that are secreted from the cells through vesicles packaged by the endoplasmic reticulum. Flowers that have longer nectaries sometimes have a vascular strand in the nectary to assist in transport over a longer distance.. \n\nFloral nectaries are used by plants to attract pollinators such as insects, hummingbirds, and other vertebrates. The pollinators feed on the nectar and depending on the location of the nectary the pollinator assists in fertilization and outcrossing of the plant as they brush against the reproductive organs, the stamen and pistil, of the plant and pick up or deposit pollen. Nectar from floral nectaries is sometimes used as a reward to insects, such as ants, that protect the plant from predators. Many floral families have evolved a nectar spur. These spurs are projections of various lengths formed from different tissues, such as the petals or sepals. They allow for pollinators to land on the elongated tissue and more easily reach the nectaries and obtain the nectar reward. Different characteristics of the spur, such as its length or position in the flower, may determine the type of pollinator that visits the flower. \n\nDefense from herbivory is often one of the roles of extrafloral nectaries. Floral nectaries can also be involved in defense. In addition to the sugars found in nectar, certain proteins may also be found in nectar secreted by floral nectaries. In tobacco plants, these proteins have antimicrobial and antifungal properties and can be secreted to defend the gynoecium from certain pathogens. \n\nFloral nectaries have evolved and diverged into the different types of nectaries due to the various pollinators that visit the flowers. In Melastomataceae, different types of floral nectaries have evolved and been lost many times. Flowers that ancestrally produced nectar and had nectaries may have lost their ability to produce nectar due to a lack of nectar consumption by pollinators, such as certain species of bees. Instead they focused on energy allocation to pollen production. Species of angiosperms that have nectaries use the nectar to attract pollinators that consume the nectar, such as birds and butterflies. In Bromeliaceae, septal nectaries are common in species that are insect or bird pollinated. In species that are wind pollinated, nectaries are often absent because there is no pollinator to provide a reward for. In flowers that are generally pollinated by long-tongued organism such as certain flies, moths, butterflies, and birds, nectaries in the ovaries are common because they are able to reach the nectar reward when pollinating. Septal and petal nectaries are often more common in species that are pollinated by short-tongued insects that cannot reach so far into the flower.\n\nExtrafloral nectaries (also known as extranuptial nectaries) are nectar-secreting plant glands that develop outside of flowers and are not involved in pollination. They are highly diverse in form, location, size, and mechanism. They have been described in virtually all above-ground plant parts—including leaves (in which case they are known as foliar nectaries), petioles, stipules, cotyledons, fruits, and stems, among others. They range from single-celled trichomes to complex cup-like structures that may or may not be vascularized. \nIn contrast to floral nectaries, nectar produced outside the flower generally have a defensive function. The nectar attracts predatory insects which will eat both the nectar and any plant-eating insects around, thus functioning as 'bodyguards'. Foraging predatory insects show a preference for plants with extrafloral nectaries, particularly some species of ants and wasps, which have been observed to directly defend the plants. Among passion flowers, for example, extrafloral nectaries prevent herbivores by attracting ants and deterring two species of butterflies from laying eggs. In many carnivorous plants, extrafloral nectaries are also used to attract insect prey.\n\nDarwin understood that extrafloral nectar \"though small in quantity, is greedily sought by insects\" but believe the \"their visits do not in any way benefit the plant\". Instead, he believed that extrafloral nectaries were excretory in nature (hydathodes). Their defensive functions were first recognized by the Italian botanist Federico Delpino in his important monograph \"Funzione mirmecofila nel regno vegetale\" (1886). Delpino's study was inspired by a disagreement with Charles Darwin, with whom he corresponded regularly.\n\nExtrafloral nectaries have been reported in over 3941 species of vascular plants belonging to 745 genera and 108 families, 99.7% of which belong to flowering plants (angiosperms), comprising 1.0 to 1.8% of all known species. They are most common among eudicots, occurring in 3642 species (of 654 genera and 89 families), particularly among rosids which comprise more than half of the known occurrences. The families showing the most recorded occurrences of extrafloral nectaries are Fabaceae, with 1069 species, Passifloraceae, with 438 species, and Malvaceae, with 301 species. The genera with the most recorded occurrences are \"Passiflora\" (322 species, Passifloraceae), \"Inga\" (294 species, Fabaceae), and \"Acacia\" (204 species, Fabaceae). Other genera with extrafloral nectaries include \"Salix\" (Salicaceae), \"Prunus\" (Rosaceae) and \"Gossypium\" (Malvaceae).\n\nFoliar nectaries have also been observed in 39 species of ferns belonging to seven genera and four families of Cyatheales and Polypodiales. They are absent, however, in bryophytes, gymnosperms, early angiosperms, magnoliids, and members of Apiales among the eudicots. Phylogenetic studies and the wide distribution of extrafloral nectaries among vascular plants point to multiple independent evolutionary origins of extrafloral nectaries in at least 457 independent lineages.\n\nThe main ingredients in nectar are sugars in varying proportions of sucrose, glucose, and fructose. In addition, nectars have diverse other phytochemicals serving to both attract pollinators and discourage predators. Carbohydrates, amino acids, and volatiles function to attract some species, whereas alkaloids and polyphenols appear to provide a protective function. \n\nThe \"Nicotiana attenuata\", a tobacco plant native to the US state of Utah, uses several volatile aromas to attract pollinating birds and moths. The strongest such aroma is benzylacetone, but the plant also adds bitter nicotine, which is less aromatic, so may not be detected by the bird until after taking a drink. Researchers speculate the purpose of this addition is to discourage the forager after only a sip, motivating it to visit other plants, therefore maximizing the pollination efficiency gained by the plant for a minimum nectar output. Neurotoxins such as aesculin are present in some nectars such as that of the California buckeye. Nectar contains water, carbohydrates, amino acids, ions and numerous other compounds.\n\n\n"}
{"id": "33505398", "url": "https://en.wikipedia.org/wiki?curid=33505398", "title": "New production", "text": "New production\n\nIn biological oceanography, new production is supported by nutrient inputs from outside the euphotic zone, especially upwelling of nutrients from deep water, but also from terrestrial and atmosphere sources (as opposite to regenerated production, which is supported by recycling of nutrients in the euphotic zone). New production depends on mixing and vertical advective processes associated with the circulation.\n\nBio-available nitrogen occurs in the ocean in several forms, including simple ionic forms such as nitrate (NO), nitrite (NO) and ammonium (NH), and more complex organic forms such as urea ((NH2)CO). These forms are utilised by autotrophic phytoplankton to synthesise organic molecules such as amino acids (the building blocks of proteins). Grazing of phytoplankton by zooplankton and larger organisms transfers this organic nitrogen up the food chain and throughout the marine food-web.\n\nWhen nitrogenous organic molecules are ultimately metabolised by organisms, they are returned to the water column as ammonium (or more complex molecules that are then metabolised to ammonium). This is known as regeneration, since the ammonium can be used by phytoplankton, and again enter the food-web. Primary production fuelled by ammonium in this way is thus referred to as regenerated production.\n\nHowever, ammonium can also be oxidised to nitrate (via nitrite), by the process of nitrification. This is performed by different bacteria in two stages :\n\nNH + O → NO + 3H + 2e\n\nNO + HO → NO + 2H + 2e\n\nCrucially, this process is believed to only occur in the absence of light (or as some other function of depth). In the ocean, this leads to a vertical separation of nitrification from primary production, and confines it to the aphotic zone. This leads to the situation whereby any nitrate in the water column must be from the aphotic zone, and must have originated from organic material transported there by sinking. Primary production fuelled by nitrate is, therefore, making use of a \"fresh\" nutrient source rather than a regenerated one. Production by nitrate is thus referred to as new production.\n\nTo sum up, production based on nitrate is using nutrient molecules newly arrived from outside the productive layer, it is termed new production. The rate of nitrate utilization remains a good measure of the new production. While if the organic matter is then eaten, respired and the nitrogen excreted as ammonia, its subsequent uptake and re-incorporation in organic matter by phytoplankton is termed recycled (or regenerated) production. The rate of ammonia utilization is, in the same sense, a measure of recycled production.\n\nThe use of N-compounds makes it possible to measure the fractions of new nitrogen and regenerated nitrogen associated with the primary production in the sea.\n\n"}
{"id": "22943850", "url": "https://en.wikipedia.org/wiki?curid=22943850", "title": "Photon-intermediate direct energy conversion", "text": "Photon-intermediate direct energy conversion\n\nPhoton-intermediate direct energy conversion (PIDEC) is a scheme for direct conversion of nuclear power to electricity.\n\nPIDEC process is somewhat similar to a concept of fluorescent light - as in the CFL, in the nuclear reactor the original type of energy generated is not useful to humans. CFL uses a fluorescent coating on the inside of the light bulb to convert that energy into visible spectrum of the light. PIDEC uses fluorescer (in the form of gas) surrounding nuclear fuel acting as photon producer - fluorescer gets excited by neutron emissions and in turn emits narrow band ultraviolet light. That light is then relatively easily converted into electricity by special photo-voltaic converter.\n\nBecause the photons emitted by fluorescer are narrow band, the conversion efficiency is much higher than efficiency of common solar cells. The overall efficiency of PIDEC is expected to be around 40%. The remaining residual heat is still high enough to use it in traditional thermalized way via Carnot Cycle e.g. steam turbine. A combined efficiency of such conversion system (PIDEC + traditional) could reach as much as 70%. In comparison, due to limitations of using solid nuclear fuel and water as coolant, current generation of nuclear plants average only about 35% conversion efficiency.\n\nPIDEC would work best with some IV Generation nuclear reactor designs. For instance in molten salt reactor low pressure liquid serves both as fuel and as coolant. This way reactor can safely operate at very high temperatures increasing the energy conversion efficiency.\n\nA lot of theoretical work has been done at University of Missouri with some patents applied for by Mark A. Prelas. Professor Prelas is best known for advocating the concept of nuclear lightbulb.\n\n"}
{"id": "1937145", "url": "https://en.wikipedia.org/wiki?curid=1937145", "title": "S-wave", "text": "S-wave\n\nIn seismology, S-waves, secondary waves, or shear waves (sometimes called an elastic S-wave) are a type of elastic wave, and are one of the two main types of elastic body waves, so named because they move through the body of an object, unlike surface waves.\n\nThe S-wave moves as a shear or transverse wave, so motion is perpendicular to the direction of wave propagation. The wave moves through elastic media, and the main restoring force comes from shear effects. These waves do not diverge, and they obey the continuity equation for incompressible media:\n\nIts name, S for secondary, comes from the fact that it is the second direct arrival on an earthquake seismogram, after the compressional primary wave, or P-wave, because S-waves travel slower in rock. Unlike the P-wave, the S-wave cannot travel through the molten outer core of the Earth, and this causes a shadow zone for S-waves opposite to where they originate. They can still appear in the solid inner core: when a P-wave strikes the boundary of molten and solid cores, S-waves will then propagate in the solid medium. And when the S-waves hit the boundary again they will in turn create P-waves. This property allows seismologists to determine the nature of the inner core.\n\nThe prediction of S-waves came out of theory in the 1800s. Starting with the stress-strain relationship for an isotropic solid in Einstein notation:\n\nwhere formula_3 is the stress, formula_4 and formula_5 are the Lamé parameters (with formula_5 as the shear modulus), formula_7 is the Kronecker delta, and the strain tensor is defined\n\nfor strain displacement u. Plugging the latter into the former yields:\n\nNewton's 2nd law in this situation gives the \"homogeneous equation of motion\" for seismic wave propagation:\n\nwhere formula_11 is the mass density. Plugging in the above stress tensor gives:\n\nApplying vector identities and making certain approximations gives the seismic wave equation in homogeneous media:\n\nwhere Newton's notation has been used for the time derivative. Taking the curl of this equation and applying vector identities eventually gives:\n\nwhich is simply the wave equation applied to the curl of u with a velocity formula_15 satisfying\n\nThis describes S-wave propagation. Taking the divergence of seismic wave equation in homogeneous media, instead of the curl, yields an equation describing P-wave propagation.\nThe steady-state SH waves are defined by the Helmholtz equation\nwhere k is the wave number.\n\n\n"}
{"id": "3641834", "url": "https://en.wikipedia.org/wiki?curid=3641834", "title": "Sulayman Mountain", "text": "Sulayman Mountain\n\nThe Sulayman Mountain (also known as \"Taht-I-Suleiman\", \"Sulayman Rock\" or \"Sulayman Throne\") is the only World Heritage Site located entirely in the country of Kyrgyzstan (Kyrgyzstan shares the Tian-Shan Silk Road Site with China and Kazakhstan). It is located in the city of Osh and was once a major place of Muslim and pre-Muslim pilgrimage. The rock rises abruptly from the surrounding plains of the Fergana Valley and is a popular place among locals and visitors, with a splendid view.(\"See\" List of World Heritage Sites in Kyrgyzstan)\nThis mountain is thought by some researchers and historians to be the famous landmark of antiquity known as the “Stone Tower”, which Claudius Ptolemy wrote about in his famous work \"Geography\". It marked the midpoint on the ancient Silk Road, the overland trade route taken by caravans between Europe and Asia.\n\nSulayman (Solomon) is a prophet in the Qur'an, and the mountain contains a shrine that supposedly marks his grave. Women who ascend to the shrine on top and crawl though an opening across the holy rock will, according to legend, give birth to healthy children. The trees and bushes on the mountain are draped with numerous \"prayer flags\", small pieces of cloth that are tied to them.\n\nAccording to the UNESCO, the mountain is \"the most complete example of a sacred mountain anywhere in Central Asia, worshipped over several millennia\". The site is still a popular place for local Muslims, with stairs leading up to the highest peak where there stands a small mosque originally built by Babur in 1510. Much of the mosque was reconstructed in the late 20th century.\n\nThe rock also contains the National Historical and Archaeological Museum Complex Sulayman that was built during the Soviet era, showing archaeological findings from the area and its history. The lower slope of the mountain is surrounded by a cemetery.\n"}
{"id": "18780840", "url": "https://en.wikipedia.org/wiki?curid=18780840", "title": "Sussex Marble", "text": "Sussex Marble\n\nSussex Marble is a fossiliferous freshwater limestone material which is prevalent in the Weald Clay of parts of Kent, East Sussex and West Sussex in southeast England. It is also called Petworth Marble, Bethersden Marble or Laughton Stone in relation to villages where it was quarried, and another alternative name is winklestone. It is referred to as \"marble\" as it polishes very well, although it is not a geologically described one as it has not been subject to metamorphosis. The matrix is made up of the shells of freshwater gastropods and \"viviparus\" winkles, similar to but larger than those making Purbeck Marble. The pale calcified remains of the shells are in a matrix of darker material. West Sussex has a good concentration of thin layers of Sussex Marble; beds typically measure no more than thick. There are often two beds—the lower formed of smaller-shelled gastropods than the upper—with a layer of calcareous clay between them.\n\nThe Weald of Kent, near the Sussex border, was the centre of quarrying activity, as the material was most prevalent there. Yeomen who owned their own farms were usually involved. Bethersden village is surrounded by \"small reed-filled and tree-fringed ponds\" formed by the filling over time of old marble workings. In the area, the Perpendicular Gothic towers of the parish churches of Biddenden, Headcorn, Smarden and Tenterden, pavements and paths in Staplehurst, and the Dering Arms, an inn next to Pluckley railway station, all use the material.\n\nIn the early 19th century, Sussex Marble quarried at Petworth rivalled many of the stones which were routinely imported from the continent, in both beauty and quality. A kind of shell marble occurring in the Wealden clay, its quarrying was concentrated on the Egremont estate at Kirdford and there are accounts of industry at nearby Plaistow. It was used in several chimney pieces at Petworth House, and in Edward the Confessor's Chapel in Westminster Abbey the tombs of Edward III and of Richard II and his Queen are both in \"grey Petworth Marble\". At Canterbury Cathedral the Archbishop's chair is an entire piece of the stone. Embellishment of the nave of Chichester Cathedral is in both Purbeck and Petworth Marbles; the latter was used for the pillars of the upper triforium which even then showed \"some decomposition of the shelly particles\". Church fittings such as altars, rails, piers and floors have been made from the material, as have memorial tablets and parts of tombs. Sussex churches with Sussex Marble fonts include St George's at Trotton, St Peter's at Ardingly and St Mary's at West Chiltington. The lychgate at St Mary Magdalene's Church, Bolney, given to the church in 1905, stands on a base of Sussex Marble.\n\nAs the material is not in regular supply, much restoration of earlier Sussex Marble work takes place using Purbeck Marble, which is considered a more stable stone. An example of this practice occurred as early as 1870, when the font at St Margaret's Church in West Hoathly had to be restored but the original Sussex Marble, quarried in Petworth, had run out.\n\nThe industry and workings are long gone although small new rural development around the Surrey/Sussex border occasionally brings up new seams of the stone. The qualities of the material are being rediscovered through British sculptors like Jon Edgar who, after a gap of nearly 200 years, are having to re-discover the ways of working it, its strengths and weaknesses.\n"}
{"id": "5895148", "url": "https://en.wikipedia.org/wiki?curid=5895148", "title": "Trans-Caribbean pipeline", "text": "Trans-Caribbean pipeline\n\nThe Trans-Caribbean gas pipeline (also known as \"Antonio Ricaurte Gas Pipeline\") is a natural gas pipeline between Venezuela and Colombia with proposed extension to Panama and probably to Nicaragua. \n\nThe construction started on 8 July 2006 with presence of presidents Hugo Chávez of Venezuela, Álvaro Uribe of Colombia and Martín Torrijos of Panama. It was inaugurated on 12 October 2007. In November 2009, Colombia reduced exports from 220 million cubic feet a day to 70 million cubic feet a day due to a drought that required an increase of gas-fired power generation to support the decrease in hydro-power plants' reservoirs. On 9 October 2013, the pipeline was attacked, temporarily suspending the supply of natural gas from Colombia to Venezuela. The attack was attributed to FARC rebels. From May 2014 to February 2015, Colombia again suspended gas exports through the pipeline due to drought. When the gas exports resumed, Colombia exported an estimated 50 million cubic feet a day, about half the amount that was exported before May 2014. On 11 June 2015, Petróleos de Venezuela S.A.(PdV) announced that it would not renew the contract to import gas from Colombia, letting the contract expire on 30 June. As a result, the current pipeline is not in use. \n\nThe first stage of the pipeline is long and it runs from Maracaibo in the state of Zulia in Venezuela to Puerto Ballena gas fields in La Guajira, Colombia. At the first stage, the pipeline pumps natural gas from Colombia to Venezuela. Transported gas is used by Petróleos de Venezuela S.A. for injection in its oil reservoirs to boost oil production. Natural gas is supplied by Ecopetrol and Chevron Corporation.\n\nThe construction of the first stage cost US$467 million. Its maximum capacity is 5 billion cubic meters of natural gas per year.\n\nThe operator of the pipeline is Petróleos de Venezuela.\n\nThe original plan of the pipeline was for Colombia to pump gas to Venezuela until 2011, when the direction of the pipeline would be reversed, allowing Venezuela to export gas to Colombia. The reversal of the pipeline was delayed by PdV multiple times, with the most recent date as December 2016. The plan to reverse the pipeline never occurred due to PdV's financial troubles.\n\n\n \n"}
{"id": "2449023", "url": "https://en.wikipedia.org/wiki?curid=2449023", "title": "Upward continuation", "text": "Upward continuation\n\nUpward continuation is a method used in oil exploration and geophysics to estimate the values of a gravitational or magnetic field by using measurements at a lower elevation and extrapolating upward, assuming continuity. This technique is commonly used to merge different measurements to a common level so as to reduce scatter and allow for easier analysis.\n\n\n"}
{"id": "25354636", "url": "https://en.wikipedia.org/wiki?curid=25354636", "title": "Urban dust dome", "text": "Urban dust dome\n\nUrban dust domes are a meteorological phenomenon in which soot, dust, and chemical emissions become trapped in the air above urban spaces. This trapping is a product of local air circulations. Calm surface winds are drawn to urban centers, they then rise above the city and descend slowly on the periphery of the developed core. This cycle is often a cause of smog through photochemical reactions that occur when strong concentrations of the pollutants in this cycle are exposed to solar radiation. These are one result of urban heat islands: pollutants concentrate in a dust dome because convection lifts pollutants into the air, where they remain because of somewhat stable air masses produced by the urban heat island.\n\nThe urban heat island which causes a city to heat up, caps the dust and other particulates at a low level in the atmosphere. If there is not a strong enough wind, then this dome that is created remains intact and causes that heated up air within the urban heat island. Though if the wind does blow strong enough, then this dome is blown downwind causing it to move out of the city.\n"}
{"id": "3625756", "url": "https://en.wikipedia.org/wiki?curid=3625756", "title": "Vito Dumas", "text": "Vito Dumas\n\nVito Dumas (September 26, 1900 – March 28, 1965) was an Argentine single-handed sailor.\n\nOn 27 June 1942, while the world was in the depths of World War II, he set out on a single-handed circumnavigation of the Southern Ocean. He left Buenos Aires in June, sailing \"LEHG II\", a 31-foot ketch an acronym representing \"four names which marked my life\". He had only the most basic and makeshift gear; he had no radio, for fear of being shot as a spy, and was forced to stuff his clothes with newspaper to keep warm.\n\nWith only three landfalls, the legs of his trip were the longest that had been made by a single-hander, and in the most ferocious oceans on the Earth; but most of all, it was a powerful retort to a world which had chosen to divide itself by war. He recounted the experience in his book \"Los Cuarenta Bramadores: La Vuelta al Mundo Por la \"Ruta Imposible\"\" (\"Alone Through The Roaring Forties\").\n\nHe donated his boat to the Argentine Navy for training, but after a few years it was neglected, and was finally wrecked against a pier at the entrance of La Plata's port in 1966. A wealthy Argentine yachtsman paid to have it restored and donated it to the Argentina Naval Museum in Tigre, a coastal river town on a backwater of the River Plate. The LEHG II is now on display in Tigre, which is a short train ride from Buenos Aires.\n\nDumas was the inspiration for an Argentine tango entitled Navegante, written by Jaime Yanin (music) and José Horacio Staffolani (lyrics). It was recorded in Buenos Aires on 5 August 1943 by the Orquesta típica of Carlos di Sarli, featuring Roberto Rufino on vocals.\n\n\n"}
