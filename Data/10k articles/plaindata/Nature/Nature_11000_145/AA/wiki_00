{"id": "10679980", "url": "https://en.wikipedia.org/wiki?curid=10679980", "title": "22° halo", "text": "22° halo\n\nA 22° halo is an optical phenomenon that belongs to the family of ice crystal halos, in the form of a ring with a radius of approximately 22° around the Sun or Moon. When visible around the moon, it is called a moon ring or winter halo. It forms as the sun- or moonlight is refracted in millions of hexagonal ice crystals suspended in the atmosphere. The halo is large; the radius is roughly the size of an outstretched hand at arm's length. A 22° halo may be visible on as many as 100 days per year—much more frequently than rainbows.\n\nEven though it is one of the most common types of halo, the exact shape and orientation of the ice crystals responsible for the 22° halo are still the topic of debate. Hexagonal, randomly oriented columns are usually put forward as the most likely candidate, but this explanation presents problems, such as the fact that the aerodynamic properties of such crystals leads them to be oriented horizontally rather than randomly. Alternative explanations include the involvement of clusters of bullet-shaped ice columns.\n\nAs light passes through the 60° apex angle of the hexagonal ice prisms it is deflected twice resulting in deviation angles ranging from 22° to 50°. The angle of minimum deviation is almost 22° (or more specifically 21.84° on average; 21.54° for red light and 22.37° for blue light). This wavelength-dependent variation in refraction causes the inner edge of the circle to be reddish while the outer edge is bluish.\n\nThe ice crystals in the clouds all deviate the light similarly, but only the ones from the specific ring at 22 degrees contribute to the effect for an observer at a set distance. As no light is refracted at angles smaller than 22°, the sky is darker inside the halo.\n\nAnother phenomenon resulting in a ring around the Sun or Moon—and therefore sometimes confused with the 22° halo—is the corona. Unlike the 22° halo, however, it is produced by water droplets instead of ice crystals and it is much smaller and more colorful.\n\nIn folklore, moon rings are said to warn of approaching storms. Like other ice halos, 22° halos appear when the sky is covered by thin cirrus or cirrostratus clouds that often come a few days before a large storm front. However, the same clouds can also occur without any associated weather change, making a 22° halo unreliable as a sign of bad weather.\n\n"}
{"id": "2513911", "url": "https://en.wikipedia.org/wiki?curid=2513911", "title": "2Mex", "text": "2Mex\n\nAlejandro Ocana, better known by his stage name 2Mex, is a rapper from Los Angeles, California. He is a member of The Visionaries and Of Mexican Descent. He has collaborated with underground hip hop artists such as Jel, Omid, Thavius Beck, Factor, Radioinactive and Isaiah \"Ikey\" Owens. He is a member of the Project Blowed crew, and he is affiliated with Shape Shifters.\n\n2Mex attended the Good Life Cafe open mic nights in 1993. Around that time, he founded Of Mexican Descent with Xololanxinxo.\n\nIn 1998, he released his first album with The Visionaries as well as the first Of Mexican Descent album, and he was featured on the OD (Omid) compilation album \"Beneath the Surface\". In 1999, he was featured (with Xololanxinxo as Of Mexican Descent) on the Fat Jack compilation album \"Cater to the DJ\", and he released his first solo album \"Fake It Till You Make It\" as The Mind Clouders with producer Mums the Word. In 2000, he was featured (as a member of the Afterlife Crew) on the Afterlife Records compilation \"Declaration of an Independent\". He then released \"B-Boys in Occupied Mexico\" on Mean Street in 2001. \"Sweat Lodge Infinite\" was released on Temporary Whatever in 2003. The self-titled album, \"2Mex\", was released in 2004. He released \"My Fanbase Will Destroy You\" on Strange Famous Records in 2010. It features guest appearances from Murs, Prince Po, Busdriver and Nobody.\n\n2Mex is featured in Ava Duvernay's award-winning documentary \"This Is the Life\", chronicling the music movement that was birthed at The Good Life Cafe in South Central, Los Angeles. The Good Life Cafe is the open-mic workshop where he first performed with Of Mexican Descent in 1993.\n\nSolo\n2Mex & Maiselph\n\nThe Visionaries\n\nOf Mexican Descent\n\nThe Mind Clouders\n\nSonGodSuns\n\n$martyr\n\nLook Daggers\n\nThe Returners\n\n2Mex, KeyKool, & LMNO\n\nGuest appearances\n\n"}
{"id": "18226967", "url": "https://en.wikipedia.org/wiki?curid=18226967", "title": "Antiproton Decelerator", "text": "Antiproton Decelerator\n\nThe Antiproton Decelerator (AD) is a storage ring at the CERN laboratory near Geneva. It was built as a successor to the Low Energy Antiproton Ring (LEAR) and started operation in the year 2000. Antiprotons are created by impinging a proton beam from the Proton Synchrotron on a metal target. The AD decelerates the resultant antiprotons to an energy of 5.3 MeV, which are then ejected to one of several connected experiments.\n\nELENA (Extra Low ENergy Antiproton) is a 30 m hexagonal storage ring situated inside the AD complex. It is designed to further decelerate the antiproton beam to an energy of 0.1 MeV for more precise measurements. The first beam circulated ELENA on 18 November 2016. The ring is expected to be fully operational in 2018. GBAR will be the first experiment to use a beam from ELENA, with the rest of the AD experiments following suit in 2019-2020.\n\nATHENA was an antimatter research project that took place at the Antiproton Decelerator. In August 2002, it was the first experiment to produce 50,000 low-energy antihydrogen atoms, as reported in \"Nature\". In 2005, ATHENA was disbanded and many of the former members worked on the subsequent ALPHA experiment.\n\nThe ATHENA apparatus comprises four main subsystems: the antiproton catching trap, the positron accumulator, the antiproton/positron mixing trap, and the antihydrogen annihilation detector. All traps in the experiment are variations on the Penning trap, which uses an axial magnetic field to transversely confine the charged particles, and a series of hollow cylindrical electrodes to trap them axially (Fig. 1a). The catching and mixing traps are adjacent to each other, and coaxial with a 3 T magnetic field from a superconducting solenoid. The positron accumulator has its own magnetic system, also a solenoid, of 0.14 T. A separate cryogenic heat exchanger in the bore of the superconducting magnet cools the catching and mixing traps to about 15 K. The ATHENA apparatus features an open, modular design that allows great experimental flexibility, particularly in introducing large numbers of positrons into the apparatus.\n\nThe catching trap slows, traps, cools, and accumulates antiprotons. To cool antiprotons, the catching trap is first loaded with electrons, which cool by synchrotron radiation in the 3 T magnetic field. Typically, the AD delivers antiprotons having kinetic energy 5.3 MeV and a pulse duration of 200 ns to the experiment at 100 s intervals. The antiprotons are slowed in a thin foil and trapped using a pulsed electric field. The antiprotons lose energy and equilibrate with the cold electrons by Coulomb interaction. The electrons are ejected before mixing the antiprotons with positrons. Each AD shot results in about cold antiprotons for interaction experiments. The positron accumulator slows, traps and accumulates positrons emitted from a radioactive source (1. Bq Na). Accumulation for 300 s yields 1. positrons, 50% of which are successfully transferred to the mixing trap, where they cool by synchrotron radiation.\n\nThe mixing trap has the axial potential configuration of a nested Penning trap (Fig. 1b), which permits two plasmas of opposite charge to come into contact. In ATHENA, the spheroidal positron cloud can be characterized by exciting and detecting axial plasma oscillations. Typical conditions are: stored positrons, a radius of 2 – 2.5 mm, a length of 32 mm, and a maximum density of 2.. Key to the observations reported here is the antihydrogen annihilation detector (Fig. 1a), situated coaxially with the mixing region, between the trap outer radius and the magnet bore. The detector is designed to provide unambiguous evidence for antihydrogen production by detecting the temporally and spatially coincident annihilations of the antiproton and positron when a neutral antihydrogen atom escapes the electromagnetic trap and strikes the trap electrodes. An antiproton typically annihilates into a few charged or neutral pions. The charged pions are detected by two layers of double-sided, position sensitive, silicon microstrips. The path of a charged particle passing through both layers can be reconstructed, and two or more intersecting tracks allow determination of the position, or vertex, of the antiproton annihilation. The uncertainty in vertex determination is approximately 4 mm and is dominated by the unmeasured curvature of the charged pions’ trajectories in the magnetic field. The temporal coincidence window is approximately 5 microseconds. The solid angle coverage of the interaction region is about 80% of 4π.\n\nA positron annihilating with an electron yields two or three photons. The positron detector, comprising 16 rows each containing 12 scintillating, pure CsI crystals, is designed to detect the two-photon events, consisting of two 511 keV photons which are always emitted back-to-back. The energy resolution of the detector is 18% FWHM at 511 keV, and the photo-peak detection efficiency for single photons is about 20%. The maximum readout rate of the whole detector is about 40 Hz. Ancillary detectors include large scintillator paddles external to the magnet, and a thin, position sensitive, silicon diode through which the incident antiproton beam passes before entering the catching trap. To produce antihydrogen atoms, a positron well in the mixing region is filled with about positrons and allowed to cool to the ambient temperature (15 K). The nested trap is then formed around the positron well. Next, approximately 104 antiprotons are launched into the mixing region by pulsing the trap from one potential configuration (dashed line, Fig. 1b) to another (solid line). The mixing time is 190 s, after which all particles are dumped and the process repeated. Events triggering the imaging silicon detector (three sides hit in the outer layer) initiate readout of both the silicon and the CsI modules.\n\nUsing this method, ATHENA could produce - for the first time - several thousands of cold antihydrogen atoms in 2002.\nThe ATHENA collaboration comprised the following institutions:\nThe ATRAP collaboration at CERN developed out of TRAP, a collaboration whose members pioneered cold antiprotons, cold positrons, and first made the ingredients of cold antihydrogen to interact. ATRAP members also pioneered accurate hydrogen spectroscopy and first observed hot antihydrogen atoms.\n\nATRAP is a collaboration between physicists around the world with the goal of creating and experimenting with antihydrogen. ATRAP accumulates positrons emitted from a radioactive Na source. \nThere are two effective ways to slow down the fast positrons by inelastic processes. The ATRAP collaboration initially chose a different method to ATHENA. The positrons which were emitted by the Na were first slowed down with a 10 µm thick titanium foil and then passed through a 2 µm thick tungsten crystal.\nWithin the crystal there is a possibility that a positively charged positron and a negatively charged electron form a Rydberg Positronium atom. In this process, the positrons lose much of their energy so that it is no longer necessary (as in ATHENA) to decelerate further with collisions in gas. When the loosely bound Rydberg positronium atom reaches the Penning trap at the end of the apparatus, it is ionized and the positron is caught in the trap.\n\nSince this method of positron accumulation was not particularly efficient, ATRAP switched to a Surko-type buffer gas accumulator as is now standard in experiments requiring large numbers of positrons. This has led to the storage of the largest ever number of positrons in an Ioffe trap.\n\nUnlike ATHENA, ATRAP has not yet been terminated and can be continuously improved and expanded. ATRAP now has a Ioffe trap, which can store the electrically neutral antihydrogen using a magnetic quadrupole field. This is possible because the magnetic moment of antihydrogen is non-zero. It is intended that laser spectroscopy will be performed on antihydrogen stored in the Ioffe trap.\n\nThe ATRAP collaboration comprises the following institutions:\nASACUSA (Atomic Spectroscopy And Collisions Using Slow Antiprotons) is an experiment testing for CPT-symmetry by laser spectroscopy of antiprotonic helium and microwave spectroscopy of the hyperfine structure of antihydrogen. It also measures atomic and nuclear cross sections of antiprotons on various targets at extremely low energies. The spokesperson for the experiment is Ryugo S. Hayano from the University of Tokyo. It was originally proposed in 1997.\n\nThe Antiproton Cell Experiment (ACE) started in 2003. It aims to assess fully the effectiveness and suitability of antiprotons for cancer therapy.\n\nThe ALPHA experiment is designed to trap neutral antihydrogen in a magnetic trap, and conduct experiments on them. The ultimate goal of this endeavour is to test CPT symmetry through comparison of the atomic spectra of hydrogen and antihydrogen (see hydrogen spectral series). The ALPHA collaboration consists of some former members of the ATHENA collaboration (the first group to produce cold antihydrogen, in 2002), as well as a number of new members.\n\nALPHA faces several challenges. Magnetic traps – wherein neutral atoms are trapped using their magnetic moments – are notoriously weak; only atoms with kinetic energies equivalent to less than one kelvin may be trapped. The cold antihydrogen created first in 2002 by the ATHENA and the ATRAP collaborations was produced by merging cold plasmas of positrons (also called antielectrons) and antiprotons. While this method has been quite successful, it creates antiatoms with kinetic energies too large to be trapped. Furthermore, to do laser spectroscopy on these anti-atoms, it is important that they are in their ground state, something which does not seem to be the case for the majority of the anti-atoms created thus far.\n\nAntiprotons are received by the Antiproton Decelerator and are 'mixed' with positrons from a specially-designed positron accumulator in a versatile Penning trap. The central region where the mixing and thus antihydrogen formation takes place is surrounded by a superconducting octupole magnet and two axially separated short solenoids \"mirror-coils\" to form a \"minimum-B\" magnetic trap. Once trapped antihydrogen can be subjected to detailed study and be compared to hydrogen.\n\nIn order to detect trapped antihydrogen atoms ALPHA also comprises a silicon vertex detector. This cylindrically shaped detector consists of three layers of silicon panels (strips). Each panel acts as a position sensitive detector for charged particles passing through. By recording how the panels are excited ALPHA can reconstruct the tracks of charged particles traveling through their detector. When an antiproton annihilates (disintegrates) the process typically results in the emission of 3-4 charged pions. These can be observed by the ALPHA detector and by reconstructing their tracks through the detector their origin, and thus the location of the annihilation, can be determined. These tracks are quite distinct from the tracks of cosmic rays which are also detected but are of high energy and pass straight through the detector. By carefully analyzing the tracks ALPHA distinguishes between cosmic rays and antiproton annihilations.\n\nTo detect successful trapping the ALPHA trap magnet that created the minimum B-field was designed to allow it to be quickly and repeatedly de-energized. The currents' decay during de-energization has a characteristic time of 9 ms, orders of magnitude faster than similar systems. This fast turn-off and the ability to suppress false signal from cosmic rays should allow ALPHA to detect the release of even a single trapped antihydrogen atom during de-energization of the trap.\n\nIn order to make antihydrogen cold enough to be trapped the ALPHA collaboration has implemented a novel technique, well known from atomic physics, called evaporative cooling. The motivation for this is that one of the main challenges of trapping antihydrogen is to make it cold enough. State-of-the art minimum-B traps like the one ALPHA comprises have depths in temperature units of order one Kelvin. As no readily available techniques exist to cool antihydrogen, the constituents must be cold and kept cold for the formation. Antiprotons and positrons are not easily cooled to cryogenic temperatures and the implementation of evaporative cooling is thus an important step towards antihydrogen trapping.\n\nALPHA is presently studying the gravitational properties of antimatter. A preliminary experiment in 2013 found that the gravitational mass of antihydrogen atoms was between -65 and 110 times their inertial mass, leaving considerable room for refinement using larger numbers of colder antihydrogen atoms.\n\nThe ALPHA collaboration comprises the following institutions:\nAEgIS (Antimatter Experiment: gravity, Interferometry, Spectroscopy), is an experiment currently being set up at the Antiproton Decelerator.\n\nAEgIS would attempt to determine if gravity affects antimatter in the same way it affects matter by testing its effect on an antihydrogen beam. The first phase of the experiment creates antihydrogen: antiprotons from the Antiproton Decelerator are coupled with positrons, making a pulse of horizontally-travelling antihydrogen atoms. These atoms are sent through a series of diffraction gratings, ultimately hitting a surface and thus annihilating. The points where the antihydrogen annihilates are measured with a precise detector. Areas behind the gratings are shadowed, while those behind the slits are not. The annihilation points reproduce a periodic pattern of light and shadowed areas. Using this pattern, it can be measured how many atoms of different velocities drop during horizontal flight. Therefore, the Earth's gravitational force on antihydrogen can be determined. It was originally proposed in 2007. Construction of the main apparatus was completed in 2012.\n\nThe AEgIS collaboration comprises the following institutions:\n\nGBAR (Gravitational Behaviour of Anti hydrogen at Rest), is a multinational collaboration at the Antiproton Decelerator (AD) of CERN.\n\nThe GBAR project, aims to measure the free fall acceleration of ultracold neutral anti hydrogen atoms in the terrestrial gravitational field. The experiment consists preparing anti hydrogen ions (one antiproton and two positrons) and sympathetically cooling them with Be ions to less than 10 μK. The ultracold ions will then be photoionized just above threshold, and the free fall time over a known distance measured.\n\nThe GBAR collaboration comprises the following institutions:\n\nBASE (Baryon Antibaryon Symmetry Experiment), is a multinational collaboration at the Antiproton Decelerator (AD) of CERN.\n\nThe goal of the Japanese/German BASE collaboration are high-precision investigations of the fundamental properties of the antiproton, namely the charge-to-mass ratio and the magnetic moment. To this end single antiprotons are stored in an advanced Penning trap system, which has a double-trap system at its core. It consists of a precision trap and an analysis trap. The precision trap is for high precision frequency measurements, the analysis trap has a strong magnetic field inhomogeneity superimposed, which is used for single particle spin flip spectroscopy. By measuring the spin flip rate as a function of the frequency of an externally applied magnetic-drive, a resonance curve is obtained. Together with a measurement of the cyclotron frequency, the magnetic moment is extracted.\n\nThe BASE collaboration developed techniques to observe the first spin flips of a single trapped proton and applied the double-trap technique to measure the magnetic moment of the proton with a fractional precision of three parts in a billion, being the most precise measurement of this fundamental property of the proton. The application of the technique to measure the magnetic moment of the antiproton with similar precision will improve the precision of this value by at least a factor of 1000, and will provide one of the most stringent tests of CPT invariance to date.\n\nThe BASE collaboration comprises the following institutions:\n\n\n"}
{"id": "31306060", "url": "https://en.wikipedia.org/wiki?curid=31306060", "title": "Antony Froggatt", "text": "Antony Froggatt\n\nAntony Froggatt is an energy policy consultant and a senior research fellow at Chatham House. He is co-author of \"The World Nuclear Industry Status Reports\".\n\nAntony Froggatt is a senior research fellow in the Energy, Environment and Development Programme at Chatham House and has been a freelance energy policy consultant since 1997, based in London. Previously, he was the Greenpeace International Nuclear Policy Campaigner.\n\nWith Mycle Schneider, Froggatt is co-author of \"The World Nuclear Industry Status Reports\", which suggest that nuclear power will continue to decline. Froggatt and Schneider wrote the \"Systems for Change\" report for the Heinrich Böll Foundation in 2010. Froggatt has commented extensively on the 2011 Japanese nuclear accidents.<ref>\n"}
{"id": "18951556", "url": "https://en.wikipedia.org/wiki?curid=18951556", "title": "Arctic Ocean", "text": "Arctic Ocean\n\nThe Arctic Ocean is the smallest and shallowest of the world's five major oceans. The International Hydrographic Organization (IHO) recognizes it as an ocean, although some oceanographers call it the Arctic Mediterranean Sea or simply the Arctic Sea, classifying it a mediterranean sea or an estuary of the Atlantic Ocean. It is also seen as the northernmost part of the all-encompassing World Ocean.\n\nLocated mostly in the Arctic north polar region in the middle of the Northern Hemisphere, the Arctic Ocean is almost completely surrounded by Eurasia and North America. It is partly covered by sea ice throughout the year and almost completely in winter. The Arctic Ocean's surface temperature and salinity vary seasonally as the ice cover melts and freezes; its salinity is the lowest on average of the five major oceans, due to low evaporation, heavy fresh water inflow from rivers and streams, and limited connection and outflow to surrounding oceanic waters with higher salinities. The summer shrinking of the ice has been quoted at 50%. The US National Snow and Ice Data Center (NSIDC) uses satellite data to provide a daily record of Arctic sea ice cover and the rate of melting compared to an average period and specific past years.\n\nHuman habitation in the North American polar region goes back at least 50,000–17,000 years ago, during the Wisconsin glaciation. At this time, falling sea levels allowed people to move across the Bering land bridge that joined Siberia to north west North America (Alaska), leading to the Settlement of the Americas.\n\nPaleo-Eskimo groups included the Pre-Dorset (c. 3200 – 850 B.C.); the Saqqaq culture of Greenland (2500 – 800 B.C.); the Independence I and Independence II cultures of northeastern Canada and Greenland (c. 2400 – 1800 B.C. and c. 800 – 1 B.C.); the Groswater of Labrador and Nunavik, and the Dorset culture (500 B.C. to 1500 A.D.), which spread across Arctic North America. The Dorset were the last major Paleo-Eskimo culture in the Arctic before the migration east from present-day Alaska of the Thule, the ancestors of the modern Inuit.\n\nThe Thule Tradition lasted from about 200 B.C. to 1600 A.D. around the Bering Strait, the Thule people being the prehistoric ancestors of the Inuit who now live in Northern Labrador.\n\nFor much of European history, the north polar regions remained largely unexplored and their geography conjectural. Pytheas of Massilia recorded an account of a journey northward in 325 BC, to a land he called \"Eschate Thule\", where the Sun only set for three hours each day and the water was replaced by a congealed substance \"on which one can neither walk nor sail\". He was probably describing loose sea ice known today as \"growlers\" or \"bergy bits\"; his \"Thule\" was probably Norway, though the Faroe Islands or Shetland have also been suggested.\n\nEarly cartographers were unsure whether to draw the region around the North Pole as land (as in Johannes Ruysch's , or Gerardus Mercator's ) or water (as with Martin Waldseemüller's ). The fervent desire of European merchants for a northern passage, the Northern Sea Route or the Northwest Passage, to \"Cathay\" (China) caused water to win out, and by 1723 mapmakers such as Johann Homann featured an extensive \"Oceanus Septentrionalis\" at the northern edge of their charts.\n\nThe few expeditions to penetrate much beyond the Arctic Circle in this era added only small islands, such as Novaya Zemlya (11th century) and Spitzbergen (1596), though since these were often surrounded by pack-ice, their northern limits were not so clear. The makers of navigational charts, more conservative than some of the more fanciful cartographers, tended to leave the region blank, with only fragments of known coastline sketched in.\n\nThis lack of knowledge of what lay north of the shifting barrier of ice gave rise to a number of conjectures. In England and other European nations, the myth of an \"Open Polar Sea\" was persistent. John Barrow, longtime Second Secretary of the British Admiralty, promoted exploration of the region from 1818 to 1845 in search of this.\n\nIn the United States in the 1850s and 1860s, the explorers Elisha Kane and Isaac Israel Hayes both claimed to have seen part of this elusive body of water. Even quite late in the century, the eminent authority Matthew Fontaine Maury included a description of the Open Polar Sea in his textbook \"The Physical Geography of the Sea\" (1883). Nevertheless, as all the explorers who travelled closer and closer to the pole reported, the polar ice cap is quite thick, and persists year-round.\n\nFridtjof Nansen was the first to make a nautical crossing of the Arctic Ocean, in 1896. The first surface crossing of the ocean was led by Wally Herbert in 1969, in a dog sled expedition from Alaska to Svalbard, with air support. The first nautical transit of the north pole was made in 1958 by the submarine USS Nautilus, and the first surface nautical transit occurred in 1977 by the icebreaker NS Arktika.\n\nSince 1937, Soviet and Russian manned drifting ice stations have extensively monitored the Arctic Ocean. Scientific settlements were established on the drift ice and carried thousands of kilometers by ice floes.\n\nIn World War II, the European region of the Arctic Ocean was heavily contested: the Allied commitment to resupply the Soviet Union via its northern ports was opposed by German naval and air forces.\n\nSince 1954 commercial airlines have flown over the Arctic Ocean (see Polar route).\n\nThe Arctic Ocean occupies a roughly circular basin and covers an area of about , almost the size of Antarctica. The coastline is long. It is surrounded by the land masses of Eurasia, North America, Greenland, and by several islands.\n\nIt is generally taken to include Baffin Bay, Barents Sea, Beaufort Sea, Chukchi Sea, East Siberian Sea, Greenland Sea, Hudson Bay, Hudson Strait, Kara Sea, Laptev Sea, White Sea and other tributary bodies of water. It is connected to the Pacific Ocean by the Bering Strait and to the Atlantic Ocean through the Greenland Sea and Labrador Sea.\n\nCountries bordering the Arctic Ocean are: Russia, Norway, Iceland, Greenland, Canada and the United States.\n\nThere are several ports and harbors around the Arctic Ocean\n\nIn Alaska, the main ports are Barrow () and Prudhoe Bay ().\n\nIn Canada, ships may anchor at Churchill (Port of Churchill) () in Manitoba, Nanisivik (Nanisivik Naval Facility) () in Nunavut, Tuktoyaktuk () or Inuvik () in the Northwest Territories.\n\nIn Greenland, the main port is at Nuuk (Nuuk Port and Harbour) ().\n\nIn Norway, Kirkenes () and Vardø () are ports on the mainland. Also, there is Longyearbyen () on Svalbard, a Norwegian archipelago, next to Fram Strait.\n\nIn Russia, major ports sorted by the different sea areas are:\n\nThe ocean's Arctic shelf comprises a number of continental shelves, including the Canadian Arctic shelf, underlying the Canadian Arctic Archipelago, and the Russian continental shelf, which is sometimes simply called the \"Arctic Shelf\" because it is greater in extent. The Russian continental shelf consists of three separate, smaller shelves, the Barents Shelf, Chukchi Sea Shelf and Siberian Shelf. Of these three, the Siberian Shelf is the largest such shelf in the world. The Siberian Shelf holds large oil and gas reserves, and the Chukchi shelf forms the border between Russian and the United States as stated in the USSR–USA Maritime Boundary Agreement. The whole area is subject to international territorial claims.\n\nAn underwater ridge, the Lomonosov Ridge, divides the deep sea North Polar Basin into two oceanic basins: the Eurasian Basin, which is between deep, and the Amerasian Basin (sometimes called the North American, or Hyperborean Basin), which is about deep. The bathymetry of the ocean bottom is marked by fault block ridges, abyssal plains, ocean deeps, and basins. The average depth of the Arctic Ocean is . The deepest point is Litke Deep in the Eurasian Basin, at .\n\nThe two major basins are further subdivided by ridges into the Canada Basin (between Alaska/Canada and the Alpha Ridge), Makarov Basin (between the Alpha and Lomonosov Ridges), Amundsen Basin (between Lomonosov and Gakkel ridges), and Nansen Basin (between the Gakkel Ridge and the continental shelf that includes the Franz Josef Land).\n\nIn large parts of the Arctic Ocean, the top layer (about ) is of lower salinity and lower temperature than the rest. It remains relatively stable, because the salinity effect on density is bigger than the temperature effect. It is fed by the freshwater input of the big Siberian and Canadian streams (Ob, Yenisei, Lena, Mackenzie), the water of which quasi floats on the saltier, denser, deeper ocean water. Between this lower salinity layer and the bulk of the ocean lies the so-called halocline, in which both salinity and temperature are rising with increasing depth.\nBecause of its relative isolation from other oceans, the Arctic Ocean has a uniquely complex system of water flow. It is classified as a mediterranean sea, which as “a part of the world ocean which has only limited communication with the major ocean basins (these being the Pacific, Atlantic, and Indian Oceans) and where the circulation is dominated by thermohaline forcing”. The Arctic Ocean has a total volume of 18.07×10 km, equal to about 1.3% of the World Ocean. Mean surface circulation is predominately cyclonic on the Eurasian side and anticyclonic in the Canadian Basin.\n\nWater enters from both the Pacific and Atlantic Oceans and can be divided into three unique water masses. The deepest water mass is called Arctic Bottom Water and begins around depth. It is composed of the densest water in the World Ocean and has two main sources: Arctic shelf water and Greenland Sea Deep Water. Water in the shelf region that begins as inflow from the Pacific passes through the narrow Bering Strait at an average rate of 0.8 Sverdrups and reaches the Chukchi Sea. During the winter, cold Alaskan winds blow over the Chukchi Sea, freezing the surface water and pushing this newly formed ice out to the Pacific. The speed of the ice drift is roughly 1–4 cm/s. This process leaves dense, salty waters in the sea that sink over the continental shelf into the western Arctic Ocean and create a halocline.\n\nThis water is met by Greenland Sea Deep Water, which forms during the passage of winter storms. As temperatures cool dramatically in the winter, ice forms and intense vertical convection allows the water to become dense enough to sink below the warm saline water below. Arctic Bottom Water is critically important because of its outflow, which contributes to the formation of Atlantic Deep Water. The overturning of this water plays a key role in global circulation and the moderation of climate.\n\nIn the depth range of is a water mass referred to as Atlantic Water. Inflow from the North Atlantic Current enters through the Fram Strait, cooling and sinking to form the deepest layer of the halocline, where it circles the Arctic Basin counter-clockwise. This is the highest volumetric inflow to the Arctic Ocean, equalling about 10 times that of the Pacific inflow, and it creates the Arctic Ocean Boundary Current. It flows slowly, at about 0.02 m/s. Atlantic Water has the same salinity as Arctic Bottom Water but is much warmer (up to 3 °C). In fact, this water mass is actually warmer than the surface water, and remains submerged only due to the role of salinity in density. When water reaches the basin it is pushed by strong winds into a large circular current called the Beaufort Gyre. Water in the Beaufort Gyre is far less saline than that of the Chukchi Sea due to inflow from large Canadian and Siberian rivers.\n\nThe final defined water mass in the Arctic Ocean is called Arctic Surface Water and is found from . The most important feature of this water mass is a section referred to as the sub-surface layer. It is a product of Atlantic water that enters through canyons and is subjected to intense mixing on the Siberian Shelf. As it is entrained, it cools and acts a heat shield for the surface layer. This insulation keeps the warm Atlantic Water from melting the surface ice. Additionally, this water forms the swiftest currents of the Arctic, with speed of around 0.3–0.6 m/s. Complementing the water from the canyons, some Pacific water that does not sink to the shelf region after passing through the Bering Strait also contributes to this water mass.\n\nWaters originating in the Pacific and Atlantic both exit through the Fram Strait between Greenland and Svalbard Island, which is about deep and wide. This outflow is about 9 Sv. The width of the Fram Strait is what allows for both inflow and outflow on the Atlantic side of the Arctic Ocean. Because of this, it is influenced by the Coriolis force, which concentrates outflow to the East Greenland Current on the western side and inflow to the Norwegian Current on the eastern side. Pacific water also exits along the west coast of Greenland and the Hudson Strait (1–2 Sv), providing nutrients to the Canadian Archipelago.\n\nAs noted, the process of ice formation and movement is a key driver in Arctic Ocean circulation and the formation of water masses. With this dependence, the Arctic Ocean experiences variations due to seasonal changes in sea ice cover. Sea ice movement is the result of wind forcing, which is related to a number of meteorological conditions that the Arctic experiences throughout the year. For example, the Beaufort High—an extension of the Siberian High system—is a pressure system that drives the anticyclonic motion of the Beaufort Gyre. During the summer, this area of high pressure is pushed out closer to its Siberian and Canadian sides. In addition, there is a sea level pressure (SLP) ridge over Greenland that drives strong northerly winds through the Fram Strait, facilitating ice export. In the summer, the SLP contrast is smaller, producing weaker winds. A final example of seasonal pressure system movement is the low pressure system that exists over the Nordic and Barents Seas. It is an extension of the Icelandic Low, which creates cyclonic ocean circulation in this area. The low shifts to center over the North Pole in the summer. These variations in the Arctic all contribute to ice drift reaching its weakest point during the summer months. There is also evidence that the drift is associated with the phase of the Arctic Oscillation and Atlantic Multidecadal Oscillation.\n\nMuch of the Arctic Ocean is covered by sea ice that varies in extent and thickness seasonally. The mean extent of the ice has been decreasing since 1980 from the average winter value of at a rate of 3% per decade. The seasonal variations are about with the maximum in April and minimum in September. The sea ice is affected by wind and ocean currents, which can move and rotate very large areas of ice. Zones of compression also arise, where the ice piles up to form pack ice.\n\nIcebergs occasionally break away from northern Ellesmere Island, and icebergs are formed from glaciers in western Greenland and extreme northeastern Canada. Icebergs are not sea ice but may becoming embedded in the pack ice. Icebergs pose a hazard to ships, of which the \"Titanic\" is one of the most famous. The ocean is virtually icelocked from October to June, and the superstructure of ships are subject to icing from October to May. Before the advent of modern icebreakers, ships sailing the Arctic Ocean risked being trapped or crushed by sea ice (although the \"Baychimo\" drifted through the Arctic Ocean untended for decades despite these hazards).\n\nUnder the influence of the Quaternary glaciation, the Arctic Ocean is contained in a polar climate characterized by persistent cold and relatively narrow annual temperature ranges. Winters are characterized by the polar night, extreme cold, frequent low-level temperature inversions, and stable weather conditions. Cyclones are only common on the Atlantic side. Summers are characterized by continuous daylight (midnight sun), and temperatures can rise above the melting point (. Cyclones are more frequent in summer and may bring rain or snow. It is cloudy year-round, with mean cloud cover ranging from 60% in winter to over 80% in summer.\n\nThe temperature of the surface of the Arctic Ocean is fairly constant, near the freezing point of seawater. Because the Arctic Ocean consists of saltwater, the temperature must reach before freezing occurs.\n\nThe density of sea water, in contrast to fresh water, increases as it nears the freezing point and thus it tends to sink. It is generally necessary that the upper of ocean water cools to the freezing point for sea ice to form. In the winter the relatively warm ocean water exerts a moderating influence, even when covered by ice. This is one reason why the Arctic does not experience the extreme temperatures seen on the Antarctic continent.\n\nThere is considerable seasonal variation in how much pack ice of the Arctic ice pack covers the Arctic Ocean. Much of the Arctic ice pack is also covered in snow for about 10 months of the year. The maximum snow cover is in March or April — about over the frozen ocean.\n\nThe climate of the Arctic region has varied significantly in the past. As recently as 55 million years ago, during the Paleocene–Eocene Thermal Maximum, the region reached an average annual temperature of . The surface waters of the northernmost Arctic Ocean warmed, seasonally at least, enough to support tropical lifeforms (the dinoflagellates \"Apectodinium augustum\") requiring surface temperatures of over .\n\nEndangered marine species in the Arctic Ocean include walruses and whales. The area has a fragile ecosystem which is slow to change and slow to recover from disruptions or damage. Lion's mane jellyfish are abundant in the waters of the Arctic, and the banded gunnel is the only species of gunnel that lives in the ocean.\n\nThe Arctic Ocean has relatively little plant life except for phytoplankton. Phytoplankton are a crucial part of the ocean and there are massive amounts of them in the Arctic, where they feed on nutrients from rivers and the currents of the Atlantic and Pacific oceans. During summer, the sun is out day and night, thus enabling the phytoplankton to photosynthesize for long periods of time and reproduce quickly. However, the reverse is true in winter when they struggle to get enough light to survive.\n\nPetroleum and natural gas fields, placer deposits, polymetallic nodules, sand and gravel aggregates, fish, seals and whales can all be found in abundance in the region.\n\nThe political dead zone near the center of the sea is also the focus of a mounting dispute between the United States, Russia, Canada, Norway, and Denmark. It is significant for the global energy market because it may hold 25% or more of the world's undiscovered oil and gas resources.\n\nThe Arctic ice pack is thinning, and in many years there is also a seasonal hole in the ozone layer.\nReduction of the area of Arctic sea ice reduces the planet's average albedo, possibly resulting in global warming in a positive feedback mechanism. Research shows that the Arctic may become ice free for the first time in human history within a few years or by 2040. Estimates vary for when the last time the Arctic was ice free: 65 million years ago when fossils indicate that plants existed there to as few as 5,500 years ago; ice and ocean cores going back 8000 years to the last warm period or 125,000 during the last intraglacial period.\n\nWarming temperatures in the Arctic may cause large amounts of fresh meltwater to enter the north Atlantic, possibly disrupting global ocean current patterns. Potentially severe changes in the Earth's climate might then ensue.\n\nAs the extent of sea ice diminishes and sea level rises, the effect of storms such as the Great Arctic Cyclone of 2012 on open water increases, as does possible salt-water damage to vegetation on shore at locations such as the Mackenzie's river delta as stronger storm surges become more likely.\n\nSea ice, and the cold conditions it sustains, serves to stabilize methane deposits on and near the shoreline, preventing the clathrate breaking down and outgassing methane into the atmosphere, causing further warming. Melting of this ice may release large quantities of methane, a powerful greenhouse gas into the atmosphere, causing further warming in a strong positive feedback cycle and; marine genus and species to become extinct.\n\nOther environmental concerns relate to the radioactive contamination of the Arctic Ocean from, for example, Russian radioactive waste dump sites in the Kara Sea and Cold War nuclear test sites such as Novaya Zemlya. In addition, Shell planned to drill exploratory wells in the Chukchi and Beaufort seas during the summer of 2012, which environmental groups filed a lawsuit about in an attempt to protect native communities, endangered wildlife, and the Arctic Ocean in the event of a major oil spill.\n\nOn 16 July 2015, five nations (United States of America, Russia, Canada, Norway, Denmark/Greenland) signed a declaration committing to keep their fishing vessels out of a 1.1 million square mile zone in the central Arctic Ocean near the North Pole. The agreement calls for those nations to refrain from fishing there until there is better scientific knowledge about the marine resources and until a regulatory system is in place to protect those resources.\n\n\n"}
{"id": "41420172", "url": "https://en.wikipedia.org/wiki?curid=41420172", "title": "Beta attenuation monitoring", "text": "Beta attenuation monitoring\n\nBeta attenuation monitoring (BAM) is a widely used air monitoring technique employing the absorption of beta radiation by solid particles extracted from air flow. This technique allows for the detection of PM and PM, which are monitored by most air pollution regulatory agencies. The main principle is based on a kind of Bouguer (Lambert–Beer) law: the amount by which the flow of beta radiation (electrons) is attenuated by a solid matter is exponentially dependent on its mass and not on any other feature (such as density, chemical composition or some optical or electrical properties) of this matter. So, the air is drawn from outside of the detector through an \"infinite\" (cycling) ribbon made from some filtering material so that the particles are collected on it. There are two sources of beta radiation placed one before and one after the region where air flow passes through the ribbon leaving particles on it; and there are also two detectors on the opposite side of the ribbon, facing the detectors. The sources' intensity and detectors' sensitivity being the same (or corrected with appropriate calibration lookup table), the intensity of beta rays detected by one of detectors is compared to that of the other. Thus one can deduce how much mass has the ribbon acquired upon being exposed to air flow; knowing the drain velocity, actual particle mass concentration in air could be assessed.\n\nThe radiation source can be a gas chamber, filled with Kr gas, or a pieces of C-rich polymer plastic, such as PMMA. Detector is simply a Geiger–Mueller counter. The particulate matter content measured is affected by the moisture content in the air, unfortunately.\n\nTo discriminate between particle of different sizes (e. g., between PM and PM), some preliminary separation could be accomplished, for example, by cyclone battery.\n\nA similar method exists, where instead of beta particle flow an X-ray Fluorescence Spectroscopic monitoring is applied on the either side of air flow contact with the ribbon. This allows to obtain not only cumulative measurement of particle mass, but also to detect their average chemical composition (technique works for potassium and elements heavier than it).\n\n"}
{"id": "7470124", "url": "https://en.wikipedia.org/wiki?curid=7470124", "title": "Bridge River Ocean", "text": "Bridge River Ocean\n\nThe Bridge River Ocean was an ancient ocean that existed between North America and the Insular Islands during the Paleozoic time. Similar to the earlier Slide Mountain Ocean, the Bridge River Ocean had a subduction zone on the ocean floor called the Insular Trench. The closure of the Bridge River Ocean occurred about 115 million years ago, during the mid Cretaceous period.\n\nThe namesake of the Bridge River Ocean is the Bridge River in the Canadian province of British Columbia, about 100 miles north of the city of Vancouver.\n\n"}
{"id": "4420711", "url": "https://en.wikipedia.org/wiki?curid=4420711", "title": "CLaMS", "text": "CLaMS\n\nCLaMS (Chemical Lagrangian Model of the Stratosphere) is a modular chemistry transport model (CTM) system developed at Forschungszentrum Jülich, Germany. CLaMS was first described by McKenna et al. (2000a,b) and was expanded into three dimensions by Konopka et al. (2004). CLaMS has been employed in recent European field campaigns THESEO, EUPLEX, TROCCINOX SCOUT-O3, and RECONCILE with a focus on simulating ozone depletion and water vapour transport. \n\nMajor strengths of CLaMS in comparison to other CTMs are \n\nUnlike other CTMs (e.g. SLIMCAT, REPROBUS), CLaMS operates on a Lagrangian model grid (see section about model grids in general circulation model): an air parcel is described by three space coordinates and a time coordinate. The time evolution path that an air parcels traces in space is called a trajectory. A specialised mixing scheme ensures that physically realistic diffusion is imposed on an ensemble of\ntrajectories in regions of high wind shear. \n\nCLaMS operates on arbitrarily resolved horizontal grids. The space coordinates are latitude, longitude and potential temperature.\n\nCLaMS is composed of four modules and several preprocessors. The four modules are\n\nIntegration of trajectories with 4th order Runge-Kutta method, integration time step 30 minutes. Vertical displacement of trajectories is calculated from radiation budget. \n\nChemistry is based on the ASAD chemistry code of the University of Cambridge. More than 100 chemical reactions involving 40+ chemical\nspecies are considered. Integration time step is 10 minutes, species\ncan be combined into chemical families to facilitate integration. The\nmodule includes a radiative transfer model for the determination of\nphotolysis rates. The module also includes heterogeneous reactions on\nNAT, ice and liquid particle surfaces.\n\nMixing is based on grid deformation of quasi uniform air parcel\ndistributions. The contraction or elongation factors of the distances\nto neighboring air parcels are examined: if a critical elongation\n(contraction) is reached, new air parcels are introduced (taken away).\nThis way, anisotropic diffusion is simulated in a physically realistic\nmanner.\n\nLagrangian sedimentation is calculated by following individual nitric\nacid trihydrate (NAT) particles that may grow or shrink by the uptake\nor release of HNO from/to the gas phase. These particle parcels are\nsimulated independently from the Lagrangian air parcels. Their\ntrajectories are determined using the horizontal winds and their\nvertical settling velocity that depends on the size of the individual\nparticles. NAT particles are nucleated assuming a constant nucleation\nrate and they evaporate where temperatures grow too high. With this,\na vertical redistribution of HNO (denitrification and\nrenitrification) is determined.\n\nA chemical transport model does not simulate the dynamics of the atmosphere. For CLaMS, the following meteorological data sets have been used\n\nTo initialize the chemical fields in CLaMS, data from a large variety of instruments have provided data.\n\nIf no observations are present, the chemical fields can be initialised\nfrom two-dimensional chemical models, chemistry-climate models,\nclimatologies, or from correlations between chemical species or\nchemical species and dynamical variables.\n\n\n\nThe details of the model CLaMS are well documented and published in the scientific literature. \n\n"}
{"id": "7760240", "url": "https://en.wikipedia.org/wiki?curid=7760240", "title": "Carbon Disclosure Project", "text": "Carbon Disclosure Project\n\nThe CDP (formerly the Carbon Disclosure Project) is an organisation based in the United Kingdom which supports companies and cities to disclose the environmental impact of major corporations. It aims to make environmental reporting and risk management a business norm, and drive disclosure, insight and action towards a sustainable economy. Since 2002 over 6,000 companies have publicly disclosed environmental information through CDP. \n\nCDP piggybacked on GRI’s concept of environmental disclosure in 2002, focusing on individual companies rather than on nations. At the time CDP had just 35 investors signing its request for climate information and 245 companies responding. Today, nearly a fifth of global greenhouse gas emissions are reported through CDP.\n\nCDP works with investors, companies, cities, states and regions towards a well below 2 degree, water secure world. \n\nSome corporations have higher greenhouse gas emissions than individual nation states. Some leading companies have moved to become carbon neutral, but for others there is the scope to reduce energy usage and greenhouse gas-emissions through the adoption of energy-efficiency methods and business planning.\n\nCDP works with over 6000 corporations, as well as over 550 cities and 100 states and regions to help them ensure that an effective carbon emissions reductions strategy is made integral to their operations. The collection of self-reported data from thousands of companies is supported by over 800 institutional investors with US$100 trillion in assets. CDP operates from Berlin, New York, London and has partners in 18 of the world's major economies which help deliver the programme globally. It has:\n\n\nMuch of the data elicited has never been collected before. This information is helpful to investors, corporations, and regulators in making informed decisions on taking action towards a sustainable economy by measuring and understanding their environmental impact and taking meaningful steps to address and limit their risk to climate change, deforestation and water security. \n\nCDP operates in most major economies worldwide and channels information and progress through individual programs. These are: Climate Change, Water, Supply Chain, Forests and Cities. Its Carbon Action initiative encourages investors to accelerate carbon reduction in high emitting industries and to implement emissions reducing projects that generate positive return on investment.\n\nCDP's climate change program aims to reduce companies’ greenhouse gas emissions and mitigate climate change risk. CDP requests information on climate risks and low carbon opportunities from the world’s largest companies on behalf of over 800 institutional investor signatories with a combined US$100 trillion in assets.\n\nIn 2014, 573 investors used the CDP Water program, collectively representing US$60 trillion in assets. The program motivates companies to disclose and reduce their environmental impacts by using the power of investors and companies. \n\nIn 2016, some 90 organizations, representing over US$2.5 trillion of purchasing power requested that their suppliers disclose information on how they are approaching climate and water risks and opportunities. Data was gathered from over 4,000 suppliers worldwide, who reported over US$12 billion worth of savings from emission reduction activities. \n\nCDP's forests program acts on behalf of over 290 signatory investors representing over US$20 trillion in assets. CDP collects information from companies through the lens of the four agricultural commodities responsible for most deforestation: timber, palm oil, cattle and soy. CDP’s forests program was first set up by the Global Canopy Programme Global Canopy Programme which remains a prime funder for the program.\n\nCDP Cities provides the global platform for cities to measure, manage and disclose their environmental data. More than 500 cities are now measuring and disclosing environmental data on an annual basis. The potential and need for this program is enormous since soon over half of the world’s population will live in cities. CDP Cities provides an easy to use global platform based upon a simple questionnaire that allows city governments to publicly disclose their greenhouse gas emission data. One of the greatest values of the annual report, first released in June 2011, is to city leaders who can identify peers who are addressing similar risks and issues with new and innovative strategies for reducing carbon emissions and for mitigating risk from climate change.\n\nCarbon Action is an investor-led initiative which shows how companies in investment portfolios are managing carbon emissions and energy efficiency. \n\nOver 300 investors with US$25 trillion in assets under management ask the world’s highest emitting companies to take three specific actions in response to climate change:\n\n\nIn 2016, the Carbon Action request went to over 1300 of the highest emitters.\n\nCDP launched a new research series at the beginning of 2015, taking a sector by sector approach.\n\nCDP recognizes companies with high-quality disclosure in its annual scoring process, with top companies making it onto CDP's A-List.\n\nScores are calculated according to a standardized methodology which measures whether and how well, a company responds to each question. A company goes through four main steps, starting with disclosure of their current position, moving to awareness which looks at whether a company is conscious of it environmental impact, to management, and finally leadership.\n\nA high CDP score is usually indicative of a company’s high environmental awareness, advanced sustainability governance and leadership to address climate change .\n\n\nThe CDP represents 822 institutional investors, holding US$95 trillion in assets. In 2010, CDP was called \"The most powerful green NGO you've never heard of\" by the Harvard Business Review. In 2012 it won the Zayed Future Energy Prize.\n\n\n\n"}
{"id": "24031477", "url": "https://en.wikipedia.org/wiki?curid=24031477", "title": "Carl Hodges", "text": "Carl Hodges\n\nCarl N. Hodges is an American atmospheric physicist and founder of the Seawater foundation. He is the main driving force behind ideas of using sea canals to irrigate deserts.\n\nThe idea is to grow sea farms in the artificial sea canals using plants like salicornia and mangrove, and introducing fish and shrimp. This is thought to be able to resurrect ecosystems, create jobs, wealth and food sources. This has been proposed as a solution for starvation and even being able to counter the effects of global warming and rising sea levels.\n\n"}
{"id": "11047633", "url": "https://en.wikipedia.org/wiki?curid=11047633", "title": "Charlton Ogburn", "text": "Charlton Ogburn\n\nCharlton Ogburn Jr. (15 March 1911, Atlanta, Georgia – 19 October 1998, Beaufort, South Carolina) was an American journalist and author, most notably of memoirs and non-fiction works. Before he established himself as a writer he served in the US army, and then as a State Department official, specialising in South-East Asian affairs.\n\nIn his later years he was best known as an advocate of the Oxfordian theory of Shakespeare authorship, leading the revival of public interest in the theory in the 1980s. He wrote over a dozen books and numerous magazine articles.\n\nOgburn was the son of lawyer Charlton Greenwood Ogburn and writer Dorothy Ogburn née Stevens. His uncle was the sociologist William Fielding Ogburn. He was raised in Savannah and New York City, graduated from Harvard in 1932 and wrote and worked in publishing. During World War II he joined military intelligence, serving in the East Asian theater, most notably as communications officer for Merrill's Marauders. He left with the rank of captain. He returned to the US to begin a career with the State Department. From 1946 to 1949, he worked at the Division of South-East Asian Affairs. He went on to work at the Department of State. He held several posts, including Political Advisor to the United States Delegation to the United Nations Security Council's Committee of Good Offices for the Indonesian Dispute.\n\nOgburn was among the first State Department officials to explicitly oppose the growing U.S. involvement in the First Indochina War, which would later evolve into the Vietnam war. In 1950 he wrote a memo in which he predicted that Ho Chi Minh would not \"wilt\" under the impact of American aid to the colonial French forces, and that any military victory would simply send Ho's troops \"underground until a more propitious occasion presented itself\". Ogburn also unsuccessfully opposed American policy of supporting the Vietnamese monarchy of Bảo Đại.\n\nAfter the success of his story \"Merrill's Marauders\", a \"Harper's Magazine\" cover story in 1957, Harper & Bros. offered an advance for a book and he quit the government to write full-time in 1957.\n\nHe committed suicide in 1998.\n\nOgburn's papers are archived at Emory University.\n\nOgburn was married twice. With his first wife, he had one son, who was also called Charlton, (referred to as Charlton Ogburn, III). The couple divorced, after which Charlton III's name was changed by his mother to William Fielding Ogburn. He was later known as Will Aldis. Ogburn married Vera M. Weidman in 1951, with whom he fathered two daughters, Nyssa and Holly Ogburn.\n\nFor most of Ogburn's life, his best-known work was \"The Marauders\" (1959), a first person account of the Burma Campaign in World War II. It was later filmed as \"Merrill's Marauders\" (1962).\n\nVersions of the following quotation are frequently misattributed to Petronius. \n\nIn fact it is from the magazine article \"Merrill's Marauders\" (\"Harper's Magazine\", 1957) that earned Ogburn his book contract. In full, it reads thus:\n\nOgburn won the John Burroughs Medal in 1967 for \"The Winter Beach\". His account of travels along the largely deserted northeastern shore is considered a classic of nature-writing. Stewart Udall wrote, \"In \"The Winter Beach\", literary courage, eloquence and wisdom have, I think, brought about a triumph.\" Roger Tory Peterson said, \"Ogburn has written a most extraordinary book... he is a very sensitive, reflective writer in the Thoreauvian tradition\". In 1976, his book, \"The Adventure of Birds\" was published with drawings by Matthew Kalmenoff.\n\nOgburn also wrote fiction. He began his literary career with \"The White Falcon\", a story published by Houghton Mifflin in 1955. His short novel \"The Bridge\" was a work of young adult fiction with illustrations by Evaline Ness. It told the story of an elderly man and his teenage granddaughter battling to preserve their way of life, threatened by greedy relatives and a dangerous storm. Another book for young adults was \"Big Caesar\", illustrated by Joe Krush, a story about a boy's interest in an old truck. In 1965 he published \"The Gold of the River Sea\", a novel based on his early experiences traveling in Brazil.\n\nToday Ogburn may be known best for several books and articles on the Shakespeare authorship question, continuing the passion of his parents, who had written several books on the topic including \"This Star of England: \"William Shakes-speare\" Man of the Renaissance\" (Coward-McCann, 1952). Ogburn junior's last and most well-known book, \"The Mysterious William Shakespeare: The Myth and the Reality\" (New York: Dodd, Mead, 1984), led directly to an appearance on William F. Buckley's \"Firing Line\", followed by a 1987 \"Frontline\" documentary on the authorship question narrated by Al Austin, and mock trials in America and Britain.\n\nMore than a thousand people attended the moot court case sponsored by American University in 1987. Three US Supreme Court justices —John Paul Stevens, Harry Blackmun and William J. Brennan—heard arguments in favor of the orthodox view of Shakespearean authorship and the Oxfordian theory that attributes the works to Edward de Vere, 17th Earl of Oxford (1550–1604). Although the justices held in favor of the traditional account of authorship, Justice Stevens later wrote an article supporting Ogburn's position, \"The Shakespeare Canon of Statutory Construction\", \"University of Pennsylvania Law Review\" (1991).\n\nOgburn's book reinvigorated the Oxfordian theory; inspired a succession of articles in \"The New Yorker\" (1988), \"Atlantic Monthly\" (1991), and \"Harper's Magazine\" (1999) and provoked a nationally broadcast three-hour teleconference on the topic \"Uncovering Shakespeare: An Update\" with moderator William F. Buckley, Jr..\n\n"}
{"id": "13104381", "url": "https://en.wikipedia.org/wiki?curid=13104381", "title": "Climate Change Research Centre", "text": "Climate Change Research Centre\n\nThe Climate Change Research Centre (abbreviated CCRC) is a research initiative established in 2007 at the University of New South Wales. The foundation Directors of the CCRC were the Australian Research Council (ARC) Federation Fellow Professor Matthew England, who established the Climate and Environmental Dynamics Laboratory in 2005, and Professor Andrew Pitman, the Director of the ARC Centre of Excellence for Climate Extremes. The current Director is Professor Katrin Meissner.\n\nThe Centre's research falls into ten broad categories:\n\n\nThe CCRC is the University of New South Wales lead node of the Australian Research Council's Centre of Excellence for Climate Extremes (2017-2025) . The CCRC formerly led the Australian Research Council's Centre of Excellence for Climate System Science (2011–2018) .\n"}
{"id": "37112306", "url": "https://en.wikipedia.org/wiki?curid=37112306", "title": "Comet ISON", "text": "Comet ISON\n\nComet ISON, formally known as C/2012 S1, was a sungrazing comet discovered on 21 September 2012 by Vitaly Nevsky (Виталий Невский, Vitebsk, Belarus) and Artyom Novichonok (Артём Новичонок, Kondopoga, Russia). The discovery was made using the reflector of the International Scientific Optical Network (ISON) near Kislovodsk, Russia. Data processing was carried out by automated asteroid-discovery program CoLiTec. Precovery images by the Mount Lemmon Survey from 28 December 2011 and by Pan-STARRS from 28 January 2012 were quickly located. Follow-up observations were made on 22 September by a team from Remanzacco Observatory in Italy using the iTelescope network. The discovery was announced by the Minor Planet Center on 24 September. Observations by Swift in January 2013 suggested that Comet ISON's nucleus was around in diameter. Later estimates were that the nucleus was only about in diameter. Mars Reconnaissance Orbiter (MRO) observations suggested the nucleus was smaller than in diameter.\n\nShortly after Comet ISON's discovery, the media reported that it might become brighter than the full Moon. However, as events transpired, it never became bright enough to be readily visible to the naked eye. Furthermore, it broke apart as it passed close to the Sun. Reports on 28 November 2013 (the day of perihelion passage) indicated that Comet ISON had partially or completely disintegrated due to the Sun's heat and tidal forces. However, later that day CIOC (NASA \"C\"omet \"I\"SON \"O\"bserving \"C\"ampaign) members discovered a coma-like feature, suggesting a small fragment of it may have survived perihelion. On 29 November 2013, the coma dimmed to an apparent magnitude of 5. By the end of 30 November 2013, the coma had further faded to below naked-eye visibility at magnitude 7. On 1 December, the coma continued to fade even further as it finished traversing the Solar and Heliospheric Observatory's view. On 2 December 2013, the CIOC announced that Comet ISON had fully disintegrated. The Hubble Space Telescope failed to detect fragments of ISON on 18 December 2013. On 8 May 2014, a detailed examination of the disintegration was published, suggesting that the comet fully disintegrated hours before perihelion.\n\nDuring routine observations on 21 November 2012, Vitali Nevski and Artyom Novichonok monitored areas of Gemini and Cancer after their observations were delayed by clouded weather for much of the night. The team used ISON's reflector near Kislovodsk, Russia, and CCD imaging to carry out their observations. Shortly after their session, Nevski processed data using CoLiTec, an automated asteroid discovery software program. In analysis he noted an unusually bright object with slow apparent movement, indicating a position outside the orbit of Jupiter based on the use of four 100-second CCD exposures. At the time of discovery, the object's apparent magnitude ranged from 19.1 to as bright as 18.8.\n\nThe group reported their discovery to the Central Bureau for Astronomical Telegrams as an asteroidal object, which was subsequently forwarded to the Minor Planet Center. However, the group later reported that the object had a cometary appearance with a coma approximately 8 arcseconds across. The object's position and cometary appearance was confirmed by several other unaffiliated observers, and as such the comet was named \"ISON\", after the international observational project and in accordance with International Astronomical Union naming guidelines. Comet ISON was precovered in analysis of Mount Lemmon Observatory imagery by G. V. Williams and Pan-STARRS imagery in Haleakalā. Precovery images from Mount Lemmon were first taken on 28 December 2011 and indicated that the comet had an estimated apparent magnitude ranging from 19.5 to 19.9. Images from Pan-STARRS were taken on 28 January 2012 and in those images the comet had an estimated apparent magnitude ranging from 19.8 to 20.6.\n\nComet ISON came to perihelion (closest approach to the Sun) on 28 November 2013 at a distance of from the center point of the Sun. Accounting for the solar radius of , Comet ISON passed approximately above the Sun's surface. Its trajectory appeared to be hyperbolic, which suggested that it was a dynamically new comet coming freshly from the Oort cloud or even a candidate interstellar comet. Near perihelion, a generic heliocentric two-body solution to the orbit suggests that the orbital period was around 400,000 years. But for objects at such high eccentricity, the Sun's barycentric coordinates are more stable than heliocentric coordinates. The orbit of a long-period comet is properly obtained when the osculating orbit is computed at an epoch after leaving the planetary region and is calculated with respect to the center of mass of the Solar System. Using JPL Horizons, the barycentric orbital elements for epoch 1 January 2050 generate a hyperbolic solution. On its closest approach, Comet ISON passed about from Mars on 1 October 2013, and the remnants of Comet ISON passed about from Earth on 26 December 2013.\n\nShortly after its discovery, similarities between the orbital elements of Comet ISON and the Great Comet of 1680 led to speculation that there might be a connection between them. Further observations of ISON, however, showed that the two comets are not related.\n\nWhen Earth passed near the orbit of Comet ISON on 14–15 January 2014, it was predicted that micron-sized dust particles blown by the Sun's radiation might cause a meteor shower or noctilucent clouds; however, both events were considered unlikely. Because Earth only passed near Comet ISON's orbit, not through the tail, the chances that a meteor shower would occur were slim. In addition, meteor showers from long-period comets that make just one pass into the inner solar system are very rare, if ever recorded. The possibility that small particles left behind on the orbital path—almost one hundred days after the nucleus has passed—could form noctilucent clouds is also slim. No such events are known to have taken place in the past under similar circumstances.\n\nAt the time of its discovery, Comet ISON's apparent magnitude was approximately 18.8, far too dim to be seen with the naked eye, but bright enough to be imaged by amateurs with large telescopes. It then followed the pattern of most comets and increased gradually in brightness on approach to the Sun.\n\nAt least a dozen spacecraft imaged Comet ISON. It was first imaged by the Swift and Deep Impact spacecraft in January and February 2013, and shown to be active with an extended tail. In April and May 2013 the Hubble Space Telescope (HST) measured Comet ISON's size, and the color, extent, and polarization of its emitted dust. The Spitzer Space Telescope (SST) observed Comet ISON on 13 June and estimated carbon dioxide outgassing at about 1 million kilograms (2.2 million pounds) per day. From 5 June to 29 August 2013, Comet ISON had an elongation less than 30 degrees from the Sun. No obvious rotational variability was detected by either Deep Impact, HST, or Spitzer. Amateur astronomer Bruce Gary recovered it on 12 August 2013 when it was 6 degrees above the horizon and 19 degrees from the Sun. Due to it brightening more slowly than predicted, Comet ISON only became visible through small telescopes during early October 2013.\n\nOn 28 September 2013, NASA launched BRRISON, a stratospheric science balloon carrying a telescope and science instruments designed to capture images and data on Comet ISON from an altitude of . However, about two and a half hours after launch, the telescope returned to its stowed position too quickly, driving it past a stow latch. Operators were unable to redeploy the telescope, resulting in mission failure.\n\nOn 1 October 2013, Comet ISON passed within of Mars. Between 29 September and 2 October, the Mars Reconnaissance Orbiter (MRO) detected Comet ISON. The twin STEREO spacecraft began detecting Comet ISON in the second week of October. October 2013 images of Comet ISON displayed a greenish tint, probably attributable to the release of cyanogen and diatomic carbon. On 31 October 2013, Comet ISON was detected with 10×50 binoculars.\n\nOn 14 November 2013, Comet ISON was reported to be visible to the naked eye by experienced observers located at dark sites. It had an appearance similar to comet C/2013 R1 that was also visible to the naked eye. Comet ISON was not expected to reach the naked-eye magnitude of 6 until mid-November, and was not expected to be observable by the general public until it brightened to about magnitude 4. On 17–18 November, when Comet ISON was brighter and much closer to the morning twilight, it passed the bright star Spica in the constellation Virgo. But due to the full Moon and glow of twilight, Comet ISON had not become bright enough to be seen without optical aid by the general public. On 22 November, it started to drop below Mercury in the bright twilight. SOHO started to view it on 27 November, first with the LASCO coronograph. On 27 November ISON brightened to magnitude −2 and passed Delta Scorpii. Around the time it reached perihelion on 28 November, it might have become extremely bright if it had remained fully intact. However, predicting the brightness of a comet is difficult, especially one that passes so close to the Sun and is affected by the forward scattering of light. Originally, media sources predicted that it might become brighter than the full Moon, but based on more recent observations, it was only expected to reach around apparent magnitude , about the same brightness as Venus. In comparison, the brightest comet since 1935 was Comet Ikeya–Seki in 1965 at , which was much brighter than Venus. On 29 November 2013, Comet ISON had dimmed to magnitude 5 in the LASCO images. By the end of 30 November 2013, it had further faded to below naked-eye visibility at magnitude 7.\n\nIn a February 2013 study, 1,897 observations were used to create a light curve. The resulting plot showed Comet ISON increasing its brightness relatively quickly at R. If this had continued to perihelion, it would have reached , brighter than the full Moon. It had since exhibited a \"slowdown event\", however, similar to the ones exhibited by many other Oort cloud comets, among them C/2011 L4. Therefore, Comet ISON's brightness increased less quickly than predicted and it did not become as bright as expected. Further observations suggested that, even if it had remained intact, it might only brighten to about . It had been determined that it was a \"baby comet\" (i.e. an object with a photometric age less than four comet years). The temperature at perihelion had been calculated to reach , sufficient to melt iron. Additionally, it was within the Roche limit, meaning it might disintegrate due to the Sun's gravity.\n\nComet ISON was expected to be brightest around the time it was closest to the Sun; but because it was less than 1° from the Sun at its closest, it would have been difficult to see against the Sun's glare. Comet ISON would have been well placed for observers in the northern hemisphere during mid to late December 2013. If it had survived its perihelion passage fully intact, it might have remained visible to the naked eye until January 2014. As Comet ISON moved north on the celestial sphere it would have passed within two degrees of Polaris on 8 January.\n\nOn 22 May 2014, the Eurasian Astronomical Society and Sternberg Astronomical Institute published preliminary results of observations of the observed meteor shower of Comet ISON from January 2014. Scientists from Ukraine and Belarus were assisted by meteor observation groups around the world. The results confirmed that particles of Comet ISON, which likely sublimated at perihelion, entered Earth's atmosphere as meteor particles. 43 meteor events were recorded after analyzing 54,000 images from 10–17 January 2014.\n\nOn 11 August 2014, astronomers released studies, using the Atacama Large Millimeter Array (ALMA) for the first time, that detailed the distribution of , , , and dust inside the comae of comets C/2012 F6 (Lemmon) and C/2012 S1 (ISON).\n\nComet ISON's formal designation was C/2012 S1. It was named \"ISON\" after the organization where its discovery was made, the Russia-based International Scientific Optical Network. The initial report of the object to the Central Bureau for Astronomical Telegrams identified the object as an asteroid, and it was listed on the Near Earth Objects Confirmation Page. Follow-up observations by independent teams were the first to report cometary features. Therefore, under the International Astronomical Union's comet-naming guidelines, Comet ISON was named after the team that discovered it, rather than the individual discoverers.\n\nAfter it was discovered in 2012, some media sources called Comet ISON the \"Comet of the Century\" and speculated that it might outshine the full Moon. An \"Astronomy Now\" columnist wrote in September 2012 that \"if predictions hold true then Comet ISON will certainly be one of the greatest comets in human history.\" As recently as October 2013, a \"Daily Mail\" columnist described Comet ISON as \"the Comet of the Century\" and said it was \"hoped to be 15 times brighter than the Moon.\"\n\nAstronomer Karl Battams criticized the media's suggestion that Comet ISON would be \"brighter than the full Moon\", saying that members of the Comet ISON Observing Campaign did not foresee ISON becoming that bright.\n\nComet ISON has been compared to Comet Kohoutek, seen in 1973–1974, another highly anticipated Oort cloud comet that peaked early and fizzled out.\n\n\n\n"}
{"id": "1341419", "url": "https://en.wikipedia.org/wiki?curid=1341419", "title": "Cyclogenesis", "text": "Cyclogenesis\n\nCyclogenesis is the development or strengthening of cyclonic circulation in the atmosphere (a low-pressure area). Cyclogenesis is an umbrella term for at least three different processes, all of which result in the development of some sort of cyclone, and at any size from the microscale to the synoptic scale.\n\n\nThe process in which an extratropical cyclone undergoes a rapid drop in atmospheric pressure (24 millibars or more) in a 24-hour period is referred to as explosive cyclogenesis, and is usually present during the formation of a nor'easter. The anticyclonic equivalent, the process of formation of high pressure areas, is anticyclogenesis. The opposite of cyclogenesis is cyclolysis.\n\nThere are four main scales, or sizes of systems, dealt with in meteorology: the macroscale, the synoptic scale, the mesoscale, and the microscale. The macroscale deals with systems with global size, such as the Madden–Julian oscillation. Synoptic scale systems cover a portion of a continent, such as extratropical cyclones, with dimensions of across. The mesoscale is the next smaller scale, and often is divided into two ranges: meso-alpha phenomena range from across (the realm of the tropical cyclone), while meso-beta phenomena range from across (the scale of the mesocyclone). The microscale is the smallest of the meteorological scales, with a size under (the scale of tornadoes and waterspouts). These horizontal dimensions are not rigid divisions but instead reflect typical sizes of phenomena having certain dynamic characteristics. For example, a system does not necessarily transition from meso-alpha to synoptic scale when its horizontal extent grows from .\n\nThe Norwegian cyclone model is an idealized formation model of cold-core cyclonic storms developed by Norwegian meteorologists during the First World War. The main concept behind this model, relating to cyclogenesis, is that cyclones progress through a predictable evolution as they move up a frontal boundary, with the most mature cyclone near the northeast end of the front and the least mature near the tail end of the front.\n\nA preexisting frontal boundary, as defined in surface weather analysis, is required for the development of a mid-latitude cyclone. The cyclonic flow begins around a disturbed section of the stationary front due to an upper level disturbance, such as a short wave or an upper-level trough, near a favorable quadrant of the upper level jet. However, enhanced along-frontal stretching rates in the lower troposphere can suppress the growth of extratropical cyclones.\n\nCyclogenesis can only occur when temperature decreases polewards (to the north, in the northern hemisphere), and pressure perturbation lines tilt westward with height. Cyclogenesis is most likely to occur in regions of cyclonic vorticity advection, downstream of a strong westerly jet. The combination of vorticity advection and thermal advection created by the temperature gradient and a low pressure center cause upward motion around the low.\n\nIf the temperature gradient is strong enough, temperature advection will increase, driving more vertical motion. This increases the overall strength of the system. Shearwise updrafts are the most important factor in determining cyclonic growth and strength.\n\nThe surface low could have a variety of causes for forming. Topography can force a surface low when dense low-level high pressure system ridges in east of a north-south mountain barrier. Mesoscale convective systems can spawn surface lows which are initially warm core. The disturbance can grow into a wave-like formation along the front and the low will be positioned at the crest. Around the low, flow will become cyclonic, by definition. This rotational flow will push polar air equatorward west of the low via its trailing cold front, and warmer air will push poleward low via the warm front. Usually the cold front will move at a quicker pace than the warm front and “catch up” with it due to the slow erosion of higher density airmass located out ahead of the cyclone and the higher density airmass sweeping in behind the cyclone, usually resulting in a narrowing warm sector. At this point an occluded front forms where the warm air mass is pushed upwards into a trough of warm air aloft, which is also known as a trowal (a trough of warm air aloft). All developing low pressure areas share one important aspect, that of upward vertical motion within the troposphere. Such upward motions decrease the mass of local atmospheric columns of air, which lower surface pressure.\n\nMaturity is after the time of occlusion when the storm has completed strengthening and the cyclonic flow is at its most intense. Thereafter, the strength of the storm diminishes as the cyclone couples with the upper level trough or upper level low, becoming increasingly cold core. The spin-down of cyclones, also known as cyclolysis, can be understood from an energetics perspective. As occlusion occurs and the warm air mass is pushed upwards over a cold air airmass, the atmosphere becomes increasingly stable and the centre of gravity of the system lowers. As the occlusion process extends further down the warm front and away from the central low, more and more of the available potential energy of the system is exhausted. This potential energy sink creates a kinetic energy source which injects a final burst of energy into the storm's motions. After this process occurs, the growth period of the cyclone, or cyclogenesis, ends, and the low begins to spin down (fill) as more air is converging into the bottom of the cyclone than is being removed out the top since upper-level divergence has decreased.\n\nOccasionally, cyclogenesis will re-occur with occluded cyclones. When this happens a new low center will form on the triple-point (the point where the cold front, warm front, and occluded front meet). During triple-point cyclogenesis, the occluded parent low will fill as the secondary low deepens into the main weathermaker.\n\nTropical cyclones exist within a mesoscale alpha domain. As opposed to mid-latitude cyclogenesis, tropical cyclogenesis is driven by strong convection organised into a central core with no baroclinic zones, or fronts, extending through their center. Although the formation of tropical cyclones is the topic of extensive ongoing research and is still not fully understood, there are six main requirements for tropical cyclogenesis: sea surface temperatures that are warm enough, atmospheric instability, high humidity in lower to middle levels of the troposphere, enough Coriolis force to develop a low pressure center, a pre-existing low level focus or disturbance, and low vertical wind shear. These warm core cyclones tend to form over the oceans between 10 and 30 degrees of the equator.\n\nMesocyclones range in size from mesoscale beta to microscale. The term mesocyclone is usually reserved for mid-level rotations within severe thunderstorms, and are warm core cyclones driven by latent heat of its associated thunderstorm activity.\n\nTornadoes form in the warm sector of extratropical cyclones where a strong upper level jet stream exists. Mesocyclones are believed to form when strong changes of wind speed and/or direction with height (\"wind shear\") sets parts of the lower part of the atmosphere spinning in invisible tube-like rolls. The convective updraft of a thunderstorm is then thought to draw up this spinning air, tilting the rolls' orientation upward (from parallel to the ground to perpendicular) and causing the entire updraft to rotate as a vertical column.\n\nAs the updraft rotates, it may form what is known as a wall cloud. The wall cloud is a spinning layer of clouds descending from the mesocyclone. The wall cloud tends to form closer to the center of the mesocyclone. It should be noted the wall clouds do not necessarily need a mesocyclone to form and do not always rotate. As the wall cloud descends, a funnel-shaped cloud may form at its center. This is the first stage of tornado formation. The presence of a mesocyclone is believed to be a key factor in the formation of the strong tornadoes associated with severe thunderstorms.\n\nTornadoes exist on the microscale or low end of the mesoscale gamma domain. The cycle begins when a strong thunderstorm develops a rotating mesocyclone a few miles up in the atmosphere, becoming a supercell. As rainfall in the storm increases, it drags with it an area of quickly descending air known as the rear flank downdraft (RFD). This downdraft accelerates as it approaches the ground, and drags the rotating mesocyclone towards the ground with it.\n\nAs the mesocyclone approaches the ground, a visible condensation funnel appears to descend from the base of the storm, often from a rotating wall cloud. As the funnel descends, the RFD also reaches the ground, creating a gust front that can cause damage a good distance from the tornado. Usually, the funnel cloud begins causing damage on the ground (becoming a tornado) within minutes of the RFD reaching the ground.\n\nWaterspouts exist on the microscale. While some waterspouts are strong (tornadic) like their land-based counterparts, most are much weaker and caused by different atmospheric dynamics. They normally develop in moisture-laden environments with little vertical wind shear along lines of convergence, such as land breezes, lines of frictional convergence from nearby landmasses, or surface troughs. Their parent cloud can be as innocuous as a moderate cumulus, or as significant as a thunderstorm. Waterspouts normally develop as their parent clouds are in the process of development, and it is theorized that they spin up as they move up the surface boundary from the horizontal wind shear near the surface, and then stretch upwards to the cloud once the low level shear vortex aligns with a developing cumulus or thunderstorm. Weak tornadoes, known as landspouts, across eastern Colorado have been witnessed to develop in a similar manner. An outbreak occurred in the Great Lakes in late September and early October 2003 along a lake effect band. September is the peak month of landspout and waterspout occurrence around Florida and for waterspout occurrence around the Great Lakes.\n\nCyclogenesis is the opposite of cyclolysis, which concerns the weakening of surface cyclones. The term has an anticyclonic (high pressure system) equivalent—Anticyclogenesis, which deals with the formation of surface high pressure systems.\n"}
{"id": "4530704", "url": "https://en.wikipedia.org/wiki?curid=4530704", "title": "Deccan thorn scrub forests", "text": "Deccan thorn scrub forests\n\nThe Deccan thorn scrub forests are a xeric shrubland ecoregion of India and southernmost Sri Lanka. Historically this area was covered by tropical dry deciduous forest, but this only remains in isolated fragments. The vegetation now consists of\nmainly of southern tropical thorn scrub type forests. These consist of open woodland with thorny trees with short trunks and low, branching crowns; spiny and xerophytic shrubs; and dry grassland. This is the habitat of the great Indian bustard and blackbuck, though these and other animals are declining in numbers; this area was at one time home to large numbers of elephants and tigers. Almost 350 species of bird have been recorded here. The remaining natural habitat is threatened by overgrazing and invasive weeds, but there are a number of small protected areas which provide a haven for the wildlife.\n\nThis ecoregion covers the arid portions of the Deccan Plateau, extending across the Indian states of Maharashtra, Telangana, Karnataka, Andhra Pradesh, and Tamil Nadu to the Northern Province of Sri Lanka. Only small patches of natural habitat remain, as most of the region has been cleared for grazing.\n\nThe annual rainfall is less than , all falling during the short rainy season, and the area receives no rainfall during the months of November to April. Temperatures can exceed during the hotter months.\n\nToday the remaining forest is mostly southern tropical thorn scrub, and also includes patches of the original vegetation, tropical dry deciduous forests. The southern tropical thorn scrub forests consist of open, low vegetation with thorny trees with short trunks and low, branching crowns that rarely meet to form a closed canopy. The trees grow up to . Typical grasses of the ecoregion include \"Chrysopogon fulvus\", \"Heteropogon contortus\", \"Eremopogon foveolatus\", \"Aristida setacea\", and \"Dactyloctenium\" species.\n\nThe second storey of the thorn scrub forests in Maharashtra is poorly developed and mainly consists spiny and xerophytic species, mostly shrubs. An ill-defined lower storey can also be seen during the brief wet season. The plant species that dominate the vegetation in these forests are Acacia species, \"Balanites roxburghii\", \"Cordia myxa\", \"Capparis\" spp., \"Prosopis\" spp., \"Azadirachta indica\", \"Cassia fistula\", \"Diospyros chloroxylon\", \"Carissa carandas\", and \"Phoenix sylvestris\". There are also several other habitat types found in these forests.\n\nThe driest, rockiest areas of the ecoregion are covered with a scrub dominated by species of \"Euphorbia\". The soil is usually bare in these areas; however, some grassy growth may also appear during the short monsoon season.\n\nThe parts of the ecoregion found in Tamil Nadu receive even less rainfall than most, and the vegetation in these parts is mainly made up of thinly spread thorny forests of \"Acacia planifrons\", with umbrella-shaped crowns.\n\nThe remaining patches of forest are also home to a large number of plants, some of medicinal and botanical interest, including an endemic cycad (\"Cycas beddomei\") and \"Psilotum nudum\". A small patch of the tree \"Shorea talura\" also exists within the Chittoor forest division, part of which is being maintained as a preservation plot by the Forest Department of Andhra Pradesh.\n\nFinally, the area between the Nallamala and Seshachalam Hills is well known for the red sanders (\"Pterocarpus santalinus\"), a rare, endemic tree species that is harvested for the medicinal value of its wood.\n\nThe dry grasslands that predominate do provide habitat for the native fauna remaining scattered amid the thorn scrub. The grasslands of southern Andhra Pradesh support a good population of the great Indian bustard (\"Ardeotis nigriceps\") and blackbuck (\"Antilope cervicapra\"), although these and other species are declining in number.\n\nThe forests used to provide habitat for three prominent mammal species, the Bengal tiger (\"Panthera tigris tigris\"), the Indian elephant (\"Elephas maximus indicus\"), whose populations have recently dwindled and may have even become locally extinct, and the nilgai antelope (\"Boselaphus tragocamelus\").\n\nThe ecoregion is home to 96 mammal species, out of which three are considered endemic: split roundleaf bat (\"Hipposideros schistaceus\"), Kondana soft-furred rat (\"Millardia kondana\"), and Elvira rat (\"Cremnomys elvira\"). Other threatened mammal species found in these forests include the tiger, gaur (\"Bos gaurus\"), dhole (\"Cuon alpinus\"), sloth bear (\"Melursus ursinus\"), chousingha (\"Tetracerus quadricornis\"), and blackbuck (\"Antilope cervicapra\").\n\nThe Deccan thorn scrub forests are home to a richer variety of birds: almost 350 species, of which three are considered near-endemic: Jerdon's courser (\"Rhinoptilus bitorquatus\"), Sri Lanka junglefowl (\"Gallus lafayetii\"), and yellow-fronted barbet (\"Megalaima flavifrons\"). Among these, the Jerdon's courser is also considered a globally threatened species, and was rediscovered in this ecoregion in 1986 after being recorded for the last time in 1900. Other globally threatened bird species such as the lesser florican (\"Sypheotides indicus\") and Indian bustard can also be found in the ecoregion.\n\nThe remaining deciduous woodland continues to be cleared for grazing land, while the pasture that has been created is itself threatened by overgrazing and invasive weeds. One large area of natural forest remains in southern Andhra Pradesh and there are eleven protected areas, but all of these are small: the largest is the Rollapadu Bird Sanctuary for the great Indian bustard near Nandikotkur in Kurnool, Andhra Pradesh.\n\n\n"}
{"id": "319464", "url": "https://en.wikipedia.org/wiki?curid=319464", "title": "Devils Postpile National Monument", "text": "Devils Postpile National Monument\n\nDevils Postpile National Monument is a National Monument located near Mammoth Mountain in eastern California. The monument protects Devils Postpile, an unusual rock formation of columnar basalt. It encompasses and includes two main tourist attractions: the Devils Postpile formation and Rainbow Falls, a waterfall on the Middle Fork of the San Joaquin River. In addition, the John Muir Trail and Pacific Crest Trail merge into one trail as they pass through the monument. Excluding a small developed area containing the monument headquarters, visitor center and a campground; the National Monument lies within the borders of the Ansel Adams Wilderness.\n\nThe monument was created in 1911 as Devil Postpile National Monument, but is widely referred to as Devils Postpile National Monument, and has been officially styled as plural without the apostrophe since the 1930s. The monument was once part of Yosemite National Park, but discovery of gold in 1905 near Mammoth Lakes prompted a boundary change that left the Postpile on adjacent public land. Later, a proposal to build a hydroelectric dam called for blasting the Postpile into the river. Influential Californians, including John Muir, persuaded the federal government to stop the demolition and, in 1911, President William Howard Taft protected the area as a National Monument.\n\nThe flora and fauna at Devils Postpile are typical of the Sierra Nevada. The monument area contains animals and plants such as black bears, pine martens, mule deer, coyotes, quaking aspen, black cottonwood, alder, and willows, as well as many wildflowers, such as cinquefoil and alpine shooting star. Dark-eyed juncos and white-crowned sparrows are common in the summer.\n\nThe most common method for accessing Devils Postpile is via the mandatory shuttle bus operated by Eastern Sierra Transit Authority in the summer months, followed by a 1/4 mile walk. The shuttle route begins at Mammoth Mountain Ski Area's Adventure Center and makes several stops throughout the valley and begins operating when the Reds Meadow Road opens in the summer, and continues through Labor Day weekend. \n\nDevils Postpile is also accessible on foot from Mammoth Lakes by hiking over Mammoth Pass and into the Reds Meadow Valley. During the winter months, there are no services available, but adventurers can visit the site via cross-country ski or snowshoe.\n\nThe name \"Devils Postpile\" refers to a dark cliff of columnar basalt. Radiometric dating indicates the formation was created by a lava flow at some time less than 100,000 years ago. The source of the lava is thought to have been somewhere near Upper Soda Springs campground at the north end of Pumice Flat on the floor of the Middle Fork of the San Joaquin River, from where it flowed to the site of the Postpile. Estimates of the formation’s thickness range from to . The lava that now makes up the Postpile was near the bottom of this mass.\n\nBecause of its great thickness, much of the mass of pooled lava cooled slowly and evenly, which is why the columns are so long and so symmetrical. Columnar jointing occurs when certain types of lava contract while cooling.\n\nA glacier later removed much of this mass of rock and left a polished surface on top of the columns with very noticeable glacial striations and glacial polish.\n\nThe Postpile's columns average in diameter, the largest being , and many are up to long. Together they look like tall posts stacked in a pile, hence the feature's name. If the lava had cooled perfectly evenly, all of the columns would be expected to be hexagonal, but some of the columns have different polygonal cross-sections due to variations in cooling. A survey of 400 of the Postpile's columns found that 44.5% were 6-sided, 37.5% 5-sided, 9.5% 4-sided, 8.0% 7-sided, and 0.5% 3-sided. Compared with other examples of columnar jointing, the Postpile has more hexagonal columns. Another feature that places the Postpile in a special category is the lack of horizontal jointing.\n\nSeveral stones from the Devils Postpile can be seen at the entrance to the United States Geological Survey headquarters lot in Reston, Virginia.\n\nAlthough the basaltic columns are impressive, they are not unique. Basalt columns are a common volcanic feature, and they occur on many scales (faster cooling produces smaller columns). Other notable sites include Svartifoss in Vatnajökull National Park in Iceland, Giant's Causeway in Northern Ireland, Fingal's Cave in Scotland, Titan's Piazza of the Mount Holyoke Range in Massachusetts, the Garni Gorge in Armenia, the Cyclopean Isles near Sicily, Sheepeater Cliff at Yellowstone National Park in Wyoming, Prismas Basálticos in Huasca de Ocampo, Mexico, the \"Organ Pipes\" formation on Mount Cargill in New Zealand, Gilbert Hill in Mumbai, Organ Pipes National Park in Australia and the \"Column Cape\" (Russian: Mis Stolbchaty) on Kunashir Island, the southernmost of the Kuril Islands, Mar Brava (Ancud) in Chile. Columnar basalt can also be seen in a high desert dry river falls area just north of Lajitas, Texas. The much more massive Devils Tower National Monument in Wyoming is superficially similar but consists of a phonolite porphyry, formed by the intrusion of igneous rock.\n\n\n\n"}
{"id": "3155220", "url": "https://en.wikipedia.org/wiki?curid=3155220", "title": "Diluvium", "text": "Diluvium\n\nHistorically, diluvium was a term in geology for superficial deposits formed by flood-like operations of water, and so contrasted with alluvium or alluvial deposits formed by slow and steady aqueous agencies. The term was formerly given to the boulder clay deposits, supposed to have been caused by the Noachian deluge.\nIn the late 20th century Russian geologist proposed the term \"diluvium\" for description of deposits created as a result of catastrophic outbursts of Pleistocene giant glacier-dammed lakes in intermontane basins of the Altai. The largest of these lakes, Chuya and Kuray, had volumes of water in hundreds of cubic kilometers, and their discharge in peak hydrograph flow rate exceeded the maximum rates of the well-known Pleistocene Lake Missoula floods in North America. The term \"diluvium\" in the meaning of A. N. Rudoy has become accepted, and the process of diluvial morpholithogenesis can be found in modern textbooks.\n\nNearly all intermountain depressions in southern Siberia and northern Mongolia hosted glacier-dammed lakes during the Pleistocene ice ages. Climatic changes and hydrostatic alterations of the ice dams were followed by repeated fillings and releases of the basin lakes. The lake outbursts had a cataclysmic character. In accordance with climatic conditions, the glaciers would protrude again into the main drainage valleys immediately after dam deformations and lake outbursts and would again dam the basins.\n\nThe greatest of the lakes (Lakes Chuya, Kuray, Uymon, Darkhat and others)\nachieved volumes up to hundreds of cubic kilometres, while the discharges of the outburst\nfloods reached millions of cubic metres per second. These floods transformed the\ndrainage valleys, repeatedly building new sediment deposits. The dating of the latter proves\nthe occurrence of large-scale floods in the Chuya and the Katun River valleys in the interval\nbetween 23 and 7 thousand years ago. During that period there were at least five\nlarge-scale flooding events. Enormous water masses were drained simultaneously and repeatedly\ntoward the south of West Siberia. The total water volume out of the Altai basins\nonly used to reach 10 thousand cubic kilometres. With some periodicity, all of the\nbasins of South Siberia were able to deliver northwards about 60 thousand cubic kilometres\nof flood waters.\n\n\n"}
{"id": "37335", "url": "https://en.wikipedia.org/wiki?curid=37335", "title": "Drought", "text": "Drought\n\nA drought is a natural disaster of below-average precipitation in a given region, resulting in prolonged shortages in the water supply, whether atmospheric, surface water or ground water. A drought can last for months or years, or may be declared after as few as 15 days. It can have a substantial impact on the ecosystem and agriculture of the affected region and harm to the local economy. Annual dry seasons in the tropics significantly increase the chances of a drought developing and subsequent bush fires. Periods of heat can significantly worsen drought conditions by hastening evaporation of water vapour.\n\nMany plant species, such as those in the family Cactaceae (or cacti), have drought tolerance adaptations like reduced leaf area and waxy cuticles to enhance their ability to tolerate drought. Some others survive dry periods as buried seeds. Semi-permanent drought produces arid biomes such as deserts and grasslands. Prolonged droughts have caused mass migrations and humanitarian crisis. Most arid ecosystems have inherently low productivity. The most prolonged drought ever in the world in recorded history occurred in the Atacama Desert in Chile (400 Years).\n\nMechanisms of producing precipitation include convective, stratiform, and orographic rainfall. Convective processes involve strong vertical motions that can cause the overturning of the atmosphere in that location within an hour and cause heavy precipitation, while stratiform processes involve weaker upward motions and less intense precipitation over a longer duration. Precipitation can be divided into three categories, based on whether it falls as liquid water, liquid water that freezes on contact with the surface, or ice. Droughts occur mainly in areas where normal levels of rainfall are, in themselves, low. If these factors do not support precipitation volumes sufficiently to reach the surface over a sufficient time, the result is a drought. Drought can be triggered by a high level of reflected sunlight and above average prevalence of high pressure systems, winds carrying continental, rather than oceanic air masses, and ridges of high pressure areas aloft can prevent or restrict the developing of thunderstorm activity or rainfall over one certain region. Once a region is within drought, feedback mechanisms such as local arid air, hot conditions which can promote warm core ridging, and minimal evapotranspiration can worsen drought conditions.\n\nWithin the tropics, distinct, wet and dry seasons emerge due to the movement of the Intertropical Convergence Zone or Monsoon trough. The dry season greatly increases drought occurrence, and is characterized by its low humidity, with watering holes and rivers drying up. Because of the lack of these watering holes, many grazing animals are forced to migrate due to the lack of water and feed to more fertile spots. Examples of such animals are zebras, elephants, and wildebeest. Because of the lack of water in the plants, bushfires are common. Since water vapor becomes more energetic with increasing temperature, more water vapor is required to increase relative humidity values to 100% at higher temperatures (or to get the temperature to fall to the dew point). Periods of warmth quicken the pace of fruit and vegetable production, increase evaporation and transpiration from plants, and worsen drought conditions.\n\nDrier and hotter weather occurs in parts of the Amazon River Basin, Colombia, and Central America during El Niño events. Winters during the El Niño are warmer and drier than average conditions in the Northwest, northern Midwest, and northern Mideast United States, so those regions experience reduced snowfalls. Conditions are also drier than normal from December to February in south-central Africa, mainly in Zambia, Zimbabwe, Mozambique, and Botswana. Direct effects of El Niño resulting in drier conditions occur in parts of Southeast Asia and Northern Australia, increasing bush fires, worsening haze, and decreasing air quality dramatically. Drier-than-normal conditions are also in general observed in Queensland, inland Victoria, inland New South Wales, and eastern Tasmania from June to August. As warm water spreads from the west Pacific and the Indian Ocean to the east Pacific, it causes extensive drought in the western Pacific. Singapore experienced the driest February in 2014 since records began in 1869, with only 6.3 mm of rain falling in the month and temperatures hitting as high as 35 °C on 26 February. The years 1968 and 2005 had the next driest Februaries, when 8.4 mm of rain fell.\n\nHuman activity can directly trigger exacerbating factors such as over farming, excessive irrigation, deforestation, and erosion adversely impact the ability of the land to capture and hold water. In arid climates, the main source of erosion is wind. Erosion can be the result of material movement by the wind. The wind can cause small particles to be lifted and therefore moved to another region (deflation). Suspended particles within the wind may impact on solid objects causing erosion by abrasion (ecological succession). Wind erosion generally occurs in areas with little or no vegetation, often in areas where there is insufficient rainfall to support vegetation.\n\nLoess is a homogeneous, typically nonstratified, porous, friable, slightly coherent, often calcareous, fine-grained, silty, pale yellow or buff, windblown (Aeolian) sediment. It generally occurs as a widespread blanket deposit that covers areas of hundreds of square kilometers and tens of meters thick. Loess often stands in either steep or vertical faces. Loess tends to develop into highly rich soils. Under appropriate climatic conditions, areas with loess are among the most agriculturally productive in the world. Loess deposits are geologically unstable by nature, and will erode very readily. Therefore, windbreaks (such as big trees and bushes) are often planted by farmers to reduce the wind erosion of loess. Wind erosion is much more severe in arid areas and during times of drought. For example, in the Great Plains, it is estimated that soil loss due to wind erosion can be as much as 6100 times greater in drought years than in wet years.\n\nActivities resulting in global climate change are expected to trigger droughts with a substantial impact on agriculture throughout the world, and especially in developing nations. Overall, global warming will result in increased world rainfall. Along with drought in some areas, flooding and erosion will increase in others. Paradoxically, some proposed solutions to global warming that focus on more active techniques, solar radiation management through the use of a space sunshade for one, may also carry with them increased chances of drought.\n\nAs a drought persists, the conditions surrounding it gradually worsen and its impact on the local population gradually increases. People tend to define droughts in three main ways:\n\n\n One can divide the effects of droughts and water shortages into three groups: environmental, economic and social.\n\nEffects vary according to vulnerability. For example, subsistence farmers are more likely to migrate during drought because they do not have alternative food-sources. Areas with populations that depend on water sources as a major food-source are more vulnerable to famine.\n\nDrought can also reduce water quality, because lower water-flows reduce dilution of pollutants and increase contamination of remaining water-sources. Common consequences of drought include:\n\nDrought is a normal, recurring feature of the climate in most parts of the world. It is among the earliest documented climatic events, present in the Epic of Gilgamesh and tied to the Biblical story of Joseph's arrival in and the later Exodus from Ancient Egypt. Hunter-gatherer migrations in 9,500 BC Chile have been linked to the phenomenon, as has the exodus of early humans out of Africa and into the rest of the world around 135,000 years ago.\n\nWell-known historical droughts include:\nThe Darfur conflict in Sudan, also affecting Chad, was fueled by decades of drought; combination of drought, desertification and overpopulation are among the causes of the Darfur conflict, because the Arab Baggara nomads searching for water have to take their livestock further south, to land mainly occupied by non-Arab farming people.\n\nApproximately 2.4 billion people live in the drainage basin of the Himalayan rivers. India, China, Pakistan, Bangladesh, Nepal and Myanmar could experience floods followed by droughts in coming decades. Drought in India affecting the Ganges is of particular concern, as it provides drinking water and agricultural irrigation for more than 500 million people. The west coast of North America, which gets much of its water from glaciers in mountain ranges such as the Rocky Mountains and Sierra Nevada, also would be affected.\n\nIn 2005, parts of the Amazon basin experienced the worst drought in 100 years. A 23 July 2006 article reported Woods Hole Research Center results showing that the forest in its present form could survive only three years of drought. Scientists at the Brazilian National Institute of Amazonian Research argue in the article that this drought response, coupled with the effects of deforestation on regional climate, are pushing the rainforest towards a \"tipping point\" where it would irreversibly start to die. It concludes that the rainforest is on the brink of being turned into savanna or desert, with catastrophic consequences for the world's climate. According to the WWF, the combination of climate change and deforestation increases the drying effect of dead trees that fuels forest fires.\n\nBy far the largest part of Australia is desert or semi-arid lands commonly known as the outback. A 2005 study by Australian and American researchers investigated the desertification of the interior, and suggested that one explanation was related to human settlers who arrived about 50,000 years ago. Regular burning by these settlers could have prevented monsoons from reaching interior Australia. In June 2008 it became known that an expert panel had warned of long term, maybe irreversible, severe ecological damage for the whole Murray-Darling basin if it did not receive sufficient water by October 2008. Australia could experience more severe droughts and they could become more frequent in the future, a government-commissioned report said on July 6, 2008. Australian environmentalist Tim Flannery, predicted that unless it made drastic changes, Perth in Western Australia could become the world’s first ghost metropolis, an abandoned city with no more water to sustain its population. The long Australian Millennial drought broke in 2010.\n\nRecurring droughts leading to desertification in East Africa have created grave ecological catastrophes, prompting food shortages in 1984–85, 2006 and 2011. During the 2011 drought, an estimated 50,000 to 150,000 people were reported to have died, though these figures and the extent of the crisis are disputed. In February 2012, the UN announced that the crisis was over due to a scaling up of relief efforts and a bumper harvest. Aid agencies subsequently shifted their emphasis to recovery efforts, including digging irrigation canals and distributing plant seeds.\n\nIn 2012, a severe drought struck the western Sahel. The Methodist Relief & Development Fund (MRDF) reported that more than 10 million people in the region were at risk of famine due to a month-long heat wave that was hovering over Niger, Mali, Mauritania and Burkina Faso. A fund of about £20,000 was distributed to the drought-hit countries.\n\nAgriculturally, people can effectively mitigate much of the impact of drought through irrigation and crop rotation. Failure to develop adequate drought mitigation strategies carries a grave human cost in the modern era, exacerbated by ever-increasing population densities. President Roosevelt on April 27, 1935, signed documents creating the Soil Conservation Service (SCS)—now the Natural Resources Conservation Service (NRCS). Models of the law were sent to each state where they were enacted. These were the first enduring practical programs to curtail future susceptibility to drought, creating agencies that first began to stress soil conservation measures to protect farm lands today. It was not until the 1950s that there was an importance placed on water conservation was put into the existing laws (NRCS 2014).\n\nStrategies for drought protection, mitigation or relief include:\n\n"}
{"id": "20653168", "url": "https://en.wikipedia.org/wiki?curid=20653168", "title": "Earth science", "text": "Earth science\n\nEarth science or geoscience includes all fields of natural science related to the planet Earth. It is the branch of science dealing with the physical constitution of the earth and its atmosphere. Earth science is the study of our planet’s physical characteristics, from earthquakes to raindrops, and floods to fossils. Earth science can be considered to be a branch of planetary science, but with a much older history. “Earth science” encompasses four main branches of study, the lithosphere, the hydrosphere, the atmosphere, and the biosphere, each of which is further broken down into more specialized fields.\n\nThere are both reductionist and holistic approaches to Earth sciences. It is also the study of the Earth and its neighbors in space. Some Earth scientists use their knowledge of the Earth to locate and develop energy and mineral resources. Others study the impact of human activity on Earth's environment, and design methods to protect the planet. Some use their knowledge about Earth processes such as volcanoes, earthquakes, and hurricanes to plan communities that will not expose people to these dangerous events. \n\nThe Earth sciences can include the study of geology, the lithosphere, and the large-scale structure of the Earth's interior, as well as the atmosphere, hydrosphere, and biosphere. Typically, Earth scientists use tools from geography, chronology, physics, chemistry, biology, and mathematics to build a quantitative understanding of how the Earth works and evolves. Earth science affects our everyday lives. For example, meteorologists study the weather and watch for dangerous storms. Hydrologists study water and warn of floods. Seismologists study earthquakes and try to predict where they will strike. Geologists study rocks and help to locate useful minerals. Earth scientists mainly work “in the field”—climbing mountains, exploring the seabed, crawling through caves, or wading in swamps. They measure and collect samples (such as rocks or river water), then they record their findings on charts and maps.\n\nThe following fields of science are generally categorized within the Earth sciences:\n\nPlate tectonics, mountain ranges, volcanoes, and earthquakes are geological phenomena that can be explained in terms of physical and chemical processes in the Earth's crust.\nBeneath the Earth's crust lies the mantle which is heated by the radioactive decay of heavy elements. The mantle is not quite solid and consists of magma which is in a state of semi-perpetual convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics.\n\nPlate tectonics might be thought of as the process by which the Earth is resurfaced. As the result of seafloor spreading, new crust and lithosphere is created by the flow of magma from the mantle to the near surface, through fissures, where it cools and solidifies. Through subduction, oceanic crust and lithosphere returns to the convecting mantle.\n\nAreas of the crust where new crust is created are called \"divergent boundaries\", those where it is brought back into the Earth are \"convergent boundaries\" and those where plates slide past each other, but no new lithospheric material is created or destroyed, are referred to as transform (or conservative) boundaries Earthquakes result from the movement of the lithospheric plates, and they often occur near convergent boundaries where parts of the crust are forced into the Earth as part of subduction.\nVolcanoes result primarily from the melting of subducted crust material. Crust material that is forced into the asthenosphere melts, and some portion of the melted material becomes light enough to rise to the surface—giving birth to volcanoes.\n\nThe troposphere, stratosphere, mesosphere, thermosphere, and exosphere are the five layers which make up Earth's atmosphere. In all, the atmosphere is made up of about 78.0% nitrogen, 20.9% oxygen, and 0.92% argon. 75% of the gases in the atmosphere are located within the troposphere, the bottom-most layer. The remaining one percent of the atmosphere (all but the nitrogen, oxygen, and argon) contains small amounts of other gases including CO and water vapors. Water vapors and CO allow the Earth's atmosphere to catch and hold the Sun's energy through a phenomenon called the greenhouse effect. This allows Earth's surface to be warm enough to have liquid water and support life. In addition to storing heat, the atmosphere also protects living organisms by shielding some of the Earth's surface from cosmic rays—of which are often incorrectly thought to be deflected by the magnetic field. The magnetic field—created by the internal motions of the core—produces the magnetosphere which protects the Earth's atmosphere from the solar wind. As the Earth is 4.5 billion years old, it would have lost its atmosphere by now if there were no protective magnetosphere.\n\nAn electromagnet is a magnet that is created by an electric current. The Earth has a solid iron inner core surrounded by fluid outer core that convects; therefore, the Earth is an electromagnet. The motion of fluid convection sustains the Earth's magnetic field.\n\nMethodologies vary depending on the nature of the subjects being studied. Studies typically fall into one of three categories: observational, experimental, or theoretical. Earth scientists often conduct sophisticated computer analysis or go to many of the world's most exotic locations to study Earth phenomena (e.g. Antarctica or hot spot island chains).\n\nA foundational idea within the study Earth science is the notion of uniformitarianism. Uniformitarianism dictates that \"ancient geologic features are interpreted by understanding active processes that are readily observed.\" In other words, any geologic processes at work in the present have operated in the same ways throughout geologic time. This enables those who study Earth's history to apply knowledge of how Earth processes operate in the present to gain insight into how the planet has evolved and changed throughout deep history.\n\nEarth science generally recognizes four spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere; these correspond to rocks, water, air and life. Also included by some are the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere and the pedosphere (corresponding to soil) as an active and intermixed sphere.\n\n\n\n\n\n\n\n\n\n"}
{"id": "5258948", "url": "https://en.wikipedia.org/wiki?curid=5258948", "title": "Ecodesign", "text": "Ecodesign\n\nEcodesign is an approach to designing products with special consideration for the environmental impacts of the product during its whole lifecycle. In a life cycle assessment, the life cycle of a product is usually divided into procurement, manufacture, use, and disposal.\n\nEcodesign is a growing responsibility and understanding of our ecological footprint on the planet. Green awareness, overpopulation, industrialization and an increased environmental population have led to the questioning of consumer values. It is imperative to search for new building solutions that are environmentally friendly and lead to a reduction in the consumption of materials and energy.\n\nAs the whole product's life cycle should be regarded in an integrated perspective, representatives from advance development, design, production, marketing, purchasing, and project management should work together on the Ecodesign of a further developed or new product. Together, they have the best chance to predict the holistic effects of changes of the product and their environmental impact.\n\nAn eco-design product has a cradle-to-cradle life cycle ensuring zero waste is created in the whole process. By mimicking life cycles in nature, eco-design is a fundamental concept in achieving a truly circular economy.\n\nEnvironmental aspects which ought to be analysed for every stage of the life cycle are:\n\n\nWaste (hazardous waste and other waste defined in environmental legislation) is only an intermediate step and the final emissions to the environment (e.g. methane and leaching from landfills) are inventoried. All consumables, materials and parts used in the life cycle phases are accounted for, and all indirect environmental aspects linked to their production.\n\nThe environmental aspects of the phases of the life cycle are evaluated according to their environmental impact on the basis of a number of parameters, such as extent of environmental impact, potential for improvement, or potential of change.\n\nAccording to this ranking the recommended changes are carried out and reviewed after a certain time.\n\nOne instrument to identify the factors that are important for the reduction of the environmental impact during all lifecycle stages is the environmental effect analysis (EEA).\n\nFor an EEA the following are taken into account:\n\n\nEcodesign concepts currently have a great influence on many aspects of design; the impact of global warming and an increase in CO₂ emissions have led companies to consider a more environmentally conscious approach to their design thinking and process.\nIn building design and construction, designers are taking on the concept of Ecodesign throughout the design process, from the choice of materials to the type of energy that is being consumed and the disposal of waste.\n\nWith respect to these concepts, online platforms dealing in only Ecodesign products are emerging, with the additional sustainable purpose of eliminating all unnecessary distribution steps between the designer and the final customer.\n\nEcoMaterials, such as the use of local raw materials, are less costly and reduce the environmental costs of shipping, fuel consumption, and CO₂ emissions generated from transportation. Certified green building materials, such as wood from sustainably managed forest plantations, with accreditations from companies such as the Forest Stewardship Council (FSC), or the Pan-European Forest Certification Council (PEFCC), can be used.\n\nSeveral other types of components and materials can be used in sustainable buildings. Recyclable and recycled materials are commonly used in construction, but it is important that they don’t generate any waste during manufacture or after their life cycle ends. Reclaimed materials such as timber at a construction site or junkyard can be given a second life by reusing them as support beams in a new building or as furniture. Stones from an excavation can be used in a retaining wall. The reuse of these items means that less energy is consumed in making new products and a new natural aesthetic quality is achieved.\n\nWater recycling systems such as rainwater tanks that harvest water for multiple purposes. Reusing grey water generated by households are a useful way of not wasting drinking water.\n\nOff-grid homes only use clean electric power. They are completely separated and disconnected from the conventional electricity grid and receive their power supply by harnessing active or passive energy systems.\n\nThese systems use the principle of harnessing the power generated from renewable and inexhaustible sources of energy, for example; solar, wind, thermal, biomass, and geothermal energy.\n\nSolar power is a widely known and used renewable energy source. An increase in technology has allowed solar power to be used in a wide variety of applications. Two types of solar panels generate heat into electricity. Thermal solar panels reduce or eliminate the consumption of gas and diesel, and reduce CO₂ emissions. Photovoltaic panels convert solar radiation into an electric current which can power any appliance. However, this is a more complex technology and is generally more expensive to manufacture than thermal panels.\n\nBiomass is the energy source created from organic materials generated through a forced or spontaneous biological process.\n\nGeothermal energy is obtained by harnessing heat from the ground. This type of energy can be used to heat and cool homes. It eliminates dependence on external energy and generates minimum waste. It is also hidden from view as it is placed underground, making it more aesthetically pleasing and easier to incorporate in a design.\n\nWind turbines are a useful application for areas without immediate conventional power sources, e.g., rural areas with schools and hospitals that need more power. Wind turbines can provide up to 30% of the energy consumed by a household but they are subject to regulations and technical specifications, such as the maximum distance at which the facility is located from the place of consumption and the power required and permitted for each property.\n\nBuildings that integrate passive energy systems (bioclimatic buildings) are heated using non-mechanical methods, thereby optimizing natural resources. The use of optimal daylight plays an integral role in passive energy systems. This involves the positioning and location of a building to allow and make use of sunlight throughout the whole year. By using the sun's rays, thermal mass is stored into the building materials such as concrete and can generate enough heat for a room.\n\nA green roof is a roof partially or completely covered with plants or other vegetation. This creates insulation that helps regulate the building's temperature. It also retains water, providing a water recycling system. It also provides soundproofing.\n\nRecycling has been used in art since the early part of the 20th century, when cubist artist Pablo Picasso (1881–1973) and Georges Braque (1882–1963) created collages from newsprints, packaging and other found materials. The \"Outside Art\" movement is recognized as a genuine expressive art form, and is celebrated because of the materials used and not in spite of them. The same principle can be used inside the home, where found objects are now displayed with pride and collecting certain objects and materials to furnish a home is now admired rather than looked down upon.\n\nThere is a huge demand in Western countries to decorate homes in a \"green\" style. A lot of effort is placed into recycled product design and the creation of a natural look. This ideal is also a part of developing countries, although their use of recycled and natural products is often based in necessity and wanting to get maximum use out of materials.\n\n\n\n"}
{"id": "212243", "url": "https://en.wikipedia.org/wiki?curid=212243", "title": "Electricity retailing", "text": "Electricity retailing\n\nElectricity retailing is the final sale of electricity from generation to the end-use consumer. This is the fourth major step in the electricity delivery process, which also includes generation, transmission and distribution.\n\nElectricity retailing began at the end of the 19th century when the bodies which generated electricity for their own use made supply available to third parties. In the beginning, electricity was primarily used for street lighting and trams. The public could buy once large scale\nelectric companies had been started.\n\nThe provision of these services was generally the responsibility of electric companies or municipal authorities who either set up their own departments or contracted the services from private entrepreneurs. Residential, commercial and industrial use of electricity was confined, initially, to lighting but this changed dramatically with the development of electric motors, heaters and communication devices.\n\nThe basic principle of supply has not changed much over time. The amount of energy used by the domestic consumer, and thus the amount charged for, is measured through an electricity meter that is usually placed near the input of a home to provide easy access to the meter reader.\n\nCustomers are usually charged a monthly service fee and additional charges based on the electrical energy (in kWh) consumed by the household or business during the month. Commercial and industrial consumers normally have more complex pricing schemes. These require meters that measure the energy usage in time intervals (such as a half-hour) to impose charges based on both the amount of energy consumed and the maximum rate of consumption, i.e. the maximum demand. This is usually called peak demand charge. Frequent reporting also allows the retailer to pass on the spot price (with some markup) to its customers.\n\nThe rapid growth in electric appliance usage in the early part of the 20th century contributed to an explosive growth in electrification around the world.\n\nThe supply of electricity to homes, offices, shops, factories, farms, and mines became the responsibility of public utilities, which were either private organizations subject to monopoly regulation or public authorities owned by local, state or national bodies.\n\nIn some countries a statutory or government-granted monopoly was created to be controlled by legislation, for example Eskom in South Africa.\nElectricity retailing in the period from approximately 1890 to 1990 consisted of managing the connection, disconnection and billing of electricity consumers by the local monopoly supplier.\n\nIn many utilities there was a marketing function which encouraged electricity usage when there was excess capacity to supply and encouraged conservation when supply was tight.\n\nAn electricity provider is often known as \"the electric company\" or \"the power company\".\n\nIn 1990 there was a significant development in the way electricity was bought and sold. In many countries, the electricity market was deregulated to open up the supply of electricity to competition. In the United Kingdom the electricity supply industry was radically reformed to establish competition, including a market in advising users about switching supplier. This trend continued in other countries (see New Zealand Electricity Market and deregulation) and the role of electricity retailing changed from what was essentially an administrative function within an integrated utility to become a risk management function within a competitive electricity market.\n\nElectricity retailers now offer fixed prices or variable for electricity to their customers and manage the risk involved in purchasing electricity from spot markets or electricity pools. This development has not been without casualties. The most notable example of poor risk management (coupled with poor market regulation) was the 2001 California electricity crisis, when Pacific Gas and Electric and Southern California Edison were driven into bankruptcy by having to purchase electricity at high spot prices and sell at low fixed rates.\n\nCustomers may choose from a number of competing suppliers. They may also opt to purchase \"green\" power, i.e. electricity sourced from renewable energy generation such as wind power or solar power.\n\nOver the past several decades, many US states have moved to deregulate their electric markets, with 24 states allowing for at least some competition among retail electric providers (REPs) including California, Texas, and New York. These companies include Ambit Energy, TXU, Just Energy, Con Edison, Champion Energy, Constellation Energy, and Reliant Energy among many others. Deregulation of electric retailers has been subject to much controversy as more states have opted for competitive markets.\n\n"}
{"id": "29946214", "url": "https://en.wikipedia.org/wiki?curid=29946214", "title": "Energy Regulators Regional Association", "text": "Energy Regulators Regional Association\n\nThe Energy Regulators Regional Association (ERRA) is a voluntary organization of independent energy regulatory bodies primarily from the Central European and Eurasian region, with Affiliates from Africa, Asia the Middle East and the USA. \n\nThe purpose and objectives of the association are:\n\nThe first energy regulatory bodies of the ERRA region were established in the mid-1990s as an essential part of restructuring and reforms taking place in these countries. ERRA began as a cooperative initiative of 12 energy regulatory bodies. They were then supported from 1999 to 2008 by the US National Association of Regulatory Utility Commissioners (NARUC), which, with the participation of USAID, arranged technical forums, meetings and study tours for mutual training and development. As a consequence, fifteen energy regulators established ERRA on 11 December 2000 in Bucharest. The association was registered in Hungary in April 2001 and its secretariat is based in Budapest. To date ERRA lists 23 full and 14 associate members.\n\n\n\nERRA activities are conducted in both English and Russian.\n\n"}
{"id": "240391", "url": "https://en.wikipedia.org/wiki?curid=240391", "title": "Fall line", "text": "Fall line\n\nA fall line (or fall zone) is the geomorphologic break that demarcates the border between an upland region of relatively hard crystalline basement rock and a coastal plain of softer sedimentary rock. A fall line is typically prominent when crossed by a river, for there will often be rapids or waterfalls. Many times a fall line will recede upstream as the river cuts out the uphill dense material, often forming \"c\"-shaped waterfalls and exposing bedrock shoals. Because of these features riverboats typically cannot travel any farther inland without portaging, unless locks are built there. On the other hand, the rapid change in elevation of the water, and the resulting energy release, makes the fall line a good location for water mills, grist mills, and sawmills. Because of the need for a river port leading to the ocean, and a ready supply of water power, settlements often develop where rivers cross a fall line.\n\nThe slope of fall zones on rivers played a role in settlement patterns. For example, the fall line represents the inland limit of navigation on many rivers. As such, many fall line cities grew around transferring people and goods between land-based and water-based transportation at this point. Also, fall lines proved useful for hydroelectric dams such as those at Rochester, New York (on the Niagara Escarpment), at Columbia, South Carolina, and at Conowingo, Maryland, on the Susquehanna River (on the Atlantic Seaboard fall line). Cities established along fall lines in the United States include:\n\n\nThe Atlantic Seaboard Fall Line, or Fall Zone, is a escarpment where the Piedmont and Atlantic Coastal Plain meet in the eastern United States. Much of the Atlantic Seaboard fall line passes through areas where no evidence of faulting is present.\n\nThe fall line marks the geologic boundary of hard metamorphosed terrain—the product of the Taconic orogeny—and the sandy, relatively flat outwash plain of the upper continental shelf, formed of unconsolidated Cretaceous and Tertiary sediments. Examples of the Fall Zone include the Potomac River's Little Falls and the rapids in Richmond, Virginia, where the James River falls across a series of rapids down to the tidal estuary of the James River. Columbia, South Carolina is similar as well with the Congaree River.\n\nBefore navigation improvements such as locks, the fall line was often the head of navigation on rivers due to rapids and waterfalls, such as the Little Falls of the Potomac River. Numerous cities were founded at the intersection of rivers and the fall line. U.S. Route 1 links many of the fall line cities.\n\n\n"}
{"id": "32631220", "url": "https://en.wikipedia.org/wiki?curid=32631220", "title": "Ground vibrations", "text": "Ground vibrations\n\nGround vibrations is a technical term that is being used to describe mostly man-made vibrations of the ground, in contrast to natural vibrations of the Earth studied by seismology. For example, vibrations caused by explosions, construction works, railway and road transport, etc. - all belong to ground vibrations. \n\nLike in seismology, ground vibrations are associated with different types of elastic waves propagating through the ground. These are surface waves, mostly Rayleigh waves, and bulk longitudinal waves and transverse waves (or shear waves) propagating into the ground depth. Typical frequency range for environmental ground vibrations is 1 – 200 Hz. Waves of lower frequencies (below 1 Hz) are usually called microseisms, and they are normally associated with natural phenomenae, e.g. water waves in the oceans. Environmental ground vibrations generated by rail and road traffic may cause annoyance to residents of nearby buildings both directly and via generated structure-borne interior noise. Very strong ground vibrations, e.g. generated by heavy lorries on bumped roads, may even cause structural damage to very close buildings. Magnitudes of ground vibrations are usually described in terms of particle vibration velocity (in mm/s or m/s). Sometimes they are also described in decibels (relative to the reference particle velocity of 10 m/s). Typical values of ground vibration particle velocity associated with vehicles passing over traffic calming road humps are in the range of 0.1 – 2 mm/s. Magnitudes of ground vibrations that are considered to be able to cause structural damage to buildings are above 10–20 mm/s.\n\nThe main sources of ground vibrations generated by railway trains are dynamic forces transmitted from tracks to the ground. These forces are associated with complex processes of interaction of moving train axles with railway tracks supported by the elastic ground. The magnitudes of these forces generally increase with the increase of train speeds. Therefore, the levels of generated ground vibrations may be substantial in the case of high-speed trains. If a train speed becomes larger than Rayleigh wave velocity in the ground, an additional very large increase in generated ground vibrations takes place. This phenomenon is termed ground vibration boom, and it is similar to sonic boom generated by supersonic aircraft.\n\nThe main mechanism responsible for generation of ground vibrations by moving cars and lorries is the dynamic forces associated with vehicle passage over road irregularities, such as bumps, peats, etc. These forces, and hence generated ground vibrations, can be reduced by keeping road surfaces in good condition.\n\nThe main sources of ground vibrations at construction are pile driving, dynamic compaction, blasting, and operation of heavy construction equipment. These vibrations may harmfully affect surrounding buildings, and their effect ranges from disturbance of residents to visible structural damage.\n\n\n\n"}
{"id": "27646672", "url": "https://en.wikipedia.org/wiki?curid=27646672", "title": "Gusher of Lies", "text": "Gusher of Lies\n\nGusher of Lies: The Dangerous Delusions of Energy Independence is a book by Robert Bryce which was released in 2008 and is published by PublicAffairs.\n\nIn 2008 the United States imported 60 per cent of its oil. Bryce argues that, given the size of its energy needs, the United States needs to continue importing oil. He also favors increasing domestic production of oil and gas. He acknowledges that energy independence has appeal as a slogan, but says that the reality is energy interdependence.\n\nAs in his later book \"Power Hungry\" he makes a case that renewable sources such as wind power and solar energy cannot meet the United States' (growing) energy requirements. Bryce dismisses ethanol fuel as having a cost that far outweighs the benefits. However, he believes that as world prices rise in the longer term, it will be economic to move to non-fossil sources.\n\nDr. Michael J. Economides writing for \"Forbes\" magazine said Bryce \"fires in all directions, at liberals, conservatives, Republicans and Democrats. His is a no-holds barred attack on what he believes is not just the sale of a pipedream in a global economy but one whose pursuit has become the basis for massive worldwide scams. If for nothing else, that makes this book a must read for anyone interested in navigating a future energy path\".\n\nWilliam Grimes wrote in the \"New York Times\" that Bryce employs a \"slashing, ad hominem style of attack that can undercut his credibility, especially when he moves away from economics and technology and ventures into politics, an arena to which he brings no particular expertise.\"\n\nRichard Whittaker writing in the \"Austin Chronicle\" said, \"Bryce makes a solid case that energy independence, a totem to both left and right, is not simply unachievable and undesirable but harmful. He depicts himself as the reasonable man facing down a cadre of xenophobes, Luddites, boondogglers, and politicos. He doesn't differentiate between the guilty parties along political lines, laying into Milton Friedman and Al Gore equally. But there are bogus conflations, like saying energy consumption equals wealth and wealth equals low child-mortality rates, even though the Btu-guzzling U.S. has one of the worst infant death rates of all industrialized nations\".\n\n\n"}
{"id": "8042665", "url": "https://en.wikipedia.org/wiki?curid=8042665", "title": "History of soil science", "text": "History of soil science\n\nThe early concepts of soil were based on ideas developed by a German chemist, Justus von Liebig (1803–1873), and modified and refined by agricultural scientists who worked on samples of soil in laboratories, greenhouses, and on small field plots. The soils were rarely examined below the depth of normal tillage. These chemists held the \"balance-sheet\" theory of plant nutrition. Soil was considered a more or less static storage bin for plant nutrients—the soils could be used and replaced. This concept still has value when applied within the framework of modern soil science, although a useful understanding of soils goes beyond the removal of nutrients from soil by harvested crops and their return in manure, lime, and fertilizer.\n\nThe early geologists generally accepted the balance-sheet theory of soil fertility and applied it within the framework of their own discipline. They described soil as disintegrated rock of various sorts—granite, sandstone, glacial till, and the like. They went further, however, and described how the weathering processes modified this material and how geologic processes shaped it into landforms such as glacial moraines, alluvial plains, loess plains, and marine terraces. Geologist Nathaniel Shaler (1841–1906) monograph (1891) on the origin and nature of soils summarized the late 19th century geological concept of soils.\n\nEarly soil surveys were made to help farmers locate soils responsive to different management practices and to help them decide what crops and management practices were most suitable for the particular kinds of soil on their farms. Many of the early workers were geologists because only geologists were skilled in the necessary field methods and in scientific correlation appropriate to the study of soils. They conceived soils as mainly the weathering products of geologic formations, defined by landform and lithologic composition. Most of the soil surveys published before 1910 were strongly influenced by these concepts. Those published from 1910 to 1920 gradually added greater refinements and recognized more soil features but retained fundamentally geological concepts.\n\nThe balance-sheet theory of plant nutrition dominated the laboratory and the geological concept dominated field work. Both approaches were taught in many classrooms until the late 1920s. Although broader and more generally useful concepts of soil were being developed by some soil scientists, especially Eugene W. Hilgard (1833–1916) and George Nelson Coffey (1875-1967) in the United States and soil scientists in Russia, the necessary data for formulating these broader concepts came from the field work of the soil survey. \n\nBeginning in 1870, the Russian school of soil science under the leadership of V. V. Dokuchaev (1846–1903) and N. M. Sibirtsev (1860–1900) was developing a new concept of soil. The Russian workers conceived of soils as independent natural bodies, each with unique properties resulting from a unique combination of climate, living matter, parent material, relief, and time. They hypothesized that properties of each soil reflected the combined effects of the particular set of genetic factors responsible for the soil's formation. Hans Jenny later emphasized the functionally relatedness of soil properties and soil formation. The results of this work became generally available to Americans through the publication in 1914 of K.D. Glinka's textbook in German and especially through its translation into English by C.F. Marbut in 1927.\n\nThe Russian concepts were revolutionary. Properties of soils no longer were based wholly on inferences from the nature of the rocks or from climate or other environmental factors, considered singly or collectively; rather, by going directly to the soil itself, the integrated expression of all these factors could be seen in the morphology of the soils. This concept required that all properties of soils be considered collectively in terms of a completely integrated natural body. In short, it made possible a science of soil.\n\nThe early enthusiasm for the new concept and for the rising new discipline of soil science led some to suggest the study of soil could proceed without regard to the older concepts derived from geology and agricultural chemistry. Certainly the reverse is true. Besides laying the foundation for a soil science with its own principles, the new concept makes the other sciences even more useful. Soil morphology provides a firm basis on which to group the results of observation, experiments, and practical experience and to develop integrated principles that predict the behavior of the soils. \n\nUnder the leadership of C. F. Marbut, the Russian concept was broadened and adapted to conditions in the United States.This concept emphasized individual soil profiles to the subordination of external soil features and surface geology. By emphasizing soil profiles, however, soil scientists at first tended to overlook the natural variability of soils which can be substantial even within a small area. Overlooking the variability of soils seriously reduced the value of the maps which showed the location of the soils.\n\nFurthermore, early emphasis on genetic soil profiles was so great as to suggest that material lacking a genetic profile, such as recent alluvium, was not soil. A sharp distinction was drawn between rock weathering and soil formation. Although a distinction between these sets of processes is useful for some purposes, rock and mineral weathering and soil formation are commonly indistinguishable.\n\nThe concept of soil was gradually broadened and extended during the years following 1930, essentially through consolidation and balance. The major emphasis had been on the soil profile. After 1930, morphological studies were extended from single pits to long trenches or a series of pits in an area of a soil. The morphology of a soil came to be described by ranges of properties deviating from a central concept instead of by a single \"typical\" profile. The development of techniques for mineralogical studies of clays also emphasized the need for laboratory studies.\n\nMarbut emphasized strongly that classification of soils should be based on morphology instead of on theories of soil genesis, because theories are both ephemeral and dynamic. He perhaps overemphasized this point to offset other workers who assumed that soils had certain characteristics without examining the soils. Marbut tried to make clear that examination of the soils themselves was essential in developing a system of Soil Classification and in making usable soil maps. In spite of this, Marbut's work reveals his personal understanding of the contributions of geology to soil science. His soil classification of 1935 depends heavily on the concept of a \"normal soil,\" the product of equilibrium on a landscape where downward erosion keeps pace with soil formation.\n\nClarification and broadening of the concept of a soil science also grew out of the increasing emphasis on detailed soil mapping. Concepts changed with increased emphasis on predicting crop yields for each kind of soil shown on the maps. Many of the older descriptions of soils had not been quantitative enough and the units of classification had been too heterogeneous for making yield and management predictions needed for planning the management of individual farms or fields.\n\nDuring the 1930s, soil formation was explained in terms of loosely conceived processes, such as \"podzolization,\" \"laterization,\" and \"calcification.\" These were presumed to be unique processes responsible for the observed common properties of the soils of a region. \n\nIn 1941 Hans Jenny's (1899–1992) Factors of Soil Formation, a system of quantitative pedology, concisely summarized and illustrated many of the basic principles of modern soil science to that date. Since 1940, time has assumed much greater significance among the factors of soil formation, and geomorphological studies have become important in determining the time that soil material at any place has been subjected to soil-forming processes. Meanwhile, advances in soil chemistry, soil physics, soil mineralogy, and soil biology, as well as in the basic sciences that underlie them, have added new tools and new dimensions to the study of soil formation. As a consequence, the formation of soil has come to be treated as the aggregate of many interrelated physical, chemical, and biological processes. These processes are subject to quantitative study in soil physics, soil chemistry, soil mineralogy, and soil biology. The focus of attention also has shifted from the study of gross attributes of the whole soil to the co-varying detail of individual parts, including grain-to-grain relationships. \n\nIn both the classification of Marbut and the 1938 classification developed by the U.S. Department of Agriculture, the classes were described mainly in qualitative terms. Classes were not defined in quantitative terms that would permit consistent application of the system by different scientists. Neither system definitely linked the classes of its higher categories, largely influenced by genetic concepts initiated by the Russian soil scientists, to the soil series and their subdivisions that were used in soil mapping in the United States. Both systems reflected the concepts and theories of soil genesis of the time, which were themselves predominantly qualitative in character. Modification of the 1938 system in 1949 corrected some of its deficiencies but also illustrated the need for a reappraisal of concepts and principles. More than 15 years of work under the leadership of Guy Smith culminated in a new soil classification system. This became the official classification system of the U.S. National Cooperative Soil Survey in 1965 and was published in 1975 as Soil Taxonomy: A Basic System of Soil Classification for Making and Interpreting Soil Surveys. The Smith system was adopted in the U.S. and many other nations for their own classification system.\n\nAnother factor has had an immense impact on soil survey, especially during the 1960s. Before 1950, the primary applications of soil surveys were farming, ranching, and forestry. Applications for highway planning were recognized in some States as early as the late 1920s, and soil interpretations were placed in field manuals for highway engineers of some States during the 1930s and 1940s. Nevertheless, the changes in soil surveys during this period were mainly responses to the needs of farming, ranching, and forestry. During the 1950s and 1960s nonfarm uses of the soil increased rapidly. This created a great need for information about the effects of soils on those nonfarm uses. \n\nA major re-evaluation of soil formation and the role of biota commenced in the 1980s, as soil-geomorphologists began to re-evaluate Charles Darwin's and Nathaniel Shaler's early ideas on the role of bioturbation in soil formation. There is now ample evidence to support Darwin's conclusions, and in many areas biota that burrow in soil are major agents of pedogenesis.\n\n\n\n"}
{"id": "52784957", "url": "https://en.wikipedia.org/wiki?curid=52784957", "title": "I Carinae", "text": "I Carinae\n\nThe Bayer designations i Carinae and I Carinae are distinct.\n\n\n"}
{"id": "31508894", "url": "https://en.wikipedia.org/wiki?curid=31508894", "title": "J-aggregate", "text": "J-aggregate\n\nA J-aggregate is a type of dye with an absorption band that shifts to a longer wavelength (bathochromic shift) of increasing sharpness (higher absorption coefficient) when it aggregates under the influence of a solvent or additive or concentration as a result of supramolecular self-organisation. The dye can be characterized further by a small Stokes shift with a narrow band. The J in J-aggregate refers to E.E. Jelley who discovered the phenomenon in 1936. The dye is also called a Scheibe aggregate after G. Scheibe who also independently published on this topic in 1937.\n\nScheibe and Jelley independently observed that in ethanol the dye PIC chloride has two broad absorption maxima at around 19,000 cm and 20,500 cm (526 and 488 nm respectively) and that in water a third sharp absorption maximum appears at 17,500 cm (571 nm). The intensity of this band further increases on increasing concentration and on adding sodium chloride. In the oldest aggregation model for PIC chloride the individual molecules are stacked like a roll of coins forming a supramolecular polymer but the true nature of this aggregation phenomenon is still under investigation. Analysis is complicated because PIC chloride is not a planar molecule. The molecular axis can tilt in the stack creating a helix pattern. In other models the dye molecules orient themselves in a brickwork, ladder, or staircase fashion. In various experiments the J-band was found to split as a function of temperature, liquid crystal phases were found with concentrated solutions and CryoTEM revealed aggregeate rods 350 nm long and 2.3 nm in diameter.\n\nJ-aggregate dyes are found with polymethine dyes in general, with cyanines, merocyanines, squaraine and perylene bisimides. Certain π-conjugated macrocycles, reported by Swager and co-workers at MIT, were also found to form J-aggregates and exhibited exceptionally high photoluminescence quantum yields. \n\nIn H-aggregates a hypsochromic shift is observed with low or no fluorescence.\n"}
{"id": "52745530", "url": "https://en.wikipedia.org/wiki?curid=52745530", "title": "Lagg (landform)", "text": "Lagg (landform)\n\nA lagg, also called a moat, is the very wet zone on the perimeter of peatland or a bog where water from the adjacent upland collects and flows slowly around the main peat mass. \n\nA lagg is an area of wetland, especially at the edge of raised bogs, in which water collects. It is often markedly different from the terrain either side and may consist of a morass of shrubs and murky water.\n\nIn addition to water gathered from surrounding uplands, the lagg also picks up water flowing down from the domed centre of a raised bog through small channels - soaks or water tracks - to the steeply sloping shoulder or \"rand\" of the bog. At the foot of the \"rand\", the water collects and meets the water of the surrounding area on the boundary between the peat soil and mineral soil.\n\n"}
{"id": "27321656", "url": "https://en.wikipedia.org/wiki?curid=27321656", "title": "Lake Köyceğiz", "text": "Lake Köyceğiz\n\nThe surroundings of the lake as a whole and particularly the banks of its Dalyan sea connection are important nature reserves and popular tourist attractions. \nThey are part of the Köyceğiz-Dalyan Special Environmental Protection Area\n\nThe lake is fed by the Namnam and Yuvarlakçay rivers and a number of mountain brooks. The water of brooks, melting water and fresh water wells, mixes with warm sulfurous water that is released from a fault and mildly brackish, oxygenated water that flows upriver with the rising tide. The depth of the lake varies from 20 to 60 metres. The lake is abundant with fish.\n\nAt the northeast and southeast of Köyceğiz Lake there are plains, whereas other parts are surrounded by hills. The highest of these are Ölemez Mountain (937 meter) in the southwest, and Bozburun Tepesi (556 meter) in the south. In the lake there are five uninhabited islands, one of them known as\" Hapishane Adası\", Prison Island. Initially the island was used for military purposes, then turned into a prison island. It is no longer in use as such and deserted.\n\nThe lake was formed about 7500 years ago, when the entire eastern part of the Mediterranean was afflicted by earthquakes. At the south side of Köyceğiz Lake, there is a NW - SE fault line, bordered by several sulfurous hot springs, amongst others Sultaniye Spa.\n\nThe lake has evolved into an ecologically important area. Together with its banks and the Dalyan basin it comprises the Köyceğiz-Dalyan Special Environmental Protection Area. \nOn the banks north of Sultaniye, northwest of Hamitköy, at Köyceğiz, at Kavakarası and at Tepearası, there are still vast marshy forests with the endemic Oriental Gumtree. The species primarily occurs in the province of Muğla, near Marmaris and in the Köyceğiz-Dalyan SEPA. The small wetlands around the lake are interesting because of their birdlife. The area attracts many birdwatchers. The area is also rich with reptiles (tortoises, turtles, terrapins, snakes) and insects (dragonflies, damselflies). Among the fish species in the lake are two endemic dwarf gobies, \"Knipowitschia byblisia\" and \"K. caunosi\", which were only described to science in 2011.\n\n"}
{"id": "17181080", "url": "https://en.wikipedia.org/wiki?curid=17181080", "title": "List of Maryland hurricanes (1950–present)", "text": "List of Maryland hurricanes (1950–present)\n\nSince 1950, 121 known hurricanes, tropical storms and tropical depressions have affected the U.S. state of Maryland. Many of these storms also affect the country's capital, Washington, D.C., since the city is located on territory ceded by Maryland. Hurricanes are the most intense classification of these storms, while tropical storms and tropical depressions are generally weaker. The Delmarva Peninsula is often affected by cyclones that brush the East Coast. Central and Western Maryland, as well as Washington, D.C., commonly receive rainfall from the remnants of storms that make landfall elsewhere and track northward. On rare occasions, the area experiences the effects of Pacific storms; one such example of this is Hurricane Tico, which made landfall on Mexico and moved inland.\n\nHurricane Agnes of the 1972 season was the deadliest storm, killing 19 people as a result of heavy flooding. The most damaging storm was Hurricane Irene, which resulted in $151 million in damage. Hurricane Hazel caused sustained hurricane-force winds (winds of 75 mph (121 km/h) or greater) in the state, the only storm during the time period to do so. No storms made landfall in Maryland at hurricane intensity. Since 1950, twelve tropical cyclones have collectively killed 61 people.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table includes all storms since 1950 that have caused reported fatalities in Maryland and Washington, D.C.\n\n"}
{"id": "42997249", "url": "https://en.wikipedia.org/wiki?curid=42997249", "title": "List of Planchonella species", "text": "List of Planchonella species\n\nThis is a list of species in the plant genus \"Planchonella\". \n\n"}
{"id": "56016045", "url": "https://en.wikipedia.org/wiki?curid=56016045", "title": "List of lyrate plants", "text": "List of lyrate plants\n\nThe following plants have leaves that are lyrate:\n\n"}
{"id": "53026436", "url": "https://en.wikipedia.org/wiki?curid=53026436", "title": "List of meteorological histories of tropical cyclones", "text": "List of meteorological histories of tropical cyclones\n\nThis page documents the list of meteorological histories of tropical cyclones. These storms have detailed information on their respective histories as tropical cyclones, including formation, peak intensity, dissipation, and other notable facts the storm was known for.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38614426", "url": "https://en.wikipedia.org/wiki?curid=38614426", "title": "List of nature centers in Vermont", "text": "List of nature centers in Vermont\n\nThis is a list of nature centers and environmental education centers in the state of Vermont. \n\nTo use the sortable tables: click on the icons at the top of each column to sort that column in alphabetical order; click again for reverse alphabetical order.\n\n\n"}
{"id": "28000284", "url": "https://en.wikipedia.org/wiki?curid=28000284", "title": "List of nuclear and radiation fatalities by country", "text": "List of nuclear and radiation fatalities by country\n\nThis is a List of nuclear and radiation fatalities by country. Not to be confused with the \"List of nuclear power accidents by country\"\n\nThis list only reports the proximate confirmed human deaths and does not go into detail about ecological, environmental or long-term effects such as birth defects or permanent loss of habitable land.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \"World Nuclear\"\n"}
{"id": "195198", "url": "https://en.wikipedia.org/wiki?curid=195198", "title": "Mist", "text": "Mist\n\nMist is a phenomenon caused by small droplets of water suspended in air. Physically, it is an example of a dispersion. It is most commonly seen where warm, moist air meets sudden cooling, such as in exhaled air in the winter, or when throwing water onto the hot stove of a sauna. It can be created artificially with aerosol canisters if the humidity and temperature conditions are right. It can also occur as part of natural weather, when humid air cools rapidly, for example when the air comes into contact with surfaces that are much cooler than the air.\n\nThe formation of mist, as of other suspensions, is greatly aided by the presence of nucleation sites on which the suspended water phase can congeal. Thus even such unusual sources as small particulates from volcanic eruptions, releases of strongly polar gases, and even the magnetospheric ions associated with polar lights can in right conditions trigger the formation of mist and can make mirrors appear foggy.\n\nCloud cover is often referred to as \"mist\" when encountered on mountains, whereas moisture suspended above a body of water or marsh area is usually called \"fog\". One difference between mist and fog is visibility. The phenomenon is called fog if the visibility is or less. In the U.K., the definition of fog is visibility less than (for driving purposes, UK Highway Code rule 226), while for pilots the distance is 1 km. Otherwise, it is known as mist.\n\nMist makes a light beam visible from the side via refraction and scattering on the suspended water droplets.\n\n\"Scotch mist\" is a light steady drizzle.\n\nMist usually occurs near the shores and is often associated with fog. Mist can be as high as mountain tops when extreme temperatures are low.\n\nFreezing mist is similar to freezing fog, only the density is less and the visibility greater (when fog falls below , it is known as freezing fog).\n\n"}
{"id": "13605193", "url": "https://en.wikipedia.org/wiki?curid=13605193", "title": "Oceanic core complex", "text": "Oceanic core complex\n\nAn oceanic core complex, or megamullion, is a seabed geologic feature that forms a long ridge perpendicular to a mid-ocean ridge. It contains smooth domes that are lined with transverse ridges like a corrugated roof. They can vary in size from 10 to 150 km in length, 5 to 15 km in width, and 500 to 1500 m in height.\n\nThe first oceanic core complexes described were identified in the Atlantic Ocean. Since then numerous such structures have been identified primarily in oceanic lithosphere formed at intermediate, slow- and ultra-slow spreading mid-ocean ridges, as well as back-arc basins. Examples include 10-1000 square km expanses of ocean floor and therefore of the oceanic lithosphere, particularly along the Mid-Atlantic Ridge and the Southwest Indian Ridge. Some of these structures have been drilled and sampled, showing that the footwall can be composed of both mafic plutonic and ultramafic rocks (gabbro and peridotite primarily, in addition to diabase), and a thin shear zone that includes hydrous phyllosilicates. Oceanic core complexes are often associated with active hydrothermal fields.\n\nOceanic core complex structures form at slow spreading oceanic plate boundaries which have a limited supply of upwelling magma. These zones have low upper mantle temperatures and long transform faults develop. Rift valleys do not develop along the expansion axes of slow spreading boundaries. Expansion takes place along low-angle detachment faults. The core complex builds on the uplifted side of the fault where most of the gabbroic (or crustal) material is stripped away to expose mantle peridotite. They comprise peridotites ultramafic rocks of mantle and to a lesser extent gabbroic rocks from the Earth's crust.\n\nEach detachment fault has three notable features: a breakaway zone where the fault began, an exposed fault surface that rides over the dome and a termination, which is usually marked by a valley and adjacent ridge.\n\nHowever, the formation process through detachment faults has its limitations, such as the scarce seismic evidence that low-angle normal faulting do exist, where the presumably significant offset along such faults, which transect the lithosphere at low angle should be involved with some friction. The rarity of eclogite in oceanic core complexes also casts doubt on the deep source in such domains. The abundance of peridotites in oceanic core complexes could be accounted for by a unique variation of ocean-ocean subduction at the junction of slow-spreading oceanic ridges and fracture zones. Analog models of subduction show that density contrast of more than 200 kg/m^3 between two juxtaposed lithospheric slabs would lead the underthrusting of the denser one to depth of circa 50 km, where remineralization of pyroxenes into garnets would increase the slab's density and drive it into the mantle, provided that the friction between the slabs would be low . There is ground to presume that at slow ridge and fracture zone intersections, the density contrast of the juxtaposed slabs would exceed 200 kg/m^3, the friction between the slabs would be low, the thermal gradient would be ca. 100 deg. C/km, and with ca. 5% water contents, the drop of the solidus of basalt at relatively low pressure would enable the co-occurrence of serpentinites and peridotites, the abundant rock-types in oceanic core complexes.\n\nSome 50 oceanic core complexes have been identified, including:\n\nScientific interest in core complexes has dramatically increased following an expedition in 1996 which mapped the Atlantis Massif. This expedition was the first to associate the complex structures with detachment faults. Research includes:\n\n\n"}
{"id": "2128950", "url": "https://en.wikipedia.org/wiki?curid=2128950", "title": "Onsite sewage facility", "text": "Onsite sewage facility\n\nOnsite (or on-site) sewage facilities (OSSF) are wastewater systems designed to treat and dispose of effluent on the same property that produces the wastewater.\n\nA septic tank and drainfield combination is a fairly common type of on-site sewage facility in the United States. OSSFs account for approximately 25% of all domestic wastewater treatment in the United States. Onite sewage facilities may also be based on small-scale aerobic and biofilter units, membrane bioreactors or sequencing batch reactors. These can be thought of as scaled down versions of municipal sewage treatment plants.\n\nThe primary mechanism of biological waste recycling in the natural environment is performed by other organisms such as animals, insects, soil microorganisms, plants, and fungi, which consume all available nutrients in the waste, leaving behind fully decomposed solids that become part of topsoil, and pure drinking water that has been stripped of everything that can possibly be consumed and utilized. This natural biological purification requires time and space to process wastes.\n\nIn virtually all engineered onsite sewage facilities, recycling and decomposition by natural organisms is still the primary mechanism of sewage disposal. Giving the organisms the time they need to decompose wastes is accomplished by establishing minimum sewage retention and settling times, and minimum liquid flow distances between sewage disposal sites and surface water or water wells.\n\nIt is normal for animals such as mice, rats, flies, and parasites to participate in the fully natural biological waste recycling process. Engineered facilities typically attempt to exclude them to prevent out of control population explosions and infestation, and prevent spread of vermin and disease.\n\nAlthough the solids collected by onsite sewage facilities can potentially be used as compost to build topsoil, these solids are often incompletely decomposed due to either a lack of onsite storage space to wait for decomposition (municipal facilities), or because the solids are being stacked in a layered structure of new waste solids on top of previously decomposed solids (septic tanks and outhouses). Due to the incomplete state of decomposition, when removed from an onsite sewage facility, these solids are typically referred to as \"sludge\" rather than \"compost\", and have powerful offensive odors arising from the microorganisms still consuming nutrients in the sludge. \n\nEngineered facilities that use water suspension to transport solids (private septic systems and municipal facilities) typically form a floating layer in their primary settling tank, consisting of low-density liquids such as oils, buoyant solids, and soap foam. This is referred to as \"scum\" and is slowly decomposed by microorganisms, eventually falling to the bottom of the settling tank as part of the sludge. When private septic tanks are emptied of solids, the tank is typically vacuumed empty and the incompletely digested scum is added to the incompletely digested sludge, further adding to its aroma and bioactivity.\n\nIf left completely undisturbed and exposed to the open air through a vent, the sludge and scum in a settling tank will eventually be turned completely into low-odor compost. By building two tanks side by side, and diverting sewage between them, one tank can be allowed to rest while the other is in use, and the resting tank can be safely and easily cleaned out by hand before it is used again. This has been proposed as a solution for onsite sewage facilities in subsistence agriculture economies where hand labor is the most abundant.\n\nAlthough human body waste is no different from the waste of any other animal, municipal facilities may be required to bury the collected solids in landfills, due to the risk of toxic contaminants placed into the shared communal sewage system, by humans unaware of the harm they are causing. Municipal facilities may also collect runoff from roadways, which contains traces of all the various chemicals used in vehicles such as brake fluid and engine oil, and those used in melting ice and snow. Private septic systems typically do not experience these issues, as the homeowner is directly aware that they must not pour toxic chemicals down the drain.\n\nMost onsite wastewater treatment systems are of the conventional type, consisting of a septic tank and a subsurface wastewater infiltration system (SWIS). Site limitations and more stringent performance requirements have led to significant improvements in the design of wastewater treatment systems and how they are managed. Over the past 20 years the onsite wastewater treatment system (OWTS) industry has developed many new treatment technologies that can achieve high performance levels on sites with size, soil, ground water, and landscape limitations that might preclude installing conventional systems. New technologies and improvements to existing technologies are based on defining the performance requirements of the system, characterizing wastewater flow and pollutant loads, evaluating site conditions, defining performance and design boundaries, and selecting a system design that addresses these factors.\n\nPerformance requirements can be expressed as numeric criteria (e.g., pollutant concentration or mass loading limits) or narrative criteria (e.g., no odors or visible sheen) and are based on the assimilative capacity of regional ground water or surface waters, water quality objectives, and public health goals. Wastewater flow and pollutant content help define system design and size and can be estimated by comparing the size and type of facility with measured effluent outputs from similar, existing facilities. Site evaluations integrate detailed analyses of regional hydrology, geology, and water resources with site specific characterization of soils, slopes, structures, property lines, and other site features to further define system design requirements and determine the physical placement of system components.\n\nMost of the alternative treatment technologies applied today treat wastes after they exit the septic tank; the tank retains settleable solids, grease, and oils and provides an environment for partial digestion of settled organic wastes. Post-tank treatment can include aerobic (with oxygen) or anaerobic (with no or low oxygen) biological treatment in suspended or fixed-film reactors, physical/chemical treatment, soil infiltration, fixed-media filtration, and/or disinfection. The application and sizing of treatment units based on these technologies are defined by performance requirements, wastewater characteristics, and site conditions.\n\nIn the United States, on site sewage facilities collect, treat, and release about of treated effluent per day from an estimated 26 million homes, businesses, and recreational facilities nationwide (U.S. Census Bureau, 1997). Recognition of the impacts of onsite systems on ground water and surface water quality (e.g., nitrate and bacteria contamination, nutrient inputs to surface waters) has increased interest in optimizing the systems' performance. Public health and environmental protection officials now acknowledge that onsite systems are not just temporary installations that will be replaced eventually by centralized sewage treatment services, but permanent approaches to treating wastewater for release and reuse in the environment. Onsite systems are recognized as viable, low-cost, long-term, decentralized approaches to wastewater treatment if they are planned, designed, installed, operated, and maintained properly (USEPA, 1997). NOTE: In addition to existing state and local oversight, decentralized wastewater treatment systems that serve more than 20 people might become subject to regulation under the USEPA's Underground Injection Control Program, although EPA has proposed not to include them (64FR22971:5/7/01).\n\nAlthough some onsite wastewater management programs have functioned successfully in the past, problems persist. Most current onsite regulatory programs focus on permitting and installation.\n\nFew programs address onsite system operation and maintenance, resulting in failures that lead to unnecessary costs and risks to public health and water resources. Moreover, the lack of coordination among agencies that oversee land use planning, zoning, development, water resource protection, public health initiatives, and onsite systems causes problems that could be prevented through a more cooperative approach. Effective management of onsite systems requires rigorous planning, design, installation, operation, maintenance, monitoring, and controls.\n\nState and tribal agencies report that onsite septic systems currently constitute the third most common source of groundwater pollution and that these systems have failed because of inappropriate siting or design or inadequate long-term maintenance (USEPA, 1996a). In the 1996 Clean Water Needs Survey (USEPA, 1996b), states and tribes also identified more than 500 communities as having failed septic systems that have caused public health problems. The discharge of partially treated sewage from malfunctioning onsite systems was identified as a principal or contributing source of degradation in 32 percent of all harvest-limited shellfish growing areas. Onsite wastewater treatment systems have also contributed to an overabundance of nutrients in ponds, lakes, and coastal estuaries, leading to the excessive growth of algae and other nuisance aquatic plants (USEPA, 1996b). In addition, onsite systems contribute to contamination of drinking water sources. USEPA estimates that 168,000 viral illnesses and 34,000 bacterial illnesses occur each year as a result of consumption of drinking water from systems that rely on improperly treated ground water. Malfunctioning septic systems have been identified as one potential source of ground water contamination (USEPA, 2000).\n\nThe potential market volume of on-site treatment is suggested to be about 35 million population equivalents for Europe.\n\n\n"}
{"id": "43133368", "url": "https://en.wikipedia.org/wiki?curid=43133368", "title": "Operation Dominic", "text": "Operation Dominic\n\nOperation Dominic was a series of 31 nuclear test explosions with a 38.1 Mt total yield conducted in 1962 by the United States in the Pacific. This test series was scheduled quickly, in order to respond in kind to the Soviet resumption of testing after the tacit 1958–1961 test moratorium. Most of these shots were conducted with free-fall bombs dropped from B-52 bomber aircraft. Twenty of these shots were to test new weapons designs; six to test weapons effects; and several shots to confirm the reliability of existing weapons. The Thor missile was also used to lift warheads into near-space to conduct high-altitude nuclear explosion tests; these shots were collectively called Operation Fishbowl.\n\nOperation Dominic occurred during a period of high Cold War tension between the United States and the Soviet Union, since the Cuban Bay of Pigs Invasion had occurred not long before. Nikita Khrushchev announced the end of a three-year moratorium on nuclear testing on August 30, 1961, and Soviet tests recommenced on September 1, initiating a series of tests that included the detonation of Tsar Bomba. President John F. Kennedy responded by authorizing \"Operation Dominic\". It was the largest nuclear weapons testing program ever conducted by the United States and the last atmospheric test series conducted by the U.S., as the Limited Test Ban Treaty was signed in Moscow the following year.\n\nThe operation was undertaken by Joint Task Force 8.\n\n"}
{"id": "42194433", "url": "https://en.wikipedia.org/wiki?curid=42194433", "title": "Power distribution center", "text": "Power distribution center\n\nA power distribution center (PDC) is electrical equipment designed to regulate the distribution of electrical power to various equipment, be that to machines in a factory or to various systems on an automotive vehicle. Typically, a switchgear supplies power to the PDC. The PDC housing contains a transformer, which steps down the incoming power to a lower voltage to feed other plant loads such as motor control centers (MCC's). Feeder breakers for these loads will be located on the PDC. PDC's will often contain additional protective relays, monitoring equipment, fuses, terminal points, etc.\n\n"}
{"id": "4430027", "url": "https://en.wikipedia.org/wiki?curid=4430027", "title": "SAIFI", "text": "SAIFI\n\nThe System Average Interruption Frequency Index (SAIFI) is commonly used as a reliability indicator by electric power utilities. SAIFI is the average number of interruptions that a customer would experience, and is calculated as\n\nformula_1\n\nwhere formula_2 is the failure rate, formula_3 is the number of customers for location formula_4 and formula_5 is the total number of customers served. In other words,\n\nformula_6\n\nSAIFI is measured in units of interruptions per customer. It is usually measured over the course of a year, and according to IEEE Standard 1366-1998 the median value for North American utilities is approximately 1.10 interruptions per customer.\n\n"}
{"id": "30677990", "url": "https://en.wikipedia.org/wiki?curid=30677990", "title": "Samuel Shenton", "text": "Samuel Shenton\n\nSamuel Shenton (March 1903 – 2 March 1971) was the founder in 1956 of the \"Flat Earth Society\", based in Dover, England. He lectured tirelessly on this to youth clubs, political and student groups and during the Space Race in the 1960s he was frequently seen on television and in newspapers promoting his views. He was well known for his promotion of the theory, but had a reputation as being a primary school dropout.\n\nSamuel Shenton was a signwriter, who lived with his wife Lillian in a ginger-brick terrace in suburban Dover. He was son of an army sergeant major, born in Great Yarmouth, and by the 1920s claimed to have invented an airship that would rise into the atmosphere and remain stationary until the earth spun westwards at to the desired destination at the same latitude. Shenton could not understand why someone had not previously thought of this idea until he discovered, in the reading room of the British Library at Bloomsbury that Archbishop Stevens, a friend of Lady Blount, the founder of the Universal Zetetic Society, had suggested an aircraft design similar to his own. When he discovered Parallax's \"Zetetic Astronomy\" he was an instant convert. \"What the authorities were concealing, Shenton decided, was the 'fact' that the earth was flat\"..\n\nHe discovered this theory in primary school, and had a reputation in his community as being an primary school drop out.\n\nShenton soon constructed a cosmology, based partly on his interpretation of Genesis, that the earth was a flat disk centred on the North Pole with the zetetic notion of the South Pole being an impenetrable wall of ice, that marked the edge of the pit that is the earth in the endless flat plane forming the universe. The sun cast a narrow beam like a flashlight moving over a table as it traced flat circles that varied over the 365-day cycles. The sun was in diameter above the earth and the moon also 32 miles in diameter but only above the earth.\n\nIn 1956 he founded the Flat Earth Society as a direct descendant of the Universal Zetetic Society but with a less religious emphasis, found a president in William Mills, a relative of one of Lady Blount's followers and held its inaugural meeting in November at Mills' home in London, with Shenton as secretary. One of the attendees, attending out of curiosity, was the \"Sky at Night\" astronomer Patrick Moore who recounted his experience in his book \"Can you speak Venusian?\". Shenton claimed that George Bernard Shaw attended one meeting, declaring the presentation was \"very persuasive\". Despite his contrary ideas, Shenton was elected a fellow of both the Royal Astronomical and Royal Geographical Societies.\n\nDespite the launch in October 1957 of Sputnik, the world's first artificial satellite, Shenton proved a popular speaker to small groups, enjoying particularly talking to children, never declining an invitation. He claimed that satellites simply circled over a flat disc-world: \"Would sailing round the Isle of Wight prove that it were spherical?\", he demanded. As manned space flight started in 1961, Shenton began to attract international media attention with his denials, telling the \"Coshocton Tribune\" on 10 May that the astronauts could never travel into orbit.\n\nWhen John Glenn orbited the world, he was sent an IFERS membership with the message \"Ok Wise Guy\" added to it. Shenton continued to lecture largely at his own expense but he suffered two strokes in 1963 probably as a result of his exertions. In January 1964 the \"New York Times\" carried a piece about the IFERS. During a parliamentary debate, Enoch Powell likened his opponents to \"flat-earthers\" and Harold Wilson reportedly slung back the insult in turn. Shenton was outraged and wrote letters of complaint.\n\nThe Gemini 4 mission marked a change of pace for his campaign and he was to receive letters from across the world for the next few years. In 1966 he produced a pamphlet, \"The Plane Truth\", which included a circular informing members \"that modern astronomy and space flight were insults to God and divine punishment for humankind's arrogance was a mere matter of time\". But the Lunar Orbiter program led to a sharp decline in membership. \"Visual images, whether they were globes, photographs or television pictures, were clearly critical to how people perceived the earth's shape .. and pre-school children could know that it was round even if they had no grasp of the words 'mathematics', 'geography', 'astronomy' and 'science'\".\n\nBy 1968, his health had deteriorated further and his signwriting business had collapsed although the media attention continued. But he stuck to his principles of 'zetetic enquiry' in which only personally acquired facts were permissible. In January 1969, IFERS had dropped to 100 members worldwide. But as his organization was dying, he found the successor he had been looking for: Ellis Hillman, a lecturer and member of the Greater London Council, agreed to be president of the IFERS, with the encouragement of Patrick Moore. Lillian Shenton was suspicious of his motives (he was developing a post-graduate course on the development of ideas about the shape of the earth) and in the event he did little for the society. Eighteen months later, Shenton had died.\n"}
{"id": "35887119", "url": "https://en.wikipedia.org/wiki?curid=35887119", "title": "Tau Gruis", "text": "Tau Gruis\n\nThe Bayer designation Tau Gruis (τ Gru / τ Gruis) is shared by two stars and a star system, in the constellation Grus:\n"}
{"id": "2469630", "url": "https://en.wikipedia.org/wiki?curid=2469630", "title": "The Neutral Ally", "text": "The Neutral Ally\n\nNorway is sometimes referred to as \"The Neutral Ally\". During World War I, while theoretically a neutral country, British pressure and anti-German sentiment in the population prompted the government to favour Britain highly in relation to Norway's large shipping fleet and vast fish supplies. The term was coined by Norwegian historian Olav Riste in the 1960s.\n\nIn 1905, when Norway gained independence, the nation's politicians agreed that Norway should remain neutral in international conflicts. Since the Great Powers had no desire for unrest in Scandinavia, they signed an agreement respecting Norway's neutrality. Still, the political direction was clear: fearing Russian ambition in the north, the sentiment was that Norway should be neutral if war broke out, and rely on help from Great Britain if attacked.\n\nThis affinity westwards was substantiated by international trade. In the early 1900s, Norway's merchant fleet was one of the largest in the world, and the country required vast supplies of oil, coal and steel to build and operate it. When war broke out in 1914, Norway was exporting great amounts of fish to Germans and British alike, much to the dismay of the British Government. The Allies started preventing the Germans from purchasing these fish stocks by overbidding them, but trade in other areas continued. Imports of Norwegian copper ore, nickel and pyrite were vital to the German war industry, and by the end of 1916, Norway's Government was put under heavy pressure. Several agreements were made, none completely satisfying to the British.\n\nOn Christmas Eve 1916, the British issued an ultimatum, informing the Norwegian Foreign Minister, Nils Claus Ihlen, that British exports of coal to Norway would cease unless trade with Germany stopped. The Norwegian Government felt they had no option but to comply with this demand. This coincided with Germany's expansion of unrestricted submarine warfare at the beginning of 1917. In total, 436 Norwegian ships were sunk in the period 1914–1917, out of 847 in the course of the whole war. More than 1150 sailors died during this period, creating an increasingly anti-German sentiment throughout the shipping nation of Norway.\n\nThus, both commerce and political sympathies tied Norway and Britain together during World War I, even though Norway remained officially neutral.\n\n"}
{"id": "23714384", "url": "https://en.wikipedia.org/wiki?curid=23714384", "title": "Tube sound", "text": "Tube sound\n\nTube sound (or valve sound) is the characteristic sound associated with a vacuum tube amplifier (valve amplifier in British English), a vacuum tube-based audio amplifier. At first, the concept of \"tube sound\" did not exist, because practically all electronic amplification of audio signals was done with vacuum tubes and other comparable methods were not known or used. After introduction of solid state amplifiers, tube sound appeared as the logical complement of transistor sound, which had some negative connotations due to crossover distortion in early transistor amplifiers. The audible significance of tube amplification on audio signals is a subject of continuing debate among audio enthusiasts.\n\nMany electric guitar, electric bass, and keyboard players in several genres also prefer the sound of tube instrument amplifiers or preamplifiers. Tube amplifiers are also preferred by some listeners for stereo systems.\n\nBefore the commercial introduction of transistors in the 1950s, electronic amplifiers used vacuum tubes (known in Great Britain as \"valves\"). By the 1960s, solid state (transistorized) amplification had become more common because of its smaller size, lighter weight, lower heat production, and improved reliability. Tube amplifiers have retained a loyal following amongst some audiophiles and musicians. Some tube designs command very high prices, and tube amplifiers have been going through a revival since Chinese and Russian markets have opened to global trade—tube production never went out of vogue in these countries.\n\nSome musicians\nprefer the distortion characteristics of tubes over transistors for electric guitar, bass, and other instrument amplifiers. In this case, generating deliberate (and in the case of electric guitars often considerable) audible distortion or \"overdrive\" is usually the goal. The term can also be used to describe the sound created by specially-designed transistor amplifiers or digital modeling devices that try to closely emulate the characteristics of the tube sound.\n\nThe tube sound is often subjectively described as having a \"warmth\" and \"richness\", but the source of this is by no means agreed on. Possible explanations mention non-linear clipping, or the higher levels of second-order harmonic distortion in single-ended designs, resulting from the tube interacting with the inductance of the output transformer.\n\nThe sound of a tube amplifier is partly a function of the circuit topologies typically used with tubes versus the topologies typically used with transistors, as much as the gain devices themselves. Beyond circuit design, there are other differences, such as the differing electronic characteristics of triode, tetrode, and pentode vacuum tubes, along with their solid-state counterparts such as bipolar transistor, FET, MOSFET, IGBT, etc. These can be further divided into differences among various models of the said device type (e.g. EL34 vs. 6L6 tetrodes). In many cases circuit topologies need to account for these differences to either homogenize their widely varying characteristics or to establish a certain operating point required by the device.\n\nThe low frequency roll-off can be explained by many tube amplifiers having high output impedance compared to transistor designs. The roll-off is due to higher device impedance and reduced feedback margins. (More feedback results in lower output impedance.) Some tube amplifier designs use minimal feedback while others use quite a bit more of it. How much feedback is optimal for tube amplifiers remains a matter of debate.\n\nTriodes (and MOSFETs) produce a monotonically decaying harmonic distortion spectrum. Even-order harmonics and odd-order harmonics are both natural number multiples of the input frequency.\n\nA psychoacoustic analysis tells us that high-order harmonics are more offensive than low. For this reason, distortion measurements should weight audible high-order harmonics more than low. The importance of high-order harmonics suggests that distortion should be regarded in terms of the complete series or of the composite wave-form that this series represents. It has been shown that weighting the harmonics by the square of the order correlates well with subjective listening tests. Weighting the distortion wave-form proportionally to the square of the frequency gives a measure of the reciprocal of the radius of curvature of the wave-form, and is therefore related to the sharpness of any corners on it. Based on said discovery, highly sophisticated methods of weighting of distortion harmonics have been developed. Since they concentrate in the origins of the distortion, they are mostly useful for the engineers who develop and design audio amplifiers, but on the other hand they may be difficult to use for the reviewers who only measure the output.\n\nA huge issue is that measurements of objective nature (for example, those indicating magnitude of scientifically quantifiable variables such as current, voltage, power, THD, dB, and so on) fail to address subjective preferences. Especially in case of designing or reviewing instrument amplifiers this is a considerable issue because design goals of such differ widely from design goals of likes of HiFi amplifiers. HiFi design largely concentrates on improving performance of objectively measurable variables. Instrument amplifier design largely concentrates on subjective issues, such as \"pleasantness\" of certain type of tone. Fine examples are cases of distortion or frequency response: HiFi design tries to minimize distortion and focuses on eliminating \"offensive\" harmonics. It also aims for ideally flat response. Musical instrument amplifier design deliberately introduces distortion and great non-linearities in frequency response. Former \"offensiveness\" of certain types of harmonics becomes a highly subjective topic, along with preferences towards certain types of frequency responses (whether flat or un-flat).\n\nPush–pull amplifiers use two nominally identical gain devices in tandem. One consequence of this is that all even-order harmonic products cancel, allowing only odd-order distortion. This is because a push–pull amplifier has a symmetric (odd symmetry) transfer characteristic. Power amplifiers are of the push-pull type to avoid the inefficiency of Class A amplifiers. \n\nA single-ended amplifier will generally produce even as well as odd harmonics. A particularly famous research about \"tube sound\" compared a selection of single-ended tube microphone preamplifiers to a selection of push-pull transistorized microphone preamplifiers. The difference in harmonic patterns of these two topologies has henceforth been often incorrectly attributed as difference of tube and solid-state devices (or even the amplifier class). Push–pull tube amplifiers can be run in class A (rarely), AB, or B. Also, a class-B amplifier may have crossover distortion that will be typically high order and thus sonically very undesirable indeed.\n\nThe distortion content of class-A circuits (SE or PP) typically monotonically reduces as the signal level is reduced, asymptotic to zero during quiet passages of music. For this reason class-A amplifiers are especially desired for classical and acoustic music since the distortion \"relative to signal\" decreases as the music gets quieter. Class-A amplifiers measure best at low power. Class-AB and B amplifiers measure best just below max rated power.\n\nLoudspeakers present a reactive load to an amplifier (capacitance, inductance and resistance). This impedance may vary in value with signal frequency and amplitude. This variable loading affects the amplifier's performance both because the amplifier has nonzero output impedance (it cannot keep its output voltage perfectly constant when the speaker load varies) and because the phase of the speaker load can change the stability margin of the amplifier. The influence of the speaker impedance is different between tube amplifiers and transistor amplifiers. The reason is that tube amplifiers normally use output transformers, and cannot use much negative feedback due to phase problems in transformer circuits. Notable exceptions are various \"OTL\" (output-transformerless) tube amplifiers, pioneered by Julius Futterman in the 1950s, or somewhat rarer tube amplifiers that replace the impedance matching transformer with additional (often, though not necessarily, transistorized) circuitry in order to eliminate parasitics and musically unrelated magnetic distortions. In addition to that, many solid-state amplifiers, designed specifically to amplify electric instruments such as guitars or bass guitars, employ current feedback circuitry. This circuitry increases the amplifier's output impedance, resulting in response similar to that of tube amplifiers.\n\nThe design of speaker crossover networks and other electro-mechanical properties may result in a speaker with a very uneven impedance curve, for a nominal 8 Ω speaker, being as low as 6 Ω at some places and as high as 30–50 Ω elsewhere in the curve. An amplifier with little or no negative feedback will always perform poorly when faced with a speaker where little attention was paid to the impedance curve.\n\nThere has been considerable debate over the characteristics of tubes versus bipolar junction transistors. Triodes and MOSFETs have certain similarities in their transfer characteristics. Later forms of the tube, the tetrode and pentode, have quite different characteristics that are in some ways similar to the bipolar transistor. Yet MOSFET amplifier circuits typically do not reproduce tube sound any more than typical bipolar designs. The reason is \"circuit differences\" between a typical tube design and a typical MOSFET design. But there are exceptions, for example designs such as the Zen series by Nelson Pass.\n\nA characteristic feature of most tube amplifier designs is the high input impedance (typically 100 kΩ or more) in modern designs and as much as 1 MΩ in classic designs. The input impedance of the amplifier is a load for the source device. Even for some modern music reproduction devices the recommended load impedance is over 50 kΩ. This implies that the input of an average tube amplifier is a problem-free load for music signal sources. By contrast, some transistor amplifiers for home use have lower input impedances, as low as 15 kΩ. Since it is possible to use high output impedance devices due to the high input impedance, other factors may need to be accounted for, such as cable capacitance and microphonics.\n\nLoudspeakers usually load audio amplifiers. In audio history, nearly all loudspeakers have been electrodynamic loudspeakers. There exists also a minority of electrostatic loudspeakers and some other more exotic loudspeakers. Electrodynamic loudspeakers transform electric current to force and force to acceleration of the diaphragm which causes sound pressure. Due to the principle of an electrodynamic speaker, most loudspeaker drivers ought to be driven by an electric current signal. The current signal drives the electrodynamic speaker more accurately, causing less distortion than a voltage signal.\n\nIn an ideal current or transconductance amplifier the output impedance approaches infinity. Practically all commercial audio amplifiers are voltage amplifiers. Their output impedances have been intentionally developed to approach zero. Due to the nature of vacuum tubes and audio transformers, the output impedance of an average tube amplifier is usually considerably higher than the modern audio amplifiers produced completely without vacuum tubes or audio transformers. Most tube amplifiers with their higher output impedance are less ideal voltage amplifiers than the solid state voltage amplifiers with their smaller output impedance.\n\nSoft clipping is a very important aspect of tube sound especially for guitar amplifiers. A hi-fi amplifier should not normally ever be driven into clipping. The harmonics added to the signal are of lower energy with soft clipping than hard clipping. However, soft clipping is not exclusive to tubes. It can be simulated in transistor circuits (below the point that real hard clipping would occur). (See \"Intentional distortion\" section.)\n\nLarge amounts of global negative feedback are not available in tube circuits, due to phase shift in the output transformer, and lack of sufficient gain without large numbers of tubes. With lower feedback, distortion is higher and predominantly of low order. The onset of clipping is also gradual. Large amounts of feedback, allowed by transformerless circuits with many active devices, leads to numerically lower distortion but with more high harmonics, and harder transition to clipping. As input increases, the feedback uses the extra gain to ensure that the output follows it accurately until the amplifier has no more gain to give and the output saturates.\n\nHowever, phase shift is largely an issue only with global feedback loops. Design architectures with local feedback can be used to compensate the lack of global negative feedback magnitude. Design \"selectivism\" is again a trend to observe: designers of sound producing devices may find the lack of feedback and resulting higher distortion beneficial, designers of sound reproducing devices with low distortion have often employed local feedback loops.\n\nSoft clipping is also not a product of lack of feedback alone: Tubes have different characteristic curves. Factors such as bias affect the load line and clipping characteristics. Fixed and cathode-biased amplifiers behave and clip differently under overdrive. The type of phase inverter circuitry can also affect greatly on softness (or lack of it) of clipping: long-tailed pair circuit, for example, has softer transition to clipping than a cathodyne. The coupling of the phase inverter and power tubes is also important, since certain types of coupling arrangements (e.g. transformer coupling) can drive power tubes to class AB2, while some other types can't.\n\nIn the recording industry and especially with microphone amplifiers it has been shown that amplifiers are often overloaded by signal transients. Russell O. Hamm, an engineer working for Walter Sear at Sear Sound Studios, wrote in 1973 that there is a major difference between the harmonic distortion components of a signal with greater than 10% distortion that had been amplified with three methods: tubes, transistors, or operational amplifiers.\n\nMastering engineer R. Steven Mintz wrote a rebuttal to Hamm's paper, saying that the circuit design was of paramount importance, more than tubes vs solid state components.\n\nHamm's paper was also countered by Dwight O. Monteith Jr and Richard R. Flowers in their article \"Transistors Sound Better Than Tubes\", which presented transistor mic preamplifier design that actually reacted to transient overloading similarly as the limited selection of tube preamplifiers tested by Hamm. Monteith and Flowers said: \"In conclusion, the high voltage transistor preamplifier presented here supports the viewpoint of Mintz: 'In the field analysis, the characteristics of a typical system using transistors depends on the design, as is the case in tube circuits. A particular 'sound' may be incurred or avoided at the designer's pleasure no matter what active devices he uses.'\"\n\nIn other words, soft clipping is not exclusive to vacuum tubes or even an inherent property of them. In practice the clipping characteristics are largely dictated by the entire circuitry and as so they can range from very soft to very hard, depending on circuitry. Same applies to both vacuum tube and solid-state -based circuitry. For example, solid-state circuitry such as operational transconductance amplifiers operated open loop, or MOSFET cascades of CMOS inverters, are frequently used in commercial applications to generate softer clipping than what is provided by generic triode gain stages. In fact, the generic triode gain stages can be observed to clip rather \"hard\" if their output is scrutinized with an oscilloscope.\n\nEarly tube amplifiers often had limited response bandwidth, in part due to the characteristics of the inexpensive passive components then available. In power amplifiers most limitations come from the output transformer; low frequencies are limited by primary inductance and high frequencies by leakage inductance and capacitance. Another limitation is in the combination of high output impedance, decoupling capacitor and grid resistor, which acts as a high-pass filter. If interconnections are made from long cables (for example guitar to amp input), a high source impedance with high cable capacitance will act as a low-pass filter.\n\nModern premium components make it easy to produce amplifiers that are essentially flat over the audio band, with less than 3 dB attenuation at 6 Hz and 70 kHz, well outside the audible range.\n\nTypical (non-OTL) tube power amplifiers could not use as much negative feedback (NFB) as transistor amplifiers due to the large phase shifts caused by the output transformers and their lower stage gains. While the absence of NFB greatly increases harmonic distortion, it avoids instability, as well as slew rate and bandwidth limitations imposed by dominant-pole compensation in transistor amplifiers. However, the effects of using low feedback principally apply only to circuits where significant phase shifts are an issue (e.g. power amplifiers). In preamplifier stages, high amounts of negative feedback can easily be employed. Such designs are commonly found from many tube-based applications aiming to higher fidelity.\n\nOn the other hand, the dominant pole compensation in transistor amplifiers is precisely controlled: exactly as much of it can be applied as needed to strike a good compromise for the given application.\n\nThe effect of dominant pole compensation is that gain is reduced at higher frequencies. There is increasingly less NFB at high frequencies due to the reduced loop gain.\n\nIn audio amplifiers, the bandwidth limitations introduced by compensation are still far beyond the audio frequency range, and the slew rate limitations can be configured such that full amplitude 20 kHz signal can be reproduced without the signal encountering slew rate distortion, which is not even necessary for reproducing actual audio material.\n\nEarly tube amplifiers had power supplies based on rectifier tubes. These supplies were unregulated, a practice which continues to this day in transistor amplifier designs. The typical anode supply was a rectifier, perhaps half-wave, a choke (inductor) and a filter capacitor. When the tube amplifier was operated at high volume, due to the high impedance of the rectifier tubes, the power supply voltage would dip as the amplifier drew more current (assuming class AB), reducing power output and causing signal modulation. The dipping effect is known as \"sag.\" Sag may be desirable effect for some electric guitarists when compared with hard clipping. As the amplifier load or output increases this voltage drop will increase distortion of the output signal. Sometimes this sag effect is desirable for guitar amplification.\n\nSome instrument tube amplifier designs use a vacuum tube rectifier instead of silicon diodes, and some designs offer the choice of both rectifiers via a switch. Such an amplifier was introduced in 1989 by Mesa/Boogie, called \"Dual Rectifier\", and the rectifier switching is the subject of a patent.\n\nWith added resistance in series with the high-voltage supply, silicon rectifiers can emulate the voltage sag of a tube rectifier. The resistance can be switched in when required.\n\nElectric guitar amplifiers often use a class-AB amplifier. In a class-A stage the average current drawn from the supply is constant with signal level, consequently it does not cause supply line sag until the clipping point is reached. Other audible effects due to using a tube rectifier with this amplifier class are unlikely.\n\nUnlike their solid-state equivalents, tube rectifiers require time to warm up before they can supply B+/HT voltages. This delay can protect rectifier-supplied vacuum tubes from cathode damage due to application of B+/HT voltages before the tubes have reached their correct operating temperature by the tube's built-in heater.\n\nThe benefit of all class-A amplifiers is the absence of crossover distortion. This crossover distortion was found especially annoying after the first silicon-transistor class-B and class-AB transistor amplifiers arrived on the consumer market. Earlier germanium-based designs with the much lower turn-on voltage of this technology and the non-linear response curves of the devices had not shown large amounts of cross-over distortion. Although crossover distortion is very fatiguing to the ear and perceptible in listening tests, it is also almost invisible (until looked for) in the traditional Total harmonic distortion (THD) measurements of that epoch. It should be pointed out that this reference is somewhat ironic given its publication date of 1952. As such, it most certainly refers to \"ear fatigue\" distortion commonly found in existing tube-type designs; the world's first prototype transistorized hi-fi amplifier did not appear until 1955 .\n\nA class-A push–pull amplifier produces low distortion for any given level of applied feedback, and also cancels the flux in the transformer cores, so this topology is often seen by HIFI-audio enthusiasts and do-it-yourself builders as the ultimate engineering approach to the tube Hi-fi amplifier for use with normal speakers. Output power of as high as 15 watts can be achieved even with classic tubes such as the 2A3 or 18 watts from the type 45. Classic pentodes such as the EL34 and KT88 can output as much as 60 and 100 watts respectively. Special types such as the V1505 can be used in designs rated at up to 1100 watts. See \"An Approach to Audio Frequency Amplifier Design\", a collection of reference designs originally published by G.E.C.\n\nSET amplifiers show poor measurements for distortion with a resistive load, have low output power, are inefficient, have poor damping factors and high measured harmonic distortion. But they perform somewhat better in dynamic and impulse response.\n\nThe triode, despite being the oldest signal amplification device, also can (depending on the device in question) have a more linear no-feedback transfer characteristic than more advanced devices such as beam tetrodes and pentodes.\n\nAll amplifiers, regardless of class, components, or topology, have some measure of distortion. This mainly harmonic distortion is a unique pattern of simple and monotonically decaying series of harmonics, dominated by modest levels of second harmonic. The result is like adding the same tone one octave higher in the case of second-order harmonics, and one octave plus one fifth higher for third-order harmonics. The added harmonic tone is lower in amplitude, at about 1–5% or less in a no feedback amp at full power and rapidly decreasing at lower output levels. Hypothetically, a single-ended power amplifier's second harmonic distortion might reduce similar harmonic distortion in a single driver loudspeaker, if their harmonic distortions were equal and amplifier was connected to the speaker so that the distortions would neutralize each other.\n\nSETs usually only produce about 2 watt (W) for a 2A3 tube amp to 8 W for a 300B up to the practical maximum of 40 W for an 805 tube amp. The resulting sound pressure level depends on the sensitivity of the loudspeaker and the size and acoustics of the room as well as amplifier power output. Their low power also makes them ideal for use as preamps. SET amps have a power consumption of a minimum of 8 times the stated stereo power. For example, a 10 W stereo SET uses a minimum of 80 W, and typically 100 W.\n\nThe special feature among tetrodes and pentodes is the possibility to obtain ultra-linear or distributed load operation with an appropriate output transformer. In practice, in addition to loading the plate terminal, distributed loading (of which ultra linear circuit is a specific form) distributes the load also to cathode and screen terminals of the tube. An Ultra-linear connection and distributed loading are both in essence negative feedback methods, which enable less harmonic distortion along with other characteristics associated with negative feedback. Ultra-linear topology has mostly been associated with amplifier circuits based on research by D. Hafler and H. Keroes of Dynaco fame. Distributed loading (in general and in various forms) has been employed by the likes of McIntosh and Audio Research.\n\nThe majority of modern commercial Hi-fi amplifier designs have until recently used class-AB topology (with more or less pure low-level class-A capability depending on the standing bias current used), in order to deliver greater power and efficiency, typically 12–25 watts and higher. Contemporary designs normally include at least some negative feedback. However, class-D topology (which is vastly more efficient than class B) is more and more frequently applied where traditional design would use class AB because of its advantages in both weight and efficiency.\n\nClass-AB push–pull topology is nearly universally used in tube amps for electric guitar applications that produce power of more than about 10 watts. While audiophile amps are primarily concerned with avoiding distortion, a guitar amp embraces it. When driven to their respective limits, tubes and transistors distort quite differently. Tubes produce distortion differently from transistors, usually in a \"softer\" fashion (that is, fewer odd-order harmonics are produced). This allows for higher levels of distortion (which is sometimes desired by the guitarist) while still being able to distinguish the harmonies of a chord. The softer profile of the tube amplifier's distortion means that the intermodulation products of the distortion closely relate to the chord harmonies. It is the consensus in the audio world that valve guitar amplifiers offer a useful additional type of sound to the palette of guitarists. There are also solid-state designs that are respected by broad consensus, so they also are seen as offering a useful sound signature option.\n\nSome individual characteristics of the tube sound, such as the waveshaping on overdrive, are straightforward to produce in a transistor circuit or digital filter. For more complete simulations, engineers have been successful in developing transistor amplifiers that produce a sound quality very similar to the tube sound. Usually this involves using a circuit topology similar to that used in tube amplifiers.\n\nIn 1982, Tom Scholz, a graduate of MIT and a member of \"Boston\", introduced the Rockman, which used JFET/BJT-based operational amplifiers and diode-based clipping circuits, but achieved a distorted sound adopted by many well known musicians. Advanced digital signal processing offers the possibility to simulate tube sound.\n\nUsing modern passive components, and modern sources, whether digital or analogue, and wide band loudspeakers, it is possible to have tube amplifiers with the characteristic wide bandwidth and \"fast\" sound of modern transistor amplifiers, including using push–pull circuits, class AB, and feedback. Some enthusiasts, such as Nelson Pass, have built amplifiers using transistors and MOSFETs that operate in class A, including single ended, and these often have the \"tube sound.\"\n\nTubes are often used to impart characteristics that many people find audibly pleasant to solid state amplifiers, such as Musical Fidelity's use of Nuvistors, tiny triode tubes, to control large bi-polar transistors in their NuVista 300 power amp. In America, Moscode and Studio Electric use this method, but use MOSFET transistors for power, rather than bi-polar. Pathos, an Italian company, has developed an entire line of hybrid amplifiers.\n\nTo demonstrate one aspect of this effect, one may use a light bulb in the feedback loop of an infinite gain multiple feedback (IGMF) circuit. The slow response of the light bulb's resistance (which varies according to temperature) can thus be used to moderate the sound and attain a tube-like \"soft limiting\" of the output, though other aspects of the \"tube sound\" would not be duplicated in this exercise.\n\nSamsung added a \"Tube Amp Effect\" to their Galaxy line of phones beginning with the Galaxy s5 within the Samsung Music app. This feature distorts music coming from attached headphones with the slight presence of hollowness. This is all done with software built into the Samsung Music App.\n\n\n"}
{"id": "1029709", "url": "https://en.wikipedia.org/wiki?curid=1029709", "title": "Volcano tectonic earthquake", "text": "Volcano tectonic earthquake\n\nA volcano tectonic earthquake is an earthquake induced by the movement (injection or withdrawal) of magma. The movement results in pressure changes in the rock around where the magma has experienced stress. At some point, the rock may break or move. The earthquakes may also be related to dike intrusion and may occur as earthquake swarms. An example is the 2007–2008 Nazko earthquake swarm in central British Columbia, Canada.\n\nOther types of seismic activity related to volcanoes and their eruptions are long period seismic waves, which are from sudden sporadic movement of magma, which is blocked from moving due to a blockage. Another is a harmonic tremor, which is steady movement of magma, deep in the mantle.\n"}
{"id": "49616076", "url": "https://en.wikipedia.org/wiki?curid=49616076", "title": "Yttriaite-(Y)", "text": "Yttriaite-(Y)\n\nYttriaite-(Y) is an exceedingly rare mineral, a natural form of yttrium oxide, YO. In terms of chemistry it is yttrium-analogue of kangite, arsenolite, avicennite and senarmontite (isometric minerals). Other minerals with the general formula AO include corundum, bismite, bixbyite, eskolaite, hematite, karelianite, sphaerobismoite, tistarite, and valentinite. Yttriaite-(Y) forms tiny inclusions in native tungsten.\n"}
