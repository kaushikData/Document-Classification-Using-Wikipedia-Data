{"id": "44760876", "url": "https://en.wikipedia.org/wiki?curid=44760876", "title": "Agfa-Commando", "text": "Agfa-Commando\n\nAgfa-Commando is the widely used name for the München-Giesing - Agfa Kamerawerke satellite camp of the Dachau concentration camp. By October 1944, the camp housed about five hundred women. They were used as slave laborers in the Agfa camera factory (part of the IG Farben group) in München-Giesing, a suburb on the S.W. side of Munich 14 miles (23 km) from the main camp of Dachau. The women assembled ignition timing devices for bombs, artillery ammunition and V-1 and V-2 rockets; they used every opportunity to sabotage the production. In January 1945, citing the lack of food, the prisoners conducted a strike, an unheard-of action in a concentration camp. Production ended on 23 April 1945 and the women marched toward Wolfratshausen, where their commander eventually surrendered to advancing American troops.\n\nDachau was the first concentration camp (known as a \"KZ\") that Reichsführer-SS Himmler had built. It was already in existence in 1933 and developed into a prototype for subsequent concentration camps such as Buchenwald, which appeared in 1937. The concentration camp was not geographically restricted to Dachau itself. At the onset of war, the SS increasingly employed concentration camp prisoners in armaments factories and these specific labor commands created a network of subcamps throughout Germany. In some cases the prisoners were accommodated in diverse, makeshift sleeping areas; in other cases the SS had them erect their own camp with watchtowers and fences. Many such subcamps, called the \"KZ-Außenlager\", were laid out in similar fashion to the concentration camps. There were also SS camp commanders (\"SS-Lagerführer\") and prisoner functionaries such as the \"camp senior\" (\"Lagerältester\") or \"block senior\" (\"Blockältester\").\n\nBetween 1927 and 1945, Agfa was the principal photographic equipment producer, and the largest photographic manufacturer in Germany. From 1941, Agfa Camera works produced exclusively for the Wehrmacht and employed a growing number of prisoners from Dachau. Most likely they were returned to the main camp in the evenings during the first years, and the subcamp in München-Giesing, where the laborers assembled timing devices, was not established until September 1944. The camp commander came in function on 12 September 1944.\n\nAbout five hundred prisoners from Eastern and Southeastern Europe, mainly Poland, arrived from Ravensbrück concentration camp on 13 September 1944. Little is known about the Polish women except that many of them were taken as slave labor in reprisal for the 1943 Warsaw Ghetto Uprising. Ludwig Eiber mentions a forty-year old Polish women who died on 7 October 1944. In December 1944, after a Christmas party, two of these prisoners escaped, dressed as Josef and Maria in some borrowed clothing. According to an unconfirmed account of Leni Leuvenberg, twenty Polish women were killed during a bombing on 25 February 1945.\n\nIn October 1944, 250 Polish prisoners were sent back to Ravensbrück, in exchange for 193 Dutch women, ten women from other West European countries and fifty women from Eastern and Southeastern Europe. Among the latter were twenty-one Slovenian political prisoners, mostly (communist) Yugoslav Partisans. The Dutch women arrived on October 15, 1944 from Ravensbrück where they had arrived in September from the Dutch concentration camp Vught. Most had been active in the resistance and had formed bonds already in Vught. They were a cohesive, supportive group; they marched singing into the cattle cars in Vught and walked singing into Ravensbrück concentration camp.\n\nOut of the 193 Dutch women, \"only\" two died just before the war's end. In comparison, a third of the Dutch women that stayed behind in Ravensbrück did not survive.\n\nVery little has been published and most facts were collected from written memoirs and oral testimony of the Dutch survivors. spent a few months as a prisoner-doctor in the camp dispensary, from December 1944. Her book \"Gefangene der Angst\" was published in 2003. She is critical of the Dutch prisoners and calls them naive. Her views became a thorny issue with the Dutch ex-prisoners, in the long drawn-out compensation claims against IG Farben. French prisoner published her memoirs in the \"Journal d’Arcachon\" in 1946-1947. In May, 2015, the stories of a number of Dutch Dachau political prisoners were published as \"Geen nummers maar namen\". The publication contains input for Renny van Ommen-de Vries, Kiky Heinsius and Loes Bueninck.\n\nThe women were housed in an apartment block in München-Giesing. Part of the apartment had been bombed out before it was completed. The complex was surrounded by a high barbed wire fence with watch towers on the four corners. In the center court of the U-shaped building stood a wooden barrack mess hall. Six or seven prisoners slept in each small room. Reveille was at 0500 hours. The prisoners were counted, and marched to the Agfa factory. They returned to the subcamp compound at 1700 hours.\n\nReligious meetings that had been held in Vught continued in secrecy in Dachau. One of the Dutch prisoners, \"Blockälteste\" Rennie van Ommen-de Vries, recalls the strength they obtained in these encounters in her biography. Since the women were not under guard in their rooms, they held regular devotions and produced their own song books. They translated parts of the Old Testament from a German Bible that was lent to them by a civilian factory worker.\n\nIn September 1944, Kurt Konrad Stirnweis, a Waffen-SS lieutenant and World War I veteran, was transferred from an artillery detail near Freising to the main camp at Dachau; he was subsequently placed in charge of Agfa-Commando.\n\nIn January, 1945 the road from the main camp in Dachau had become impassable as a result of the Allied bombings. The meals now became the responsibility of the Agfa management. The soup deteriorated by the day, and few women were spared digestive problems and complications from undernourishment. Disease was rampant: there were outbreaks of typhoid fever, scarlet fever and tuberculosis. Conditions at the main camp were no better; as the war drew to a close, Dachau became increasingly overcrowded with prisoners evacuated from other concentration camps. Consequently, transfer from the Agfa subcamp to the main camp's dispensary was close to a death sentence.\n\nWhen the factory took over the distribution of the soup and started watering it down, while at the same time trying to raise the production quotas, the Dutch women spontaneously crossed their arms and stopped their work. The Slovenian women joined the protest. Strikes were unheard of in the concentration camps, so this would lead to severe punishments. In the end the women made their point that they just could not work under the conditions of a starvation diet and constant bombing raids. The chief Gestapo agent Willy Bach came down from the headquarters in Dachau and tried to find the instigators, but no one came forward. In the end, Mary Vaders, who had arrived from Ravensbrück on October 15, 1944, was selected at random and incarcerated in the Dachau bunker cell for seven weeks of solitary confinement. She came back damaged but unbroken. The remaining Dutch and Slovenian women were punished with hours standing in formation in the court yard.\n\nAs the war drew to a close and American personnel began to encircle the region, production at the factory halted on 23 April 1945. The Allied bombings and the advance of the Allied forces had cut off the supplies of raw material and distribution of the products. The camp commander was ordered to evacuate the prisoners and begin their death march in a southerly direction. The women were given a small sausage and a piece of bread for the journey, with their standard bowl of soup for their previous evening meal. Against his SS-superiors' orders, Stirnweis halted the march on 28 April just outside the town of Wolfratshausen and further persuaded a farmer named Walser to shelter the five hundred remaining prisoners in his hayloft. Despite specific orders to the contrary, he did not resume the march, but let the women shelter in place until the American troops drew closer.\n\nOn 1 May 1945 Stirnweis surrendered to the 12th Infantry Regiment of the 4th Infantry Division of the US Army and asked for protection of the prisoners. After about a week on the farm, being fed by the generous Walser couple the women were relocated nearby, in the abandoned labor camp Föhrenwald. This was the largest and longest-lived resettlement camp in post-war Europe. From Föhrenwald, the women were repatriated by the Red Cross.\n\nInitially, and based on cursory evidence, Stirnweis was accused of participating in cruelties and criminal usage of prisoners of war and civilians and sentenced to two years of labor after the war. However, the testimony of many of the women revealed no evidence of atrocities committed at the work detail at Agfa Camera works. According to former prisoners' testimony, sub-camp commander Lieutenant Kurt Konrad Stirnweis was a reasonable man. His sentence was abrogated upon the testimony of his former charges.\n\nHis deputy, a 29-year-old Latvian named Alexander Djerin, was sentenced to six years imprisonment for his cruel treatment of the prisoners, commencing 9 May 1945. Although there was no suggestion in the trial records that Sergeant Djerin had mistreated the women, he was convicted of mistreatment of prisoners during his work at Dachau.\n\nIn April 1945, a group of twenty-two war correspondents was quartered in a villa on the Isar river in Grünwald, another Munich suburb. Just before the women prisoners were transferred from the Walser farm to Föhrenwald, two of the men came looking for women to help in their kitchen. Rennie van Ommen-de Vries and Nel Niemantsverdriet accepted their offer.\n\nMost of the correspondents had come up together from North Africa through Italy. War correspondent Ernie Pyle and cartoonist Bill Mauldin often were among them. Their job was to document the atrocities of Dachau and to accompany government VIPs and several Hollywood executives. One of the latter was film director William Wyler.\n\nThe press was under the command of Colonel Max Boyd, his next in command was Major (later Lieutenant Colonel) Jay R. Vessels (Minneapolis, Mn.), Air Corps Public Relations Officer. Claude Farmer was the driver and Don Jordan the cook. The journalists included Sholem Asch's son, Nathan Asch; AP reporter working at the Seattle Times Harry Cowe; Charley Green (from St. Paul, Mn.); Art Everett (from Bay City, Mi.); and Paul Zimmer (from Oakland, Ca.).\n\n\n"}
{"id": "1128411", "url": "https://en.wikipedia.org/wiki?curid=1128411", "title": "Antonio Pigafetta", "text": "Antonio Pigafetta\n\nAntonio Pigafetta (; c. 1491 – c. 1531) was an Italian scholar and explorer from the Republic of Venice. He traveled with the Portuguese explorer Ferdinand Magellan and his crew by order of the King Charles I of Spain on their voyage around the world. During the expedition, he served as Magellan's assistant and kept an accurate journal which later assisted him in translating the Cebuano language. It is the first recorded document concerning the language.\n\nPigafetta was one of the 18 men who returned to Spain in 1522, out of the approximately 240 who set out three years earlier. The voyage completed the first circumnavigation of the world; Juan Sebastián Elcano had served as captain after Magellan's death during the voyage in 1521 in the Philippines. Pigafetta's surviving journal is the source for much of what is known about Magellan and Elcano's voyage.\n\nAt least one warship of the Italian Navy, a destroyer of the \"Navigatori\" class, was named after him in 1931.\n\nPigafetta belonged to a rich family city of Vicenza in northeast Italy. In his youth he studied astronomy, geography and cartography. He then served on board the ships of the Knighters of Rhodes at the beginning of the 16th century. Until 1519, he accompanied the papal nuncio, Monsignor Francesco Chieregati, to Spain.\n\nIn Seville, Pigafetta heard of Magellan's planned expedition and decided to join, accepting the title of supernumerary (\"sobresaliente\"), and a modest salary of 1,000 maravedís. During the voyage, which started in August 1519, Pigafetta collected extensive data concerning the geography, climate, flora, fauna and the native inhabitants of the places that the expedition visited. His meticulous notes proved invaluable to future explorers and cartographers, mainly due to his inclusion of nautical and linguistic data, and also to latter-day historians because of its vivid, detailed style. The only other sailor to maintain a journal during the voyage was Francisco Albo, \"Victoria's\" last pilot, who kept a formal logbook.\n\nPigafetta was wounded on Mactan in the Philippines, where Magellan was killed in the Battle of Mactan in April 1521 by the local ruler Lapu-Lapu. Nevertheless, he recovered and was among the 18 who accompanied Juan Sebastián Elcano on board the \"Victoria\" on the return voyage to Spain.\n\nUpon reaching port in Sanlúcar de Barrameda in the modern Province of Cadiz in September 1522, three years after his departure, Pigafetta returned to the Republic of Venice. He related his experiences in the \"Report on the First Voyage Around the World\" (), which was composed in Italian and was distributed to European monarchs in handwritten form before it was eventually published by Italian historian Giovanni Battista Ramusio in 1550–59. The account centers on the events in the Mariana Islands and the Philippines, although it included several maps of other areas as well, including the first known use of the word \"Pacific Ocean\" (\"Oceano Pacifico\") on a map. The original document was not preserved.\n\nHowever, it was not through Pigafetta's writings that Europeans first learned of the circumnavigation of the globe. Rather, it was through an account written by a Flanders-based writer Maximilianus Transylvanus, which was published in 1523. Transylvanus had been instructed to interview some of the survivors of the voyage when Magellan's surviving ship \"Victoria\" returned to Spain in September 1522 under the command of Juan Sebastian Elcano. After Magellan and Elcano's voyage, Pigafetta utilized the connections he had made prior to the voyage with the Knights of Rhodes to achieve membership in the order\n\n\n\n"}
{"id": "2391422", "url": "https://en.wikipedia.org/wiki?curid=2391422", "title": "Aquanator", "text": "Aquanator\n\nThe Aquanator is a small-scale tidal-power device, a device which uses rows of hydrofoils to generate electricity from water currents. It was invented by Australian inventor Michael Perry.\n\nThe Aquanator invention was announced in 2004. A contract to test the device was signed with Country Energy on 26 September 2004.\n\nIts test site was located at . In beginning of 2006 it was connected to grid. However, the device test site was decommissioned in May 2008 by its owner Atlantis Resources.\n\nThe Aquanator used ocean current to produce electricity. It was intended to generate power even with a small flow of 1.5 knots. The test device had a capacity of 5 kW. The aquanator’s slow moving hydrofoil design was meant to provide a green energy source which would not harm ocean life as faster moving turbines might.\n\nThe aquanator was meant to be cheaper than diesel fuels, with costs about the same amount as wind power and will be one sixth the price of diesel-powered systems.\n\n\n"}
{"id": "35997283", "url": "https://en.wikipedia.org/wiki?curid=35997283", "title": "Atmospheric carbon cycle", "text": "Atmospheric carbon cycle\n\nThe atmosphere is one of the Earth's major carbon reservoirs and an important component of the global carbon cycle, holding approximately 720 gigatons of carbon. Atmospheric carbon plays an important role in the greenhouse effect. The most important carbon compound in this respect is the gas carbon dioxide (). Although it is a small percentage of the atmosphere (approximately 0.04% on a molar basis), it plays a vital role in retaining heat in the atmosphere and thus in the greenhouse effect. Other gases with effects on the climate containing carbon in the atmosphere are methane and chlorofluorocarbons (the latter is entirely anthropogenic). Emissions by humans in the past 200 years have almost doubled the amount carbon dioxide in the atmosphere.\n\nThe concentration of mostly carbon-based greenhouse gases has increased dramatically since the onset of the industrial era. This makes an understanding of the carbon component of the atmosphere highly important. The two main carbon greenhouse gases are methane and carbon dioxide.\n\nMethane (CH) is one of the more potent greenhouse gases and is mainly produced by the digestion or decay of biological organisms. It is considered the second most important greenhouse gas, yet the methane cycle in the atmosphere is currently only poorly understood. The amount of methane produced and absorbed yearly varies widely.\n\nLarge stores of methane can be found in the form of methane ice under permafrost and on continental shelves. Additional methane is produced by the anaerobic decay of organic material and is produced in organisms' digestive tracts, soil, etc. Natural methane production accounts 10-30% of global methane sources.\n\nAnthropogenic methane is produced in various ways, e.g. by raising cattle or through the decay of trash in landfills. It is also produced by several industrial sources, including the mining and distribution of fossil fuels. More than 70% of atmospheric methane comes from biogenic sources. Methane levels have risen gradually since the onset of the industrial era, from ~700 ppb in 1750 to ~1775 ppb in 2005.\n\nMethane can be removed from the atmosphere through a reaction of the photochemically produced hydroxyl free radical (OH). It can also leave the atmosphere by entering the stratosphere, where it is destroyed, or by being absorbed into soil sinks. Because methane reacts fairly quickly with other compounds, it does not stay in the atmosphere as long as many other greenhouse gases, e.g. carbon dioxide. It has an atmospheric lifetime of about eight years. This keeps the concentration of methane in the atmosphere relatively low and is the reason that it currently plays a secondary role in the greenhouse effect to carbon dioxide, despite the fact that it produces a much more powerful greenhouse effect per volume.\n\nCarbon dioxide () has a large warming effect on global temperatures through the greenhouse effect. Although individual CO molecules have a short residence time in the atmosphere, it takes an extremely long time for carbon dioxide levels to sink after sudden rises, due to e.g. volcanic eruptions or human activity and among the many long-lasting greenhouse gases, it is the most important because it makes up the largest fraction of the atmosphere.\nSince the industrial revolution, the CO concentration in the atmosphere has risen from about 280 ppm to almost 400 ppm. Although the amount of CO introduced makes up only a small portion of the global carbon cycle, carbon dioxide's long residence time makes these emissions relevant for the total carbon balance. The increased carbon dioxide concentration strengthens the greenhouse effect, causing changes to the global climate. Of the increased amounts of carbon dioxide that are introduced to the atmosphere each year, approximately 80% are from the combustion of fossil fuels and cement production. The other ~20% originate from land use change and deforestation. Because gaseous carbon dioxide does not react quickly with other chemicals, the main processes that change the carbon dioxide content of the atmosphere involve exchanges with the earth's other carbon reservoirs, as explained in the following sections.\n\nAtmospheric carbon is exchanged quickly between the oceans and the terrestrial biosphere. This means that at times the atmosphere acts as a sink, and at other times as a source of carbon. The following section introduces exchanges between the atmospheric and other components of the global carbon cycle.\n\nCarbon is exchanged with varying speed with the terrestrial biosphere. It is absorbed in the form of carbon dioxide by autotrophs and converted into organic compounds. Carbon is also released from the biosphere into the atmosphere in the course of biological processes. Aerobic respiration converts organic carbon into carbon dioxide and a particular type of anaerobic respiration converts it into methane. After respiration, both carbon dioxide and methane are typically emitted into the atmosphere. Organic carbon is also released into the atmosphere during burning.\n\nThe residence time of carbon in the terrestrial biosphere varies and is dependent on a large number of factors. The uptake of carbon into the biosphere occurs on various time scales. Carbon is absorbed primarily during plant growth. A pattern of increased carbon uptake is observable both over the course of the day (less carbon is absorbed at night) and over the course of the year (less carbon is absorbed in winter). While organic matter in animals generally decays quickly, releasing much of its carbon into the atmosphere through respiration, carbon stored as dead plant matter can stay in the biosphere for as much as a decade or more. Different plant types of plant matter decay at different rates - for example, woody substances retain their carbon longer than soft, leafy material. Active carbon in soils can stay sequestered for up to a thousand years, while inert carbon in soils can stay sequestered for more than a millennium.\n\nLarge amounts of carbon are exchanged each year between the ocean and the atmosphere. A major controlling factor in oceanic-atmospheric carbon exchange is thermohaline circulation. In regions of ocean upwelling, carbon rich water from the deep ocean comes to the surface and releases carbon into the atmosphere as carbon dioxide. Large amounts of carbon dioxide are dissolved in cold water in higher latitudes. This water sinks down and brings the carbon into the deeper ocean levels, where it can stay for anywhere between decades and several centuries. Ocean circulation events cause this process to be variable. For example, during El Nino events there is less deep ocean upwelling, leading to lower outgasing of carbon dioxide into the atmosphere.\n\nBiological processes also lead to ocean-atmosphere carbon exchange. Carbon dioxide equilibriates between the atmosphere and the ocean's surface layers. As autotrophs add or subtract carbon dioxide from the water through photosynthesis or respiration, they modify this balance, allowing the water to absorb more carbon dioxide or causing it to emit carbon dioxide into the atmosphere.\n\nCarbon is generally exchanged very slowly between the atmosphere and geosphere. Two exceptions are volcanic eruptions and the combustion of fossil fuels, both of which release high amounts carbon into the atmosphere very quickly. Fresh silicate rock that is exposed through geological processes absorbs carbon from the atmosphere when it is exposed to air by the processes of weathering and erosion.\n\nHuman activities change the amount of carbon in the atmosphere directly through the burning of fossil fuels and other organic material, thus oxidizing the organic carbon and producing carbon dioxide. Another human-caused source of carbon dioxide is cement production. The burning of fossil fuels and cement production are the main reasons for the increase in atmospheric CO since the beginning of the industrial era.\n\nOther human-caused changes in the atmospheric carbon cycle are due to anthropogenic changes to carbon reservoirs. Deforestation, for example, decreases the biosphere's ability to absorb carbon, thus increasing the amount of carbon in the atmosphere.\n\nAs the industrial use of carbon by humans is a very new dynamic on a geologic scale, it is important to be able to track sources and sinks of carbon in the atmosphere. One way of doing so is by observing the proportion of stable carbon isotopes present in the atmosphere. The two main carbon isotopes are C and C. Plants absorb the lighter isotope, C, more readily than C. Because fossil fuels originate mainly from plant matter, the C/C ratio in the atmosphere falls when large amounts of fossil fuels are burned, releasing C. Conversely, an increase in the C/C in the atmosphere suggests a higher biospheric carbon uptake. The ratio of the annual increase in atmospheric CO compared to CO emissions from fossil fuel and cement manufactured is called the \"airborne fraction.\". The airborne fraction has been around 60% since the 1950s, indicating that about 60% of the new carbon dioxide in the atmosphere each year originated from human sources. For clarity, this is not meant to suggest that 60% of the uptake of carbon dioxide into the atmosphere comes from human activity. It means that the atmosphere exchanges around 210 giga-tonnes of carbon annually, but absorbs between 6 and 10 giga-tonnes more than it loses. Of this net gain, about 60% is attributable to the burning of fossil fuels.\n\n"}
{"id": "43106272", "url": "https://en.wikipedia.org/wiki?curid=43106272", "title": "Barcelona power station", "text": "Barcelona power station\n\nBarcelona power station (also Power station of Barcelona) is a combined cycle thermoelectric plant located at Pier Energy of Port of Barcelona, in Barcelona, Spain. It has 2 thermal units of 425 MW, which use natural gas as fuel, and with a total electric power of 850 MW. It is owned by the multinational company Gas Natural.\n\n"}
{"id": "49513408", "url": "https://en.wikipedia.org/wiki?curid=49513408", "title": "Bioavailability (soil)", "text": "Bioavailability (soil)\n\nBioavailability, in environmental and soil sciences, represents the amount of an element or compound that is accessible to an organism for uptake or adsorption across its cellular membrane. In environmental and agricultural applications, bioavailability most often refers to availability of contaminants, such as organic pollutants or heavy metals, in soil systems and is also used frequently in determining potential risk of land application of sewage sludge or other inorganic/organic waste materials.\n\nAlmost exclusively, plant roots and soil organisms uptake contaminants that are dissolved in water. Therefore, the bioavailable fraction is often likened to the dissolved (aqueous) fraction in these systems. Depending on its chemical properties, a contaminant may or may not be found in the aqueous phase. Organic contaminants may become sorbed or sequestered in organic matter through weak Van der Waals interactions or through hydrogen- or covalent bonding. Ionic compounds, such as heavy metals, can be precipitated into the solid phase. Volatile compounds can be lost as aerosols to the soil atmosphere. In these forms, contaminants are relatively inaccessible to microbial or plant uptake and must dissociate and re-dissolve into the soil solution to become biological available.\n\nBioavailability is a function of soil properties, time, environmental conditions, and plant and microbial characteristics \n\nSite-specific characteristics have a major influence on contaminant bioavailability and no standardized tests have been developed. However, there are a number of chemical and biological tests used to estimate bioavailability including a direct measurement of contaminant bioaccumulation in earthworms (\"Eisenia fetida\"). Estimates of bioavailability can also be obtained from chemical solid-phase soil extractions. Fugacity modelling of bioavailability is based on the solubility and partitioning of compounds into aqueous and non-aqueous phases. This model describes the tendency for contaminants to be dissolved in the soil solution.\n\n"}
{"id": "6814366", "url": "https://en.wikipedia.org/wiki?curid=6814366", "title": "Black Moshannon State Park", "text": "Black Moshannon State Park\n\nBlack Moshannon State Park is a Pennsylvania state park in Rush Township, Centre County, Pennsylvania, United States. It surrounds Black Moshannon Lake, formed by a dam on Black Moshannon Creek, which has given its name to the lake and park. The park is just west of the Allegheny Front, east of Philipsburg on Pennsylvania Route 504, and is largely surrounded by Moshannon State Forest. A bog in the park provides a habitat for diverse wildlife not common in other areas of the state, such as carnivorous plants, orchids, and species normally found farther north. As home to the \"[l]argest reconstituted bog/wetland complex in Pennsylvania\".\n\nHumans have long used the Black Moshannon area for recreational, industrial, and subsistence purposes. The Seneca tribe used it as hunting and fishing grounds. European settlers cleared some land for farming, then clear-cut the vast stands of old-growth White Pine and Eastern Hemlock to meet the needs of a growing nation during the late 19th century. Black Moshannon State Park rose from the ashes of a depleted forest that was largely destroyed by wildfire in the years following the lumber era. The forests were rehabilitated by the Civilian Conservation Corps during the Great Depression of the 1930s. Many of the buildings built by the Civilian Conservation Corps stand in the park today and are protected on the list of National Register of Historic Places in three historic districts.\n\nBlack Moshannon State Park is open year-round for recreation and has an extensive network of trails which allow hiking, biking, and viewing the bog habitat at the Black Moshannon State Natural Area. The park is in Pennsylvania Important Bird Area #33, where bird watchers have recorded 175 different species. It is also home to many rare and unusual plants and animals, due to its location atop the Allegheny Plateau; the lake is at an elevation of about . Much of the park is open for hunting and the lake and creek are open for fishing, boating, and swimming. In winter it is a popular destination for cross-country skiing, and was home to a small downhill skiing area from 1965 to 1982. Picnics and camping are also popular, and the \"Friends of Black Moshannon State Park\" group promotes the park and all of the recreational activities associated with it.\n\nHumans have lived in what is now Pennsylvania since at least 10,000 BC. The first settlers were Paleo-Indian nomadic hunters known from their stone tools. The hunter-gatherers of the Archaic period, which lasted locally from 7000 to 1000 BC, used a greater variety of more sophisticated stone artefacts. The Woodland period marked the gradual transition to semi-permanent villages and horticulture, between 1000 BC and 1500 AD. Archeological evidence found in the state from this time includes a range of pottery types and styles, burial mounds, pipes, bows and arrow, and ornaments.\n\nBlack Moshannon Creek is in the West Branch Susquehanna River drainage basin, whose earliest recorded inhabitants were the Iroquoian-speaking Susquehannocks. They were a matriarchial society that lived in large long houses in stockaded villages. Decimated by disease and warfare with the Five Nations of the Iroquois, by 1675 they had died out, moved away, or been assimilated into other tribes.\n\nAfter this, the lands of the West Branch Susquehanna River valley were under the nominal control of the Iroquois. The Iroquois lived in long houses, primarily in what is now New York, and had a strong confederacy which gave them power beyond their numbers. To fill the void left by the demise of the Susquehannocks, the Iroquois encouraged displaced tribes from the east to settle in the West Branch watershed, including the Lenape (or Delaware).\n\nThe Seneca, members of the Iroquois Confederacy, were inhabitants in the area of Black Moshannon Lake, which was a series of beaver ponds at the time. They and other Native Americans, including the Lenape, hunted, fished, and traded in the region. The Great Shamokin Path, the major native east–west path connecting the Susquehanna and Allegheny River basins, crossed Black Moshannon Creek at a ford a few miles downstream from the park; however, no trails of the indigenous peoples are recorded as having passed through the park itself. The park's Indian Trail for hiking and cross-country skiing recalls such native paths as it runs through an open forest of oak and pine trees, with occasional clearings and a grove of hawthorns.\n\nThe French and Indian War (1754–1763) led to the migration of many Native Americans westward to the Ohio River basin. On November 5, 1768, the British acquired the \"New Purchase\" from the Iroquois in the Treaty of Fort Stanwix, including what is now Black Moshannon State Park. After the American Revolutionary War, Native Americans almost entirely left Pennsylvania.\n\nWhile there are no known archeological sites within Black Moshannon State Park, the name \"Moshannon\" is derived from a Lenape (Delaware) term for Moshannon and Black Moshannon Creeks: \"Mos'hanna'unk,\" which means \"elk river place.\" The name \"Black Moshannon\" refers to the dark color of the water, a result of plant tannins from the local vegetation and bog.\n\nPrior to the arrival of William Penn and his Quaker colonists in 1682, it has been estimated that up to 90 percent of what is now Pennsylvania was covered with woods: over of white pine, eastern hemlock, and a mix of hardwoods. The forests near the three original counties, Philadelphia, Bucks, and Chester, were the first to be harvested, as the early settlers used the readily available timber to build homes, barns, and ships, and cleared the land for agriculture. The demand for lumber slowly increased and by the time of the American Revolution the lumber industry had reached the interior and mountainous regions of Pennsylvania.\n\nLumber became one of the leading industries in Pennsylvania. Trees were used to furnish fuel to heat homes, tannin for the many tanneries that were spread throughout the state, and wood for construction, furniture, and barrel making. Large areas of forest were harvested by colliers to fire iron furnaces. Rifle stocks and shingles were made from Pennsylvania timber, as were a wide variety of household utensils, and the first Conestoga wagons. The Philadelphia–Erie Pike (present day Pennsylvania Route 504) opened the Black Moshannon area to settlers by 1821. The first settlers opened the Antes Tavern along the Pike, trapped fur-bearing animals, and cleared land for farming.\n\nBy the mid-19th century, the demand for lumber reached the area, where eastern white pine and eastern hemlock covered the surrounding mountainsides. Lumbermen came and harvested the trees and sent them down Black Moshannon and Moshannon Creeks to the West Branch Susquehanna River, then along that to the Susquehanna Boom and sawmills at Williamsport. Lumber was also transported by sled and wagon over the ridges and through the valleys to Philipsburg, Julian and Unionville.\nThe Beaver Mill Lumber Company became one of the largest single lumber operations in all of Pennsylvania, and four lumber boomtowns, Beaver Mills, Star Mill, Underwood Mills, and Antes, altered the landscape in the Black Moshannon area. A dam was built at the site of an old beaver dam, and the mill ponds for the lumber mills flooded the old beaver ponds. The communities featured general stores, blacksmith shops, liveries, taverns, schools, and even a ten-pin bowling alley. The area helped to meet the nation's need for timber in mining operations, construction, and railroads.\n\nA number of trails in the park today recall this time. The Seneca Trail for cross-country skiing and hiking passes through a second growth forest of oak and cherry trees that shade the stumps of the old growth pines harvested during the lumber era. The Shingle Mill Trail begins at the main parking area near the dam on Black Moshannon Lake and follows the banks of Black Moshannon Creek to the Allegheny Front Trail for approximately 2 miles. The remains of Star Mill, a sawmill built in 1879 that operated until the end of the lumber era, are on the Star Mill Trail. This loop trail for hiking and cross-country skiing is nearly flat, with a view of Black Moshannon Lake.\n\nThis boom era was not to last; before long the lumber was gone, and once the trees were all clear-cut, the loggers left the area. The lumbermen left behind a barren landscape that was devastated by erosion and wildfires. In the late 19th and early 20th centuries, the Commonwealth of Pennsylvania bought thousands of acres of deforested and burned land, then began the project of reforestation. By the 1930s, the land that became Black Moshannon State Park was already a place for picnics and camping, on the aptly named \"Tent Hill\", and people swam and fished in the old mill pond. The Tent Hill Trail still runs from the campsites to the Black Moshannon Lake.\n\nThe Civilian Conservation Corps (CCC) was a work relief program for young men from unemployed families, established in 1933. As part of President Franklin D. Roosevelt's New Deal legislation, it was designed to combat unemployment during the Great Depression. The CCC operated in every U.S. state.\n\nThe original facilities at Black Moshannon State Park were constructed by the Civilian Conservation Corps, from 1933 to 1937. The CCC created many jobs for unemployed industrial workers from throughout the United States. Black Moshannon State Park is the result of one of many projects undertaken by the CCC throughout central Pennsylvania. However, many people used the area for recreation as far back as the early 1900s.\n\nBeaver Meadow CCC Camp S-71 was built in May 1933 near the abandoned village of Beaver Mills, and was one of the first to expand recreational facilities in Pennsylvania. Over 200 young men moved in and began the work of conserving soil, water, and timber in the area. They cut roads through the growing forest to aid in fighting the wildfires that sprang up, and planted many acres of Red Pines as part of the reforestation effort.\n\nMost of the CCC-built park facilities are still in use today, including log cabins, picnic pavilions, a food concession stand, and miles of trails. Early on, the CCC constructed a dam at Black Moshannon Lake, on the site of the former mill pond dam. CCC Camp S-71 closed in January 1937 and Black Moshannon State Park officially re-opened that same year.\n\nIn 1987, three separate historic districts incorporating the existing CCC structures in Black Moshannon State Park were placed on the National Register of Historic Places. The structures in all three districts were built between 1933 and 1937 and are designated as part of either the Beach and Day Use, Family Cabin, or Maintenance Historic Districts.\n\nEighteen structures in the Beach and Day Use Historic District are protected as contributing properties, including seven \"standard\" pavilions, a larger picnic shelter, and three water pump shelters. These last were built of native stone and covered with pebbles, and have since been converted to small picnic pavilions. The concession building, beach bathhouse, and museum are also protected.\n\nThe Family Cabin Historic District consists of 16 contributing properties: 13 log cabins, one lodge, and two latrines. Cabins 1–12, half with one room and half with two, are in a line along a road, similar to 1930s motor courts. The cabin layout at Black Moshannon State Park is unique compared to CCC-built cabins at other Pennsylvania state parks. The cabins at the other parks reflect the \"rustic\" style of cabin layout promoted by the National Park Service. The Lodge, also known as Cabin 13, is a large rectangular clapboard-sided building with a stone fireplace, while Cabin 14 is L-shaped with an open porch.\n\nThe Maintenance Historic District includes four CCC-built structures. The storage building is a wood-frame structure with a gable roof, similar to military storage buildings built in the 1930s and 1940s. A three-bay garage of standard military design is included in this historic district. The Park Manager's residence is a -story gable-roofed house, with modern vinyl siding.\n\nSince its establishment in 1937, Black Moshannon State Park has undergone several changes. In 1941, Governor Arthur James announced plans to expand the park to by annexing surrounding state forest land. \"Black Moshannon Airport\" was built on land taken from the state park and Moshannon State Forest just prior to the Second World War, was operational by 1942, and renamed \"Mid-State Airport\" in 1962. As of 2008, it is officially known as \"Mid-State Regional Airport\" and covers . While the airport has been designated a Keystone Opportunity Zone to encourage business growth, there are limitations in state law that prohibit any further development on park or forest lands.\n\nThe CCC-built dam forming Black Moshannon Lake was replaced in the 1970s by the current structure. The park experienced major developments between 1971 and 1980. As of 2008, post-war facilities include the park office, six modern cabins, boat launches, showerhouses, and modern restroom facilities. A former CCC-built pavilion is now the Environmental Learning Center.\nThere is a wastewater treatment plant near the dam for effluent from the park, airport, and some private homes.\n\nBlack Moshannon State Park was the site of a ski resort from the 1960s until 1982. The state legislature authorized \"construction of ski facilities\" at the park in 1961, which were operational by 1965. Although managed by the state, a commercial operator was sought as early as 1969, and in 1980 it was leased to a private contractor, before being closed in 1982. The ski area was primitive by modern standards: skiers were lifted to the top of the slope by one of two tow ropes or Poma lifts, and the slopes had about of vertical drop. As of 2008, the ski lodge, renamed Cabin 20, remains open and is rented out to park visitors, while the Ski Slope Trail is a hiking trail that passes near the former ski slope. It begins at the beach parking lot, climbs Rattlesnake Mountain, and crosses Pennsylvania Route 504 near a historical marker for the Philadelphia–Erie Turnpike.\n\nBy the 1980s, the park started to receive official recognition for its unique resources. The three Historic Districts were added to the National Register of Historic Places in 1987 in recognition of their CCC-built structures. That same year the state celebrated Black Moshannon State Park's \"50th Anniversary\". In 1994, the DCNR established the \"Black Moshannon Bog Natural Area\" as part of a program to recognize areas of \"unique scenic, geologic or ecological value.\" In 1997 the park's Important Bird Area (IBA) was one of the first 73 IBAs established in Pennsylvania. By 2001 yearly attendance at Black Moshannon State Park was over 350,000. As of 2008, the Pennsylvania Department of Conservation and Natural Resources (DCNR) Bureau of Parks, which administers all 120 Pennsylvania state parks, had chosen Black Moshannon as one of its \"25 Must-See Pennsylvania State Parks\", citing its location atop the Allegheny Plateau and within the state forest, its many trails and rare plants, and its status as a state park natural area and the \"[l]argest reconstituted bog/wetland complex in Pennsylvania\".\n\nThe rocks underlying the Black Moshannon Creek drainage basin are primarily shale, sandstone, and coal. Three major rock formations are present in Black Moshannon State Park, all from the Carboniferous period. These sedimentary rocks formed in or near shallow seas roughly 300 to 350 million years ago. The Mississippian Burgoon Formation is composed of buff-colored sandstone and conglomerate. The late Mississippian Mauch Chunk Formation is formed with grayish-red shale, siltstone, sandstone, and conglomerate. The third is the early Pennsylvanian Pottsville Formation, which is a gray conglomerate that may contain sandstone, siltstone, and shale, as well as anthracite coal.\n\nThe park is at an elevation of atop the Allegheny Plateau, just west of the Allegheny Front, a steep escarpment which rises in , and marks the transition between the Ridge-and-Valley Appalachians to the east and the Allegheny Plateau to the west. The Allegheny Plateau and Appalachian mountains were all formed in the Alleghenian orogeny some 300 million years ago, when Gondwana (specifically what became Africa) and what became North America collided, forming Pangaea.\n\nThe lake within the park is at an elevation of about , and the park itself sits in a natural basin. The basin and the underlying sandstone trap water and thus form the lake and surrounding bogs. The higher elevation leads to a cooler climate, and the basin helps trap denser, cooler air, leading to longer winters and milder summers.\n\nThe cooler climate also means the park is home to animals and plants typically found much further north. The Allegheny Plateau has a continental climate, with occasional severe low temperatures in winter and average daily temperature ranges of 20 °F (11 °C) in winter and 26 °F (14 °C) in summer. In 1972, long-term average monthly temperatures ranged from a high of in July to a low of in January. The mean annual precipitation for the Black Moshannon Creek watershed is 40 to 42 inches (1016 to 1067 mm). The soil in the park is mostly derived from sandstone and as such does not have much capacity to neutralize acid rain. The highest recorded temperature at the park was in 1988, and the record low was in 1994.\nWithin Black Moshannon State Park there is a State Park Natural Area protecting the bogs. The park itself is part of a much larger Important Bird Area, which includes most of the surrounding state forest, airport, and private properties.\n\nThe bogs at the park contain large amounts of sphagnum moss; this decomposes very slowly, causing layers of dead moss to build up at the bottom of the bog, creating peat. In 1994, of bog at the state park were protected as the \"Black Moshannon Bog Natural Area\"; this was originally conceived as part of the \"State Parks 2000\" strategic plan of the DCNR, and fourteen years later the total area of bog protected as a Natural Area had increased to .\n\nMost bogs exist in glaciated areas, but Black Moshannon State Park is on the Allegheny Plateau. This area was not covered by glaciers during the last ice age. The bogs formed here because of the beds of sandstone that lie flat, a short distance below the surface of the earth. The sandstone formations in the park do not absorb water very well, so any depression in them will collect water, as has happened here. The bogs extend the shores of the lake. Migratory shorebirds that visit here include greater and lesser yellowlegs, least sandpiper, solitary sandpiper, and the spotted sandpiper, which has been confirmed as using the IBA as a breeding grounds.\n\nThe water in the bog is low in nutrients and high in acidity, which makes it difficult for most plants to live there. Only specialized plants can thrive in the park bogs: there are three species of carnivorous plants and seventeen varieties of orchid. Wild cranberries and blueberries grow in the bog along with sedges, leatherleaf shrubs, Arctic cotton grass, and viburnums. The bogs are all protected by the state of Pennsylvania.\n\nWhite-tailed deer, wild turkey, ruffed grouse, opossum, raccoon, hawks, chipmunks, porcupine, woodpeckers, and flying, red, and eastern gray squirrels are all fairly common in the park. Black bears also inhabit Black Moshannon State Park. Many of these animals were decimated due to the effects of deforestation, pollution and unregulated hunting and trapping that took place during the late 19th century. Hunting controls established by the Pennsylvania Game Commission and the work of the Civilian Conservation Corps and Pennsylvania Department of Conservation and Natural Resources in re-establishing the second growth forest have led to the strong comeback of game species at Black Moshannon State Park and throughout the forests of Pennsylvania.\n\nThe lake is home to American beavers, as well as great blue heron, swans, snow geese, common loons, and many other types of waterfowl, with Canada goose, ring-necked duck, mallard, and wood duck the most commonly seen. The bogs, marshes, and swamps contain frogs and salamanders, and provide a habitat for carnivorous plants like the pitcher plant and sundew. Black Moshannon State Park is home to many common species of songbirds, including ovenbirds.\n\nThe conifer and mixed-forests of the park and its surroundings provide habitats for northern saw-whet owl, blue-headed vireo, hermit thrush, dark-eyed junco, and magnolia, pine, yellow-rumped, Blackburnian, and black-throated green warblers. The deciduous forests provide habitats for songbirds, such as scarlet tanager and red-eyed vireo.\n\nAn outbreak of the non-native gypsy moth in the mid-1980s nearly devastated the woods in a small valley. Selective timber cuts harvested the trees that were affected by species of moth. Today the Sleepy Hollow Trail for hiking, biking, and cross-country skiing loops through the new growth in the area, which provides an ideal habitat for populations of white-tailed deer and wild turkey.\n\nPennsylvania Important Bird Area #33 encompasses . The land includes parts of the state park and surrounding Moshannon State Forest, as well as Pennsylvania State Game Lands No. 33, Mid-State Regional Airport (which borders both the park and forest), and some other nearby parcels of private land. The Pennsylvania Audubon Society has designated of Black Moshannon State Park as an IBA, which is an area designated as a globally important habitat for the conservation of bird populations.\nOrnithologists and bird watchers have recorded a total of 175 species at the IBA. Several factors contribute to the high total of bird species observed: there is a large area of forest in the IBA, as well as great habitat diversity. The location of the IBA along the Allegheny Front also contributes to the diverse bird populations.\n\nBlack Moshannon Lake and the bogs of the Natural Area are especially important to the IBA. They serve as a stopover for migratory waterfowl and shorebirds. Waterfowl observed at the park include pied-billed and Slavonian grebes, common loon, American black duck, ruddy duck, blue-winged and green-winged teal, tundra swan, long-tailed duck, hooded and red-breasted merganser, greater and lesser scaup, northern pintail, bufflehead, American wigeon, and northern shoveler.\n\nPennsylvania IBA #33 is on the Allegheny Front, which is along a prime migratory path for a variety of birds of prey. The golden eagle, bald eagle, osprey, and northern harrier pass through the area during their annual migration periods. It is possible that the bald eagle may nest within the IBA, but this has not been confirmed. Raptors which do nest in the forests of the IBA include the northern goshawk, red-shouldered, broad-winged, red-tailed, sharp-shinned, and Cooper's hawks.\n\nThe cool, damp habitat provided by the bogs at Black Moshannon State Park provides a home for some birds that are at the southern limit of their habitat in central Pennsylvania. The Canada warbler and northern waterthrush nest in the bogs, as do the alder flycatcher, common yellowthroat, swamp sparrow, red-winged blackbird, and gray catbird. The olive-sided flycatcher, which is designated as locally extinct in Pennsylvania, has been seen during the breeding season at Black Moshannon State Natural Area. Bird watchers have observed nesting barred owls in the IBA, as well as Virginia rail and sora.\n\nTwenty-one cabins can be used by visitors at Black Moshannon State Park. Thirteen are rustic cabins, built by the CCC, with electric lights, a kitchen stove and a wood-burning stove, refrigerator, and bunk beds. Six are modern cabins, including the former ski lodge, with electric heat, a bedroom, living room, kitchen, and bath. There are also two deluxe camping cottages with electric heat and similar amenities as the rustic cabins. All cabin renters need to bring their own household items such as linens and cookware. Pets are prohibited in all cabins in the park.\n\nThere are seventy-two campsites at Black Moshannon State Park. Each campsite has access to washhouses with flush toilets, showers, and laundry tubs. The campsites also have fire rings and picnic tables. There is also an organized group tenting area, which can accommodate a group of up to 60 persons. Some sites allow pets; there are also twelve full hook-up sites available. These have electric service, water and sewer hook-ups as well. Nine sites are tent-only.\n\nThe sandy beach on Black Moshannon Lake is open from Memorial Day weekend through Labor Day weekend, and the beach bathhouse was built by the CCC. Beginning in 2008, lifeguards will not be posted at the beach.\n\nThere are eight picnic pavilions built by the CCC in the park, which can be reserved for a fee. In addition to the pavilions, Black Moshannon State Park has 250 picnic tables in four large picnic areas. The use of these picnic tables and unreserved pavilions is first come, first served, and they are free of charge.\n\nSome modern activities are prohibited in Black Moshannon State Park. The operation of drones and similar radio controlled devices is prohibited. All-terrain vehicle usage is also prohibited and closely monitored by DCNR. Further, removing any natural items is illegal as is firewood cutting without a permit.\n\nBoating is a popular use of the waters of Black Moshannon Lake, which covers . Canoes, sailboats, and electric motor boats are all permitted on Black Moshannon Lake, provided they are properly registered with the state. Edward Gertler, author of a series of canoeing books, calls Black Moshannon Creek \"about the best whitewater run in the West Branch Susquehanna Watershed\" in \"Keystone Canoeing\", and the first stretch of Class 2+ whitewater for canoeing and kayaking begins in the park, just downstream of the dam.\n\nCold water fishing is available in Black Moshannon Creek and several of its tributaries, where anglers will find rainbow and brown trout that have been stocked there for sport fishing by the Pennsylvania Fish and Boat Commission. Black Moshannon Lake's waters are warmer than those of the creek, and so hold many different species of fish, including largemouth bass, yellow perch, chain pickerel, bullhead catfish, northern pike, bluegill, and crappie.\n\nHunting is permitted in most of Black Moshannon State Park. It helps to prevent an overpopulation of animals and the resulting overbrowsing of the understory. The most common game species are ruffed grouse, eastern gray squirrel, wild turkey, and white-tailed deer. However, the hunting of groundhogs is prohibited.\n\nThere are of trails at Black Moshannon State Park that are open to hiking, mountain biking, cross-country skiing, and snowmobiling. All trails are open to hiking, most are open to skiing during the winter months, and select trails are open to snowmobiles and mountain bikes. The park is especially popular among cross-country skiing enthusiasts due to its high elevation. Skiers will find trails that are largely free of rocks, with a layer of grass beneath the snow. Sleepy Hollow, Seneca, Indian, and Hay Road Trails are most frequently used. Seven of the park's thirteen trails are described above, the remaining six follow.\n\nThe Friends of Black Moshannon State Park is a volunteer organization that promotes the recreational use of the park through a summer festival. The group also works with the park staff to maintain the park lands, serve as campground hosts, survey the eastern bluebird population, and organize conservation projects.\n\nThe Summer Festival usually takes place over the fourth weekend of July. Events at the festival recall the lumbering history of the park. Log rolling, axe throwing, and cross-cut sawing events are held, as are horseshoe and seed spitting contests. Black Moshannon Lake is open to canoe races and fishing. A Saturday night bonfire party is held at the beach, with music and refreshments.\n\nThe following state parks are within of Black Moshannon State Park:\n\n"}
{"id": "14122543", "url": "https://en.wikipedia.org/wiki?curid=14122543", "title": "Burns temperature", "text": "Burns temperature\n\nThe Burns temperature, \"T\", is the temperature where a ferroelectric material, previously in paraelectric state, starts to present randomly polarized nanoregions, that are polar precursor cluster. This behaviour is typical of several, but not all, ferroelectric materials, and was observed in lead titanate (PbTiO), potassium niobate (KNbO), lead lanthanum zirconate titanate (PLZT), lead magnesium niobate (PMN), lead zinc niobate (PZN), KSr(NbO), and strontium barium niobate (SBN), NaBiO(NBT).\n\nThe Burns temperature, named from Gerald Burns, who studied this phenomenon with collaboration of Frank H. Dacol, has not been well understood yet.\n\n"}
{"id": "6111", "url": "https://en.wikipedia.org/wiki?curid=6111", "title": "Chemical vapor deposition", "text": "Chemical vapor deposition\n\nChemical vapor deposition (CVD) is a deposition method used to produce high quality, high-performance, solid materials, typically under vacuum. The process is often used in the semiconductor industry to produce thin films.\n\nIn typical CVD, the wafer (substrate) is exposed to one or more volatile precursors, which react and/or decompose on the substrate surface to produce the desired deposit. Frequently, volatile by-products are also produced, which are removed by gas flow through the reaction chamber.\n\nMicrofabrication processes widely use CVD to deposit materials in various forms, including: monocrystalline, polycrystalline, amorphous, and epitaxial. These materials include: silicon (dioxide, carbide, nitride, oxynitride), carbon (fiber, nanofibers, nanotubes, diamond and graphene), fluorocarbons, filaments, tungsten, titanium nitride and various high-k dielectrics.\n\nCVD is practiced in a variety of formats. These processes generally differ in the means by which chemical reactions are initiated.\n\nMost modern CVD is either LPCVD or UHVCVD.\n\nCVD is commonly used to deposit conformal films and augment substrate surfaces in ways that more traditional surface modification techniques are not capable of. CVD is extremely useful in the process of atomic layer deposition at depositing extremely thin layers of material. A variety of applications for such films exist. Gallium arsenide is used in some integrated circuits (ICs) and photovoltaic devices. Amorphous polysilicon is used in photovoltaic devices. Certain carbides and nitrides confer wear-resistance. Polymerization by CVD, perhaps the most versatile of all applications, allows for super-thin coatings which possess some very desirable qualities, such as lubricity, hydrophobicity and weather-resistance to name a few. CVD of metal-organic frameworks, a class of crystalline nanoporous materials, has recently been demonstrated. Applications for these films are anticipated in gas sensing and low-k dielectrics\nCVD techniques are adventageous for membrane coatings as well, such as those in desalination or water treatment, as these coatings can be sufficiently uniform (conformal) and thin that they do not clog membrane pores.\n\nPolycrystalline silicon is deposited from trichlorosilane (SiHCl) or silane (SiH), using the following reactions:\n\nThis reaction is usually performed in LPCVD systems, with either pure silane feedstock, or a solution of silane with 70–80% nitrogen. Temperatures between 600 and 650 °C and pressures between 25 and 150 Pa yield a growth rate between 10 and 20 nm per minute. An alternative process uses a hydrogen-based solution. The hydrogen reduces the growth rate, but the temperature is raised to 850 or even 1050 °C to compensate. Polysilicon may be grown directly with doping, if gases such as phosphine, arsine or diborane are added to the CVD chamber. Diborane increases the growth rate, but arsine and phosphine decrease it.\n\nSilicon dioxide (usually called simply \"oxide\" in the semiconductor industry) may be deposited by several different processes. Common source gases include silane and oxygen, dichlorosilane (SiClH) and nitrous oxide (NO), or tetraethylorthosilicate (TEOS; Si(OCH)). The reactions are as follows:\n\nThe choice of source gas depends on the thermal stability of the substrate; for instance, aluminium is sensitive to high temperature. Silane deposits between 300 and 500 °C, dichlorosilane at around 900 °C, and TEOS between 650 and 750 °C, resulting in a layer of \"low- temperature oxide\" (LTO). However, silane produces a lower-quality oxide than the other methods (lower dielectric strength, for instance), and it deposits nonconformally. Any of these reactions may be used in LPCVD, but the silane reaction is also done in APCVD. CVD oxide invariably has lower quality than thermal oxide, but thermal oxidation can only be used in the earliest stages of IC manufacturing.\n\nOxide may also be grown with impurities (alloying or \"doping\"). This may have two purposes. During further process steps that occur at high temperature, the impurities may diffuse from the oxide into adjacent layers (most notably silicon) and dope them. Oxides containing 5–15% impurities by mass are often used for this purpose. In addition, silicon dioxide alloyed with phosphorus pentoxide (\"P-glass\") can be used to smooth out uneven surfaces. P-glass softens and reflows at temperatures above 1000 °C. This process requires a phosphorus concentration of at least 6%, but concentrations above 8% can corrode aluminium. Phosphorus is deposited from phosphine gas and oxygen:\n\nGlasses containing both boron and phosphorus (borophosphosilicate glass, BPSG) undergo viscous flow at lower temperatures; around 850 °C is achievable with glasses containing around 5 weight % of both constituents, but stability in air can be difficult to achieve. Phosphorus oxide in high concentrations interacts with ambient moisture to produce phosphoric acid. Crystals of BPO can also precipitate from the flowing glass on cooling; these crystals are not readily etched in the standard reactive plasmas used to pattern oxides, and will result in circuit defects in integrated circuit manufacturing.\n\nBesides these intentional impurities, CVD oxide may contain byproducts of the deposition. TEOS produces a relatively pure oxide, whereas silane introduces hydrogen impurities, and dichlorosilane introduces chlorine.\n\nLower temperature deposition of silicon dioxide and doped glasses from TEOS using ozone rather than oxygen has also been explored (350 to 500 °C). Ozone glasses have excellent conformality but tend to be hygroscopic – that is, they absorb water from the air due to the incorporation of silanol (Si-OH) in the glass. Infrared spectroscopy and mechanical strain as a function of temperature are valuable diagnostic tools for diagnosing such problems.\n\nSilicon nitride is often used as an insulator and chemical barrier in manufacturing ICs. The following two reactions deposit silicon nitride from the gas phase:\n\nSilicon nitride deposited by LPCVD contains up to 8% hydrogen. It also experiences strong tensile stress, which may crack films thicker than 200 nm. However, it has higher resistivity and dielectric strength than most insulators commonly available in microfabrication (10 Ω·cm and 10 MV/cm, respectively).\n\nAnother two reactions may be used in plasma to deposit SiNH:\n\nThese films have much less tensile stress, but worse electrical properties (resistivity 10 to 10 Ω·cm, and dielectric strength 1 to 5 MV/cm).\n\nCVD for tungsten is achieved from tungsten hexafluoride (WF), which may be deposited in two ways:\n\nOther metals, notably aluminium and copper, can be deposited by CVD. , commercially cost-effective CVD for copper did not exist, although volatile sources exist, such as Cu(hfac). Copper is typically deposited by electroplating. Aluminum can be deposited from triisobutylaluminium (TIBAL) and related organoaluminium compounds.\n\nCVD for molybdenum, tantalum, titanium, nickel is widely used. These metals can form useful silicides when deposited onto silicon. Mo, Ta and Ti are deposited by LPCVD, from their pentachlorides. Nickel, molybdenum, and tungsten can be deposited at low temperatures from their carbonyl precursors. In general, for an arbitrary metal \"M\", the chloride deposition reaction is as follows:\n\nwhereas the carbonyl decomposition reaction can happen spontaneously under thermal treatment or acoustic cavitation and is as follows:\n\nthe decomposition of metal carbonyls is often violently precipitated by moisture or air, where oxygen reacts with the metal precursor to form metal or metal oxide along with carbon dioxide.\n\nNiobium(V) oxide layers can be produced by the thermal decomposition of niobium(V) ethoxide with the loss of diethyl ether according to the equation:\n\nMany variations of CVD can be utilized to synthesize graphene. Although many advancements have been made, the processes listed below are not commercially viable yet.\nThe most popular carbon source used to produce graphene is methane gas. Less popular choices include petroleum asphalt, notable for being inexpensive but more difficult to work with.\nThe use of catalyst is viable in changing the physical process of graphene production. Notable examples include iron nanoparticles, nickel foam, and gallium vapor. These catalysts can either be used in situ during graphene buildup, or situated at some distance away at the deposition area. Some catalysts require another step to remove them from the sample material.\n\nThe direct growth of high-quality, large single-crystalline domains of graphene on a dielectric substrate is of vital importance for applications in electronics and optoelectronics. Combining the advantages of both catalytic CVD and the ultra-flat dielectric substrate, gaseous catalyst-assisted CVD paves the way for synthesizing high-quality graphene for device applications while avoiding the transfer process.\nPhysical conditions such as surrounding pressure, temperature, carrier gas, and chamber material play a big role in production of graphene.\n\nMost systems use LPCVD with pressures ranging from 1 to 1500 Pa. However, some still use APCVD. Low pressures are used more commonly as they help prevent unwanted reactions and produce more uniform thickness of deposition on the substrate.\n\nOn the other hand, temperatures used range from 800–1050 °C. High temperatures translate to an increase of the rate of reaction. Caution has to be exercised as high temperatures do pose higher danger levels in addition to greater energy costs.\nHydrogen gas and inert gases such as argon are flowed into the system. These gases act as a carrier, enhancing surface reaction and improving reaction rate, thereby increasing deposition of graphene onto the substrate.\nStandard quartz tubing and chambers are used in CVD of graphene. Quartz is chosen because it has a very high melting point and is chemically inert. In other words, quartz does not interfere with any physical or chemical reactions regardless of the conditions.\nRaman spectroscopy, X-ray spectroscopy, transmission electron microscopy (TEM), and scanning electron microscopy (SEM) are used to examine and characterize the graphene samples.\n\nRaman spectroscopy is used to characterize and identify the graphene particles; X-ray spectroscopy is used to characterize chemical states; TEM is used to provide fine details regarding the internal composition of graphene; SEM is used to examine the surface and topography.\n\nSometimes, atomic force microscopy (AFM) is used to measure local properties such as friction and magnetism.\n\nCold wall CVD technique can be used to study the underlying surface science involved in graphene nucleation and growth as it allows unprecedented control of process parameters like gas flow rates, temperature and pressure as demonstrated in a recent study. The study was carried out in a home-built vertical cold wall system utilizing resistive heating by passing direct current through the substrate. It provided conclusive insight into a typical surface-mediated nucleation and growth mechanism involved in two-dimensional materials grown using catalytic CVD under conditions sought out in the semiconductor industry.\n\nIn spite of graphene's exciting electronic and thermal properties, it is unsuitable as a transistor for future digital devices, due to the absence of a bandgap between the conduction and valence bands. This makes it impossible to switch between on and off states with respect to electron flow. Scaling things down, graphene nanoribbons of less than 10 nm in width do exhibit electronic bandgaps and are therefore potential candidates for digital devices. Precise control over their dimensions, and hence electronic properties, however, represents a challenging goal, and the ribbons typically possess rough edges that are detrimental to their performance.\n\nCVD can be used to produce a synthetic diamond by creating the circumstances necessary for carbon atoms in a gas to settle on a substrate in crystalline form. CVD of diamonds has received much attention in the materials sciences because it allows many new applications that had previously been considered too expensive. CVD diamond growth typically occurs under low pressure (1–27 kPa; 0.145–3.926 psi; 7.5–203 Torr) and involves feeding varying amounts of gases into a chamber, energizing them and providing conditions for diamond growth on the substrate. The gases always include a carbon source, and typically include hydrogen as well, though the amounts used vary greatly depending on the type of diamond being grown. Energy sources include hot filament, microwave power, and arc discharges, among others. The energy source is intended to generate a plasma in which the gases are broken down and more complex chemistries occur. The actual chemical process for diamond growth is still under study and is complicated by the very wide variety of diamond growth processes used.\n\nUsing CVD, films of diamond can be grown over large areas of substrate with control over the properties of the diamond produced. In the past, when high pressure high temperature (HPHT) techniques were used to produce a diamond, the result was typically very small free standing diamonds of varying sizes. With CVD diamond growth areas of greater than fifteen centimeters (six inches) diameter have been achieved and much larger areas are likely to be successfully coated with diamond in the future. Improving this process is key to enabling several important applications.\n\nThe growth of diamond directly on a substrate allows the addition of many of diamond's important qualities to other materials. Since diamond has the highest thermal conductivity of any bulk material, layering diamond onto high heat producing electronics (such as optics and transistors) allows the diamond to be used as a heat sink. Diamond films are being grown on valve rings, cutting tools, and other objects that benefit from diamond's hardness and exceedingly low wear rate. In each case the diamond growth must be carefully done to achieve the necessary adhesion onto the substrate. Diamond's very high scratch resistance and thermal conductivity, combined with a lower coefficient of thermal expansion than Pyrex glass, a coefficient of friction close to that of Teflon (polytetrafluoroethylene) and strong lipophilicity would make it a nearly ideal non-stick coating for cookware if large substrate areas could be coated economically.\n\nCVD growth allows one to control the properties of the diamond produced. In the area of diamond growth, the word \"diamond\" is used as a description of any material primarily made up of sp3-bonded carbon, and there are many different types of diamond included in this. By regulating the processing parameters—especially the gases introduced, but also including the pressure the system is operated under, the temperature of the diamond, and the method of generating plasma—many different materials that can be considered diamond can be made. Single crystal diamond can be made containing various dopants. Polycrystalline diamond consisting of grain sizes from several nanometers to several micrometers can be grown. Some polycrystalline diamond grains are surrounded by thin, non-diamond carbon, while others are not. These different factors affect the diamond's hardness, smoothness, conductivity, optical properties and more.\n\nCommercially, mercury cadmium telluride is of continuing interest for detection of infrared radiation. Consisting of an alloy of CdTe and HgTe, this material can be prepared from the dimethyl derivatives of the respective elements.\n\n\n"}
{"id": "1903345", "url": "https://en.wikipedia.org/wiki?curid=1903345", "title": "Chladni's law", "text": "Chladni's law\n\nChladni's law, named after Ernst Chladni, relates the frequency of modes of vibration for flat circular surfaces with fixed center as a function of the numbers \"m\" of diametric (linear) nodes and \"n\" of radial (circular) nodes. It is stated as the equation\nwhere \"C\" and \"p\" are coefficients which depend on the properties of the plate.\n\nFor flat circular plates, \"p\" is roughly 2, but Chladni's law can also be used to describe the vibrations of cymbals, handbells, and church bells in which case \"p\" can vary from 1.4 to 2.4. In fact, \"p\" can even vary for a single object, depending on which family of modes is being examined.\n\n"}
{"id": "2108156", "url": "https://en.wikipedia.org/wiki?curid=2108156", "title": "Cooperative Institute for Limnology and Ecosystems Research", "text": "Cooperative Institute for Limnology and Ecosystems Research\n\nThe Cooperative Institute for Limnology and Ecosystems Research (CILER) fosters research collaborations between the National Oceanic and Atmospheric Administration (NOAA) Office of Oceanic and Atmospheric Research (OAR) Great Lakes Environmental Research Laboratory (GLERL), Michigan State University (MSU), and the University of Michigan (UM). It is one of 16 NOAA Cooperative Institutes (CIs).\n\nThe CILER research themes are:\n\n"}
{"id": "186630", "url": "https://en.wikipedia.org/wiki?curid=186630", "title": "Cosmic Background Explorer", "text": "Cosmic Background Explorer\n\nThe Cosmic Background Explorer (COBE ), also referred to as Explorer 66, was a satellite dedicated to cosmology, which operated from 1989 to 1993. Its goals were to investigate the cosmic microwave background radiation (CMB) of the universe and provide measurements that would help shape our understanding of the cosmos.\n\nCOBE's measurements provided two key pieces of evidence that supported the Big Bang theory of the universe: that the CMB has a near-perfect black-body spectrum, and that it has very faint anisotropies. Two of COBE's principal investigators, George Smoot and John Mather, received the Nobel Prize in Physics in 2006 for their work on the project. According to the Nobel Prize committee, \"the COBE-project can also be regarded as the starting point for cosmology\nas a precision science\".\n\nCOBE was followed by two more advanced spacecraft: WMAP operated from 2001-2010 and Planck from 2009-2013.\n\nIn 1974, NASA issued an Announcement of Opportunity for astronomical missions that would use a small- or medium-sized Explorer spacecraft. Out of the 121 proposals received, three dealt with studying the cosmological background radiation. Though these proposals lost out to the Infrared Astronomical Satellite (IRAS), their strength made NASA further explore the idea. In 1976, NASA formed a committee of members from each of 1974's three proposal teams to put together their ideas for such a satellite. A year later, this committee suggested a polar-orbiting satellite called COBE to be launched by either a Delta rocket or the Space Shuttle. It would contain the following instruments:\n\nNASA accepted the proposal provided that the costs be kept under $30 million, excluding launcher and data analysis. Due to cost overruns in the Explorer program due to IRAS, work on constructing the satellite at Goddard Space Flight Center (GSFC) did not begin until 1981. To save costs, the infrared detectors and liquid helium dewar on COBE would be similar to those used on IRAS.\n\nCOBE was originally planned to be launched on a Space Shuttle mission STS-82-B in 1988 from Vandenberg Air Force Base, but the Challenger explosion delayed this plan when the Shuttles were grounded. NASA kept COBE's engineers from going to other space agencies to launch COBE, but eventually, a redesigned COBE was placed into sun-synchronous orbit on November 18, 1989 aboard a Delta rocket. A team of American scientists announced, on April 23, 1992 that they had found the primordial \"seeds\" (CMBE anisotropy) in data from COBE. The announcement was reported worldwide as a fundamental scientific discovery and ran on the front page of \"The New York Times\".\n\nThe Nobel Prize in Physics for 2006 was jointly awarded to John C. Mather, NASA Goddard Space Flight Center, and George F. Smoot, University of California, Berkeley, \"for their discovery of the blackbody form and anisotropy of the cosmic microwave background radiation.\"\n\nCOBE was an Explorer class satellite, with technology borrowed heavily from IRAS, but with some unique characteristics.\n\nThe need to control and measure all the sources of systematic errors required a rigorous and integrated design. COBE would have to operate for a minimum of 6 months, and constrain the amount of radio interference from the ground, COBE and other satellites as well as radiative interference from the Earth, Sun and Moon. The instruments required temperature stability and to maintain gain, and a high level of cleanliness to reduce entry of stray light and thermal emission from particulates.\n\nThe need to control systematic error in the measurement of the CMB anisotropy and measuring the zodiacal cloud at different elongation angles for subsequent modeling required that the satellite rotate at a 0.8 rpm spin rate. The spin axis is also tilted back from the orbital velocity vector as a precaution against possible deposits of residual atmospheric gas on the optics as well against the infrared glow that would result from fast neutral particles hitting its surfaces at extremely high speed.\nIn order to meet the twin demands of slow rotation and three-axis attitude control, a sophisticated pair of yaw angular momentum wheels were employed with their axis oriented along the spin axis . These wheels were used to carry an angular momentum opposite that of the entire spacecraft in order to create a zero net angular momentum system.\n\nThe orbit would prove to be determined based on the specifics of the spacecraft’s mission. The overriding considerations were the need for full sky coverage, the need to eliminate stray radiation from the instruments and the need to maintain thermal stability of the dewar and the instruments. A circular Sun-synchronous orbit satisfied all these requirements. A 900 km altitude orbit with a 99° inclination was chosen as it fit within the capabilities of either a Shuttle (with an auxiliary propulsion on COBE) or a Delta rocket. This altitude was a good compromise between Earth's radiation and the charged particle in Earth's radiation belts at higher altitudes. An ascending node at 6 p.m. was chosen to allow COBE to follow the boundary between sunlight and darkness on Earth throughout the year.\n\nThe orbit combined with the spin axis made it possible to keep the Earth and the Sun continually below the plane of the shield, allowing a full sky scan every six months.\n\nThe last two important parts pertaining to the COBE mission were the dewar and Sun-Earth shield. The dewar was a 650-liter superfluid helium cryostat designed to keep the FIRAS and DIRBE instruments cooled during the duration of the mission. It was based on the same design as one used on IRAS and was able to vent helium along the spin axis near the communication arrays. The conical Sun-Earth shield protected the instruments from direct solar and Earth based radiation as well as radio interference from Earth and the COBE's transmitting antenna. Its multilayer insulating blankets provided thermal isolation for the dewar.\n\nThe science mission was conducted by the three instruments detailed previously: DIRBE, FIRAS and the DMR. The instruments overlapped in wavelength coverage, providing consistency check on measurements in the regions of spectral overlap and assistance in discriminating signals from our galaxy, Solar System and CMB.\n\nCOBE's instruments would fulfill each of their objectives as well as making observations that would have implications outside COBE’s initial scope.\n\nDuring the long gestation period of COBE, there were two significant astronomical developments. First, in 1981, two teams of astronomers, one led by David Wilkinson of Princeton and the other by Francesco Melchiorri of the University of Florence, simultaneously announced that they detected a quadrupole distribution of CMB using balloon-borne instruments. This finding would have been the detection of the black-body distribution of CMB that FIRAS on COBE was to measure.\nIn particular, the Florence group claimed a detection of intermediate angular scale\nanisotropies at the level 100 microkelvins in agreement with later measurements made by the BOOMERanG experiment.\n\nHowever, a number of other experiments attempted to duplicate their results and were unable to do so.\n\nSecond, in 1987 a Japanese-American team led by Andrew Lange and Paul Richards of UC Berkeley and Toshio Matsumoto of Nagoya University made an announcement that CMB was not that of a true black body. In a sounding rocket experiment, they detected an excess brightness at 0.5 and 0.7 mm wavelengths.\n\nWith these developments serving as a backdrop to COBE’s mission, scientists eagerly awaited results from FIRAS. The results of FIRAS were startling in that they showed a perfect fit of the CMB and the theoretical curve for a black body at a temperature of 2.7 K, thus proving the Berkeley-Nagoya results erroneous.\n\nFIRAS measurements were made by measuring the spectral difference between a 7° patch of the sky against an internal black body. The interferometer in FIRAS covered between 2 and 95 cm in two bands separated at 20 cm. There are two scan lengths (short and long) and two scan speeds (fast and slow) for a total of four different scan modes. The data were collected over a ten-month period.\n\nThe DMR was able to spend four years mapping the detectable anisotropy of cosmic background radiation as it was the only instrument not dependent on the dewar’s supply of helium to keep it cooled. This operation was able to create full sky maps of the CMB by subtracting out galactic emissions and dipole at various frequencies. The cosmic microwave background fluctuations are extremely faint, only one part in 100,000 compared to the 2.73 kelvins average temperature of the radiation field. The cosmic microwave background radiation is a remnant of the Big Bang and the fluctuations are the imprint of density contrast in the early universe. The density ripples are believed to have produced structure formation as observed in the universe today: clusters of galaxies and vast regions devoid of galaxies (NASA).\n\nDIRBE also detected 10 new far-IR emitting galaxies in the region not surveyed by IRAS as well as nine other candidates in the weak far-IR that may be spiral galaxies.\n\nGalaxies that were detected at the 140 and 240 μm were also able to provide information on very cold dust (VCD). At these wavelengths, the mass and temperature of VCD can be derived.\n\nWhen these data were joined with 60 and 100 μm data taken from IRAS, it was found that the far-infrared luminosity arises from cold (≈17–22 K) dust associated with diffuse HI cirrus clouds, 15-30% from cold (≈19 K) dust associated with molecular gas, and less than 10% from warm (≈29 K) dust in the extended low-density HII regions.\n\nOn top of the findings DIRBE had on galaxies, it also made two other significant contributions to science.\nThe DIRBE instrument was able to conduct studies on interplanetary dust (IPD) and determine if its origin was from asteroid or cometary particles. The DIRBE data collected at 12, 25, 50 and 100 μm were able to conclude that grains of asteroidal origin populate the IPD bands and the smooth IPD cloud.\n\nThe second contribution DIRBE made was a model of the Galactic disk as seen edge-on from our position. According to the model, if our Sun is 8.6 kpc from the Galactic center, then the Sun is 15.6 pc above the midplane of the disk, which has a radial and vertical scale lengths of 2.64 and 0.333 kpc, respectively, and is warped in a way consistent with the HI layer. There is also no indication of a thick disk.\n\nTo create this model, the IPD had to be subtracted out of the DIRBE data. It was found that this cloud, which as seen from Earth is Zodiacal light, was not centered on the Sun, as previously thought, but on a place in space a few million kilometers away. This is due to the gravitation influence of Saturn and Jupiter.\n\nIn addition to the science results detailed in the last section, there are numerous cosmological questions left unanswered by COBE’s results. A direct measurement of the extragalactic background light (EBL) can also provide important constraints on the integrated cosmological history of star formation, metal and dust production, and the conversion of starlight into infrared emissions by dust.\n\nBy looking at the results from DIRBE and FIRAS in the 140 to 5000 μm we can detect that the integrated EBL intensity is ≈16 nW/(m·sr). This is consistent with the energy released during nucleosynthesis and constitutes about 20–50% of the total energy released in the formation of helium and metals throughout the history of the universe. Attributed only to nuclear sources, this intensity implies that more than 5–15% of the baryonic mass density implied by big bang nucleosynthesis analysis has been processed in stars to helium and heavier elements.\n\nThere were also significant implications into star formation. COBE observations provide important constraints on the cosmic star formation rate, and help us calculate the EBL spectrum for various star formation histories. Observation made by COBE require that star formation rate at redshifts of \"z\" ≈ 1.5 to be larger than that inferred from UV-optical observations by a factor of 2. This excess stellar energy must be mainly generated by massive stars in yet-undetected dust enshrouded galaxies or extremely dusty star forming regions in observed galaxies. The exact star formation history cannot unambiguously be resolved by COBE and further observations must be made in the future.\n\nOn June 30, 2001, NASA launched a follow-up mission to COBE led by DMR Deputy Principal Investigator Charles L. Bennett. The Wilkinson Microwave Anisotropy Probe has clarified and expanded upon COBE's accomplishments. Following WMAP, the European Space Agency's probe, Planck has continued to increase the resolution at which the background has been mapped.\n\n\n\n"}
{"id": "2995499", "url": "https://en.wikipedia.org/wiki?curid=2995499", "title": "Crosstalk", "text": "Crosstalk\n\nIn electronics, crosstalk is any phenomenon by which a signal transmitted on one circuit or channel of a transmission system creates an undesired effect in another circuit or channel. Crosstalk is usually caused by undesired capacitive, inductive, or conductive coupling from one circuit or channel to another.\n\nCrosstalk is a significant issue in structured cabling, audio electronics, integrated circuit design, wireless communication and other communications systems.\n\nIn structured cabling, crosstalk refers to electromagnetic interference from one unshielded twisted pair to another twisted pair, normally running in parallel. Signals traveling through adjacent pairs of wire interfere with each other. The pair causing the interference is called the “disturbing pair,” while the pair experiencing the interference is the “disturbed pair”.\n\n\n\n\n\n\nIn stereo audio reproduction, crosstalk can refer to signal leaking across from one program channel to another, reducing channel separation and stereo imaging. Crosstalk between channels in mixing consoles, and between studio feeds is a much more noticeable problem, as these are likely to be carrying very different programmes or material.\n\nCrosstalk is an electrical effect and can be quantified with a crosstalk measurement. Crosstalk measurements are made on audio systems to determine the amount of signal leaking across from one channel to another. The Independent Broadcasting Authority published a weighting curve for use in crosstalk measurement that gives due emphasis to the subjective audibility of different frequencies. In the absence of any international standards, this is still in use despite the demise of the IBA.\n\nGood crosstalk performance for a stereo system is not difficult to achieve in today's digital audio systems, though it was hard to keep below the desired figure of -30 dB or so on vinyl recordings and FM radio.\n\nIn telecommunication or telephony, crosstalk is often distinguishable as pieces of speech or in-band signaling tones leaking from other people's connections. If the connection is analog, twisted pair cabling can often be used to reduce crosstalk. Alternatively, the signals can be converted to digital form, which is typically less susceptible to crosstalk.\n\nIn wireless communication, crosstalk is often denoted co-channel interference, and is related to adjacent-channel interference.\n\nIn integrated circuit design, crosstalk normally refers to a signal affecting another nearby signal. Usually the coupling is capacitive, and to the nearest neighbor, but other forms of coupling and effects on signal further away are sometimes important, especially in analog designs. See signal integrity for tools used to measure and prevent this problem, and substrate coupling for a discussion of crosstalk conveyed through the integrated circuit substrate. There are a wide variety of repair solutions, with increased spacing, wire re-ordering, and shielding being the most common.\n\nIn full-field optical coherence tomography, \"crosstalk\" refers to the phenomenon that due to highly scattering objects, multiple scattered photons reach the image plane and generate coherent signal after traveling a pathlength that matches that of the sample depth within a coherence length.\n\nIn stereoscopic 3D displays, \"crosstalk\" refers to the incomplete isolation of the left and right image channels so that one leaks or bleeds into the other - like a double exposure, which produces a ghosting effect.\n\n\n"}
{"id": "7214368", "url": "https://en.wikipedia.org/wiki?curid=7214368", "title": "Energy &amp; Environment", "text": "Energy &amp; Environment\n\nEnergy & Environment (\"E&E\") is an academic journal \"covering the direct and indirect environmental impacts of energy acquisition, transport, production and use\". Its editor-in-chief since 1998 is Sonja Boehmer-Christiansen. It is known for easygoing peer-review and publishing climate change denial papers.\n\nThe journal is abstracted and indexed in the Social Sciences Citation Index, Scopus, EBSCO databases, Current Contents/Social & Behavioral Sciences, and Compendex. According to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 0.319, ranking it 90th out of 93 journals in the category \"Environmental Studies\".\n\nThe journal's mission statement states that the publication's \"objective is to inform across professional and disciplinary boundaries and debate the social, economic, political and technological implications of environmental controls, as well as interrogate the science claims made to justify environmental regulations of the energy industries, including transport.\"\n\n\"Energy & Environment\" was first published in 1989; David Everest (Department of the Environment, United Kingdom) was its founding editor. Following his death in 1998, Boehmer-Christiansen became the journal's editor. She and several members of the journal's editorial advisory board previously had been associated with \"the Energy and Environment Groups\" at the Science and Technology Policy Unit (University of Sussex), with John Surrey. Benny Peiser has served as co-editor.\n\nThe journal is regarded as \"a small journal that caters to climate change denialists\". It has played an important role in attacking climate science and scientists, for example Michael E. Mann.\n\nSeveral scientists and socials scientists such Gavin Schmidt, Roger A. Pielke, Jr. Stephan Lewandowsky and Michael Ashley, have criticised that \"E&E\" has low standards of peer review and little impact. In addition, Ralph Keeling criticized a paper in the journal which claimed that CO levels were above 400 ppm in 1825, 1857 and 1942, writing in a letter to the editor, \"Is it really the intent of E&E to provide a forum for laundering pseudo-science?\" \n\nA 2005 article in \"Environmental Science & Technology\" stated that the journal is \"obscure\" and that \"scientific claims made in Energy & Environment have little credibility among scientists.\" Boehmer-Christiansen acknowledged that the journal's \"impact rating has remained too low for many ambitious young researchers to use it\", but blamed this on \"the negative attitudes of the Intergovernmental Panel on Climate Change (IPCC)/Climatic Research Unit people.\" According to Hans von Storch, the journal “tries to give people who do not have a platform a platform,” which \"is then attractive for skeptic papers. They know they can come through and that interested people make sure the paper enters the political realm.”\n\nWhen asked about the publication in the Spring of 2003 of a revised version of the paper at the center of the Soon and Baliunas controversy, Boehmer-Christiansen said, \"I'm following my political agenda -- a bit, anyway. But isn't that the right of the editor?\" \n\nPart of the journal's official mission statement reads: \"E&E has consistently striven to publish many ‘voices’ and to challenge conventional wisdoms. Perhaps more so than other European energy journal, the editor has made E&E a forum for more sceptical analyses of ‘climate change’ and the advocated solutions\".\n\n"}
{"id": "55445511", "url": "https://en.wikipedia.org/wiki?curid=55445511", "title": "Environmental Modelling &amp; Software", "text": "Environmental Modelling &amp; Software\n\nEnvironmental Modelling & Software publishes contributions, in the form of research articles, reviews and short communications, on recent advances in environmental modelling and/or software. The aim is to improve our capacity to represent, understand, predict or manage the behaviour of environmental systems at all practical scales, and to communicate those improvements to a wide scientific and professional audience.\n\nIt seeks presentation of: \n• Generic frameworks, techniques and issues which either integrate a range of disciplines and sectors or apply across a range \n• Model development, model evaluation, process identification and applications in diverse sectors of the environment (as outlined below) provided they reveal insights and contribute to the store of knowledge. Insights can relate to the generality and limitations of the modelling, methods, the model application and/or the systems modelled. Insights should be ones that are generalizable in some way and are likely to be of interest to those studying other systems and, preferably, other system types.\n• Development and application of environmental software, information and decision support systems\n• Real-world applications of software technologies - particularly state-of-the-art environmental software able to deal with complex requirements, conflicting user perspectives, and/or evolving data structures. Aspects related to software usability, reliability, verification and validation should be backed up with quantitative results as much as possible. Development and maintenance costs, and adoption and penetration of the software in the target user groups should be addressed. Licensing issues and open source access should be clearly specified.\n• Issues and methods related to the integrated modeling, assessment and management of environmental systems - including relevant policy and institutional analysis, public participation principles and methods, decision making methods, model integration, quality assurance and evaluation of models, data and procedure. \n\nJournal Metrics\n"}
{"id": "44338182", "url": "https://en.wikipedia.org/wiki?curid=44338182", "title": "Epsilon composite", "text": "Epsilon composite\n\nEpsilon Composite is a French company created in 1987 by Stephane LULL, its current CEO.\nTurnover in 2012 was 21M€ with 190 employees.\n\nEpsilon Composite produces a wide range of Carbon-fiber-reinforced polymer (CFRP) products for various applications:\n\nThe main production process used by the company is pultrusion.\n\nEpsilon Composite is mainly dealing with international customers (90% of the turnover comes from exports), mostly from Japan and Germany. The production site is located in Gaillan, France\n\n"}
{"id": "35843308", "url": "https://en.wikipedia.org/wiki?curid=35843308", "title": "Geikielite", "text": "Geikielite\n\nGeikielite is a magnesium titanium oxide mineral with formula: MgTiO. It is a member of the ilmenite group. It crystallizes in the trigonal system forming typically opaque, black to reddish black crystals.\n\nIt was first described in 1892 for an occurrence in the Ceylonese gem bearing gravel placers. It was named for Scottish geologist Sir Archibald Geikie (1835–1924). It occurs in metamorphosed impure magnesian limestones, in serpentinite derived from ultramafic rocks, in kimberlites and carbonatites. Associated minerals include rutile, spinel, clinohumite, perovskite, diopside, serpentine, forsterite, brucite, hydrotalcite, chlorite and calcite.\n\n"}
{"id": "34225248", "url": "https://en.wikipedia.org/wiki?curid=34225248", "title": "Graded shoreline", "text": "Graded shoreline\n\nA graded shoreline is a stage in the cycle of coastal development characterised by a flat and straight coastline. It is formed under the influence of wind and water from the original bays, islands, peninsulas and promontories. Sand and gravel is carried away and dumped at other locations depending on the direction and strength of sea currents. Typical of graded shorelines are the formation of dunes, wide sandy beaches and sometimes a lagoon or a spit. Where two graded shorelines meet, a headland may form with a sandy reef in the sea beyond it. Parallel to the graded shoreline sandbanks may form as a result of sediments transported away from the shore. \n\n\n"}
{"id": "3305140", "url": "https://en.wikipedia.org/wiki?curid=3305140", "title": "Grindylow", "text": "Grindylow\n\nIn English folklore, grindylow or grundylow is a creature in the counties of Yorkshire and Lancashire. The name is thought to be connected to Grendel, a name or term used in \"Beowulf\" and in many Old English charters where it is seen in connection with meres, bogs and lakes.\n\nGrindylows are said to grab children with their long sinewy arms and drown them if they come too close to the waters edge. Grindylows have been used as a shadowed figure to frighten children away from pools, marshes or ponds where they could drown.\n\nPeg Powler and Jenny Greenteeth are similar water spirits.\n\n\n\n\n\n"}
{"id": "12220457", "url": "https://en.wikipedia.org/wiki?curid=12220457", "title": "Gulf Gateway Deepwater Port", "text": "Gulf Gateway Deepwater Port\n\nGulf Gateway Deepwater Port was the first offshore liquefied natural gas LNG import facility.\n\nGulf Gateway was owned by Excelerate Energy Limited Partnership. It was located in Block 603 of the West Cameron Area, South Addition at a distance of approximately from the Louisiana Coast, Gulf Gateway had a baseload capacity of per day with a peak capacity of per day. Unlike the four LNG terminals which were built in the US before it, Gulf Gateway utilized a special type of LNG Carrier which can vaporize LNG onboard the ship offshore rather than on land.. In addition the vessel, the terminal consisted of a submerged turret loading (STL) buoy system, a new-build piled platform to support a gas custody transfer metering station, and associated pipelines to connect the subsea offloading buoy system to two pipeline grids.\n\nOffshore construction of Gulf Gateway commenced in August 2004 and was completed in February 2005 at a cost of approximately US $70 million. First cargo delivery occurred on March 17, 2005 from the world’s first Energy Bridge Regasification Vehicle (EBRV) ,also known as a floating storage and regasification unit, the \"Excelsior\". \n\nThe Gulf Gateway was in operation in the Gulf of Mexico during Hurricane Katrina and Hurricane Ike and was not affected by either storm. \n\nThe terminal was closed in 2011, due to the shift in the supply-demand balance in the United States from the proliferation of shale gas. The components of the terminal were removed for use at other similar terminals.\n\n"}
{"id": "47551734", "url": "https://en.wikipedia.org/wiki?curid=47551734", "title": "Highest temperature recorded on Earth", "text": "Highest temperature recorded on Earth\n\nAccording to the World Meteorological Organization (WMO), the highest registered air temperature on Earth was in Furnace Creek Ranch, California, located in the Death Valley desert in the United States, on July 10, 1913. The WMO itself admits that \"[a]fter examining the temperature record in detail, [it was] noted that this temperature may be the result of a sandstorm that occurred at the time. Such a storm may have caused superheated surface materials to hit upon the temperature in the shelter\" - what would mean the recording did not refer to air temperature. Weather historians such as Christopher C. Burt also claim that the 1913 Death Valley reading is \"a myth\", and is at least too high. This same conclusion has also been reached by historians Dr. Arnold Court and William Taylor Reid. If the 1913 record were to be decertified, the highest recorded air temperature on Earth would be , recorded both in Death Valley on June 20, 2013, and in Mitribah, Kuwait on July 21, 2016.\n\nThe standard measuring conditions for temperature are in the air, 1.5 meters above the ground, and shielded from direct sunlight. From 1922 until 2012, the WMO record for the highest official temperature on Earth was , registered on September 13, 1922 in ‘Aziziya, Libya. In January 2012, the WMO decertified the 1922 record, citing persuasive evidence that it was a faulty reading recorded in error by an inexperienced observer. Temperatures measured directly on the ground may exceed air temperatures by 30 to 50 °C. The theoretical maximum possible ground surface temperature has been estimated to be between 90 and 100 °C (between 194 and 212 °F) for dry, darkish soils of low thermal conductivity. While there is no highest confirmed ground temperature, a reading of 93.9 °C (201 °F) was allegedly recorded in Furnace Creek Ranch on July 15, 1972.\n\nTemperature measurements via satellite also tend to capture occurrence of higher records but, due to complications involving satellite's altitude loss (a side effect of atmospheric friction), these measurements are often considered less reliable than ground-positioned thermometers. The highest recorded temperature taken by a satellite is , which was measured in the Flaming Mountains of China in 2008. Other satellite measurements of ground temperature taken between 2003 and 2009, taken with the MODIS infrared spectroradiometer on the Aqua satellite, found a maximum temperature of 70.7 °C (159.3 °F), which was recorded in 2005 in the Lut Desert, Iran. The Lut Desert was also found to have the highest maximum temperature in 5 of the 7 years measured (2004, 2005, 2006, 2007 and 2009). These measurements reflect averages over a large region and so are lower than the maximum point surface temperature.\n\nThe following are unverified claims of extreme heat over the current world record of . \n\n"}
{"id": "2431522", "url": "https://en.wikipedia.org/wiki?curid=2431522", "title": "List of North Carolina state forests", "text": "List of North Carolina state forests\n\nThe State of North Carolina has a group of ten protected areas known as State Forests which are managed by the North Carolina Forest Service, an agency of the North Carolina Department of Agriculture and Consumer Services. The seven of the state forests are known as \"State Educational Forests\", and they are primarily used to educate the public about the forest environment, forestry and forest management. One state forest was designated as a \"State Recreational Forest\" in recognition of its high recreational value and use. All the state forests provide some recreational facilities for hiking and picnicking.\n\n\n"}
{"id": "32104858", "url": "https://en.wikipedia.org/wiki?curid=32104858", "title": "List of dicotyledons of Montana", "text": "List of dicotyledons of Montana\n\nThere are at least 2109 species of dicotyledons found in Montana according to the Montana Field Guide. This is a list of Dicotyledoneae orders found in Montana. The Montana Natural Heritage Program has identified a number of dicot species as \"Species of Concern\". Some of these species are exotics (not native to Montana).\n\n\n"}
{"id": "40763097", "url": "https://en.wikipedia.org/wiki?curid=40763097", "title": "List of drying lakes", "text": "List of drying lakes\n\nA number of lakes throughout the world are drying or completely dry due to irrigation or urban use diverting inflow. Climate change is also a factor in drying some lakes.\n"}
{"id": "15562427", "url": "https://en.wikipedia.org/wiki?curid=15562427", "title": "List of ecoregions in Western Sahara", "text": "List of ecoregions in Western Sahara\n\nThe following is a list of ecoregions in Western Sahara, according to the Worldwide Fund for Nature (WWF).\n\n\n\n\n\n"}
{"id": "24694946", "url": "https://en.wikipedia.org/wiki?curid=24694946", "title": "List of invasive species in Australasia", "text": "List of invasive species in Australasia\n\nLists of invasive species in Australasia, the invasive species within the Australasia geographic region and Australasian ecozone of southern Oceania. \n\nIt includes the introduced invasive plant and animal species naturalized within the nations of Australia and New Zealand; the international island of New Guinea, within the nation of Papua New Guinea and in Western New Guinea province of Indonesia; and neighbouring islands in the Pacific Ocean.\n\n\n"}
{"id": "20900220", "url": "https://en.wikipedia.org/wiki?curid=20900220", "title": "List of mountains of Sri Lanka", "text": "List of mountains of Sri Lanka\n\nThe following page lists all mountain peaks of Sri Lanka that are over in elevation.\n"}
{"id": "36817738", "url": "https://en.wikipedia.org/wiki?curid=36817738", "title": "List of plants in the Gibraltar Botanic Gardens", "text": "List of plants in the Gibraltar Botanic Gardens\n\nThis List of plants in the Gibraltar Botanic Gardens is based on data published by the gardens and updated annually. The gardens collection includes nearly 2,000 different species and over half of these are succulents. The gardens are noted for their collection of species from the African Aloe genus. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20941824", "url": "https://en.wikipedia.org/wiki?curid=20941824", "title": "List of rivers of the Rocky Mountains", "text": "List of rivers of the Rocky Mountains\n\nThis is a partial list of rivers of the Rocky Mountains in Canada and the United States. For a full listing of rivers in the Canadian portion of the range, see List of rivers of the Canadian Rockies.\n\n\n\n\n\n\n\n"}
{"id": "48211692", "url": "https://en.wikipedia.org/wiki?curid=48211692", "title": "List of star systems within 30–35 light-years", "text": "List of star systems within 30–35 light-years\n\nThis is a list of star systems within 30-35 light years of Earth.\n\n"}
{"id": "48211837", "url": "https://en.wikipedia.org/wiki?curid=48211837", "title": "List of star systems within 35–40 light-years", "text": "List of star systems within 35–40 light-years\n\nThis is a list of star systems within 35-40 light years of Earth.\n\n"}
{"id": "96450", "url": "https://en.wikipedia.org/wiki?curid=96450", "title": "Mafui'e", "text": "Mafui'e\n\nIn Polynesian mythology (specifically Samoa), Mafui'e is the god of earthquakes.\n\nMafui'e was also the keeper of fire. Ti'iti'i, a demi-god, won the fire from him in a battle; thus, fire was brought to the people of Samoa.\n"}
{"id": "57060297", "url": "https://en.wikipedia.org/wiki?curid=57060297", "title": "Magnetohydrodynamic converter", "text": "Magnetohydrodynamic converter\n\nA magnetohydrodynamic converter (MHD converter) is an electromagnetic machine with no moving parts involving magnetohydrodynamics, the study of the kinetics of electrically conductive fluids (liquid or ionized gas) in the presence of electromagnetic fields. Such converters act on the fluid using the Lorentz force to operate in two possible ways: either as an electric generator called an MHD generator, extracting energy from a fluid in motion; or as an electric motor called an MHD accelerator or magnetohydrodynamic drive, putting a fluid in motion by injecting energy. MHD converters are indeed reversible, like many electromagnetic devices.\n\nMichael Faraday first attempted to test an MHD converter in 1832. MHD converters involving plasmas were highly studied in the 1960s and 1970s, with many government funding and dedicated international conferences. The research almost stopped after it was considered the electrothermal instability would severely limit the efficiency of such converters when intense magnetic fields are used, although solutions may exist.\n<br>\n\nA magnetohydrodynamic generator is an MHD converter that transforms the kinetic energy of an electrically conductive fluid, in motion with respect to a steady magnetic field, into electricity. MHD power generation has been tested extensively in the 1960s with liquid metals and plasmas as working fluids.\n\nBasically, a plasma is hurtling down within a channel whose walls are fitted with electrodes. Electromagnets create a uniform transverse magnetic field within the cavity of the channel. The Lorentz force then acts upon the trajectory of the incoming electrons and positive ions, separating the opposite charge carriers according to their sign. As negative and positive charges are spatially separated within the chamber, an electric potential difference can be retrieved across the electrodes. While work is extracted from the kinetic energy of the incoming high-velocity plasma, the fluid slows down during the process.\n\nA magnetohydrodynamic accelerator is an MHD converter that imparts motion to an electrically conductive fluid initially at rest, using cross electric current and magnetic field both applied within the fluid. MHD propulsion has been mostly tested with models of ships and submarines in seawater. Studies are also ongoing since the early 1960s about aerospace applications of MHD to aircraft propulsion and flow control to enable hypersonic flight: action on the boundary layer to prevent laminar flow to become turbulent, shock wave mitigation or cancellation for thermal control and reduction of the wave drag and form drag, inlet flow control and airflow velocity reduction with an MHD generator section ahead of a scramjet or turbojet to extend their regimes at higher Mach numbers, combined to an MHD accelerator in the exhaust nozzle fed by the MHD generator through a bypass system. Research on various designs are also conducted on electromagnetic plasma propulsion for space exploration.\n\nIn an MHD accelerator, the Lorentz force accelerates all charge carriers in the same direction whatever their sign, as well as neutral atoms and molecules of the fluid through collisions. The fluid is ejected toward the rear and as a reaction, the vehicle accelerates forward.\n\n\n"}
{"id": "175959", "url": "https://en.wikipedia.org/wiki?curid=175959", "title": "Mains electricity", "text": "Mains electricity\n\nMains electricity (as it is known in the UK and Canada; US terms include grid power, wall power, and domestic power) is the general-purpose alternating-current (AC) electric power supply. It is the form of electrical power that is delivered to homes and businesses, and it is the form of electrical power that consumers use when they plug domestic appliances, televisions and electric lamps into wall outlets.\n\nThe two principal properties of the electric power supply, voltage and frequency, differ between regions. A voltage of (nominally) 230 V and a frequency of 50 Hz is used in Europe, most of Africa, most of Asia, much of South America and Australia. In North America, the most common combination is 120 V and a frequency of 60 Hz. Other voltages exist, and some countries may have, for example, 230 V but 60 Hz. This is a concern to travellers, since portable appliances designed for one voltage and frequency combination may not operate with, or may even be destroyed by another. The use of different and incompatible plugs and sockets in different regions and countries provides some protection from accidental use of appliances with incompatible voltage and frequency requirements.\n\nIn the US, mains electric power is referred to by several names including \"household power\", \"household electricity\", \"house current\", \"powerline\", \"domestic power\", \"wall power\", \"line power\", \"AC power\", \"city power\", \"street power\".\n\nIn the UK, mains electric power is generally referred to as \"the mains\".\n\n\"For a list of voltages, frequencies, and wall plugs by country, see Mains electricity by country\"\n\nWorldwide, many different mains power systems are found for the operation of household and light commercial electrical appliances and lighting. The different systems are primarily characterized by their\n\nAll these parameters vary among regions. The voltages are generally in the range 100–240 V (always expressed as root-mean-square voltage). The two commonly used frequencies are 50 Hz and 60 Hz. Single-phase or three-phase power is most commonly used today, although two-phase systems were used early in the 20th century. Foreign enclaves, such as large industrial plants or overseas military bases, may have a different standard voltage or frequency from the surrounding areas. Some city areas may use standards different from that of the surrounding countryside (e.g. in Libya). Regions in an effective state of anarchy may have no central electrical authority, with electric power provided by incompatible private sources.\n\nMany other combinations of voltage and utility frequency were formerly used, with frequencies between 25 Hz and 133 Hz and voltages from 100 V to 250 V. Direct current (DC) has been almost completely displaced by alternating current (AC) in public power systems, but DC was used especially in some city areas to the end of the 20th century. The modern combinations of 230 V/50 Hz and 120 V/60 Hz, listed in IEC 60038, did not apply in the first few decades of the 20th century and are still not universal. Industrial plants with three-phase power will have different, higher voltages installed for large equipment (and different sockets and plugs), but the common voltages listed here would still be found for lighting and portable equipment.\n\nElectricity is used for lighting, heating, cooling, electric motors and electronic equipment. The US Energy Information Administration (EIA) has published:\n\nEstimated US residential electricity consumption by end use, for the year 2016\n\nElectronic appliances (such as those in the televisions, computer and related equipment categories above, representing 9% of the total), typically use an AC to DC converter or AC adapter to power the device. This is often capable of operation over the approximate range of 100 V to 250 V and at 50 Hz to 60 Hz. The other categories are typically AC applications and usually have much more restricted input ranges. A study by the Building Research Establishment in the UK states that \"The existing 230 V system is well suited to the future of electricity whether through design or Darwinian processes. Any current perceived weakness is generally a result of cost reduction and market forces rather than any fundamental technical difficulties. Questions as to whether there are alternatives to the existing 230 V AC system are often overshadowed by legacy issues, the future smart agenda and cost in all but specific situations. Where opportunities do exist they are often for specific parts of the overall load and often small parts in terms of total demand.\"\n\nIn many countries, household power is single-phase electric power, with two or three wired contacts at each outlet. Neutral and line wires carry current and are defined as live parts.\n\n\nIn northern and central Europe, residential electrical supply is commonly 400 V three-phase electric power, which gives 230 V between any single phase and neutral; house wiring may be a mix of three-phase and single-phase circuits, but three-phase residential use is rare in the UK. High-power appliances such as kitchen stoves, water heaters and maybe household power heavy tools like log splitters may be supplied from the 400 V three-phase power supply.\n\nVarious earthing systems are used to ensure that the ground and neutral wires have zero voltage with respect to earth, to prevent shocks when touching grounded electrical equipment. In some installations, there may be two line conductors which carry alternating currents in a single-phase three-wire. Small portable electrical equipment is connected to the power supply through flexible cables (these exist with either two or three insulated conductors) terminated in a plug, which is inserted into a fixed receptacle (socket). Larger household electrical equipment and industrial equipment may be permanently wired to the fixed wiring of the building. For example, in North American homes a window-mounted self-contained air conditioner unit would be connected to a wall plug, whereas the central air conditioning for a whole home would be permanently wired. Larger plug and socket combinations are used for industrial equipment carrying larger currents, higher voltages, or three phase electric power. These are often constructed with tougher plastics and possess inherent weather-resistant properties needed in some applications.\n\nCircuit breakers and fuses are used to detect short circuits between the line and neutral or ground wires or the drawing of more current than the wires are rated to handle (overload protection) to prevent overheating and possible fire. These protective devices are usually mounted in a central panel—most commonly a distribution board or consumer unit—in a building, but some wiring systems also provide a protection device at the socket or within the plug. Residual-current devices, also known as ground-fault circuit interrupters and appliance leakage current interrupters, are used to detect \"ground faults\" - flow of current in other than the neutral and line wires (like the ground wire or a person). When a ground fault is detected, the device quickly cuts off the circuit.\n\nMost of Europe, Africa, Asia, Australia, New Zealand and most of South America use a supply that is within 6% of 230 V. In the UK and Australia the nominal supply voltage is 230 V +10%/−6% to accommodate the fact that most supplies are in fact still 240 V. The United States uses a supply voltage of 120 volts +/- 6%. Japan, Taiwan, Saudi Arabia, North America, Central America and some parts of northern South America use a voltage between 100 V and 127 V. The 230 V standard has become widespread so that 230 V equipment can be used in most parts of the world with the aid of an adapter or a change to the equipment's connection plug for the specific country.\n\nA distinction should be made between the voltage at the point of supply (nominal voltage at the point of interconnection between the electrical utility and the user) and the voltage rating of the equipment (utilization voltage). Typically the utilization voltage is 3% to 5% lower than the nominal system voltage; for example, a nominal 208 V supply system will be connected to motors with \"200 V\" on their nameplates. This allows for the voltage drop between equipment and supply. Voltages in this article are the nominal supply voltages and equipment used on these systems will carry slightly lower nameplate voltages. Power distribution system voltage is nearly sinusoidal in nature. Voltages are expressed as root mean square (RMS) voltage. Voltage tolerances are for steady-state operation. Momentary heavy loads, or switching operations in the power distribution network, may cause short-term deviations out of the tolerance band and storms and other unusual conditions may cause even larger transient variations. In general, power supplies derived from large networks with many sources are more stable than those supplied to an isolated community with perhaps only a single generator.\n\nThe choice of supply voltage is due more to historical reasons than optimization of the electric power distribution system—once a voltage is in use and equipment using this voltage is widespread, changing voltage is a drastic and expensive measure. A 230 V distribution system will use less conductor material than a 120 V system to deliver a given amount of power because the current, and consequently the resistive loss, is lower. While large heating appliances can use smaller conductors at 230 V for the same output rating, few household appliances use anything like the full capacity of the outlet to which they are connected. Minimum wire size for hand-held or portable equipment is usually restricted by the mechanical strength of the conductors. Electrical appliances are used extensively in homes in both 230 V and 120 V system countries. National electrical codes prescribe wiring methods intended to minimize the risk of electric shock and fire.\n\nMany areas, such as the US, which use (nominally) 120 V make use of three-wire, split-phase 240 V systems to supply large appliances. In this system a 240 V supply has a centre-tapped neutral to give two 120 V supplies which can also supply 240 V to loads connected between the two line wires. Three-phase systems can be connected to give various combinations of voltage, suitable for use by different classes of equipment. Where both single-phase and three-phase loads are served by an electrical system, the system may be labelled with both voltages such as 120/208 or 230/400 V, to show the line-to-neutral voltage and the line-to-line voltage. Large loads are connected for the higher voltage. Other three-phase voltages, up to 830 volts, are occasionally used for special-purpose systems such as oil well pumps. Large industrial motors (say, more than 250 hp or 150 kW) may operate on medium voltage. On 60 Hz systems a standard for medium voltage equipment is 2400/4160 V (2300/4000 V in the US) whereas 3300 V is the common standard for 50 Hz systems.\n\nFollowing voltage harmonisation, electricity supplies within the European Union are now nominally 230 V ±10% at 50 Hz. For a transition period (1995–2008), countries that had previously used 220 V changed to a narrower asymmetric tolerance range of 230 V +6%/−10% and those (like the UK) that had previously used 240 V changed to 230 V +10%/−6%. No change in voltage is required by either system as both 220 V and 240 V fall within the lower 230 V tolerance bands (230 V ±6%). Some areas of the UK still have 250 volts for legacy reasons, but these also fall within the 10% tolerance band of 230 volts. In practice, this allows countries to continue to supply the same voltage (220 or 240 V), at least until existing supply transformers are replaced. Equipment (with the exception of filament bulbs) used in these countries is designed to accept any voltage within the specified range. In the United States and Canada, national standards specify that the nominal voltage at the source should be 120 V and allow a range of 114 V to 126 V (RMS) (−5% to +5%). Historically 110 V, 115 V and 117 V have been used at different times and places in North America. Mains power is sometimes spoken of as 110 V; however, 120 V is the nominal voltage.\n\nIn 2000, Australia converted to 230 V as the nominal standard with a tolerance of +10%/−6%, this superseding the old 240 V standard, AS2926-1987. As in the UK, 240 V is within the allowable limits and \"240 volt\" is a synonym for mains in Australian and British English. In Japan, the electrical power supply to households is at 100 V. Eastern and northern parts of Honshū (including Tokyo) and Hokkaidō have a frequency of 50 Hz, whereas western Honshū (including Nagoya, Osaka, and Hiroshima), Shikoku, Kyūshū and Okinawa operate at 60 Hz. The boundary between the two regions contains four back-to-back high-voltage direct-current (HVDC) substations which interconnect the power between the two grid systems; these are Shin Shinano, Sakuma Dam, Minami-Fukumitsu, and the Higashi-Shimizu Frequency Converter. To accommodate the difference, frequency-sensitive appliances marketed in Japan can often be switched between the two frequencies.\n\nThe world's first public electricity supply was a water wheel driven system constructed in the small English town of Godalming in 1881. It was an alternating current (AC) system using a Siemens alternator supplying power for both street lights and consumers at two voltages, 250 V for arc lamps, and 40 V for incandescent lamps.\n\nThe world's first large scale central plant—Thomas Edison’s steam powered station at Holborn Viaduct in London—started operation in January 1882, providing direct current (DC) at 110 V. The Holborn Viaduct station was used as a proof of concept for the construction of the much larger Pearl Street Station in Manhattan, the world's first permanent commercial central power plant. The Pearl Street Station also provided DC at 110 V, considered to be a \"safe\" voltage for consumers, beginning September 4, 1882.\n\nAC systems started appearing in the US in the mid–1880s, using higher distribution voltage stepped down via transformers to the same 110 V customer utilization voltage that Edison used. In 1883 Edison patented a three–wire distribution system to allow DC generation plants to serve a wider radius of customers to save on copper costs. By connecting two groups of 110 V lamps in series more load could be served by the same size conductors run with 220 V between them; a neutral conductor carried any imbalance of current between the two sub-circuits. AC circuits adopted the same form during the War of Currents, allowing lamps to be run at around 110 V and major appliances to be connected to 220 V. Nominal voltages gradually crept upward to 112 V and 115 V, or even 117 V. After World War II the standard voltage in the U.S. became 117 V, but many areas lagged behind even into the 1960s. In 1967 the nominal voltage rose to 120 V, but conversion of appliances was slow. Today, virtually all American homes and businesses have access to 120 and 240 V at 60 Hz. Both voltages are available on the three wires (two \"hot\" legs of opposite phase and one \"neutral\" leg).\n\nIn 1899, the Berliner Elektrizitäts-Werke (BEW), a Berlin electrical utility, decided to greatly increase its distribution capacity by switching to 220 V nominal distribution, taking advantage of the higher voltage capability of newly developed metal filament lamps. The company was able to offset the cost of converting the customer's equipment by the resulting saving in distribution conductors cost. This became the model for electrical distribution in Germany and the rest of Europe and the 220 V system became common. North American practice remained with voltages near 110 V for lamps.\n\nIn the first decade after the introduction of alternating current in the US (from the early 1880s to about 1893) a variety of different frequencies were used, with each electric provider setting their own, so that no single one prevailed. The most common frequency was 133⅓ Hz. The rotation speed of induction generators and motors, the efficiency of transformers, and flickering of carbon arc lamps all played a role in frequency setting. Around 1893 the Westinghouse Electric Company in the United States and AEG in Germany decided to standardize their generation equipment on 60 Hz and 50 Hz respectively, eventually leading to most of the world being supplied at one of these two frequencies. Today most 60 Hz systems deliver nominal 120/240 V, and most 50 Hz nominally 230 V. The significant exceptions are in Brazil, which has a synchronized 60 Hz grid with both 127 V and 220 V as standard voltages in different regions, and Japan, which has two frequencies: 50 Hz for East Japan and 60 Hz for West Japan.\n\nTo maintain the voltage at the customer's service within the acceptable range, electrical distribution utilities use regulating equipment at electrical substations or along the distribution line. At a substation, the step-down transformer will have an automatic on-load tap changer, allowing the ratio between transmission voltage and distribution voltage to be adjusted in steps. For long (several kilometres) rural distribution circuits, automatic voltage regulators may be mounted on poles of the distribution line. These are autotransformers, again, with on-load tapchangers to adjust the ratio depending on the observed voltage changes. At each customer's service, the step-down transformer has up to five taps to allow some range of adjustment, usually ±5% of the nominal voltage. Since these taps are not automatically controlled, they are used only to adjust the long-term average voltage at the service and not to regulate the voltage seen by the utility customer.\n\nThe stability of the voltage and frequency supplied to customers varies among countries and regions. \"Power quality\" is a term describing the degree of deviation from the nominal supply voltage and frequency. Short-term surges and drop-outs affect sensitive electronic equipment such as computers and flat panel displays. Longer-term power outages, brown-outs and black outs and low reliability of supply generally increase costs to customers, who may have to invest in uninterruptible power supply or stand-by generator sets to provide power when the utility supply is unavailable or unusable. Erratic power supply may be a severe economic handicap to businesses and public services which rely on electrical machinery, illumination, climate control and computers. Even the best quality power system may have breakdowns or require servicing. As such, companies, governments and other organizations sometimes have backup generators at sensitive facilities, to ensure that power will be available even in the event of a power outage or black out.\n\nPower quality can also be affected by distortions of the current or voltage waveform in the form of harmonics of the fundamental (supply) frequency, or non-harmonic (inter)modulation distortion such as that caused by RFI or EMI interference. In contrast, harmonic distortion is usually caused by conditions of the load or generator. In multi-phase power, phase shift distortions caused by imbalanced loads can occur.\n\n"}
{"id": "26731341", "url": "https://en.wikipedia.org/wiki?curid=26731341", "title": "Nelson Coast temperate forest", "text": "Nelson Coast temperate forest\n\nThe Nelson Coast temperate forests are an ecoregion in New Zealand.\n\nThese forests are located on the flanks of the Paparoa Range and other mountains at the top of South Island. The area is thickly forested and has high rainfall, especially on the west-facing slopes but less so on the sheltered eastern side, which has golden sand beaches. Natural features of the region include: the limestone Pancake Rocks near the town of Punakaiki on the edge of Paparoa National Park; Farewell Spit at the north of the island, the longest sandspit in New Zealand; the nearby Te Waikoropupu Springs; and the karst areas on the flanks of Mount Owen, New Zealand in Kahurangi National Park.\n\nThere are small areas of northern rata \"(Metrosideros robusta)\", rimu, and miro rainforest hardwoods as well as karaka \"(Corynocarpus laevigatus)\" and the Nikau palm \"(Rhopalostylis sapida)\" near the coast. However most of the area is covered in Southern beech forest containing of all four species of Southern beech; red beech, silver beech and hard beech in the lowlands and mountain beech higher up. In the less fertile rocks areas there are yellow pine \"(Halocarpus biformis)\" and Dracophyllums including the endemic D. townsonii and the mountain neinei (D. traversii). The alpine plants found here including Celmisia dallii occur as these peaks, along with Fiordland at the southern end of the island, were a high-altitude refuge from the effects of the last ice age.\n\nThe forests are home to a number of endemic species including two flightless birds that no longer survive in the lowland areas of the island, these are the western weka and the largest kiwi, the great spotted kiwi. The varied habitats in the region support a mixture of other birds found here include the kea, the New Zealand kaka parrot\" (Nestor meridionalis)\", the New Zealand pigeon or kereru \"(Hemiphaga novaeseelandiae)\" and the New Zealand falcon or karearea \"(Falco novaeseelandiae)\". Farewell Spit in particular is an important site for wading birds and is on a migration route. \nThe area is also rich in invertebrates including almost half of the known species of amber snails \"(Powelliphanta)\".\n\nThe forests are threatened by logging and mining, although it is impossible to get permits for large-scale mining in the Kahurangi. The karst limestone is particularly fragile and even damaged by recreational caving. The wildlife is threatened by introduced species but they are largely intact at higher elevations. Much of the ecoregion is protected within three national parks; the large Kahurangi National Park and Paparoa National Parks on the west coast, and Abel Tasman National Park on the north east coast. The coastline is unspoilt too and some of it, including Farewell Spit, is protected as nature reserves of national parks. Efforts are being made to control invasive introduced species, particularly possums although deer, chamois, hares and goats are also targeted for control. Birds eggs and snails are vulnerable to rats, possums, stoats and wild pigs. Forest fires are always a threat.\n"}
{"id": "68498", "url": "https://en.wikipedia.org/wiki?curid=68498", "title": "Ocean thermal energy conversion", "text": "Ocean thermal energy conversion\n\nOcean thermal energy conversion (OTEC) uses the temperature difference between cooler deep and warmer shallow or surface seawaters to run a heat engine and produce useful work, usually in the form of electricity. OTEC can operate with a very high capacity factor and so can operate in base load mode.\n\nAmong ocean energy sources, OTEC is one of the continuously available renewable energy resources that could contribute to base-load power supply. The resource potential for OTEC is considered to be much larger than for other ocean energy forms [World Energy Council, 2000]. Up to 88,000 TWh/yr of power could be generated from OTEC without affecting the ocean’s thermal structure [Pelc and Fujita, 2002].\n\nSystems may be either closed-cycle or open-cycle. Closed-cycle OTEC uses working fluids that are typically thought of as refrigerants such as ammonia or R-134a. These fluids have low boiling points, and are therefore suitable for powering the system’s generator to generate electricity. The most commonly used heat cycle for OTEC to date is the Rankine cycle, using a low-pressure turbine. Open-cycle engines use vapour from the seawater itself as the working fluid.\n\nOTEC can also supply quantities of cold water as a by-product. This can be used for air conditioning and refrigeration and the nutrient-rich deep ocean water can feed biological technologies. Another by-product is fresh water distilled from the sea.\n\nOTEC theory was first developed in the 1880s and the first bench size demonstration model was constructed in 1926. Currently the world's only operating OTEC plant is in Japan, overseen by Saga University.\n\nAttempts to develop and refine OTEC technology started in the 1880s. In 1881, Jacques Arsene d'Arsonval, a French physicist, proposed tapping the thermal energy of the ocean. D'Arsonval's student, Georges Claude, built the first OTEC plant, in Matanzas, Cuba in 1930. The system generated 22 kW of electricity with a low-pressure turbine. The plant was later destroyed in a storm.\n\nIn 1935, Claude constructed a plant aboard a 10,000-ton cargo vessel moored off the coast of Brazil. Weather and waves destroyed it before it could generate net power. (Net power is the amount of power generated after subtracting power needed to run the system).\n\nIn 1956, French scientists designed a 3 MW plant for Abidjan, Ivory Coast. The plant was never completed, because new finds of large amounts of cheap petroleum made it uneconomical.\n\nIn 1962, J. Hilbert Anderson and James H. Anderson, Jr. focused on increasing component efficiency. They patented their new \"closed cycle\" design in 1967. This design improved upon the original closed-cycle Rankine system, and included this in an outline for a plant that would produce power at lower cost than oil or coal. At the time, however, their research garnered little attention since coal and nuclear were considered the future of energy.\n\nJapan is a major contributor to the development of OTEC technology. Beginning in 1970 the Tokyo Electric Power Company successfully built and deployed a 100 kW closed-cycle OTEC plant on the island of Nauru. The plant became operational on 14 October 1981, producing about 120 kW of electricity; 90 kW was used to power the plant and the remaining electricity was used to power a school and other places. This set a world record for power output from an OTEC system where the power was sent to a real (as opposed to an experimental) power grid. 1981 also saw a major development in OTEC technology when Russian engineer, Dr. Alexander Kalina, used a mixture of ammonia and water to produce electricity. This new ammonia-water mixture greatly improved the efficiency of the power cycle. In 1994 Saga University designed and constructed a 4.5 kW plant for the purpose of testing a newly invented Uehara cycle, also named after its inventor Haruo Uehara. This cycle included\nabsorption and extraction processes that allow this system to outperform the Kalina cycle by 1-2%. Currently, the Institute of Ocean Energy, Saga University, is the leader in OTEC power plant research and also focuses on many of the technology's secondary benefits.\n\nThe 1970s saw an uptick in OTEC research and development during the post 1973 Arab-Israeli War, which caused oil prices to triple. The U.S. federal government poured $260 million into OTEC research after President Carter signed a law that committed the US to a production goal of 10,000 MW of electricity from OTEC systems by 1999.\n\nIn 1974, The U.S. established the Natural Energy Laboratory of Hawaii Authority (NELHA) at Keahole Point on the Kona coast of Hawaii. Hawaii is the best US OTEC location, due to its warm surface water, access to very deep, very cold water, and high electricity costs. The laboratory has become a leading test facility for OTEC technology. In the same year, Lockheed received a grant from the U.S. National Science Foundation to study OTEC. This eventually led to an effort by Lockheed, the US Navy, Makai Ocean Engineering, Dillingham Construction, and other firms to build the world's first and only net-power producing OTEC plant, dubbed \"Mini-OTEC\" For three months in 1979, a small amount of electricity was generated.\n\nResearch related to making open-cycle OTEC a reality began earnestly in 1979 at the Solar Energy Research Institute (SERI) with funding from the US Department of Energy. Evaporators and suitably configured direct-contact condensers were developed and patented by SERI (see ). An original design for a power-producing experiment, then called the 165-kW experiment was described by Kreith and Bharathan (, and ) as the Max Jakob Memorial Award Lecture. The initial design used two parallel axial turbines, using last stage rotors taken from large steam turbines. Later, a team led by Dr. Bharathan at the National Renewable Energy Laboratory (NREL) developed the initial conceptual design for up-dated 210 kW open-cycle OTEC experiment (). This design integrated all components of the cycle, namely, the evaporator, condenser and the turbine into one single vacuum vessel, with the turbine mounted on top to prevent any potential for water to reach it. The vessel was made of concrete as the first process vacuum vessel of its kind. Attempts to make all components using low-cost plastic material could not be fully achieved, as some conservatism was required for the turbine and the vacuum pumps developed as the first of their kind. Later Dr. Bharathan worked with a team of engineers at the Pacific Institute for High Technology Research (PICHTR) to further pursue this design through preliminary and final stages. It was renamed the Net Power Producing Experiment (NPPE) and was constructed at the Natural Energy Laboratory of Hawaii (NELH) by PICHTR by a team led by Chief Engineer Don Evans and the project was managed by Dr. Luis Vega.\n\nIn 2002, India tested a 1 MW floating OTEC pilot plant near Tamil Nadu. The plant was ultimately unsuccessful due to a failure of the deep sea cold water pipe. Its government continues to sponsor research.\n\nIn 2006, Makai Ocean Engineering was awarded a contract from the U.S. Office of Naval Research (ONR) to investigate the potential for OTEC to produce nationally significant quantities of hydrogen in at-sea floating plants located in warm, tropical waters. Realizing the need for larger partners to actually commercialize OTEC, Makai approached Lockheed Martin to renew their previous relationship and determine if the time was ready for OTEC. And so in 2007, Lockheed Martin resumed work in OTEC and became a subcontractor to Makai to support their SBIR, which was followed by other subsequent collaborations\n\nIn March 2011, Ocean Thermal Energy Corporation signed an Energy Services Agreement (ESA) with the Baha Mar resort, Nassau, Bahamas, for the world's first and largest seawater air conditioning (SWAC) system. In June 2015, the project was put on pause while the resort resolved financial and ownership issues. In August 2016, it was announced that the issues had been resolved and that the resort would open in March 2017. It is expected that the SWAC system's construction will resume at that time.\n\nIn July 2011, Makai Ocean Engineering completed the design and construction of an OTEC Heat Exchanger Test Facility at the Natural Energy Laboratory of Hawaii. The purpose of the facility is to arrive at an optimal design for OTEC heat exchangers, increasing performance and useful life while reducing cost (heat exchangers being the #1 cost driver for an OTEC plant). And in March 2013, Makai announced an award to install and operate a 100 kilowatt turbine on the OTEC Heat Exchanger Test Facility, and once again connect OTEC power to the grid.\n\nIn July 2016, the Virgin Islands Public Services Commission approved Ocean Thermal Energy Corporation's application to become a Qualified Facility. The Company is thus permitted to begin negotiations with the Virgin Islands Water and Power Authority (WAPA) for a Power Purchase Agreement (PPA) pertaining to an Ocean Thermal Energy Conversion (OTEC) plant on the island of St. Croix. This would be the world's first commercial OTEC plant.\n\nIn March 2013, Saga University with various Japanese industries completed the installation of a new OTEC plant. Okinawa Prefecture announced the start of the OTEC operation testing at Kume Island on April 15, 2013. The main aim is to prove the validity of computer models and demonstrate OTEC to the public. The testing and research will be conducted with the support of Saga University until the end of FY 2016. IHI Plant Construction Co. Ltd, Yokogawa Electric Corporation, and Xenesys Inc were entrusted with constructing the 100 kilowatt class plant within the grounds of the Okinawa Prefecture Deep Sea Water Research Center. The location was specifically chosen in order to utilize existing deep seawater and surface seawater intake pipes installed for the research center in 2000. The pipe is used for the intake of deep sea water for research, fishery, and agricultural use.[19]\nThe plant consists of two 50 kW units in double Rankine configuration. The OTEC facility and deep seawater research center are open to free public tours by appointment in English and Japanese. Currently, this is one of only two fully operational OTEC plants in the world. This plant operates continuously when specific tests are not underway.\n\nIn 2011, Makai Ocean Engineering completed a heat exchanger test facility at NELHA. Used to test a variety of heat exchange technologies for use in OTEC, Makai has received funding to install a 105 kW turbine. Installation will make this facility the largest operational OTEC facility, though the record for largest power will remain with the Open Cycle plant also developed in Hawaii.\n\nIn July 2014, DCNS group partnered with Akuo Energy announced NER 300 funding for their NEMO project. If successful, the 16MW gross 10MW net offshore plant will be the largest OTEC facility to date. DCNS plans to have NEMO operational by 2020.\n\nAn ocean thermal energy conversion power plant built by Makai Ocean Engineering went operational in Hawaii in August 2015 . The governor of Hawaii, David Ige, \"flipped the switch\" to activate the plant. This is the first true closed-cycle ocean Thermal Energy Conversion (OTEC) plant to be connected to a U.S. electrical grid . It is a demo plant capable of generating 105 kilowatts, enough to power about 120 homes.\n\nA heat engine gives greater efficiency when run with a large temperature difference. In the oceans the temperature difference between surface and deep water is greatest in the tropics, although still a modest 20 to 25 °C. It is therefore in the tropics that OTEC offers the greatest possibilities. OTEC has the potential to offer global amounts of energy that are 10 to 100 times greater than other ocean energy options such as wave power. OTEC plants can operate continuously providing a base load supply for an electrical power generation system.\n\nThe main technical challenge of OTEC is to generate significant amounts of power efficiently from small temperature differences. It is still considered an emerging technology. Early OTEC systems were 1 to 3 percent thermally efficient, well below the theoretical maximum 6 and 7 percent for this temperature difference. Modern designs allow performance approaching the theoretical maximum Carnot efficiency.\n\nCold seawater is an integral part of each of the three types of OTEC systems: closed-cycle, open-cycle, and hybrid. To operate, the cold seawater must be brought to the surface. The primary approaches are active pumping and desalination. Desalinating seawater near the sea floor lowers its density, which causes it to rise to the surface.\n\nThe alternative to costly pipes to bring condensing cold water to the surface is to pump vaporized low boiling point fluid into the depths to be condensed, thus reducing pumping volumes and reducing technical and environmental problems and lowering costs.\n\nClosed-cycle systems use fluid with a low boiling point, such as ammonia (having a boiling point around -33 °C at atmospheric pressure), to power a turbine to generate electricity. Warm surface seawater is pumped through a heat exchanger to vaporize the fluid. The expanding vapor turns the turbo-generator. Cold water, pumped through a second heat exchanger, condenses the vapor into a liquid, which is then recycled through the system.\n\nIn 1979, the Natural Energy Laboratory and several private-sector partners developed the \"mini OTEC\" experiment, which achieved the first successful at-sea production of net electrical power from closed-cycle OTEC. The mini OTEC vessel was moored off the Hawaiian coast and produced enough net electricity to illuminate the ship's light bulbs and run its computers and television.\n\nOpen-cycle OTEC uses warm surface water directly to make electricity. The warm seawater is first pumped into a low-pressure container, which causes it to boil. In some schemes, the expanding vapour drives a low-pressure turbine attached to an electrical generator. The vapour, which has left its salt and other contaminants in the low-pressure container, is pure fresh water. It is condensed into a liquid by exposure to cold temperatures from deep-ocean water. This method produces desalinized fresh water, suitable for drinking water, irrigation or aquaculture.\n\nIn other schemes, the rising vapour is used in a gas lift technique of lifting water to significant heights. Depending on the embodiment, such vapour lift pump techniques generate power from a hydroelectric turbine either before or after the pump is used.\n\nIn 1984, the \"Solar Energy Research Institute\" (now known as the National Renewable Energy Laboratory) developed a vertical-spout evaporator to convert warm seawater into low-pressure steam for open-cycle plants. Conversion efficiencies were as high as 97% for seawater-to-steam conversion (overall steam production would only be a few percent of the incoming water). In May 1993, an open-cycle OTEC plant at Keahole Point, Hawaii, produced close to 80 kW of electricity during a net power-producing experiment. This broke the record of 40 kW set by a Japanese system in 1982.\n\nA hybrid cycle combines the features of the closed- and open-cycle systems. In a hybrid, warm seawater enters a vacuum chamber and is flash-evaporated, similar to the open-cycle evaporation process. The steam vaporizes the ammonia working fluid of a closed-cycle loop on the other side of an ammonia vaporizer. The vaporized fluid then drives a turbine to produce electricity. The steam condenses within the heat exchanger and provides desalinated water (see heat pipe).\n\nA popular choice of working fluid is ammonia, which has superior transport properties, easy availability, and low cost. Ammonia, however, is toxic and flammable. Fluorinated carbons such as CFCs and HCFCs are not toxic or flammable, but they contribute to ozone layer depletion. Hydrocarbons too are good candidates, but they are highly flammable; in addition, this would create competition for use of them directly as fuels. The power plant size is dependent upon the vapor pressure of the working fluid. With increasing vapor pressure, the size of the turbine and heat exchangers decreases while the wall thickness of the pipe and heat exchangers increase to endure high pressure especially on the evaporator side.\n\nOTEC has the potential to produce gigawatts of electrical power, and in conjunction with electrolysis, could produce enough hydrogen to completely replace all projected global fossil fuel consumption. Reducing costs remains an unsolved challenge, however. OTEC plants require a long, large diameter intake pipe, which is submerged a kilometer or more into the ocean's depths, to bring cold water to the surface.\n\nLand-based and near-shore facilities offer three main advantages over those located in deep water. Plants constructed on or near land do not require sophisticated mooring, lengthy power cables, or the more extensive maintenance associated with open-ocean environments. They can be installed in sheltered areas so that they are relatively safe from storms and heavy seas. Electricity, desalinated water, and cold, nutrient-rich seawater could be transmitted from near-shore facilities via trestle bridges or causeways. In addition, land-based or near-shore sites allow plants to operate with related industries such as mariculture or those that require desalinated water.\n\nFavored locations include those with narrow shelves (volcanic islands), steep (15-20 degrees) offshore slopes, and relatively smooth sea floors. These sites minimize the length of the intake pipe. A land-based plant could be built well inland from the shore, offering more protection from storms, or on the beach, where the pipes would be shorter. In either case, easy access for construction and operation helps lower costs.\n\nLand-based or near-shore sites can also support mariculture or chilled water agriculture. Tanks or lagoons built on shore allow workers to monitor and control miniature marine environments. Mariculture products can be delivered to market via standard transport.\n\nOne disadvantage of land-based facilities arises from the turbulent wave action in the surf zone. OTEC discharge pipes should be placed in protective trenches to prevent subjecting them to extreme stress during storms and prolonged periods of heavy seas. Also, the mixed discharge of cold and warm seawater may need to be carried several hundred meters offshore to reach the proper depth before it is released, requiring additional expense in construction and maintenance.\n\nOne way that OTEC systems can avoid some of the problems and expenses of operating in a surf zone is by building them just offshore in waters ranging from 10 to 30 meters deep (Ocean Thermal Corporation 1984). This type of plant would use shorter (and therefore less costly) intake and discharge pipes, which would avoid the dangers of turbulent surf. The plant itself, however, would require protection from the marine environment, such as breakwaters and erosion-resistant foundations, and the plant output would need to be transmitted to shore.\n\nTo avoid the turbulent surf zone as well as to move closer to the cold-water resource, OTEC plants can be mounted to the continental shelf at depths up to . A shelf-mounted plant could be towed to the site and affixed to the sea bottom. This type of construction is already used for offshore oil rigs. The complexities of operating an OTEC plant in deeper water may make them more expensive than land-based approaches. Problems include the stress of open-ocean conditions and more difficult product delivery. Addressing strong ocean currents and large waves adds engineering and construction expense. Platforms require extensive pilings to maintain a stable base. Power delivery can require long underwater cables to reach land. For these reasons, shelf-mounted plants are less attractive. \n\nFloating OTEC facilities operate off-shore. Although potentially optimal for large systems, floating facilities present several difficulties. The difficulty of mooring plants in very deep water complicates power delivery. Cables attached to floating platforms are more susceptible to damage, especially during storms. Cables at depths greater than 1000 meters are difficult to maintain and repair. Riser cables, which connect the sea bed and the plant, need to be constructed to resist entanglement. \n\nAs with shelf-mounted plants, floating plants need a stable base for continuous operation. Major storms and heavy seas can break the vertically suspended cold-water pipe and interrupt warm water intake as well. To help prevent these problems, pipes can be made of flexible polyethylene attached to the bottom of the platform and gimballed with joints or collars. Pipes may need to be uncoupled from the plant to prevent storm damage. As an alternative to a warm-water pipe, surface water can be drawn directly into the platform; however, it is necessary to prevent the intake flow from being damaged or interrupted during violent motions caused by heavy seas. \n\nConnecting a floating plant to power delivery cables requires the plant to remain relatively stationary. Mooring is an acceptable method, but current mooring technology is limited to depths of about . Even at shallower depths, the cost of mooring may be prohibitive.\n\nOTEC projects under consideration include a small plant for the U.S. Navy base on the British overseas territory island of Diego Garcia in the Indian Ocean. Ocean Thermal Energy Corporation (formerly OCEES International, Inc.) is working with the U.S. Navy on a design for a proposed 13-MW OTEC plant, to replace the current diesel generators. The OTEC plant would also provide 1.25 million gallons per day of potable water. This project is currently waiting for changes in US military contract policies. OTE has proposed building a 10-MW OTEC plant on Guam.\n\nOcean Thermal Energy Corporation (OTE) currently has plans to install two 10 MW OTEC plants in the US Virgin Islands and a 5-10 MW OTEC facility in The Bahamas. OTE has also designed the world’s largest Seawater Air Conditioning (SWAC) plant for a resort in The Bahamas, which will use cold deep seawater as a method of air-conditioning. In mid-2015, the 95%-complete project was temporarily put on hold while the resort resolved financial and ownership issues. In August 22, 2016, the government of the Bahamas announced that a new agreement had been signed under which the Baha Mar resort will be completed. On September 27, 2016, Bahamian Prime Minister Perry Christie announced that construction had resumed on Baha Mar, and that the resort was slated to open in March 2017.\n\nOTE expects to have the SWAC plant up and running within two years of Baha Mar's opening.\n\nLockheed Martin's Alternative Energy Development team has partnered with Makai Ocean Engineering\nto complete the final design phase of a 10-MW closed cycle OTEC pilot system which planned to become operational in Hawaii in the 2012-2013 time frame. This system was designed to expand to 100-MW commercial systems in the near future. In November, 2010 the U.S. Naval Facilities Engineering Command (NAVFAC) awarded Lockheed Martin a US$4.4 million contract modification to develop critical system components and designs for the plant, adding to the 2009 $8.1 million contract and two Department of Energy grants totaling over $1 million in 2008 and March 2010.\nA small but operational ocean thermal energy conversion (OTEC) plant was inaugurated in Hawaii in August 2015. The opening of the research and development 100-kilowatt facility marked the first time a closed-cycle OTEC plant was connected to the U.S. grid.\n\nOn April 13, 2013 Lockheed contracted with the Reignwood Group to build a 10 megawatt plant off the coast of southern China to provide power for a planned resort on Hainan island. A plant of that size would power several thousand homes. The Reignwood Group acquired Opus Offshore in 2011 which forms its Reignwood Ocean Engineering division which also is engaged in development of deepwater drilling.\n\nCurrently the only continuously operating OTEC system is located in Okinawa Prefecture, Japan. The Governmental support, local community support, and advanced research carried out by Saga University were key for the contractors, IHI Plant Construction Co. Ltd, Yokogawa Electric Corporation, and Xenesys Inc, to succeed with this project. Work is being conducted to develop a 1MW facility on Kume Island requiring new pipelines. In July 2014, more than 50 members formed the Global Ocean reSource and Energy Association (GOSEA) an international organization formed to promote the development of the Kumejima Model and work towards the installation of larger deep seawater pipelines and a 1MW OTEC Facility. The companies involved in the current OTEC projects, along with other interested parties have developed plans for offshore OTEC systems as well. - For more details, see \"Currently Operating OTEC Plants\" above.\n\nOn March 5, 2014, Ocean Thermal Energy Corporation (OTEC) and the 30th Legislature of the United States Virgin Islands (USVI) signed a Memorandum of Understanding to move forward with a study to evaluate the feasibility and potential benefits to the USVI of installing on-shore Ocean Thermal Energy Conversion (OTEC) renewable energy power plants and Seawater Air Conditioning (SWAC) facilities. The benefits to be assessed in the USVI study include both the baseload (24/7) clean electricity generated by OTEC, as well as the various related products associated with OTEC and SWAC, including abundant fresh drinking water, energy-saving air conditioning, sustainable aquaculture and mariculture, and agricultural enhancement projects for the Islands of St Thomas and St Croix. The Honorable Shawn-Michael Malone, President of the USVI Senate, commented on his signing of the Memorandum of Understanding (MOU) authorizing OTE's feasibility study. \"“The most fundamental duty of government is to protect the health and welfare of its citizens,\" said Senator Malone. \"These clean energy technologies have the potential to improve the air quality and environment for our residents, and to provide the foundation for meaningful economic development. Therefore, it is our duty as elected representatives to explore the feasibility and possible benefits of OTEC and SWAC for the people of USVI.”\"\n\nOn July 18, 2016, OTE's application to be a Qualifying Facility was approved by the Virgin Islands Public Services Commission. OTE also received permission to begin negotiating contracts associated with this project.\n\nSouth Korea's Research Institute of Ships and Ocean Engineering (KRISO) received Approval in Principal from Bureau Veritas for their 1MW offshore OTEC design. No timeline was given for the project which will be located 6 km offshore of the Republic of Kiribati.\n\nAkuo Energy and DCNS were awarded NER300 funding on July 8, 2014 for their NEMO (New Energy for Martinique and Overseas) project which is expected to be a 10.7MW-net offshore facility completed in 2020. The award to help with development totaled 72 million Euro.\n\nOn February 16, 2018, Global OTEC Resources announced plans to build a 150 kW plant in the Maldives, designed bespoke for hotels and resorts. “\"All these resorts draw their power from diesel generators. Moreover, some individual resorts consume 7,000 litres of diesel a day to meet demands which equates to over 6,000 tonnes of CO2 annually\"” said Director, Dan Grech. The EU awarded a grant and Global OTEC resources launched a crowdfunding campaign for the rest.\n\nOTEC has uses other than power production.\n\nDesalinated water can be produced in open- or hybrid-cycle plants using surface condensers to turn evaporated seawater into potable water. System analysis indicates that a 2-megawatt plant could produce about of desalinated water each day. Another system patented by Richard Bailey creates condensate water by regulating deep ocean water flow through surface condensers correlating with fluctuating dew-point temperatures. This condensation system uses no incremental energy and has no moving parts.\n\nOn March 22, 2015, Saga University opened a Flash-type desalination demonstration facility on Kumejima. This satellite of their Institute of Ocean Energy uses post-OTEC deep seawater from the Okinawa OTEC Demonstration Facility and raw surface seawater to produce desalinated water. Air is extracted from the closed system with a vacuum pump. When raw sea water is pumped into the flash chamber it boils, allowing pure steam to rise and the salt and remaining seawater to be removed. The steam is returned to liquid in a heat exchanger with cold post-OTEC deep seawater. The desalinated water can be used in hydrogen production or drinking water (if minerals are added).\n\nThe cold seawater made available by an OTEC system creates an opportunity to provide large amounts of cooling to industries and homes near the plant. The water can be used in chilled-water coils to provide air-conditioning for buildings. It is estimated that a pipe in diameter can deliver 4,700 gallons of water per minute. Water at could provide more than enough air-conditioning for a large building. Operating 8,000 hours per year in lieu of electrical conditioning selling for 5-10¢ per kilowatt-hour, it would save $200,000-$400,000 in energy bills annually.\n\nThe InterContinental Resort and Thalasso-Spa on the island of Bora Bora uses an SWAC system to air-condition its buildings. The system passes seawater through a heat exchanger where it cools freshwater in a closed loop system. This freshwater is then pumped to buildings and directly cools the air.\n\nIn 2010, Copenhagen Energy opened a district cooling plant in Copenhagen, Denmark. The plant delivers cold seawater to commercial and industrial buildings, and has reduced electricity consumption by 80 percent. Ocean Thermal Energy Corporation (OTE) has designed a 9800-ton SDC system for a vacation resort in The Bahamas.\n\nOTEC technology supports chilled-soil agriculture. When cold seawater flows through underground pipes, it chills the surrounding soil. The temperature difference between roots in the cool soil and leaves in the warm air allows plants that evolved in temperate climates to be grown in the subtropics. Dr. John P. Craven, Dr. Jack Davidson and Richard Bailey patented this process and demonstrated it at a research facility at the Natural Energy Laboratory of Hawaii Authority (NELHA). The research facility demonstrated that more than 100 different crops can be grown using this system. Many normally could not survive in Hawaii or at Keahole Point.\n\nJapan has also been researching agricultural uses of Deep Sea Water since 2000 at the Okinawa Deep Sea Water Research Institute on Kume Island. The Kume Island facilities use regular water cooled by Deep Sea Water in a heat exchanger run through pipes in the ground to cool soil. Their techniques have developed an important resource for the island community as they now produce spinach, a winter vegetable, commercially year round. An expansion of the deep seawater agriculture facility was completed by Kumejima Town next to the OTEC Demonstration Facility in 2014. The new facility is for researching the economic practicality of chilled-soil agriculture on a larger scale.\n\nAquaculture is the best-known byproduct, because it reduces the financial and energy costs of pumping large volumes of water from the deep ocean. Deep ocean water contains high concentrations of essential nutrients that are depleted in surface waters due to biological consumption. This \"artificial upwelling\" mimics the natural upwellings that are responsible for fertilizing and supporting the world's largest marine ecosystems, and the largest densities of life on the planet.\n\nCold-water delicacies, such as salmon and lobster, thrive in this nutrient-rich, deep, seawater. Microalgae such as \"Spirulina\", a health food supplement, also can be cultivated. Deep-ocean water can be combined with surface water to deliver water at an optimal temperature.\n\nNon-native species such as salmon, lobster, abalone, trout, oysters, and clams can be raised in pools supplied by OTEC-pumped water. This extends the variety of fresh seafood products available for nearby markets. Such low-cost refrigeration can be used to maintain the quality of harvested fish, which deteriorate quickly in warm tropical regions. In Kona, Hawaii, aquaculture companies working with NELHA generate about $40 million annually, a significant portion of Hawaii’s GDP.\n\nThe NELHA plant established in 1993 produced an average of 7,000 gallons of freshwater per day. KOYO USA was established in 2002 to capitalize on this new economic opportunity. KOYO bottles the water produced by the NELHA plant in Hawaii. With the capacity to produce one million bottles of water every day, KOYO is now Hawaii’s biggest exporter with $140 million in sales.\n\nHydrogen can be produced via electrolysis using OTEC electricity. Generated steam with electrolyte compounds added to improve efficiency is a relatively pure medium for hydrogen production. OTEC can be scaled to generate large quantities of hydrogen. The main challenge is cost relative to other energy sources and fuels.\n\nThe ocean contains 57 trace elements in salts and other forms and dissolved in solution. In the past, most economic analyses concluded that mining the ocean for trace elements would be unprofitable, in part because of the energy required to pump the water. Mining generally targets minerals that occur in high concentrations, and can be extracted easily, such as magnesium. With OTEC plants supplying water, the only cost is for extraction.\nThe Japanese investigated the possibility of extracting uranium and found developments in other technologies (especially materials sciences) were improving the prospects.\n\nBecause OTEC facilities are more-or-less stationary surface platforms, their exact location and legal status may be affected by the United Nations Convention on the Law of the Sea treaty (UNCLOS). This treaty grants coastal nations 12- and zones of varying legal authority from land, creating potential conflicts and regulatory barriers. OTEC plants and similar structures would be considered artificial islands under the treaty, giving them no independent legal status. OTEC plants could be perceived as either a threat or potential partner to fisheries or to seabed mining operations controlled by the International Seabed Authority.\n\nFor OTEC to be viable as a power source, the technology must have tax and subsidy treatment similar to competing energy sources. Because OTEC systems have not yet been widely deployed, cost estimates are uncertain. One study estimates power generation costs as low as US $0.07 per kilowatt-hour, compared with $0.05 - $0.07 for subsidized wind systems.\n\nBeneficial factors that should be taken into account include OTEC's lack of waste products and fuel consumption, the area in which it is available, (often within 20° of the equator) the geopolitical effects of petroleum dependence, compatibility with alternate forms of ocean power such as wave energy, tidal energy and methane hydrates, and supplemental uses for the seawater.\n\nA rigorous treatment of OTEC reveals that a 20 °C temperature difference will provide as much energy as a hydroelectric plant with 34 m head for the same volume of water flow.\nThe low temperature difference means that water volumes must be very large to extract useful amounts of heat. A 100MW power plant would be expected to pump on the order of 12 million gallons (44,400 tonnes) per minute. For comparison, pumps must move a mass of water greater than the weight of the \"battleship Bismarck\", which weighed 41,700 tonnes, every minute. This makes pumping a substantial parasitic drain on energy production in OTEC systems, with one Lockheed design consuming 19.55 MW in pumping costs for every 49.8 MW net electricity generated. For OTEC schemes using heat exchangers, to handle this volume of water the exchangers need to be enormous compared to those used in conventional thermal power generation plants, making them one of the most critical components due to their impact on overall efficiency. A 100 MW OTEC power plant would require 200 exchangers each larger than a 20-foot shipping container making them the single most expensive component.\n\nThe total insolation received by the oceans (covering 70% of the earth's surface, with clearness index of 0.5 and average energy retention of 15%) is: \n\nWe can use Beer–Lambert–Bouguer's law to quantify the solar energy absorption by water,\n\nwhere, \"y\" is the depth of water, \"I\" is intensity and \"μ\" is the absorption coefficient.\nSolving the above differential equation,\n\nThe absorption coefficient \"μ\" may range from 0.05 m for very clear fresh water to 0.5 m for very salty water.\n\nSince the intensity falls exponentially with depth \"y\", heat absorption is concentrated at the top layers. Typically in the tropics, surface temperature values are in excess of , while at , the temperature is about . The warmer (and hence lighter) waters at the surface means there are no thermal convection currents. Due to the small temperature gradients, heat transfer by conduction is too low to equalize the temperatures. The ocean is thus both a practically infinite heat source and a practically infinite heat sink.\n\nThis temperature difference varies with latitude and season, with the maximum in tropical, subtropical and equatorial waters. Hence the tropics are generally the best OTEC locations.\n\nIn this scheme, warm surface water at around enters an evaporator at pressure slightly below the saturation pressures causing it to vaporize.\n\nWhere \"H\" is enthalpy of liquid water at the inlet temperature, \"T\".\n\nThis temporarily superheated water undergoes volume boiling as opposed to pool boiling in conventional boilers where the heating surface is in contact. Thus the water partially flashes to steam with two-phase equilibrium prevailing. Suppose that the pressure inside the evaporator is maintained at the saturation pressure, \"T\".\n\nHere, \"x\" is the fraction of water by mass that vaporizes. The warm water mass flow rate per unit turbine mass flow rate is 1/\"x\".\n\nThe low pressure in the evaporator is maintained by a vacuum pump that also removes the dissolved non-condensable gases from the evaporator. The evaporator now contains a mixture of water and steam of very low vapor quality (steam content). The steam is separated from the water as saturated vapor. The remaining water is saturated and is discharged to the ocean in the open cycle. The steam is a low pressure/high specific volume working fluid. It expands in a special low pressure turbine.\n\nHere, \"H\" corresponds to \"T\". For an ideal isentropic (reversible adiabatic) turbine,\n\nThe above equation corresponds to the temperature at the exhaust of the turbine, \"T\". \"x\" is the mass fraction of vapor at state 5.\n\nThe enthalpy at \"T\" is,\n\nThis enthalpy is lower. The adiabatic reversible turbine work = \"H\"-\"H\".\n\nActual turbine work \n\nThe condenser temperature and pressure are lower. Since the turbine exhaust is to be discharged back into the ocean, a direct contact condenser is used to mix the exhaust with cold water, which results in a near-saturated water. That water is now discharged back to the ocean.\n\n\"H\"=\"H\", at \"T\". \"T\" is the temperature of the exhaust mixed with cold sea water, as the vapour content now is negligible,\n\nThe temperature differences between stages include that between warm surface water and working steam, that between exhaust steam and cooling water, and that between cooling water reaching the condenser and deep water. These represent external irreversibilities that reduce the overall temperature difference.\n\nThe cold water flow rate per unit turbine mass flow rate,\n\nTurbine mass flow rate, formula_11\n\nWarm water mass flow rate, formula_12\n\nCold water mass flow rate formula_13\n\na Developed starting in the 1960s by J. Hilbert Anderson of Sea Solar Power, Inc. In this cycle, \"Q\" is the heat transferred in the evaporator from the warm sea water to the working fluid. The working fluid exits the evaporator as a gas near its dew point.\n\nThe high-pressure, high-temperature gas then is expanded in the turbine to yield turbine work, \"W\". The working fluid is slightly superheated at the turbine exit and the turbine typically has an efficiency of 90% based on reversible, adiabatic expansion.\n\nFrom the turbine exit, the working fluid enters the condenser where it rejects heat, \"-Q\", to the cold sea water. The condensate is then compressed to the highest pressure in the cycle, requiring condensate pump work, \"W\". Thus, the Anderson closed cycle is a Rankine-type cycle similar to the conventional power plant steam cycle except that in the Anderson cycle the working fluid is never superheated more than a few degrees Fahrenheit. Owing to viscous effects, working fluid pressure drops in both the evaporator and the condenser. This pressure drop, which depends on the types of heat exchangers used, must be considered in final design calculations but is ignored here to simplify the analysis. Thus, the parasitic condensate pump work, \"W\", computed here will be lower than if the heat exchanger pressure drop was included. The major additional parasitic energy requirements in the OTEC plant are the cold water pump work, \"W\", and the warm water pump work, \"W\". Denoting all other parasitic energy requirements by \"W\", the net work from the OTEC plant, \"W\" is\n\nThe thermodynamic cycle undergone by the working fluid can be analyzed without detailed consideration of the parasitic energy requirements. From the first law of thermodynamics, the energy balance for the working fluid as the system is\n\nwhere is the net work for the thermodynamic cycle. For the idealized case in which there is no working fluid pressure drop in the heat exchangers,\n\nand\n\nso that the net thermodynamic cycle work becomes\n\nSubcooled liquid enters the evaporator. Due to the heat exchange with warm sea water, evaporation takes place and usually superheated vapor leaves the evaporator. This vapor drives the turbine and the 2-phase mixture enters the condenser. Usually, the subcooled liquid leaves the condenser and finally, this liquid is pumped to the evaporator completing a cycle.\n\nCarbon dioxide dissolved in deep cold and high pressure layers is brought up to the surface and released as the water warms. \n\nMixing of deep ocean water with shallower water brings up nutrients and makes them available to shallow water life. This may be an advantage for aquaculture of commercially important species, but may also unbalance the ecological system around the power plant. \n\nOTEC plants use very large flows of warm surface seawater and cold deep seawater to generate constant renewable power. The deep seawater is oxygen deficient and generally 20-40 times more nutrient rich (in nitrate and nitrite) than shallow seawater. When these plumes are mixed, they are slightly denser than the ambient seawater. Though no large scale physical environmental testing of OTEC has been done, computer models have been developed to simulate the effect of OTEC plants.\n\nIn 2010, a computer model was developed to simulate the physical oceanographic effects of one or several 100 megawatt OTEC plant(s). The model suggests that OTEC plants can be configured such that the plant can conduct continuous operations, with resulting temperature and nutrient variations that are within naturally occurring levels. Studies to date suggest that by discharging the OTEC flows downwards at a depth below 70 meters, the dilution is adequate and nutrient enrichment is small enough so that 100-megawatt OTEC plants could be operated in a sustainable manner on a continuous basis.\n\nThe nutrients from an OTEC discharge could potentially cause increased biological activity if they accumulate in large quantities in the photic zone. In 2011 a biological component was added to the hydrodynamic computer model to simulate the biological response to plumes from 100 megawatt OTEC plants. In all cases modeled (discharge at 70 meters depth or more), no unnatural variations occurs in the upper 40 meters of the ocean's surface. The picoplankton response in the 110 - 70 meter depth layer is approximately a 10-25% increase, which is well within naturally occurring variability. The nanoplankton response is negligible. The enhanced productivity of diatoms (microplankton) is small. The subtle phytoplankton increase of the baseline OTEC plant suggests that higher-order biochemical effects will be very small.\n\nA previous Final Environmental Impact Statement (EIS) for the United States' NOAA from 1981 is available, but needs to be brought up to current oceanographic and engineering standards. Studies have been done to propose the best environmental baseline monitoring practices, focusing on a set of ten chemical oceanographic parameters relevant to OTEC. Most recently, NOAA held an OTEC Workshop in 2010 and 2012 seeking to assess the physical, chemical, and biological impacts and risks, and identify information gaps or needs.\n\nThe Tethys database provides access to scientific literature and general information on the potential environmental effects of OTEC.\n\nThe performance of direct contact heat exchangers operating at typical OTEC boundary conditions is important to the Claude cycle. Many early Claude cycle designs used a surface condenser since their performance was well understood. However, direct contact condensers offer significant disadvantages. As cold water rises in the intake pipe, the pressure decreases to the point where gas begins to evolve. If a significant amount of gas comes out of solution, placing a gas trap before the direct contact heat exchangers may be justified. Experiments simulating conditions in the warm water intake pipe indicated about 30% of the dissolved gas evolves in the top of the tube. The trade-off between pre-dearation of the seawater and expulsion of non-condensable gases from the condenser is dependent on the gas evolution dynamics, deaerator efficiency, head loss, vent compressor efficiency and parasitic power. Experimental results indicate vertical spout condensers perform some 30% better than falling jet types.\n\nBecause raw seawater must pass through the heat exchanger, care must be taken to maintain good thermal conductivity. Biofouling layers as thin as can degrade heat exchanger performance by as much as 50%. A\\1977 study in which mock heat exchangers were exposed to seawater for ten weeks concluded that although the level of microbial fouling was low, the thermal conductivity of the system was significantly impaired. The apparent discrepancy between the level of fouling and the heat transfer impairment is the result of a thin layer of water trapped by the microbial growth on the surface of the heat exchanger.\n\nAnother study concluded that fouling degrades performance over time, and determined that although regular brushing was able to remove most of the microbial layer, over time a tougher layer formed that could not be removed through simple brushing. The study passed sponge rubber balls through the system. It concluded that although the ball treatment decreased the fouling rate it was not enough to completely halt growth and brushing was occasionally necessary to restore capacity. The microbes regrew more quickly later in the experiment (i.e. brushing became necessary more often) replicating the results of a previous study. The increased growth rate after subsequent cleanings appears to result from selection pressure on the microbial colony.\n\nContinuous use of 1 hour per day and intermittent periods of free fouling and then chlorination periods (again 1 hour per day) were studied. Chlorination slowed but did not stop microbial growth; however chlorination levels of .1 mg per liter for 1 hour per day may prove effective for long term operation of a plant. The study concluded that although microbial fouling was an issue for the warm surface water heat exchanger, the cold water heat exchanger suffered little or no biofouling and only minimal inorganic fouling.\n\nBesides water temperature, microbial fouling also depends on nutrient levels, with growth occurring faster in nutrient rich water. The fouling rate also depends on the material used to construct the heat exchanger. Aluminium tubing slows the growth of microbial life, although the oxide layer which forms on the inside of the pipes complicates cleaning and leads to larger efficiency losses. In contrast, titanium tubing allows biofouling to occur faster but cleaning is more effective than with aluminium.\n\nThe evaporator, turbine, and condenser operate in partial vacuum ranging from 3% to 1% of atmospheric pressure. The system must be carefully sealed to prevent in-leakage of atmospheric air that can degrade or shut down operation. In closed-cycle OTEC, the specific volume of low-pressure steam is very large compared to that of the pressurized working fluid. Components must have large flow areas to ensure steam velocities do not attain excessively high values.\n\nAn approach for reducing the exhaust compressor parasitic power loss is as follows. After most of the steam has been condensed by spout condensers, the non-condensible gas steam mixture is passed through a counter current region which increases the gas-steam reaction by a factor of five. The result is an 80% reduction in the exhaust pumping power requirements.\n\nIn winter in coastal Arctic locations, the delta T between the seawater and ambient air can be as high as 40 °C (72 °F). Closed-cycle systems could exploit the air-water temperature difference. Eliminating seawater extraction pipes might make a system based on this concept less expensive than OTEC. This technology is due to H. Barjot, who suggested butane as cryogen, because of its boiling point of and its non-solubility in water. Assuming a level of efficiency of realistic 4%, calculations show that the amount of energy generated with one cubic meter water at a temperature of in a place with an air temperature of equals the amount of energy generated by letting this cubic meter water run through a hydroelectric plant of 4000 feet (1,200 m) height.\n\nBarjot Polar Power Plants could be located on islands in the polar region or designed as swimming barges or platforms attached to the ice cap. The weather station Myggbuka at Greenlands east coast for example, which is only 2,100 km away from Glasgow, detects monthly mean temperatures below during 6 winter months in the year.\n\nIn 1979 SERI proposed using the Seebeck effect to produce power with a total conversion efficiency of 2%\n\nIn 2014 Liping Liu, Associate Professor at Rutgers University, envisioned an OTEC system that utilises the solid state thermoelectric effect rather than the fluid cycles traditionally used.\n\n\n\n"}
{"id": "6831958", "url": "https://en.wikipedia.org/wiki?curid=6831958", "title": "Patrick J. Lynch", "text": "Patrick J. Lynch\n\nPatrick James Lynch (born November 14, 1953 in Monterey, California) is an American author, artist, biomedical illustrator, and photographer. He lives in North Haven, Connecticut with his wife Susan E. Grajek. He has one daughter, Kathryn, and a son-in-law, Zubin.\n\nHe has written two editions of the \"Web Style Guide\" for Yale University Press with his co-author Sarah Horton of Dartmouth College. The \"Web Style Guide\" has been published in eleven languages, and was named one of Amazon's top 10 Web books in 1999. \n\nLynch also writes and illustrates field guides with his friend and co-author Noble S. Proctor, a renowned American birder, naturalist, and emeritus professor at Southern Connecticut State University. Their \"Field Guide to North Atlantic Wildlife\" was published by Yale University Press in 2005. Lynch's latest field guide is \"A Field Guide to Long Island Sound\" with color illustrations, published in 2017.\n\n"}
{"id": "19570879", "url": "https://en.wikipedia.org/wiki?curid=19570879", "title": "Piston effect", "text": "Piston effect\n\nPiston effect refers to the forced-air flow inside a tunnel or shaft caused by moving vehicles. It is one of numerous phenomena that engineers and designers must consider when developing a range of structures.\n\nIn open air, when a vehicle travels along, air pushed aside can move in any direction except into the ground. Inside a tunnel, air is confined by the tunnel walls to move along the tunnel. Behind the moving vehicle, as air has been pushed away, suction is created, and air is pulled to flow into the tunnel. In addition, because of fluid viscosity, the surface of the vehicle drags the air to flow with vehicle, a force experienced as skin drag by the vehicle. This movement of air by the vehicle is analogous to the operation of a mechanical piston as inside a reciprocating compressor gas pump, hence the name 'piston effect.' The effect is also similar to the pressure fluctuations inside drainage pipes as waste water pushes air in front of it.\nThe piston effect is very pronounced in railway tunnels, because the cross sectional area of trains is large and in many cases almost completely fills the tunnel cross section. The wind felt by the passengers on underground railway platforms (that do not have platform screen doors installed) when a train is approaching is air flow from the piston effect. The effect is less pronounced in road vehicle tunnels, as the cross-sectional area of vehicle is small compared to the total cross-sectional area of the tunnel. Single track tunnels experience the maximum effect but clearance between rolling stock and the tunnel as well as the shape of the front of the train affect its strength.\n\nAir flow caused by the piston effect can exert large forces on the installations inside the tunnel and so these installations have to be carefully designed and installed properly. Non-return dampers are sometimes needed to prevent stalling of ventilation fans caused by this air flow.\n\nThe piston effect has to be considered by building designers in relation to smoke movement within an elevator shaft. A moving elevator car forces the air in front of it out of the shaft and pulls air into the shaft behind it with the effect most apparent in elevator systems with a fast moving car in a single shaft. This means that in a fire a moving elevator may push smoke into lower floors.\n\nThe piston effect is used in tunnel ventilation. In railway tunnels, the train pushes out the air in front of it toward the closest ventilation shaft in front, and sucks air into the tunnel from the closest ventilation shaft behind it. The piston effect can also assist ventilation in road vehicle tunnels.\n\nIn underground rapid transit systems, the piston effect contributes to ventilation and in some cases provides enough air movement to make mechanical ventilation unnecessary. At wider stations with multiple tracks, air quality remains the same and can even improve when mechanical ventilation is disabled. At narrow platforms with a single tunnel, however, air quality worsens when relying on the piston effect alone for ventilation. This still allows for potential energy savings by taking advantage of the piston effect rather than mechanical ventilation where possible.\n\nTunnel boom is a loud boom sometimes generated by high-speed trains when they exit tunnels. These shock waves can disturb nearby residents and damage trains and tunnel structures. The perception of this sound by humans is similar to that of a sonic boom from supersonic aircraft, but unlike a sonic boom, tunnel boom is not caused by trains exceeding the speed of sound. Instead, tunnel boom results from the structure of the tunnel preventing the air around the train from escaping in all directions. As a train passes through a tunnel, it generates compression waves in front of it. These waves coalesce into a shock wave that generates a loud boom when it reaches the tunnel exit. The strength of this wave is proportional to the cube of the train's speed, so the effect is much more pronounced with faster trains.\n\nTunnel boom can disturb residents near the mouth of the tunnel, and it is exacerbated in mountain valleys where the sound can echo. Reducing these disturbances is a significant challenge for high-speed lines such as Japan's Shinkansen and the French TGV. Tunnel boom has become a principal limitation to increased train speeds in Japan where the mountainous terrain requires frequent tunnels. Japan has created a law limiting noise to 70 dB in residential areas, which applies to many tunnel exit zones.\n\nMethods of reducing the phenomenon include making the train's profile highly aerodynamic, adding hoods to tunnel entrances, installing perforated walls at tunnel exits, and drilling vent holes in the tunnel. (This is somewhat like fitting a silencer on a firearm, but on a far bigger scale.)\n\n\n\n"}
{"id": "44596", "url": "https://en.wikipedia.org/wiki?curid=44596", "title": "Positronium", "text": "Positronium\n\nPositronium (Ps) is a system consisting of an electron and its anti-particle, a positron, bound together into an exotic atom, specifically an onium. The system is unstable: the two particles annihilate each other to predominantly produce two or three gamma-rays, depending on the relative spin states. The orbit and energy levels of the two particles are similar to that of the hydrogen atom (which is a bound state of a proton and an electron). However, because of the reduced mass, the frequencies of the spectral lines are less than half of the corresponding hydrogen lines.\n\nThe mass of positronium is 1.022 MeV, which is twice the electron mass minus the binding energy of a few eV. The ground state of positronium, like that of hydrogen, has two possible configurations depending on the relative orientations of the spins of the electron and the positron.\n\nThe \"singlet\" state, , with antiparallel spins (\"S\" = 0, \"M\" = 0) is known as \"para\"-positronium (\"p\"-Ps). It has a mean lifetime of 0.125 ns and decays preferentially into two gamma rays with energy of each (in the center-of-mass frame). By detecting these photons the position at which the decay occurred can be determined. This process is used in positron-emission tomography. \"Para\"-positronium can decay into any even number of photons (2, 4, 6, ...), but the probability quickly decreases with the number: the branching ratio for decay into 4 photons is .\n\n\"Para-\"positronium lifetime in vacuum is approximately\n\nThe \"triplet\" state, S, with parallel spins (\"S\" = 1, \"M\" = −1, 0, 1) is known as \"ortho\"-positronium (\"o\"-Ps). It has a mean lifetime of , and the leading decay is three gammas. Other modes of decay are negligible; for instance, the five-photons mode has branching ratio of ≈.\n\n\"Ortho\"-positronium lifetime in vacuum can be calculated approximately as:\n\nHowever more accurate calculations with corrections to order O(α²) yield a value of 7.040 μs for the decay rate, corresponding to a lifetime of .\n\nPositronium in the 2S state is metastable having a lifetime of against annihilation. The positronium created in such an excited state will quickly cascade down to the ground state, where annihilation will occur more quickly.\n\nMeasurements of these lifetimes and energy levels have been used in precision tests of quantum electrodynamics, confirming quantum electrodynamics (QED) predictions to high precision. Annihilation can proceed via a number of channels, each producing gamma rays with total energy of (sum of the electron and positron mass-energy), usually 2 or 3, with up to 5 recorded.\n\nThe annihilation into a neutrino–antineutrino pair is also possible, but the probability is predicted to be negligible. The branching ratio for \"o\"-Ps decay for this channel is (electron neutrino–antineutrino pair) and (for other flavour) in predictions based on the Standard Model, but it can be increased by non-standard neutrino properties, like relatively high magnetic moment. The experimental upper limits on branching ratio for this decay (as well as for a decay into any \"invisible\" particles) are < for \"p\"-Ps and < for \"o\"-Ps.\n\nWhile precise calculation of positronium energy levels uses the Bethe–Salpeter equation or the Breit equation, the similarity between positronium and hydrogen allows a rough estimate. In this approximation, the energy levels are different because of a different effective mass, \"m\"*, in the energy equation (see electron energy levels for a derivation):\n\nwhere:\n\nThus, for positronium, its reduced mass only differs from the electron by a factor of 2. This causes the energy levels to also roughly be half of what they are for the hydrogen atom.\n\nSo finally, the energy levels of positronium are given by\n\nThe lowest energy level of positronium () is −6.8 electronvolts (eV). The next level is . The negative sign implies a bound state. Positronium can also be considered by a particular form of the two-body Dirac equation; Two point particles with a Coulomb interaction can be exactly separated in the (relativistic) center-of-momentum frame and the resulting ground-state energy has been obtained very accurately using finite element methods of J. Shertzer. The Dirac equation whose Hamiltonian comprises two Dirac particles and a static Coulomb potential is not relativistically invariant. But if one adds the (or , where is the fine-structure constant) terms, where , then the result is relativistically invariant. Only the leading term is included. The contribution is the Breit term; workers rarely go to because at one has the Lamb shift, which requires quantum electrodynamics.\n\nStjepan Mohorovičić predicted the existence of positronium in a 1934 article published in \"Astronomische Nachrichten\", in which he called it the \"electrum\". Other sources credit Carl Anderson as having predicted its existence in 1932 while at Caltech. It was experimentally discovered by Martin Deutsch at MIT in 1951 and became known as positronium. Many subsequent experiments have precisely measured its properties and verified predictions of QED. There was a discrepancy known as the ortho-positronium lifetime puzzle that persisted for some time, but was eventually resolved with further calculations and measurements. Measurements were in error because of the lifetime measurement of unthermalised positronium, which was only produced at a small rate. This had yielded lifetimes that were too long. Also calculations using relativistic QED are difficult to perform, so they had been done to only the first order. Corrections that involved higher orders were then calculated in a non-relativistic QED.\n\nMolecular bonding was predicted for positronium. Molecules of positronium hydride (PsH) can be made. Positronium can also form a cyanide and can form bonds with halogens or lithium.\n\nThe first observation of di-positronium molecules—molecules consisting of two positronium atoms—was reported on 12 September 2007 by David Cassidy and Allen Mills from University of California, Riverside.\n\nPositronium in high energy states has been predicted to be the dominant form of atomic matter in the universe in the far future, if proton decay is a reality.\n\n\n"}
{"id": "39799214", "url": "https://en.wikipedia.org/wiki?curid=39799214", "title": "Sadler effect", "text": "Sadler effect\n\nThe Sadler effect describes variation in apparent sediment accumulation rates and bed thicknesses back through time inherent to the geological sedimentary record. Peter Sadler analysed what structure you would expect in a stratigraphic section under the hypothesis that bigger geological events – episodes of deposition, erosion, and the gaps between those events – are rarer. He showed that under these conditions it is inevitable that, on average, thinner stratigraphic sections, which cover shorter amounts of time, record faster accumulation rates than thicker sections, which record longer amounts of time.\n\nThe effect equivalently states that more ancient packages of sediment in the stratigraphic record will record slower sedimentation rates stretched over longer periods of time. For instance, it explains the fact that in general, the more ancient geological periods of the Phanerozoic are longer than the more recent ones; i.e., the periods of the Palaeozoic are much longer than those in the Cenozoic. Conversely, it also explains that the maximum sediment accumulation rates seen in the Cambrian at the start of the Phanerozoic are almost two orders of magnitude lower than those observed in the Quaternary, at its end.\n\nThe Sadler effect provides a powerful framework for understanding how information extracted from any given stratigraphic section differs from what should be expected under constant conditions – that is, it provides a null hypothesis for analysing stratigraphy. It also provides techniques to estimate the completeness of a given stratigraphic section on a given timescale. Sections are less complete at shorter timescales, which means that at sufficiently short timescales and for some purposes, some sedimentary successions may contain essentially no useful information.\n\nFor example, the Sadler effect has since been used to investigate whether apparent increases in global sedimentation rates across the last 5 Ma are real; how we might read the record of sediments deposited on continental margins; to interpret fluvial processes such as river avulsion; and to understand what information, and which processes at what timescales, can be preserved in sediments.\n\n"}
{"id": "34650191", "url": "https://en.wikipedia.org/wiki?curid=34650191", "title": "Slowest animals", "text": "Slowest animals\n\n"}
{"id": "168927", "url": "https://en.wikipedia.org/wiki?curid=168927", "title": "Somatic cell nuclear transfer", "text": "Somatic cell nuclear transfer\n\nIn genetics and developmental biology, somatic cell nuclear transfer (SCNT) is a laboratory strategy for creating a viable embryo from a body cell and an egg cell. The technique consists of taking an enucleated oocyte (egg cell) and implanting a donor nucleus from a somatic (body) cell. It is used in both therapeutic and reproductive cloning. Dolly the Sheep became famous for being the first successful case of the reproductive cloning of a mammal. In January 2018, a team of scientists in Shanghai announced the successful cloning of two female crab-eating macaques (named Zhong Zhong and Hua Hua) from fetal nuclei. \"Therapeutic cloning\" refers to the potential use of SCNT in regenerative medicine; this approach has been championed as an answer to the many issues concerning embryonic stem cells (ESC) and the destruction of viable embryos for medical use, though questions remain on how homologous the two cell types truly are.\n\nSomatic cell nuclear transfer is a technique for cloning in which the nucleus of a somatic cell is transferred to the cytoplasm of an enucleated egg. When this is done, the cytoplasmic factors affect the nucleus to become a zygote. The blastocyst stage is developed by the egg which helps to create embryonic stem cells from the inner cell mass of the blastocyst. The first animal that was developed by this technique was Dolly, the sheep, in 1996.\n\nThe process of somatic cell nuclear transplant involves two different cells. The first being a female gamete, known as the ovum (egg/oocyte). In human SCNT (Somatic Cell Nuclear Transfer) experiments, these eggs are obtained through consenting donors, utilizing ovarian stimulation. The second being a somatic cell, referring to the cells of the human body. Skin cells, fat cells, and liver cells are only a few examples. The nucleus of the donor egg cell is removed and discarded, leaving it 'deprogrammed.' What is left is a somatic cell and an denucleated egg cell. These are then fused by inserting the somatic cell into the 'empty' ovum. After being inserted into the egg, the somatic cell nucleus is reprogrammed by its host egg cell. The ovum, now containing the somatic cell's nucleus, is stimulated with a shock and will begin to divide. The egg is now viable and capable of producing an adult organism containing all the necessary genetic information from just one parent. Development will ensue normally and after many mitotic divisions, this single cell forms a blastocyst (an early stage embryo with about 100 cells) with an identical genome to the original organism (i.e. a clone). Stem cells can then be obtained by the destruction of this clone embryo for use in therapeutic cloning or in the case of reproductive cloning the clone embryo is implanted into a host mother for further development and brought to term.\n\nSomatic cell nuclear transplantation has become a focus of study in stem cell research. The aim of carrying out this procedure is to obtain pluripotent cells from a cloned embryo. These cells genetically matched the donor organism from which they came. This gives them the ability to create patient specific pluripotent cells, which could then be used in therapies or disease research.\n\nEmbryonic stem cells are undifferentiated cells of an embryo. These cells are deemed to have a pluripotent potential because they have the ability to give rise to all of the tissues found in an adult organism. This ability allows stem cells to create any cell type, which could then be transplanted to replace damaged or destroyed cells. Controversy surrounds human ESC work due to the destruction of viable human embryos. Leading scientists to seek an alternative method of obtaining stem cells, SCNT is one such method.\n\nA potential use of stem cells genetically matched to a patient would be to create cell lines that have genes linked to a patient's particular disease. By doing so, an \"in vitro\" model could be created, would be useful for studying that particular disease, potentially discovering its pathophysiology, and discovering therapies. For example, if a person with Parkinson's disease donated his or her somatic cells, the stem cells resulting from SCNT would have genes that contribute to Parkinson's disease. The disease specific stem cell lines could then be studied in order to better understand the condition.\n\nAnother application of SCNT stem cell research is using the patient specific stem cell lines to generate tissues or even organs for transplant into the specific patient. The resulting cells would be genetically identical to the somatic cell donor, thus avoiding any complications from immune system rejection.\n\nOnly a handful of the labs in the world are currently using SCNT techniques in human stem cell research. In the United States, scientists at the Harvard Stem Cell Institute, the University of California San Francisco, the Oregon Health & Science University, Stemagen (La Jolla, CA) and possibly Advanced Cell Technology are currently researching a technique to use somatic cell nuclear transfer to produce embryonic stem cells. In the United Kingdom, the Human Fertilisation and Embryology Authority has granted permission to research groups at the Roslin Institute and the Newcastle Centre for Life. SCNT may also be occurring in China.\n\nIn 2005, a South Korean research team led by Professor Hwang Woo-suk, published claims to have derived stem cell lines via SCNT, but supported those claims with fabricated data. Recent evidence has proved that he in fact created a stem cell line from a parthenote.\n\nThough there has been numerous successes with cloning animals, questions remain concerning the mechanisms of reprogramming in the ovum. Despite many attempts, success in creating human nuclear transfer embryonic stem cells has been limited. There lies a problem in the human cell's ability to form a blastocyst; the cells fail to progress past the eight cell stage of development. This is thought to be a result from the somatic cell nucleus being unable to turn on embryonic genes crucial for proper development. These earlier experiments used procedures developed in non-primate animals with little success.\n\nA research group from the Oregon Health & Science University demonstrated SCNT procedures developed for primates successfully using skin cells. The key to their success was utilizing oocytes in metaphase II (MII) of the cell cycle. Egg cells in MII contain special factors in the cytoplasm that have a special ability in reprogramming implanted somatic cell nuclei into cells with pluripotent states. When the ovum's nucleus is removed, the cell loses its genetic information. This has been blamed for why enucleated eggs are hampered in their reprogramming ability. It is theorized the critical embryonic genes are physically linked to oocyte chromosomes, enucleation negatively affects these factors. Another possibility is removing the egg nucleus or inserting the somatic nucleus causes damage to the cytoplast, affecting reprogramming ability.\n\nTaking this into account the research group applied their new technique in an attempt to produce human SCNT stem cells. In May 2013, the Oregon group reported the successful derivation of human embryonic stem cell lines derived through SCNT, using fetal and infant donor cells. Using MII oocytes from volunteers and their improved SCNT procedure, human clone embryos were successfully produced. These embryos were of poor quality, lacking a substantial inner cell mass and poorly constructed trophectoderm. The imperfect embryos prevented the acquisition of human ESC. The addition of caffeine during the removal of the ovum's nucleus and injection of the somatic nucleus improved blastocyst formation and ESC isolation. The ESC obtain were found to be capable of producing teratomas, expressed pluripotent transcription factors, and expressed a normal 46XX karyotype, indicating these SCNT were in fact ESC-like. This was the first instance of successfully using SCNT to reprogram human somatic cells. This study used fetal and infantile somatic cells to produce their ESC.\n\nIn April 2014, an international research team expanded on this break through. There remained the question of whether the same success could be accomplished using adult somatic cells. Epigenetic and age related changes were thought to possibly hinder an adult somatic cells ability to be reprogrammed. Implementing the procedure pioneered by the Oregon research group they indeed were able to grow stem cells generated by SCNT using adult cells from two donors, aged 35 and 75.Indicating age does not impede a cells ability to be reprogrammed\n\nLate April 2014, the New York Stem Cell Foundation was successful in creating SCNT stem cells derived from adult somatic cells. One of these lines of stem cells was derived from the donor cells of a type 1 diabetic. The group was then able to successfully culture these stem cells and induce differentiation. When injected into mice, cells of all three of the germ layers successfully formed. The most significant of these cells, were those who expressed insulin and were capable of secreting the hormone. These insulin producing cells could be used for replacement therapy in diabetics, demonstrating real SCNT stem cell therapeutic potential.\n\nThe impetus for SCNT-based stem cell research has been decreased by the development and improvement of alternative methods of generating stem cells. Methods to reprogram normal body cells into pluripotent stem cells were developed in humans in 2007. The following year, this method achieved a key goal of SCNT-based stem cell research: the derivation of pluripotent stem cell lines that have all genes linked to various diseases. Some scientists working on SCNT-based stem cell research have recently moved to the new methods of induced pluripotent stem cells. Though recent studies have put in question how similar iPS cells are to embryonic stem cells. Epigenetic memory in iPS affects the cell lineage it can differentiate into. For instance, an iPS cell derived from a blood cell will be more efficient at differentiating into blood cells, while it will be less efficient at creating a neuron. This raises the question of how well iPS cells can mimic the gold standard ESC in experiments, as stem cells are defined as having the ability to differentiate into any cell type. SCNT stem cells do not pose such a problem and continue to remain relevant in stem cell studies.\n\nThis technique is currently the basis for cloning animals (such as the famous Dolly the sheep), and has been theoretically proposed as a possible way to clone humans. Using SCNT in reproductive cloning has proven difficult with limited success. High fetal and neonatal death make the process very inefficient. Resulting cloned offspring are also plagued with development and imprinting disorders in non-human species. For these reasons, along with moral and ethical objections, reproductive cloning in humans is proscribed in more than 30 countries. Most researchers believe that in the foreseeable future it will not be possible to use the current cloning technique to produce a human clone that will develop to term. It remains a possibility, though critical adjustments will be required to overcome current limitations during early embryonic development in human SCNT.\n\nThere is also the potential for treating diseases associated with mutations in mitochondrial DNA. Recent studies show SCNT of the nucleus of a body cell afflicted with one of these diseases into a healthy oocyte prevents the inheritance of the mitochondrial disease. This treatment does not involve cloning but would produce a child with three genetic parents. A father providing a sperm cell, one mother providing the egg nucleus and another mother providing the enucleated egg cell.\n\nIn 2018, the first successful cloning of primates using somatic cell nuclear transfer, the same method as Dolly the sheep, with the birth of two live female clones (crab-eating macaques named \"Zhong Zhong\" and \"Hua Hua\") was reported.\n\nInterspecies nuclear transfer (iSCNT) is a means of somatic cell nuclear transfer used to facilitate the rescue of endangered species, or even to restore species after their extinction. The technique is similar to SCNT cloning which typically is between domestic animals and rodents, or where there is a ready supply of oocytes and surrogate animals. However, the cloning of highly endangered or extinct species requires the use of an alternative method of cloning. Interspecies nuclear transfer utilizes a host and a donor of two different organisms that are closely related species and within the same genus. In 2000, Robert Lanza was able to produce a cloned fetus of a gaur, \"Bos gaurus\", combining it successfully with a domestic cow, \"Bos taurus\".\n\nInterspecies nuclear transfer provides evidence of the universality of the triggering mechanism of the cell nucleus reprogramming. For example, Gupta et al., explored the possibility of producing transgenic cloned embryos by interspecies somatic cell nuclear transfer (iSCNT) of cattle, mice, and chicken donor cells into enucleated pig oocytes. Moreover, NCSU23 medium, which was designed for in vitro culture of pig embryos, was able to support the in vitro development of cattle, mice, and chicken iSCNT embryos up to the blastocyst stage. Furthermore, ovine oocyte cytoplast may be used for remodeling and reprogramming of human somatic cells back to the embryonic stage.\n\nSCNT can be inefficient. Stresses placed on both the egg cell and the introduced nucleus in early research were enormous, resulting in a low percentage of successfully reprogrammed cells. For example, in 1996 Dolly the sheep was born after 277 eggs were used for SCNT, which created 29 viable embryos. Only three of these embryos survived until birth, and only one survived to adulthood. As the procedure was not automated, but had to be performed manually under a microscope, SCNT was very resource intensive. The biochemistry involved in reprogramming the differentiated somatic cell nucleus and activating the recipient egg was also far from understood. However, by 2014, researchers were reporting success rates of 70-80% with cloning pigs and in 2016 a Korean company, Sooam Biotech, was reported to be producing 500 cloned embryos a day.\n\nIn SCNT, not all of the donor cell's genetic information is transferred, as the donor cell's mitochondria that contain their own mitochondrial DNA are left behind. The resulting hybrid cells retain those mitochondrial structures which originally belonged to the egg. As a consequence, clones such as Dolly that are born from SCNT are not perfect copies of the donor of the nucleus. This fact may also hamper the potential benefits of SCNT derived tissues/organs for therapy, as there may be an immunoresponse to the non-self mtDNA after transplant.\n\nProposals to use nucleus transfer techniques in human stem cell research raise a set of concerns beyond the moral status of any created embryo. These have led to some individuals and organizations who are \"not\" opposed to human embryonic stem cell research to be concerned about, or opposed to, SCNT research.\n\nOne concern is that blastula creation in SCNT-based human stem cell research will lead to the reproductive cloning of humans. Both processes use the same first step: the creation of a nuclear transferred embryo, most likely via SCNT. Those who hold this concern often advocate for strong regulation of SCNT to preclude implantation of any derived products for the intention of human reproduction, or its prohibition.\n\nA second important concern is the appropriate source of the eggs that are needed. SCNT requires human eggs, which can only be obtained from women. The most common source of these eggs today are eggs that are produced and in excess of the clinical need during IVF treatment. This is a minimally invasive procedure, but it does carry some health risks, such as ovarian hyperstimulation syndrome.\n\nOne vision for successful stem cell therapies is to create custom stem cell lines for patients. Each custom stem cell line would consist of a collection of identical stem cells each carrying the patient's own DNA, thus reducing or eliminating any problems with rejection when the stem cells were transplanted for treatment. For example, to treat a man with Parkinson's disease, a cell nucleus from one of his cells would be transplanted by SCNT into an egg cell from an egg donor, creating a unique lineage of stem cells almost identical to the patient's own cells. (There would be differences. For example, the mitochondrial DNA would be the same as that of the egg donor. In comparison, his own cells would carry the mitochondrial DNA of his mother.)\n\nPotentially millions of patients could benefit from stem cell therapy, and each patient would require a large number of donated eggs in order to successfully create a single custom therapeutic stem cell line. Such large numbers of donated eggs would exceed the number of eggs currently left over and available from couples trying to have children through assisted reproductive technology. Therefore, healthy young women would need to be induced to sell eggs to be used in the creation of custom stem cell lines that could then be purchased by the medical industry and sold to patients. It is so far unclear where all these eggs would come from.\n\nStem cell experts consider it unlikely that such large numbers of human egg donations would occur in a developed country because of the unknown long-term public health effects of treating large numbers of healthy young women with heavy doses of hormones in order to induce hyperovulation (ovulating several eggs at once). Although such treatments have been performed for several decades now, the long-term effects have not been studied or declared safe to use on a large scale on otherwise healthy women. Longer-term treatments with much lower doses of hormones are known to increase the rate of cancer decades later. Whether hormone treatments to induce hyperovulation could have similar effects is unknown. There are also ethical questions surrounding paying for eggs. In general, marketing body parts is considered unethical and is banned in most countries. Human eggs have been a notable exception to this rule for some time.\n\nTo address the problem of creating a human egg market, some stem cell researchers are investigating the possibility of creating artificial eggs. If successful, human egg donations would not be needed to create custom stem cell lines. However, this technology may be a long way off.\n\nSCNT involving human cells is currently legal for research purposes in the United Kingdom, having been incorporated into the Human Fertilisation and Embryology Act 1990. Permission must be obtained from the Human Fertilisation and Embryology Authority in order to perform or attempt SCNT.\n\nIn the United States, the practice remains legal, as it has not been addressed by federal law. However, in 2002, a moratorium on United States federal funding for SCNT prohibits funding the practice for the purposes of research. Thus, though legal, SCNT cannot be federally funded. American scholars have recently argued that because the product of SCNT is a clone embryo, rather than a human embryo, these policies are morally wrong and should be revised.\n\nIn 2003, the United Nations adopted a proposal submitted by Costa Rica, calling on member states to \"prohibit all forms of human cloning in as much as they are incompatible with human dignity and the protection of human life.\" This phrase may include SCNT, depending on interpretation.\n\nThe Council of Europe's \"Convention on Human Rights and Biomedicine\" and its \"Additional Protocol to the Convention for the Protection of Human Rights and Dignity of the Human Being with regard to the Application of Biology and Medicine, on the Prohibition of Cloning Human Being\" appear to ban SCNT of human beings. Of the Council's 45 member states, the \"Convention\" has been signed by 31 and ratified by 18. The \"Additional Protocol\" has been signed by 29 member nations and ratified by 14.\n\n\n\n"}
{"id": "54656014", "url": "https://en.wikipedia.org/wiki?curid=54656014", "title": "South Seas", "text": "South Seas\n\nThe term South Seas commonly refers to the South Pacific. Geographically, all areas to the south of Panama's degree of latitude belong to the South Sea.\n\nCentral archipelagos are the Society Islands (French Polynesia/Tahiti), the Samoa-archipelago and the Fiji islands. The term South Sea is often used synonymously for Oceania and in a narrow sense for Polynesia (Polynesian Triangle). The Hawaii islands, New Zealand (Aotearoa) and the Easter Island (\"Rapa Nui\") build the vertices.\n\nThe Spanish conquistador Vasco Núñez de Balboa coined the term \"South Sea\" when he traveled through the isthmus of Panama and named the ocean lying ahead (Pacific) \"Mar del sur\" („South Ocean“).\n\nNúñez de Balboas and his soldiers tried to travel to the peak of the mountain to see the huge sea, but when they arrived at the foot of the mountain there where only 69 out of 190 soldiers left. He didn't want to share the experience of being the first to see the unknown ocean so he commanded his crew to stand still and wait. On 25 September 1513, he was the first European to see the Pacific Ocean. After looking at the ocean for some time, he told his crew to come up to share his happiness and his pride with them.\n\nHe declared the \"South Ocean\" to be property of his king, when he realized the salty water of the ocean, after he set foot into the water at the opening of the Saban river.\n\nIn a figurative sense, the South Sea is an often idealised and distant region.\n\nIn 1773, when James Cook came to Tahiti for the second time, he was accompanied by two scientifically educated Germans Johann Reinhold Forster and Georg Forster.\n\nThe report of the discoverers determined the Europeans' picture of the South Sea for a long time. On these grounds, Joseph Banks wrote: \n\nLouis Antoine de Bougainville's romantic travel report \"Voyage autour du monde\" as well as Georg Forster's travel description \"A Voyage Round The World\" (1777) confirmed Jean-Jacques Rousseau's image of the \"noble savages\". He describes the country as \"„jardin d’Eden“\" (Garden of Eden), which provided his residents with everything required to live. He saw the islanders as pure humans not yet poisoned by civilization. The report \"Voyage autour du monde\" inspired Denis Diderot to write his essay \"Supplément au voyage de Bougainville,\" a plea for sexual freedom.\n\nPaul Gauguin, a French artist, contributed to this picture as well. His pictures do not resemble reality, but the exotic paradise he envisioned.\n\nEven the German writer Erich Scheurmann benefited from these desires, which can be seen in his fictional travel reports of a South Sea chief. The travel reports were published between 1915 and 1920 under the title The Papalagi. Fifty years after the book was published it became a cult book and more than 1.7 million copies were sold in the German language only.\n"}
{"id": "1044268", "url": "https://en.wikipedia.org/wiki?curid=1044268", "title": "Spark spread", "text": "Spark spread\n\nThe spark spread is the theoretical gross margin of a gas-fired power plant from selling a unit of electricity, having bought the fuel required to produce this unit of electricity. All other costs (operation and maintenance, capital and other financial costs) must be covered from the spark spread. The term was first coined by Tony West's trading team on the trading floor of National Power Ltd in Swindon, UK during the late 1990s and quickly came into common usage as other traders realised the trading and hedging opportunities.\n\nThe term dark spread, quark spread and bark spread refers to the similarly defined difference between cash streams (spread) for coal-fired power plants, nuclear power plants and bio-mass power plants respectively. These indicators of power plant economics are useful for tracing energy markets. For operating or investment decisions published \"spread\" data are not applicable. Local market conditions, actual plant efficiencies and other plant costs have to be considered. The higher the dark spread the better, for the generator; an IPP with a dark spread of €15/MWh will be more profitable than a competitor with a dark spread of only €10/MWh.\n\nFurther definition of clean spread indicators include the price of carbon dioxide emission allowances (see: Emission trading).\n\nThe spark spread \"SS\" is defined as:\n\nA precise definition of a spark spread has to be given by the source publishing such indicators. Definitions should specify energy (electricity and fuel) prices considered (delivery point & conditions) and the plant efficiency used for the calculation. Also, any plant operating costs that may be included should be stated. Typically, an efficiency of 50 % is considered for gas-fired plants, and 36% for coal-fired plants.\n\nIn the UK, a non-rounded efficiency of 49.13% is used for calculating the gas conversion. In reality, each gas-fired plant has a different fuel efficiency, but 49.13% is used as a standard in the UK market because it provides an easy conversion between gas and power volumes. The spark spread value is therefore the power price minus the gas cost divided by 0.4913, i.e. Spark Spread = Power Price – (Gas cost/0.4913). As of August 2006, UK dark spreads were in the range of 10–30 £/MWh, while UK spark spreads were in the range of 4–9 £/MWh.\n\nIn countries that are covered by the European Union Emissions Trading Scheme, generators have to consider also the cost of carbon dioxide emission allowances that will be under a cap and trade regime. Emission trading has started in the EU in January 2005.\n\nThe Clean Spark Spread is calculated using a gas emissions intensity factor of 0.411 tCO2/MWh. Therefore, the clean spark spread is\ncalculated by subtracting the carbon price per tonne (multiplied by 0.411) from the ‘dirty’ spark spread, i.e. Clean Spark Spread = Spark Spread\n– (Carbon Price*0.411).\n\nClean spark spread or \"spark green spread\" represents the net revenue a generator makes from selling power, having bought gas and the required number of carbon allowances. This spread is calculated by adjusting the cost of natural gas in MMBtu for the efficiency of the generation and subsequently applying the market cost of procuring or opportunity cost of setting aside an emissions allowance such as a European Union Allowance (EUA) in the European Union Emissions Trading Scheme (EU ETS).\n\nLet S: spark spread, E: electricity price, G: gas cost, Ng: number of carbon credits necessary to cover gas operation, Pcc: price of a carbon credit.\n\nThen, Clean spark spread = E - G - Ng*Pcc = S - Ng*Pcc\n\nClean dark spread or \"dark green spread\" refers to an analogous indicator for coal-fired generation of electricity. The spark green spread and the dark green spread are especially important in areas where coal-fired electricity generation is prevalent as the convergence of the spreads will lead to an important decision point.\n\nLet D: dark spread, E: electricity price, C: coal cost, Nc: number of carbon credits necessary to cover coal operation (2–2.5x that of gas), Pcc: price of a carbon credit.\n\nThen, Clean dark spread = E - C - Nc*Pcc = D - Nc*Pcc\n\nClimate spread: The difference between the dark green spread and the spark green spread is known as the \"Climate Spread\".\n\nClimate spread = Clean dark spread - Clean spark spread = (D - Nc*Pcc) - (S - Ng*Pcc) = (D - S) - (Nc - Ng)*Pcc.\n\nNote: (D - S) and (Nc - Ng) are positive numbers.\n\nIn a carbon constrained economy a power producer in a geographic area where coal is currently the preferred method by which electricity is generated may eventually encounter a negative climate spread if carbon credit prices rise. This would mean that when taking into consideration the cost to produce plus the cost of compliance with a cap and trade (coal is on average 2.5 times as polluting as natural gas for the same MWh of electricity), natural gas would be a better decision. This would begin to cause more internal abatement via power generation fuel switching and less reliance on flexible mechanisms. This is important due to concerns regarding supplementarity.\n\nClimate spread is also interesting in that it is the fundamental driver for the price of carbon credits. Since the ETS cap-and-trade system covers the major polluting industries, power generation by coal- and gas-fired power plants, by far the largest power sources, create the most carbon credit demand within the ETS. To cover emissions on an ever-tightening ration of free EUA allowances, a coal-fired powered power plant will either have to abate internally or buy credits. If the price of marginal internal abatement is lower than the price of carbon credits, the firm will choose internal abatement. However marginal abatement becomes more and more expensive, at some point forcing the plant to buy credits – thus the carbon credit price is equal to the marginal cost of abatement to the extent that European power plants have chosen to abate.\n\nClean Dark Spreads are a reflection of the cost of generating power from coal after taking into account fuel (coal) and carbon allowance costs. A positive spread effectively means that it is profitable to generate electricity on a Baseload basis for the period in question, while a negative spread means that generation would be a loss-making activity. The Clean Spark Spreads do not take into account additional generating charges (beyond fuel and carbon), such as operational costs.\n\nBoth the UK and German Dark Spread tables use a fuel efficiency factor of 35% for the coal conversion, and an energy conversion factor of 7.1 for converting tonnes/coal into MWh/electricity. In reality, each type of coal has a different energy value and each coal-fired plant has a different fuel efficiency, but 35% is accepted as a broad standard. At the time of writing (March 2007) there is no liquid Dark Spread traded market in either the UK or Germany. The Dark Spread value is the power price minus the coal price divided by 0.35, i.e. Dark Spread = Power price – (Coal price/0.35).\n\nThe Clean Dark Spread is calculated using a coal emissions intensity factor of 0.971 tCO2/MWh. Therefore, the Clean Dark Spread is calculated by subtracting the carbon price (multiplied by 0.971) from the ‘dirty’ spark spread, i.e. Clean Dark Spread = Dark Spread – (Carbon Price*0.971).\n\nSpark spread can be used to assess the loss of revenue if a power station is switched from a normal running scenario to one where it is held in reserve to provide power when a large population of wind, or other renewable generators, is unable to generate.\n\nIn theory, the power station operator would be indifferent to such non-running as long as he was paid the spread it would have earned during the normally expected number of hour run. In fact, if paid the expected spark spread for the hours it had expected to run in normal operating mode, the operator would be better off, because it would not incur the variable operating and maintenance costs (O&M costs), which are proportional to the electrical energy produced.\n\nAn assessment of the lost revenues is needed if some power plants, such as wind turbines, have absolute priority (must-run plants). A dispatching authority will in this case order the other plants to decrease power. In some countries plant operators are entitled to receive compensation for such interventions. In a competitive electricity market the situation can be handled by a balancing mechanism, in which any imbalance from the schedule (typically a day-ahead schedule) is penalized, either using the price from a balancing market or a calculated price.\nThus, since UK spark spreads were in the range of 4–9 £/MWh – on average £6.5/MWh, or 0.65 p/kWh, we can assess the likely cost of relegating existing power stations to a standby role for a large penetration of renewables as being around 0.65 p/kWh.\n\n\n"}
{"id": "46726166", "url": "https://en.wikipedia.org/wiki?curid=46726166", "title": "Tabriz Museum of Natural History", "text": "Tabriz Museum of Natural History\n\nTabriz Museum of Natural History is a museum of wild life in the city of Tabriz at north western Iran, established in 1993 by Department of Environment of Iran. The museum includes many of taxidermy of wild mammals, reptiles, birds, and aquatics which are inhabiting in Iranian Azerbaijan, Iran, and some other countries. It also have a section for sculptures of dinosaurs. \n"}
{"id": "44713704", "url": "https://en.wikipedia.org/wiki?curid=44713704", "title": "The Ecology Center (Orange County)", "text": "The Ecology Center (Orange County)\n\nThe Ecology Center is an educational nonprofit located in Orange County, California, that focuses on environmental awareness. The organization was founded by Evan Marks in 2008 on a two-acre plot that was once an 1898 homestead, the historical Joel Cogdon house. It is well known locally for its annual Green Feast event. The Ecology Center's \"Water Shed\" program involves educating Orange County residents about water conservation, and its \"Grow Your Own\" program builds gardens for local schools.\n"}
{"id": "40751834", "url": "https://en.wikipedia.org/wiki?curid=40751834", "title": "Tilted block faulting", "text": "Tilted block faulting\n\nTilted block faulting, also called rotational block faulting, is a mode of structural evolution in extensional tectonic events, a result of tectonic plates stretching apart. When the upper lithospheric crust experiences extensional pressures, the brittle crust fractures, creating detachment faults. These normal faults express themselves on a regional scale; upper crust fractures into tilted fault blocks, and ductile lower crust ascends. This results in uplift, cooling, and exhumation of ductily deformed deeper crust. The large unit of tilted blocks and associated crust can help form an integral part of metamorphic core complexes and can occur on both continental and oceanic crust.\n\nThe term \"tilted block faulting\" is a literal description of rotational extension on planar faults, which results in a uniform rotation of faults and crust. Often a \"domino-style\" stacking of the fault blocks occurs, creating the basis of the terminology.\n\nDuring extensional time periods, large, gently-dipping normal faults, called detachment faults, can form due to relative separation of the two sides surrounding the fault. Typically, these faults can have an offset on the order of one to tens of kilometers. As the region continues to experience extensive pressures, there is an isostatic effect which moves ductile crust material underneath the fault complex. This fault system can shear the footwall, creating domal mountain ranges, which on a large scale can develop into formations known as metamorphic core complexes. If extension at the surface exceeds about 50 percent, decompression melting may permit magmas to form; these will deform the footwall, resulting in a complex associated with intrusive and extrusive igneous rocks. Rocks above the detachment fault form normal faults and, at the same time, shear in a \"layer-parallel\" motion. This action creates a series of fault blocks, which are progressively tilted as the detachment fault progresses. The fracturing of the fault blocks can occur in a similar time frame or develop progressively.\n\nAs the fault blocks rotate and tip, erosion occurs, filling the basins that are formed with associated sediment from the block into the \"down-dropped corners\". The basin-fill occurs concurrently with the exhumation. Calculations examining sediment infill suggest that differences in core complexes can be controlled by erosion rates and hanging wall resistance of the fault. Tilted blocks are formed under specific crustal conditions, where the lower crust is relatively warm, not hot. Hotter crust will lead to a type of formation known as a \"rolling hinge\" complex. The geometry of the tilted block system can be greatly affected by subsidence and isostasy.\n\nCore complexes containing rotational fault blocks occur throughout the world. There are excellent examples in the Southwestern United States, including Arizona and Baja California. The more than 25 metamorphic core complexes in this region were formed during crustal extension during the mid-Cenozoic era. Block faulting of this nature is common in extensional settings and has been found to be an important part of physical geological models from sites around the globe, including Europe and China. Due to the availability and applicability of the systems, interest in core complexes and rotational extension systems remains high.\n\n"}
{"id": "6906913", "url": "https://en.wikipedia.org/wiki?curid=6906913", "title": "Townsend discharge", "text": "Townsend discharge\n\nThe Townsend discharge or Townsend avalanche is a gas ionisation process where free electrons are accelerated by an electric field, collide with gas molecules, and consequently free additional electrons. Those electrons are in turn accelerated and free additional electrons. The result is an avalanche multiplication that permits electrical conduction through the gas. The discharge requires a source of free electrons and a significant electric field; without both, the phenomenon does not occur.\n\nThe Townsend discharge is named after John Sealy Townsend, who discovered the fundamental ionisation mechanism by his work between 1897 and 1901.\n\nThe avalanche occurs in a gaseous medium that can be ionised (such as air). The electric field and the mean free path of the electron must allow free electrons to acquire an energy level (velocity) that can cause impact ionisation. If the electric field is too small, then the electrons do not acquire enough energy. If the mean free path is too short, the electron gives up its acquired energy in a series of non-ionising collisions. If the mean free path is too long, then the electron reaches the anode before colliding with another molecule.\n\nThe avalanche mechanism is shown in the accompanying diagram. The electric field is applied across a gaseous medium; initial ions are created with ionising radiation (for example, cosmic rays). An original ionisation event produces an ion pair; the positive ion accelerates towards the cathode while the free electron accelerates towards the anode. If the electric field is strong enough, the free electron can gain sufficient velocity (energy) to liberate another electron when it next collides with a molecule. The two free electrons then travel towards the anode and gain sufficient energy from the electric field to cause further impact ionisations, and so on. This process is effectively a chain reaction that generates free electrons. The total number of electrons reaching the anode is equal to the number of collisions, plus the single initiating free electron. Initially, the number of collisions grows exponentially. The limit to the multiplication in an electron avalanche is known as the Raether limit.\n\nThe Townsend avalanche can have a large range of current densities. In common gas-filled tubes, such as those used as gaseous ionisation detectors, magnitudes of currents flowing during this process can range from about 10 amperes to about 10 amperes.\n\nTownsend's early experimental apparatus consisted of planar parallel plates forming two sides of a chamber filled with a gas. A direct current high-voltage source was connected between the plates; the lower voltage plate being the cathode while the other was the anode. He forced the cathode to emit electrons using the photoelectric effect by irradiating it with X-rays, and he found that the current flowing through the chamber depended on the electric field between the plates. However, this current showed an exponential increase as the plate gaps became small, leading to the conclusion that the gas ions were multiplying as they moved between the plates due to the high electric field.\n\nTownsend observed currents varying exponentially over ten or more orders of magnitude with a constant applied voltage when the distance between the plates was varied. He also discovered that gas pressure influenced conduction: he was able to generate ions in gases at low pressure with a much lower voltage than that required to generate a spark. This observation overturned conventional thinking about the amount of current that an irradiated gas could conduct.\n\nThe experimental data obtained from his experiments are described by the following formula\n\nwhere\nThe almost constant voltage between the plates is equal to the breakdown voltage needed to create a self-sustaining avalanche: it \"decreases\" when the current reaches the glow discharge regime. Subsequent experiments revealed that the current rises faster than predicted by the above formula as the distance increases: two different effects were considered in order to better model the discharge: positive ions and cathode emission.\n\nTownsend put forward the hypothesis that positive ions also produce ion pairs, introducing a coefficient formula_2 expressing the number of ion pairs generated per unit length by a positive ion (cation) moving from anode to cathode. The following formula was found\n\nsince formula_4, in very good agreement with experiments.\n\nThe \"first Townsend coefficient\" ( α ), also known as \"first Townsend avalanche coefficient\" is a term used where secondary ionisation occurs because the primary ionisation electrons gain sufficient energy from the accelerating electric field, or from the original ionising particle. The coefficient gives the number of secondary electrons produced by primary electron per unit path length.\n\nTownsend, Holst and Oosterhuis also put forward an alternative hypothesis, considering the augmented emission of electrons by the cathode caused by impact of positive ions. This introduced \"Townsend's second ionisation coefficient\" formula_5; the average number of electrons released from a surface by an incident positive ion, according to the following formula:\n\nThese two formulas may be thought as describing limiting cases of the effective behavior of the process: either can be used to describe the same experimental results. Other formulas describing various intermediate behaviors are found in the literature, particularly in reference 1 and citations therein.\n\nA Townsend discharge can be sustained only over a limited range of gas pressure and electric field intensity. The accompanying plot shows the variation of voltage drop and the different operating regions for a gas-filled tube with a constant pressure, but a varying current between its electrodes. The Townsend avalanche phenomena occurs on the sloping plateau B-D. Beyond D the ionisation is sustained.\n\nAt higher pressures, discharges occur more rapidly than the calculated time for ions to traverse the gap between electrodes, and the streamer theory of spark discharge of Raether, Meek and Loeb is applicable. In highly non-uniform electric fields, the corona discharge process is applicable. See Electron avalanche for further description of these mechanisms.\n\nDischarges in vacuum require vaporization and ionisation of electrode atoms. An arc can be initiated without a preliminary Townsend discharge; for example when electrodes touch and are then separated.\n\nThe starting of Townsend discharge sets the upper limit to the blocking voltage a glow discharge gas-filled tube can withstand. This limit is the Townsend discharge breakdown voltage, also called ignition voltage of the tube.\n\nThe occurrence of Townsend discharge, leading to glow discharge breakdown shapes the current-voltage characteristic of a gas discharge tube such as a neon lamp in a way such that it has a negative differential resistance region of the S-type. The negative resistance can be used to generate electrical oscillations and waveforms, as in the relaxation oscillator whose schematic is shown in the picture on the right. The sawtooth shaped oscillation generated has frequency\n\nAvalanche multiplication during Townsend discharge is naturally used in gas phototubes, to amplify the photoelectric charge generated by incident radiation (visible light or not) on the cathode: achievable current is typically 10~20 times greater respect to that generated by vacuum phototubes.\n\nTownsend avalanche discharges are fundamental to the operation of gaseous ionisation detectors such as the Geiger–Müller tube and the proportional counter in either detecting ionising radiation or measuring its energy. The incident radiation will ionise atoms or molecules in the gaseous medium to produce ion pairs, but different use is made by each detector type of the resultant avalanche effects.\n\nIn the case of a GM tube the high electric field strength is sufficient to cause complete ionisation of the fill gas surrounding the anode from the initial creation of just one ion pair. The GM tube output carries information that the event has occurred, but no information about the energy of the incident radiation.\n\nIn the case of proportional counters, multiple creation of ion pairs occurs in the \"ion drift\" region near the cathode. The electric field and chamber geometries are selected so that an \"avalanche region\" is created in the immediate proximity of the anode. A negative ion drifting towards the anode enters this region and creates a localised avalanche that is independent of those from other ion pairs, but which can still provide a multiplication effect. In this way spectroscopic information on the energy of the incident radiation is available by the magnitude of the output pulse from each initiating event.\n\nThe accompanying plot shows the variation of ionisation current for a co-axial cylinder system. In the ion chamber region, there are no avalanches and the applied voltage only serves to move the ions towards the electrodes to prevent re-combination.\nIn the proportional region, localised avalanches occur in the gas space immediately round the anode which are numerically proportional to the number of original ionising events. Increasing the voltage further increases the number of avalanches until the Geiger region is reached where the full volume of the fill gas around the anodes ionised, and all proportional energy information is lost. Beyond the Geiger region the gas is in continuous discharge owing to the high electric field strength.\n\n\n\n"}
{"id": "47863556", "url": "https://en.wikipedia.org/wiki?curid=47863556", "title": "Volcanic ash", "text": "Volcanic ash\n\nVolcanic ash consists of fragments of pulverized rock, minerals and volcanic glass, created during volcanic eruptions and measuring less than 2 mm (0.079 inches) in diameter. The term volcanic ash is also often loosely used to refer to all explosive eruption products (correctly referred to as tephra), including particles larger than 2 mm. Volcanic ash is formed during explosive volcanic eruptions when dissolved gases in magma expand and escape violently into the atmosphere. The force of the escaping gas shatters the magma and propels it into the atmosphere where it solidifies into fragments of volcanic rock and glass. Ash is also produced when magma comes into contact with water during phreatomagmatic eruptions, causing the water to explosively flash to steam leading to shattering of magma. Once in the air, ash is transported by wind up to thousands of kilometers away.\n\nDue to its wide dispersal, ash can have a number of impacts on society, including human and animal health, disruption to aviation, disruption to critical infrastructure (e.g., electric power supply systems, telecommunications, water and waste-water networks, transportation), primary industries (e.g., agriculture), buildings and structures.\n\nVolcanic ash is formed during explosive volcanic eruptions, phreatomagmatic eruptions and during transport in pyroclastic density currents.\n\nExplosive eruptions occur when magma decompresses as it rises, allowing dissolved volatiles (dominantly water and carbon dioxide) to exsolve into gas bubbles. As more bubbles nucleate a foam is produced, which decreases the density of the magma, accelerating it up the conduit. Fragmentation occurs when bubbles occupy ~70-80 vol% of the erupting mixture. When fragmentation occurs, violently expanding bubbles tear the magma apart into fragments which are ejected into the atmosphere where they solidify into ash particles. Fragmentation is a very efficient process of ash formation and is capable of generating very fine ash even without the addition of water.\n\nVolcanic ash is also produced during phreatomagmatic eruptions. During these eruptions fragmentation occurs when magma comes into contact with bodies of water (such as the sea, lakes and marshes) groundwater, snow or ice. As the magma, which is significantly hotter than the boiling point of water, comes into contact with water an insulating vapor film forms (Leidenfrost effect). Eventually this vapor film will collapse leading to direct coupling of the cold water and hot magma. This increases the heat transfer which leads to the rapid expansion of water and fragmentation of the magma into small particles which are subsequently ejected from the volcanic vent. Fragmentation causes an increase in contact area between magma and water creating a feedback mechanism, leading to further fragmentation and production of fine ash particles.\n\nPyroclastic density currents can also produce ash particles. These are typically produced by lava dome collapse or collapse of the eruption column. Within pyroclastic density currents particle abrasion occurs as particles interact with each other resulting in a reduction in grain size and production of fine grained ash particles. In addition, ash can be produced during secondary fragmentation of pumice fragments, due to the conservation of heat within the flow. These processes produce large quantities of very fine grained ash which is removed from pyroclastic density currents in co-ignimbrite ash plumes.\n\nPhysical and chemical characteristics of volcanic ash are primarily controlled by the style of volcanic eruption. Volcanoes display a range of eruption styles which are controlled by magma chemistry, crystal content, temperature and dissolved gases of the erupting magma and can be classified using the volcanic explosivity index (VEI). Effusive eruptions (VEI 1) of basaltic composition produce <10 m of ejecta, whereas extremely explosive eruptions (VEI 5+) of rhyolitic and dacitic composition can inject large quantities (>10 m) of ejecta into the atmosphere. Another parameter controlling the amount of ash produced is the duration of the eruption: the longer the eruption is sustained, the more ash will be produced. For example, the second phase of the 2010 eruptions of Eyjafjallajökull was classified as VEI 4 despite a modest 8 km high eruption column, but the eruption continued for a month, which allowed a large volume of ash to be ejected into the atmosphere.\n\nThe types of minerals present in volcanic ash are dependent on the chemistry of the magma from which it erupted. Considering that the most abundant elements found in silicate magma are silicon and oxygen, the various types of magma (and therefore ash) produced during volcanic eruptions are most commonly explained in terms of their silica content. Low energy eruptions of basalt produce a characteristically dark coloured ash containing ~45 - 55% silica that is generally rich in iron (Fe) and magnesium (Mg). The most explosive rhyolite eruptions produce a felsic ash that is high in silica (>69%) while other types of ash with an intermediate composition (e.g., andesite or dacite) have a silica content between 55-69%.\n\nThe principal gases released during volcanic activity are water, carbon dioxide, sulfur dioxide, hydrogen, hydrogen sulfide, carbon monoxide and hydrogen chloride. These sulfur and halogen gases and metals are removed from the atmosphere by processes of chemical reaction, dry and wet deposition, and by adsorption onto the surface of volcanic ash.\n\nIt has long been recognised that a range of sulfate and halide (primarily chloride and fluoride) compounds are readily mobilised from fresh volcanic ash.; It is considered most likely that these salts are formed as a consequence of rapid acid dissolution of ash particles within eruption plumes, which is thought to supply the cations involved in the deposition of sulfate and halide salts.\n\nWhile some 55 ionic species have been reported in fresh ash leachates the most abundant species usually found are the cations Na, K, Ca and Mg and the anions Cl, F and SO. Molar ratios between ions present in leachates suggest that in many cases these elements are present as simple salts such as NaCl and CaSO. In a sequential leaching experiment on ash from the 1980 eruption of Mount St. Helens, chloride salts were found to be the most readily soluble, followed by sulfate salts Fluoride compounds are in general only sparingly soluble (e.g., CaF, MgF), with the exception of fluoride salts of alkali metals and compounds such as calcium hexafluorosilicate (CaSiF). The pH of fresh ash leachates is highly variable, depending on the presence of an acidic gas condensate (primarily as a consequence of the gases SO, HCl and HF in the eruption plume) on the ash surface.\n\nThe crystalline-solid structure of the salts act more as an insulator than a conductor. However, once the salts are dissolved into a solution by a source of moisture (e.g., fog, mist, light rain, etc.), the ash may become corrosive and electrically conductive. A recent study has shown that the electrical conductivity of volcanic ash increases with (1) increasing moisture content, (2) increasing soluble salt content, and (3) increasing compaction (bulk density). The ability of volcanic ash to conduct electric current has significant implications for electric power supply systems.\n\nVolcanic ash particles erupted during magmatic eruptions are made up of various fractions of vitric (glassy, non-crystalline), crystalline or lithic (non-magmatic) particles. Ash produced during low viscosity magmatic eruptions (e.g., Hawaiian and Strombolian basaltic eruptions) produce a range of different pyroclasts dependent on the eruptive process. For example, ash collected from Hawaiian lava fountains consists of sideromelane (light brown basaltic glass) pyroclasts which contain rare microlites (small quench crystals) and phenocrysts. Slightly more viscous eruptions of basalt (e.g., Strombolian) form a variety of pyroclasts from irregular sideromelane droplets to blocky tachylite (black to dark brown microcrystalline pyroclasts). In contrast, most high-silica ash (e.g. rhyolite) consists of pulverised products of pumice (vitric shards), individual phenocrysts (crystal fraction) and some lithic fragments (xenoliths).\n\nAsh generated during phreatic eruptions primarily consists of hydrothermally altered lithic and mineral fragments, commonly in a clay matrix. Particle surfaces are often coated with aggregates of zeolite crystals or clay and only relict textures remain to identify pyroclast types.\n\nThe morphology (shape) of volcanic ash is controlled by a plethora of different eruption and kinematic processes. Eruptions of low-viscosity magmas (e.g., basalt) typically form droplet shaped particles. This droplet shape is, in part, controlled by surface tension, acceleration of the droplets after they leave the vent, and air friction. Shapes range from perfect spheres to a variety of twisted, elongate droplets with smooth, fluidal surfaces.\n\nThe morphology of ash from eruptions of high-viscosity magmas (e.g., rhyolite, dacite, and some andesites) is mostly dependent on the shape of vesicles in the rising magma before disintegration. Vesicles are formed by the expansion of magmatic gas before the magma has solidified. Ash particles can have varying degrees of vesicularity and vesicular particles can have extremely high surface area to volume ratios. Concavities, troughs, and tubes observed on grain surfaces are the result of broken vesicle walls. Vitric ash particles from high-viscosity magma eruptions are typically angular, vesicular pumiceous fragments or thin vesicle-wall fragments while lithic fragments in volcanic ash are typically equant, or angular to subrounded. Lithic morphology in ash is generally controlled by the mechanical properties of the wall rock broken up by spalling or explosive expansion of gases in the magma as it reaches the surface.\n\nThe morphology of ash particles from phreatomagmatic eruptions is controlled by stresses within the chilled magma which result in fragmentation of the glass to form small blocky or pyramidal glass ash particles. Vesicle shape and density play only a minor role in the determination of grain shape in phreatomagmatic eruptions. In this sort of eruption, the rising magma is quickly cooled on contact with ground or surface water. Stresses within the \"quenched\" magma cause fragmentation into five dominant pyroclast shape-types: (1) blocky and equant; (2) vesicular and irregular with smooth surfaces; (3) moss-like and convoluted; (4) spherical or drop-like; and (5) plate-like.\n\nThe density of individual particles varies with different eruptions. The density of volcanic ash varies between 700–1200 kg/m for pumice, 2350–2450 kg/m for glass shards, 2700–3300 kg/m for crystals, and 2600–3200 kg/m for lithic particles. Since coarser and denser particles are deposited close to source, fine glass and pumice shards are relatively enriched in ash fall deposits at distal locations. The high density and hardness (~5 on the Mohs Hardness Scale) together with a high degree of angularity, make some types of volcanic ash (particularly those with a high silica content) very abrasive.\n\nVolcanic ash consists of particles (pyroclasts) with diameters <2 mm (particles >2 mm are classified as lapilli), and can be as fine as 1 μm. The overall grain size distribution of ash can vary greatly with different magma compositions. Few attempts have been made to correlate the grain size characteristics of a deposit with those of the event which produced it, though some predictions can be made. Rhyolitic magmas generally produce finer grained material compared to basaltic magmas, due to the higher viscosity and therefore explosivity. The proportions of fine ash are higher for silicic explosive eruptions, probably because vesicle size in the pre-eruptive magma is smaller than those in mafic magmas. There is good evidence that pyroclastic flows produce high proportions of fine ash by communition and it is likely that this process also occurs inside volcanic conduits and would be most efficient when the magma fragmentation surface is well below the summit crater.\n\nAsh particles are incorporated into eruption columns as they are ejected from the vent at high velocity. The initial momentum from the eruption propels the column upwards. As air is drawn into the column, the bulk density decreases and it starts to rise buoyantly into the atmosphere. At a point where the bulk density of the column is the same as the surrounding atmosphere, the column will cease rising and start moving laterally. Lateral dispersion is controlled by prevailing winds and the ash may be deposited hundreds to thousands of kilometres from the volcano, depending on eruption column height, particle size of the ash and climatic conditions (especially wind direction and strength and humidity).\n\nAsh fallout occurs immediately after the eruption and is controlled by particle density. Initially, coarse particles fall out close to source. This is followed by fallout of accretionary lapilli, which is the result of particle agglomeration within the column. Ash fallout is less concentrated during the final stages as the column moves downwind. This results in an ash fall deposit which generally decreases in thickness and grain size exponentially with increasing distance from the volcano. Fine ash particles may remain in the atmosphere for days to weeks and be dispersed by high-altitude winds. These particles can impact on the aviation industry (refer to impacts section) and, combined with gas particles, can affect global climate.\n\nVolcanic ash plumes can form above pyroclastic density currents, these are called co-ignimbrite plumes. As pyroclastic density currents travel away from the volcano, smaller particles are removed from the flow by elutriation and form a less dense zone overlying the main flow. This zone then entrains the surrounding air and a buoyant co-ignimbrite plume is formed. These plumes tend to have higher concentrations of fine ash particles compared to magmatic eruption plumes due to the abrasion within the pyroclastic density current.\n\nPopulation growth has caused the progressive encroachment of urban development into higher risk areas, closer to volcanic centres, increasing the human exposure to volcanic ash fall events.\n\nInfrastructure is critical to supporting modern societies, particularly in urban areas, where high population densities create high demand for services. These infrastructure networks and systems support urban living, and provide lifeline services upon which we depend for our health, education, transport and social networking. Infrastructure networks and services support a variety of facilities across a broad range of sectors.\n\nVolcanic ash fall events can disrupt and or damage the infrastructure upon which society depends. Several recent eruptions have illustrated the vulnerability of urban areas that received only a few millimetres or centimetres of volcanic ash. This has been sufficient to cause disruption of transportation, electricity, water, sewage and storm water systems. Costs have been incurred from business disruption, replacement of damaged parts and insured losses. Ash fall impacts on critical infrastructure can also cause multiple knock-on effects, which may disrupt many different sectors and services.\n\nVolcanic ash fall is physically, socially, and economically disruptive. Volcanic ash can affect both proximal areas and areas many hundreds of kilometres from the source, and causes disruptions and losses in a wide variety of different infrastructure sectors. Impacts are dependent on: ash fall thickness; the duration of the ash fall; the grain size and chemistry of the ash; whether the ash is wet or dry; and any preparedness, management and prevention (mitigation) measures employed to reduce effects from the ash fall. Different sectors of infrastructure and society are affected in different ways and are vulnerable to a range of impacts or consequences. These are discussed in the following sections.\n\nVolcanic ash can cause disruption to electric power supply systems at all levels of power generation, transformation, transmission and distribution. There are four main impacts arising from ash-contamination of apparatus used in the power delivery process:\n\n\nIf the resulting short-circuit current is high enough to trip the circuit breaker then disruption of service will occur. Ash-induced flashover across transformer insulation (bushings) can burn, etch or crack the insulation irreparably and will likely result in the disruption of power supply.\n\n\nFollowing an eruption, it is very common for the public to hold fears about chemical contamination of water supplies. However, in general, the physical impacts of an ashfall will tend to overwhelm problems caused by the release of chemical contaminants from fresh volcanic ash. Impacts vary according to the type of treatment system.\n\nGroundwater-fed systems are resilient to impacts from ashfall, although airborne ash can interfere with the operation of well-head pumps. Electricity outages caused by ashfall can also disrupt electrically powered pumps if there is no backup generation.\n\nFor surface water sources such as lakes and reservoirs, the volume available for dilution of ionic species leached from ash is generally large. The most abundant components of ash leachates (Ca, Na, Mg, K, Cl, F and SO) occur naturally at significant concentrations in most surface waters and therefore are not affected greatly by inputs from volcanic ashfall, and are also of low concern in drinking water, with the possible exception of fluorine. The elements iron, manganese and aluminium are commonly enriched over background levels by volcanic ashfall. These elements may impart a metallic taste to water, and may produce red, brown or black staining of whiteware, but are not considered a health risk. Volcanic ashfalls are not known to have caused problems in water supplies for toxic trace elements such as mercury (Hg) and lead (Pb) which occur at very low levels in ash leachates.\n\nA further point to note is that drinking-water treatment commonly involves the addition of treatment chemicals such as aluminium sulfate or ferric chloride as flocculants, lime for pH adjustment, chlorine for disinfection and fluoride compounds for dental health.\n\nThe physical impacts of ashfall can affect the operation of water treatment plants. Ash can block intake structures, cause severe abrasion damage to pump impellers and overload pump motors. Many water treatment plants have an initial coagulation/flocculation step that is automatically adjusted to turbidity (the level of suspended solids, measured in nephelometric turbidity units) in the incoming water. In most cases, changes in turbidity caused by suspended ash particles will be within the normal operating range of the plant and can be managed satisfactorily by adjusting the addition of coagulant. Ashfalls will be more likely to cause problems for plants that are not designed for high levels of turbidity and which may omit coagulation/flocculation treatment. Ash can enter filtration systems such as open sand filters both by direct fallout and via intake waters. In most cases, increased maintenance will be required to manage the effects of an ashfall, but there will not be service interruptions.\n\nThe final step of drinking water treatment is disinfection to ensure that final drinking water is free from infectious microorganisms. As suspended particles (turbidity) can provide a growth substrate for microorganisms and can protect them from disinfection treatment, it is extremely important that the water treatment process achieves a good level of removal of suspended particles.\n\nMany small communities obtain their drinking water from diverse sources (lakes, streams, springs and groundwater wells). Levels of treatment vary widely, from rudimentary systems with coarse screening or settling followed by disinfection (usually chlorination), to more sophisticated systems using a filtration step. It should be noted that unless a high quality source is used, such as secure groundwater, disinfection alone is unlikely to guarantee that drinking water is safe from protozoa such as Giardia and Cryptosporidium, which are relatively resistant to standard disinfectants and which require additional removal steps such as filtration.\n\nVolcanic ashfall is likely to have major effects on these systems. Ash will clog intake structures, cause abrasion damage to pumps and block pipes, settling ponds and open filters. High levels of turbidity are very likely to interfere with disinfection treatment and doses may have to be adjusted to compensate. It is essential to monitor chlorine residuals in the distribution system.\n\nMany households, and some small communities, rely on rainwater for their drinking water supplies. Roof-fed systems are highly vulnerable to contamination by ashfall, as they have a large surface area relative to the storage tank volume. In these cases, leaching of chemical contaminants from the ashfall can become a health risk and drinking of water is not recommended. Prior to an ashfall, downpipes should be disconnected so that water in the tank is protected. A further problem is that the surface coating of fresh volcanic ash can be acidic. Unlike most surface waters, rainwater generally has a very low alkalinity (acid-neutralising capacity) and thus ashfall may acidify tank waters. This may lead to problems with plumbosolvency, whereby the water is more aggressive towards materials that it comes into contact with. This can be a particular problem if there are lead-head nails or lead flashing used on the roof, and for copper pipes and other metallic plumbing fittings.\n\nDuring ashfall events large demands are commonly placed on water resources for cleanup and shortages can result. Shortages compromise key services such as firefighting and can lead to a lack of water for hygiene, sanitation and drinking. Municipal authorities need to monitor and manage this water demand carefully, and may need to advise the public to utilise cleanup methods that do not use water (e.g., cleaning with brooms rather than hoses).\n\nWastewater networks may sustain damage similar to water supply networks. It is very difficult to exclude ash from the sewerage system. Systems with combined storm water/sewer lines are most at risk. Ash will enter sewer lines where there is inflow/infiltration by stormwater through illegal connections (e.g., from roof downpipes), cross connections, around manhole covers or through holes and cracks in sewer pipes.\n\nAsh-laden sewage entering a treatment plant is likely to cause failure of mechanical prescreening equipment such as step screens or rotating screens. Ash that penetrates further into the system will settle and reduce the capacity of biological reactors as well as increasing the volume of sludge and changing its composition.\n\nThe principal damage sustained by aircraft flying into a volcanic ash cloud is abrasion to forward-facing surfaces, such as the windshield and leading edges of the wings, and accumulation of ash into surface openings, including engines. Abrasion of windshields and landing lights will reduce visibility forcing pilots to rely on their instruments. However, some instruments may provide incorrect readings as sensors (e.g., pitot tubes) can become blocked with ash. Ingestion of ash into engines causes abrasion damage to compressor fan blades. The ash erodes sharp blades in the compressor, reducing its efficiency. The ash melts in the combustion chamber to form molten glass. The ash then solidifies on turbine blades, blocking air flow and causing the engine to stall.\n\nThe composition of most ash is such that its melting temperature is within the operating temperature (>1000 °C) of modern large jet engines. The degree of impact depends upon the concentration of ash in the plume, the length of time the aircraft spends within the plume and the actions taken by the pilots. Critically, melting of ash, particularly volcanic glass, can result in accumulation of resolidified ash on turbine nozzle guide vanes, resulting in compressor stall and complete loss of engine thrust. The standard procedure of the engine control system when it detects a possible stall is to increase power which would exacerbate the problem. It is recommended that pilots reduce engine power and quickly exit the cloud by performing a descending 180° turn. Volcanic gases, which are present within ash clouds, can also cause damage to engines and acrylic windshields, although this damage may not surface for many years.\n\nThere are many instances of damage to jet aircraft as a result of an ash encounter. On 24 June 1982 a British Airways Boeing 747-236B (Flight 9) flew through the ash cloud from the eruption of Mount Galunggung, Indonesia resulting in the failure of all four engines. The plane descended 24,000 feet (7,300 m) in 16 minutes before the engines restarted, allowing the aircraft to make an emergency landing. On 15 December 1989 a KLM Boeing 747-400 (Flight 867) also lost power to all four engines after flying into an ash cloud from Mount Redoubt, Alaska. After dropping 14,700 feet (4,500 m) in four minutes, the engines were started just 1–2 minutes before impact. Total damage was US$80 million and it took 3 months' work to repair the plane. In the 1990s a further US$100 million of damage was sustained by commercial aircraft (some in the air, others on the ground) as a consequence of the 1991 eruption of Mount Pinatubo in the Philippines.\n\nIn April 2010 airspace all over Europe was affected, with many flights cancelled-which was unprecedented-due to the presence of volcanic ash in the upper atmosphere from the eruption of the Icelandic volcano Eyjafjallajökull. On 15 April 2010 the Finnish Air Force halted training flights when damage was found from volcanic dust ingestion by the engines of one of its Boeing F-18 Hornet fighters. On 22 April 2010 UK RAF Typhoon training flights were also temporarily suspended after deposits of volcanic ash were found in a jet's engines. In June 2011 there were similar closures of airspace in Chile, Argentina, Brazil, Australia and New Zealand, following the eruption of Puyehue-Cordón Caulle, Chile.\n\nVolcanic ash clouds are very difficult to detect from aircraft as no onboard cockpit instruments exist to detect them. However, a new system called Airborne Volcanic Object Infrared Detector (AVOID) has recently been developed by Nicarnica Aviation, a daughter company of the Norwegian Institute for Air Research, which will allow pilots to detect ash plumes up to 100 km (62 mi) ahead and fly safely around them. The system uses two fast-sampling infrared cameras, mounted on a forward-facing surface, that are tuned to detect volcanic ash. This system can detect ash concentrations of <1 mg/m to > 50 mg/m, giving pilots approximately 7–10 minutes warning. The camera is currently being tested by easyJet airline company.\n\nIn addition, ground and satellite based imagery, radar, and lidar can be used to detect ash clouds. This information is passed between meteorological agencies, volcanic observatories and airline companies through Volcanic Ash Advisory Centers (VAAC). There is one VAAC for each of the nine regions of the world. VAACs can issue advisories describing the current and future extent of the ash cloud.\n\nVolcanic ash not only affects in-flight operations but can affect ground-based airport operations as well. Small accumulations of ash can reduce visibility, create slippery runways and taxiways, infiltrate communication and electrical systems, interrupt ground services, damage buildings and parked aircraft. Ash accumulation of more than a few millimeters requires removal before airports can resume full operations. Ash does not disappear (unlike snowfalls) and must be disposed of in a manner that prevents it from being remobilised by wind and aircraft.\n\nAsh may disrupt transportation systems over large areas for hours to days, including roads and vehicles, railways and ports and shipping. Falling ash will reduce the visibility which can make driving difficult and dangerous. In addition, fast travelling cars will stir up ash, creating billowing clouds which perpetuate ongoing visibility hazards. Ash accumulations will decrease traction, especially when wet, and cover road markings. Fine-grained ash can infiltrate openings in cars and abrade most surfaces, especially between moving parts. Air and oil filters will become blocked requiring frequent replacement. Rail transport is less vulnerable, with disruptions mainly caused by reduction in visibility.\n\nMarine transport can also be impacted by volcanic ash. Ash fall will block air and oil filters and abrade any moving parts if ingested into engines. Navigation will be impacted by a reduction in visibility during ash fall. Vesiculated ash (pumice and scoria) will float on the water surface in ‘pumice rafts’ which can clog water intakes quickly, leading to over heating of machinery.\n\nTelecommunication and broadcast networks can be affected by volcanic ash in the following ways: attenuation and reduction of signal strength; damage to equipment; and overloading of network through user demand. Signal attenuation due to volcanic ash is not well documented; however, there have been reports of disrupted communications following the 1969 Surtsey eruption and 1991 Mount Pinatubo eruption. Research by the New Zealand-based Auckland Engineering Lifelines Group determined theoretically that impacts on telecommunications signals from ash would be limited to low frequency services such as satellite communication. Signal interference may also be caused by lightning, as this is frequently generated within volcanic eruption plumes.\n\nTelecommunication equipment may become damaged due to direct ash fall. Most modern equipment requires constant cooling from air conditioning units. These are susceptible to blockage by ash which reduces their cooling efficiency. Heavy ash falls may cause telecommunication lines, masts, cables, aerials, antennae dishes and towers to collapse due to ash loading. Moist ash may also cause accelerated corrosion of metal components.\n\nReports from recent eruptions suggest that the largest disruption to communication networks is overloading due to high user demand. This is common of many natural disasters.\n\nComputers may be impacted by volcanic ash, with their functionality and usability decreasing during ashfall, but it is unlikely they will completely fail. The most vulnerable components are the mechanical components, such as cooling fans, cd drives, keyboard, mice and touch pads. These components can become jammed with fine grained ash causing them to cease working; however, most can be restored to working order by cleaning with compressed air. Moist ash may cause electrical short circuits within desktop computers; however, will not affect laptop computers.\n\nDamage to buildings and structures can range from complete or partial roof collapse to less catastrophic damage of exterior and internal materials. Impacts depend on the thickness of ash, whether it is wet or dry, the roof and building design and how much ash gets inside a building. The specific weight of ash can vary significantly and rain can increase this by 50-100%. Problems associated with ash loading are similar to that of snow; however, ash is more severe as 1) the load from ash is generally much greater, 2) ash does not melt and 3) ash can clog and damage gutters, especially after rain fall. Impacts for ash loading depend on building design and construction, including roof slope, construction materials, roof span and support system, and age and maintenance of the building. Generally flat roofs are more susceptible to damage and collapse than steeply pitched roofs. Roofs made of smooth materials (sheet metal or glass) are more likely to shed ash than roofs made with rough materials (thatch, asphalt or wood shingles). Roof collapse can lead to widespread injuries and deaths and property damage. For example, the collapse of roofs from ash during the 15 June 1991 Mount Pinatubo eruption killed about 300 people.\n\nAsh particles of less than 10 µm diameter suspended in the air are known to be inhalable, and people exposed to ash falls have experienced respiratory discomfort, breathing difficulty, eye and skin irritation, and nose and throat symptoms. Most of these effects are short-term and are not considered to pose a significant health risk to those without pre-existing respiratory conditions. The health effects of volcanic ash depend on the grain size, mineralogical composition and chemical coatings on the surface of the ash particles. Additional factors related to potential respiratory symptoms are the frequency and duration of exposure, the concentration of ash in the air and the respirable ash fraction; the proportion of ash with less than 10 µm diameter, known as PM. The social context may also be important.\n\nChronic health effects from volcanic ash fall are possible, as exposure to free crystalline silica is known to cause silicosis. Minerals associated with this include quartz, cristobalite and tridymite, which may all be present in volcanic ash. These minerals are described as ‘free’ silica as the SiO is not attached to another element to create a new mineral. However, magmas containing less than 58% SiO are thought to be unlikely to contain crystalline silica.\n\nThe exposure levels to free crystalline silica in the ash are commonly used to characterise the risk of silicosis in occupational studies (for people who work in mining, construction and other industries,) because it is classified as a human carcinogen by the International Agency for Research on Cancer. Guideline values have been created for exposure, but with unclear rationale; UK guidelines for particulates in air (PM10) are 50 µg/m and USA guidelines for exposure to crystalline silica are 50 µg/m. It is thought that the guidelines on exposure levels could be exceeded for short periods of time without significant health effects on the general population.\n\nThere have been no documented cases of silicosis developed from exposure to volcanic ash. However, long-term studies necessary to evaluate these effects are lacking.\n\nIngesting ash may be harmful to livestock, causing abrasion of the teeth, and in cases of high fluorine content, fluorine poisoning (toxic at levels of >100 µg/g) for grazing animals. It is known from the 1783 eruption of Laki in Iceland that fluorine poisoning occurred in humans and livestock as a result of the chemistry of the ash and gas, which contained high levels of Hydrogen Fluoride. Following the 1995/96 Mount Ruapehu eruptions in New Zealand, two thousand ewes and lambs died after being affected by fluorosis while grazing on land with only 1–3 mm of ash fall. Ash ingestion may also cause gastrointestinal blockages. Sheep that ingested ash from the 1991 Mount Hudson volcanic eruption in Chile, suffered from diarrhoea and weakness. The added weight of ash in the wool led to fatigue and sheep could not stand up.\n\nVolcanic ash can have a detrimental impact on the environment which can be difficult to predict due to the large variety of environmental conditions that exist within the ash fall zone. Natural waterways can be impacted in the same way as urban water supply networks. Ash will increase water turbidity which can reduce the amount of light reaching lower depths, which can inhibit growth of submerged aquatic plants and consequently affect species which are dependent on them such as fish and shellfish. High turbidity can also affect the ability of fish gills to absorb dissolved oxygen. Acidification will also occur, which will reduce the pH of the water and impact the fauna and flora living in the environment. Fluoride contamination will occur if the ash contains high concentrations of fluoride.\n\nAsh accumulation will also affect pasture, plants and trees which are part of the horticulture and agriculture industries. Thin ash falls (<20 mm) may put livestock off eating, and can inhibit transpiration and photosynthesis and alter growth. There may be an increase in pasture production due to a mulching effect and slight fertilizing effect, such as occurred following the 1980 Mount St. Helens and 1995/96 Mt Ruapehu eruptions. Heavier falls will completely bury pastures and soil leading to death of pasture and sterilization of the soil due to oxygen deprivation. Plant survival is dependent on ash thickness, ash chemistry, compaction of ash, amount of rainfall, duration of burial and the length of plant stalks at the time of ash fall. The acidic nature of ash will lead to elevated soil sulfur levels and lowered soil pH, which can reduce the availability of essential minerals and alter the soil’s characteristics so that crops and plants will not survive. Ash will also impact upon arable crops, such as fruit, vegetables and grain. Ash can burn plant and crop tissue reducing quality, contaminate crops during harvest and damage plants from ash loading.\n\nYoung forests (trees <2 years old) are most at risk from ash falls and are likely to be destroyed by ash deposits >100 mm. Ash fall is unlikely to kill mature trees, but ash loading may break large branches during heavy ash falls (>500 mm). Defoliation of trees may also occur, especially if there is a coarse ash component within the ash fall.\n\nLand rehabilitation after ash fall may be possible depending on the ash deposit thickness. Rehabilitation treatment may include: direct seeding of deposit; mixing of deposit with buried soil; scraping of ash deposit from land surface; and application of new topsoil over the ash deposit.\n\nCritical infrastructure and infrastructure services are vital to the functionality of modern society, to provide: medical care, policing, emergency services, and lifelines such as water, wastewater, and power and transportation links. Often critical facilities themselves are dependent on such lifelines for operability, which makes them vulnerable to both direct impacts from a hazard event and indirect effects from lifeline disruption.\n\nThe impacts on lifelines may also be inter-dependent. The vulnerability of each lifeline may depend on: the type of hazard, the spatial density of its critical linkages, the dependency on critical linkages, susceptibility to damage and speed of service restoration, state of repair or age, and institutional characteristics or ownership.\n\nThe 2010 eruption of Eyjafjallajokull in Iceland highlighted the impacts of volcanic ash fall in modern society and our dependence on the functionality of infrastructure services. During this event the airline industry suffered business interruption losses of €1.5-2.5 billion from the closure of European airspace for six days in April 2010 and subsequent closures into May 2010. Ash fall from this event is also known to have caused local crop losses in agricultural industries, losses in the tourism industry, destruction of roads and bridges in Iceland (in combination with glacial melt water), and costs associated with emergency response and clean-up. However, across Europe there were further losses associated with travel disruption, the insurance industry, the postal service, and imports and exports across Europe and worldwide. These consequences demonstrate the interdependency and diversity of impacts from a single event.\n\nPreparedness for ashfalls should involve sealing buildings, protecting infrastructure and homes, and storing sufficient supplies of food and water to last until the ash fall is over and clean-up can begin. Dust masks can be worn to reduce inhalation of ash and mitigate against any respiratory health affects. Goggles can be worn to protect against eye irritation.\n\nThe International Volcanic Ashfall Impacts Working Group of IAVCEI maintains a regularly updated database of impacts and mitigations strategies.\n\nAt home, staying informed about volcanic activity, and having contingency plans in place for alternative shelter locations, constitutes good preparedness for an ash fall event. This can prevent some impacts associated with ash fall, reduce the effects, and increase the human capacity to cope with such events. A few items such as a flashlight, plastic sheeting to protect electronic equipment from ash ingress, and battery operated radios, are extremely useful during ash fall events.\n\nThe protection of infrastructure must also be considered within emergency preparedness. Critical facilities that need to remain operable should be identified, and all others should be shut down to reduce damage. It is also important to keep ash out of buildings, machinery and lifeline networks (in particular water and wastewater systems,) to prevent some of the damage caused by ash particles. Windows and doors should be closed and shuttered if possible, to prevent ingress of ash into buildings.\n\nCommunication plans should be made beforehand to inform of mitigation actions being undertaken. Spare parts and back-up systems should be in place prior to ash fall events to reduce service disruption and return functionality as quickly as possible. Good preparedness also includes the identification of ash disposal sites, before ash fall occurs, to avoid further movement of ash and to aid clean-up. Protective equipment such as eye protection and dust masks should be deployed for clean-up teams in advance of ash fall events.\n\nSome effective techniques for the management of ash have been developed including cleaning methods and cleaning apparatus, and actions to mitigate or limit damage. The latter include covering of openings such as: air and water intakes, aircraft engines and windows during ash fall events. Roads may be closed to allow clean-up of ash falls, or speed restrictions may be put in place, in order to prevent motorists from developing motor problems and becoming stranded following an ash fall. To prevent further effects on underground water systems or waste water networks, drains and culverts should be unblocked and ash prevented from entering the system. Ash can be moistened (but not saturated) by sprinkling with water, to prevent remobilisation of ash and to aid clean-up. Prioritisation of clean-up operations for critical facilities and coordination of clean-up efforts also constitute good management practice.\n\nVolcanic ash's primary use is that of a soil enricher. Once the minerals in ash are washed into the soil by rain or other natural processes, it mixes with the soil to create an andisol layer. This layer is highly rich in nutrients and is very good for agricultural use; the presence of lush forests on volcanic islands is often as a result of trees growing and flourishing in the phosphorus and nitrogen-rich andisol.\n\n\n"}
{"id": "23580615", "url": "https://en.wikipedia.org/wiki?curid=23580615", "title": "Wood Awards", "text": "Wood Awards\n\nThe Wood Awards (until 2003 the Carpenters' Award) is a British award for working with wood. The award, which was launched in 1971, is bestowed on winners of several categories within buildings and furniture. Awards are presented in The Carpenters Hall following the decision of the architects, engineers, furniture designers / makers, timber specialists and architectural journalists who judge the competition. The Awards are sponsored by several commercial organisations and the Worshipful Company of Carpenters. \n\nEach year there is one winner and one \"Highly Commended\" project in seven categories, and a \"Gold Award\" for the best of the seven category winners. \n\nA list of winners and highly commended projects, 2008-, is available online. \n\n"}
