{"id": "39496493", "url": "https://en.wikipedia.org/wiki?curid=39496493", "title": "(285263) 1998 QE2", "text": "(285263) 1998 QE2\n\n, provisional designation , is a dark asteroid and synchronous binary system, classified as near-Earth object and potentially hazardous asteroid of the Amor group, approximately 3 kilometers in diameter. It was discovered on 19 August 1998, by astronomers of the LINEAR program at Lincoln Laboratory's Experimental Test Site near Socorro, New Mexico, in the United States. Its sub-kilometer minor-planet moon was discovered by radar on 30 May 2013. \n\nAs an Amor asteroid the orbit of is entirely beyond Earth's orbit. The asteroid orbits the Sun at a distance of 1.0–3.8 AU once every 3 years and 9 months (1,378 days; semi-major axis of 2.42 AU). Its orbit has an eccentricity of 0.57 and an inclination of 13° with respect to the ecliptic. The \"Earth minimum orbit intersection distance\" with the orbit of the asteroid is , which translates into 13.4 lunar distances. As with many members of the Amor group, this asteroid has an aphelion beyond the orbit of Mars (at 1.66 AU) which also makes it a Mars-crosser.\n\nThe sooty surface of suggested that it might have previously been a comet that experienced a close encounter with the Sun. However, the Tisserand parameter with respect to Jupiter (T=3.2) does not make it obvious whether was ever a comet, since cometary T values are typically below 3.\n\nOn May 31, 2013, approached within (15 lunar distances) of Earth at 20:59 UT (4:59 pm EDT). This was the closest approach the asteroid will make to Earth for at least the next two centuries. It is a very strong radar target for Goldstone from May 30 to June 9 and will be one for Arecibo from June 6 to June 12. At its closest approach the asteroid had an apparent magnitude of 11 and therefore required a small telescope to be seen.\n\nIntegrating the orbital solution shows the asteroid passed from Earth on June 8, 1975, with an apparent magnitude of about 13.9. The next notable close approach will be May 27, 2221, when the asteroid will pass Earth at a distance of .\n\nGoldstone radar observations on May 29, 2013 discovered that is orbited by a minor-planet moon approximately 600–800 meters in diameter. In radar images, the satellite appears brighter than because it is rotating significantly more slowly, which compresses the radar return of the satellite along the Doppler axis. This makes the satellite appear narrow and bright compared to . The satellite orbits the primary every 32 hours with a maximum separation of . Once the satellite's orbit is well determined, astronomers and astrophysicists will be able to determine the mass and density of .\n\nThe surface of is covered with a sooty substance, making it optically dark with a geometric albedo of 0.06, meaning it absorbs 94% of the light that hits it, which is indicative for a carbonaceous surface of a C-type asteroid. The asteroid is covered with craters and is dark, red, and primitive.\n\nWith a diameter between 2.7 and 3.2 kilometers, is one of largest known potentially hazardous asteroid \"(see PHA-list)\". Conversely, the \"Collaborative Asteroid Lightcurve Link\" assumes a standard albedo for stony asteroids of 0.20 and calculates a diameter of 1.08 kilometers based on an absolute magnitude of 17.2.\n\n"}
{"id": "46474111", "url": "https://en.wikipedia.org/wiki?curid=46474111", "title": "ANEMOON Foundation", "text": "ANEMOON Foundation\n\nThe ANEMOON Foundation, in Dutch Stichting ANEMOON, is a foundation dedicated to the study of marine life in the Netherlands. \"Anemoon\" is a Dutch word that literally means anemone, as in sea anemone, a marine organism portrayed in the logo of the foundation, although here the word is used as an acronym, based on the Dutch words \"ANalyse Educatie Marien Oecologisch ONderzoek\", meaning \"analysis, education and marine ecological research\". ANEMOON was founded in 1993 to focus on encouraging and supporting research on marine fauna and flora that is carried out by volunteers, i.e. citizen scientists. ANEMOON is one of ten non-governmental data-managing organizations (PGOs, Particuliere Gegevensbeherende Organisaties) which compile data on the fauna and flora of the Netherlands; these ten PGOs feed data into the \"Foundation for Flora and Fauna Research\" and the \"\", or NDFF, a conservation body.\n\n"}
{"id": "23567977", "url": "https://en.wikipedia.org/wiki?curid=23567977", "title": "Ballast water discharge and the environment", "text": "Ballast water discharge and the environment\n\nBallast water discharges by ships can have a negative impact on the marine environment. The discharge of ballast water and sediments by ships is governed globally under the Ballast Water Management Convention, since its entry into force in September 2017. It is also controlled through national regulations, which may be separate from the Convention, such as in the United States.\n\nCruise ships, large tankers, and bulk cargo carriers use a huge amount of ballast water, which is often taken on in the coastal waters in one region after ships discharge wastewater or unload cargo, and discharged at the next port of call, wherever more cargo is loaded. Ballast water discharge typically contains a variety of biological materials, including plants, animals, viruses, and bacteria. These materials often include non-native, nuisance, exotic species that can cause extensive ecological and economic damage to aquatic ecosystems, along with serious human health issues including death.\n\nThere are hundreds of organisms carried in ballast water that cause problematic ecological effects outside of their natural range. The International Maritime Organization (IMO) lists the ten most unwanted species as:\n\nThe ballast tanks in New Zealand carry animals and plants that kill ecosystems. Ballast tanks are only used in cargo ships there. Ballast water is controlled under the Biosecurity Act 1993.\n\nA form of cholera, \"Vibrio cholerae\", previously reported only in Bangladesh apparently arrived via ballast water in Peru in 1991, killing more than 10,000 people over the following three years.\n\nThe zebra mussel, which is native to the Caspian and Black Seas, arrived in Lake St. Clair in the ballast water of a transatlantic freighter in 1988. Within 10 years it had spread to all of the five neighbouring Great Lakes. The economic cost of this introduction has been estimated by the U.S. Fish and Wildlife Service at about $5 billion.\n\nBallast water discharges are believed to be the leading source of invasive species in U.S. marine waters, thus posing public health and environmental risks, as well as significant economic cost to industries such as water and power utilities, commercial and recreational fisheries, agriculture, and tourism. Studies suggest that the economic cost just from introduction of pest mollusks (zebra mussels, the Asian clam, and others) to U.S. aquatic ecosystems is more than $6 billion per year.\n\nCongress passed the National Invasive Species Act in 1996 in order to regulate ballast water discharges. The Coast Guard issued ballast water regulations in 2012. Under the authority of the Clean Water Act, the Environmental Protection Agency (EPA) published its latest \"Vessel General Permit\" in 2013. The permit sets numeric ballast water discharge limits for commercial vessels in length or greater. EPA issued a separate permit for smaller commercial vessels in 2014.\n\nAmong 818 ports in the Pacific region, Singapore alone accounts for an estimated of 26 percent of cross-region (long range) species exchange. Via targeted ballast management on Singapore and a few other \"influential\" ports, cross-region species exchange to/from the Pacific region can be combinatorially reduced.\n\nTo react to the growing concerns about environmental impact of ballast water discharge, the International Maritime Organization (IMO) adopted in 2004 the \"International Convention for the Control and Management of Ships' Ballast Water and Sediments\" to control the environmental damage from ballast water.\nThe Convention will require all ships to implement a \"Ballast water management plan\" including a ballast water record book and carrying out ballast water management procedures to a given standard.\nGuidelines are given for additional measures then the guidelines.\n\nThe goals of the convention are to minimise damage to the environment by:\n\nControl measures include:\n\nThe IMO convention was ratified by enough countries and entered into force on September 8, 2017.\n\n\n"}
{"id": "41799316", "url": "https://en.wikipedia.org/wiki?curid=41799316", "title": "Barack Obama on mass surveillance", "text": "Barack Obama on mass surveillance\n\nU.S. president Barack Obama has received widespread criticism due to his support of government surveillance. President Obama had released many statements on mass surveillance as a result.\n\nAs a senator, Obama condemned the Patriot Act for violating the rights of American citizens. He argued that it allowed government agents to perform extensive and in-depth searches on American citizens without a search warrant. He also argued that it was possible to secure the United States against terrorist attacks while preserving individual liberty. In 2011, Obama signed a four-year renewal of the Patriot Act, specifically provisions allowing roaming wiretaps and government searches of business records. Obama argued that the renewal was needed to protect the United States from terrorist attacks. However, the renewal was criticized by several members of Congress who argued that the provisions did not do enough to curtail excessive searches. Obama also received criticism for his reversal on privacy protection.\n\nIn June 2013, reports from a cache of top secret documents leaked by ex-NSA contractor Edward Snowden revealed that the U.S. National Security Agency (NSA) and its international partners had created a global system of surveillance that was responsible for the mass collection of information on American and foreign citizens.\n\nObama initially defended NSA mass surveillance programs when they were first leaked. He argued that NSA surveillance was transparent and claimed that the NSA is unable and had made no attempt to monitor the phone calls and e-mails of American citizens. Following Snowden's admittance to leaking classified documents regarding national surveillance, Obama attempted to ignore the issue of NSA surveillance. It was speculated that Obama did this to avoid complicating the Department of Justice investigation into Snowden.\n\nIn August 2013, Obama argued that his administration was already in the process of reviewing the NSA surveillance programs when they were leaked by Snowden. Obama stated that it would have been best for the American people to have never learned about the programs. He also criticized Snowden for not using existing systems within the federal government for whistleblowers. The latter statement was criticized as Snowden would have been directed to one of the committees responsible for protecting the secrecy of NSA surveillance if he had used the existing whistle-blower system. However, he also promised to make public information about government surveillance and work with Congress to increase public confidence in the government.\n\nOn January 17, 2014, President Obama gave a public address on mass surveillance. \n\nObama's speech was criticized for being deliberately vague and not going far enough to protect civil liberties.\n\nRepresentatives for Google, Facebook and Yahoo stated that Obama's proposed reforms represented positive progress, but that they did not ultimately do enough to protect privacy rights. A representative for Mozilla noted that mass surveillance had damaged the open Internet and caused balkanization and distrust.\n\nSen. Rand Paul criticized the remarks, saying: \n\nDianne Feinstein, a member of the Senate Intelligence Committee, stated that all but two or three of the members of her committee support Obama. Likewise, she criticized \"privacy people\" for not understanding the threat terrorists pose to the United States. Mike Rogers, the chair of the House Intelligence Committee, praised Obama's stance on NSA surveillance. Peter King, another member of the House Intelligence Committee, questioned the need for the proposed reform of NSA surveillance, but admitted that they were necessary to calm down the \"ACLU types\".\n\nReactions from global leaders were limited. Great Britain and Russia, both states with extensive surveillance programs, offered no comments. Dilma Rousseff, the current president of Brazil and an outspoken critic of NSA surveillance, also refused to comment. In Germany, a government spokesperson demanded greater protection for non-Americans in reaction to the speech. Der Spiegel accused the NSA of turning the internet into a weapons system. The European Union stated that Obama's pledge to reform the phone data collection is a step in the right direction, but demanded that actual laws be passed regarding this reform.\n\nThe Electronic Frontier Foundation and The Day We Fight Back released a report card\" evaluating Obama's reform: \n\nA full point was awarded in each category where Obama fully made the promised reform. However, partial points were awarded for reforms that had not been fully completed, but where the EFF and The Day We Fight Back felt that progress as being made. Obama received praise for adding independent advocates to the Foreign Intelligence Surveillance Act (FISA) courts and opposing the FISA Improvements Act. However, it was also noted that Obama had not made any progress on giving metadata storage responsibility to a third party, ending the undermining of encryption standards, increasing transparency within the NSA and protecting whistleblowers.\n\nOn January 18, Obama spoke to ZDF in an attempt to improve the United States relations with Germany, which a German foreign office official said were \"worse than … the low-point in 2003 during the Iraq War\" due to the surveillance leaks. Obama promised that he would not let revelations about mass surveillance damage German-American relations and admitted that it would take a long time for the United States to regain the trust of the German people. However, he maintained that the surveillance was necessary for international security.\n\nGerman reactions to the speeches given by Obama on January 17 and 18 ranged from skeptical to outright hostile. Members of the German media argued that they were hopeful that Obama would bring about needed reform. However, they also noted that his statements were vague and argued that they did not represent legitimate reform. Many German political leaders responded with outright hostility. Thomas Oppermann, the chairman of the German Social Democrats, demanded a no-spy treaty and stated that American surveillance constituted a crime. The German attorney general argued that there were grounds for a criminal investigation into the NSA's tapping of Angela Merkel's cell phone.\n\nOn March 25, 2014, Obama promised to end the NSA's collection and storage of bulk phone-call data. Despite this promise, his administration continued to seek reauthorization of the telephone metadata program. It is approved every 90 days by the FISC, with the most recent authority set to expire June 1, 2015. In a plan submitted by the Obama Administration to Congress, the NSA would be required to conduct searches of data at phone companies. They would also need to receive a warrant from a federal judge to conduct the search.\n\nThe overhaul proposal received support from the American Civil Liberties Union. A representative of the organization claimed that it was a crucial first step in reining in NSA surveillance. The overhaul was criticized by several officials, however, because it would force telephone carriers to store customers metadata that they previously not legally obligated to keep. Reactions from the carriers were generally positive. A representative of Sprint Corporation stated that the carrier was examining the president's proposal with great interest.\n\nAs of March 2015, the administration's proposals have not been implemented and the NSA retains the authority to collect and store telephone record metadata.\n\nOn May 24, 2017, a declassified FISA report marked \"Top Secret\" was published, noting that the NSA routinely violated the 4th Amendment of Americans and abused intelligence tools to do so. The Obama administration self-disclosed the problems at a closed-door hearing on Oct. 26 before the Foreign Intelligence Surveillance Court, two weeks before the 2016 election. The report labeled the matter a “very serious Fourth Amendment issue,\" citing an \"institutional lack of candor\" on the part of the administration. It also criticized the NSA as having “disregard” for rules and “deficient” oversight.\n\nMore than 5 percent, or one out of every 20 searches seeking upstream Internet data on Americans inside the NSA’s so-called Section 702 database, violated the safeguards the Obama administration vowed to follow in 2011. In addition, there was a three-fold increase in NSA data searches about Americans and a rise in the unmasking of U.S. person’s identities in intelligence reports after the administration loosened privacy rules in 2011. Many of the searches involved any and all mentions of foreign targets.\n\nOfficials like former National Security Adviser Susan Rice have argued their activities were legal under the so-called minimization rule changes the Obama administration made, and that the intelligence agencies were strictly monitored to avoid abuses. The FISA court and the NSA's own internal watchdog entity disputes this claim, stating that the administration conducting such queries were \"in violation of that prohibition, with much greater frequency than had been previously disclosed to the Court.”\n\nThe FISA report also indicated hundreds of incidences in which the FBI illegally shared raw surveillance data illegally obtained by the NSA with private entities. Earlier in May, then-FBI Director James Comey told lawmakers his agency used sensitive espionage data gathered about Americans without a warrant only when it was “lawfully collected, carefully overseen and checked.” The ruling in the report declared that “The Court is nonetheless concerned about the FBI’s apparent disregard of minimization rules and whether the FBI is engaging in similar disclosures of raw Section 702 information that have not been reported.”\n\nIn a declassified report from 2015, the internal watchdog had concerns as early as 2012 that the FBI was submitting \"deficient” reports indicating it had a clean record complying with spy data gathered on Americans without a warrant. While Section 702 of the Foreign Surveillance Act, last updated by Congress in 2008, allowed the NSA to share with the FBI spy data collected without a warrant, the FISA report indicates FBI compliance problems began months after the updated legislation was implemented. The FBI’s very first compliance report in 2009 declared it had not found any instances in which agents accessed NSA intercepts supposedly gathered overseas about an American who in fact was on U.S. soil. The Inspector General, however, said it reviewed the same data and easily found evidence that the FBI accessed NSA data gathered on a person who likely was in the United States, making it illegal to review without a warrant.\n\nOn April 28, the NSA issued a rare press release indicating it will no longer monitor all internet communications that mention a foreign intelligence target.\n\nNeema Singh Guliani, the ACLU's legislative counsel in Washington, DC stated, “I think what this emphasizes is the shocking lack of oversight of these programs.\" Chris Farrell, Director of Investigations for the watchdog group Judicial Watch asserted, \"This is an abuse of power and authority like we have never seen in this country.\"\n\n"}
{"id": "29400312", "url": "https://en.wikipedia.org/wiki?curid=29400312", "title": "Biocapacity", "text": "Biocapacity\n\nThe biocapacity or biological capacity of an ecosystem is an estimate of its production of certain biological materials such as natural resources, and its absorption and filtering of other materials such as carbon dioxide from the atmosphere. “Useful biological \n\nBiocapacity is expressed in terms of global hectares per person, thus is dependent on human population. A global hectare is an adjusted unit that represents the average biological productivity of all productive hectares on Earth in a given year (because not all hectares produce the same amount of ecosystem services). Biocapcity is calculated from United Nations population and land use data, and may be reported at various regional levels, such as a city, a country, or the world as a whole. \n\nFor example, there were 12 billion hectares of biologically productive land and water on this planet in 2008. Dividing by the number of people alive in that year, 6.7 billion, gives a biocapacity of 1.8 global hectares per person . This assumes that no land is set aside for other species that consume the same biological material as humans.\n\nBiocapacity is used together with Ecological Footprint as a method of measuring Human impact on the environment. Biocapacity and Ecological Footprint are tools created by the Global Footprint Network, used in sustainability studies around the world.\n\nAn increase in global population can result in a decrease in biocapacity. This is usually due to the fact that the Earth’s resources have to be shared; therefore, there becomes little to supply the increasing demand of the increasing population. Currently, this issue can be resolved by outsourcing. However, resources will run out due to the increasing demands and as a result a collapse of an ecosystem can be the consequence of such actions. When the ecological footprint becomes greater than the biocapacity of the population, a biocapacity deficit is suspected.\n'Global biocapacity' is a term sometimes used to describe the total capacity of an ecosystem to support various continuous activity and changes. When the ecological footprint of a population exceeds the biocapacity of the environment it lives in, this can be called an 'ecological overshoot'. A 2008 report stated that people were using an equivalence of 1.5 Earths to compensate for their needs. However other sources have suggested that depletion of cropland, grazing land, forest land, fishing grounds, and built-up land is not occurring on an aggregate, global level. Hence, virtually all of the ecological overshoot comes from the measure of the rate at which carbon dioxide is accumulating in the atmosphere. Additional stresses of greenhouse gases, climate change, and ocean acidification can also aggravate the problem. In reference to the definition of biocapacity: 1.5 Earths means renewable resources will eventually result in depletion because they are being produced faster and more often than the resources can be re-grown. Therefore, it will one year and six months for the resources we use to be able to regenerate again and in addition absorb all the waste we manufactured as well. So instead of taking one year, we are now in-taking enough resources that should last us one year and six months.\n\nIn addition, if this matter becomes severe, an ecological reserve will be set on areas to preserve their ecosystems. Awareness about our depleting resources include: agricultural land, forest resources and rangeland. Biocapacity used in correlation to ecological footprint can therefore suggest whether a specific population, region, country or part of a world is living in the means of their capital. Accordingly,§98 the study of biocapacity and ecological footprint is known as the Ecological Footprint Analysis (EFA).\n\nBiocapacity is also affected on the technology used during the year. With new technologies emerging, it is not whether the technology in that year is good or bad but whether how the technology impacts resource supply and demand; which in return affects biocapacity. Hence what is considered “useful” can change from year to year (e.g. use of corn (maize) stover for cellulosic ethanol production would result in corn stover becoming a useful material, and thus increase the biocapacity of maize cropland).\n\nMoreover, environmentalists have created ecological footprint calculators for a single person(s) to determine whether they are encompassing more than what is available for them in their population. Consequently, biocapacity results will be applied to their ecological footprint to determine how much they may contribute or take away from sustainable development.\n\nIn general, biocapacity is the amount of resources available to people at a specific moment in time to a specific population (supply) and to differentiate between ecological footprint – that is the demand constructed on behalf of a regional ecosystem. Biocapacity is able to determine the human impacts on Earth. By determining productivity of land, hence the resources available for human consumption, biocapacity will be able to predict and perhaps examine the effects on the ecosystems closely based on collected results of human consumption.\n.The biocapacity of an area is calculated by multiplying the actual physical area by the yield factor and the appropriate equivalence factor. Biocapacity is usually expressed in global hectares (gha). Since global hectares is able to convert human consumptions like food and water into a measurement, biocapacity can be applied to determine the carrying capacity of the Earth.\n\n\n"}
{"id": "13478480", "url": "https://en.wikipedia.org/wiki?curid=13478480", "title": "Biological exponential growth", "text": "Biological exponential growth\n\nBiological exponential growth is the exponential growth of biological organisms. When the resources availability is unlimited in the habitat, the population of an organism living in the habitat grows in an exponential or geometric fashion. Population growth in which the number of individuals increase by a constant multiple in each generation. The potential for population growth can be demonstrated in the laboratory under conditions that provide abundant resources and space. For example, a few fruit flies in a large culture jar containing an abundant food source may reproduce rapidly. One female fruit fly may lay more than 50 eggs. Reproductive adults develop in about 14 days, with approximately equal numbers of male and female offspring. For each female that began the population, 50 flies are expected 2 weeks later. Each female in the second generation produces 50 more flies after 2 more weeks, and so on. In other words, the population is experiencing exponential growth. Slow exponential growth is when a population grows slowly yet exponential because the population has long live spans. While a rapid exponential growth refers to a population that grows ( and dies ) rapidly because the population has short life spans.\n\nResource availability is obviously essential for the unimpeded growth of a population. Ideally, when resources in the habitat are unlimited, each species has the ability to realise fully its innate potential to grow in number, as Charles Darwin observed while developing his theory of natural selection.\n\nIf, in a hypothetical population of size \"N\", the birth rates (per capita) are represented as \"b\" and death rates (per capita) as \"d\", then the increase or decrease in \"N\" during a time period \"t\" will be:\n\nformula_1\n\n(b-d) is called the 'intrinsic rate of natural increase' and is a very important parameter chosen for assessing the impacts of any biotic or abiotic factor on population growth.\n\nAny species growing exponentially under unlimited resource conditions can reach enormous population densities in a short time. Darwin showed how even a slow growing animal like the elephant could reach an enormous population if there were unlimited resources for its growth in its habitat.\n\n"}
{"id": "18539917", "url": "https://en.wikipedia.org/wiki?curid=18539917", "title": "Computational geophysics", "text": "Computational geophysics\n\nComputational geophysics entails rapid numerical computations that help analyses of geophysical data and observations. High-performance computing is involved, due to the size and complexity of the geophysical data to be processed. The main computing demanding tasks are 3D and 4D images building of the earth sub-surface, Modeling and Migration of complex media, Tomography and inverse problems.\n\nIn Canada, Computational geophysics is offered as a university major in the form of a BSc (Hon.) with Co-op at Carleton University. Elsewhere, Rice University has a Center for Computational Geophysics, while Princeton University, the University of Texas and Albert-Ludwigs-Universität have similar programs. Seoul National University has offered postdoctoral positions in the field, while experts, laboratories, projects, internships, undergrad/graduate programs and/or facilities in the program exist at the University of Queensland, Wyoming University, Boston University, Stanford University, Uppsala University, Kansas State University, Kingston University, Australian National University, University of California, San Diego, University of Washington, ETH Zurich, University of Sydney, Appalachian State University, University of Minnesota, University of Tasmania, Bahria University, Boise State University, University of Michigan, University of Oulu, University of Utah, and others.\n\n"}
{"id": "15295920", "url": "https://en.wikipedia.org/wiki?curid=15295920", "title": "Conspectus of the ornithological fauna of the USSR", "text": "Conspectus of the ornithological fauna of the USSR\n\nConspectus of the ornithological fauna of the USSR () is a 1991 Russian language publication by ornithologist L. S. Stepanyan.\n\nIt contains a list of bird species recorded from the former Soviet Union, together with details of their distribution, taxonomic relationships and subspecies.\n\nThe following is a list of the species splits & lumps proposed by Stepanyan:\n\n\n"}
{"id": "11360081", "url": "https://en.wikipedia.org/wiki?curid=11360081", "title": "Earth's critical zone", "text": "Earth's critical zone\n\nEarth's critical zone is the “heterogeneous, near surface environment in which complex interactions involving rock, soil, water, air, and living organisms regulate the natural habitat and determine the availability of life-sustaining resources” (National Research Council, 2001). The Critical Zone, surface and near-surface environment, sustains nearly all terrestrial life.\n\nThe Critical Zone is an interdisciplinary field of research exploring the interactions among the land surface, vegetation, and water bodies, and extends through the pedosphere, unsaturated vadose zone, and saturated groundwater zone. Critical Zone science is the integration of Earth surface processes (such as landscape evolution, weathering, hydrology, geochemistry, and ecology) at multiple spatial and temporal scales and across anthropogenic gradients. These processes impact mass and energy exchange necessary for biomass productivity, chemical cycling, and water storage.\n\nThe Critical Zone is studied at Critical Zone Observatories,where multiple scientific communities study various aspects of the Critical Zone that can lead to synthesized understanding of complex systems.\n\nTsakalotos first introduced the term \"Critical Zone\" in chemistry literature to describe the binary mixture of two fluids, but has since been adopted to refer to the 'connection of vegetation to soil and weathered materials' by Gail Ashley.\n\nIn October 2003, scientists attended the first Weathering System Science Workshop. Participants agreed to promote outreach activities to broaden the profile of involved Earth scientists and crafted a set of questions that would drive further development of Weathering System Science. This field of science was considered to include all aspects of chemistry, biology, physics, and geology of the Critical Zone.\n\nThe Weather System Science Consortium was established in early 2004 and later was changed to the Critical Zone Exploration Network (CZEN) in 2006.\n\nIn October 2005 WSSC/CZEN solicitated a call for proposals to initiate seed sites that would help grow and establish a critical zone network. In tota, 10 sites have been awarded grants, and 9 sites have received continued funding.\n\nIn 2005 the University of Delaware hosted an NSF-sponsored workshop (Frontiers in Exploration of the Critical Zone) that resulted in a call by scientists for an international initiative to study the critical zone. As key parts of that initiative, the scientists called for the development of an international Critical Zone initiative and a systematic approach to the investigation of processes in the critical zone across a broad array of sciences, including geology, soil science, biology, ecology, chemistry, geochemistry, geomorphology and hydrology. A booklet (Frontiers in Exploration of the Critical Zone) was produced from the meeting.\n\nOn July 24, 2006 NSF posted a solicitation for proposals for Critical Zone Observatories (CZO) within the Division of Earth Sciences. See https://www.nsf.gov/funding/pgm_summ.jsp?pims_id=500044\n\nEstablished in 2009, the Delaware Environmental Institute (DENIN) is a multidisciplinary initiative bringing together scientists, engineers and policy specialists to provide solutions to pressing environmental needs and produce strategies to address emerging environmental challenges by conducting research and promoting and coordinating knowledge partnerships that integrate environmental science, engineering and policy. DENIN fosters a culture of scholarship that leverages the combined talents of affiliates and fellows through collaborative working groups, joint proposal development and, where synergistic, project resource coordination.\n\n"}
{"id": "492429", "url": "https://en.wikipedia.org/wiki?curid=492429", "title": "Electret", "text": "Electret\n\nAn electret (formed of \"electr-\" from \"electricity\" and \"-et\" from \"magnet\") is a dielectric material that has a quasi-permanent electric charge or dipole polarisation. An electret generates internal and external electric fields, and is the electrostatic equivalent of a permanent magnet. Although Oliver Heaviside coined this term in 1885, materials with electret properties were already known to science and had been studied since the early 1700s. One particular example is the electrophorus, a device consisting of a slab with electret properties and a separate metal plate. The electrophorus was originally invented by Johan Carl Wilcke in Sweden and again by Alessandro Volta in Italy.\n\nElectrets, like magnets, are dipoles. Another similarity is the radiant fields: They produce an electrostatic field (as opposed to a magnetic field) around their perimeter. When a magnet and an electret are near one another, a rather unusual phenomenon occurs: while stationary, neither has any effect on one another. However, when an electret is moved with respect to a magnetic pole, a force is felt which acts perpendicular to the magnetic field, pushing the electret along a path 90 degrees to the expected direction of 'push' as would be felt with another magnet.\n\nThere is a similarity between an electret and the dielectric layer used in capacitors; the difference is that dielectrics in capacitors have an induced polarisation that is only transient, dependent on the potential applied on the dielectric, while dielectrics with electret properties exhibit quasi-permanent charge storage or dipole polarisation in addition. Some materials also display ferroelectricity; i.e. they react to the external fields with a hysteresis of the polarisation; ferro-electrics can retain the polarisation permanently because they are in thermo-dynamic equilibrium, and are used in ferroelectric capacitors. Although electrets are only in a metastable state, those fashioned from very low leakage materials can retain excess charge or polarisation for many years. An electret microphone is a type of condenser microphone that eliminates the need for a power supply by using a permanently charged material.\n\nAn electret is a stable dielectric material with a quasi-permanently embedded static electric charge (which, due to the high resistance of the material, will not decay for time periods of up to hundreds of years) and/or a quasi-permanently oriented dipole polarisation. The name comes from electron (Greek word for amber) and magnet and was coined by Oliver Heaviside in 1885; drawing analogy to the formation of a magnet by alignment of magnetic domains in a piece of iron. Historically, electrets were made by first melting a suitable dielectric material such as a polymer or wax that contains polar molecules, and then allowing it to re-solidify in a powerful electrostatic field. The polar molecules of the dielectric align themselves to the direction of the electrostatic field, producing a dipole electret with a permanent electrostatic 'bias'. Modern electrets are usually made by embedding excess charges into a highly insulating dielectric, e.g. by means of an electron beam, a corona discharge, injection from an electron gun, electric breakdown across a gap or a dielectric barrier, etc.\n\nThere are two types of electrets:\n\nCellular space charge electrets with internal bipolar charges at the voids provide a new class of electret materials, that mimic ferroelectrics, hence they are known as ferroelectrets. Ferroelectrets display strong piezoelectricity, comparable to ceramic piezoelectric materials.\n\nSome dielectric materials are capable of acting both ways.\n\nElectret materials are quite common in nature. Quartz and other forms of silicon dioxide, for example, are naturally occurring electrets. Today, most electrets are made from synthetic polymers, e.g. fluoropolymers, polypropylene, polyethyleneterephthalate, etc. Real-charge electrets contain either positive or negative excess charges or both, while oriented-dipole electrets contain oriented dipoles. The quasi-permanent internal or external electric fields created by electrets can be exploited in various applications.\n\nBulk electrets can be prepared by cooling a suitable dielectric material within a strong electric field, after melting it. The field repositions the charge carriers or aligns the dipoles within the material. When the material cools, solidification \"freezes\" them in position. Materials used for electrets are usually waxes, polymers or resins. One of the earliest recipes consists of 45% carnauba wax, 45% white rosin, and 10% white beeswax, melted, mixed together, and left to cool in a static electric field of several kilovolts/cm. The thermo-dielectric effect, related to this process, was first described by Brazilian researcher Joaquim Costa Ribeiro.\n\nElectrets can also be manufactured by embedding excess negative charge within a dielectric using a particle accelerator, or by \"stranding\" charges on, or near, the surface using high voltage corona discharges, a process called \"corona charging\". Excess charge within an electret decays exponentially. The decay constant is a function of the material's relative dielectric constant and its bulk resistivity. Materials with extremely high resistivity, such as PTFE, may retain excess charge for many hundreds of years. Most commercially produced electrets are based on fluoropolymers (e.g. amorphous Teflon) machined to thin films.\n\nElectret materials have found commercial and technical interest. For example, they are used in electret microphones and in copy machines. They are also used in some types of air filters, for electrostatic collection of dust particles, in electret ion chambers for measuring ionizing radiation or radon and in Vibration Energy Harvesting.\n\n\n\n"}
{"id": "6389531", "url": "https://en.wikipedia.org/wiki?curid=6389531", "title": "Environmental impact of aviation", "text": "Environmental impact of aviation\n\nThe environmental impact of aviation occurs because aircraft engines emit heat, noise, particulates and gases which contribute to climate change and global dimming. Airplanes emit particles and gases such as carbon dioxide (), water vapor, hydrocarbons, carbon monoxide, nitrogen oxides, sulfur oxides, lead, and black carbon which interact among themselves and with the atmosphere.\n\nDespite emission reductions from automobiles and more fuel-efficient and less polluting turbofan and turboprop engines, the rapid growth of air travel in the past years contributes to an increase in total pollution attributable to aviation. From 1992 to 2005, passenger kilometers increased 5.2 percent per year. And in the European Union, greenhouse gas emissions from aviation increased by 87 percent between 1990 and 2006.\n\nComprehensive research shows that despite anticipated efficiency innovations to airframes, engines, aerodynamics and flight operations, there is no end in sight, even many decades out, to rapid growth in CO emissions from air travel and air freight, due to projected continual growth in air travel. This is because international aviation emissions have escaped international regulation up to the ICAO triennial conference in October 2016 agreed on the CORSIA offset scheme, and because of the lack of taxes on aviation fuel worldwide, lower fares become more frequent than otherwise, which gives a competitive advantage over other transportation modes. Unless market constraints are put in place, this growth in aviation's emissions will result in the sector's emissions amounting to all or nearly all of the annual global emissions budget by mid-century, if climate change is to be held to a temperature increase of 2 °C or less.\n\nThere is an ongoing debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.\n\nLike all human activities involving combustion, most forms of aviation release carbon dioxide (CO) and other greenhouse gases into the Earth's atmosphere, contributing to the acceleration of global warming and (in the case of CO) ocean acidification. These concerns are highlighted by the present volume of commercial aviation and its rate of growth. Globally, about 8.3 million people fly daily (3 billion occupied seats per year), twice the total in 1999. U.S. airlines alone burned about 16.2 billion gallons of fuel during the twelve months between October 2013 and September 2014.\n\nIn addition to the CO released by most aircraft in flight through the burning of fuels such as Jet-A (turbine aircraft) or Avgas (piston aircraft), the aviation industry also contributes greenhouse gas emissions from ground airport vehicles and those used by passengers and staff to access airports, as well as through emissions generated by the production of energy used in airport buildings, the manufacture of aircraft and the construction of airport infrastructure.\n\nWhile the principal greenhouse gas emission from powered aircraft in flight is CO, other emissions may include nitric oxide and nitrogen dioxide (together termed oxides of nitrogen or NO), water vapour and particulates (soot and sulfate particles), sulfur oxides, carbon monoxide (which bonds with oxygen to become CO immediately upon release), incompletely burned hydrocarbons, tetraethyllead (piston aircraft only), and radicals such as hydroxyl, depending on the type of aircraft in use. Emissions weighting factor (EWFs) i.e., the factor by which aviation emissions should be multiplied to get the -equivalent emissions for annual fleet average conditions is in the range 1.3–2.9.\n\nIn 1999 the contribution of civil aircraft-in-flight to global CO emissions was estimated to be around 2%. However, in the cases of high-altitude airliners which frequently fly near or in the stratosphere, non-CO altitude-sensitive effects may increase the total impact on anthropogenic (human-made) climate change significantly. A 2007 report from Environmental Change Institute / Oxford University posits a range closer to 4 percent cumulative effect.\nSubsonic aircraft-in-flight contribute to climate change in four ways:\n\nCO emissions from aircraft-in-flight are the most significant and best understood element of aviation's total contribution to climate change. The level and effects of CO emissions are currently believed to be broadly the same regardless of altitude (i.e. they have the same atmospheric effects as ground-based emissions). In 1992, emissions of CO from aircraft were estimated at around 2 percent of all such anthropogenic emissions, and that year the atmospheric concentration of CO attributable to aviation was around 1 percent of the total anthropogenic increase since the industrial revolution, having accumulated primarily over just the last 50 years.\n\nAt the high altitudes flown by large jet airliners around the tropopause, emissions of NO are particularly effective in forming ozone (O) in the upper troposphere. High altitude (8–13 km) NO emissions result in greater concentrations of O than surface NO emissions, and these in turn have a greater global warming effect. The effect of O surface concentrations are regional and local, but it becomes well mixed globally at mid and upper tropospheric levels.\n\nNO emissions also reduce ambient levels of methane, another greenhouse gas, resulting in a climate cooling effect. But this effect does not offset the O forming effect of NO emissions. It is now believed that aircraft sulfur and water emissions in the stratosphere tend to deplete O, partially offsetting the NO-induced O increases. These effects have not been quantified. This problem does not apply to aircraft that fly lower in the troposphere, such as light aircraft or many commuter aircraft.\n\nOne of the products of burning hydrocarbons with oxygen is water vapour, a greenhouse gas. Water vapour produced by aircraft engines at high altitude, under certain atmospheric conditions, condenses into droplets to form condensation trails, or contrails. Contrails are visible line clouds that form in cold, humid atmospheres and are thought to have a global warming effect (though one less significant than either CO emissions or NO induced effects). Contrails are uncommon (though by no means rare) from lower-altitude aircraft, or from propeller-driven aircraft or rotorcraft.\n\nCirrus clouds have been observed to develop after the persistent formation of contrails and have been found to have a global warming effect over-and-above that of contrail formation alone. There is a degree of scientific uncertainty about the contribution of contrail and cirrus cloud formation to global warming and attempts to estimate aviation's overall climate change contribution do not tend to include its effects on cirrus cloud enhancement. However, a 2015 study found that artificial cloudiness caused by contrail \"outbreaks\" reduces the difference between daytime and nighttime temperatures. The former are decreased and the latter are increased, in comparison to temperatures the day before and the day after such outbreaks. On days with outbreaks the day/night temperature difference was diminished by about 6F° in the U.S. South and 5F° in the Midwest.\n\nLeast significant on a mass basis is the release of soot and sulfate particles. Soot absorbs heat and has a warming effect; sulfate particles reflect radiation and have a small cooling effect. In addition, particles can influence the formation and properties of clouds, including both line-shaped contrails and naturally-occurring cirrus clouds. The impact of \"spreading contrails and cirrus clouds that evolve from them -- collectively known as contrail cirrus -- have a greater radiative forcing (RF) today than all aviation CO emissions since the first powered airplane flight\". Of the particles emitted by aircraft engines, the soot particles are thought to be most important for contrail formation since they are large enough to serve as condensation nuclei for water vapor. All aircraft powered by combustion will release some amount of soot; although, recent studies suggest that reducing the aromatic content of jet fuel decreases the amount of soot produced.\n\nEmissions of passenger aircraft per passenger kilometre vary extensively because of differing factors such as the size and type of aircraft, the altitude and the percentage of passenger or freight capacity of a particular flight, and the distance of the journey and number of stops en route. Also, the effect of a given amount of emissions on climate (radiative forcing) is greater at higher altitudes: see below. Some representative figures for CO emissions are provided by LIPASTO's survey of average direct emissions (not accounting for high-altitude radiative effects) of airliners expressed as CO and CO equivalent per passenger kilometre:\n\n\nThese emissions are similar to a four-seat car with one person on board; however, flying trips often cover longer distances than would be undertaken by car, so the total emissions are much higher. For perspective, per passenger a typical economy-class New York to Los Angeles round trip produces about 715 kg (1574 lb) of CO (but is equivalent to 1,917 kg (4,230 lb) of CO when the high altitude \"climatic forcing\" effect is taken into account). Within the categories of flights above, emissions from scheduled jet flights are substantially higher than turboprop or chartered jet flights. About 60 percent of aviation emissions arise from international flights, and these flights are not covered by the Kyoto Protocol and its emissions reduction targets. However, in a more recent development:The United Nations’ aviation arm overwhelmingly ratified an agreement Thursday (06.Oct.2016) to control global warming emissions from international airline flights, the first climate-change pact to set worldwide limits on a single industry. The agreement, adopted overwhelmingly by the 191-nation International Civil Aviation Organization at a meeting in Montreal, sets airlines’ carbon emissions in the year 2020 as the upper limit of what carriers are allowed to discharge.Figures from British Airways suggest carbon dioxide emissions of 100g per passenger kilometre for large jet airliners (a figure which does not account for the production of other pollutants or condensation trails).\n\nIn 2013 the World Bank published a study of the effect on emissions of its staff's travel in business class or first class, versus using economy class. Among the factors considered was that these premium classes displace proportionately more economy seats for the same total aircraft space capacity, and the associated differing load factors and weight factors. This was not accounted for in prior standard carbon accounting methods. The study concluded that when considering respective average load factors (percent of occupied seats) in each of the seating classes, the carbon footprints of business class and first class are three-times and nine-times higher than economy class. A related article by the International Council on Clean Transport notes further regarding the effect of seating configurations on carbon emissions that:\n\nIn attempting to aggregate and quantify the total climate impact of aircraft emissions the Intergovernmental Panel on Climate Change (IPCC) has estimated that aviation's total climate impact is some 2–4 times that of its direct CO emissions alone (excluding the potential impact of cirrus cloud enhancement). This is measured as radiative forcing. While there is uncertainty about the exact level of impact of NO and water vapour, governments have accepted the broad scientific view that they do have an effect. Globally in 2005, aviation contributed \"possibly as much as 4.9 percent of radiative forcing.\" UK government policy statements have stressed the need for aviation to address its total climate change impacts and not simply the impact of CO.\n\nThe IPCC has estimated that aviation is responsible for around 3.5 percent of anthropogenic climate change, a figure which includes both CO and non-CO induced effects. The IPCC has produced scenarios estimating what this figure could be in 2050. The central case estimate is that aviation's contribution could grow to 5 percent of the total contribution by 2050 if action is not taken to tackle these emissions, though the highest scenario is 15%. Moreover, if other industries achieve significant cuts in their own greenhouse gas emissions, aviation's share as a proportion of the remaining emissions could also rise.\n\nEven though there have been significant improvements in fuel efficiency through aircraft technology and operational management as described here, these improvements are being continually eclipsed by the increase in air traffic volume.\n\nA December 2015 report finds that aircraft could generate of carbon pollution through to 2050, consuming almost 5 percent of the remaining global climate budget. Without regulation, global aviation emissions may triple by mid-century and could emit more than of carbon annually under a high-growth, business-as-usual scenario. Efforts to bring aviation emissions under an effective global accord have so far largely failed, despite there being a number of technological and operational improvements on offer.\n\nFrom 1992 to 2005, passenger kilometers increased 5.2 percent per year, even with the disruptions of 9/11 and two significant wars. Since the onset of the current recession:\n\nIn a 2008 presentation and paper Professor Kevin Anderson of the Tyndall Centre for Climate Change Research showed how continued aviation growth in the UK threatens the ability of that nation to meet CO emission reduction goals necessary to contain the century-end temperature increase to even 4 or 6C°. (See also: the 4 Degrees and Beyond International Climate Conference (2009) and its proceedings.) His charts show the projected domestic aviation carbon emission increase for the UK as growing from 11 MT in 2006 to 17 MT in 2012, at the UK's historic annual emission growth rate of 7%. Beyond 2012 if the growth rate were reduced to 3 percent yearly, carbon emissions in 2030 would be 28 MT, which is 70 percent of the UK's entire carbon emissions budget that year for all sectors of society. This work also suggests the foreseeable future which confronts many other nations that have high dependency on aviation. \"Hypermobile Travelers\", an academic study by Stefan Gössling et al. (2009) in the book \"Climate Change and Aviation\", also points to the dilemma caused by the increasing hypermobility of air travelers both in particular nations and globally.\n\nWhile it is true that late model jet aircraft are significantly more fuel efficient (and thus emit less in particular) than the earliest jet airliners, new airliner models in the 2000s were barely more efficient on a seat-mile basis than the latest piston-powered airliners of the late-1950s (e.g., Constellation L-1649-A and DC-7C). Claims for a high gain in efficiency for airliners over recent decades (while true in part) has been biased high in most studies, by using the early inefficient models of jet airliners as a baseline. Those aircraft were optimized for increased revenue, including increased speed and cruising altitude, and were quite fuel inefficient in comparison to their piston-powered forerunners.\n\nToday, turboprop aircraft – probably in part because of their lower cruising speeds and altitudes (similar to the earlier piston-powered airliners) compared to jet airliners – play an obvious role in the overall fuel efficiency of major airlines that have regional carrier subsidiaries. For example, although Alaska Airlines scored at the top of a 2011–2012 fuel efficiency ranking, if its large regional carrier – turbo-prop equipped Horizon Air – were dropped from the lumped-in consideration, the airline's ranking would be somewhat lower, as noted in the ranking study.\n\nAircraft manufacturers are striving for reductions in both and NOx emissions with each new generation of design of aircraft and engine. While the introduction of more modern aircraft represents an opportunity to reduce emissions per passenger kilometre flown, aircraft are major investments that endure for many decades, and replacement of the international fleet is therefore a long-term proposition which will greatly delay realizing the climate benefits of many kinds of improvements. Engines can be changed at some point, but nevertheless airframes have a long life. Moreover, rather than being linear from one year to the next the improvements to efficiency tend to diminish over time, as reflected in the histories of both piston and jet powered aircraft.\n\nA 2014 life-cycle assessment of the cradle-to-grave reduction in CO by a carbon-fiber-reinforced polymer (CFRP) airliner such as a Boeing 787 – including its manufacture, operations and eventual disposal – has shown that by 2050 such aircraft could reduce the airline industry's CO emissions by 14–15%, compared use of conventional airliners. The benefit of CFRP technology is not higher than that amount of reduction, despite the lighter weight and substantially lower fuel consumption of such aircraft, \"because of the limited fleet penetration by 2050 and the increased demand for air travel due to lower operating costs.\" \n\nResearch projects such as Boeing's ecoDemonstrator program have sought to identify ways of improving the efficiency of commercial aircraft operations. The U.S. government has encouraged such research through grant programs, including the FAA's Continuous Lower Energy, Emissions and Noise (CLEEN) program, and NASA's Environmentally Responsible Aviation (ERA) Project.\n\nAdding an electric drive to the airplane's nose wheel may improve fuel efficiency during ground handling. This addition would allow taxiing without the use of the main engines.\n\nAnother proposed change is the integrating of an Electromagnetic Aircraft Launch System to the airstrips of airports. Some companies such as Airbus are currently researching this possibility. The adding of EMALS would allow the civilian aircraft to use considerably less fuel (as a lot of fuel is used during take off, in comparison to cruising, when calculated per km flown). The idea is to have the aircraft take off at regular aircraft speed, and only use the catapult for take-off, not for landing.\n\nOther opportunities arise from the optimization of airline timetables, route networks and flight frequencies to increase load factors (minimize the number of empty seats flown), together with the optimization of airspace. However, these are each one-time gains, and as these opportunities are successively fulfilled, diminishing returns can be expected from the remaining opportunities.\n\nAnother possible reduction of the climate-change impact is the limitation of cruise altitude of aircraft. This would lead to a significant reduction in high-altitude contrails for a marginal trade-off of increased flight time and an estimated 4 percent increase in CO emissions. Drawbacks of this solution include very limited airspace capacity to do this, especially in Europe and North America and increased fuel burn because jet aircraft are less efficient at lower cruise altitudes.\n\nWhile they are not suitable for long-haul or transoceanic flights, turboprop aircraft used for commuter flights bring two significant benefits: they often burn considerably less fuel per passenger mile, and they typically fly at lower altitudes, well inside the tropopause, where there are no concerns about ozone or contrail production.\n\nSome scientists and companies such as GE Aviation and Virgin Fuels are researching biofuel technology for use in jet aircraft. Some aircraft engines, like the Wilksch WAM120 can (being a 2-stroke Diesel engine) run on straight vegetable oil. Also, a number of Lycoming engines run well on ethanol.\n\nIn addition, there are also several tests done combining regular petrofuels with a biofuel. For example, as part of this test Virgin Atlantic Airways flew a Boeing 747 from London Heathrow Airport to Amsterdam Schiphol Airport on 24 February 2008, with one engine burning a combination of coconut oil and babassu oil. Greenpeace's chief scientist Doug Parr said that the flight was \"high-altitude greenwash\" and that producing organic oils to make biofuel could lead to deforestation and a large increase in greenhouse gas emissions. Also, the majority of the world's aircraft are not large jetliners but smaller piston aircraft, and with major modifications many are capable of using ethanol as a fuel. Another consideration is the vast amount of land that would be necessary to provide the biomass feedstock needed to support the needs of aviation, both civil and military.\n\nIn December 2008, an Air New Zealand jet completed the world's first commercial aviation test flight partially using jatropha-based fuel. Jatropha, used for biodiesel, can thrive on marginal agricultural land where many trees and crops won't grow, or would produce only slow growth yields. Air New Zealand set several general sustainability criteria for its Jatropha, saying that such biofuels must not compete with food resources, that they must be as good as traditional jet fuels, and that they should be cost competitive with existing fuels.\n\nIn January 2009, Continental Airlines used a sustainable biofuel to power a commercial aircraft for the first time in North America. This marks the first sustainable biofuel demonstration flight by a commercial carrier using a twin-engined aircraft, a Boeing 737-800, powered by CFM International CFM56-7B engines. The biofuel blend included components derived from algae and jatropha plants.\n\nOne fuel biofuel alternative to avgas that is under development is Swift Fuel. Swift fuel was approved as a test fuel by ASTM International in December 2009, allowing the company to continue their research and to pursue certification testing. Mary Rusek, president and co-owner of Swift Enterprises predicted at that time that \"100SF will be comparably priced, environmentally friendlier and more fuel-efficient than other general aviation fuels on the market\".\n\nAs of June 2011, revised international aviation fuel standards officially allow commercial airlines to blend conventional jet fuel with up to 50 percent biofuels. The renewable fuels \"can be blended with conventional commercial and military jet fuel through requirements in the newly issued edition of ASTM D7566, Specification for Aviation Turbine Fuel Containing Synthesized Hydrocarbons\".\n\nIn December 2011, the FAA announced it is awarding $7.7 million to eight companies to advance the development of drop-in commercial aviation biofuels, with a special focus on ATJ (alcohol to jet) fuel. As part of its CAAFI (Commercial Aviation Alternative Fuel Initiative) and CLEEN (Continuous Lower Emissions, Energy and Noise) programs, the FAA plans to assist in the development of a sustainable fuel (from alcohols, sugars, biomass, and organic matter such as pyrolysis oils) that can be \"dropped in\" to aircraft without changing current infrastructure. The grant will also be used to research how the fuels affect engine durability and quality control standards.\n\nFinally, liquified natural gas is another fuel that is used in some airplanes. Besides the lower GHG emissions (depending from where the natural gas was obtained from), another major benefit to airplane operators is the price, which is far lower than the price for jet fuel.\n\nThe German video short \"The Bill\" explores how travel and its impacts are commonly viewed in everyday developed-world life, and the social pressures that are at play. British writer George Marshall has investigated common rationalizations that act as barriers to making personal choices to travel less, or to justify recent trips. In an informal research project, \"one you are welcome to join\", he says, he deliberately steered conversations with people who are attuned to climate change problems to questions about recent long-distance flights and why the travel was justified. Reflecting on actions contrary to their beliefs, he noted, \"(i)ntriguing as their dissonance may be, what is especially revealing is that every one of these people has a career that is predicated on the assumption that information is sufficient to generate change – an assumption that a moment's introspection would show them was deeply flawed.\"\n\nFor example, by 2003 Access Grid technology has already been successfully used to host several international conferences, and technology has likely progressed substantially since then. The Tyndall Centre for Climate Change Research has been systematically studying means to change common institutional and professional practices that have led to large carbon footprints of travel by research scientists, and issued a report.\n\nOver 130 airlines have \"frequent flyer programs\" based at least in part on miles, kilometers, points or segments for flights taken. Globally, such programs included about 163 million people as reported in 2006. These programs benefit airlines by habituating people to air travel and, through the mechanics of partnerships with credit card companies and other businesses, in which high profit margin revenue streams can amount to selling free seats for a high price. The only part of United Airlines business that was making money when the company filed for bankruptcy in 2002 was its frequent flyer program.\n\nConcerning business travel, \"The ease of international air travel and the fact that, for most of us, the costs are met by our employers, means that ... globe trotting conference travel is often regarded as a perk of the job.\" However, the perk usually is not only the business trip itself, but also the frequent flyer points which the individual accrues by taking the trip, and which can be redeemed later for personal air travel. Thus a conflict of interest is established, whereby bottom-up pressure may be created within a firm or government agency for travel that is really not necessary. Even when such conflict is not a motivation, the perk of frequent flyer miles can be expected to lead in many cases to personal trips that would not be taken if a ticket had to be paid for with personal funds.\n\nBy just using an airline-sponsored credit card to pay one's household expenses, personal or business bills, or even expense bills charged to an employer, frequent flyer points can be racked up quickly. Thus, free travel—for which the individual has to pay nothing extra—becomes a reality. Across society, this too can be expected to lead to much air travel—and greenhouse gas emissions—that otherwise would not occur.\n\nSeveral studies have contemplated the elimination of frequent flyer programmes (FFPs), on the grounds of anti-competitiveness, ethics, conflict with society's overall well-being, or climate effects. There is a record of governments disallowing or banning FFPs and of industry players requesting bans. Denmark did not allow the programs until 1992, then changing its policy because its airlines were disadvantaged. In 2002, Norway banned domestic FFPs in order to promote competition among its airlines. In the U.S. in 1989, a vice president of Braniff \"said the government should consider ordering an end to frequent-flyer programs, which he said allow unfair competition.\"\n\nA Canadian study said that because of competition no airline could unilaterally end its FFP, but that a national government could use its regulatory power to end the programs broadly, which in Canada's case would also require North America-wide cooperation. In further analysis, a Scandinavian study which recommended an end to frequent flyer plans said, \"the only possible way of prohibiting FFPs successfully now that they have spread from the US to Europe to the Far East would be to do so on a global basis. The basis exists: it could be done by the World Trade Organization.\" A recent study which surveyed frequent flyers in the U.K. and Norway, looked into behavioral addition to frequent flying and the \"flyer's dilemma\" of the conflict between \"the social and personal benefits of flying and air travel's impact on climate change.\" It concluded that:\n\nThis means government action.\n\nOne means for reducing the environmental impact of aviation is to constrain demand for air travel, through increased fares in place of expanded airport capacity. Several studies have explored this:\n\n\nGreenhouse gas emissions from fuel consumption in international aviation, in contrast to those from domestic aviation and from energy use by airports, are excluded from the scope of the first period (2008–2012) of the Kyoto Protocol, as are the non-CO climate effects. Instead, governments agreed to work through the International Civil Aviation Organization (ICAO) to limit or reduce emissions and to find a solution to the allocation of emissions from international aviation in time for the second period of the Kyoto Protocol starting from 2009; however, the Copenhagen climate conference failed to reach an agreement.\n\nRecent research points to this failure as a substantial obstacle to global policy including a emissions reduction pathway that would avoid dangerous climate change by keeping the increase in the average global temperature below a 2 °C rise.\n\nAs part of that process the ICAO has endorsed the adoption of an open emissions trading system to meet CO emissions reduction objectives. Guidelines for the adoption and implementation of a global scheme are currently being developed, and were to be presented to the ICAO Assembly in 2007, although the prospects of a comprehensive inter-governmental agreement on the adoption of such a scheme are uncertain.\n\nWithin the European Union, however, the European Commission has resolved to incorporate aviation in the European Union Emissions Trading Scheme (ETS). A new directive was adopted by the European Parliament in July 2008 and approved by the Council in October 2008. It became effective on 1 January 2012.\n\nResearchers at the Overseas Development Institute investigated the possible effects on Small Island Developing States (SIDS) of the European Union's decision to limit the supply of Certified Emission Reductions (CERs) to its ETS market to Least Developed Countries (LDCs) from 2013. Most SIDS are highly vulnerable to the effects of climate change and rely heavily on tourism as a basis for their economies, so this decision could place them at some disadvantage. The researchers therefore highlight the need to ensure that any regulatory frameworks put in place to tackle climate change take into account the development needs of the most vulnerable countries affected.\n\nA report published by researchers at the Centre for Aviation, Transport and Environment at Manchester Metropolitan University found that the only way to have a significant impact on emissions was to put a price on carbon and to use a market-based measure (MBM), such as the EU Emissions Trading Scheme (ETS).\n\nIn October 2016 the UN agency International Civil Aviation Organization (ICAO) finalized an agreement among its 191 member nations to address the more than (2010) of carbon dioxide emitted annually by international passenger and cargo flights. The agreement will use an offsetting scheme called CORSIA (the Carbon Offsetting and Reduction Scheme for International Aviation) under which forestry and other carbon-reducing activities are directly funded, amounting to about 2 percent of annual revenues for the sector. Rules against 'double counting' should ensure that existing forest protection efforts are not recycled. The scheme does not take effect until 2021 and will be voluntary until 2027, but many countries, including the US and China, have promised to begin at its 2020 inception date. Under the agreement, the global aviation emissions target is a 50 percent reduction by 2050 relative to 2005. NGO reaction to the deal was mixed.\n\nThe agreement has critics. It is not aligned with the 2015 Paris climate agreement, which set the objective of restricting global warming to 1.5 to 2 °C. A late draft of the agreement would have required the air transport industry to assess its share of global carbon budgeting to meet that objective, but the text was removed in the agreed version. CORSIA will regulate only about 25 percent of aviation's international emissions, since it grandfather's all emissions below the 2020 level, allowing unregulated growth until then. Only 65 nations will participate in the initial voluntary period, not including significant emitters Russia, India and perhaps Brazil. The agreement does not cover domestic emissions, which are 40 percent of the global industry's overall emissions. One observer of the ICAO convention made this summary, \"Airline claims that flying will now be green are a myth. Taking a plane is the fastest and cheapest way to fry the planet and this deal won't reduce demand for jet fuel one drop. Instead offsetting aims to cut emissions in other industries...\" Another critic called it \"a timid step in the right direction.\"\n\nA report published in the science journal \"Nature Climate Change\" forecasts that increasing levels will result in a significant increase in in-flight turbulence experienced by transatlantic airline flights by the middle of the 21st century. The lead author of the study, Paul Williams, a researcher at the National Center for Atmospheric Science, at the University of Reading stated, \"air turbulence does more than just interrupt the service of in-flight drinks. It injures hundreds of passengers and aircrew every year – sometimes fatally. It also causes delays and damage to planes.\"\n\nAircraft noise is seen by advocacy groups as being very hard to get attention and action on. The fundamental issues are increased traffic at larger airports and airport expansion at smaller and regional airports.\nAviation authorities and airlines have developed Continuous Descent Approach procedures to reduce noise footprint. Current applicable noise standards effective since 2014 are FAA Stage 4 and (equivalent) EASA Chapter 4. Aircraft with lower standards are restricted to a time window or, on many airports, banned completely. Stage 5 will become effective between 2017-2020. \nQuantification and comparison of noise effects per seat-distance takes into account that noise from cruise levels usually does not reach the earth surface (as opposed to surface-transportation) but is concentrated on and in proximity of airports.\n\nAirports can generate significant water pollution due to their extensive use and handling of jet fuel, lubricants and other chemicals. Airports install spill control structures and related equipment (e.g., vacuum trucks, portable berms, absorbents) to prevent chemical spills, and mitigate the impacts of spills that do occur.\n\nIn cold climates, the use of deicing fluids can also cause water pollution, as most of the fluids applied to aircraft subsequently fall to the ground and can be carried via stormwater runoff to nearby streams, rivers or coastal waters. Airlines use deicing fluids based on ethylene glycol or propylene glycol as the active ingredient.\n\nEthylene glycol and propylene glycol are known to exert high levels of biochemical oxygen demand (BOD) during degradation in surface waters. This process can adversely affect aquatic life by consuming oxygen needed by aquatic organisms for survival. Large quantities of dissolved oxygen (DO) in the water column are consumed when microbial populations decompose propylene glycol.\n\nSufficient dissolved oxygen levels in surface waters are critical for the survival of fish, macroinvertebrates, and other aquatic organisms. If oxygen concentrations drop below a minimum level, organisms emigrate, if able and possible, to areas with higher oxygen levels or eventually die. This effect can drastically reduce the amount of usable aquatic habitat. Reductions in DO levels can reduce or eliminate bottom feeder populations, create conditions that favor a change in a community’s species profile, or alter critical food-web interactions.\n\nUltrafine particles (UFPs) are emitted by aircraft engines during near-surface level operations including taxi, takeoff, climb, descent, and landing, as well as idling at gates and on taxiways. Other sources of UFPs include ground support equipment operating around the terminal areas. In 2014, an air quality study found the area impacted by ultrafine particles from the takeoffs and landings downwind of Los Angeles International Airport to be of much greater magnitude than previously thought. Typical UFP emissions during takeoff are on the order of 10–10 particles emitted per kilogram of fuel burned. Non-volatile soot particle emissions are 10–10 particles per kilogram fuel on a number basis and 0.1–1 gram per kilogram fuel on a mass basis, depending on the engine and fuel characteristics.\n\nSome 167,000 piston engine aircraft—about three-quarters of private planes in the United States—release lead (Pb) into the air due to leaded aviation fuel. From 1970 to 2007, general aviation aircraft emitted about 34,000 tons of lead into the atmosphere according to the Environmental Protection Agency. Lead is recognized as a serious environmental threat by the Federal Aviation Administration if inhaled or ingested leading to adverse effects on the nervous system, red blood cells and cardiovascular and immune systems with infants and young children especially sensitive to even low levels of lead, which may contribute to behavioral and learning problems, lower IQ and autism.\n\nFlying high, passengers and crews of jet airliners are exposed to at least 10 times the cosmic ray dose that people at sea level receive. Every few years, a geomagnetic storm permits a solar particle event to penetrate down to jetliner altitudes. Aircraft flying polar routes near the geomagnetic poles are at particular risk.\n\nAirport buildings, taxiways and runways take possession of a part of our ecosystem. Most of aircraft movement however is positioned in air at altitude and so away from direct interaction with sensitive nature or human detection. This is opposed to roads, railways and canals being very significant in use of area and dividing of ecological structures while required for surface transportation for as many miles as the distance traveled.\n\n\n\n\n\n\n"}
{"id": "2341773", "url": "https://en.wikipedia.org/wiki?curid=2341773", "title": "Ffyona Campbell", "text": "Ffyona Campbell\n\nFfyona Campbell (born 1967 in Totnes, Devon) is an English long distance walker who walked around the world. She covered over 11 years and raised £180,000 for charity. She wrote about her experience in a series of three books.\n\nBorn in 1967, into a family with a long Royal Naval tradition. During her childhood and early teens the Campbells moved home 24 times – which resulted in Ffyona attending 15 schools.\n\nAfter leaving home and school at 16, she raised the necessary sponsorship to enable her to walk from John o' Groats to Lands End. Walking 20 to 25 miles a day six days a week, she completed the journey in 49 days and was the youngest person at that time to have done it. Through sponsorship of the London \"Evening Standard\", she raised £25,000 for the Royal Marsden Cancer Hospital.\n\nAt 18, she set off from New York City crossing the United States towards Los Angeles. The media schedule to coincide with the sponsors' public relations events en route was demanding, requiring hours of interviews at the end of each long day. During this part of the walk she became pregnant and had an abortion, and secretly covered 1000 miles of the journey in her back-up van.\n\nAt 21 she walked across Australia, 50 miles a day for 3,200 miles from Sydney to Perth in 95 days, beating the men's record for this journey. She suffered severe sunburn, dehydration as well as intense blistering of the feet but was determined not to miss out any miles. She wrote about this journey in her book \"Feet of Clay\".\n\nOn 2 April 1991, she left Cape Town, South Africa and walked the length of Africa covering over before arriving in Tangiers, Morocco two years later on 1 September 1993. She had been joined by her former boyfriend, British survival expert Ray Mears, for five months during the journey through Zaire after an uprising had forced her and her team to abandon the support vehicle and be evacuated by the French Foreign Legion along with all the other expats. She was able to return to central Africa within weeks of the evacuation and continued walking from the place she had left. During the stretch across the Sahara, she walked an extra 4,000 km around a war zone to avoid missing out any steps. She reached Tangiers and was greeted by the international media. The walk raised awareness of Survival International, an organisation which helps protect the lives of threatened tribal people. She wrote about this journey in her book \"On Foot through Africa\".\n\nIn April 1994, she left Algeciras, Spain and walked through Europe on the Via de la Plata through Spain, through France, crossing to Britain at Dover. She then completed the last walking from Dover back to John o' Groats accompanied by young people from Raleigh International who came to find out just how far they could walk if they really put their minds to it. She arrived at John O'Groat's, the world's end, on 14 October 1994. She was shadowed by a BBC film crew and presenter Janet Street-Porter. At the time, Campbell was hailed as the first woman to walk around the world.\n\nCampbell raised half the amount for charity in one go when one of the organisers at Raleigh International sold the advertising space on her forehead during her well-publicised return. After a period in hospital for a back operation, she walked across America again for her own personal satisfaction as she had had to miss out a section in the middle due to her pregnancy. She wrote about that journey in her final book, \"The Whole Story\". The media castigated her for her initial deception.\n\nInspired by the hunter-gatherers she met on her journey – Aborigines, Bushmen, Pygmies and Native Americans – Campbell returned to Australia after the end of the world walk to live with the Aborigines. After three months she returned to Britain to learn how to be a hunter-gatherer in her own country and to work out what had separated us from the life we must have loved so much. She wrote about her adventures in her fourth book, \"The Hunter-Gatherer Way\" and is now teaching others how to be hunter-gatherers in Britain through her company Wild Food Walks based in Devon.\n\n\n\n"}
{"id": "57516456", "url": "https://en.wikipedia.org/wiki?curid=57516456", "title": "Helen G. Cruickshank", "text": "Helen G. Cruickshank\n\nHelen Gere Cruickshank (20 February 1902 – 31 March 1994) was an American nature writer and photographer of birds in their natural habitats in many areas of the world.\n\nIn 1937, she married Allan D. Cruickshank, a lecturer, writer, and photographer for the Audubon Society. Husband and wife formed a highly effective partnership for photography and bird study. He took the black and white photos and she took the color slides on their bird study expeditions. The Academy of Natural Sciences of Drexel University (formerly named the Philadelphia Academy of Natural Science) has a bird image collection named VIREO (Visual REsource For Ornithology) with over 180,000 thousand photographs, thousands of which were taken by the Cruickshanks.\n\nHelen Cruickshank won the 1949 John Burroughs Medal for her 1948 book \"Flight into Sunshine: Bird Experiences in Florida\". Brevard County, Florida established the Helen and Allan Cruickshank Sanctuary, a 140-acre wildlife refuge near Rockledge, Florida. The Florida Ornithological Society sponsors the Helen G. and Allan D. Cruickshank Education Award.\n\n"}
{"id": "1092116", "url": "https://en.wikipedia.org/wiki?curid=1092116", "title": "International Code of Marketing of Breast-milk Substitutes", "text": "International Code of Marketing of Breast-milk Substitutes\n\nThe International Code of Marketing of Breast-milk Substitutes (also known as the WHO Code) is an international health policy framework for breastfeeding promotion adopted by the World Health Assembly (WHA) of the World Health Organization (WHO) in 1981. The Code was developed as a global public health strategy and recommends restrictions on the marketing of breast milk substitutes, such as infant formula, to ensure that mothers are not discouraged from breastfeeding and that substitutes are used safely if needed. The Code also covers ethical considerations and regulations for the marketing of feeding bottles and teats. A number of subsequent WHA resolutions have further clarified or extended certain provisions of the Code.\n\nSince 1981, 84 countries have enacted legislation implementing all or many of the provisions of the Code and subsequent relevant WHA resolutions.\n\nThe Code aims to shield breastfeeding from commercial promotion that affects mothers, health workers and health care systems. The Code and resolutions also contain specific provisions and recommendations relating to labelling of infant formula and other breastmilk substitutes.\n\n\n\n\n\n\n\n\n\nIn line with the recommendation for exclusive breastfeeding in WHA resolution 54.2 [2001], all complementary foods must be labeled as suitable for use by infants from six months and not earlier.\n\nThe baby food industry has been the subject of pointed criticism from non-governmental organizations, international agencies and campaign groups for failing to abide by the Code. One of the largest food and beverage manufacturers in the world, the Swiss giant Nestlé, has been the subject of an international boycott campaign since 1977 for its milk-substitute marketing practices prior to and since the development of the Code (see Nestlé boycott).\n\nOn its own, the International Code is not legally enforceable. Companies are only subject to legal sanctions for failing to abide by the Code where it has been incorporated into the legislature of a nation state. Many countries have fully or partially adopted the Code as law. Other countries have no legislation on baby food marketing at all.\n\nCode violations by baby food manufacturers are still widespread, especially (but not exclusively) in countries that have not implemented the Code as a national measure or where monitoring and enforcement is weak. The WHO, International Baby Food Action Network (IBFAN), UNICEF, Save the Children and other international organizations perform monitoring of implementation of the Code across the world both independently and with governments.\n\n\n"}
{"id": "143338", "url": "https://en.wikipedia.org/wiki?curid=143338", "title": "Land (economics)", "text": "Land (economics)\n\nIn economics, land comprises all naturally occurring resources as well as geographic land. Examples include particular geographical locations, mineral deposits, forests, fish stocks, atmospheric quality, geostationary orbits, and portions of the electromagnetic spectrum. Supply of these resources is fixed.\n\nNatural resources are fundamental to the production of all goods, including capital goods. Location values are not be confused with values imparted by fixed capital improvements. In classical economics, land is considered one of the three factors of production (also sometimes called the three producer goods) along with capital, and labor. Land is sometimes merged with capital to simplify micro-economics. However, a common mistake is combining land and capital in macro-analysis.\n\nLand was sometimes defined in classical and neoclassical economics as the \"original and indestructible powers of the soil.\"\n\nIncome derived from ownership or control of natural resources is referred to as rent.\n\nGeorgists hold that this implies a perfectly inelastic supply curve (i.e., zero elasticity), suggesting that a land value tax that recovers the rent of land for public purposes would not affect the opportunity cost of using land, but would instead only decrease the value of owning it. This view is supported by evidence that although land can come on and off the market, market inventories of land show if anything an inverse relationship to price (i.e., negative elasticity).\n\nAs a tangible asset land is represented in accounting as a fixed asset or a capital asset.\n\nLand, particularly geographic locations and mineral deposits, has historically been the cause of much conflict and dispute; land reform programs, which are designed to redistribute possession and/or use of geographic land, are often the cause of much controversy, and conflicts over the economic rent of mineral deposits have contributed to many civil wars.\n\nSome United Kingdom and common wealth universities offer a courses in \"land economy\", where economics is studied alongside law, business regulation, surveying and the built and natural environments. This mode of study at Cambridge dates back to 1917 when William Cecil Dampier suggested the creation of a school of rural economy at the university.\n\nThe sustainable use of land is the focus of some economic theories.\n\n\n"}
{"id": "55858784", "url": "https://en.wikipedia.org/wiki?curid=55858784", "title": "Lasse Nordlund", "text": "Lasse Nordlund\n\nLasse Nordlund (1965) is a Finnish social thinker who is known for his experiment of living completely self-sufficiently in the Finnish countryside, producing his clothes and tools and cultivating his food from his small farm and what he gathered from the surrounding country.\n\nBorn in Helsinki to a Finnish father and a German mother, Nordlund spent a typical youth in Germany and was interested in studying physics. However, he began to doubt the viability and meaning of conventional living and decided to try and live a self-sufficient lifestyle.\n\nThe background to Nordlund's experiment is his conviction that only subsistence living is materially sustainable on a long-term basis.\n\nNordlund could not, by his own admission, distinguish between a pine or a fir tree when he started his project, but started by studying books and experimenting to produce goods and obtain food. Eventually, he was able to live in Valtimo, in Finnish North Karelia, for the years between 1992 and 2004 in complete self-sufficiency.\n\nAccording to Nordlund, one person needs about five ares (500m²) of farmland to live on without buying any additional food. The most important food crop for him is the rutabaga, a common Finnish staple. In his self-sufficient economy, Nordlund consumes 200 kg of mushrooms per year and about the same in berries preserved without additives or sugar. He forsook livestock because he projected the maintenance for example, animal feed) to be too time-consuming to make sense. Though he does accept the value of animal husbandry as wool and leather are handy clothing materials able to outlast cotton cloth. Nordlund became accustommed to life without salt, though many believe it to be life-threatening. He argues that he has been able to live without salt for years, although initially leaving out salt caused him some health issues. Instead of toilet paper, he uses dried sphagnum moss. In the early years of self-reliance, Nordlund's days were full of work, but eventually he was able to relax and take up hobbies from his youth, such as flute playing and working on equations.\n\nHe is an author and a speaker on sustainability topics.\n\nToday, he lives with his family, still mostly self-sufficiently.\n\nIn 2008, Palladium books published a work written by Lasse Nordlund and Maria Dorff \"The Foundations of Our Life, Reflections about Human labour, Money and Energy from Self-sufficiency Standpoint\".\n\n"}
{"id": "5893868", "url": "https://en.wikipedia.org/wiki?curid=5893868", "title": "List of Migidae species", "text": "List of Migidae species\n\nThis page lists all described species of the spider family Migidae as of May 3, 2011.\n\n\"Calathotarsus\" \n\n\"Goloboffia\" \n\n\"Heteromigas\" \n\n\"Mallecomigas\" \n\n\"Micromesomma\" \n\n\"Migas\" \n\n\"Moggridgea\" \n\n\"Paramigas\" \n\n\"Poecilomigas\" \n\n\"Thyropoeus\" \n"}
{"id": "2433001", "url": "https://en.wikipedia.org/wiki?curid=2433001", "title": "List of West Virginia state forests", "text": "List of West Virginia state forests\n\nWest Virginia contains a network of eight state forests that help to protect over of wooded lands in the state. Most of the forests are managed by the West Virginia Division of Forestry, although Kanawha State Forest is managed as a state park by the Division of Natural Resources. All of the forests except for Calvin Price contain recreational facilities managed in cooperation with the DNR.\n\n"}
{"id": "9013565", "url": "https://en.wikipedia.org/wiki?curid=9013565", "title": "List of caneberries diseases", "text": "List of caneberries diseases\n\nThis article is a list of diseases of caneberries (\"Rubus\" spp.).\n\n"}
{"id": "9020558", "url": "https://en.wikipedia.org/wiki?curid=9020558", "title": "List of coffee diseases", "text": "List of coffee diseases\n\nThis article is a list of diseases of coffee (\"Coffea arabica, Coffea canephora\").\n\n"}
{"id": "49012596", "url": "https://en.wikipedia.org/wiki?curid=49012596", "title": "List of earthquakes in Tonga", "text": "List of earthquakes in Tonga\n\nThis is a list of earthquakes in Tonga:\n\nSources\n"}
{"id": "52593742", "url": "https://en.wikipedia.org/wiki?curid=52593742", "title": "List of fjords of Russia", "text": "List of fjords of Russia\n\nThis is a list of the most important fjords of the Russian Federation.\nIn spite of the vastness of the Arctic coastlines of the federation there are relatively few fjords in Russia. Fjords are circumscribed to certain areas only; over thirty are in Novaya Zemlya —including lakes which are structurally fjords, with a few others in the Barents Sea coast of the Kola Peninsula, the Severnaya Zemlya archipelago, the Bering Sea coast of the Chukchi Peninsula and the southeastern shores of Kamchatka.\n\n\n"}
{"id": "27030200", "url": "https://en.wikipedia.org/wiki?curid=27030200", "title": "List of longest undammed rivers", "text": "List of longest undammed rivers\n\nThis is a list of the longest undammed rivers of the world, ordered by length.\n\n"}
{"id": "12694121", "url": "https://en.wikipedia.org/wiki?curid=12694121", "title": "List of mountain peaks of Alaska", "text": "List of mountain peaks of Alaska\n\nThis article comprises three sortable tables of major mountain peaks of the U.S. State of Alaska.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nOf the 100 highest major summits of Alaska, only Denali exceeds elevation, four peaks exceed , 23 peaks exceed, 61 peaks exceed , 92 peaks exceed , and all 100 peaks exceed elevation. Five of these peaks lie on the international border with Yukon and five lie on the international border with British Columbia. All ten of the highest major summits of the United States are in Alaska.\n\nOf the 100 most prominent summits of Alaska, only Denali exceeds of topographic prominence, six peaks exceed , 26 peaks exceed , 65 peaks are ultra-prominent summits with at least, and all 100 peaks exceed of topographic prominence. Four of these peaks lie on the international border with British Columbia and four lie on the international border with Yukon.\n\nOf the 50 most isolated major summits of Alaska, only Denali exceeds of topographic isolation, four peaks exceed , 16 peaks exceed , 38 peaks exceed , and all 50 peaks exceed of topographic isolation. Two of these peaks lie on the international border with British Columbia.\n\n\n"}
{"id": "12444155", "url": "https://en.wikipedia.org/wiki?curid=12444155", "title": "List of mountain peaks of the United States", "text": "List of mountain peaks of the United States\n\nThis article comprises three sortable tables of major mountain peaks of the United States of America.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nOf the 100 highest major summits of the United States, only Denali exceeds elevation, four peaks exceed , and all 100 peaks exceed elevation. \n\nOf these 100 summits, 53 are located in Colorado, 23 in Alaska, 14 in California, five in Wyoming, two in Hawaii, and one each in Washington, Utah, and New Mexico. Five of these summits are located on the international border between Alaska and Yukon, and one is located on the international border between Alaska and British Columbia. The ten highest major summits of the United States are all located in Alaska.\n\nOf the 50 most prominent summits of the United States, only Denali exceeds of topographic prominence, three peaks exceed , ten peaks exceed , 45 peaks exceed , and all 50 peaks exceed of topographic prominence. All of these peaks are ultra-prominent summits.\n\nOf these 50 peaks, 27 are located in Alaska, five in Washington, five in California, three in Hawaii, three in Wyoming, two in Nevada, two in Oregon, and one each in Colorado, Utah, and Arizona. Three of these summits lie on the international border between Alaska and Yukon, and one lies on the international border between Alaska and British Columbia.\n\nOf the 50 most isolated major summits of the United States, only Denali exceeds of topographic isolation, Mauna Kea exceeds , Mount Whitney exceeds , seven peaks exceed , 12 peaks exceed , 44 peaks exceed , and all 50 peaks exceed of topographic isolation.\n\nOf these 50 peaks, 18 are located in Alaska, four in California, three in Washington, two in Hawaii, two in Colorado, two in Wyoming, two in Arizona, two in Nevada, two in Utah, two in New York, two in Oregon, and one each in North Carolina, New Hampshire, Arkansas, West Virginia, New Mexico, Maine, Idaho, South Dakota, and Montana. One of these summits lies on the international border between Alaska and British Columbia.\n\n\n"}
{"id": "37389326", "url": "https://en.wikipedia.org/wiki?curid=37389326", "title": "List of non-fiction environmental writers", "text": "List of non-fiction environmental writers\n\nThis is a list of non-American, non-fiction environmental writers. \n\n\n"}
{"id": "12541385", "url": "https://en.wikipedia.org/wiki?curid=12541385", "title": "List of rivers of Colombia", "text": "List of rivers of Colombia\n\n\n\n\n\n\n"}
{"id": "238882", "url": "https://en.wikipedia.org/wiki?curid=238882", "title": "List of rivers of Switzerland", "text": "List of rivers of Switzerland\n\nThe following is a list of rivers in Switzerland:\n\n\n\nSwitzerland is drained into four directions: \n\nRivers that flow into other rivers are sorted by the proximity of their points of confluence to the sea (the lower in the list, the more upstream). Some rivers (e.g. Danube) do not flow through Switzerland themselves, but they are mentioned for having Swiss tributaries. They are given in \"italics\". \n\n\n"}
{"id": "8591814", "url": "https://en.wikipedia.org/wiki?curid=8591814", "title": "List of stars in Puppis", "text": "List of stars in Puppis\n\nThis is the list of notable stars in the constellation Puppis, sorted by decreasing brightness.\n\nThis constellation's Bayer designations (Greek-letter star names) were given while it was still considered part of the constellation of Argo Navis. After Argo Navis was broken up into Carina, Vela, and Puppis, these Greek-letter designations were kept, so that Puppis does not have a full complement of Greek-letter designations. For example, since Argo Navis's alpha star went to Carina, there is no Alpha Puppis.\n\n\n"}
{"id": "733975", "url": "https://en.wikipedia.org/wiki?curid=733975", "title": "Marriage at Cana", "text": "Marriage at Cana\n\nThe transformation of water into wine at the Marriage at Cana or Wedding at Cana is the first miracle attributed to Jesus in the Gospel of John. In the Gospel account, Jesus, his mother and his disciples are invited to a wedding, and when the wine runs out, Jesus delivers a sign of his glory by turning water into wine.\n\nThe location of Cana has been subject to the debate of Christ among biblical scholars and archeologists; several villages in Galilee are possible candidates.\n\n states that while Jesus was at a wedding in Cana with his disciples, the party ran out of wine. Jesus' mother (unnamed in John's Gospel) told Jesus, \"They have no wine,\" and Jesus replied, \"Oh Woman, what has this to do with me? My hour has not yet come.\" His mother then said to the servants, \"Do whatever he tells you\" (). Jesus ordered the servants to fill containers with water and to draw out some and take it to the chief steward waiter. After tasting it, without knowing where it came from, the steward remarked to the bridegroom that he had departed from the custom of serving the best wine first by serving it last (). John adds that: \"Jesus did this, the first of his signs, in Cana of Galilee, and it revealed his glory; and his disciples believed in him\" ().\n\nAlthough none of the Synoptic Gospels mention the marriage at Cana, Christian tradition based on holds that this is the first public miracle of Jesus. It is considered to have symbolic importance as the first of the seven signs in the Gospel of John by which Jesus' divine status is attested, and around which the gospel is structured.\nInterpreted allegorically, the good news and hope implied by the story are in the words of the steward of the Feast when he tasted the good wine, \"Everyone serves the good wine first, and then the inferior wine after the guests have become drunk. But you have kept the good wine until now\" (John 2:10, NRSV). This could be interpreted by saying simply that it is always darkest before the dawn, but good things are on the way. The more usual interpretation, however, is that this is a reference to the appearance of Jesus, whom the author of the Fourth Gospel regards as being himself \"the good wine\".\n\nThe story has had considerable importance in the development of Christian pastoral theology. Fulton J. Sheen thought that it is very likely that it was one of Mary's relatives who were being married. The gospel account of Jesus being invited to a marriage, attending, and using his divine power to save the celebrations from disaster are taken as evidence of his approval for marriage and earthly celebrations, in contrast to the more austere views of Paul the Apostle as found, for example, in 1 Corinthians 7. It has also been used as an argument against Christian teetotalism.\n\nAccording to Bill Day, the miracle may also be interpreted as the antitype of Moses' first public miracle of . This would establish a symbolic link between Moses as the first savior of the Jews through their escape from Egypt and Jesus as the spiritual savior of all people.\n\nSome commentators have speculated about the identity of the unnamed bridegroom. One tradition, represented by Thomas Aquinas among others, holds that the bridegroom was St John the Evangelist himself. Bishop John Spong suggests in his book \"Born of a Woman\" that the event was actually the wedding of Jesus himself to Mary Magdalene. In 1854, at a time when polygamy was an element of mainstream Mormon practice, the Mormon elder Orson Hyde made a similar suggestion, arguing that Jesus was a polygamist and that the event at Cana was his wedding to Mary Magdalene, Martha and Mary of Bethany.\n\nThe exact location of Cana has been subject to debate among scholars. Modern scholars maintain that since the Gospel of John was addressed to Jewish Christians of the time, it isn’t likely that the evangelist would mention a place that did not exist.\n\nLocations which are candidates for historical Cana are:\n\nAccording to the Catholic Encyclopedia of 1914, a tradition dating back to the 8th century identifies Cana with the modern Arab town of Kafr Kanna, in Galilee, about 7 km northeast of Nazareth, Israel. Other suggested alternatives include the ruined village of Khirbet Qana, also called Kenet-l-Jalil, about 9 km further north, and Ain Qana, in Southern Lebanon, which is closer to Nazareth and considered to be a better candidate on etymological grounds. Some Christians, especially Lebanese Christians, believe the southern Lebanese village of Qana to have been the actual location of this event.\n\nIn the journal \"Biblical Archaeological Review\", Michael Homan argues that many biblical scholars have misinterpreted early texts, translating to 'wine' when the more sensible translation is 'beer'.\n\nDepictions of \"Marriage at Cana\" are numerous in art history.\n\nSaint Columba of Iona supposedly performed an identical miracle when he served as a deacon in Ireland under Finnian of Movilla, replenishing the supply of sacramental wine for a Mass.\n\n\n"}
{"id": "38775498", "url": "https://en.wikipedia.org/wiki?curid=38775498", "title": "Maxwell Knight", "text": "Maxwell Knight\n\nCharles Henry Maxwell Knight OBE, known as Maxwell Knight, (b. South Norwood, 9 July 1900 – 27 January 1968) was a British spymaster, naturalist and broadcaster, reputedly a model for the James Bond character \"M\". He played major roles in surveillance of an early British Fascist party as well as the main Communist Party.\n\nKnight was the son of Hugh Coleraine Knight, a solicitor, and his wife, Ada Phyllis (née Hancock). He was christened in Holy Innocents Church, South Norwood on 3 August 1900.\n\nHe saw service during the First World War. Having been a naval cadet, he was appointed to the temporary rank of Midshipman in the Royal Naval Reserve on 2 May 1918.\n\nIn July 1918 he attended a hydrophone officers' course, and in August served for a short time as First Class Hydrophone Officer aboard the trawler, \"Ninus\". On 1 September 1918 he was appointed to the armed merchant cruiser, HMS \"Andes\". In December 1918 Captain C.T.H. Cooper of the \"Andes\" described him as \"a promising young officer.\" He was demobilised in February 1919.\n\nHaving left the navy, he worked as a teacher in a preparatory school and as a freelance journalist.\n\nIn an unpublished memoir, Knight recalled that he joined the first of the Fascist Movements in Britain, Rotha Lintorn-Orman's British Fascisti, in 1924, \"at the request of the late Sir George Makgill who was then running agents on behalf of Sir Vernon Kell, Director General of the British Security Service, MI5. I remained with this organisation until 1930 when it more or less became ineffectual. My association with this body was at all times for the purposes of obtaining information for HM Government and also for the purposes of finding likely people who might be used by this department for the same purposes.\"\n\nHe served as the organisation's director of intelligence.\n\nDuring the 1920s, on Knight's instructions, six British Fascists, posing as Communists, joined the Communist Party of Great Britain to work as penetrator agents for Makgill's section 2B.\n\nIn time Knight became MI5's chief 'agent runner', being deployed principally against the Communist Party.  He rose to be head of section B5(b), responsible for infiltrating agents into potentially subversive groups, based for much of its existence at 308, Hood House, Dolphin Square in London, separate from the rest of MI5.\n\nJoan Miller, who was recruited as an agent by Knight and had a close personal relationship with him, remembered that he felt very deeply about the threat of Communism: \"his views on this subject, you might say, amounted almost to an obsession. He was equally adamant in his aversion to Jews and homosexuals, but prepared to suspend these prejudices in certain cases. 'Bloody Jews' was one of his expressions (you have only to read the popular novels of the period - thrillers in particular - to understand just how widespread this particular prejudice was).\"\n\nNotwithstanding this, Miller imagined Knight himself was a homosexual, although his third wife, Susi Maxwell Knight, rejected the allegation.\n\nA respected case officer, Knight achieved successes with the infiltration of political groups, leading to the internment and imprisonment of fascists and fascist sympathisers regarded as a threat to the United Kingdom, such as Albert Williams, Percy Glading, George Whomack, Anna Wolkoff, Tyler Kent, leading anti-semite, Captain Archibald Maule Ramsay MP and Oswald Mosley.\n\nDuring his career with MI5, Knight found that there was \"a very long standing and ill-founded prejudice against the employment of women as agents\", a position with which he did not agree. Indeed, many of his best agents were women. Agents working under him included Olga Gray (who infiltrated the leadership of the Communist Party of Great Britain), Joan Miller (who \"penetrated the anti-semitic underworld of British Fascism\"), and Tom Driberg.\n\nHis early warnings of communist infiltration of MI5 were not taken seriously. Patricia Craig notes that his paper, \"The Comintern is not dead\", which predicted with great accuracy the developments in Russia’s policy with regard to Britain after the war, \"was dismissed as 'over-theoretical' by Roger Hollis, and various other Soviet experts considered it unimpressive.\" Moreover, when, in 1941 Anthony Blunt informed Harry Pollitt that Tom Driberg was an informer, and Driberg was expelled from the Communist Party, Knight developed the suspicion that his unit had been infiltrated by the KGB, but Blunt's treachery remained undiscovered for some years.\n\nA failure of Knight's section was the entrapment of Ben Greene, an anti-war activist, interned on the orders of the then Home Secretary, Sir John Anderson, as a result of false evidence given by Knight's agent provocateur, Harald Kurtz.\n\nHaving been gazetted as a Second Lieutenant on the Special List in September 1939, Knight was given the army rank of Major during the Second World War, but designated as a \"Civil Assistant, General Staff, War Office.\" He was appointed as an Officer of the Civil Division of the Order of the British Empire in June 1943.\n\nApparently rejected and humiliated by Knight’s failure to consummate their marriage, his first wife took an overdose of barbiturates and died in 1934, leaving her large fortune to Knight.\nKnight was married, secondly, in 1937 to Lois Mary Coplestone, and thirdly, in 1944, to Susan Barnes (\"Susi\").\n\nIan Fleming, the author of the James Bond series of books, used an amalgam of Knight and his former superior Rear Admiral John Henry Godfrey, Director of the Naval Intelligence Division, as a model for the character \"M,\" Bond's boss.\n\nIn 1946, Knight, who had been an ardent naturalist since childhood, began what was to become a successful broadcasting career on BBC radio, appearing in, and hosting such programmes as \"Naturalist\", \"Country Questions\" and \"Nature Parliament\". He appeared occasionally on television: in Peter Scott's \"Look\" and in \"Animal, Vegetable or Mineral.\"\n\nKnight conducted his broadcasting career alongside his work in intelligence, until 1956, when he retired early from MI5, on the grounds of ill health, suffering from angina.\n\nKnight, who wrote of his pet cuckoo, 'Goo,' in \"A Cuckoo in the House\" (1955), was \"one of many agents feeding into the phenomenon of British spies being bird enthusiasts. ‘Birdwatcher’ is old intelligence slang for spy. And the cuckoo – which infiltrates and imitates – was an ideal muse for a spy like Knight.\"\n\nFellow naturalist and Knight enthusiast, Helen Macdonald, writes that \"the cuckoo’s life beautifully mirrored the concerns of Knight’s own. First, its sex life was mysterious and secretive. So was Knight’s: for years, he’d maintained a hearty heterosexual facade while picking up rough trade in local cinemas and employing local motorcycle mechanics for reasons other than repairing motorcycles. Second, cuckoos were the avian equivalents of the officer-controller of penetration agents; they ‘insinuated’ their ‘chameleon eggs’ into the nests of their ‘dupes’.\"\n\nSpies and birders, she writes elsewhere, \"have the same skills, the ability to identify, recognise, be unobtrusive, invisible, hide. You pay careful attention to your surroundings. You never feel part of the crowd.\"\n\nKnight was married in Sherborne Abbey on 29 December 1925 to Gwladys Evelyn Amy Poole.\n\nKnight spent his last years at \"The Wing,\" Josselyns, Midgham, near Reading in Berkshire, where he died, from heart failure, on 24 January 1968.\n\nHe was survived by his third wife, Susi Maxwell Knight.\n\nAfter his death, the Maxwell Knight Memorial Fund was set up, which provided for the Maxwell Knight Young Naturalists' Library in the education centre of the Natural History Museum.\n\nIn October, 2015, a hitherto unpublished 50,000-word manuscript, entitled The Frightened Face of Nature, written by Knight in 1964, and discovered by Professor John E. Cooper and Simon H. King in Knight's personal filing cabinet, was published in \"The Guardian\", under the headline, \"Spectre of destruction\": The Lost Manuscript of the Real-Life 'M'\n\nHe published 34 books and wrote many magazine articles on nature topics.\n\n\n\n\n"}
{"id": "7150805", "url": "https://en.wikipedia.org/wiki?curid=7150805", "title": "National parks of Azerbaijan", "text": "National parks of Azerbaijan\n\nNational Parks of Azerbaijan are run by the Ministry of Ecology and Natural Resources in the Republic of Azerbaijan. The first national park established was Zangezur National Park in 2003. Since then a further 8 national parks have been established; the most recent being the Samur-Yalama National Park in 2012. \n\nThe national parks are the public lands or bodies of water of special environmental, historical and other importance, which bear the status of governmental protection. They serve to the purposes of environmental protection, educational, scientific, cultural researches, etc.\n\nAs a country located in the Caucasus between the Black and Caspian Seas, Azerbaijan has a rich flora and fauna and widest biodiversity among the European states and enormous natural resources. The lands with specially protected ecosystems play the crucial roles in biodiversity preservation. The activity of such natural bodies promotes the preservation of rare and endangered species of plants and animals. Azerbaijan has a total of 9 national parks (as well as 13 state natural parks and 21 state reserves), which are listed in the table with the year of their establishment and their surface area.\n\nThe total surface area of the 9 national parks is . In total more than 3,58% of Azerbaijan is under protection by the government as national parks.\n\nThe Zangezur National Park (it was renamed and expanded in 2009 from former Ordubad National Park) is characterized by rich biological diversity. It has 58 species of animals (35 of vertebrates and 23 of insects) and 39 species of plants which are included into the Red Book of Azerbaijan. The National Park comprises such rare and endangered species as Persian leopard, the mountain sheep-moufflon, bezoar goat, white-tail sea eagle, golden eagle, little bustard.\n\nThe Shirvan National Park has a semi-desert landscape and water body of approximately 40 square kilometres. The national park has an extremely rich ornithological fauna. Rare and valuable species of birds (turaj, little bustard, bustard, swans, flamingo, etc.) winter and nest in the marshy areas. Djeyran gazelles are the most widely spread mammals in the region.\n\nAg-Gol National Park is located in the Mil plain of the Kur-Araz lowlands, it has semi-desert landscape and deserved the title of a bird paradise, as the most important winter and nest place of birds. \nThe ornithological fauna of this reserve is very rich. Over 140 species of birds are found in this place including 89 species of nesting birds (Partridge, spoonbill, swan, teal, bustard, etc.). Ag-Gol national park is designed to protect the marshy ecological systems, as the nesting and wintering places of migratory and water birds. Ag-Gol has been incorporated into the list of UNESCO's convention \"On internationally important marshy areas as the residing places of birds\"\n\nHirkan National Park is located in the Lenkoran Lowland and the Talysh Mountains, and is 99% covered by forests in a primarily mountainous region, and is strictly protected.\n\nHirkan National Park preserves relict and endemic plants species of Tertiary period. Forests of Hirkan account for 150 out of 435 types of trees and bushes. One can come across such types of trees, included into the Red Book of Azerbaijan as, Hirkan box tree, iron tree, chestnut leave oak, fig-tree, Hirkan pear-tree, Silk Acacia, Caucasus palm-tree, Caspian gleditsia, butcher's broom, alder-tree, such animals as the Persian leopard, the Talysh pheasant, golden eagle, etc.\n\nThe area of Altyaghach is 90.5% covered by temperate deciduous broadleaved forests. The major types of trees are iron trees, Caucasus hornbeam, Oriental beech, cud, birch-tree, etc. The national park is home to the rare East Caucasian tur (Capra cylindricornis), a mountain dwelling goat antelope found only in the eastern half of the Caucasus Mountains. Animals as the roe deer, bear, wild boar, lynx, fox, rabbit, squirrel, wolf, etc. are found on the territory of this park.\n\nThe predecessor of Absheron National Park during Soviet times was the Absheron State Nature Preserve which was created in July 1969 in order to protect gazelle, Caspian seal and water birds inhabited in the territory. The climate of the area is mild-hot, specific to semi-desert and dry steppe. Types and phytomass of flora is too poor here, plants are changed respective of water and saltiness regime of area. Sea coastal sand plants (42,6%), meadows with jigilgamish and paz grass (13,2%), one-year saline grasses (5,2%) etc. are spread. Ephemeras also develop well in early spring. In dry area gazelle, jackal, fox, rabbit, badger, in Caspian waters seal and various fishes, birds such as silver gull, wheezing swan, grey and red-headed black, white-eyed black ducks, big white bittern, sandpiper, bald-coot, marsh belibagli, sea bozcha and other migrant birds have inhabited here.\nAnimals and birds inhabiting in Shirvan National Park, names of which have been included in the Red Books, exist in Absheron National Park as well.\n\nThe Shakhdag National Park was created in 2006, and became the largest national park not only Azerbaijan but in the whole Caucasus.\n\nThe Shahdag National Park is located in northern Azerbaijan, on the border with Russia and Georgia at the Greater Caucasus Mountains. The World Bank has allocated a $17 million loan and $8 million grant for the national park's creation, while the government of Japan has provided $8 million as a grant for the project implementation. Shahdag National Park will help address ecological issues and build a tourist infrastructure in the Caucasus for visitors .\n\nThe Göygöl National Park was created in 2008. The predecessor of Göygöl National Park during Soviet times was the \"Goy Gol State Reserve\" that was established in 1925. The Göygöl National Park is located in eastern Azerbaijan, on the northern slopes of the Lesser Caucasus and includes Lake Göygöl after which it is named. The area of Göygöl is almost entirely covered by forests and has a rich flora with over 420 plant species, including 20 which are endemic to the area. It also has a rich fauna, with mammals such as brown bears, Caucasian red deer, roe deer, lynx, etc. and birds such as the lammergeyer, raven, mountain partridge.\n\nThe Samur-Yalama National Park was created in 2012 and is currently the newest national park of Azerbaijan. The major part of the national park is in the Caspian coastal zone and is covered with forests. The types of habitats include the littoral zone, forest, bush and steppe. Some characteristic species are black kite, eastern imperial eagle, otter, reed cat, lynx, chamois, Caspian red deer and brown bear. In the coastal waters belonging to the national park, stellate sturgeon, brown trout, eel, pikeperch and Caspian kutum can be found.\n\n\n"}
{"id": "3025956", "url": "https://en.wikipedia.org/wiki?curid=3025956", "title": "Oxfordian (stage)", "text": "Oxfordian (stage)\n\nThe Oxfordian is, in the ICS' geologic timescale, the earliest age of the Late Jurassic epoch, or the lowest stage of the Upper Jurassic series. It spans the time between 163.5 ± 4 Ma and 157.3 ± 4 Ma (million years ago). The Oxfordian is preceded by the Callovian and is followed by the Kimmeridgian.\n\nThe Oxfordian stage was called \"Clunch Clay and Shale\" by William Smith (1815–1816); in 1818 W. Buckland described them under the unwieldy title \"Oxford, Forest or Fen Clay\". The term Oxfordian was introduced by Alcide d'Orbigny in 1844. The name is derived from the English county of Oxford, where the beds are well developed, but they crop out almost continuously from Dorsetshire to the coast of Yorkshire, generally forming low, broad valleys. They are well exposed at Weymouth, Oxford, Bedford, Peterborough, and in the cliffs at Scarborough, Red Cliff and Gristhorpe Bay. Rocks of this age are found also in Uig and Skye.\n\nThe base of the Oxfordian stage is defined as the point in the stratigraphic record where the ammonite species \"Brightia thuouxensis\" first appears. A global reference profile for the base (a GSSP) had in 2009 not yet been assigned. The top of the Oxfordian stage (the base of the Kimmeridgian) is at the first appearance of ammonite species \"Pictonia baylei\".\n\nIn the Tethys domain, the Oxfordian contains six ammonite biozones:\n\n\n"}
{"id": "11574945", "url": "https://en.wikipedia.org/wiki?curid=11574945", "title": "Paraná and Etendeka traps", "text": "Paraná and Etendeka traps\n\nThe Paraná-Etendeka traps (or Paraná and Etendeka Plateau; or Paraná and Etendeka Province) comprise a large igneous province that includes both the main Paraná traps (in Paraná Basin, a South American geological basin) as well as the smaller severed portions of the flood basalts at the Etendeka traps (in northwest Namibia and southwest Angola). The original basalt flows occurred 128 to 138 million years ago. The province had a post-flow surface area of 1.5 x 10 km² (580,000 miles²) and an original volume projected to be in excess of 2.3 x 10 km³.\n\nThe basalt samples at Paraná and Etendeka have an age of about 132 Ma, during the Valanginian stage of the Early Cretaceous. Indirectly, the rifting and extension are probably the origin of the Paraná and Etendeka traps and it could be the origin of the Gough and Tristan da Cunha Islands as well, as they are connected by the Walvis Ridge (Gough/Tristan hotspot). The seamounts of the Rio Grande Rise (25°S to 35°S) that go eastwards from the Paraná side are part of this traps system.\n\nInterpretations of geochemistry, including isotopes, have led geologists to conclude that the magmas forming the traps and associated igneous rocks originated by melting of asthenosphic mantle due to the arrival of a mantle plume to the base of Earth's lithosphere. Then much of the magma was contaminated with crustal materials prior to their eruption. Some plutonic rocks related to the traps escaped crustal contamination reflecting more directly the source of the magmas in the mantle.\n\nA type of rock called ignimbrite is found in some parts of the traps indicating explosive volcanic activity. The Paraná Traps possibly contains the site of the single largest explosive volcanic eruption known in Earth's history.\n\n"}
{"id": "19191080", "url": "https://en.wikipedia.org/wiki?curid=19191080", "title": "Probabilistic prognosis", "text": "Probabilistic prognosis\n\nProbabilistic prognosis means the anticipation of future events based on a probabilistic structure of past experiences of individual and present situations. Past experiences and present situations help to create hypothesis about the forthcoming future and attributes certain probabilities to them. According to probabilistic prognosis, the preparation of an individual occurs to corresponding actions.\n\nThis ability is the result of biological evolution in a probabilistically organized environment. The prognosis of living creatures optimizes the results of their actions, and therefore they are adequate, exactly to those variable characteristics of environment on which the success of action – satisfaction of needs, achieving goals, depends on.\n\nUnder some pathological conditions, such as schizophrenia, local damage of the brain, the probabilistic prognosis mechanism can be disturbed. The theory of probabilistic prognosis originated as a continuation of Nikolai Bernstein's ideas stated in his work \"Essays of the Physiology of Movement and Physiology of Activity\" (1966).\nA new direction of studies of probabilistic prognosis started with works of Professor J. Feigenberg (1969), Professor M. Tsiskaridze (1969), and Professor V. Ivannikov (1971), and later were widely explored by other scientists. Studies of probabilistic prognosis remain topical until now and have many followers.\n\n"}
{"id": "39050866", "url": "https://en.wikipedia.org/wiki?curid=39050866", "title": "Protoflight", "text": "Protoflight\n\nProtoflight is a portmanteau of \"prototype\" and \"flight hardware\". As defined by NASA Technical Standard NASA-STD-7002A, it refers to a strategy where no test-dedicated qualification article exists and all production (flight) hardware is intended for flight. An example of a program using protoflight methods is the Mars Orbiter Laser Altimeter project.\n\nA protoflight approach carries a higher technical risk approach compared to a full qualification test program since it has no demonstrated life capability over the anticipated life cycle of the hardware, but is a technology development design process that utilizes higher risk tolerances, agile management practices, and quick responsiveness that are needed for certain prototype flight projects or missions.\n\n"}
{"id": "4148025", "url": "https://en.wikipedia.org/wiki?curid=4148025", "title": "Radiation-absorbent material", "text": "Radiation-absorbent material\n\nRadiation-absorbent material, usually known as RAM, is a material which has been specially designed and shaped to absorb incident RF radiation (also known as non-ionising radiation), as effectively as possible, from as many incident directions as possible. The more effective the RAM, the lower the resulting level of reflected RF radiation. Many measurements in electromagnetic compatibility (EMC) and antenna radiation patterns require that spurious signals arising from the test setup, including reflections, are negligible to avoid the risk of causing measurement errors and ambiguities.\n\nOne of the most effective types of RAM comprises arrays of pyramid shaped pieces, each of which is constructed from a suitably lossy material. To work effectively, all internal surfaces of the anechoic chamber must be entirely covered with RAM. Sections of RAM may be temporarily removed to install equipment but they must be replaced before performing any tests. To be sufficiently lossy, RAM can be neither a good electrical conductor nor a good electrical insulator as neither type actually absorbs any power. Typically pyramidal RAM will comprise a rubberized foam material impregnated with controlled mixtures of carbon and iron. The length from base to tip of the pyramid structure is chosen based on the lowest expected frequency and the amount of absorption required. For low frequency damping, this distance is often 24 inches, while high-frequency panels are as short as 3–4 inches. Panels of RAM are typically installed on the walls of an EMC test chamber with the tips pointing inward to the chamber. Pyramidal RAM attenuates signal by two effects: scattering and absorption. Scattering can occur both coherently, when reflected waves are in-phase but directed away from the receiver, or incoherently where waves are picked up by the receiver but are out of phase and thus have lower signal strength. This incoherent scattering also occurs within the foam structure, with the suspended carbon particles promoting destructive interference. Internal scattering can result in as much as 10 dB of attenuation. Meanwhile, the pyramid shapes are cut at angles that maximize the number of bounces a wave makes within the structure. With each bounce, the wave loses energy to the foam material and thus exits with lower signal strength. An alternative type of RAM comprises flat plates of ferrite material, in the form of flat tiles fixed to all interior surfaces of the chamber. This type has a smaller effective frequency range than the pyramidal RAM and is designed to be fixed to good conductive surfaces. It is generally easier to fit and more durable than the pyramidal type RAM but is less effective at higher frequencies. Its performance might however be quite adequate if tests are limited to lower frequencies (ferrite plates have a damping curve that makes them most effective between 30–1000 MHz). There is also a hybrid type, a ferrite in pyramidal shape. Containing the advantages of both technologies, the frequency range can be maximized while the pyramid remains small (10 cm).\n\nRadar-absorbent RAM is used in stealth technology to disguise a vehicle or structure from radar detection. A material's absorbency at a given frequency of radar wave depends upon its composition. RAM cannot perfectly absorb radar at any frequency, but any given composition does have greater absorbency at some frequencies than others; no one RAM is suited to absorption of all radar frequencies. A common misunderstanding is that RAM makes an object invisible to radar. A radar-absorbent material can significantly reduce an object's radar cross-section in specific radar frequencies, but it does not result in \"invisibility\" on any frequency. Bad weather may contribute to deficiencies in stealth capability.\n\nThe earliest forms of stealth coating were the materials called \"Sumpf\" and \"Schornsteinfeger\", a coating used by the German navy during World War II for the snorkels (or periscopes) of submarines, to lower their reflectivity in the 20-cm radar band the Allies used. The material had a layered structure and was based on graphite particles and other semiconductive materials embedded in a rubber matrix. The material's efficiency was partially reduced by the action of sea water.\n\nA related use was planned for the Horten Ho 229 aircraft. The adhesive which bonded plywood sheets in its skin was impregnated with graphite particles which were intended to reduce its visibility to Britain's radar.\n\nOne of the most commonly known types of RAM is iron ball paint. It contains tiny spheres coated with carbonyl iron or ferrite. Radar waves induce molecular oscillations from the alternating magnetic field in this paint, which leads to conversion of the radar energy into heat. The heat is then transferred to the aircraft and dissipated. The iron particles in the paint are obtained by decomposition of iron pentacarbonyl and may contain traces of carbon, oxygen, and nitrogen. One technique used in the F-117A Nighthawk and other such stealth aircraft is to use electrically isolated carbonyl iron balls of specific dimensions suspended in a two-part epoxy paint. Each of these microscopic spheres is coated in silicon dioxide as an insulator through a proprietary process. Then, during the fabrication panel process, while the paint is still liquid, a magnetic field is applied with a specific Gauss strength and at a specific distance to create a magnetic field patterns in the carbonyl iron balls within the liquid paint ferrofluid. The paint then cures [hardens] whilst the magnetic field holds the particles in suspension, locking the balls into their magnetic pattern. Some experimentation has been done applying opposing north-south magnetic fields to opposing sides of the painted panels causing the carbonyl iron particles to align (standing up on end so they are three-dimensionally parallel to the magnetic field). The carbonyl iron ball paint is most effective when the balls are evenly dispersed, electrically isolated, and present a gradient of progressively greater density to the incoming radar waves. A related type of RAM consists of neoprene polymer sheets with ferrite grains or conductive carbon black particles (containing about 0.30% of crystalline graphite by cured weight) embedded in the polymer matrix. The tiles were used on early versions of the F-117A Nighthawk, although more recent models use painted RAM. The painting of the F-117 is done by industrial robots so the paint can be applied consistently in specific layer thicknesses and densities. The plane is covered in tiles \"glued\" to the fuselage and the remaining gaps are filled with iron ball \"glue.\" The United States Air Force introduced a radar-absorbent paint made from both ferrofluidic and nonmagnetic substances. By reducing the reflection of electromagnetic waves, this material helps to reduce the visibility of RAM-painted aircraft on radar. The Israeli firm Nanoflight has also made a radar-absorbing paint that uses nanoparticles. The Republic of China (Taiwan)'s military has also successfully developed radar-absorbing paint which is currently used on Taiwanese stealth warships and the Taiwanese-built stealth jet fighter which is currently in development in response to the development of stealth technology by their rival, the mainland People's Republic of China which is known to have displayed both stealth warships and planes to the public.\n\nFoam absorber is used as lining of anechoic chambers for electromagnetic radiation measurements. This material typically consists of a fireproofed urethane foam loaded with conductive carbon black [carbonyl iron spherical particles, and/or crystalline graphite particles] in mixtures between 0.05% and 0.1% (by weight in finished product), and cut into square pyramids with dimensions set specific to the wavelengths of interest. Further improvements can be made when the conductive particulates are layered in a density gradient, so the tip of the pyramid has the lowest percentage of particles and the base contains the highest density of particles. This presents a \"soft\" impedance change to incoming radar waves and further reduces reflection (echo). The length from base to tip, and width of the base of the pyramid structure is chosen based on the lowest expected frequency when a wide-band absorber is sought. For low-frequency damping in military applications, this distance is often 24 in, while high-frequency panels are as short as 3-4 in. An example of a high-frequency application would be the police radar (speed-measuring radar K and Ka band), the pyramids would have a dimension around 4 in long and a 2 x 2-in base. That pyramid would set on a 2 x 2-in cubical base that is 1in high (total height of pyramid and base of about 5 in). The four edges of the pyramid are softly sweeping arcs giving the pyramid a slightly \"bloated\" look. This arc provides some additional scatter and prevents any sharp edge from creating a coherent reflection. Panels of RAM are installed with the tips of the pyramids pointing toward the radar source. These pyramids may also be hidden behind an outer nearly radar-transparent shell where aerodynamics are required. Pyramidal RAM attenuates signal by scattering and absorption. Scattering can occur both coherently, when reflected waves are in-phase but directed away from the receiver, or incoherently where waves may be reflected back to the receiver but are out of phase and thus have lower signal strength. A good example of coherent reflection is in the faceted shape of the F-117A stealth aircraft which presents angles to the radar source such that coherent waves are reflected away from the point of origin (usually the detection source). Incoherent scattering also occurs within the foam structure, with the suspended conductive particles promoting destructive interference. Internal scattering can result in as much as 10 dB of attenuation. Meanwhile, the pyramid shapes are cut at angles that maximize the number of bounces a wave makes within the structure. With each bounce, the wave loses energy to the foam material and thus exits with lower signal strength. Other foam absorbers are available in flat sheets, using an increasing gradient of carbon loadings in different layers. Absorption within the foam material occurs when radar energy is converted to heat in the conductive particle. Therefore, in applications where high radar energies are involved, cooling fans are used to exhaust the heat generated. \n\nA Jaumann absorber or Jaumann layer is a radar-absorbent substance. When first introduced in 1943, the Jaumann layer consisted of two equally spaced reflective surfaces and a conductive ground plane. One can think of it as a generalized, multilayered Salisbury screen, as the principles are similar. Being a resonant absorber (i.e. it uses wave interfering to cancel the reflected wave), the Jaumann layer is dependent upon the λ/4 spacing between the first reflective surface and the ground plane and between the two reflective surfaces (a total of λ/4 + λ/4 ). Because the wave can resonate at two frequencies, the Jaumann layer produces two absorption maxima across a band of wavelengths (if using the two layers configuration). These absorbers must have all of the layers parallel to each other and the ground plane that they conceal. More elaborate Jaumann absorbers use series of dielectric surfaces that separate conductive sheets. The conductivity of those sheets increases with proximity to the ground plane.\n\nSplit-ring resonators (SRRs) in various test configurations have been shown to be extremely effective as radar absorbers. SRR technology can be used in conjunction with the technologies above to provide a cumulative absorption effect. SRR technology is particularly effective when used on faceted shapes that have perfectly flat surfaces that present no direct reflections back to the radar source (such as the F-117A). This technology uses photographic process to create a resist layer on a thin (about 0.007 in) copper foil on a dielectric backing (thin circuit board material) etched into tuned resonator arrays, each individual resonator being in a \"C\" shape (or other shape—such as a square). Each SRR is electrically isolated and all dimensions are carefully specified to optimize absorption at a specific radar wavelength. Not being a closed loop \"O\", the opening in the \"C\" presents a gap of specific dimension which acts as a capacitor. At 35 GHz, the diameter of the \"C\" is near 5 mm. The resonator can be tuned to specific wavelengths and multiple SRRs can be stacked with insulating layers of specific thicknesses between them to provide a wide-band absorption of radar energy. When stacked, the smaller SRRs (high-frequency) in the range face the radar source first (like a stack of donuts that get progressively larger as one moves away from the radar source) stacks of three have been shown to be effective in providing wide-band attenuation. SRR technology acts very much in the same way that antireflective coatings operate at optical wavelengths. SRR technology provides the most effective radar attenuation of any technologies known previously and is one step closer to reaching complete invisibility (total stealth, \"cloaking\"). Work is also progressing in visual wavelengths, as well as infrared wavelengths (LIDAR-absorbing materials). \n\nRadars work in the microwave frequency range, which can be absorbed by multi-wall nanotubes (MWNTs). Applying the MWNTs to the aircraft would cause the radar to be absorbed and therefore seem to have a smaller radar cross-section. One such application could be to paint the nanotubes onto the plane. Recently there has been some work done at the University of Michigan regarding carbon nanotubes usefulness as stealth technology on aircraft. It has been found that in addition to the radar absorbing properties, the nanotubes neither reflect nor scatter visible light, making it essentially invisible at night, much like painting current stealth aircraft black except much more effective. Current limitations in manufacturing, however, mean that current production of nanotube-coated aircraft is not possible. One theory to overcome these current limitations is to cover small particles with the nanotubes and suspend the nanotube-covered particles in a medium such as paint, which can then be applied to a surface, like a stealth aircraft.\n\nSilicon carbide is the same material used in many sharpening stones or discs. This material in finely powdered form is added to standard heat resistant paint base to absorb radar on stealth aircraft. To check radar or general electromagnetic wave absorption of materials, they can be tested in an ordinary microwave oven. Water works very well as does silicon carbide, which is why such materials heat well. Most metals reflect radio waves so they are also detected by radar, and thus cannot be used or heated in a microwave oven.\n\n\n\n"}
{"id": "11121523", "url": "https://en.wikipedia.org/wiki?curid=11121523", "title": "Roberto Mantovani", "text": "Roberto Mantovani\n\nRoberto Mantovani (25 March 1854 – 10 January 1933), was an Italian geologist and violinist.\n\nMantovani was born in Parma. His father, Timoteo, died seven months after his birth. His mother, Luigia Ferrari, directed him to studies, and at the age of 11 he was accepted as a boarder in the Royal School of Music, where he was conferred with the Honorary Degree in August 1872. He always preferred the exact sciences and literature to music.\n\nIn 1889 and 1909 Mantovani published a hypothesis of an expanding earth and continental drift. He assumed that a closed continent covered the entire surface of a smaller earth. Through volcanic activity because of thermal expansion this continent broke, so that the new continents were drifting away from each other because of further expansion of the rip-zones, where now the oceans lie.\nAlfred Wegener saw similarities to his own theory, but did not support Mantovani's earth-expansion hypothesis. He wrote: \nHe died in Paris.\n\n"}
{"id": "1994795", "url": "https://en.wikipedia.org/wiki?curid=1994795", "title": "Scale height", "text": "Scale height\n\nIn various scientific contexts, a scale height is a distance over which a quantity decreases by a factor of \"e\" (approximately 2.72, the base of natural logarithms). It is usually denoted by the capital letter \"H\".\n\nFor planetary atmospheres, scale height is the increase in altitude for which the atmospheric pressure decreases by a factor of \"e\". The scale height remains constant for a particular temperature. It can be calculated by\n\nor equivalently\n\nwhere:\n\n\nThe pressure (force per unit area) at a given altitude is a result of the weight of the overlying atmosphere. If at a height of \"z\" the atmosphere has density \"ρ\" and pressure \"P\", then moving upwards at an infinitesimally small height \"dz\" will decrease the pressure by amount \"dP\", equal to the weight of a layer of atmosphere of thickness \"dz\".\n\nThus:\n\nwhere \"g\" is the acceleration due to gravity. For small \"dz\" it is possible to assume \"g\" to be constant; the minus sign indicates that as the height increases the pressure decreases. Therefore, using the equation of state for an ideal gas of mean molecular mass \"M\" at temperature \"T,\" the density can be expressed as\n\nCombining these equations gives\n\nwhich can then be incorporated with the equation for \"H\" given above to give:\n\nwhich will not change unless the temperature does. Integrating the above and assuming where \"P\" is the pressure at height \"z\" = 0 (pressure at sea level) the pressure at height \"z\" can be written as:\n\nThis translates as the pressure decreasing exponentially with height.\n\nIn Earth's atmosphere, the pressure at sea level \"P\" averages about 1.01×10 Pa, the mean molecular mass of dry air is 28.964 u and hence 28.964 × 1.660×10 = 4.808×10 kg, and \"g\" = 9.81 m/s². As a function of temperature the scale height of Earth's atmosphere is therefore 1.38/(4.808×9.81)×10 = 29.26 m/deg. This yields the following scale heights for representative air temperatures.\n\nThese figures should be compared with the temperature and density of Earth's atmosphere plotted at NRLMSISE-00, which shows the air density dropping from 1200 g/m at sea level to 0.5 = .125 g/m at 70 km, a factor of 9600, indicating an average scale height of 70/ln(9600) = 7.64 km, consistent with the indicated average air temperature over that range of close to 260 K.\n\nNote:\n\nApproximate atmospheric scale heights for selected Solar System bodies follow.\n\n"}
{"id": "10882863", "url": "https://en.wikipedia.org/wiki?curid=10882863", "title": "Securing America's Energy Independence Act of 2007", "text": "Securing America's Energy Independence Act of 2007\n\nOn January 18, 2007, Congressmen Michael McNulty (D-NY) and Dave Camp (R-MI) introduced legislation to reinvest funds in America's most abundant renewable resource – solar power. \nTitle: \"Securing America's Energy Independence Act of 2007.\", H.R. 550 \nJanuary 18, 2007: Referred to the House Committee on Ways and Means.\n\nFebruary 14, 2007 – Senators Gordon H. Smith (R-OR) and Ken Salazar (D-CO) introduced legislation to stimulate investment in America's most abundant renewable resource – solar power. \nTitle: \"Securing America's Energy Independence Act of 2007.\", S. 590\nFebruary 14, 2007 Referred to Senate committee. Status: Read twice and referred to the Committee on Finance.\n\n"}
{"id": "1085343", "url": "https://en.wikipedia.org/wiki?curid=1085343", "title": "Semi-empirical mass formula", "text": "Semi-empirical mass formula\n\nIn nuclear physics, the semi-empirical mass formula (SEMF) (sometimes also called Weizsäcker's formula, or the Bethe–Weizsäcker formula, or the Bethe–Weizsäcker mass formula to distinguish it from the Bethe–Weizsäcker process) is used to approximate the mass and various other properties of an atomic nucleus from its number of protons and neutrons. As the name suggests, it is based partly on theory and partly on empirical measurements. The theory is based on the liquid drop model proposed by George Gamow, which can account for most of the terms in the formula and gives rough estimates for the values of the coefficients. It was first formulated in 1935 by German physicist Carl Friedrich von Weizsäcker, and although refinements have been made to the coefficients over the years, the structure of the formula remains the same today.\n\nThe SEMF gives a good approximation for atomic masses and several other effects, but does not explain the appearance of magic numbers of protons and neutrons, and the extra binding-energy and measure of stability that are associated with these numbers of nucleons.\n\nThe liquid drop model in nuclear physics treats the nucleus as a drop of incompressible nuclear fluid of very high density. It was first proposed by George Gamow and then developed by Niels Bohr and John Archibald Wheeler. The nucleus is made of nucleons (protons and neutrons), which are held together by the nuclear force (a residual effect of the strong force). This is very similar to the structure of a spherical liquid drop made of microscopic molecules. This is a crude model that does not explain all the properties of the nucleus, but does explain the spherical shape of most nuclei. It also helps to predict the nuclear binding energy and to assess how much is available for consumption.\n\nMathematical analysis of the theory delivers an equation which attempts to predict the binding energy of a nucleus in terms of the numbers of protons and neutrons it contains. This equation has five terms on its right hand side. These correspond to the cohesive binding of all the nucleons by the nuclear force, a surface energy term, the electrostatic (Coulomb) mutual repulsion of the protons, an asymmetry term (derivable from the protons and neutrons occupying independent quantum momentum states) and a pairing term (partly derivable from the protons and neutrons occupying independent quantum spin states).\n\nIf we consider the sum of the following five types of energies, then the picture of a nucleus as a drop of incompressible liquid roughly accounts for the observed variation of binding energy of the nucleus:\n\n\"Volume energy\". When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. So, this nuclear energy is proportional to the volume.\n\n\"Surface energy\". A nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.\n\n\"Coulomb Energy\". The electric repulsion between each pair of protons in a nucleus contributes toward decreasing its binding energy.\n\n\"Asymmetry energy\" (also called Pauli Energy). An energy associated with the Pauli exclusion principle. Were it not for the Coulomb energy, the most stable form of nuclear matter would have the same number of neutrons as protons, since unequal numbers of neutrons and protons imply filling higher energy levels for one type of particle, while leaving lower energy levels vacant for the other type.\n\n\"Pairing energy\". An energy which is a correction term that arises from the tendency of proton pairs and neutron pairs to occur. An even number of particles is more stable than an odd number.\n\nIn the following formula, let \"A\" be the total number of nucleons, \"Z\" the number of protons, and \"N\" the number of neutrons, so that \"A\" = \"Z\" + \"N\".\n\nThe mass of an atomic nucleus is given by\nwhere formula_2 and formula_3 are the rest mass of a proton and a neutron, respectively, and formula_4 is the binding energy of the nucleus. The semi-empirical mass formula states that the binding energy will take the following form:\n\nEach of the terms in this formula has a theoretical basis, as will be explained below. The coefficients formula_6, formula_7, formula_8, formula_9 and a coefficient that appears in the formula for formula_10 are determined empirically.\n\nThe term formula_11 is known as the \"volume term\". The volume of the nucleus is proportional to \"A\", so this term is proportional to the volume, hence the name.\n\nThe basis for this term is the strong nuclear force. The strong force affects both protons and neutrons, and as expected, this term is independent of \"Z\". Because the number of pairs that can be taken from \"A\" particles is formula_12, one might expect a term proportional to formula_13. However, the strong force has a very limited range, and a given nucleon may only interact strongly with its nearest neighbors and next nearest neighbors. Therefore, the number of pairs of particles that actually interact is roughly proportional to \"A\", giving the volume term its form.\n\nThe coefficient formula_14 is smaller than the binding energy of the nucleons to their neighbours formula_15, which is of order of 40 MeV. This is because the larger the number of nucleons in the nucleus, the larger their kinetic energy is, due to the Pauli exclusion principle. If one treats the nucleus as a Fermi ball of formula_16 nucleons, with equal numbers of protons and neutrons, then the total kinetic energy is formula_17, with formula_18 the Fermi energy which is estimated as 28 MeV. Thus the expected value of formula_14 in this model is formula_20, not far from the measured value.\n\nThe term formula_21 is known as the \"surface term\". This term, also based on the strong force, is a correction to the volume term.\n\nThe volume term suggests that each nucleon interacts with a constant number of nucleons, independent of \"A\". While this is very nearly true for nucleons deep within the nucleus, those nucleons on the surface of the nucleus have fewer nearest neighbors, justifying this correction. This can also be thought of as a surface tension term, and indeed a similar mechanism creates surface tension in liquids.\n\nIf the volume of the nucleus is proportional to \"A\", then the radius should be proportional to formula_22 and the surface area to formula_23. This explains why the surface term is proportional to formula_23. It can also be deduced that formula_7 should have a similar order of magnitude as formula_6.\n\nThe term formula_27 or formula_28 is known as the \"Coulomb\" or \"electrostatic term\".\n\nThe basis for this term is the electrostatic repulsion between protons. To a very rough approximation, the nucleus can be considered a sphere of uniform charge density. The potential energy of such a charge distribution can be shown to be\n\nwhere \"Q\" is the total charge and \"R\" is the radius of the sphere. Identifying \"Q\" with formula_30, and noting as above that the radius is proportional to formula_22, we get close to the form of the Coulomb term. However, because electrostatic repulsion will only exist for more than one proton, formula_32 becomes formula_33. The value of formula_34 can be approximately calculated using the equation above:\n\nEmpirical nuclear radius:\n\nQuantum charge integers:\n\nSolving by substitution:\n\nPotential energy of charge distribution:\n\nElectrostatic Coulomb constant:\n\nThe value of formula_8 using the fine structure constant:\n\nwhere formula_43 is the fine structure constant and formula_44 is the radius of a nucleus, giving formula_45 to be approximately 1.25 femtometers. formula_46 is the proton Compton radius and formula_2 the proton mass. This gives formula_8 an approximate theoretical value of 0.691 MeV, not far from the measured value.\n\nThe term formula_50 or formula_51 is known as the \"asymmetry term\". Note that as formula_52, the parenthesized expression can be rewritten as formula_53. The form formula_54 is used to keep the dependence on \"A\" explicit, as it will be important for a number of uses of the formula.\n\nThe theoretical justification for this term is more complex. The Pauli exclusion principle states that no two identical fermions can occupy exactly the same quantum state in an atom. At a given energy level, there are only finitely many quantum states available for particles. What this means in the nucleus is that as more particles are \"added\", these particles must occupy higher energy levels, increasing the total energy of the nucleus (and decreasing the binding energy). Note that this effect is not based on any of the fundamental forces (gravitational, electromagnetic, etc.), only the Pauli exclusion principle.\n\nProtons and neutrons, being distinct types of particles, occupy different quantum states. One can think of two different \"pools\" of states, one for protons and one for neutrons. Now, for example, if there are significantly more neutrons than protons in a nucleus, some of the neutrons will be higher in energy than the available states in the proton pool. If we could move some particles from the neutron pool to the proton pool, in other words change some neutrons into protons, we would significantly decrease the energy. The imbalance between the number of protons and neutrons causes the energy to be higher than it needs to be, \"for a given number of nucleons\". This is the basis for the asymmetry term.\n\nThe actual form of the asymmetry term can again be derived by modelling the nucleus as a Fermi ball of protons and neutrons. Its total kinetic energy is \nwhere formula_56, formula_57 are the numbers of protons and neutrons and formula_58, formula_59 are their Fermi energies. Since the latter are proportional to formula_60 and formula_61, respectively, one gets\nThe leading expansion in the difference formula_63 is then \nAt the zeroth order expansion the kinetic energy is just the Fermi energy formula_65 multiplied by formula_66. Thus we get\n\nThe first term contributes to the volume term in the semi-empirical mass formula, and the second term is minus the asymmetry term (remember the kinetic energy contributes to the total binding energy with a \"negative\" sign).\n\nformula_18 is 38 MeV, so calculating formula_9 from the equation above, we get only half the measured value. The discrepancy is explained by our model not being accurate: nucleons in fact interact with each other, and are not spread evenly across the nucleus. For example, in the shell model, a proton and a neutron with overlapping wavefunctions will have a greater strong interaction between them and stronger binding energy. This makes it energetically favourable (i.e. having lower energy) for protons and neutrons to have the same quantum numbers (other than isospin), and thus increase the energy cost of asymmetry between them.\n\nOne can also understand the asymmetry term intuitively, as follows. It should be dependent on the absolute difference formula_70, and the form formula_71 is simple and differentiable, which is important for certain applications of the formula. In addition, small differences between \"Z\" and \"N\" do not have a high energy cost. The \"A\" in the denominator reflects the fact that a given difference formula_70 is less significant for larger values of \"A\".\n\nThe term formula_10 is known as the \"pairing term\" (possibly also known as the pairwise interaction). This term captures the effect of spin-coupling. It is given by:\n\nwhere formula_75 is found empirically to have a value of about 1000 keV, slowly decreasing with mass number \"A\". The dependence on mass number is commonly parametrized as\n\nThe value of the exponent \"k\" is determined from experimental binding energy data. In the past its value was often assumed to be −3/4, but modern experimental data indicate that a value of −1/2 is nearer the mark:\n\nDue to the Pauli exclusion principle the nucleus would have a lower energy if the number of protons with spin up were equal to the number of protons with spin down. This is also true for neutrons. Only if both \"Z\" and \"N\" are even can both protons and neutrons have equal numbers of spin up and spin down particles. This is a similar effect to the asymmetry term.\n\nThe factor formula_79 is not easily explained theoretically. The Fermi ball calculation we have used above, based on the liquid drop model but neglecting interactions, will give an formula_80 dependence, as in the asymmetry term. This means that the actual effect for large nuclei will be larger than expected by that model. This should be explained by the interactions between nucleons; For example, in the shell model, two protons with the same quantum numbers (other than spin) will have completely overlapping wavefunctions and will thus have greater strong interaction between them and stronger binding energy. This makes it energetically favourable (i.e. having lower energy) for protons to form pairs of opposite spin. The same is true for neutrons.\n\nThe coefficients are calculated by fitting to experimentally measured masses of nuclei. Their values can vary depending on how they are fitted to the data. Several examples are as shown below, with units of megaelectronvolts.\n\nThe semi-empirical mass formula provides a good fit to heavier nuclei, and a poor fit to very light nuclei, especially He. This is because the formula does not consider the internal shell structure of the nucleus. For light nuclei, it is usually better to use a model that takes this structure into account.\n\nBy maximizing \"E\"(\"A\",\"Z\") with respect to \"Z\", we find the best neutrons to protons ratio \"N/Z\" for a given atomic weight \"A\".\nWe get \nThis is roughly 1 for light nuclei, but for heavy nuclei the ratio grows in good agreement with nature.\n\nBy substituting the above value of \"Z\" back into \"E\" one obtains the binding energy as a function of the atomic weight, \"E\"(\"A\").\nMaximizing \"E\"(\"A\")/\"A\" with respect to \"A\" gives the nucleus which is most strongly bound, i.e. most stable. The value we get is \"A\" = 63 (copper), close to the measured values of \"A\" = 62 (nickel) and \"A\" = 58 (iron).\n\n\n"}
{"id": "44718", "url": "https://en.wikipedia.org/wiki?curid=44718", "title": "Soapstone", "text": "Soapstone\n\nSoapstone (also known as steatite or soaprock) is a talc-schist, which is a type of metamorphic rock. It is largely composed of the mineral talc, thus is rich in magnesium. It is produced by dynamothermal metamorphism and metasomatism, which occur in the zones where tectonic plates are subducted, changing rocks by heat and pressure, with influx of fluids, but without melting. It has been a medium for carving for thousands of years.\n\nPetrologically, soapstone is composed dominantly of talc, with varying amounts of chlorite and amphiboles (typically tremolite, anthophyllite, and cummingtonite, hence its obsolete name, magnesiocummingtonite), and trace to minor iron-chromium oxides. It may be schistose or massive. Soapstone is formed by the metamorphism of ultramafic protoliths (\"e.g.\" dunite or serpentinite) and the metasomatism of siliceous dolostones.\n\nBy mass, \"pure\" steatite is roughly 63.37% silica, 31.88% magnesia, and 4.74% water. It commonly contains minor quantities of other oxides such as CaO or AlO.\n\nPyrophyllite, a mineral very similar to talc, is sometimes called soapstone in the generic sense, since its physical characteristics and industrial uses are similar, and because it is also commonly used as a carving material. However, this mineral typically does not have such a soapy feel as soapstone.\n\nSoapstone is relatively soft because of its high talc content, talc having a definitional value of 1 on the Mohs hardness scale. Softer grades may feel similar to soap when touched, hence the name. No fixed hardness is given for soapstone because the amount of talc it contains varies widely, from as little as 30% for architectural grades such as those used on countertops, to as much as 80% for carving grades.\n\nSoapstone is often used as an insulator for housing and electrical components, due to its durability and electrical characteristics and because it can be pressed into complex shapes before firing. Soapstone undergoes transformations when heated to temperatures of 1000–1200°C into enstatite and cristobalite; in the Mohs scale, this corresponds to an increase in hardness to 5.5–6.5.\n\nAncient Egyptian Scarab signet/amulets were most commonly made from glazed steatite.\nSoapstone is used for inlaid designs, sculpture, coasters, and kitchen countertops and sinks. The Inuit often used soapstone for traditional carvings. Some Native American tribes and bands make bowls, cooking slabs, and other objects from soapstone; historically, this was particularly common during the Late Archaic archaeological period.\n\nLocally quarried soapstone was used for gravemarkers in 19th century northeast Georgia, US, around Dahlonega, and Cleveland, as simple field stone and \"slot and tab\" tombs.\n\nVikings hewed soapstone directly from the stone face, shaped it into cooking pots, and sold these at home and abroad.\n\nSoapstone is sometimes used for construction of fireplace surrounds, cladding on wood-burning stoves, and as the preferred material for woodburning masonry heaters because it can absorb, store, and evenly radiate heat due to its high density and magnesite (MgCO) content. It is also used for countertops and bathroom tiling because of the ease of working the material and its property as the \"quiet stone\". A weathered or aged appearance occurs naturally over time as the patina is enhanced.\n\nThe ancient trading city of Tepe Yahya in southeastern Iran was a center for the production and distribution of soapstone in the fifth to third millennia BC. It was also used in Minoan Crete. At the Palace of Knossos, archaeological recovery has included a magnificent libation table made of steatite.\nThe Yoruba people of West Nigeria used soapstone for several statues, most notably at Esie, where archaeologists have uncovered hundreds of male and female statues about half of life size. The Yoruba of Ife also produced a miniature soapstone obelisk with metal studs called superstitiously \"the staff of Oranmiyan\".\nSoapstone has been used in India for centuries as a medium for carving. Mining to meet worldwide demand for soapstone is threatening the habitat of India's tigers.\n\nIn Brazil, especially in Minas Gerais, due to the abundance of soapstone mines in that Brazilian state, local artisans still craft objects from that material, including pots and pans, wine glasses, statues, jewel boxes, coasters, and vases. These handicrafts are commonly sold in street markets found in cities across the state. Some of the oldest towns, notably Congonhas, Tiradentes, and Ouro Preto, still have some of their streets paved with soapstone from colonial times.\n\nSome Native Americans use soapstone for smoking pipes; numerous examples have been found among artifacts of different cultures and are still in use today. Its low heat conduction allows for prolonged smoking without the pipe heating up uncomfortably.\n\nSome wood-burning stoves make use of soapstone to take advantage of its useful thermal and fire-resistant properties.\n\nSoapstone is also used to carve Chinese seals.\n\nCurrently, soapstone is most commonly used for architectural applications, such as counter tops, floor tiles, showerbases, and interior surfacing. \n\nThe active North American soapstone mines include one south of Quebec City with products marketed by Canadian Soapstone, the Treasure and Regal mines in Beaverhead County, Montana mined by the Barretts Minerals Company, and another in Central Virginia operated by the Alberene Soapstone Company. Architectural soapstone is mined in Canada, Brazil, India, and Finland and imported into the United States.\n\nWelders and fabricators use soapstone as a marker due to its resistance to heat; it remains visible when heat is applied. It has also been used for many years by seamstresses, carpenters, and other craftsmen as a marking tool because its marks are visible and not permanent.\n\nSoapstone can be used to create molds for casting objects from soft metals, such as pewter or silver. The soft stone is easily carved and is not degraded by heating. The slick surface of soapstone allows the finished object to be easily removed.\n\nSoapstones can be put in a freezer and later used in place of ice cubes to chill alcoholic beverages without diluting. Sometimes called whiskey stones, these were first introduced around 2007. Most whiskey stones feature a semipolished finish, retaining the soft look of natural soapstone, while others are highly polished.\n\nSteatite ceramics are low-cost biaxial porcelains of nominal composition (MgO)(SiO). Steatite is used primarily for its dielectric and thermal insulating properties in applications such as tile, substrates, washers, bushings, beads, and pigments. It is also used for high-voltage insulators, which have to stand large mechanical loads, e.g. insulators of mast radiators.\n\nPeople can be exposed to soapstone in the workplace by breathing it in, skin contact, or eye contact. \n\nThe Occupational Safety and Health Administration has set the legal limit (permissible exposure limit) for soapstone exposure in the workplace as 20 million particles per cubic foot over an 8-hour workday. The National Institute for Occupational Safety and Health has set a recommended exposure limit of 6 mg/m total exposure and 3 mg/m respiratory exposure over an 8-hour workday. At levels of 3000 mg/m, soapstone is immediately dangerous to life and health.\n\n\n\n"}
{"id": "42752", "url": "https://en.wikipedia.org/wiki?curid=42752", "title": "Sonoluminescence", "text": "Sonoluminescence\n\nSonoluminescence is the emission of short bursts of light from imploding bubbles in a liquid when excited by sound.\n\nThe sonoluminescence effect was first discovered at the University of Cologne in 1934 as a result of work on sonar. H. Frenzel and H. Schultes put an ultrasound transducer in a tank of photographic developer fluid. They hoped to speed up the development process. Instead, they noticed tiny dots on the film after developing and realized that the bubbles in the fluid were emitting light with the ultrasound turned on. It was too difficult to analyze the effect in early experiments because of the complex environment of a large number of short-lived bubbles. (This experiment is also ascribed to N. Marinesco and J. J. Trillat in 1933, which also credits them with independent discovery). This phenomenon is now referred to as multi-bubble sonoluminescence (MBSL).\n\nIn 1960 Dr. Peter Jarman from Imperial College of London proposed the most reliable theory of SL phenomenon. The collapsing bubble generates an imploding shock wave that compresses and heats the gas at the center of the bubble to extremely high temperature.\n\nIn 1989 an experimental advance was introduced by D. Felipe Gaitan and Lawrence Crum, who produced stable single-bubble sonoluminescence (SBSL). In SBSL, a single bubble trapped in an acoustic standing wave emits a pulse of light with each compression of the bubble within the standing wave. This technique allowed a more systematic study of the phenomenon, because it isolated the complex effects into one stable, predictable bubble. It was realized that the temperature inside the bubble was hot enough to melt steel, as seen in an experiment done in 2012; the temperature inside the bubble as it collapsed reached about 12,000 kelvins. Interest in sonoluminescence was renewed when an inner temperature of such a bubble well above one million kelvins was postulated. This temperature is thus far not conclusively proven; rather, recent experiments conducted by the University of Illinois at Urbana–Champaign indicate temperatures around .\n\nSonoluminescence can occur when a sound wave of sufficient intensity induces a gaseous cavity within a liquid to collapse quickly. This cavity may take the form of a pre-existing bubble, or may be generated through a process known as cavitation. Sonoluminescence in the laboratory can be made to be stable, so that a single bubble will expand and collapse over and over again in a periodic fashion, emitting a burst of light each time it collapses. For this to occur, a standing acoustic wave is set up within a liquid, and the bubble will sit at a pressure anti-node of the standing wave. The frequencies of resonance depend on the shape and size of the container in which the bubble is contained.\n\nSome facts about sonoluminescence: \n\nSpectral measurements have given bubble temperatures in the range from to , the exact temperatures depending on experimental conditions including the composition of the liquid and gas. Detection of very high bubble temperatures by spectral methods is limited due to the opacity of liquids to short wavelength light characteristic of very high temperatures.\n\nWriting in \"Nature\", chemists David J. Flannigan and Kenneth S. Suslick describe a method of determining temperatures based on the formation of plasmas. Using argon bubbles in sulfuric acid, their data show the presence of ionized molecular oxygen O, sulfur monoxide, and atomic argon populating high-energy excited states, which confirms a hypothesis that the bubbles have a hot plasma core. The ionization and excitation energy of dioxygenyl cations, which they observed, is 18 electronvolts. From this they conclude the core temperatures reach at least 20,000 kelvins.\n\nThe dynamics of the motion of the bubble is characterized to a first approximation by the Rayleigh–Plesset equation (named after Lord Rayleigh and Milton Plesset):\n\nThis is an approximate equation that is derived from the Navier–Stokes equations (written in spherical coordinate system) and describes the motion of the radius of the bubble \"R\" as a function of time \"t\". Here, \"μ\" is the viscosity, \"p\" the pressure, and \"γ\" the surface tension. The over-dots represent time derivatives. This equation, though approximate, has been shown to give good estimates on the motion of the bubble under the acoustically driven field except during the final stages of collapse. Both simulation and experimental measurement show that during the critical final stages of collapse, the bubble wall velocity exceeds the speed of sound of the gas inside the bubble. Thus a more detailed analysis of the bubble's motion is needed beyond Rayleigh–Plesset to explore the additional energy focusing that an internally formed shock wave might produce.\n\nThe mechanism of the phenomenon of sonoluminescence is unknown. Hypotheses include: hotspot, bremsstrahlung radiation, collision-induced radiation and corona discharges, nonclassical light, proton tunneling, electrodynamic jets and fractoluminescent jets (now largely discredited due to contrary experimental evidence).\n\nIn 2002, M. Brenner, S. Hilgenfeldt, and D. Lohse published a 60-page review that contains a detailed explanation of the mechanism. An important factor is that the bubble contains mainly inert noble gas such as argon or xenon (air contains about 1% argon, and the amount dissolved in water is too great; for sonoluminescence to occur, the concentration must be reduced to 20–40% of its equilibrium value) and varying amounts of water vapor. Chemical reactions cause nitrogen and oxygen to be removed from the bubble after about one hundred expansion-collapse cycles. The bubble will then begin to emit light. The light emission of highly compressed noble gas is exploited technologically in the argon flash devices.\n\nDuring bubble collapse, the inertia of the surrounding water causes high pressure and high temperature, reaching around 10,000 kelvins in the interior of the bubble, causing the ionization of a small fraction of the noble gas present. The amount ionized is small enough for the bubble to remain transparent, allowing volume emission; surface emission would produce more intense light of longer duration, dependent on wavelength, contradicting experimental results. Electrons from ionized atoms interact mainly with neutral atoms, causing thermal bremsstrahlung radiation. As the wave hits a low energy trough, the pressure drops, allowing electrons to recombine with atoms and light emission to cease due to this lack of free electrons. This makes for a 160-picosecond light pulse for argon (even a small drop in temperature causes a large drop in ionization, due to the large ionization energy relative to photon energy). This description is simplified from the literature above, which details various steps of differing duration from 15 microseconds (expansion) to 100 picoseconds (emission).\n\nComputations based on the theory presented in the review produce radiation parameters (intensity and duration time versus wavelength) that match experimental results with errors no larger than expected due to some simplifications (e.g., assuming a uniform temperature in the entire bubble), so it seems the phenomenon of sonoluminescence is at least roughly explained, although some details of the process remain obscure.\n\nAny discussion of sonoluminescence must include a detailed analysis of metastability. Sonoluminescence in this respect is what is physically termed a bounded phenomenon meaning that the sonoluminescence exists in a bounded region of parameter space for the bubble; a coupled magnetic field being one such parameter. The magnetic aspects of sonoluminescence are very well documented.\n\nAn unusually exotic hypothesis of sonoluminescence, which has received much popular attention, is the Casimir energy hypothesis suggested by noted physicist Julian Schwinger and more thoroughly considered in a paper by Claudia Eberlein of the University of Sussex. Eberlein's paper suggests that the light in sonoluminescence is generated by the vacuum within the bubble in a process similar to Hawking radiation, the radiation generated at the event horizon of black holes. According to this vacuum energy explanation, since quantum theory holds that vacuum contains virtual particles, the rapidly moving interface between water and gas converts virtual photons into real photons. This is related to the Unruh effect or the Casimir effect. The argument has been made that sonoluminescence releases too large an amount of energy and releases the energy on too short a time scale to be consistent with the vacuum energy explanation, although other credible sources argue the vacuum energy explanation might yet prove to be correct.\n\nSome have argued that the Rayleigh–Plesset equation described above is unreliable for predicting bubble temperatures and that actual temperatures in sonoluminescing systems can be far higher than 20,000 kelvins. Some research claims to have measured temperatures as high as 100,000 kelvins, and speculates temperatures could reach into the millions of kelvins. Temperatures this high could cause thermonuclear fusion. This possibility is sometimes referred to as bubble fusion and is likened to the implosion design used in the fusion component of thermonuclear weapons.\n\nOn January 27, 2006, researchers at Rensselaer Polytechnic Institute claimed to have produced fusion in sonoluminescence experiments.\n\nExperiments in 2002 and 2005 by R. P. Taleyarkhan using deuterated acetone showed measurements of tritium and neutron output consistent with fusion. However, the papers were considered low quality and there were doubts cast by a report about the author's scientific misconduct. This made the report lose credibility among the scientific community.\n\nPistol shrimp (also called \"snapping shrimp\") produce a type of sonoluminescence from a collapsing bubble caused by quickly snapping its claw. The animal snaps a specialized claw shut to create a cavitation bubble that generates acoustic pressures of up to 80 kPa at a distance of 4 cm from the claw. As it extends out from the claw, the bubble reaches speeds of 60 miles per hour (97 km/h) and releases a sound reaching 218 decibels. The pressure is strong enough to kill small fish. The light produced is of lower intensity than the light produced by typical sonoluminescence and is not visible to the naked eye. The light and heat produced may have no direct significance, as it is the shockwave produced by the rapidly collapsing bubble which these shrimp use to stun or kill prey. However, it is the first known instance of an animal producing light by this effect and was whimsically dubbed \"shrimpoluminescence\" upon its discovery in 2001. It has subsequently been discovered that another group of crustaceans, the mantis shrimp, contains species whose club-like forelimbs can strike so quickly and with such force as to induce sonoluminescent cavitation bubbles upon impact.\n\n\n\n\nNewer research papers largely ruling out the vacuum energy explanation\n"}
{"id": "42160919", "url": "https://en.wikipedia.org/wiki?curid=42160919", "title": "Stop CSG Party", "text": "Stop CSG Party\n\nThe Stop CSG Party (Coal Seam Gas) was a registered minor political party in Australia that ran candidates in the 2013 federal election.\n\nThe party has been involved in Glenn Druery's Minor Party Alliance.\n\nThe party was deregistered by the Australian Electoral Commission in March 2015, after failing to respond to the AEC's notice to confirm eligibility for registration.\n\n\n"}
{"id": "4400374", "url": "https://en.wikipedia.org/wiki?curid=4400374", "title": "Surface runoff", "text": "Surface runoff\n\nSurface runoff (also known as overland flow) is the flow of water that occurs when excess stormwater, meltwater, or other sources flows over the Earth's surface. This might occur because soil is saturated to full capacity, because rain arrives more quickly than soil can absorb it, or because impervious areas (roofs and pavement) send their runoff to surrounding soil that cannot absorb all of it. Surface runoff is a major component of the water cycle. It is the primary agent in soil erosion by water.\n\nRunoff that occurs on the ground surface before reaching a channel is also called a nonpoint source. If a nonpoint source contains man-made contaminants, or natural forms of pollution (such as rotting leaves) the runoff is called nonpoint source pollution. A land area which produces runoff that drains to a common point is called a drainage basin. When runoff flows along the ground, it can pick up soil contaminants including petroleum, pesticides, or fertilizers that become discharge or nonpoint source pollution.\n\nIn addition to causing water erosion and pollution, surface runoff in urban areas is a primary cause of urban flooding which can result in property damage, damp and mold in basements, and street flooding.\n\n Surface runoff can be generated either by rainfall, snowfall or by the melting of snow, or glaciers.\n\nSnow and glacier melt occur only in areas cold enough for these to form permanently. Typically snowmelt will peak in the spring and glacier melt in the summer, leading to pronounced flow maxima in rivers affected by them. The determining factor of the rate of melting of snow or glaciers is both air temperature and the duration of sunlight. In high mountain regions, streams frequently rise on sunny days and fall on cloudy ones for this reason.\n\nIn areas where there is no snow, runoff will come from rainfall. However, not all rainfall will produce runoff because storage from soils can absorb light showers. On the extremely ancient soils of Australia and Southern Africa, proteoid roots with their extremely dense networks of root hairs can absorb so much rainwater as to prevent runoff even when substantial amounts of rain fall. In these regions, even on less infertile cracking clay soils, high amounts of rainfall and potential evaporation are needed to generate any surface runoff, leading to specialised adaptations to extremely variable (usually ephemeral) streams.\n\nThis occurs when the rate of rainfall on a surface exceeds the rate at which water can infiltrate the ground, and any depression storage has already been filled. This is called flooding excess overland flow, Hortonian overland flow (after Robert E. Horton), or unsaturated overland flow. This more commonly occurs in arid and semi-arid regions, where rainfall intensities are high and the soil infiltration capacity is reduced because of surface sealing, or in paved areas. This occurs largely in city areas where pavements prevent water from flooding.\n\nWhen the soil is saturated and the depression storage filled, and rain continues to fall, the rainfall will immediately produce surface runoff. The level of antecedent soil moisture is one factor affecting the time until soil becomes saturated. This runoff is called saturation excess overland flow, saturated overland flow or Dunne runoff.\n\nSoil retains a degree of moisture after a rainfall. This residual water moisture affects the soil's infiltration capacity. During the next rainfall event, the infiltration capacity will cause the soil to be saturated at a different rate. The higher the level of antecedent soil moisture, the more quickly the soil becomes saturated. Once the soil is saturated, runoff occurs.\n\nAfter water infiltrates the soil on an up-slope portion of a hill, the water may flow laterally through the soil, and exfiltrate (flow out of the soil) closer to a channel. This is called subsurface return flow or throughflow.\n\nAs it flows, the amount of runoff may be reduced in a number of possible ways: a small portion of it may evapotranspire; water may become temporarily stored in microtopographic depressions; and a portion of it may infiltrate as it flows overland. Any remaining surface water eventually flows into a receiving water body such as a river, lake, estuary or ocean.\n\nUrbanization increases surface runoff by creating more impervious surfaces such as pavement and buildings that do not allow percolation of the water down through the soil to the aquifer. It is instead forced directly into streams or storm water runoff drains, where erosion and siltation can be major problems, even when flooding is not. Increased runoff reduces groundwater recharge, thus lowering the water table and making droughts worse, especially for agricultural farmers and others who depend on the water wells.\n\nWhen anthropogenic contaminants are dissolved or suspended in runoff, the human impact is expanded to create water pollution. This pollutant load can reach various receiving waters such as streams, rivers, lakes, estuaries and oceans with resultant water chemistry changes to these water systems and their related ecosystems.\n\nA 2008 report by the United States National Research Council identified urban stormwater as a leading source of water quality problems in the U.S.\n\nAs humans continue to alter the climate through the addition of greenhouse gases to the atmosphere, precipitation patterns are expected to change as the atmospheric capacity for water vapor increases. This will have direct consequences on runoff amounts.\n\nSurface runoff can cause erosion of the Earth's surface; eroded material may be deposited a considerable distance away. There are four main types of soil erosion by water: splash erosion, sheet erosion, rill erosion and gully erosion. Splash erosion is the result of mechanical collision of raindrops with the soil surface: soil particles which are dislodged by the impact then move with the surface runoff. Sheet erosion is the overland transport of sediment by runoff without a well defined channel. Soil surface roughness causes may cause runoff to become concentrated into narrower flow paths: as these incise, the small but well-defined channels which are formed are known as rills. These channels can be as small as one centimeter wide or as large as several meters. If runoff continue to incise and enlarge rills, they may eventually grow to become gullies. Gully erosion can transport large amounts of eroded material in a small time period. \n\nReduced crop productivity usually results from erosion, and these effects are studied in the field of soil conservation. The soil particles carried in runoff vary in size from about .001 millimeter to 1.0 millimeter in diameter. Larger particles settle over short transport distances, whereas small particles can be carried over long distances suspended in the water column. Erosion of silty soils that contain smaller particles generates turbidity and diminishes light transmission, which disrupts aquatic ecosystems.\n\nEntire sections of countries have been rendered unproductive by erosion. On the high central plateau of Madagascar, approximately ten percent of that country's land area, virtually the entire landscape is devoid of vegetation, with erosive gully furrows typically in excess of 50 meters deep and one kilometer wide. Shifting cultivation is a farming system which sometimes incorporates the slash and burn method in some regions of the world. Erosion causes loss of the fertile top soil and reduces its fertility and quality of the agricultural produce.\n\nModern industrial farming is another major cause of erosion. In some areas in the American corn belt, more than 50 percent of the original topsoil has been carried away within the last 100 years.\n\nThe principal environmental issues associated with runoff are the impacts to surface water, groundwater and soil through transport of water pollutants to these systems. Ultimately these consequences translate into human health risk, ecosystem disturbance and aesthetic impact to water resources. Some of the contaminants that create the greatest impact to surface waters arising from runoff are petroleum substances, herbicides and fertilizers. Quantitative uptake by surface runoff of pesticides and other contaminants has been studied since the 1960s, and early on contact of pesticides with water was known to enhance phytotoxicity. In the case of surface waters, the impacts translate to water pollution, since the streams and rivers have received runoff carrying various chemicals or sediments. When surface waters are used as potable water supplies, they can be compromised regarding health risks and drinking water aesthetics (that is, odor, color and turbidity effects). Contaminated surface waters risk altering the metabolic processes of the aquatic species that they host; these alterations can lead to death, such as fish kills, or alter the balance of populations present. Other specific impacts are on animal mating, spawning, egg and larvae viability, juvenile survival and plant productivity. Some researches show surface runoff of pesticides, such as DDT, can alter the gender of fish species genetically, which transforms male into female fish.\n\nSurface runoff occurring within forests can supply lakes with high loads of mineral nitrogen and phosphorus leading to eutrophication. Runoff waters within coniferous forests are also enriched with humic acids and can lead to humification of water bodies Additionally, high standing and young islands in the tropics and subtropics can undergo high soil erosion rates and also contribute large material fluxes to the coastal ocean. Such land derived runoff of sediment nutrients, carbon, and contaminants can have large impacts on global biogeochemical cycles and marine and coastal ecosystems. \n\nIn the case of groundwater, the main issue is contamination of drinking water, if the aquifer is abstracted for human use. Regarding soil contamination, runoff waters can have two important pathways of concern. Firstly, runoff water can extract soil contaminants and carry them in the form of water pollution to even more sensitive aquatic habitats. Secondly, runoff can deposit contaminants on pristine soils, creating health or ecological consequences.\n\nThe other context of agricultural issues involves the transport of agricultural chemicals (nitrates, phosphates, pesticides, herbicides etc.) via surface runoff. This result occurs when chemical use is excessive or poorly timed with respect to high precipitation. The resulting contaminated runoff represents not only a waste of agricultural chemicals, but also an environmental threat to downstream ecosystems.\n\nFlooding occurs when a watercourse is unable to convey the quantity of runoff flowing downstream. The frequency with which this occurs is described by a return period. Flooding is a natural process, which maintains ecosystem composition and processes, but it can also be altered by land use changes such as river engineering. Floods can be both beneficial to societies or cause damage. Agriculture along the Nile floodplain took advantage of the seasonal flooding that deposited nutrients beneficial for crops. However, as the number and susceptibility of settlements increase, flooding increasingly becomes a natural hazard. In urban areas, surface runoff is the primary cause of urban flooding, known for its repetitive and costly impact on communities. Adverse impacts span loss of life, property damage, contamination of water supplies, loss of crops, and social dislocation and temporary homelessness. Floods are among the most devastating of natural disasters.\n\nMitigation of adverse impacts of runoff can take several forms:\n\n\nLand use controls. Many world regulatory agencies have encouraged research on methods of minimizing total surface runoff by avoiding unnecessary hardscape. Many municipalities have produced guidelines and codes (zoning and related ordinances) for land developers that encourage minimum width sidewalks, use of pavers set in earth for driveways and walkways and other design techniques to allow maximum water infiltration in urban settings. An example land use control program can be seen in the city of Santa Monica, California. \n\nErosion controls have appeared since medieval times when farmers realized the importance of contour farming to protect soil resources. Beginning in the 1950s these agricultural methods became increasingly more sophisticated. In the 1960s some state and local governments began to focus their efforts on mitigation of construction runoff by requiring builders to implement erosion and sediment controls (ESCs). This included such techniques as: use of straw bales and barriers to slow runoff on slopes, installation of silt fences, programming construction for months that have less rainfall and minimizing extent and duration of exposed graded areas. Montgomery County, Maryland implemented the first local government sediment control program in 1965, and this was followed by a statewide program in Maryland in 1970.\n\nFlood control programs as early as the first half of the twentieth century became quantitative in predicting peak flows of riverine systems. Progressively strategies have been developed to minimize peak flows and also to reduce channel velocities. Some of the techniques commonly applied are: provision of holding ponds (also called detention basins) to buffer riverine peak flows, use of energy dissipators in channels to reduce stream velocity and land use controls to minimize runoff.\n\nChemical use and handling. Following enactment of the U.S. Resource Conservation and Recovery Act (RCRA) in 1976, and later the Water Quality Act of 1987, states and cities have become more vigilant in controlling the containment and storage of toxic chemicals, thus preventing releases and leakage. Methods commonly applied are: requirements for double containment of underground storage tanks, registration of hazardous materials usage, reduction in numbers of allowed pesticides and more stringent regulation of fertilizers and herbicides in landscape maintenance. In many industrial cases, pretreatment of wastes is required, to minimize escape of pollutants into sanitary or stormwater sewers.\n\nThe U.S. Clean Water Act (CWA) requires that local governments in urbanized areas (as defined by the Census Bureau) obtain stormwater discharge permits for their drainage systems. Essentially this means that the locality must operate a stormwater management program for all surface runoff that enters the municipal separate storm sewer system (\"MS4\"). EPA and state regulations and related publications outline six basic components that each local program must contain:\nOther property owners which operate storm drain systems similar to municipalities, such as state highway systems, universities, military bases and prisons, are also subject to the MS4 permit requirements.\n\nRunoff is analyzed by using mathematical models in combination with various water quality sampling methods. Measurements can be made using continuous automated water quality analysis instruments targeted on pollutants such as specific organic or inorganic chemicals, pH, turbidity etc. or targeted on secondary indicators such as dissolved oxygen. Measurements can also be made in batch form by extracting a single water sample and conducting any number of chemical or physical tests on that sample.\n\nIn the 1950s or earlier hydrology transport models appeared to calculate quantities of runoff, primarily for flood forecasting. Beginning in the early 1970s computer models were developed to analyze the transport of runoff carrying water pollutants, which considered dissolution rates of various chemicals, infiltration into soils and ultimate pollutant load delivered to receiving waters.\nOne of the earliest models addressing chemical dissolution in runoff and resulting transport was developed in the early 1970s under contract to the United States Environmental Protection Agency (EPA). This computer model formed the basis of much of the mitigation study that led to strategies for land use and chemical handling controls.\n\nOther computer models have been developed (such as the DSSAM Model) that allow surface runoff to be tracked through a river course as reactive water pollutants. In this case the surface runoff may be considered to be a line source of water pollution to the receiving waters.\n\n\n"}
{"id": "14647482", "url": "https://en.wikipedia.org/wiki?curid=14647482", "title": "Tangie", "text": "Tangie\n\nA tangie (or \"tongie\") is a shape-shifting sea spirit in the folklore of the Orkney and Shetland Islands in the British Isles. A sea horse or merman, it takes on the appearance of either a horse or an aged man. Usually described as being covered with seaweed, its name derives from \"tang\" or seaweed of the genus \"Fucus\".\n\nIt is known for terrorizing lonely travellers, especially young women on roads at night near the lochs, whom it will abduct and devour under the water.\n\nSimilar yet distinctive from the nogel, a tangie is able to cause derangement in humans and animals.\n\nThe tangie plays a major role in the Shetland legend of Black Eric, a sheep rustler. The tangie he rode gave him supernatural assistance when he raided and harassed surrounding crofts. In his final battle with crofter Sandy Breamer, Black Eric fell to his death in the sea. The tangie then continued to terrorize the area, particularly the young women he was hoping to abduct.\n\n"}
{"id": "9724850", "url": "https://en.wikipedia.org/wiki?curid=9724850", "title": "Tiny Ionospheric Photometer", "text": "Tiny Ionospheric Photometer\n\nThe tiny ionospheric photometer (TIP) is a small space-based photometer that observes the Earth's ionosphere at 135.6 nm. The TIP instruments were designed and built by the US Naval Research Laboratory (NRL) and are a part of the COSMIC program.\n\nAlthough each TIP instrument is fairly simple in design and operation, the value of this instrument is that six of them were launched at once, and they observe the earth simultaneously from three orbital planes spaced equally apart around the earth. The data of this instrument when combined with the data from the other COSMIC payloads allows a 3D tomographic analysis of the Earth's ionosphere to be performed.\n\n"}
{"id": "32147", "url": "https://en.wikipedia.org/wiki?curid=32147", "title": "Union of International Associations", "text": "Union of International Associations\n\nThe Union of International Associations (UIA) is a non-profit non-governmental research institute and documentation center based in Brussels, Belgium, and operating under United Nations mandate. It was founded in 1907 under the name Central Office of International Associations by Henri La Fontaine, the 1913 Nobel Peace Prize laureate, and Paul Otlet, a founding father of what is now called information science.\n\nThe UIA is an independent research institute and a repository for current and historical information on the work of global civil society. It serves two main purposes: to document and promote public awareness of the work of international organizations (both INGOs and IGOs), international meetings, and world problems. The UIA also supports and facilitates the work of international associations through training and networking opportunities.\n\nIt has consultative status with ECOSOC and UNESCO.\n\n\nThe two founders started work setting up the Central Office of International Organisations and then conducting a survey of international organisations with headquarters in Belgium. Then with the help of the sociologist Cyril van Overbergh they extend this research to organisations based elsewhere. After collaborating with Alfred Fried on the production of \"Annuaire de la Vie Internationale\" they produced their own edition without him.\n\n\n"}
{"id": "1682880", "url": "https://en.wikipedia.org/wiki?curid=1682880", "title": "Şüräle", "text": "Şüräle\n\nShurale (Tatar and Bashkir: Шүрәле, ]; ) is a forest spirit in Bashkir and Tatar mythology. According to legends, Şüräle lives in forests. He has long fingers, a horn on its forehead, and a woolly body. He lures victims to a thicket and can tickle them to death.\n\nŞüräle closely resembles other similar characters from the folklore such as Arçuri of the Chuvashes, Pitsen (Picen) of the Siberian Tatars and Yarımtıq of the Ural Tatars.\n\nHe can shapeshift into many different forms. As a human, he looks like a peasant with glowing eyes, and his shoes are on backwards. A person who befriends Şüräle can learn the secrets of magic. Farmers and shepherds would make pacts with the leshy to protect their crops and sheep. Şüräle has many tricks, including leading peasants astray, making them sick, or tickling them to death. They are also known to hide the axes of woodcutters. A person gets lost in the woods when a Şüräle crosses their path. To find the way out, you have to turn your clothes inside out and wear shoes on opposite feet.\n\nInspired by the Tatar folklore, Ghabdulla Tuqay wrote a poem \"Şüräle\". Şüräle was Tuqay's pseudonym. The first Tatar ballet by Farit Yarullin had its name after \"Şüräle\".\n\n\n\n\nArçura/Şüräle: Mythical Spirits of the Volga-Ural Forests, Rustem Sulteev. http://akademiai.com/doi/abs/10.1556/062.2018.71.1.4?journalCode=062"}
