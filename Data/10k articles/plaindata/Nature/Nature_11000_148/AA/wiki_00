{"id": "31293393", "url": "https://en.wikipedia.org/wiki?curid=31293393", "title": "AEM rubber", "text": "AEM rubber\n\nAEM rubber, also abbreviated AECM (ISO 1629), is an ethylene acrylic rubber with the formula: \n"}
{"id": "51530149", "url": "https://en.wikipedia.org/wiki?curid=51530149", "title": "Anorogenic magmatism", "text": "Anorogenic magmatism\n\nIn geology anorogenic magmatism is the formation, intrusion or eruption of magmas in other settings than convergent plate margins such as continental collision and subduction where orogeny is common.\n"}
{"id": "27832295", "url": "https://en.wikipedia.org/wiki?curid=27832295", "title": "Big Bambú", "text": "Big Bambú\n\nBig Bambú is a work of installation art by identical twin artists Doug and Mike Starn. Variations on the Big Bambu theme have been constructed at several locations around the world. Combining architecture and sculpture, it examines the tension between chaos and order in nature.\n\nBig Bambú had its first installation in the artists' studio in Beacon, New York. From April to October 2010 it was the featured exhibition at the Metropolitan Museum of Art Roof Garden. In 2011 another incarnation of Big Bambú was installed as a collateral exhibition of the 54th Biennale in Venice, Italy. In 2013, it was installed on the Japanese island of Teshima during the Setouchi Triennale art festival. In 2014, it was presented in the Israel Museum, Jerusalem.\nBig Bambú is made of thousands of bamboo poles, lashed together to form a complex structure through which visitors walk on elevated bamboo paths even as a crew continues to build a new part of the structure.\n\nThe name is taken from the Cheech & Chong album \"Big Bambu\"\n\nIn the original installation in the artists studio in Beacon, New York, Big Bambú is in continual motion as a crew disassembles one end and continues to build the other end. The piece was reconfigured into a gothic letter \"T\" to be photographed for the cover of the fifth anniversary edition of the \"New York Times\" style magazine.\n\nThe installation on the roof of the Metropolitan Museum of Art was conceived as a giant wave cresting over the rooftop. Art critic Karen Wilkin wrote that the experience of walking on the roof terrace under the sculpture felt like \"wandering through a bamboo grove.\" She described the piece as not a \"significant sculpture... it's more of a phenomenon. But it's a delightful addition to the Met for the next six months—a temporary, ecologically correct folly designed to entertain.\"\n\nBig Bambú is built of several types of bamboo, primarily a Japanese type called Madake, and also thin Meyeri bamboo and thick moso bamboo. All of the bamboo was grown in Georgia and South Carolina. The construction was undertaken by the artists working together with a team of twenty qualified rock climbers. Construction continued throughout the exhibition's six-month run, with the sculpture ultimately reaching 100 feet long, 50 feet wide, 50 feet high and using 3,200 bamboo poles. Museum visitors were required to wear rubber-soled, close-toed shoes to climb through the structure. Visitors could walk underneath the sculpture without obtaining a ticket and with no restriction on footwear.\n\n\"Big Bambu: 5000 arms to hold you\", constructed in the sculpture garden of the Israel Museum in Jerusalem in 2014, is 16 meters (52.5 feet) high and covers an area of over 700 square meters (7,500 square feet). Visitors are invited to climb on the framework of 10,000 bamboo poles bound by rope, which forms a labyrinth of winding paths and offers panoramic views of the Jerusalem cityscape.\n\nIn 2018 a \"Big Bambú\" installation was made at Ordrupgård Museum in Denmark.\n\n"}
{"id": "2598295", "url": "https://en.wikipedia.org/wiki?curid=2598295", "title": "Burdigalian", "text": "Burdigalian\n\nThe Burdigalian is, in the geologic timescale, an age or stage in the early Miocene. It spans the time between 20.43 ± 0.05 Ma and 15.97 ± 0.05 Ma (million years ago). Preceded by the Aquitanian, the Burdigalian was the first and longest warming period of the Miocene and is succeeded by the Langhian.\n\nThe name Burdigalian comes from \"Burdigala\", the Latin name for the city of Bordeaux, France. The Burdigalian stage was introduced in scientific literature by Charles Depéret in 1892.\n\nThe base of the Burdigalian is at the first appearance of foram species \"Globigerinoides altiaperturus\" and the top of magnetic chronozone C6An. , an official GSSP for the Burdigalian had not yet been assigned.\n\nThe top of the Burdigalian (the base of the Langhian) is defined by the first appearance of foram species \"Praeorbulina glomerosa\" and is also coeval with the top of magnetic chronozone C5Cn.1n. \n\nFamous Burdigalian palaeontologic localities include the Turritellenplatte of Ermingen in Germany and the Dominican amber deposits of Hispaniola.\n\nPossible human evolutionary ancestors such as Victoriapithecus evolved during this time interval.\n\n\n"}
{"id": "559041", "url": "https://en.wikipedia.org/wiki?curid=559041", "title": "Buxenus", "text": "Buxenus\n\nIn Gallo-Roman religion, Buxenus was an epithet of the Gaulish Mars, known from a single inscription found in Velleron in the Vaucluse.\n"}
{"id": "6673", "url": "https://en.wikipedia.org/wiki?curid=6673", "title": "Central Powers", "text": "Central Powers\n\nThe Central Powers (; ; / ; ), consisting of Germany, , the Ottoman Empire and Bulgaria—hence also known as the Quadruple Alliance ()—was one of the two main coalitions that fought World War I (1914–18).\n\nIt faced and was defeated by the Allied Powers that had formed around the Triple Entente. The Powers' origin was the alliance of Germany and Austria-Hungary in 1879. Despite having joined the alliance before, Italy refused to take part in World War I on the side of the Central Powers; the Ottoman Empire and Bulgaria did not join until after World War I had begun, even though the Ottoman Empire had retained close relations with both Germany and Austria-Hungary since the beginning of the 20th century.\n\nThe Central Powers consisted of the German Empire and the Austro-Hungarian Empire at the beginning of the war. The Ottoman Empire joined the Central Powers later in 1914. In 1915, the Kingdom of Bulgaria joined the alliance. The name \"Central Powers\" is derived from the location of these countries; all four (including the other groups that supported them except for Finland and Lithuania) were located between the Russian Empire in the east and France and the United Kingdom in the west. Finland, Azerbaijan, and Lithuania joined them in 1918 before the war ended and after the Russian Empire collapsed.\n\nThe Central Powers were composed of the following nations:\n\nIn early July 1914, in the aftermath of the assassination of Austro-Hungarian Archduke Franz Ferdinand and the immediate likelihood of war between Austria-Hungary and Serbia, Kaiser Wilhelm II and the German government informed the Austro-Hungarian government that Germany would uphold its alliance with Austria-Hungary and defend it from possible Russian intervention if a war between Austria-Hungary and Serbia took place. When Russia enacted a general mobilization, Germany viewed the act as provocative. The Russian government promised Germany that its general mobilization did not mean preparation for war with Germany but was a reaction to the events between Austria-Hungary and Serbia. The German government regarded the Russian promise of no war with Germany to be nonsense in light of its general mobilization, and Germany, in turn, mobilized for war. On 1 August, Germany sent an ultimatum to Russia stating that since both Germany and Russia were in a state of military mobilization, an effective state of war existed between the two countries. Later that day, France, an ally of Russia, declared a state of general mobilization.\n\nIn August 1914, Germany waged war on Russia, the German government justified military action against Russia as necessary because of Russian aggression as demonstrated by the mobilization of the Russian army that had resulted in Germany mobilizing in response.\n\nAfter Germany declared war on Russia, France with its alliance with Russia prepared a general mobilization in expectation of war. On 3 August 1914, Germany responded to this action by declaring war on France. Germany, facing a two-front war, enacted what was known as the Schlieffen Plan, that involved German armed forces needing to move through Belgium and swing south into France and towards the French capital of Paris. This plan was hoped to quickly gain victory against the French and allow German forces to concentrate on the Eastern Front. Belgium was a neutral country and would not accept German forces crossing its territory. Germany disregarded Belgian neutrality and invaded the country to launch an offensive towards Paris. This caused Great Britain to declare war against the German Empire, as the action violated the Treaty of London that both nations signed in 1839 guaranteeing Belgian neutrality and defense of the kingdom if a nation reneged.\n\nSubsequently, several states declared war on Germany in late August 1914, with Italy declaring war on Austria-Hungary in 1915 and Germany on 27 August 1916, the United States declaring war on Germany on 6 April 1917 and Greece declaring war on Germany in July 1917.\n\nUpon its founding in 1871, the German Empire controlled Alsace-Lorraine as an \"imperial territory\" incorporated from France after the Franco-Prussian War. It was held as part of Germany's sovereign territory.\n\nGermany held multiple African colonies at the time of World War I. All of Germany's African colonies were invaded and occupied by Allied forces during the war.\n\nCameroon, German East Africa, and German Southwest Africa were German colonies in Africa. Togoland was a German protectorate in Africa.\n\n\nThe Kiautschou Bay concession was a German dependency in East Asia leased from China in 1898. It was occupied by Japanese forces following the Siege of Tsingtao.\n\n\nGerman New Guinea was a German protectorate in the Pacific. It was occupied by Australian forces in 1914.\n\nGerman Samoa was a German protectorate following the Tripartite Convention. It was occupied by the New Zealand Expeditionary Force in 1914.\n\nAustria-Hungary regarded the assassination of Archduke Franz Ferdinand as being orchestrated with the assistance of Serbia. The country viewed the assassination as setting a dangerous precedent of encouraging the country's South Slav population to rebel and threaten to tear apart the multinational country. Austria-Hungary formally sent an ultimatum to Serbia demanding a full-scale investigation of Serbian government complicity in the assassination, and complete compliance by Serbia in agreeing to the terms demanded by Austria-Hungary. Serbia submitted to accept most of the demands, however Austria-Hungary viewed this as insufficient and used this lack of full compliance to justify military intervention. These demands have been viewed as a diplomatic cover for what was going to be an inevitable Austro-Hungarian declaration of war on Serbia.\n\nAustria-Hungary had been warned by Russia that the Russian government would not tolerate Austria-Hungary crushing Serbia. However, with Germany supporting Austria-Hungary's actions, the Austro-Hungarian government hoped that Russia would not intervene and that the conflict with Serbia would be a regional conflict.\n\nAustria-Hungary's invasion of Serbia resulted in Russia declaring war on the country and Germany in turn declared war on Russia, setting off the beginning of the clash of alliances that resulted in the World War.\n\nAustria-Hungary was internally divided into two states with their own governments, joined in communion through the Habsburg throne. Austrian Cisleithania contained various duchies and principalities but also the Kingdom of Bohemia, the Kingdom of Dalmatia, the Kingdom of Galicia and Lodomeria. Hungarian Transleithania comprised the Kingdom of Hungary and the Kingdom of Croatia-Slavonia. In Bosnia and Herzegovina sovereign authority was shared by both Austria and Hungary.\n\nThe Ottoman Empire joined the war on the side of the Central Powers in November 1914. The Ottoman Empire had gained strong economic connections with Germany through the Berlin-to-Baghdad railway project that was still incomplete at the time. The Ottoman Empire made a formal alliance with Germany signed on 2 August 1914. The alliance treaty expected that the Ottoman Empire would become involved in the conflict in a short amount of time. However, for the first several months of the war the Ottoman Empire maintained neutrality though it allowed a German naval squadron to enter and stay near the strait of Bosphorus. Ottoman officials informed the German government that the country needed time to prepare for conflict. Germany provided financial aid and weapons shipments to the Ottoman Empire.\n\nAfter pressure escalated from the German government demanding that the Ottoman Empire fulfill its treaty obligations, or else Germany would expel the country from the alliance and terminate economic and military assistance, the Ottoman government entered the war with the recently acquired cruisers from Germany, the \"Yavuz Sultan Selim\" (formerly \"SMS Goeben\") and the \"Midilli\" (formerly \"SMS Breslau\") launching a naval raid on the Russian port of Odessa, thus engaging in a military action in accordance with its alliance obligations with Germany. Russia and the Triple Entente declared war on the Ottoman Empire.\n\nBulgaria was still resentful after its defeat in July 1913 at the hands of Serbia, Greece and Romania. It signed a treaty of defensive alliance with the Ottoman Empire on 19 August 1914. It was the last country to join the Central Powers, which Bulgaria did in October 1915 by declaring war on Serbia. It invaded Serbia in conjunction with German and Austro-Hungarian forces. Bulgaria held claims on the region of Vardar Macedonia then held by Serbia following the Balkan Wars of 1912–1913 and (from the Bulgarian point of view), the costly Treaty of Bucharest (1913). As a condition of entering WW1 on the side of the Central Powers, Bulgaria was granted the right to reclaim that territory.\n\nThe Dervish movement was a rebel Somali movement led by Mohammed Abdullah Hassan seeking independence of Somali territories. Dervish forces fought against Italian and British forces in Italian Somaliland and British Somaliland during World War I in the Somaliland Campaign. The Dervish movement received support from Germany and the Ottoman Empire. It also briefly received support by the Ethiopian Empire from 1915–1916.\n\nIn opposition to the Union of South Africa, which had joined the war, Boer rebels founded the South African Republic in 1914 and engaged in the Maritz Rebellion. Germany assisted the rebels, and the rebels operated in and out of the German colony of German South-West Africa. The rebels were defeated by British imperial forces.\n\nThe Sultanate of Darfur forces fought against British forces in Anglo-Egyptian Sudan during World War I in the Anglo-Egyptian Darfur Expedition.\n\nDuring 1917 and 1918, the Finns under Carl Gustaf Emil Mannerheim and Lithuanian nationalists fought Russia for a common cause. With the Bolshevik attack of late 1917, the General Secretariat of Ukraine sought military protection first from the Central Powers and later from the armed forces of the Entente.\n\nThe Ottoman Empire also had its own allies in Azerbaijan and the Northern Caucasus. The three nations fought alongside each other under the Army of Islam in the Battle of Baku.\n\nThe Belarusian People's Republic was a client state of Germany created in 1918.\n\nThe Duchy of Courland and Semigallia was a client state of Germany created in 1918.\n\nThe Crimean Regional Government was a client state of Germany created in 1918.\n\nThe Don Republic was closely associated with the German Empire and fought against the Bolsheviks.\n\nThe Kingdom of Finland was a client state of Germany created in 1918. Prior to the declaration of the kingdom, Finland existed as an autonomous Grand Duchy of Russia since 1809.\n\nThe Kuban People's Republic was a client state of Germany created in 1918.\n\nThe Kingdom of Lithuania was a client state of Germany created in 1918.\n\nThe Mountainous Republic of the Northern Caucasus was associated with the Central Powers.\n\nThe Democratic Republic of Georgia declared independence in 1918 which then to led border conflicts between newly formed republic and Ottoman Empire. Soon after Ottoman Empire invaded the republic and quickly reached Borjomi. This forced Georgia to ask for help from Germany which they were granted. Germany forced the Ottomans to withdraw from Georgian territories and recognize Georgian soverignty. Germany, Georgia and the Ottomans signed a peace treaty, the Treaty of Batum which ended the conflict with the last two. in return Georgia become a German \"ally\". This time period of Georgian-German friendship was known as German Caucasus expedition.\n\nThe Kingdom of Poland was a client state of Germany created in 1916. This government was recognized by the emperors of Germany and Austria-Hungary in November 1916, and it adopted a constitution in 1917. The decision to create a Polish State was taken by Germany in order to attempt to legitimize its military occupation amongst the Polish inhabitants, following upon German propaganda sent to Polish inhabitants in 1915 that German soldiers were arriving as liberators to free Poland from subjugation by Russia.\nThe state was utilized by the German government alongside punitive threats to induce Polish landowners living in the German-occupied Baltic territories to move to the state and sell their Baltic property to Germans in exchange for moving to Poland, and efforts were made to induce similar emigration of Poles from Prussia to the state.\n\nThe Ukrainian State was a client state of Germany led by Hetman Pavlo Skoropadskyi, who overthrew the government of the Ukrainian People's Republic.\n\nThe United Baltic Duchy was a proposed client state of Germany created in 1918.\n\nIn 1918, the Azerbaijan Democratic Republic, facing Bolshevik revolution and opposition from the Muslim Musavat Party, was then occupied by the Ottoman Empire, which expelled the Bolsheviks while supporting the Musavat Party. The Ottoman Empire maintained a presence in Azerbaijan until the end of the war in November 1918.\n\nJabal Shammar was an Arab state in the Middle East that was closely associated with the Ottoman Empire.\n\nStates listed in this section were not officially members of the Central Powers, but at some point during the war engaged in cooperation with one or more Central Powers members on level that makes their neutrality disputable.\n\nThe Ethiopian Empire was officially neutral throughout World War I but widely suspected of sympathy for the Central Powers between 1915 and 1916. At the time, Ethiopia was one of the few independent states in Africa and a major power in the Horn of Africa. Its ruler, Lij Iyasu, was widely suspected of harbouring pro-Islamic sentiments and being sympathetic to the Ottoman Empire. The German Empire also attempted to reach out to Iyasu, dispatching several unsuccessful expeditions to the region to attempt to encourage it to collaborate in an Arab Revolt-style uprising in East Africa. One of the unsuccessful expeditions was led by Leo Frobenius, a celebrated ethnographer and personal friend of Kaiser Wilhelm II. Under Iyasu's directions, Ethiopia probably supplied weapons to the Muslim Dervish rebels during the Somaliland Campaign of 1915 to 1916, indirectly helping the Central Powers' cause. \n\nFearing the rising influence of Iyasu and the Ottoman Empire, the Christian nobles of Ethiopia conspired against Iyasu over 1915. Iyasu was first excommunicated by the Ethiopian Orthodox Patriarch and eventually deposed in a coup d'état on 27 September 1916. A less pro-Ottoman regent was installed on the throne.\n\nOther movements supported the efforts of the Central Powers for their own reasons, such as the radical Irish Nationalists who launched the Easter Rising in Dublin in April 1916; they referred to their \"gallant allies in Europe\". However, the majority of Irish Nationalists supported the British and allied war effort up until 1916 when the Irish political landscape was changing. In 1914, Józef Piłsudski was permitted by Germany and Austria-Hungary to form independent Polish legions. Piłsudski wanted his legions to help the Central Powers defeat Russia and then side with France and the UK and win the war with them.\n\nBulgaria signed an armistice with the Allies on 29 September 1918, following a successful Allied advance in Macedonia. The Ottoman Empire followed suit on 30 October 1918 in the face of British and Arab gains in Palestine and Syria. Austria and Hungary concluded ceasefires separately during the first week of November following the disintegration of the Habsburg Empire and the Italian offensive at Vittorio Veneto; Germany signed the armistice ending the war on the morning of 11 November 1918 after the Hundred Days Offensive, and a succession of advances by New Zealand, Australian, Canadian, Belgian, British, French and US forces in north-eastern France and Belgium. There was no unified treaty ending the war; the Central Powers were dealt with in separate treaties.\n\n\n"}
{"id": "58903", "url": "https://en.wikipedia.org/wiki?curid=58903", "title": "Cloud forest", "text": "Cloud forest\n\nA cloud forest, also called a water forest, is a generally tropical or subtropical, evergreen, montane, moist forest characterized by a persistent, frequent or seasonal low-level cloud cover, usually at the canopy level, formally described in the International Cloud Atlas (2017) as silvagenitus. Cloud forests often exhibit an abundance of mosses covering the ground and vegetation, in which case they are also referred to as mossy forests. Mossy forests usually develop on the saddles of mountains, where moisture introduced by settling clouds is more effectively retained.\n\nDependent on local climate, which is affected by the distance to the sea, the exposition and the latitude (from 23°N to 25°S), the altitude varies from 500 m to 4000 m above sea level. Typically, there is a relatively small band of altitude in which the atmospheric environment is suitable for cloud forest development. This is characterized by persistent fog at the vegetation level, resulting in the reduction of direct sunlight and thus of evapotranspiration. Within cloud forests, much of the moisture available to plants arrives in the form of fog drip, where fog condenses on tree leaves and then drips onto the ground below.\n\nAnnual rainfall can range from 500 to 10,000 mm/year and mean temperature between 8 and 20 °C.\n\nWhile cloud forest today is the most widely used term, in some regions, these ecosystems or special types of cloud forests are called mossy forest, elfin forest, montane thicket, and dwarf cloud forest.\n\nThe definition of cloud forest can be ambiguous, with many countries not using the term (preferring such terms as Afromontane forest and upper montane rain forest, montane laurel forest, or more localised terms such as the Bolivian \"yungas\", and the \"laurisilva\" of the Atlantic Islands), and occasionally subtropical and even temperate forests in which similar meteorological conditions occur are considered to be cloud forests.\n\nOnly 1% of the global woodland consists of cloud forests.\n\nImportant areas of cloud forest are in Central and South America, East and Central Africa, Indonesia, Malaysia, the Philippines, Papua-New Guinea, and in the Caribbean.\n\nIn comparison with lower tropical moist forests, cloud forests show a reduced tree stature combined with increased stem density and generally the lower diversity of woody plants. Trees in these regions are generally shorter and more heavily stemmed than in lower-altitude forests in the same regions, often with gnarled trunks and branches, forming dense, compact crowns. Their leaves become smaller, thicker and harder with increasing altitude. The high moisture promotes the development of a high biomass and biodiversity of epiphyte, particularly bryophytes, lichens, ferns (including filmy ferns), bromeliads and orchids. The number of endemic plants can be very high.\n\nAn important feature of cloud forests is the tree crowns can intercept the wind-driven cloud moisture, part of which drips to the ground. This fog drip occurs when water droplets from the fog adhere to the needles or leaves of trees or other objects, coalesce into larger drops and then drop to the ground. It can be an important contribution to the hydrologic cycle.\n\nDue to the high water content of the soil, the reduced solar radiation and the low rates of decomposition and mineralization, the soil acidity is very high, with more humus and peat often forming the upper soil layer.\n\nStadtmüller (1987) distinguishes two general types of tropical montane cloud forests:\n\nAlthough far from being universally accepted as true cloud forests, several forests in temperate regions have strong similarities with tropical cloud forests. The term is further confused by occasional reference to cloud forests in tropical countries as \"temperate\" due to the cooler climate associated with these misty forests.\n\n\n\nIn 1970, the original extent of cloud forests on the Earth was around 50 million hectares. Population growth, poverty and uncontrolled land use have contributed to the loss of cloud forests. The 1990 Global Forest Survey found that 1.1% of tropical mountain and highland forests were lost each year, which was higher than in any other tropical forests. In Colombia, one of the countries with the largest area of cloud forests, only 10–20% of the initial cloud forest cover remains. Significant areas have been converted to plantations, or for use in agriculture and pasture. Significant crops in montane forest zones include tea and coffee, and the logging of unique species causes changes to the forest structure.\n\nIn 2004, an estimated one-third of all cloud forests on the planet were protected at that time.\n\nBecause of their delicate dependency on local climates, cloud forests will be strongly affected by global climate change. Results show that the extent of environmentally suitable areas for cloud forest in Mexico will sharply decline in the next 70 years. A number of climate models suggest low-altitude cloudiness will be reduced, which means the optimum climate for many cloud forest habitats will increase in altitude. Linked to the reduction of cloud moisture immersion and increasing temperature, the hydrological cycle will change, so the system will dry out. This would lead to the wilting and the death of epiphytes, which rely on high humidity. Frogs and lizards are expected to suffer from increased drought. Calculations suggest the loss of cloud forest in Mexico would lead to extinction of up to 37 vertebrates specific to that region. In addition, climate changes can result in a higher number of hurricanes, which may increase damage to tropical montane cloud forests. All in all, the results of climate change will be a loss in biodiversity, altitude shifts in species ranges and community reshuffling, and, in some areas, complete loss of cloud forests.\n\nCloud-forest conditions are hard and expensive to replicate in a glasshouse because it is necessary to maintain a very high humidity. This is usually expensive as a high temperature must usually be maintained as well, and a high temperature combined with high humidity calls for good air circulation or else fungi and algae will develop. Such displays usually are quite small, but there are some notable exceptions. For many years, the Singapore Botanic Gardens have a so-called coolhouse, whereas the newly opened (2012) Gardens by the Bay feature a very big coolhouse that is simply named \"Cloud Forest\". The latter features a -high artificial mountain clad in epiphytes such as orchids, ferns, clubmosses, bromeliads and others.\n\n\n"}
{"id": "23718511", "url": "https://en.wikipedia.org/wiki?curid=23718511", "title": "Cold-hardy citrus", "text": "Cold-hardy citrus\n\nCold-hardy citrus is citrus with increased frost tolerance and which may be cultivated far beyond traditional citrus growing regions. Citrus species and citrus hybrids typically described as cold-hardy generally display an ability to withstand wintertime temperatures below . Cold-hardy citrus may be generally accepted 'true' species (e.g. Satsuma mandarin, Kumquat) or hybrids (e.g. Citrange) involving various other citrus species. All citrus fruits are technically edible, though some have bitter flavors often regarded as unpleasant, and this variability is also seen in cold-hardy citrus fruits. Those listed as \"inedible fresh\" or \"semi-edible\" can (like all citrus) be cooked to make marmalade.\n\nVarieties of true citrus considered cold-hardy, ordered from most to least hardy: \n\nInterspecific hybrid varieties considered cold-hardy, ordered from most to least hardy:\n\n"}
{"id": "28113064", "url": "https://en.wikipedia.org/wiki?curid=28113064", "title": "Coniferous swamp", "text": "Coniferous swamp\n\nConiferous swamps are forested wetlands in which the dominant trees are lowland conifers such as northern white cedar (\"Thuja occidentalis\"). The soil in these swamp areas is typically saturated for most of the growing season and is occasionally inundated by seasonal storms or by winter snow melt. \n\nThe substrate is usually organic in nature and may contain peat in varying amounts or be composed entirely of muck. The swamp substrate is typically nutrient-rich and neutral to alkaline but can be acidic and nutrient-poor. \n\nConiferous swamps vary in composition, with different species of conifer dominating, and varying amounts of deciduous hardwoods growing within the swamp. A wide diversity of plants is represented within the swamps, with certain species dominating in a variety of microhabitats dependent on factors such as available sunlight (as in cases of trees downed by wind or disease), soil Ph, standing groundwater, and differences of elevation within the swamp such as tussocks and nurse logs.\n\nThe different types of coniferous swamps are referred to according to their dominant trees. \"Rich conifer swamp\" is dominated by Northern white-cedar and typically occurs south of the climatic tension zone throughout the Midwest and northeastern United States and adjacent areas in Canada. North of the climatic tension zone, tamarack (\"Larix laricina\") is the dominant species of conifer in minerotrophic wetlands classified as \"rich tamarack swamp\". A roughly equal mix of hardwood trees and conifers is known as a \"hardwood-conifer swamp\".\n\nA variety of both evergreen and deciduous trees may be present in the rich conifer swamp in addition to the dominant species.\n\n\n\n\nA variety of grasses and sedges may be present including multiple varieties of \"carex\".\n\n\n\n\n"}
{"id": "9506027", "url": "https://en.wikipedia.org/wiki?curid=9506027", "title": "Criteria air pollutants", "text": "Criteria air pollutants\n\nCriteria air Pollutants (CAP), or criteria pollutants, are a set of air pollutants that cause smog, acid rain, and other health hazards. CAPs are typically emitted from many sources in industry, mining, transportation, electricity generation and agriculture. In many cases they are the products of the combustion of fossil fuels or industrial processes.\n\nThe six criteria air pollutants were the first set of pollutants recognized by the United States Environmental Protection Agency as needing standards on a national level. The Clean Air Act requires the EPA to set US National Ambient Air Quality Standards (NAAQS) for the six CAPs. The NAAQS are health based and the EPA sets two types of standards: primary and secondary. The primary standards are designed to protect the health of 'sensitive' populations such as asthmatics, children, and the elderly. The secondary standards are concerned with protecting the environment. They are designed to address visibility, damage to crops, vegetation, buildings, and animals.\n\nThe EPA established the NAAQS according to Sections 108 and 109 of the U.S. Clean Air Act, which was last amended in 1990. These sections require the EPA \"(1) to list widespread air pollutants that reasonably may be expected to endanger public health or welfare; (2) to issue air quality criteria for them that assess the latest available scientific information on nature and effects of ambient exposure to them; (3) to set primary NAAQS to protect human health with adequate margin of safety and to set secondary NAAQS to protect against welfare effects (e.g., effects on vegetation, ecosystems, visibility, climate, manmade materials, etc); and (5) to periodically review and revise, as appropriate, the criteria and NAAQS for a given listed pollutant or class of pollutants.\"\n\n\nIn 2009, the EPA Administrator found that under section 202(a) of the Clean Air Act greenhouse gases threaten both the public health and the public welfare, and that greenhouse gas emissions from motor vehicles contribute to that threat. This final action has two distinct 'findings,' which are:\n\n1) The 'Endangerment Finding' in which the Administrator found that the mix of atmospheric concentrations of six key, well-mixed greenhouse gases threatens both the public health and the public welfare of current and future generations. These six greenhouse gases are: carbon dioxide (CO), methane (CH), nitrous oxide (NO), hydrofluorocarbons (HFCs), perfluorocarbons (PFCs), and sulfur hexafluoride (SF). These greenhouse gases in the atmosphere constitute the \"air pollution\" that threatens both public health and welfare.\n\n2) The 'Cause or Contribute Finding,' in which the Administrator found that the combined greenhouse gas emissions from new motor vehicles and motor vehicle engines contribute to the atmospheric concentrations of these key greenhouse gases and hence to the threat of climate change.\n\nThe EPA issued these endangerment findings in response to the 2007 supreme court case Massachusetts v. EPA, when the court determined that greenhouse gases are air pollutants according to the Clean Air Act. The court made the decision that the EPA must determine whether greenhouse gas emissions from new motor vehicles \"cause or contribute to air pollution which may be reasonably be anticipated to endanger public health or welfare, or whether the science is too uncertain to make a reasoned decision\" (EPA's Endangerment Finding).\n\nThe EPA determined that, according to this decision, there are six greenhouse gases that need to be regulated. These include: \nThis action allowed the EPA to set the greenhouse gas emission standards to light-duty vehicles proposed jointly with the Department of Transportation's Corporate Average Fuel Economy (CAFE) standards in 2009.\n\nOn December 2, 2009, the Center for Biological Diversity and 350.org requested that the EPA recognize that carbon dioxide and other GHGs are reasonably anticipated to endanger public health and welfare. Petitioners proposed that EPA list carbon dioxide as a criteria air pollutant, as outlined in the Clean Air Act. They also requested that the EPA set NAAQS for carbon dioxide at no greater than 350 ppm- a \"level that accurately reflects the most recent scientific knowledge.\" Petitioners further requested that EPA designate the five other greenhouse gases, highlighted in Mass v. EPA, as criteria pollutants as well and establish pollution caps for them. Furthermore, the petitioners proposed that nitrogen trifluoride (NF) be regulated as a criteria air pollutant in addition to the other six.\n\n\n"}
{"id": "13236337", "url": "https://en.wikipedia.org/wiki?curid=13236337", "title": "Di-positronium", "text": "Di-positronium\n\nDi-positronium, or dipositronium, is a molecule consisting of two atoms of positronium. It was predicted to exist in 1946 by John Archibald Wheeler, and subsequently studied theoretically, but was not observed until 2007 in an experiment performed by David Cassidy and Allen Mills at the University of California, Riverside. \nThe researchers made the positronium molecules by firing intense bursts of positrons into a thin film of porous silicon dioxide. Upon slowing down in the silica, the positrons captured ordinary electrons to form positronium atoms. Within the silica, these were long lived enough to interact, forming molecular di-positronium. Advances in trapping and manipulating positrons, and spectroscopy techniques have enabled studies of Ps-Ps interactions. In 2012, Cassidy et al. were able to produce the excited molecular positronium L = 1 state.\n\n\n"}
{"id": "23936744", "url": "https://en.wikipedia.org/wiki?curid=23936744", "title": "Dinosaurs (book)", "text": "Dinosaurs (book)\n\nDinosaurs: The Most Complete, Up-to-Date Encyclopedia for Dinosaur Lovers of All Ages is a book by Dr. Thomas R. Holtz, Jr., with illustrations by Luis Rey. It was published in 2007 by Random House.\n\n"}
{"id": "10485042", "url": "https://en.wikipedia.org/wiki?curid=10485042", "title": "Donnelley/Depue State Park", "text": "Donnelley/Depue State Park\n\nDonnelley/Depue State Fish and Wildlife Area is an Illinois state park on in Putnam County, Illinois, United States.\n\n"}
{"id": "2336395", "url": "https://en.wikipedia.org/wiki?curid=2336395", "title": "Earth X", "text": "Earth X\n\nEarth X is a 1999 comic book limited series published by Marvel Comics. \"Earth X\" was written by Jim Krueger with art by John Paul Leon. Based on Alex Ross' notes, the series features a dystopian version of the Marvel Universe.\n\nThe series was followed by two sequels, Universe X and Paradise X. The universe of \"Earth X\" is designated as Earth-9997.\n\n\"Earth X\" began in 1997 when \"Wizard\" magazine asked Alex Ross to create a possible dystopian future for Marvel. Ross designed a future where all ordinary humans had gained superpowers, and he examined how some of the most well-known Marvel characters (including Spider-Man, Captain America and the Incredible Hulk) would manage a world where their superhero powers had now become commonplace. The issue of \"Wizard\" that contained the Ross article sold out rapidly. Demand was so extensive that in 1999 (in affiliation with Marvel), they republished the article as the \"Earth X Sketchbook\", which also sold out. Based on this indicator of fan interest, Marvel commissioned Ross to create a full series based on his notes.\n\nEarth X is one of a number of planets implanted with a gestating Celestial egg. About ten years after the end of the heroic age, Black Bolt releases the mutagenic Terrigen Mists into Earth's atmosphere, seeking to transform humanity into Inhumans so that his people would not suffer persecution.\n\nHe blinded Uatu the Watcher to prevent him from witnessing his actions, and Black Bolt and the Inhumans leave Earth. Unable to operate his observation equipment, Uatu transports X-51 (Machine Man)—who has long since given up super-heroics to imitate the life of his human creator—to the Moon to act as Earth's new Watcher. X-51 gets increasingly annoyed at Uatu's assurance of the heroes' defeat.\n\nAfter killing Red Skull, Captain America quits the Avengers, considering himself unfit for the team. Shortly afterward, Reed Richards constructs a worldwide network of vibranium power centers to solve the looming energy crisis, but the experiment fails when one of Reed's scientists falls into the reactor, causing a worldwide explosive chain reaction. The scientist that fell into the reactor became encased within vibranium and discovered that she can manipulate the rare metal, becoming the Iron Maiden.\n\nThe Terrigen Mists begin mutating Earth's human population, though much of the world blames \"Plague X\" on Richards' failed experiment. Benny Beckley, the young son of Comet Man, gains the ability to control the actions of others and becomes known as Skull. Nearly all of the world's telepaths are killed by the backlash caused by Beckley's power manifestation. Meanwhile, Doctor Doom and Namor the Sub-Mariner fight the Fantastic Four. Doom is killed in an explosion along with Susan Richards. Namor kills Johnny Storm. Franklin Richards responds by cursing Namor, causing one side of his body to burst into flame upon contact with air, forcing Namor back into the ocean. A distraught Reed Richards dons Doom's armor and exiles himself to Castle Doom in Latveria.\n\nSoon after, the Absorbing Man absorbs Ultron. The Vision defeats the Absorbing Man with a computer virus. Attempting to isolate the virus by turning to stone, the Absorbing Man is shattered by the Vision after murdering the Avengers. Absorbing Man's pieces are scattered amongst the world's leaders so that he can never be reassembled.\n\nNorman Osborn manipulates America into electing him President by using alien DNA to create the Hydra—a parasite collective that mind-controls its host bodies—and grants Tony Stark (one of the last unmutated humans) political asylum in exchange for constructing robotic replicas of the fallen Avengers to battle the Hydra menace.\n\nLoki tricked Odin into changing Thor into a female and discovers an elaborate Celestial manipulation on a cosmic scale: ancient humans were modified by the Celestials, given the ability to develop powers in order to act as a type of anti-bodies meant to stop radical elements such as deviants and alien species from harming the Earth and the Celestial embryo growing within it. Along with Earth, the Celestials modified the native species of many planets harboring Celestial embryos. Species affected by Celestial modification gradually undergo an evolution process where they develop powers either through natural means such as mutants, or unnatural means such as meta-humans like Spiderman. Species with Celestial modifications in theory hold enough power to rival the celestials, but are inhibited by a clever psychological safeguard put in place by the Celestials. The safeguard relates to constructivist philosophy, that essentially states that people are shaped by their environment. The deities and devils within the Marvel universe are species that were modified by Celestials and unwittingly inhibit their own abilities due to the fact that they self-identify as a Marvel deity or devil. These evolved beings hold their sense of identity being reinforced by those that interact with them. Loki realizes that he was only the evil son of Odin and a Norse God because when he evolved beyond his mortal form and his psyche became a tabula rasa. The first interaction that he had with another in that form were Nordic humans who thought that he was an evil god and thus he became one.\n\nMeanwhile, Captain America and his partner Redwing learn that the Skull is gathering a mind-controlled army. After Redwing is overcome by the Skull's powers, Captain America retreats and recruits allies to fight the Skull.\n\nMeanwhile, the Inhuman Royal Family return from space and contact Reed Richards, hoping to reunite with their people. While trying to find the lost Inhuman nation with Cerebro, Richards discovers Bolt's actions. The Skull's army reaches New York, overtaking it, and Captain America and his allies fall to the Skull's powers. While the Skull is distracted, Captain America kills the child dictator and liberates his followers.\n\nBefore the heroes can celebrate, the Celestials arrive on Earth to germinate the embryo. As the Celestials prepare to attack New York, Loki arrives with a host of Asgardians found in the afterlife and announces he's not bad because he formed the Avengers. Tony Stark sacrifices himself while trying to hold off the arriving Celestials. Black Bolt is killed just after using his voice to travel across the universe and call for Galactus, who is revealed to be devouring worlds in order to destroy Celestial embryos growing within them. Galactus kills several Celestials and forces the others to flee. Afterwards, Galactus consumes Earth's Celestial embryo. Galactus prepares to leave, and Reed requests that he remove his helmet. Galactus reluctantly agrees, revealing that he is actually Franklin Richards. Franklin Richards previously convinced himself that he was Galactus, willingly sacrificing his own identity to turn into the destroyer of worlds after realizing that Galactus needed to exist in order to keep the Celestial population in check. This needed to be done after the Fantastic Four killed the original Galactus with the ultimate nullifier. Reed Richards also realizes this fact and makes the heartbreaking decision to not remind Franklin of who he originally was for the sake of the universe.\n\nX-51 realizes the Watchers' true purpose is to watch over Celestial eggs because one of their numbers failed to stop the birth of Galactus millions of years ago. X-51 destroys Uatu's ears and decides to destroy all Celestial eggs gestating inside the various planets within the universe. Reed converts his vibranium power network into \"Human Torches\", hoping to burn off the Terrigen Mists and restore Earth's human population.\n\nWith the Celestial embryo gone, the Earth's mass is reduced, causing a shift in orbit and polarity as well as drastic worldwide climate changes. One-fourth of New York's population dies as temperatures plummet. The Tong of Creel, a cult dedicated to reassembling the Absorbing Man, begins killing those who hold his fragments. Under Mephisto's influence, Pope Immortus founds a church advocating mutant dominance of the galaxy and the destruction of Reed's Human Torches. Meanwhile, Mar-Vell is reincarnated as the child of the synthetic Him and Her, though his soul remains in the Realm of the Dead. Captain America becomes the Mar-Vell child's guardian and embarks on a worldwide quest with his new ward to obtain various items in order to deal with Earth's restless mutant population and prepare for an impending war in the Realm of the Dead. Arriving at Zero Street, the duo is attacked by the Night People, and Captain America sacrifices his life to save the Mar-Vell child.\n\nMar-Vell reveals that other than creating constructivist safeguards to stop species that they modified from being able to challenge their supremacy, the Celestials also schemed to manipulate causality and fatalism by helping to create beings such as death and Mephisto to distort truth and reality. Mar-Vell also discovered that beings that travel through time actually create new universes in the process of doing so and Celestials, acting through others such as Mephisto, encouraged time travel as it created entirely new universes for the Celestial to inhabit and grow in number.\n\nWhen the Tong of Creel finally reassembles the Absorbing Man in New York, he attacks the city's Human Torch. Battling New York's heroes, the Absorbing Man absorbs Manhattan itself, adding its buildings and streets to his being, but Loki and Iron Maiden convince the villain to transform himself into vibranium and use his mass to stabilize the planet's fluctuating orbit and polarity. Meanwhile, in the Realm of the Dead, Mar-Vell leads an army of deceased heroes and villains against Thanos and Death. With the artifacts collected by himself and Captain America in his possession, Mar-Vell shows Thanos how Death has manipulated him and convinces Thanos to use the Ultimate Nullifier on the entity.\n\nWith Death destroyed, Mar-Vell constructs a Paradise in the center of the Negative Zone for the dead to inhabit. However, those among the living find themselves unable to die.\n\nMeanwhile, X-51 decides that the inhabitants of alternate Earths should be warned about the Celestial embryos he believes are growing within their planets. He spreads the alarm across the multiverse by recruiting and dispatching Heralds from alternate timelines such as Bloodstorm (Ororo Monroe, Earth-1298), Deathlok (Luther Manning, Earth-7484), Hyperion (Earth-1121), Killraven (Earth-691), Iron Man 2020 (Earth-8410), Spider-Girl (Earth-1122), and Wolverine (Days of Future Past Earth-811).\n\nAfter banishing the Watchers of Earth-9997 to alternate worlds with the hope that their presence will lead to the discovery and destruction of each Celestial embryo, X-51 takes his Heralds to his Earth, where he will aid each in achieving his or her wishes. In Mar-Vell's Paradise, the High Evolutionary's equipment transforms the souls of Black Bolt, Captain America, Daredevil (Matt Murdock), Dr. Doom, Giant-Man, Phoenix, and Tony Stark into the Avenging Host, charged with ushering souls from the Realm of the Dead to Paradise. Those who enter Paradise consume a piece of the Cosmic Cube, enabling them to create their own, seemingly perfect pocket reality. But as more souls enter Paradise, it begins to expand and consume entire worlds within the Negative Zone, causing Blastaar and Annihilus to attack the Baxter Building in New York.\n\nReed Richards, Bruce Banner, the Beast, and several other brilliant scientists convene to discuss a solution to Death's absence. They decide to access the imprisoned Jude the Entropic Man, who can turn others to dust on contact, and synthesize his essence into a chemical to end the suffering of those unable to die. With the chemical complete, Reed, growing suspicious of Mar-Vell's motives, plans to use Pym Particles to slow Paradise's rapid growth within the Negative Zone. Mephisto frees Jude from captivity, convincing him to go on a killing spree. Mephisto then steers Jude to Britain, where Mephisto hopes to find the Siege Perilous, which will allow him to traverse the multiverse. With the help of Merlin, Doctor Strange, Psylocke and the sacrifice of a recently resurrected Meggan, King Britain is able to slay Mephisto with Excalibur. Meanwhile, in Paradise, Reed and a legion of heroes confront Mar-Vell. After Paradise is nearly conquered in the name of the Supreme Intelligence by the arriving souls of the Kree military, Mar-Vell explains to Reed that he (Reed) is to become the new Eternity.\n\nUsing his new role as Eternity, Reed is able to end the conflict and free the remaining heroes from their Cosmic Cube-induced dream-worlds. Once this is accomplished, Mar-Vell explains to Reed that his plan is to build a wall around their universe, preventing any further influence from the Celestials. Feeling that his work is not yet complete, Mar-Vell tells the people of Paradise that he is going to the source of Excalibur, which is strongly implied to be the original universe.\n\n\n\nAfter Mar-Vell killed Death, he reshaped part of the Realm of the Dead into a paradise and selected a group of dead heroes to be its guardians called the Avenging Host where most of its members have angel-like forms.\n\n\nThe Fantastic Four no longer exist.\n\n\n\nThis is the new incarnation of the X-Men that is led by Cyclops. They were former members of the second Daredevil's circus:\n\n\n\n\n\nInitially, the \"Earth X\" storyline was purported as being the future of Earth-616. However, the series often substantially retconned the origins and workings of characters to better suit the story, to the point where they were no longer reconcilable with their counterparts in the mainstream Marvel Universe. One example is the revelation in \"Paradise X\" that Wolverine is not a mutant, but instead one of the few remaining \"pure strain humans\", free from the genetic manipulations of the Celestials (as well as a descendent of Moon-Boy). Marvel editors solved these discrepancies by officially declaring that anything stated in \"Earth X\" would not be considered canonical. It is later revealed in issue #11 of \"Paradise X\" that the events shown in the series are not set in an alternative future, but rather an alternative present (the issue reveals that \"Paradise X\" is set in 2003, the year of publication).\n\nThe \"Paradise X\" series was never properly concluded, due to editorial interference midway through its publication. Due to dwindling sales, the \"X\" and \"A\" specials, which were intended to be double-sized issues, were both reduced to 22 pages and the intended ending was never used. Writer Jim Krueger expressed dismay at the loss of pages and not being able to use the original ending. In the intended ending, Captain America, suspecting Captain Marvel's treachery, would have killed Marvel just as Marvel put the energy wall around the universe to keep out the Celestials and Elders. At this final moment, having ascended to the throne of Paradise, Captain America would have realized that Marvel's intentions were good. \"Cap would have sat on the throne, completely unworthy of it. And this, this would have been the final testing necessary to make Cap worthy of it.\"\n\nAdditionally, a planned limited series, \"Tales of Earth X\", was proposed but never greenlighted. The series, set before the events of \"Earth X\", would have revealed the final days of characters like Professor X before the mutation that turned the world into mutants and killed all of Earth's psychics.\n\nA one-shot called \"The Earth X Companion\" was released in 2008, containing production notes and sketches by Jim Krueger and Alex Ross as well as a short story drawn by artist Bill Reinhold detailing the fate of one of Nick Fury's Life Model Decoys.\n\nIn September 2005, Marvel released a 592-page hardcover deluxe edition of \"Earth X\". This new edition includes 12 issues of the \"Earth X\" regular series, the #0 and #X bookends, the #1/2 issue (drawn by artist Bill Reinhold) and the Epilogue. It also contains extras pulled from the Graphitti hardcover, Marvel's trade paperback and the sketchbooks.\n\nThe various volumes include:\n\n\nAccompanying volumes include:\n\n\n\n"}
{"id": "43400705", "url": "https://en.wikipedia.org/wiki?curid=43400705", "title": "Ekeby oak tree", "text": "Ekeby oak tree\n\nThe Ekeby oak tree () is an oak tree in Ekerö outside Stockholm, Sweden, close to Ekebyhov Castle. It is the largest living deciduous tree in Sweden by volume. \n\nThe Ekeby oak is approximately 500 years old. It was declared a natural monument in 1956. There are many old trees around Ekebyhov Castle; the oak, sometimes called \"Ekeröjätten\" (the Ekerö giant) stands alone in a field south of the castle, where it had no competition for space from other trees. It was measured in 2008 as the largest tree by volume in Sweden.\n"}
{"id": "60879", "url": "https://en.wikipedia.org/wiki?curid=60879", "title": "Electroluminescence", "text": "Electroluminescence\n\nElectroluminescence (EL) is an optical phenomenon and electrical phenomenon in which a material emits light in response to the passage of an electric current or to a strong electric field. This is distinct from black body light emission resulting from heat (incandescence), from a chemical reaction (chemiluminescence), sound (sonoluminescence), or other mechanical action (mechanoluminescence).\n\nElectroluminescence is the result of radiative recombination of electrons & holes in a material, usually a semiconductor. The excited electrons release their energy as photons - light. Prior to recombination, electrons and holes may be separated either by doping the material to form a p-n junction (in semiconductor electroluminescent devices such as light-emitting diodes) or through excitation by impact of high-energy electrons accelerated by a strong electric field (as with the phosphors in electroluminescent displays).\n\nIt has been recently shown that as a solar cell improves its light-to-electricity efficiency (improved open-circuit voltage), it will also improve its electricity-to-light (EL) efficiency.\n\nElectroluminescent devices are fabricated using either organic or inorganic electroluminescent materials. The active materials are generally semiconductors of wide enough bandwidth to allow exit of the light.\n\nThe most typical inorganic thin-film EL (TFEL) is ZnS:Mn with yellow-orange emission. Examples of the range of EL material include:\n\nThe most common electroluminescent (EL) devices are composed of either powder (primarily used in lighting applications) or thin films (for information displays.)\n\n\"Light-emitting capacitor\", or LEC, is a term used since at least 1961 to describe electroluminescent panels. General Electric has patents dating to 1938 on flat electroluminescent panels that are still made as night lights and backlights for instrument panel displays. Electroluminescent panels are a capacitor where the dielectric between the outside plates is a phosphor that gives off photons when the capacitor is charged. By making one of the contacts transparent, the large area exposed emits light.\n\nElectroluminescent automotive instrument panel backlighting, with each gauge pointer also an individual light source, entered production on 1960 Chrysler and Imperial passenger cars, and was continued successfully on several Chrysler vehicles through 1967.\n\nSylvania Lighting Division in Salem and Danvers, MA, produced and marketed an EL night lamp (right), under the trade name \"Panelescent\" at roughly the same time that the Chrysler instrument panels entered production. These lamps have proven extremely reliable, with some samples known to be still functional after nearly 50 years of continuous operation. Later in the 1960s, Sylvania's Electronic Systems Division in Needham, MA developed and manufactured several instruments for the Apollo Lunar Lander and Command Module using electroluminescent display panels manufactured by the Electronic Tube Division of Sylvania at Emporium, PA. Raytheon, Sudbury, MA, manufactured the Apollo guidance computer, which used a Sylvania electroluminescent display panel as part of its display-keyboard interface (DSKY).\n\nPowder phosphor-based electroluminescent panels are frequently used as backlights for liquid crystal displays. They readily provide gentle, even illumination for the entire display while consuming relatively little electric power. This makes them convenient for battery-operated devices such as pagers, wristwatches, and computer-controlled thermostats, and their gentle green-cyan glow is common in the technological world. They require relatively high voltage (between 60 and 600 volts). For battery-operated devices, this voltage must be generated by a converter circuit within the device. This converter often makes an audible whine or siren sound while the backlight is activated. For line-voltage-operated devices, it may be supplied directly from the power line. Electroluminescent nightlights operate in this fashion. Brightness per unit area increases with increased voltage and frequency.\n\nThin film phosphor electroluminescence was first commercialized during the 1980s by Sharp Corporati\n\non in Japan, Finlux (Oy Lohja Ab) in Finland, and Planar Systems in the US. Here, bright, long-life light emission is achieved in thin film yellow-emitting manganese-doped zinc sulfide material. Displays using this technology were manufactured for medical and vehicle applications where ruggedness and wide viewing angles were crucial, and liquid crystal displays were not well developed. In 1992, Timex introduced its Indiglo EL display on some watches.\n\nRecently, blue-, red-, and green-emitting thin film electroluminescent materials that offer the potential for long life and full color electroluminescent displays have been developed.\n\nIn either case, the EL material must be enclosed between two electrodes and at least one electrode must be transparent to allow escape of the produced light. Glass coated with indium tin oxide is commonly used as the front (transparent) electrode while the back electrode is coated with reflective metal. Additionally, other transparent conducting materials, such as carbon nanotube coatings or PEDOT can be used as the front electrode.\n\nThe display applications are primarily passive (i.e., voltages are driven from edge of the display cf. driven from a transistor on the display). Similar to LCD trends, there have also been Active Matrix EL (AMEL) displays demonstrated, where circuitry is added to prolong voltages at each pixel. The solid-state nature of TFEL allows for a very rugged and high-resolution display fabricated even on silicon substrates. AMEL displays of 1280x1024 at over 1000 lines per inch (lpi) have been demonstrated by a consortium including Planar Systems.\n\nElectroluminescent technologies have low power consumption compared to competing lighting technologies, such as neon or fluorescent lamps. This, together with the thinness of the material, has made EL technology valuable to the advertising industry. Relevant advertising applications include electroluminescent billboards and signs. EL manufacturers are able to control precisely which areas of an electroluminescent sheet illuminate, and when. This has given advertisers the ability to create more dynamic advertising that is still compatible with traditional advertising spaces.\n\nAn EL film is a so-called Lambertian radiator: unlike with neon lamps, filament lamps, or LEDs, the brightness of the surface appears the same from all angles of view; electroluminescent light is not directional and therefore hard to compare with (thermal) light sources measured in lumens or lux. The light emitted from the surface is perfectly homogeneous and is well-perceived by the eye. EL film produces single-frequency (monochromatic) light that has a very narrow bandwidth, is absolutely uniform and visible from a great distance.\nIn principle, EL lamps can be made in any color. However, the commonly used greenish color closely matches the peak sensitivity of human vision, producing the greatest apparent light output for the least electrical power input. Unlike neon and fluorescent lamps, EL lamps are not negative resistance devices so no extra circuitry is needed to regulate the amount of current flowing through them.\nA new technology now being used is based on multispectral phosphors that emit light from 600 to 400nm depending on the drive frequency; this is similar to the colour changing effect seen with aqua EL sheet but on a larger scale.\n\nElectroluminescent lighting is now used as an application for public safety identification involving alphanumeric characters on the roof of vehicles for clear visibility from an aerial perspective.\n\nElectroluminescent lighting, especially electroluminescent wire (EL wire), has also made its way into clothing as many designers have brought this technology to the entertainment and night life industry.\n\nEngineers have developed an electroluminescent \"skin\" that can stretch more than six times its original size while still emitting light. This hyper-elastic light-emitting capacitor (HLEC) can endure more than twice the strain of previously tested stretchable displays. It consists of layers of transparent hydrogel electrodes sandwiching an insulating elastomer sheet. The elastomer changes luminance and capacitance when stretched, rolled and otherwise deformed. In addition to its ability to emit light under a strain of greater than 480% its original size, the group's HLEC was shown to be capable of being integrated into a soft robotic system. Three six-layer HLEC panels were bound together to form a crawling soft robot, with the top four layers making up the light-up skin and the bottom two the pneumatic actuators. The discovery could lead to significant advances in health care, transportation, electronic communication and other areas.\n\n\n"}
{"id": "16174922", "url": "https://en.wikipedia.org/wiki?curid=16174922", "title": "Energy subsidies", "text": "Energy subsidies\n\nEnergy subsidies are measures that keep prices for consumers below market levels or for producers above market levels, or reduce costs for consumers and producers. Energy subsidies may be direct cash transfers to producers, consumers, or related bodies, as well as indirect support mechanisms, such as tax exemptions and rebates, price controls, trade restrictions, and limits on market access. They may also include energy conservation subsidies. The development of today's major modern energy industries have all relied on substantial subsidy support.\n\nGlobal fossil fuel subsidies represented 6.5% of global GDP in 2015. The elimination of these subsidies is widely seen as one of the most effective ways of reducing global carbon emissions.\n\nMain arguments for energy subsidies are:\n\nMain arguments against energy subsidies are:\n\nTypes of energy subsidies are:\n\nOverall, energy subsidies require coordination and integrated implementation, especially in light of globalization and increased interconnectedness of energy policies, thus their regulation at the World Trade Organization is often seen as necessary.\n\nThe degree and impact of fossil fuel studies is extensively studied. Because fossil fuels are a leading contributor to climate change through greenhouse gases, fossil fuel subsidies increase emissions and exacerbate climate change. The OECD’s inventory in 2015 determined an overall value of $160bn-$200bn per year between 2010 and 2014 relying on the WTO's 1994 definition of fossil fuel subsidies as “financial contribution by a government” which “confers a benefit” on its recipient. This is the only internationally agreed definition of the term.\n\nA 2016 IMF study estimated that global fossil fuel subsidies were $5.3 trillion in 2015, which represents 6.5% of global GDP. The study found that \"China was the biggest subsidizer in 2013 ($1.8 trillion), followed by the United States ($0.6 trillion), and Russia, the European Union, and India (each with about $0.3 trillion).\" The authors estimated that the elimination of \"subsidies would have reduced global carbon emissions in 2013 by 21% and fossil fuel air pollution deaths 55%, while raising revenue of 4%, and social welfare by 2.2%, of global GDP.\" This study is controversial for its radical break with previous definitions of subsidies by redefining externalities as a subsidy, as well as an excessively broad application of social costs as oil externalities. The externalities accounted for are broad enough that oil companies not paying for automobile accidents is considered a subsidy. \n\nAccording to the \"International Energy Agency\", the elimination of fossil fuel subsidies worldwide would be the one of the most effective ways of reducing greenhouse gases and battling global warming. In May 2016, the G7 nations set for the first time a deadline for ending most fossil fuel subsidies; saying government support for coal, oil and gas should end by 2025.\n\nAccording to the OECD, subsidies supporting fossil fuels, particularly coal and oil, represent greater threats to the environment than subsidies to renewable energy. Subsidies to nuclear power contribute to unique environmental and safety issues, related mostly to the risk of high-level environmental damage, although nuclear power contributes positively to the environment in the areas of air pollution and climate change. According to Fatih Birol, Chief Economist at the International Energy Agency without a phasing out of fossil fuel subsidies, countries will not reach their climate targets.\n\nA 2010 study by Global Subsidies Initiative compared global relative subsidies of different energy sources. Results show that fossil fuels receive 0.8 US cents per kWh of energy they produce (although it should be noted that the estimate of fossil fuel subsidies applies only to consumer subsidies and only within non-OECD countries), nuclear energy receives 1.7 cents / kWh, renewable energy (excluding hydroelectricity) receives 5.0 cents / kWh and bio-fuels receive 5.1 cents / kWh in subsidies.\n\nIn 2011, IEA chief economist Faith Birol said the current $409 billion equivalent of fossil fuel subsidies are encouraging a wasteful use of energy, and that the cuts in subsidies is the biggest policy item that would help renewable energies get more market share and reduce CO emissions.\n\nGlobal renewable energy subsidies reached $88 billion in 2011. According to the OECD, subsidies to renewable energy are generally considered more environmentally beneficial than fossil fuel subsidies, although the full range of environmental effects should be taken into account.\n\nAccording to International Energy Agency (IEA) (2011) energy subsidies artificially lower the price of energy paid by consumers, raise the price received by producers or lower the cost of production. \"Fossil fuels subsidies costs generally outweigh the benefits. Subsidies to renewables and low-carbon energy technologies can bring long-term economic and environmental benefits\". In November 2011, an IEA report entitled \"Deploying Renewables 2011\" said \"subsidies in green energy technologies that were not yet competitive are justified in order to give an incentive to investing into technologies with clear environmental and energy security benefits\". The IEA's report disagreed with claims that renewable energy technologies are only viable through costly subsidies and not able to produce energy reliably to meet demand. \"A portfolio of renewable energy technologies is becoming cost-competitive in an increasingly broad range of circumstances, in some cases providing investment opportunities without the need for specific economic support,\" the IEA said, and added that \"cost reductions in critical technologies, such as wind and solar, are set to continue.\"\n\nFossil-fuel consumption subsidies were $409 billion in 2010, oil products being half of it. Renewable-energy subsidies were $66 billion in 2010 and will reach $250 billion by 2035, according to IEA. Renewable energy is subsidized in order to compete in the market, increase their volume and develop the technology so that the subsidies become unnecessary with the development. Eliminating fossil-fuel subsidies could bring economic and environmental benefits. Phasing out fossil-fuel subsidies by 2020 would cut primary energy demand 5%. Since the start of 2010, \"at least 15 countries have taken steps to phase out fossil-fuel subsidies.\" According to IEA onshore wind may become competitive around 2020 in the European Union.\n\nAccording to the IEA the phase-out of fossil fuel subsidies, over $500 billion annually, will reduce 10% greenhouse gas emissions by 2050.\n\nThe International Energy Agency estimates that governments subsidised fossil fuels by US $548 billion in 2013. Ten countries accounted for almost three-quarters of this figure. At their meeting in September 2009 the G-20 countries committed to \"rationalize and phase out over the medium term inefficient fossil fuel subsidies that encourage wasteful consumption\". The 2010s have seen many countries reducing energy subsidies, for instance in July 2014 Ghana abolished all diesel and gasoline subsidies, whilst in the same month Egypt raised diesel prices 63% as part of a raft of reforms intended to remove subsidies within 5 years.\n\nThe public energy subsidies for energy in Finland in 2013 were €700 million for fossil energy and €60 million for renewable energy (mainly wood and wind).\n\nThe energy policy of Turkey includes heavily subsidizing coal.\n\nAccording to a Congressional Budget Office testimony, roughly three-fourths of the projected cost of tax preferences for energy in 2016 was for renewable energy and energy efficiency. An estimated $10.9 billion was directed toward renewable energy; $2.7 billion, went to energy efficiency or electricity transmission. Fossil fuels accounted for most of the remaining cost of energy-related tax preferences—an estimated $4.6 billion.\n\nAccording to a 2015 estimate by the Obama administration, the US oil industry benefited from subsidies of about $4.6 billion per year. A 2017 study by researchers at Stockholm Environment Institute published in the journal \"Nature Energy\" estimated that nearly half of U.S. oil production would be unprofitable without subsidies.\n\nOn March 13, 2013, Terry M. Dinan, senior advisor at the Congressional Budget Office, testified before the Subcommittee on Energy of the Committee on Science, Space, and Technology in the U.S. House of Representatives that federal energy tax subsidies would cost $16.4 billion that fiscal year, broken down as follows:\nIn addition, Dinan testified that the U.S. Department of Energy would spend an additional $3.4 billion on financial Support for energy technologies and energy efficiency, broken down as follows:\n\n\nA 2011 study by the consulting firm Management Information Services, Inc. (MISI) estimated the total historical federal subsidies for various energy sources over the years 1950–2010. The study found that oil, natural gas, and coal received $369 billion, $121 billion, and $104 billion (2010 dollars), respectively, or 70% of total energy subsidies over that period. Oil, natural gas, and coal benefited most from percentage depletion allowances and other tax-based subsidies, but oil also benefited heavily from regulatory subsidies such as exemptions from price controls and higher-than-average rates of return allowed on oil pipelines. The MISI report found that non-hydro renewable energy (primarily wind and solar) benefited from $74 billion in federal subsidies, or 9% of the total, largely in the form of tax policy and direct federal expenditures on research and development (R&D). Nuclear power benefited from $73 billion in federal subsidies, 9% of the total, largely in the form of R&D, while hydro power received $90 billion in federal subsidies, 12% of the total.\n\nA 2009 study by the Environmental Law Institute assessed the size and structure of U.S. energy subsidies in 2002–08. The study estimated that subsidies to fossil fuel-based sources totaled about $72 billion over this period and subsidies to renewable fuel sources totaled $29 billion. The study did not assess subsidies supporting nuclear energy.\n\nThe three largest fossil fuel subsidies were:\n\nThe three largest renewable fuel subsidies were:\nIn the United States, the federal government has paid US$74 billion for energy subsidies to support R&D for nuclear power ($50 billion) and fossil fuels ($24 billion) from 1973 to 2003. During this same timeframe, renewable energy technologies and energy efficiency received a total of US $26 billion. It has been suggested that a subsidy shift would help to level the playing field and support growing energy sectors, namely solar power, wind power, and bio-fuels. However, many of the \"subsidies\" available to the oil and gas industries are general business opportunity credits, available to all US businesses (particularly, the foreign tax credit mentioned above). The value of industry-specific (oil, gas, and coal) subsidies in 2006 was estimated by the Texas State Comptroller to be $6.25 billion - about 60% of the amount calculated by the Environmental Law Institute. The balance of federal subsidies, which the comptroller valued at $7.4 billion, came from shared credits and deductions, and oil defense (spending on the Strategic Petroleum Reserve, energy infrastructure security, etc.).\n\nCritics allege that the most important subsidies to the nuclear industry have not involved cash payments, but rather the shifting of construction costs and operating risks from investors to taxpayers and ratepayers, burdening them with an array of risks including cost overruns, defaults to accidents, and nuclear waste management. Critics claim that this approach distorts market choices, which they believe would otherwise favor less risky energy investments.\n\nMany energy analysts, such as Clint Wilder, Ron Pernick and Lester Brown, have suggested that energy subsidies need to be shifted away from mature and established industries and towards high growth clean energy. They also suggest that such subsidies need to be reliable, long-term and consistent, to avoid the periodic difficulties that the wind industry has had in the United States.\n\nA 2012 study authored by researchers at the Breakthrough Institute, Brookings Institution, and World Resources Institute estimated that between 2009 and 2014 the federal government will spend $150 billion on clean energy through a combination of direct spending and tax expenditures. Renewable electricity (mainly wind, solar, geothermal, hydro, and tidal energy) will account for the largest share of this expenditure, 32.1%, while spending on liquid biofuels will account for the next largest share, 16.1%. Spending on multiple and other forms of clean energy, including energy efficiency, electric vehicles and advanced batteries, high-speed rail, grid and transportation electrification, nuclear, and advanced fossil fuel technologies, will account for the remaining share, 51.8%. Moreover, the report finds that absent federal action, spending on clean energy will decline by 75%, from $44.3 billion in 2009 to $11.0 billion in 2014.\n\nFrom civilian nuclear power to hydro, wind, solar, and shale gas, the United States federal government has played a central role in the development of new energy industries.\n\nAmerica's nuclear power industry, which currently supplies about 20% of the country's electricity, has its origins in the Manhattan Project to develop atomic weapons during World War II. From 1942 to 1945, the United States invested $20 billion (2003 dollars) into a massive nuclear research and deployment initiative. But the achievement of the first nuclear weapon test in 1945 marked the beginning, not the end, of federal involvement in nuclear technologies. President Dwight D. Eisenhower's “Atoms for Peace” address in 1953 and the 1954 Atomic Energy Act committed the United States to develop peaceful uses for nuclear technology, including commercial energy generation. The new National Laboratory system, established by the Manhattan Project, was maintained and expanded, and the government poured money into nuclear energy research and development. Recognizing that research was not sufficient to spur the development of a nascent, capital-intensive industry, the federal government created financial incentives to spur the deployment of nuclear energy. For example, the 1957 Price Anderson Act limited the liability of nuclear energy firms in case of serious accident and helped firms secure capital with federal loan guarantees. In the favorable environment created by such incentives, more than 100 nuclear plants were built in the United States by 1973.\n\nCommercial wind power, today one of the fastest growing energy sectors, was also enabled through government support. In the 1980s, the federal government pursued two different R&D efforts for wind turbine development. The first was a “big science” effort by NASA and the Department of Energy (DOE) to use U.S. expertise in high-technology research and products to develop new large-scale wind turbines for electricity generation, largely from scratch. A second, more successful R&D effort, sponsored by the DOE, focused on component innovations for smaller turbines that used the operational experience of existing turbines to inform future research agendas. Joint research projects between the government and private firms produced a number of innovations that helped increase the efficiency of wind turbines, including twisted blades and special-purpose airfoils. Publicly funded R&D was coupled with efforts to build a domestic market for new turbines. At the federal level, this included tax credits and the passage of the Public Utilities Regulatory Policy Act (PURPA), which required that utilities purchase power from some small renewable energy generators at avoided cost. Both federal and state support for wind turbine development helped drive costs down considerably, but policy incentives at both the federal and state level were discontinued at the end of the decade. However, after a nearly five-year federal policy hiatus in the late 1980s, the U.S. government enacted new policies to support the industry in the early 1990s. The National Renewable Energy Laboratory (NREL) continued its support for wind turbine R&D, and also launched the Advanced Wind Turbine Program (AWTP). The goal of the AWTP was to reduce the cost of wind power to rates that would be competitive in the U.S. market. Policymakers also introduced new mechanisms to spur the demand of new wind turbines and boost the domestic market, including a 1.5 cents per kilowatt-hour tax credit (adjusted over time for inflation) included in the 1992 Energy Policy Act. Today the wind industry's main subsidy support comes from the federal production tax credit.\n\nThe development of commercial solar power was also dependent on government support. Solar PV technology was born in the United States, when Daryl Chapin, Calvin Fuller, and Gerald Pearson at Bell Labs first demonstrated the silicon solar photovoltaic cell in 1954. The first cells recorded efficiencies of four percent, far lower than the 25 percent efficiencies typical of some silicon crystalline cells today. With the cost out of reach for most applications, developers of the new technology had to look elsewhere for an early market. As it turned out, solar PV did make economic sense in one market segment: aerospace. The United States Army and Air Force viewed the technology as an ideal power source for a top-secret project on earth-orbiting satellites. The government contracted with Hoffman Electronics to provide solar cells for its new space exploration program. The first commercial satellite, the Vanguard I, launched in 1958, was equipped with both silicon solar cells and chemical batteries. By 1965, NASA was using almost a million solar PV cells. Strong government demand and early research support for solar cells paid off in the form of dramatic declines in the cost of the technology and improvements in its performance. From 1956 to 1973, the price of PV cells declined from $300 to $20 per watt. Beginning in the 1970s, as costs were declining, manufacturers began producing solar PV cells for terrestrial applications. Solar PV found a new niche in areas distant from power lines where electricity was needed, such as oil rigs and Coast Guard lighthouses. The government continued to support the industry through the 1970s and early 1980s with new R&D efforts under Presidents Richard Nixon and Gerald Ford, both Republicans, and President Jimmy Carter, a Democrat. As a direct result of government involvement in solar PV development, 13 of the 14 top innovations in PV over the past three decades were developed with the help of federal dollars, nine of which were fully funded by the public sector.\n\nMore recently than nuclear, wind, or solar, the development of the shale gas industry and subsequent boom in shale gas development in the United States was enabled through government support. The history of shale gas fracking in the United States was punctuated by the successive developments of massive hydraulic fracturing (MHF), microseismic imaging, horizontal drilling, and other key innovations that when combined made the once unreachable energy resource technically recoverable. Along each stage of the innovation pipeline – from basic research to applied R&D to cost-sharing on demonstration projects to tax policy support for deployment – public-private partnerships and federal investments helped push hydraulic fracturing in shale into full commercial competitiveness. Through a combination of federally funded geologic research beginning in the 1970s, public-private collaboration on demonstration project and R&D priorities, and tax policy support for unconventional technologies, the federal government played a key role in the development of shale gas in the United States.\n\nInvestigations have uncovered the crucial role of the government in the development of other energy technologies and industries, including aviation and jet engines, synthetic fuels, advanced natural gas turbines, and advanced diesel internal combustion engines.\n\nIn Venezuela, energy subsidies were equivalent to about 8.9 percent of the country's GDP in 2012. Fuel subsidies were 7.1 percent while electricity subsidies were 1.8 percent. In order to fund this the government used about 85 percent of its tax revenue on these subsidies. It is estimated the subsidies have caused Venezuela to consume 20 percent more energy than without them. The fuel subsidies are given more heavily to the richest part of the population who are consuming the most energy. The fuel subsidies maintained a cost of about $0.01 US for a liter of gasoline at the pump since 1996 until president Nicolas Maduro reduced the national subsidy in 2016 to make it roughly $0.60 US per liter (The local currency is Bolivar and the price per liter of gas is 6 Bolivars). Fuel consumption has increased overall since the 1996 policy began even though the production of oil has fallen more than 350,000 barrels a day since 2008 under that policy. PDVSA, the Venezuelan state oil company, has been losing money on these domestic transactions since the enactment of these policies. These losses can also be attributed to the 2005 Petrocaribe agreement, under which Venezuela sells many surrounding countries petroleum at a reduced or preferable price; essentially a subsidy by Venezuela for countries that are a part of the agreement. The subsidizing of fossil fuels and consequent low cost of fuel at the pump has caused the creation of a large black market. Criminal groups smuggle fuel out of Venezuela to adjacent nations (mainly Colombia). This is due to the large profits that can be gained by this act, as fuel is much more expensive in Colombia than in Venezuela. Despite the fact that this issue is already well known in Venezuela, and insecurity in the region continues to rise, the state has not yet lowered or eliminated these fossil fuel subsidies.\n\nRussia is one of the world’s energy powerhouses. It holds the world’s largest natural gas reserves (27% of total), the second-largest coal reserves, and the eighth-largest oil reserves. Russia is the world's third-largest energy subsidizer as of 2015. The country subsidizes electricity and natural gas as well as oil extraction. Approximately 60% of the subsidies go to natural gas, with the remainder spent on electricity (including under-pricing of gas delivered to power stations). For oil extraction the government gives tax exemptions and duty reductions amounting to about 22 billion dollars a year. Some of the tax exemptions and duty reductions also apply to natural gas extraction, though the majority is allocated for oil. In 2013 Russia offered the first subsidies to renewable power generators. The large subsidies of Russia are costly and it is recommended in order to help the economy that Russia lowers its domestic subsidies. However, the potential elimination of energy subsidies in Russia carries the risk of social unrest that makes Russian authorities reluctant to remove them.\n\nIn February 2011 and January 2012 the UK Energy Fair group, supported by other organisations and environmentalists, lodged formal complaints with the European Union's Directorate General for Competition, alleging that the Government was providing unlawful state aid in the form of subsidies for nuclear power industry, in breach of European Union competition law.\n\nOne of the largest subsidies is the cap on liabilities for nuclear accidents which the nuclear power industry has negotiated with governments. “Like car drivers, the operators of nuclear plants should be properly insured,” said Gerry Wolff, coordinator of the Energy Fair group. The group calculates that, \"if nuclear operators were fully insured against the cost of nuclear disasters like those at Chernobyl and Fukushima, the price of nuclear electricity would rise by at least €0.14 per kWh and perhaps as much as €2.36, depending on assumptions made\". According to the most recent statistics, subsidies for fossil fuels in Europe are exclusively allocated to coal (€10 billion) and natural gas (€6 billion). Oil products do not receive any subsidies.\n\n\n"}
{"id": "15422907", "url": "https://en.wikipedia.org/wiki?curid=15422907", "title": "European Pollen Database", "text": "European Pollen Database\n\nThe European Pollen Database (EPD) is a freely available database of pollen frequencies, past and present, in the larger European area.\n\nThe database is hosted by the \"Institut Méditerranéen d'Ecologie et de la Biodiversité (IMBE)\".\n\n\n"}
{"id": "3052112", "url": "https://en.wikipedia.org/wiki?curid=3052112", "title": "Fergusonite", "text": "Fergusonite\n\nFergusonite is a mineral comprising a complex oxide of various rare-earth elements. The chemical formula of fergusonite species is (Y,REE)NbO, where REE = rare-earth elements in solid solution with Y. Yttrium is usually dominant (the species fergusonite-(Y)), but sometimes Ce or Nd may predominate in molar proportion (species fergusonite-(Ce) and fergusonite-(Nd)). All the other rare-earth elements are present in subordinate amount, and tantalum substitutes for some of the niobium. There are Fergusonite-beta-(Nd), Fergusonite-beta-(Y), Fergusonite-beta-(Ce) too, but they are classified as 4.DG.10 in the Nickel–Strunz system. The mineral has tetragonal crystal symmetry and the same structure as scheelite (calcium tungstate, CaWO), but can be metamict (amorphous) due to radiation damage from its small content of thorium. It is found as needle-like or prismatic crystals in pegmatite. It was named after British politician and mineral collector Robert Ferguson of Raith (1767–1840).\n\n"}
{"id": "23428417", "url": "https://en.wikipedia.org/wiki?curid=23428417", "title": "GPoT Center", "text": "GPoT Center\n\nGPoT Center (Turkish: \"Küresel Siyasal Eğilimler Merkezi\") officially the Global Political Trends Center is a research unit, which was established by Mensur Akgun and Sylvia Tiryaki at the Istanbul Kültür University in Turkey in 2008. The mission of the Center is to conduct research, projects, to produce innovative publications analyzing the latest issues in the international relations, to formulate viable political recommendations and to boost the dialogue between civil society, academia and media.\n\nGPoT Center, as one of the leading Turkish think tanks, runs projects in various areas and has organized numerous conferences, workshops and round-table meetings focused on: the EU - Turkey relations, the Cyprus question, the Turkey - Armenia relations, Turkish - Arab dialogue, NATO, second track diplomacy between Syria and Israel, Arab-Israeli conflict and other issues related to the international agenda in general and the agenda of Turkish foreign policy in particular.\n\nThe projects implemented by GPoT Center often involve participation of high-level politicians and opinion makers, however the Center is also active in organization of second-track diplomacy meetings aiming at effective conflict solution. For the purposes of second-track diplomacy, GPoT Center brings together representatives of conflicting parties, i.e. civil society activists, opinion leaders, as well as academics to discuss issues pertaining to solution of the relevant problem. The meetings are usually conducted in friendly atmosphere and under Chatam House rules.\n\nThe Center publishes various types of publications available in the electronic as well as printed form. It has published a number of policy briefs analyzing issues in international relations and shorter essays, i.e. GPoT Briefs focusing on one topic from the current global issues. GPoT Center published its first two books on Cyprus issue in the summer 2009 and has since then published other books on developments in Cyprus, Israeli-Turkish relations, as well as democratization of the Middle East.\n\nThe Center cooperates with institutions from Sweden - SIIA, Lebanon - CAUS, Qatar - ADF, United Kingdom - LSE, Czech Republic - EUROPEUM, Slovak Republic - SFPA, Armenia - EPF, YPC and Internews, Egypt - Ibn-Khaldun Center, and France - ENS. GPoT Center is also part of larger research and academic networks such as the United Nations's Alliance of Civilizations, International Research and Security Network, United Nations' Committee on the Exercise of the Inalienable Rights of the Palestinian People and Anna Lindh Euro-Mediterranean Foundation for the Dialogue Between Cultures.\n\n"}
{"id": "1327978", "url": "https://en.wikipedia.org/wiki?curid=1327978", "title": "Garden of Ridván, Baghdad", "text": "Garden of Ridván, Baghdad\n\nThe Garden of Ridván (literally \"garden of paradise\") or Najibiyyih Garden was a wooded garden in what is now Baghdad's Rusafa District, on the banks of the Tigris river. It is notable as the location where Bahá'u'lláh, founder of the Bahá'í Faith, stayed for twelve days from April 21 to May 2, 1863, after the Ottoman Empire exiled him from Baghdad and before commencing his journey to Constantinople. During his stay in this garden, Bahá'u'lláh announced to his followers that he was the messianic figure of He whom God shall make manifest, whose coming had been foretold by the Báb. These events are celebrated annually during the Festival of Ridván.\n\nThe garden was located in a large agricultural area immediately north of the walls of the city of Baghdad, about from the city's northern Mu'azzam gate. Located on the eastern bank of the Tigris River in what is now the Bab al-Mu'azzam neighbourhood of Baghdad's Rusafa District, it was directly opposite the district in which Bahá'u'lláh lived during his stay in the city, on the river's western bank.\n\nA ground plan drawn in the 1850s by officers of the Indian Navy \"(pictured)\" shows the garden immediately adjacent to the city's citadel, with four avenues meeting at a circular area in the centre. A structure, possibly the garden palace, is located at the edge of the garden near the riverbank. The garden was described as a wooded garden having four \"flower-bordered avenues\" lined with roses, which were collected by gardeners during Bahá'u'lláh's stay and piled in the center of his tent to be offered to visitors. \"So great would be the heap,\" the chronicler Nabíl-i-A`zam relates, \"that when His companions gathered to drink their morning tea in His presence, they would be unable to see each other across it.\" Nightingales were said to sing loudly in the garden, which, together with the fragrance of the roses, \"created an atmosphere of beauty and enchantment\". By the side of the river, upstream from Najib Pasha's palace, was an open space in the garden where one of Bahá'u'lláh's companions raised a tent for him, around which a small village of tents was later raised for the rest of his family.\n\nIn travelling to Constantinople, Bahá'u'lláh's caravan would take a road that would bring them by the garden, thus it was a logical choice for them stop there in order to assemble and to receive visitors. Access to the garden from the opposite riverbank was possible by way of a ferry across the Tigris, as in Bahá'u'lláh's case, or by \"floating bridge\", as in the case of the governor and other friends who followed.\n\nThe Najibiyyih Garden, as it was first known, was named for Muhammad Najib Pasha, the \"wāli\" (governor) of Baghdad from 1842 to 1847, who built the garden and an attached palace in what was originally an agricultural area outside the city. Although Najib Pasha died in May 1851, the garden was presumably in the hands of his heirs when it was used by Bahá'u'lláh, during the period of April–May 1863.\n\nDespite its importance to the Bahá'í community, the garden was never owned by the Bahá'ís. It was purchased by the government in 1870, and was used as a guest house for Nasruddin-Shah—who was responsible for Bahá'u'lláh's imprisonment and exile—when he visited Iraq in 1870. The park was further developed during the governorship of Midhat Pasha (1869–1872), who leveled the road leading to the garden and built another road, approximately 400–500 meters in length. The garden was cleared during the early twentieth century, to make way for the Royal Hospital. Baghdad Medical City, a large complex of teaching hospitals, now stands in its place.\n\nBahá'u'lláh, after being imprisoned in Persia for his involvement with the Bábí community, was exiled to Baghdad by Nasruddin-Shah, arriving in the spring of 1853. Over the next decade in Baghdad, his influence grew to the point where the Persian government feared he might use it to threaten their sovereignty from abroad. In response, the Persian ambassador in Constantinople demanded Bahá'u'lláh be banished from Baghdad, to which the Ottoman government eventually acceded.\n\nBahá'u'lláh entered the Najibiyyih Garden on April 22, 1863, in order to receive visitors and allow his family to prepare for his upcoming trip to Constantinople. He crossed the Tigris in a small boat accompanied by his sons `Abdu'l-Bahá, Mírzá Mihdí and Mírzá Muhammad `Alí, his secretary Mirza Aqa Jan and some others. After their arrival in the garden, Bahá'u'lláh announced his mission and station for the first time to a small group of family and friends. For the next eleven days Bahá'u'lláh received visitors including the governor of Baghdad. Bahá'u'lláh's family was not able to join him until April 30, the ninth day, since the river had risen and made travel to the garden difficult. On the twelfth day of their stay in the garden, Bahá'u'lláh and his family left the garden and started on their travel to Constantinople.\n\nIt was Bahá'u'lláh who gave the garden the name of \"Ridván\" (\"paradise\") during his stay, and the name was thereafter applied to the twelve-day Festival of Ridván—known as the \"King of Festivals\"—celebrated annually by Bahá'ís between 21 April and 2 May. Certain days of this festival are tied to major events that took place during the period of Bahá'u'lláh's stay in the garden: the first day celebrates his arrival in the garden; the ninth day, the arrival of his family; and the twelfth day, his caravan's departure towards Constantinople. These three days are major Bahá'í holy days, on which work must be suspended.\n\n\n\n"}
{"id": "63461", "url": "https://en.wikipedia.org/wiki?curid=63461", "title": "Goldberry", "text": "Goldberry\n\nGoldberry is a supporting character from J. R. R. Tolkien's \"The Lord of the Rings\". Also known as the \"River-woman's daughter\", she is the wife of Tom Bombadil. Goldberry is described as a beautiful and (seemingly) young woman with golden hair.\n\nGoldberry first appeared in Tolkien's writings in his 1934 poem, 'The Adventures of Tom Bombadil', [re-worked in \"The Adventures of Tom Bombadil\" (1962)]. The poem tells of how she drags Tom into the river, before he escapes, returning later to capture her and make her his bride.\n\nIn \"The Fellowship of the Ring\", the first book of \"The Lord of the Rings\", Frodo Baggins and his companions Sam, Merry, and Pippin encounter Goldberry and Tom in the Old Forest near Buckland. The couple gives them shelter in their cottage after the Hobbits are rescued from Old Man Willow. Their stay is brief but puzzling, for Tom and Goldberry are clearly more than they seem.\n\nAlthough Goldberry's origins are uncertain, Bombadil clearly identifies her as having been found by him in the river and her title \"River-woman's daughter\" strongly suggests that she is not a mortal human being, but rather a spirit of the river Withywindle in the Old Forest of Tolkien's Middle-earth. This is similar to the many named river spirits of traditional English folklore such as Jenny Greenteeth or Peg Powler of the River Tees, (though Goldberry is a noticeably gentler figure), or to the naiads of the Greeks. Otherwise, she and Bombadil are enigmas in Tolkien's Middle-earth legendarium, not fitting easily into any of his definitions of sentient beings in his imagined world.\n\nOne frequently proposed explanation is that she is a (minor) Maia associated with the element of water and in some way with the river Withywindle in particular, though that is by no means the only possible answer. John D. Rateliff suggested that, at least in terms of Tolkien's early mythology, she should be seen as one of the wide category of fays, spirits, and elementals (including the Maia): \"Thus Melian is a 'fay', (as, in all probability, are Goldberry and Bombadil; the one a nymph, the other a \"genius loci\").\n\nGoldberry and Tom are notably absent from most media adaptations of \"The Lord of the Rings\". Filmmakers Ralph Bakshi and Peter Jackson stated that the reason the characters were omitted from their films was because, in their view, he (Bombadil) does little to advance the story, and would make their films unnecessarily long.\n\nGoldberry appears in the massively multiplayer online role-playing game \"\". She can be found in \"Goldberry's Glade\" in the Old Forest. Her race is referred to as \"River-maid\". The game also features another member of this race, Goldberry's sister Naruhel, known as the Red Maid, who is of a darker and crueler nature. This is an original character not featured in Tolkien's writings.\n\n\n\n"}
{"id": "48962287", "url": "https://en.wikipedia.org/wiki?curid=48962287", "title": "Gäu", "text": "Gäu\n\nIn the south German language (of the Alemannic-speaking area, or in Switzerland), a gäu landscape (\"gäulandschaft\") refers to an area of open, level countryside. These regions typically have fertile soils resulting from depositions of loess (an exception is the \"Arme Gäue\" [\"Poor Gäus\"] of the Baden-Württemberg Gäu).\n\nThe intensive use of the \"Gäu\" regions for crops has displaced the originally wooded countryside (→\"climax vegetation\" – in contrast with the steppe heath theory and disputed megaherbivore hypothesis). The North German equivalent of such landscapes is \"börde.\"\n\n"}
{"id": "8109926", "url": "https://en.wikipedia.org/wiki?curid=8109926", "title": "Hanson Formation", "text": "Hanson Formation\n\nThe Hanson Formation is a geologic formation on Mount Kirkpatrick, Antarctica. It is one of only two major dinosaur-bearing rock formations found on the continent of Antarctica to date; the other is the Santa Marta Formation from the Late Cretaceous. The formation has yielded only a handful of Mesozoic specimens so far and most of it is as yet unexcavated. Part of the Victoria Group of the Transantarctic Mountains, it is below the Prebble Formation and above the Falla Formation.\n\nThe first dinosaur to be discovered from the Hanson Formation was the predator \"Cryolophosaurus\" in 1991, which was then formally described in 1994. Alongside these dinosaur remains were fossilized trees, suggesting that plant matter had once grown on Antarctica's surface before it drifted southward. Other finds from the formation include tritylodonts, herbivorous mammal-like reptiles and crow-sized pterosaurs. Surprisingly were the discovery of prosauropod remains, which were found commonly on other continents only until the Early Jurassic. However, the bone fragments found at the Hanson Formation were dated until the Middle Jurassic, millions of years later. In 2004, paleontologists discovered partial remains of a large sauropod dinosaur that has not formally been described yet.\n\nThe following dinosaur fossils have been found in the formation:\n"}
{"id": "36971117", "url": "https://en.wikipedia.org/wiki?curid=36971117", "title": "Hyperloop", "text": "Hyperloop\n\nA Hyperloop is a proposed mode of passenger and/or freight transportation, first used to describe an open-source vactrain design released by a joint team from Tesla and SpaceX. Drawing heavily from Robert Goddard's vactrain, a hyperloop is a sealed tube or system of tubes through which a pod may travel free of air resistance or friction conveying people or objects at high speed while being very efficient.\n\nElon Musk's version of the concept, first publicly mentioned in 2012, incorporates reduced-pressure tubes in which pressurized capsules ride on air bearings driven by linear induction motors and axial compressors.\n\nThe Hyperloop Alpha concept was first published in August 2013, proposing and examining a route running from the Los Angeles region to the San Francisco Bay Area, roughly following the Interstate 5 corridor. The paper conceived of a hyperloop system that would propel passengers along the route at a speed of , allowing for a travel time of 35 minutes, which is considerably faster than current rail or air travel times. Preliminary cost estimates for this LA–SF suggested route were included in the white paper— for a passenger-only version, and for a somewhat larger-diameter version transporting passengers and vehicles—although transportation analysts had doubts that the system could be constructed on that budget; some analysts claimed that the Hyperloop would be several billion dollars overbudget, taking into consideration construction, development, and operation costs.\n\nThe Hyperloop concept has been explicitly \"open-sourced\" by Musk and SpaceX, and others have been encouraged to take the ideas and further develop them.\n\nTo that end, a few companies have been formed, and several interdisciplinary student-led teams are working to advance the technology. SpaceX built an approximately subscale track for its pod design competition at its headquarters in Hawthorne, California.\n\nSome experts are skeptical, saying that the proposals ignore the expenses and risks of developing the technology and that the idea is \"completely impractical\". Claims have also been made that the Hyperloop is too susceptible to disruption from a power outage or terror attacks to be considered safe.\n\nThe general idea of trains or other transportation traveling through evacuated tubes dates back more than a century, although the atmospheric railway was never a commercial success. \n\nMusk first mentioned that he was thinking about a concept for a \"fifth mode of transport\", calling it the \"Hyperloop\", in July 2012 at a PandoDaily event in Santa Monica, California. This hypothetical high-speed mode of transportation would have the following characteristics: immunity to weather, collision free, twice the speed of a plane, low power consumption, and energy storage for 24-hour operations. The name \"Hyperloop\" was chosen because it would go in a loop. Musk envisions the more advanced versions will be able to go at hypersonic speed. In May 2013, Musk likened the Hyperloop to a \"cross between a Concorde and a railgun and an air hockey table\".\n\nFrom late 2012 until August 2013, a group of engineers from both Tesla and SpaceX worked on the conceptual modeling of Hyperloop. An early system design was published in the Tesla and SpaceX blogs which describes one potential design, function, pathway, and cost of a hyperloop system. According to the alpha design, pods would accelerate to cruising speed gradually using a linear electric motor and glide above their track on air bearings through tubes above ground on columns or below ground in tunnels to avoid the dangers of grade crossings. An ideal hyperloop system will be more energy-efficient, quiet, and autonomous than existing modes of mass transit. Musk has also invited feedback to \"see if the people can find ways to improve it\". The Hyperloop Alpha was released as an open source design. The word mark \"HYPERLOOP\", applicable to \"high-speed transportation of goods in tubes\" was issued to SpaceX on April 4, 2017.\n\nIn June 2015, SpaceX announced that it would build a test track to be located next to SpaceX's Hawthorne facility. The track would be used to test pod designs supplied by third parties in the competition.\n\nBy November 2015, with several commercial companies and dozens of student teams pursuing the development of Hyperloop technologies, the \"Wall Street Journal\" asserted that \"The Hyperloop Movement\", as some of its unaffiliated members refer to themselves, is officially bigger than the man who started it.\"\n\nThe MIT Hyperloop team developed the first Hyperloop pod prototype, which they unveiled at the MIT Museum on May 13, 2016. Their design uses electrodynamic suspension for levitating and eddy current braking.\n\nOn January 29, 2017, approximately one year after phase one of the Hyperloop pod competition, the MIT Hyperloop pod demonstrated the first ever low-pressure Hyperloop run in the world. Within this first competition the Delft University team from the Netherlands achieved the highest overall competition score. The awards for the \"fastest pod\" and the \"best performance in flight\" were won by the team WARR Hyperloop from the Technical University of Munich (TUM), Germany. The team from the Massachusetts Institute of Technology (MIT) placed third overall in the competition, judged by SpaceX engineers.\n\nThe second Hyperloop pod competition took place from August 25–27, 2017. The only judging criteria being top speed provided it is followed by successful deceleration. WARR Hyperloop from the Technical University of Munich won the competition by reaching a top speed of 324 km/h (201 mph) and therefore breaking the previous record of 310 km/h for hyperloop prototypes set by Hyperloop One.\n\nDevelopments in high-speed rail have historically been impeded by the difficulties in managing friction and air resistance, both of which become substantial when vehicles approach high speeds. The vactrain concept theoretically eliminates these obstacles by employing magnetically levitating trains in evacuated (airless) or partly evacuated tubes, allowing for speeds of thousands of miles per hour. However, the high cost of maglev and the difficulty of maintaining a vacuum over large distances has prevented this type of system from ever being built. The Hyperloop resembles a vactrain system but operates at approximately of pressure.\n\nThe Hyperloop concept operates by sending specially designed \"capsules\" or \"pods\" through a steel tube maintained at a partial vacuum. In Musk's original concept, each capsule floats on a layer of air provided under pressure to air-caster \"skis\", similar to how pucks are levitated above an air hockey table, while still allowing faster speeds than wheels can sustain. Hyperloop One's technology uses passive maglev for the same purpose. Linear induction motors located along the tube would accelerate and decelerate the capsule to the appropriate speed for each section of the tube route. With rolling resistance eliminated and air resistance greatly reduced, the capsules can glide for the bulk of the journey. In Musk's original Hyperloop concept, an electrically driven inlet fan and axial compressor would be placed at the nose of the capsule to \"actively transfer high-pressure air from the front to the rear of the vessel\", resolving the problem of air pressure building in front of the vehicle, slowing it down. A fraction of the air is shunted to the skis for additional pressure, augmenting that gain passively from lift due to their shape. Hyperloop One's system does away with the compressor.\n\nIn the alpha-level concept, passenger-only pods are to be in diameter and projected to reach a top speed of to maintain aerodynamic efficiency. The design proposes passengers experience a maximum inertial acceleration of 0.5 g, about 2 or 3 times that of a commercial airliner on takeoff and landing.\n\nA number of routes have been proposed for Hyperloop systems that meet the approximate distance conditions for which a Hyperloop is hypothesized to provide improved transport times. Route \"proposals\" range from speculation described in company releases to business cases to signed agreements.\n\nThe route suggested in the 2013 alpha-level design document was from the Greater Los Angeles Area to the San Francisco Bay Area. That conceptual system would begin around Sylmar, just south of the Tejon Pass, follow Interstate 5 to the north, and arrive near Hayward on the east side of San Francisco Bay. Several proposed branches were also shown in the design document, including Sacramento, Anaheim, San Diego, and Las Vegas.\n\nNo work has been done on the route proposed in Musk's alpha-design; one cited reason is that it would terminate on the fringes of the two major metropolitan areas (Los Angeles and San Francisco), resulting in significant cost savings in construction, but requiring that passengers traveling to and from Downtown Los Angeles and San Francisco, and any other community beyond Sylmar and Hayward, to transfer to another transportation mode in order to reach their final destination. This would significantly lengthen the total travel time to those destinations.\n\nA similar problem already affects present-day air travel, where on short routes (like LAX-SFO) the flight time is only a rather small part of door to door travel time. Critics have argued that this would significantly reduce the proposed cost and/or time savings of Hyperloop as compared to the California High-Speed Rail project that will serve downtown stations in both San Francisco and Los Angeles. Passengers travelling financial centre to financial centre are estimated to save about two hours by taking the Hyperloop instead of driving the whole distance.\n\nOthers questioned the cost projections for the suggested California route. Some transportation engineers argued in 2013 that they found the alpha-level design cost estimates unrealistically low given the scale of construction and reliance on unproven technology. The technological and economic feasibility of the idea is unproven and a subject of significant debate.\n\nIn November, 2017, Arrivo announced a plan for a maglev automobile transport system from Aurora, Colorado to Denver International Airport, the first leg of a system from downtown Denver. Its contract describes completion of the first leg in 2021. In February 2018, Hyperloop Transportation Technologies announced a similar plan for a loop connecting Chicago and Cleveland and a loop connecting Washington and New York City.\n\nHyperloop Transportation Technologies are in process to sign a Letter of Intent with the Indian Government for a proposed route between Chennai and Bengaluru. If things go as planned, the distance of 345 km could be covered in 30 minutes. HTT also signed an agreement with Andhra Pradesh government to build India's first Hyperloop project connecting Amaravathi to Vijayawada in a 6-minute ride.\nOn February 22, 2018, Hyperloop One has entered into a MOU (Memorandum of Understanding) with the Government of Maharashtra to build a hyperloop transportation system between Mumbai and Pune that would cut the travel time from the current 180 minutes to just 20 minutes.\n\nIndore-based Dinclix GroundWorks' DGWHyperloop advocates a Hyperloop corridor between Mumbai and Delhi, via Indore, Kota, and Jaipur.\n\nMany of the active Hyperloop routes being planned currently are outside of the U.S. Hyperloop One published the world's first detailed business case for a route between Helsinki and Stockholm, which would tunnel under the Baltic Sea to connect the two capitals in under 30 minutes. Hyperloop One is also well underway on a feasibility study with DP World to move containers from its Port of Jebel Ali in Dubai. Hyperloop One on November 8, 2016, announced a new feasibility study with Dubai's Roads and Transport Authority for passenger and freight routes connecting Dubai with the greater United Arab Emirates. Hyperloop One is also working on passenger routes in Moscow and a cargo Hyperloop to connect Hunchun in north-eastern China to the Port of Zarubino, near Vladivostok and the North Korean border on Russia's Far East. In May 2016, Hyperloop One kicked off their Global Challenge with a call for comprehensive proposals of hyperloop networks around the world. In September 2017, Hyperloop One selected 10 routes from 35 of the strongest proposals: Toronto–Montreal, Cheyenne–Denver–Pueblo, Miami–Orlando, Dallas–Laredo–Houston, Chicago–Columbus–Pittsburgh, Mexico City–Guadalajara, Edinburgh–London, Glasgow–Liverpool, Bengaluru–Chennai, and Mumbai–Chennai.\n\nOthers have put forward European routes, including a Paris to Amsterdam route proposed by Delft Hyperloop. A Warsaw University of Technology team is evaluating potential routes from Cracow to Gdańsk across Poland proposed by Hyper Poland.\n\nTransPod is exploring the possibility of Hyperloop routes which would connect Toronto and Montreal, Toronto to Windsor, and Calgary to Edmonton. Toronto and Montreal, the largest cities in Canada, are currently connected by Ontario Highway 401, the busiest highway in North America.\n\nHyperloop Transportation Technologies (HTT) reportedly signed an agreement with the government of Slovakia in March 2016 to perform impact studies, with potential links between Bratislava, Vienna, and Budapest, but there have been no developments on that since. In January 2017, HTT signed an agreement to explore the route Bratislava—Brno—Prague in Central Europe.\n\nIn 2017, SINTEF, the largest independent research organization in Scandinavia, announced they are considering building a test lab for Hyperloop in Norway.\n\nIn September 2013, Ansys Corporation ran computational fluid dynamics simulations to model the aerodynamics of the capsule and shear stress forces that the capsule would be subjected to. The simulation showed that the capsule design would need to be significantly reshaped to avoid creating supersonic airflow, and that the gap between the tube wall and capsule would need to be larger. Ansys employee Sandeep Sovani said the simulation showed that Hyperloop has challenges but that he is convinced it is feasible.\n\nIn October 2013, the development team of the OpenMDAO software framework released an unfinished, conceptual open-source model of parts of the Hyperloop's propulsion system. The team asserted that the model demonstrated the concept's feasibility, although the tube would need to be in diameter, significantly larger than originally projected. However, the team's model is not a true working model of the propulsion system, as it did not account for a wide range of technological factors required to physically construct a Hyperloop based on Musk's concept, and in particular had no significant estimations of component weight.\n\nIn November 2013, MathWorks analyzed the proposal's suggested route and concluded that the route was mainly feasible. The analysis focused on the acceleration experienced by passengers and the necessary deviations from public roads in order to keep the accelerations reasonable; it did highlight that maintaining a trajectory along I-580 east of San Francisco at the planned speeds was not possible without significant deviation into heavily populated areas.\n\nIn January 2015, a paper based on the NASA OpenMDAO open-source model reiterated the need for a larger diameter tube and a reduced cruise speed closer to Mach 0.85. It recommended removing on-board heat exchangers based on thermal models of the interactions between the compressor cycle, tube, and ambient environment. The compression cycle would only contribute 5% of the heat added to the tube, with 95% of the heat attributed to radiation and convection into the tube. The weight and volume penalty of on-board heat exchangers would not be worth the minor benefit, and regardless the steady-state temperature in the tube would only reach above ambient temperature.\n\nAccording to Musk, various aspects of the Hyperloop have technology applications to other Musk interests, including surface transportation on Mars and electric jet propulsion.\n\nResearchers associated with MIT's department of Aeronautics and Astronautics published research in June 2017 that verified the challenge of aerodynamic design near the Kantrowitz limit that had been theorized in the original SpaceX Alpha-design concept released in 2013.\n\nIn 2017, Dr. Richard Geddes and others formed the Hyperloop Advanced Research Partnership to act as a clearinghouse of Hyperloop public domain reports and data.\n\nAccording to Musk, Hyperloop would be useful on Mars as no tubes would be needed because Mars' atmosphere is about 1% the density of the Earth's at sea level. For the Hyperloop concept to work on Earth, low-pressure tubes are required to reduce air resistance. However, if they were to be built on Mars, the lower air resistance would allow a Hyperloop to be created with no tube, only a track.\n\nVirgin Hyperloop One (formerly Hyperloop One, and before that, Hyperloop Technologies) was incorporated in 2014 and has built a team of 280+, including engineers, technicians, welders, and machinists. It has raised more than in capital from investors including DP World, Sherpa Capital, Formation 8, 137 Ventures, Caspian Venture Capital, Fast Digital, GE Ventures, and SNCF.\n\nHyperloop One was founded by Shervin Pishevar and Brogan BamBrogan. BamBrogan left the company in July 2016, along with three of the other founding members of Arrivo. Hyperloop One then selected Josh Giegel, a former SpaceX engineer, to be a co-founder.\n\nHyperloop One has a 75,000-square foot Innovation Campus in downtown LA and a 100,000-square foot machine and tooling shop in North Las Vegas. By 2017, it had completed a 500m Development Loop (DevLoop) in North Las Vegas, Nevada.\n\nOn May 11, 2016, Hyperloop One conducted the first live trial of Hyperloop technology, demonstrating that its custom linear electric motor could propel a sled from 0 to 110 miles an hour in just over one second. The acceleration exerted approximately 2.5 g on the sled. The sled was stopped at the end of the test by hitting a pile of sand at the end of the track, because the test was not intended to test braking components.\n\nIn July 2016, Hyperloop One released a preliminary study that suggested a Hyperloop connection between Helsinki and Stockholm would be feasible, reducing the travel time between the cities to half an hour. The construction costs were estimated by Hyperloop One to be around ( at 2016 exchange rates).\n\nIn August 2016, Hyperloop One announced a deal with the world's third largest ports operator, DP World, to develop a cargo offloader system at DP World's flagship port of Jebel Ali in Dubai. Hyperloop One also broke ground on DevLoop, its full-scale Hyperloop test track.\n\nIn November 2016, Hyperloop One disclosed that it has established a high-level working group relationship with the governments of Finland and the Netherlands to study the viability of building Hyperloop proof of operations centers in those countries. Hyperloop One also has a feasibility study underway with Dubai's Roads and Transport Authority for passenger systems in the UAE. Other feasibility studies are underway in Russia, Los Angeles, and the Netherlands.\n\nIn May 12, 2017, Hyperloop One performed its first full-scale Hyperloop test, becoming the first company in the world to test a full-scale Hyperloop. The system-wide test integrated Hyperloop components including vacuum, propulsion, levitation, sled, control systems, tube, and structures.\n\nOn July 12, 2017, the company revealed images of its first generation pod prototype, which will be used at the DevLoop test site in Nevada to test aerodynamics.\n\nOn October 12, 2017, the company received a \"significant investment\" from the Virgin Group founder Richard Branson, leading to a rebrand of the name.\n\nIn February 2018, Richard Branson of Virgin Hyperloop One announced that he had a preliminary agreement with the Maharashtra State government to build the Mumbai-Pune Hyperloop.\n\nHyperloop Transportation Technologies (HTT) was a group of more than 800 engineers and professionals located around the world. Some collaborate part-time; others are full-time employees and contributors. Some members are full-time paid employees; others work in exchange for salary and stock options.\n\nHTT announced in May 2015 that a deal had been finalized with landowners to build a test track along a stretch of road near Interstate 5 between Los Angeles and San Francisco. In December 2016, Hyperloop Transportation Technologies and the government of Abu Dhabi announced plans to conduct a feasibility study on a Hyperloop link between the UAE capital and Al Ain, reducing travel time between Abu Dhabi and Al Ain to just under 10-minutes.\nIn September 2017, HTT announced and signed an agreement with the Andhra Pradesh government to build a track from Amaravathi to Vijayawada in a public-private partnership, and suggested that the more than one hour trip could be reduced to 5 minutes through the project.\nFor yet undisclosed reason, neither the test track that HTT announced in May 2015 nor any other test track has been built in the last 3 years.\n\nTransPod Inc. is a Canadian company designing and manufacturing ultra-high-speed tube transportation technology and vehicles. In November 2016 TransPod raised a $15 million USD seed round from Angelo Investments, an Italian high-tech holding group, specializing in advanced technologies for the railway, space, and aviation industries.\n\nIn September 2017, TransPod released a scientific peer-reviewed publication in the journal Procedia Engineering. The paper was premiered at the EASD EURODYN 2017 conference, and presents the physics of the TransPod system.\n\nTransPod vehicles are being designed to travel at over 1,000 km/h between cities using fully electric propulsion and zero need for fossil fuels. The TransPod tube system is distinct from the hyperloop concept proposed by Elon Musk's Hyperloop Alpha white paper. The TransPod system uses moving electromagnetic fields to propel the vehicles with stable levitation off the bottom surface, rather than compressed air. TransPod is stated to contain further developments beyond hyperloop. To achieve fossil-fuel-free propulsion, TransPod \"pods\" take advantage of electrically-driven linear induction motor technology, with active real-time control and sense-space systems. The cargo transport TransPod pods will be able to carry payloads of 10–15 tons and have compatibility with wooden pallets, as well as various unit load devices such as LD3 containers, and AAA containers.\n\nAt the InnoTrans Rail Show 2016 in Berlin, TransPod premiered their vehicle concept, alongside implementation of Coelux technology—an artificial skylight to emulate natural sunlight within the passenger pods.\n\nTransPod has partnered with investor Angelo Investments' member companies MERMEC, SITAEL, and Blackshape Aircraft. With international staff of over 1,000 employees, 650 of whom are engineers, they will collaborate with the development and testing of the TransPod tube system It has since expanded from its Toronto, Canada headquarters at MaRS Discovery District to open offices in Toulouse, France and Bari, Italy. TransPod is additionally partnered with university researchers, engineering firm IKOS, REC Architecture and Liebherr-Aerospace.\n\nTransPod is developing routes worldwide and in Canada such as Toronto-Montreal, Toronto-Windsor, and Calgary-Edmonton. TransPod is preparing to build a test track for the pod vehicles in Canada. This track will be extendable as part of a full route pending a combination of private and public funding to construct the line.\n\nIn July 2017, TransPod released an initial cost study which outlines the viability of building a hyperloop line in Southwestern Ontario between the cities of Windsor and Toronto. The study indicates a TransPod tube system would cost half the projected cost of a high-speed rail line along the same route, while operating at more than four times the top speed of high speed rail.\n\nTransPod has announced plans for a test track to be constructed in the town of Droux near Limoges in collaboration with the French department of Haute-Vienne. The proposed test track would exceed 3 km in length, and operate as a half-scale system 2 m in diameter. In February 2018 Vincent Leonie, vice president of Limoges Métropole and a deputy mayor of Limoges, has announced agreements for the \"Hyperloop Limoges\" organization have been signed to promote and accelerate the technology.\n\nEstablished in 2015, DGWHyperloop is a subsidiary of Dinclix GroundWorks, an engineering company based in Indore, India. DGWHyperloop's initial proposals include a Hyperloop-based corridor between Delhi and Mumbai called the Delhi Mumbai Hyperloop Corridor (DMHC). The company has partnered with many government agencies, private companies, and institutions for its research on Hyperloop. DGWHyperloop is the only Indian company working on implementing the Hyperloop system across the nation.\n\nArrivo is a technology architecture and engineering company founded in Los Angeles in 2016. With an early focus on \"making hyperloops cheap to use and profitable to operate\", Arrivo aims to shift the transportation industry into a mode of arrival. In November 2017, it disclosed a plan to build a link for automobiles to Denver International Airport using maglev train technology by 2021.\n\nHardt Global Mobility was founded in 2016 in Delft, emerging from the TU Delft Hyperloop team who won at the SpaceX Pod Competition.\n\nThe Dutch team is setting up a full-scale testing center for hyperloop technology in Delft. Hardt has received over €600,000 in funding for the initial rounds of testing, with plans to raise more to build a high-speed test line by 2019. At the unveiling of the test track, Dutch Minister of Infrastructure and Environment Schultz van Haegen said a Hyperloop system could help cement the Netherlands' position as a gateway to Europe by transporting freight arriving at Rotterdam's sprawling port.\n\nIn October 9, 2017 a report was released with information from Hardt Global Mobility and Hyperloop One. The report has been sent to the Dutch House of Representatives and judges the added value of a hyperloop test track facility. The report recommends building a test track of 5 km in Flevoland.\n\nHyper Chariot is a startup company based in Santa Monica, United States, that began promoting itself in June 2017. The company has an ambitious plan. On July 27, 2017 it announced a partnership with AML Superconductivity and Magnetics for the development of the vehicle and related propulsion system.\n\nZeleros was founded in Valencia (Spain) in November 2016 by Daniel Orient (CTO), David Pistoni (CEO) and Juan Vicén (CMO), former leaders of the Hyperloop UPV team from Universitat Politècnica de València. The team was awarded \"Top Design Concept\" and \"Propulsion/Compression Subsystem Technical Excellence\" by SpaceX at Hyperloop Design Weekend. After building Spain's first Hyperloop prototype with the support of Purdue University, and building a 12-meter research test-track in Spain at the University, the company was awarded in November 2017 the international Everis Foundation prize and wants to develop new technologies for a more efficient and sustainable transportation.\n\nA number of student and non-student teams were participating in a Hyperloop pod competition in 2015–16, and at least 22 of them built hardware to compete on a sponsored hyperloop test track in mid-2016.\n\nIn June 2015, SpaceX announced that they would sponsor a Hyperloop pod design competition, and would build a subscale test track near SpaceX's headquarters in Hawthorne, California for the competitive event in 2016. SpaceX stated in their announcement, \"Neither SpaceX nor Elon Musk is affiliated with any Hyperloop companies. While we are not developing a commercial Hyperloop ourselves, we are interested in helping to accelerate development of a functional Hyperloop prototype.\"\n\nMore than 700 teams had submitted preliminary applications by July, and detailed competition rules were released in August. \"Intent to Compete\" submissions were due in September 2015 with more detailed tube and technical specification released by SpaceX in October. A preliminary design briefing was held in November 2015, where more than 120 student engineering teams were selected to submit \"Final Design Packages\" due by January 13, 2016.\n\nA \"Design Weekend\" was held at Texas A&M University January 29–30, 2016, for all invited entrants. Engineers from the Massachusetts Institute of Technology were named the winners of the competition. Finishing second was Delft University of Technology from the Netherlands, followed by the University of Wisconsin–Madison, Virginia Tech, and the University of California, Irvine. While the MIT team took best overall, the University of Washington team won the Safety Subsystem Award and Delft University won the Pod Innovation Award. In the Design Category, the winner team was Hyperloop UPV from Universitat Politecnica de Valencia, Spain. On January 29, 2017, Delft Hyperloop (Delft University of Technology) won the final stage of the SpaceX Hyperloop competition, ahead of the Technical University of Munich and Massachusetts Institute of Technology.\n\nSome critics of Hyperloop focus on the experience—possibly unpleasant and frightening—of riding in a narrow, sealed, windowless capsule inside a sealed steel tunnel, that is subjected to significant acceleration forces; high noise levels due to air being compressed and ducted around the capsule at near-sonic speeds; and the vibration and jostling. Even if the tube is initially smooth, ground may shift with seismic activity. At high speeds, even minor deviations from a straight path may add considerable buffeting. This is in addition to practical and logistical questions regarding how to best deal with safety issues such as equipment malfunction, accidents, and emergency evacuations.\n\nOther maglev trains are already in use, which avoid much of the added costs of Hyperloop. The SCMaglev in Japan has demonstrated without a vacuum tube, by using an extremely aerodynamic train design. It also avoids the cost and time required to pressurize and depressurize the exit and entry points of a Hyperloop tube.\n\nThere is also the criticism of design technicalities in the tube system. Prof. John Hansman has stated problems, such as how a slight misalignment in the tube would be compensated for and the potential interplay between the air cushion and the low-pressure air. He has also questioned what would happen if the power were to go out when the pod was miles away from a city. Prof. Richard Muller has also expressed concern regarding \"[the Hyperloop's] novelty and the vulnerability of its tubes, [which] would be a tempting target for terrorists\", and that the system could be disrupted by everyday dirt and grime.\n\nThe alpha proposal projected that cost savings compared with conventional rail would come from a combination of several factors. The small profile and elevated nature of the alpha route would enable Hyperloop to be constructed primarily in the median of Interstate 5. However, whether this would be truly feasible is a matter of debate. The low profile would reduce tunnel boring requirements and the light weight of the capsules is projected to reduce construction costs over conventional passenger rail. It was asserted that there would be less right-of-way opposition and environmental impact as well due to its small, sealed, elevated profile versus that of a rail easement; however, other commentators contend that a smaller footprint does not guarantee less opposition. In criticizing this assumption, mass transportation writer Alon Levy said, \"In reality, an all-elevated system (which is what Musk proposes with the Hyperloop) is a bug rather than a feature. Central Valley land is cheap; pylons are expensive, as can be readily seen by the costs of elevated highways and trains all over the world\". Michael Anderson, a professor of agricultural and resource economics at UC Berkeley, predicted that costs would amount to around .\n\nThe Hyperloop white paper suggests that of each one-way passenger ticket between Los Angeles and San Francisco would be sufficient to cover initial capital costs, based on amortizing the cost of Hyperloop over 20 years with ridership projections of 7.4 million per year in each direction and does not include operating costs (although the proposal asserts that electric costs would be covered by solar panels). No total ticket price was suggested in the alpha design. The projected ticket price has been questioned by Dan Sperling, director of the Institute of Transportation Studies at UC Davis, who stated that \"there's no way the economics on that would ever work out.\"\n\nThe early cost estimates of the Hyperloop are a subject of debate. A number of economists and transportation experts have expressed the belief that the price tag dramatically understates the cost of designing, developing, constructing, and testing an all-new form of transportation. \"The Economist\" said that the estimates are unlikely to \"be immune to the hypertrophication of cost that every other grand infrastructure project seems doomed to suffer.\"\n\nPolitical impediments to the construction of such a project in California will be very large. There is a great deal of \"political and reputation capital\" invested in the existing mega-project of California High-Speed Rail. Replacing that with a different design would not be straightforward given California's political economy. Texas has been suggested as an alternate for its more amenable political and economic environment.\n\nBuilding a successful Hyperloop sub-scale demonstration project could reduce the political impediments and improve cost estimates. Musk has suggested that he may be personally involved in building a demonstration prototype of the Hyperloop concept, including funding the development effort.\n\nThe solar panels Musk plans to install along the length of the Hyperloop system have been criticized by Prof. Roger Goodall, as not being feasible enough to return enough energy to power the Hyperloop system, arguing that the air pumps and propulsion would require much more power than the solar panels could generate.\n\nThe concept of transportation of passengers in pneumatic tubes is not new. The first patent to transport goods in tubes was taken out in 1799 by the British mechanical engineer and inventor George Medhurst. In 1812, Medhurst wrote a book detailing his idea of transporting passengers and goods through air-tight tubes using air propulsion.\n\nIn the early 1800s, there were other similar systems proposed or experimented with and were generally known as an atmospheric railway although this term is also used for systems where the propulsion is provided by a separate pneumatic tube to the train tunnel itself.\n\nOne of the earliest was the Dalkey Atmospheric Railway which operated near Dublin between 1844 and 1854.\n\nThe Crystal Palace pneumatic railway operated in London around 1864 and used large fans, some in diameter, that were powered by a steam engine. The tunnels are now lost but the line operated successfully for over a year.\n\nOperated from 1870 to 1873, the Beach Pneumatic Transit was a one-block-long prototype of an underground tube transport public transit system in New York City. The system worked at near-atmospheric pressure, and the passenger car moved by means of higher-pressure air applied to the back of the car while somewhat lower pressure was maintained on the front of the car.\n\nIn the 1910s, vacuum trains were first described by American rocket pioneer Robert Goddard. While the Hyperloop has significant innovations over early proposals for reduced pressure or vacuum-tube transportation apparatus, the work of Goddard \"appears to have the greatest overlap with the Hyperloop\".\n\nPrinceton Physicist Gerard K. O'Neill wrote about transcontinental trains using magnetic propulsion in his book \"2081: A Hopeful View of the Human Future\". While a work of fiction, this book was an attempt to predict future technologies in everyday life. In his prediction, he envisioned these trains which used magnetic levitation running in underground tunnels which had much of the air evacuated to increase speed and reduce friction. He also demonstrated a scale prototype device that accelerated a mass using magnetic propulsion to high speeds. It was called a mass driver and was a central theme in his non-fiction book on space colonization \"The High Frontier\".\n\nSwissmetro was a proposal to run a maglev train in a low-pressure environment. Concessions were granted to Swissmetro in the early 2000s to connect the Swiss cities of St. Gallen, Zurich, Basel, and Geneva. Studies of commercial feasibility reached differing conclusions and the vactrain was never built.\n\nChina was reported to be building a vacuum based maglev train in August 2010 according to a laboratory at Jiaotong University. It was expected to cost ( at the August 2010 exchange rate) more per kilometer than regular high-speed rail. , it has not been built.\n\nThe ET3 Global Alliance (ET3) was founded by Daryl Oster in 1997 with the goal of establishing a global transportation system using passenger capsules in frictionless maglev full-vacuum tubes. Oster and his team met with Elon Musk on September 18, 2013, to discuss the technology, resulting in Musk promising an investment in a prototype of ET3's proposed design.\n\nThere are multiple examples of depressurized tubes in literature and media going back to the 19th century. Harry Harrison's 1972 book \"Tunnel Through the Deeps\" is an early steampunk book that give explicit details about how such a system would work both on land and at sea – including the use of underwater bridges to float the tubes across the depths beyond the continental shelf. Gene Roddenberry's follow-on to \"Star Trek\", \"Genesis II\", used a very similar concept – called a \"subshuttle\" in the program – to quickly move characters from place to place.\n\n\n"}
{"id": "1473312", "url": "https://en.wikipedia.org/wiki?curid=1473312", "title": "Ice volcano", "text": "Ice volcano\n\nAn ice volcano is a conical mound of ice formed over a terrestrial lake via the eruption of water and slush through an ice shelf. The process is wave-driven, with wind providing the energy for the waves to cut through the ice and form the volcanoes. After being ejected into the atmosphere, the liquid water and slush freeze and fall back to the surface, growing the formation. Ice may also be erupted. The phenomenon is most often observed along the southern coast of Lake Erie and Lake Ontario, when the temperature is below freezing and the wind blows onshore with a velocity of at least 40 km/h. They are known to reduce coastal erosion there. The formations are temporary — they are frequently destroyed by storms and warm weather, and once the lake wholly freezes over, eruptions are no longer possible.\n\nThere is no consensus name for this phenomenon. Due to its similarity to volcanism and particularly cryovolcanism, the term \"ice volcano\" is frequently used, but it remains controversial. Unlike geysers and related structures, ice volcanoes are not hydrothermal.\n\nThe uplifts may attract a number of visitors, but they are dangerous, and experts warn that people may fall through the ice or slip into the cold lake. Ice volcanoes are used by snowy owls as hunting platforms to search for waterfowl.\n\nThese features are distinct from pressure ridges, which are uplifts formed by the compression of ice against a shoreline or another floe. Instead, ice volcanoes are created by waves colliding with irregularities at the edge of an ice sheet. The abnormalities concentrate the wave energy in a small area, where the ice is eroded to form a V-shaped channel. Spray, ice, and slush splashing out of the feature create a volcanic cone at the channel's shoreward end. This process takes only a few hours. The lakeward end of the channel may then by sealed by ice, but the volcano may continue to erupt. A wave amplitude of at least one meter is needed to induce eruptions, so ice volcanoes are rarely active without storm-force winds. Formation near land is suppressed by reefs and shoals, which absorb the wave energy needed for the phenomenon. Nonetheless, they may produce larger cones further out at sea, where the greater depth makes this possible. Formation is more thoroughly suppressed by powerful storms, which erode the ice too fast for mound creation.\n\nOne type of ice volcano, known as a cold spot, does not require waves to break against the edge of an ice shelf. Instead, water and slush erupt through a region of weak ice near the coast and form a mound. This is analogous to a geological hotspot.\n\nLandfast ice is required, so the volcanoes normally form near land. They are found in successive rows, and within one row, the features usually have equal height and spacing. However, when comparing two rows, the height and spacing may be drastically different. Ice volcanoes range in height from less than one meter to ten meters, with the largest ones located far from the shore. Eruptions over ten meters high have been observed, but it is believed that the height of the eruptions are proportional to the size of the mounds. A single eruption may increase the height of the volcano by several centimeters. When an eruption occurs above 0 °C, however, the water erodes the uplift instead of expanding it. Spacing is determined by the amplitude and direction of the waves. In general, the appearance and number of ice volcanoes change considerably between winters.\n\nDifferent types of ice volcanoes have been compared to shield volcanoes and stratovolcanoes. They are noted for their symmetry. Cold spot volcanoes are particularly symmetrical, but their eruption has not been observed.\n"}
{"id": "38717496", "url": "https://en.wikipedia.org/wiki?curid=38717496", "title": "Indian Ocean Experiment", "text": "Indian Ocean Experiment\n\nThe Indian Ocean Experiment (INDOEX) was a 1999 multinational scientific study designed to measure the transport of air pollution from Southeast Asia into the Indian Ocean. The project was led by Veerabhadran Ramanathan.\n\n\n"}
{"id": "343555", "url": "https://en.wikipedia.org/wiki?curid=343555", "title": "Interval signal", "text": "Interval signal\n\nAn interval signal, or tuning signal, is a characteristic sound or musical phrase used in international broadcasting, numbers stations, and by some domestic broadcasters, played before commencement or during breaks in transmission, but most commonly between programmes in different languages. It serves several purposes:\n\nThe practice began in Europe in the 1920s and 1930s and was carried over into shortwave broadcasts. The use of interval signals has declined with the advent of digital tuning systems, but has not vanished. Interval signals were not required on commercial channels in the USA, where jingles were used as identification.\n\n « \\relative c { g4 g4. e8 g f d g a8. g16 g2 \\fermata \\bar \"|.\"\n } \\\\ \\relative c\" {\\partial 4 \n\n\n\nClassical Radio Station WQXR-FM in New York City, during its ownership by The New York Times Company, played different variations of a classical infused gong with the ID read at the same time as \"The Classical Station of the New York Times, WQXR, New York (And WQXR.com 2000-2009) \n\nNumbers stations are often named after their interval signals, such as The Lincolnshire Poacher or Magnetic Fields after \"Magnetic Fields Part 1\" by Jean-Michel Jarre.\n\n"}
{"id": "31615507", "url": "https://en.wikipedia.org/wiki?curid=31615507", "title": "List of Ramsar sites in Japan", "text": "List of Ramsar sites in Japan\n\nThe Ramsar Convention (Convention on Wetlands of International Importance especially as Waterfowl Habitat) is an international treaty for the conservation and sustainable use of wetlands. Adopted in 1971, it entered into force in 1975 and as of December 2015 has 169 Contracting Parties. Japan was the twenty-fourth party to accede, on 17 October 1980. Kushiro-shitsugen was the first of Japan's fifty Ramsar Sites as of the twelfth Conference of the Contracting Parties (2015).\n\n\n"}
{"id": "55156767", "url": "https://en.wikipedia.org/wiki?curid=55156767", "title": "List of broadcasting languages by country", "text": "List of broadcasting languages by country\n\nForeign broadcasting is broadcasting with a foreign element.\n\nThe foreign element may be the territory of the audience, its ethnicity, or both.\n\nExternal and selected other foreign language audio broadcasting services\n\nBold: 10 major international broadcasting languages (Arabic, Chinese, English, French, German, Persian, Portuguese, Russian, Spanish, Turkish)\n\"Italic\": major language(s) of the respective country\n\nSelected external and other foreign language TV services\n"}
{"id": "21303512", "url": "https://en.wikipedia.org/wiki?curid=21303512", "title": "List of glaciers in the United States", "text": "List of glaciers in the United States\n\nThis is a list of glaciers existing in the United States, currently or in recent centuries. These glaciers are located in nine states, all in the Rocky Mountains or farther west. The southernmost named glacier among them is the Lilliput Glacier in Tulare County, east of the Central Valley of California.\nThere are approximately 664 named glaciers in Alaska according to the Geographic Names Information System (GNIS).\n\n\n\n\n\nThe Pacific Coast Ranges include glaciers in the three states on the Pacific Coast. \n\nThere are approximately 186 named glaciers in Washington according to the Geographic Names Information System (GNIS). However, the 1980 eruption of Mount Saint Helens eliminated nine of its eleven named glaciers and only the new glacier known as Crater Glacier has been reestablished since.\n\n\n\n\n\n\n\n\n\n\n\nMount St. Helens once had eleven named glaciers, but the 1980 eruption of the volcano eliminated nine glaciers and the two remaining aren't recognized. One newly formed glacier now resides in the caldera of the volcano.\n\nThere are 35 named glaciers in Oregon according to the Geographic Names Information System (GNIS).\n\n\n\n\n\n\nThere are 20 named glaciers in California according to the Geographic Names Information System (GNIS).\nMount Shasta is a volcano with seven named glaciers in the northern region of California.\n\n\nBasin and Range Province lies east of the Coast Ranges and west of the Rockies. There are no active glaciers in the Basin and Range Province and Wheeler Peak Glacier is considered by some to be a rock glacier.\n\nAccording to the Geographic Names Information System (GNIS), there are sixteen named glaciers in Colorado. \nAccording to early mountain explorers and scientists, Colorado once had over eighteen glaciers before 1880. \n\"See also Glaciers of Colorado\"\n\"See also \"\n\nAccording to the Geographic Names Information System (GNIS), there are 60 named glaciers in Montana.\n\n\"There are no visible glaciers in Utah. Timpanogos Glacier article is documentation of a buried glacier.\"\n\nAccording to the Geographic Names Information System (GNIS), there are 37 named glaciers in Wyoming..\n\n\n"}
{"id": "21355232", "url": "https://en.wikipedia.org/wiki?curid=21355232", "title": "List of national parks of the United States", "text": "List of national parks of the United States\n\nThe United States has 60 protected areas known as national parks that are operated by the National Park Service, an agency of the Department of the Interior. National parks must be established by an act of the United States Congress. A bill creating the first national park, Yellowstone, was signed into law by President Ulysses S. Grant in 1872, followed by Mackinac National Park in 1875 (decommissioned in 1895), and then Rock Creek Park (later merged into National Capital Parks), Sequoia and Yosemite in 1890. The Organic Act of 1916 created the National Park Service \"to conserve the scenery and the natural and historic objects and wildlife therein, and to provide for the enjoyment of the same in such manner and by such means as will leave them unimpaired for the enjoyment of future generations.\" Many current national parks had been previously protected as national monuments by the president under the Antiquities Act before being upgraded by Congress. Seven national parks (including six in Alaska) are paired with a national preserve, areas with different levels of protection that are administered together but considered separate units and whose areas are not included in the figures below.\nCriteria for the selection of national parks include natural beauty, unique geological features, unusual ecosystems, and recreational opportunities (though these criteria are not always considered together). National monuments, on the other hand, are frequently chosen for their historical or archaeological significance. Fourteen national parks are designated UNESCO World Heritage Sites (WHS), while 21 national parks are designated UNESCO Biosphere Reserves (BR). Eight national parks are designated in both UNESCO programs.\n\nTwenty-eight states have national parks, as do the territories of American Samoa and the United States Virgin Islands. California has the most (nine), followed by Alaska (eight), Utah (five), and Colorado (four). The largest national park is Wrangell–St. Elias in Alaska: at over , it is larger than each of the nine smallest states. The next three largest parks are also in Alaska. The smallest park is Gateway Arch National Park, Missouri, at approximately . The total area protected by national parks is approximately , for an average of but a median of only . \n\nThe national parks set a visitation record in 2017, with more than 84 million visitors. The most-visited national park is Great Smoky Mountains National Park in North Carolina and Tennessee, with over 11.3 million visitors in 2017, followed by Arizona's Grand Canyon National Park, with over 6.2 million. In contrast, only 11,177 people visited the remote Gates of the Arctic National Park and Preserve in Alaska in the same year.\n\nA few former national parks are no longer designated as such, or have been disbanded. Other units of the National Park Service ( altogether) are broadly referred to as national parks within the National Park System.\n\n\n"}
{"id": "40458573", "url": "https://en.wikipedia.org/wiki?curid=40458573", "title": "List of power stations in Algeria", "text": "List of power stations in Algeria\n\nThis page lists all power stations in Algeria.\n\n\n"}
{"id": "395044", "url": "https://en.wikipedia.org/wiki?curid=395044", "title": "List of rivers of Poland", "text": "List of rivers of Poland\n\nFollowing is a list of rivers, which are at least partially, if not predominantly located within Poland.\n\nBaltic Sea\n\nBaltic Sea\n"}
{"id": "31630387", "url": "https://en.wikipedia.org/wiki?curid=31630387", "title": "List of tornadoes in the 2011 Super Outbreak", "text": "List of tornadoes in the 2011 Super Outbreak\n\nDuring April 25–28, 2011, the local weather forecast offices of the National Weather Service confirmed 359 tornadoes in the United States, and Environment Canada confirmed another in Ontario. These tornadoes were part of a major outbreak of tornadoes, the 2011 Super Outbreak, in which 360 tornadoes touched down across 21 states in the Southern, Midwestern, and Northeastern United States and in Ontario, Canada. As the outbreak developed on April 25, numerous tornadoes touched down across Texas and Arkansas, with an EF3 near Hot Springs Village, Arkansas that caused significant damage and killed one person and a long-track EF2 tornado in the Vilonia, Arkansas area that killed four people and injured 16 others after being on the ground for over one hour. April 26 saw no deaths and mostly weaker tornadoes, with the notable tornadoes of the day being an EF2 tornado that tracked across parts of Texas and into Louisiana and a brief EF3 that struck Campbell Army Airfield, causing $1 million (2011 USD) in damage. From the 27th to 28th, a series of devastating, long-tracked, violent tornadoes killed hundreds of people throughout an area extending from Mississippi to Virginia. This included eleven EF4s and four EF5s. One particularly devastating and long-lived EF5 wedge tornado tore across northern Alabama and into Tennessee, killing 72 people and devastating several small towns, particularly Hackleburg, Phil Campbell, Tanner, and Harvest. A large, long-tracked EF4 was broadcast live on multiple TV stations as it caused catastrophic damage in densely populated areas of Tuscaloosa and Birmingham, Alabama, killing 64 people. Numerous other small towns including Smithville, Mississippi; Cordova, Alabama; Rainsville, Alabama; Ohatchee, Alabama; Cullman, Alabama; Trenton, Georgia; Ringgold, Georgia; Apison, Tennessee; and Glade Spring, Virginia sustained devastating, direct hits from intense tornadoes, with several producing death tolls well into the double digits. 319 additional tornado-related deaths occurred within those two days before the outbreak came to an end, bringing the total death toll to 324.\n\n<nowiki>*</nowiki> – One tornado touched down in Ontario, Canada and was rated as an F0. It is counted as an EF0 in this table.\n\n"}
{"id": "50330491", "url": "https://en.wikipedia.org/wiki?curid=50330491", "title": "MICROSCOPE (satellite)", "text": "MICROSCOPE (satellite)\n\nThe Micro-Satellite à traînée Compensée pour l'Observation du Principe d'Equivalence (MICROSCOPE) is a class minisatellite operated by CNES to test the universality of free fall (the equivalence principle) with a precision to the order of , 100 times more precise than can be achieved on Earth. It was launched on 25 April 2016 alongside Sentinel-1B and other small satellites.\n\nTo test the equivalence principle (i.e. the similarity of free fall for two bodies of different composition in an identical gravity field), two differential accelerometers are used successively. If the equivalence principle is verified, the two sets of masses will be subjected to the same acceleration. If different accelerations have to be applied, the principle will be violated.\n\nThe principal experiment is the Twin-Space Accelerometer for Gravity Experiment (T-SAGE), built by ONERA and composed of two identical accelerometers and their associated, concentric cylindrical masses. One accelerometer serves as a reference and contains two platinum-rhodium alloy masses, while the other is the test instrument and contains two masses with different neutron–proton ratios: one mass of platinum-rhodium alloy and another mass of titanium-aluminium-vanadium alloy (TA6V). The masses are maintained within their test areas by electrostatic repulsion, designed to render them motionless with respect to the satellite.\n\nIt is necessary to create a thermally benign environment for the accelerometers. To that end, a Sun-synchronous orbit provides constant illumination; the experiments are mounted on the end of the satellite bus away from the Sun; and to maintain thermal isolation from the satellite itself, the modes of thermal connection were modelled and wire connections were minimised.\n\nThe satellite employs a Drag-Free Attitude Control System (DFACS), also called the Acceleration and Attitude Control System (AACS), that uses a double-redundant primary and backup set of four microthrusters (sixteen total) to \"fly\" the satellite around the test masses. This system takes into account the dynamic forces acting on the spacecraft, including aerodynamic forces due to residual atmosphere, solar pressure forces due to photon impacts, electromagnetic forces within the Earth's magnetosphere, and gravitational forces in the Sun-Earth-Moon system.\n\nMICROSCOPE was successfully launched on 25 April 2016 at 21:02:13 UTC from the Guiana Space Centre outside Kourou, French Guiana. It was carried by a Soyuz-STA booster with a Fregat-M upper stage. Other payloads on this flight were the European Space Agency's Sentinel-1B Earth observation satellite and three CubeSats: OUFTI-1 from the University of Liège, e-st@r-II from the Polytechnic University of Turin, and AAUSAT-4 from Aalborg University.\n\nOn 4 December 2017, the first results were published. The equivalence principle was measured to hold true within a precision of , improving prior measurements by an order of magnitude.\n\nAfter completing its mission goals MICROSCOPE was de-commisioned on 18 October 2018. The spacecraft was first passivized and then two 4.5 meter inflatable booms of IDEAS (Innovative DEorbiting Aerobrake System) were deployed to passively de-orbit it through drag enhancement. With this innovative de-orbiting method, MICROSCOPE is expected to reenter the Earth's atmosphere within 25 years instead of 73 years.\n\n\n"}
{"id": "51063830", "url": "https://en.wikipedia.org/wiki?curid=51063830", "title": "Mediterranean wetlands", "text": "Mediterranean wetlands\n\nMediterranean Wetlands\n\nWetlands are everywhere, in all climates and in every country, and they may briefly be defined as transitional spaces between land and water. In a more detailed sense, according to the Ramsar Convention on Wetlands, wetlands are defined as:\n\n\"areas of marsh, fen, peat land or water, whether natural or artificial, permanent or temporary, with water that is static or flowing, fresh, brackish or salt, including areas of marine water the depth of which at low tide does not exceed six metres […..] and may incorporate riparian and coastal zones adjacent to the wetlands, and islands or bodies of marine water deeper than six metres at low tide lying within the wetlands\".\n\nThe Ramsar Classification of wetland types includes 42 types which can be broadly divided into:\n• Marine and coastal wetlands\n• Inland wetlands\n• Human-made wetlands\n\nAmong the richest ecosystems in the world, wetlands have an exceptional value, but are also the most threatened. In the Mediterranean, they support high concentrations of birds, mammals, reptiles, amphibians, fish and invertebrate species that cannot be found anywhere else in the world.\n- Key figures on biodiversity in Med region\n\nFurthermore, these ecosystems insure directly – and for free – services for millions of people. As natural infrastructures, they play a number of roles in the environment, principally water purification, flood control, carbon sink and shoreline stability.\n\nUnfortunately, wetlands continue to be among the world's most threatened ecosystems; 50% of the Mediterranean wetlands disappeared during the 20th century, and those that remain have been degraded or artificialized. There are 15 to 22 million hectares of wetlands in the Mediterranean Region, a fourth of which are artificial, such as dam reservoirs and fish-farming ponds.\n\nThe Mediterranean region is also a hotspot of global biodiversity, but the greatest direct threat to that biodiversity is the disappearance of these ecosystems. Plants and animals combined, one out of three Mediterranean wetland species is endangered.\n\nA scientific survey of 314 Mediterranean coastal wetlands shows that human activities, such as agriculture, have converted 71% of Mediterranean natural coastal wetland habitat into farmland, 21% by artificialization, and 8% by urban expansion. \n\nOne of the greatest threats to the Mediterranean wetlands is water extraction. Irrigated agriculture is the greatest water consumer in the region, accounting for 2/3 of total consumption. Rivers flow rates are falling due to water extraction, retention by dams, and the effects of climate change.\n\nWetlands lie at the interface between land and water, and are thus very diverse. The Ramsar Convention classified wetlands into three main types:\n\n• Marine/coastal\n\n• Continental\n\n• Artificial\n\nThe Mediterranean region is unique because of its special type of climate and its very long history of human use. Wetlands in the Mediterranean Basin are varied, and there are many different types in every country. For example:\n\n\n\n\nMan benefits from wetlands' natural functions and services, which help to meet the needs of millions of people – to cultivate soil, to fish for food, to cut trees for building, to hunt the waterfowl for leisure activities, to organize ecotourism programs in remarkable sites, to use their water for our domestic or agricultural needs, etc.\n\n\nSome cultural values developed in wetland areas are non-material and have become part of the life cycle and livelihood of local residents, such as rituals, beliefs, the seasonal work calendar, and different traditional techniques.\n\n\nThe Mediterranean wetlands have a high percentage of endemic species. They not only provide breeding and wintering for millions of birds, they also play a role as a stopping place for an even larger number of birds during migration periods.\n\n\nEnvironmental economics assumes that by giving a monetary value to nature, it would allow us to better take into account its importance in development choices. The Millennium Ecosystem Assessment gave wetlands a value of US$15 trillion in 1997.\n\nWetlands greatly influence the flow and quality of water, thus providing benefits to humans. They store and slowly release surface water, rain, snowmelt and flood waters, maintaining soil humidity during dry periods. But the most important service is the supply of clean water for drinking, agriculture, and industries. Scientists have estimated that a 2,500 acre wetland in Georgia saves $1 million in water pollution control costs annually (OTA 1993).\n\nDespite all the services they provide to human societies, Mediterranean wetlands continue to face many pressures from human activities. Their loss and degradation, started centuries ago, are often considered to be an unfortunate but unavoidable effect of the human development process and needs.\n\nThese negative impacts result from a number of pressures such as agriculture, urbanization, industry, fishing, tourism development, energy, transport and hunting. Each kind of pressure may impact several types of wetland services. For example, agricultural development may impact wetland surfaces, hydraulic regimes, water quality, and wetland types/ landscapes.\n\nThe Convention on Wetlands, called the Ramsar Convention, is an intergovernmental treaty that provides the framework for national action and international cooperation for the conservation and wise use of wetlands and their resources.\n\nRamsar's 15 regional initiatives support cooperation and capacity-building on wetland-related issues in specific regions or sub-regions. There are two types of regional initiatives: Ramsar Regional Centres for training and capacity building, and networks for regional cooperation. In 2016 there are 15 Ramsar Regional Initiatives covering regions of the Mediterranean, Asia, Africa and South America. Those regional networks provide a platform for collaboration between governments, technical experts, international NGOs, local communities and private companies.\n\nEstablished in 1991, the Mediterranean Wetlands Regional Initiative brings together 26 Mediterranean and peri-Mediterranean countries that are Parties to the Convention on Wetlands. Palestine and a number of organizations and wetland centres are also part of the MedWet Initiative.\nThe MedWet Mission is to ensure and support the effective conservation of the functions and values of Mediterranean wetlands and the sustainable use of their resources and services.\n\nThe Mediterranean Wetland Observatory (MWO) was established in 2008 at the request of the Mediterranean Wetlands Committee (MedWet/Com) as a multi-partner project coordinated by Tour du Valat (TdV), the Research Institute for the Conservation of Mediterranean Wetlands, based in Arles, France. Founded more than 50 years ago by Luc Hoffmann, the Tour du Valat has since then developed its research activities for the conservation of Mediterranean wetlands with a major mission: \"Better understanding of wetlands for better management\".\n\nThe main objective of the MWO is to act as a wetland management tool serving the MedWet Initiative's countries. The ultimate aim of this regional tool, in collaboration with MedWet, is to help to improve political decisions regarding the conservation and sustainable management of wetlands, particularly in terms of legislation, governance and best practices.\n\n\n\n"}
{"id": "5679875", "url": "https://en.wikipedia.org/wiki?curid=5679875", "title": "Mu Cancri", "text": "Mu Cancri\n\nMu Cancri (μ Cancri, μ Cnc, Mu Cnc) is the name of several stars\n\n\nMu Cancri also sometimes just means Mu2 Cancri.\n"}
{"id": "47151883", "url": "https://en.wikipedia.org/wiki?curid=47151883", "title": "Narmada Pushkaram", "text": "Narmada Pushkaram\n\nNarmada Pushkaram is a festival of River Narmada normally occurs once in 12 years. This Pushkaram is observed for a period of 12 days from the time of entry of Jupiter into Vrushabha Rasi (Taurus).\n\nAmarkantak temple, Omkareshawar Temple, Chausath Yogini Temple, Chaubis Avatar Temple, Maheshwar Maheshwar Temple, Nemawar Siddheshwar Mandir and Bhojpur Shiva Temple are very ancient and famous. Omkareshawar is one of the twelve Jyothirlingas and Amrarkantak are the best places to take holy bath in the Naramada river.\n\n"}
{"id": "26047109", "url": "https://en.wikipedia.org/wiki?curid=26047109", "title": "National Energy Commission", "text": "National Energy Commission\n\nThe National Energy Commission (NEC; ) is an agency established in 2010 to coordinate the overall energy policies for the People's Republic of China. The body includes 23 members from other agencies such as environment, finance, central bank, National Development and Reform Commission.\n\nThe purpose of this new commission is to draft a new energy development strategy, evaluate energy security and coordinate international cooperation on climate change, carbon reduction and energy efficiency.\n\nChina had a Ministry of Energy established in 1988 but it was disbanded five years after its portfolio overlapped with existing ministries.\n\nIn 2003, National Energy Bureau was created under the National Development and Reform Commission (NDRC) which reports to the Chinese State Council, which has broad administrative and planning control over energy in the Chinese economy.\n\nIn 2008, National Energy Administration (NEA) was established but lacked power to carry out its tasks because the energy sector management was spread between various agencies.\n\nChina has experienced power outages, concerns of growing imported energy demands, energy security and challenges coordinating energy supply and demand.\n\n\n\n\n"}
{"id": "51286480", "url": "https://en.wikipedia.org/wiki?curid=51286480", "title": "Phi Ceti", "text": "Phi Ceti\n\nThe Bayer designation Phi Ceti (φ Cet / φ Ceti) is shared by four stars, in the constellation Cetus:\n\nAll of them were the Arabs' Al Nithām. According to the catalogue of stars in the \"Technical Memorandum 33-507 - A Reduced Star Catalog Containing 537 Named Stars\", \"Al Nitham\" were the title for four stars :φ Ceti as \"Al Nitham I\", φ Ceti as \"Al Nitham II\", φ Ceti as \"Al Nitham III\" and φ Ceti as \"Al Nitham IV\". \n\nφ Ceti and φ Ceti were member of asterism 天溷 (\"Tiān Hùn\"), \"Celestial Pigsty\", \"Legs\" mansion.\n"}
{"id": "49153903", "url": "https://en.wikipedia.org/wiki?curid=49153903", "title": "Rizalite", "text": "Rizalite\n\nRizalites are black stones of meteoritic origin found in the Philippines. They are considered to be some 710,000 years old.\n\nAs such, they are considered tektites - bodies composed of black or dark brown natural glass formed from terrestrial debris ejected during extraterrestrial, meteorite impacts. They generally range in size from millimeters to centimeters.\n\nThe stones were named by the Philippine national hero José Rizal.\n"}
{"id": "27051743", "url": "https://en.wikipedia.org/wiki?curid=27051743", "title": "Sacred waters", "text": "Sacred waters\n\nAs opposed to holy water, water elevated with the sacramental blessing of a cleric (Altman 2002:131), sacred waters are characterized by tangible topographical land formations such as rivers, lakes, springs, and oceans. These organic bodies of water have attained religious significance not from the modern alteration or blessing, but were sanctified through mythological or historical figures. Sacred waters have been exploited for cleansing, healing, initiations, and death rites (Altman 2002:6).\n\nUbiquitous and perpetual fixations with water occur across religious traditions. It tends to be a central element in the creations accounts of almost every culture with mythological, cosmological, and theological myths (Altman 2002:3-6,13-20). In this way, many groups characterize water as \"living water\", or the \"water of life\" (Varner 2004:19, Altman 2002:2, Strang 2004:83). This means that it gives life and is the fundamental element from which life arises. Each religious or cultural group that feature waters as sacred substances tends to favor certain categorizations of some waters more than others, usually those that are most accessible to them and that best integrate into their rituals (Altman 2002:3).\n\nWhile all rivers are sacred in Hinduism, the Ganges River is particularly revered. In the Vedic myths, the goddess Ganga descended upon the earth to purify and prepare the dead (Alley 2008:171, Haberman 2006:60-61, Narayanan 2001:190-191). The River Ganges (Ganga) in India is seen as the physical embodiment of this goddess. Since the river waters are as both inherently pure themselves and having major purificatory qualities (Alley 2008:173-174, Nelson 2008:102), people come to bathe in them, drink from them, leave offerings for them, and give their physical remains to them.\n\nThe Ganges is said to purify the soul of negative \"karma\", corporeal sins, and even impurities from previous lives (Nelson 2008:102). At sunrise along the Ganges, pilgrims descend the \"ghat\" steps to drink of the waters, bathe themselves in the waters and perform ablutions where they submerge their entire bodies. These practitioners desire to imbibe and surround themselves with the Ganges’s waters so that they can be purified (Altman 2002:136-138, 181-183, 196-198). Hindu conceptualizations of the sacred are fluid and renewable. Purity and pollution exist upon a continuum where most entities, including people, can become sacred and then become stagnated and full of sin once again (Lamb 2008:341-346). Performing these rituals is also an act to become closer to the Hindu deities, and ultimately the Divine.\n\nThe Ganges is one of the most highly favored sites for funerary rituals in India. It is presumed that if a deceased person is cleansed by the Ganges, it will help liberate their soul, or expedite the number of lives they need to achieve this (Altman 2002:137, McClaymond 2008:315). In the traditional funerary ceremony, a dead person is placed upon a funeral pyre until the body becomes cremated, then the ashes are sent upon the river (Michaels 2004:136-139). Many Hindus go to great lengths to purify themselves one last time before death. When this is not possible, family members will actually mail the ashes to a priest so that he can perform the ceremony of entering the waters (Altman 2008:136-137).\n\nManu, the mythic law giver, gave directives and prohibitions regarding the river: “impure objects like urine, feces, spit; or anything which has these elements, blood, or poison should not be cast into the water” (Narayanan 2001:183-184). Few or none of his directives hold forth along most places down the Ganges today. Journalist Joshua Hammer wrote a very illustrative account of his personal visit to the Ganges in which he described seeing both animal and human corpses floating down the river or sometimes embedded in heaps of garbage. People continued to bathe, and children to play in very murky waters; the color in some parts completely changed from toxic sewage and runoff (Hammer 2007). As the Ganges River remains interwoven into daily existence, Hindus are vulnerable to urban contamination. Irony lies in that this river is venerated for her ability to wash away contaminants, yet is being stagnated.\n\nLake Titicaca is widely known as being a sacred place for the Inca people. The Inca Empire origins lie in Lake Titicaca. Ancient Incan myths describe the Incas as being blessed by the sun because the sun first emerged from Lake Titicaca. Since then, the sun organizes social order and the movement of the sun organizes rituals and gatherings. The first emergence of people in the time of the sun emergence is said to be the elite in their caste system. The origin of the elite was and continues to be contested among the people on the Island of Lake Titicaca. Thus, creating competition to become part of the elite rank (Bauer 1998:240-246). In recent times, the pollution of Lake Titicaca has built up and caused an increase of green algae. The people of Lake Titicaca Special Projects continuously are creating ways to bring awareness to the importance of a clean lake for their society (Holston 2008:42).\n\nThe ancient Mayans valued social order and their society flourished because of the structure of their order. The ancient Mayans strived and focused their actions on pleasing their many gods. Essentially, the Mayans believed that the world consisted of three layers: the watery underworld, the middle earthly realm, and the sky realm. The Mayans viewed bodies of water as a direct connection to the watery underworld and underground water obtained through a cave as an even better connection to spirits and deities. Cenotes are very important to the Mayas. The famous cenote at Chichen Itza proves to be important with the many findings of artifacts and skeletal remains. Sacrifices were common at this site among the ancient Mayans. Different people were sacrificed and findings show that most of the people were men and children (Bruhns 1999:209). Like any archeological site, looting is problem in preserving and studying the cenote at Chichen Itza.\n\nThe Navajo and Hopi people have long embraced the water underneath and around the Black Mesa area as sacred to their people. The people have long lived around and became dependent on springs and wells of the Black Mesa. These waters are the only source of drinking water, water for livestock, and water for agriculture for the Navajo and Hopi people. In respect for the water, these people carryout religious and ceremonial tributes to the water of the Black Mesa. These waters have organized their people around the Black Mesa and resulted in the reliance of the waters for all aspects of their lives. With the emergence of Peabody Energy came threats to the preservation of their sacred water. Peabody Energy pumps water out from underneath the Black Mesa to transport their mining minerals. In May 2002 the Navajo and Hopi people from northeastern Arizona joined their people in St. Louis Missouri to fight against Peabody Energy and its shareholders. In January 2002 Peabody proposed and was granted the right to use thirty-two percent more Navajo Aquifer (Naquifer) water than they had already been using. The significant increase in water pumped out of the Naquifer, dramatically affected the drinkability of the water from the springs and wells connected to the Naquifer. Before the significant increase of pumping, the water was clean enough to drink without any kind of purification. Another result of the pumping is the noticeable drop in the water levels of the springs and wells. The drop in water levels was almost immediately recognized after Peabody was granted permission to pump out more water. This had caused disruption in the ceremonial and cultural lives of the Navajo and Hopi people as well as disruption to their farming (Lee 2010).\n\n"}
{"id": "13015576", "url": "https://en.wikipedia.org/wiki?curid=13015576", "title": "Shortwave relay station", "text": "Shortwave relay station\n\nShortwave relay stations are transmitter sites used by international broadcasters to extend their coverage to areas that cannot be reached easily from their home state. E.g., the BBC operates an extensive net of relay stations.\n\nThese days the programs are fed to the relay sites by satellite, cable/optical fiber or the Internet. Frequencies, transmitter power and antennas depend on the desired coverage. Some regional relays even operate in the medium wave or FM bands.\n\nRelay stations are also important to reach listeners in countries that practice radio jamming. Depending on the effect of the shortwave dead zone the target countries can jam the programs only locally, e.g. for bigger cities. For this purpose Radio Free Europe/Radio Liberty with studios in Munich, Germany operated a relay station in Portugal, in the extreme west of Europe, to reach then-communist Eastern Europe.\n\nTwo and only one broadcasting technology couples all of the components of a traditional shortwave relay station into one unit: the ALLISS module. For persons totally unfamiliar with the concepts of how shortwave relay stations operate this design may be the most understandable.\n\nThe ALLISS module is a fully rotatable antenna system for high power (typically 500 kW only) shortwave radio broadcasting—it essentially is a self contained shortwave relay station.\n\nMost of the world's shortwave relay stations do not use this technology, due to its cost (15m EUR per ALLISS module: Transmitter + Antenna + Automation equipment).\n\nA traditional shortwave relay station—depending on how many transmitters and antennas that it will have—may take up to two years to plan. After planning is completed, it may take up to five years to construct the relay station.\n\nThe historically long design and planning cycle for shortwave relay stations ended in the 1990s. Many advanced software planning tools (not related to the relay station design proper) became available. Choosing a series of sites for a relay station is about 100 times faster using Google Earth, for example. With the modern graphical version of Ioncap, simplified propagation studies can completed in less than a week for any chosen site.\n\nIn some cases, existing relay stations can have their designs more or less duplicated, thus speeding up development time. However, there is one general exception to this: the ALLISS Module. From initial planning to deployment of ALLISS Modules may take a mere 1.5 years to 9 months depending on the number of modules deployed at one time in a particular sector of a country.\n\nThese are considered general operating parameters:\n\nGeneral requirements of shortwave relay stations:\n\n\nThe IEEE Book series \"The History of International Broadcasting\" (Volume I) describes mobile shortwave relay stations used by the German propaganda ministry during WWII, to avoid them being located by radio direction finding and bombed by the Allies. They consisted of a generator truck, transmitter truck and an antenna truck, and are thought to have had a radiated power of about 50 kW. Radio Industry Zagreb (RIZ Transmitters) currently produces mobile shortwave transmitters.\n\nThe International broadcasting center of TDF (Télédiffusion de France) is at Issoudun/Saint-Aoustrille. As of 2011, Issoudun is utilized by TDF for shortwave transmissions. The site uses 12 rotary ALLISS antennas fed by 12 transmitters of 500 kW each to transmit shortwave broadcasts by Radio France Internationale (RFI), along with other broadcast services.\n\n"}
{"id": "26904", "url": "https://en.wikipedia.org/wiki?curid=26904", "title": "Silurian", "text": "Silurian\n\nThe Silurian is a geologic period and system spanning 24.6 million years from the end of the Ordovician Period, at million years ago (Mya), to the beginning of the Devonian Period, Mya. As with other geologic periods, the rock beds that define the period's start and end are well identified, but the exact dates are uncertain by several million years. The base of the Silurian is set at a series of major Ordovician–Silurian extinction events when 60% of marine species were wiped out.\n\nA significant evolutionary milestone during the Silurian was the diversification of jawed fish and bony fish. Multi-cellular life also began to appear on land in the form of small, bryophyte-like and vascular plants that grew beside lakes, streams, and coastlines, and terrestrial arthropods are also first found on land during the Silurian. However, terrestrial life would not greatly diversify and affect the landscape until the Devonian.\n\nThe Silurian system was first identified by British geologist Roderick Murchison, who was examining fossil-bearing sedimentary rock strata in south Wales in the early 1830s. He named the sequences for a Celtic tribe of Wales, the Silures, inspired by his friend Adam Sedgwick, who had named the period of his study the Cambrian, from the Latin name for Wales. This naming does not indicate any correlation between the occurrence of the Silurian rocks and the land inhabited by the Silures (. , ). In 1835 the two men presented a joint paper, under the title \"On the Silurian and Cambrian Systems, Exhibiting the Order in which the Older Sedimentary Strata Succeed each other in England and Wales,\" which was the germ of the modern geological time scale. As it was first identified, the \"Silurian\" series when traced farther afield quickly came to overlap Sedgwick's \"Cambrian\" sequence, however, provoking furious disagreements that ended the friendship. Charles Lapworth resolved the conflict by defining a new Ordovician system including the contested beds. An early alternative name for the Silurian was \"Gotlandian\" after the strata of the Baltic island of Gotland.\n\nThe French geologist Joachim Barrande, building on Murchison's work, used the term \"Silurian\" in a more comprehensive sense than was justified by subsequent knowledge. He divided the Silurian rocks of Bohemia into eight stages. His interpretation was questioned in 1854 by Edward Forbes, and the later stages of Barrande, F, G and H, have since been shown to be Devonian. Despite these modifications in the original groupings of the strata, it is recognized that Barrande established Bohemia as a classic ground for the study of the earliest fossils.\n\nThe Llandovery Epoch lasted from mya, and is subdivided into three stages: the , lasting until , the , lasting to , and the . The epoch is named for the town of Llandovery in Carmarthenshire, Wales.\n\nThe Wenlock, which lasted from mya, is subdivided into the (to ) and ages. It is named after Wenlock Edge in Shropshire, England. During the Wenlock, the oldest-known tracheophytes of the genus \"Cooksonia\", appear. The complexity of slightly later Gondwana plants like \"Baragwanathia\", which resembled a modern clubmoss, indicates a much longer history for vascular plants, extending into the early Silurian or even Ordovician. The first terrestrial animals also appear in the Wenlock, represented by air-breathing millipedes from Scotland.\n\nThe Ludlow, lasting from mya, comprises the stage, lasting until , and the stage. It is named for the town of Ludlow (and neighbouring Ludford) in Shropshire, England.\n\nThe Přídolí, lasting from mya, is the final and shortest epoch of the Silurian. It is named after one locality at the \"Homolka a Přídolí\" nature reserve near the Prague suburb Slivenec in the Czech Republic. \"Přídolí\" is the old name of a cadastral field area.\n\nIn North America a different suite of regional stages is sometimes used:\n\nIn Estonia the following suite of regional stages is used:\n\nWith the supercontinent Gondwana covering the equator and much of the southern hemisphere, a large ocean occupied most of the northern half of the globe. The high sea levels of the Silurian and the relatively flat land (with few significant mountain belts) resulted in a number of island chains, and thus a rich diversity of environmental settings.\n\nDuring the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian icecaps were less extensive than those of the late-Ordovician glaciation. The southern continents remained united during this period. The melting of icecaps and glaciers contributed to a rise in sea level, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. The continents of Avalonia, Baltica, and Laurentia drifted together near the equator, starting the formation of a second supercontinent known as Euramerica.\n\nWhen the proto-Europe collided with North America, the collision folded coastal sediments that had been accumulating since the Cambrian off the east coast of North America and the west coast of Europe. This event is the Caledonian orogeny, a spate of mountain building that stretched from New York State through conjoined Europe and Greenland to Norway. At the end of the Silurian, sea levels dropped again, leaving telltale basins of evaporites extending from Michigan to West Virginia, and the new mountain ranges were rapidly eroded. The Teays River, flowing into the shallow mid-continental sea, eroded Ordovician Period strata, forming deposits of Silurian strata in northern Ohio and Indiana.\n\nThe vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include two phases of the Tethys, the Proto-Tethys and Paleo-Tethys, the Rheic Ocean, the Iapetus Ocean (a narrow seaway between Avalonia and Laurentia), and the newly formed Ural Ocean.\n\nThe Silurian period enjoyed relatively stable and warm temperatures, in contrast with the extreme glaciations of the Ordovician before it, and the extreme heat of the ensuing Devonian. Sea levels rose from their Hirnantian low throughout the first half of the Silurian; they subsequently fell throughout the rest of the period, although smaller scale patterns are superimposed on this general trend; fifteen high-stands can be identified, and the highest Silurian sea level was probably around 140 m higher than the lowest level reached.\n\nDuring this period, the Earth entered a long, warm greenhouse phase, supported by high CO levels of 4500 ppm, and warm shallow seas covered much of the equatorial land masses. Early in the Silurian, glaciers retreated back into the South Pole until they almost disappeared in the middle of Silurian. The period witnessed a relative stabilization of the Earth's general climate, ending the previous pattern of erratic climatic fluctuations. Layers of broken shells (called coquina) provide strong evidence of a climate dominated by violent storms generated then as now by warm sea surfaces. Later in the Silurian, the climate cooled slightly, but closer to the Silurian-Devonian boundary, the climate became warmer.\n\nThe climate and carbon cycle appears to be rather unsettled during the Silurian, which has a higher concentration of isotopic excursions than any other period. The Ireviken event, Mulde event and Lau event each represent isotopic excursions following a minor mass extinction and associated with rapid sea-level change, in addition to the larger extinction at the end of the Silurian. Each one leaves a similar signature in the geological record, both geochemically and biologically; pelagic (free-swimming) organisms were particularly hard hit, as were brachiopods, corals and trilobites, and extinctions rarely occur in a rapid series of fast bursts.\n\nThe Silurian was the first period to see megafossils of extensive terrestrial biota, in the form of moss-like miniature forests along lakes and streams. However, the land fauna did not have a major impact on the Earth until it diversified in the Devonian.\n\nThe first fossil records of vascular plants, that is, land plants with tissues that carry water and food, appeared in the second half of the Silurian period. The earliest-known representatives of this group are \"Cooksonia\". Most of the sediments containing \"Cooksonia\" are marine in nature. Preferred habitats were likely along rivers and streams. \"Baragwanathia\" appears to be almost as old, dating to the early Ludlow (420 million years) and has branching stems and needle-like leaves of 10–20 cm. The plant shows a high degree of development in relation to the age of its fossil remains. Fossils of this plant have been recorded in Australia, Canada and China. \"Eohostimella heathana\" is an early, probably terrestrial, \"plant\" known from compression fossils of early Silurian (Llandovery) age. The chemistry of its fossils is similar to that of fossilised vascular plants, rather than algae.\n\nThe first bony fish, the Osteichthyes, appeared, represented by the Acanthodians covered with bony scales; fish reached considerable diversity and developed movable jaws, adapted from the supports of the front two or three gill arches. A diverse fauna of eurypterids (sea scorpions)—some of them several meters in length—prowled the shallow Silurian seas of North America; many of their fossils have been found in New York state. Leeches also made their appearance during the Silurian Period. Brachiopods, bryozoa, molluscs, hederelloids, tentaculitoids, crinoids and trilobites were abundant and diverse. Endobiotic symbionts were common in the corals and stromatoporoids.\n\nReef abundance was patchy; sometimes fossils are frequent but at other points are virtually absent from the rock record.\n\nThe earliest-known animals fully adapted to terrestrial conditions appear during the Mid-Silurian, including the millipede \"Pneumodesmus\". Some evidence also suggests the presence of predatory trigonotarbid arachnoids and myriapods in Late Silurian facies. Predatory invertebrates would indicate that simple food webs were in place that included non-predatory prey animals. Extrapolating back from Early Devonian biota, Andrew Jeram \"et al.\" in 1990 suggested a food web based on as-yet-undiscovered detritivores and grazers on micro-organisms.\n\n\n"}
{"id": "233636", "url": "https://en.wikipedia.org/wiki?curid=233636", "title": "Spherical Earth", "text": "Spherical Earth\n\nThe earliest reliably documented mention of the spherical Earth concept dates from around the 6th century BC when it appeared in ancient Greek philosophy but remained a matter of speculation until the 3rd century BC, when Hellenistic astronomy established the spherical shape of the Earth as a physical given and calculated Earth's circumference. The paradigm was gradually adopted throughout the Old World during Late Antiquity and the Middle Ages. A practical demonstration of Earth's sphericity was achieved by Ferdinand Magellan and Juan Sebastián Elcano's expedition's circumnavigation (1519–1522).\n\nThe concept of a spherical Earth displaced earlier beliefs in a flat Earth: In early Mesopotamian mythology, the world was portrayed as a flat disk floating in the ocean with a hemispherical sky-dome above, and this forms the premise for early world maps like those of Anaximander and Hecataeus of Miletus. Other speculations on the shape of Earth include a seven-layered ziggurat or cosmic mountain, alluded to in the Avesta and ancient Persian writings (see seven climes).\n\nThe realization that the figure of the Earth is more accurately described as an ellipsoid dates to the 17th century, as described by Isaac Newton in \"Principia\". In the early 19th century, the flattening of the earth ellipsoid was determined to be of the order of 1/300 (Delambre, Everest). The modern value as determined by the US DoD World Geodetic System since the 1960s is close to 1/298.25.\n\nThe Earth is massive enough that gravity maintains it as a roughly spherical shape. Its formation into a sphere was made easy by its primordial hot, liquid phase.\n\nThe Solar System formed from a dust cloud that was at least partially the remnant of one or more supernovas that created heavy elements by nucleosynthesis. Grains of matter accreted through electrostatic interaction. As they grew in mass, gravity took over in gathering yet more mass, releasing the potential energy of their collisions and in-falling as heat. The protoplanetary disk also had a greater proportion of radioactive elements than the Earth today because, over time, those elements decayed. Their decay heated the early Earth even further, and continue to contribute to Earth's internal heat budget. The early Earth was thus mostly liquid.\n\nA sphere is the only stable shape for a non-rotating, gravitationally self-attracting liquid. The outward acceleration caused by the Earth's rotation is greater at the equator than at the poles (where is it zero), so the sphere gets deformed into an ellipsoid, which represents the shape having the lowest potential energy for a rotating, fluid body. This ellipsoid is slightly fatter around the equator than a perfect sphere would be. Earth's shape is also slightly lumpy because it is composed of different materials of different densities that exert slightly different amounts of gravitational force per volume.\n\nThe liquidity of a hot, newly formed planet allows heavier elements to sink down to the middle and forces lighter elements closer to the surface, a process known as planetary differentiation. This event is known as the iron catastrophe; the most abundant heavier elements were iron and nickel, which now form the Earth's core.\n\nThough the surface rocks of the Earth have cooled enough to solidify, the outer core of the planet is still hot enough to remain liquid. Energy is still being released; volcanic and tectonic activity has pushed rocks into hills and mountains and blown them out of calderas. Meteors also create impact craters and surrounding ridges. However, if the energy release ceases from these processes, then they tend to erode away over time and return toward the lowest potential-energy curve of the ellipsoid. Weather powered by solar energy can also move water, rock, and soil to make the Earth slightly out of round.\n\nEarth undulates as the shape of its lowest potential energy changes daily due to the gravity of the Sun and Moon as they move around with respect to the Earth. This is what causes tides in the oceans' water, which can flow freely along the changing potential.\n\nThe IAU definitions of planet and dwarf planet require that a Sun-orbiting body has undergone the rounding process to reach a roughly spherical shape, an achievement known as hydrostatic equilibrium. The same spheroidal shape can be seen from smaller rocky planets like Mars to gas giants like Jupiter.\n\nAny natural Sun-orbiting body that has not reached hydrostatic equilibrium is classified by the IAU as a small Solar System body (SSB). These come in many non-spherical shapes which are lumpy masses accreted haphazardly by in-falling dust and rock; not enough mass falls in to generate the heat needed to complete the rounding. Some SSSBs are just collections of relatively small rocks that are weakly held next to each other by gravity but are not actually fused into a single big bedrock. Some larger SSSBs are nearly round but have not reached hydrostatic equilibrium. The small Solar System body 4 Vesta is large enough to have undergone at least partial planetary differentiation.\n\nStars like the Sun are also spheroidal due to gravity's effects on their plasma, which is a free-flowing fluid. Ongoing stellar fusion is a much greater source of heat for stars compared to the initial heat released during formation.\n\nThe roughly spherical shape of the Earth can be confirmed by many different types of observation from ground level, aircraft, and spacecraft. The shape causes a number of phenomena that a flat Earth would not. Some of these phenomena and observations would be possible on other shapes, such as a curved disc or torus, but no other shape would explain all of them.\n\nMany pictures have been taken of the entire Earth by satellites launched by a variety of governments and private organizations. From high orbits, where half the planet can be seen at once, it is plainly spherical. The only way to piece together all the pictures taken of the ground from lower orbits so that all the surface features line up seamlessly and without distortion is to put them on an approximately spherical surface.\n\nAstronauts in low Earth orbit can personally see the curvature of the planet, and travel all the way around several times a day.\n\nThe astronauts who travelled to the Moon have seen the entire Moon-facing half at once, and can watch the sphere rotate once a day (approximately; the Moon is also moving with respect to the Earth).\n\nPeople in high-flying aircraft or skydiving from high-altitude balloons can plainly see the curvature of the Earth. Commercial aircraft do not necessarily fly high enough to make this obvious. Trying to measure the curvature of the horizon by taking a picture is complicated by the fact that camera lenses can produce distorted images depending on the angle used. An extreme version of this effect can be seen in the fisheye lens. Scientific measurements would require a carefully calibrated lens.\n\nThe fastest way for an airplane to travel between two distant cities is a great circle route, which deviates significantly from what would be the fastest straight-line travel path on a flat Earth.\n\nPhotos of the ground taken from airplanes over a large enough area also do not fit seamlessly together on a flat surface, but do fit on a roughly spherical surface. Aerial photographs of large areas must be corrected to account for curvature.\n\nOn a completely flat Earth with no visual interference (such as trees, hills, or atmospheric haze) the ground itself would never obscure distant objects; one would be able to see all the way to the edge of the surface. A spherical surface has a horizon which is closer when viewed from a lower altitude. In theory, a person standing on the surface with eyes above the ground can see the ground up to about away, but a person at the top of the Eiffel Tower at can see the ground up to about away.\n\nThis phenomenon would seem to present a method to verify that the Earth's surface is locally convex. If the degree of curvature was determined to be the same everywhere on the Earth's surface, and that surface was determined to be large enough, it would show that the Earth is spherical.\n\nIn practice, this turns out to be an unreliable method of measurement, due to variations in atmospheric refraction. This additional effect can give the impression that the earth's surface is flat, curved more convexly than it is, or even that it is concave, by bending light travelling near the surface of the earth (as happened in various trials of the famous Bedford Level experiment).\n\nThe phenomenon of variable atmospheric bending can be empirically confirmed by noting that sometimes the refractive layers of air can cause the image of a distant object to be broken into pieces or even turned upside down. This is commonly seen at sunset, when the sun's shape is distorted, but has also been photographed happening for ships, and has caused the city of Chicago to appear normally, upside down, and broken into pieces from across Lake Michigan (from where it is normally below the horizon). Because of their longer wavelengths, radio waves are even more susceptible to atmospheric refraction and reflection, which can cause radio and television signals to be received from towers thousands of miles away which cannot be seen with visible light.\n\nWhen the atmosphere is relatively well-mixed, the visual effects generally expected of a spherical Earth can be observed. For example, ships travelling on large bodies of water (such as the ocean) disappear over the horizon progressively, such that the highest part of the ship can still be seen even when lower parts cannot, proportional to distance from the observer. The same is true of the coastline or mountain when viewed from a ship or from across a large lake or flat terrain.\n\nThe shadow of the Earth on the Moon during a lunar eclipse is always a dark circle that moves from one side of the moon to the other (partially grazing it during a partial eclipse). This could be produced by a flat disc that always faces the Moon head-on during the eclipse, but this is inconsistent with the fact that the Moon is only rarely directly overhead during an eclipse. For each eclipse, the local surface of the Earth is pointed in a somewhat different direction. The shadow of a circular disc held at an angle is an oval, not a circle as is seen during the eclipse. The idea of the Earth being a flat disc is also inconsistent with the fact that a given lunar eclipse is only visible from half of the Earth at a time.\n\nThe only shape that casts a round shadow no matter which direction it is pointed is a sphere, and the ancient Greeks deduced that this must mean the Earth is spherical.\n\nOn a perfectly spherical Earth, flat terrain or ocean, when viewed from the surface, blocks exactly half the sky - a hemisphere of 180°. Moving away from the surface of the Earth means that the ground blocks less and less of the sky. For example, when viewed from the Moon, the Earth blocks only a small portion of the sky, because it is so distant. This phenomenon of geometry means that when viewed from a high mountain, flat ground or ocean blocks less than 180° of the sky. The rate of change in the angle blocked by the sky as altitude increases is different for a disc than a sphere, and values observed show that the Earth is locally convex. (The angles blocked would also be different for a mountain close to the edge of a flat Earth compared to a mountain in the middle of a flat Earth, and this is not observed.) In theory, measurements of this type from all around the Earth would confirm that it is a complete sphere (as opposed to some other shape with convex areas) though actually taking all those measurements would be very expensive.\n\nUsing other evidence to hypothesize a spherical shape, the medieval Iranian scholar Al-Biruni used this phenomenon to calculate the Earth's circumference to within of the correct value.\n\nThe fixed stars can be demonstrated to be very far away, by diurnal parallax measurements (a technique known at least as early as Ancient Greece). Unlike the Sun, Moon, and planets, they do not change position with respect to one another (at least not perceptibly over the span of a human lifetime); the shapes of the constellations are always the same. This makes them a convenient reference background for determining the shape of the Earth. Adding distance measurements on the ground allows calculation of the Earth's size.\n\nThe fact that different stars are visible from different locations on the Earth was noticed in ancient times. Aristotle wrote that some stars are visible from Egypt which are not visible from Europe. This would not be possible if the Earth was flat.\n\nAt the North Pole it is continuously nighttime for six months of the year and the same hemisphere of stars (a 180° view) are always visible making one counterclockwise rotation every 24 hours. The star Polaris (the \"North Star\") is almost at the center of this rotation (which is directly overhead). Some of the 88 modern constellations visible are Ursa Major (including the Big Dipper), Cassiopeia, and Andromeda. The other six months of the year, it is continuously daytime and the light from the Sun mostly blots out the stars. (The location of the poles can be defined by these phenomena, which only occur there; more than 24 hours of continuous daylight can occur north of the Arctic Circle and south of the Antarctic Circle.)\n\nAt the South Pole, a completely non-overlapping set of constellations are visible during the six months of continuous nighttime, including Orion, Crux, and Centaurus. This 180° hemisphere of stars rotate clockwise once every 24 hours, around a point directly overhead (where there do not happen to be any particularly bright stars).\n\nThe fact that the stars visible from the north and south poles do not overlap must mean that the two observation spots are on opposite sides of the Earth, which is not possible if the Earth is a single-sided disc, but is possible for other shapes (like a sphere, but also any other convex shape like a donut or dumbbell).\n\nFrom any point on the equator, 360° of stars are visible over the course of the night, as the sky rotates around a line drawn from due north to due south (which could be defined as \"the directions to walk to get to the poles in the shortest amount of time\"). When facing east, the stars visible from the north pole are on the left, and the stars visible from the south pole are on the right. This means the equator must be facing at a 90° angle from the poles.\n\nThe direction any intermediate spot on the Earth is facing can also be calculated by measuring the angles of the fixed stars and determining how much of the sky is visible. For example, New York City is about 40° north of the equator. The apparent motion of the Sun blots out slightly different parts of the sky from day to day, but over the course of the entire year it sees a dome of 280° (360° - 80°). So for example, both Orion and the Big Dipper are visible during at least part of the year.\n\nMaking stellar observations from a representative set of points across the Earth, combined with knowing the shortest on-the-ground distance between any two given points makes an approximate sphere the only possible shape for the Earth.\n\nKnowing the difference in angle between two points on the Earth's surface and the surface distance between them allows a calculation of the Earth's size. Using observations at Rhodes (in Greece) and Alexandria (in Egypt) and the distance between them, the Ancient Greek philosopher Posidonius actually did use this technique to calculate the circumference of the planet to within perhaps 4% of the correct value (though modern equivalents of his units of measure are not precisely known).\n\nSince the 1500s, many people have sailed or flown completely around the world in all directions, and none have discovered an edge or impenetrable barrier. (See Circumnavigation, Arctic exploration, and History of Antarctica.)\n\nSome flat Earth theories that propose the world is a north-pole-centered disc, conceive of Antarctica as an impenetrable ice wall that encircles the planet and hides any edges. This disc model explains east-west circumnavigation as simply moving around the disc in a circle. (East-west paths form a circle in both disc and spherical geometry.) It is possible in this model to traverse the North Pole, but it is not possible to perform a circumnavigation that includes the South Pole (which it posits does not exist).\n\nExplorers, government researchers, commercial pilots, and tourists have been to Antarctica and found that it is not a large ring that encircles the entire world, but actually a roughly disc-shaped continent smaller than South America but larger than Australia, with an interior that can in fact be traversed in order to take a shorter path from e.g. the tip of South America to Australia than would be possible on a disc.\n\nThe first land crossing of the entirety of Antarctica was the Commonwealth Trans-Antarctic Expedition in 1955-58, and many exploratory airplanes have since passed over the continent in various directions.\n\nOn a flat Earth, an omnidirectional Sun (emitting light in all directions, as it does) would illuminate the entire surface at the same time, and all places would experience sunrise and sunset at the horizon at the same time (with some small variations due to mountains and valleys). With a spherical Earth, half the planet is in daylight at any given time (the hemisphere facing the Sun) and the other half is experiencing nighttime. When a given location on the spherical Earth is in sunlight, its antipode - the location exactly on the opposite side of the Earth - is always experiencing nighttime. The spherical shape of the Earth causes the Sun to rise and set at different times in different places, and different locations get different amounts of sunlight each day. \n\nIn order to explain day and night, time zones, and the seasons, some flat Earth theorists propose that the Sun does not emit light in all directions, but acts more like a spotlight, only illuminating part of the flat Earth at a time. This theory is not consistent with observation; at sunrise and sunset, a spotlight Sun would be up in the sky at least a little bit, rather than at the horizon where it is always actually observed. A spotlight Sun would also appear at different angles in the sky with respect to a flat ground than it does with respect to a curved ground. Assuming light travels in straight lines, actual measurements of the Sun's angle in the sky from locations very distant from each other are only consistent with a geometry where the Sun is very far away and is being seen from a hemispherical surface (the daylight half of the Earth). These two phenomena are related: a low-altitude spotlight Sun would spent most of the day near the horizon for most locations on Earth (which is not observed), but rise and set fairly close to the horizon. A high-altitude Sun would spend more of the day away from the horizon, but rise and set fairly far from the horizon (which is not observed).\n\nAncient timekeeping reckoned \"noon\" as the time of day when the sun is highest in the sky, with the rest of the hours in the day measured against that. During the day, the apparent solar time can be measured directly with a sundial. In ancient Egypt, the first known sundials divided the day into 12 hours, though because the length of the day changed with the season, the length of the hours also changed. Sundials that defined hours as always being the same duration appeared in the Renaissance. In Western Europe, clock towers and striking clocks were used in the Middle Ages to keep people nearby appraised of the local time, though compared to modern times this was less important in a largely agrarian society.\n\nBecause the Sun reaches its highest point at different times for different longitudes (about four minutes of time for every degree of longitude difference east or west), the local solar noon in each city is different except for those directly north or south of each other. This means that the clocks in different cities could be offset from each other by minutes or hours. As clocks became more precise and industrialization made timekeeping more important, cities switched to mean solar time, which ignores minor variations in the timing of local solar noon over the year, due to the elliptical nature of the Earth's orbit, and its tilt.\n\nThe differences in clock time between cities was not generally a problem until the advent of railroad travel in the 1800s, which both made travel between distant cities much faster than by walking or horse, and also required passengers to show up at specific times to meet their desired trains. In the United Kingdom, railroads gradually switched to Greenwich Mean Time (set from local time at the Greenwich observatory in London), followed by public clocks across the country generally, forming a single time zone. In the United States, railroads published schedules based on local time, then later based on standard time for that railroad (typically the local time at the railroad's headquarters), and then finally based on four standard time zones shared across all railroads, where neighboring zones differed by exactly one hour. At first railroad time was synchronized by portable chronometers, and then later by telegraph and radio signals.\n\nSan Francisco is at 122.41°W longitude and Richmond, Virginia is at 77.46°W longitude. They are both at about 37.6°N latitude (±.2°). The approximately 45° of longitude difference translates into about 180 minutes, or 3 hours, of time between sunsets in the two cities, for example. San Francisco is in the Pacific Time zone, and Richmond is in the Eastern Time zone, which are three hours apart, so the local clocks in each city show that the sun sets at about the same time when using the local time zone. But a phone call from Richmond to San Francisco at sunset will reveal that there are still three hours of daylight left in California.\n\nOn a flat Earth with an omnidirectional Sun, all places would experience the same amount of daylight every day, and all places would get daylight at the same time. Actual day length varies considerably, with places closer to the poles getting very long days in the summer and very short days in the winter, with northerly summer happening at the same time as southerly winter. Places north of the Arctic Circle and south of the Antarctic Circle get no sunlight for at least one day a year, and get 24-hour sunlight for at least one day a year. Both the poles experience sunlight for 6 months and darkness for 6 months, at opposite times.\n\nThe movement of daylight between the northern and southern hemispheres happens because of the axial tilt of the Earth. The imaginary line around which the Earth spins, which goes between the North Pole and South Pole, is tilted about 23° from the oval that describes its orbit around the Sun. The Earth always points in the same direction as it moves around the Sun, so for half the year (summer in the Northern Hemisphere), the North Pole is pointed slightly toward the Sun, keeping it in daylight all the time because the Sun lights up the half of the Earth that is facing it (and the North Pole is always in that half due to the tilt). For the other half of the orbit, the South Pole is tilted slightly toward the Sun, and it is winter in the Northern Hemisphere. This means that at the equator, the Sun is not directly overhead at noon, except around the autumnal equinox and vernal equinox, when one spot on the equator is pointed directly at the Sun.\n\nThe length of the day varies because as the Earth rotates some places (near the poles) pass through only a short curve near the top or bottom of the sunlight half; other places (near the equator) travel along much longer curves through the middle.\n\nThe length of twilight would be very different on a flat Earth. On a round Earth, the atmosphere above the ground is lit for a while before sunrise and after sunset are observed at ground level, because the Sun is still visible from higher altitudes. Longer twilights are observed at higher latitudes (near the poles) due to a shallower angle of the Sun's apparent movement compared to the horizon. On a flat Earth, the Sun's shadow would reach the upper atmosphere very quickly, except near the closest edge of the Earth, and would always set at the same angle to the ground (which is not what is observed). The \"spotlight Sun\" theory is also not consistent with this observation, since the air cannot be lit without the ground below it also being lit (except for shadows of mountains and other surface obstacles).\n\nOn a given day, if many different cities measure the angle of the Sun at local noon, the resulting data, when combined with the known distances between cities, shows that the Earth has 180 degrees of north-south curvature. (A full range of angles will be observed if the north and south poles are included, and the day chosen is either the autumnal or spring equinox.) This is consistent with many rounded shapes, including a sphere, and is inconsistent with a flat shape.\n\nSome claim that this experiment assumes a very distant Sun, such that the incoming rays are essentially parallel, and if a flat Earth is assumed, that the measured angles can allow one to calculate the distance to the Sun, which must be small enough that its incoming rays are not very parallel. However, if more than two relatively well-separated cities are included in the experiment, the calculation will make clear whether the Sun is distant or nearby. For example, on the equinox, the 0 degree angle from the North Pole and the 90 degree angle from the equator predict a Sun which would have to be located essentially next to the surface of a flat Earth, but the difference in angle between the equator and New York City would predict a Sun much further away if the Earth is flat. Because these results are contradictory, the surface of the Earth cannot be flat; the data \"is\" consistent with a nearly spherical Earth and a Sun which is very far away compared with the diameter of the Earth.\n\nUsing the knowledge that the Sun is very far away, the ancient Greek geographer Eratosthenes performed an experiment using the differences in the observed angle of the Sun from two different locations to calculate the circumference of the Earth. Though modern telecommunications and timekeeping were not available, he was able to make sure the measurements happened at the same time by having them taken when the Sun was highest in the sky (local noon) at both locations. Using slightly inaccurate assumptions about the locations of two cities, he came to a result within 15% of the correct value.\n\nOn level ground, the difference in the distance to the horizon between lying down and standing up is large enough to watch the Sun set twice by quickly standing up immediately after seeing it set for the first time while lying down. This also can be done with a cherry picker or a tall building with a fast elevator. On a flat Earth, one would not be able to see the Sun again (unless standing near the edge closest to the Sun) due to a much faster-moving Sun shadow. \n\nWhen the supersonic Concorde took off not long after sunset from London and flew westward to New York faster than the sunset was advancing westward on the ground, passengers observed a sunrise in the west. After landing in New York, passengers saw a second sunset in the west.\n\nBecause the speed of the Sun's shadow is slower in polar regions (due to the steeper angle), even a subsonic aircraft can overtake the sunset when flying at high latitudes. One photographer used a roughly circular route around the North Pole to take pictures of 24 sunsets in the same 24-hour period, pausing westward progress in each time zone to let the shadow of the Sun catch up. The surface of the Earth rotates at at 80° north or south, and at the equator.\n\nBecause the Earth is spherical, long-distance travel sometimes requires heading in different directions than one would head on a flat Earth.\n\nFor example, consider an airplane that travels in a straight line, takes a 90-degree right turn, travels another , takes another 90-degree right turn, and travels a third time. On a flat Earth, the aircraft would have travelled along three sides of a square, and arrive at a spot about from where it started. But because the Earth is spherical, in reality it will have travelled along three sides of a triangle, and arrive back very close to its starting point. If the starting point is the North Pole, it would have travelled due south from the North Pole to the equator, then west for a quarter of the way around the Earth, and then due north back to the North Pole.\n\nIn spherical geometry, the sum of angles inside a triangle is greater than 180° (in this example 270°, having arrived back at the north pole a 90° angle to the departure path) unlike on a flat surface, where it is always exactly 180°.\n\nA meridian of longitude is a line where local solar noon occurs at the same time each day. These lines define \"north\" and \"south\". These are perpendicular to lines of latitude that define \"east\" and \"west\", where the Sun is at the same angle at local noon on the same day. If the Sun were travelling from east to west over a flat Earth, meridian lines would always be the same distance apart - they would form a square grid when combined with lines of latitude. In reality, meridian lines get farther apart as one travels toward the equator, which is only possible on a round Earth. In places where land is plotted on a grid system, this causes discontinuities in the grid. For example, in areas of the Midwestern United States that use the Public Land Survey System, the northernmost and westernmost sections of a township deviate from what would otherwise be an exact square mile. The resulting discontinuities are sometimes reflected directly in local roads, which have kinks where the grid cannot follow completely straight lines.\n\nLow-pressure weather systems with inward winds (such as a hurricane) spin counterclockwise north of the equator, but clockwise south of the equator. This is due to the Coriolis force, and requires that (assuming they are attached to each other and rotating in the same direction) the north and southern halves of the Earth are angled in opposite directions (e.g. the north is facing toward Polaris and the south is facing away from it).\n\nThe laws of gravity, chemistry, and physics that explain the formation and rounding of the Earth are well-tested through experiment, and applied successfully to many engineering tasks.\n\nFrom these laws, we know the amount of mass the Earth contains, and that a non-spherical planet the size of the Earth would not be able to support itself against its own gravity. A flat disc the size of the Earth, for example, would likely crack, heat up, liquefy, and re-form into a roughly spherical shape. On a disc strong enough to maintain its shape, gravity would not pull downward with respect to the surface, but would pull toward the center of the disc, contrary to what is observed on level terrain (and which would create major problems with oceans flowing toward the center of the disk).\n\nIgnoring the other concerns, some flat Earth theorists explain the observed surface \"gravity\" by proposing that the flat Earth is constantly accelerating upwards. Such a theory would also leave open for explanation the tides seen in Earth's oceans, which are conventionally explained by the gravity exerted by the Sun and Moon.\n\nObservation of Foucault pendulums, popular in science museums around the world, demonstrate both that the world is spherical and that it rotates (not that the stars are rotating around it).\n\nThe mathematics of navigation by GPS assume that satellites are moving in known orbits around an approximately spherical surface. The accuracy of GPS navigation in determining latitude and longitude and the way these numbers map onto locations on the ground show that these assumptions are correct. The same is true for the operational GLONASS system run by Russia, and the in-development European Galileo, Chinese BeiDou, and Indian IRNSS.\n\nSatellites, including communications satellites used for television, telephone, and Internet connections, would not stay in orbit unless the modern theory of gravitation were correct. The details of which satellites are visible from which places on the ground at which times prove an approximately spherical shape of the Earth. (Undersea cables are also used for intercontinental communications.)\n\nRadio transmitters are mounted on tall towers because they generally rely on line-of-sight propagation. The distance to the horizon is further at higher altitude, so mounting them higher significantly increases the area they can serve. Some signals can be transmitted at much longer distances, but only if they are at frequencies where they can use groundwave propagation, tropospheric propagation, tropospheric scatter, or ionospheric propagation to reflect or refract signals around the curve of the Earth.\n\nThe design of some large structures needs to take the shape of the Earth into account. For example, the towers of the Humber Bridge, although both vertical with respect to gravity, are farther apart at the top than the bottom due to the local curvature.\n\nThe Hebrew Bible imagined a three-part world, with the heavens (\"shamayim\") above, earth (\"eres\") in the middle, and the underworld (\"sheol\") below. After the 4th century BCE this was gradually replaced by a Greek scientific cosmology of a spherical earth surrounded by multiple concentric heavens.\n\nThough the earliest written mention of a spherical Earth comes from ancient Greek sources, there is no account of how the sphericity of the Earth was discovered. A plausible explanation is that it was \"the experience of travellers that suggested such an explanation for the variation in the observable altitude of the pole and the change in the area of circumpolar stars, a change that was quite drastic between Greek settlements\" around the eastern Mediterranean Sea, particularly those between the Nile Delta and Crimea.\n\nIn \"The Histories\", written 431–425 BC, Herodotus cast doubt on a report of the Sun observed shining from the north. He stated that the phenomenon was observed during a circumnavigation of Africa undertaken by Phoenician explorers employed by Egyptian pharaoh Necho II c. 610–595 BC (, 4.42) who claimed to have had the Sun on their right when circumnavigating in a clockwise direction. To modern historians, these details confirm the truth of the Phoenicians' report and even open the possibility that the Phoenicians knew about the spherical model. However, nothing certain about their knowledge of geography and navigation has survived.\n\nEarly Greek philosophers alluded to a spherical Earth, though with some ambiguity. Pythagoras (6th century BC) was among those said to have originated the idea, but this might reflect the ancient Greek practice of ascribing every discovery to one or another of their ancient wise men. Some idea of the sphericity of the Earth seems to have been known to both Parmenides and Empedocles in the 5th century BC, and although the idea cannot reliably be ascribed to Pythagoras, it might nevertheless have been formulated in the Pythagorean school in the 5th century BC although some disagree. After the 5th century BC, no Greek writer of repute thought the world was anything but round.\n\nPlato (427–347 BC) travelled to southern Italy to study Pythagorean mathematics. When he returned to Athens and established his school, Plato also taught his students that Earth was a sphere, though he offered no justifications. \"My conviction is that the Earth is a round body in the centre of the heavens, and therefore has no need of air or of any similar force to be a support\". If man could soar high above the clouds, Earth would resemble \"one of those balls which have leather coverings in twelve pieces, and is decked with various colours, of which the colours used by painters on Earth are in a manner samples.\"\nIn Timaeus, his one work that was available throughout the Middle Ages in Latin, we read that the Creator \"made the world in the form of a globe, round as from a lathe, having its extremes in every direction equidistant from the centre, the most perfect and the most like itself of all figures\", though the word \"world\" here refers to the heavens.\n\nAristotle (384–322 BC) was Plato's prize student and \"the mind of the school\". Aristotle observed \"there are stars seen in Egypt and [...] Cyprus which are not seen in the northerly regions.\" Since this could only happen on a curved surface, he too believed Earth was a sphere \"of no great size, for otherwise the effect of so slight a change of place would not be quickly apparent.\" (\"De caelo\", 298a2–10)\n\nAristotle provided physical and observational arguments supporting the idea of a spherical Earth:\n\n\nThe concepts of symmetry, equilibrium and cyclic repetition permeated Aristotle's work. In his \"Meteorology\" he divided the world into five climatic zones: two temperate areas separated by a torrid zone near the equator, and two cold inhospitable regions, \"one near our upper or northern pole and the other near the ... southern pole,\" both impenetrable and girdled with ice (\"Meteorologica\", 362a31–35). Although no humans could survive in the frigid zones, inhabitants in the southern temperate regions could exist.\n\nAristotle's theory of natural place relied on a spherical Earth to explain why heavy things go down (toward what Aristotle believed was the center of the Universe), and things like air and fire go up. In this geocentric model, the structure of the universe was believed to be a series of perfect spheres. The Sun, Moon, planets and fixed stars were believed to move on celestial spheres around a stationary Earth.\n\nThough Aristotle's theory of physics survived in the Christian world for many centuries, the heliocentric model was eventually shown to be a more correct explanation of the Solar System than the geocentric model, and atomic theory was shown to be a more correct explanation of the nature of matter than classical elements like earth, water, air, fire, and aether.\n\nIn proposition 2 of the First Book of his treatise \"On floating bodies,\" Archimedes demonstrates that \"The surface of any fluid at rest is the surface of a sphere whose centre is the same as that of the earth,\". Subsequently, in propositions 8 and 9 of the same work, he assumes the result of proposition 2 that the Earth is a sphere and that the surface of a fluid on it is a sphere centered on the center of the Earth.\n\nEratosthenes, a Greek astronomer from Hellenistic Cyrenaica (276–194 BC), estimated Earth's circumference around 240 BC. He had heard that in Syene the Sun was directly overhead at the summer solstice whereas in Alexandria it still cast a shadow. Using the differing angles the shadows made as the basis of his trigonometric calculations he estimated a circumference of around 250,000 \"stades\". The length of a 'stade' is not precisely known, but Eratosthenes's figure only has an error of around five to fifteen percent. Eratosthenes used rough estimates and round numbers, but depending on the length of the stadion, his result is within a margin of between 2% and 20% of the actual meridional circumference, . Note that Eratosthenes could only measure the circumference of the Earth by assuming that the distance to the Sun is so great that the rays of sunlight are practically parallel.\n\nSeventeen hundred years after Eratosthenes, Christopher Columbus studied Eratosthenes's findings before sailing west for the Indies. However, ultimately he rejected Eratosthenes in favour of other maps and arguments that interpreted Earth's circumference to be a third smaller than reality. If, instead, Columbus had accepted Eratosthenes findings, then he may have never gone west, since he didn't have the supplies or funding needed for the much longer voyage.\n\nSeleucus of Seleucia (c. 190 BC), who lived in the city of Seleucia in Mesopotamia, wrote that the Earth is spherical (and actually orbits the Sun, influenced by the heliocentric theory of Aristarchus of Samos).\n\nPosidonius (c. 135 – 51 BC) put faith in Eratosthenes's method, though by observing the star Canopus, rather than the sun in establishing the Earth's circumference. In Ptolemy's \"Geographia\", his result was favoured over that of Eratosthenes. Posidonius furthermore expressed the distance of the sun in earth radii.\n\nFrom its Greek origins, the idea of a spherical earth, along with much of Greek astronomical thought, slowly spread across the globe and ultimately became the adopted view in all major astronomical traditions.\n\nIn the West, the idea came to the Romans through the lengthy process of cross-fertilization with Hellenistic civilization. Many Roman authors such as Cicero and Pliny refer in their works to the rotundity of the earth as a matter of course.\n\nIt has been suggested that seafarers probably provided the first observational evidence that the Earth was not flat, based on observations of the horizon. This argument was put forward by the geographer Strabo (c. 64 BC – 24 AD), who suggested that the spherical shape of the Earth was probably known to seafarers around the Mediterranean Sea since at least the time of Homer, citing a line from the \"Odyssey\" as indicating that the poet Homer knew of this as early as the 7th or 8th century BC. Strabo cited various phenomena observed at sea as suggesting that the Earth was spherical. He observed that elevated lights or areas of land were visible to sailors at greater distances than those less elevated, and stated that the curvature of the sea was obviously responsible for this.\n\nClaudius Ptolemy (90–168 AD) lived in Alexandria, the centre of scholarship in the 2nd century. In the \"Almagest,\" which remained the standard work of astronomy for 1,400 years, he advanced many arguments for the spherical nature of the Earth. Among them was the observation that when a ship is sailing towards mountains, observers note these seem to rise from the sea, indicating that they were hidden by the curved surface of the sea. He also gives separate arguments that the Earth is curved north-south and that it is curved east-west.\n\nHe compiled an eight-volume \"Geographia\" covering what was known about the earth. The first part of the \"Geographia\" is a discussion of the data and of the methods he used. As with the model of the Solar System in the \"Almagest\", Ptolemy put all this information into a grand scheme. He assigned coordinates to all the places and geographic features he knew, in a grid that spanned the globe (although most of this has been lost). Latitude was measured from the equator, as it is today, but Ptolemy preferred to express it as the length of the longest day rather than degrees of arc (the length of the midsummer day increases from 12h to 24h as you go from the equator to the polar circle). He put the meridian of 0 longitude at the most western land he knew, the Canary Islands.\n\n\"Geographia\" indicated the countries of \"Serica\" and \"Sinae\" (China) at the extreme right, beyond the island of \"Taprobane\" (Sri Lanka, oversized) and the \"Aurea Chersonesus\" (Southeast Asian peninsula).\n\nPtolemy also devised and provided instructions on how to create maps both of the whole inhabited world (\"oikoumenè\") and of the Roman provinces. In the second part of the \"Geographia,\" he provided the necessary topographic lists, and captions for the maps. His \"oikoumenè\" spanned 180 degrees of longitude from the Canary Islands in the Atlantic Ocean to China, and about 81 degrees of latitude from the Arctic to the East Indies and deep into Africa. Ptolemy was well aware that he knew about only a quarter of the globe.\n\nKnowledge of the spherical shape of the Earth was received in scholarship of Late Antiquity as a matter of course, in both Neoplatonism and Early Christianity. Calcidius's fourth-century Latin commentary on and translation of Plato's \"Timaeus\", which was one of the few examples of Greek scientific thought that was known in the Early Middle Ages in Western Europe, discussed Hipparchus's use of the geometrical circumstances of eclipses to compute the relative diameters of the Sun, Earth, and Moon.\n\nTheological doubt informed by the flat Earth model implied in the Hebrew Bible inspired some early Christian scholars such as Lactantius, John Chrysostom and Athanasius of Alexandria, but this remained an eccentric current. Learned Christian authors such as Basil of Caesarea, Ambrose and Augustine of Hippo were clearly aware of the sphericity of the Earth. \"Flat Earthism\" lingered longest in Syriac Christianity, which tradition laid greater importance on a literalist interpretation of the Old Testament. Authors from that tradition, such as Cosmas Indicopleustes, presented the Earth as flat as late as in the 6th century. This last remnant of the ancient model of the cosmos disappeared during the 7th century. From the 8th century and the beginning medieval period, \"no cosmographer worthy of note has called into question the sphericity of the Earth.\"\n\nGreek ethnographer Megasthenes, c. 300 BC, has been interpreted as stating that the contemporary Brahmans believed in a spherical earth as the center of the universe. With the spread of Greek culture in the east, Hellenistic astronomy filtered eastwards to ancient India where its profound influence became apparent in the early centuries AD. The Greek concept of an Earth surrounded by the spheres of the planets and that of the fixed stars, vehemently supported by astronomers like Varahamihir and Brahmagupta, strengthened the astronomical principles. Some ideas were found possible to preserve, although in altered form.\n\nThe works of the classical Indian astronomer and mathematician, Aryabhatta (476–550 AD), deal with the sphericity of the Earth and the motion of the planets. The final two parts of his Sanskrit magnum opus, the \"Aryabhatiya\", which were named the \"Kalakriya\" (\"reckoning of time\") and the \"Gol\" (\"sphere\"), state that the Earth is spherical and that its circumference is 4,967 yojanas. In modern units this is , close to the current equatorial value of .\n\nKnowledge of the sphericity of the Earth survived into the medieval corpus of knowledge by direct transmission of the texts of Greek antiquity (Aristotle), and via authors such as Isidore of Seville and Beda Venerabilis.\nIt became increasingly traceable with the rise of scholasticism and medieval learning.\nSpread of this knowledge beyond the immediate sphere of Greco-Roman scholarship was necessarily gradual, associated with the pace of Christianisation of Europe. For example, the first evidence of knowledge of the spherical shape of the Earth in Scandinavia is a 12th-century Old Icelandic translation of \"Elucidarius\".\n\nA non-exhaustive list of more than a hundred Latin and vernacular writers from Late Antiquity and the Middle Ages who were aware that the earth was spherical has been compiled by Reinhard Krüger, professor for Romance literature at the University of Stuttgart.\nAmpelius, Chalcidius, Macrobius, Martianus Capella,\nBasil of Caesarea, Ambrose of Milan, Aurelius Augustinus, Paulus Orosius, Jordanes, Cassiodorus, Boethius, Visigoth king Sisebut.\n\nIsidore of Seville, Beda Venerabilis, Theodulf of Orléans, Vergilius of Salzburg,\nIrish monk Dicuil, Rabanus Maurus, King Alfred of England, Remigius of Auxerre, Johannes Scotus Eriugena, , Gerbert d’Aurillac (Pope Sylvester II).\n\nNotker the German of Sankt-Gallen, Hermann of Reichenau, Hildegard von Bingen, Petrus Abaelardus, Honorius Augustodunensis, Gautier de Metz, Adam of Bremen, Albertus Magnus, Thomas Aquinas, Berthold of Regensburg, Guillaume de Conches, , Abu-Idrisi, Bernardus Silvestris, Petrus Comestor, Thierry de Chartres, Gautier de Châtillon, Alexander Neckam, Alain de Lille, Averroes, Snorri Sturluson, Moshe ben Maimon, Lambert of Saint-Omer, Gervasius of Tilbury, Robert Grosseteste, Johannes de Sacrobosco, Thomas de Cantimpré, Peire de Corbian, Vincent de Beauvais, Robertus Anglicus, , Ristoro d'Arezzo, Roger Bacon, Jean de Meung, Brunetto Latini, Alfonso X of Castile.\n\nMarco Polo, Dante Alighieri, Meister Eckhart, Enea Silvio Piccolomini (Pope Pius II), Perot de Garbalei (\"divisiones mundi\"), Cecco d'Ascoli, , Levi ben Gershon, Konrad of Megenberg, Nicole Oresme, Petrus Aliacensis, , Toscanelli, , Jean de Mandeville, Christine de Pizan, Geoffrey Chaucer, William Caxton, Martin Behaim, Christopher Columbus.\n\nBishop Isidore of Seville (560–636) taught in his widely read encyclopedia, \"The Etymologies,\" that the Earth was \"round\". The bishop's confusing exposition and choice of imprecise Latin terms have divided scholarly opinion on whether he meant a sphere or a disk or even whether he meant anything specific. Notable recent scholars claim that he taught a spherical earth. Isidore did not admit the possibility of people dwelling at the antipodes, considering them as legendary and noting that there was no evidence for their existence.\n\n\nThe monk Bede (c. 672–735) wrote in his influential treatise on computus, \"The Reckoning of Time\", that the Earth was round. He explained the unequal length of daylight from \"the roundness of the Earth, for not without reason is it called 'the orb of the world' on the pages of Holy Scripture and of ordinary literature. It is, in fact, set like a sphere in the middle of the whole universe.\" (De temporum ratione, 32). The large number of surviving manuscripts of \"The Reckoning of Time,\" copied to meet the Carolingian requirement that all priests should study the computus, indicates that many, if not most, priests were exposed to the idea of the sphericity of the Earth. Ælfric of Eynsham paraphrased Bede into Old English, saying, \"Now the Earth's roundness and the Sun's orbit constitute the obstacle to the day's being equally long in every land.\"\n\nBede was lucid about earth's sphericity, writing \"We call the earth a globe, not as if the shape of a sphere were expressed in the diversity of plains and mountains, but because, if all things are included in the outline, the earth's circumference will represent the figure of a perfect globe... For truly it is an orb placed in the centre of the universe; in its width it is like a circle, and not circular like a shield but rather like a ball, and it extends from its centre with perfect roundness on all sides.\"\n\nThe 7th-century Armenian scholar Anania Shirakatsi described the world as \"being like an egg with a spherical yolk (the globe) surrounded by a layer of white (the atmosphere) and covered with a hard shell (the sky).\"\n\nIslamic astronomy was developed on the basis of a spherical earth inherited from Hellenistic astronomy. The Islamic theoretical framework largely relied on the fundamental contributions of Aristotle (\"De caelo\") and Ptolemy (\"Almagest\"), both of whom worked from the premise that the earth was spherical and at the centre of the universe (geocentric model).\n\nEarly Islamic scholars recognized Earth's sphericity, leading Muslim mathematicians to develop spherical trigonometry in order to further mensuration and to calculate the distance and direction from any given point on the Earth to Mecca. This determined the \"Qibla,\" or Muslim direction of prayer.\n\nAround 830 AD, Caliph al-Ma'mun commissioned a group of Muslim astronomers and geographers to measure the distance from Tadmur (Palmyra) to Raqqa in modern Syria. They found the cities to be separated by one degree of latitude and the meridian arc distance between them to be 66 miles and thus calculated the Earth's circumference to be 24,000 miles.\n\nAnother estimate given by his astronomers was 56 Arabic miles (111.8 km) per degree, which corresponds to a circumference of 40,248 km, very close to the currently modern values of 111.3 km per degree and 40,068 km circumference, respectively.\n\nAndalusian polymath Ibn Hazm stated that the proof of the Earth's sphericity \"is that the Sun is always vertical to a particular spot on Earth\".\n\nAl-Farghānī (Latinized as Alfraganus) was a Persian astronomer of the 9th century involved in measuring the diameter of the Earth, and commissioned by Al-Ma'mun. His estimate given above for a degree (56 Arabic miles) was much more accurate than the 60 Roman miles (89.7 km) given by Ptolemy. Christopher Columbus uncritically used Alfraganus's figure as if it were in Roman miles instead of in Arabic miles, in order to prove a smaller size of the Earth than that propounded by Ptolemy.\n\n\nAbu Rayhan Biruni (973–1048) used a new method to accurately compute the Earth's circumference, by which he arrived at a value that was close to modern values for the Earth's circumference. His estimate of 6,339.6 km for the Earth radius was only 31.4 km less than the modern mean value of 6,371.0 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, Biruni developed a new method of using trigonometric calculations based on the angle between a plain and mountain top. This yielded more accurate measurements of the Earth's circumference and made it possible for a single person to measure it from a single location.\nBiruni's method was intended to avoid \"walking across hot, dusty deserts,\" and the idea came to him when he was on top of a tall mountain in India. From the top of the mountain, he sighted the angle to the horizon which, along with the mountain's height (which he calculated beforehand), allowed him to calculate the curvature of the Earth.\nHe also made use of algebra to formulate trigonometric equations and used the astrolabe to measure angles.\n\nAccording to John J. O'Connor and Edmund F. Robertson,\n\nMuslim scholars who held to the round Earth theory used it for a quintessentially Islamic purpose: to calculate the distance and direction from any given point on the Earth to Mecca. This determined the Qibla, or Muslim direction of prayer.\n\nA terrestrial globe (Kura-i-ard) was among the presents sent by the Persian Muslim astronomer Jamal-al-Din to Kublai Khan's Chinese court in 1267. It was made of wood on which \"seven parts of water are represented in green, three parts of land in white, with rivers, lakes etc.\" Ho Peng Yoke remarks that \"it did not seem to have any general appeal to the Chinese in those days\".\n\nDuring the High Middle Ages, the astronomical knowledge in Christian Europe was extended beyond what was transmitted directly from ancient authors by transmission of learning from Medieval Islamic astronomy. An early student of such learning was Gerbert d'Aurillac, the later Pope Sylvester II.\n\nSaint Hildegard (Hildegard von Bingen, 1098–1179), depicted the spherical earth several times in her work \"Liber Divinorum Operum\".\n\nJohannes de Sacrobosco (c. 1195 – c. 1256 AD) wrote a famous work on Astronomy called \"Tractatus de Sphaera,\" based on Ptolemy, which primarily considers the sphere of the sky. However, it contains clear proofs of the earth's sphericity in the first chapter.\n\nMany scholastic commentators on Aristotle's \"On the Heavens\" and Sacrobosco's \"Treatise on the Sphere\" unanimously agreed that the earth is spherical or round. Grant observes that no author who had studied at a medieval university thought that the earth was flat.\n\nThe \"Elucidarium\" of Honorius Augustodunensis (c. 1120), an important manual for the instruction of lesser clergy, which was translated into Middle English, Old French, Middle High German, Old Russian, Middle Dutch, Old Norse, Icelandic, Spanish, and several Italian dialects, explicitly refers to a spherical Earth. Likewise, the fact that Bertold von Regensburg (mid-13th century) used the spherical Earth as an illustration in a sermon shows that he could assume this knowledge among his congregation. The sermon was preached in the vernacular German, and thus was not intended for a learned audience.\n\nDante's \"Divine Comedy,\" written in Italian in the early 14th century, portrays Earth as a sphere, discussing implications such as the different stars visible in the southern hemisphere, the altered position of the sun, and the various timezones of the Earth.\n\nThe Portuguese exploration of Africa and Asia, Columbus's voyage to the Americas (1492) and, finally, Ferdinand Magellan's circumnavigation of the earth (1519–21) provided practical evidence of the global shape of the earth.\n\nThe first direct demonstration of Earth's sphericity came in the form of the first circumnavigation in history, an expedition captained by Portuguese explorer Ferdinand Magellan. The expedition was financed by the Spanish Crown. On August 10, 1519, the five ships under Magellan's command departed from Seville. They crossed the Atlantic Ocean, passed through what is now called the Strait of Magellan, crossed the Pacific, and arrived in Cebu, where Magellan was killed by Philippine natives in a battle. His second in command, the Spaniard Juan Sebastián Elcano, continued the expedition and, on September 6, 1522, arrived at Seville, completing the circumnavigation. Charles I of Spain, in recognition of his feat, gave Elcano a coat of arms with the motto \"Primus circumdedisti me\" (in Latin, \"You went around me first\").\n\nA circumnavigation alone does not prove that the earth is spherical. It could be cylindric or irregularly globular or one of many other shapes. Still, combined with trigonometric evidence of the form used by Eratosthenes 1,700 years prior, the Magellan expedition removed any reasonable doubt in educated circles in Europe. The Transglobe Expedition (1979–1982) was the first expedition to make a circumpolar circumnavigation, traveling the world \"vertically\" traversing both of the poles of rotation using only surface transport.\n\nIn the 17th century, the idea of a spherical Earth, now considerably advanced by Western astronomy, ultimately spread to Ming China, when Jesuit missionaries, who held high positions as astronomers at the imperial court, successfully challenged the Chinese belief that the Earth was flat and square.\n\nThe \"Ge zhi cao\" (格致草) treatise of Xiong Mingyu (熊明遇) published in 1648 showed a printed picture of the Earth as a spherical globe, with the text stating that \"the round Earth certainly has no square corners\". The text also pointed out that sailing ships could return to their port of origin after circumnavigating the waters of the Earth.\n\nThe influence of the map is distinctly Western, as traditional maps of Chinese cartography held the graduation of the sphere at 365.25 degrees, while the Western graduation was of 360 degrees. Also of interest to note is on one side of the world, there is seen towering Chinese pagodas, while on the opposite side (upside-down) there were European cathedrals. The adoption of European astronomy, facilitated by the failure of indigenous astronomy to make progress, was accompanied by a sinocentric reinterpretation that declared the imported ideas Chinese in origin:\n\nEuropean astronomy was so much judged worth consideration that numerous Chinese authors developed the idea that the Chinese of antiquity had anticipated most of the novelties presented by the missionaries as European discoveries, for example, the rotundity of the Earth and the \"heavenly spherical star carrier model.\" Making skillful use of philology, these authors cleverly reinterpreted the greatest technical and literary works of Chinese antiquity. From this sprang a new science wholly dedicated to the demonstration of the Chinese origin of astronomy and more generally of all European science and technology.\n\nAlthough mainstream Chinese science until the 17th century held the view that the earth was flat, square, and enveloped by the celestial sphere, this idea was criticized by the Jin-dynasty scholar Yu Xi (fl. 307–345), who suggested that the Earth could be either square or round, in accordance with the shape of the heavens. The Yuan-dynasty mathematician Li Ye (c. 1192–1279) firmly argued that the Earth was spherical, just like the shape of the heavens only smaller, since a square Earth would hinder the movement of the heavens and celestial bodies in his estimation. The 17th-century \"Ge zhi cao\" treatise also used the same terminology to describe the shape of the Earth that the Eastern-Han scholar Zhang Heng (78–139 AD) had used to describe the shape of the sun and moon (i.e. that the former was as round as a crossbow bullet, and the latter was the shape of a ball).\n\nGeodesy, also called geodetics, is the scientific discipline that deals with the measurement and representation of the Earth, its gravitational field and geodynamic phenomena (polar motion, Earth tides, and crustal motion) in three-dimensional time-varying space.\n\nGeodesy is primarily concerned with positioning and the gravity field and geometrical aspects of their temporal variations, although it can also include the study of Earth's magnetic field. Especially in the German speaking world, geodesy is divided into geomensuration (\"Erdmessung\" or \"höhere Geodäsie\"), which is concerned with measuring the Earth on a global scale, and surveying (\"Ingenieurgeodäsie\"), which is concerned with measuring parts of the surface.\n\nThe Earth's shape can be thought of in at least two ways;\n\nAs the science of geodesy measured Earth more accurately, the shape of the geoid was first found not to be a perfect sphere but to approximate an oblate spheroid, a specific type of ellipsoid. More recent measurements have measured the geoid to unprecedented accuracy, revealing mass concentrations beneath Earth's surface.\n\n\n\n"}
{"id": "7208118", "url": "https://en.wikipedia.org/wiki?curid=7208118", "title": "Stellar mass loss", "text": "Stellar mass loss\n\nStellar mass loss is a phenomenon observed in some massive stars. It occurs when a triggering event causes the ejection of a large portion of the star's mass. Stellar mass loss can also occur when a star gradually loses material to a binary companion or into interstellar space. \n\nA number of factors can contribute to the loss of mass in giant stars, including:\n\nOften when a star is a member of a pair of close-orbiting binary stars, the tidal attraction of the gasses near the center of mass are sufficient to pull gas from one star onto its partner. This effect is especially prominent when the partner is a white dwarf, neutron star, or black hole.\n\nCertain classes of stars, especially Wolf-Rayet stars are sufficiently massive and distended that their hold on their upper layers is rather weak. Often, events such as solar flares and coronal mass ejections will then be sufficiently powerful to blast some of the upper material into space.\n\nStars which have entered the red giant phase are notorious for rapid mass loss. As above, the gravitational hold on the upper layers is weakened, and they may be shed into space by violent events such as the beginning of a helium flash in the core. The final stage of a red giant's life will also result in prodigious mass loss as the star loses its outer layers to form a planetary nebula.\n\n\n\n"}
{"id": "708955", "url": "https://en.wikipedia.org/wiki?curid=708955", "title": "Triglav (mythology)", "text": "Triglav (mythology)\n\nTriglav (; ), also sometimes called \"Troglav\", is a deity in Slavic theology.\n\nMount Triglav, the highest mountain in Slovenia, is unlikely to have any connection to the Slavic deity Triglav. However, such a claim is made for Mount Troglav, the highest peak of Dinara in Bosnia and Herzegovina.\n\nTriglav is allegedly depicted as representation of three major Slavic gods that vary from one Slavic tribe to others that serve as the representatives of the above-mentioned realms. An early variation included Svarog, Perun, and Dazhbog. Later, Dazhbog was replaced by Svetovid or Veles. Triglav is usually described as a fusion of these gods.\nTriglav is depicted as a three-headed man sometimes with bands of (gold) blindfolds over his eyes, or a man with three goat heads. Several temples dedicated to Triglav existed near present-day Szczecin, Poland. The Polish village of Trzygłów () is where, according to Johannes Bugenhagen in his \"Pomerania\" (1517) an image of Triglav was buried in the village to protect it from the priests of Otto of Bamberg in the 1120's. During the period of Christianization, such temples and statues of Triglav were generally completely destroyed.\n\nAccording to the writing of Ebbo, Triglav's heads were believed to represent sky, earth and the underworld. It was believed that Triglav has three heads because he ruled these three kingdoms and had a golden binding over his eyes and lips so he could not see people's sins nor speak about them.\n\n"}
{"id": "31036", "url": "https://en.wikipedia.org/wiki?curid=31036", "title": "Triode", "text": "Triode\n\nA triode is an electronic amplifying vacuum tube (or \"thermionic valve\" in British English) consisting of three electrodes inside an evacuated glass envelope: a heated filament or cathode, a grid, and a plate (anode). Developed from Lee De Forest's 1906 Audion, a partial vacuum tube that added a grid electrode to the thermionic diode (Fleming valve), the triode was the first practical electronic amplifier and the ancestor of other types of vacuum tubes such as the tetrode and pentode. Its invention founded the electronics age, making possible amplified radio technology and long-distance telephony. Triodes were widely used in consumer electronics devices such as radios and televisions until the 1970s, when transistors replaced them. Today, their main remaining use is in high-power RF amplifiers in radio transmitters and industrial RF heating devices. In recent years there has been a resurgence in demand for low power triodes due to renewed interest in tube-type audio systems by audiophiles who prefer the sound of tube-based electronics.\n\nThe name \"triode\" was coined by British physicist William Eccles sometime around 1920, derived from the Greek τρίοδος, \"tríodos\", from \"tri-\" (three) and \"hodós\" (road, way), originally meaning the place where three roads meet.\n\nBefore thermionic valves were invented, Philipp Lenard used the principle of grid control while conducting photoelectric experiments in 1902.\n\nThe first vacuum tube used in radio was the thermionic diode or Fleming valve, invented by John Ambrose Fleming in 1904 as a detector for radio receivers. It was an evacuated glass bulb containing two electrodes, a heated filament and a plate (anode).\n\nTriodes came about in 1906 when American engineer Lee De Forest and Austrian physicist Robert von Lieben independently patented tubes that added a third electrode, a grid, between the filament and plate to control current. Von Lieben's partially-evacuated three-element tube, patented in March 1906, contained a trace of mercury vapor and was intended to amplify weak telephone signals. Starting in October 1906 De Forest patented a number of three-element tube designs by adding an electrode to the diode, which he called Audions, intended to be used as radio detectors. The one which became the design of the triode, in which the grid was located between the filament and plate, was patented January 29, 1907. Like the von Lieben vacuum tube, De Forest's Audions were incompletely evacuated and contained some gas at low pressure. Von Lieben's tube did not see much development due to his death at the outbreak World War I, 7 years after he invented it.\n\nDe Forest's Audion did not see much use until its ability to amplify was recognized around 1912 by several researchers, who used it to build the first successful amplifying radio receivers and electronic oscillators. The many uses for amplification motivated its rapid development. By 1913 improved versions with higher vacuum were developed by Harold Arnold at American Telephone and Telegraph Company, which had purchased the rights to the Audion from De Forest, and Irving Langmuir at General Electric, who named his tube the \"Pliotron\", These were the first vacuum tube triodes. The name \"triode\" appeared later, when it became necessary to distinguish it from other kinds of vacuum tubes with more or fewer elements (e.g. diodes, tetrodes, pentodes, etc.). There were lengthy lawsuits between De Forest and von Lieben, and De Forest and the Marconi Company, who represented John Ambrose Fleming, the inventor of the diode.\n\nThe discovery of the triode's amplifying ability in 1912 revolutionized electrical technology, creating the new field of \"electronics\", the technology of active (amplifying) electrical devices. The triode was immediately applied to many areas of communication. Triode \"continuous wave\" radio transmitters replaced the cumbersome inefficient \"damped wave\" spark gap transmitters, allowing the transmission of sound by amplitude modulation (AM). Amplifying triode radio receivers, which had the power to drive loudspeakers, replaced weak crystal radios, which had to be listened to with earphones, allowing families to listen together. This resulted in the evolution of radio from a commercial message service to the first mass communication medium, with the beginning of radio broadcasting around 1920. Triodes made transcontinental telephone service possible. Vacuum tube triode repeaters, invented at Bell Telephone after its purchase of the Audion rights, allowed telephone calls to travel beyond the unamplified limit of about 800 miles. The opening by Bell of the first transcontinental telephone line was celebrated 3 years later, on January 25, 1915. Other inventions made possible by the triode were television, public address systems, electric phonographs, and talking motion pictures.\n\nThe triode served as the technological base from which later vacuum tubes developed, such as the tetrode (Walter Schottky, 1916) and pentode (Gilles Holst and Bernardus Dominicus Hubertus Tellegen, 1926), which remedied some of the shortcomings of the triode detailed below.\n\nThe triode was very widely used in consumer electronics such as radios, televisions, and audio systems until it was replaced in the 1960s by the transistor, invented in 1947, which brought the \"vacuum tube era\" introduced by the triode to a close. Today triodes are mostly used in high-power applications for which solid state semiconductor devices are unsuitable, such as radio transmitters and industrial heating equipment. However, more recently the triode and other vacuum tube devices have been experiencing a resurgence and comeback in high fidelity audio and musical equipment. They also remain in use as vacuum fluorescent displays (VFDs), which come in a variety of implementations but all are essentially triode devices.\n\nAll triodes have a hot cathode electrode heated by a filament, which releases electrons, and a flat metal plate electrode to which the electrons are attracted, with a grid consisting of a screen of wires between them to control the current. These are sealed inside a glass container from which the air has been removed to a high vacuum, about 10 atm. Since the filament eventually burns out, the tube has a limited lifetime and is made as a replaceable unit; the electrodes are attached to terminal pins which plug into a socket. The operating lifetime of a triode is about 2000 hours for small tubes and 10,000 hours for power tubes.\n\nLow power triodes have a concentric construction \"(see drawing right)\", with the grid and anode as circular or oval cylinders surrounding the cathode. The cathode is a narrow metal tube down the center. Inside the cathode is a filament called the \"heater\" consisting of a narrow strip of high resistance tungsten wire, which heats the cathode red-hot (800 - 1000 °C). This type is called an \"indirectly heated cathode\" The cathode is coated with a mixture of alkaline earth oxides such as calcium and thorium oxide which reduces its work function so it produces more electrons. The grid is constructed of a helix or screen of thin wires surrounding the cathode. The anode is a cylinder or rectangular box of sheet metal surrounding the grid. It is blackened to radiate heat and is often equipped with heat-radiating fins. The electrons travel in a radial direction, from cathode through the grid to the anode. The elements are held in position by mica or ceramic insulators and are supported by stiff wires attached to the base, where the electrodes are brought out to connecting pins. A \"getter\", a small amount of shiny barium metal evaporated onto the inside of the glass, helps maintain the vacuum by absorbing gas released in the tube over time.\n\nHigh-power triodes generally use a filament which serves as the cathode (a directly heated cathode) because the emission coating on indirectly heated cathodes is destroyed by the higher ion bombardment in power tubes. A thoriated tungsten filament is most often used, in which thorium in the tungsten forms a monolayer on the surface which increases electron emission. These generally run at higher temperatures than indirectly heated cathodes. The envelope of the tube is often made of more durable ceramic rather than glass, and all the materials have higher melting points to withstand higher heat levels produced. Tubes with anode power dissipation over several hundred watts are usually actively cooled; the anode, made of heavy copper, projects through the wall of the tube and is attached to a large external finned metal heat sink which is cooled by forced air or water.\n\nA type of low power triode for use at ultrahigh frequencies (UHF), the \"lighthouse\" tube, has a planar construction to reduce interelectrode capacitance and lead inductance, which gives it the appearance of a \"lighthouse\". The disk-shaped cathode, grid and plate form planes up the center of the tube - a little like a sandwich with spaces between the layers. The cathode at the bottom is attached to the tube's pins, but the grid and plate are brought out to low inductance terminals on the upper level of the tube: the grid to a metal ring halfway up, and the plate to a metal button at the top. These are one example of \"disk seal\" design. Smaller examples dispense with the octal pin base shown in the illustration and rely on contact rings for all connections, including heater and D.C. cathode.\n\nAs well, high-frequency performance is limited by transit time: the time required for electrons to travel from cathode to anode. Transit time effects are complicated, but one simple effect is input conductance, also known as grid loading. At extreme high frequencies, electrons arriving at the grid may become out of phase with those departing towards the anode. This imbalance of charge causes the grid to exhibit a reactance that is much less than its low-frequency \"open circuit\" characteristic.\n\nTransit time effects are reduced by reduced spacings in the tube. Tubes such as the 416B (a Lighthouse design) and the 7768 (an all-ceramic miniaturised design) are specified for operation to 4 GHz. They feature greatly reduced grid-cathode spacings in the order of 0.1 mm.\n\nThese greatly reduced grid spacings also give a much higher amplification factor than conventional axial designs. The 7768 has an amplification factor of 225, compared with 100 for the 6AV6 used in domestic radios and about the maximum possible for an axial design.\n\nAnode-grid capacitance is not especially low in these designs. The 6AV6 anode-grid capacitance is 2 picofarads (pF), the 7768 has a value of 1.7 pF. The close electrode spacing used in microwave tubes \"increases\" capacitances, but this increase is offset by their overall reduced dimensions compared to lower-frequency tubes.\n\nIn the triode, electrons are released into the tube from the metal cathode by heating it, a process called thermionic emission. The cathode is heated red hot by a separate current flowing through a thin metal filament. In a few triodes, the filament itself is the cathode, while in most the filament heats a separate cathode electrode. Virtually all the air is removed from the tube, so the electrons can move freely. The negative electrons are attracted to the positively charged anode, and flow through the spaces between the grid wires to it, creating a current through the tube from cathode to anode.\n\nThe magnitude of this current can be controlled by a voltage applied between the cathode and the grid. The grid acts like a gate for the electrons. A more negative voltage on the grid will repel some of the electrons, so fewer get through to the anode, reducing the anode current. A positive voltage on the grid will attract more electrons from the cathode, so more reach the anode, increasing the anode current. Therefore, a low power varying (AC) signal applied to the grid can control a much more powerful anode current, resulting in amplification. Variation in the grid voltage will cause identical proportional variations in the anode current. By placing a suitable load resistance in the anode circuit, the varying current will cause a varying voltage across the resistance which can be much larger than the input voltage variations, resulting in voltage gain.\n\nThe triode is a normally \"on\" device; and current flows to the anode with zero voltage on the grid. The anode current is progressively reduced as the grid is made more negative with respect to the cathode. Usually a constant DC voltage (\"bias\") is applied to the grid to set the DC current through the tube, and the varying signal voltage is superimposed on it. A sufficiently negative voltage on the grid, usually around 3-5 volts in small tubes such as the 6AV6, but as much as –130 volts in early audio power devices such as the '45, will prevent any electrons from getting through to the anode, turning off the anode current. This is called the \"cutoff voltage\". Since below cutoff the anode current ceases to respond to the grid voltage, the voltage on the grid must remain above the cutoff voltage for faithful (linear) amplification.\n\nThe triode is very similar in operation to the n-channel JFET; it is normally on, and progressively switched off as the grid/gate is pulled increasingly negative of the source/cathode. Cutoff voltage is equivalent to the JFET's pinch-off voltage (V)or VGS(off); the point at which current stops flowing entirely.\n\nAlthough S.G. Brown's Type G Telephone Relay (using a magnetic \"earphone\" mechanism driving a carbon microphone element) was able to give power amplification and had been in use as early as 1914, it was a purely mechanical device with limited frequency range and fidelity. It was suited only to a limited range of audio frequencies - essentially voice frequencies.\n\nThe triode was the first non-mechanical device to provide power gain at audio and radio frequencies, and made radio practical. Triodes are used for amplifiers and oscillators. Many types are used only at low to moderate frequency and power levels. Large water-cooled triodes may be used as the final amplifier in radio transmitters, with ratings of thousands of watts. Specialized types of triode (\"lighthouse\" tubes, with low capacitance between elements) provide useful gain at microwave frequencies.\n\nVacuum tubes are obsolete in mass-marketed consumer electronics, having been overtaken by less expensive transistor-based solid-state devices. However, more recently, vacuum tubes have been making somewhat of a comeback. Triodes continue to be used in certain high-power RF amplifiers and transmitters. While proponents of vacuum tubes claim their superiority in areas such as high-end and professional audio applications, empirical evidence shows that the solid-state MOSFET is virtually identical in performance.\n\nIn triode datasheets, characteristics linking the anode current (I) to anode voltage (V) and grid voltage (V) are usually given. From here, a circuit designer can choose the operating point of the particular triode.\n\nIn the example characteristic shown on the image, if an anode voltage V of 200V and a grid voltage bias of −1 volt are selected, a plate (anode) current of 2.25mA will be present (using the yellow curve on the graph). Changing the grid voltage will change the plate current; by suitable choice of a plate load resistor, amplification is obtained.\n\nIn the class-A triode amplifier, an anode resistor would be connected between the anode and the positive voltage source. For example, with R = 10000 ohms, the voltage drop on it would be\n\nV = I × R = 22.5V if an anode current of I = 2.25mA is chosen.\n\nIf the input voltage amplitude (at the grid) changes from −1.5V to −0.5V (difference of 1V), the anode current will change from 1.2 to 3.3mA (see image). This will change the resistor voltage drop from 12 to 33V (a difference of 21V).\n\nSince the grid voltage changes from −1.5V to −0.5V, and the anode resistor voltage drops from 12 to 33 V, an amplification of the signal resulted. The amplification factor is 21: output voltage amplitude divided by input voltage amplitude.\n\n\n"}
{"id": "1453046", "url": "https://en.wikipedia.org/wiki?curid=1453046", "title": "USS Peacock (1813)", "text": "USS Peacock (1813)\n\nUSS \"Peacock\" was a sloop-of-war in the United States Navy during the War of 1812.\n\nThe \"Peacock\" was authorized by Act of Congress 3 March 1813, laid down 9 July 1813, by Adam and Noah Brown at the New York Navy Yard, and launched 19 September 1813. She served in the War of 1812, capturing twenty ships. Subsequently, she served in the Mediterranean Squadron, and in the \"Mosquito Fleet\" suppressing Caribbean piracy. She patrolled the South American coast during the colonial wars of independence. She was decommissioned in 1827 and broken up in 1828 to be rebuilt as the USS \"Peacock\" (1828), intended as an exploration ship. She sailed as part of the United States Exploring Expedition in 1838. The \"Peacock\" ran aground and broke up on the Columbia Bar without loss of life in 1841.\n\nDuring the War of 1812, the \"Peacock\" made three cruises under the command of Master Commandant Lewis Warrington. Departing New York 12 March 1814, she sailed with supplies to the naval station at St. Mary's, Georgia. Off Cape Canaveral, Florida 29 April, she captured her an enemy warship, the British brig , which the \"Peacock\" sent to Savannah, and which the United States Navy took into service as the USS \"Epervier\".\n\nThe \"Peacock\" departed Savannah on 4 June on her second cruise; proceeding to the Grand Banks and along the coasts of Ireland and Spain, she returned via the West Indies to New York. She captured 14 enemy vessels of various sizes during this journey. On 14 August the \"Peacock\" captured the \"William\", Whiteway, master, of Bristol, and scuttled her.\n\nThe \"Peacock\" departed New York 23 January 1815 with the and and rounded the Cape of Good Hope into the Indian Ocean, where she captured three valuable prizes.\n\nOn 30 June she captured the 16-gun brig \"Nautilus\", which was under the command of Lieutenant Charles Boyce of the Bombay Marine of the British East India Company in the Straits of Sunda, in the final naval action of the war. Boyce informed Warrington that the war had ended. Warrington suspected a ruse and ordered Boyce to surrender. When Boyce refused, Warrington opened fire, killing one seaman, two European invalids, and three lascars, wounding Boyce severely, as well as mortally wounding the first lieutenant, and also wounding five lascars. American casualties amounted to some four or five men wounded. When Boyce provided documents proving that the Treaty of Ghent ending the war had been ratified, Warrington released his victims, though at no point did he in any way inquire about the Boyce's condition or that of any of the injured on the \"Nautilus\". The \"Peacock\" returned to New York on 30 October. A court of inquiry in Boston a year later exonerated Warrington of all blame. In his report on the incident, Warrington reported that the British casualties had only been lascars.\n\nThe \"Peacock\" left New York on 13 June 1816, bound for France, with the Honorable Albert Gallatin and party aboard. After pulling into Havre de Grâce 2 July, she proceeded to join the Mediterranean Squadron. But for a year of Mediterranean–United States—and return transits, 15 November 1818 – 17 November 1819, the sloop remained with this squadron until 8 May 1821, when she departed for home; she then went into ordinary at the Washington Navy Yard 10 July.\n\nPirates were ravaging West Indian shipping in the 1820s and on 3 June 1822, the \"Peacock\" became flagship of Commodore David Porter’s West India Squadron, that rooted out the pirates. The \"Peacock\" served in the expedition that included the Revenue Marine schooner \"Louisiana\" and the British schooner HMS \"Speedwell\". The trio broke up a pirate establishment at Bahia Honda Key , 28–30 September capturing four vessels. They burnt two and prize crews took the other two to New Orleans. Eighteen of the captured pirate crew members were sent to New Orleans for trial. The \"Peacock\" captured the schooner \"Pilot\" on 10 April 1823 and another sloop 16 April. In September, \"malignant fever\" necessitated a recess from activities, and the \"Peacock\" pulled into Norfolk, Virginia on 28 November.\n\nIn July 1823, the sloop was involved in the Battle of Lake Maracaibo and Mr. Peter Storms decided to join the Independentist cause, who won their independence on 3 August. Later, in March 1824, the sloop proceeded to the Pacific and for some months cruised along the west coast of South America, where the colonies were struggling for independence. In September 1825, the \"Peacock\" under the command of Commodore Thomas ap Catesby Jones, sailed to the Kingdom of Hawaiʻi, where a treaty of friendship, commerce and navigation was negotiated. From 24 July 1826 until 6 January 1827, the sloop visited other Pacific islands to protect American commerce and the whaling industry. Returning to South America from Hawaii, the ship was struck by a whale, suffering serious damage. Nevertheless, she reached Callao, from which she departed 25 June for New York.\n\nthe \"Peacock\" returned to New York in October 1827 to be decommissioned, broken up and rebuilt in 1828 for a planned expedition of exploration. Her size and configuration stayed about the same, but her guns were reduced to ten: eight long 24-pounders and two long 9-pounders. When plans for the exploratory voyage stalled in Congress, she re-entered regular service in the West Indies from 1829–31. Following refit, both the \"Peacock\" and the newly commissioned \"Boxer\", a 10-gun schooner, were ordered to assist the frigate \"Potomac\", which had just sailed on the first Sumatran Expedition. The two ships were also charged with diplomatic missions. \"Boxer\" left Boston Harbor about the middle of February 1832, with orders to proceed to Liberia and from thence to join the \"Peacock\" off the coast of Brazil; The \"Peacock\" sailed on 8 March 1832 under Commander David Geisinger.\n\nThe \"Peacock\" conveyed Mr. Francis Baylies and family to the United Provinces of the River Plate (Argentina) to assume the post of United States chargé d’affaires in the wake of the USS \"Lexington\" raid on the Falkland Islands in 1831. On arrival both the British line-of-battle ship \"Plantagenet\" and H. B. M. frigate \"Druid\" complimented her flag by playing \"Hail, Columbia\". Also aboard was \nPresident Andrew Jackson's \"special confidential agent\" Edmund Roberts in the official status of Captain's clerk.\n\n25June 1832, having left orders for the \"Boxer\" to follow to Bencoolen, the \"Peacock\" departed war-stricken Montevideo for the Cape of Good Hope. After taking water at Tristan da Cunha and rounding the Cape, then trying to keep about latitude 38° or 39°, on 9 or 10 August a sea of uncommon height and volume struck the ship, and threw her nearly on her beam ends, completely overwhelmed the gig in the starboard-quarter, crushed it into atoms in a moment, and buried the first three ratlines of the mizen-shrouds under water.\n\nPicking up the southeast trade wind around (,) on 28 August 1832, the \"Peacock\" sailed to Bencoolen where the Dutch Resident reported that the \"Potomac\" had completed her mission. Under orders to gather information before going to Cochin-China, The \"Peacock\" sailed for Manila by way of Long Island and 'Crokatoa' (Krakatoa), where hot springs found on the eastern side of the islands one hundred and fifty feet from the shore boiled furiously up, through many fathoms of water. Her chronometers proving useless, the \"Peacock\" threaded the Sunda Strait by dead reckoning. Diarrhoea and dysentery prevailed among the crew from Angier to Manila; after a fortnight there, cholera struck despite the overall cleanliness of the ship. The \"Peacock\" lost seven crewmen, and many who did recover died later in the voyage of other diseases. No new case of cholera occurred after 2 November 1833 while the \"Peacock\" was under way for Macao. Within two leagues of the Lemma or Ladrone islands, she took aboard a pilot after settling on a fee of thirteen dollars and a bottle of rum.\n\nAfter six weeks in the vicinity of Canton City, China, with the onset of the winter northeast monsoon and no sign of the \"Boxer\", the \"Peacock\" sailed from Lintin Island in the Pearl River estuary for the bay of Turan (modern Danang) as best for access to Hué, capital of Cochin-China her task was to explore the possibilities of expanding trade with the kingdom. Contrary winds from the northwest rather than the expected northeast quarter, coupled with a strong southward current, caused her to lose ground on every tack until, on 6 January 1833, she entered what enquiries disclosed to be the Vung-lam harbour of Phu Yen province.\n\nOn 8 February, Roberts having failed to gain permission to proceed to Hué due to miscommunication, the \"Peacock\" weighed anchor for the gulf of Siam, where on the 18th she anchored about from the mouth of the river Menam at , as was ascertained by frequent lunar observations and by four chronometers.\n\nOn 20 March 1833 Roberts concluded the Siamese-American Treaty of Amity and Commerce with the minister representing King Rama III, and the \"Peacock\" departed on 5 April to call on Singapore, where she stayed between 1 and 11 May.\n\nOn 29 August in the Red Sea, while bound to Mocha, the \"Peacock\" encountered the \"Nautilus\", the same brig the \"Peacock\" had attacked after the end of the War of 1812 with Great Britain. The \"Nautilus\" was sailing to Surat as escort to four brigs crowded with mussulman pilgrims returning from Mecca. This time the \"Peacock\" did not attack the \"Nautilus\".\n\nArriving 13 September off Muscat, Roberts concluded a treaty with Sultan Said bin Sultan, and departed on 7 October 1833. On the voyage from Muscat to Mozambique, Roberts omitted the particulars of each day, but stated that what he had written served \"to show the absolute necessity of having first-rate chronometers, or the lunar observations carefully attended to; and never omitted to be taken when practicable.\"\n\nThe \"Peacock\" returned Roberts to Rio de Janeiro on 17 January 1834, where on 1 March 1834 he boarded the \"Lexington\" to return to Boston.\n\nOn 23 April 1835, the \"Peacock\", under the command of C. K. Stribling, and accompanied by the U.S. Schooner \"Enterprise\", Lieutenant Commanding A. S. Campbell, departed New York Harbor. Roberts was once again aboard the \"Peacock\", and the two vessels were under the command of Commodore Edmund P. Kennedy. The mission first sailed to Brazil, then round the Cape of Good Hope to Zanzibar, for Roberts to return ratifications of the two treaties.\n\nOn 21 September 1835 at 2 am southeast of Masirah Island about four hundred miles from Muscat, \"Peacock\" grounded on a coral reef in about 2 1/4 fathoms. Mr. Roberts, and six men under the command of Passed Midshipman William Rogers, left in a small boat to effect a rescue. The crew hove overboard 11 of 22 guns, re-floated the ship on the 23rd, and repelled Arab marauders before making sail the next day. On the 28th the \"Peacock\" was off Muscat when she encountered the sloop-of-war \"Sultan\" under the Muscat flag, and under the command of Mr. Taylor. Said bin Sultan later recovered the guns that had been thrown overboard and shipped them to Roberts free of charge. The \"Peacock\" later obtained this letter:I certify that during the period I have navigated the Arabian coast, and been employed in the trigonometrical survey of the same, now executing by order of the Bombay government, that I have ever found it necessary to be careful to take nocturnal as well as diurnal observations, as frequent as possible, owing to the rapidity and fickleness of the currents, which, in some parts, I have found running at the rate of three and four knots an hour, and I have known the \"Palinurus\" set between forty and fifty miles dead in shore, in a dead calm, during the night. \nIt is owing to such currents, that I conceive the United States ship of war \"Peacock\" run aground, as have many British ships in previous years, on and near the same spot; when at the changes of the monsoons, and sometimes at the full and change, you have such thick weather, as to prevent the necessary observations being taken with accuracy and the navigator standing on with confidence as to his position, and with no land in sight, finds himself to his sorrow, often wrong, owing to a deceitful and imperceptible current, which has set him with rapidity upon it. The position of Mazeira Island, is laid down by Owen many miles too much to the westward.\n\nGiven under my hand this 10th day of November, 1835.\n\nS. B. Haines. \nCommander of the \nHonourable East India Company's \nsurveying brig \"Palinurus.\" \nTo sailing master,\n\nJohn Weems, U. S. Navy.\nA second attempt at negotiation with Cochin-China failed as Roberts fell desperately ill of dysentery; he withdrew to Macao where he died 12 June 1836. William Ruschenberger, M.D., (1807–1895) commissioned on this voyage, and gives an account of it up until 27 October 1837 when the \"Peacock\" anchors opposite the city of Norfolk, Virginia, after an absence of more than two years and a half.\n\nThe \"Peacock\" joined the United States Exploring Expedition in 1838, where she did good service before getting stuck on a bar of the Columbia River in Oregon. She broke up on 17–19 July 1841 after all the crew and much of the scientific data had been taken off, though Titian Peale lost most of his notes.\n\n\n"}
