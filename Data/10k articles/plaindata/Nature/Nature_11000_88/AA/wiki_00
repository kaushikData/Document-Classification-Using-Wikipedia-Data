{"id": "2576454", "url": "https://en.wikipedia.org/wiki?curid=2576454", "title": "Ambrogio Fogar", "text": "Ambrogio Fogar\n\nAmbrogio Fogar (IPA: [ˈfɔːgaɾ]; 13 August 1941 – 23 August 2005) was an Italian sailor, writer, rally driver and all-round adventurer. He was a Commander of the Order of Merit of the Italian Republic.\n\nHis exploits included a number of successful long-distance sailing feats, such as becoming the first Italian to sail single-handedly from east to west around the world. In 1978, after being capsized by killer whales, he survived more than ten weeks in a life raft in the South Atlantic along with a friend, journalist Mauro Mancini, who died of pneumonia two days after the two were rescued. After this, Fogar largely abandoned sailing. He made an unsuccessful attempt to walk to the North Pole and competed several times in the Dakar Rally.\nIn 1992 Fogar was paralysed from the neck down following a jeep accident while rallying in Turkmenistan. This did not end his adventurous spirit, and in 1997, in a wheelchair, he competed in a round-Italy yacht race.\n\nHe died in 2005 of a heart attack and is buried at the Monumental Cemetery of Milan, Italy.\n"}
{"id": "28161436", "url": "https://en.wikipedia.org/wiki?curid=28161436", "title": "Animal sacrifice in Hinduism", "text": "Animal sacrifice in Hinduism\n\nPractices of Hindu animal sacrifice are mostly associated with Shaktism, and in currents of folk Hinduism strongly rooted in local tribal traditions. Animal sacrifices were carried out in ancient times in India. Hindu scriptures such as the Gita, and some Puranas forbid animal sacrifice.\n\nA Sanskrit term used for animal sacrifice is \"bali\", in origin meaning \"tribute, offering or oblation\" generically (\"vegetable oblations [... and] animal oblations,\"). \nBali among other things \"refers to the blood of an animal\" and is sometimes known as Jhatka Bali among Hindus.\n\nThe Kalika Purana distinguishes \"bali\" (sacrifice), \"mahabali\" (great sacrifice), for the ritual killing of goats, elephant, respectively, though the reference to humans in Shakti theology is symbolic and done in effigy in modern times. For instance, Sir John Woodroffe published a commentary on the Karpuradistotram, where he writes that the sacrificial animals listed in verse 19 are symbols for the six enemies, with \"man\" representing pride.\n\nIt is a ritual that is practiced today and is mentioned in Medieval Hinduism too. It is important to note that the practice of animal sacrifice is not a required ritual in some sects of Hinduism. The majority of practicing Hindus today choose not to participate in or acknowledge the practice.\nAdherents of the Sakta sect of Hinduism hold this to be a central tenet of their belief.\n\nThe Ashvamedha ritual - in which a horse is sacrificed - is mentioned in the Vedic texts such as the \"Yajurveda\". In the epic \"Ramayana\", Rama performed the Ashvamedha sacrifice for becoming the Chakravartin emperor. In the epic \"Mahabharata\", Yudhishtra performs the Ashwamedha after winning the Kurukshetra war to become the Chakravartin emperor. The Mahabharata also contains a description of an Ashvamedha performed by the Chedi king Uparichara Vasu, however, no animals were sacrificed in this story.However, sacrifices were common in this ritual. The rulers of the Gupta empire, the Chalukya dynasty, and the Chola dynasty all performed the Ashvamedha.\n\nIn the medieval Bhagavata Purana, Krishna tells people not to perform animal sacrifices. Animal sacrifices are forbidden by the Bhagavata Purana in the \"Kaliyuga\", the present age. The Brahma Vaivarta Purana describes animal sacrifices as \"kali-varjya\" or prohibited in the \"Kaliyuga\". The Adi Purana, Brihan-naradiya Purana and Aditya Purana also forbid animal sacrifice in Kaliyuga.\n\nAnimal sacrifice is a part of some Durga puja celebrations during the Navratri in eastern states of India. The goddess is offered sacrificial animal in this ritual in the belief that it stimulates her violent vengeance against the buffalo demon. According to Christopher Fuller, the animal sacrifice practice is rare among Hindus during Navratri, or at other times, outside the Shaktism tradition found in the eastern Indian states of West Bengal, Odisha and Assam. Further, even in these states, the festival season is one where significant animal sacrifices are observed. In some Shakta Hindu communities, the slaying of buffalo demon and victory of Durga is observed with a symbolic sacrifice instead of animal sacrifice.\n\nThe Rajput of Rajasthan worship their weapons and horses on Navratri, and formerly offered a sacrifice of a goat to a goddess revered as Kuldevi – a practice that continues in some places. The ritual requires slaying of the animal with a single stroke. In the past this ritual was considered a rite of passage into manhood and readiness as a warrior. The \"Kuldevi\" among these Rajput communities is a warrior-pativrata guardian goddess, with local legends tracing reverence for her during Rajput-Muslim wars.\n\nThe tradition of animal sacrifice is being substituted with vegetarian offerings to the Goddess in temples and households around Banaras in Northern India.\nAnimal Sacrifice is practiced by Shaktism tradition where ritual offering is made to a Devi. In Southern Indian states of Karnataka, Andhra Pradesh, and Tamil Nadu. It is most notably performed in front of Local Deities or Clan Deities. In Karnataka, the Goddess receiving the sacrifice tends to be Renuka. The animal is either a male buffalo or a goat. The Kathar or Kutadi community of Maharashtra while observing the Pachvi ceremony, after delivery of a child in the family, offer worship to their family deity, Saptashrungi and also offer a sacrifice of a goat. Following this they hold the naming ceremony of the child on the 12th day.\n\nIn some Sacred groves of India, particularly in Western Maharashtra, animal sacrifice is practiced to pacify female deities that are supposed to rule the Groves. Animal sacrifice is also practiced by caste Hindus to placate deities at temples. In region around Pune, Goats and fowls are sacrificed to the God Vetala The goddess temples in Assam and West Bengal in India and Nepal where this takes place involves slaying of goats, chickens and sometimes male Water buffalos. \n\nAnimal sacrifice is practiced in some Eastern states of India and Nepal. For example, one of the largest animal sacrifice in Nepal occurs over the three-day-long Gadhimai festival. In 2009 it was speculated that more than 250,000 animals were killed while 5 million devotees attended the festival. The Gadhimai festival was banned by the Nepal government in 2015.\n\nFor instance, \"Kandhen Budhi\" is the reigning deity of Kantamal in Boudh district of Orissa, India. Every year, animals like goat and fowl are sacrificed before the deity on the occasion of her annual \"Yatra\"/\"Jatra\" (festival) held in the month of \"Aswina\" (September–October). The main attraction of \"Kandhen Budhi Yatra\" is \"Ghusuri Puja\". \"Ghusuri\" means a child pig, which is sacrificed to the goddess every three years. During the Bali Jatra, male goats are offered as a sacrifice to the goddess Samaleswari in her temple in Sambalpur, Orissa.\n\n\"Bali Jatra\" of Sonepur in Orissa, India is also an annual festival celebrated in the month of \"Aswina\" (September–October) when animal sacrifice is an integral part of the ritual worship of deities namely \"Samaleswari\", \"Sureswari\" and \"Khambeswari\". \"Bali\" refers to animal sacrifice and hence this annual festival is called \"Bali Jatra\".\n\nAnimal Sacrifice is practiced by some Hindus on the Indonesian island of Bali. The religious belief of \"Tabuh Rah\", a form of animal sacrifice of Balinese Hinduism includes a religious cockfight where a rooster is used in religious custom by allowing him to fight against another rooster in a religious and spiritual cockfight, a spiritual appeasement exercise of \"Tabuh Rah\". The spilling of blood is necessary as purification to appease the evil spirits, and ritual fights follow an ancient and complex ritual as set out in the sacred \"lontar\" manuscripts.\n\nA popular Hindu ritual form of worship of North Malabar region in the Indian state of Kerala is the blood offering to Theyyam gods. Theyyam deities are propitiated through the cock sacrifice where the religious cockfight is a religious exercise of offering blood to the Theyyam gods .\n\nMethods for sacrificing range from decapitation, strangulation, to a spike being driven into the heart of the animal.\n\nJhatka is the prescribed method for Hindu ritual slaughter, however other methods such as strangulation and the use of a wooden spile (sphya) driven into the heart is used. The reason for this is priests see an animal making a noise as a bad omen and the animal making noise indicates that it is suffering. The Jhatka method requires the instant killing of the animal in a single decapitating blow with an axe or sword. Those Hindus who eat meat prescribe meat killed by the Jhatka method.\n\n\n\n\n \n"}
{"id": "24692513", "url": "https://en.wikipedia.org/wiki?curid=24692513", "title": "Ankaramite", "text": "Ankaramite\n\nAnkaramite is volcanic rock type of mafic composition. It is a dark porphyritic variety of basanite containing abundant pyroxene and olivine phenocrysts. It contains minor amounts of plagioclase and accessory biotite, apatite, and iron oxides.\n\nIts type locality is Ankaramy in Madagascar. It was first described in 1916.\n"}
{"id": "4456373", "url": "https://en.wikipedia.org/wiki?curid=4456373", "title": "Anthroposystem", "text": "Anthroposystem\n\nThe term anthroposystem is used to describe the anthropological analogue to the ecosystem. In other words, the anthroposystem model serves to compare the flow of materials through human systems to those in naturally occurring systems. As defined by Santos, an anthroposystem is \"the orderly combination or arrangement of physical and biological environments for the purpose of maintaining human civilization...built by man to sustain his kind.\" The anthroposystem is intimately linked to economic and ecological systems as well.\nBoth the anthroposystem and ecosystem can be divided into three groups: producers, consumers, and recyclers. In the ecosystem, the producers or autotrophs consist of plants and some bacteria capable of producing their own food via photosynthesis or chemical synthesis, the consumers consist of animals that obtain energy from grazing and/or by feeding on other animals and the recyclers consist of decomposers such as fungi and bacteria.\n\nIn the anthroposystem, the producers consist of the energy production through fossil fuels, manufacturing with non-fuel minerals and growing food; the consumers consist of humans and domestic animals and the recyclers consist of the decomposing or recycling activities (i.e. waste water treatment, metal and solid waste recycling). <br>\n\nThe ecosystem is sustainable whereas the anthroposystem is not. The ecosystem is a closed loop in which nearly everything is recycled whereas the anthroposystem is an open loop where very little is recycled. In contrast to the ecosystem, the anthroposystem's producers and consumers are significantly more spatially displaced than those in the ecosystem and thus, more energy is required to transfer matter to a producer or recycler. Currently, a large majority of this energy comes from non-renewable fossil fuels.\n\nAdditionally, recycling is a naturally occurring component of the ecosystem, and is responsible for much of the resources used by the system. Under the anthroposystem model, however, recycling does not naturally occur. Outside input is relied on for material and energy supplies, and recycling systems that do exist are artificially created. The process of improving the flow of energy, such that waste can be reused as input resources, is known as industrial ecology.\n\nA matrix can be used to describe the anthropological network of producers, consumers and recyclers and the movement of materials between each. <br>\n\nHowever, the matrix model of the anthroposystem - based on a model for the ecosystem - fails in acknowledging the physical redistribution of mobilized matter. In developing the anthroposystem model, there is a trade-off between simplicity and completeness. A simple representative model can be created involving only producers, consumers, and recyclers, but this is an open, incomplete system. More components and analogues (such as a matrix that encompasses the producers, consumers and recyclers) can be added to the system to make a more complete model, but the model loses simplicity in the process. Though the anthroposystem concept is flawed in this manner, it is a very good starting point for analyzing human activities and their effects on the environment.\n\nWhen viewing the Earth as one large anthroposystem, we are essentially eliminating the uncertainty in material flow. All goods (i.e. fossil fuels) will still exist in the system but in a new form (i.e. pollutants). Therefore, the Laws of Conservation of Matter and Conservation of Energy can be applied to analyze how material flow will impact the environment. \n\n"}
{"id": "15376172", "url": "https://en.wikipedia.org/wiki?curid=15376172", "title": "Atmospheric instability", "text": "Atmospheric instability\n\nAtmospheric instability is a condition where the Earth's atmosphere is generally considered to be unstable and as a result the weather is subjected to a high degree of variability through distance and time. Atmospheric stability is a measure of the atmosphere's tendency to discourage or deter vertical motion, and vertical motion is directly correlated to different types of weather systems and their severity. In unstable conditions, a lifted thing, such as a parcel of air will be warmer than the surrounding air at altitude. Because it is warmer, it is less dense and is prone to further ascent.\n\nIn meteorology, instability can be described by various indices such as the Bulk Richardson Number, lifted index, K-index, convective available potential energy (CAPE), the Showalter, and the Vertical totals. These indices, as well as atmospheric instability itself, involve temperature changes through the troposphere with height, or lapse rate. Effects of atmospheric instability in moist atmospheres include thunderstorm development, which over warm oceans can lead to tropical cyclogenesis, and turbulence. In dry atmospheres, inferior mirages, dust devils, steam devils, and fire whirls can form. Stable atmospheres can be associated with drizzle, fog, increased air pollution, a lack of turbulence, and undular bore formation.\n\nThere are two primary forms of atmospheric instability:\n\n\nUnder convective instability thermal mixing through convection in the form of warm air rising leads to the development of clouds and possibly precipitation or convective storms. Dynamic instability is produced through the horizontal movement of air and the physical forces it is subjected to such as the Coriolis force and pressure gradient force. Dynamic lifting and mixing produces cloud, precipitation and storms often on a synoptic scale.\n\nWhether or not the atmosphere has stability depends partially on the moisture content. In a very dry troposphere, a temperature decrease with height less than 9.8C per kilometer ascent indicates stability, while greater changes indicate instability. This lapse rate is known as the dry adiabatic lapse rate. In a completely moist troposphere, a temperature decrease with height less than 6C per kilometer ascent indicates stability, while greater changes indicate instability. In the range between 6C and 9.8C temperature decrease per kilometer ascent, the term conditionally unstable is used.\n\nThe lifted index (LI), usually expressed in kelvins, is the temperature difference between the temperature of the environment Te(p) and an air parcel lifted adiabatically Tp(p) at a given pressure height in the troposphere, usually 500 hPa (mb). When the value is positive, the atmosphere (at the respective height) is stable and when the value is negative, the atmosphere is unstable. Thunderstorms are expected with values below -2, and severe weather is anticipated with values below -6.\n\nThe K index is derived arithmetically: K-index = (850 hPa temperature - 500 hPa temperature) + 850 hPa dew point - 700 hPa dew point depression\n\n\nConvective available potential energy (CAPE), sometimes, simply, available potential energy (APE), is the amount of energy a parcel of air would have if lifted a certain distance vertically through the atmosphere. CAPE is effectively the positive buoyancy of an air parcel and is an indicator of atmospheric instability, which makes it valuable in predicting severe weather. CIN, convective inhibition, is effectively negative buoyancy, expressed B-; the opposite of convective available potential energy (CAPE), which is expressed as B+ or simply B. As with CAPE, CIN is usually expressed in J/kg but may also be expressed as m/s, as the values are equivalent. In fact, CIN is sometimes referred to as negative buoyant energy (NBE).\n\nIt is a form of fluid instability found in thermally stratified atmospheres in which a colder fluid overlies a warmer one. When an air mass is unstable, the element of the air mass that is displaced upwards is accelerated by the pressure differential between the displaced air and the ambient air at the (higher) altitude to which it was displaced. This usually creates vertically developed clouds from convection, due to the rising motion, which can eventually lead to thunderstorms. It could also be created in other phenomenon, such as a cold front. Even if the air is cooler on the surface, there is still warmer air in the mid-levels, that can rise into the upper-levels. However, if there is not enough water vapor present, there is no ability for condensation, thus storms, clouds, and rain will not form.\n\nThe Bulk Richardson Number (BRN) is a dimensionless number relating vertical stability and vertical wind shear (generally, stability divided by shear). It represents the ratio of thermally-produced turbulence and turbulence generated by vertical shear. Practically, its value determines whether convection is free or forced. High values indicate unstable and/or weakly sheared environments; low values indicate weak instability and/or strong vertical shear. Generally, values in the range of around 10 to 45 suggest environmental conditions favorable for supercell development.\n\nThe Showalter index is a dimensionless number computed by taking the temperature at the 850 hPa level which is then taken dry adiabatically up to saturation, then up to the 500 hPa level, which is then subtracted by the observed 500 hPa level temperature. If the value is negative, then the lower portion of the atmosphere is unstable, with thunderstorms expected when the value is below −3. The application of the Showalter index is especially helpful when there is a cool, shallow air mass below 850 hPa that conceals the potential convective lifting. However, the index will underestimate the potential convective lifting if there are cool layers that extend above 850 hPa and it does not consider diurnal radiative changes or moisture below 850 hPa.\n\nStable conditions, such as during a clear and calm night, will cause pollutants to become trapped near ground level. Drizzle occurs within a moist air mass when it is stable. Air within a stable layer is not turbulent. Conditions associated with a marine layer, a stable atmosphere common on the west side of continents near cold water currents, leads to overnight and morning fog. Undular bores can form when a low level boundary such as a cold front or outflow boundary approaches a layer of cold, stable air. The approaching boundary will create a disturbance in the atmosphere producing a wave-like motion, known as a gravity wave. Although the undular bore waves appear as bands of clouds across the sky, they are transverse waves, and are propelled by the transfer of energy from an oncoming storm and are shaped by gravity. The ripple like appearance of this wave is described as the disturbance in the water when a pebble is dropped into a pond or when a moving boat creates waves in the surrounding water. The object displaces the water or medium the wave is travelling through and the medium moves in an upward motion. However, because of gravity, the water or medium is pulled back down and the repetition of this cycle creates the transverse wave motion.\n\nWithin an unstable layer in the troposphere, the lifting of air parcels will occur, and continue for as long as the nearby atmosphere remains unstable. Once overturning through the depth of the troposphere occurs (with convection being capped by the relatively warmer, more stable layer of the stratosphere), deep convective currents lead to thunderstorm development when enough moisture is present. Over warm ocean waters and within a region of the troposphere with light vertical wind shear and significant low level spin (or vorticity), such thunderstorm activity can grow in coverage and develop into a tropical cyclone. Over hot surfaces during warm days, unstable dry air can lead to significant refraction of the light within the air layer, which causes inferior mirages.\n\nWhen winds are light, dust devils can develop on dry days within a region of instability at ground level. Small-scale, tornado-like circulations can occur over or near any intense surface heat source, which would have significant instability in its vicinity. Those that occur near intense wildfires are called fire whirls, which can spread a fire beyond its previous bounds. A steam devil is a rotating updraft that involves steam or smoke. They can form from smoke issuing from a power plant smokestack. Hot springs and warm lakes are also suitable locations for a steam devil to form, when cold arctic air passes over the relatively warm water.\n\n"}
{"id": "45259", "url": "https://en.wikipedia.org/wiki?curid=45259", "title": "Biosafety", "text": "Biosafety\n\nBiosafety is the prevention of large-scale loss of biological integrity, focusing both on ecology and human health.\nThese prevention mechanisms include conduction of regular reviews of the biosafety in laboratory settings, as well as strict guidelines to follow. Biosafety is used to protect from harmful incidents. Many laboratories handling biohazards employ an ongoing risk management assessment and enforcement process for biosafety. Failures to follow such protocols can lead to increased risk of exposure to biohazards or pathogens. Human error and poor technique contribute to unnecessary exposure and compromise the best safeguards set into place for protection.\n\nThe international Cartagena Protocol on Biosafety deals primarily with the agricultural definition but many advocacy groups seek to expand it to include post-genetic threats: new molecules, artificial life forms, and even robots which may compete directly in the natural food chain.\n\nBiosafety in agriculture, chemistry, medicine, exobiology and beyond will likely require the application of the precautionary principle, and a new definition focused on the biological nature of the threatened organism rather than the nature of the threat.\n\nWhen biological warfare or new, currently hypothetical, threats (i.e., robots, new artificial bacteria) are considered, biosafety precautions are generally not sufficient. (link to incident report, i.e. such as problems with CDC research labs in 2014)The new field of biosecurity addresses these complex threats.\n\nBiosafety level refers to the stringency of biocontainment precautions deemed necessary by the Centers for Disease Control and Prevention (CDC) for laboratory work with infectious materials.\n\nTypically, institutions that experiment with or create potentially harmful biological material will have a committee or board of supervisors that is in charge of the institution's biosafety. They create and monitor the biosafety standards that must be met by labs in order to prevent the accidental release of potentially destructive biological material. (note that in the US, several groups are involved, and efforts are being made to improve processes for government run labs, but there is no unifying regulatory authority for all labs.\n\nBiosafety is related to several fields:\n\n\nHigh-security facilities are necessary when working with synthetic biology as there are possibilities of bioterrorism acts or release of harmful chemicals and or organisms into the environment. A complete understanding of experimental risks associated with synthetic biology is helping to enforce the knowledge and effectiveness of biosafety.\nWith the potential future creation of man-made unicellular organisms, some are beginning to consider the effect that these organisms will have on biomass already present. Scientists estimate that within the next few decades, organism design will be sophisticated enough to accomplish tasks such as creating biofuels and lowering the levels of harmful substances in the atmosphere. Scientist that favor the development of synthetic biology claim that the use of biosafety mechanisms such as suicide genes and nutrient dependencies will ensure the organisms cannot survive outside of the lab setting in which they were originally created. Organizations like the ETC Group argue that regulations should control the creation of organisms that could potentially harm existing life. They also argue that the development of these organisms will simply shift the consumption of petroleum to the utilization of biomass in order to create energy. These organisms can harm existing life by affecting the prey/predator food chain, reproduction between species, as well as competition against other species (species at risk, or act as an invasive species).\nSynthetic vaccines are now being produced in the lab. These have caused a lot of excitement in the pharmaceutical industry as they will be cheaper to produce, allow quicker production, as well enhance the knowledge of virology and immunology.\n\nBiosafety, in medicine and health care settings, specifically refers to proper handling of organs or tissues from biological origin, or genetic therapy products, viruses with respect to the environment, to ensure the safety of health care workers, researchers, lab staff, patients, and the general public. Laboratories are assigned a biosafety level numbered 1 through 4 based on their potential biohazard risk level. The employing authority, through the laboratory director, is responsible for ensuring that there is adequate surveillance of the health of laboratory personnel. The objective of such surveillance is to monitor for occupationally acquired diseases. The World Health Organization attributes human error and poor technique as the primary cause of mishandling of biohazardous materials.\n\nBiosafety is becoming also a global concern and require multilevel resources and international collaboration to monitor, prevent and correct accidents from unintended and malicious release and also to prevent that bioterrorists get their hands-on biologics sample to create biologic weapons of mass destruction. Even people outside of the health sector needs to be involved as in the case of the Ebola outbreak the impact that it had on businesses and travel required that private sectors, international banks together pledged more than $2 billion to combat the epidemic. The bureau of international Security and nonproliferation (ISN) is responsible for managing a broad range of U.S. nonproliferation policies, programs, agreements, and initiatives, and biological weapon is one their concerns\nBiosafety has its risks and benefits. All stakeholders must try to find a balance between cost-effectiveness of safety measures and use evidence-based safety practices and recommendations, measure the outcomes and consistently reevaluate the potential benefits that biosafety represents for human health.\nBiosafety level designations are based on a composite of the design features, construction, containment facilities, equipment, practices and operational procedures required for working with agents from the various risk groups.\n\nClassification of biohazardous materials is subjective and the risk assessment is determined by the individuals most familiar with the specific characteristics of the organism. There are several factors taken into account when assessing an organism and the classification process.\n\n\nSee World Health Organization Biosafety Laboratory Guidelines: World Health Organization Biosafety Laboratory Guildlines\n\nInvestigations have shown that there are hundreds of unreported biosafety accidents, with laboratories self-policing the handling of biohazardous materials and lack of reporting. Poor record keeping, improper disposal, and mishandling biohazardous materials result in increased risks of biochemical contamination for both the public and environment.\n\nAlong with the precautions taken during the handling process of biohazardous materials, the World Health Organization recommends:\nStaff training should always include information on safe methods for highly hazardous procedures that are commonly encountered by all laboratory personnel and which involve:\n\nIn June 2009, the Trans-Federal Task Force On Optimizing Biosafety and Biocontainment Oversight recommended the formation of an agency to coordinate high safety risk level labs (3 and 4), and voluntary, non-punitive measures for incident reporting. However, it is unclear as to what changes may or may not have been implemented following their recommendations.\n\nThe United States Code of Federal Regulations is the codification (law), or collection of laws specific to a specific to a jurisdiction that represent broad areas subject to federal regulation. Title 42 of the Code of Federal Regulations addresses laws concerning Public Health issues including biosafety which can be found under the citation 42 CFR 73 to 42 CFR 73.21 by accessing the US Code of Federal Regulations (CFR) website.\n\nTitle 42 Section 73 of the CFR addresses specific aspects of biosafety including Occupational safety and health, transportation of biohazardous materials and safety plans for laboratories using potential biohazards. While biocontainment, as defined in the Biosafety in Microbiological and Biomedical Laboratories and Primary Containment for Biohazards: Selection, Installation and Use of Biosafety Cabinets manuals available at the Centers for Disease Control and Prevention website much of the design, implementation and monitoring of protocols are left up to state and local authorities.\nThe United States CFR states \"An individual or entity required to register [as a user of biological agents] must develop and implement a written biosafety plan that is commensurate with the risk of the select agent or toxin\" which is followed by 3 recommended sources for laboratory reference.\n\n\nWhile clearly the needs of biocontainment and biosafety measures vary across government, academic and private industry laboratories, biological agents pose similar risks independent of their locale. Laws relating to biosafety are not easily accessible and there are few federal regulations that are readily available for a potential trainee to reference outside of the publications recommended in 42 CFR 73.12. Therefore, training is the responsibility of lab employers and is not consistent across various laboratory types thereby increasing the risk of accidental release of biological hazards that pose serious health threats to the humans, animals and the ecosystem as a whole.\n\nMany government agencies have made guidelines and recommendations in an effort to increase biosafety measures across laboratories in the United States. Agencies involved in producing policies surrounding biosafety within a hospital, pharmacy or clinical research laboratory include: the CDC, FDA, USDA, DHHS, DoT, EPA and potentially other local organizations including public health departments. The federal government does set some standards and recommendations for States to meet their standards, most of which fall under the Occupational Health and Safety Act of 1970. but currently, there is no single federal regulating agency directly responsible for ensuring the safety of biohazardous handling, storage, identification, clean-up and disposal. In addition to the CDC, the Environmental Protection Agency has some of the most accessible information on ecological impacts of biohazards, how to handle spills, reporting guidelines and proper disposal of agents dangerous to the environment. Many of these agencies have their own manuals and guidance documents relating to training and certain aspects of biosafety directly tied to their agency's scope, including transportation, storage and handling of blood borne pathogens. (OSHA, IATA). The American Biological Safety Association (ABSA) has a list of such agencies and links to their websites, along with links to publications and guidance documents to assist in risk assessment, lab design and adherence to laboratory exposure control plans. Many of these agencies were members of the 2009 Task Force on BioSafety. There was also a formation of a Blue Ribbon Study Panel on Biodefense, but this is more concernend with national defense programs and biosecurity.\n\nUltimately states and local governments, as well as private industry labs, are left to make the final determinants for their own biosafety programs, which vary widely in scope and enforcement across the United States. Not all state programs address biosafety from all necessary perspectives, which should not just include personal safety, but also emphasize an full understanding among laboratory personnel of quality control and assurance, exposure potential impacts on the environment, and general public safety.\n\nState occupational safety plans are often focused on transportation, disposal, and risk assessment, allowing caveats for safety audits, but ultimately leaves the training in the hands of the employer. 22 states have approved Occupational Safety plans by OSHA that are audited annually for effectiveness. These plans apply to private and public sector workers, and not necessarily state/ government workers, and not all specifically have a comprehensive program for all aspects of biohazard management from start to finish. Sometimes biohazard management plans are limited only to workers in transportation specific job titles. The enforcement and training on such regulations can vary from lab to lab based on the State's plans for occupational health and safety. With the exception of DoD lab personnel, CDC lab personnel, First responders, and DoT employees, enforcement of training is inconsistent, and while training is required to be done, specifics on the breadth and frequency of refresher training does not seem consistent from state to state; penalties may never be assessed without larger regulating bodies being aware of non-compliance, and enforcement is limited.\n\nMedical waste management was identified as an issue in the 1980s; with the Medical Waste Tracking Act of 1988 becoming the new standard in biohazard waste disposal.\n\nAlthough the Federal Government, EPA & DOT provide some oversight of regulated medical waste storage, transportation, and disposal the majority of biohazard medical waste is regulated at the state level. Each state is responsible for regulation and management of their own bioharzardous waste with each state varying in their regulatory process. Record keeping of biohazardous waste also varies between states.\n\nMedical healthcare centers, hospitals veterinary clinics, clinical laboratories and other facilities generate over one million tons of waste each year. Although the majority of this waste is as harmless as common household waste, as much as 15 percent of this waste poses a potential infection hazard, according to the Environmental Protection Agency (EPA). Medical waste is required to be rendered non-infectious before it can be disposed of. There are several different methods to treat and dispose of biohazardous waste. In the United States, the \nprimary methods for treatment and disposal of biohazard, medical and sharps waste may include:\n\nDifferent forms of biohazardous wasted required different treatments for their proper waste management. This is determined largely be each states regulations. Currently, there are several contracted companies that focus on medical, sharps and biological hazard disposal. Stericycle is one of the leaders in medical waste and pharmaceutical disposal in the United States.\n\nThe United States Government has made it clear that biosafety is to be taken very seriously. In 2014, incidents with Anthrax and Ebola pathogens in CDC laboratories (), prompted the CDC director Tom Frieden to issue a moratorium for research with these types of select agents. An investigation concluded that there was a lack of adherence to safety protocols and \"inadequate safeguards\" in place. This indicated a lack of proper training or reinforcement of training and supervision on regular basis for lab personnel.\n\nFollowing these incidents, the CDC established an \"External Laboratory Safety Workgroup\" (ELSW), and suggestions have been made to reform effectiveness of the Federal Select Agent Program. The White house issued a report on national biosafety priorities in 2015, outlining next steps for a national biosafety and security program, and addressed biological safety needs for health research, national defense, and public safety.\n\nIn 2016, the Association of Public Health Laboratories (APHL) had a presentation at their annual meeting focused on improving biosafety culture. This same year, The UPMC Center for Health Security issued a case study report including reviews of ten different nations' current biosafety regulations, including the United States. Their goal was to \"provide a foundation for identifying national‐level biosafety norms and enable initial assessment of biosafety priorities necessary for developing effective national biosafety regulation and oversight.\"\n\n\n"}
{"id": "23387624", "url": "https://en.wikipedia.org/wiki?curid=23387624", "title": "Cellulose fiber", "text": "Cellulose fiber\n\nCellulose fibers (), spelled as \"fibre\" in most of the English speaking world, are fibers made with ethers or esters of cellulose, which can be obtained from the bark, wood or leaves of plants, or from other plant-based material. In addition to cellulose, the fibers may also contain hemicellulose and lignin, with different percentages of these components altering the mechanical properties of the fibers.\n\nThe main applications of cellulose fibers are in the textile industry, as chemical filters, and as fiber-reinforcement composites, due to their similar properties to engineered fibers, being another option for biocomposites and polymer composites.\n\nThe market for cellulose fibers has witnessed strong growth due to increasing demand from the textile industry. The demand for environmentally friendly, skin friendly and bio-degradable fabrics is a key factor driving this market.\n\nCellulose was discovered in 1838 by the French chemist Anselme Payen, who isolated it from plant matter and determined its chemical formula. Cellulose was used to produce the first successful thermoplastic polymer, celluloid, by Hyatt Manufacturing Company in 1870. Production of rayon (\"artificial silk\") from cellulose began in the 1890s, and cellophane was invented in 1912. In 1893, Arthur D. Little of Boston, invented yet another cellulosic product, acetate, and developed it as a film. The first commercial textile uses for acetate in fiber form were developed by the Celanese Company in 1924. Hermann Staudinger determined the polymer structure of cellulose in 1920. The compound was first chemically synthesized (without the use of any biologically derived enzymes) in 1992, by Kobayashi and Shoda.\n\nCellulose is a polymer made of repeating glucose molecules attached end to end. A cellulose molecule may be from several hundred to over 10,000 glucose units long. Cellulose is similar in form to complex carbohydrates like starch and glycogen. These polysaccharides are also made from multiple subunits of glucose. The difference between cellulose and other complex carbohydrate molecules is how the glucose molecules are linked together. In addition, cellulose is a straight chain polymer, and each cellulose molecule is long and rod-like. This differs from starch, which is a coiled molecule. A result of these differences in structure is that, compared to starch and other carbohydrates, cellulose cannot be broken down into its glucose subunits by any enzymes produced by animals.\n\nNatural cellulose fibers are fibers that are still recognizable as being from a part of the original plant because they are only processed as much as needed to clean the fibers for use. For example, cotton fibers look like the soft fluffy cotton balls that they come from. Linen fibers look like the strong fibrous strands of the flax plant. All \"natural\" fibers go through a process where they are separated from the parts of the plant that are not used for the end product, usually through harvesting, separating from chaff, scouring, etc. The presence of linear chains of thousands of glucose units linked together allows a great deal of hydrogen bonding between OH groups on adjacent chains, causing them to pack closely into cellulose fibers. As a result, cellulose exhibits little interaction with water or any other solvent. Cotton and wood, for example, are completely insoluble in water and have considerable mechanical strength. Since cellulose does not have a helical structure like amylose, it does not bind to iodine to form a colored product.\n\nManufactured cellulose fibers come from plants that are processed into a pulp and then extruded in the same ways that synthetic fibers like polyester or nylon are made. Rayon or viscose is one of the most common \"manufactured\" cellulose fibers, and it can be made from wood pulp.\n\nNatural fibers are composed by microfibrils of cellulose in a matrix of hemicellulose and lignin. This type of structure and the chemical composition of them is responsible for the mechanical properties that can be observed. Because the natural fibers make hydrogen bonds between the long chains, they have the necessary stiffness and strength.\n\nThe major constituents of natural fibers (lignocelluloses) are cellulose, hemicellulose, lignin, pectin and ash. The percentage of each component varies for each different type of fiber, however, generally, are around 60-80% cellulose, 5–20% lignin, and 20% of moisture, besides hemicellulose and a small percent of residual chemical components. The properties of the fiber change depending on the amount of each component, since the hemicellulose is responsible for the moisture absorption, bio- and thermal degradation whereas lignin ensures thermal stability but is responsible for the UV degradation. The chemical composition of commons natural fibers are shown below, and can change if the fibers are a bast fiber (obtained from the bark), a core fiber (obtained from the wood), or a leaf fiber (obtained from the leaves).\n\nCellulose fiber response to mechanical stresses change depending on fiber type and chemical structure present. Information about main mechanical properties are shown in the chart below and can be compared to properties of commonly used fibers such glass fiber, aramid fiber, and carbon fiber.\n\nComposite materials are a class of material most often made by the combination of a matrix and a fiber. This combination mixes the properties of the matrix and the fiber to create a new material that may be stronger than the fiber alone. Recently, cellulose fibers have begun to be used as a fiber-reinforcement material, especially in conjunction with polymers (as can be observed in the table), in biocomposites and fiber-reinforced plastic. The macroscopic characteristics of fibers can influence the behavior of the resulting composites. Of interest are the following physical and mechanical properties of cellulosic fibers:\n\n\nIn the textile industry regenerated cellulose is used as fibers such as rayon, (including modal, and the more recently developed Lyocell). Cellulose fibers are manufactured from dissolving pulp. Cellulose-based fibers are of two types, regenerated or pure cellulose such as from the cupro-ammonium process and modified cellulose such as the cellulose acetates.\n\nThe first artificial fiber, known as artificial silk, became known as viscose around 1894, and finally rayon in 1924. A similar product known as cellulose acetate was discovered in 1865. Rayon and acetate are both artificial fibers, but not truly synthetic, being made from wood. Although these artificial fibers were discovered in the mid-nineteenth century, successful modern manufacture began much later.\n\nThe cellulose fibers infiltration/filter aid applications can provide a protective layer to filter elements as powdered cellulose, besides promoting improved throughput and clarity. As ashless and non-abrasive filtration, make cleanup effortless after the filtering process without damage in pumps or valves. They effectively filter metallic impurities and absorb up to 100% of emulsified oil and boiler condensates. In general, cellulose fibers in filtration applications can greatly improve filtration performance when used as a primary or remedial precoat in the following ways: \n\n\nIn comparison with engineered fibers, cellulose fibers have important advantages as low density, low cost, they can be recyclable, and are biodegradable. Due to its advantages cellulose fibers can be used as a substituent for glass fibers in composites materials.\n\nWhat is often marketed as \"bamboo fiber\" is actually not the fibers that grow in their natural form from the bamboo plants, but instead a highly processed bamboo pulp that is extruded as fibers. Although the process is not as environmentally friendly as \"bamboo fiber\" appears, planting & harvesting bamboo for fiber is much more sustainable and environmentally friendly than harvesting more slow growing trees and clearing existing forest habitats for timber plantations.\n\n\n"}
{"id": "22770161", "url": "https://en.wikipedia.org/wiki?curid=22770161", "title": "David Nelson (botanical collector)", "text": "David Nelson (botanical collector)\n\nDavid Nelson (died 20 July 1789) was gardener-botanist on the third voyage of James Cook, and botanist on under William Bligh at the time of the famous mutiny.\n\nNothing is known of his ancestry or early life. In 1776, while working as a gardener at Kew Gardens, he accepted a position as a servant to William Bayly, the official astronomer on . He was promoted to able seaman; however, his real task as arranged between Joseph Banks and Cook was to collect as many botanical specimens as possible for the Royal Gardens, as Cook, who had failed to attract an established botanist to the position. He received a small amount of botanical training and instruction from Banks and William Aiton before embarking. During the voyage, he also made a significant collection of native Hawaiian birds, which is now housed in the British Museum.\n\nOn returning to London in 1780, he worked as a gardener at Kew Gardens for seven years, before Banks arranged his appointment as botanist to Bligh's voyage to Tahiti to obtain breadfruit trees. Nelson had control of the Great Cabin and 1015 potted breadfruit trees which were intended for the West Indies. He was caught up in the mutiny and, remaining loyal to the captain, was one of the 19 men cast adrift in a small boat. He survived the famous 3800-mile voyage to Timor, but a few days after arriving in Kupang, died of a fever. Bligh would later name Mount Nelson, Tasmania in his honor.\n\n"}
{"id": "191162", "url": "https://en.wikipedia.org/wiki?curid=191162", "title": "Drainage basin", "text": "Drainage basin\n\nA drainage basin is any area of land where precipitation collects and drains off into a common outlet, such as into a river, bay, or other body of water. The drainage basin includes all the surface water from rain runoff, snowmelt, and nearby streams that run downslope towards the shared outlet, as well as the groundwater underneath the earth's surface. Drainage basins connect into other drainage basins at lower elevations in a hierarchical pattern, with smaller sub-drainage basins, which in turn drain into another common outlet.\n\nOther terms used interchangeably with \"drainage basin\" are catchment area, catchment basin, drainage area, river basin, and water basin. In North America, the term watershed is commonly used to mean a drainage basin, though in other English-speaking countries, it is used only in its original sense, that of a drainage divide.\n\nIn a closed drainage basin, or endorheic basin, the water converges to a single point inside the basin, known as a sink, which may be a permanent lake, a dry lake, or a point where surface water is lost underground. \n\nThe drainage basin acts as a funnel by collecting all the water within the area covered by the basin and channelling it to a single point. Each drainage basin is separated topographically from adjacent basins by a perimeter, the drainage divide, making up a succession of higher geographical features (such as a ridge, hill or mountains) forming a barrier.\n\nDrainage basins are similar but not identical to hydrologic units, which are drainage areas delineated so as to nest into a multi-level hierarchical drainage system. Hydrologic units are defined to allow multiple inlets, outlets, or sinks. In a strict sense, all drainage basins are hydrologic units but not all hydrologic units are drainage basins.\n\nDrainage basins of the principal oceans and seas of the world. Grey areas are endorheic basins that do not drain to the oceans.\n\nThe following is a list of the major ocean basins:\n\nThe five largest river basins (by area), from largest to smallest, are the basins of the Amazon (7M km), the Congo (4M km), the Nile (3.4M km), the Río de la Plata (3.2M km), and the Mississippi (3M km). The three rivers that drain the most water, from most to least, are the Amazon, Ganga, and Congo rivers.\n\nEndorheic drainage basins are inland basins that do not drain to an ocean. Around 18% of all land drains to endorheic lakes or seas or sinks. The largest of these consists of much of the interior of Asia, which drains into the Caspian Sea, the Aral Sea, and numerous smaller lakes. Other endorheic regions include the Great Basin in the United States, much of the Sahara Desert, the drainage basin of the Okavango River (Kalahari Basin), highlands near the African Great Lakes, the interiors of Australia and the Arabian Peninsula, and parts in Mexico and the Andes. Some of these, such as the Great Basin, are not single drainage basins but collections of separate, adjacent closed basins.\n\nIn endorheic bodies of standing water where evaporation is the primary means of water loss, the water is typically more saline than the oceans. An extreme example of this is the Dead Sea.\n\nDrainage basins have been historically important for determining territorial boundaries, particularly in regions where trade by water has been important. For example, the English crown gave the Hudson's Bay Company a monopoly on the fur trade in the entire Hudson Bay basin, an area called Rupert's Land. Bioregional political organization today includes agreements of states (e.g., international treaties and, within the U.S.A., interstate compacts) or other political entities in a particular drainage basin to manage the body or bodies of water into which it drains. Examples of such interstate compacts are the Great Lakes Commission and the Tahoe Regional Planning Agency.\n\nIn hydrology, the drainage basin is a logical unit of focus for studying the movement of water within the hydrological cycle, because the majority of water that discharges from the basin outlet originated as precipitation falling on the basin. A portion of the water that enters the groundwater system beneath the drainage basin may flow towards the outlet of another drainage basin because groundwater flow directions do not always match those of their overlying drainage network. Measurement of the discharge of water from a basin may be made by a stream gauge located at the basin's outlet.\n\nRain gauge data is used to measure total precipitation over a drainage basin, and there are different ways to interpret that data. If the gauges are many and evenly distributed over an area of uniform precipitation, using the arithmetic mean method will give good results. In the Thiessen polygon method, the drainage basin is divided into polygons with the rain gauge in the middle of each polygon assumed to be representative for the rainfall on the area of land included in its polygon. These polygons are made by drawing lines between gauges, then making perpendicular bisectors of those lines form the polygons. The isohyetal method involves contours of equal precipitation are drawn over the gauges on a map. Calculating the area between these curves and adding up the volume of water is time consuming.\n\nIsochrone maps can be used to show the time taken for runoff water within a drainage basin to reach a lake, reservoir or outlet, assuming constant and uniform effective rainfall.\n\nDrainage basins are the principal hydrologic unit considered in fluvial geomorphology. A drainage basin is the source for water and sediment that moves from higher elevation through the river system to lower elevations as they reshape the channel forms.\n\nDrainage basins are important in ecology. As water flows over the ground and along rivers it can pick up nutrients, sediment, and pollutants. With the water, they are transported towards the outlet of the basin, and can affect the ecological processes along the way as well as in the receiving water source.\n\nModern use of artificial fertilizers, containing nitrogen, phosphorus, and potassium, has affected the mouths of drainage basins. The minerals are carried by the drainage basin to the mouth, and may accumulate there, disturbing the natural mineral balance. This can cause eutrophication where plant growth is accelerated by the additional material.\n\nBecause drainage basins are coherent entities in a hydro-logical sense, it has become common to manage water resources on the basis of individual basins. In the U.S. state of Minnesota, governmental entities that perform this function are called \"watershed districts\". In New Zealand, they are called catchment boards. Comparable community groups based in Ontario, Canada, are called conservation authorities. In North America, this function is referred to as \"watershed management\".\nIn Brazil, the National Policy of Water Resources, regulated by Act n° 9.433 of 1997, establishes the drainage basin as the territorial division of Brazilian water management.\n\nWhen a river basin crosses at least one political border, either a border within a nation or an international boundary, it is identified as a transboundary river. Management of such basins becomes the responsibility of the countries sharing it. Nile Basin Initiative, OMVS for Senegal River, Mekong River Commission are a few examples of arrangements involving management of shared river basins.\n\nManagement of shared drainage basins is also seen as a way to build lasting peaceful relationships among countries.\n\nThe catchment is the most significant factor determining the amount or likelihood of flooding.\n\nCatchment factors are: topography, shape, size, soil type, and land use (paved or roofed areas). Catchment topography and shape determine the time taken for rain to reach the river, while catchment size, soil type, and development determine the amount of water to reach the river.\n\nGenerally, topography plays a big part in how fast runoff will reach a river. Rain that falls in steep mountainous areas will reach the primary river in the drainage basin faster than flat or lightly sloping areas (e.g., > 1% gradient).\n\nShape will contribute to the speed with which the runoff reaches a river. A long thin catchment will take longer to drain than a circular catchment.\n\nSize will help determine the amount of water reaching the river, as the larger the catchment the greater the potential for flooding. It is also determined on the basis of length and width of the drainage basin.\n\nSoil type will help determine how much water reaches the river. Certain soil types such as sandy soils are very free-draining, and rainfall on sandy soil is likely to be absorbed by the ground. However, soils containing clay can be almost impermeable and therefore rainfall on clay soils will run off and contribute to flood volumes. After prolonged rainfall even free-draining soils can become saturated, meaning that any further rainfall will reach the river rather than being absorbed by the ground. If the surface is impermeable the precipitation will create surface run-off which will lead to higher risk of flooding; if the ground is permeable, the precipitation will infiltrate the soil.\n\nLand use can contribute to the volume of water reaching the river, in a similar way to clay soils. For example, rainfall on roofs, pavements, and roads will be collected by rivers with almost no absorption into the groundwater.\n\n\n"}
{"id": "11004842", "url": "https://en.wikipedia.org/wiki?curid=11004842", "title": "Environmental quality", "text": "Environmental quality\n\nEnvironmental quality is a set of properties and characteristics of the environment, either generalized or local, as they impinge on human beings and other organisms. It is a measure of the condition of an environment relative to the requirements of one or more species and or to any human need or purpose.\n\nEnvironmental quality includes the natural environment as well as the built environment, such as air and water purity or pollution, noise and the potential effects which such characteristics may have on physical and mental health.\n\nIn the United States, the term is applied with a body of federal and state standards and regulations that are monitored by regulatory agencies. All states in the U.S. have some form of a department or commission that is responsible for a variety of activities such as monitoring quality, responding to citizen complaints, and enforcing environmental regulations. The agency with the lead implementation responsibility for most major federal environmental laws (e.g. Clean Air Act, Clean Water Act) is the US Environmental Protection Agency (EPA). Other federal agencies with significant oversight roles include the Council on Environmental Quality, Department of the Interior and the Army Corps of Engineers.\n\nIn the United Kingdom, the environment has been the primary responsibility of the Department for Environment, Food and Rural Affairs (DEFRA). Predecessor bodies were merged in 2001 to create this department with a broader remit to link rural activities to the natural environment. Some responsibilities are devolved to the Scottish Government and are exercised by the Scottish Environment Protection Agency (SEPA) and the National Assembly for Wales, while delivery of environmental initiatives often use partners, including: British Waterways, Environment Agency, Forestry Commission, and Natural England. DEFRA also has a remit to oversee impacts of activities within the built environment and the United Kingdom Climate Change Programme.\n\n\n"}
{"id": "3239174", "url": "https://en.wikipedia.org/wiki?curid=3239174", "title": "Environmental standard", "text": "Environmental standard\n\n\"Environmental standards are legal and administrative regulations or civil-law rules, which help to transform yet undetermined legal concept of environmental protection into definite norms via operationalization and standardization of measurable quantities. Reaching environmental standards is a complex process, which takes into account scientific insights of different disciplines, normative beliefs and values, and the general social context.\" Different environmental activities have different concerns and therefore different standards.\n\nThe considered definition of environment is the human environment as distinguished form the natural environment. The human environment considers that humans are permanently interlinked with surroundings, which are not just the natural elements as air, water and soil, but also culture, communication, cooperations and institutions. The humans live together in an anthropology distinct community. By setting environmental standards the nature should be protect against further damages and preserve of already existing damages, caused by the human way of living.\n\nEnvironmental standards were set within legal, administrative or private context.\n\nWhen environmental standards were discussed two main driving forces, rooted at every human being, are influencing the development process. On the one hand the radical ecocentrism of human beings and on the other side a moderate anthropocentrism. These contrary forces lead to problems in establishing standards, but the idea of shaping the world and preserve it for future generations is the leading keynote. Within the last years the sensibility of people towards the topic of environmental issues is increasing consequently thereof the demand of environmental protections has risen. This movement is caused by the development in science and medical knowledge as well as the improvement of measurement systems. With these improvements the knowledge about the impact of human way of living towards the environment, human and animal health is explored. The development in science is fundamental for the setting of the environmental standards within the last years.\n\nEnvironmental standards often define a desired state (e.g. lake pH should be between 6.5 and 7.5) or limit alterations (e.g. no more than 50% of natural forest may be damaged). To belong to a specific state or limit, which should define the standard, most likely statistical methods were used.\n\nConcerning environmental issues uncertainties must always be considered. The first step to develop a standard is the evaluation of the specific risk. The expected value of the occurrence of the risk must be calculated. Than the possible damage should be classified. Three different types of damages were distinguished: changes due to physical-chemical environmental damages, ecological damages in plants and animals and damages done to human health. To establish an acceptable risk, in view of the expected collective benefit, the risk-induced costs and the costs of risk avoidance must be social balanced. The comparison is difficult to express in monetary units. Furthermore the risks have multiple dimensions, which should reached a correlation at the end of the balance process. At the balancing process the following steps should be considered:\n\n\nInto the balancing process the fairness of distributing the risks and the resilience (benefits) with respect to sustaining the productivity of the environment should be observed too. In addition to the standard, an implementation rule, indicating under what circumstances the standard will be considered violated, is commonly part of the regulations. Penalties and other procedures for dealing with regions out of compliance with the standard may be part of the legislation. \n\nEnvironmental standards were set by different institutions. Most of the standards were based on the principle of voluntary self-commitment.\n\nWith 193 member states the largest intergovernmental organisation, the environmental policy of the UN has a huge impact for the setting of international environmental standards. At the Earth submit 1992 in Rio the member states acknowledged their negative impact towards the environment for the first time. During this and the following Millennium Declaration the first development goals for environmental issues were set. Since then the risk of catastrophic caused by extreme acts of God has enhanced, by overuse of natural resources and the global warming. At the Paris Agreement 2015 the UN determined 17 Goals for a sustainable development. Besides the fight against global poverty the main focus of the goals is the preservation of our planet. These goals set a basement for global environmentalism. The environmental areas of water, energy, oceans, ecosystems, sustainable production and consume behaviour and climate protection were covered by the goals. The goals contain explanations which mediums where required to reach them.\n\nQuestionable is the follow-up and inspection, whether the member states fulfil the settled goals, because some members perceive inspection, or any other control form any external parties as an intervention into their inner affairs. Caused by this the implementation and follow-up are only controlled by the Voluntary National Reviews. The main control is done by statistical values, which are called indicators. These indicators deliver information, if the goals are reached.\n\nWithin the “Treaty on the Functioning of the European Union“ the Union integrate a self-commitment towards the environment. In Title XX, Article 191.1 it is settled: “Union policy on the environment shall contribute to pursuit of the following objectives: — preserving, protecting and improving the quality of the environment, — protecting human health, — prudent and rational utilization of natural resources, — promoting measures at international level to deal with regional or worldwide environmental; problems, and in particular combating climate change.” All environmental actions are based on this article and lead to a lot of environmental laws. The European Union covers the topics: air, biotechnologies, chemical, civil protection, climate change, environmental economics, health, industry and technology, internationals, land use, nature and biodiversity, noise, protection of the ozonosphere, soil, sustainable development, waste and water.\n\nFurthermore the European Environment Agency (EEA) with consults the members states about environmental issues, including standards. \n\nSee also: Environmental policy of the European Union\n\nIn the United States of America the development of standards is decentralised. The standards were developed by more than hundred different institutions, a lot of them are private ones. It is a plural system, which is mainly affected by the market and partly fragmentated. In the following you find two concrete examples:Standards in the United States\n\nThe ambient standards, also known as the National Ambient Air Quality Standards (NAAQS), is set by the Environmental Protection Agency (EPA) to regulate how many pollutants are in the air. As part of the Clean Air Act initiative, ambient standards are set to ensure that people are breathing clean air. Furthermore, these regulation help preserve public welfare from any known pollutants that could potentially damage it. Furthermore, the enforcement of these set standards are designed to prevent further degradation of air quality. States may set their own ambient standards but must be lower than the national one in order to be enforced. The NAAQS regulates the six criteria air pollutants: sulfur dioxide (SO), particulate matter (PM), carbon monoxide (CO), ozone (O), nitrogen dioxide (NO), and lead (Pb). To ensure that the ambient standards are met, the EPA uses the Federal Reference Method (FRM) and Federal Equivalent Method (FEM) systems measure the amount of pollutants in the air are within the limits.\n\nEmission standards are national regulations, managed by the EPA, that control the amount of pollutants that can be released into atmosphere. These standards are put into place in order to maintain air quality and human health and regulate the release of greenhouse gases, such as carbon dioxide (CO), oxides of nitrogen and oxides of sulfur. The standards set are established in two phases to stay up to date, with final projections aiming to collectively save Americans $1.7 trillion in fuel costs and reduce the amount of greenhouse gas emissions (GHG) by 6 billion metric tons. Similar to the ambient standards, individuals states may also tighten regulations to their liking. For example, California set their own emissions standards through the California Air Resources Board (CARB), whose numbers were beginning to be adopted by other states. Emission standards also regulate the amount of pollutants released by heavy industry and for electricity.\n\nThe technological standards set by the EPA do not necessarily enforce the usage of specific technologies, but sets a minimum performance levels for different industries. Furthermore, these regulations allows data collection to be more precise, allowing for the standards set based off those information to be feasible. The EPA often encourage technological improvement by setting standards that are unachievable with current technologies. These standards are always set based on the industry's top performers to promote the overall improvement of the industry as a whole.\n\nThe International Organization of Standardization is the institution, which develops a large number of voluntary standards. With 163 member states it has a comprehensive outreach which their standards. The standards set by the ISO were often transmitted into national standards by different nations. About 363.000 companies and organisations worldwide have the ISO 14001 certificate, a standard for an environmental management, created to improve the environmental performance of an organisation and legal aspects as well as reaching environmental aims. Most of the national and international environmental management standards, including the ISO 14000 series. \n\nGreenpeace is one of the best known non-governmental organisation, which deals with biodiversity and environment. With their activities they have a great global impact on environmental issues. By activities recording special environmental issues they encourage the public attention and enforce governances or companies to adapt / set environmental standards. Their main focus is on forests, the sea, climate change and toxic chemicals. For example, referring to the toxic chemicals they set a standard together with the textile sector. They created the concept 2020, which has the idea to banish all toxic chemicals out of the textile production until 2020.\n\nThe WWF focus on how to produce the maximum yield in agriculture while conserving biodiversity. They try to educate, protect, reach policy changes and incentives to reach they aim. \n\nEnvironmental standards in the economy are set through external motivation. First of all companies need to fulfill the environmental law of the countries, in which they are operating. Moreover environmental standards were based on voluntary self-commitment, which means companies implement standards for their business, which exceed the level of the requirements of governmental regulations. If companies set further-reaching standards they try to fulfill the wishes of stakeholders. At the process of setting environmental standards three different stakeholders have the main influence. The first stakeholder, the government, is already mentioned and it is strongest determinate. Followed by the influence of the customers. Nowadays there is an increasing amount of people, wo consider environmental factor to their purchasing decision. The third stakeholder, who force companies to set environmental standards are industrial participants. If companies are part of industrial networks they are forced to fulfill the codes of conduct of this networks. This code of conducts is often set to improve the collective reputation of an industry. Another driving force of industry participants could be a reaction to a competitors action.\n\nThe environmental standards set by companies their self can be divided into two dimensions. First the operational environmental policies. This can be the environmental management, audits, controls or technologies. In this dimension the regulations tend to be closely connect with other function areas, e.g. lean production. Furthermore it could be occupied that multinational companies tend to set cross-country harmonised environmental government regulations and reach therefore higher performance level of environmental standards. The second dimension is the message send in advertising and public communications. It´s often criticized that companies focus on the second dimension. To satisfy the stakeholders requirement companies were focused on the public impression of their environmental self-commitment standards. Often the real implementation doesn´t play an important role. A lot of companies settle the responsibility for the implementation in low budget departments. The workers, which were in charge of the standards missing time and financial resources to guaranty a real implementation. Furthermore within the implementation goal conflicts arising. The biggest concern of companies is that the environmental protect is more expansive compared to the gained beneficial effects. But there are a lot of positive cost-benefit-calculation for environmental standards, set by companies their self. It is observed, that companies often set environmental standards after a public crisis. To the criticised nuisances a higher importance is assigned after the crises, to improve the environmental standards were implement after. Sometimes environmental standards were already set by companies to avoid public crises. If the environmental self-commitment standards are effective is controversial. \n\n\n"}
{"id": "2781008", "url": "https://en.wikipedia.org/wiki?curid=2781008", "title": "Falls of the Ohio State Park", "text": "Falls of the Ohio State Park\n\nFalls of the Ohio State Park is a state park in Indiana. It is located on the banks of the Ohio River at Clarksville, Indiana, across from Louisville, Kentucky. The park is part of the Falls of the Ohio National Wildlife Conservation Area. The exposed fossil beds of the Jeffersonville Limestone dated from the Devonian period are the main feature of the park. The Falls was the site where Lewis & Clark met for the Lewis and Clark Expedition.\n\nThe park includes an interpretive center open to the public. In 1990 the Indiana state government hired Terry Chase, a well-established exhibit developer, to design the center's displays. Building started in September 1992, costing $4.9 million with a total area of . The center functions as a museum with exhibits that concentrate on the natural history related to findings in the nearby fossil beds as well as the human history of the Louisville area, covering pre-settlement, early settlement, and Louisville and southern Indiana history all the way up through the 20th century.\nUnlike at other Indiana state parks, annual entrance permits do not allow unlimited free access (rather, only five people per pass per visit) to the interpretive center, as fees are still needed to reimburse the town of Clarksville for building the center.\n\nThe Woodland Loop Trail has ten new stainless steel markers denoting the plant life of the trails, thanks to an Eagle Scout project.\n\nStrange wildlife has a habit of showing up in the park. Living alligators and crocodiles have also been seen in the park. In August 2006 a fisherman hooked a dead octopus. Zachary Treitz, a 21-year-old Louisville college student, admitted he had put the octopus there after purchasing it dead from a local seafood shop for a film project.\n\n\n"}
{"id": "18197969", "url": "https://en.wikipedia.org/wiki?curid=18197969", "title": "Flag administration", "text": "Flag administration\n\nA merchant vessel flies a flag, typically at its stern. This flag indicates the legal jurisdiction to which it is subject. For example, a vessel may fly this flag of the Greece; in this case the authority responsible for enforcing maritime regulations would be the Flag Administration of Greece.\n"}
{"id": "54497950", "url": "https://en.wikipedia.org/wiki?curid=54497950", "title": "Floating solar", "text": "Floating solar\n\nFloating solar or FPV (Floating photovoltaic), refers to an array of solar panels on a structure that floats on a body of water, typically an artificial basin or a lake.\n\nThis technology has had a rapid growth on the renewable energy market since 2016 and in 2017 has overcome the 200 MW of installed power. The first 20 plants, of a few dozen of kWp have been built between 2008 and 2014 as reported in the MIRARCO paper that analyzed the birth of this technology.\n\nThe installed power in 2018 is foreseen to be near to 1 GW .\n\nThere are several reasons for this development:\n\n\nAmerican, Danish, French, Italian and Japanese nationals were the first to register patents for floating solar. In Italy the first registered patent, regarding PV modules on water, goes back to February 2008.\n\nThe MIRARCO (Mining Innovation Rehabilitation and Applied Research Corporation Ontario, CANADA) research group quotes several solutions that were put forward in the years 2008-2011 and 2012-2014. Without being exhaustive the installations can be classified into three categories:\n\n\nIt is Impossible to give a detailed analysis of the many small PV floating plants built in the first 10 years. The plot here below is based on data taken from the web for FPV with more than 500 kW of power. In the Asian Clean Energy Summit in Singapore (Oct. 2017) two number where quoted by the World- Bank Group: 200 MWp for installation in 2017 and a forecast of 750 MWp for 2018.\n"}
{"id": "38128895", "url": "https://en.wikipedia.org/wiki?curid=38128895", "title": "Gasholder house", "text": "Gasholder house\n\nA gasholder house is a structure built to surround an iron gasholder in which gas is stored until it is needed. Before the 1870s, most iron gasholders were constructed without a building structure, but following practices already common in the New England, gasholders houses were adopted in New York. Additionally, gasholder houses were constructed in England as early as 1825, although the mild climate made them less of an advantage.\n\nGasholder houses were built to protect the iron gas holder from the elements, and enabled it to be built from thinner plates. A gasholder house provided a number of advantages:\n\nThe gasholder house also provides economic advantage by reducing the condensation of gas in cold weather, and provided an attractive architectural element of the gas complex.\n\nThere are eleven known gasholder houses in the United States, with the structure in Troy, New York being one of the largest remaining structures of this type.\n"}
{"id": "49448561", "url": "https://en.wikipedia.org/wiki?curid=49448561", "title": "HUPX Hungarian Power Exchange", "text": "HUPX Hungarian Power Exchange\n\nThe Day-ahead Power Market was launched in July 2010 as part of the liberalization of the Hungarian energy sector. \n\nHUPX Intraday Market enables Members to trade 15 minute intraday products on the Hungarian Power Exchange. An organized Intraday Market provides more opportunities for market players to reduce their need for imbalance energy, to optimize power generation closer in time to delivery and it is also appropriate for managing forecast errors or unforeseen power plant outages.\n\nHUPX is also offering physically settled futures trading. This Market provides weekly, monthly, quarterly and yearly baseload/peakload contracts. Since the fourth quarter of 2015 initiator transaction fee was reduced to 0 EUR/MWh, which contributed to a significant boost in the liquidity. The traded volumes jumped from 3 627,3 GWh in 2014 to 6 485,6 GWh in 2015. In 1 January 2016 HUPX PHF listed 31 members. In addition to the settlement of transactions concluded on the exchange, OTC transactions can be registered as well.\n\n4M MC is an ATC based day-ahead implicit allocation process striving on the compatibility with the EU Target Model while taking into account the fact that 4M solution is to be considered as an interim step before the Central Eastern European (“CEE”) regional solution. The primary target of the market coupling is to assure a deeper integration of regional power markets. The process aims at maximizing the energy flow from the low price area to the high price area by taking into account the available cross-border capacity. Consequently, price levels among the individual market areas converge. By this procedure, not only the security of supply can be increased but the price volatility on relevant power markets can be moderated and the power sale and purchase can become more effective.\nThe Czech, Slovak, Hungarian and Romanian NRAs, TSOs and PXs jointly implemented the extension of CZ-SK-HU Market Coupling to Romanian day-ahead electricity market (4M MC) on 19 November 2014 based on PCR solution.\n\nAccording to the prevailing regulations of the Hungarian Electric Energy Act, there are six types of entities that can become HUPX Members on HUPX:\n\n"}
{"id": "4150964", "url": "https://en.wikipedia.org/wiki?curid=4150964", "title": "How to Survive a Robot Uprising", "text": "How to Survive a Robot Uprising\n\nHow to Survive a Robot Uprising: Tips on Defending Yourself Against the Coming Rebellion is a semi-satirical book by Daniel Wilson published in November 2005.\n\nThe book gives tongue-in-cheek advice on how one can survive in the event that robots become too intelligent and rebel against the human race. \"How to Survive a Robot Uprising\" is partially based on scientific fact, and is a prime example of deadpan humor.\n\n\"Wired\" magazine gave it a 2006 Rave Award, calling it \"equal parts sci-fi send-up and technical primer\". Maclean's called the book \"very funny and highly informative.\"\n\nIn 2007, the American Library Association designated the book a 2007 ALA “Popular Paperback for Young Adults”.\n\nWilson received a Ph.D. in Robotics from Carnegie Mellon University's Robotics Institute in Pittsburgh, Pennsylvania.\n\nIn the summer of 2005, Paramount Pictures optioned film rights to the book and hired Thomas Lennon and Robert Ben Garant (both members of \"The State\" comedy troupe and co-creators of the \"Reno 911!\" television series) to write a script based on the book. On April 26, 2006 comedian Mike Myers signed with Paramount to star in the movie adaptation.\n\n\n"}
{"id": "17781801", "url": "https://en.wikipedia.org/wiki?curid=17781801", "title": "IFF World Ranking", "text": "IFF World Ranking\n\nThe IFF World Ranking is a ranking of the national teams of member countries of the International Floorball Federation.\n\nThe ranking is based on the standings of the most recent Floorball World Championships .\n\nThe ranking is also used to determine the seeding of the teams for the next Floorball World Championships.\n"}
{"id": "20165918", "url": "https://en.wikipedia.org/wiki?curid=20165918", "title": "Japan Median Tectonic Line", "text": "Japan Median Tectonic Line\n\n, also Median Tectonic Line (MTL), is Japan's longest fault system. The MTL begins near Ibaraki Prefecture, where it connects with the Itoigawa-Shizuoka Tectonic Line (ISTL) and the Fossa Magna. It runs parallel to Japan's volcanic arc, passing through central Honshū to near Nagoya, through Mikawa Bay, then through the Inland Sea from the Kii Channel and Naruto Strait to Shikoku along the Sadamisaki Peninsula and the Bungo Channel and Hōyo Strait to Kyūshū.\n\nThe sense of motion on the MTL is right-lateral strike-slip, at a rate of about 5–10 mm/yr. This sense of motion is consistent with the direction of oblique convergence at the Nankai Trough. The rate of motion on the MTL is much less than the rate of convergence at the plate boundary, making it difficult to distinguish the motion on the MTL from interseismic elastic straining in GPS data.\n\nThe upper part of Japanese islands was created at the edge of Eurasian plate 180 million years ago.Then, 50 million years later, the lower part was created at the southern part of Yangtze continent. The lower part then rode on Izanagi plate and moved to the upper part. These islands were incorporated and formed the Japanese islands 70 million years ago. The boundary line between the upper part and lower part is the MTL.\n\nThe most important part of the MTL is called which is an active fault and considered hazardous. It is separated into six regions defined by the most recent seismic activities. Although the MTL is a less seismically active region than others, earthquakes with magnitudes of 6 to 7 have occurred. The oldest known activity occurred about 3000 years ago, and the most recent in the 16th century. It is expected that the seismic activities will continue in the future. The boundaries of the regions are still being discussed, so the regions are not well defined.\n\n\nThe Great Hanshin earthquake of 1995 occurred on the Nojima Fault, a branch of the MTL. Approximately 6,434 people lost their lives; about 4,600 of them were from Kobe. It caused approximately ten trillion yen ($100 billion) in damage, 2.5% of Japan's GDP at the time.\n\nThe MTL goes from Shizuoka Prefecture to Nagano Prefecture. At Nagano prefecture, it lies between the Ina mountains and the Akaishi mountains. Then, it goes to Tino-shi, the northern part of Nagano prefecture.\n\nThe MTL goes from Shimonita in Gunma Prefecture to the Hiki hill district in Saitama Prefecture, and then continues down to the Kantō Plain. In the central Kanto Plain, boring tests have shown that the MTL lies 3000 meters beneath ground level in Saitama Prefecture. It is not clear where the MTL goes from there, but it is thought that it passes beneath Kashima or Ōarai on the Pacific coast in Ibaraki Prefecture.\n\nThe MTL passes south of Awaji Island as it crosses the Kii Channel from Shikoku to the Kii Peninsula. On the Kii Peninsula, it passes north of the Kinokawa River, from Wakayama on the west coast past Gojō in Nara Prefecture (where the river is called the Yoshino River) to Mie Prefecture on the east coast. There is an active shear zone north of the river in the southern foothills of the Izumi Mountains, which separate Wakayama Prefecture from Osaka Prefecture. East of Nara Prefecture, shear zones along the MTL are less active. The MTL takes a northern turn and passes along the eastern foothills of the Kongō Range, connecting to the shear zone along the east side of the Ikoma Mountains between Nara Prefecture and Osaka Prefecture.\n\nThe MTL is known to pass through the region between Kunisaki Peninsula and Saganoseki Peninsula in Ōita Prefecture. However, because the Ōno River basin south of Ōita is covered by late Mesozoic sedimentary rock, the geological continuity in the westward direction is unclear. According to one theory, the MTL may turn south around the Ōno River, connecting Usuki in Ōita Prefecture to the Yatsushiro tectonic line in Kumamoto Prefecture. According to another theory, it may connect Ōita directly to the Kumamoto tectonic line.\n\nAfter crossing the Hōyo Strait from Kyushu to Shikoku, the MTL passes north of the Sadamisaki Peninsula to land at Iyo on the west coast of Ehime Prefecture. It continues eastward past the city of Saijō and along the foothills of the Ishizuchi Range. From Miyoshi in Tokushima Prefecture it passes along the northern side of the Yoshino River until it arrives at the city of Tokushima on the east coast of the island. There are active shear zones to the north of the MTL: north of Iyo and, in eastern Shikoku, along the southern foothills of the Sanuki Mountains, at the northern border of Tokushima Prefecture.\n\nThere is a museum dedicated to the tectonic line in Minami-Alps, Yamanashi.\n"}
{"id": "376544", "url": "https://en.wikipedia.org/wiki?curid=376544", "title": "Kitt Peak", "text": "Kitt Peak\n\nKitt Peak () is a mountain in the U.S. state of Arizona, and at is the highest point in the Quinlan Mountains. It is the location of the Kitt Peak National Observatory. The radio telescope at the Observatory is one of ten dishes comprising the Very Long Baseline Array radio telescope.\n\nThe peak was named in English by County Surveyor George J. Roskruge for his sister, Phillippa, who was the wife of William F. Kitt. On his 1893 Pima County Survey map, Roskruge spelled the name 'Kits'. At the request of the wife of George F. Kitt, the spelling was changed by decision in 1930.\n\nKitt Peak is the second-highest peak on the Tohono O'odham Indian Reservation, and as such is the second-most sacred after Baboquivari Peak. Near the summit is I'itoi's Garden, the summer residence of the nation's elder brother deity. The name Ioligam means \"red stick\" in reference to the abundance of manzanita bushes on and around the mountain.\n"}
{"id": "19159702", "url": "https://en.wikipedia.org/wiki?curid=19159702", "title": "Latex", "text": "Latex\n\nLatex is a stable dispersion (emulsion) of polymer microparticles in an aqueous medium. It is found in nature, but synthetic latexes can be made by polymerizing a monomer such as styrene that has been emulsified with surfactants.\n\nLatex as found in nature is a milky fluid found in 10% of all flowering plants (angiosperms). It is a complex emulsion consisting of proteins, alkaloids, starches, sugars, oils, tannins, resins, and gums that coagulate on exposure to air. It is usually exuded after tissue injury. In most plants, latex is white, but some have yellow, orange, or scarlet latex. Since the 17th century, latex has been used as a term for the fluid substance in plants. It serves mainly as defense against herbivorous insects. Latex is not to be confused with plant sap; it is a separate substance, separately produced, and with separate functions.\n\nThe word latex is also used to refer to natural latex rubber, particularly non-vulcanized rubber. Such is the case in products like latex gloves, latex condoms and latex clothing.\n\nOriginally, the name given to latex by indigenous Equator tribes who cultivated the plant was “caoutchouc”, from the words “caa” (tear) and “ochu” (tree), because of the way it is collected.\n\nThe cells (laticifers) in which latex is found make up the laticiferous system, which can form in two very different ways. In many plants, the laticiferous system is formed from rows of cells laid down in the meristem of the stem or root. The cell walls between these cells are dissolved so that continuous tubes, called latex vessels, are formed. Since these vessels are made of many cells, they are known as \"articulated laticifers\". This method of formation is found in the poppy family and in the rubber trees (Para rubber tree, members of the family Euphorbiaceae, members of the mulberry and fig family, such as the Panama rubber tree \"Castilla elastica\"), and members of the family Asteraceae. For instance, \"Parthenium argentatum\" the guayule plant, is in the tribe Heliantheae; other latex-bearing Asteraceae with articulated laticifers include members of the Cichorieae, a clade whose members produce latex, some of them in commercially interesting amounts. This includes \"Taraxacum kok-saghyz\", a species cultivated for latex production.\n\nIn the milkweed and spurge families, on the other hand, the laticiferous system is formed quite differently. Early in the development of the seedling, latex cells differentiate, and as the plant grows these latex cells grow into a branching system extending throughout the plant. In many euphorbs, the entire structure is made from a single cell – this type of system is known as a \"non-articulated laticifer\", to distinguish it from the multi-cellular structures discussed above. In the mature plant, the entire laticiferous system is descended from a single cell or group of cells present in the embryo.\n\nThe laticiferous system is present in all parts of the mature plant, including roots, stems, leaves, and sometimes the fruits. It is particularly noticeable in the cortical tissues. Latex is usually exuded as a white liquid, but is some cases it can be clear, yellow or red, as in Cannabaceae.\n\nLatex is produced by 20,000 species from over 40 families occurring in multiple lineages in both dicotyledonous and monocotyledonous types of plant. It is also found in conifers and pteridophytes. Among tropical plant species 14% create latex, as opposed to 6% of temperate plant species. Several members of the fungal kingdom also produce latex upon injury, such as \"Lactarius deliciosus\" and other milk-caps. This suggests it is the product of convergent evolution and has been selected for on many separate occasions.\n\nLatex functions to protect the plant from herbivores. The idea was first proposed in 1887 by Joseph F. James, who noted that latex carries with it at the same time such disagreeable properties that it becomes a better protection to the plant from enemies than all the thorns, prickles, or hairs that could be provided. In this plant, so copious and so distasteful has the sap become that it serves a most important purpose in its economy. \nEvidence showing this defense function include the finding that slugs will eat leaves drained of their latex but not intact ones, that many insects sever the veins carrying latex before they feed, and that the latex of \"Asclepias humistrata\" (sandhill milkweed) kills by trapping 30% of newly hatched monarch butterfly caterpillars.\n\nOther evidence is that latex contains 50–1000× higher concentrations of defense substances than other plant tissues. These toxins include ones that are also toxic to the plant and consist of a diverse range of chemicals that are either poisonous or \"antinutritive\". Latex is actively moved to the area of injury; in the case of \"Cryptostegia grandiflora\", this can be more than 70 cm.\n\nThe clotting property of latex is functional in this defense since it limits wastage and its stickiness traps insects and their mouthparts.\n\nIt has been noted that while there exist other explanations for the existence of latex including storage and movement of plant nutrients, waste, and maintenance of water balance that \"[e]ssentially none of these functions remain credible and none have any empirical support\".\n\nThe latex of many species can be processed to produce many materials.\n\nNatural rubber is the most important product obtained from latex; more than 12,000 plant species yield latex containing rubber, though in the vast majority of those species the rubber is not suitable for commercial use. This latex is used to make many other products including mattresses, gloves, swim caps, condoms, catheters and balloons.\n\nBalatá and gutta percha latex contain an inelastic polymer related to rubber.\n\nLatex from the chicle and jelutong trees is used in chewing gum.\n\nDried latex from the opium poppy is called opium, the source of several useful alkaloids, such as morphine, codeine and papaverine, as well as the street drug heroin.\n\nSynthetic latexes are used in coatings (e.g. latex paint) and glues because they solidify by coalescence of the polymer particles as the water evaporates, and therefore can form films without releasing potentially toxic organic solvents in the environment. Other uses include cement additives, and to conceal information on scratchcards. Latex, usually styrene-based, is also used in immunoassays.\n\nLatex is used in many types of clothing. Worn on the body (or applied directly by painting) it tends to be skin-tight, producing a \"second skin\" effect.\n\nSome people have a serious latex allergy, and exposure to latex products such as latex gloves can cause anaphylactic shock. Guayule latex has only 2% of the levels of protein found in \"Hevea\" latexes, and is being researched as a lower-allergen substitute. Additionally, chemical processes may be employed to reduce the amount of antigenic protein in \"Hevea\" latex, yielding alternative materials such as Vytex Natural Rubber Latex which provide significantly reduced exposure to latex allergens.\n\nAbout half of people with spina bifida are also allergic to natural latex rubber, as well as people who have had multiple surgeries, and people who have had prolonged exposure to natural latex.\n\nSeveral species of the microbe genera Actinomycetes, Streptomyces, Nocardia, Micromonospora, and Actinoplanes are capable of consuming rubber latex. However, the rate of biodegradation is slow, and the growth of bacteria utilizing rubber as a sole carbon source is also slow.\n\n"}
{"id": "1273889", "url": "https://en.wikipedia.org/wiki?curid=1273889", "title": "List of F5 and EF5 tornadoes", "text": "List of F5 and EF5 tornadoes\n\nAmong the most violent known meteorological events are tornadoes. Each year, more than 2,000 tornadoes occur worldwide, with the vast majority occurring in the United States and Europe. In order to assess the intensity of these events, meteorologist Ted Fujita devised a method to estimate maximum winds within the storm based on damage caused; this became known as the Fujita scale. At the top end of the scale, which ranks from 0 to 5, are F5 tornadoes. These storms were estimated to have had winds between and . Following two particularly devastating tornadoes in 1997 and 1999, engineers questioned the reliability of the scale. Ultimately, a new scale was devised that took into account 28 different damage indicators; this became known as the Enhanced Fujita scale. With building designs taken more into account, winds in an EF5 tornado were estimated to be in excess of .\n\nSince 1950, there have been 59 officially rated F5 and EF5 tornadoes in the United States and 1 F5 in Canada. Additionally, the works of tornado expert Thomas P. Grazulis revealed the existence of several dozen more between 1880 and 1995. Grazulis also put into question the ratings of several currently rated F5 tornadoes. Outside the United States and Canada, seven tornadoes have been rated F5: two each in France, Germany, and Italy and one in Russia. Several other tornadoes are also documented as possibly attaining this status.\n\nSince structures are completely destroyed in both cases, the identification and assignment of scale between an EF4 tornado and an EF5 is often very difficult.\n\nThe tornadoes on this list have been officially rated F5 by an official government source. Unless otherwise noted, the tornadoes on this list have been rated F5 by the National Weather Service (NWS), as shown in the archives of the Storm Prediction Center and National Climatic Data Center (NCDC).\n\nPrior to 1950, assessments of F5 tornadoes are mostly based on the works of Thomas Grazulis. The NCDC accepted 38 of his classifications from between 1880 and 1950 as F5s. In addition to the accepted ones, he rated a further 25 during the same period. From 1950 to 1970 tornadoes were assessed retrospectively, primarily using information recorded in government databases, and newspaper photographs and descriptions. Beginning in 1971, tornadoes were rated by the NWS using on-site damage surveys.\n\nFor United States tornadoes as of February 1, 2007, the Fujita scale has been recalibrated to more accurately match tornado speeds with their damage and to augment and refine damage descriptors. The new system is called the Enhanced Fujita scale. No earlier tornadoes will be reclassified, and no new tornadoes in the United States will be rated F5. France and Canada later adopted the EF-scale in years following.\n\nIn all, 63 tornadoes have been officially rated F5/EF5 since 1950: 59 in the United States and one each in Italy, France, Russia, and Canada. The works of Grazulis also revealed 16 more F5 tornadoes between 1950 and 1995, with four later being accepted by the NCDC. Since the implementation of the Enhanced Fujita scale on February 1, 2007, there have been nine officially rated EF5 tornadoes in the United States from May 4th, 2007-May 20th, 2013 to date, while previously fifty tornadoes were officially rated F5 on the original Fujita scale from May 11th, 1953-May 3rd, 1999.\n\n\n\n\n"}
{"id": "36444983", "url": "https://en.wikipedia.org/wiki?curid=36444983", "title": "List of Phaeocollybia species", "text": "List of Phaeocollybia species\n\nThis is a list of species in the agaric genus \"Phaeocollybia\".\n"}
{"id": "6183078", "url": "https://en.wikipedia.org/wiki?curid=6183078", "title": "List of Sites of Special Scientific Interest in Gordon and Aberdeen", "text": "List of Sites of Special Scientific Interest in Gordon and Aberdeen\n\nThe following is a list of Sites of Special Scientific Interest in the Gordon and Aberdeen Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "6169380", "url": "https://en.wikipedia.org/wiki?curid=6169380", "title": "List of Sites of Special Scientific Interest in Kincardine and Deeside", "text": "List of Sites of Special Scientific Interest in Kincardine and Deeside\n\nThe following is a list of Sites of Special Scientific Interest in the Kincardine and Deeside Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "17807759", "url": "https://en.wikipedia.org/wiki?curid=17807759", "title": "List of countries by oil consumption", "text": "List of countries by oil consumption\n\nThe total worldwide oil consumption is 93 million bbl/day according to the International Energy Agency (IEA).\n\nIn 2010, world energy consumption of refined products increased 3.8%; which was the first increase since 2004. According to Enerdata, this trend was supported by fast-growing demand for road and air transport, particularly in developing countries. In China, demand for refined products surged by 12% due to increasing needs. Asia accounted for more than 40% of the overall increase in consumption.\n\nIn Latin America, demand rose sharply by 5.7%, representing 13% of the increase. In CIS, consumption grew by 7.3% (8.9% in Russia), while rising 4.2% in the Middle East (driven by Saudi Arabia and Kuwait). Each region contributed 8% to the overall increase.\n\nDespite this growth, the US remains by far the largest user of oil, consuming more than China in 2010.\nChina may overtake the US around 2030. \n\nThis is a list of countries by oil consumption. \n"}
{"id": "25488797", "url": "https://en.wikipedia.org/wiki?curid=25488797", "title": "List of largest power stations", "text": "List of largest power stations\n\nThis article lists the largest power stations in the world, the ten overall and the five of each type, in terms of current installed electrical capacity. Non-renewable power stations are those that run on coal, fuel oils, nuclear, natural gas, oil shale and peat, while renewable power stations run on fuel sources such as biomass, geothermal heat, hydro, solar energy, solar heat, tides, waves and the wind. Only the most significant fuel source is listed for power stations that run on multiple sources.\n\nAt present, the largest power generating facility ever built is the Three Gorges Dam in China. The facility generates power by utilizing 32 Francis turbines each having a capacity of and two turbines, totalling the installed capacity to , more than twice the installed capacity of the largest nuclear power station, the Kashiwazaki-Kariwa (Japan) at . As of 2017 no power station comparable to Three Gorges is under construction, as the largest under construction power stations are hydroelectric Baihetan Dam () and Belo Monte Dam ().\n\nAlthough currently only a proposal, the Grand Inga Dam in the Congo would surpass all existing power stations, including the Three Gorges Dam, if construction commences as planned. The design targets to top in installed capacity, nearly twice that of the Three Gorges. Another proposal, Penzhin Tidal Power Plant Project, presumes an installed capacity up to .\n\nAt any point in time since the early 20th century, the largest power station in the world has been a hydroelectric power plant.\n\nThe following table lists the largest operating power station, and the largest single generating unit within each country.\n\n\n"}
{"id": "4265532", "url": "https://en.wikipedia.org/wiki?curid=4265532", "title": "List of old-growth forests", "text": "List of old-growth forests\n\nThis is a list of existing old-growth (\"virgin\") forests, or remnants of forest, of at least . ecoregion information from \"Terrestrial Ecoregions of the World\".\n\nIn Australia, the 1992 \"National Forest Policy Statement\" (NFPS) made specific provision for the protection of old growth forests. The NFPS initiated a process for undertaking assessments of forests for conservation values, including old growth values. A working group of state and Australian Government agencies took the NFPS definition into consideration in developing a definition that was accepted by all governments (JANIS 1997). \nIn 2008, only a relatively small area (15%) of Australia's forests (mostly tall, wet forests) had been assessed for old-growth values.\n\nOf the of forest in Australia assessed for their old-growth status, (22%) is classified as old-growth. Almost half of Australia's identified old-growth forest is in NSW, mostly on public land. More than 73% of Australia's identified old-growth forests are in formal or informal nature conservation reserves.\n\nIn 2001, Western Australia became the first state in Australia to cease logging in old-growth forests.\n\nThe term \"old-growth forests\" is rarely used in New Zealand, instead, \"The Bush\" is used to refer to native forests. There are large contiguous areas of forest cover that are protected areas.\n\n"}
{"id": "45031274", "url": "https://en.wikipedia.org/wiki?curid=45031274", "title": "List of protected areas of Roskilde Municipality", "text": "List of protected areas of Roskilde Municipality\n\nThis list of protected areas of Roskilde Municipality is a list of protected areas of Roskilde Municipality, Denmark. It is based on a list compiled by the Danish Society for Nature Conservation. The list comprises all localities that are under landscape protection. For heritage listed buildings, see the list of listed buildings in Roskilde Municipality.\n"}
{"id": "4079600", "url": "https://en.wikipedia.org/wiki?curid=4079600", "title": "Lists of useful plants", "text": "Lists of useful plants\n\nThis page contains a list of useful plants, meaning a plant that has been or can be co-opted by humans to fulfill a particular need. Rather than listing all plants on one page, this page instead collects the lists and categories for the different ways in which a plant can be used; some plants may fall into several of the categories or lists below, and some lists overlap (for example, the term \"crop\" covers both edible and non-edible agricultural products).\n\n\n\n\n\n\n"}
{"id": "47958635", "url": "https://en.wikipedia.org/wiki?curid=47958635", "title": "National Hydrogen and Fuel Cell Day", "text": "National Hydrogen and Fuel Cell Day\n\nNational Hydrogen and Fuel Cell Day was created by the Fuel Cell and Hydrogen Energy Association to help raise awareness of fuel cell and hydrogen technologies and to celebrate how far the industry has come as well as the vast potential the technologies have today and in future. \n\nOctober 8th (10.08) was chosen in reference to the atomic weight of hydrogen (1.008).\n\nNational Hydrogen and Fuel Cell Day was officially launched on October 8, 2015. \n\nThe Fuel Cell and Hydrogen Energy Association (FCHEA), its members, industry organizations, allied groups, state and federal governments and individuals are commemorating National Hydrogen and Fuel Cell Day with a variety of activities and events across the country.\n\n\n"}
{"id": "17390365", "url": "https://en.wikipedia.org/wiki?curid=17390365", "title": "Natural History of an Alien", "text": "Natural History of an Alien\n\nNatural History of an Alien also known as Anatomy of an Alien in the US was an early Discovery Channel mockumentary similar to \"Alien Planet\", aired in 1998. This mockumentary featured various alien ecosystem projects from the Epona Project to Ringworld. It also featured many notable scientists and science fiction authors such as Dr. Jack Cohen, Derek Briggs, Christopher McKay, David Wynn-Williams, Emily Holton, Peter Cattermole, Brian Aldiss, Sil Read, Wolf Read, Edward Smallwood, Adega Zuidema, Steve Hanly, Kevin Warwick and Dougal Dixon.\n\n\nThe viewer is in an intergalactic spaceship named the S.S. Attenborough run by a small green alien.\n\nEarth during the Cambrian.\n\nThey visit asteroids and talk about the possibility of panspermia seeding solar system with life.\n\nThey visit a high gravity planet, where they encounter many insect-like aliens who have adapted to 1.5 times Earth's gravity. High gravity means a thicker atmosphere and easier flight.\n\nSurface Gravity: 149% of Earth\n\nAtmospheric Pressure: 15 times Earth's\n\nThey visit the science fiction world of \"Helliconia\" which was created by Brian Aldiss. It's a binary system and they show how life can adapt to having 2 suns.\n\nThey visit the science fiction world of \"Sulfuria\" which was created by Dougal Dixon. It is a sulfur rich world that is similar to Io.\n\nThey visit Epona an imaginary ecosystem created by group scientists and science fiction writers called The Epona Project. On this elaborate ecosystem are species such as \"Pagoda Tree\", \"Spring Croc\" and \"Uther\".\n\nThey visit the science fiction world of \"Greenworld\" which was created by Dougal Dixon. It is an Earth-like planet filled with lush rainforests. Some of the more notable species include the \"Curlywhorl\", \"Pud\" and \"Armored Kwank\", all derived from seastar-like ancestors.\n\nThe ship encounters an artificial lifeforms from a robotic cube ship. It uses solar panels to gather energy and mines asteroids to get resources to grow. It even sends down a probe to \"Greenworld\" explore it. One such probe looked like a metal centipede in appearance.\n\n"}
{"id": "23580", "url": "https://en.wikipedia.org/wiki?curid=23580", "title": "Paleogene", "text": "Paleogene\n\nThe Paleogene (; also spelled Palaeogene or Palæogene; informally Lower Tertiary or Early Tertiary) is a geologic period and system that spans 43 million years from the end of the Cretaceous Period million years ago (Mya) to the beginning of the Neogene Period Mya. It is the beginning of the Cenozoic Era of the present Phanerozoic Eon. The Paleogene is most notable for being the time during which mammals diversified from relatively small, simple forms into a large group of diverse animals in the wake of the Cretaceous–Paleogene extinction event that ended the preceding Cretaceous Period. \n\nThis period consists of the Paleocene, Eocene, and Oligocene epochs. The end of the Paleocene (55.5/54.8 Mya) was marked by the Paleocene–Eocene Thermal Maximum, one of the most significant periods of global change during the Cenozoic, which upset oceanic and atmospheric circulation and led to the extinction of numerous deep-sea benthic foraminifera and on land, a major turnover in mammals. The terms 'Paleogene System' (formal) and 'lower Tertiary System' (informal) are applied to the rocks deposited during the 'Paleogene Period'. The somewhat confusing terminology seems to be due to attempts to deal with the comparatively fine subdivisions of time possible in the relatively recent geologic past, for which more details are preserved. By dividing the Tertiary Period into two periods instead of directly into five epochs, the periods are more closely comparable to the duration of 'periods' of the preceding Mesozoic and Paleozoic Eras.\n\nThe global climate during the Paleogene departed from the hot and humid conditions of the late Mesozoic era and began a cooling and drying trend which, despite having been periodically disrupted by warm periods such as the Paleocene–Eocene Thermal Maximum, persisted until the temperature begun to rise again during the Anthropocene, ca. 1945. The trend was partly caused by the formation of the Antarctic Circumpolar Current, which significantly lowered oceanic water temperatures. A 2018 published study estimated that during the early Palaeogene about 56-48 million years ago, annual air temperatures, over land and at mid-latitude, averaged about 23–29 °C (± 4.7 °C), which is 5–10 °C higher than most previous estimates. Or for comparison, it was 10 to 15 °C higher than current annual mean temperatures in these areas; the authors suggest that the current atmospheric carbon dioxide trajectory, if it continues, could establish these temperatures again.\n\nDuring the Paleogene, the continents continued to drift closer to their current positions. India was in the process of colliding with Asia, subsequently forming the Himalayas. The Atlantic Ocean continued to widen by a few centimeters each year. Africa was moving north to meet with Europe and form the Mediterranean Sea, while South America was moving closer to North America (they would later connect via the Isthmus of Panama). Inland seas retreated from North America early in the period. Australia had also separated from Antarctica and was drifting toward Southeast Asia.\n\nMammals began a rapid diversification during this period. After the Cretaceous–Paleogene extinction event, which saw the demise of the non-avian dinosaurs, mammals transformed from a few small and generalized forms that began to evolve into most of the modern varieties we see today. Some of these mammals would evolve into large forms that would dominate the land, while others would become capable of living in marine, specialized terrestrial, and airborne environments. Those that took to the oceans became modern cetaceans, while those that took to the trees became primates, the group to which humans belong. Birds, which were already well established by the end of the Cretaceous, also experienced an adaptive radiation as they took over the skies left empty by the now extinct Pterosaurs. In comparison to birds and mammals, most other branches of life remained relatively unchanged during this period.\n\nAs the Earth cooled, tropical plants became less numerous and were now restricted to equatorial regions. Deciduous plants, which could survive through the seasonal climates the world was now experiencing, became more common.\n\nThe Paleogene is notable in the context of offshore oil drilling, and especially in Gulf of Mexico oil exploration, where it is commonly referred to as the \"Lower Tertiary\". These rock formations represent the current cutting edge of deep-water oil discovery.\n\nLower Tertiary rock formations encountered in the Gulf of Mexico oil industry usually tend to be comparatively high temperature and high pressure reservoirs, often with high sand content (70%+) or under very thick evaporite sediment layers.\n\nLower Tertiary explorations to date include (partial list):\n\n"}
{"id": "41492", "url": "https://en.wikipedia.org/wiki?curid=41492", "title": "Path loss", "text": "Path loss\n\nPath loss (or path attenuation) is the reduction in power density (attenuation) of an electromagnetic wave as it propagates through space. Path loss is a major component in the analysis and design of the link budget of a telecommunication system.\n\nThis term is commonly used in wireless communications and propagation. Path loss may be due to many effects, such as free-space loss, refraction, diffraction, reflection, aperture-medium coupling loss, and absorption. Path loss is also influenced by terrain contours, environment (urban or rural, vegetation and foliage), propagation medium (dry or moist air), the distance between the transmitter and the receiver, and the height and location of antennas.\n\nPath loss normally includes \"propagation losses\" caused by the natural expansion of the radio wave front in free space (which usually takes the shape of an ever-increasing sphere), \"absorption losses\" (sometimes called penetration losses), when the signal passes through media not transparent to electromagnetic waves, \"diffraction losses\" when part of the radiowave front is obstructed by an opaque obstacle, and losses caused by other phenomena.\n\nThe signal radiated by a transmitter may also travel along many and different paths to a receiver simultaneously; this effect is called multipath. Multipath waves combine at the receiver antenna, resulting in a received signal that may vary widely, depending on the distribution of the intensity and relative propagation time of the waves and bandwidth of the transmitted signal. The total power of interfering waves in a Rayleigh fading scenario varies quickly as a function of space (which is known as \"small scale fading\"). Small-scale fading refers to the rapid changes in radio signal amplitude in a short period of time or distance of travel.\n\nIn the study of wireless communications, path loss can be represented by the path loss exponent, whose value is normally in the range of 2 to 4 (where 2 is for propagation in free space, 4 is for relatively lossy environments and for the case of full specular reflection from the earth surface—the so-called flat earth model). In some environments, such as buildings, stadiums and other indoor environments, the path loss exponent can reach values in the range of 4 to 6. On the other hand, a tunnel may act as a waveguide, resulting in a path loss exponent less than 2.\n\nPath loss is usually expressed in dB. In its simplest form, the path loss can be calculated using the formula\n\nwhere formula_2 is the path loss in decibels, formula_3 is the path loss exponent, formula_4 is the distance between the transmitter and the receiver, usually measured in meters, and formula_5 is a constant which accounts for system losses.\n\nRadio and antenna engineers use the following simplified formula (derived from the Friis Transmission Formula) for the signal path loss between the feed points of two isotropic antennas in free space:\n\nPath loss in dB: \n\nwhere formula_2 is the path loss in decibels, formula_8 is the wavelength and formula_4 is the transmitter-receiver distance in the same units as the wavelength. Note the power density in space has no dependency on formula_8; The variable formula_8 exists in the formula to account for the effective capture area of the isotropic receiving antenna.\n\nCalculation of the path loss is usually called \"prediction\". Exact prediction is possible only for simpler cases, such as the above-mentioned \"free space\" propagation or the \"flat-earth model\". For practical cases the path loss is calculated using a variety of approximations.\n\n\"Statistical\" methods (also called \"stochastic\" or \"empirical\") are based on measured and averaged losses along typical classes of radio links. Among the most commonly used such methods are Okumura-Hata, the COST Hata model, W.C.Y.Lee, etc. These are also known as \"radio wave propagation models\" and are typically used in the design of cellular networks and public land mobile networks (PLMN). For wireless communications in the very high frequency (VHF) and ultra high frequency (UHF) frequency band (the bands used by walkie-talkies, police, taxis and cellular phones), one of the most commonly used methods is that of Okumura-Hata as refined by the COST 231 project. Other well-known models are those of Walfisch-Ikegami, W.C.Y. Lee, and Erceg. For FM radio and TV broadcasting the path loss is most commonly predicted using the ITU model as described in P.1546 (successor to P.370) recommendation.\n\nDeterministic methods based on the physical laws of wave propagation are also used; ray tracing is one such method. These methods are expected to produce more accurate and reliable predictions of the path loss than the empirical methods; however, they are significantly more expensive in computational effort and depend on the detailed and accurate description of all objects in the propagation space, such as buildings, roofs, windows, doors, and walls. For these reasons they are used predominantly for short propagation paths. Among the most commonly used methods in the design of radio equipment such as antennas and feeds is the finite-difference time-domain method.\n\nThe path loss in other frequency bands (medium wave (MW), shortwave (SW or HF), microwave (SHF)) is predicted with similar methods, though the concrete algorithms and formulas may be very different from those for VHF/UHF. Reliable prediction of the path loss in the SW/HF band is particularly difficult, and its accuracy is comparable to weather predictions.\n\nEasy approximations for calculating the path loss over distances significantly shorter than the distance to the radio horizon:\n\n\nIn cellular networks, such as UMTS and GSM, which operate in the UHF band, the value of the path loss in built-up areas can reach 110–140 dB for the first kilometer of the link between the base transceiver station (BTS) and the mobile. The path loss for the first ten kilometers may be 150–190 dB (\"Note\": These values are very approximate and are given here only as an illustration of the range in which the numbers used to express the path loss values \"can eventually be\", these are not definitive or binding figures—the path loss may be very different for the same distance along two different paths and it can be different even along the same path if measured at different times.)\n\nIn the radio wave environment for mobile services the mobile antenna is close to the ground. Line-of-sight propagation (LOS) models are highly modified. The signal path from the BTS antenna normally elevated above the roof tops is refracted down into the local physical environment (hills, trees, houses) and the LOS signal seldom reaches the antenna. The environment will produce several deflections of the direct signal onto the antenna, where typically 2-5 deflected signal components will be vectorially added.\n\nThese refraction and deflection processes cause loss of signal strength, which changes when the mobile antenna moves (Raleigh fading), causing instantaneous variations of up to 20 dB. The network is therefore designed to provide an excess of signal strength compared to LOS of 8-25 dB depending on the nature of the physical environment, and another 10 dB to overcome the fading due to movement.\n\n"}
{"id": "15181192", "url": "https://en.wikipedia.org/wiki?curid=15181192", "title": "Plastic crystal", "text": "Plastic crystal\n\nA plastic crystal is a crystal composed of weakly interacting molecules that possess some orientational or conformational degree of freedom. The name plastic crystal refers to the mechanical softness of such phases: they resemble waxes and are easily deformed. If the internal degree of freedom is molecular rotation, the name rotor phase or rotatory phase is also used. Typical examples are the modifications Methane I and Ethane I. \nIn addition to the conventional molecular plastic crystals, there are also emerging ionic plastic crystals, particularly organic ionic plastic crystals (OIPCs) and protic organic ionic plastic crystals (POIPCs). POIPCs are solid protic organic salts formed by proton transfer from a Brønsted acid to a Brønsted base and in essence are protic ionic liquids in the molten state, have found to be promising solid-state proton conductors for high temperature proton exchange membrane fuel cells. Examples include 1,2,4-triazolium perfluorobutanesulfonate and imidazolium methanesulfonate.\n\nIf the internal degree of freedom freezes in a disordered way, an orientational glass is obtained.\n\nThe orientational degree of freedom may be an almost free rotation, or it may be a jump diffusion between a restricted number of possible orientations, as was shown for carbon tetrabromide.\n\nThe X-ray diffraction patterns of plastic crystals are characterized by strong diffuse intensity in addition to the sharp Bragg peaks. In a powder pattern this intensity appears to resemble an amorphous background as one would expect for a liquid, but for a single crystal the diffuse contribution reveals itself to be highly structured. The Bragg peaks can be used to determine an average structure but due to the large amount of disorder this is not very insightful. It is the structure of the diffuse scattering that reflects the details of the constrained disorder in the system. Recent advances in two-dimensional detection at synchrotron beam lines facilitate the study of such patterns.\n\nLike liquid crystals, plastic crystals can be considered a transitional stage between real solids and real liquids and can be considered \"soft matter\". Another common denominator is the simultaneous presence of order and disorder. Both types of phases are usually observed between the true solid and liquid phases on the temperature scale:\n\nThe difference between liquid and plastic crystals is easily observed in X-ray diffraction. Plastic crystals possess strong long range order and therefore show sharp Bragg reflections. Liquid crystals show no or very broad Bragg peaks because the order is not long range. The molecules that give rise to liquid crystalline behavior often have a strongly elongated or disc like shape. Plastic crystals consist usually of almost spherical objects. In this respect one could see them as opposites.\n"}
{"id": "1790153", "url": "https://en.wikipedia.org/wiki?curid=1790153", "title": "Power Shift", "text": "Power Shift\n\nPower Shift is an annual youth summit which has been held in New Zealand, Australia, Canada, the United Kingdom and the United States. Other Power Shift Conferences are also being organised by members of the International Youth Climate Movement including Africa, Japan and India. The focus of the events is on climate change policy.\n\nThe first Power Shift conference was held from November 2 to 5, 2007 in Washington D.C., and was organised by the Energy Action Coalition. The second American conference occurred two years later on February 27 to March 2, 2009.\n\nFollowing on from the success of the American format, the Australian Youth Climate Coalition organised a Power Shift event in Sydney, Australia on July 11 to July 13, 2009. Similarly, the UK Youth Climate Coalition has scheduled the first British version of the event to run from October 9 to October 12, 2009. The first New Zealand Power Shift will be run between the 7th and 9 December 2012 in Auckland.\n\nThe aim of the Power Shift Conferences is to build the youth climate movement in their respective nations, which is achieved through workshops, expert panel discussions, keynote speakers, and a lobby day or a \"Day of Action\" as it is alternatively known.\n\nThe first Power Shift Conference took place from November 2 to 5 in 2007 with between 5,000 and 6,000 students and young people in attendance. It is claimed that due to the number of young people who attended the conference, it became the largest activist youth event on climate change in history. At the University of Maryland, College Park, a rally of between 2,000 and 3,000 people on the steps of the Capitol building and a Lobby Day. The event was also attended by a number of keynote speakers which included Al Gore. The main aim of the first conference was to urge elected officials to pass legislation which would include three planks taken from the platform of the climate advocacy coalition 1Sky:\n\nOn February 27 to March 2, 2009, the second American Power Shift Conference took place. Similarly to the first summit, it included workshops, panel discussions, and speakers focusing on addressing climate change and environmental justice. Casper ter Kuile the co-director of the UK Youth Climate Coalition states that it is more than a youth movement, it is \"social movement\" . This time, keynote speakers included Van Jones, Bill McKibben of 350.org, Ralph Nader, and Speaker of the House Nancy Pelosi.\n\nThe third American Power Shift took place April 15 to 18, 2011 in Washington, D.C. at the Walter E. Washington Convention Center. The conference had over 10,000 attenders. People came to support various environmental movements, many in protest of President Barack Obama's alleged weakness on environmental issues. Guest speakers included former U.S. vice-president Al Gore, Greenpeace Executive Director Phil Radford, and environmental advocate Van Jones.\n\nThe fourth Power Shift conference in the US was also the first outside of Washington. It was instead held in Pittsburgh, PA at the David L. Lawrence Convention Center on October 19–21, 2013. Keynote speakers included Gasland director Josh Fox, Sierra Club director Michael Brune, and Kandi Mossett of the Indigenous Environmental Network. The program included a rally against coal production and the organization of protests against the Keystone XL Pipeline.\n\nIn 2006, a series of student conferences on energy security were organized under the name PowerShift by the 20/20 Vision Education Fund, a 501(c)3 organization based out of Maryland. (They were not affiliated with the 2007 and later Power Shift conferences organized by the Energy Action Coalition.) Over 250 people participated in the first conference which took place in Kalamazoo, MI on April 1, 2006 and featured former Central Intelligence Agency director Jim Woolsey giving a keynote speech on U.S. oil dependence and national security. The conferences also featured a simulated energy wargame called Oil Shockwave, developed by non-profit groups Securing America's Future Energy (SAFE) and the National Commission on Energy Policy. The website for the conference series has been taken down, but is viewable using the Internet Wayback Machine.\n\nTwo years later in 2009, the Australian Youth Climate Coalition, in partnership with the University of Western Sydney, GetUp and Greenpeace, organised the Australian Power Shift Conference on July 11 to 13, 2009. Approximately 1,500 young people attended the summit. Guests included former Vice President of the United States Al Gore, the swimmer Ian Thorpe and the actress Brooke Satchwell. The event concluded with a flashmob action outside the Sydney Opera House. In 2010, the AYCC held three regional Power Shift Conferences in Adelaide, Canberra and Geelong.\n\nIn 2011, the AYCC organised Power Shift Conferences in Perth and Brisbane with over 1,000 young people. Speakers included Bill McKibben, Kumi Naidoo, Anna Rose, Dick Smith and Dr. Karl Kruszelnicki.\n\nIn 2013 the AYCC will hold Australia's largest ever Power Shift, in Melbourne from the 13–15 July.\n\nThe first Canadian Power Shift event was held from October 23 to October 26, 2009 in Ottawa, Ontario by the Canadian Youth Climate Coalition. Subsequent summits were held in Ottawa, ON in 2012; Victoria, BC in 2013; Halifax, NS in 2014; and Edmonton, AB in 2016.\n\nA collaboration between 350.org Aotearoa and Generation Zero, a student environmental movement, and other individuals, will bring \"Power Shift NZ-Pacific\" to New Zealand for the first time between the 7th and 9 December 2012 at the University of Auckland.\n\nConfirmed speakers so far include; American Environmentalist Bill McKibben, Mayor of Auckland Len Brown and actress Lucy Lawless.\n\nFrom 9–12 October 2009, the UK version of Power Shift was held at the Institute of Education in London. Modelled on a similar event to one organised by the Australian Youth Climate Coalition and carrying the same name as the Energy Action Coalition event in the USA, the event intended to develop the youth climate movement and provide young people with training on public speaking. The training was based on the techniques developed by Marshall Ganz, a civil rights activist who is credited with devising the successful grassroots organizing model and public narrative training for 2008 Barack Obama presidential campaign. The Power Shift event intended to serve as a feeder to the International Day of Action organised by 350.org on October 24.\n\nA PowerShift UK event scheduled in 2011 was cancelled, but large-scale youth climate movement events have been taking place annually under the name Shared Planet since 1999, organised by the UK's largest student-led climate campaigning network People & Planet.\n\nOn 3–4 May 2014 a Power Shift UK event was held in London, titled 'Breaking Down the Barriers and Diversifying the Climate Movement'. Open to people from all walks of society who want to contribute to action on climate change, the event brought people across the UK together for a weekend of discussions and workshops, to share stories and ideas for action, and to challenge assumptions about diversifying the climate movement.\n\n\n"}
{"id": "1935166", "url": "https://en.wikipedia.org/wiki?curid=1935166", "title": "Preserve Our Islands", "text": "Preserve Our Islands\n\nPreserve Our Islands (previously known as Protect Our Islands) is a grassroots organization, created by Senator Sharon Nelson and residents of Maury Island, Washington, United States, that is opposed to Glacier Northwest and their efforts to mine aggregate on Maury Island.\n\nThe Northwest Aggregates Company, also known as Glacier Northwest or Glaciers, had announced that they wanted to expand their surface gravel mine in Maury Island. Specifically, in May 1998, the mining company intended to fill Sea Tac’s third runway; therefore, they sought for a shoreline exemption in order to fix their broken pier and loading facility located on a land owned by the State of Washington. Moreover, they requested a permit that would allow them to mine 7.5 million tons of gravel per year. Upon learning the extent of Glacier’s expansion and driven by the need to protect the island’s habitats and wildlife, Sharon Nelson and other homeowners and islanders formed Protect Our Islands (POI) (later named Preserve Our Islands) in opposition of the mine expansion.\n\nPOI had convinced the Washington State Department of Ecology to send biologists to study Glacier’s proposed mining area, which was located above Maury Island’s main water supply. They found high levels of arsenic and lead in the soil, elements that could contaminate the island’s aquifer, a permeable, underground layer of water-bed rock that acts as a reservoir for groundwater. This contamination can lead to development disabilities in humans who drink the water. Despite Glacier’s proposal of building a 30-feet high, 150-feet wide, and 2,100-feet long sealed berm that would isolate the soil and prevent any contamination, POI and other opponents were still not reassured of the safety of the island’s water supply.\n\nInitially, POI and other opponents of the mine were on the losing end as the Court of Appeals ruled in favor of Glacier, the United States Army Corps of Engineers (U.S.A.C.E.) approved Glacier’s permit, and the Washington State Supreme Court denied to hear POI’s case against Glacier. Eventually, things were looking up for the opposition. First, Doug Sutherland, a supporter of the mine expansion, lost the position of Commissioner of Public Lands to Peter J. Goldmark, an opponent of the mine expansion. Second, a statewide effort to restore Puget Sound was initiated by Governor Chris Gregoire. Third, with the support of the Backbone Campaign, POI organized a protest at the Glacier mine area with more than 500 protesters; this garnered media attention and gave this issue greater visibility. Fourth, in 2009, US District Judge Ricardo Martinez declared that construction on the loading facility can no longer continue until the U.S.A.C.E. thoroughly assesses the environmental implications of Glacier’s project.\n\nIn the end, Glacier Northwest ended the mine expansion and the site was turned into the 235-acre King County Marine Park, after it was purchased by King County in January 8, 2011.\n"}
{"id": "9130187", "url": "https://en.wikipedia.org/wiki?curid=9130187", "title": "Rolag", "text": "Rolag\n\nA rolag (Scottish Gaelic: roileag) is a roll of fibre generally used to spin woollen yarn. A rolag is created by first carding the fibre, using handcards, and then by gently rolling the fibre off the cards. If properly prepared, a rolag will be uniform in width, distributing the fibres evenly. The word derives from the Scottish Gaelic word for a small roll.\n\nAnimal fibres have traditionally been used to create rolags, but today's spinners use many different fibre materials, including manufactured and plant fibres.\n"}
{"id": "325363", "url": "https://en.wikipedia.org/wiki?curid=325363", "title": "Sahara", "text": "Sahara\n\nThe Sahara (, ; , ', 'the Great Desert') is the largest hot desert and the third largest desert in the world after Antarctica and the Arctic. Its area of is comparable to the area of China or the United States. The name 'Sahara' is derived from a dialectal Arabic word for \"desert\", ' ( ).\n\nThe desert comprises much of North Africa, excluding the fertile region on the Mediterranean Sea coast, the Atlas Mountains of the Maghreb, and the Nile Valley in Egypt and Sudan. It stretches from the Red Sea in the east and the Mediterranean in the north to the Atlantic Ocean in the west, where the landscape gradually changes from desert to coastal plains. To the south, it is bounded by the Sahel, a belt of semi-arid tropical savanna around the Niger River valley and the Sudan Region of Sub-Saharan Africa. The Sahara can be divided into several regions including: the western Sahara, the central Ahaggar Mountains, the Tibesti Mountains, the Aïr Mountains, the Ténéré desert, and the Libyan Desert.\n\nFor several hundred thousand years, the Sahara has alternated between desert and savanna grassland in a 41,000 year cycle caused by changes (\"precession\") in the Earth's axis as it rotates around the sun, which change the location of the North African Monsoon. It is next expected to become green in about 15,000 years (17,000 AD). There is a suggestion that the last time that the Sahara was converted from savanna to desert it was partially due to overgrazing by the cattle of the local population.\n\nThe Sahara covers large parts of Algeria, Chad, Egypt, Libya, Mali, Mauritania, Niger, Western Sahara, Sudan and Tunisia. It covers , amounting to 31% of Africa. If all areas with a mean annual precipitation of less than 250 mm were included, the Sahara would be . It is one of three distinct physiographic provinces of the African massive physiographic division.\n\nThe Sahara is mainly rocky hamada (stone plateaus); ergs (sand seas - large areas covered with sand dunes) form only a minor part, but many of the sand dunes are over high. Wind or rare rainfall shape the desert features: sand dunes, dune fields, sand seas, stone plateaus, gravel plains (\"reg\"), dry valleys (\"wadi\"), dry lakes (\"oued\"), and salt flats (\"shatt\" or \"chott\"). Unusual landforms include the Richat Structure in Mauritania.\n\nSeveral deeply dissected mountains, many volcanic, rise from the desert, including the Aïr Mountains, Ahaggar Mountains, Saharan Atlas, Tibesti Mountains, Adrar des Iforas, and the Red Sea hills. The highest peak in the Sahara is Emi Koussi, a shield volcano in the Tibesti range of northern Chad.\n\nThe central Sahara is hyperarid, with sparse vegetation. The northern and southern reaches of the desert, along with the highlands, have areas of sparse grassland and desert shrub, with trees and taller shrubs in wadis, where moisture collects. In the central, hyperarid region, there are many subdivisions of the great desert: Tanezrouft, the Ténéré, the Libyan Desert, the Eastern Desert, the Nubian Desert and others. These extremely arid areas often receive no rain for years.\n\nTo the north, the Sahara skirts the Mediterranean Sea in Egypt and portions of Libya, but in Cyrenaica and the Maghreb, the Sahara borders the Mediterranean forest, woodland, and scrub eco-regions of northern Africa, all of which have a Mediterranean climate characterized by hot summers and cool and rainy winters. According to the botanical criteria of Frank White and geographer Robert Capot-Rey, the northern limit of the Sahara corresponds to the northern limit of date palm cultivation and the southern limit of the range of esparto, a grass typical of the Mediterranean climate portion of the Maghreb and Iberia. The northern limit also corresponds to the isohyet of annual precipitation.\n\nTo the south, the Sahara is bounded by the Sahel, a belt of dry tropical savanna with a summer rainy season that extends across Africa from east to west. The southern limit of the Sahara is indicated botanically by the southern limit of \"Cornulaca monacantha\" (a drought-tolerant member of the Chenopodiaceae), or northern limit of \"Cenchrus biflorus\", a grass typical of the Sahel. According to climatic criteria, the southern limit of the Sahara corresponds to the isohyet of annual precipitation (this is a long-term average, since precipitation varies annually).\n\nImportant cities located in the Sahara include Nouakchott, the capital of Mauritania; Tamanrasset, Ouargla, Béchar, Hassi Messaoud, Ghardaïa, and El Oued in Algeria; Timbuktu in Mali; Agadez in Niger; Ghat in Libya; and Faya-Largeau in Chad.\n\nThe Sahara is the world's largest low-latitude hot desert. The area is located in the horse latitudes under the subtropical ridge, a significant belt of semi-permanent subtropical warm-core high pressure where the air from upper levels of the troposphere tends to sink towards the ground. This steady descending airflow causes a warming and a drying effect in the upper troposphere. The sinking air prevents evaporating water from rising and, therefore, prevents the adiabatic cooling, which makes cloud formation extremely difficult to nearly impossible.\n\nThe permanent dissolution of clouds allows unhindered light and thermal radiation. The stability of the atmosphere above the desert prevents any convective overturning, thus making rainfall virtually non-existent. As a consequence, the weather tends to be sunny, dry and stable with a minimal risk of rainfall. Subsiding, diverging, dry air masses associated with subtropical high-pressure systems are extremely unfavorable for the development of convectional showers. The subtropical ridge is the predominant factor that explains the hot desert climate (Köppen climate classification \"BWh\") of this vast region. The lowering of air is the strongest and the most effective over the eastern part of the Great Desert, in the Libyan Desert which is the sunniest, driest and the most nearly \"rain-less\" place on the planet rivaling the Atacama Desert, lying in Chile and Peru.\n\nThe rainfall inhibition and the dissipation of cloud cover are most accentuated over the eastern section of the Sahara rather than the western. The prevailing air mass lying above the Sahara is the continental tropical (cT) air mass, which is hot and dry. Hot, dry air masses primarily form over the North-African desert from the heating of the vast continental land area, and it affects the whole desert during most of the year. Because of this extreme heating process, a thermal low is usually noticed near the surface, and is the strongest and the most developed during the summertime. The Sahara High represents the eastern continental extension of the Azores High, centered over the North Atlantic Ocean. The subsidence of the Sahara High nearly reaches the ground during the coolest part of the year while it is confined to the upper troposphere during the hottest periods.\n\nThe effects of local surface low pressure are extremely limited because upper-level subsidence still continues to block any form of air ascent. Also, to be protected against rain-bearing weather systems by the atmospheric circulation itself, the desert is made even drier by his geographical configuration and location. Indeed, the extreme aridity of the Sahara can not be only explained by the subtropical high pressure. The Atlas Mountains, found in Algeria, Morocco and Tunisia also help to enhance the aridity of the northern part of the desert. These major mountain ranges act as a barrier causing a strong rain shadow effect on the leeward side by dropping much of the humidity brought by atmospheric disturbances along the polar front which affects the surrounding Mediterranean climates.\n\nThe primary source of rain in the Sahara is the Intertropical Convergence Zone, a continuous belt of low-pressure systems near the equator which bring the brief, short and irregular rainy season to the Sahel and southern Sahara. Rainfall in this giant desert has to overcome the physical and atmospheric barriers that normally prevent the production of precipitation. The harsh climate of the Sahara is characterized by: extremely low, unreliable, highly erratic rainfall; extremely high sunshine duration values; high temperatures year-round; negligible rates of relative humidity; a significant diurnal temperature variation; and extremely high levels of potential evaporation which are the highest recorded worldwide.\n\nThe sky is usually clear above the desert and the sunshine duration is extremely high everywhere in the Sahara. Most of the desert has more than 3,600 h of bright sunshine annually or over 82% of the time, and a wide area in the eastern part experiences in excess of 4,000 h of bright sunshine a year or over 91% of the time. The highest values are very close to the theoretical maximum value. A value of 4,300 h or 98% of the time would be recorded in Upper Egypt (Aswan, Luxor) and in the Nubian Desert (Wadi Halfa). The annual average direct solar irradiation is around 2,800 kWh/(m year) in the Great Desert. The Sahara has a huge potential for solar energy production.\n\nThe constantly high position of the sun, the extremely low relative humidity, and the lack of vegetation and rainfall make the Great Desert the hottest continuously large area worldwide, and the hottest place on Earth during summer in some spots. The average high temperature exceeds during the hottest month nearly everywhere in the desert except at very high altitudes. The highest officially recorded average high temperature was in a remote desert town in the Algerian Desert called Bou Bernous with an elevation of above sea level. It is the world’s highest recorded average high temperature and only Death Valley, California rivals it. Other hot spots in Algeria such as Adrar, Timimoun, In Salah, Ouallene, Aoulef, Reggane with an elevation between above sea level get slightly lower summer average highs around during the hottest months of the year. Salah, well known in Algeria for its extreme heat, has average high temperatures of , , and in June, July, August and September respectively. There are even hotter spots in the Sahara, but they are located in extremely remote areas, especially in the Azalai, lying in northern Mali. The major part of the desert experiences around three to five months when the average high strictly exceeds . The southern central part of the desert experiences up to six or seven months when the average high temperature strictly exceeds which shows the constancy and the length of the really hot season in the Sahara. Some examples of this are: Bilma, Niger and Faya-Largeau, Chad. The annual average daily temperature exceeds everywhere and can approach in the hottest regions year-round. However, most of the desert has a value in excess of .\n\nSand and ground temperatures are even more extreme. During daytime, the sand temperature is extremely high as it can easily reach or more. A sand temperature of 83.5 °C (182.3 °F) has been recorded in Port Sudan. Ground temperatures of have been recorded in the Adrar of Mauritania and a value of 75 °C (167 °F) has been measured in Borkou, northern Chad.\n\nDue to lack of cloud cover and very low humidity, the desert usually features high diurnal temperature variations between days and nights. However, it is a myth that the nights are cold after extremely hot days in the Sahara. The average diurnal temperature range is typically between . The lowest values are found along the coastal regions due to high humidity and are often even lower than , while the highest values are found in inland desert areas where the humidity is the lowest, mainly in the southern Sahara. Still, it is true that winter nights can be cold as it can drop to the freezing point and even below, especially in high-elevation areas. The frequency of subfreezing winter nights in the Sahara is strongly influenced by the North Atlantic Oscillation (NAO), with warmer winter temperatures during negative NAO events and cooler winters with more frosts when the NAO is positive. This is because the weaker clockwise flow around the eastern side of the subtropical anticyclone during negative NAO winters, although too dry to produce more than negligible precipitation, does reduce the flow of dry, cold air from higher latitudes of Eurasia into the Sahara significantly.\n\nThe average annual rainfall ranges from very low in the northern and southern fringes of the desert to nearly non-existent over the central and the eastern part. The thin northern fringe of the desert receives more winter cloudiness and rainfall due to the arrival of low pressure systems over the Mediterranean Sea along the polar front, although very attenuated by the rain shadow effects of the mountains and the annual average rainfall ranges from to . For example, Biskra, Algeria and Ouarzazate, Morocco are found in this zone. The southern fringe of the desert along the border with the Sahel receives summer cloudiness and rainfall due to the arrival of the Intertropical Convergence Zone from the south and the annual average rainfall ranges from to . For example, Timbuktu, Mali and Agadez, Niger are found in this zone. The vast central hyper-arid core of the desert is virtually never affected by northerly or southerly atmospheric disturbances and permanently remains under the influence of the strongest anticyclonic weather regime, and the annual average rainfall can drop to less than . In fact, most of the Sahara receives less than . Of the of desert land in the Sahara, an area of about (about 31% of the total area) receives an annual average rainfall amount of or less, while some (about 17% of the total area) receives an average of or less. The annual average rainfall is virtually zero over a wide area of some in the eastern Sahara comprising deserts of: Libya, Egypt and Sudan (Tazirbu, Kufra, Dakhla, Kharga, Farafra, Siwa, Asyut, Sohag, Luxor, Aswan, Abu Simbel, Wadi Halfa) where the long-term mean approximates per year. Rainfall is very unreliable and erratic in the Sahara as it may vary considerably year by year. In full contrast to the negligible annual rainfall amounts, the annual rates of potential evaporation are extraordinarily high, roughly ranging from per year to more than per year in the whole desert. Nowhere else on Earth has air been found as dry and evaporative as in the Sahara region. However, at least two instances of snowfall have been recorded in Sahara, in February 1979 and December 2016, both in the town of Ain Sefra.\n\nThe climate of the Sahara has undergone enormous variations between wet and dry over the last few hundred thousand years, believed to be caused by long-term changes in the North African climate cycle that alters the path of the North African Monsoon - usually southward. The cycle is caused by a 41000-year cycle in which the tilt of the earth changes between 22° and 24.5°. At present (2000 AD), we are in a dry period, but it is expected that the Sahara will become green again in 15000 years (17000 AD). When the North African monsoon is at its strongest annual precipitation and subsequent vegetation in the Sahara region increase, resulting in conditions commonly referred to as the \"green Sahara\". For a relatively weak North African monsoon, the opposite is true, with decreased annual precipitation and less vegetation resulting in a phase of the Sahara climate cycle known as the \"desert Sahara\".\n\nThe idea that changes in insolation (solar heating) caused by long-term changes in the Earth's orbit are a controlling factor for the long-term variations in the strength of monsoon patterns across the globe was first suggested by Rudolf Spitaler in the late nineteenth century, The hypothesis was later formally proposed and tested by the meteorologist John Kutzbach in 1981. Kutzbach's ideas about the impacts of insolation on global monsoonal patterns have become widely accepted today as the underlying driver of long term monsoonal cycles. Kutzbach never formally named his hypothesis and as such it is referred to here as the \"Orbital Monsoon Hypothesis\" as suggested by Ruddiman in 2001.\n\nDuring the last glacial period, the Sahara was much larger than it is today, extending south beyond its current boundaries. The end of the glacial period brought more rain to the Sahara, from about 8000 BC to 6000 BC, perhaps because of low pressure areas over the collapsing ice sheets to the north. Once the ice sheets were gone, the northern Sahara dried out. In the southern Sahara, the drying trend was initially counteracted by the monsoon, which brought rain further north than it does today. By around 4200 BC, however, the monsoon retreated south to approximately where it is today, leading to the gradual desertification of the Sahara. The Sahara is now as dry as it was about 13,000 years ago.\n\nThe Sahara pump theory describes this cycle. During periods of a wet or \"Green Sahara\", the Sahara becomes a savanna grassland and various flora and fauna become more common. Following inter-pluvial arid periods, the Sahara area then reverts to desert conditions and the flora and fauna are forced to retreat northwards to the Atlas Mountains, southwards into West Africa, or eastwards into the Nile Valley. This separates populations of some of the species in areas with different climates, forcing them to adapt, possibly giving rise to allopatric speciation.\n\nIt is also proposed that humans accelerated the drying out period from 6,000–2,500 BC by pastoralists overgrazing available grassland.\n\nThe growth of speleothems (which requires rainwater) was detected in Hol-Zakh, Ashalim, Even-Sid, Ma'ale-ha-Meyshar, Ktora Cracks, Nagev Tzavoa Cave, and elsewhere, and has allowing tracking of prehistoric rainfall. The Red Sea coastal route was extremely arid before 140 and after 115 kya. Slightly wetter conditions appear at 90–87 kya, but it still was just one tenth the rainfall around 125 kya. In the southern Negev Desert speleothems did not grow between 185–140 kya (MIS 6), 110–90 (MIS 5.4–5.2), nor after 85 kya nor during most of the interglacial period (MIS 5.1), the glacial period and Holocene. This suggests that the southern Negev was arid to hyper-arid in these periods.\n\nDuring the Last Glacial Maximum (LGM) the Sahara desert was more extensive than it is now with the extent of the tropical forests being greatly reduced, and the lower temperatures reduced the strength of the Hadley Cell. This is a climate cell which causes rising tropical air of the Inter-Tropical Convergence Zone (ITCZ) to bring rain to the tropics, while dry descending air, at about 20 degrees north, flows back to the equator and brings desert conditions to this region. It is associated with high rates of wind-blown mineral dust, and these dust levels are found as expected in marine cores from the north tropical Atlantic. But around 12,500 BC the amount of dust in the cores in the Bølling/Allerød phase suddenly plummets and shows a period of much wetter conditions in the Sahara, indicating a Dansgaard-Oeschger (DO) event (a sudden warming followed by a slower cooling of the climate). The moister Saharan conditions had begun about 12,500 BC, with the extension of the ITCZ northward in the northern hemisphere summer, bringing moist wet conditions and a savanna climate to the Sahara, which (apart from a short dry spell associated with the Younger Dryas) peaked during the Holocene thermal maximum climatic phase at 4000 BC when mid-latitude temperatures seem to have been between 2 and 3 degrees warmer than in the recent past. Analysis of Nile River deposited sediments in the delta also shows this period had a higher proportion of sediments coming from the Blue Nile, suggesting higher rainfall also in the Ethiopian Highlands. This was caused principally by a stronger monsoonal circulation throughout the sub-tropical regions, affecting India, Arabia and the Sahara. Lake Victoria only recently became the source of the White Nile and dried out almost completely around 15 kya.\n\nThe sudden subsequent movement of the ITCZ southwards with a Heinrich event (a sudden cooling followed by a slower warming), linked to changes with the El Niño-Southern Oscillation cycle, led to a rapid drying out of the Saharan and Arabian regions, which quickly became desert. This is linked to a marked decline in the scale of the Nile floods between 2700 and 2100 BC.\n\nThe Sahara comprises several distinct ecoregions. With their variations in temperature, rainfall, elevation, and soil, these regions harbor distinct communities of plants and animals.\n\nThe Atlantic coastal desert is a narrow strip along the Atlantic coast where fog generated offshore by the cool Canary Current provides sufficient moisture to sustain a variety of lichens, succulents, and shrubs. It covers an area of in the south of Morocco and Mauritania.\n\nThe North Saharan steppe and woodlands is along the northern desert, next to the Mediterranean forests, woodlands, and scrub ecoregions of the northern Maghreb and Cyrenaica. Winter rains sustain shrublands and dry woodlands that form a transition between the Mediterranean climate regions to the north and the hyper-arid Sahara proper to the south. It covers in Algeria, Egypt, Libya, Mauritania, Morocco, and Tunisia.\n\nThe Sahara Desert ecoregion covers the hyper-arid central portion of the Sahara where rainfall is minimal and sporadic. Vegetation is rare, and this ecoregion consists mostly of sand dunes (\"erg, chech, raoui\"), stone plateaus (\"hamadas\"), gravel plains (\"reg\"), dry valleys (\"wadis\"), and salt flats. It covers of: Algeria, Chad, Egypt, Libya, Mali, Mauritania, Niger, and Sudan.\n\nThe South Saharan steppe and woodlands ecoregion is a narrow band running east and west between the hyper-arid Sahara and the Sahel savannas to the south. Movements of the equatorial Intertropical Convergence Zone (ITCZ) bring summer rains during July and August which average but vary greatly from year to year. These rains sustain summer pastures of grasses and herbs, with dry woodlands and shrublands along seasonal watercourses. This ecoregion covers in Algeria, Chad, Mali, Mauritania, and Sudan.\n\nIn the West Saharan montane xeric woodlands, several volcanic highlands provide a cooler, moister environment that supports Saharo-Mediterranean woodlands and shrublands. The ecoregion covers , mostly in the Tassili n'Ajjer of Algeria, with smaller enclaves in the Aïr of Niger, the Dhar Adrar of Mauritania, and the Adrar des Iforas of Mali and Algeria.\n\nThe Tibesti-Jebel Uweinat montane xeric woodlands ecoregion consists of the Tibesti and Jebel Uweinat highlands. Higher and more regular rainfall and cooler temperatures support woodlands and shrublands of Date palm, acacias, myrtle, oleander, tamarix, and several rare and endemic plants. The ecoregion covers in the Tibesti of Chad and Libya, and Jebel Uweinat on the border of Egypt, Libya, and Sudan.\n\nThe Saharan halophytics is an area of seasonally flooded saline depressions which is home to halophytic (salt-adapted) plant communities. The Saharan halophytics cover including: the Qattara and Siwa depressions in northern Egypt, the Tunisian salt lakes of central Tunisia, Chott Melghir in Algeria, and smaller areas of Algeria, Mauritania, and the southern part of Morocco.\n\nThe Tanezrouft is one of the harshest regions on Earth as well as one of the hottest and driest parts of the Sahara, with no vegetation and very little life. It is along the borders of Algeria, Niger, and Mali, west of the Hoggar mountains.\n\nThe flora of the Sahara is highly diversified based on the bio-geographical characteristics of this vast desert. Floristically, the Sahara has three zones based on the amount of rainfall received – the Northern (Mediterranean), Central and Southern Zones. There are two transitional zones – the Mediterranean-Sahara transition and the Sahel transition zone.\n\nThe Saharan flora comprises around 2800 species of vascular plants. Approximately a quarter of these are endemic. About half of these species are common to the flora of the Arabian deserts.\n\nThe central Sahara is estimated to include five hundred species of plants, which is extremely low considering the huge extent of the area. Plants such as acacia trees, palms, succulents, spiny shrubs, and grasses have adapted to the arid conditions, by growing lower to avoid water loss by strong winds, by storing water in their thick stems to use it in dry periods, by having long roots that travel horizontally to reach the maximum area of water and to find any surface moisture, and by having small thick leaves or needles to prevent water loss by evapotranspiration. Plant leaves may dry out totally and then recover.\nSeveral species of fox live in the Sahara including: the fennec fox, pale fox and Rüppell's fox. The addax, a large white antelope, can go nearly a year in the desert without drinking. The dorcas gazelle is a north African gazelle that can also go for a long time without water. Other notable gazelles include the rhim gazelle and dama gazelle.\n\nThe Saharan cheetah (northwest African cheetah) lives in Algeria, Togo, Niger, Mali, Benin, and Burkina Faso. There remain fewer than 250 mature cheetahs, which are very cautious, fleeing any human presence. The cheetah avoids the sun from April to October, seeking the shelter of shrubs such as balanites and acacias. They are unusually pale. The other cheetah subspecies (northeast African cheetah) lives in Chad, Sudan and the eastern region of Niger. However, it is currently extinct in the wild in Egypt and Libya. There are approximately 2000 mature individuals left in the wild.\nOther animals include the monitor lizards, hyrax, sand vipers, and small populations of African wild dog, in perhaps only 14 countries and red-necked ostrich. Other animals exist in the Sahara (birds in particular) such as African silverbill and black-faced firefinch, among others. There are also small desert crocodiles in Mauritania and the Ennedi Plateau of Chad.\n\nThe deathstalker scorpion can be long. Its venom contains large amounts of agitoxin and scyllatoxin and is very dangerous; however, a sting from this scorpion rarely kills a healthy adult. The Saharan silver ant is unique in that due to the extreme high temperatures of their habitat, and the threat of predators, the ants are active outside their nest for only about ten minutes per day.\n\nDromedary camels and goats are the domesticated animals most commonly found in the Sahara. Because of its qualities of endurance and speed, the dromedary is the favourite animal used by nomads.\n\nHuman activities are more likely to affect the habitat in areas of permanent water (oases) or where water comes close to the surface. Here, the local pressure on natural resources can be intense. The remaining populations of large mammals have been greatly reduced by hunting for food and recreation. In recent years development projects have started in the deserts of Algeria and Tunisia using irrigated water pumped from underground aquifers. These schemes often lead to soil degradation and salinization.\n\nResearchers from Hacettepe University (Yücekutlu, N. et al., 2011) have reported that Saharan soil may have bio-available iron and also some essential macro and micro nutrient elements suitable for use as fertilizer for growing wheat.\n\nPeople lived on the edge of the desert thousands of years ago since the end of the last glacial period. The Sahara was then a much wetter place than it is today. Over 30,000 petroglyphs of river animals such as crocodiles survive, with half found in the Tassili n'Ajjer in southeast Algeria. Fossils of dinosaurs, including \"Afrovenator\", \"Jobaria\" and \"Ouranosaurus\", have also been found here. The modern Sahara, though, is not lush in vegetation, except in the Nile Valley, at a few oases, and in the northern highlands, where Mediterranean plants such as the olive tree are found to grow. It was long believed that the region had been this way since about 1600 BCE, after shifts in the Earth's axis increased temperatures and decreased precipitation, which led to the abrupt desertification of North Africa about 5,400 years ago. However, this theory has recently been called into dispute, when samples taken from several 7 million year old sand deposits led scientists to reconsider the timeline for desertification.\n\nThe Kiffian culture is a prehistoric industry, or domain, that existed between 10,000 and 8,000 years ago in the Sahara, during the Neolithic Subpluvial. Human remains from this culture were found in 2000 at a site known as Gobero, located in Niger in the Ténéré Desert. The site is known as the largest and earliest grave of Stone Age people in the Sahara desert. The Kiffians were skilled hunters. Bones of many large savannah animals that were discovered in the same area suggest that they lived on the shores of a lake that was present during the Holocene Wet Phase, a period when the Sahara was verdant and wet. The Kiffian people were tall, standing over six feet in height. Craniometric analysis indicates that this early Holocene population was closely related to the Late Pleistocene Iberomaurusians and early Holocene Capsians of the Maghreb, as well as mid-Holocene Mechta groups. Traces of the Kiffian culture do not exist after 8,000 years ago, as the Sahara went through a dry period for the next thousand years. After this time, the Tenerian culture colonized the area.\n\nGobero was discovered in 2000 during an archaeological expedition led by Paul Sereno, which sought dinosaur remains. Two distinct prehistoric cultures were discovered at the site: the early Holocene Kiffian culture, and the middle Holocene Tenerian culture. The Kiffians were a prehistoric people who preceded the Tenerians and vanished approximately 8000 years ago, when the desert became very dry. The desiccation lasted until around 4600 BC, when the earliest artefacts associated with the Tenerians have been dated to. Some 200 skeletons have been discovered at Gobero. The Tenerians were considerably shorter in height and less robust than the earlier Kiffians. Craniometric analysis also indicates that they were osteologically distinct. The Kiffian skulls are akin to those of the Late Pleistocene Iberomaurusians, early Holocene Capsians, and mid-Holocene Mechta groups, whereas the Tenerian crania are more like those of Mediterranean groups. Graves show that the Tenerians observed spiritual traditions, as they were buried with artifacts such as jewelry made of hippo tusks and clay pots. The most interesting find is a triple burial, dated to 5300 years ago, of an adult female and two children, estimated through their teeth as being five and eight years old, hugging each other. Pollen residue indicates they were buried on a bed of flowers. The three are assumed to have died within 24 hours of each other, but as their skeletons hold no apparent trauma (they did not die violently) and they have been buried so elaborately - unlikely if they had died of a plague - the cause of their deaths is a mystery.\n\nUan Muhuggiag appears to have been inhabited from at least the 6th millennium BC to about 2700 BC, although not necessarily continuously. The most noteworthy find at Uan Muhuggiag is the well-preserved mummy of a young boy of approximately 2 1/2 years old. The child was in a fetal position, then embalmed, then placed in a sack made of antelope skin, which was insulated by a layer of leaves. The boy's organs were removed, as evidenced by incisions in his stomach and thorax, and an organic preservative was inserted to stop his body from decomposing. An ostrich eggshell necklace was also found around his neck. Radiocarbon dating determined the age of the mummy to be approximately 5600 years old, which makes it about 1000 years older than the earliest previously recorded mummy in ancient Egypt. In 1958-1959, an archaeological expedition led by Antonio Ascenzi conducted anthropological, radiological, histological and chemical analyses on the Uan Muhuggiag mummy. The specimen was determined to be that of a 30-month old child of uncertain sex, who possessed Negroid features. A long incision on the specimen's abdominal wall also indicated that the body had been initially mummified by evisceration and later underwent natural desiccation. One other individual, an adult, was found at Uan Muhuggiag, buried in a crouched position. However, the body showed no evidence of evisceration or any other method of preservation. The body was estimated to date from about 7500 BP.\n\nDuring the Neolithic Era, before the onset of desertification around 9500 BCE, the central Sudan had been a rich environment supporting a large population ranging across what is now barren desert, like the Wadi el-Qa'ab. By the 5th millennium BCE, the people who inhabited what is now called Nubia, were full participants in the \"agricultural revolution\", living a settled lifestyle with domesticated plants and animals. Saharan rock art of cattle and herdsmen suggests the presence of a cattle cult like those found in Sudan and other pastoral societies in Africa today. Megaliths found at Nabta Playa are overt examples of probably the world's first known archaeoastronomy devices, predating Stonehenge by some 2,000 years. This complexity, as observed at Nabta Playa, and as expressed by different levels of authority within the society there, likely formed the basis for the structure of both the Neolithic society at Nabta and the Old Kingdom of Egypt.\n\nBy 6000 BCE predynastic Egyptians in the southwestern corner of Egypt were herding cattle and constructing large buildings. Subsistence in organized and permanent settlements in predynastic Egypt by the middle of the 6th millennium BCE centered predominantly on cereal and animal agriculture: cattle, goats, pigs and sheep. Metal objects replaced prior ones of stone. Tanning of animal skins, pottery and weaving were commonplace in this era also. There are indications of seasonal or only temporary occupation of the Al Fayyum in the 6th millennium BCE, with food activities centering on fishing, hunting and food-gathering. Stone arrowheads, knives and scrapers from the era are commonly found. Burial items included pottery, jewelry, farming and hunting equipment, and assorted foods including dried meat and fruit. Burial in desert environments appears to enhance Egyptian preservation rites, and the dead were buried facing due west.\n\nBy 3400 BCE, the Sahara was as dry as it is today, due to reduced precipitation and higher temperatures resulting from a shift in the Earth's orbit. As a result of this aridification, it became a largely impenetrable barrier to humans, with the remaining settlements mainly being concentrated around the numerous oases that dot the landscape. Little trade or commerce is known to have passed through the interior in subsequent periods, the only major exception being the Nile Valley. The Nile, however, was impassable at several cataracts, making trade and contact by boat difficult.\n\nThe people of Phoenicia, who flourished from 1200–800 BCE, created a confederation of kingdoms across the entire Sahara to Egypt. They generally settled along the Mediterranean coast, as well as the Sahara, among the people of ancient Libya, who were the ancestors of people who speak Berber languages in North Africa and the Sahara today, including the Tuareg of the central Sahara.\n\nThe Phoenician alphabet seems to have been adopted by the ancient Libyans of north Africa, and Tifinagh is still used today by Berber-speaking Tuareg camel herders of the central Sahara.\n\nSometime between 633 BCE and 530 BCE, Hanno the Navigator either established or reinforced Phoenician colonies in Western Sahara, but all ancient remains have vanished with virtually no trace.\n\nBy 500 BCE, Greeks arrived in the desert. Greek traders spread along the eastern coast of the desert, establishing trading colonies along the Red Sea. The Carthaginians explored the Atlantic coast of the desert, but the turbulence of the waters and the lack of markets caused a lack of presence further south than modern Morocco. Centralized states thus surrounded the desert on the north and east; it remained outside the control of these states. Raids from the nomadic Berber people of the desert were of constant concern to those living on the edge of the desert.\n\nAn urban civilization, the Garamantes, arose around 500 BCE in the heart of the Sahara, in a valley that is now called the Wadi al-Ajal in Fezzan, Libya. The Garamantes achieved this development by digging tunnels far into the mountains flanking the valley to tap fossil water and bring it to their fields. The Garamantes grew populous and strong, conquering their neighbors and capturing many slaves (who were put to work extending the tunnels). The ancient Greeks and the Romans knew of the Garamantes and regarded them as uncivilized nomads. However, they traded with them, and a Roman bath has been found in the Garamantes' capital of Garama. Archaeologists have found eight major towns and many other important settlements in the Garamantes' territory. The Garamantes' civilization eventually collapsed after they had depleted available water in the aquifers and could no longer sustain the effort to extend the tunnels further into the mountains.\n\nThe Berber people occupied (and still occupy with Arabs) much of the Sahara. The Garamantes Berbers built a prosperous empire in the heart of the desert. The Tuareg nomads continue to inhabit and move across wide Sahara surfaces to the present day.\n\nThe Byzantine Empire ruled the northern shores of the Sahara from the 5th to the 7th centuries. After the Muslim conquest of Arabia, specifically the Arabian peninsula, the Muslim conquest of North Africa began in the mid-7th to early 8th centuries and Islamic influence expanded rapidly on the Sahara. By the end of 641 all of Egypt was in Muslim hands. Trade across the desert intensified, and a significant slave trade crossed the desert. It has been estimated that from the 10th to 19th centuries some 6,000 to 7,000 slaves were transported north each year.\nIn the 16th century the northern fringe of the Sahara, such as coastal regencies in present-day Algeria and Tunisia, as well as some parts of present-day Libya, together with the semi-autonomous kingdom of Egypt, were occupied by the Ottoman Empire. From 1517 Egypt was a valued part of the Ottoman Empire, ownership of which provided the Ottomans with control over the Nile Valley, the east Mediterranean and North Africa. The benefit of the Ottoman Empire was the freedom of movement for citizens and goods. Traders exploited the Ottoman land routes to handle the spices, gold and silk from the East, manufactured goods from Europe, and the slave and gold traffic from Africa. Arabic continued as the local language and Islamic culture was much reinforced. The Sahel and southern Sahara regions were home to several independent states or to roaming Tuareg clans.\n\nEuropean colonialism in the Sahara began in the 19th century. France conquered the regency of Algiers from the Ottomans in 1830, and French rule spread south from Algeria and eastwards from Senegal into the upper Niger to include present-day Algeria, Chad, Mali then French Sudan including Timbuktu, Mauritania, Morocco (1912), Niger, and Tunisia (1881). By the beginning of the 20th century, the trans-Saharan trade had clearly declined because goods were moved through more modern and efficient means, such as airplanes, rather than across the desert.\n\nThe French Colonial Empire was the dominant presence in the Sahara. It established regular air links from Toulouse (HQ of famed Aéropostale), to Oran and over the Hoggar to Timbuktu and West to Bamako and Dakar, as well as trans-Sahara bus services run by La Companie Transsaharienne (est. 1927). A remarkable film shot by famous aviator Captain René Wauthier documents the first crossing by a large truck convoy from Algiers to Tchad, across the Sahara.\n\nEgypt, under Muhammad Ali and his successors, conquered Nubia in 1820–22, founded Khartoum in 1823, and conquered Darfur in 1874. Egypt, including the Sudan, became a British protectorate in 1882. Egypt and Britain lost control of the Sudan from 1882 to 1898 as a result of the Mahdist War. After its capture by British troops in 1898, the Sudan became an Anglo-Egyptian condominium.\n\nSpain captured present-day Western Sahara after 1874, although Rio del Oro remained largely under Sahrawi influence. In 1912, Italy captured parts of what was to be named Libya from the Ottomans. To promote the Roman Catholic religion in the desert, Pope Pius IX appointed a delegate Apostolic of the Sahara and the Sudan in 1868; later in the 19th century his jurisdiction was reorganized into the Vicariate Apostolic of Sahara.\n\nEgypt became independent of Britain in 1936, although the Anglo-Egyptian treaty of 1936 allowed Britain to keep troops in Egypt and to maintain the British-Egyptian condominium in the Sudan. British military forces were withdrawn in 1954.\n\nMost of the Saharan states achieved independence after World War II: Libya in 1951; Morocco, Sudan, and Tunisia in 1956; Chad, Mali, Mauritania, and Niger in 1960; and Algeria in 1962. Spain withdrew from Western Sahara in 1975, and it was partitioned between Mauritania and Morocco. Mauritania withdrew in 1979; Morocco continues to hold the territory.\n\nIn the post-World War II era, several mines and communities have developed to utilize the desert's natural resources. These include large deposits of oil and natural gas in Algeria and Libya, and large deposits of phosphates in Morocco and Western Sahara.\n\nA number of Trans-African highways have been proposed across the Sahara, including the Cairo–Dakar Highway along the Atlantic coast, the Trans-Sahara Highway from Algiers on the Mediterranean to Kano in Nigeria, the Tripoli – Cape Town Highway from Tripoli in Libya to N'Djamena in Chad, and the Cairo – Cape Town Highway which follows the Nile. Each of these highways is partially complete, with significant gaps and unpaved sections.\n\nThe people of the Sahara are of various origins. Among them the Amaziɣ including the Turūq, various Arabized Amaziɣ groups such as the Hassaniya-speaking Sahrawis, whose populations include the Znaga, a tribe whose name is a remnant of the pre-historic Zenaga language. Other major groups of people include the: Toubou, Nubians, Zaghawa, Kanuri, Hausa, Songhai, Beja, and Fula/Fulani (; ).\n\nArabic dialects are the most widely spoken languages in the Sahara. Arabic, Berber and its variants now regrouped under the term Amazigh (which includes the Guanche language spoken by the original Berber inhabitants of the Canary Islands) and Beja languages are part of the Afro-Asiatic or Hamito-Semitic family. Unlike neighboring West Africa and the central governments of the states that comprise the Sahara, the French language bears little relevance to inter-personal discourse and commerce within the region, its people retaining staunch ethnic and political affiliations with Tuareg and Berber leaders and culture. The legacy of the French colonial era administration is primarily manifested in the territorial reorganization enacted by the Third and Fourth republics, which engendered artificial political divisions within a hitherto isolated and porous region. Diplomacy with local clients was conducted primarily in Arabic, which was the traditional language of bureaucratic affairs. Mediation of disputes and inter-agency communication was served by interpreters contracted by the French government, who, according to Keenan, \"documented a space of intercultural mediation,\" contributing much to preserving the indigenous cultural identities in the region.\n\n\n\n"}
{"id": "39728832", "url": "https://en.wikipedia.org/wiki?curid=39728832", "title": "Sensitive flame", "text": "Sensitive flame\n\nA sensitive flame is a gas flame which under suitable adjustment of pressure resonates readily with sounds or air vibrations in the vicinity. Noticed by both the American scientist John LeConte and the English physicist William Fletcher Barrett, they recorded the effect that a shrill note had upon a gas flame issuing from a tapering jet. The phenomenon caught the attention of the Irish physicist John Tyndall who gave a lecture on the process to the Royal Institution in January 1867.\nWhile not necessary to observe the effect, a higher flame temperature allows for easier observation. Sounds at lower to mid-range frequencies have little to no effect on the flame. However, shrill noises at high frequencies produce noticeable effects on the flame. Even the ticking of a pocket watch was observed as producing a high enough frequency to affect the flame.\n\n\n"}
{"id": "37648353", "url": "https://en.wikipedia.org/wiki?curid=37648353", "title": "Smiling Bears", "text": "Smiling Bears\n\nSmiling Bears: A Zookeeper Explores the Behavior and Emotional Life of Bears is a 2009 book by Canadian writer Else Poulsen, first published by Greystone Books. In the book, the author chronicles her insights gleaned as a zookeeper responsible for rehabilitating \"bears in crisis\". Jeffrey Moussaieff Masson called \"Smiling Bears\" \"An inspiring trip into the minds and reality of bears.\"\n\n\"Smiling Bears\" received shortlist recognition for the 2010 \"Edna Staebler Award for Creative Non-Fiction\".\n\n\n"}
{"id": "5586326", "url": "https://en.wikipedia.org/wiki?curid=5586326", "title": "Stellar birthline", "text": "Stellar birthline\n\nThe stellar birthline is a predicted line on the Hertzsprung–Russell diagram that relates the effective temperature and luminosity of pre-main-sequence stars at the start of their contraction. Prior to this point, the objects are accreting protostars, and are so deeply embedded in the cloud of dust and gas from which they are forming that they radiate only in far infrared and millimeter wavelengths. Once stellar winds disperse this cloud, the star becomes visible as a pre-main-sequence object. The set of locations on the Hertzsprung–Russell diagram where these newly visible stars reside is called the \"birthline\", and is found above the main sequence.\n\nThe location of the stellar birthline depends in detail on the accretion rate onto the star and geometry of this accretion, i.e. whether or not it is occurring through an accretion disk. This means that the birthline is not an infinitely thin curve, but has a finite thickness in the Hertzsprung-Russell diagram.\n\n\n"}
{"id": "16148649", "url": "https://en.wikipedia.org/wiki?curid=16148649", "title": "Stereoautograph", "text": "Stereoautograph\n\nThe Stereoautograph is a complex opto-mechanical measurement instrument for the evaluation of analog or digital photograms. It is based on the stereoscopy effect by using two aero photos or two photograms of the topography or of buildings from different standpoints.\n\nIt was invented by Eduard von Orel in 1907.\n\nThe photograms or photographic plates are oriented by measured passpoints in the field or on the building. This procedure can be carried out digitally (by methods of triangulation and projective geometry or iteratively (repeated angle corrections by congruent rays). The accuracy of modern autographs is about 0.001 mm.\n\nWell known are the instruments of the companies Wild Heerbrugg (Leica), e.g. analog A7, B8 of the 1980s and the digital autographs beginning in the 1990s, or special instruments of Zeiss and Contraves.\n\n"}
{"id": "57896281", "url": "https://en.wikipedia.org/wiki?curid=57896281", "title": "TXS 0506+056", "text": "TXS 0506+056\n\nTXS 0506+056 is a very high energy blazar – a quasar with a relativistic jet pointing directly towards Earth – of BL Lac-type. With a redshift of 0.3365 ± 0.0010, it is about from Earth. Its approximate location on the sky is off the left shoulder of the constellation Orion. Discovered as a radio source in 1983, the blazar has since been observed across the entire electromagnetic spectrum.\n\nTXS 0506+056 is the first known source of high energy astrophysical neutrinos, identified following the IceCube-170922A neutrino event in an early example of multi-messenger astronomy. The only astronomical sources previously observed by neutrino detectors were the Sun and supernova 1987A, which were detected decades earlier at much lower neutrino energies.\n\nThe object has been detected by numerous astronomical surveys, so has numerous valid source designations. The most commonly used, TXS 0506+056, comes from its inclusion in the Texas Survey of radio sources (standard abbreviation TXS) and its approximate equatorial coordinates in the B1950 equinox used by that survey.\nTXS 0506+056 was first discovered as a radio source in 1983. It was identified as an active galaxy in the 1990s, and a possible blazar in the early 2000s. By 2009 it was regarded as a confirmed blazar and catalogued as a BL Lac object. Gamma rays from TXS 0506+056 were detected by the EGRET and Fermi Space Telescope missions.\n\nRadio observations using very-long-baseline interferometry have shown apparent superluminal motion in the blazar's jet. TXS 0506+056 is one of the blazars regularly monitored by the OVRO 40 meter Telescope, so has an almost-continuous radio light curve recorded from 2008 onwards.\n\nThe gamma-ray flux from TXS 0506+056 is highly variable, by at least a factor of a thousand, but on average it is in the top 4% of brightest gamma-ray sources on the sky. It is also very bright in radio waves, in the top 1% of sources. Given its distance, this makes TXS 0506+056 one of the most intrinsically powerful BL Lac objects known, particularly in high-energy gamma rays.\n\nOn September 22, 2017, the IceCube Neutrino Observatory detected a high energy muon neutrino, dubbed IceCube-170922A. The neutrino carried an energy of ~290 tera–electronvolts (TeV); for comparison, the Large Hadron Collider can generate a maximum energy of 13 TeV. Within one minute of the neutrino detection, IceCube sent an automated alert to astronomers around the world with coordinates to search for a possible source.\n\nA search of this region in the sky, 1.33 degrees across, yielded only one likely source: TXS 0506+056, a previously-known blazar, which was found to be in a flaring state of high gamma ray emission. It was subsequently observed at other wavelengths of light across the electromagnetic spectrum, including radio, infrared, optical, X-rays and gamma-rays. The detection of both neutrinos and light from the same object was an early example of multi-messenger astronomy.\n\nA search of archived neutrino data from IceCube found evidence for an earlier flare of lower-energy neutrinos in 2014-2015 (a form of precovery), which supports identification of the blazar as a source of neutrinos. An independent analysis found no gamma-ray flare during this earlier period of neutrino emission, but supported its association with the blazar. The neutrinos emitted by TXS 0506+056 are six orders of magnitude higher in energy than those from any previously-identified astrophysical neutrino source.\n\nThe observations of high energy neutrinos and gamma-rays from this source imply that it is also a source of cosmic rays, because all three should be produced by the same physical processes, though no cosmic rays from TXS 0506+056 have been directly observed. In the blazar, a charged pion was produced by the interaction of a high-energy proton or nucleus (i.e. a cosmic ray) with the radiation field or with matter. The pion then decayed into a lepton and the neutrino. The neutrino interacts only weakly with matter, so it escaped the blazar. Upon reaching Earth, the neutrino interacted with the Antarctic ice to produce a muon, which was observed by the Cherenkov radiation it generated as it moved through the IceCube detector.\n\n\n"}
{"id": "4722043", "url": "https://en.wikipedia.org/wiki?curid=4722043", "title": "Timeline of World War I", "text": "Timeline of World War I\n\n"}
{"id": "526237", "url": "https://en.wikipedia.org/wiki?curid=526237", "title": "Twilight", "text": "Twilight\n\nTwilight on Earth is the illumination of the lower atmosphere when the Sun itself is not directly visible because it is below the horizon. Twilight is produced by sunlight scattering in the upper atmosphere, illuminating the lower atmosphere so that Earth's surface is neither completely lit nor completely dark. The word \"twilight\" is also used to denote the periods of time when this illumination occurs.\n\nThe farther the Sun is below the horizon, the dimmer the twilight (other things such as atmospheric conditions being equal). When the Sun reaches 18° below the horizon, the twilight's brightness is nearly zero, and evening twilight becomes nighttime. When the Sun again reaches 18° below the horizon, nighttime becomes morning twilight. Owing to its distinctive quality, primarily the absence of shadows and the appearance of objects silhouetted against the lit sky, twilight has long been popular with photographers, who sometimes refer to it as \"sweet light\", and painters, who often refer to it as the blue hour, after the French expression \"l'heure bleue\".\n\nTwilight should not be confused with auroras, which can have a similar appearance in the night sky at high latitudes.\nBy analogy with evening twilight, the word \"twilight\" is also sometimes used metaphorically, to imply that something is losing strength and approaching its end. For example, very old people may be said to be \"in the twilight of their lives\". The collateral adjective for \"twilight\" is \"crepuscular\", which may be used to describe the behavior of insects, fish, and mammals that are most active during this period.\nTwilight is defined according to the solar elevation angle \"θ\", which is the position of the geometric center of the sun relative to the horizon. There are three established and widely accepted \"subcategories\" of twilight: civil twilight (nearest the horizon), nautical twilight, and astronomical twilight (farthest from the horizon).\n\nThree \"subcategories\" of twilight are established and widely accepted: civil twilight (brightest), nautical twilight, and astronomical twilight (darkest).\n\nMorning civil twilight begins when the geometric center of the sun is 6° below the horizon and ends at sunrise. Evening civil twilight begins at sunset and ends when the geometric center of the sun reaches 6° below the horizon. In the United States' military, the initialisms BMCT (begin morning civil twilight, \"i.e.\" civil dawn) and EECT (end evening civil twilight, \"i.e.\" civil dusk) are used to refer to the start of morning civil twilight and the end of evening civil twilight, respectively. Civil dawn is preceded by morning nautical twilight and civil dusk is followed by evening nautical twilight.\n\nUnder clear weather conditions, civil twilight approximates the limit at which solar illumination suffices for the human eye to clearly distinguish terrestrial objects. Enough illumination renders artificial sources unnecessary for most outdoor activities. At civil dawn and at civil dusk sunlight clearly defines the horizon while the brightest stars and planets can appear. As observed from the Earth (see apparent magnitude), sky-gazers know Venus, the brightest planet, as the \"morning star\" or \"evening star” because they can see it during civil twilight.\n\nLawmakers have enshrined the concept of civil twilight. Such statutes typically use a fixed period after sunset or before sunrise (most commonly 20–30 minutes), rather than how many degrees the sun is below the horizon. Examples include the following periods: when drivers of automobiles must turn on their headlights (called lighting-up time in the UK); when hunting is restricted; when the crime of burglary is to be treated as nighttime burglary, which carries stiffer penalties in some jurisdictions.\n\nThe period may affect when extra equipments, such as anti-collision lights, are required for aircraft to operate. In the US, civil twilight for aviation is defined in Part 1.1 of the Federal Aviation Regulations (FARs) as the time listed in the American Air Almanac.\n\nMorning nautical twilight begins (nautical dawn) when the geometric center of the sun is 12 degrees below the horizon in the morning and ends when the geometric center of the sun is 6 degrees below the horizon in the morning. Evening nautical twilight begins when the geometric center of the sun is 6 degrees below the horizon in the evening and ends (nautical dusk) when the geometric center of the sun is 12 degrees below the horizon in the evening.\n\nNautical dawn is the moment when the geometric center of the Sun is 12 degrees below the horizon in the morning. It is preceded by morning astronomical twilight and followed by morning nautical twilight. Nautical dusk is the moment when the geometric center of the Sun is 12 degrees below the horizon in the evening. It marks the beginning of evening astronomical twilight and the end of evening nautical twilight.\nBefore nautical dawn and after nautical dusk, sailors cannot navigate via the horizon at sea as they cannot clearly see the horizon. At nautical dawn and nautical dusk, the human eye finds it difficult, if not impossible, to discern traces of illumination near the sunset or sunrise point of the horizon (\"first light” after nautical dawn but before civil dawn and \"nightfall\" after civil dusk but before nautical dusk).\n\nSailors can take reliable star sightings of well-known stars, during the stage of nautical twilight when they can distinguish a visible horizon for reference (i.e. after astronomic dawn or before astronomic dusk). \nUnder good atmospheric conditions with the absence of other illumination, during nautical twilight, the human eye may distinguish general outlines of ground objects but cannot participate in detailed outdoor operations.\n\nNautical twilight has military considerations as well. The initialisms BMNT (begin morning nautical twilight, \"i.e.\" nautical dawn) and EENT (end evening nautical twilight, \"i.e.\" nautical dusk) are used and considered when planning military operations. A military unit may treat BMNT and EENT with heightened security, e.g. by \"standing to\", in which everyone assumes a defensive position. This is partially due to tactics dating back to the French and Indian War (part of the Seven Years' War of 1756–1763), when combatants on both sides would launch attacks at nautical dawn or dusk.\n\nAstronomical dawn is the moment when the geometric center of the Sun is 18 degrees below the horizon in the morning. Astronomical dusk is the moment when the geometric center of the Sun is 18 degrees below the horizon in the evening. After astronomical dusk and before astronomical dawn, the sky is not illuminated by the sun.\n\nMorning astronomical twilight begins (astronomical dawn) when the geometric center of the sun is 18° below the horizon in the morning and ends when the geometric center of the sun is 12° below the horizon in the morning. Evening astronomical twilight begins when the geometric center of the sun is 12° below the horizon in the evening and ends (astronomical dusk) when the geometric center of the sun is 18° below the horizon in the evening.\nIn some places (away from urban light pollution, moonlight, auroras, and other sources of light), where the sky is dark enough for nearly all astronomical observations, astronomers can easily make observations of point sources such as stars both during and after astronomical twilight in the evening and both before and during astronomical twilight in the morning. However, some critical observations, such as of faint diffuse items such as nebulae and galaxies, may require observation beyond the limit of astronomical twilight. Theoretically, the faintest stars detectable by the naked eye (those of approximately the sixth magnitude) will become visible in the evening at astronomical dusk, and become invisible at astronomical dawn.\nHowever, in other places, especially those with skyglow, astronomical twilight may be almost indistinguishable from night. In the evening, even when astronomical twilight has yet to end and in the morning when astronomical twilight has already begun, most casual observers would consider the entire sky fully dark. Because of light pollution, observers in some localities, generally in large cities, may never have the opportunity to view even fourth-magnitude stars, irrespective of the presence of any twilight at all, and to experience truly dark skies.\n\nObservers within 48.5 degrees of the Equator can view twilight twice each day on every date of the year between astronomical dawn, nautical dawn, or civil dawn, and sunrise as well as between sunset and civil dusk, nautical dusk, or astronomical dusk. This also occurs for most observers at higher latitudes on many dates throughout the year, except those around the summer solstice. However, at latitudes closer than 9 degrees to either Pole, the Sun cannot rise above the horizon nor sink more than 18 degrees below it on the same day on any date, this example of twilight cannot occur because the angular difference between solar noon and solar midnight elevates less than 18 degrees.\n\nAt latitudes greater than about 48.5 degrees North or South, on dates near the summer solstice, twilight can last from sunset to sunrise, since the Sun does not go more than 18 degrees below the horizon, so complete darkness does not occur even at solar midnight. These latitudes include many densely populated regions of the Earth, including the entire United Kingdom and other countries in northern Europe.\n\nIn Arctic and Antarctic latitudes in wintertime, the polar night only rarely produces complete darkness for 24 hours each day. This can occur only at locations within 5.5 degrees of latitude of the Pole, and there only on dates close to the winter solstice. At all other latitudes and dates, the polar night includes a daily period of twilight, when the Sun is not far below the horizon. Around winter solstice, when the solar declination changes slowly, complete darkness lasts several weeks at the Pole itself, e.g., from May 11 to July 31 at Amundsen–Scott South Pole Station. North Pole has the experience of this from November 12 to January 30.\n\nAt latitudes within 9 degrees of either Pole, as the sun's angular elevation difference is less than 18 degrees, twilight can last for the entire 24 hours. This occurs for one day at latitudes near 9 degrees from the Pole and extends up to several weeks the further towards the Pole one goes. The only permanent settlement to experience this condition is Alert, Nunavut, Canada, where it occurs for a week in late February, and again in late October.\n\nThe apparent travel of the sun occurs at the rate of 15 degrees per hour (360° per day), but sunrise and sunset happen typically at oblique angles to the horizon and the actual duration of any twilight period will be a function of that angle, being longer for more oblique angles. This angle of the sun's motion with respect to the horizon changes with latitude as well as the time of year (affecting the angle of the Earth's axis with respect to the sun).\n\nAt Greenwich, England (51.5°N), the duration of civil twilight will vary from 33 minutes to 48 minutes, depending on the time of year. At the equator, conditions can go from day to night in as little as 20–25 minutes. This is true because at low latitudes the sun's apparent movement is perpendicular to the observer's horizon. But at the poles, civil twilight can be as long as 2–3 weeks. In the Arctic and Antarctic regions, twilight (if there is any) can last for several hours. There is no astronomical twilight at the poles near the winter solstice (for about 74 days at the North Pole and about 80 days at the South Pole). As one gets closer to the Arctic and Antarctic circles, the sun's disk moves toward the observer's horizon at a lower angle. The observer's earthly location will pass through the various twilight zones less directly, taking more time.\n\nWithin the polar circles, twenty-four-hour daylight is encountered in summer, and in regions very close to the poles, twilight can last for weeks on the winter side of the equinoxes. Outside the polar circles, where the angular distance from the polar circle is less than the angle which defines twilight (see above), twilight can continue through local midnight near the summer solstice. The precise position of the polar circles, and the regions where twilight can continue through local midnight, varies slightly from year to year with Earth's axial tilt. The lowest latitudes at which the various twilights can continue through local midnight are approximately 60.561° (60°33′43″) for civil twilight, 54.561° (54°33′43″) for nautical twilight and 48.561° (48°33′43″) for astronomical twilight.\n\nThese are the largest cities of their respective countries where the various twilights can continue through local solar midnight:\n\n\nAlthough Helsinki, Oslo, Stockholm, Tallinn, and Saint Petersburg also enter into nautical twilight after sunset, they do have noticeably lighter skies at night during the summer solstice than other locations mentioned in their category above, because they do not go far into nautical twilight. A white night is a night with only civil twilight which lasts from sunset to sunrise.\n\nAt the winter solstice within the polar circle, twilight can extend through solar noon at latitudes below 72.561° (72°33′43″) for civil twilight, 78.561° (78°33′43″) for nautical twilight, and 84.561° (84°33′43″) for astronomical twilight.\n\nTwilight on Mars is longer than on Earth, lasting for up to two hours before sunrise or after sunset. Dust high in the atmosphere scatters light to the night side of the planet. Similar twilights are seen on Earth following major volcanic eruptions.\n\nIn Christian practice, \"vigil\" observances often occur during twilight on the evening before major feast days or holidays. For example, the \"Easter Vigil\" is held in the hours of darkness between sunset on Holy Saturday and sunrise on Easter Day — most commonly in the evening of Holy Saturday or midnight — and is the first celebration of Easter, days traditionally being considered to begin at sunset.\n\nTwilight is sacred in Hinduism. It is called गोधूळिवेळ \"gōdhūḷivēḷ\" in Marathi or गोधूलिवेला \"godhūlivelā\" in Hindi, గొధూళివేళ \"godhoolivela\" in Telugu, literally \"cow dust time\". Many rituals, including Sandhyavandanam and Puja, are performed at twilight hour. Eating of food is not advised during this time. Sometimes it is referred to as Asurasandhya vela. It is believed that Asuras are active during these hours. One of the avatars of Lord Vishnu, Narasimha, is closely associated with the twilight period. According to Hindu scriptures, a daemonic king, Hiranakashipa, performed penance and obtained a boon from Brahma that he could not be killed during day or night and neither by human nor animal. Lord Vishnu appeared in a half-man half-lion form (neither human nor animal), ended the life of Hiranakashipa during twilight (neither day nor night).\n\nTwilight is important in Islam as it determines when certain universally obligatory prayers are to be recited. Morning twilight is when morning prayers (\"Fajr\") are done, while evening twilight is the time for evening prayers (\"Maghrib prayer\"). Suhoor (early morning meal) time for fasting (during Ramadan and other times) ends at morning twilight (at dawn). Fasting is from the beginning of dawn to sunset. There is also an important discussion in Islamic jurisprudence between \"true dawn\" and \"false dawn\".\n\nIn Judaism, twilight is considered neither day nor night; consequently it is treated as a safeguard against encroachment upon either. For example, the twilight of Friday is reckoned as Sabbath eve, and that of Saturday as Sabbath day; and the same rule applies to festival days.\n\n\n"}
{"id": "51467331", "url": "https://en.wikipedia.org/wiki?curid=51467331", "title": "Uganda Energy Credit Capitalisation Company", "text": "Uganda Energy Credit Capitalisation Company\n\nThe Uganda Energy Credit Capitalisation Company (UECCC) is a company owned by the government of Uganda. It is responsible for coordinating funding from the Ugandan government, international development partners and the private sector, to invest in renewable energy infrastructure in Uganda, with emphasis on the promotion of private sector participation.\n\nUECCC's headquarters is located in Amber House, at 29-33 Kampala Road, in the centre of Kampala, Uganda's capital and largest city. The coordinates of the company headquarters are 00°18'48.0\"N, 32°34'55.0\"E (Latitude:0.313340; Longitude:32.581949).\n\nThe company was established in 2009 and coordinates investment into renewable energy sources in the country. The company offers technical, financial, and advisory services to the lending financial institution and to the renewable energy project developer. Services offered include the following: (a) Liquidity refinance option (b) Cash reserving (c) Partial risk guarantee (d) Solar refinance facility to participating microfinance institutions (e) Bridge financing facility (f) Subordinated debt finance (g) Interest rate buy down and (h) Transaction advisory services. Participating international development partners include the World Bank and KfW.\n\n\n"}
{"id": "32502", "url": "https://en.wikipedia.org/wiki?curid=32502", "title": "Vacuum", "text": "Vacuum\n\nVacuum is space devoid of matter. The word stems from the Latin adjective \"vacuus\" for \"vacant\" or \"void\". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a \"perfect\" vacuum, which they sometimes simply call \"vacuum\" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.\n\nThe \"quality\" of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%. Much higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm. Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average in intergalactic space. According to modern understanding, even if all matter could be removed from a volume, it would still not be \"empty\" due to vacuum fluctuations, dark energy, transiting gamma rays, cosmic rays, neutrinos, and other phenomena in quantum physics. In the study of electromagnetism in the 19th century, vacuum was thought to be filled with a medium called aether. In modern particle physics, the vacuum state is considered the ground state of a field.\n\nVacuum has been a frequent topic of philosophical debate since ancient Greek times, but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643, and other experimental techniques were developed as a result of his theories of atmospheric pressure. A torricellian vacuum is created by filling a tall glass container closed at one end with mercury, and then inverting it in a bowl to contain the mercury (see below).\n\nVacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes, and a wide array of vacuum technology has since become available. The recent development of human spaceflight has raised interest in the impact of vacuum on human health, and on life forms in general.\nThe word \"vacuum\" comes , noun use of neuter of \"vacuus\", meaning \"empty\", related to \"vacare\", meaning \"be empty\".\n\n\"Vacuum\" is one of the few words in the English language that contains two consecutive letters '\"u\"'.\n\nHistorically, there has been much dispute over whether such a thing as a vacuum can exist. Ancient Greek philosophers debated the existence of a vacuum, or void, in the context of atomism, which posited void and atom as the fundamental explanatory elements of physics. Following Plato, even the abstract concept of a featureless void faced considerable skepticism: it could not be apprehended by the senses, it could not, itself, provide additional explanatory power beyond the physical volume with which it was commensurate and, by definition, it was quite literally nothing at all, which cannot rightly be said to exist. Aristotle believed that no void could occur naturally, because the denser surrounding material continuum would immediately fill any incipient rarity that might give rise to a void.\n\nIn his \"Physics\", book IV, Aristotle offered numerous arguments against the void: for example, that motion through a medium which offered no impediment could continue \"ad infinitum\", there being no reason that something would come to rest anywhere in particular. Although Lucretius argued for the existence of vacuum in the first century BC and Hero of Alexandria tried unsuccessfully to create an artificial vacuum in the first century AD, it was European scholars such as Roger Bacon, Blasius of Parma and Walter Burley in the 13th and 14th century who focused considerable attention on these issues. Eventually following Stoic physics in this instance, scholars from the 14th century onward increasingly departed from the Aristotelian perspective in favor of a supernatural void beyond the confines of the cosmos itself, a conclusion widely acknowledged by the 17th century, which helped to segregate natural and theological concerns.\n\nAlmost two thousand years after Plato, René Descartes also proposed a geometrically based alternative theory of atomism, without the problematic nothing–everything dichotomy of void and atom. Although Descartes agreed with the contemporary position, that a vacuum does not occur in nature, the success of his namesake coordinate system and more implicitly, the spatial–corporeal component of his metaphysics would come to define the philosophically modern notion of empty space as a quantified extension of volume. By the ancient definition however, directional information and magnitude were conceptually distinct.\n\nIn the medieval Middle Eastern world, the physicist and Islamic scholar, Al-Farabi (Alpharabius, 872–950), conducted a small experiment concerning the existence of vacuum, in which he investigated handheld plungers in water. He concluded that air's volume can expand to fill available space, and he suggested that the concept of perfect vacuum was incoherent. However, according to Nader El-Bizri, the physicist Ibn al-Haytham (Alhazen, 965–1039) and the Mu'tazili theologians disagreed with Aristotle and Al-Farabi, and they supported the existence of a void. Using geometry, Ibn al-Haytham mathematically demonstrated that place (\"al-makan\") is the imagined three-dimensional void between the inner surfaces of a containing body. According to Ahmad Dallal, Abū Rayhān al-Bīrūnī also states that \"there is no observable evidence that rules out the possibility of vacuum\". The suction pump later appeared in Europe from the 15th century.\n\nMedieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising. The commonly held view that nature abhorred a vacuum was called \"horror vacui\". Speculation that even God could not create a vacuum if he wanted to was shut down by the 1277 Paris condemnations of Bishop Etienne Tempier, which required there to be no restrictions on the powers of God, which led to the conclusion that God could create a vacuum if he so wished.\nJean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.\n\nThe 17th century saw the first attempts to quantify measurements of partial vacuum. Evangelista Torricelli's mercury barometer of 1643 and Blaise Pascal's experiments both demonstrated a partial vacuum.\n\nIn 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that teams of horses could not separate two hemispheres from which the air had been partially evacuated. Robert Boyle improved Guericke's design and with the help of Robert Hooke further developed vacuum pump technology. Thereafter, research into the partial vacuum lapsed until 1850 when August Toepler invented the Toepler Pump and Heinrich Geissler invented the mercury displacement pump in 1855, achieving a partial vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, which renewed interest in further research.\n\nWhile outer space provides the most rarefied example of a naturally occurring partial vacuum, the heavens were originally thought to be seamlessly filled by a rigid indestructible material called aether. Borrowing somewhat from the pneuma of Stoic physics, aether came to be regarded as the rarefied air from which it took its name, (see Aether (mythology)). Early theories of light posited a ubiquitous terrestrial and celestial medium through which light propagated. Additionally, the concept informed Isaac Newton's explanations of both refraction and of radiant heat. 19th century experiments into this luminiferous aether attempted to detect a minute drag on the Earth's orbit. While the Earth does, in fact, move through a relatively dense medium in comparison to that of interstellar space, the drag is so minuscule that it could not be detected. In 1912, astronomer Henry Pickering commented: \"While the interstellar absorbing medium may be simply the ether, [it] is characteristic of a gas, and free gaseous molecules are certainly there\".\n\nLater, in 1930, Paul Dirac proposed a model of the vacuum as an infinite sea of particles possessing negative energy, called the Dirac sea. This theory helped refine the predictions of his earlier formulated Dirac equation, and successfully predicted the existence of the positron, confirmed two years later. Werner Heisenberg's uncertainty principle formulated in 1927, predict a fundamental limit within which instantaneous position and momentum, or energy and time can be measured. This has far reaching consequences on the \"emptiness\" of space between particles. In the late 20th century, so-called virtual particles that arise spontaneously from empty space were confirmed.\n\nThe strictest criterion to define a vacuum is a region of space and time where all the components of the stress–energy tensor are zero. This means that this region is devoid of energy and momentum, and by consequence, it must be empty of particles and other physical fields (such as electromagnetism) that contain energy and momentum.\n\nIn general relativity, a vanishing stress-energy tensor implies, through Einstein field equations, the vanishing of all the components of the Ricci tensor. Vacuum does not mean that the curvature of space-time is necessarily flat: the gravitational field can still produce curvature in a vacuum in the form of tidal forces and gravitational waves (technically, these phenomena are the components of the Weyl tensor). The black hole (with zero electric charge) is an elegant example of a region completely \"filled\" with vacuum, but still showing a strong curvature.\n\nIn classical electromagnetism, the vacuum of free space, or sometimes just \"free space\" or \"perfect vacuum\", is a standard reference medium for electromagnetic effects. Some authors refer to this reference medium as \"classical vacuum\", a terminology intended to separate this concept from QED vacuum or QCD vacuum, where vacuum fluctuations can produce transient virtual particle densities and a relative permittivity and relative permeability that are not identically unity.\n\nIn the theory of classical electromagnetism, free space has the following properties:\n\nThe vacuum of classical electromagnetism can be viewed as an idealized electromagnetic medium with the constitutive relations in SI units:\nrelating the electric displacement field D to the electric field E and the magnetic field or \"H\"-field H to the magnetic induction or \"B\"-field B. Here r is a spatial location and \"t\" is time.\n\nIn quantum mechanics and quantum field theory, the vacuum is defined as the state (that is, the solution to the equations of the theory) with the lowest possible energy (the ground state of the Hilbert space). In quantum electrodynamics this vacuum is referred to as 'QED vacuum' to distinguish it from the vacuum of quantum chromodynamics, denoted as QCD vacuum. QED vacuum is a state with no matter particles (hence the name), and also no photons. As described above, this state is impossible to achieve experimentally. (Even if every matter particle could somehow be removed from a volume, it would be impossible to eliminate all the blackbody photons.) Nonetheless, it provides a good model for realizable vacuum, and agrees with a number of experimental observations as described next.\n\nQED vacuum has interesting and complex properties. In QED vacuum, the electric and magnetic fields have zero average values, but their variances are not zero. As a result, QED vacuum contains vacuum fluctuations (virtual particles that hop into and out of existence), and a finite energy called vacuum energy. Vacuum fluctuations are an essential and ubiquitous part of quantum field theory. Some experimentally verified effects of vacuum fluctuations include spontaneous emission and the Lamb shift. Coulomb's law and the electric potential in vacuum near an electric charge are modified.\n\nTheoretically, in QCD multiple vacuum states can coexist. The starting and ending of cosmological inflation is thought to have arisen from transitions between different vacuum states. For theories obtained by quantization of a classical theory, each stationary point of the energy in the configuration space gives rise to a single vacuum. String theory is believed to have a huge number of vacua – the so-called string theory landscape.\n\nOuter space has very low density and pressure, and is the closest physical approximation of a perfect vacuum. But no vacuum is truly perfect, not even in interstellar space, where there are still a few hydrogen atoms per cubic meter.\n\nStars, planets, and moons keep their atmospheres by gravitational attraction, and as such, atmospheres have no clearly delineated boundary: the density of atmospheric gas simply decreases with distance from the object. The Earth's atmospheric pressure drops to about at of altitude, the Kármán line, which is a common definition of the boundary with outer space. Beyond this line, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar winds, so the definition of pressure becomes difficult to interpret. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather. Astrophysicists prefer to use number density to describe these environments, in units of particles per cubic centimetre.\n\nBut although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. Most artificial satellites operate in this region called low Earth orbit and must fire their engines every few days to maintain orbit. The drag here is low enough that it could theoretically be overcome by radiation pressure on solar sails, a proposed propulsion system for interplanetary travel. Planets are too massive for their trajectories to be significantly affected by these forces, although their atmospheres are eroded by the solar winds.\n\nAll of the observable universe is filled with large numbers of photons, the so-called cosmic background radiation, and quite likely a correspondingly large number of neutrinos. The current temperature of this radiation is about 3 K, or −270 degrees Celsius or −454 degrees Fahrenheit.\n\nThe quality of a vacuum is indicated by the amount of matter remaining in the system, so that a high quality vacuum is one with very little matter left in it. Vacuum is primarily measured by its absolute pressure, but a complete characterization requires further parameters, such as temperature and chemical composition. One of the most important parameters is the mean free path (MFP) of residual gases, which indicates the average distance that molecules will travel between collisions with each other. As the gas density decreases, the MFP increases, and when the MFP is longer than the chamber, pump, spacecraft, or other objects present, the continuum assumptions of fluid mechanics do not apply. This vacuum state is called \"high vacuum\", and the study of fluid flows in this regime is called particle gas dynamics. The MFP of air at atmospheric pressure is very short, 70 nm, but at 100 mPa (~) the MFP of room temperature air is roughly 100 mm, which is on the order of everyday objects such as vacuum tubes. The Crookes radiometer turns when the MFP is larger than the size of the vanes.\n\nVacuum quality is subdivided into ranges according to the technology required to achieve it or measure it. These ranges do not have universally agreed definitions, but a typical distribution is shown in the following table. As we travel into orbit, outer space and ultimately intergalactic space, the pressure varies by several orders of magnitude.\n\n\nVacuum is measured in units of pressure, typically as a subtraction relative to ambient atmospheric pressure on Earth. But the amount of relative measurable vacuum varies with local conditions. On the surface of Jupiter, where ground level atmospheric pressure is much higher than on Earth, much higher relative vacuum readings would be possible. On the surface of the moon with almost no atmosphere, it would be extremely difficult to create a measurable vacuum relative to the local environment.\n\nSimilarly, much higher than normal relative vacuum readings are possible deep in the Earth's ocean. A submarine maintaining an internal pressure of 1 atmosphere submerged to a depth of 10 atmospheres (98 metres; a 9.8 metre column of seawater has the equivalent weight of 1 atm) is effectively a vacuum chamber keeping out the crushing exterior water pressures, though the 1 atm inside the submarine would not normally be considered a vacuum.\n\nTherefore, to properly understand the following discussions of vacuum measurement, it is important that the reader assumes the relative measurements are being done on Earth at sea level, at exactly 1 atmosphere of ambient atmospheric pressure.\n\nThe SI unit of pressure is the pascal (symbol Pa), but vacuum is often measured in torrs, named for Torricelli, an early Italian physicist (1608–1647). A torr is equal to the displacement of a millimeter of mercury (mmHg) in a manometer with 1 torr equaling 133.3223684 pascals above absolute zero pressure. Vacuum is often also measured on the barometric scale or as a percentage of atmospheric pressure in bars or atmospheres. Low vacuum is often measured in millimeters of mercury (mmHg) or pascals (Pa) below standard atmospheric pressure. \"Below atmospheric\" means that the absolute pressure is equal to the current atmospheric pressure.\n\nIn other words, most low vacuum gauges that read, for example 50.79 Torr. Many inexpensive low vacuum gauges have a margin of error and may report a vacuum of 0 Torr but in practice this generally requires a two-stage rotary vane or other medium type of vacuum pump to go much beyond (lower than) 1 torr.\n\nMany devices are used to measure the pressure in a vacuum, depending on what range of vacuum is needed.\n\nHydrostatic gauges (such as the mercury column manometer) consist of a vertical column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight is in equilibrium with the pressure differential between the two ends of the tube. The simplest design is a closed-end U-shaped tube, one side of which is connected to the region of interest. Any fluid can be used, but mercury is preferred for its high density and low vapour pressure. Simple hydrostatic gauges can measure pressures ranging from 1 torr (100 Pa) to above atmospheric. An important variation is the McLeod gauge which isolates a known volume of vacuum and compresses it to multiply the height variation of the liquid column. The McLeod gauge can measure vacuums as high as 10 torr (0.1 mPa), which is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-controlled properties. These indirect measurements must be calibrated via a direct measurement, most commonly a McLeod gauge.\n\nThe kenotometer is a particular type of hydrostatic gauge, typically used in power plants using steam turbines. The kenotometer measures the vacuum in the steam space of the condenser, that is, the exhaust of the last stage of the turbine.\n\nMechanical or elastic gauges depend on a Bourdon tube, diaphragm, or capsule, usually made of metal, which will change shape in response to the pressure of the region in question. A variation on this idea is the capacitance manometer, in which the diaphragm makes up a part of a capacitor. A change in pressure leads to the flexure of the diaphragm, which results in a change in capacitance. These gauges are effective from 10 torr to 10 torr, and beyond.\n\nThermal conductivity gauges rely on the fact that the ability of a gas to conduct heat decreases with pressure. In this type of gauge, a wire filament is heated by running current through it. A thermocouple or Resistance Temperature Detector (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 torr to 10 torr, but they are sensitive to the chemical composition of the gases being measured.\n\nIonization gauges are used in ultrahigh vacuum. They come in two types: hot cathode and cold cathode. In the hot cathode version an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10 torr to 10 torr. The principle behind cold cathode version is the same, except that electrons are produced in a discharge created by a high voltage electrical discharge. Cold cathode gauges are accurate from 10 torr to 10 torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.\n\nVacuum is useful in a variety of processes and devices. Its first widespread use was in the incandescent light bulb to protect the filament from chemical degradation. The chemical inertness produced by a vacuum is also useful for electron beam welding, cold welding, vacuum packing and vacuum frying. Ultra-high vacuum is used in the study of atomically clean substrates, as only a very good vacuum preserves atomic-scale clean surfaces for a reasonably long time (on the order of minutes to days). High to ultra-high vacuum removes the obstruction of air, allowing particle beams to deposit or remove materials without contamination. This is the principle behind chemical vapor deposition, physical vapor deposition, and dry etching which are essential to the fabrication of semiconductors and optical coatings, and to surface science. The reduction of convection provides the thermal insulation of thermos bottles. Deep vacuum lowers the boiling point of liquids and promotes low temperature outgassing which is used in freeze drying, adhesive preparation, distillation, metallurgy, and process purging. The electrical properties of vacuum make electron microscopes and vacuum tubes possible, including cathode ray tubes. Vacuum interrupters are used in electrical switchgear. Vacuum arc processes are industrially important for production of certain grades of steel or high purity materials. The elimination of air friction is useful for flywheel energy storage and ultracentrifuges.\n\nVacuums are commonly used to produce suction, which has an even wider variety of applications. The Newcomen steam engine used vacuum instead of pressure to drive a piston. In the 19th century, vacuum was used for traction on Isambard Kingdom Brunel's experimental atmospheric railway. Vacuum brakes were once widely used on trains in the UK but, except on heritage railways, they have been replaced by air brakes.\n\nManifold vacuum can be used to drive accessories on automobiles. The best-known application is the vacuum servo, used to provide power assistance for the brakes. Obsolete applications include vacuum-driven windscreen wipers and Autovac fuel pumps. Some aircraft instruments (Attitude Indicator (AI) and the Heading Indicator (HI)) are typically vacuum-powered, as protection against loss of all (electrically powered) instruments, since early aircraft often did not have electrical systems, and since there are two readily available sources of vacuum on a moving aircraft, the engine and an external venturi.\nVacuum induction melting uses electromagnetic induction within a vacuum.\n\nMaintaining a vacuum in the Condenser is an important aspect of the efficient operation of steam turbines. A steam jet ejector or liquid ring vacuum pump is used for this purpose. The typical vacuum maintained in the Condenser steam space at the exhaust of the turbine (also called Condenser Backpressure) is in the range 5 to 15 kPa (absolute), depending on the type of condenser and the ambient conditions.\n\nEvaporation and sublimation into a vacuum is called outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure. Outgassing has the same effect as a leak and can limit the achievable vacuum. Outgassing products may condense on nearby colder surfaces, which can be troublesome if they obscure optical instruments or react with other materials. This is of great concern to space missions, where an obscured telescope or solar cell can ruin an expensive mission.\n\nThe most prevalent outgassing product in vacuum systems is water absorbed by chamber materials. It can be reduced by desiccating or baking the chamber, and removing absorbent materials. Outgassed water can condense in the oil of rotary vane pumps and reduce their net speed drastically if gas ballasting is not used. High vacuum systems must be clean and free of organic matter to minimize outgassing.\n\nUltra-high vacuum systems are usually baked, preferably under vacuum, to temporarily raise the vapour pressure of all outgassing materials and boil them off. Once the bulk of the outgassing materials are boiled off and evacuated, the system may be cooled to lower vapour pressures and minimize residual outgassing during actual operation. Some systems are cooled well below room temperature by liquid nitrogen to shut down residual outgassing and simultaneously cryopump the system.\n\nFluids cannot generally be pulled, so a vacuum cannot be created by suction. Suction can spread and dilute a vacuum by letting a higher pressure push fluids into it, but the vacuum has to be created first before suction can occur. The easiest way to create an artificial vacuum is to expand the volume of a container. For example, the diaphragm muscle expands the chest cavity, which causes the volume of the lungs to increase. This expansion reduces the pressure and creates a partial vacuum, which is soon filled by air pushed in by atmospheric pressure.\n\nTo continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind positive displacement pumps, like the manual water pump for example. Inside the pump, a mechanism expands a small sealed cavity to create a vacuum. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.\n\nThe above explanation is merely a simple introduction to vacuum pumping, and is not representative of the entire range of pumps in use. Many variations of the positive displacement pump have been developed, and many other pump designs rely on fundamentally different principles. Momentum transfer pumps, which bear some similarities to dynamic pumps used at higher pressures, can achieve much higher quality vacuums than positive displacement pumps. Entrapment pumps can capture gases in a solid or absorbed state, often with no moving parts, no seals and no vibration. None of these pumps are universal; each type has important performance limitations. They all share a difficulty in pumping low molecular weight gases, especially hydrogen, helium, and neon.\n\nThe lowest pressure that can be attained in a system is also dependent on many things other than the nature of the pumps. Multiple pumps may be connected in series, called stages, to achieve higher vacuums. The choice of seals, chamber geometry, materials, and pump-down procedures will all have an impact. Collectively, these are called \"vacuum technique\". And sometimes, the final pressure is not the only relevant characteristic. Pumping systems differ in oil contamination, vibration, preferential pumping of certain gases, pump-down speeds, intermittent duty cycle, reliability, or tolerance to high leakage rates.\n\nIn ultra high vacuum systems, some very \"odd\" leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the adsorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The permeability of the metallic chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.\n\nThe lowest pressures currently achievable in laboratory are about 10 torr (13 pPa). However, pressures as low as (6.7 fPa) have been indirectly measured in a 4 K cryogenic vacuum system. This corresponds to ≈100 particles/cm.\n\nHumans and animals exposed to vacuum will lose consciousness after a few seconds and die of hypoxia within minutes, but the symptoms are not nearly as graphic as commonly depicted in media and popular culture. The reduction in pressure lowers the temperature at which blood and other body fluids boil, but the elastic pressure of blood vessels ensures that this boiling point remains above the internal body temperature of Although the blood will not boil, the formation of gas bubbles in bodily fluids at reduced pressures, known as ebullism, is still a concern. The gas may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Swelling and ebullism can be restrained by containment in a flight suit. Shuttle astronauts wore a fitted elastic garment called the Crew Altitude Protection Suit (CAPS) which prevents ebullism at pressures as low as 2 kPa (15 Torr). Rapid boiling will cool the skin and create frost, particularly in the mouth, but this is not a significant hazard.\n\nAnimal experiments show that rapid and complete recovery is normal for exposures shorter than 90 seconds, while longer full-body exposures are fatal and resuscitation has never been successful. A study by NASA on eight chimpanzees found all of them survived two and a half minute exposures to vacuum. There is only a limited amount of data available from human accidents, but it is consistent with animal data. Limbs may be exposed for much longer if breathing is not impaired. Robert Boyle was the first to show in 1660 that vacuum is lethal to small animals.\n\nAn experiment indicates that plants are able to survive in a low pressure environment (1.5 kPa) for about 30 minutes.\n\nCold or oxygen-rich atmospheres can sustain life at pressures much lower than atmospheric, as long as the density of oxygen is similar to that of standard sea-level atmosphere. The colder air temperatures found at altitudes of up to 3 km generally compensate for the lower pressures there. Above this altitude, oxygen enrichment is necessary to prevent altitude sickness in humans that did not undergo prior acclimatization, and spacesuits are necessary to prevent ebullism above 19 km. Most spacesuits use only 20 kPa (150 Torr) of pure oxygen. This pressure is high enough to prevent ebullism, but decompression sickness and gas embolisms can still occur if decompression rates are not managed.\n\nRapid decompression can be much more dangerous than vacuum exposure itself. Even if the victim does not hold his or her breath, venting through the windpipe may be too slow to prevent the fatal rupture of the delicate alveoli of the lungs. Eardrums and sinuses may be ruptured by rapid decompression, soft tissues may bruise and seep blood, and the stress of shock will accelerate oxygen consumption leading to hypoxia. Injuries caused by rapid decompression are called barotrauma. A pressure drop of 13 kPa (100 Torr), which produces no symptoms if it is gradual, may be fatal if it occurs suddenly.\n\nSome extremophile microorganisms, such as tardigrades, can survive vacuum conditions for periods of days or weeks.\n\n\n\n"}
{"id": "1278615", "url": "https://en.wikipedia.org/wiki?curid=1278615", "title": "Wave loading", "text": "Wave loading\n\nWave loading is most commonly the application of a pulsed or wavelike load to a material or object. This is most commonly used in the analysis of piping, ships, or building structures which experience wind, water, or seismic disturbances.\n\n"}
{"id": "4274023", "url": "https://en.wikipedia.org/wiki?curid=4274023", "title": "Yoldia Sea", "text": "Yoldia Sea\n\nYoldia Sea is a name given by geologists to a variable brackish-water stage in the Baltic Sea basin that prevailed after the Baltic ice lake was drained to sea level during the Weichsel glaciation. Dates for the Yoldia sea are obtained mainly by radiocarbon dating material from ancient sediments and shore lines and from clay-varve chronology. \nThey tend to vary by as much as a thousand years, but a good estimate is 10,300 – 9500 radiocarbon years BP, equivalent to ca 11,700-10,700 calendar years BP. The sea ended gradually when isostatic rise of Scandinavia closed or nearly closed its effluents, altering the balance between saline and fresh water. The Yoldia Sea became Ancylus Lake. The Yoldia Sea stage had three phases of which only the middle phase had brackish water.\n\nThe name of the sea is adapted from the obsolete name of the bivalve, \"Portlandia arctica\" (previously known as \"Yoldia arctica\"), found around Stockholm. This bivalve requires cold saline water. \nIt characterizes the middle phase of the Yoldia Sea, during which saline water poured into the Baltic, before the acceleration of glacial melting.\n\nThe Baltic Ice Lake, the Yoldia Sea, the Ancylus Lake and the Littorina Sea are four recognized stages in the postglacial progression of the Baltic basin – there are also transition periods which can be considered as substages. From earliest to most recent they run: \n\nThe Baltic Ice Lake came to an end when it overflowed through central Sweden and drained, a process complete by about 10,300 BP (radiocarbon years). The straits through the present Stockholm region (via Lake Vänern and the Strait of Närke) to the Atlantic were the only outlet at that time. When lake level reached sea level the difference in salinity caused a backflow from the North Sea, creating saline regions in which the marine bivalve Yoldia flourished. This phase lasted until about 10,000 BP.\n\nSubsequently, increased melting of the glacier provided additional fresh water and the lake became stratified (meromictic), with salt water on the bottom and fresh on top. Over the life of the sea and from location to location the salinity was a variable. Whether it is possible to speak of stages of salinity that would apply uniformly to the whole sea is debatable.\n\nAt about 10,000 BP, the exit continued to rise and the lake/sea broke through Denmark creating the first Great Belt channels. The total opening was less than 1 km wide and included two channels at the northern end. The Great Belt channels was blocked again by rising land from the post-glacial rebound that created Ancylus Lake.\n\nGeographically, the Gulf of Bothnia remained under the ice. The Gulf of Finland was open but most of Finland was an archipelago, over which debris carried by glacial streams gradually spread. A land bridge joined Germany to southern Sweden through Denmark. Relieved of its weight of ice, Finland rose gradually and unevenly from the sea. Parts of the Yoldia shoreline are above sea level today while other parts remain below. The Yoldia Sea toward its end was about 30m below current sea level. A channel at the location of the Neva River connected Yoldia Sea to Lake Ladoga.\n\nThe Yoldia Sea existed entirely within the Boreal Blytt-Sernander period. The forests and species lining its shores were boreal. Mesolithic cultures continued to occupy Denmark/south Sweden and the southern shores of the sea. The sea as an ecologic system came to an end when Scandinavia rose sufficiently to block the flow through the Stockholm area and the saline balance shifted toward a lacustrine ecology once again.\n"}
