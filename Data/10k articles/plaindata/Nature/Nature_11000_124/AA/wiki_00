{"id": "7777962", "url": "https://en.wikipedia.org/wiki?curid=7777962", "title": "Agrinierite", "text": "Agrinierite\n\nAgrinierite (K(Ca,Sr)(UO)O(OH)·5HO) is a mineral often found in the oxidation zone of uranium deposits. It is named for Henry Agrinier (1928–1971), an engineer for the Commissariat à l'Énergie Atomique.\n\n"}
{"id": "1172161", "url": "https://en.wikipedia.org/wiki?curid=1172161", "title": "Biaxial nematic", "text": "Biaxial nematic\n\nA biaxial nematic is a spatially homogeneous liquid crystal with three distinct optical axes. This is to be contrasted to a simple nematic, which has a single preferred axis, around which the system is rotationally symmetric. The symmetry group of a biaxial nematic is formula_1 i.e. that of a rectangular right parallelepiped, having 3 orthogonal formula_2 axes and three orthogonal mirror planes. In a frame co-aligned with optical axes the second rank order parameter tensor of a biaxial nematic has the form \n\nwhere \n\nformula_4 is the standard nematic scalar order parameter\n\nformula_5 a measure of the biaxiality. \n\nThe first report of a thermotropic biaxial nematic appeared in 2004 based on a boomerang shaped oxadiazole bent-core mesogen. The biaxial nematic phase for this particular compound only occurs at temperatures around 200 °C and is preceded by as yet unidentified smectic phases.\n\nIt is also found that this material can segregate into chiral domains of opposite handedness. For this to happen the boomerang shaped molecules adopt a helical superstructure.\n\nIn one azo bent-core mesogen a thermal transition is found from a uniaxial N to a biaxial nematic N mesophase, as predicted by theory and simulation. This transition is observed on heating from the N phase with Polarizing optical microscopy as a change in Schlieren texture and increased light transmittance and from x-ray diffraction as the splitting of the nematic reflection. The transition is a second order transition with low energy content and therefore not observed in differential scanning calorimetry. The positional order parameter for the uniaxial nematic phase is 0.75 to 1.5 times the mesogen length and for the biaxial nematic phase 2 to 3.3 times the mesogen length.\n\nAnother strategy towards biaxial nematics is the use of mixtures of classical rodlike mesogens and disklike discotic mesogens. The biaxial nematic phase is expected to be located below the minimum in the rod-disk phase diagram. In one study a miscible system of rods and disks is actually found although the biaxial nematic phase remains elusive.\n\n"}
{"id": "2428300", "url": "https://en.wikipedia.org/wiki?curid=2428300", "title": "Charles Green (astronomer)", "text": "Charles Green (astronomer)\n\nCharles Green (baptised 26 December 1734 – 29 January 1771) was a British astronomer, noted for his assignment by the Royal Society in 1768 to the expedition sent to the Pacific Ocean in order to observe the transit of Venus aboard James Cook's \"Endeavour\".\n\nBorn sometime in December 1734, Green was the youngest son of Joshua Green, a prosperous farmer who lived near Swinton in Yorkshire. His education, according to his future brother-in-law William Wales, was chiefly at a school near Denmark Street in Soho, London. This school was run by his eldest brother, the clergyman Rev. John Green, and the younger Green went on to become an assistant teacher there, continuing his studies in astronomy until he joined the staff of the Royal Greenwich Observatory in 1760.\n\nGreen was appointed as Assistant to the Astronomer Royal, James Bradley, succeeding the astronomer Charles Mason who left to join the expedition to the Cape of Good Hope to observe the 1761 transit of Venus. Following Bradley's death in 1762, Green continued in the role of assistant for Bradley's successor Nathaniel Bliss. Due to the ill-health of Bliss, much of the work of practical observational astronomy devolved upon Green at this time.\n\nIn 1763, Green, along with the astronomer Nevil Maskelyne, was instructed by the Board of Longitude to make the voyage to Barbados to act as monitors of the test of John Harrison's H4 chronometer, which was in the running for the Longitude Prize, the prize offered by the British Government for the devisor of an accurate method of determining the longitude of a ship at sea. Their duties, after they arrived in Barbados and were awaiting the chronometer's arrival on another ship, involved observation of the Galilean moons of Jupiter to determine the local longitude, and after the arrival of the mechanism which was in the care of Harrison's son William, the comparison of the results of the chronometer with their astronomical observations to judge the mechanism's accuracy. On the voyage to Barbados, the pair were also using their observations to determine the accuracy of the lunar distance method, Maskelyne's own method of calculating the longitude of a ship at sea. An argument ensued between Maskelyne and William Harrison, which dragged in Green, as Harrison was of the opinion that Maskelyne was not an impartial observer and was a rival of his father in the pursuit of the Longitude Prize. A compromise was reached in which the monitoring of the Harrison mechanism was undertaken by Maskelyne and Green on alternate days.\n\nGreen's return to England in the autumn of 1764 coincided with the death of Nathaniel Bliss; Green returned to Greenwich to continue Bliss' work until the appointment of a new Astronomer Royal. The new Astronomer Royal proved to be Nevil Maskelyne. Green briefly served under him as assistant, but left Greenwich after a disagreement with his new superior. Wales' account says that Green was involved in a survey to determine the feasibility of transporting water from the River Colne to provide water to Marylebone in London, although Green's survey confirmed the fears of mill owners downstream that the canal would result in a failure of the water supply needed for their operations and led to the scheme being dropped. Although this is unmentioned in Wales's short biography, Green is recorded as being the purser on the HMS \"Aurora\" in 1768. In that same year, the Royal Society appointed him to accompany James Cook's voyage to observe the 1769 transit of Venus; despite their earlier disagreement, Maskelyne who was serving on the Society's \"Transit of Venus Committee\" recommended his former assistant for the post.\n\nGreen joined Cook's first voyage of circumnavigation in 1768, accompanied by a servant. Green was one of two official astronomers appointed by the Royal Society to observe the transit; the other was Cook himself, who was a capable observer in his own right. Green was to receive 200 guineas from the Society for the appointment. (Cook was to receive 100 guineas for his role in making the observations.) The Society provided instruments for the expedition, including two reflector telescopes built by James Short, two clocks and an astronomical quadrant; these were supplemented by a telescope in the possession of Daniel Solander and another provided by the Navy.\n\nThe expedition arrived at Tahiti, the chosen site for the observation of the transit, on 11 April 1769, anchoring in Matavai Bay on the north-western coast of the island two days later. A camp (Fort Venus) was established and the astronomical equipment was in place by the start of May. Of paramount importance for Green was the calculation of an accurate position of the location, which he calculated using the lunar distance method and by observation of the moons of Jupiter. The removal of the quadrant from the camp - either by the local Tahitians or by members of the expedition who traded it to the Tahitians - threatened to derail the enterprise, but it was soon recovered, albeit disassembled into pieces and with some damage, by the expedition botanist Joseph Banks, accompanied by Green.\n\nThe day of the transit, June 3, was a clear day and the transit was visible in its entirety. Although Cook had taken the precaution of sending two groups to outlying islands, the principal observation party remained at Fort Venus, where Green, Cook and (independently) Solander recorded the times of the transit. Cook noted in his log that the times recorded by the three observers for the times of contact differed significantly; this anomaly, now generally attributed to the \"black drop effect\", would cast significant doubt, in the eyes of the Royal Society and Nevil Maskelyne, on the usefulness and value of the observations later.\n\nDue to Green's death on the homeward voyage, the work of collating and analysing his results fell first to Cook, and ultimately to Nevil Maskelyne. Cook admitted that Green's papers were in a disorganised state, when he inspected them after Green's death, and that some of the timings were inconsistently recorded in various of the astronomer's papers. The Astronomer Royal was critical of Green's work, especially of the observations Green made to calculate the latitude and longitude of Fort Venus\n\nIt has been noted that although the times for the contacts given by the three observers (Green, Cook and Solander) present at Fort Venus vary, they show no pattern of inconsistency attributable to one observer alone.\n\nFollowing the departure from Tahiti, the \"Endeavour\" continued south-westwards as Cook carried out his orders to further explore the Southern Ocean. The expedition reached New Zealand in October 1769, giving Green an opportunity to observe the transit of Mercury from land. A landing was made at an inlet that would be subsequently called Mercury Bay on 3 November; the transit was successfully observed by Green on November 9.\n\nOn reaching Australia, and during Cook's voyage along the coast, he named an island, Green Island, after the astronomer in June 1770. Green, by this time had contracted scurvy.\n\nThe \"Endeavour\" was forced to make for Batavia (present-day Jakarta) for repairs. Disease were rife in the Dutch-controlled city, including malaria and dysentery; Green contracted the latter, dying on 29 January 1771, twelve days after the ship's departure from the port. Cook, in recording Green's death in his log, went on to add that Green had been in ill-health for some time and his lifestyle had contributed to his early death. An account published in a London newspaper described his final hours: \"He had been ill some time, and was directed by the surgeon to keep himself warm, but in a fit of phrensy he got up in the night and put his legs out of the portholes, which was the occasion of his death.\"\n\nGreen married Elizabeth Long in March 1768 in London; the records of the Royal Society mention making a provision of £50 per annum for his wife for the duration of the expedition. Green's sister Mary married the astronomer William Wales, who was part of a Royal Society expedition to Hudson Bay to observe the 1769 transit of Venus and who replaced Green for Cook's next voyage.\n\n\n"}
{"id": "1670323", "url": "https://en.wikipedia.org/wiki?curid=1670323", "title": "Cyrille Pierre Théodore Laplace", "text": "Cyrille Pierre Théodore Laplace\n\nCyrille Pierre Théodore Laplace (7 November 1793 – 24 January 1875) was a French navigator famous for his circumnavigation of the globe on board \"La Favorite\". He was pivotal in the opening of French trade in the Pacific and was instrumental in the establishment of the Hawaiian Catholic Church. He achieved the rank of captain.\n\nLaplace was born at sea on 7 November 1793. He joined the French Navy and fought in the Indian and Atlantic Oceans, along with battles in the West Indies. He was promoted from Aspirant to Ship-of-the-Line Lieutenant in 1823, and to Frigate Captain in 1828. He had been awarded he Cross of Saint-Louis in 1825. He, at some point, was in command of a schooner in Gorée, Senegal.\n\nAs British, American and Dutch voyages began solidifying their interests in Australia, Hawaii and New Guinea, the French government sought to secure the religious freedoms and rights of French residents in the South Pacific. Voyages such as that of Jules Dumont d'Urville in the 1820s had already collected vast amounts of knowledge of the area, and the French government were hoping to secure its economic opportunities. Having sent out two voyages already, that of the \"Astrolabe\" and the \"Bayonnaise\", the French began drawing plans for a third expedition. The original route planned was designed to complement that of Hyacinthe de Bougainville, re-establishing French influence on the Indo-China area. Laplace's prime objective was to re-establish French influence in Indo-China, to \"show the flag\" in the area. He was also asked to gather information that may be of use to merchants, such as customs, harbour regulations, conditions of entry and market information. On 30 December 1829, Laplace departed Toulon with a crew of 177 aboard \"La Favorite\".\n\nLaplace reached Gibraltar in one week, and decided to set sail for Gorée, where he spent a week. The ship then made its way south, crossing the Equator on the 4 February, and sighting the Cape of Good Hope on 6 March. Being forced to skip the scheduled stop in Cape Town due to poor weather, Laplace attempted to continue on to Île Bourbon. Forced south by winds, \"La Favorie\" was hit by a hurricane on 28 March. The hurricane reached Bourbon on 1 April, ravaging the island, and was closely followed by another, forcing Laplace to hurriedly set sail for Mauritius. Upon returning to St. Denis, a shark killed one of his crew members who was attempting to desert, and Laplace was unable to recover an anchor he had lost near the island.\n\nOn 1 May 1830, Laplace sailed towards India. Following the completion of one of his hydrological assignments, he continued on to Mahé, Seychelles. Six days later the crew had made their way past the Maldives and Ceylon (now known as Sri Lanka), before making landfall at Pondicherry on 9 June. In Madras (now known as Chennai), Laplace was able to purchase new anchors and cables, and left on 28 June. On the night of the 29th, \"La Favorite\" became grounded on a mudbank. The initial attempt to save the ship failed, but it was freed thanks to the help of nearby Indians. Laplace continued on to Yanaon before leaving the Indian coast on 2 August.\n\nLaplace arrived in Singapore on 17 August, the first of a long series of South East Asian cities. In Manila, many of his crew grew sick; cholera was diagnosed on the land and the crew were reporting having colds and fevers. On 28 September, \"La Favorite\" recorded its only cholera fatality; the epidemic cleared up before long. After travelling via Macao, Laplace arrived in Canton (today Guangzhou), with the intent of discussing the difficulties of French traders with the Chinese authorities. Laplace managed to secure France as the most favoured nation of China. Setting sail on 18 December, Laplace arrived at Tourane (today Da Nang) on 21 December. There, he came across the \"S. Michel\" wrecked off the coast and the crew sick. Laplace attempted to allow the crew on board, but many died from their illnesses. In Tourane, Laplace was met with hostility, as the authorities were paranoid of his intentions in the area. It was suspected that \"La Favorite\" had a large army concealed in the hold, and Chinese soldiers were brought into the city to oversee Laplace's actions. Laplace left the area in frustration of Emperor Minh Mạng's isolationist policies, the diplomatic mission having failed.\n\nOn 5 March, Laplace sailed south towards the Natuna Islands, charting the area as they moved. In particular, Laplace attempted to complement Bougainville's earlier workaround around the Anambas, not finishing his work until 4 April. He then set sail for Surabaya in the Dutch East Indies. There he was informed of the events of the July Revolution, and that a war was expected. After dysentery struck the ship, Laplace moved on to Banyuwangi. During the trip, several men died, while morale decreased. Laplace attributed the ships poor health to the decrease of wine rations.\n\nLaplace set sail for Hobart, and sighted Mewstone on 6 July. Two more men were buried on Bruny Island, before \"La Favorite\" was allowed to anchor at Hobart on 11 July. The sick men were sent to hospital, although three men were to die there. Laplace left Hobart on 7 August, and set sail for Sydney, arriving on 16 August. In Sydney the crew went to numerous social events, left to a twenty-one gun salute on 21 September 1831. Laplace proposed a short break in New Zealand, and anchored in the Bay of Islands on 2 October. There the crew gave gifts of guns to the Māori chieftains, who in turn greeted them with the haka. Laplace and his crew were 'disgusted' by the display, considering the Māori to be 'savages'. On 11 October, the crew took a number of detailed observations of New Zealand, including the Kawakawa River. These observations were taken to be an attempt to claim New Zealand for France, causing excitement in Sydney and even led to the British Government requesting clarification from the French. This concern, nonetheless, hurried the development of the Treaty of Waitangi.\n\nAfter his stay in New Zealand, Laplace headed east, arriving in Valparaíso on 14 November. He continued south past Cape Horn and reached Rio de Janeiro on 23 January 1832. After being farewelled by the crew of a British warship, Laplace set sail for France. After reaching Gibraltar on 11 April, \"La Favorite\" anchored in Toulon harbour on 21 April. Laplace's voyage was generally successful, despite the setback in China. His account of the voyage, \"Voyage autour du Monde par les Mers de l'Inde et de la Chine\", was published by the French government in 1833.\n\nIn 1837, Laplace once again undertook a voyage, this time as captain of the \"Artémise\". The aim of this voyage was primarily political; Laplace delivered a manifesto on the treatment of Catholics in the Kingdom of Hawaii in July 1839 which came to be known as the Laplace Affair. \n\n\n"}
{"id": "19072971", "url": "https://en.wikipedia.org/wiki?curid=19072971", "title": "Daragang Magayon", "text": "Daragang Magayon\n\nDaragang Magayon () is the heroine that appears in the legend of Mt. Mayon in Albay, Philippines.\n\nMagayon was the only daughter of Makusog (strong), the tribal chief of Rawis, whose mother was named Dawani (fairy), who died shortly after giving birth to her. She grew up to be a very beautiful and sweet woman that struck the swains from faraway tribes who vied for her attention. However, none of these young men could captivated the heart of Magayon, not even the handsome but haughty Pagtuga (eruption), a hunter and the chief of the Iriga tribe. He gave fabulous gifts to Magayon, yet it was not enough to capture her attention.\n\nOne day, Daragang Magayon was bathing in the Yawa river when she slipped on the rocks. Unfortunately, she did not know how to swim. Panganoron was passing by and saw what was happening to Daragang Magayon, so he saved her from the river. He began to court her, and after some time Magayon accepted his proposal and he received her father's blessings. \n\nHowever, when Pagtuga knew about the relationship between Panganoron and Magayon, Pagtuga kidnapped Magayon's father and asked Magayon to be his wife in exchange for her father's freedom.\n\nPanganoron knew about the situation so he asked his warriors to join him in the war with Pagtuga in the mountains. The war was fierce and breathtaking. The people and Magayon watched the war between the two of them. In the end, Panganoron killed Pagtuga. Upon his victory, Magayon ran to embrace her lover.\n\nAs Magayon ran toward her lover, an arrow shot by one of Pagtuga's warriors struck Panganoron, killing him. Magayon held Panganoron as he died in her arms. \n\nPagtuga's warriors surrounded the lovers as Magayon took the knife from Panganoron's side and shouted Panganoron's name before stabbing herself. Her father and tribesmen witnessed how Magayon died with her lover. \n\nHer father buried them together. As time went on, they noticed something about the place where Makusog buried the lovers: It started to form into a volcano, and when the people saw it, Makusog named it Mt. Mayon, after his daughter's name. Mt. Mayon is as beautiful as Daragang Magayon.\n\nSome people said that it's a curse because she took her own life, but myths and legends said Magayon is the volcano and Panganoron is the clouds that surround the beautiful volcano.\n\n"}
{"id": "25480269", "url": "https://en.wikipedia.org/wiki?curid=25480269", "title": "Dredge-up", "text": "Dredge-up\n\nA dredge-up is a period in the evolution of a star where a surface convection zone extends down to the layers where material has undergone nuclear fusion. As a result, the fusion products are mixed into the outer layers of the stellar atmosphere where they can appear in the spectrum of the star.\n\nThe first dredge-up occurs when a main-sequence star enters the red-giant branch. As a result of the convective mixing, the outer atmosphere will display the spectral signature of hydrogen fusion: the C/C and C/N ratios are lowered, and the surface abundances of lithium and beryllium may be reduced.\n\nThe second dredge-up occurs in stars with 4–8 solar masses. When helium fusion comes to an end at the core, convection mixes the products of the CNO cycle. This second dredge-up results in an increase in the surface abundance of He and N, whereas the amount of C and O decreases.\n\nThe third dredge-up occurs after a star enters the asymptotic giant branch and a flash occurs along a helium-burning shell. This dredge-up causes helium, carbon and the \"s\"-process products to be brought to the surface. The result is an increase in the abundance of carbon relative to oxygen, which can create a carbon star.\n\nThe names of the dredge-ups are set by the evolutionary and structural state of the star in which each occurs, not by the sequence experienced by the star. As a result, lower-mass stars experience the first and third dredge-ups in their evolution but not the second.\n"}
{"id": "16962881", "url": "https://en.wikipedia.org/wiki?curid=16962881", "title": "Emotional selection", "text": "Emotional selection\n\nEmotional Selection is a psychological theory of dreaming that describes dreams as modifiers and tests of mental schemas to better meet waking human needs. It was introduced by Richard Coutts in 2008 and refined by him in 2010. According to emotional selection, during non-REM sleep, the mind processes dreams with content intended to improve the ability of mental schemas. For example, individuals struggling with self-perceptions of incompetence may process dreams in which they successfully navigate complex situations, those who struggle to meet belongingness needs may have dreams of entering a partner relationship, and so forth. Regardless, the theme of the non-REM dream is tentatively accommodated by mental schemas. Because schemas coexist as a network, accommodations can introduce accidental, maladaptive conflicts and therefore are ideally tested prior to full integration. Therefore, during subsequent REM sleep, a second set of dreams is executed in the form of test scenarios. If schema accommodations performed during prior non-REM dreams alleviate anxiety, frustration, sadness, or in other ways appear emotionally adaptive during REM dream tests, they would be selected for retention. Those accommodations that compare negatively to existing, unchanged schemas would be abandoned or further modified and tested.\n\nREM dreams are often described in the academic literature as having bizarre themes. According to Coutts, this description fits neatly in emotional selection. During the development of complex systems, engineers often test with rigorous, outwardly bizarre scenarios, such as intentionally crashing expensive automobiles, dropping functioning electronics onto hard surfaces, or vibrating scale models of buildings on shake tables. Such extreme, costly tests assure that designs meet specifications. The themes of REM dreams are often likewise extreme and bizarre, such as being chase down an alley by a monster, finding oneself naked in the presence of a crowd of people, or realizing that one's teeth or hair are falling out. When such themes are reviewed as tests of the ability of mental schemas to cope with extreme stress, they appear rational.\n\nAccording to emotional selection, pleasant dreams, such as having sex, finding riches, or suddenly being able to fly, are tests that evaluate a person's ability to cope with success. This concept may appear to be an oxymoron. However, the challenges of coping with success are often poignantly depicted by people with self-destructive behaviors, such as substance abuse. Success thrust upon those with a poor self-image can cause such people to reject the notion that they are worthy of good fortune, which is why related schemas must be modified and tested with dreams. From Coutts' 2008 paper, \"People strive for independence, yet find solace in the company of others; they place a high priority on our personal safety, yet quickly jeopardize it to help those in need; they are sexually attracted to many people, yet seek loving, monogamous relationships. Schemas help strike the balances necessary for navigating the complex, often contradictory landscape that comprises life.\"\n\nThe schemas targeted by emotional selection are those essential for meeting human needs, such as those define by Abraham Maslow and Henry Murray. Consequently, emotional selection theory agrees with evolutionary forces by describing a role for dreams as adaptively enhancing mental schemas.\n\nThat dreams facilitate the selection of adaptive mental schemas supports the survival value description of emotional communication in general. The German philosopher Ferdinand Fellmann proposed in 2009 emotional selection as a third form of evolutionary selection besides natural and sexual selection. Loving, monogamous pair-bonding seems to be the favored field where sexual selection is being transformed in emotional selection specific for human courtship and mating.\n\nThe concept of emotional selection fits the recent trend of evolutionary psychology which suggests that individual differences are more than the raw material upon which natural selection operates as a homogenizing force. Instead, personality and individual differences are created by “psychosocial selection” in the more intense forms of pair-bonding in primate sociality. Pair-bonds are based on detecting and supporting emotional complexity in partners with whom we maintain long-term intimate intercourse.\n\n\n"}
{"id": "23631222", "url": "https://en.wikipedia.org/wiki?curid=23631222", "title": "Energy in Italy", "text": "Energy in Italy\n\nItaly consumed about 185 Mtoe of primary energy in 2010. This came mostly from fossil fuels. Among the most used resources are petroleum (mostly used for the transport sector), natural gas (used for electric energy production and heating), coal and renewables.\n\nAn important share of electricity comes from import, mainly from Switzerland and France. The share of primary energy dedicated to electricity production is above 35%, and grew steadily since the 1970s.\n\nElectricity is produced mainly from natural gas, which accounts for the source of more than half of the total final electric energy produced. Another important source is hydroelectric power, which was practically the only source of electricity until 1960. Wind and solar power grew rapidly between 2010 and 2013 thanks to high incentives.\n\nItaly has few energy resources, and most of supplies are imported.\n\nIn 2014 Italy consumed 291.083 TWh (4790 kWh/person) in electricity, consumption in household were 1057 kWh/person.\nItaly is a net importer of electricity: the country imported 46,747.5 GWh and exported 3,031.1 GWh in 2014. Gross production in 2014 was 279.8 TWh. The main power sources are natural gas and hydroelectricity.\n\nItaly has no nuclear power since it was banished in 1987 by referendum.\nIn Tuscany was built the first geothermal power station. In 2014 the geothermal production was 5.92 TWh. All active Italian geothermal power stations are now in Tuscany.\n\nAccording to Energy Information Administration, the 2009 Italian CO emissions from energy consumption were 408 Mt, slightly below Indonesia 413 mt. worldwide, Italy was ranked 17th in 2009 according to this list. The Italian emissions decline of 9% in 2008–2009 was rather influenced by the European economic recession 2008–2009 than large sustainable changes in energy consumption. From 2008 to 2009 change was at a 6.9% decline in Europe and at a 7.5 increase in Asia & Oceania.\n\nEmissions of consumption could be a more significant indicator rather than national greenhouse production: many European companies have moved production from Europe to Asia the last ten years, which does not necessarily change the overall emissions of the world or the company. According to \"the Guardian\", the most widely cited international dataset for consumption emissions is from year 2001 including the consumption emissions per capita of all greenhouse gases. Italy’s footprint in 2001 was 12 ton CO per person (rank no 21) Italy’s domestic share of greenhouse gas emissions was 62%.\n\n"}
{"id": "8148314", "url": "https://en.wikipedia.org/wiki?curid=8148314", "title": "Evansburg State Park", "text": "Evansburg State Park\n\nEvansburg State Park is a Pennsylvania state park in Lower Providence, Lower Salford, Skippack, Towamencin, and Worcester Townships in Montgomery County, Pennsylvania in the United States. The park has a variety of habitats including forests, meadows, old fields, and farmland. The park offers a variety of recreational opportunities including picnicking, golf, ball fields, biking, hiking, hunting, horseback riding, and fishing. Evansburg State Park is near Collegeville and Norristown just off Pennsylvania Route 363.\n\nThe land on which Evansburg State Park is located was originally part of a massive tract of land purchased from the Lenape (Delaware) by William Penn in 1684. The land was quickly settled according to Willam Penn's planned \"Holy Experiment\". The first settlers were Mennonites. They fled religious persecution in Europe for the religious freedom, promised by William Penn in his colony. The Mennonites cleared the land of its old-growth forests and built farms, stores and mills that were powered by the waters of Skippack Creek. The area developed very rapidly and the Skippack Pike was constructed in 1714 to provide access to the markets of Philadelphia.\n\nThe area in and surrounding Evansburg State Park remained largely rural until World War II. The growth of suburbs and industry forever changed the landscape of the Skippack Valley. Evansburg State Park was established in 1979 to protect the rural qualities of the area and to provide outdoor recreational opportunities for the people of southeastern Pennsylvania. The original plan was for the construction of a high dam and lake for recreation, but this was met by local opposition, and the plan was scaled back.\n\nThe Indenhofen Farm is operated by the Skippack Historical Society and open to the public. Kuster Mill is also located in the park.\n\nSkippack Creek is stocked with brown trout and rainbow trout; other fish in the creek include smallmouth bass, catfish, sucker, carp, panfish, and freshwater eel. All fishers are expected to follow the rules and regulations of the Pennsylvania Fish and Boat Commission.\n\nAbout of Evansburg State Park are open to hunting. Hunters are expected to follow the rules and regulations of the Pennsylvania Game Commission. The common game species are squirrels, pheasant, rabbits and white-tailed deer. The hunting of groundhogs is prohibited.\n\nThe golf course at Evansburg State Park is known as Skippack Golf Club. It is an 18-hole , par 70 golf course. It was designed by Herris & Benahia, and opened in 1950. The course is managed by Skippack Golf Club, LLC under contract with the Pennsylvania Department of Conservation and Natural Resources.\n\nEvansburg State Park has of trails that are open to hiking, horseback riding, cross-country skiing, and mountain biking. The of hiking trails are all rated as easy trails and pass through a variety of habitats. A mountain bike trail () is open at the south end of the park. Bikers are prohibited from using the hiking and equestrian trails. They are permitted to ride on the park roads. Fifteen miles of horseback riding trails are open at Evansburg State Park. Riders are also permitted to use the shoulders of the roads of the park.\n\n"}
{"id": "46439815", "url": "https://en.wikipedia.org/wiki?curid=46439815", "title": "Feminist biology", "text": "Feminist biology\n\nFeminist biology is an approach to biology that is concerned with the influence of gender values, the removal of gender bias, and the understanding of the overall role of social values in biological research and practices. Feminist Biology, was founded by, among others, Dr. Ruth Bleier of the University of Wisconsin-Madison (who authored the 1984 work “Science and Gender: A Critique of Biology and Its Theories on Women” and inspired the university’s endowed fellowship for feminist biology), it aims to enhance biology by incorporating feminist critique in matters varying from the mechanisms of cell biology and sex selection to the assessment of the meaning of words such as “gender” and “sex.”\nOverall, the field is broadly defined and pertains itself to philosophies behind both biological and feminist practice. These considerations make feminist biology debatable and conflictive with itself, particularly when concerning matters of biological determinism, whereby descriptive sex terms of male and female are intrinsically confining, or extreme post-modernism, whereby the body is viewed more as a social construct. Despite opinions ranging from determinist to postmodernist, however, biologists, feminists, and feminist biologists of varying labels alike have made claims to the utility of applying feminist ideology to biological practice and procedure.\n\nDonna J. Haraway, a biologist and primatologist hailing from the University of California, put forth male bias criticisms in 1989 concerning the study of human evolution and culture via primatology by denoting a prominent lack of focus in female primates. Haraway contributed to a large discovery of behaviors in primate groups regarding mate selection, and female-female interactions derived from observing female primates, citing feminist influences as she studied the female primates in their own merit.\n\nSimilarly, feminist cell biologists of The Biology and Gender Study Group have criticized androcentrism in the study of behavior between sexes and “come to look at feminist critique as [they] would any experimental control.” They cite a general trend of “active” biological description associated with the sperm gamete and “passive” description to the ovum, comparing such description to an archetypal hero facing many challenges before it finding its static, female home. The group criticized the diction employed by biological readings and textbooks, stating that the more active and risk-associated traits of the ovum (such as its own survival from the whittling of 2 million oocytes) are dismissed for the sake of a patterned narrative.\n\nAnne Fausto-Sterling, a professor of Biology and Gender Studies at Brown University, assesses the complexity of defining sex through a dichotomous lens in a variety of her works such as Sexing the Body: Gender Politics and the Construction of Sexuality as well as in an article piece she wrote titled “The Five Sexes: Why Male and Female Are Not Enough.” She addresses the existence of intersex individuals and the lack of acknowledgment of their state of being in the context of a dichotomously defined world of sexes – even, if not especially, by medical professionals and surgeons who understand intersex anatomy to a point to where they can surgically alter it to one of the sexes. She states: “Ironically, a more sophisticated knowledge of the complexity of sexual systems has led to the repression of such intricacy.” Fausto-Sterling continues by advocating the reevaluation of what is considered urgent medical intervention in light of the influence she believes social stigma has had on standard medical procedure – which in turn could help open up the possible directions that science could take.\n\nThe motivations of advocating feminist biology are diverse. One of the most common motivations is to challenge the gender biases originated from science, by discerning a more objective, scientific truth from culturally influenced practices. Many individuals argue the emergence and development of modern science involved the domination of a female world and the exclusion of women. Reductionism, for instance, is a view that all matters in the universe are arranged hierarchically, and that causation only occurs at the lower levels of this hierarchy. A tight link exists between reductive mechanistic science and biological determinism, contributing to the argument that biological causes are the only causes, or the most important cause, of 'feminine' behavior. This link is due to the reductive assumption that causation acts in an upward direction from lower levels of organization to higher levels of organization. Many feminist biologists focus on dispelling such stigmatic prejudices that influential figures have accepted as scientifically true. \n\nThere is an ongoing debate on whether a feminist critique should be incorporated in the sciences, especially biology. Some argue that feminist biology is a form of politicization of science, calling to question the legitimacy of feminist biology altogether. On another level, there is debate even within the feminist community on how to deal with biological sex differences. Some account on the importance of accepting biological sex differences to reach gender equality, while others contend that there sex differences are overly emphasized in society, contributing to gender stereotypes. Individuals such as Carla Fehr offer constructive criticism for the future of feminist philosophy in the field of biology; she proposes feminist biologists to consider novel questions pertaining to subjects such as the research of genomics in relation to gender. \n"}
{"id": "29342125", "url": "https://en.wikipedia.org/wiki?curid=29342125", "title": "Grasshopper (robot weather station)", "text": "Grasshopper (robot weather station)\n\nThe Grasshopper was a project by the United States Air Force and US Navy to develop portable robot weather station deployed by parachute from long range aircraft in the early 1950s. The Grasshopper was designed to be deployed by parachute into enemy territory and radio back basic weather information for air strikes.\n\nWith the USAF \"Grasshopper\" after being parachuted down from a long range aircraft, a small explosive charge disconnects the parachute upon impact with the ground. After a selected preset time, a second explosive charge would deploy the legs of the unit while setting it upright on the ground. Finally, a third explosive charge would extend the antenna and make the unit ready to begin taking weather measurements, broadcasting them back three times a day at selected timed-intervals (so other aircraft can pick up the short range signal). Reports also stated that the Grasshopper could be used to guide in strike aircraft with the internal clock set for the approximate time the strike aircraft would arrive near the target.\n\n"}
{"id": "2600253", "url": "https://en.wikipedia.org/wiki?curid=2600253", "title": "Green ubatuba", "text": "Green ubatuba\n\nGreen ubatuba (Port. verde ubatuba) is the commercial name of a Brazilian charnockite, although it is often sold as a granite. It comes from the Ubatuba area of Brazil, where it forms part of the Neoproterozoic Ribeira Belt. As its name indicates, it is green in color, dark green and almost a black appearance when seen in low light. It contains large (10 cm) phenocrysts of alkali feldspar. It is widely used in home renovations and landscaping.\n"}
{"id": "184726", "url": "https://en.wikipedia.org/wiki?curid=184726", "title": "Heat transfer", "text": "Heat transfer\n\nHeat transfer is a discipline of thermal engineering that concerns the generation, use, conversion, and exchange of thermal energy (heat) between physical systems. Heat transfer is classified into various mechanisms, such as thermal conduction, thermal convection, thermal radiation, and transfer of energy by phase changes. Engineers also consider the transfer of mass of differing chemical species, either cold or hot, to achieve heat transfer. While these mechanisms have distinct characteristics, they often occur simultaneously in the same system.\n\nHeat conduction, also called diffusion, is the direct microscopic exchange of kinetic energy of particles through the boundary between two systems. When an object is at a different temperature from another body or its surroundings, heat flows so that the body and the surroundings reach the same temperature, at which point they are in thermal equilibrium. Such spontaneous heat transfer always occurs from a region of high temperature to another region of lower temperature, as described in the second law of thermodynamics.\n\nHeat convection occurs when bulk flow of a fluid (gas or liquid) carries heat along with the flow of matter in the fluid. The flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called \"natural convection\". All convective processes also move heat partly by diffusion, as well. Another form of convection is forced convection. In this case the fluid is forced to flow by use of a pump, fan or other mechanical means.\n\nThermal radiation occurs through a vacuum or any transparent medium (solid or fluid or gas). It is the transfer of energy by means of photons in electromagnetic waves governed by the same laws. K is thermal conductivity. T1-T2 is the temperature difference between the two ends.A is the area of cross section.If the area of cross section is not uniform the equation can only be applied to thin layer\n\nHeat is defined in physics as the transfer of thermal energy across a well-defined boundary around a thermodynamic system. The thermodynamic free energy is the amount of work that a thermodynamic system can perform. Enthalpy is a thermodynamic potential, designated by the letter \"H\", that is the sum of the internal energy of the system (U) plus the product of pressure (P) and volume (V). Joule is a unit to quantify energy, work, or the amount of heat.\n\nHeat transfer is a process function (or path function), as opposed to functions of state; therefore, the amount of heat transferred in a thermodynamic process that changes the state of a system depends on how that process occurs, not only the net difference between the initial and final states of the process.\n\nThermodynamic and mechanical heat transfer is calculated with the heat transfer coefficient, the proportionality between the heat flux and the thermodynamic driving force for the flow of heat. Heat flux is a quantitative, vectorial representation of heat-flow through a surface.\n\nIn engineering contexts, the term \"heat\" is taken as synonymous to thermal energy. This usage has its origin in the historical interpretation of heat as a fluid (\"Caloric\") that can be transferred by various causes, and that is also common in the language of laymen and everyday life.\n\nThe transport equations for thermal energy (Fourier's law), mechanical momentum (Newton's law for fluids), and mass transfer (Fick's laws of diffusion) are similar, and analogies among these three transport processes have been developed to facilitate prediction of conversion from any one to the others.\n\nThermal engineering concerns the generation, use, conversion, and exchange of heat transfer. As such, heat transfer is involved in almost every sector of the economy. Heat transfer is classified into various mechanisms, such as thermal conduction, thermal convection, thermal radiation, and transfer of energy by phase changes.\n\nThe fundamental modes of heat transfer are:\n\n\nBy transferring matter, energy—including thermal energy—is moved by the physical transfer of a hot or cold object from one place to another. This can be as simple as placing hot water in a bottle and heating a bed, or the movement of an iceberg in changing ocean currents. A practical example is thermal hydraulics. This can be described by the formula:\n\nwhere \n\nOn a microscopic scale, heat conduction occurs as hot, rapidly moving or vibrating atoms and molecules interact with neighboring atoms and molecules, transferring some of their energy (heat) to these neighboring particles. In other words, heat is transferred by conduction when adjacent atoms vibrate against one another, or as electrons move from one atom to another. Conduction is the most significant means of heat transfer within a solid or between solid objects in thermal contact. Fluids—especially gases—are less conductive. Thermal contact conductance is the study of heat conduction between solid bodies in contact. The process of heat transfer from one place to another place without the movement of particles is called conduction. Example: Heat transfer through Metal rods. \"Steady state conduction\" (see Fourier's law) is a form of conduction that happens when the temperature difference driving the conduction is constant, so that after an equilibration time, the spatial distribution of temperatures in the conducting object does not change any further. In steady state conduction, the amount of heat entering a section is equal to amount of heat coming out.\n\n\"Transient conduction\" (see Heat equation) occurs when the temperature within an object changes as a function of time. Analysis of transient systems is more complex and often calls for the application of approximation theories or numerical analysis by computer.\n\nThe flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called \"natural convection\". All convective processes also move heat partly by diffusion, as well. Another form of convection is forced convection. In this case the fluid is forced to flow by using a pump, fan or other mechanical means.\n\nConvective heat transfer, or convection, is the transfer of heat from one place to another by the movement of fluids, a process that is essentially the transfer of heat via mass transfer. Bulk motion of fluid enhances heat transfer in many physical situations, such as (for example) between a solid surface and the fluid. Convection is usually the dominant form of heat transfer in liquids and gases. Although sometimes discussed as a third method of heat transfer, convection is usually used to describe the combined effects of heat conduction within the fluid (diffusion) and heat transference by bulk fluid flow streaming. The process of transport by fluid streaming is known as advection, but pure advection is a term that is generally associated only with mass transport in fluids, such as advection of pebbles in a river. In the case of heat transfer in fluids, where transport by advection in a fluid is always also accompanied by transport via heat diffusion (also known as heat conduction) the process of heat convection is understood to refer to the sum of heat transport by advection and diffusion/conduction.\n\nFree, or natural, convection occurs when bulk fluid motions (streams and currents) are caused by buoyancy forces that result from density variations due to variations of temperature in the fluid. \"Forced\" convection is a term used when the streams and currents in the fluid are induced by external means—such as fans, stirrers, and pumps—creating an artificially induced convection current.\n\nConvective cooling is sometimes described as Newton's law of cooling: \n\nIn a body of fluid that is heated from underneath its container, conduction and convection can be considered to compete for dominance. If heat conduction is too great, fluid moving down by convection is heated by conduction so fast that its downward movement will be stopped due to its buoyancy, while fluid moving up by convection is cooled by conduction so fast that its driving buoyancy will diminish. On the other hand, if heat conduction is very low, a large temperature gradient may be formed and convection might be very strong.\n\nThe Rayleigh number (formula_4) is the product of the Grashof (formula_5) and Prandtl (formula_6) numbers. It is a measure which determines the relative strength of conduction and convection.\n\nwhere\n\nThe Rayleigh number can be understood as the ratio between the rate of heat transfer by convection to the rate of heat transfer by conduction; or, equivalently, the ratio between the corresponding timescales (i.e. conduction timescale divided by convection timescale), up to a numerical factor. This can be seen as follows, where all calculations are up to numerical factors depending on the geometry of the system.\n\nThe buoyancy force driving the convection is roughly formula_9, so the corresponding pressure is roughly formula_10. In steady state, this is canceled by the shear stress due to viscosity, and therefore roughly equals formula_11, where \"V\" is the typical fluid velocity due to convection and formula_12 the order of its timescale. The conduction timescale, on the other hand, is of the order of formula_13.\n\nConvection occurs when the Rayleigh number is above 1,000–2,000.\n\nThermal radiation occurs through a vacuum or any transparent medium (solid or fluid or gas). It is the transfer of energy by means of photons in electromagnetic waves governed by the same laws.\n\nThermal radiation is energy emitted by matter as electromagnetic waves, due to the pool of thermal energy in all matter with a temperature above absolute zero. Thermal radiation propagates without the presence of matter through the vacuum of space.\n\nThermal radiation is a direct result of the random movements of atoms and molecules in matter. Since these atoms and molecules are composed of charged particles (protons and electrons), their movement results in the emission of electromagnetic radiation, which carries energy away from the surface.\n\nThe Stefan-Boltzmann equation, which describes the rate of transfer of radiant energy, is as follows for an object in a vacuum :\n\nFor radiative transfer between two objects, the equation is as follows:\n\nwhere \nRadiation is typically only important for very hot objects, or for objects with a large temperature difference.\n\nRadiation from the sun, or solar radiation, can be harvested for heat and power. Unlike conductive and convective forms of heat transfer, thermal radiation can be concentrated in a small spot by using reflecting mirrors, which is exploited in concentrating solar power generation. For example, the sunlight reflected from mirrors heats the PS10 solar power tower and during the day it can heat water to . \n\nPhase transition or phase change, takes place in a thermodynamic system from one phase or state of matter to another one by heat transfer. Phase change examples are the melting of ice or the boiling of water.\nThe Mason equation explains the growth of a water droplet based on the effects of heat transport on evaporation and condensation.\n\nPhase transitions involve the four fundamental states of matter:\n\nThe boiling point of a substance is the temperature at which the vapor pressure of the liquid equals the pressure surrounding the liquid and the liquid evaporates resulting in an abrupt change in vapor volume.\n\nSaturation temperature means boiling point. The saturation temperature is the temperature for a corresponding saturation pressure at which a liquid boils into its vapor phase. The liquid can be said to be saturated with thermal energy. Any addition of thermal energy results in a phase transition.\n\nAt standard atmospheric pressure and low temperatures, no boiling occurs and the heat transfer rate is controlled by the usual single-phase mechanisms. As the surface temperature is increased, local boiling occurs and vapor bubbles nucleate, grow into the surrounding cooler fluid, and collapse. This is \"sub-cooled nucleate boiling\", and is a very efficient heat transfer mechanism. At high bubble generation rates, the bubbles begin to interfere and the heat flux no longer increases rapidly with surface temperature (this is the departure from nucleate boiling, or DNB).\n\nAt similar standard atmospheric pressure and high temperatures, the hydrodynamically-quieter regime of film boiling is reached. Heat fluxes across the stable vapor layers are low, but rise slowly with temperature. Any contact between fluid and the surface that may be seen probably leads to the extremely rapid nucleation of a fresh vapor layer (\"spontaneous nucleation\"). At higher temperatures still, a maximum in the heat flux is reached (the critical heat flux, or CHF).\n\nThe Leidenfrost Effect demonstrates how nucleate boiling slows heat transfer due to gas bubbles on the heater's surface. As mentioned, gas-phase thermal conductivity is much lower than liquid-phase thermal conductivity, so the outcome is a kind of \"gas thermal barrier\".\n\nCondensation occurs when a vapor is cooled and changes its phase to a liquid. During condensation, the latent heat of vaporization must be released. The amount of the heat is the same as that absorbed during vaporization at the same fluid pressure.\n\nThere are several types of condensation:\n\nMelting is a thermal process that results in the phase transition of a substance from a solid to a liquid. The internal energy of a substance is increased, typically with heat or pressure, resulting in a rise of its temperature to the melting point, at which the ordering of ionic or molecular entities in the solid breaks down to a less ordered state and the solid liquefies. Molten substances generally have reduced viscosity with elevated temperature; an exception to this maxim is the element sulfur, whose viscosity increases to a point due to polymerization and then decreases with higher temperatures in its molten state.\n\nHeat transfer can be modeled in the following ways.\n\nClimate models study the radiant heat transfer by using quantitative methods to simulate the interactions of the atmosphere, oceans, land surface, and ice.\n\nThe heat equation is an important partial differential equation that describes the distribution of heat (or variation in temperature) in a given region over time. In some cases, exact solutions of the equation are available; in other cases the equation must be solved numerically using computational methods.\n\nLumped system analysis often reduces the complexity of the equations to one first-order linear differential equation, in which case heating and cooling are described by a simple exponential solution, often referred to as Newton's law of cooling.\n\nSystem analysis by the lumped capacitance model is a common approximation in transient conduction that may be used whenever heat conduction within an object is much faster than heat conduction across the boundary of the object. This is a method of approximation that reduces one aspect of the transient conduction system—that within the object—to an equivalent steady state system. That is, the method assumes that the temperature within the object is completely uniform, although its value may be changing in time.\n\nIn this method, the ratio of the conductive heat resistance within the object to the convective heat transfer resistance across the object's boundary, known as the \"Biot number\", is calculated. For small Biot numbers, the approximation of \"spatially uniform temperature within the object\" can be used: it can be presumed that heat transferred into the object has time to uniformly distribute itself, due to the lower resistance to doing so, as compared with the resistance to heat entering the object.\n\nHeat transfer has broad application to the functioning of numerous devices and systems. Heat-transfer principles may be used to preserve, increase, or decrease temperature in a wide variety of circumstances. Heat transfer methods are used in numerous disciplines, such as automotive engineering, thermal management of electronic devices and systems, climate control, insulation, materials processing, and power station engineering.\n\nThermal insulators are materials specifically designed to reduce the flow of heat by limiting conduction, convection, or both. Thermal resistance is a heat property and the measurement by which an object or material resists to heat flow (heat per time unit or thermal resistance) to temperature difference.\n\nRadiance or spectral radiance are measures of the quantity of radiation that passes through or is emitted. Radiant barriers are materials that reflect radiation, and therefore reduce the flow of heat from radiation sources. Good insulators are not necessarily good radiant barriers, and vice versa. Metal, for instance, is an excellent reflector and a poor insulator.\n\nThe effectiveness of a radiant barrier is indicated by its reflectivity, which is the fraction of radiation reflected. A material with a high reflectivity (at a given wavelength) has a low emissivity (at that same wavelength), and vice versa. At any specific wavelength, reflectivity=1 - emissivity. An ideal radiant barrier would have a reflectivity of 1, and would therefore reflect 100 percent of incoming radiation. Vacuum flasks, or Dewars, are silvered to approach this ideal. In the vacuum of space, satellites use multi-layer insulation, which consists of many layers of aluminized (shiny) Mylar to greatly reduce radiation heat transfer and control satellite temperature.\n\nA heat engine is a system that performs the conversion of a flow of thermal energy (heat) to mechanical energy to perform mechanical work. \n\nA thermocouple is a temperature-measuring device and widely used type of temperature sensor for measurement and control, and can also be used to convert heat into electric power.\n\nA thermoelectric cooler is a solid state electronic device that pumps (transfers) heat from one side of the device to the other when electric current is passed through it. It is based on the Peltier effect.\n\nA thermal diode or thermal rectifier is a device that causes heat to flow preferentially in one direction.\n\nA heat exchanger is used for more efficient heat transfer or to dissipate heat. Heat exchangers are widely used in refrigeration, air conditioning, space heating, power generation, and chemical processing. One common example of a heat exchanger is a car's radiator, in which the hot coolant fluid is cooled by the flow of air over the radiator's surface.\n\nCommon types of heat exchanger flows include parallel flow, counter flow, and cross flow. In parallel flow, both fluids move in the same direction while transferring heat; in counter flow, the fluids move in opposite directions; and in cross flow, the fluids move at right angles to each other. Common constructions for heat exchanger include shell and tube, double pipe, extruded finned pipe, spiral fin pipe, u-tube, and stacked plate.\n\nA heat sink is a component that transfers heat generated within a solid material to a fluid medium, such as air or a liquid. Examples of heat sinks are the heat exchangers used in refrigeration and air conditioning systems or the radiator in a car. A heat pipe is another heat-transfer device that combines thermal conductivity and phase transition to efficiently transfer heat between two solid interfaces.\n\nEfficient energy use is the goal to reduce the amount of energy required in heating or cooling. In architecture, condensation and air currents can cause cosmetic or structural damage. An energy audit can help to assess the implementation of recommended corrective procedures. For instance, insulation improvements, air sealing of structural leaks or the addition of energy-efficient windows and doors.\n\n\nClimate engineering consists of carbon dioxide removal and solar radiation management. Since the amount of carbon dioxide determines the radiative balance of Earth atmosphere, carbon dioxide removal techniques can be applied to reduce the radiative forcing. Solar radiation management is the attempt to absorb less solar radiation to offset the effects of greenhouse gases.\n\nThe greenhouse effect is a process by which thermal radiation from a planetary surface is absorbed by atmospheric greenhouse gases, and is re-radiated in all directions. Since part of this re-radiation is back towards the surface and the lower atmosphere, it results in an elevation of the average surface temperature above what it would be in the absence of the gases.\n\nThe principles of heat transfer in engineering systems can be applied to the human body in order to determine how the body transfers heat. Heat is produced in the body by the continuous metabolism of nutrients which provides energy for the systems of the body. The human body must maintain a consistent internal temperature in order to maintain healthy bodily functions. Therefore, excess heat must be dissipated from the body to keep it from overheating. When a person engages in elevated levels of physical activity, the body requires additional fuel which increases the metabolic rate and the rate of heat production. The body must then use additional methods to remove the additional heat produced in order to keep the internal temperature at a healthy level.\n\nHeat transfer by convection is driven by the movement of fluids over the surface of the body. This convective fluid can be either a liquid or a gas. For heat transfer from the outer surface of the body, the convection mechanism is dependent on the surface area of the body, the velocity of the air, and the temperature gradient between the surface of the skin and the ambient air. The normal temperature of the body is approximately 37 °C. Heat transfer occurs more readily when the temperature of the surroundings is significantly less than the normal body temperature. This concept explains why a person feels “cold” when not enough covering is worn when exposed to a cold environment. Clothing can be considered an insulator which provides thermal resistance to heat flow over the covered portion of the body. This thermal resistance causes the temperature on the surface of the clothing to be less than the temperature on the surface of the skin. This smaller temperature gradient between the surface temperature and the ambient temperature will cause a lower rate of heat transfer than if the skin were not covered.\n\nIn order to ensure that one portion of the body is not significantly hotter than another portion, heat must be distributed evenly through the bodily tissues. Blood flowing through blood vessels acts as a convective fluid and helps to prevent any buildup of excess heat inside the tissues of the body. This flow of blood through the vessels can be modeled as pipe flow in an engineering system. The heat carried by the blood is determined by the temperature of the surrounding tissue, the diameter of the blood vessel, the thickness of the fluid, velocity of the flow, and the heat transfer coefficient of the blood. The velocity, blood vessel diameter, and the fluid thickness can all be related with the Reynolds Number, a dimensionless number used in fluid mechanics to characterize the flow of fluids.\n\nLatent heat loss, also known as evaporative heat loss, accounts for a large fraction of heat loss from the body. When the core temperature of the body increases, the body triggers sweat glands in the skin to bring additional moisture to the surface of the skin. The liquid is then transformed into vapor which removes heat from the surface of the body. The rate of evaporation heat loss is directly related to the vapor pressure at the skin surface and the amount of moisture present on the skin. Therefore, the maximum of heat transfer will occur when the skin is completely wet. The body continuously loses water by evaporation but the most significant amount of heat loss occurs during periods of increased physical activity.\n\nEvaporative cooling happens when water vapor is added to the surrounding air. The energy needed to evaporate the water is taken from the air in the form of sensible heat and converted into latent heat, while the air remains at a constant enthalpy. Latent heat describes the amount of heat that is needed to evaporate the liquid; this heat comes from the liquid itself and the surrounding gas and surfaces. The greater the difference between the two temperatures, the greater the evaporative cooling effect. When the temperatures are the same, no net evaporation of water in air occurs; thus, there is no cooling effect.\n\nIn quantum physics, laser cooling is used to achieve temperatures of near absolute zero (−273.15 °C, −459.67 °F) of atomic and molecular samples to observe unique quantum effects that can only occur at this heat level.\n\n\nMagnetic evaporative cooling is a process for lowering the temperature of a group of atoms, after pre-cooled by methods such as laser cooling. Magnetic refrigeration cools below 0.3K, by making use of the magnetocaloric effect.\n\nRadiative cooling is the process by which a body loses heat by radiation. Outgoing energy is an important effect in the Earth's energy budget. In the case of the Earth-atmosphere system, it refers to the process by which long-wave (infrared) radiation is emitted to balance the absorption of short-wave (visible) energy from the Sun. Convective transport of heat and evaporative transport of latent heat both remove heat from the surface and redistribute it in the atmosphere.\n\nThermal energy storage includes technologies for collecting and storing energy for later use. It may be employed to balance energy demand between day and nighttime. The thermal reservoir may be maintained at a temperature above or below that of the ambient environment. Applications include space heating, domestic or process hot water systems, or generating electricity.\n\n\n"}
{"id": "1120640", "url": "https://en.wikipedia.org/wiki?curid=1120640", "title": "Horatio Hale", "text": "Horatio Hale\n\nHoratio Emmons Hale (May 3, 1817 – December 28, 1896) was an American-Canadian ethnologist, philologist and businessman who studied language as a key for classifying ancient peoples and being able to trace their migrations. He was the first to discover that the Tutelo language of Virginia belonged to the Siouan family, and to identify the Cherokee language as a member of the Iroquoian family of languages. In addition, he published a work \"Iroquois Book of Rites\" (1883), based on interpreting the Iroquois wampum belts, as well as his studies with tribal leaders.\n\nAfter his marriage to a Canadian woman in 1855, Hale moved to Ontario. He continued to publish articles in American scholarly journals, while living in Canada for the rest of his life.\n\nHoratio was born on 3 May 1817, at Newport, New Hampshire, in the United States, was the son of David Hale, a lawyer, and of Sarah Josepha, who, after the death of her husband, turned to writing and became a prominent magazine editor. \nEntering Harvard College in 1833, Hale showed a marked faculty for languages. \nHis first essay in original work appeared the next year, and attracted the attention of the college authorities. \nIt consisted of an Algonkin vocabulary, which he gathered from a band of Indians who had camped on the college grounds. \nThree years later, when the United States Exploring Expedition to little-known portions of the globe was organised under Charles Wilkes, Hale was recommended, while yet an undergraduate, for the post of ethnologist and philologist, and obtained the appointment. \nFrom 1838 to 1842, he was employed in the work of the expedition, visiting South America, Australasia, Polynesia, and North-western America, then known as Oregon. \nFrom this point he returned overland. \nThe Hale Passages of Puget Sound were named in recognition of his service to the expedition. The expedition went on to Polynesia. Of the reports of that expedition, Hale prepared the sixth volume, \"Ethnography and Philology\" (1846), which is said to have laid the foundations of the ethnography of Polynesia. He continued to travel and study abroad.\nHaving taken his degree of M. A., Hale made a short tour of Europe, and, on his return, studied law. \nHe was admitted to the Chicago bar in 1855. \nIn 1856, the Hales moved to Clinton, Ontario, Canada, where he administered the estate of his father-in-law. He began to involve himself locally in real estate development and other business and educational endeavours.\nHe continued to reside in Clinton till his death, devoting much attention to the development of the Ontario school system. \nHe was influential in introducing co-education of the sexes in high schools and collegiate institutes, in increasing the grants to these institutions, in establishing the normal school system, and in improving the methods of examination.\nThe vicinity of the Canadian reserves on the banks of the Thames and Grand River gave Hale ample opportunity for further investigation into American-Indian questions. \nHe discovered, and in 1883 published, under the title, \"The Iroquois Book of Rites,\" two Indian manuscripts, dating between 1714 and 1735, which is the only literary American-Indian work extant. \nHis judicious introductions, careful translation and editing add much to the value of the work.\nIn 1884, at its Montreal meeting, he reorganised the section of anthropology as an independent department of the British Association for the Advancement of Science. \nHe had already done a like service for the American Association. At the request of the British committee, he undertook the supervision of the anthropological section's work in the Canadian North-west and British Columbia. \nThe reports, which are very elaborate, appeared in the published 'Proceedings' from 1885 to 1897. \nContinuing a member of the committee, he was asked to accept the position of vice-president at the association's meeting in Toronto (1896), but declined on the ground of ill-health.\nAmong other learned bodies Hale was an honorary fellow of the Anthropological Institute of Great Britain, to which he contributed his latest papers.\n\nHe died on 29 December 1896 at Clinton, Ontario.\n\nIn 1854, at Jersey city in the state of New Jersey, he married Margaret, daughter of William Pugh, formerly justice of the peace for the township of Goderich in the county of Huron, Canada West.\n\nHale returned to his study of First Nations and Native Americans. \nHe was mentored by the Iroquois chiefs George Henry Martin Johnson and John Fraser, whom he met while visiting the Six Nations of the Grand River First Nation. \nIn addition he traveled to the United States to consult with other native informants. Hale documented the oral history and rituals of the Iroquois Confederacy. \nHe was assisted in interpreting the group's wampum belts, which recounted their history. His work resulted in his publishing \"Iroquois Book of Rites\" (1883). He also studied the Iroquois languages, determining that Mohawk was the oldest and that the Laurentian languages were also Iroquoian.\n\nHale made many valuable contributions to the science of ethnology, attracting attention particularly by his theory of the origin of the diversities of human languages and dialects—a theory suggested by his study of child-languages, or the languages invented by little children. He also emphasized the importance of languages as tests of mental capacity, demonstrating that Native American languages were complex and had a high capacity for classification.\n\nHe used language as a criterion for the classification of human groups. He was the first to discover that the Tutelo language of Virginia belonged to the Siouan family, as well as the first to identify the Cherokee language as a member of the Iroquoian family of languages.\n\nBesides writing numerous magazine articles, Hale read a number of valuable papers before learned societies. These include:\n\n\n"}
{"id": "11724245", "url": "https://en.wikipedia.org/wiki?curid=11724245", "title": "Hydrogen-like atom", "text": "Hydrogen-like atom\n\nA hydrogen-like ion is any atomic nucleus which has one electron and thus is isoelectronic with hydrogen. These ions carry the positive charge formula_1, where formula_2 is the atomic number of the atom. Examples of hydrogen-like ions are He, Li, Be and B. Because hydrogen-like ions are two-particle systems with an interaction depending only on the distance between the two particles, their (non-relativistic) Schrödinger equation can be solved in analytic form, as can the (relativistic) Dirac equation. The solutions are one-electron functions and are referred to as \"hydrogen-like atomic orbitals\".\n\nOther systems may also be referred to as \"hydrogen-like atoms\", such as muonium (an electron orbiting a muon), positronium (an electron and a positron), certain exotic atoms (formed with other particles), or Rydberg atoms (in which one electron is in such a high energy state that it sees the rest of the atom practically as a point charge).\n\nIn the solution to the Schrödinger equation, which is non-relativistic, hydrogen-like atomic orbitals are eigenfunctions of the one-electron angular momentum operator L and its \"z\" component \"L\". A hydrogen-like atomic orbital is uniquely identified by the values of the principal quantum number \"n\", the angular momentum quantum number \"l\", and the magnetic quantum number \"m\". The energy eigenvalues do not depend on \"l\" or \"m\", but solely on \"n\". To these must be added the two-valued spin quantum number \"m\" = ±½, setting the stage for the Aufbau principle. This principle restricts the allowed values of the four quantum numbers in electron configurations of more-electron atoms. In hydrogen-like atoms all degenerate orbitals of fixed \"n\" and \"l\", \"m\" and \"s\" varying between certain values (see below) form an atomic shell.\n\nThe Schrödinger equation of atoms or atomic ions with more than one electron has not been solved analytically, because of the computational difficulty imposed by the Coulomb interaction between the electrons. Numerical methods must be applied in order to obtain (approximate) wavefunctions or other properties from quantum mechanical calculations. Due to the spherical symmetry (of the Hamiltonian), the total angular momentum J of an atom is a conserved quantity. Many numerical procedures start from products of atomic orbitals that are eigenfunctions of the one-electron operators L and \"L\". The radial parts of these atomic orbitals are sometimes numerical tables or are sometimes Slater orbitals. By angular momentum coupling many-electron eigenfunctions of J (and possibly S) are constructed.\n\nIn quantum chemical calculations hydrogen-like atomic orbitals cannot serve as an expansion basis, because they are not complete. The non-square-integrable continuum (E > 0) states must be included to obtain a complete set, i.e., to span all of one-electron Hilbert space.\n\nIn the simplest model, the atomic orbitals of hydrogen-like ions are solutions to the Schrödinger equation in a spherically symmetric potential. In this case, the potential term is the potential given by Coulomb's law:\n\nwhere\n\nAfter writing the wave function as a product of functions:\n(in spherical coordinates), where formula_5 are spherical harmonics, we arrive at the following Schrödinger equation:\nwhere formula_7 is, approximately, the mass of the electron (more accurately, it is the reduced mass of the system consisting of the electron and the nucleus), and formula_8 is the reduced Planck constant.\n\nDifferent values of \"l\" give solutions with different angular momentum, where \"l\" (a non-negative integer) is the quantum number of the orbital angular momentum. The magnetic quantum number \"m\" (satisfying formula_9) is the (quantized) projection of the orbital angular momentum on the \"z\"-axis. See here for the steps leading to the solution of this equation.\n\nIn addition to \"l\" and \"m\", a third integer \"n\" > 0, emerges from the boundary conditions placed on \"R\". The functions \"R\" and \"Y\" that solve the equations above depend on the values of these integers, called \"quantum numbers\". It is customary to subscript the wave functions with the values of the quantum numbers they depend on. The final expression for the normalized wave function is:\n\nwhere:\nparity due to angular wave function is formula_21.\n\nThe quantum numbers \"n\", \"l\" and \"m\" are integers and can have the following values:\n\nFor a group-theoretical interpretation of these quantum numbers, see this article. Among other things, this article gives group-theoretical reasons why formula_25 and formula_26.\n\nEach atomic orbital is associated with an angular momentum L. It is a vector operator, and the eigenvalues of its square \"L\" ≡ L + L + L are given by:\n\nThe projection of this vector onto an arbitrary direction is quantized. If the arbitrary direction is called \"z\", the quantization is given by:\n\nwhere \"m\" is restricted as described above. Note that \"L\" and \"L\" commute and have a common eigenstate, which is in accordance with Heisenberg's uncertainty principle. Since \"L\" and \"L\" do not commute with \"L\", it is not possible to find a state that is an eigenstate of all three components simultaneously. Hence the values of the \"x\" and \"y\" components are not sharp, but are given by a probability function of finite width. The fact that the \"x\" and \"y\" components are not well-determined, implies that the direction of the angular momentum vector is not well determined either, although its component along the \"z\"-axis is sharp.\n\nThese relations do not give the total angular momentum of the electron. For that, electron spin must be included.\n\nThis quantization of angular momentum closely parallels that proposed by Niels Bohr (see Bohr model) in 1913, with no knowledge of wavefunctions.\n\nIn a real atom, the spin of a moving electron can interact with the electric field of the nucleus through relativistic effects, a phenomenon known as spin-orbit interaction. When one takes this coupling into account, the spin and the orbital angular momentum are no longer conserved, which can be pictured by the electron precessing. Therefore, one has to replace the quantum numbers \"l\", \"m\" and the projection of the spin \"m\" by quantum numbers that represent the total angular momentum (including spin), \"j\" and \"m\", as well as the quantum number of parity.\n\nSee the next section on the Dirac equation for a solution that includes the coupling.\n\nIn 1928 in England Paul Dirac found an equation that was fully compatible with Special Relativity. The equation was solved for hydrogen-like atoms the same year (assuming a simple Coulomb potential around a point charge) by the German Walter Gordon. Instead of a single (possibly complex) function as in the Schrödinger equation, one must find four complex functions that make up a bispinor. The first and second functions (or components of the spinor) correspond (in the usual basis) to spin \"up\" and spin \"down\" states, as do the third and fourth components.\n\nThe terms \"spin up\" and \"spin down\" are relative to a chosen direction, conventionally the z direction. An electron may be in a superposition of spin up and spin down, which corresponds to the spin axis pointing in some other direction. The spin state may depend on location.\n\nAn electron in the vicinity of a nucleus necessarily has non-zero amplitudes for the third and fourth components. Far from the nucleus these may be small, but near the nucleus they become large.\n\nThe eigenfunctions of the Hamiltonian, which means functions with a definite energy (and which therefore do not evolve except for a phase shift), have energies characterized not by the quantum number \"n\" only (as for the Schrödinger equation), but by \"n\" and a quantum number \"j\", the total angular momentum quantum number. The quantum number \"j\" determines the sum of the squares of the three angular momenta to be \"j\"(\"j\"+1) (times \"ħ\", see Planck constant). These angular momenta include both orbital angular momentum (having to do with the angular dependence of ψ) and spin angular momentum (having to do with the spin state). The splitting of the energies of states of the same principal quantum number \"n\" due to differences in \"j\" is called fine structure. The total angular momentum quantum number \"j\" ranges from 1/2 to \"n\"−1/2.\n\nThe orbitals for a given state can be written using two radial functions and two angle functions. The radial functions depend on both the principal quantum number \"n\" and an integer \"k\", defined as:\n\nwhere ℓ is the azimuthal quantum number that ranges from 0 to \"n\"−1. The angle functions depend on \"k\" and on a quantum number \"m\" which ranges from −\"j\" to \"j\" by steps of 1. The states are labeled using the letters S, P, D, F et cetera to stand for states with ℓ equal to 0, 1, 2, 3 et cetera (see azimuthal quantum number), with a subscript giving \"j\". For instance, the states for \"n\"=4 are given in the following table (these would be prefaced by \"n\", for example 4S):\n\nThese can be additionally labeled with a subscript giving \"m\". There are 2\"n\" states with principal quantum number \"n\", 4\"j\"+2 of them with any allowed \"j\" except the highest (\"j\"=\"n\"−1/2) for which there are only 2\"j\"+1. Since the orbitals having given values of \"n\" and \"j\" have the same energy according to the Dirac equation, they form a basis for the space of functions having that energy.\n\nThe energy, as a function of \"n\" and |\"k\"| (equal to \"j\"+1/2), is:\n\n(The energy of course depends on the zero-point used.) Note that if were able to be more than 137 (higher than any known element) then we would have a negative value inside the square root for the S and P orbitals, which means they would not exist. The Schrödinger solution corresponds to replacing the inner bracket in the second expression by 1. The accuracy of the energy difference between the lowest two hydrogen states calculated from the Schrödinger solution is about 9 ppm (90 μeV too low, out of around 10 eV), whereas the accuracy of the Dirac equation for the same energy difference is about 3 ppm (too high). The Schrödinger solution always puts the states at slightly higher energies than the more accurate Dirac equation. The Dirac equation gives some levels of hydrogen quite accurately (for instance the 4P state is given an energy only about eV too high), others less so (for instance, the 2S level is about eV too low). The modifications of the energy due to using the Dirac equation rather than the Schrödinger solution is of the order of α, and for this reason α is called the fine structure constant.\n\nThe solution to the Dirac equation for quantum numbers \"n\", \"k\", and \"m\", is:\n\nformula_31\n\nwhere the Ωs are columns of the two spherical harmonics functions shown to the right. formula_32 signifies a spherical harmonic function:\n\nin which formula_34 is an associated Legendre polynomial. (Note that the definition of Ω may involve a spherical harmonic that doesn't exist, like formula_35, but the coefficient on it will be zero.)\n\nHere is the behavior of some of these angular functions. The normalization factor is left out to simplify the expressions.\n\nFrom these we see that in the S orbital (\"k\" = −1), the top two components of Ψ have zero orbital angular momentum like Schrödinger S orbitals, but the bottom two components are orbitals like the Schrödinger P orbitals. In the P solution (\"k\" = 1), the situation is reversed. In both cases, the spin of each component compensates for its orbital angular momentum around the \"z\" axis to give the right value for the total angular momentum around the \"z\" axis.\n\nThe two Ω spinors obey the relationship:\n\nTo write the functions formula_41 and formula_42 let us define a scaled radius ρ:\n\nwith\n\nwhere E is the energy (formula_45) given above. We also define γ as:\n\nWhen \"k\" = −\"n\" (which corresponds to the highest \"j\" possible for a given \"n\", such as 1S, 2P, 3D...), then formula_41 and formula_42 are:\n\nwhere \"A\" is a normalization constant involving the Gamma function:\n\nNotice that because of the factor Zα, \"f\"(\"r)\" is small compared to \"g\"(\"r\"). Also notice that in this case, the energy is given by\n\nand the radial decay constant \"C\" by\n\nIn the general case (when \"k\" is not −\"n\"), formula_54 are based on two generalized Laguerre polynomials of order formula_55 and formula_56:\n\nwith \"A\" now defined as\n\nAgain \"f\" is small compared to \"g\" (except at very small \"r\") because when \"k\" is positive the first terms dominate, and α is big compared to γ−\"k\", whereas when \"k\" is negative the second terms dominate and α is small compared to γ−\"k\". Note that the dominant term is quite similar to corresponding the Schrödinger solution – the upper index on the Laguerre polynomial is slightly less (2γ+1 or 2γ−1 rather than 2ℓ+1, which is the nearest integer), as is the power of ρ (γ or γ−1 instead of ℓ, the nearest integer). The exponential decay is slightly faster than in the Schrödinger solution.\n\nThe normalization factor makes the integral over all space of the square of the absolute value equal to 1.\n\nHere is the 1S orbital, spin up, without normalization:\n\nNote that γ is a little less than 1, so the top function is similar to an exponentially decreasing function of \"r\" except that at very small \"r\" it theoretically goes to infinity. But the value of the formula_61 only surpasses 10 at a value of \"r\" smaller than formula_62 which is a very small number (much less than the radius of a proton) unless is very large.\n\nThe 1S orbital, spin down, without normalization, comes out as:\n\nWe can mix these in order to obtain orbitals with the spin oriented in some other direction, such as:\n\nwhich corresponds to the spin and angular momentum axis pointing in the x direction. Adding \"i\" times the \"down\" spin to the \"up\" spin gives an orbital oriented in the y direction.\n\nTo give another example, the 2P orbital, spin up, is proportional to:\n\nNotice that when ρ is small compared to α (or \"r\" is small compared to formula_67) the \"S\" type orbital dominates (the third component of the bispinor).\n\nFor the 2S spin up orbital, we have:\n\nNow the first component is S-like and there is a radius near ρ = 2 where it goes to zero, whereas the bottom two-component part is P-like.\n\nIn addition to bound states, in which the energy is less than that of an electron infinitely separated from the nucleus, there are solutions to the Dirac equation at higher energy, corresponding to an unbound electron interacting with the nucleus. These solutions are not normalizable, but solutions can be found which tend toward zero as goes to infinity (which is not possible when formula_69 except at the above-mentioned bound-state values of ). There are similar solutions with formula_70 These negative-energy solutions are just like positive-energy solutions having the opposite energy but for a case in which the nucleus repels the electron instead of attracting it, except that the solutions for the top two components switch places with those for the bottom two.\n\nNegative-energy solutions to Dirac's equation exist even in the absence of a Coulomb force exerted by a nucleus. Dirac hypothesized that we can consider almost all of these states to be already filled. If one of these negative-energy states is not filled, this manifests itself as though there is an electron which is \"repelled\" by a positively-charged nucleus. This prompted Dirac to hypothesize the existence of positively-charged electrons, and his prediction was confirmed with the discovery of the positron.\n\nThe Dirac equation with a simple Coulomb potential generated by a point-like non-magnetic nucleus was not the last word, and its predictions differ from experimental results as mentioned earlier. More accurate results include the Lamb shift (radiative corrections arising from quantum electrodynamics) and hyperfine structure.\n\n\n"}
{"id": "3854018", "url": "https://en.wikipedia.org/wiki?curid=3854018", "title": "Jacobsite", "text": "Jacobsite\n\nJacobsite is a manganese iron oxide mineral. It is in the spinel group and forms a solid solution series with magnetite. The chemical formula is MnFeO or with oxidation states and substitutions: \n(Mn,Fe,Mg)(Fe,Mn)O.\n\nIt occurs as a primary phase or as alteration of other manganese minerals during metamorphism of manganese deposits. Typical associated minerals include hausmannite, galaxite, braunite, pyrolusite, coronadite, hematite and magnetite.\n\nIt was first described in 1869 and named for the Jakobsberg Mine, Nordmark, Filipstad, Värmland, Sweden.\n\n"}
{"id": "183950", "url": "https://en.wikipedia.org/wiki?curid=183950", "title": "Kerogen", "text": "Kerogen\n\nKerogen is a solid organic matter in sedimentary rocks. Consisting of an estimated 10 tons of carbon, it is the most abundant source of organic compounds on earth, exceeding the total organic content of living matter by 10,000 fold. It is insoluble in normal organic solvents and it does not have a specific chemical formula. Upon heating, kerogen converts in part to liquid and gaseous hydrocarbons. Petroleum and natural gas form from kerogen. Based on its origin, kerogen may be classified as algal, mixed terrestrial and marine.\n\nThe name \"kerogen\" was introduced by the Scottish organic chemist Alexander Crum Brown in 1906. It means in Greek \"wax birth\" (Greek: κηρός \"wax\" and -gen, γένεση \"birth\").\nKerogen is a mixture of organic chemical compounds that make up a portion of the organic matter in sedimentary rocks. As kerogen is a mixture of organic material, rather than a specific chemical, it cannot be given a chemical formula. Its chemical composition can vary distinctively from sample to sample. For example, kerogen from the Green River Formation oil shale deposit of western North America contains elements in the proportions carbon 215 : hydrogen 330 : oxygen 12 : nitrogen 5 : sulfur 1.\n\nKerogen is insoluble in normal organic solvents because of the high molecular weight (upwards of 1,000 daltons or 1000 Da; 1 Da = 1 atomic mass unit) of its component compounds. The soluble portion is known as bitumen. When heated to the right temperatures in the Earth's crust, (\"oil window\" c. 50–150 °C, \"gas window\" c. 150–200 °C, both depending on how quickly the source rock is heated) some types of kerogen release crude oil or natural gas, collectively known as hydrocarbons (fossil fuels). When such kerogens are present in high concentration in rocks such as shale, they form possible source rocks. Shales rich in kerogens that have not been heated to a warmer temperature to release their hydrocarbons may form oil shale deposits.\n\nKerogen arises from the degradation of living matter, such as diatoms, planktons, spores and pollens. In this break-down process, large biopolymers from proteins and carbohydrates dismantle partially or completely. (This break-down process can be viewed as the reverse of photosynthesis). These dismantled components are units that can then polycondense to form polymers. This polymerization usually happens alongside the formation of a mineral component (geopolymer) resulting in a sedimentary rock like kerogen shale.\n\nThe formation of polymers in this way accounts for the large molecular weights and diverse chemical compositions associated with kerogen. The smallest units are the fulvic acids, the medium units are the humic, and the largest units are the humins. When organic matter is contemporaneously deposited with geologic material, subsequent sedimentation and progressive burial or overburden provide significant pressure and a temperature gradient. When these humic precursors are subjected to sufficient geothermal pressures for sufficient geologic time, they begin to undergo certain specific changes to become kerogen. Such changes are indicative of the maturity stage of a particular kerogen. These changes include loss of hydrogen, oxygen, nitrogen, and sulfur, which leads to loss of other functional groups that further promote isomerization and aromatization which are associated with increasing depth or burial. Aromatization then allows for neat molecular stacking in sheets, which in turn increases molecular density and \"vitrinite reflectance\" properties, as well as changes in spore coloration, characteristically from yellow to orange to brown to black with increasing depth.\n\nKerogen breaks down in the subsurface to generate oil and gas, which form the source of hydrocarbons in conventional reservoirs. In unconventional resources, many of which are referred to as shale, the produced hydrocarbons have not been expelled from the source rock, but instead are stored and transported within the shale. Most kerogens of relevance to the oil and gas industry are marine (). Much of the porosity in shale is hosted within kerogen, and the recent development of economic shale resources has led to increased research into the composition of kerogen. Studies using NMR spectroscopy have found that carbon in kerogen can range from almost entirely aliphatic (sp hybridized) to almost entirely aromatic (sp hybridized). with kerogens of higher type and/or higher thermal maturity typically having higher abundance of aromatic carbon.\n\n\"Labile\" kerogen breaks down to form heavy hydrocarbons (i.e., oils), \"refractory\" kerogen breaks down to form light hydrocarbons (i.e., gases), and \"inert\" kerogen forms graphite.\n\nA Van Krevelen diagram is one example of classifying kerogens, where they tend to form groups when the ratios of hydrogen to carbon and oxygen to carbon are compared.\n\nType 1 oil shales yield larger amount of volatile or extractable compounds than other types upon pyrolysis. Hence, from the theoretical view, Type 1 kerogen oil shales provide the highest yield of oil and are the most promising deposits in terms of conventional oil retorting.\n\n\nType II kerogen is common in many oil shale deposits. It is based on marine organic materials, which are formed in reducing environments. Sulfur is found in substantial amounts in the associated bitumen and generally higher than the sulfur content of Type I or III. Although pyrolysis of Type II kerogen yields less oil than Type I, the amount acquired is still sufficient to consider Type II bearing rocks as potential oil sources \n\nThey all have great tendencies to produce petroleum and are all formed from lipids deposited under reducing conditions.\n\nSimilar to Type II but high in sulfur.\n\n\nKerogen Type III is formed from terrestrial plant matter that is lacking in lipids or waxy matter. It forms from cellulose, the carbohydrate polymer that forms the rigid structure of terrestrial plants, lignin, a non-carbohydrate polymer formed from phenyl-propane units that binds the strings of cellulose together, and terpenes and phenolic compounds in the plant. Type III kerogen involving rocks are found to be the least productive upon pyrolysis and probably the least favorable deposits for oil generation\n\nHydrogen: carbon ratio < 0.5\n\nType IV kerogen contains mostly decomposed organic matter in the form of polycyclic aromatic hydrocarbons. They have no potential to produce hydrocarbons.\n\nCarbonaceous chondrite meteorites contain kerogen-like components. Such material is thought to have formed the terrestrial planets. Kerogen materials have been detected also in interstellar clouds and dust around stars.\n\nThe \"Curiosity\" rover has detected organic deposits similar to kerogen in mudstone samples in Gale Crater on Mars using a revised drilling technique. The presence of benzene and propane also indicates the possible presence of kerogen-like materials, from which hydrocarbons are derived.\n\n\n"}
{"id": "31945999", "url": "https://en.wikipedia.org/wiki?curid=31945999", "title": "List of Acacia species known to contain psychoactive alkaloids", "text": "List of Acacia species known to contain psychoactive alkaloids\n\nThis is a list of \"Acacia\" species (\"sensu lato\") that are known to contain psychoactive alkaloids, or are suspected of containing such alkaloids due to being psychoactive. The presence and constitution of alkaloids in nature can be highly variable, due to environmental and genetic factors.\n\nSpecies containing a concentration of alkaloids of 0-0.02% include:\n\n\n"}
{"id": "17684409", "url": "https://en.wikipedia.org/wiki?curid=17684409", "title": "List of Areas of Special Scientific Interest in County Antrim", "text": "List of Areas of Special Scientific Interest in County Antrim\n\nThis is a list of the Areas of Special Scientific Interest (ASSIs) in County Antrim in Northern Ireland, United Kingdom.\n\nIn Northern Ireland the body responsible for designating ASSIs is the Northern Ireland Environment Agency – a division of the Department of Environment (DoE).\n\nUnlike the SSSIs, ASSIs include both natural environments \"and\" man-made structures. As with SSSIs, these sites are designated if they have criteria based on fauna, flora, geological or physiographical features. On top of this, structures are also covered, such as the Whitespots mines in Conlig, according to several criterion including rarity, recorded history and intrinsic appeal. \n\nFor other sites in the rest of the United Kingdom, see List of SSSIs by Area of Search.\n\nThe data in the table is taken from the Northern Ireland Environment Agency's website in the form of citation sheets for each ASSI.\n\n"}
{"id": "1934667", "url": "https://en.wikipedia.org/wiki?curid=1934667", "title": "List of Earth observation satellites", "text": "List of Earth observation satellites\n\nPartial list of Earth observation satellites by series/program.\n\n\nThe invention of climate research through the use of satellite remote telemetry began in the 1960s through development of space probes to study other planets. During the U.S. economic decline in 1977, with much of NASA's money going toward the Shuttle program, the Reagan Administration proposed to reduce spending on planetary exploration. During this time, new scientific evidence emerged from ice and sediment cores that Earth's climate had experienced rapid changes in temperature, running contrary to the previously held belief that the climate changed on a geological time scale. These changes increased political interest in gathering remote-sensing data on the Earth itself and stimulated the science of climatology.\n17 Satellites, 4 in operation\n\n10 Satellites 5 in operation.\n\n\"successor to planned National Polar-orbiting Operational Environmental Satellite System (NPOESS) program\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11 Geostationary Weather Satellites, 5 in operation:\n\n2 Polar Orbiting Weather Satellites, all in operation:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25089530", "url": "https://en.wikipedia.org/wiki?curid=25089530", "title": "List of Sites of Special Scientific Interest in Rutland", "text": "List of Sites of Special Scientific Interest in Rutland\n\nRutland is a landlocked ceremonial county in the East Midlands of England. In 1974 it was merged to be part of the administrative county of Leicestershire, but in 1997 it was separated to become a unitary local authority, which is responsible for all local services apart from the police and fire service. It is mainly rural, but has two market towns, Oakham, the county town, and Uppingham. The county has an area of , and the 2011 census showed a population of 37,400.\n\nIn England, Sites of Special Scientific Interest (SSSIs) are designated by Natural England, a non-departmental public body which is responsible for protecting England's natural environment. Designation as an SSSI gives legal protection to the most important wildlife and geological sites. As of November 2017, there are 19 SSSIs in the county. Sixteen are designated for their biological importance, one for its geological importance and two under both criteria. \n\nThe largest site is Rutland Water at . It is the largest man-made reservoir in Europe, a Ramsar internationally important wetland site and a Special Protection Area under the European Union Directive on the Conservation of Wild Birds. The smallest is Tolethorpe Road Verges at , which has several regionally uncommon plants on Jurassic limestone.\n\n\n"}
{"id": "25601705", "url": "https://en.wikipedia.org/wiki?curid=25601705", "title": "List of United States federal environmental statutes", "text": "List of United States federal environmental statutes\n\nThe laws listed below meet the following criteria: (1) they were passed by the United States Congress, and (2) pertain to (a) the regulation of the interaction of humans and the natural environment, or (b) the conservation and/or management of natural or historic resources. They need not be wholly codified in the United States Code.\n\n"}
{"id": "2948660", "url": "https://en.wikipedia.org/wiki?curid=2948660", "title": "List of extinct plants", "text": "List of extinct plants\n\nThe following is a list of extinct plants only.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"See Lazarus species\"\n\n\n\n\n"}
{"id": "36244978", "url": "https://en.wikipedia.org/wiki?curid=36244978", "title": "List of forests managed by the Forestry Commission", "text": "List of forests managed by the Forestry Commission\n\nThe Forestry Commission manages about one million hectares of land across Great Britain. The Commission manages 660,000 hectares of forest in Scotland, 250,000 hectares in England and 126,000 hectares in Wales. These forests range from small scale urban forests to many of the largest forests in Britain. The Forestry Commission was set up in 1919 to carry out afforestation programmes across Britain for timber production. It is also responsible for maintaining and developing recreational facilities within the forests.\n\n"}
{"id": "7346214", "url": "https://en.wikipedia.org/wiki?curid=7346214", "title": "List of fountains in the Kansas City metropolitan area", "text": "List of fountains in the Kansas City metropolitan area\n\nThis page contains a list of fountains in Kansas City metropolitan area. It is primarily composed of fountains in Kansas City, Missouri officially recognized by the City of Fountains Foundation, but also includes fountains throughout the Kansas City Metropolitan Area.\n\nIn 1992, the city added \"City of Fountains\" to its official corporate seal.\n\nFountains form part of the Kansas City's core identity and culture, and it even used a graphic design of a stylized fountain in its official logo. \nInterest in fountains arose during the City Beautiful movement in the 1890s. In 1898, George Kessler, a landscape architect and urban planner, designed the first fountain built by the city of Kansas City, Missouri at 15th and The Paseo. Although it was destroyed in 1941, another fountain that he designed the same year still exists and remains the oldest in the city. Now known as The Women's Leadership Fountain, it is located at 9th Street and The Paseo. The design originally included an oval, cut limestone basin with water spraying upwards from nozzles in the center of the its pool surrounded by a raised sidewalk, a flower garden, gas lamps, and a balustrade above to the south. The fountain was rebuilt in 1970 and 1990 and began its third major restoration in January 2008 planned for completion in 2009. Kessler went on to include numerous plans for fountains in his urban designs of the park and boulevard system.\n\nTypically, most of the first fountains in Kansas City served practical rather than decorative purposes. In 1904, the Humane Society of Kansas City (Kansas) – established to prevent cruelty to women, children and animals – built a characteristic fountain near the west end of Minnesota Avenue at North 3rd Street. Water poured out of spigots in lions' mouths so that people could get clean water in their cups. This water fell into a granite basin at a height for horses to drink. The overflow from the basin went into four small pools at street level for dogs to drink. A street light was on top; in 1967, the fountain was given to the Wyandotte County Museum. The Humane Society went on to mount more than 100 fountains, including ones made of bronze created for people alone for sanitary purposes.\n\nFountain building and the use of decorative statuary exploded in the 1920s after developer J.C. Nichols used them extensively in the development of Country Club Plaza. The most famous fountain in Kansas City is appropriately named J.C. Nichols Memorial Fountain. The figures were originally created by French sculptor Henri-Léon Gréber in 1910 for \"Harbor Hill,\" the estate of Clarence Mackay in Roslyn, New York. The four allegorical equestrian figures reportedly represent four great rivers of the world — the Mississippi River, Volga River, Seine River, and Rhine River. The work is enlivened by sculptures of little children riding dolphins in the pool surrounding the main figures.\n\nThe \"William Volker Memorial Fountain\" includes the last sculptures by Swedish artist Carl Milles. The five-piece ensemble of bronze statuary shows Saint Martin of Tours, patron saint of France, on horseback, giving his clothes to a beggar surrounded by two angels (one amusingly wearing a wristwatch) and a curious little demon in hiding. The sculptures rest between two pools of water with jet sprays along Volker Boulevard, and sits above a dramatic three-tier, waterfall into a basin on Brush Creek.\n\nThe \"Eagle Scout Memorial Fountain\" was originally part of the Seventh Avenue clock created by A.A. Weinman for the Pennsylvania Station in New York City. When the station was torn down, Kansas City petitioned to obtain the clock sculpture and replaced its face with an Eagle Scout tribute.\n\nThe \"Waterworks Spectacular\" has been dousing the outfield during baseball games at Kauffman Stadium for over 30 years.\n\n\n\n\n\n\n\n\n"}
{"id": "6083777", "url": "https://en.wikipedia.org/wiki?curid=6083777", "title": "List of glaciers in Iceland", "text": "List of glaciers in Iceland\n\nThe glaciers and ice caps of Iceland cover 11.1% of the land area of the country (about 11,400 km² out of the total area of 103,125 km²) and have a considerable impact on its landscape and meteorology. An ice cap is a mass of glacial ice that covers less than 50,000 km² of land area covering a highland area and they feed outlet glaciers. Glaciers are also contributing to the Icelandic economy, with tourists flocking to the country to see glaciers on snowmobiles and on glacier hiking tours.\n\nMany Icelandic ice caps and glaciers lie above volcanoes, such as Grímsvötn and Bárðarbunga, which lie under the largest ice cap, Vatnajökull. The caldera of Grímsvötn is 100 km² in area, and Bárðarbunga is 60 km².\n\nWhen volcanic activity occurs under the glacier, the resulting meltwater can lead to a sudden glacial lake outburst flood, known in Icelandic as jökulhlaup, but jökulhlaups are most often caused by accumulation of meltwater due to geothermal activity underneath the glacier. Such jökulhlaups have occasionally triggered volcanic eruptions through the sudden release of pressure. \n\nIceland is losing ice due to climate change. Okjökull glacier in Borgarfjörður, West Iceland, has lost its glacier title and is now simply known as “Ok”. In order to fit the criteria glaciers need to be thick enough to sink and move under their own weight, which Okjökull is not. Okjökull is the first Icelandic glacier to lose its title.\n\nThe Icelandic word for \"glacier\" is \"jökull\".\n\nThese 13 largest glaciers have an aggregate area of 11,181 km² (out of about 11,400 km² for all glaciers of Iceland).\n\n\n"}
{"id": "9019057", "url": "https://en.wikipedia.org/wiki?curid=9019057", "title": "List of mint diseases", "text": "List of mint diseases\n\nThis article is a list of diseases of mint (\"Mentha piperita\", \"M. cardiaca\", \"M. spicata\" and \"M. arvensis\").\n"}
{"id": "15910202", "url": "https://en.wikipedia.org/wiki?curid=15910202", "title": "List of oil pipelines", "text": "List of oil pipelines\n\nThis is a list of oil pipelines.\n\n\n\n\n\n\n\n\n"}
{"id": "34788201", "url": "https://en.wikipedia.org/wiki?curid=34788201", "title": "List of rivers of Jordan", "text": "List of rivers of Jordan\n\nThis is a list of rivers in Jordan. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name. Many of these rivers are seasonal. \n\nThe Jordan River is the major river flowing into the Dead Sea from the north. It also is the northern part of the western border of Jordan. From south to north the rivers flowing into it are:\nOther rivers that flow directly into the Dead Sea are:\n\n\n\n\n"}
{"id": "20414480", "url": "https://en.wikipedia.org/wiki?curid=20414480", "title": "List of rivers of the Bahamas", "text": "List of rivers of the Bahamas\n\nThere are no rivers in the Bahamas. In March 2018 waves breaking over cliffs near the Glass Window Bridge in Eleuthera caused a flow of water across the island that washed out the Queens Highway, and was temporarily called a new river. There are also many tidal creeks, which resemble rivers.\n\nThis is a list of creeks in The Bahamas.\n\n\n\n"}
{"id": "4315825", "url": "https://en.wikipedia.org/wiki?curid=4315825", "title": "List of tributaries of the Gila River", "text": "List of tributaries of the Gila River\n\nThis is a List of tributaries of the Gila River, the final river drainage of the Colorado River, and covers the entire width of southern Arizona. The headwaters of the Gila River begin in the Gila National Forest of New Mexico. The northern branch tributary to the Gila River in the Gila National Forest, is the San Francisco River.\n\nThe map highlights the Gila River extending eastward across southern Arizona to the southwestern corner of Arizona and its input into the Colorado River, from its origins about 400 miles east in the southwestern corner of the state of New Mexico. The highlighted river north and adjacent is the Salt River which has the Verde River connected just prior to its drainage into the Gila River. The Verde and Salt rivers effectively drain the western and central portion of the Mogollon Rim, NW to SE across the state, and the White Mountain region of the Mogollon Rim, in east-central, and east Arizona.\n\nThe Verde River drainage into the Salt River, at its very northwestern drainages, east of Kingman, Arizona, even drains the South Rim, of the Grand Canyon. Thus, the Gila River watershed, may cover more than sixty (60) percent of the state.\n\nFrom west–to–east the Hassayampa River's final destination is the dry desert, and groundwater. The second drainage, the Agua Fria River enters the Gila in western Phoenix, but not as a flowing river. The Agua Fria puts most of its winter drainage, or snowmelt/rain floodwaters into Lake Pleasant Regional Park. The largest of the \"three\" north/south rivers, the Verde River(plus the East Verde), goes through Horseshoe Reservoir and Bartlett Reservoir before joining the Salt River. (The Salt River enters the Gila, 40 miles downstream, through central Phoenix, and five miles before the Agua Fria River tributary. The Hassayampa River is 35 miles farther downstream, and just prior to the tributary of Centennial Wash.)\n\nThe Verde River drainage extends north and northwest of the Agua Fria River, and Hassayampa River drainages, and continues to the South Rim of the Grand Canyon. The combined: Salt River–Verde River Drainage would be the major, (and central), component of the Mogollon Rim drainages of central Arizona.\n\n\n"}
{"id": "7894669", "url": "https://en.wikipedia.org/wiki?curid=7894669", "title": "List of water deities", "text": "List of water deities\n\nA water deity is a deity in mythology associated with water or various bodies of water. Water deities are common in mythology and were usually more important among civilizations in which the sea or ocean, or a great river was more important. Another important focus of worship of water deities has been springs or holy wells.\n\nAs a form of animal worship, whales and snakes (hence dragons) have been regarded as godly deities throughout the world (other animals are such as turtles, fish, crabs, and sharks). In Asian lore, whales and dragons sometimes have connections. Serpents are also common as a symbol or as serpentine deities, sharing many similarities with dragons.\n\n\n\n\n\n\n\n\n\nHonorable Kings of the Water Immortals (\"Shuixian Zunwang\")\n\nDragon Kings of the Four Seas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1937788", "url": "https://en.wikipedia.org/wiki?curid=1937788", "title": "Marid", "text": "Marid\n\nMarid ( \"\") is an Arabic word meaning \"rebellious\", which is sometimes applied to supernatural beings.\n\nThe word \"mārid\" is an active participle of the root \"m-r-d\" (مرد), whose primary meaning is \"recalcitrant, rebellious\". \"Lisān al-`arab\", the encyclopedic dictionary of classical Arabic compiled by Ibn Manzur, reports only forms of this general meaning. It is found as an attribute of evil spirits in the Qur'an (aṣ-Ṣāffāt, 37:7), which speaks of a \"safeguard against every rebellious devil\" (شَيْطَانٍ مَارِدٍ, \"shaitān mārid\").\n\nThe Wehr-Cowan dictionary of modern written Arabic also gives secondary meanings of \"demon\" and \"giant\". Lane's \"Arabic-English Lexicon\" cites a source where it \"is said to be applied to an evil jinnee of the most powerful class\", but this distinction is not universal. For example, in the standard MacNaghten edition of \"One Thousand and One Nights\" one finds the words \"marid\" and \"ifrit\" used interchangeably (e.g., in \"The Story of the Fisherman\").\n\nA mārid is explicitly mentioned in Sirat Sayf ibn Dhi-Yazan. Accordingly Sayf demands from the marid to lead him to Solomons hoard. But following his nature, the demon does the exact oppisite of that he was commanded. Later he learned from Khidr, he must command the opposite of that he desires him to do.\n\nIn Jonathan Stroud's \"Bartimaeus Sequence\" novel series, marids are the most powerful type of demons summoned by magicians.\n\nIn the \"Dungeons & Dragons\" tabletop game, marids are genies from the Elemental Plane of Water.\n\n"}
{"id": "3694602", "url": "https://en.wikipedia.org/wiki?curid=3694602", "title": "Natural frequency", "text": "Natural frequency\n\nNatural frequency is the frequency at which a system tends to oscillate in the absence of any driving or damping force.\n\nThe motion pattern of a system oscillating at its natural frequency is called the normal mode (if all parts of the system move sinusoidally with that same frequency).\n\nIf the oscillating system is driven by an external force at the frequency at which the amplitude of its motion is greatest (close to a natural frequency of the system), this frequency is called resonant frequency.\n\nFree vibrations of an elastic body are called \"natural vibrations\" and occur at a frequency called the natural frequency. Natural vibrations are different from forced vibrations which happen at frequency of applied force (forced frequency). If forced frequency is equal to the natural frequency, the amplitude of vibration increases manifold. This phenomenon is known as resonance.\n\nIn a mass-spring system, with mass \"m\" and spring stiffness \"k\", the natural frequency can be calculated as:\n\nIn electrical circuits, \"s\" is a natural frequency of variable \"x\" if the zero-input response of \"x\" includes the term formula_2, where formula_3 is a constant dependent on initial state of the circuit, network topology, and element values. In a network, \"s\" is a natural frequency of the network if it is a natural frequency of some voltage or current in the network. Natural frequencies depend \"only\" on network topology and element values but not the input. It can be shown that the set of natural frequencies in a network can be obtained by calculating the poles of all impedance and admittance functions of the network. All poles of the network transfer function are also natural frequencies of the corresponding response variable; however there may exist some natural frequencies that are not a pole of the network function. These frequencies happen at some special initial states.\n\nIn LC and RLC circuits, the natural frequency of a circuit can be calculated as:\n\n\n"}
{"id": "5734415", "url": "https://en.wikipedia.org/wiki?curid=5734415", "title": "Northern Congolian forest-savanna mosaic", "text": "Northern Congolian forest-savanna mosaic\n\nThe Northern Congolian forest-savanna mosaic is a forest and savanna ecoregion of central Africa, part of the belt of transitional forest-savanna mosaic that lie between Africa's equatorial forests and the tropical dry forests, savannas, and grasslands that lie to the north and south. The Northern Congolian forest-savanna mosaic lies between the equatorial Congolian forests to the south and the drier East Sudanian savanna to the north. It extends from the Cameroon Highlands in the west, across central Cameroon and the southern Central African Republic to southwestern South Sudan and northeastern Democratic Republic of the Congo, where it is bounded on the east by flooded grasslands of the Sudd, the eastern block of the East Sudanian savanna, and the Albertine Rift montane forests.\n"}
{"id": "54421490", "url": "https://en.wikipedia.org/wiki?curid=54421490", "title": "Radiative levitation", "text": "Radiative levitation\n\nRadiative levitation is the name given to a phenomenon that causes the spectroscopically-derived abundance of heavy elements in the photospheres of hot stars to be very much higher than solar abundance or than the expected bulk abundance; for example, the spectrum of the star Feige 86 has gold and platinum abundances three to ten thousand times higher than solar .\n\nThe mechanism is that heavier elements have large photon absorption cross-sections when partially ionized (see opacity), so efficiently absorb photons from the radiation coming from the core of the star, and some of the energy of the photons gets converted to outward momentum, effectively 'kicking' the heavy atom towards the photosphere. The effect is strong enough that very hot white dwarfs are significantly less bright in the EUV and X-ray bands than would be expected from a black-body model. \n\nThe countervailing process is gravitational settling, where, in very high gravitational fields, the effects of diffusion even in a hot atmosphere are cancelled out to the point that the heavier elements will sink unobservably to the bottom and lighter elements settle on the top.\n"}
{"id": "200011", "url": "https://en.wikipedia.org/wiki?curid=200011", "title": "Red supergiant star", "text": "Red supergiant star\n\nRed supergiants are stars with a supergiant luminosity class (Yerkes class I) of spectral type K or M. They are the largest stars in the universe in terms of volume, although they are not the most massive or luminous. Betelgeuse and Antares are the brightest and best known red supergiants (RSGs), indeed the only first magnitude red supergiant stars.\n\nStars are classified as supergiants on the basis of their spectral luminosity class. This system uses certain diagnostic spectral lines to estimate the surface gravity of a star, hence determining its size relative to its mass. Larger stars are more luminous at a given temperature and can now be grouped into bands of differing luminosity.\n\nThe luminosity differences between stars is most apparent at low temperatures, where giant stars are much brighter than main-sequence stars. Supergiant have the lowest surface gravities and hence are the largest and brightest at a particular temperature.\n\nThe \"Yerkes\" or \"Morgan-Keenan\" (MK) classification system is almost universal. It groups stars into five main luminosity groups designated by roman numerals:\n\nSpecific to supergiants, the luminosity class is further divided into normal supergiants of class Ib and bright supergiants of class Ia. The intermediate class Iab is also used. Exceptionally bright, low surface gravity, stars with strong indications of mass loss may be designated by luminosity class 0 (zero) although this is rarely seen. More often the designation Ia-0 will be used, and more commonly still Ia. These hypergiant spectral classifications are very rarely applied to red supergiants, although the term hypergiant is sometimes used for the most extended and unstable red supergiants.\n\nThe \"red\" part of \"red supergiant\" refers to the cool temperature. Red supergiants are the coolest supergiants, M-type and at least some K-type stars although there is no precise cutoff. K-type supergiants are uncommon compared to M-type, because they are a short-lived transition stage and somewhat unstable. The K-type stars, especially early or hotter K types, are sometimes described as orange supergiants (e.g. Zeta Cephei), or even as yellow (e.g. yellow hypergiant HR 5171A).\n\nRed supergiants are cool and large. They have spectral types of K and M, hence temperatures below 4,100 K. They are typically several hundred to over a thousand times the radius of the Sun, although size is not the primary factor in a star being designated as a supergiant. A bright cool giant star can easily be larger than a hotter supergiant. For example, Alpha Herculis is classified as a giant star with a radius of while Epsilon Pegasi is a K2 supergiant of only .\n\nAlthough red supergiants are much cooler than the Sun, they are so much larger that they are highly luminous, typically . There is an upper limit to the luminosity of a red supergiant at around . Stars above this luminosity would be too unstable and simply don't form.\n\nRed supergiants have masses between about and . Main-sequence stars more massive than about do not expand and cool to become red supergiants. Red supergiants at the upper end of the possible mass and luminosity range are the largest known. Their low surface gravities and high luminosities cause extreme mass loss, millions of times higher than the Sun, producing observable nebulae surrounding the star. By the end of their lives red supergiants may have lost a substantial fraction of their initial mass. The more massive supergiants lose mass much more rapidly and all red supergiants appear to reach a similar mass of the order of by the time their cores collapse. The exact value depends on the initial chemical makeup of the star and its rotation rate.\n\nMost red supergiants show some degree of visual variability, but only rarely with a well-defined period or amplitude. Therefore, they are usually classified as irregular or semiregular variables. They even have their own sub-classes, SRC and LC for slow semi-regular and slow irregular supergiant variables respectively. Variations are typically slow and of small amplitude, but amplitudes up to four magnitudes are known.\n\nStatistical analysis of many known variable red supergiants shows a number of likely causes for variation: just a few stars show large amplitudes and strong noise indicating variability at many frequencies, thought to indicate powerful stellar winds that occur towards the end of the life of a red supergiant; more common are simultaneous radial mode variations over a few hundred days and probably non-radial mode variations over a few thousand days; only a few stars appear to be truly irregular, with small amplitudes, likely due to photospheric granulation. Red supergiant photospheres contain a relatively small number of very large convection cells compared to stars like the Sun. This causes variations in surface brightness that can lead to visible brightness variations as the star rotates.\n\nThe spectra of red supergiants are similar to other cool stars, dominated by a forest of absorption lines of metals and molecular bands. Some of these features are used to determine the luminosity class, for example certain near-infrared cyanogen band strengths and the Ca II triplet.\n\nMaser emission is common from the circumstellar material around red supergiants. Most commonly this arises from HO and SiO, but hydroxyl (OH) emission also occurs from narrow regions. In addition to high resolution mapping of the circumstellar material around red supergiants, VLBI or VLBA observations of masers can be used to derive accurate parallaxes and distances to their sources. Currently this has been applied mainly to individual objects, but it may become useful for analysis of galactic structure and discovery of otherwise obscured red supergiant stars.\n\nSurface abundances of red supergiants are dominated by hydrogen even though hydrogen at the core has been completely consumed. In the latest stages of mass loss before a star explodes, surface helium may become enriched to levels comparable with hydrogen. In theoretical extreme mass loss models, sufficient hydrogen may be lost that helium becomes the most abundant element at the surface. When pre-red supergiant stars leave the main sequence, oxygen is more abundant than carbon at the surface, and nitrogen is less abundant than either, reflecting abundances from the formation of the star. Carbon and oxygen are quickly depleted and nitrogen enhanced as a result of the dredge-up of CNO-processed material from the fusion layers.\n\nRed supergiants are observed to rotate slowly or very slowly. Models indicate that even rapidly rotating main-sequence stars should be braked by their mass loss so that red supergiants hardly rotate at all. Those red supergiants such as Betelgeuse that do have modest rates of rotation may have acquired it after reaching the red supergiant stage, perhaps though binary interaction. The cores of red supergiants are still rotating and the differential rotation rate can be very large.\n\nSupergiant luminosity classes are easy to determine and apply to large numbers of stars, but they group a number of very different types of star into a single category. An evolutionary definition restricts the term supergiant to those massive stars which start core helium fusion without developing a degenerate helium core and without undergoing a helium flash. They will universally go on to burn heavier elements and undergo core collapse resulting in a supernova.\n\nLess massive stars may develop a supergiant spectral luminosity class at relatively low luminosity, around , when they are on the asymptotic giant branch (AGB) undergoing helium shell burning. Researchers now prefer to categorise these as AGB stars distinct from supergiants because they are less massive, have different chemical compositions at the surface, undergo different types of pulsation and variability, and will evolve in a different way, usually producing a planetary nebula and white dwarf. Most AGB stars will not become supernovae although there is interest in a class of \"super-AGB\" stars, those almost massive enough to undergo full carbon fusion, which may produce peculiar supernovae although without ever developing an iron core. One notable group of low mass high luminosity stars are the RV Tauri variables, AGB or post-AGB stars lying on the instability strip and showing distinctive semi-regular variations.\n\nRed supergiants develop from main-sequence stars with masses between about and . Higher-mass stars never cool sufficiently to become red supergiants. Lower-mass stars develop a degenerate helium core during a red giant phase, undergo a helium flash before fusing helium on the horizontal branch, evolve along the AGB while burning helium in a shell around a degenerate carbon-oxygen core, then rapidly lose their outer layers to become a white dwarf with a planetary nebula. AGB stars may develop spectra with a supergiant luminosity class as they expand to extreme dimensions relative to their small mass, and they may reach luminosities tens of thousands times the sun's. Intermediate \"super-AGB\" stars, around , can undergo carbon fusion and may produce an electron capture supernova through the collapse of an oxygen-neon core.\n\nMain-sequence stars, burning hydrogen in their cores, with masses between will have temperatures between about 25,000K and 32,000K and spectral types of early B, possibly very late O. They are already very luminous stars of due to rapid CNO cycle fusion of hydrogen and they have fully convective cores. In contrast to the Sun, the outer layers of these hot main-sequence stars are not convective.\n\nThese pre-red supergiant main-sequence stars exhaust the hydrogen in their cores after 5-20 million years. They then start to burn a shell of hydrogen around the now-predominantly helium core, and this causes them to expand and cool into supergiants. Their luminosity increases by a factor of about three. The surface abundance of helium is now up to 40% but there is little enrichment of heavier elements.\n\nThe supergiants continue to cool and most will rapidly pass through the Cepheid instability strip, although the most massive will spend a brief period as yellow hypergiants. They will reach late K or M class and become a red supergiant. Helium fusion in the core begins smoothly either while the star is expanding or once it is already a red supergiant, but this produces little immediate change at the surface. Red supergiants develop deep convection zones reaching from the surface over halfway to the core and these cause strong enrichment of nitrogen at the surface, with some enrichment of heavier elements.\n\nSome red supergiants undergo \"blue loops\" where they temporarily increase in temperature before returning to the red supergiant state. This depends on the mass, rate of rotation, and chemical makeup of the star. While many red supergiants will not experience a blue loop, some can have several. Temperatures can reach 10,000K at the peak of the blue loop. The exact reasons for blue loops vary in different stars, but they are always related to the helium core increasing as a proportion of the mass of the star and forcing higher mass loss rates from the outer layers.\n\nAll red supergiants will exhaust the helium in their cores within one or two million years and then start to burn carbon. This continues with fusion of heavier elements until an iron core builds up, which then inevitably collapses to produce a supernova. The time from the onset of carbon fusion until core collapse is no more than a few thousand years. In most cases, core collapse occurs while the star is still a red supergiant, the large remaining hydrogen-rich atmosphere is ejected, and this produces a type II supernova spectrum. The opacity of this ejected hydrogen decreases as it cools and this causes an extended delay to the drop in brightness after the initial supernova peak, the characteristic of a type II-P supernova.\n\nThe most luminous red supergiants, at near solar metallicity, are expected to lose most of their outer layers before their cores collapse, hence they evolve back to yellow hypergiants and luminous blue variables. Such stars can explode as type II-L supernovae, still with hydrogen in their spectra but not with sufficient hydrogen to cause an extended brightness plateau in their light curves. Stars with even less hydrogen remaining may produce the uncommon type IIb supernova, where there is so little hydrogen remaining that the hydrogen lines in the initial type II spectrum fade to the appearance of a type Ib supernova.\n\nThe observed progenitors of type II-P supernovae all have temperatures between 3,500K and 4,400K and luminosities between and . This matches the expected parameters of lower mass red supergiants. A small number of progenitors of type II-L and type IIb supernovae have been observed, all having luminosities around and somewhat higher temperatures up to 6,000K. These are a good match for slightly higher mass red supergiants with high mass loss rates. There are no known supernova progenitors corresponding to the most luminous red supergiants, and it is expected that these evolve to Wolf Rayet stars before exploding.\n\nRed supergiants are necessarily no more than about 25 million years old and such massive stars are expected to form only in relatively large clusters of stars, so they are expected to be found mostly near prominent clusters. However they are fairly short-lived compared to other phases in the life of a star and only form from relatively uncommon massive stars, so there will generally only be small numbers of red supergiants in each cluster at any one time. For example, in the substantial Double Clusters in Perseus there is just a single red supergiant, S Persei, while the massive Hodge 301 cluster in the Tarantula Nebula contains three. Until the 21st century the largest number of red supergiants known in a single cluster was five in NGC 7419. Most red supergiants are found singly, for example Betelgeuse in the Orion OB1 Association and Antares in the Scorpius-Centaurus Association.\n\nSince 2006, a series of massive clusters have been identified near the base of the Crux-Scutum Arm of the galaxy, each containing multiple red supergiants. RSGC1 contains at least 12 red supergiants, RSGC2 (also known as Stephenson 2) contains at least 26, RSGC3 contains at least 8, and RSGC4 (also known as Alicante 8) contains at least 8. A total of 80 confirmed red supergiants have been identified within a small area of the sky in the direction of these clusters. These four clusters appear to be part of a massive burst of star formation 10-20 million years ago at the near end of the bar at the centre of the galaxy. Similar massive clusters have been found near the far end of the galactic bar, but not such large numbers of red supergiants.\n\nRed supergiants are rare stars, but they are visible at great distance and are often variable so there are a number of well-known naked-eye examples:\n\nOther examples have become known on account of their enormous size, more than :\n\n"}
{"id": "15822723", "url": "https://en.wikipedia.org/wiki?curid=15822723", "title": "Relic woods", "text": "Relic woods\n\nRelic woods () is a natural monument (Protected areas of Ulyanovsk Oblast).\n\nContains two areas:\n1.22 metres high vegetation, consists of linden (near 90%) and birch (near 10%) trees. The store of woodpulp are 310 cube metres at 1 hectare. Square: 48,8 hectares.\n2.23 metres high vegetation, consists of linden (near 90%) and birch (near 10%) trees. The store of woodpulp are 380 cube metres at 1 hectare. Square: 11,3 hectares.\nAnother rocks: maple, filbert.\nAll manage works are prohibited with the aim to save value vegetation. Researching and scientific work is not conducing.\n\ninterest represent radical lime woods age of 80–100 years.\n\n"}
{"id": "4363024", "url": "https://en.wikipedia.org/wiki?curid=4363024", "title": "Shi Zhengrong", "text": "Shi Zhengrong\n\nShi Zhengrong (, born on February 10, 1963) is a Chinese-Australian businessman. He is the founder and, up to March 2013, chairman and chief executive officer of Suntech Power.\n\nShi was born in Yangzhong, Jiangsu province, China. His identical twin brother is Chen Henglong, who is also a tycoon. He finished his undergraduate study at Changchun University of Science and Technology in Changchun, and obtained his Master's degree from Shanghai Institute of Optics and Fine Mechanics, the Chinese Academy of Sciences. Afterward, Shi went to the University of New South Wales's School of Photovoltaic and Renewable Energy Engineering where he obtained his doctorate degree on solar power technology.\n\nHe acquired Australian citizenship and returned to China in 2001 to set up his solar power company - Suntech Power. According to Hurun Report's China Rich List 2013, he had a personal net worth of US$330 million.\n\nShi was elected Fellow of the Australian Academy of Technological Sciences and Engineering in 2009.\n\nAmid fierce price competition on its products, on 20 March 2013, the Suntech board declared bankruptcy in the wake of defaulting on US$541 million-worth of bonds, Shi had been demoted from chairman to director earlier that month. The \"Financial Times\", quoting the \"Shanghai Securities News\", reported at the time that Shi's movements were being restricted and that he was not allowed to leave China pending an investigation into his role at Suntech. By 2016, he was living in Shanghai and frequently visiting Australia. As of 2017 and 2018, Dr. Shi Zhengrong had been seen actively giving key note speeches at solar conferences and promoting the use of solar technologies in both China and overseas.\n\nHe has donated funds to a renewable energy research unit at the University of NSW, Australia \"because he felt it was not getting an appropriate level of government support\", according to Australian Greens Senator Christine Milne.\n\n"}
{"id": "40523438", "url": "https://en.wikipedia.org/wiki?curid=40523438", "title": "Specific potential energy", "text": "Specific potential energy\n\nSpecific potential energy is potential energy of an object per unit of mass of that object.\n\nIn a gravitational field it is the acceleration of gravity times height, formula_1.\n\nSpecific mechanical energy\n"}
{"id": "19651322", "url": "https://en.wikipedia.org/wiki?curid=19651322", "title": "Storage ring", "text": "Storage ring\n\nA storage ring is a type of circular particle accelerator in which a continuous or pulsed particle beam may be kept circulating typically for many hours. Storage of a particular particle depends upon the mass, momentum and usually the charge of the particle to be stored. Storage rings most commonly store electrons, positrons, or protons. \n\nStorage rings are most often used to store electrons that radiate synchrotron radiation. Over 50 facilities based on electron storage rings exist and are used for a variety of studies in chemistry and biology. Storage rings can also be used to produce polarized high-energy electron beams through the Sokolov-Ternov effect. The best-known application of storage rings is their use in particle accelerators and in particle colliders, where two counter-rotating beams of stored particles are brought into collision at discrete locations. The resulting subatomic interactions are then studied in a surrounding particle detector. Examples of such facilities are LHC, LEP, PEP-II, KEKB, RHIC, Tevatron and HERA.\n\nA storage ring is a type of synchrotron. While a conventional synchrotron serves to accelerate particles from a low to a high energy state with the aid of radio-frequency accelerating cavities, a storage ring keeps particles stored at a constant energy and radio-frequency cavities are only used to replace energy lost through synchrotron radiation and other processes.\n\nGerard K. O'Neill proposed the use of storage rings as building blocks for a collider in 1956. A key benefit of storage rings in this context is that the storage ring can accumulate a high beam flux from an injection accelerator that achieves a much lower flux.\n\nA force must be applied to particles in such a way that they are constrained to move approximately in a circular path. This may be accomplished using either dipole electrostatic or dipole magnetic fields, but because most storage rings store relativistic charged particles it turns out that it is most practical to utilise magnetic fields produced by dipole magnets. However, electrostatic accelerators have been built to store very low energy particles, and quadrupole fields may be used to store (uncharged) neutrons; these are comparatively rare, however.\n\nDipole magnets alone only provide what is called weak focusing, and a storage ring composed of only these sorts of magnetic elements results in the particles having a relatively large beam size. Interleaving dipole magnets with an appropriate arrangement of quadrupole and sextupole magnets can give a suitable strong focusing system that can give a much smaller beam size. The FODO and Chasman-Green lattice structures are simple examples of strong focusing systems, but there are many others.\n\nDipole and quadrupole magnets deflect different particle energies by differing amounts, a property called chromaticity by analogy with physical optics. The spread of energies that is inherently present in any practical stored particle beam will therefore give rise to a spread of transverse and longitudinal focusing, as well as contributing to various particle beam instabilities. Sextupole magnets (and higher order magnets) are used to correct for this phenomenon, but this in turn gives rise to nonlinear motion that is one of the main problems facing designers of storage rings.\n\nAs the bunches will travel many millions of kilometers (considering that they will be moving at near the speed of light for many hours), any residual gas in the beam pipe will result in many, many collisions. This will have the effect of increasing the size of the bunch, and increasing the energy spread. Therefore, a better vacuum yields better beam dynamics. Also, single large-angle scattering events from either the residual gas, or from other particles in the bunch (Touschek effect), can eject particles far enough that they are lost on the walls of the accelerator vacuum vessel. This gradual loss of particles is called beam lifetime, and means that storage rings must be periodically injected with a new complement of particles.\n\nInjection of particles into a storage ring may be accomplished in a number of ways, depending on the application of the storage ring. The simplest method uses one or more pulsed deflecting dipole magnets (injection kicker magnets) to steer an incoming train of particles onto the stored beam path; the kicker magnets are turned off before the stored train returns to the injection point, thus resulting in a stored beam. This method is sometimes called single-turn injection.\n\nMulti-turn injection allows accumulation of many incoming trains of particles, for example if a large stored current is required. For particles such as protons where there is no significant beam damping, each injected pulse is placed onto a particular point in the stored beam transverse or longitudinal phase space, taking care not to eject previously-injected trains by using a careful arrangement of beam deflection and coherent oscillations in the stored beam. If there is significant beam damping, for example radiation damping of electrons due to synchrotron radiation, then an injected pulse may be placed on the edge of phase space and then left to damp in transverse phase space into the stored beam before injecting a further pulse. Typical damping times from synchrotron radiation are tens of milliseconds, allowing many pulses per second to be accumulated.\n\nIf extraction of particles is required (for example in a chain of accelerators), then single-turn extraction may be performed analogously to injection. Resonant extraction may also be employed.\n\nThe particles must be stored for very large numbers of turns potentially larger than 10 billion. This long term stability is challenging, and one must combine the magnet design with tracking codes. and analytical tools in order to understand and optimize the long term stability.\n\nIn the case of electron storage rings, radiation damping eases the stability problem by providing a non-Hamiltonian motion returning the electrons to the design orbit on the order of the thousands of turns. Together with diffusion from the fluccuations in the radiated photon energies, an equilibrium beam distribution is reached. One may look at for further details on some of these topics.\n\n"}
{"id": "13800210", "url": "https://en.wikipedia.org/wiki?curid=13800210", "title": "Suess effect", "text": "Suess effect\n\nThe Suess effect is a change in the ratio of the atmospheric concentrations of heavy isotopes of carbon (C and C) by the admixture of large amounts of fossil-fuel derived CO, which is depleted in CO and contains no CO. It is named for the Austrian chemist Hans Suess, who noted the influence of this effect on the accuracy of radiocarbon dating. More recently, the Suess effect has been used in studies of climate change. The term originally referred only to dilution of atmospheric CO. The concept was later extended to dilution of CO and to other reservoirs of carbon such as the oceans and soils.\n\nCarbon has three naturally occurring isotopes. About 99% of carbon on Earth is carbon-12 (C), about 1% is carbon-13 (C), and a trace amount is carbon-14 (C). The C and C isotopes are stable, while C decays radioactively to nitrogen-14 (N) with a half life of 5730 years. C on Earth is produced nearly exclusively by the interaction of cosmic radiation with the upper atmosphere. A C atom is created when a thermal neutron displaces a proton in N. Minuscule amounts of C are produced by other radioactive processes, and a significant amount was released into the atmosphere during nuclear testing before the Limited Test Ban Treaty. Natural C production and hence atmospheric concentration varies only slightly over time.\n\nPlants take up C by fixing atmospheric carbon through photosynthesis. Animals then take C into their bodies when they consume plants (or consume other animals that consume plants). Thus, living plants and animals have the same ratio of C to C as the atmospheric CO. Once organisms die they stop exchanging carbon with the atmosphere, and thus no longer take up new C. Radioactive decay then gradually depletes the C in the organism. This effect is the basis of radiocarbon dating.\n\nPhotosynthetically fixed carbon in terrestrial plants is depleted in C compared to atmospheric CO. This depletion is slight in C4 plants but much greater in C3 plants which form the bulk of terrestrial biomass worldwide. Depletion in CAM plants vary between the values observed for C3 and C4 plants. In addition, most fossil fuels originate from C3 biological material produced tens to hundreds of millions of years ago. C4 plants did not become common until about 6 to 8 million years ago, and although CAM photosynthesis is present in modern relatives of the Lepidodendrales of the Carboniferous lowland forests, even if these plants also had CAM photosynthesis they were not a major component of the total biomass.\n\nFossil fuels such as coal and oil are made primarily of plant material that was deposited millions of years ago. This period of time equates to thousands of half-lives of C, so essentially all of the C in fossil fuels has decayed. Fossil fuels also are depleted in C relative to the atmosphere, because they were originally formed from living organisms. Therefore, the carbon from fossil fuels that is returned to the atmosphere through combustion is depleted in both C and C compared to atmospheric carbon dioxide.\n\n\n\n"}
{"id": "90325", "url": "https://en.wikipedia.org/wiki?curid=90325", "title": "Tepēyōllōtl", "text": "Tepēyōllōtl\n\nIn Aztec mythology, Tepēyōllōtl (\"heart of the mountains\"; also Tepeyollotli) was the god of earthquakes, echoes and jaguars. He is the god of the Eighth Hour of the Night, and is depicted as a jaguar leaping towards the sun. In the calendar, Tepeyollotl rules over both the third day, Calli (house), and the third trecena, 1-Mazatl (deer). He is the eighth Lord of the Night.\n\nThe word is derived as a compound of the Nahuatl words ' (\"mountain\"), and ' (\"heart\" or \"interior\"). Tepeyollotl is usually depicted as cross-eyed holding the typical white staff with green feathers. Sometimes Tezcatlipoca wore Tepeyollotl for an animal skin or disguise to trick other gods into not knowing who he was.\n"}
{"id": "102451", "url": "https://en.wikipedia.org/wiki?curid=102451", "title": "Tetrode", "text": "Tetrode\n\nA tetrode is a vacuum tube (called \"valve\" in British English) having four active electrodes. The four electrodes in order from the centre are: a thermionic cathode, first and second grids and a plate (called \"anode\" in British English). There are several varieties of tetrodes, the most common being the screen-grid tube and the beam tetrode. In screen-grid tubes and beam tetrodes, the first grid is the control grid and the second grid is the screen grid. In other tetrodes one of the grids is a control grid, while the other may have a variety of functions.\n\nThe tetrode was developed in the 1920s by adding an additional grid to the first amplifying vacuum tube, the triode, to correct limitations of the triode. During the period 1913 to 1927, three distinct types of tetrode valves appeared. All had a normal control grid whose function was to act as a primary control for current passing through the tube, but they differed according to the intended function of the other grid. In order of historical appearance these are: the space-charge grid tube, the bi-grid valve, and the screen-grid tube. The last of these appeared in two distinct variants with different areas of application: the screen-grid valve proper, which was used for medium-frequency, small signal amplification, and the beam tetrode which appeared later, and was used for audio or radio-frequency power amplification. The former was quickly superseded by the rf pentode, while the latter was initially developed as an alternative to the pentode as an audio power amplifying device. The beam tetrode was also developed as a high power radio transmitting tube.\n\nTetrodes were widely used in many consumer electronic devices such as radios, televisions, and audio systems until transistors replaced valves in the 1960s and 70s. Beam tetrodes have remained in use until quite recently in power applications such as audio amplifiers and radio transmitters.\n\nThe tetrode functions in a similar way to the triode, from which it was developed. A current through the heater or filament heats the cathode, which causes it to emit electrons by thermionic emission. A positive voltage is applied between the plate and cathode, causing a flow of electrons from the cathode to plate through the two grids. A varying voltage applied to the control grid can control this current, causing variations in the plate current. With a resistive or other load in the plate circuit, the varying current will result in a varying voltage at the plate. With proper biasing, this voltage will be an amplified (but inverted) version of the AC voltage applied to the control grid, thus the tetrode can provide voltage gain. In the tetrode, the function of the other grid varies according to the type of tetrode; this is discussed below.\n\nThe space charge grid tube was the first type of tetrode to appear. In the course of his research into the action of the \"audion\" triode tube of Lee de Forest, Irving Langmuir found that the action of the heated thermionic cathode was to create a space charge, or cloud of electrons, around the cathode. This cloud acted as a virtual cathode. With low applied anode voltage, many of the electrons in the space charge returned to the cathode, and did not contribute to the anode current; only those at its outer limit would be affected by the electric field due to the anode, and would be accelerated towards it. However, if a grid bearing a low positive applied potential (about 10V) were inserted between the cathode and the control grid, the space charge could be made to extend further away from the cathode. This had two advantageous effects, both related to the influence of the electric fields of the other electrodes (anode and control grid) on the electrons of the space charge. Firstly, a significant increase in anode current could be achieved with low anode voltage; the valve could be made to work well with lower applied anode voltage. Secondly the transconductance (rate of change of anode current with respect to control grid voltage) of the tube was increased. The latter effect was particularly important since it increased the voltage gain available from the valve. \n\nSpace-charge valves remained useful devices throughout the valve era, and were used in applications such as car radios operating directly from a 12V supply, where only a low anode voltage was available. The same principle was applied to other types of multi-grid tubes such as pentodes. As an example, the Sylvania 12K5 is described as \"a tetrode designed for space-charge operation. It is intended for service as a power amplifier driver where the potentials are obtained directly from a 12V automobile battery.\" The space-charge grid was operated at +12V, the same as the anode supply voltage.\nAnother important application of the space-charge tetrode was as an electrometer tube for detecting and measuring extremely small currents. For example, the General Electric FP54 was described as a \"space-charge grid tube ... designed to have a very high input impedance and a very low grid current. It is designed particularly for amplification of direct currents smaller than about 10 amperes, and has been found capable of measuring currents as small as 5 x 10 amperes. It has a current amplification factor of 250,000, and operates with an anode voltage of 12v, and space-charge grid voltage of +4V.\"\n\nNote that when a space-charge grid is added to a triode, the first grid in the resulting tetrode is the space-charge grid, and the second grid is the control grid.\n\nIn the bi-grid type of tetrode, both grids are intended to carry electrical signals, so both are control grids. The first example to appear in Britain was the Marconi-Osram FE1, which was designed by H.J. Round, and became available in 1920. The tube was intended to be used in a reflex circuit (for example the single-valve ship receiver Type 91)where the same valve performed the multiple functions of rf amplifier, af amplifier, and diode detector. The rf signal was applied to one control grid, and the af signal to the other. This type of tetrode was used in many imaginative ways in the period before the appearance of the screen-grid valve revolutionised receiver design.\n\nOne application is shown in the illustration. This is recognisable as an AM telephony transmitter in which the second grid and the anode form a power oscillator, and the first grid acts as a modulating electrode. The anode current in the valve, and hence the RF output amplitude, is modulated by the voltage on G1, which is derived from a carbon microphone.\n\nA tube of this type could also be used as a direct conversion CW receiver. Here the valve oscillates as a consequence of coupling between the first grid and the anode, while the second grid is coupled to the antenna. The AF beat frequency is audible in the headphones. The valve acts as a self-oscillating product detector.\nAnother, very similar application of the bi-grid valve was as a self oscillating frequency mixer in early superhet receivers \nOne control grid carried the incoming rf signal, while the other was connected into an oscillator circuit which generated the local oscillation within the same valve. Since the anode current of the bi-grid valve was proportional both to the signal on the first grid, and also to the oscillator voltage on the second grid, the required multiplication of the two signals was achieved, and the intermediate frequency signal appeared in an appropriately tuned circuit connected to the anode. In all of the applications mentioned, the bi-grid tetrode acted as an analogue multiplier (analog multiplier) which multiplied together the signals applied to the two grids.\n\nThe super-sonic heterodyne (superhet) receiver principle was invented in France by Lucien Levy in 1917 (p 66), though credit is usually also given to Edwin Armstrong. The original reason for the invention of the superhet was that before the appearance of the screen-grid valve, there was no type of valve which could give good gain at radio frequencies (i.e. frequencies much above 100 kHz), so a technique was applied whereby the incoming rf signal was \"mixed\" (i.e. multiplied) with a locally generated oscillatory voltage (the local oscillator) so as to produce a beat frequency at the significantly lower frequency of about 30 kHz. This represented the incoming signal in all important respects, but at a lower frequency, at which it could be successfully amplified by a multistage tuned triode amplifier before being applied to the detector. This was a complicated technique. It went out of use when screen-grid valves appeared which could act as satisfactory rf amplifiers capable of amplifying the incoming signal without lowering its frequency (see Screen grid valve, below). Superhet receivers reappeared in the early 1930s when, because of the proliferation of transmitting stations, their greater selectivity became an important advantage; almost all receivers operate on this principle today, albeit with a higher if frequency.\n\nIn the screen grid class of tetrode, the main function of the second grid is to act as an electrostatic screen between the anode and the control grid (i.e. the first grid) in order to reduce the internal capacitance between control grid and anode. The first true screen-grid valve, with a screen grid designed for this purpose, was patented by Hiroshi Ando in 1919, and the first practical versions were built by N. H. Williams and Albert Hull at General Electric and Bernard Tellegen at Phillips in 1926.\n\nThis type of tetrode was developed to correct deficiencies in the triode tube which became apparent when attempts were made to use triodes as small-signal radio-frequency amplifiers. In the triode, the control grid was next to the plate. Capacitance between these two electrodes caused instability and oscillation when both anode and grid were connected into tuned resonant circuits, as was the case in early radios, or in any application where the anode circuit presented an inductive reactive load to the valve. Oscillation could only be avoided by using very small stage gain at frequencies above about 100 kHz, and at frequencies above 1 MHz, triodes are virtually useless in tuned amplifiers in which both anode and grid circuits are tuned to the same frequency. A second advantage of the tetrode, which also arose from reduced anode-grid capacitance, was that, when the anode voltage was in a certain range, changing its value had much less effect on the anode current than was the case with triodes. This corresponds to an increased anode slope resistance, and allowed much higher voltage gain by permitting a larger external load resistance. At the time of the introduction of screen-grid valves (around 1927) a typical small triode used for small-signal amplification had an anode slope resistance of 20kOhms or less, and a grid-anode capacitance of 1 to 5pF, while the corresponding figures for a typical screen grid valve were 1MOhm and 0.004pF - in some cases much less capacitance.\n\nScreen-grid valves were capable of greater voltage gain and higher frequency capability than triodes, and permitted the development of the first true RF amplifiers in the MF and HF frequency ranges in radio equipment. They were commonly used as the first stage of radio-frequency amplification in domestic radio receivers in the period 1927 to 1930, after which they were superseded in this application by RF pentodes. Triodes were ill-suited to this type of application on account of their relatively high anode-grid capacitance and low anode resistance.\n\nIn normal operation the screen grid is connected to a positive DC voltage slightly less than the plate voltage, and bypassed to the cathode with a capacitor, so it was at AC ground. To take full advantage of the very low internal grid-anode capacitance, the valve must be used in circuits in which the shielding between anode and grid is continued externally. In the case illustrated (S625), the valve was intended to be inserted into a hole in an external, grounded, sheet-metal shield aligned to correspond with the position of the internal screen-grid. The input, or control-grid circuit was on one side of the shield, while the anode, or output circuit was on the other. In the case of the Osram Music Magnet, each entire stage of the 2-stage rf amplifier, as well as the tuned detector stage, was enclosed in an individual large aluminium screening box. These boxes have been removed in the illustration, but the up-turned edges of the bases of the boxes can be seen.\n\nThe reason for the limited applicability of the screen-grid valve, and its rapid replacement by the RF pentode (introduced around 1930) was the peculiar anode characteristic (i.e. variation of anode current with respect to anode voltage) of the former type of tube.\n\nIn normal applications, the anode voltage was about 150V, while that of the screen-grid was about 60V (Thrower p 183). As the screen grid is positive with respect to the cathode, it collects a certain fraction (perhaps a quarter) of the electrons which would otherwise pass from the grid region to the anode. This causes current to flow in the screen grid circuit. Usually, the screen current due to this cause is small, and of little interest. However, if the anode voltage should be below that of the screen, the screen grid can also collect secondary electrons ejected from the anode by the impact of the energetic primary electrons. Both effects tend to reduce the anode current. If the anode voltage is increased from a low value, with the screen grid at its normal operating voltage (60V, say) the anode current initially increases rapidly because more of those electrons which pass through the screen-grid are collected by the anode rather than passing back to the screen grid. This part of the tetrode anode characteristic resembles the corresponding part of that of a triode or pentode. However, when the anode voltage is increased further, the electrons arriving at the anode have sufficient energy to cause copious secondary emission, and many of these secondary electrons will be captured by the screen, which is at a higher positive voltage than the anode. This causes the anode current to fall rather than increase when the anode voltage is increased. In some cases the anode current can actually become negative (current flows out of the anode); this is possible since each primary electron may produce more than one secondary. Falling positive anode current accompanied by rising anode voltage gives the anode characteristic a region of negative slope, and this corresponds to a negative resistance which can cause instability in certain circuits. In a higher range of anode voltage, the anode voltage sufficiently exceeds that of the screen for an increasing proportion of the secondary electrons to be attracted back to the anode, so the anode current increases once more, and the slope of the anode characteristic becomes positive again. In a yet higher range of anode voltages, the anode current becomes substantially constant, since all of the secondary electrons now return to the anode, and the main control of current through the tube is the voltage of the control grid. This is the normal operating mode of the tube.\nThe anode characteristic of a screen-grid valve is thus quite unlike that of a triode. It includes a range of anode voltages, where the anode voltage is less than that of the screen grid, in which there is a distinctive negative resistance characteristic, sometimes called \"tetrode kink\". This is usually undesirable, although it can be exploited as in the dynatron oscillator (Eastman, p 431). The approximately constant-current region of low slope at high anode voltage is also a distinctive characteristic of the tetrode. This is highly desirable, since it corresponds to a high source resistance in the anode circuit, and greatly enhances the voltage gain which the device can produce. Early screen-grid valves had amplification factors (i.e. the product of transconductance and anode slope resistance) ten times that of comparable small-signal triodes. The high anode resistance (i.e. low slope) in the normal operating range is a consequence of the electrostatic shielding action of the screen grid, since it prevents the electric field due to the anode from penetrating to the control grid region, where it might otherwise influence the passage of electrons, increasing the electron current when the anode voltage is high, reducing it when low.\n\nIn practice the negative-resistance kink of the anode characteristic limits the usefulness of the screen-grid valve to applications where only small signals are amplified, so that the variations in anode voltage are also correspondingly small, and the operating conditions never depart from the region of high positive impedance (low positive slope) at large anode voltage. The secondary emission can be prevented from contributing to the screen current (and thus detracting from the anode current) by adding a suppressor grid, making a pentode, or beam plates to make a beam tetrode/kinkless tetrode, which can be used in power amplifiers where the variations of anode voltage, due to the presence of a large signal voltage, are much greater. The anode characteristics of these tubes are very similar to pentodes. In both cases the anode current rises rapidly to an approximately constant value when the anode voltage rises to as little as a few tens of volts, and the region of high positive anode resistance (low slope) extends from this low value up to the maximum anode voltage which could be several hundred volts or more.\n\nThe negative resistance operating region of the tetrode is exploited in the dynatron oscillator, which is an example of a negative resistance oscillator. (Eastman, p431)\n\n \nThe high value of the anode slope resistance of tetrodes (mentioned above) makes them capable of high voltage and power gain, and is also potentially a cause of high anode efficiency which, if it could be exploited, would make tetrodes superior to triodes as power amplifying devices in applications such as audio power amplifiers, and the output stages of radio transmitters. For a triode power amplifier working with a transformer or inductive load in Class A, the maximum theoretical efficiency is 25%. This low figure is in part a consequence of the low anode slope impedance (Ra) of this type of tube; the low value of a triode Ra is almost always much less than the optimal anode load impedance in a power amplifier. For a pentode or tetrode, however, Ra is usually sufficiently high for the optimal load impedance to be achieved, and under these circumstances the maximum theoretical efficiency rises to 50%. This gives these latter types of tube an important practical advantage over triodes, which is of particular value when high power outputs are required.\n\nHowever, the tetrode kink limits the permissible variation of anode voltage, and restricts the use of screen-grid valves to small-signal applications. The suppressor grid of the pentode eliminates the kink in the anode characteristic by preventing secondary electrons, which originate in the anode, from reaching the screen grid, and thus permits a wider excursion of anode voltage, as is required for power amplification. The same effect can be produced in the case of a tetrode by introducing two modifications. Firstly, the wires of the screen grid are aligned with those of the control grid so that the former lie in the electron 'shadow' created by the latter. This reduces the screen grid current, thereby giving greater efficiency, and also concentrates the electrons into dense beams in the space between the screen grid and the anode. The intense negative space charge of these beams prevents secondary electrons from the anode from reaching the screen grid, thus eliminated the tetrode kink. Secondly, in small valves whose electrode structure is supported in the conventional way with vertical wire rods and mica spacers, it was found to be necessary to introduce sheet-metal beam-forming electrodes between the screen grid and the anode. The purpose of these beam-plates is to constrain the electron beams into parts of the electrode system which are sections of a cylinder. (See sectional view, right). Successful creation of the electron beam between screen grid and anode required for a kinkless anode characteristic depends on the details of the geometry of the electrode structure of the beam tetrode. In the cases where the electrodes have complete cylindrical symmetry, a kinkless characteristic can be achieved without the need for beam-plates, alignment of the screen grid wires with those of the control grid being sufficient. This form of construction is usually adopted in the larger tubes with an anode power rating of 100W or more. The Eimac 4CX250B (rated at 250W anode dissipation) is an example of this class of beam tetrode. Note that a radically different approach is taken to the design of the support system for the electrodes in these types (see illustration). The 4CX250B is described by its manufacturer as a 'radial beam tetrode', drawing attention to the symmetry of its electrode system.\n\nThe overall effect of the original developments was to produce a highly effective power amplifier tube, whose anode characteristic is very much like that of a pentode, but which has greater efficiency as a result of reduced screen current. A further bonus was that third harmonic distortion was much reduced relative to a comparable pentode (Terman pp 198–9). Beam tetrode audio power output valves were introduced in 1937, and quickly replaced conventional pentodes in this application. Later developments produced beam power tubes which were capable of high-power output at frequencies extending into the UHF region.\n\nThe beam tetrode was invented in Britain by two EMI engineers, Cabot Bull and Sidney Rodda, as an attempt to circumvent the power pentode, whose patent was owned by Philips. It was patented in 1933.\nAlthough the beam-plates (when present) could be counted as a fifth electrode (as in a pentode), this type of tube is nevertheless classified as a tetrode, perhaps to underline the difference in principle from that employed in true pentodes, which rely upon the effect of a suppressor grid. Beam tetrodes were widely used as audio power amplifying tubes in consumer and industrial electronic equipment such as radios and televisions until the 1960s when they were replaced by transistors. Their main use now is in high power industrial applications such as radio transmitters. Low power consumer beam tetrodes are still used in a few legacy and specialty vacuum tube audio power amplifier devices such as tube guitar amplifiers; the KT66 and KT88 are popular examples in audio equipment, while QY4-400 is an example having 400W anode dissipation, capable of applications in radio transmitters up to 100 MHz. The 4CX250B, mentioned above can be operated at full 250W anode dissipation up to 500 MHz. Many other types abound.\n\nAn alternative approach to the problem of eliminating the tetrode kink was introduced by Hivac in 1935. It was found by J.H Owen Harries that if the separation of the anode from the screen grid were varied, a critical separation could be found (about 3 cm) where the kink in the tetrode's anode characteristic disappeared, and the amplification of the valve became particularly distortion-free. Both fidelity and efficiency exceeded those of the available pentodes of the time. A range of tetrodes of this type were introduced, aimed at the domestic receiver market, some having 2V directly heated filaments, intended for low-power battery-operated sets, others with 4V or higher, indirectly heated cathodes for mains operation. Output powers ranged from 0.5W to 11.5W . Confusingly, several of these new valves bore the same type number as existing pentodes with almost identical characteristics. Examples include Y220 (0.5W, 2V filament), AC/Y (3W, 4V heater), AC/Q (11.5W, 4V heater), etc.\n\n"}
{"id": "28812491", "url": "https://en.wikipedia.org/wiki?curid=28812491", "title": "Ventilation air methane thermal oxidizer", "text": "Ventilation air methane thermal oxidizer\n\nVentilation air methane thermal oxidizers (or VAMTOX) are a type of processing equipment used for greenhouse gas abatement related to underground mining operations that destroys gaseous methane at a high temperature.\n\nVentilation Air Methane Thermal Oxidizers are used to destroy methane in the exhaust air of underground coal mine shafts. Methane is a greenhouse gas that burns to form carbon dioxide (CO) and water vapour. Carbon dioxide is 25 times less potent than methane when emitted into the atmosphere with regards to global warming. Concentrations of methane in ventilation exhaust air of coal and trona mines are very dilute; typically below 1% and often below 0.5%. Flow rates are so high that ventilation air methane constitutes the largest source of methane emissions at most mines. This methane emission wastes energy and contributes significantly to global greenhouse gas (GHG) emissions.\n\nThermal oxidation is the most widely accepted air pollution control technologies used in industrial applications. Ventilation Air Methane Thermal Oxidizers are commonly referred to as a VAMTOX. They are very specific and extremely efficient – energy recovery efficiency can reach 95%. This is achieved through the storage of heat in dense ceramic stoneware. Ventilation Air Methane Thermal Oxidizers are used for the very low methane concentrations operate continuously. These systems can destroy 95-98+% methane (CH) that would otherwise be emitted. Ventilation Air Methane Thermal Oxidizers can be designed with hot gas bypass systems, re-circulation heat exchangers that convert heat into energy, and oxygen monitoring to reduce any possible carbon monoxide and/or nitrous oxide production. Methane streams allow the VAMTOX to operate at reduced or zero fuel usage, which makes these systems ideal for mine shaft ventilation operations.\n\nVAMTOX systems have a system of valves and dampers that direct the methane flow across the ceramic bed. On system start up, the system preheats and raises the temperature of the heat exchange material in the oxidizer bed to or above the auto-oxidation temperature of methane (1,000 °C or 1,832 °F). Then the preheating system is turned off and mine exhaust air is introduced. When the methane-filled air reaches the preheated bed, it oxidizes and releases heat. This heat is transferred to the bed, thereby maintaining its temperature to support continued operation. The oxidation process is flameless. Once the bed is preheated, the process needs no auxiliary energy so long as adequate inflow methane concentrations are maintained. The VAMTOX system exhaust gases can be used to raise steam, which can provide electrical power through a turbine generator.\n\nWorks cited\n\n"}
{"id": "25339735", "url": "https://en.wikipedia.org/wiki?curid=25339735", "title": "Weeping tree", "text": "Weeping tree\n\nWeeping trees are characterized by soft, limp twigs. This characterization may lead to a bent crown and pendulous branches that can cascade to the ground. While weepyness occurs in nature, most weeping trees are cultivars. Because of their shape, weeping trees are popular in landscaping; generally they need a lot of space and are solitary so that their effect is more pronounced. There are over a hundred different types of weeping trees. Some trees, such as the cherry, have a variety of weeping cultivars.\nThere are currently around 550 weeping cultivars in 75 different genera, although many have now disappeared from cultivation.\n\n\n\n\n"}
{"id": "20541773", "url": "https://en.wikipedia.org/wiki?curid=20541773", "title": "Wind turbine", "text": "Wind turbine\n\nA wind turbine, or alternatively referred to as a wind energy converter, is a device that converts the wind's kinetic energy into electrical energy.\n\nWind turbines are manufactured in a wide range of vertical and horizontal axis. The smallest turbines are used for applications such as battery charging for auxiliary power for boats or caravans or to power traffic warning signs. Slightly larger turbines can be used for making contributions to a domestic power supply while selling unused power back to the utility supplier via the electrical grid. Arrays of large turbines, known as wind farms, are becoming an increasingly important source of intermittent renewable energy and are used by many countries as part of a strategy to reduce their reliance on fossil fuels. One assessment claimed that, , wind had the \"lowest relative greenhouse gas emissions, the least water consumption demands and... the most favourable social impacts\" compared to photovoltaic, hydro, geothermal, coal and gas.\n\nThe windwheel of Hero of Alexandria (10 AD – 70 AD) marks one of the first recorded instances of wind powering a machine in history. However, the first known practical wind power plants were built in Sistan, an Eastern province of Persia (now Iran), from the 7th century. These \"Panemone\" were vertical axle windmills, which had long vertical drive shafts with rectangular blades. Made of six to twelve sails covered in reed matting or cloth material, these windmills were used to grind grain or draw up water, and were used in the gristmilling and sugarcane industries.\n\nWind power first appeared in Europe during the Middle Ages. The first historical records of their use in England date to the 11th or 12th centuries and there are reports of German crusaders taking their windmill-making skills to Syria around 1190. By the 14th century, Dutch windmills were in use to drain areas of the Rhine delta. Advanced wind turbines were described by Croatian inventor Fausto Veranzio. In his book Machinae Novae (1595) he described vertical axis wind turbines with curved or V-shaped blades.\n\nThe first electricity-generating wind turbine was a battery charging machine installed in July 1887 by Scottish academic James Blyth to light his holiday home in Marykirk, Scotland. Some months later American inventor Charles F. Brush was able to build the first automatically operated wind turbine after consulting local University professors and colleagues Jacob S. Gibbs and Brinsley Coleberd and successfully getting the blueprints peer-reviewed for electricity production in Cleveland, Ohio. Although Blyth's turbine was considered uneconomical in the United Kingdom, electricity generation by wind turbines was more cost effective in countries with widely scattered populations.\n\nIn Denmark by 1900, there were about 2500 windmills for mechanical loads such as pumps and mills, producing an estimated combined peak power of about 30 (MW). The largest machines were on towers with four-bladed diameter rotors. By 1908 there were 72 wind-driven electric generators operating in the United States from 5 kW to 25 kW. Around the time of World War I, American windmill makers were producing 100,000 farm windmills each year, mostly for water-pumping.\n\nBy the 1930s, wind generators for electricity were common on farms, mostly in the United States where distribution systems had not yet been installed. In this period, high-tensile steel was cheap, and the generators were placed atop prefabricated open steel lattice towers.\n\nA forerunner of modern horizontal-axis wind generators was in service at Yalta, USSR in 1931. This was a 100 kW generator on a tower, connected to the local 6.3 kV distribution system. It was reported to have an annual capacity factor of 32 percent, not much different from current wind machines.\n\nIn the autumn of 1941, the first megawatt-class wind turbine was synchronized to a utility grid in Vermont. The Smith-Putnam wind turbine only ran for 1,100 hours before suffering a critical failure. The unit was not repaired, because of a shortage of materials during the war.\n\nThe first utility grid-connected wind turbine to operate in the UK was built by John Brown & Company in 1951 in the Orkney Islands.\n\nDespite these diverse developments, developments in fossil fuel systems almost entirely eliminated any wind turbine systems larger than supermicro size. In the early 1970s, however, anti-nuclear protests in Denmark spurred artisan mechanics to develop microturbines of 22 kW. Organizing owners into associations and co-operatives lead to the lobbying of the government and utilities and provided incentives for larger turbines throughout the 1980s and later. Local activists in Germany, nascent turbine manufacturers in Spain, and large investors in the United States in the early 1990s then lobbied for policies that stimulated the industry in those countries.\n\nWind Power Density (WPD) is a quantitative measure of wind energy available at any location. It is the mean annual power available per square meter of swept area of a turbine, and is calculated for different heights above ground. Calculation of wind power density includes the effect of wind velocity and air density.\n\nWind turbines are classified by the wind speed they are designed for, from class I to class III, with A to C referring to the turbulence intensity of the wind. \n\nConservation of mass requires that the amount of air entering and exiting a turbine must be equal. Accordingly, Betz's law gives the maximal achievable extraction of wind power by a wind turbine as 16/27 (59.3%) of the total kinetic energy of the air flowing through the turbine.\n\nThe maximum theoretical power output of a wind machine is thus 16/27 times the kinetic energy of the air passing through the effective disk area of the machine. If the effective area of the disk is A, and the wind velocity v, the maximum theoretical power output P is:\n\nwhere \"ρ\" is the air density.\n\nWind-to-rotor efficiency (including rotor blade friction and drag) are among the factors impacting the final price of wind power.\nFurther inefficiencies, such as gearbox losses, generator and converter losses, reduce the power delivered by a wind turbine. To protect components from undue wear, extracted power is held constant above the rated operating speed as theoretical power increases at the cube of wind speed, further reducing theoretical efficiency. In 2001, commercial utility-connected turbines deliver 75% to 80% of the Betz limit of power extractable from the wind, at rated operating speed.\n\nEfficiency can decrease slightly over time, one of the main reasons being dust and insect carcasses on the blades which alters the aerodynamic profile and essentially reduces the lift to drag ratio of the airfoil. Analysis of 3128 wind turbines older than 10 years in Denmark showed that half of the turbines had no decrease, while the other half saw a production decrease of 1.2% per year. Ice accretion on turbine blades has also been found to greatly reduce the efficiency of wind turbines, which is a common challenge in cold climates where in-cloud icing and freezing rain events occur. Vertical turbine designs have much lower efficiency than standard horizontal designs.\n\nWind turbines can rotate about either a horizontal or a vertical axis, the former being both older and more common. They can also include blades, or be bladeless. Vertical designs produce less power and are less common.\n\nLarge three-bladed horizontal-axis wind turbines (HAWT), with the blades upwind of the tower produce the overwhelming majority of windpower in the world today. These turbines have the main rotor shaft and electrical generator at the top of a tower, and must be pointed into the wind. Small turbines are pointed by a simple wind vane, while large turbines generally use a wind sensor coupled with a yaw system. Most have a gearbox, which turns the slow rotation of the blades into a quicker rotation that is more suitable to drive an electrical generator. Some turbines use a different type of generator suited to slower rotational speed input. These don't need a gearbox, and are called direct-drive, meaning they couple the rotor directly to the generator with no gearbox in between. While permanent magnet direct-drive generators can be more costly due to the rare earth materials required, these gearless turbines are sometimes preferred over gearbox generators because they \"eliminate the gear-speed increaser, which is susceptible to significant accumulated fatigue torque loading, related reliability issues, and maintenance costs.\" \nMost horizontal axis turbines have their rotors upwind of its supporting tower. Downwind machines have been built, because they don't need an additional mechanism for keeping them in line with the wind. In high winds, the blades can also be allowed to bend which reduces their swept area and thus their wind resistance. Despite these advantages, upwind designs are preferred, because the change in loading from the wind as each blade passes behind the supporting tower can cause damage to the turbine.\n\nTurbines used in wind farms for commercial production of electric power are usually three-bladed. These have low torque ripple, which contributes to good reliability. The blades are usually colored white for daytime visibility by aircraft and range in length from . The size and height of turbines increase year by year. Offshore wind turbines are built up to 8(MW) today and have a blade length up to . Usual tubular steel towers of multi megawatt turbines have a height of 70m to 120m and in extremes up to 160m.\n\nVertical-axis wind turbines (or VAWTs) have the main rotor shaft arranged vertically. One advantage of this arrangement is that the turbine does not need to be pointed into the wind to be effective, which is an advantage on a site where the wind direction is highly variable. It is also an advantage when the turbine is integrated into a building because it is inherently less steerable. Also, the generator and gearbox can be placed near the ground, using a direct drive from the rotor assembly to the ground-based gearbox, improving accessibility for maintenance. However, these designs produce much less energy averaged over time, which is a major drawback.\n\nThe key disadvantages include the relatively low rotational speed with the consequential higher torque and hence higher cost of the drive train, the inherently lower power coefficient, the 360-degree rotation of the aerofoil within the wind flow during each cycle and hence the highly dynamic loading on the blade, the pulsating torque generated by some rotor designs on the drive train, and the difficulty of modelling the wind flow accurately and hence the challenges of analysing and designing the rotor prior to fabricating a prototype.\n\nWhen a turbine is mounted on a rooftop the building generally redirects wind over the roof and this can double the wind speed at the turbine. If the height of a rooftop mounted turbine tower is approximately 50% of the building height it is near the optimum for maximum wind energy and minimum wind turbulence. While wind speeds within the built environment are generally much lower than at exposed rural sites, noise may be a concern and an existing structure may not adequately resist the additional stress.\n\nSubtypes of the vertical axis design include:\n\n\"Eggbeater\" turbines, or Darrieus turbines, were named after the French inventor, Georges Darrieus. They have good efficiency, but produce large torque ripple and cyclical stress on the tower, which contributes to poor reliability. They also generally require some external power source, or an additional Savonius rotor to start turning, because the starting torque is very low. The torque ripple is reduced by using three or more blades which results in greater solidity of the rotor. Solidity is measured by blade area divided by the rotor area. Newer Darrieus type turbines are not held up by guy-wires but have an external superstructure connected to the top bearing.\n\nA subtype of Darrieus turbine with straight, as opposed to curved, blades. The cycloturbine variety has variable pitch to reduce the torque pulsation and is self-starting. The advantages of variable pitch are: high starting torque; a wide, relatively flat torque curve; a higher coefficient of performance; more efficient operation in turbulent winds; and a lower blade speed ratio which lowers blade bending stresses. Straight, V, or curved blades may be used.\n\nThese are drag-type devices with two (or more) scoops that are used in anemometers, \"Flettner\" vents (commonly seen on bus and van roofs), and in some high-reliability low-efficiency power turbines. They are always self-starting if there are at least three scoops.\n\nTwisted Savonius is a modified savonius, with long helical scoops to provide smooth torque. This is often used as a rooftop windturbine and has even been adapted for ships.\n\nThe parallel turbine is similar to the crossflow fan or centrifugal fan. It uses the ground effect. Vertical axis turbines of this type have been tried for many years: a unit producing 10 kW was built by Israeli wind pioneer Bruce Brill in the 1980s.\n\nWind turbine design is a careful balance of cost, energy output, and fatigue life. These factors are balanced using a range of computer modelling techniques.\n\nWind turbines convert wind energy to electrical energy for distribution. Conventional horizontal axis turbines can be divided into three components:\n\nA 1.5 (MW) wind turbine of a type frequently seen in the United States has a tower high. The rotor assembly (blades and hub) weighs . The nacelle, which contains the generator, weighs . The concrete base for the tower is constructed using reinforcing steel and contains of concrete. The base is in diameter and thick near the center.\n\nDue to data transmission problems, structural health monitoring of wind turbines is usually performed using several accelerometers and strain gages attached to the nacelle to monitor the gearbox and equipments. Currently, digital image correlation and stereophotogrammetry are used to measure dynamics of wind turbine blades. These methods usually measure displacement and strain to identify location of defects. Dynamic characteristics of non-rotating wind turbines have been measured using digital image correlation and photogrammetry. Three dimensional point tracking has also been used to measure rotating dynamics of wind turbines.\n\nMaterials that are typically used for the rotor blades in wind turbines are composites, as they tend to have a high stiffness, high strength, high fatigue resistance, and low weight. Typical resins used for these composites include polyester and epoxy, while glass and carbon fibers have been used for the reinforcing material. Construction may use manual layup techniques or composite resin injection molding. As the price of glass fibers is only about one tenth the price of carbon fiber, glass fiber is still dominant.\n\nAs competition in the wind market increases, companies are seeking ways to draw greater efficiency from their designs. One of the predominant ways wind turbines have gained performance is by increasing rotor diameters, and thus blade length. Retrofitting current turbines with larger blades mitigates the need and risks associated with a system-level redesign. As the size of the blade increases, its tendency to deflect also increases. Thus, from a materials perspective, the stiffness-to-weight is of major importance. As the blades need to function over a 100 million load cycles over a period of 20–25 years, the fatigue life of the blade materials is also of utmost importance. By incorporating carbon fiber into parts of existing blade systems, manufacturers may increase the length of the blades without increasing their overall weight. For instance, the spar cap, a structural element of a turbine blade, commonly experiences high tensile loading, making it an ideal candidate to utilize the enhanced tensile properties of carbon fiber in comparison to glass fiber. Higher stiffness and lower density translates to thinner, lighter blades offering equivalent performance. In a 10 (MW) turbine—which will become more common in offshore systems by 2021—blades may reach over 100 m in length and weigh up to 50 metric tons when fabricated out of glass fiber. A switch to carbon fiber in the structural spar of the blade yields weight savings of 20 to 30 percent, or approximately 15 metric tons.\n\nSome of the most common materials which are being used for turbine blades now and will be in the future are summarized below:\n\nThe stiffness of composites is determined by the stiffness of fibers and their volume content. Typically, E-glass fibers are used as main reinforcement in the composites. Typically, the glass/epoxy composites for wind blades contain up to 75 weight % glass. This increases the stiffness, tensile and compression strength. A promising source of the composite materials in the future is glass fibers with modified compositions like S-glass, R-glass etc. Some other special glasses developed by Owens Corning are ECRGLAS, Advantex and most recently WindStrand glass fibers. \n\nThese include E-glass/carbon, E-glass/aramid and they present an exciting alternative to pure glass or carbon reinforcements. that the full replacement would lead to 80% weight savings, and cost increase by 150%, while a partial (30%) replacement would lead to only 90% cost increase and 50% weight reduction for 8 m turbine. The world currently longest wind turbine rotor blade, the 88.4 m long blade from LM Wind Power is made of carbon/glass hybrid composites. However, additional investigations are required for the optimal composition of the materials \n\nAdditions of small amount (0.5 weight %) of nanoreinforcement (carbon nanotubes or nanoclay in the polymer matrix of composites, fiber sizing or interlaminar layers can allow to increase the fatigue resistance, shear or compressive strength as well as fracture toughness of the composites by 30–80%. Research has also shown that the incorporation of small amount of carbon nanotubes/CNT can increase the lifetime up to 1500%.\n\nWhile the material cost is significantly higher for all-glass fiber blades than for hybrid glass/carbon fiber blades, there is a potential for tremendous savings in manufacturing costs when labor price is considered. Utilizing carbon fiber enables for simpler designs that use less raw material. The chief manufacturing process in blade fabrication is the layering of plies. By reducing the number of layers of plies, as is enabled by thinner blade design, the cost of labor may be decreased, and in some cases, equate to the cost of labor for glass fiber blades.\n\nMaterials for wind turbine parts other than the rotor blades (including the rotor hub, gearbox, frame, and tower) are largely composed of steel. Modern turbines use a couple of tons of copper for generators, cables, and such. Smaller wind turbines have begun incorporating more aluminum based alloys into these components in an effort to make the turbines lighter and more efficient, and may continue to be used increasingly if fatigue and strength properties can be improved. Prestressed concrete has been increasingly used for the material of the tower, but still requires much reinforcing steel to meet the strength requirement of the turbine. Additionally, step-up gearboxes are being increasingly replaced with variable speed generators, increasing the demand for magnetic materials in wind turbines. In particular, this would require an increased supply of the rare earth metal neodymium.\n\nInterest in recycling blades varies in different markets and depends on the waste legislation and local economics. A challenge in recycling blades is related to the composite material, which is made of a thermosetting matrix and glass fibers or a combination of glass and carbon fibers. Thermosetting matrix cannot be remolded to form new composites. So the options are either to reuse the blade and the composite material elements as they are found in the blade or to transform the composite material into a new source of material. \nIn Germany, wind turbine blades are commercially recycled as part of an alternative fuel mix for a cement factory.\n\nA study of the material consumption trends and requirements for wind energy in Europe found that bigger turbines have a higher consumption of precious metals but lower material input per kW generated. The current material consumption and stock was compared to input materials for various onshore system sizes. In all EU countries the estimates for 2020 exceeded and doubled the values consumed in 2009. These countries would need to expand their resources to be able to meet the estimated demand for 2020. For example, currently the EU has 3% of world supply of fluorspar and it requires 14% by 2020. Globally, the main exporting countries are South Africa, Mexico and China. This is similar with other critical and valuable materials required for energy systems such as magnesium, silver and indium. In addition, the levels of recycling of these materials is very low and focusing on that could alleviate issues with supply in the future. It is important to note that since most of these valuable materials are also used in other emerging technologies, like LEDs, PVs and LCDs, it is projected that demand for them will continue to increase.\n\nA report by the United States Geological Survey estimated the projected materials requirement in order to fulfill the US commitment to supplying 20% of its electricity from wind power by 2030. They did not address requirements for small turbines or offshore turbines since those were not widely deployed in 2008, when the study was created. They found that there are increases in common materials such as cast iron, steel and concrete that represent 2–3% of the material consumption in 2008. Between 110,000 and 115,000 metric tons of fiber glass would be required annually, equivalent to 14% of consumption in 2008. They did not see a high increase in demand for rare metals compared to available supply, however rare metals that are also being used for other technologies such as batteries which are increasing its global demand need to be taken into account. Land, whbich might not be considered a material, is an important resource in deploying wind technologies. Reaching the 2030 goal would require 50,000 square kilometers of onshore land area and 11,000 square kilometers of offshore. This is not considered a problem in the US due to its vast area and the ability to use land for farming and grazing. A greater limitation for the technology would be the variability and transmission infrastructure to areas of higher demand.\n\nPermanent magnets for wind turbine generators contain rare earth metals such as Nd, Pr, Tb, and Dy. Systems that use magnetic direct drive turbines require higher amounts of rare metals. Therefore, an increase in wind production would increase the demand for these resources. It is estimated that the additional demand for Nd in 2035 may be 4,000 to 18,000 tons and Dy could see an increase of 200 to 1200 tons. These values represent a quarter to half of current production levels. However, since technologies are developing rapidly, driven by supply and price of materials these estimated levels are extremely uncertain.\n\nReliance on rare earth minerals for components has risked expense and price volatility as China has been main producer of rare earth minerals (96% in 2009) and had been reducing its export quotas of these materials. In recent years, however, other producers have increased production of rare earth minerals and China has removed its reduced export quota on rare earths leading to an increased supply and decreased cost of rare earth minerals, increasing the viability of the implementation of variable speed generators in wind turbines on a large scale.\n\nDue to increased technology and wide implementation, the global glass fiber market might reach US$17.4 billion by 2024, compared to US$8.5 billion in 2014. Since it is the most widely used material for reinforcement in composites around the globe, the expansion of end use applications such as construction, transportation and wind turbines has fueled its popularity. Asia Pacific held the major share of the global market in 2014 with more than 45% volume share. However China is currently the largest producer. The industry receives subsidies from the Chinese government allowing them to export it cheaper to the US and Europe. However, due to the higher demand in the near future some price wars have started to developed to implement anti dumping strategies such as tariffs on Chinese glass fiber.\n\nA few localities have exploited the attention-getting nature of wind turbines by placing them on public display, either with visitor centers around their bases, or with viewing areas farther away. The wind turbines are generally of conventional horizontal-axis, three-bladed design, and generate power to feed electrical grids, but they also serve the unconventional roles of technology demonstration, public relations, and education.\n\nSmall wind turbines may be used for a variety of applications including on- or off-grid residences, telecom towers, offshore platforms, rural schools and clinics, remote monitoring and other purposes that require energy where there is no electric grid, or where the grid is unstable. Small wind turbines may be as small as a fifty-watt generator for boat or caravan use. Hybrid solar and wind powered units are increasingly being used for traffic signage, particularly in rural locations, as they avoid the need to lay long cables from the nearest mains connection point. The U.S. Department of Energy's National Renewable Energy Laboratory (NREL) defines small wind turbines as those smaller than or equal to 100 kilowatts. Small units often have direct drive generators, direct current output, aeroelastic blades, lifetime bearings and use a vane to point into the wind.\n\nLarger, more costly turbines generally have geared power trains, alternating current output, flaps and are actively pointed into the wind. Direct drive generators and aeroelastic blades for large wind turbines are being researched.\n\nOn most horizontal wind turbine farms, a spacing of about 6–10 times the rotor diameter is often upheld. However, for large wind farms distances of about 15 rotor diameters should be more economical, taking into account typical wind turbine and land costs. This conclusion has been reached by research conducted by Charles Meneveau of the Johns Hopkins University, and Johan Meyers of Leuven University in Belgium, based on computer simulations that take into account the detailed interactions among wind turbines (wakes) as well as with the entire turbulent atmospheric boundary layer.\n\nRecent research by John Dabiri of Caltech suggests that vertical wind turbines may be placed much more closely together so long as an alternating pattern of rotation is created allowing blades of neighbouring turbines to move in the same direction as they approach one another.\n\nWind turbines need regular maintenance to stay reliable and available, in the best case turbines are available to generate energy 98% of the time.\n\nModern turbines usually have a small onboard crane for hoisting maintenance tools and minor components. However, large heavy components like generator, gearbox, blades and so on are rarely replaced and a heavy lift external crane is needed in those cases. If the turbine has a difficult access road, a containerized crane can be lifted up by the internal crane to provide heavier lifting.\n\nInstallation of new wind turbines can be controversial. An alternative is repowering, where existing wind turbines are replaced with bigger, more powerful ones, sometimes in smaller numbers while keeping or increasing capacity.\n\nOlder turbines were in some early cases not required to be removed when reaching the end of their life. Some still stand, waiting to be recycled or repowered.\n\nA demolition industry develops to recycle offshore turbines at a cost of DKK 2–4 million per (MW), to be guaranteed by the owner.\n\nWind turbines are generally inexpensive. They will produce electricity at between two and six cents per kilowatt hour, which is one of the lowest-priced renewable energy sources. And as technology needed for wind turbines continues to improve, the prices will decrease as well. In addition, there is no competitive market for wind energy, as it does not cost money to get ahold of wind. The main cost of wind turbines are the installation process. The average cost is between $48,000 and $65,000 to install. However, the energy harvested from the turbine will offset the installation cost, as well as provide virtually free energy for years after.\n\nWind turbines provide a clean energy source, emitting no greenhouse gases and no waste product. Over 1,500 tons of carbon dioxide per year can be eliminated by using a one megawatt turbine instead of one megawatt of energy from a fossil fuel. Being environmentally friendly and green is a large advantage of wind turbines.\n\nWind turbines can be very large, reaching over tall and with blades long, and people have often complained about their visual impact.\n\nEnvironmental impact of wind power includes effect on wildlife, but can be mitigated if proper monitoring and mitigation strategies are implemented. Thousands of birds, including rare species, have been killed by the blades of wind turbines, though wind turbines contribute relatively insignificantly to anthropogenic avian mortality. For every bird killed by a wind turbine in the US, nearly 500,000 are killed by each of feral cats and buildings. In comparison, conventional coal fired generators contribute significantly more to bird mortality, by incineration when caught in updrafts of smoke stacks and by poisoning with emissions byproducts (including particulates and heavy metals downwind of flue gases). Further, marine life is affected by water intakes of steam turbine cooling towers (heat exchangers) for nuclear and fossil fuel generators, by coal dust deposits in marine ecosystems (e.g. damaging Australia's Great Barrier Reef) and by water acidification from combustion monoxides.\n\nEnergy harnessed by wind turbines is intermittent, and is not a \"dispatchable\" source of power; its availability is based on whether the wind is blowing, not whether electricity is needed. Turbines can be placed on ridges or bluffs to maximize the access of wind they have, but this also limits the locations where they can be placed. In this way, wind energy is not a particularly reliable source of energy. However, it can form part of the energy mix, which also includes power from other sources. Notably, the relative available output from wind and solar sources is often inversely proportional (balancing). Technology is also being developed to store excess energy, which can then make up for any deficits in supplies.\n\n\n\n"}
