{"id": "38188983", "url": "https://en.wikipedia.org/wiki?curid=38188983", "title": "(471240) 2011 BT15", "text": "(471240) 2011 BT15\n\n, provisional designation , is a stony, sub-kilometer sized asteroid and fast rotator, classified as a near-Earth object and potentially hazardous asteroid of the Apollo group. It had been one of the objects with the highest impact threat on the Palermo Technical Impact Hazard Scale.\n\nIt was discovered on 24 January 2011, by a team of astronomers at Pan-STARRS, the \"Panoramic Survey Telescope and Rapid Response System\" at Haleakala Observatory on Hawaii, United States. The discovery was made using a 1.8-meter Ritchey–Chrétien telescope. At the time of discovery, the object had an apparent magnitude of 22.\n\nBefore the 2013 recovery it had an observation arc of 41 days with an uncertainty parameter of 7. Due to precovery images from 2007 it now has an observation arc of more than 5 years. It makes close approaches to Earth and Mars.\n\nOn 28 December 2013, it passed from Earth. The December 2013 passage was studied by the Goldstone Deep Space Network and further refined the orbit.\n\nWhile listed on the Sentry Risk Table, virtual clones of the asteroid that fit the uncertainty region in the known trajectory showed a 1 in 71,000 chance that the asteroid could impact Earth on 5 January 2080.\n\nIn 2013 it had the 5th highest impact threat on the Palermo Technical Impact Hazard Scale. It was removed from the Sentry Risk Table on 17 June 2013.\n\nWith a 2080 Palermo Technical Scale of −3.58, the odds of impact by in 2080 were about 3800 times less than the background hazard level of Earth impacts which is defined as the average risk posed by objects of the same size or larger over the years until the date of the potential impact. JPL Horizons shows that the nominal pass will be on 17 January 2080 at a distance of from Earth.\n\nThis near-Earth object is characterized as a common, stony S-type asteroid by the \"Collaborative Asteroid Lightcurve Link\" (CALL).\n\nIn January 2014, a rotational lightcurve of was obtained from photometric observations by American astronomer Brian Warner at the CS3-Palmer Divide Station () in California. Lightcurve analysis gave a well-defined rotation period of 0.109138 hours (393 seconds) with a brightness amplitude of 0.61 magnitude ().\n\nAccording to the CALL and JPL's impact-risk table, this near-Earth object measures 136 and 150 meters, respectively. For its size estimate, CALL uses a standard for stony asteroids of 0.20 with an absolute magnitude of 21.7.\n\nAs of 2017, this minor planet remains unnamed.\n\n"}
{"id": "19658495", "url": "https://en.wikipedia.org/wiki?curid=19658495", "title": "2008 TC3", "text": "2008 TC3\n\nIt was the first time that an asteroid impact had been predicted prior to its entry into the atmosphere as a meteor.\n\nThe asteroid was discovered by Richard A. Kowalski at the Catalina Sky Survey (CSS) 1.5-meter telescope at Mount Lemmon, north of Tucson, Arizona, US, on October 6, 06:39 UTC, 19 hours before the impact.\n\nIt was notable as the first such body to be observed and tracked prior to reaching Earth. The process of detecting and tracking a near-Earth object, an effort sometimes referred to as Spaceguard, was put to the test. In total, 586 astrometric and almost as many photometric observations were performed by 27 amateur and professional observers in less than 19 hours and reported to the Minor Planet Center, which in eleven hours issued 25 Minor Planet Electronic Circulars with new orbit solutions as observations poured in. On October 7, 01:49 UTC, the asteroid entered the shadow of the Earth, which made further observations impossible.\n\nImpact predictions were performed by University of Pisa's CLOMON 2 semi-automatic monitoring system as well as Jet Propulsion Laboratory's Sentry system. Spectral observations that were performed by astronomers at the 4.2-meter William Herschel Telescope at La Palma, Canary Islands are consistent with either a C-type or M-type asteroid.\n\nThe meteor entered Earth's atmosphere above northern Sudan at 02:46 UTC (05:46 local time) on October 7, 2008 with a velocity of at an azimuth of 281 degrees and an altitude angle of 19 degrees to the local horizon. It exploded tens of kilometers above the ground with the energy of 0.9 to 2.1 kilotons of TNT over a remote area of the Nubian Desert, causing a large fireball or bolide.\n\n\"The Times\" reported that the meteor's \"light was so intense that it lit up the sky like a full moon and an airliner away reported seeing the bright flash.\" A webcam captured the flash lighting up El-Gouna beach 725 kilometres north of the explosion (see ). A low-resolution image of the explosion was captured by the weather satellite Meteosat 8. The Meteosat images place the fireball at . Infrasound detector arrays in Kenya also detected a sound wave from the direction of the expected impact corresponding to energy of 1.1 to 2.1 kilotons of TNT. Asteroids of this size hit Earth about two or three times a year.\n\nThe trajectory showed intersection with Earth's surface at roughly though the object was expected to break up perhaps west as it descended, somewhat east of the Nile River, and about south of the Egypt–Sudan border.\n\nAccording to U.S. government sources U.S. satellites detected the impact at 02:45:40 UT, with the initial detection at at altitude and final explosion at at altitude. These images have not been publicly released.\n\nA search of the impact zone that began on December 6, 2008, turned up of rock in some 600 fragments. These meteorites are collectively named Almahata Sitta, which means \"Station Six\" in Arabic and is a train station between Wadi Halfa and Khartoum, Sudan. This search was led by Peter Jenniskens from the SETI Institute, California and Muawia Shaddad of the University of Khartoum in Sudan and carried out with the collaboration of students and staff of the University of Khartoum. The initial 15 meteorites were found in the first three days of the search. Numerous witnesses were interviewed, and the hunt was guided with a search grid and specific target area produced by NASA's Jet Propulsion Laboratory in Pasadena, California.\n\nSamples of the Almahata Sitta meteorite were sent for analysis to a consortium of researchers led by Jenniskens, the Almahata Sitta consortium, including NASA Ames in California, the Johnson Space Center in Houston, the Carnegie Institution of Washington, and Fordham University in New York City. The first sample measured was an anomalous ultra-fine-grained porous polymict ureilite achondrite, with large carbonaceous grains. Reflectance spectra of the meteorite, combined with the astronomical observations, identified asteroid 2008 TC as an F-type asteroid class. These fragile anomalous dark carbon-rich ureilites are now firmly linked to the group of F-class asteroids. Amino acids have been found on the meteorite. The nanodiamonds found in the meteorite were shown to have grown slowly, implying that the source is another planet in the solar system.\n\nRichard Kowalski, who discovered the object, received a tiny fragment of Almahatta Sitta, a gift from friends and well-wishers on the Minor Planet Mailing List, which Kowalski founded in order to help connect professional and amateur astronomers.\n\n\n"}
{"id": "12534519", "url": "https://en.wikipedia.org/wiki?curid=12534519", "title": "Amplitude adjusting", "text": "Amplitude adjusting\n\nThe Amplitude adjusting (also referred to as Amplitude control) enables the power control of electric loads, which are operated with AC voltage. A representative application is the heating control of industrial high temperature furnaces.\n\nContrary to the conventional phase angle or full wave control, \nduring amplitude control only the Amplitude of the sinusoidal supply current is changed. The level of the amplitude only depends on the consumed power. The sinus oscillation does not change.\n\nBecause current and voltage are in phase, only real power is taken from the mains for amplitude control. So the current consumption from the mains is considerably lower than the current consumption in case of phase-angle operation.\n\nThe continuous current flow causes a mild operation of the used heater elements and consequently significant longer lifetimes are realized. Depending on the ambient conditions the lifetime can be twice as long. \n\nEspecially the surface damage of the heater elements at thresholds can be reduced.\n\nThe amplitude control eliminates the flicker effects and harmonics, as usual for Thyristor units, so that the standard specifications according to EN 61000-3-2 and EN 61000-3-3 are observed.\n\nReactive power compensation is not required, reducing equipment costs.\n\nSinus units or IGBT power converters for power control of:\n\n"}
{"id": "713060", "url": "https://en.wikipedia.org/wiki?curid=713060", "title": "Aurora (mythology)", "text": "Aurora (mythology)\n\nAurora () is the Latin word for dawn, and the goddess of dawn in Roman mythology and Latin poetry.\nLike Greek \"Eos\" and Rigvedic \"Ushas\", \"Aurora\" continues the name of an earlier Indo-European dawn goddess, \"Hausos\".\n\nIn Roman mythology, Aurora renews herself every morning and flies across the sky, announcing the arrival of the sun. Her parentage was flexible: for Ovid, she could equally be \"Pallantis\", signifying the daughter of Pallas, or the daughter of Hyperion. She has two siblings, a brother (Sol, the sun) and a sister (Luna, the moon). Roman writers rarely imitated Hesiod and later Greek poets by naming Aurora as the mother of the Anemoi (the Winds), who were the offspring of Astraeus, the father of the stars.\n\nAurora appears most often in sexual poetry with one of her mortal lovers. A myth taken from the Greek by Roman poets tells that one of her lovers was the prince of Troy, Tithonus. Tithonus was a mortal, and would therefore age and die. Wanting to be with her lover for all eternity, Aurora asked Jupiter to grant immortality to Tithonus. Jupiter granted her wish, but she failed to ask for eternal youth to accompany his immortality, and he became forever old. Aurora turned him into a cicada.\n\nFrom Homer's \"Iliad\":\n\nShakespeare's \"Romeo and Juliet\" (I.i), Montague says of his lovesick son Romeo\n\nIn traditional Irish folk songs, such as \"Lord Courtown\"\n\nIn \"On Imagination,\" by Phillis Wheatley\n\nIn the poem \"Tithonus\" by Alfred, Lord Tennyson, Aurora is described thus:\n\nIn singer-songwriter Björk's \"Vespertine\" track, Aurora is described as:\n\nThe post-punk rock band The Sexual Side Effects's track \"Aurora\" alludes to the goddess:\n\nIn Chapter 8 of Charlotte Brontë's \"Villette\", Madame Beck fires her old Governess first thing in the morning and is described by the narrator, Lucy Snowe: \"All this, I say, was done between the moment of Madame Beck's issuing like Aurora from her chamber, and that in which she coolly sat down to pour out her first cup of coffee.\"\n\n\n\n"}
{"id": "8069949", "url": "https://en.wikipedia.org/wiki?curid=8069949", "title": "Best management practice for water pollution", "text": "Best management practice for water pollution\n\nBest management practices (BMPs) is a term used in the United States and Canada to describe a type of water pollution control. Historically the term has referred to auxiliary pollution controls in the fields of industrial wastewater control and municipal sewage control, while in stormwater management (both urban and rural) and wetland management, BMPs may refer to a principal control or treatment technique as well.\n\nBeginning in the 20th century, designers of industrial and municipal sewage pollution controls typically utilized engineered systems (e.g. filters, clarifiers, biological reactors) to provide the central components of pollution control systems, and used the term \"BMPs\" to describe the supporting functions for these systems, such as operator training and equipment maintenance.\n\nStormwater management, as a specialized area within the field of environmental engineering, emerged later in the 20th century, and some practitioners have used the term BMP to describe both structural or engineered control devices and systems (e.g. retention ponds) to treat polluted stormwater, as well as operational or procedural practices (e.g. minimizing use of chemical fertilizers and pesticides). Other practitioners prefer to use the term Stormwater control measure, due to the varied definitions of the term \"BMP\" and its use in non-stormwater practice.\n\nCongress referred to BMP in several sections of the U.S. Clean Water Act (CWA) but did not define the term.\n\nIn implementing the CWA, the U.S. Environmental Protection Agency (EPA) defined BMP in the federal wastewater permit regulations, initially to refer to auxiliary procedures for industrial wastewater controls. \n\nLater the Agency added a reference to stormwater management BMPs.\n\nIndustrial wastewater BMPs are considered an adjunct to engineered treatment systems. Typical BMPs include operator training, maintenance practices, and spill control procedures for treatment chemicals. There are also many BMPs available which are specific to particular industrial processes, for example:\n\nStormwater management BMPs are control measures taken to mitigate changes to both quantity and quality of urban runoff caused through changes to land use. Generally BMPs focus on water quality problems caused by increased impervious surfaces from land development. BMPs are designed to reduce stormwater volume, peak flows, and/or nonpoint source pollution through evapotranspiration, infiltration, detention, and filtration or biological and chemical actions. BMPs also can improve receiving-water quality by extending the duration of outflows in comparison to inflow duration (known as hydrograph extension), which dilutes the stormwater discharged into a larger volume of upstream flow.\n\nStormwater BMPs can be classified as \"structural\" (i.e., devices installed or constructed on a site) or \"non-structural\" (procedures, such as modified landscaping practices or street sweeping). There are a variety of BMPs available; selection typically depends on site characteristics and pollutant removal objectives. EPA has published a series of stormwater BMP fact sheets for use by local governments, builders and property owners.\n\nStormwater management BMPs can be also categorized into four basic types:\n\n\n"}
{"id": "6396431", "url": "https://en.wikipedia.org/wiki?curid=6396431", "title": "Big Shoals State Park", "text": "Big Shoals State Park\n\nBig Shoals State Park in Hamilton County, Florida is a Florida State Park. It is approximately east of White Springs, off US 41. The park is situated on the Suwannee River and features Limestone bluffs as well as the biggest whitewater rapids in all of Florida. The park also features more than of hiking and nature trails, and freshwater fishing.\n\n\n"}
{"id": "55442187", "url": "https://en.wikipedia.org/wiki?curid=55442187", "title": "Bioassay", "text": "Bioassay\n\nA bioassay is an analytical method to determine concentration or potency of a substance by its effect on living cells or tissues. Bioassays were used to estimate the potency of agents by observing their effects on living animals (in vivo) or tissues (in vitro).\n\nA bioassay experiment can either be qualitative or quantitative, direct or indirect. If the measured response is binary, the assay is qualititative, if not, it is quantitative.\n\nBioassay is used to detect biological hazards or give a quality assessment of a mixture. Bioassay is often used to monitor water quality and also sewage discharge and its impact on surrounding . It is also used to assess the environmental impact and safety of new technologies and facilities.\n\nBioassay is a biochemical test to estimate the relative potency of a sample compound to a standard compound. Typical bioassay involves a \"stimulus\" (ex. drugs) applied to a \"subject\" (ex. animals, tissues, plants) and a \"response\" (ex. death) of the subject is triggered and measured. The intensity of stimulus is varied by doses and depending on this intensity of stimulus, a change/response will be followed by a subject.\n\nThe first uses of bioassay dates back to as early as the late 19th century, when the foundation of bioassays was laid down by a German physician, Paul Ehrlich. He introduced the concept of standardization by the reactions of living matter. His bioassay on diphtheria antitoxin was the first bioassay to receive recognition. His use of bioassay was able to disocover that administration of gradually increasing dose of diphteria in animals stimulated production of antiserum.\n\nMany of the early bioassays consisted of using animals to test carcinogenicity of chemicals. One well known example is a \"canary in the coal mine\" experiment. To test for methane, miners would take methane-sensitive canaries to coal mines to ensure safe air. In 1915, Yamaigiwa Katsusaburo and Koichi Ichikawa tested the carcinogenicity of coal tar using inner surface of rabbit's ears.\n\nThrough 1940s and 1960s, animal bioassay was primarily used to test for toxicity and safety of drugs, food additives and pesticides.\n\nIn late 1960s and 1970s, reliance on bioassay increased as the public concern for occupational and environmental hazards increased. While before this health risks of certain chemicals such as pesticide was tested in animal bioassay, it was still rare and testing was not seen often.\n\n- The stimulus/standard sufficiently produces measurable and specific response. The response must be clear, easily recognized, and directly measured.\n\n- The relationship between the dose and the response is first ascertained. Then the dose corresponding to a given response is obtained from the relation for each preparation separately.\n\n- The assay involves 'all or none' response (ex. life or death). The response is produced by threshold effect.\n\n- quantitative analytical method that measures absorbance of color change from antigen-antibody reaction (ex. Direct, indirect, sandwich, competitive). ELISA is used to measure variety of substances in human body from cortisol levels for stress to glucose level for diabetes.\n\nHome pregnancy test involves ELISA to detect the increase of human chorionic gonadotropin (hCG) during pregnancy.\n\nHIV test also uses indirect ELISA to detect HIV antibody caused by infection.\n\n"}
{"id": "3387416", "url": "https://en.wikipedia.org/wiki?curid=3387416", "title": "Cable landing point", "text": "Cable landing point\n\nA cable landing point is the location where a submarine or other underwater cable makes landfall. The term is most often used for the landfall points of submarine telecommunications cables and submarine power cables. The landing will either be direct (in the case of a point-to-point cable system) or via a branch from a main cable using a submarine branching unit. The branch can be many kilometres long.\n\nCable landing points are usually carefully chosen to be in areas:\n\n\nSuch locations are rare, and will usually be the shared landfall point for several cable systems. \n\nFrequently, there will be a nearby cable landing station, or cable termination station, which may well be shared between multiple cable systems, but in some cases, the cable may be laid many miles inland before reaching its termination point.\n\nA cable landing station may or may not be required, depending on whether, for example, the submarine cable requires power in order to provide power to submarine repeaters or amplifiers. The voltages applied to the cables can be high—3,000 to 4,000 volts for a typical trans-Atlantic telecommunications cable system, and 1,000 volts for a cross-channel telecommunications cable system. Submarine power cables can operate at many kilovolts: for example, the Fenno-Skan power cable operates at 400 kV DC.\n\nA cable termination station is the point at which the submarine cable connects into the land-based infrastructure or network. A cable termination station may be the same facility as the cable landing station, or may be many miles away. The termination station will usually be the point where high-capacity 'backhaul' land-based network connects to areas of high demand, which are usually centres of high population density, rather than the usually remote locations of cable landing points/landing stations/termination stations. A good example of this is the Endeavour cable system which connects Australia to Hawaii. The cable landing point in Sydney is Tamarama Beach, some distance from the cable termination station in Paddington. \n\nFor power cables the term cable termination station is not strictly determined. It is either the point where the underwater cable ends and where the overhead powerline starts or if the whole line is implemented as cable, the first cable sleeve on the land. However one can also say that the substation or HVDC static inverter plant, where the connection to the grid is made describe as cable termination station. However, this station can be far away from the coast.\n\n\n\n"}
{"id": "171208", "url": "https://en.wikipedia.org/wiki?curid=171208", "title": "Carding", "text": "Carding\n\nCarding is a mechanical process that disentangles, cleans and intermixes fibres to produce a continuous web or sliver suitable for subsequent processing. This is achieved by passing the fibers between differentially moving surfaces covered with card clothing. It breaks up locks and unorganised clumps of fibre and then aligns the individual fibers to be parallel with each other. In preparing wool fibre for spinning, carding is the step that comes after teasing.\n\nThe word is derived from the Latin \"carduus\" meaning thistle or teasel, as dried vegetable teasels were first used to comb the raw wool. \n\nThese ordered fibres can then be passed on to other processes that are specific to the desired end use of the fibre: Cotton, batting, felt, woollen or worsted yarn, etc. Carding can also be used to create blends of different fibres or different colours. When blending, the carding process combines the different fibres into a homogeneous mix. Commercial cards also have rollers and systems designed to remove some vegetable matter contaminants from the wool.\n\nCommon to all carders is card clothing. Card clothing is made from a sturdy flexible backing in which closely spaced wire pins are embedded. The shape, length, diameter, and spacing of these wire pins are dictated by the card designer and the particular requirements of the application where the card cloth will be used. A later version of the card clothing product developed during the latter half of the 19th century and found only on commercial carding machines, whereby a single piece of serrated wire was wrapped around a roller, became known as metallic card clothing.\n\nCarding machines are known as cards. Fibre may be carded by hand for hand spinning.\n\nScience historian Joseph Needham ascribes the invention of bow-instruments used in textile technology to India. The earliest evidence for using bow-instruments for carding comes from India (2nd century CE). These carding devices, called \"kaman\" (bow) and \"dhunaki\", would loosen the texture of the fibre by the means of a vibrating string.\n\nAt the turn of the eighteenth century, wool in England was being carded using pairs of hand cards, it was a two-stage process: 'working' with the cards opposed and 'stripping' where they are in parallel.\n\nIn 1748 Lewis Paul of Birmingham, England, invented two hand driven carding machines. The first used a coat of wires on a flat table moved by foot pedals. This failed. On the second, a coat of wire slips was placed around a card which was then wrapped around a cylinder.\nDaniel Bourn obtained a similar patent in the same year, and probably used it in his spinning mill at Leominster, but this burnt down in 1754. The invention was later developed and improved by Richard Arkwright and Samuel Crompton. Arkwright's second patent (of 1775) for his carding machine was subsequently declared invalid (1785) because it lacked originality.\n\nFrom the 1780s, the carding machines were set up in mills in the north of England and mid-Wales. Priority was given to cotton but woollen fibres were being carded in Yorkshire in 1780. With woollen, two carding machines were used: the first or the scribbler opened and mixed the fibres, the second or the condenser mixed and formed the web. The first in Wales was in a factory at Dolobran near Meifod in 1789. These carding mills produced yarn particularly for the Welsh flannel industry.\n\nBy 1838, the Spen Valley, centred on Cleckheaton had at least 11 card clothing factories and by 1893, it was generally accepted as the card cloth capital of the world, though by 2008 only two manufacturers of metallic and flexible card clothing remained in England, Garnett Wire Ltd. dating back to 1851 and Joseph Sellers & Son Ltd established in 1840.\n\nBaird from Scotland took carding to Leicester, Massachusetts in the 1780s. In the 1890s, the town produced one-third of all hand and machine cards in North America. John and Arthur Slater, from Saddleworth went over to work with Slater in 1793.\n\nA 1780s scribbling mill would be driven by a water wheel. There were 170 scribbling mills around Leeds at that time. Each scribbler would require to operate. Modern machines are driven by belting from an electric motor or an overhead shaft via two pulleys.\n\nCarding: the fibres are separated and then assembled into a loose strand (sliver or tow) at the conclusion of this stage.\n\nIn a wider sense carding can refer to the four processes of willowing, lapping, carding and drawing. In willowing the fibers are loosened. In lapping the dust is removed to create a flat sheet or lap of fibres; Carding itself is the combing of the tangled lap into a thick rope or sliver of 1/2 inch in diameter, it can then be optionally combed, is used to remove the shorter fibres, creating a stronger yarn.\nIn drawing a drawing frame combines 4 slivers into one. Repeated drawing increases the quality of the sliver allowing for finer counts to be spun. Each sliver will have thin and thick spots, and by combining several slivers together a more consistent size can be reached. Since combining several slivers produces a very thick rope of cotton fibres, directly after being combined the slivers are separated into rovings. These rovings (or slubbings) are then what are used in the spinning process.\n\nFor machine processing, a roving is about the width of a pencil. The rovings are collected in a drum and proceed to the slubbing frame which adds twist, and winds onto bobbins. Intermediate Frames are used to repeat the slubbing process to produce a finer yarn, and then the roving frames reduces it to a finer thread, gives more twist, makes more regular and even in thickness, and winds onto a smaller tube.\n\nPredating mechanised weaving, hand loom weaving was a cottage industry that used the same processes but on a smaller scale. These skills have survived as an artisan craft in less developed societies- and as art form and hobby in advanced societies.\n\nHand cards are typically square or rectangular paddles manufactured in a variety of sizes from to . The working face of each paddle can be flat or cylindrically curved and wears the card cloth. Small cards, called flick cards, are used to flick the ends of a lock of fibre, or to tease out some strands for spinning off.\n\nA pair of cards is used to brush the wool between them until the fibres are more or less aligned in the same direction. The aligned fibre is then peeled from the card as a rolag. Carding is an activity normally done outside or over a drop cloth, depending on the wool's cleanliness. Rolag is peeled from the card. \n\nThis product (rovings, rolags, and batts) can be used for spinning.\n\nCarding of wool can either be done \"in the grease\" or not, depending on the type of machine and on the spinner's preference. \"In the grease\" means that the lanolin that naturally comes with the wool has not been washed out, leaving the wool with a slightly greasy feel. The large drum carders do not tend to get along well with lanolin, so most commercial worsted and woollen mills wash the wool before carding. Hand carders (and small drum carders too, though the directions may not recommend it) can be used to card lanolin rich wool.\nThe simplest machine carder is the drum carder. Most drum carders are hand-cranked but some are powered by an electric motor. These machines generally have two rollers, or drums, covered with card clothing. The licker-in, or smaller roller meters fibre from the infeed tray onto the larger storage drum. The two rollers are connected to each other by a belt- or chain-drive so that their relative speeds cause the storage drum to gently pull fibres from the licker-in. This pulling straightens the fibres and lays them between the wire pins of the storage drum's card cloth. Fibre is added until the storage drum's card cloth is full. A gap in the card cloth facilitates removal of the batt when the card cloth is full.\n\nSome drum carders have a soft-bristled brush attachment that presses the fibre into the storage drum. This attachment serves to condense the fibres already in the card cloth and adds a small amount of additional straightening to the condensed fibre.\n\nCottage carding machines differ significantly from the simple drum card. These carders do not store fibre in the card cloth as the drum carder does but, rather, fibre passes through the workings of the carder for storage or for additional processing by other machines.\n\nA typical cottage carder has a single large drum (the swift) accompanied by a pair of in-feed rollers (nippers), one or more pairs of worker and stripper rollers, a fancy, and a doffer. In-feed to the carder is usually accomplished by hand or by conveyor belt and often the output of the cottage carder is stored as a batt or further processed into roving and wound into bumps with an accessory bump winder. The cottage carder in the supports both outputs.\n\nRaw fibre, placed on the in-feed table or conveyor is moved to the nippers which restrain and meter the fiber onto the swift. As they are transferred to the swift, many of the fibres are straightened and laid into the swift's card cloth. These fibres will be carried past the worker / stripper rollers to the fancy.\n\nAs the swift carries the fibres forward, from the nippers, those fibres that are not yet straightened are picked up by a worker and carried over the top to its paired stripper. Relative to the surface speed of the swift, the worker turns quite slowly. This has the effect of reversing the fibre. The stripper, which turns at a higher speed than the worker, pulls fibres from the worker and passes them to the swift. The stripper's relative surface speed is slower than the swift's so the swift pulls the fibres from the stripper for additional straightening.\n\nStraightened fibres are carried by the swift to the fancy. The fancy's card cloth is designed to engage with the swift's card cloth so that the fibres are lifted to the tips of the swift's card cloth and carried by the swift to the doffer. The fancy and the swift are the only rollers in the carding process that actually touch.\n\nThe slowly turning doffer removes the fibres from the swift and carries them to the fly comb where they are stripped from the doffer. A fine web of more or less parallel fibre, a few fibres thick and as wide as the carder's rollers, exits the carder at the fly comb by gravity or other mechanical means for storage or further processing.\n\n\n\n"}
{"id": "13399188", "url": "https://en.wikipedia.org/wiki?curid=13399188", "title": "Communal roosting", "text": "Communal roosting\n\nCommunal roosting is an animal behavior where a group of individuals, typically of the same species, congregate in an area for a few hours based on an external signal and will return to the same site with the reappearance of the signal. Environmental signals are often responsible for this grouping, including nightfall, high tide, or rainfall. The distinction between communal roosting and cooperative breeding is the absence of chicks in communal roosts. While communal roosting is generally observed in birds, the behavior has also been seen in bats, primates, and insects. The size of these roosts can measure in the thousands to millions of individuals, especially among avian species.\n\nThere are many benefits associated with communal roosting including: increased foraging ability, decreased thermoregulatory demands, decreased predation, and increased conspecific interactions. While there are many proposed evolutionary concepts for how communal roosting evolved, no specific hypothesis is currently supported by the scientific community as a whole.\n\nOne of the adaptive explanations for communal roosting is the hypothesis that individuals are benefited by the exchange of information at communal roosts. This idea is known as the information center hypothesis (ICH) and proposed by Peter Ward and Amotz Zahavi in 1973 states that bird assemblages such as communal roosts act as information hubs for distributing knowledge about food source location. When food patch knowledge is unevenly distributed amongst certain flock members, the other \"clueless\" flock members can follow and join these knowledgeable members to find good feeding locations. To quote Ward and Zahavi on the evolutionary reasons as to how communal roosts came about, \"...communal roosts, breeding colonies and certain other bird assemblages have been evolved primarily for the efficient exploitation of unevenly-distributed food sources by serving as ' information-centres.' \" \n\nThe two strategies hypothesis was put forth by Patrick Weatherhead in 1983 as an alternative to the then popular information center hypothesis. This hypothesis proposes that different individuals join and participate in communal roosts for different reasons that are based primarily on their social status. Unlike the ICH, not all individuals will join a roost in order to increase their foraging capabilities. This hypothesis explains that while roosts initially evolved due to information sharing among older and more experienced foragers, this evolution was aided by the benefits that more experienced foragers gained due to the fact that as better forages they acquired a status of high rank within the roost. As dominant individuals, they are able to obtain the safest roosts, typically those highest in the tree or closest to the center of the roost. In these roosts, the less dominant and unsuccessful foragers act as a physical predation buffer for the dominant individuals. This is similar to the selfish herd theory, which states that individuals within herds will utilize conspecifics as physical barriers from predation. The younger and less dominant individuals will still join the roost because they gain some safety from predation through the dilution effect, as well as the ability to learn from the more experienced foragers that are already in the roost.\n\nSupport for the two strategies hypothesis has been shown in studies of roosting rooks (\"Corvus frugilegus\"). A 1977 study of roosting rooks by Ian Swingland showed that an inherent hierarchy exists within rook communal roosts. In this hierarchy, the most dominant individuals have been shown to routinely occupy the roosts highest in the tree, and while they pay a cost (increased energy use to keep warm) they are safer from terrestrial predators. Despite this enforced hierarchy, lower ranking rooks remained with the roost, indicating that they still received some benefit from their participation in the roost. When weather conditions worsened, the more dominant rooks forced the younger and less dominant out of their roosts. Swingland proposed that the risk of predation at lower roosts was outweighed by the gains in reduced thermal demands. Similar support for the two strategies hypothesis has also been found in red-winged blackbird roosts. In this species the more dominant males will regularly inhabit roosts in thicker brush, where they are better hidden from predators than the less dominant individuals, that are forced to roost at the edge of the brush.\n\nThe TSH makes several assumptions that must be met in order for the theory to work. The first major assumption is that within communal roosts there are certain roosts that possess safer or more beneficial qualities than other roosts. The second assumption is that the more dominant individuals will be capable of securing these roosts, and finally dominance rank must be a reliable indicator of foraging ability.\n\nProposed by Heinz Richner and Phillip Heeb in 1996, the recruitment center hypothesis (RCH) explains the evolution of communal roosting as a result of group foraging. The RCH also explains behaviors seen at communal roosts such as: the passing of information, aerial displays, and the presence or lack of calls by leaders. This hypothesis assumes:\nThese factors decrease relative food competition since control over a food source by an individual is not correlated to the duration or richness of said source. The passing of information acts to create a foraging group. Group foraging decreases predation and increases relative feeding time at the cost of sharing a food source. The decrease in predation is due to the dilution factor and an early warning system created by having multiple animals alert. Increases in relative feeding are explained by decreasing time spent watching for predators and social learning. Recruiting new members to food patches benefits successful foragers by increasing relative numbers. With the addition of new members to a group the benefits of group foraging increase until the group size is larger than the food source is able to support. Less successful foragers benefit by gaining knowledge of where food sources are located. Aerial displays are used to recruit individuals to participate in group foraging. However, not all birds display since not all birds are members in a group or are part of a group that is seeking participants. In the presence of patchy resources, Richner and Heeb propose the simplest manner would be to form a communal roost and recruit participants there. In other words, recruitment to foraging groups explains the presence of these communal roosts.\n\nSupport for the RCH has been shown in ravens (\"Covus corax\"). Reviewing a previous study by John Marzluff, Bernd Heinrich, and Colleen Marzluff, Etienne Danchin and Heinz Richner demonstrate that the collected data proves the RCH instead of the Information Center Hypothesis supported by Marzluff, et al. Both knowledgeable and naïve (\"clueless\") birds are shown to make up the roosts and leave them at the same time, with the naïve birds being led to the food sources. Aerial demonstrations were shown to peak around the same time as the discovery of a new food source. These communities were made up of non-breeders which forage in patchily distributed food environments, following the assumptions made by Richner and Heeb.\n\nAt this point in time there has been no additional scientific evidence excluding RCH or any evidence of overwhelming support. What is overlooked by RCH is that information may also be passed within the communal roost which increases and solidifies the community.\n\nBirds in a communal roost can reduce the impact of wind and cold weather by sharing body heat through huddling, which reduces the overall energy demand of thermoregulation. A study by Guy Beauchamp explained that black-billed magpies (\"Pica hudsonia\") often formed the largest roosts during the winter. The magpies tend to react very slowly at low body temperatures, leaving them vulnerable to predators. Communal roosting in this case would improve their reactivity by sharing body heat, allowing them to detect and respond to predators much more quickly.\n\nA large roost with many members can visually detect predators easier, allowing individuals to respond and alert others quicker to threats. Individual risk is also lowered due to the dilution effect, which states that an individual in a large group will have a low probability of being preyed upon. Similar to the selfish-herd theory, communal roosts have demonstrated a hierarchy of sorts where older members and better foragers nest in the interior of the group, decreasing their exposure to predators. Younger birds and less able foragers located on the outskirts still demonstrate some safety from predation due to the dilution effect.\n\nAccording to the ICH, successful foragers share knowledge of favorable foraging sites with unsuccessful foragers at a communal roost, making it energetically advantageous for individuals to communally roost and forage more easily. Additionally with a greater number of individuals at a roost, the searching range of a roost will increase and improve the probability of finding favorable foraging sites.\n\nThere are also potentially improved mating opportunities, as demonstrated by red-billed choughs (\"Pyrrhocorax pyrrhocorax)\", which have a portion of a communal roost dedicated to individuals that lack mates and territories.\n\nIt is costly for territorial species to physically travel to and from roosts, and in leaving their territories they open themselves up to takeovers. Communal roosts may draw the attention of potential predators, as the roost becomes audibly and visibly more conspicuous due to the number of members. There is also a decrease in the local food supply as a greater number of members results in competition for food. A large number of roost members can also increases the exposure to droppings, causing plumage to deteriorate and leaving birds vulnerable to dying from exposure as droppings reduce the ability of feathers to shed water.\n\nCommunal roosting has been observed in numerous avian species. As previously mentioned, rooks (\"Corvus frugilegus\") are known to form large nocturnal roosts, these roosts can contain anywhere from a few hundred to over a thousand individuals. These roosts then disband at daybreak when the birds return to foraging activities. Studies have shown that communal roosting behavior is mediated by light intensity, which is correlated with sunset, where rooks will return to the roost when the ambient light has sufficiently dimmed.\n\nAcorn woodpeckers (\"Melanerpes formicivorus\") are known to form communal roosts during the winter months. In these roosts two to three individuals will share a cavity during the winter. Within these tree cavities woodpeckers share their body heat with each other and therefore decrease the thermoregulatory demands on the individuals within the roost. Small scale communal roosting during the winter months has also been observed in Green Woodhoopoes (Phoeniculus purpureus). Winter communal roosts in these species typically contain around five individuals.\n\nTree swallows (\"Tachycineta bicolor\") located in southeastern Louisiana are known to form nocturnal communal roosts and have been shown to exhibit high roost fidelity, with individuals often returning to the same roost they had occupied on the previous night. Research has shown that swallows form communal roosts due to the combined factors of conspecific attraction, where individual swallows are likely to aggregate around other swallows of the same species, and roost fidelity. Tree swallows will form roosts numbering in hundreds or thousands of individuals.\n\nRed-billed choughs (\"Pyrrhocorax pyrrhocorax\") roost in what has been classified as either a main roost or a sub roost. Main roosts are constantly in use, whereas the sub roosts are used irregularly by individuals lacking both a mate and territory. These sub roosts are believed to help improve the ability of non-breeding choughs to find a mate and increase their territory ranges.\n\nInterspecies roosts have been observed between different bird species. In San Blas, Mexico, the great egret (\"Ardea alba\"), the little blue heron (\"Egretta caerulea\"), the tricolored heron (\"Egretta tricolor\"), and the snowy egret (\"Egretta thula\") are known to form large communal roosts. It has been shown that the snowy egret determines the general location of the roost due to the fact that the other three species rely on it for its abilities to find food sources. In these roosts there is often a hierarchical system, where the more dominant species (in this case the snowy egret) will typically occupy the more desirable higher perches. Interspecies roosts have also been observed among other avian species.\n\nCommunal roosting has also been well documented among insects, particularly butterflies. The passion-vine butterfly (\"Heliconius erato)\" is known to form nocturnal roosts, typically comprising four individuals. It is believed that these roosts deter potential predators due to the fact that predators attack roosts less often than they do individuals.\n\nCommunal roosting behavior has also been observed in the neotropical zebra longwing butterfly (\"Heliconius charitonius\") in the La Cinchona region of Costa Rica. A study of this roost showed that individuals vary in their roost fidelity, and that they tend to form smaller sub roosts. The same study observed that in this region communal roosting can be mediated by heavy rainfall.\n\nCommunal roosting has also been observed in south Peruvian tiger beetles of the subfamily \"Cicindelidae\". These species of tiger beetle have been observed to form communal roosts comprising anywhere from two to nine individuals at night and disbanding during the day. It is hypothesized that these beetles roost high in the treetops in order to avoid ground-based predators.\n\nWhile there are few observations of communal roosting mammals, the trait has been seen in several species of bats. The little brown bat (\"Myotis lucifugus\") is known to participate in communal roosts of up to thirty seven during cold nights in order to decrease thermoregulatory demands, with the roost disbanding at daybreak.\n\nSeveral other species of bats, including the hoary bat (\"Lasiurus cinereus\") and the big brown bat (\"Eptesicus fuscus\") have also been observed to roost communally in maternal colonies in order to reduce the thermoregulatory demands on both the lactating mothers and juveniles.\n\n"}
{"id": "106285", "url": "https://en.wikipedia.org/wiki?curid=106285", "title": "Crater Lake", "text": "Crater Lake\n\nCrater Lake (Klamath: giiwas) is a crater lake in south-central Oregon in the western United States. It is the main feature of Crater Lake National Park and is famous for its deep blue color and water clarity. The lake partly fills a nearly -deep caldera that was formed around 7,700 (± 150) years ago\nby the collapse of the volcano Mount Mazama. There are no rivers flowing into or out of the lake; the evaporation is compensated for by rain and snowfall at a rate such that the total amount of water is replaced every 250 years. With a depth of , the lake is the deepest in the United States. In the world, it ranks ninth for maximum depth, and third for mean (average) depth.\n\nCrater Lake is also known for the \"Old Man of the Lake\", a full-sized tree which is now a log that has been bobbing vertically in the lake for over a century. The low temperature of the water has slowed the decomposition of the wood, hence its longevity.\n\nCrater Lake features two small islands. Wizard Island, located near the western shore of the lake, is a cinder cone approximately 316 acres (128 ha) in size. Phantom Ship, a natural rock pillar, is located near the southern shore.\n\nWhile having no indigenous fish population, the lake was stocked from 1888 to 1941 with a variety of fish. Since then, several species have formed self-sustaining populations.\nSince 2002, one of the state's regular-issue license plate designs has featured Crater Lake.\nThe commemorative Oregon State Quarter, which was released by the United States Mint in 2005, features an on its reverse.\n\nCrater Lake is in Klamath County, approximately northwest of the county seat of Klamath Falls, and about northeast of the city of Medford.\n\nIn June 1853, John Wesley Hillman became the first non-Native American explorer to report sighting the lake he named the \"Deep Blue Lake.\" The lake was renamed at least three times, as Blue Lake, Lake Majesty, and finally Crater Lake.\n\nThe lake is across, with a caldera rim ranging in elevation from and an average lake depth of . The lake's maximum depth has been measured at , which fluctuates slightly as the weather changes. On the basis of maximum depth, Crater Lake is the deepest lake in the United States, the second-deepest in North America (after Great Slave Lake in Canada), and the ninth-deepest lake in the world. Crater Lake is often cited as the seventh-deepest lake in the world, but this ranking excludes Lake Vostok in Antarctica, which is beneath about of ice, and the recent depth soundings of O'Higgins/San Martín Lake, which is along the border of Chile and Argentina.\n\nWhen considering the mean, or average depth of lakes, Crater Lake becomes the deepest lake in the Western Hemisphere and the third-deepest in the world. Crater Lake Institute Director and limnologist Owen Hoffman states \"Crater Lake is the deepest, when compared on the basis of average depth among lakes whose basins are entirely above sea level. The average depths of Lakes Baikal and Tanganyika are deeper than Crater Lake; however, both have basins that extend below sea level.\"\n\nMount Mazama, part of the Cascade Range volcanic arc, was built up mostly of andesite, dacite, and rhyodacite over a period of at least 400,000 years. The caldera was created in a massive volcanic eruption between 6,000 and 8,000 years ago that led to the subsidence of Mount Mazama. About of rhyodacite was erupted in this event. Since that time, all eruptions on Mazama have been confined to the caldera.\n\nLava eruptions later created a central platform, Wizard Island, Merriam Cone, and other, smaller volcanic features, including a rhyodacite dome that was eventually created atop the central platform. Sediments and landslide debris also covered the caldera floor.\n\nEventually, the caldera cooled, allowing rain and snow to accumulate and form a lake. Landslides from the caldera rim thereafter formed debris fans and turbidite sediments on the lake bed. Fumaroles and hot springs remained common and active during this period. Also after some time, the slopes of the lake's caldera rim more or less stabilized, streams restored a radial drainage pattern on the mountain, and dense forests began to revegetate the barren landscape. It is estimated that about 720 years was required to fill the lake to its present depth of . Much of this occurred during a period when the prevailing climate was less moist than at present.\n\nSome hydrothermal activity remains along the lake floor, suggesting that at some time in the future, Mazama may erupt once again.\n\nCrater Lake features a subalpine climate, with the rare dry-summer type (Köppen classification \"Dsc\") owing to its high elevation and – like all of Oregon – the strong summer influence of the North Pacific High. In the summer, the weather is mild and dry, but in the winter is cold and the powerful influence of the Aleutian Low allows for enormous snowfalls averaging per year and maximum snow cover averaging . This snow does not usually melt until mid-July, and allows for substantial glaciers on adjacent mountains. In the winter of 1949/1950 as much as of snow fell, while the less complete snow cover records show cover as high as occurred during another particularly unsettled winter in 1981/1982. The heaviest daily snowfall was , which occurred as recently as February 28, 1971; or more in one storm has occurred in both June and September. Hard frost is possible even into the summer, and the average window for freezing temperatures is August 19 through July 7, while for measurable (≥) snowfall, October 1 through June 15.\n\nDue to several unique factors, mainly that the lake has no inlets or tributaries, the waters of Crater Lake are some of the purest in the world because of the absence of pollutants. Clarity readings from a Secchi disk have consistently been in the high-30 meter to mid-20 meter (80 to 115-foot) range, which is very clear for any natural body of water. In 1997, scientists recorded a record clarity of .\n\nThe lake has relatively high levels of dissolved salts, total alkalinity, and conductivity. The average pH has generally ranged between 7 and 8.\n\nThe Klamath tribe of Native Americans, whose oral history describes their ancestors witnessing the collapse of Mount Mazama and the formation of Crater Lake, regard the lake as an \"abode to the Great Spirit\". Klamath oral history tells of a battle between the sky god Skell and the god of the underworld Llao (a prominent feature at Crater Lake is Llao Rock). Mount Mazama was destroyed in the battle, creating Crater Lake, called \"giiwas\" in the Klamath language. The Klamath people used Crater Lake in vision quests, which often involved climbing the caldera walls and other dangerous tasks. Those who were successful in such quests were often regarded as having more spiritual powers. The tribe still holds Crater Lake in high regard as a spiritual site.\n\n\n\n"}
{"id": "557189", "url": "https://en.wikipedia.org/wiki?curid=557189", "title": "Creatures 2", "text": "Creatures 2\n\nCreatures 2 is the second game in the Creatures artificial life game series made by Creature Labs, and the sequel to 1996 game \"Creatures\". It features three species: the cute, dependent Norns, the cantankerous Grendels and the industrious Ettins. The game tries to simulate life, and includes a complex two-dimensional ecology of plants, animals and insects, which provide the environment for the three main species to live and develop in. The player interacts with the world using a hand-shaped cursor, and tries to encourage the creatures' development by manipulating various objects around the world, guiding the creatures using the cursor and encouraging the creatures to speak.\n\nMany new gameplay features included in Creatures 2 not present in the original game include a new physics model and a global weather system, along with brand new applets and a world twice the size of the Creatures 1 world.\n\nThe executable file for the game was in fact an interpreter for its scripting language, thus allowing users to make total conversions or derivative works from the game.\n\nLike the other games in the series, Creatures 2 is mostly open-ended, with no predetermined goals, allowing the player to raise Norns at their own pace. In each new world, the player begins in the incubator cavern area. The hatchery, one of the game's in-built applets, allows the player to add Norn eggs to the world, which can be hatched via the incubator. Norns may also be downloaded from the internet and imported into to use in the game.\n\nOnce Norns reach adolescence at around an hour old, they are ready to breed, and female Norns will begin an oestrogen cycle. When Norns mate, there is a long kissing sound with a pop at the end – known as a kisspop. Like real life, not all mating results in a pregnancy. The Norn reproductive process can be monitored via the Breeder's Kit.\n\nAt the beginning, the player is only able to navigate a small area of Albia. As their Norns explore the world, however, more areas will be able to visited. If a Norn falls into water and is picked up, it cannot be placed in an unvisited area.\n\nLike the previous game, every plant, animal and insect in Albia is a separate system called an autonomous agent, or just \"agent\" for short. Many user-created agents have been made, which can be injected into the world using the Injector Kit applet, which also includes an analysis page giving the opportunity to check new objects for potentially harmful contents before injecting them.\n\nAlbia, the world in which Creatures 2 is set, is a vast disc-world filled with a wide variety of flora, ranging from the Cacti that inhabit the desert to the exclusive mushroom-like Gelsemium plant that can only be found in a single cavern. Albia is also home to a substantial amount of animal life, such as bees which pollinate flowers and create honey, and Zander fish, which provide a food source for creatures near the oceans. Scattered throughout the world are various seed launchers, which can be used to replenish stocks if any life-forms become extinct.\n\nAlbia also has four seasons: Spring, Summer, Winter and Autumn, and boasts its very own climate, with weather such as snow mainly occurring during the colder months, and rain mostly during the Spring. The game also has an on-screen thermometer, allowing the player to monitor Albia's temperature at all times.\n\nCreatures 2's release was widely anticipated. It is likely that around 450,000 copies of the original Creatures had been sold by the time the game was released, and a huge following had developed behind it. The \"Early Adoption Program\", a promotion where 250 copies of Creatures 2 were given away prior to its official release in September 1998, was scheduled to begin in July 1998, but was pushed back as the game was not close to release quality at the point at which the EAP was scheduled to start. In the end, most received their copies several weeks after the release date.\n\nUpon release, the game suffered from numerous flaws, such as a deficient Norn genome that lead to a condition dubbed \"One Hour Stupidity Syndrome (OHSS)\", which caused Norns to forgot basic survival skills such as eating and sleeping. The task of creating the new Norn genome was given to Sandra Linkletter (Slink), who was also a member of the Creatures community. During development, many problems emerged, most likely due to disagreements between Slink and the Creatures 2 team. Slink later claimed that her design had been compromised by the geneticist brought on to finish her work, Eric Goodwin, and later accused Cyberlife of deriving their update from hers.\n\nCyberlife later released a patch with an improved genome after several weeks had passed. Several users also created improved Norn genomes, such as the Canny Norn genome created by Chris Double and Lis Morris.\n\n\n"}
{"id": "53191309", "url": "https://en.wikipedia.org/wiki?curid=53191309", "title": "D Cancri", "text": "D Cancri\n\nThe Bayer designation d Cancri is shared by two star systems in the constellation Cancer:\n\n\nδ Cancri\n"}
{"id": "56295", "url": "https://en.wikipedia.org/wiki?curid=56295", "title": "Dagobah", "text": "Dagobah\n\nDagobah is a star system in the \"Star Wars\" films \"The Empire Strikes Back\" and \"Return of the Jedi\". It also appears in a deleted scene from \"\". Yoda went into exile on Dagobah after a lightsaber battle with Darth Sidious.\n\nThe planet shown in Dagobah, in the Sluis sector, is a world of murky swamps, steaming bayous, and petrified forests, resembling Earth during the Carboniferous Period.\n\nIn the \"Star Wars\" storyline, after the Great Jedi Purge and his subsequent failure to defeat Darth Sidious, Jedi Master Yoda went into voluntary exile on Dagobah. Here, Yoda lived near a cave infused with the dark side of the Force, which kept him from detection by Emperor Palpatine.\n\nIn \"The Empire Strikes Back\", Luke Skywalker is directed by the Force ghost of Obi-Wan Kenobi, to seek Yoda on Dagobah. After training with Yoda, Skywalker leaves Dagobah early to rescue his friends Han Solo, Princess Leia, and Chewbacca, who he senses are in danger on the planet Bespin. Skywalker also briefly returns to Dagobah in \"Return of the Jedi\", in which he has a final conversation with Yoda before Yoda's death. Luke then speaks to Obi-Wan Kenobi's ghost about the conflicting stories of Luke's parentage.\n\n\"Revenge of the Sith\" did not show how Yoda came to the planet in its theatrical version; but this was portrayed in a deleted scene that George Lucas says was removed so that \"Revenge\" would not have \"too many endings\". The scene of Yoda arriving on Dagobah was rumored to be reinserted into the DVD release of the film in November 2005; however, the film's theatrical cut has been transferred to DVD intact, with the \"Exile to Dagobah\" scene featured in the \"Deleted Scenes\" section on Disc II instead. In the novel of the movie, it is stated that Yoda went to Dagobah in an escape pod launched from Bail Organa's starcruiser.\n\n\"\" featured Dagobah in a sixth season episode, in which Yoda traveled to the planet as part of his own training to gain immortality through the Force.\n\nIn the \"Star Wars\" expanded universe, Yoda confronted a Bpfasshi Dark Jedi on Dagobah, some years before the events in \"The Empire Strikes Back\", and the cave where the Dark Jedi died became strong in the dark side of the Force. \"Star Wars Tales\" later retconned this event to take place earlier, with another Jedi of Yoda's species named Minch replacing Yoda. Dagobah was also visited by the main characters of the \"Galaxy of Fear\" series, where it was home for a time to a tribe of cannibals. During the events of \"\" (for the PC), a team of Jedi Knights from the New Jedi Order visit Dagobah, to find the cave drained of its former menace.\n\nAfter the events of Episode VI, the New Republic founds a military base on one of the greatest mountains of the planet, named 'Mount Yoda' after the Jedi Master.\n\n\n"}
{"id": "21501800", "url": "https://en.wikipedia.org/wiki?curid=21501800", "title": "Electricity distribution companies by country", "text": "Electricity distribution companies by country\n\nThis is a list of Electricity distribution companies by country.\n\n\n\n\n\n\n\n\nAdditionally, there are dozens of small regional companies, some of which are listed in List of Canadian electric utilities.\n\n\n\n\n\n\n\n\n\n\n- Electricité de Strasbourg (ES Energies)<br>\n- Sorégies<br>\n- Gaz Electricité de Grenoble (GEG)<br>\n- Électricité de France SA<br>\n- Engie SA<br>\n- Cie Nationale du Rhône SA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also Public electricity supplier (of UK).\n\n\nSee also List of United States electric companies.\n\n"}
{"id": "9258", "url": "https://en.wikipedia.org/wiki?curid=9258", "title": "Ethics", "text": "Ethics\n\nEthics or moral philosophy is a branch of philosophy that involves systematizing, defending, and recommending concepts of right and wrong conduct. The field of ethics, along with aesthetics, concern matters of value, and thus comprise the branch of philosophy called axiology.\n\nEthics seeks to resolve questions of human morality by defining concepts such as good and evil, right and wrong, virtue and vice, justice and crime. As a field of intellectual inquiry, moral philosophy also is related to the fields of moral psychology, descriptive ethics, and value theory.\n\nThree major areas of study within ethics recognized today are:\n\nThe English word \"ethics\" is derived from the Ancient Greek word \"ēthikós\" (), meaning \"relating to one's character\", which itself comes from the root word \"êthos\" () meaning \"character, moral nature\". This was borrowed into Latin as \"ethica\" and then into French as \"éthique\", from which it was borrowed into English.\n\nRushworth Kidder states that \"standard definitions of \"ethics\" have typically included such phrases as 'the science of the ideal human character' or 'the science of moral duty'. Richard William Paul and Linda Elder define ethics as \"a set of concepts and principles that guide us in determining what behavior helps or harms sentient creatures\". The \"Cambridge Dictionary of Philosophy\" states that the word \"ethics\" is \"commonly used interchangeably with 'morality' ... and sometimes it is used more narrowly to mean the moral principles of a particular tradition, group or individual.\" Paul and Elder state that most people confuse ethics with behaving in accordance with social conventions, religious beliefs and the law and don't treat ethics as a stand-alone concept.\n\nThe word \"ethics\" in English refers to several things. It can refer to philosophical ethics or moral philosophy—a project that attempts to use reason to answer various kinds of ethical questions. As the English philosopher Bernard Williams writes, attempting to explain moral philosophy: \"What makes an inquiry a philosophical one is reflective generality and a style of argument that claims to be rationally persuasive.\" Williams describes the content of this area of inquiry as addressing the very broad question, \"how one should live\". Ethics can also refer to a common human ability to think about ethical problems that is not particular to philosophy. As bioethicist Larry Churchill has written: \"Ethics, understood as the capacity to think critically about moral values and direct our actions in terms of such values, is a generic human capacity.\" Ethics can also be used to describe a particular person's own idiosyncratic principles or habits. For example: \"Joe has strange ethics.\"\n\nMeta-ethics is the branch of philosophical ethics that asks how we understand, know about, and what we mean when we talk about what is right and what is wrong. An ethical question pertaining to a particular practical situation—such as, \"Should I eat this particular piece of chocolate cake?\"—cannot be a meta-ethical question (rather, this is an applied ethical question). A meta-ethical question is abstract and relates to a wide range of more specific practical questions. For example, \"Is it ever possible to have secure knowledge of what is right and wrong?\" is a meta-ethical question.\n\nMeta-ethics has always accompanied philosophical ethics. For example, Aristotle implies that less precise knowledge is possible in ethics than in other spheres of inquiry, and he regards ethical knowledge as depending upon habit and acculturation in a way that makes it distinctive from other kinds of knowledge. Meta-ethics is also important in G.E. Moore's \"Principia Ethica\" from 1903. In it he first wrote about what he called \"the naturalistic fallacy\". Moore was seen to reject naturalism in ethics, in his Open Question Argument. This made thinkers look again at second order questions about ethics. Earlier, the Scottish philosopher David Hume had put forward a similar view on the difference between facts and values.\n\nStudies of how we know in ethics divide into cognitivism and non-cognitivism; this is quite akin to the thing called descriptive and non-descriptive. Non-cognitivism is the view that when we judge something as morally right or wrong, this is neither true nor false. We may, for example, be only expressing our emotional feelings about these things. Cognitivism can then be seen as the claim that when we talk about right and wrong, we are talking about matters of fact.\n\nThe ontology of ethics is about value-bearing things or properties, i.e. the kind of things or stuff referred to by ethical propositions. Non-descriptivists and non-cognitivists believe that ethics does not need a specific ontology since ethical propositions do not refer. This is known as an anti-realist position. Realists, on the other hand, must explain what kind of entities, properties or states are relevant for ethics, how they have value, and why they guide and motivate our actions.\n\nNormative ethics is the study of ethical action. It is the branch of ethics that investigates the set of questions that arise when considering how one ought to act, morally speaking. Normative ethics is distinct from meta-ethics because normative ethics examines standards for the rightness and wrongness of actions, while meta-ethics studies the meaning of moral language and the metaphysics of moral facts. Normative ethics is also distinct from descriptive ethics, as the latter is an empirical investigation of people's moral beliefs. To put it another way, descriptive ethics would be concerned with determining what proportion of people believe that killing is always wrong, while normative ethics is concerned with whether it is correct to hold such a belief. Hence, normative ethics is sometimes called prescriptive, rather than descriptive. However, on certain versions of the meta-ethical view called moral realism, moral facts are both descriptive and prescriptive at the same time.\n\nTraditionally, normative ethics (also known as moral theory) was the study of what makes actions right and wrong. These theories offered an overarching moral principle one could appeal to in resolving difficult moral decisions.\n\nAt the turn of the 20th century, moral theories became more complex and were no longer concerned solely with rightness and wrongness, but were interested in many different kinds of moral status. During the middle of the century, the study of normative ethics declined as meta-ethics grew in prominence. This focus on meta-ethics was in part caused by an intense linguistic focus in analytic philosophy and by the popularity of logical positivism.\n\nVirtue ethics describes the character of a moral agent as a driving force for ethical behavior, and it is used to describe the ethics of Socrates, Aristotle, and other early Greek philosophers. Socrates (469–399 BC) was one of the first Greek philosophers to encourage both scholars and the common citizen to turn their attention from the outside world to the condition of humankind. In this view, knowledge bearing on human life was placed highest, while all other knowledge was secondary. Self-knowledge was considered necessary for success and inherently an essential good. A self-aware person will act completely within his capabilities to his pinnacle, while an ignorant person will flounder and encounter difficulty. To Socrates, a person must become aware of every fact (and its context) relevant to his existence, if he wishes to attain self-knowledge. He posited that people will naturally do what is good if they know what is right. Evil or bad actions are the results of ignorance. If a criminal was truly aware of the intellectual and spiritual consequences of his or her actions, he or she would neither commit nor even consider committing those actions. Any person who knows what is truly right will automatically do it, according to Socrates. While he correlated knowledge with virtue, he similarly equated virtue with joy. The truly wise man will know what is right, do what is good, and therefore be happy.\n\nAristotle (384–323 BC) posited an ethical system that may be termed \"virtuous\". In Aristotle's view, when a person acts in accordance with virtue this person will do good and be content. Unhappiness and frustration are caused by doing wrong, leading to failed goals and a poor life. Therefore, it is imperative for people to act in accordance with virtue, which is only attainable by the practice of the virtues in order to be content and complete. Happiness was held to be the ultimate goal. All other things, such as civic life or wealth, were only made worthwhile and of benefit when employed in the practice of the virtues. The practice of the virtues is the surest path to happiness.\n\nAristotle asserted that the soul of man had three natures: body (physical/metabolism), animal (emotional/appetite), and rational (mental/conceptual). Physical nature can be assuaged through exercise and care; emotional nature through indulgence of instinct and urges; and mental nature through human reason and developed potential. Rational development was considered the most important, as essential to philosophical self-awareness and as uniquely human. Moderation was encouraged, with the extremes seen as degraded and immoral. For example, courage is the moderate virtue between the extremes of cowardice and recklessness. Man should not simply live, but live well with conduct governed by virtue. This is regarded as difficult, as virtue denotes doing the right thing, in the right way, at the right time, for the right reason.\n\nThe Stoic philosopher Epictetus posited that the greatest good was contentment and serenity. Peace of mind, or \"apatheia\", was of the highest value; self-mastery over one's desires and emotions leads to spiritual peace. The \"unconquerable will\" is central to this philosophy. The individual's will should be independent and inviolate. Allowing a person to disturb the mental equilibrium is, in essence, offering yourself in slavery. If a person is free to anger you at will, you have no control over your internal world, and therefore no freedom. Freedom from material attachments is also necessary. If a thing breaks, the person should not be upset, but realize it was a thing that could break. Similarly, if someone should die, those close to them should hold to their serenity because the loved one was made of flesh and blood destined to death. Stoic philosophy says to accept things that cannot be changed, resigning oneself to the existence and enduring in a rational fashion. Death is not feared. People do not \"lose\" their life, but instead \"return\", for they are returning to God (who initially gave what the person is as a person). Epictetus said difficult problems in life should not be avoided, but rather embraced. They are spiritual exercises needed for the health of the spirit, just as physical exercise is required for the health of the body. He also stated that sex and sexual desire are to be avoided as the greatest threat to the integrity and equilibrium of a man's mind. Abstinence is highly desirable. Epictetus said remaining abstinent in the face of temptation was a victory for which a man could be proud.\n\nModern virtue ethics was popularized during the late 20th century in large part as a response to G. E. M. Anscombe's \"Modern Moral Philosophy\". Anscombe argues that consequentialist and deontological ethics are only feasible as universal theories if the two schools ground themselves in divine law. As a deeply devoted Christian herself, Anscombe proposed that either those who do not give ethical credence to notions of divine law take up virtue ethics, which does not necessitate universal laws as agents themselves are investigated for virtue or vice and held up to \"universal standards\", or that those who wish to be utilitarian or consequentialist ground their theories in religious conviction. Alasdair MacIntyre, who wrote the book \"After Virtue\", was a key contributor and proponent of modern virtue ethics, although some claim that MacIntyre supports a relativistic account of virtue based on cultural norms, not objective standards. Martha Nussbaum, a contemporary virtue ethicist, objects to MacIntyre's relativism, among that of others, and responds to relativist objections to form an objective account in her work \"Non-Relative Virtues: An Aristotelian Approach\". However, Nussbaum's accusation of relativism appears to be a misreading. In \"Whose Justice, Whose Rationality?\", MacIntyre's ambition of taking a rational path beyond relativism was quite clear when he stated \"rival claims made by different traditions […] are to be evaluated […] without relativism\" (p. 354) because indeed \"rational debate between and rational choice among rival traditions is possible” (p. 352). \"Complete Conduct Principles for the 21st Century\" blended the Eastern virtue ethics and the Western virtue ethics, with some modifications to suit the 21st Century, and formed a part of contemporary virtue ethics.\n\nHedonism posits that the principal ethic is maximizing pleasure and minimizing pain. There are several schools of Hedonist thought ranging from those advocating the indulgence of even momentary desires to those teaching a pursuit of spiritual bliss. In their consideration of consequences, they range from those advocating self-gratification regardless of the pain and expense to others, to those stating that the most ethical pursuit maximizes pleasure and happiness for the most people.\n\nFounded by Aristippus of Cyrene, Cyrenaics supported immediate gratification or pleasure. \"Eat, drink and be merry, for tomorrow we die.\" Even fleeting desires should be indulged, for fear the opportunity should be forever lost. There was little to no concern with the future, the present dominating in the pursuit of immediate pleasure. Cyrenaic hedonism encouraged the pursuit of enjoyment and indulgence without hesitation, believing pleasure to be the only good.\n\nEpicurean ethics is a hedonist form of virtue ethics. Epicurus \"...presented a sustained argument that pleasure, correctly understood, will coincide with virtue.\" He rejected the extremism of the Cyrenaics, believing some pleasures and indulgences to be detrimental to human beings. Epicureans observed that indiscriminate indulgence sometimes resulted in negative consequences. Some experiences were therefore rejected out of hand, and some unpleasant experiences endured in the present to ensure a better life in the future. To Epicurus, the \"summum bonum\", or greatest good, was prudence, exercised through moderation and caution. Excessive indulgence can be destructive to pleasure and can even lead to pain. For example, eating one food too often makes a person lose a taste for it. Eating too much food at once leads to discomfort and ill-health. Pain and fear were to be avoided. Living was essentially good, barring pain and illness. Death was not to be feared. Fear was considered the source of most unhappiness. Conquering the fear of death would naturally lead to a happier life. Epicurus reasoned if there were an afterlife and immortality, the fear of death was irrational. If there was no life after death, then the person would not be alive to suffer, fear or worry; he would be non-existent in death. It is irrational to fret over circumstances that do not exist, such as one's state of death in the absence of an afterlife.\n\nState consequentialism, also known as Mohist consequentialism, is an ethical theory that evaluates the moral worth of an action based on how much it contributes to the basic goods of a state. The \"Stanford Encyclopedia of Philosophy\" describes Mohist consequentialism, dating back to the 5th century BC, as \"a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare\". Unlike utilitarianism, which views pleasure as a moral good, \"the basic goods in Mohist consequentialist thinking are ... order, material wealth, and increase in population\". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The \"material wealth\" of Mohist consequentialism refers to basic needs like shelter and clothing, and the \"order\" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability.\n\nStanford sinologist David Shepherd Nivison, in \"The Cambridge History of Ancient China\", writes that the moral goods of Mohism \"are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth ... if people have plenty, they would be good, filial, kind, and so on unproblematically.\" The Mohists believed that morality is based on \"promoting the benefit of all under heaven and eliminating harm to all under heaven\". In contrast to Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain.\n\nConsequentialism refers to moral theories that hold the consequences of a particular action form the basis for any valid moral judgment about that action (or create a structure for judgment, see rule consequentialism). Thus, from a consequentialist standpoint, a morally right action is one that produces a good outcome, or consequence. This view is often expressed as the aphorism \"The ends justify the means\".\n\nThe term \"consequentialism\" was coined by G. E. M. Anscombe in her essay \"Modern Moral Philosophy\" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick. Since then, the term has become common in English-language ethical theory.\n\nThe defining feature of consequentialist moral theories is the weight given to the consequences in evaluating the rightness and wrongness of actions. In consequentialist theories, the consequences of an action or rule generally outweigh other considerations. Apart from this basic outline, there is little else that can be unequivocally said about consequentialism as such. However, there are some questions that many consequentialist theories address:\n\nOne way to divide various consequentialisms is by the many types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase and positive effect, and the best action is one that results in that effect for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral \"pleasure\". Other theories adopt a package of several goods, all to be promoted equally. Whether a particular consequentialist theory focuses on a single good or many, conflicts and tensions between different good states of affairs are to be expected and must be adjudicated.\n\nUtilitarianism is an ethical theory that argues the proper course of action is one that maximizes a positive effect, such as \"happiness\", \"welfare\", or the ability to live according to personal preferences. Jeremy Bentham and John Stuart Mill are influential proponents of this school of thought. In \"A Fragment on Government\" Bentham says 'it is the greatest happiness of the greatest number that is the measure of right and wrong' and describes this as a fundamental axiom. In \"An Introduction to the Principles of Morals and Legislation\" he talks of 'the principle of utility' but later prefers \"the greatest happiness principle\".\n\nUtilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that the morally correct action is the one that produces the best outcome for all people affected by the action. John Stuart Mill, in his exposition of utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. Other noteworthy proponents of utilitarianism are neuroscientist Sam Harris, author of \"The Moral Landscape\", and moral philosopher Peter Singer, author of, amongst other works, \"Practical Ethics\".\n\nThe major division within utilitarianism is between \"act utilitarianism\" and \"rule utilitarianism\". In act utilitarianism, the principle of utility applies directly to each alternative act in a situation of choice. The right act is the one that brings about the best results (or the least amount of bad results). In rule utilitarianism, the principle of utility determines the validity of rules of conduct (moral principles). A rule like promise-keeping is established by looking at the consequences of a world in which people break promises at will and a world in which promises are binding. Right and wrong are the following or breaking of rules that are sanctioned by their utilitarian value. A proposed \"middle ground\" between these two types is Two-level utilitarianism, where rules are applied in ordinary circumstances, but with an allowance to choose actions outside of such rules when unusual situations call for it.\n\nDeontological ethics or deontology (from Greek , \"deon\", \"obligation, duty\"; and , \"-logia\") is an approach to ethics that determines goodness or rightness from examining acts, or the rules and duties that the person doing the act strove to fulfill. This is in contrast to consequentialism, in which rightness is based on the consequences of an act, and not the act by itself. Under deontology, an act may be considered right even if the act produces a bad consequence, if it follows the \"rule\" or moral law. According to the deontological view, people have a \"duty\" to act in a way that does those things that are inherently good as acts (\"truth-telling\" for example), or follow an objectively obligatory rule (as in rule utilitarianism).\n\nImmanuel Kant's theory of ethics is considered deontological for several different reasons. First, Kant argues that to act in the morally right way, people must act from duty (\"deon\"). Second, Kant argued that it was not the consequences of actions that make them right or wrong but the motives (expressed as maxims) of the person who carries out the action. Kant's argument that to act in the morally right way, one must act from duty, begins with an argument that the highest good must be both good in itself, and good without qualification. Something is 'good in itself' when it is intrinsically good, and 'good without qualification' when the addition of that thing never makes a situation ethically worse. Kant then argues that those things that are usually thought to be good, such as intelligence, perseverance and pleasure, fail to be either intrinsically good or good without qualification. Pleasure, for example, appears to not be good without qualification, because when people take pleasure in watching someone suffer, they make the situation ethically worse. He concludes that there is only one thing that is truly good:\n\nNothing in the world—indeed nothing even beyond the world—can possibly be conceived which could be called good without qualification except a \"good will\".\n\nAssociated with the pragmatists, Charles Sanders Peirce, William James, and especially John Dewey, pragmatic ethics holds that moral correctness evolves similarly to scientific knowledge: socially over the course of many lifetimes. Thus, we should prioritize social reform over attempts to account for consequences, individual virtue or duty (although these may be worthwhile attempts, if social reform is provided for).\n\nCare ethics contrasts with more well-known ethical models, such as consequentialist theories (e.g. utilitarianism) and deontological theories (e.g., Kantian ethics) in that it seeks to incorporate traditionally feminized virtues and values that—proponents of care ethics contend—are absent in such traditional models of ethics. These values include the importance of empathetic relationships and compassion.\n\nCare-focused feminism is a branch of feminist thought, informed primarily by ethics of care as developed by Carol Gilligan and Nel Noddings. This body of theory is critical of how caring is socially assigned to women, and consequently devalued. They write, “Care-focused feminists regard women’s capacity for care as a human strength,” that should be taught to and expected of men as well as women. Noddings proposes that ethical caring has the potential to be a more concrete evaluative model of moral dilemma than an ethic of justice. Noddings’ care-focused feminism requires practical application of relational ethics, predicated on an ethic of care.\n\nRole ethics is an ethical theory based on family roles. Unlike virtue ethics, role ethics is not individualistic. Morality is derived from a person's relationship with their community. Confucian ethics is an example of role ethics though this is not straightforwardly uncontested. Confucian roles center around the concept of filial piety or \"xiao\", a respect for family members. According to Roger T. Ames and Henry Rosemont, \"Confucian normativity is defined by living one's family roles to maximum effect.\" Morality is determined through a person's fulfillment of a role, such as that of a parent or a child. Confucian roles are not rational, and originate through the \"xin\", or human emotions.\n\nAnarchist ethics is an ethical theory based on the studies of anarchist thinkers. The biggest contributor to the anarchist ethics is the Russian zoologist, geographer, economist, and political activist Peter Kropotkin.\n\nStarting from the premise that the goal of ethical philosophy should be to help humans adapt and thrive in evolutionary terms, Kropotkin's ethical framework uses biology and anthropology as a basis – in order to scientifically establish what will best enable a given social order to thrive biologically and socially – and advocates certain behavioural practices to enhance humanity's capacity for freedom and well-being, namely practices which emphasise solidarity, equality, and justice.\n\nKropotkin argues that ethics itself is evolutionary, and is inherited as a sort of a social instinct through cultural history, and by so, he rejects any religious and transcendental explanation of morality. The origin of ethical feeling in both animals and humans can be found, he claims, in the natural fact of \"sociality\" (mutualistic symbiosis), which humans can then combine with the instinct for justice (i.e. equality) and then with the practice of reason to construct a non-supernatural and anarchistic system of ethics. Kropotkin suggests that the principle of equality at the core of anarchism is the same as the Golden rule: This principle of treating others as one wishes to be treated oneself, what is it but the very same principle as equality, the fundamental principle of anarchism? And how can any one manage to believe himself an anarchist unless he practices it? We do not wish to be ruled. And by this very fact, do we not declare that we ourselves wish to rule nobody? We do not wish to be deceived, we wish always to be told nothing but the truth. And by this very fact, do we not declare that we ourselves do not wish to deceive anybody, that we promise to always tell the truth, nothing but the truth, the whole truth? We do not wish to have the fruits of our labor stolen from us. And by that very fact, do we not declare that we respect the fruits of others' labor? By what right indeed can we demand that we should be treated in one fashion, reserving it to ourselves to treat others in a fashion entirely different? Our sense of equality revolts at such an idea.\n\nThe 20th century saw a remarkable expansion and evolution of critical theory, following on earlier Marxist Theory efforts to locate individuals within larger structural frameworks of ideology and action.\n\nAntihumanists such as Louis Althusser, Michel Foucault and structuralists such as Roland Barthes challenged the possibilities of individual agency and the coherence of the notion of the 'individual' itself. This was on the basis that personal identity was, at least in part, a social construction. As critical theory developed in the later 20th century, post-structuralism sought to problematize human relationships to knowledge and 'objective' reality. Jacques Derrida argued that access to meaning and the 'real' was always deferred, and sought to demonstrate via recourse to the linguistic realm that \"there is no outside-text/non-text\" (\"il n'y a pas de hors-texte\" is often mistranslated as \"there is nothing outside the text\"); at the same time, Jean Baudrillard theorised that signs and symbols or simulacra mask reality (and eventually the absence of reality itself), particularly in the consumer world.\n\nPost-structuralism and postmodernism argue that ethics must study the complex and relational conditions of actions. A simple alignment of ideas of right and particular acts is not possible. There will always be an ethical remainder that cannot be taken into account or often even recognized. Such theorists find narrative (or, following Nietzsche and Foucault, genealogy) to be a helpful tool for understanding ethics because narrative is always about particular lived experiences in all their complexity rather than the assignment of an idea or norm to separate and individual actions.\n\nZygmunt Bauman says postmodernity is best described as modernity without illusion, the illusion being the belief that humanity can be repaired by some ethic principle. Postmodernity can be seen in this light as accepting the messy nature of humanity as unchangeable.\n\nDavid Couzens Hoy states that Emmanuel Levinas's writings on the face of the Other and Derrida's meditations on the relevance of death to ethics are signs of the \"ethical turn\" in Continental philosophy that occurred in the 1980s and 1990s. Hoy describes post-critique ethics as the \"obligations that present themselves as necessarily to be fulfilled but are neither forced on one or are enforceable\" (2004, p. 103).\n\nHoy's post-critique model uses the term \"ethical resistance\". Examples of this would be an individual's resistance to consumerism in a retreat to a simpler but perhaps harder lifestyle, or an individual's resistance to a terminal illness. Hoy describes Levinas's account as \"not the attempt to use power against itself, or to mobilize sectors of the population to exert their political power; the ethical resistance is instead the resistance of the powerless\"(2004, p. 8).\n\nHoy concludes that\n\nApplied ethics is a discipline of philosophy that attempts to apply ethical theory to real-life situations. The discipline has many specialized fields, such as engineering ethics, bioethics, geoethics, public service ethics and business ethics.\n\nApplied ethics is used in some aspects of determining public policy, as well as by individuals facing difficult decisions. The sort of questions addressed by applied ethics include: \"Is getting an abortion immoral?\" \"Is euthanasia immoral?\" \"Is affirmative action right or wrong?\" \"What are human rights, and how do we determine them?\" \"Do animals have rights as well?\" and \"Do individuals have the right of self-determination?\"\n\nA more specific question could be: \"If someone else can make better out of his/her life than I can, is it then moral to sacrifice myself for them if needed?\" Without these questions, there is no clear fulcrum on which to balance law, politics, and the practice of arbitration—in fact, no common assumptions of all participants—so the ability to formulate the questions are prior to rights balancing. But not all questions studied in applied ethics concern public policy. For example, making ethical judgments regarding questions such as, \"Is lying always wrong?\" and, \"If not, when is it permissible?\" is prior to any etiquette.\n\nPeople, in general, are more comfortable with dichotomies (two opposites). However, in ethics, the issues are most often multifaceted and the best-proposed actions address many different areas concurrently. In ethical decisions, the answer is almost never a \"yes or no\", \"right or wrong\" statement. Many buttons are pushed so that the overall condition is improved and not to the benefit of any particular faction.\n\nBioethics is the study of controversial ethics brought about by advances in biology and medicine. Bioethicists are concerned with the ethical questions that arise in the relationships among life sciences, biotechnology, medicine, politics, law, and philosophy. It also includes the study of the more commonplace questions of values (\"the ethics of the ordinary\") that arise in primary care and other branches of medicine.\n\nBioethics also needs to address emerging biotechnologies that affect basic biology and future humans. These developments include cloning, gene therapy, human genetic engineering, astroethics and life in space, and manipulation of basic biology through altered DNA, RNA and proteins, e.g. \"three parent baby, where baby is born from genetically modified embryos, would have DNA from a mother, a father and from a female donor. Correspondingly, new bioethics also need to address life at its core. For example, biotic ethics value organic gene/protein life itself and seek to propagate it. With such life-centered principles, ethics may secure a cosmological future for life.\n\nBusiness ethics (also corporate ethics) is a form of applied ethics or professional ethics that examines ethical principles and moral or ethical problems that arise in a business environment, including fields like medical ethics. Business ethics represents the practices that any individual or group exhibits within an organization that can negatively or positively affect the businesses core values. It applies to all aspects of business conduct and is relevant to the conduct of individuals and entire organizations.\n\nBusiness ethics has both normative and descriptive dimensions. As a corporate practice and a career specialization, the field is primarily normative. Academics attempting to understand business behavior employ descriptive methods. The range and quantity of business ethical issues reflect the interaction of profit-maximizing behavior with non-economic concerns. Interest in business ethics accelerated dramatically during the 1980s and 1990s, both within major corporations and within academia. For example, today most major corporations promote their commitment to non-economic values under headings such as ethics codes and social responsibility charters. Adam Smith said, \"People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices.\" Governments use laws and regulations to point business behavior in what they perceive to be beneficial directions. Ethics implicitly regulates areas and details of behavior that lie beyond governmental control. The emergence of large corporations with limited relationships and sensitivity to the communities in which they operate accelerated the development of formal ethics regimes.\n\nIn \"Moral Machines: Teaching Robots Right from Wrong\", Wendell Wallach and Colin Allen conclude that issues in machine ethics will likely drive advancement in understanding of human ethics by forcing us to address gaps in modern normative theory and by providing a platform for experimental investigation. The effort to actually program a machine or artificial agent to behave as though instilled with a sense of ethics requires new specificity in our normative theories, especially regarding aspects customarily considered common-sense. For example, machines, unlike humans, can support a wide selection of learning algorithms, and controversy has arisen over the relative ethical merits of these options. This may reopen classic debates of normative ethics framed in new (highly technical) terms.\n\nMilitary ethics are concerned with questions regarding the application of force and the ethos of the soldier and are often understood as applied professional ethics. Just war theory is generally seen to set the background terms of military ethics. However individual countries and traditions have different fields of attention.\n\nMilitary ethics involves multiple subareas, including the following among others:\n\nPolitical ethics (also known as political morality or public ethics) is the practice of making moral judgements about political action and political agents.\n\nPublic sector ethics is a set of principles that guide public officials in their service to their constituents, including their decision-making on behalf of their constituents. Fundamental to the concept of public sector ethics is the notion that decisions and actions are based on what best serves the public's interests, as opposed to the official's personal interests (including financial interests) or self-serving political interests.\n\nPublication ethics is the set of principles that guide the writing and publishing process for all professional publications. To follow these principles, authors must verify that the publication does not contain plagiarism or publication bias. As a way to avoid misconduct in research these principles can also apply to experiments that are referenced or analyzed in publications by ensuring the data is recorded honestly and accurately.\n\nPlagiarism is the failure to give credit to another author’s work or ideas, when it is used in the publication. It is the obligation of the editor of the journal to ensure the article does not contain any plagiarism before it is published. If a publication that has already been published is proven to contain plagiarism, the editor of the journal can retract the article.\n\nPublication bias occurs when the publication is one-sided or \"prejudiced against results\". In best practice, an author should try to include information from all parties involved, or affected by the topic. If an author is prejudiced against certain results, than it can \"lead to erroneous conclusions being drawn\".\n\nMisconduct in research can occur when an experimenter falsifies results. Falsely recorded information occurs when the researcher \"fakes\" information or data, which was not used when conducting the actual experiment. By faking the data, the researcher can alter the results from the experiment to better fit the hypothesis they originally predicted. When conducting medical research, it is important to honor the healthcare rights of a patient by protecting their anonymity in the publication.\n\"Respect for autonomy\" is the principle that decision-making should allow individuals to be autonomous; they should be able to make decisions that apply to their own lives. This means that individuals should have control of their lives.\n\"Justice\" is the principle that decision-makers must focus on actions that are fair to those affected. Ethical decisions need to be consistent with the ethical theory. There are cases where the management has made decisions that seem to be unfair to the employees, shareholders, and other stakeholders (Solomon, 1992, pp49). Such decisions are unethical.\n\nRelational ethics are related to an ethics of care. They are used in qualitative research, especially ethnography and autoethnography. Researchers who employ relational ethics value and respect the connection between themselves and the people they study, and \"...between researchers and the communities in which they live and work.\" (Ellis, 2007, p. 4). Relational ethics also help researchers understand difficult issues such as conducting research on intimate others that have died and developing friendships with their participants. Relational ethics in close personal relationships form a central concept of contextual therapy.\n\nAnimal ethics is a term used in academia to describe human-animal relationships and how animals ought to be treated. The subject matter includes animal rights, animal welfare, animal law, speciesism, animal cognition, wildlife conservation, the moral status of nonhuman animals, the concept of nonhuman personhood, human exceptionalism, the history of animal use, and theories of justice.\n\nMoral psychology is a field of study that began as an issue in philosophy and that is now properly considered part of the discipline of psychology. Some use the term \"moral psychology\" relatively narrowly to refer to the study of moral development. However, others tend to use the term more broadly to include any topics at the intersection of ethics and psychology (and philosophy of mind). Such topics are ones that involve the mind and are relevant to moral issues. Some of the main topics of the field are moral responsibility, moral development, moral character (especially as related to virtue ethics), altruism, psychological egoism, moral luck, and moral disagreement.\n\nEvolutionary ethics concerns approaches to ethics (morality) based on the role of evolution in shaping human psychology and behavior. Such approaches may be based in scientific fields such as evolutionary psychology or sociobiology, with a focus on understanding and explaining observed ethical preferences and choices.\n\nDescriptive ethics is on the less philosophical end of the spectrum since it seeks to gather particular information about how people live and draw general conclusions based on observed patterns. Abstract and theoretical questions that are more clearly philosophical—such as, \"Is ethical knowledge possible?\"—are not central to descriptive ethics. Descriptive ethics offers a value-free approach to ethics, which defines it as a social science rather than a humanity. Its examination of ethics doesn't start with a preconceived theory but rather investigates observations of actual choices made by moral agents in practice. Some philosophers rely on descriptive ethics and choices made and unchallenged by a society or culture to derive categories, which typically vary by context. This can lead to situational ethics and situated ethics. These philosophers often view aesthetics, etiquette, and arbitration as more fundamental, percolating \"bottom up\" to imply the existence of, rather than explicitly prescribe, theories of value or of conduct. The study of descriptive ethics may include examinations of the following:\n\n\n\n"}
{"id": "21968037", "url": "https://en.wikipedia.org/wiki?curid=21968037", "title": "FDU materials", "text": "FDU materials\n\nFDU Materials are a class of regularly structured mesoporous organic materials first synthesized at Fudan University in Shanghai, China (hence FDU). FDU-14 -15 and -16 are formed by polymerizing resol around a lyotropic liquid crystal template and then removing the template by calcination.\n\n"}
{"id": "1786604", "url": "https://en.wikipedia.org/wiki?curid=1786604", "title": "Global Standard Stratigraphic Age", "text": "Global Standard Stratigraphic Age\n\nIn the stratigraphy sub-discipline of geology, a Global Standard Stratigraphic Age, abbreviated GSSA, is a chronological reference point and criterion in the geologic record used to define the boundaries (an internationally sanctioned benchmark point) between different geological periods, epochs or ages on the overall geologic time scale in a chronostratigraphically useful rock layer. A worldwide multidisciplinary effort has been ongoing since 1974 to define such important metrics. The points and strata need be widespread and contain an identifiable sequence of layers or other unambiguous marker (identifiable or quantifiable) attributes.\n\nGSSAs, and the generally more recent and preferred benchmark GSSPs are defined by the International Commission on Stratigraphy (ICS) under the auspices of their parent organization, the International Union of Geological Sciences (IUGS), and are used primarily for time dating of rock layers older than 630 million years ago, lacking a good fossil record. The ICS first attempts to meet the standards of the GSSPs (see below) and if those fail, usually have enough information to make a preliminary selection of several competing GSSA prospects or proposals.\n\nThe geologic record becomes spotty prior to about 542 million years ago. This is because the Earth's crust in geological time scales is constantly being recycled by tectonic and weathering forces, and older rocks and especially readily accessible exposed strata that can act as a time calibration are rare.\n\nFor more recent periods, a Global Boundary Stratotype Section and Point (GSSP), largely based on paleontology and improved methods of fossil dating, is used to define such boundaries. In contrast to GSSAs, GSSPs are based on important events and transitions within a particular stratigraphic section. In older sections, there is insufficient fossil record or well preserved sections to identify the key events necessary for a GSSP, so GSSAs are defined based on fixed dates and selected criteria.\n\n\n\n"}
{"id": "21820838", "url": "https://en.wikipedia.org/wiki?curid=21820838", "title": "International Association of Cryospheric Sciences", "text": "International Association of Cryospheric Sciences\n\nThe International Association of Cryospheric Sciences, or IACS, is the eighth association of the International Union of Geodesy and Geophysics (IUGG). It was launched by the IUGG Council on 2007-07-04, developing from the International Commission of Snow and Ice of the International Association of Hydrological Sciences (IAHS) via the transitional Union Commission for the Cryospheric Sciences (UCCS).\n\nFormation of this new Association is recognition of the importance of the cryosphere in the study of Earth System Science, and particularly at a time of significant global change. Accordingly, cryospheric sciences is an umbrella term for the study of the cryosphere (not unlike atmospheric sciences, encompassing meteorology, climatology, and aeronomy); as an interdisciplinary Earth science, many disciplines contribute to it, most notably geology, hydrology, and meteorology and climatology; in this sense, it is comparable to glaciology. IACS has historic connections going back to the establishment of the Commission Internationale des Glaciers (International Glacier Commission) in 1894.\n\nThe broad objectives of IACS are:\n\n(at present IACS has responsibility within the International Council of Science for the World Glacier Monitoring Service (WGMS)).\n\nIACS is structured around a number of disciplinary Divisions. Currently these are:\n\n\nA number of working groups under these Divisions address scientific problems of the cryosphere that are timely and well constrained. Recent examples include a working group on \"Intercomparison of Forest Snow Process Models\", and working groups that have developed a new \"International Classification for Seasonal Snow on the Ground\" and a new \"Glossary of Mass Balance and Related Terms\".\n\n"}
{"id": "51139305", "url": "https://en.wikipedia.org/wiki?curid=51139305", "title": "JJ Electronic", "text": "JJ Electronic\n\nJJ Electronic, s.r.o is one of the world's remaining producers of vacuum tubes. They are based in Čadca, in the Kysuce region of Slovakia.\nMost of the products that JJ offers are audio preamplifier tubes, and tubes for audio power amplifiers. These vacuum tubes are mainly used for guitar and hi-fi amplifiers. Technically, the vacuum tubes produced by JJ Electronic are mainly low-noise triodes, beam tetrodes and power pentodes. Double diode vacuum tubes for AC-to-DC rectifiers are also produced. JJ also produces electrolytic capacitors for higher voltage purposes, generally for use in audio amplifiers. JJ also manufactures their own line of high-end audio amplifiers and guitar amplifiers. \nIn 2015, the company sales amounted to EUR 8.5 million and net income came to EUR 3.8 million. Most of the products are exported to the United States.\n\nBefore 1989, TESLA was the main Czechoslovak producer of electron tubes. While TESLA vacuum tubes were exported all over the world, and were known for their quality, the company did not survive the change of economic system after 1989 in combination with the downturn in the vacuum tube market. JJ Electronic was founded in 1993, using the old TESLA machines for the manufacture of vacuum tubes. Eventually, JJ Electronic started to produce their own line of vacuum tubes and electrolytic capacitors, being mainly targeted at high-end audiophile and guitar amplifier applications.\n\nLow power vacuum tubes\n\nPower vacuum tubes\n"}
{"id": "13794001", "url": "https://en.wikipedia.org/wiki?curid=13794001", "title": "Janus-faced molecule", "text": "Janus-faced molecule\n\nA Janus molecule (or Janus-faced molecule) is a molecule which can represent both beneficial and toxic effects. Examples are nitric oxide and cholesterol. In the case of cholesterol, the property that makes cholesterol useful in cell membranes, namely its absolute insolubility in water, also makes it lethal. When cholesterol accumulates in the wrong place, for example within the wall of an artery, it cannot be readily mobilized, and its presence eventually leads to the development of an atherosclerotic plaque.\nJanus-faced molecules are widely spread in our biology, but often their mechanism of action is difficult to blend in existing therapies on pathogenetic and mechanistic explanations. Therefore, both in academia as well as in pharmaceutical industry people shy away from digging too deep, as Janus-faced molecules act based on the context in which these molecules operate. \n\n"}
{"id": "17810613", "url": "https://en.wikipedia.org/wiki?curid=17810613", "title": "Jonathan Elphick", "text": "Jonathan Elphick\n\nJonathan Elphick is a natural history author, editor and consultant. He is an eminent ornithologist, a qualified zoologist; Fellow of the Zoological Society of London and a Fellow of the Linnean Society of London. He is author of \"The Birdwatcher's Handbook: A Guide to the Birds of Britain and Ireland\"; \"Birds: The Art of Ornithology\" and \"The Natural History Museum Atlas of Bird Migration: Tracing the Great Journeys of the World's Birds\", which received Bird Watching Magazine's 'Best Bird Reference Book of the Year'; as well as co-author of the \"Encyclopedia of Animals\"; the \"RSPB Pocket Birds; A Unique Photographic Guide to the Birds of Britain and Europe\" with Jonathan Woodward and \"The National Parks and other Wild Places of Britain and Ireland\", with photography by David Tipling.\n\nHe has also been consultant, editor or author on a variety of other books, articles and CD-ROMs including \"Coastline\" with Greenpeace, and the BBC production of \"The Realms of the Russian Bear\".\n\nBorn in 1945, in Prestatyn, in what was then the historic county of Flintshire () until re-organisation made it part of Clwyd and then Denbighshire; Jonathan Elphick was raised in North Wales, surrounded by a mountainous and beautiful mosaic of habitats: including traditional mixed farmland, woods, rivers, lakes, hills and coastlines. He spent the first four years of his life in Rhyl, then moved to Dyserth, where he attended Ysgol Hiraddug, Dyserth, moving on to St Asaph's Grammar School and ultimately University College, Swansea (1964–1968), one of the four colleges comprising the University of Wales.\n\nBirdlife in North Wales in the early 1950s was relatively unaffected by agricultural developments and other changes, that proved to be major factors in the decline of many British birds.\n\nInspired by, among others, the writings of Bruce Campbell one of the foremost ornithologists of the day, Elphick's journey through ornithological literature began with the paperback \"Bird Watching for Beginners\" and \"Campbell's Birds in Colour\". Migrating from the \"Bird Recognition\" paperback guides (by the ornithologist James Fisher) he came under the influence of the Nature Conservancy warden at Newborough Warren National Nature Reserve, on the Isle of Anglesey, Peter Hope-Jones. In 1969 Elphick gained a BSc. degree in Zoology and secured a job as a Natural history editor, working as an in-house editor for various publishers including Dorling Kindersley, eventually going freelance and specialising in Birds.\n\nElphick worked for five years as researcher on the best-selling book \"Birds Britannica\", written by Mark Cocker. He specialises in the cultural history of birds and our changing attitudes to them. His work encompasses a variety of aspects that interest him, including practical field ornithology, biological research, the history of natural history, as well as ornithological art, all of which inform his personal reaction to birds. Elphick is a keen conservationist who has travelled extensively to both study and make people aware of the importance of the Conservation movement at home and abroad.\n\nThe Natural History Museum in London is the repository for some half a million works on paper and one million books, which are the sources for some exceptionally beautiful and important images of birds. Many are from rare sources. Elphick's work is a selection of art works from this archive, including the work of artists such as John James Audubon, along with Victorian explorers, who catalogued the world's avifauna before the age of photography. It documents the work of many natural history artists, such as John Gould, William MacGillivray and Ferdinand Bauer. The text interweaves ornithological science, art history, biography, travel and other aspects of the subject to paint a picture of the artists and the birds they painted.\n\nElphick's latest work, undertaken with the photographer David Tipling, and due to be published later this year, is \"GREAT BIRDS: 200 Star Species of Britain\".\n\nElphick is part of a team of naturalists and authors, including the British wildlife photographer David Tipling and the naturalist and author Mark Cocker, undertaking the Birds and People project. \"Birds and People\" is a ten-year-long, groundbreaking collaboration between the publishers Random House and BirdLife International, to survey and document worldwide, the cultural significance of birds. The \"Birds and People\" project involves an open internet forum, for individuals worldwide to document their reflections, experiences and stories about bird.. The final book is intended as a global chorus on the relationship between human beings and birds.\n\n"}
{"id": "47227390", "url": "https://en.wikipedia.org/wiki?curid=47227390", "title": "Knee (geography)", "text": "Knee (geography)\n\nA knee, or river knee, is a bend in a river changing its course significantly within a short distance to a different direction (in an angle of around 90 degrees). It is different from a riverbend which is a single isolated bend, and from a meander which consists of several bends in a sinuous course, both without changing the river's main course. In European history, many river knees have proven to be strategically favorable locations to found cities.\n\nMany rivers have significant bends due to geological reasons:\n\n\nFurthermore, some riverbends are called \"knees\" although they are actually single bends or a part of meanders:\n\n"}
{"id": "3029165", "url": "https://en.wikipedia.org/wiki?curid=3029165", "title": "Ladinian", "text": "Ladinian\n\nThe Ladinian is a stage and age in the Middle Triassic series or epoch. It spans the time between Ma and ~237 Ma (million years ago). The Ladinian was preceded by the Anisian and succeeded by the Carnian (part of the Upper or Late Triassic).\n\nThe Ladinian is coeval with the Falangian Chinese regional stage.\n\nThe Ladinian was established by Austrian geologist Alexander Bittner in 1892. Its name comes from the Ladin people that live in the Italian Alps (in the Dolomites, then part of Austria-Hungary).\n\nThe base of the Ladinian stage is defined as the place in the stratigraphic record where the ammonite species \"Eoprotrachyceras curionii\" first appears or the first appearance of the conodont \"Budurovignathus praehungaricus\". The global reference profile for the base (the GSSP) is at an outcrop in the river bed of the Caffaro river at Bagolino, in the province of Brescia, northern Italy. The top of the Ladinian (the base of the Carnian) is at the first appearance of ammonite species \"Daxatina canadensis\".\n\nThe Ladinian is sometimes subdivided into two subages or substages, the Fassanian (early or lower) and the Longobardian (late or upper). The Ladinian contains four ammonite biozones, which are evenly distributed among the two substages:\n\nMany Ladinian and Carnian vertebrates have been discovered in the paleorrota in Brazil: Rhynchosaurs, thecodonts, exaeretodonts, \"Staurikosaurus\", \"Guaibasaurus\", \"Saturnalia tupiniquim\", \"Sacisaurus\", \"Unaysaurus\", and many others. Paleorrota lies within the Santa Maria Formation and the Caturrita Formation.\n\nVertebrates of Ladinian age include:\n\n\n\n"}
{"id": "9216083", "url": "https://en.wikipedia.org/wiki?curid=9216083", "title": "Lawsone", "text": "Lawsone\n\nLawsone (2-hydroxy-1,4-naphthoquinone), also known as hennotannic acid, is a red-orange dye present in the leaves of the henna plant (\"Lawsonia inermis\") as well as in the flower of water hyacinth (\"Eichhornia crassipes\"). Humans have used henna extracts containing lawsone as hair and skin dyes for more than 5000 years. Lawsone reacts chemically with the protein keratin in skin and hair, in a process known as Michael addition, resulting in a strong permanent stain that lasts until the skin or hair is shed. The darker colored ink is due to more lawsone-keratin interactions occurring, which evidently break down as the concentration of lawsone decreases and the tattoo fades. Lawsone strongly absorbs UV light, and aqueous extracts can be effective sunless tanning and sunscreens. Chemically, lawsone is similar to juglone, which is found in walnuts.\n\nLawsone isolation from \"Lawsonia Inermis\" can be difficult due to its easily biodegradeable nature. Isolation involves four steps:\nDuring the rinse, the lawsone will be the bottom as it has such a high density and the chlorophyll molecules will all be on the top of the mixture.\n\nLawsone is hypothesized to undergo a reaction similar to Strecker synthesis in reactions with amino acids. Recent research has been conducted on lawsone's potential applications in the forensic science field. Since lawsone shows many similarities with ninhydrin, the current reagent for latent fingerprint development, studies have been conducted to see if lawsone can be used in this field. As of now the research is inconclusive, but optimistic. Lawsone non-specifically targets primary amino acids, and displays photoluminescence with forensic light sources. It has a characteristic purple/brown coloration as opposed to the purple/blue associated with ninhydrin.\nLawsone shows promise as a reagent for fingerprint detection because of its photoluminescence maximized at 640nm, which is high enough that it avoids background interference common for ninhydrin.\n\nThe naphthoquinones lawsone methyl ether and methylene-3,3'-bilawsone are some of the active compounds in \"Impatiens balsamina\" leaves.\n"}
{"id": "43181138", "url": "https://en.wikipedia.org/wiki?curid=43181138", "title": "List of Delphinium species", "text": "List of Delphinium species\n\nSpecies of \"Delphinium\" include:\n\n\n\nReassigned:\n"}
{"id": "2431226", "url": "https://en.wikipedia.org/wiki?curid=2431226", "title": "List of Massachusetts state forests", "text": "List of Massachusetts state forests\n\nMassachusetts, with forests covering (59%) of its land area, administers more than of state forest, wildlife and watershed land under the cabinet level Executive Office of Energy and Environmental Affairs. Lands are managed by Department of Conservation and Recreation, Division of State Parks and Recreation (), Division of Fisheries and Wildlife (MassWildlife) (), and the Division of Water Supply Protection's Office of Watershed Management (). \n\n\n"}
{"id": "3485932", "url": "https://en.wikipedia.org/wiki?curid=3485932", "title": "List of Narnian creatures", "text": "List of Narnian creatures\n\nNarnian creatures are any non-human inhabitants of Narnia, the fantasy world created by C. S. Lewis as a setting for his \"The Chronicles of Narnia\". This is a series of commentaries on the creatures of Narnia. Entries include information on physical, habitual, and behavioural elements of the creatures, as well as noting any important members of the species. Each commentary draws on specific references and citations from the books and officially sanctioned Disney films. Many animals that are found in our world are also present in Narnia, and some species include talking variations. At the birth of Narnia, Aslan the lion stares at certain animals and breathes upon them. This enabled them to think and talk in a manner similar to humans, and also altered their size (MN). Smaller talking beasts – such as rodents, birds and small mammals – are generally larger than their non-talking counterparts, whereas larger talking beasts are generally smaller than average. There is never any mention of talking fish or insects, although there are Naiads, or water-spirits. Lewis freely drew on various sources for inspiration; the creatures contained in this list include many from classical mythology and English folklore.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNarnian creatures that are not mentioned in \"The Magician's Nephew\" but can be seen in the original ink illustrations by Pauline Baynes include antelopes, bisons, camels, Cape buffaloes, chickens, cobras, crocodiles, cranes, dolphins, ducks, ferrets, frogs, flamingos, gazelles, Geese, giraffes, goats, herons, hippopotamuses, hornbills, hyenas, lemurs, lizards, llamas, monkeys, mongoose, moose, otters, ostrichs, pandas, parrots, penguins, porcupines, puffins, raccoons, rats, sharks, shrews, skunks, snakes, storks, turkeys, warthogs, weasels, whales, yaks, and zebras. An Ipotane that was meant to be a Centaur can be seen in one of the illustrations in \"The Lion, the Witch, and the Wardrobe\".\n\nUnidentifiable mythical creatures are listed below:\n\n\nNarnian creatures that are not mentioned in C.S. Lewis' books have appeared in film or game adaptations of \"The Chronicles of Narnia\" include the following:\n\nThe following creatures appear in the animated adaption that was made by Children's Television Workshop:\n\n\nThe following creatures appear in the mini-series adaption that was made by BBC:\n\n\n\nIn concept art for the Disney version of LWW, some unused creatures include Naiads, Sprites, People of the Toadstool, Gorgons, Succubus, Manticores, Rhinotaurs, and the Tree Spirits that are on the White Witch's side.\n\nOriginal text for this article provided by Joshua Bell of NarniaMUSH.\n"}
{"id": "4551223", "url": "https://en.wikipedia.org/wiki?curid=4551223", "title": "List of Pinguicula species", "text": "List of Pinguicula species\n\nThe genus \"Pinguicula\" contains the 83 species of butterworts, belonging to the bladderwort family (Lentibulariaceae). It has a natural distribution across most of the Northern Hemisphere, though over half of the species are concentrated in Mexico and Central America. Siegfried Jost Casper systematically divided them into three subgenera with 15 sections. Subsequent phylogenetic research showed that many of these groupings are polyphyletic, but they are used below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "36815036", "url": "https://en.wikipedia.org/wiki?curid=36815036", "title": "List of Sites of Special Scientific Interest in Neath Port Talbot", "text": "List of Sites of Special Scientific Interest in Neath Port Talbot\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in the Neath Port Talbot Area of Search (AoS).\n"}
{"id": "55306960", "url": "https://en.wikipedia.org/wiki?curid=55306960", "title": "List of species protected by CITES Appendix II", "text": "List of species protected by CITES Appendix II\n\nThis is a list of species of plants and animals protected by Appendix II of the Convention on International Trade in Endangered Species of Wild Fauna and Flora, commonly abbreviated as CITES. There are no fungi listed in any appendix.\n\n\n\n"}
{"id": "8210029", "url": "https://en.wikipedia.org/wiki?curid=8210029", "title": "List of wind turbine manufacturers", "text": "List of wind turbine manufacturers\n\nThis is a list of notable wind turbine manufacturers and businesses that manufacture major wind turbine components.\n\n\nCurrent manufacturers: \n\nPast manufacturers:\n\n\n\n"}
{"id": "26275470", "url": "https://en.wikipedia.org/wiki?curid=26275470", "title": "Low-velocity zone", "text": "Low-velocity zone\n\nThe low-velocity zone (LVZ) occurs close to the boundary between the lithosphere and the asthenosphere in the upper mantle. It is characterized by unusually low seismic shear wave velocity compared to the surrounding depth intervals. This range of depths also corresponds to anomalously high electrical conductivity.It is present between about 80 and 300 km depth. This appears to be universally present for S waves, but may be absent in certain regions for P waves. A second low-velocity zone (not generally referred to as the LVZ, but as ULVZ) has been detected in a thin ≈50 km layer at the core-mantle boundary. These LVZs may have important implications for plate tectonics and the origin of the Earth's crust.\n\nThe LVZ has been interpreted to indicate the presence of a significant degree of partial melting, and alternatively as a natural consequence of a thermal boundary layer and the effects of pressure and temperature on the elastic wave velocity of mantle components in the solid state. In any event, a very limited amount of melt (about 1%) is needed to produce these effects. Water in this layer can lower the melting point, and may play an important part in its composition.\n\nThe existence of the low-velocity zone was first proposed from the observation of slower than expected seismic wave arrivals from earthquakes in 1959 by Beno Gutenberg. He noted that between 1° to 15° from the epicenter the longitudinal arrivals showed an exponential decrease in amplitude after which they showed a sudden large increase. The presence of a low-velocity layer that defocussed the seismic energy, followed by a high velocity gradient that concentrated it, provided an explanation for these observations.\n\nThe LVZ shows a reduction in velocity of about 3–6% with the effect being more pronounced with S-waves compared to P-waves. As is evident from the figure, the reduction and depth over which reduction occurs varies with the choice of tectonic province, that is, regions differ in their seismic characteristics. Following the drop, the base of the zone is marked by an increase in velocity, but it has not been possible to decide whether this transition is sharp or gradual. This lower boundary, found beneath the continental lithosphere and oceanic lithosphere away from mid-ocean ridges, is sometimes referred to as the Lehmann discontinuity and occurs at about 220±30 km depth. The interval also shows a reduction in Q, the seismic quality factor (representing a relatively high degree of seismic attenuation), and a relatively high electrical conductivity.\n\nThe LVZ is present at the base of the lithosphere except in areas of thick continental shield where no velocity anomaly is apparent.\n\nThe interpretation of these observations is complicated by the effects of seismic anisotropy, which may greatly reduce the actual scale of the velocity anomaly. However, because of the reductions in Q and electrical resistivity in the LVZ, it is generally interpreted as a zone in which there is a small degree of partial melting. For this to occur at the depths where the LVZ is observed, small amounts of water and/or carbon dioxide must be present to depress the melting point of the silicate minerals. Only 0.05–0.1 % water would be sufficient to cause the 1% of melting necessary to produce the observed changes in physical properties. The lack of LVZ beneath continental shields is explained by the much lower geothermal gradient, preventing any degree of partial melting.\n\n"}
{"id": "21709412", "url": "https://en.wikipedia.org/wiki?curid=21709412", "title": "Mechanical vapor recompression", "text": "Mechanical vapor recompression\n\nMechanical vapor recompression (MVR) is an energy recovery process where energy is added to low-pressure vapor (usually water vapor) by compressing it. The result is a smaller volume of vapor at a higher temperature and pressure, which can be used to do useful work. Typically, the compressed vapor can be used to heat the mother liquor to produce the low pressure vapor.\n\nMechanical vapor recompression is used chiefly in industrial processes such as evaporation and distillation. Heat from the condenser, which would otherwise be lost, can be recovered and used in the evaporation process.\n\nMVR was used in the Cristiani compressed steam system for locomotive transmission. Although it was technically feasible, it failed to become popular because of its complexity.\n\nAlternatives to mechanical vapor recompression (MVR) are:\n\n\n\nA combination of the three methods may be used depending on the process. For instance, a 3-effect evaporator circuit may be installed using MVR to transfer heat.\n"}
{"id": "3075977", "url": "https://en.wikipedia.org/wiki?curid=3075977", "title": "NASA Clean Air Study", "text": "NASA Clean Air Study\n\nThe NASA Clean Air Study was led by the National Aeronautics and Space Administration (NASA) in association with the Associated Landscape Contractors of America (ALCA). Its results suggest that certain common indoor plants may provide a natural way of removing toxic agents such as benzene, formaldehyde and trichloroethylene from the air, helping neutralize the effects of sick building syndrome. However, the study was not conducted under realistic home or office conditions, and the few studies that have been conducted under such conditions show mixed results. \n\nThe first list of air-filtering plants was compiled by NASA as part of a clean air study published in 1989, which researched ways to clean air in space stations. As well as absorbing carbon dioxide and releasing oxygen, as all plants do, these plants also eliminate significant amounts of benzene, formaldehyde and trichloroethylene. The second and third lists are from B. C. Wolverton's book and paper and focus on removal of specific chemicals.\n\nNASA researchers suggest efficient air cleaning is accomplished with at least one plant per 100 square feet of home or office space . Other more recent research has shown that micro-organisms in the potting mix (soil) of a potted plant remove benzene from the air, and that some plant species also contribute to removing benzene.\n\nMost of the plants on the list originated in tropical or subtropical environments. Due to their ability to flourish on reduced sunlight, their leaf composition allows them to photosynthesize well in household light.\n\n\n"}
{"id": "53121641", "url": "https://en.wikipedia.org/wiki?curid=53121641", "title": "Nano-suction technology", "text": "Nano-suction technology\n\nNano-suction is a technology that uses vacuum, negative fluid pressure and millions of nano-sized suction cups to securely adhere any object to a flat non-porous surface. When the nano-suction object is pressed against a flat surface, millions of miniature suction cups create a large vacuum, generating a strong suction force that can hold a tremendous amount of weight. The nature of the technology allows easy removal without residue, and makes it reusable.\n\nThere have been a wide range of applications of nano-suction technology, also known as \"anti-gravity\", ranging from hooks, frames, mirrors, notepad organisers, mobile phone cases and large houseware products.\n\n"}
{"id": "20281570", "url": "https://en.wikipedia.org/wiki?curid=20281570", "title": "Netherlands fallacy", "text": "Netherlands fallacy\n\nThe Netherlands fallacy refers to an error Paul R. Ehrlich and his co-authors claim others make in assuming that the environmental impacts of the Netherlands and other rich nations are contained within their national borders.\n\nEcologists since the late 20th century have analyzed the ecological sink status and sink capacities of poor nations. As polluting industries migrate from rich to poor nations, the national ecological footprint of rich nations shrinks, whereas the international ecological footprint may increase or also decrease. The nature of the fallacy is to ignore increasing environmental damage in many developing nations and in international waters attributable to the imported goods or changes in the economy of such nations directly due to developed nations.\n\nSuch an approach may lead to incorrect assertions such as the environmental impact of a particular developed country is reducing, when a holistic, international approach suggests the opposite. This may in turn support over-optimistic predictions toward the improvement of global environmental conditions.\n\n\n"}
{"id": "18953024", "url": "https://en.wikipedia.org/wiki?curid=18953024", "title": "Paleocene", "text": "Paleocene\n\nThe Paleocene () or Palaeocene, the \"old recent\", is a geological epoch that lasted from about . It is the first epoch of the Paleogene Period in the modern Cenozoic Era. As with many geologic periods, the strata that define the epoch's beginning and end are well identified, but the exact ages remain uncertain.\n\nThe Paleocene Epoch is bracketed by two major events in Earth's history. It started with the mass extinction event at the end of the Cretaceous, known as the Cretaceous–Paleogene (K–Pg) boundary. This was a time marked by the demise of non-avian dinosaurs, giant marine reptiles and much other fauna and flora. The die-off of the dinosaurs left unfilled ecological niches worldwide. The Paleocene ended with the Paleocene–Eocene Thermal Maximum, a geologically brief (~0.2 million year) interval characterized by extreme changes in climate and carbon cycling.\n\nThe name \"Paleocene\" comes from Ancient Greek and refers to the \"old(er)\" (, \"palaios\") \"new\" (, \"kainos\") fauna that arose during the epoch.\n\nThe K–Pg boundary that marks the separation between Cretaceous and Paleocene is visible in the geological record of much of the Earth by a discontinuity in the fossil fauna and high iridium levels. There is also fossil evidence of abrupt changes in flora and fauna. There is some evidence that a substantial but very short-lived climatic change may have happened in the very early decades of the Paleocene. There are several theories about the cause of the K–Pg extinction event, with most evidence supporting the impact of a 10 km diameter asteroid forming the buried Chicxulub crater on the coast of Yucatan, Mexico.\n\nThe end of the Paleocene (≈55.8 Ma) was also marked by a time of major change, one of the most significant periods of global change during the Cenozoic. The Paleocene–Eocene Thermal Maximum upset oceanic and atmospheric circulation and led to the extinction of numerous deep-sea benthic foraminifera and a major turnover in mammals on land.\n\nThe Paleocene is divided into three stages, the Danian, the Selandian and the Thanetian, as shown in the table above. Additionally, the Paleocene is divided into six Mammal Paleogene zones.\n\nThe early Paleocene was cooler and drier than the preceding Cretaceous, though temperatures rose sharply during the Paleocene–Eocene Thermal Maximum. The climate became warm and humid worldwide towards the Eocene boundary, with subtropical vegetation growing in Greenland and Patagonia, crocodilians swimming off the coast of Greenland, and early primates evolving in the tropical palm forests of northern Wyoming. The Earth's poles were cool and temperate; North America, Europe, Australia and southern South America were warm and temperate; equatorial areas had tropical climates; and north and south of the equatorial areas, climates were hot and arid, not dissimilar to today's global desert belts around 30 degrees northern and southern latitude.\n\nIn many ways, the Paleocene continued processes that had begun during the late Cretaceous Period. During the Paleocene, the continents continued to drift toward their present positions. Supercontinent Laurasia had not yet separated into three continents - Europe and Greenland were still connected, North America and Asia were still intermittently joined by a land bridge, while Greenland and North America were beginning to separate. The Laramide orogeny of the late Cretaceous continued to uplift the Rocky Mountains in the American west, which ended in the succeeding epoch.\n\nSouth and North America remained separated by equatorial seas (they joined during the Neogene); the components of the former southern supercontinent Gondwanaland continued to split apart, with Africa, South America, Antarctica and Australia pulling away from each other. Africa was heading north towards Europe, slowly closing the Tethys Ocean, and India began its migration to Asia that would lead to a tectonic collision and the formation of the Himalayas.\n\nThe inland seas in North America (Western Interior Seaway) and Europe had receded by the beginning of the Paleocene, making way for new land-based flora and fauna.\n\nWarm seas circulated throughout the world, including the poles. The earliest Paleocene featured a low diversity and abundance of marine life, but this trend reversed later in the epoch. Tropical conditions gave rise to abundant marine life, including coral reefs. With the demise of marine reptiles at the end of the Cretaceous, sharks became the top predators. At the end of the Cretaceous, the ammonites and many species of foraminifera became extinct.\n\nMarine fauna also came to resemble modern fauna, with only the marine mammals and the Carcharhinid sharks missing.\n\nTerrestrial Paleocene strata immediately overlying the K–Pg boundary is in places marked by a \"fern spike\": a bed especially rich in fern fossils. Ferns are often the first species to colonize areas damaged by forest fires; thus the fern spike may indicate post-Chicxulub crater devastation.\n\nIn general, the Paleocene is marked by the development of modern plant species. Cacti and palm trees appeared. Paleocene and later plant fossils are generally attributed to modern genera or to closely related taxa.\n\nThe warm temperatures worldwide gave rise to thick tropical, sub-tropical and deciduous forest cover around the globe (the first recognizably modern rainforests) with ice-free polar regions covered with coniferous and deciduous trees. With no large browsing dinosaurs to thin them, Paleocene forests were probably denser than those of the Cretaceous.\n\nFlowering plants (angiosperms), first seen in the Cretaceous, continued to develop and proliferate, and along with them coevolved the insects that fed on these plants and pollinated them.\n\nMammals had first appeared in the Late Triassic, evolving from advanced cynodonts, and developed alongside the dinosaurs, exploiting ecological niches untouched by the larger and more famous Mesozoic animals: in the insect-rich forest underbrush and high up in the trees. These smaller mammals (as well as birds, reptiles, amphibians, and insects) survived the mass extinction at the end of the Cretaceous which wiped out the non-avian dinosaurs, and mammals diversified and spread throughout the world.\n\nWhile early mammals were small nocturnal animals that mostly ate soft plant material and small animals such as insects, the demise of the non-avian dinosaurs and the beginning of the Paleocene saw mammals growing bigger and occupying a wider variety of ecological niches. Ten million years after the death of the non-avian dinosaurs, the world was filled with rodent-like mammals, medium-sized mammals scavenging in forests, and large herbivorous and carnivorous mammals hunting other mammals, birds, and reptiles.\n\nFossil evidence from the Paleocene is scarce, and there is relatively little known about mammals of the time. Because of their small size (constant until late in the epoch) early mammal bones are not well preserved in the fossil record, and most of what we know comes from fossil teeth (a much tougher substance), and only a few skeletons.\n\nThe brain to body mass ratios of these archaic mammals were quite low.\n\nMammals of the Paleocene include:\n\nBecause of the climatic conditions of the Paleocene, reptiles were more widely distributed over the globe than at present. Among the sub-tropical reptiles found in North America during this epoch are champsosaurs (fully aquatic reptiles), crocodilia, soft-shelled turtles, palaeophid snakes, varanid lizards, and \"Protochelydra zangerli \" (similar to modern snapping turtles).\n\nExamples of champsosaurs of the Paleocene include \"Champsosaurus gigas\", the largest champsosaur ever discovered. This creature was unusual among Paleocene non-squamate reptiles in that \"C. gigas\" became larger than its known Mesozoic ancestors: \"C. gigas\" is more than twice the length of the largest Cretaceous specimens (3 meters versus 1.5 meters). Another genus, \"Simoedosaurus\", was similarly large; it appears rather suddenly in the fossil record, as its closest relatives occurred in the Early Cretaceous. Reptiles as a whole decreased in size after the K–Pg event. Champsosaurs declined towards the end of the Paleocene and became extinct during the Miocene.\n\nExamples of Paleocene crocodylians are \"Borealosuchus\" (formerly \"Leidyosuchus\") \"formidabilis\", the apex predator and the largest animal of the Wannagan Creek fauna, and the alligatorid \"Wannaganosuchus\".\n\nNon-avian dinosaurs may have survived to some extent into the early Danian stage of the Paleocene Epoch circa 64.5 Mya. The controversial evidence for such is a hadrosaur leg bone found from Paleocene strata in New Mexico; but such stray late forms may be derived fossils.\n\nSeveral species of snakes, such as \"Titanoboa\" and \"Gigantophis\", grew to over 6 meters long.\n\nBirds began to re-diversify during the epoch, occupying new niches. Genetic studies suggest that nearly all modern bird clades can trace their origin to this epoch, with Neornithes having undergone an extremely fast, \"star-like\" radiation of species in the early Palaeocene in response to the vacancy of niches left by the KT event.\n\nLarge flightless birds have been found in late Paleocene deposits, including the omnivorous \"Gastornis\" in Europe and carnivorous terror birds in South America, the latter of which survived until the Pleistocene.\n\nIn the late Paleocene, early owl types appeared, such as \"Ogygoptynx\" in the United States and \"Berruornis\" in France.\n\n"}
{"id": "24769245", "url": "https://en.wikipedia.org/wiki?curid=24769245", "title": "Polyaniline nanofibers", "text": "Polyaniline nanofibers\n\nPolyaniline nanofibers are a high aspect form of polyaniline, a polymer consisting of aniline monomers, which appears as discrete long threads with an average diameter between 30 nm and 100 nm. Polyaniline is one of the oldest known conducting polymers, being known for over 150 years. Polyaniline nanofibers are often studied for their potential to enhance the properties of polyaniline or have additional beneficial properties due to the addition of a nanostructure to the polymer. Properties that make polyaniline useful can be seen in the nanofiber form as well, such as facile synthesis, environmental stability, and simple acid/base doping/dedoping chemistry. These and other properties have led to the formation of various applications for polyaniline nanofibers as actuators, memory devices, and sensors.\n\nMethods for the polymerization of polyaniline nanofibers seen in literature primarily include [redox|chemical oxidative] polymerization, interfacial synthesis, and \"rapid mixing\" methods. Other less common methods include nanofiber seeding, electrosynthesis, electrospinning, and preforming polymerization in dilute aniline solutions.\n\nChemical oxidative polymerization is a traditional and commonly used method for the polymerization of aniline in large quantities. When aniline is mixed with an oxidant in an acidic solution, polymerization will occur. The most important parameter to be controlled in this method for the synthesis of polyaniline nanofibers is the domination of homogeneous nucleation over heterogeneous nucleation. Homogeneous nucleation describes when the nuclei are formed spontaneously in solution while heterogeneous nucleation describes when the nuclei are grown on other species. In the early stages of this polymerization, only nanofibers are formed since there are no heteronuclei available for heterogeneous nucleation. However, if the reaction is left uncontrolled, heterogeneous nucleation will begin to dominate as the polyaniline will preferentially grow on existing particles, leading to irreversible agglomeration. The reaction can be made to favor homogeneous nucleation throughout by increasing reaction speed, temperature of the reaction, and allowing the reaction to proceed without stirring.\n\nThe diameter of the polyaniline nanofibers can be controlled with this method through choice of acid. Hydrochloric acid produces nanofibers with a diameter of about 30 nm, while camphorsulfonic acid and perchloric acid produce a diameter of 50 nm and 120 nm respectively. Under normal synthetic methods polyaniline derivatives, such as ones that are alkyl and fluoro substituted, do not exhibit a well-defined fibrous shape, however, in the presence of an aniline oligomer nanofibers of certain derivatives can be synthesized. While the most common oxidant is ammonium peroxydisulfate (APS), various others can be used. One study shows the use of potassium biiodate (KH(IO)) as an oxidant, claiming it to lead to polyaniline nanofibers that are longer, have higher crystallinity, and have higher electrical conductivity.\n\nIn interfacial synthesis, the polymerization happens at the interface between an aqueous and an organic layer. A typical reaction involves an aqueous solution of acid and oxidant and an organic layer of aniline together. This creates the reactive interface for polymerization to occur. As polymerization proceeds, the polyaniline nanofibers will diffuse into the water layer, leaving the reactive interface. This prevents overgrowth onto the existing wires, allowing for homogeneous nucleation to continue occurring. Conditions in the interfacial synthesis can be tuned, such as the type of acid used as well as the oxidant used.\n\nPolyaniline nanofibers can also be synthesized through \"rapid mixing\" reactions. This method attempts to prevent overgrowth that would compromise the nanofiber nature of the polymer by stopping the polymerization immediately after nanofibers have been formed. This is achieved by the rapid mixing of the monomer, aniline, and an initiator solution. At the start of the reaction, the initiator is consumed rapidly and completely depleted when the nanofibers are formed. Without initiator remaining, the synthesis of polyaniline is halted.\n\nPolyaniline nanofibers have been used in the creation of monolithic actuators. They can be used in this application due to their ability to be flash-welded. When exposed to light, polyaniline converts the absorbed energy directly into heat. In a polyaniline film, the heat is dispersed throughout the polymer. In polyaniline nanofibers, however, the heat is trapped within the individual fibers. Therefore, if the intensity of the light is great enough, it will cause the temperature of the nanofibers to rise rapidly, which causes them to weld together or burn. With a moderate flash intensity, the nanofibers will melt rapidly to form a smooth film. Using mask, welds in specific patterns can be made using this technique. In a thick enough sample of nanofibers, only the side exposed to the flash will be welded, creating an asymmetric film where one side remains intact as nanofibers while the other side is effectively crosslinked due to welding. These asymmetric films demonstrate rapid reversible actuation in the presence of acids and bases, in the form of bending and curling. The advantages polyaniline nanofiber asymmetric films have over other actuators include the ease of synthesis, large degree of bending, patternability, and no delamination. These actuators could be used in the development of microtweezers, microvalves, artificial muscles, chemical sensors, and patterned actuator structures.\n\nResearch has shown that polyaniline nanofibers can also be used to create nonvolatile plastic digital memory devices when decorated with various metal, such as gold, nanoparticles. Gold nanoparticles are grown inside dedoped polyaniline nanofibers using a redox reaction. A plastic composite film is placed between two electrodes, and an external bias is used to program ON-OFF states. The switching mechanism is thought to be caused through an interaction between the polyaniline nanofibers and the gold nanoparticles, where charge is transferred to the gold nanoparticles from the polyaniline nanofibers due to an induced electric field. Switching between the ON-OFF states has shown to be rapid, with times of less than 25ns. The retention time of these simple devices are on the order of days after programming, and write-read-erase cycles have been demonstrated.\n\nPolyaniline nanofibers have been shown to be incredibly successful as chemical sensors, as they perform better than conventional polyaniline films in numerous tests. This performance difference has been attributed to their high surface area, porosity, and small diameters which enhance diffusion of materials through the nanofibers. Polyaniline nanofiber sensors function through a change in resistance. The polyaniline nanofiber film is placed on an electrode, where a current flows through. The resistance of the electrode changes when the target interacts with the film, which allows the target to be detected.\n\nOne study proposes the creation of hydrogen gas sensors using polyaniline nanofibers. It shows that both doped and dedoped polyaniline nanofibers can be used for detection of hydrogen gas through resistance changes, but the dedoped nanofibers were more stable and had better reproducibility.\n\nAnother study shows the potential of polyaniline nanofibers as NO gas sensors. NO gas acts as a strong oxidizing agent to the emeraldine form of polyaniline nanofibers, which causes resistance changes greater than three orders of magnitude at 100 ppm.\n\nSensing targets can be expanded through adding materials to the polyaniline nanofibers. One study proposes polyaniline nanofiber composites with metal salts for the detection of hydrogen sulfide. Hydrogen sulfide is a weak acid that is dangerous at low ppm, but polyaniline nanofibers can only give a robust response to strong acids. Metal salts can react with hydrogen sulfate to form a metal sulfide precipitate and a strong acid. By combining metal salts and polyaniline nanofibers, detection of hydrogen sulfide can be performed.\n\nAnother study decorated polyaniline nanofibers with gold nanoparticles to detect volatile sulfur compounds in expired human breath. These sensors can potentially be used in various breath analyses and also in disease diagnosis for diseases with malodor biomarker gases.\n\nHumidity sensors have also been prepared using polyaniline nanofibers. These sensors were prepared through electrospinning of a N,N-dimethylformamide solution of polyaniline nanofibers, poly(vinyl butyral) (PVB), and poly(ethylene oxide) (PEO). These sensors were shown to have high sensitivity, with resistance changes of three orders of magnitude. Furthermore, the sensors showed good sensing linearity, fast response, small hysterics, and good repeatability.\n\n"}
{"id": "6166022", "url": "https://en.wikipedia.org/wiki?curid=6166022", "title": "Proto-Tethys Ocean", "text": "Proto-Tethys Ocean\n\nThe Proto-Tethys Ocean was an ancient ocean that existed from the latest Ediacaran to the Carboniferous (550–330 Ma).\n\nThe name \"Proto-Tethys\" have been used inconsistently for several concepts for a supposed predecessor of the Paleo-Tethys Ocean, a palaeocean that separated the margins of Gondwana, often referred to as peri-Gondwana, from various continents and Gondwana-derived continental fragments from Precambrian times and onwards.\nAccording to , after the Cadomian orogenic events 550 Ma, the Proto-Tethys Ocean formed the eastern part of an oceanic domain (of which the Iapetus and Tornquist oceans formed the western parts) which subducted under the northern margins of Gondwana. In this model the Proto-Tethys separated the North China and Baltica continents from Gondwana. In the Early Ordovician 500-480 Ma, the Proto-Tethys was subducted under Cadomia as a result of the Chamrousse back-arc basin. used the name 'Ran Ocean' for a similar concept, the Cambrian-Ordovician ocean that separated Baltica from Gondwana.\nOther geologists dispute the existence of such an ocean.\n\nThe ocean formed when Pannotia disintegrated, Proto-Laurasia (Laurentia, Baltica, and Siberia) rifted away from a supercontinent that would become Gondwana. Proto-Tethys formed between these two supercontinents. The ocean was bordered by Panthalassic Ocean to the north, separating it from Panthalassa by island arcs and Kazakhstania. The Proto-Tethys expanded during the Cambrian. The ocean was at its widest during the Late Ordovician to Middle Silurian. The ocean was situated between the Siberia to the west, and Gondwana to the east. The ocean began to shrink during the Late Silurian, when North China, and South China moved away from Gondwana and headed north. In the late Devonian, the microcontinent of Kazakhstania collided with Siberia, shrinking the ocean even more. The ocean closed when the North China craton collided with Siberia-Kazakstania continent in the Carboniferous, while the Paleo-Tethys Ocean expanded.\n\n\n"}
{"id": "2168920", "url": "https://en.wikipedia.org/wiki?curid=2168920", "title": "SNAP-10A", "text": "SNAP-10A\n\nSNAP-10A (Systems for Nuclear, Auxiliary Power), also called SNAPSHOT is an experimental nuclear powered satellite launched into space in 1965. It is the only fission power system launched into space by the United States. The reactor stopped working after just 43 days due to a non-nuclear electrical component failure. The Systems Nuclear Auxiliary Power Program (SNAP) reactor was specifically developed for satellite use in the 1950s and early 1960s under the supervision of the U.S. Atomic Energy Commission.\n\nSNAP-10A was launched from Vandenberg Air Force Base by an ATLAS Agena D rocket on 3 April 1965 into a low Earth orbit altitude of approx. 1,300 km. It is in a slightly retrograde polar orbit\n— this ensured that the spent rocket stages landed in the ocean. Its nuclear electrical source, made up of thermoelectric elements, was intended to produce over 500 watts of electrical power for one year. After 43 days, an onboard voltage regulator within the spacecraft – unrelated to the SNAP reactor – failed, causing the reactor core to be shut down, after reaching a maximum output of 590 watts.\n\nAfter the 1965 system failure, the reactor was left in a Earth orbit for an expected duration of 4,000 years.\n\nIn November 1979 the vehicle began shedding, eventually losing 50 pieces of traceable debris. The reasons were unknown, but the cause could have been a collision. Although the main body remains in place, radioactive material may have been released. Later research, published in 2008 and based on Haystack data, suggests that there are another 60 or more pieces of debris of size <10 cm \n\n, more than 30 small fission power system nuclear reactors have been sent into space in Soviet RORSAT satellites; also, over 40 radioisotope thermoelectric generators have been used globally (principally US and USSR) on space missions.<ref name=\"nasa/doe2010\">\n</ref>\n\nThe SNAP-10A has three major components – a compact nuclear reactor, the reactor reflector and control system, a heat transfer and power conversion system.\n\nThe reactor measures 39.62 cm (15.6 in) long, 22.4 cm (8.8 in) diameter and holds 37 fuel rods containing U as uranium-zirconium-hydride fuel. The SNAP-10A reactor was designed for a thermal power output of 30 kW and unshielded weighs . The reactor can be identified at the top of the SNAP-10A unit.\n\nReflectors were arranged around the outside of the reactor to provide the means to control the reactor. The reflectors were composed of a layer of beryllium, which would reflect neutrons, thus allowing the reactor to begin and maintain the fission process. The reflectors were held in place by a retaining band anchored by an explosive bolt. When the reflector was ejected from the unit, the reactor could not sustain the nuclear fission reaction and the reactor permanently shut down.\n\nThe eutectic sodium-potassium (NaK) alloy was used as a coolant in the SNAP-10A. The NaK was circulated through the core and thermoelectric converters by a liquid metal direct current conduction-type pump. The thermoelectric converters (identified as the long white \"apron\") are doped silicon germanium materials, thermally coupled, but electrically isolated from the NaK heat transfer medium. The temperature difference between the NaK on one side of the thermoelectric converter and the cold of space on the other created an electric potential and usable electricity.\n\nThe SNAP reactor program necessitated a safety program and led to the inception of the Aerospace Nuclear Safety Program. The program was established to evaluate the nuclear hazards associated with the construction, launch, operation and disposal of SNAP systems and to develop designs to assure their radiological safety.\n\nAtomics International had primary responsibility for safety, while Sandia National Laboratories was responsible for the Aerospace Safety Independent Review and conducted many of the safety tests. Before launch was permitted, proof had to be obtained that under all circumstances the launch of the reactor would not pose a serious threat.\n\nA variety of tests were successfully completed and several videos of the development and tests are available for viewing. The Idaho National Laboratory conducted three destructive tests of SNAP nuclear reactors at Test Area North prior to the launch of SNAP-10A. The SNAPTRAN-3 destructive experiment, on 1 April 1964, simulated a rocket crash into the ocean, purposely sending radioactive debris across the Idaho desert.\n\nAtomics International, then a division of North American Aviation was the prime contractor for the SNAP-10A development. Most of the systems development and reactor testing was conducted at the Santa Susana Field Laboratory, Ventura County, California using a number of specialized facilities. A United States Department of Energy video depicting the development and fabrication of the SNAP-10A is available.\nThe company also developed and tested other compact nuclear reactors including the SNAP Experimental Reactor (SER), SNAP-2, SNAP-8 Developmental Reactor (SNAP8-DR) and SNAP-8 Experimental Reactor (SNAP-8ER) units at the Santa Susana Field Laboratory. Atomics International also built and operated the Sodium Reactor Experiment, the first U.S. nuclear power plant to supply electricity to a public power system.\n\nThe testing and development involving radioactive materials caused environmental contamination at the former Atomics International Santa Susana Field Laboratory (SSFL) facilities. The United States Department of Energy is responsible for the identification and cleanup of the radioactive contamination. (The SSFL was also used for the unrelated testing and development of rocket engines by Rocketdyne primarily for NASA.) The DOE website supporting the site cleanup details the historical development of nuclear energy at SSFL including additional SNAP testing and development information.\n\n\n"}
{"id": "31937372", "url": "https://en.wikipedia.org/wiki?curid=31937372", "title": "Self-praise of Shulgi (Shulgi D)", "text": "Self-praise of Shulgi (Shulgi D)\n\nSelf-praise of Shulgi (Shulgi D) is a Sumerian myth, written on clay tablets dated to between 2100 and 2000 BC.\n\nThe myth was discovered on the University of Pennsylvania Museum of Archaeology and Anthropology, catalogue of the Babylonian section (CBS), tablet number 11065 from their excavations at the temple library at Nippur. This was translated by George Aaron Barton in 1918 and first published as \"Sumerian religious texts\" in \"Miscellaneous Babylonian Inscriptions\", number three, entitled \"Hymn to Dungi\" (Dungi was later renamed to Shulgi). The tablet is by by at its thickest point. Barton noted that similar hymns were published by Stephen Herbert Langdon and introduced into Sumerian religion at the time of the Third dynasty of Ur onwards. He dates the tablet to the reign of Shulgi, saying \"The script of our tablets shows that this copy was made during the time of the First Dynasty of Babylon, but that does not preclude an earlier date for the composition of the original.\" Further tablets were used by Jacob Klein to expand and translate the myth again in 1981. He used several other tablets from the University Museum in Pennsylvania including CBS 8289. He also included translations from tablets in the Nippur collection of the Museum of the Ancient Orient in Istanbul, catalogue number 4571. He also used tabled 5379 from the Louvre in Paris.\n\nIn the story, Shulgi is praised and compared to all manner of animals and wondrous things such as a tree.\n\nHis interactions and relationships with a large number of the pantheon of Sumerian gods are described along with victories in foreign lands and description of the royal barge.\n\nSamuel Noah Kramer suggests that Shulgi hymns speaking about the achievements of the king focussed on the two areas of social behaviour and religion. He is both shown to be concerned for social justice, law and equity along with being faithful in his priestly rites and interaction with the gods. He notes \"uppermost in their minds was the Ekur, the holy temple of Nippur where virtually every king in the hymnal repertoire brought gifts, offerings, and sacrifices to Enlil.\"\n\n\n"}
{"id": "41133818", "url": "https://en.wikipedia.org/wiki?curid=41133818", "title": "Sloshsat-FLEVO", "text": "Sloshsat-FLEVO\n\nSLOSHSAT-FLEVO (Sloshsat Facility for Liquid Experimentation and Verification in Orbit) is a microsatellite launched to investigate the dynamics of fluids in microgravity. FLEVO stands for Facility for Liquid Experimentation and Verification in Orbit. Multiple sensors were used to monitor the behavior of water in an instrumented tank and how sloshing affects the attitude control of launchers and space vehicles.\n\nThe project is a joint program between ESA, the Netherlands Agency for Aerospace Programmes, and the Israel Space Agency. The primary contractor is the National Aerospace Laboratory providing the spacecraft structure and power systems. The ejection system and ground support equipment were contracted by NEWTEC. The ISA was responsible for supplying the sub-propulsion system which was built and assembled by Rafael.\n\nThe spacecraft itself is a 90-cm cube microsatellite covered by solar cells and fitted with 12 small thrusters.\n\nOriginally Sloshsat-FLEVO was to be launched from the Space Shuttle and use the shuttle as a data relay, but after the Space Shuttle Columbia disaster the satellite was modified to be launched on board the Ariene qualification flight.\nThe satellite was launched from the Guiana Space Centre launch site at Kourou, French Guiana on 12 February 2005.\n\nThe SLOSHSAT-FLEVO is the first satellite entirely dedicated to liquid research in space. The satellite was equipped with an 87-litre cylindrical tank containing 33.5 litres of de-ionised water. 270 sensors were placed on the tank's walls to measure the sloshing behavior by calculating the thickness of the water. Three accelerometers and a fibre-optic gyroscope were used to measure the motion of the spacecraft. An array of temperature, pressure and fluid velocity sensors were also installed on the craft.\n\nAfter depletion of the propellants and completion of the experiment, the satellite was turned off. ESA estimate that without active debris removal the orbit would decay by 2335.\n\n\n"}
{"id": "20662407", "url": "https://en.wikipedia.org/wiki?curid=20662407", "title": "Snow patches in Scotland", "text": "Snow patches in Scotland\n\nLong-lying snow patches in Scotland have been noted from at least the 18th century, with snow patches on Ben Nevis being observed well into summer and autumn. Indeed, the summit observatory, which operated from 1883 to 1904, reported that snow survived on the north-east cliffs through more years than it vanished.\n\nMore recently, additional and methodical field study on the subject has been carried out by others, most notably by ecologist Dr Adam Watson. Most of this work concentrated on the mountains of north-east Scotland (in particular, the Cairngorms), but more recent observations by him and others has shed light on various locations throughout Scotland where long-lying snow persists. The available information systematically gathered by observers over the last 50 years or so, and greatly increased since the 1990s, has built up a level of knowledge that points to Scotland’s snow patches being now amongst the best documented in the world.\n\nThere are many locations on the Scottish mountains where snow lies regularly into July, August and even September, but the two main areas where snow virtually always lies longer than anywhere else are the Cairngorms and the Lochaber mountains. These areas contain all of Scotland's mountains in excess of , including Ben Nevis.\n\nIn some years snow can persist all summer, in some locations lasting through to the next winter. In 2015 some 73 patches were still present in late November at a time when the next winter's snows had started accumulating. The last time so many patches had survived all year was 1994.\n\nOther locations where snow has been known to survive:\n\nAs well as containing five of Scotland's highest mountains, the Cairngorms are the range where snow persists longest, and in more locations, than anywhere else in the UK. Ben Macdui, Cairn Gorm and Braeriach all contain long-lying patches that have been observed for many years.\n\nOn Ben Macdui, snow has been known to persist at a few locations from one winter to the next, but the location where more survivals have been noted than any other is , close to the Garbh Uisge Beag, which drains into Loch Avon. This patch sits at an altitude of .\n\nLying at the north-eastern shoulder of Cairn Gorm is Ciste Mhearad. This hollow contains a patch which, hitherto, was known to persist through many years, but has done so only once (in 2015) since 2000. Observations in 2007 and 2008 revealed that September was the month when final melting occurred for this patch. It sits at an altitude of and is located at approximately .\n\nBraeriach's Garbh Choire Mòr is the place which contains Britain's most persistent snow beds. Snow has been absent from this corrie just six times in the last century: 1933, 1959, 1996, 2003, 2006 & 2017. Sitting at an altitude of about , these patches are located around ; the two most long-lasting patches are known as \"the Pinnacles\" and \"the Sphinx\" after the rock climbs lying above them. It has been claimed that Garbh Choire Mòr (as well as Coire an Lochain in the northern corries) may have contained a glacier as recently as the 19th century.\n\nIn 1994, the Cairngorms and surrounding mountains of north-east Scotland had 55 surviving patches, an exceptional number.\n\nAs well as containing Scotland's highest mountain (Ben Nevis), Aonach Mòr, Aonach Beag and Càrn Mòr Dearg make up the other three mountains in excess of in this area.\n\nAs already noted, Ben Nevis has long been known to hold snow late into the year. However, historical reports from the 19th century and early 20th century of snow being ever present on the mountain are virtually impossible to substantiate, so must remain speculative. Nevertheless, what is certainly true is that snow often persists from one winter to the next. Analysis of Ben Nevis's snow is not as comprehensive as that of the Cairngorms, but recent observations show that Ben Nevis has been snow-free only once since 2006 (in 2017). The largest patch, at Observatory Gully, sits at an altitude of around . The slightly lower patch at Point 5 gully has also been known to survive from one winter to the next.\n\nAonach Mòr has a corrie known to hold snow from one year to the next: Coire an Lochain. One of these patches, sitting behind a protalus rampart, sometimes survives longer than the patch slightly higher up against the tall cliffs.\n\nBelow the cliffs of the north-east ridge on Aonach Beag there is a relatively little known snow-patch which, despite its low altitude (approximately ), has been Scotland's largest at the time of the arrival of the lasting new winter snows of 2007 and 2008. This patch does not appear in known literature on the subject and this suggests that it is very much under-recorded, which may be because it cannot readily be seen, even from the top of Aonach Mòr or Aonach Beag.\n\n\n"}
{"id": "630518", "url": "https://en.wikipedia.org/wiki?curid=630518", "title": "The Evolution of Melanism", "text": "The Evolution of Melanism\n\nThe Evolution of Melanism: a study of recurring necessity; with special reference to industrial melanism in the Lepidoptera is a 1973 science book by the lepidopterist Bernard Kettlewell.\n\nThe book includes Kettlewell's original papers in the journal \"Heredity\" on his classic predation experiments on the peppered moth. It also covers Kettlewell's experiments in the Hebrides.\n\n\n"}
{"id": "24983737", "url": "https://en.wikipedia.org/wiki?curid=24983737", "title": "The Ornithology of Australia", "text": "The Ornithology of Australia\n\nThe Ornithology of Australia comprises three volumes (of an uncompleted set) of lithographed, hand-coloured, illustrations of Australian birds with accompanying text. It was authored by Silvester Diggles of Brisbane and was originally issued in 21 parts, each part containing six plates (126 plates in all) with short descriptive letterpress, in imperial quarto format, with the leaves of the plates 39 cm in height. The parts were printed for the author by T.P. Pugh. Altogether, between 1863 and 1875, Diggles, with his niece Rowena Birkett who hand-coloured each plate, produced 325 plates illustrating some 600 Australian birds.\n\nIn 1877 Diggles produced what was essentially a two-volume second edition of the previously published work, using 123 of the same plates and renamed \"Companion to Gould's Handbook, or, Synopsis of the birds of Australia, containing nearly one-third of the whole, or about 220 examples, for the most part from the original drawings\", though the spine title was simply \"Diggles' Ornithology\". Volume 1 had 59 plates, volume 2 had 64, printed by Thorne & Greenwell of Brisbane. The title referred to John Gould's 1865 book \"Handbook to the Birds of Australia\".\n\nDiggles lacked the funds to continue publishing the series of lithographs. From 1875 his health began to deteriorate, and he died in 1880. His manuscript and original plates, including those unpublished, were later acquired from Diggles’ son by publishing firm Angus & Robertson which presented them to the Mitchell Library in Sydney. In August 1990 a facsimile edition of \"The Ornithology of Australia\", volume 1, parts 1–7, was published by State Publishing South Australia ().\n\n"}
{"id": "8307722", "url": "https://en.wikipedia.org/wiki?curid=8307722", "title": "Travel to the Earth's center", "text": "Travel to the Earth's center\n\nTravelling to the Earth's center is a popular theme in science fiction. Some subterranean fiction involves traveling to the Earth's center and finding either a Hollow Earth or Earth's molten core.\n\nPlanetary scientist David J. Stevenson suggested sending a probe to the core as a thought experiment. Humans have drilled over 12 kilometers (7.67 miles) in the Sakhalin-I. In terms of depth below the surface, the Kola Superdeep Borehole SG-3 retains the world record at in 1989 and still is the deepest artificial point on Earth.\n\nThe idea of a so-called \"Hollow Earth\", once popular in fantasy adventure literature, is that the planet Earth has a hollow interior and an inner surface habitable by human beings. Although the scientific community has made clear that this is pseudoscience, the idea nevertheless is a less popular feature of many fantasy and science fiction stories and of some conspiracy theories.\n\nThe most famous example of a hollow-Earth fantasy is Jules Verne's 1864 science-fiction novel \"Journey to the Center of the Earth\", which has been adapted many times as a feature film and for television.\n\nThe 2003 film \"The Core\", loosely based on the novel \"Core\", tells the story of a team that has to drill to the center of the Earth and detonate a series of nuclear explosions in order to restart the rotation of Earth's core. The drilling equipment, dubbed \"Virgil\", includes a powerful, snake-like laser drill, a small nuclear reactor for power, a shell (of \"unobtainium\", a fictional material) to protect against intense heat and pressure (and generate energy to drive the engine), a powerful x-ray camera for viewing outside, and a system of impellers for movement and control. The only part of the Earth that turns out to be hollow is a gigantic geode, and soon after the drill moves through it, the hole it created fills with magma.\n\nThe 1986 animated television show Inhumanoids featured regular visits to the Inner Core in most of its 13 episodes. Each of the three villainous creatures theoretically ruled over certain layers of the inner earth, and their evil schemes were thwarted by the human Earth Corps, who often allied with various races of subterranean beings equally threatened by the Inhumanoids.\n\nDuring season 3 of the \"Teenage Mutant Ninja Turtles\" cartoon, the Technodrome is located at the Earth's core, and transport modules are used to drill up to the streets. This season also features the episode \"Turtles at the Earth's Core\", where a dinosaur lives in a deep cave, and a crystal of energy that works like the Sun to keep the dinosaurs alive.\nAs Krang, Shredder, Bebop and Rocksteady steal the crystal to power the Technodrome, the trouble begins.\n\nDon Rosa's 1995 \"Uncle Scrooge\" story \"The Universal Solvent\" imagines a way to travel to the planet's core using 1950s technology, although this would be impossible in reality. The fictional solvent referred to in the story's title has the power to condense everything except diamonds into a kind of super-dense dust. The solvent is accidentally spilled and, as it dissolves everything in its path, it bores a cylindrical shaft into the center of the planet. As part of a recovery effort, a makeshift platform is constructed that descends into the shaft in free fall, automatically deploying an electric motor and wheels as it approaches zero gravity, then using rocket engines to enable it to ascend again to the Earth's surface. The author Rosa describes this fantasy journey in great detail: the supposed structure of the Earth is illustrated, and the shaft is kept in a vacuum to protect against the lethal several thousand kilometers of atmosphere that it would otherwise be exposed to. The ducks must wear space suits and go without food for several days, and they are not entirely certain that the super-dense heat shield will hold. The author maintains continuity with Carl Barks, explaining that the earthquakes in the story are created by spherical Fermies and Terries.\n\nIn Tales to Astonish #2 (1959) \"I Fell to the Center of the Earth\", an archaeologist named Dr. Burke who is on an expedition to Asia travels to the center of the Earth (and also, as he later finds out, backwards in time)--and encounters neanderthals and dinosaurs.\n\nIn the Doctor Who episode, \"The Runaway Bride\", a Racnoss warship is found at the center of the planet.\n\n\n"}
{"id": "1047944", "url": "https://en.wikipedia.org/wiki?curid=1047944", "title": "Two-phase electric power", "text": "Two-phase electric power\n\nTwo-phase electrical power was an early 20th-century polyphase alternating current electric power distribution system. Two circuits were used, with voltage phases differing by one-quarter of a cycle, 90°. Usually circuits used four wires, two for each phase. Less frequently, three wires were used, with a common wire with a larger-diameter conductor. Some early two-phase generators had two complete rotor and field assemblies, with windings physically offset to provide two-phase power. The generators at Niagara Falls installed in 1895 were the largest generators in the world at that time and were two-phase machines. Three-phase systems eventually replaced the original two-phase power systems for power transmission and utilization. There remain few two-phase distribution systems, with examples in Philadelphia, Pennsylvania; many buildings in Center City are permanently wired for two-phase and Hartford, Connecticut.\nThe advantage of two-phase electrical power over single-phase was that it allowed for simple, self-starting electric motors. In the early days of electrical engineering, it was easier to analyze and design two-phase systems where the phases were completely separated. It was not until the invention of the method of symmetrical components in 1918 that polyphase power systems had a convenient mathematical tool for describing unbalanced load cases. The revolving magnetic field produced with a two-phase system allowed electric motors to provide torque from zero motor speed, which was not possible with a single-phase induction motor (without an additional starting means.) Induction motors designed for two-phase operation use a similar winding configuration as capacitor start single-phase motors (however, in a two-phase induction motor, the impedances of the two windings are identical, whereas in a single-phase induction motor, the impedances can be, and usually are, quite different, to reduce cost without sacrificing starting performance; indeed, some single-phase capacitor start/capacitor run induction motors have superior starting characteristics when compared to two- or three-phase induction motors.)\n\nThree-phase electric power requires less conductor mass for the same voltage and overall amount of power, compared with a two-phase four-wire circuit of the same carrying capacity. It has replaced two-phase power for commercial distribution of electrical energy, but two-phase circuits are still found in certain control systems. These power pulsations tend to cause increased mechanical noise in transformer and motor laminations due to magnetostriction and torsional vibration in generator and motor drive shafts.\n\nTwo-phase circuits typically use two separate pairs of current-carrying conductors. Alternatively, three wires may be used, but the common conductor carries the vector sum of the phase currents, which requires a larger conductor. Three-phase can share conductors so that the three phases can be carried on three conductors of the same size. In electrical power distribution, a requirement of only three conductors, rather than four, represented a considerable distribution-wire cost savings due to the expense of conductors and installation.\n\nTwo-phase power can be derived from a three-phase source using two transformers in a Scott connection: One transformer primary is connected across two phases of the supply. The second transformer is connected to a center-tap of the first transformer, and is wound for 86.6% of the phase-to-phase voltage on the three-phase system. The secondaries of the transformers will have two phases 90 degrees apart in time, and a balanced two-phase load will be evenly balanced over the three supply phases.\n\n\n\n"}
{"id": "488815", "url": "https://en.wikipedia.org/wiki?curid=488815", "title": "Utility frequency", "text": "Utility frequency\n\nThe utility frequency, (power) line frequency (American English) or mains frequency (British English) is the nominal frequency of the oscillations of alternating current (AC) in an electric power grid transmitted from a power station to the end-user. In large parts of the world this is 50 Hz, although in the Americas and parts of Asia it is typically 60 Hz. Current usage by country or region is given in the list of mains power around the world.\n\nDuring the development of commercial electric power systems in the late 19th and early 20th centuries, many different frequencies (and voltages) had been used. Large investment in equipment at one frequency made standardization a slow process. However, as of the turn of the 21st century, places that now use the 50 Hz frequency tend to use 220–240 V, and those that now use 60 Hz tend to use 100–127 V. Both frequencies coexist today (Japan uses both) with no great technical reason to prefer one over the other and no apparent desire for complete worldwide standardization.\n\nUnless specified by the manufacturer to operate on both 50 and 60 Hz, appliances may not operate efficiently or even safely if used on anything other than the intended frequency.\n\nIn practice, the exact frequency of the grid varies around the nominal frequency, reducing when the grid is heavily loaded, and speeding up when lightly loaded. However, most utilities will adjust the frequency of the grid over the course of the day to ensure a constant number of cycles occur. This is used by some clocks to accurately maintain their time.\n\nSeveral factors influence the choice of frequency in an AC system. Lighting, motors, transformers, generators and transmission lines all have characteristics which depend on the power frequency. All of these factors interact and make selection of a power frequency a matter of considerable importance. The best frequency is a compromise among contradictory requirements.\n\nIn the late 19th century, designers would pick a relatively high frequency for systems featuring transformers and arc lights, so as to economize on transformer materials, but would pick a lower frequency for systems with long transmission lines or feeding primarily motor loads or rotary converters for producing direct current. When large central generating stations became practical, the choice of frequency was made based on the nature of the intended load. Eventually improvements in machine design allowed a single frequency to be used both for lighting and motor loads. A unified system improved the economics of electricity production, since system load was more uniform during the course of a day.\n\nThe first applications of commercial electric power were incandescent lighting and commutator-type electric motors. Both devices operate well on DC, but DC could not be easily changed in voltage, and was generally only produced at the required utilization voltage.\n\nIf an incandescent lamp is operated on a low-frequency current, the filament cools on each half-cycle of the alternating current, leading to perceptible change in brightness and \"flicker\" of the lamps; the effect is more pronounced with arc lamps, and the later mercury-vapor and fluorescent lamps. Open arc lamps made an audible buzz on alternating current, leading to experiments with high-frequency alternators to raise the sound above the range of human hearing.\n\nCommutator-type motors do not operate well on high-frequency AC, because the rapid changes of current are opposed by the inductance of the motor field. Though commutator-type \"universal\" motors are common in AC household appliances and power tools, they are small motors, less than 1 kW. The induction motor was found to work well on frequencies around 50 to 60 Hz, but with the materials available in the 1890s would not work well at a frequency of, say, 133 Hz. There is a fixed relationship between the number of magnetic poles in the induction motor field, the frequency of the alternating current, and the rotation speed; so, a given standard speed limits the choice of frequency (and the reverse). Once AC electric motors became common, it was important to standardize frequency for compatibility with the customer's equipment.\n\nGenerators operated by slow-speed reciprocating engines will produce lower frequencies, for a given number of poles, than those operated by, for example, a high-speed steam turbine. For very slow prime mover speeds, it would be costly to build a generator with enough poles to provide a high AC frequency. As well, synchronizing two generators to the same speed was found to be easier at lower speeds. While belt drives were common as a way to increase speed of slow engines, in very large ratings (thousands of kilowatts) these were expensive, inefficient and unreliable. After about 1906, generators driven directly by steam turbines favored higher frequencies. The steadier rotation speed of high-speed machines allowed for satisfactory operation of commutators in rotary converters.\nThe synchronous speed N in RPM is calculated using the formula,\nwhere f is the frequency in Hertz and P is the number of poles.\nDirect-current power was not entirely displaced by alternating current and was useful in railway and electrochemical processes. Prior to the development of mercury arc valve rectifiers, rotary converters were used to produce DC power from AC. Like other commutator-type machines, these worked better with lower frequencies.\n\nWith AC, transformers can be used to step down high transmission voltages to lower customer utilization voltage. The transformer is effectively a voltage conversion device with no moving parts and requiring little maintenance. The use of AC eliminated the need for spinning DC voltage conversion motor-generators that require regular maintenance and monitoring.\n\nSince, for a given power level, the dimensions of a transformer are roughly inversely proportional to frequency, a system with many transformers would be more economical at a higher frequency.\n\nElectric power transmission over long lines favors lower frequencies. The effects of the distributed capacitance and inductance of the line are less at low frequency.\n\nGenerators can only be interconnected to operate in parallel if they are of the same frequency and wave-shape. By standardizing the frequency used, generators in a geographic area can be interconnected in a grid, providing reliability and cost savings.\n\nMany different power frequencies were used in the 19th century.\n\nVery early isolated AC generating schemes used arbitrary frequencies based on convenience for steam engine, water turbine and electrical generator design. Frequencies between 16⅔ Hz and 133⅓ Hz were used on different systems. For example, the city of Coventry, England, in 1895 had a unique 87 Hz single-phase distribution system that was in use until 1906. The proliferation of frequencies grew out of the rapid development of electrical machines in the period 1880 through 1900.\n\nIn the early incandescent lighting period, single-phase AC was common and typical generators were 8-pole machines operated at 2,000 RPM, giving a frequency of 133 hertz.\n\nThough many theories exist, and quite a few entertaining urban legends, there is little certitude in the details of the history of 60 Hz vs. 50 Hz.\n\nThe German company AEG (descended from a company founded by Edison in Germany) built the first German generating facility to run at 50 Hz. At the time, AEG had a virtual monopoly and their standard spread to the rest of Europe. After observing flicker of lamps operated by the 40 Hz power transmitted by the Lauffen-Frankfurt link in 1891, AEG raised their standard frequency to 50 Hz in 1891.\n\nWestinghouse Electric decided to standardize on a higher frequency to permit operation of both electric lighting and induction motors on the same generating system. Although 50 Hz was suitable for both, in 1890 Westinghouse considered that existing arc-lighting equipment operated slightly better on 60 Hz, and so that frequency was chosen. The operation of Tesla's induction motor, licensed by Westinghouse in 1888, required a lower frequency than the 133 Hz common for lighting systems at that time. In 1893 General Electric Corporation, which was affiliated with AEG in Germany, built a generating project at Mill Creek, California using 50 Hz, but changed to 60 Hz a year later to maintain market share with the Westinghouse standard.\n\nThe first generators at the Niagara Falls project, built by Westinghouse in 1895, were 25 Hz, because the turbine speed had already been set before alternating current power transmission had been definitively selected. Westinghouse would have selected a low frequency of 30 Hz to drive motor loads, but the turbines for the project had already been specified at 250 RPM. The machines could have been made to deliver 16⅔ Hz power suitable for heavy commutator-type motors, but the Westinghouse company objected that this would be undesirable for lighting and suggested 33⅓ Hz. Eventually a compromise of 25 Hz, with 12-pole 250 RPM generators, was chosen. Because the Niagara project was so influential on electric power systems design, 25 Hz prevailed as the North American standard for low-frequency AC.\n\nA General Electric study concluded that 40 Hz would have been a good compromise between lighting, motor, and transmission needs, given the materials and equipment available in the first quarter of the 20th century. Several 40 Hz systems were built. The Lauffen-Frankfurt demonstration used 40 Hz to transmit power 175 km in 1891. A large interconnected 40 Hz network existed in north-east England (the Newcastle-upon-Tyne Electric Supply Company, NESCO) until the advent of the National Grid (UK) in the late 1920s, and projects in Italy used 42 Hz. The oldest continuously operating commercial hydroelectric power station in the United States, Mechanicville Hydroelectric Plant, still produces electric power at 40 Hz and supplies power to the local 60 Hz transmission system through frequency changers. Industrial plants and mines in North America and Australia sometimes were built with 40 Hz electrical systems which were maintained until too uneconomic to continue. Although frequencies near 40 Hz found much commercial use, these were bypassed by standardized frequencies of 25, 50 and 60 Hz preferred by higher volume equipment manufacturers.\n\nThe Ganz Company of Hungary had standardized on 5000 alternations per minute (41 Hz) for their products, so Ganz clients had 41 Hz systems that in some cases ran for many years.\n\nIn the early days of electrification, so many frequencies were used that no one value prevailed (London in 1918 had ten different frequencies). As the 20th century continued, more power was produced at 60 Hz (North America) or 50 Hz (Europe and most of Asia). Standardization allowed international trade in electrical equipment. Much later, the use of standard frequencies allowed interconnection of power grids. It wasn't until after World War II with the advent of affordable electrical consumer goods that more uniform standards were enacted.\n\nIn the United Kingdom, a standard frequency of 50 Hz was declared as early as 1904, but significant development continued at other frequencies. The implementation of the National Grid starting in 1926 compelled the standardization of frequencies among the many interconnected electrical service providers. The 50 Hz standard was completely established only after World War II.\n\nBy about 1900, European manufacturers had mostly standardized on 50 Hz for new installations. The German Verband der Elektrotechnik (VDE), in the first standard for electrical machines and transformers in 1902, recommended 25 Hz and 50 Hz as standard frequencies. VDE did not see much application of 25 Hz, and dropped it from the 1914 edition of the standard. Remnant installations at other frequencies persisted until well after the Second World War.\n\nBecause of the cost of conversion, some parts of the distribution system may continue to operate on original frequencies even after a new frequency is chosen. 25 Hz power was used in Ontario, Quebec, the northern United States, and for railway electrification. In the 1950s, many 25 Hz systems, from the generators right through to household appliances, were converted and standardized. Until 2009, some 25 Hz generators were still in existence at the Sir Adam Beck 1 (these were retrofitted to 60 Hz) and the Rankine generating stations (until its 2009 closure) near Niagara Falls to provide power for large industrial customers who did not want to replace existing equipment; and some 25 Hz motors and a 25 Hz power station exist in New Orleans for floodwater pumps. The 15 kV AC rail networks, used in Germany, Austria, Switzerland, Sweden and Norway, still operate at 16⅔ Hz or 16.7 Hz.\n\nIn some cases, where most load was to be railway or motor loads, it was considered economic to generate power at 25 Hz and install rotary converters for 60 Hz distribution. Converters for production of DC from alternating current were available in larger sizes and were more efficient at 25 Hz compared with 60 Hz. Remnant fragments of older systems may be tied to the standard frequency system via a rotary converter or static inverter frequency changer. These allow energy to be interchanged between two power networks at different frequencies, but the systems are large, costly, and waste some energy in operation.\n\nRotating-machine frequency changers used to convert between 25 Hz and 60 Hz systems were awkward to design; a 60 Hz machine with 24 poles would turn at the same speed as a 25 Hz machine with 10 poles, making the machines large, slow-speed and expensive. A ratio of 60/30 would have simplified these designs, but the installed base at 25 Hz was too large to be economically opposed.\n\nIn the United States, Southern California Edison had standardized on 50 Hz. Much of Southern California operated on 50 Hz and did not completely change frequency of their generators and customer equipment to 60 Hz until around 1948. Some projects by the Au Sable Electric Company used 30 Hz at transmission voltages up to 110,000 volts in 1914.\n\nInitially in Brazil, electric machinery were imported from Europe and United States, implying the country had both 50 Hz and 60 Hz standards according to each region. In 1938, the federal government made a law, \"Decreto-Lei 852\", intended to bring the whole country under 50 Hz within eight years. The law didn't work, and in the early 1960s it was decided that Brazil would be unified under 60 Hz standard, because most developed and industrialized areas used 60 Hz; and a new law \"Lei 4.454\" was declared in 1964. Brazil underwent a frequency conversion program to 60 Hz that was not completed until 1978.\n\nIn Mexico, areas operating on 50 Hz grid were converted during the 1970s, uniting the country under 60 Hz.\n\nIn Japan, the western part of the country (Nagoya and west) uses 60 Hz and the eastern part (Tokyo and east) uses 50 Hz. This originates in the first purchases of generators from AEG in 1895, installed for Tokyo, and General Electric in 1896, installed in Osaka. The boundary between the two regions contains four back-to-back HVDC substations which convert the frequency; these are Shin Shinano, Sakuma Dam, Minami-Fukumitsu, and the Higashi-Shimizu Frequency Converter.\n\nUtility frequencies in North America in 1897\n\nUtility frequencies in Europe to 1900\n\nEven by the middle of the 20th century, utility frequencies were still not entirely standardized at the now-common 50 Hz or 60 Hz. In 1946, a reference manual for designers of radio equipment listed the following now obsolete frequencies as in use. Many of these regions also had 50 cycle, 60 cycle or direct current supplies.\n\nFrequencies in use in 1946 (as well as 50 Hz and 60 Hz)\nWhere regions are marked (*), this is the only utility frequency shown for that region.\n\nOther power frequencies are still used. Germany, Austria, Switzerland, Sweden and Norway use traction power networks for railways, distributing single-phase AC at 16⅔ Hz or 16.7 Hz. A frequency of 25 Hz is used for the Austrian Mariazell Railway, as well as Amtrak and SEPTA's traction power systems in the United States. Other AC railway systems are energized at the local commercial power frequency, 50 Hz or 60 Hz.\n\nTraction power may be derived from commercial power supplies by frequency converters, or in some cases may be produced by dedicated traction powerstations. In the 19th Century, frequencies as low as 8 Hz were contemplated for operation of electric railways with commutator motors.\nSome outlets in trains carry the correct voltage, but using the original train network frequency like 16⅔ Hz or 16.7 Hz.\n\nPower frequencies as high as 400 Hz are used in aircraft, spacecraft, submarines, server rooms for computer power, military equipment, and hand-held machine tools. Such high frequencies cannot be economically transmitted long distances; the increased frequency greatly increases series impedance due to the inductance of transmission lines, making power transmission difficult. Consequently, 400 Hz power systems are usually confined to a building or vehicle.\n\nTransformers, for example, can be made smaller because the magnetic core can be much smaller for the same power level. Induction motors turn at a speed proportional to frequency, so a high frequency power supply allows more power to be obtained for the same motor volume and mass. Transformers and motors for 400 Hz are much smaller and lighter than at 50 or 60 Hz, which is an advantage in aircraft and ships. A United States military standard MIL-STD-704 exists for aircraft use of 400 Hz power.\n\nRegulation of power system frequency for timekeeping accuracy was not commonplace until after 1926 with Laurens Hammond's invention of the electric clock driven by a synchronous motor. During the 1920s, Hammond gave away hundreds of such clocks to power station owners in the U.S. and Canada as incentive to maintain a steady 60-cycle frequency, thus rendering his inexpensive clock uniquely practical in any business or home in North America. Developed in 1933, The Hammond Organ uses a synchronous AC clock motor to maintain correct speed of its internal 'tone wheel' generator, thus keeping all notes pitch perfect, based on power-line frequency stability.\n\nToday, AC-power network operators regulate the daily average frequency so that clocks stay within a few seconds of correct time. In practice the nominal frequency is raised or lowered by a specific percentage to maintain synchronization. Over the course of a day, the average frequency is maintained at the nominal value within a few hundred parts per million. In the synchronous grid of Continental Europe, the deviation between network phase time and UTC (based on International Atomic Time) is calculated at 08:00 each day in a control center in Switzerland. The target frequency is then adjusted by up to ±0.01 Hz (±0.02%) from 50 Hz as needed, to ensure a long-term frequency average of exactly 50 Hz × 60 s/min × 60 min/h × 24 h/d = cycles per day. In North America, whenever the error exceeds 10 seconds for the east, 3 seconds for Texas, or 2 seconds for the west, a correction of ±0.02 Hz (0.033%) is applied. Time error corrections start and end either on the hour or on the half-hour. Efforts to remove the TEC in North America are described at electric clock.\n\nReal-time frequency meters for power generation in the United Kingdom are available online – an official National Grid one, and an unofficial one maintained by Dynamic Demand.\nReal-time frequency data of the synchronous grid of Continental Europe is available on websites such as and . The Frequency Monitoring Network (FNET) at the University of Tennessee measures the frequency of the interconnections within the North American power grid, as well as in several other parts of the world. These measurements are displayed on the FNET website.\n\nIn the United States, the Federal Energy Regulatory Commission made Time Error Correction mandatory in 2009. In 2011, The North American Electric Reliability Corporation (NERC) discussed a proposed experiment that would relax frequency regulation requirements for electrical grids which would reduce the long-term accuracy of clocks and other devices that use the 60 Hz grid frequency as a time base.\n\nThe primary reason for accurate frequency control is to allow the flow of alternating current power from multiple generators through the network to be controlled. The trend in system frequency is a measure of mismatch between demand and generation, and is a necessary parameter for load control in interconnected systems.\n\nFrequency of the system will vary as load and generation change. Increasing the mechanical input power to any individual synchronous generator will not greatly affect the overall system frequency, but will produce more electric power from that unit. During a severe overload caused by tripping or failure of generators or transmission lines the power system frequency will decline, due to an imbalance of load versus generation. Loss of an interconnection while exporting power (relative to system total generation) will cause system frequency to increase upstream of the loss, but may cause a collapse downstream of the loss, as generation is now not keeping pace with consumption. Automatic generation control (AGC) is used to maintain scheduled frequency and interchange power flows. Control systems in power stations detect changes in the network-wide frequency and adjust mechanical power input to generators back to their target frequency. This counteracting usually takes a few tens of seconds due to the large rotating masses involved (although the large masses serve to limit the magnitude of short-term disturbances in the first place). Temporary frequency changes are an unavoidable consequence of changing demand. Exceptional or rapidly changing mains frequency is often a sign that an electricity distribution network is operating near its capacity limits, dramatic examples of which can sometimes be observed shortly before major outages. Large generating stations including solar farms can reduce their average output and use the headroom between operating load and maximum capacity to assist in providing grid regulation; response of solar inverters is faster than generators, because they have no rotating mass. As variable resources such as solar and wind replace traditional generation and the inertia they provided, algorithms have had to become more sophisticated. Energy storage, such as batteries, are fulfilling the regulation role to an expanding degree as well.\n\nFrequency protective relays on the power system network sense the decline of frequency and automatically initiate load shedding or tripping of interconnection lines, to preserve the operation of at least part of the network. Small frequency deviations (i.e.- 0.5 Hz on a 50 Hz or 60 Hz network) will result in automatic load shedding or other control actions to restore system frequency.\n\nSmaller power systems, not extensively interconnected with many generators and loads, will not maintain frequency with the same degree of accuracy. Where system frequency is not tightly regulated during heavy load periods, the system operators may allow system frequency to rise during periods of light load, to maintain a daily average frequency of acceptable accuracy. Portable generators, not connected to a utility system, need not tightly regulate their frequency, because typical loads are insensitive to small frequency deviations.\n\nLoad-frequency control (LFC) is a type of integral control that restores the system frequency and power flows to adjacent areas back to their values before a change in load. The power transfer between different areas of a system is known as \"net tie-line power\".\n\nThe general control algorithm for LFC was developed by Nathan Cohn in 1971. The algorithm involves defining the term \"area control error\" (ACE), which is the sum of the net tie-line power error and the product of the frequency error with a frequency bias constant. When the area control error is reduced to zero, the control algorithm has returned the frequency and tie-line power errors to zero.\n\nAC-powered appliances can give off a characteristic hum, often called \"mains hum\", at the multiples of the frequencies of AC power that they use (see Magnetostriction). It is usually produced by motor and transformer core laminations vibrating in time with the magnetic field. This hum can also appear in audio systems, where the power supply filter or signal shielding of an amplifier is not adequate.\n\nMost countries chose their television vertical synchronization rate to approximate the local mains supply frequency. This helped to prevent power line hum and magnetic interference from causing visible beat frequencies in the displayed picture of analogue receivers.\nAnother use of this side effect has resulted in its use as a forensic tool. When a recording is made that captures audio near an AC appliance or socket, the hum is also inadvertently recorded. The peaks of the hum repeat every AC cycle (every (1000/50) round 2 ms for 50 Hz AC, or every (1000/60) round 2 ms for 60 Hz AC). Any edit of the audio that is not a multiplication of the time between the peaks will distort the regularity, introducing a phase shift. A continuous wavelet transform analysis will show discontinuities that may tell if the audio has been cut.\n\n\n"}
