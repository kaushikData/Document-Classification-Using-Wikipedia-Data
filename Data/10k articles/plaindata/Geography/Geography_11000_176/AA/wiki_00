{"id": "1817315", "url": "https://en.wikipedia.org/wiki?curid=1817315", "title": "Am Buachaille", "text": "Am Buachaille\n\nAm Buachaille is a sea stack, or vertical rock formation composed of Torridonian Sandstone, south-west of Sandwood Bay in the Scottish county of Sutherland. It lies at the tip of the Rubh' a Bhuachaille headland around north of Kinlochbervie.\n\nThe stack is high and was first climbed in 1968 by the mountaineers Tom Patey, Ian Clough and John Cleare. At least four climbing routes are identified on Am Buachaille which is considered a \"famous\" sea stack climb and has been called the \"most serious of 'the big three' Scottish stacks\" and a \"truly great stack\". The easiest route is graded Hard Very Severe (HVS) and access to the stack involves a swim at low tide.\n\nThe name means \"the herdsman\" or \"the shepherd\" in Scottish Gaelic.\n"}
{"id": "16657678", "url": "https://en.wikipedia.org/wiki?curid=16657678", "title": "Back garden", "text": "Back garden\n\nA back garden is a residential garden located at the rear of a property, on the other side of the house from the front garden. Such gardens have a special place in English suburban and gardening culture.\n\nA back garden arises when the main building divides the surrounding gardens into two. This happens especially in the high density housing of British cities and towns. A semi-detached house typical of the British suburbs of the 20th century will have front gardens which face the road and provide access. The back gardens in such cases will be more secluded and access will typically be via the dwelling or by a path around the side.\n\nIf the housing is terraced, then no side path is possible and access may be provided by an alley which runs behind the rear of the terrace. While buildings opening directly onto a street may not have a front garden, most will have some space at the back, however small; the exception being back-to-back houses found in northern industrial towns in England such as Leeds, but now mostly demolished. A private back yard with a \"privy\" (toilet) was a defining feature of the byelaw terraced house, a type of dwelling built to comply with the Public Health Act 1875.\n\nA front garden is a formal and semi-public space and so subject to the constraints of convention and law. However, the back garden is more private and casual, and so can be put to more purposes.\n\nIn Britain there are over 10 million back gardens. British planning require minimum distances between the rear faces of adjacent dwellings and so there is usually space for a back garden of some sort. In other countries, such as Australia, this does not apply and preference for buildings with a large footprint has tended to squeeze out the space at the rear.\n\nBecause of weather constraints, it is usual to use a garden more in the summer than in the winter, although some usages are traditional, such as for a bonfire on Bonfire Night, 5 November. Similarly, daytime usage is more common than nighttime.\nFunctionally, it may be used for:\n\n\nIn fact, its functional and recreational use is so varied, that it cannot be easily categorised. Many of the freedoms of the use of the back garden come from the restrictions, social or legal of what are not done in the front.\n\nUsually, clothes are not dried, vegetables are not grown, and sunbathing is not carried out in a front garden. All these can happen in the privacy of the back garden.\n\nTraditionally, people treat a back garden as private to themselves, and not those they are neighbours to. The social etiquette of how one can greet and interact one's neighbours may be complex and defined by many informal social rules.\n\nIn some areas, talking to one's neighbours over the back wall (the side wall following the property boundary line) is usual, and is a welcome form of neighbourliness and in other places it is not.\n\n"}
{"id": "26016875", "url": "https://en.wikipedia.org/wiki?curid=26016875", "title": "Bermuda National Grid", "text": "Bermuda National Grid\n\nThe Bermuda National Grid 2000 (BNG) is a kind of Transverse Mercator projection. It is not a Universal Transverse Mercator (UTM) projection, as it has an origin and other parameters that are different from those used in UTM.\n\nGrid Parameters:\n\nOnline resources:\n"}
{"id": "1512317", "url": "https://en.wikipedia.org/wiki?curid=1512317", "title": "Broad Fourteens", "text": "Broad Fourteens\n\nThe Broad Fourteens is an area of the southern North Sea that is fairly consistently fourteen fathoms (84 feet/26 metres) deep. Thus, on a nautical chart with depths given in fathoms, a broad area with many \"14\" notations can be seen.\n\nThe Broad Fourteens region is located off the coast of the Netherlands and south of the Dogger Bank, roughly between longitude 3°E and 4°30'E and latitude 52°30'N and 53°30'N.\n\nThe area is known to the Dutch and German navies as the \"Breeveertien\".\n\nGeologically it is comparable to the Long Forties, another submerged plateau that has related origins.\n\nThe area has been the scene of many naval engagements throughout history. For example:\n\n\nThe shallowness of this area means that the largest oil tankers, when fully loaded cannot traverse the Broad Fourteens to reach the English Channel from the North Sea because their draft is too deep.\n\n"}
{"id": "54471473", "url": "https://en.wikipedia.org/wiki?curid=54471473", "title": "Cartographica", "text": "Cartographica\n\nThe Cartographica is an interdisciplinary peer-reviewed academic journal and the official publication of the Canadian Cartographic Association. \"Cartographica\" is published four times a year by the University of Toronto Press.\n\nThe journal is abstracted and indexed in:\n"}
{"id": "13429077", "url": "https://en.wikipedia.org/wiki?curid=13429077", "title": "Cartography of Asia", "text": "Cartography of Asia\n\nThe cartography of Asia can refer to the representation of Asia on a map, or to depictions of the world by cartographers from Asia. Depictions of portions of Asia have existed on maps as early as the 6th century BCE, with maps being drafted to depict the Babylonian, Hellenistic Greek, and Han dynasty empires. \n\nDuring the Middle Ages, Muslim geographers drew maps with more accurate depictions of Southern, Western, and Central Asia, and European maps began to more frequently represent Asia's landmass. Chinese geography from this period includes more detailed portrayals of the Indian Ocean, Arabian Peninsula, and East Africa. European maps of Asia would become much more accurate during the European Age of Discovery, starting in the 15th century.\n\nModern maps of Asia make use of digitization, photographic surveys, and satellite imagery.\n\nBabylon in Southwest Asia is at the center of the very earliest world maps, beginning with the Babylonian world map in the 6th century BCE; it is a clay tablet 'localized' world map of Babylon, rivers, encircling ocean, and terrain, surrounded by 'islands' in a 7-star format. In classical Greek geography, \"Asia\" is one of three major landmasses, besides Europe and Libya. Asia is given higher resolution in Hellenistic geography, in particular on Ptolemy world map. Cartography of India begins with early charts for navigation and constructional plans for buildings. Chinese geography from the 2nd century BC (Han dynasty) becomes aware of Turkestan, where Hellenistic Greek and Han Chinese spheres of influence overlap.\n\nIn medieval T and O maps, Asia makes for half the world's landmass, with Africa and Europe accounting for a quarter each. With the High Middle Ages, Southwest and Central Asia receive better resolution in Muslim geography, and the 11th century map by Mahmud al-Kashgari is the first world map drawn from a Central Asian point of view. In the same period, European explorers of the Silk road like William Rubruck and Marco Polo increase geographical knowledge of Asia in the west, in particular establishing that the Caspian Sea is not connected to the northern ocean.\n\nChinese exploration by medieval times extends Chinese geographical knowledge to the Indian Ocean, the Arabian peninsula and East Africa as well as Southeast Asia.\n\nEuropean maps of Asia become much more detailed from the 15th century, the 1459 Fra Mauro map showing a reasonable complete picture, including correctly placed Korea and Japan.\n\nModern map making techniques in Asia, like other parts of the world, employ digitization, photographic surveys and printing. Satellite imageries, aerial photographs and video surveying techniques are also used.\n\n\n\n\n"}
{"id": "52942632", "url": "https://en.wikipedia.org/wiki?curid=52942632", "title": "Central Place", "text": "Central Place\n\nCentral Place is mixed-use development, consisting primarily of CEB Tower (headquarters of CEB Inc.) to the south, and a residential tower to the north, with a plaza between them in Arlington, Virginia. The office tower is home to Virginia's second public observation deck, after the City Hall of Richmond, Virginia, located nearly above sea level, and is the highest public location in the Washington metropolitan area.\n\nDesign began in 2002 and construction was set to begin in 2008, but was delayed to 2014 due to the Great Recession in the United States. The complex is located directly above the Rosslyn station of the Washington Metro and some bedrock, presenting engineering challenges.\n\n"}
{"id": "469300", "url": "https://en.wikipedia.org/wiki?curid=469300", "title": "Citrus production", "text": "Citrus production\n\nCitrus fruits are the highest-value fruit crop in terms of international trade. There are two main markets for citrus fruit:\n\nOranges account for the majority of citrus production but the industry also sees significant quantities of grapefruits, pomeloes, lemons, and limes.\n\nWhile the origin of citrus fruits cannot be precisely identified, researchers believe they began to appear in Southeast Asia at least 4,000 BC. From there, they slowly spread to northern Africa, mainly through migration and trade. During the period of the Roman Empire demand by higher-ranking members of society, along with increased trade, allowed the fruits to spread to southern Europe. Citrus fruits spread throughout Europe during the Middle Ages, and were then brought to the Americas by Spanish explorers. Worldwide trade in citrus fruits didn't appear until the 20th century and trade in orange juice developed as late as 1940.\n\nTotal production and consumption of citrus fruit has grown strongly since the 1980s. Current annual worldwide citrus production is estimated at over 70 million tons, with more than half of this being oranges. According to the United Nations Conference on Trade and Development (UNCTAD), the rise in citrus production is mainly due to the increase in cultivation areas, improvements in transportation and packaging, rising incomes and consumer preference for healthy foods.\n\nThis trend was projected to change from 2000 to 2010 since the high production levels have slowed the rate of new plantings.\n\nCitrus fruits are produced all over the world; according to UNCTAD, as of 2004 there were 140 citrus-producing countries. Around 70% of the world's total citrus production is grown in the Northern Hemisphere, in particular countries around the Mediterranean and the United States, although Brazil is the largest citrus producer. \n\nIn the United States, most orange juice and grapefruit is produced in Florida, while citrus fruits for consumption as fresh fruit are mainly grown in California, Arizona, and Texas. Smaller markets for citrus growth in the United States originate in South Carolina, Georgia, Oklahoma, Tennessee, and the gulf coastal states, including Louisiana, Alabama, Mississippi, and Georgia, as well as North Carolina. Independent cultivars are found in Kentucky, Virginia, and even Missouri, Southern Illinois, and far Southern Kansas. The farther north the range, the more seasonal the cultivation. Florida produces approximately 100 million boxes annually (each box is 90 lbs).\n\nChina could be a major player in the orange juice and processed citrus markets, except for high tariffs on citrus that make domestic sale more profitable. Though citrus originated in southeast Asia, current citrus production is low due to lower-than-average yields, high production and marketing costs, and disease.\n\nCitrus production in Europe continues to decline, although the clementines produced by Spain are increasing in popularity among consumers.\n\nAbout a third of citrus fruit production goes for processing: more than 80% of this is for orange juice production. Demand for fresh and processed oranges continues to rise in excess of production, especially in developed countries.\n\nThe two main players are Florida in the United States and São Paulo in Brazil. Production of orange juice between these two makes up roughly 85% of the world market. Brazil exports 99 percent of its production, while 90 percent of Florida’s production is consumed in the United States.\n\nOrange juice is traded internationally in the form of frozen concentrated orange juice to reduce the volume used, so that storage and transportation costs are lower.\n\nCitrus production is often cut short in many areas by outbreaks of bacteria known as \"Xanthomonas axonopodis\", or Citrus canker, which cause unsightly lesions on all parts of the plant, affecting tree vitality and early drop of fruit. While not harmful to human consumption, the fruit becomes too unsightly to be sold, and entire orchards are often destroyed to protect the outbreak from spreading.\n\nCitrus canker affects all varieties of citrus trees, and recent outbreaks in Australia, Brazil, and the United States have slowed citrus production in parts of those countries. Citrus leafminer moths are a major concern where citrus canker exists. The openings created by citrus leafminer make the tree highly susceptible to the \"X. axonopodis\" bacteria which leads to citrus canker.\n\nHuanglongbing (HLB), called citrus greening within the industry, is recognized as the deadliest citrus disease the Florida citrus industry has ever faced. This can be attributed to the economic costs of implementing new care-taking strategies, and overall tree loss creating a loss of revenues. A look at total Florida citrus-growing acreage provides a tangible impression to the hardships citrus greening provides; in 2000 there was 665,529 commercially producing citrus acres, while in 2011 there were 473,086 commercially producing citrus acres in Florida. Every year citrus reports indicate a continued loss of citrus production. Citrus greening is being attributed for a total output impact of -4.51 billion, and a loss of 8,257 jobs within Florida. The disease has now spread throughout the entire state, and affects every Florida citrus grower. The disease is spread through an insect vector, the Asian citrus psyllid. The psyllid was previously introduced into Florida in 1998. At this time citrus greening was never found within the state, thus the psyllids spread was left unchecked. By the time citrus greening had reached Florida psyllid populations were well established throughout the state of Florida. The first positive case of greening disease was in August 2005, when a greening positive citrus tree was discovered in Miami-Dade County. It was at this time the entire Florida citrus industry changed its citriculture practices overnight. Intensive pesticide applications, aggressive removal of citrus greening positive trees, and the complete switch from outdoor to indoor citrus nursery operations transpired.\n\nSymptoms of citrus greening are numerous, and can be varied in citrus trees. A tree will develop yellow shoots instead of the expected deep green colors. The disease presents itself on the leaves by giving an asymmetrical blotchy-mottle appearance. This is the key diagnosing characteristic of citrus greening. On affected limbs, fruit tend to be lopsided. The fruit will also never ripen and have a sour taste, making them unmarketable for both juice and fresh fruit productions. In later stages of infection the tree will suffer from heavy leaf drop, high percentages of fruit drop, and deep twig die back. A greening positive citrus tree’s canopies will be airy due to the defoliation the disease causes. After a tree becomes infected with citrus greening it becomes uneconomical and may die within 2–5 years.\n\nVector control of citrus greening began when the disease was first introduced in 2005. All commercial citrus growers are advised in applying two dormancy pesticide sprays. These broad-spectrum pesticide sprays are applied in winter when adult psyllid populations decline to almost exclusive overwintering adults. With this strategy, significant reductions of populations withhold for up to 6 months. This fact is crucial as it protects the spring flush, which accounts for over 70% of new leaves for the year, from the infectious psyllid attacks. The spring flush typically occurs 3 months past winter. More aggressive citrus grove care-takers may employ a wide host of pesticides to try to keep psyllid populations low yearround. These growers may spray pesticides up to 7 times a year rotating various pesticides to employ different modes of actions against the psyllid. This is done in an attempt to prevent resistances of psyllids to the various pesticides. The spraying of pesticides is the only method of control for the citrus greening vector, the psyllid. It is impossible to kill all psyllids through pesticides, thus strategic timing of pesticide sprays are done to try and slow the gradual spread of HLB throughout the citrus grove. Unfortunately, at this time it is inevitable that a commercial citrus grove will reach 100 percent infection rates even with aggressive sprays.\n\nProductivity of a citrus groves can be retained at pre-Huanglongbing levels through a three pronged strategy. Current research is aimed at the goal of giving the greatest yields for the lowest costs. Indeed, these lower costs are necessitated by the increased per acre cost of caretaking brought upon by Huanglongbing infection. Huanglongbing forces the commercial citrus caretaker to spray his/her block of citrus many more times a year than normal, considerably increasing costs. Per pound prices of citrus must continue increasing for citrus to remain profitable due to disease pressure.\n\nFoliar fertilizers are now being sprayed on citrus trees at considerably higher rates than before citrus greening disease. Inspiration for the mixture of foliar nutrients was drawn from a local citrus grower, Maury Boyd. Mr. Boyd was the first to try a strategy of not removing greening positive citrus trees, and instead attempt aggressive nutritional sprays. His grove was as a result the first to remain economical with a high percentage rate of greening disease infection. His spray program is under considerable research by the University of Florida’s IFAS department. Further research is still needed, and being carried out to determine which specific fertilizer compounds, and the quantities used are the most efficient.\n\nVector control of the psyllid, which is the sole means for citrus greening to spread is now done routinely. Before citrus greening disease was introduced, commercial citrus growers did not have to spray pesticides targeting insects. Current research is aimed at pesticide application timing, and pesticide choice for efficacy.\n\nAfter the introduction of citrus greening disease, all commercial nurseries, where new young trees are purchased, were relocated indoors and a bud wood registration program was enacted. Previously young citrus trees were grown outdoors before disease pressure became an issue. This is to certify that Florida grove owners are able to purchase clean citrus trees for the planting of citrus groves. A greening positive young citrus tree will never reach maturity, even with intensive sprays.\n\nThese three keystone citriculture practices vector control, foliar nutrition, and certified young trees make up the new best management practices for commercial citrus growers against the citrus greening disease fight.\n\nNitrogen, Potassium and Phosphorus are the main macronutrients needed in citrus production, as well as Calcium, Magnesium and Sulfur.\n\nNitrogen is important for overall tree and leaf growth and peel thickness, and fruit acidity. \n\nPhosphorus improves root growth, fruit yield and weight and juice while it reduces peel thickness. \n\nPotassium maintains fruit size, acidity, juice and disease tolerance, and it is taken up largely by the citrus fruit; too little K can lead to splitting and plugging of the fruit.\n\nBoron is involved in many enzymatic systems, when it is deficient, the tree suffers in fruit and leaf quality, and the tree loses apical dominance. \n\nCopper affects photosynthesis and fruit set. Deficiency causes drooping of shoots, \"brown gum\" eruptions, and death may occur from the tip. If copper based fungicides are used, this is not usually a problem. \n\nActing as a catalyst for reactions, this element is a cofactor for many enzymes, important for sweetness, increasing total soluble solids and boosting vitamin C and juice content for fruit. Iron deficiency is the most common of the micronutrients, causing symptoms of increased prominence of leaf veins and leaves turning white. \n\nDeficiency is associated with zinc and iron reductions as well, causing a mottled yellowing of the leaf, almost of a variegated quality. It improves overall sugar production in fruit juice. \n\n\n"}
{"id": "3841204", "url": "https://en.wikipedia.org/wiki?curid=3841204", "title": "Clipper route", "text": "Clipper route\n\nIn sailing, the clipper route was the traditional route derived from the Brouwer Route and sailed by clipper ships between Europe and the Far East, Australia and New Zealand. The route ran from west to east through the Southern Ocean, in order to make use of the strong westerly winds of the Roaring Forties. Many ships and sailors were lost in the heavy conditions along the route, particularly at Cape Horn, which the clippers had to round on their return to Europe.\n\nThe clipper route fell into commercial disuse with the introduction of steam ships, and the opening of the Suez and Panama Canals. However, it remains the fastest sailing route around the world, and as such has been the route for several prominent yacht races, such as the \"Around Alone\" and \"Vendée Globe\".\n\nThe clipper route from England to Australia and New Zealand, returning via Cape Horn, offered captains the fastest circumnavigation of the world, and hence potentially the greatest rewards; many grain, wool and gold clippers sailed this route, returning home with valuable cargos in a relatively short time. However, this route, passing south of the three great capes and running for much of its length through the Southern Ocean, also carried the greatest risks, exposing ships to the hazards of fierce winds, huge waves, and icebergs. This combination of the fastest ships, the highest risks, and the greatest rewards combined to give this route a particular aura of romance and drama.\n\nThis route ran from England down the east Atlantic Ocean to the Equator, crossing at about the position of Saint Peter and Paul Rocks, around 20 degrees west. A good sailing time for the to this point would have been around 21 days; however, an unlucky ship could spend an additional three weeks crossing the doldrums.\n\nThe route then ran south through the western South Atlantic, following the natural circulation of winds and currents, passing close to Trindade, then curving south-east past Tristan da Cunha. The route crossed the Greenwich meridian at about 40 degrees south, taking the clippers into the Roaring Forties after about sailed from Plymouth. A good time for this run would have been about 43 days.\n\nOnce into the forties, a ship was also inside the ice zone, the area of the Southern Ocean where there was a significant chance of encountering icebergs. Safety would dictate keeping to the north edge of this zone, roughly along the parallel of 40 degrees south; however, the great circle route from the Cape of Good Hope to Australia, curving down to 60 degrees south, is shorter, and would also offer the strongest winds. Ship's masters would therefore go as far south as they dared, weighing the risk of ice against a fast passage.\n\nThe clipper ships bound for Australia and New Zealand would call at a variety of ports. A ship sailing from Plymouth to Sydney, for example, would cover around ; a fast time for this passage would be around 100 days. \"Cutty Sark\" made the fastest passage on this route by a clipper, in 72 days. \"Thermopylae\" made the slightly shorter passage from London to Melbourne, , in just 61 days in 1868–69.\n\nThe following map traces the outbound route of the 1874 voyage from London to Adelaide, South Australia, of the clipper ship \"City of Adelaide\", which today is the world's oldest surviving clipper ship. The latitudes and longitudes are obtained from the surviving diary of 21-year-old passenger James McLauchlan.\n\nThe return passage continued east from Australia; ships stopping at Wellington would pass through the Cook Strait, but otherwise this tricky passage was avoided, with ships passing instead around the south end of New Zealand. Once again, eastbound ships would be running more or less within the ice zone, staying as far south as possible for the shortest route and strongest winds. Most ships stayed north of the latitude of Cape Horn, at 56 degrees south, following a southward dip in the ice zone as they approached the Horn.\n\nThe Horn itself had, and still has, an infamous reputation among sailors. The strong winds and currents which flow perpetually around the Southern Ocean without interruption are funnelled by the Horn into the relatively narrow Drake Passage; coupled with turbulent cyclones coming off the Andes, and the shallow water near the Horn, this combination of factors can create violently hazardous conditions for ships.\n\nThose ships which survived the Horn then made the passage back up the Atlantic, following the natural wind circulation up the eastern South Atlantic and more westerly in the North Atlantic. A good run for the from Sydney to Plymouth would be around 100 days; \"Cutty Sark\" made it in 84 days, and \"Thermopylae\" in 77 days. \"Lightning\" made the longer passage from Melbourne to Liverpool in 65 days in 1854–55, completing a circumnavigation of the world in 5 months, 9 days, which included 20 days spent in port.\n\nThe later windjammers, which were usually large four-masted barques optimized on cargo and handling rather than running, usually made the voyage in 90 to 105 days. The fastest recorded time on Great Grain Races was on Finnish four-masted barque \"Parma\", 83 days in 1933. Her master on the voyage was the Finnish captain Ruben de Cloux.\n\nThe following map traces the homeward route of the 1867 voyage from Adelaide to London of the clipper ship \"City of Adelaide\". The latitudes and longitudes were obtained from the surviving diary of 15-year-old passenger Frederick Bullock. A visual comparison of this map with the previous map shows that the \"City of Adelaide\" travelled a figure-of-eight route around the North and South Atlantic Oceans, following the natural circulation of winds and currents. On other homeward voyages the \"City of Adelaide\" sometimes travelled around Cape Horn.\n\nThe route sailed by a sailing ship was always heavily dictated by the wind conditions, which are generally reliable from the west in the forties and fifties. Even here, however, winds are variable, and the precise route and distance sailed would depend on the conditions on a particular voyage. Ships in the deep Southern Ocean could find themselves faced with persistent headwinds, or even becalmed.\n\nSailing ships attempting to go against the route, however, could have even greater problems. In 1922, \"Garthwray\" attempted to sail west around the Horn carrying cargo from the Firth of Forth to Iquique, Chile. After two attempts to round the Horn the \"wrong way\", her master gave up and sailed east instead, reaching Chile from the other direction.\n\nEven more remarkable was the voyage of \"Garthneill\" in 1919. Attempting to sail from Melbourne to Bunbury, Western Australia, a distance of , she was unable to make way against the forties winds south of Australia, and was faced by strong westerly winds again when she attempted to pass through the Torres Strait to the north. She finally turned and sailed the other way, passing the Pacific, Cape Horn, the Atlantic, the Cape of Good Hope, and the Indian Ocean to finally arrive in Bunbury after 76 days at sea.\n\nIt is also worth pointing out that the first person to circumnavigate the world solo, Joshua Slocum in the Spray, did it rounding Cape Horn from east to west. His was not the fastest circumnavigation on record, and he took more than one try to get through Cape Horn.\n\nThe introduction of steam ships, and the opening of the Suez and Panama Canals, spelled the demise of the clipper route as a major trade route. However, it remains the fastest sailing route around the world, and so the growth in recreational long-distance sailing has brought about a revival of sailing on the route.\n\nThe first person to attempt a high-speed circumnavigation of the clipper route was Francis Chichester. Chichester was already a notable aviation pioneer, who had flown solo from London to Sydney, and also a pioneer of single-handed yacht racing, being one of the founders of the Single-Handed Trans-Atlantic Race (the \"OSTAR\"). After the success of the \"OSTAR\", Chichester started looking into a clipper-route circumnavigation. He wanted to make the fastest ever circumnavigation in a small boat, but specifically set himself the goal of beating a \"fast\" clipper-ship passage of 100 days to Sydney. He set off in 1966, and completed the run to Sydney in 107 days; after a stop of 48 days, he returned via Cape Horn in 119 days.\n\nChichester's success inspired several others to attempt the next logical step: a non-stop single-handed circumnavigation along the clipper route. The result was the Sunday Times Golden Globe Race, which was not only the first single-handed round-the world yacht race, but in fact the first round-the world yacht race in any format. Possibly the strangest yacht race ever run, it culminated in a successful non-stop circumnavigation by just one competitor, Robin Knox-Johnston, who became the first person to sail the clipper route single-handed and non-stop. However Bernard Moitessier decided to withdraw from the race after rounding Cape Horn in a promising position. Instead of heading home to Europe he decided to continue to Tahiti, completing his circumnavigation south of Cape Town and doing actual one and a half when arriving in Papeete.\n\nToday, there are several major races held regularly along the clipper route. The Volvo Ocean Race is a crewed race with stops which sails the clipper route every four years. Two single-handed races, inspired by Chichester and the Golden Globe race, are the \"Around Alone\", which circumnavigates with stops, and the \"Vendée Globe\", which is non-stop.\n\nIn March 2005, Bruno Peyron and crew on the catamaran \"Orange II\" set a new world record for a circumnavigation by the clipper route, of 50 days, 16 hours, 20 minutes and 4 seconds.\n\nAlso in 2005, Ellen MacArthur set a new world record for a single-handed non-stop circumnavigation in the trimaran \"B&Q/Castorama\". Her time along the clipper route of 71 days, 14 hours, 18 minutes 33 seconds is the fastest ever circumnavigation of the world by a single-hander. While this record still leaves MacArthur as the fastest female singlehanded circumnavigator, in 2008 Francis Joyon eclipsed that record in the trimaran \"IDEC\" with a time of 57 days, 13 hours, 34 minutes 6 seconds.\n\n\n"}
{"id": "5222", "url": "https://en.wikipedia.org/wiki?curid=5222", "title": "Colombia", "text": "Colombia\n\nColombia ( , ; ), officially the Republic of Colombia (), is a country largely situated in the northwest of South America, with territories in Central America. Colombia shares a border to the northwest with Panama, to the east with Venezuela and Brazil and to the south with Ecuador and Peru. It shares its maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Haiti and the Dominican Republic. The sovereign state of Colombia is a unitary, constitutional republic comprising thirty-two departments.\n\nColombia has been inhabited by various indigenous peoples since 12,000 BCE, including the Muisca, Quimbaya, and the Tairona. The Spanish arrived in 1499 and by the mid-16th century conquered and colonized much of the region, establishing the New Kingdom of Granada, with Santafé de Bogotá as its capital. Independence from Spain was achieved in 1819, but by 1830 the \"Gran Colombia\" Federation was dissolved, with what is now Colombia and Panama emerged as the Republic of New Granada. The new nation experimented with federalism as the Granadine Confederation (1858), and then the United States of Colombia (1863), before the Republic of Colombia was finally declared in 1886. Panama seceded in 1903. Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict, which escalated in the 1990s but then decreased from 2005 onward.\n\nColombia is one of the most ethnically and linguistically diverse countries in the world, with its rich cultural heritage reflecting various European, Middle Eastern, African, and indigenous influences. Its urban centres are mostly located in the highlands of the Andes mountains and the Caribbean coast.\n\nColombian territory also encompasses Amazon rainforest, tropical grassland and both Caribbean and Pacific coastlines. Subsequently, it is one of the world's 17 megadiverse countries, and the most densely biodiverse of these per square kilometer.\n\nColombia is a middle power and regional actor in Latin America, with the fourth-largest economy. It is part of the CIVETS group of six leading emerging markets and is a member of the UN, the WTO, the OECD, the OAS, the Pacific Alliance, and other international organizations. Colombia has a diversified economy with macroeconomic stability and favorable growth prospects in the long run.\n\nThe name \"Colombia\" is derived from the last name of Christopher Columbus (, ). It was conceived by the Venezuelan revolutionary Francisco de Miranda as a reference to all the New World, but especially to those portions under Spanish rule (by then from Mississippi river to Patagonia). The name was later adopted by the Republic of Colombia of 1819, formed from the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).\n\nWhen Venezuela, Ecuador and Cundinamarca came to exist as independent states, the former Department of Cundinamarca adopted the name \"Republic of New Granada\". New Granada officially changed its name in 1858 to the Granadine Confederation. In 1863 the name was again changed, this time to United States of Colombia, before finally adopting its present name – the Republic of Colombia – in 1886.\n\nTo refer to this country, the Colombian government uses the terms \"Colombia\" and \"República de Colombia\".\n\nOwing to its location, the present territory of Colombia was a corridor of early human migration from Mesoamerica and the Caribbean to the Andes and Amazon basin. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley southwest of Bogotá. These sites date from the Paleoindian period (18,000–8000 BCE). At Puerto Hormiga and other sites, traces from the Archaic Period (~8000–2000 BCE) have been found. Vestiges indicate that there was also early occupation in the regions of El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found at San Jacinto, dates to 5000–4000 BCE.\nIndigenous people inhabited the territory that is now Colombia by 12,500 BCE. Nomadic hunter-gatherer tribes at the El Abra, Tibitó and Tequendama sites near present-day Bogotá traded with one another and with other cultures from the Magdalena River Valley. Between 5000 and 1000 BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Zenú, Quimbaya, and Tairona developed the political system of \"cacicazgos\" with a pyramidal structure of power headed by caciques. The Muisca inhabited mainly the area of what is now the Departments of Boyacá and Cundinamarca high plateau (\"Altiplano Cundiboyacense\") where they formed the Muisca Confederation. They farmed maize, potato, quinoa and cotton, and traded gold, emeralds, blankets, ceramic handicrafts, coca and especially rock salt with neighboring nations. The Tairona inhabited northern Colombia in the isolated mountain range of Sierra Nevada de Santa Marta. The Quimbaya inhabited regions of the Cauca River Valley between the Western and Central Ranges of the Colombian Andes. Most of the Amerindians practiced agriculture and the social structure of each indigenous community was different. Some groups of indigenous people such as the Caribs lived in a state of permanent war, but others had less bellicose attitudes. The Incas expanded their empire onto the southwest part of the country.\n\nAlonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499. Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean coast in 1500. Christopher Columbus navigated near the Caribbean in 1502. In 1508, Vasco Núñez de Balboa accompanied an expedition to the territory through the region of Gulf of Urabá and they founded the town of Santa María la Antigua del Darién in 1510, the first stable settlement on the continent. \n\nSanta Marta was founded in 1525, and Cartagena in 1533. Spanish conquistador Gonzalo Jiménez de Quesada led an expedition to the interior in April 1536, and christened the districts through which he passed \"New Kingdom of Granada\". In August 1538, he founded provisionally its capital near the Muisca cacicazgo of Bacatá, and named it \"Santa Fe\". The name soon acquired a suffix and was called Santa Fe de Bogotá. Two other notable journeys by early conquistadors to the interior took place in the same period. Sebastián de Belalcázar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popayán, in 1537; from 1536 to 1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the \"city of gold\". The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.\n\nThe conquistadors made frequent alliances with the enemies of different indigenous communities. Indigenous allies were crucial to conquest, as well as to creating and maintaining empire. Indigenous peoples in New Granada experienced a decline in population due to conquest as well as Eurasian diseases, such as smallpox, to which they had no immunity. With the risk that the land was deserted, the Spanish Crown sold properties to all persons interested in colonised territories creating large farms and possession of mines.\n\nIn the 16th century, the nautical science in Spain reached a great development thanks to numerous scientific figures of the Casa de Contratación and nautical science was an essential pillar of the Iberian expansion.\n\nIn 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital at Lima. In 1547, New Granada became the Captaincy-General of New Granada within the viceroyalty.\n\nIn 1549, the Royal Audiencia was created by a royal decree, and New Granada was ruled by the Royal Audience of Santa Fe de Bogotá, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popayán, Guayana and Cartagena. But important decisions were taken from the colony to Spain by the Council of the Indies.\nIn the 16th century, Europeans began to bring slaves from Africa. Spain was the only European power that could not establish factories in Africa to purchase slaves and therefore the Spanish empire relied on the asiento system, awarding merchants (mostly from Portugal, France, England and the Dutch Empire) the license to trade enslaved people to their overseas territories. Also there were people who defended the human rights and freedoms of oppressed peoples. The indigenous peoples could not be enslaved because they were legally subjects of the Spanish Crown and to protect the indigenous peoples, several forms of land ownership and regulation were established: \"resguardos\", \"encomiendas\" and \"haciendas\".\nIn 1717 the Viceroyalty of New Granada was originally created, and then it was temporarily removed, to finally be reestablished in 1739. The Viceroyalty had Santa Fé de Bogotá as its capital. This Viceroyalty included some other provinces of northwestern South America which had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador and Panama. So, Bogotá became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained somewhat backward compared to those two cities in several economic and logistical ways.\n\nAfter Great Britain declared war on Spain in 1739, Cartagena quickly became the British forces' top target but an upset Spanish victory during the War of Jenkins' Ear, a war with Great Britain for economic control of the Caribbean, cemented Spanish dominance in the Caribbean until the Seven Years' War.\n\nThe 18th-century priest, botanist and mathematician José Celestino Mutis was delegated by Viceroy Antonio Caballero y Góngora to conduct an inventory of the nature of the New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada which classified plants, wildlife and founded the first astronomical observatory in the city of Santa Fe de Bogotá. In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogotá where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco José de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.\n\nSince the beginning of the periods of conquest and colonization, there were several rebel movements against Spanish rule, but most were either crushed or remained too weak to change the overall situation. The last one that sought outright independence from Spain sprang up around 1810 and culminated in the Colombian Declaration of Independence, issued on 20 July 1810, a day which is now celebrated as the nation's Independence Day. This movement followed the independence of St. Domingue (present-day Haiti) in 1804, which provided some support to an eventual leader of this rebellion: Simón Bolívar. Francisco de Paula Santander also would play a decisive role.\n\nA movement was initiated by Antonio Nariño, who opposed Spanish centralism and led the opposition against the Viceroyalty. Cartagena became independent in November 1811. In 1811 the United Provinces of New Granada were proclaimed, headed by Camilo Torres Tenorio. The emergence of two distinct ideological currents among the patriots (federalism and centralism) gave rise to a period of instability. Shortly after the Napoleonic Wars ended, Ferdinand VII, recently restored to the throne in Spain, unexpectedly decided to send military forces to retake most of northern South America. The viceroyalty was restored under the command of Juan Sámano, whose regime punished those who participated in the patriotic movements, ignoring the political nuances of the juntas. The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Simón Bolívar, who finally proclaimed independence in 1819. The pro-Spanish resistance was defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela.\n\nThe territory of the Viceroyalty of New Granada became the Republic of Colombia, organized as a union of the current territories of Colombia, Panama, Ecuador, Venezuela, parts of Guyana and Brazil and north of Marañón River. The Congress of Cúcuta in 1821 adopted a constitution for the new Republic. Simón Bolívar became the first President of Colombia, and Francisco de Paula Santander was made Vice President. However, the new republic was unstable and three countries emerged from the collapse of Gran Colombia in 1830 (New Granada, Ecuador and Venezuela).\n\nColombia was the first constitutional government in South America, and the Liberal and Conservative parties, founded in 1848 and 1849 respectively, are two of the oldest surviving political parties in the Americas. Slavery was abolished in the country in 1851.\n\nInternal political and territorial divisions led to the dissolution of Gran Colombia in 1830. The so-called \"Department of Cundinamarca\" adopted the name \"New Granada\", which it kept until 1858 when it became the \"Confederación Granadina\" (Granadine Confederation). After a two-year civil war in 1863, the \"United States of Colombia\" was created, lasting until 1886, when the country finally became known as the Republic of Colombia. Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899–1902).\n\nThe United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of the Department of Panama in 1903 and the establishment of it as a nation. The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson–Urrutia Treaty. Colombia and Peru went to war because of territory disputes far in the Amazon basin. The war ended with a peace deal brokered by the League of Nations. The League finally awarded the disputed area to Colombia in June 1934.\n\nSoon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as \"La Violencia\" (\"The Violence\"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eliécer Gaitán on 9 April 1948. The ensuing riots in Bogotá, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.\n\nColombia entered the Korean War when Laureano Gómez was elected president. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the resistance of the Colombian troops at Old Baldy.\n\nThe violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'état and negotiated with the guerrillas, and then under the military junta of General Gabriel París.\n\nAfter Rojas' deposition, the Colombian Conservative Party and Colombian Liberal Party agreed to create the National Front, a coalition which would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices. The National Front ended \"La Violencia\", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress. Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, the ELN and the M-19 to fight the government and political apparatus.\n\nSince the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between government forces, leftist guerrilla groups and right wing paramilitaries. The conflict escalated in the 1990s, mainly in remote rural areas. Since the beginning of the armed conflict, human rights defenders have fought for the respect for human rights, despite staggering opposition. Several guerrillas' organizations decided to demobilize after peace negotiations in 1989–1994.\n\nThe United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism. Mercenaries and multinational corporations such as Chiquita Brands International are some of the international actors that have contributed to the violence of the conflict.\n\nOn 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.\n\nThe administration of President Álvaro Uribe (2002–10), adopted the democratic security policy which included an integrated counter-terrorism and counter-insurgency campaign. The Government economic plan also promoted confidence in investors. As part of a controversial peace process the AUC (right-wing paramilitaries) as a formal organization had ceased to function. In February 2008, millions of Colombians demonstrated against FARC and other outlawed groups.\n\nAfter peace negotiations in Cuba, the Colombian government of President Juan Manuel Santos and guerrilla of FARC-EP announced a final agreement to end the conflict. However, a referendum to ratify the deal was unsuccessful. Afterward, the Colombian government and the FARC signed a revised peace deal in November 2016, which the Colombian congress approved. In 2016, President Santos was awarded the Nobel Peace Prize. The Government began a process of attention and comprehensive reparation for victims of conflict. Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW. A Special Jurisdiction for Peace has been created to investigate, clarify, prosecute and punish serious human rights violations and grave breaches of international humanitarian law which occurred during the armed conflict and to satisfy victims' right to justice. During his visit to Colombia, Pope Francis paid tribute to the victims of the conflict.\n\nColombia's relations with Venezuela have fluctuated due to ideological differences between both governments. Colombia has offered humanitarian support with food and medicines to mitigate the shortage of supplies in Venezuela. Latin America rejects Trump's military threat against Venezuela because South America has a long history of resisting authoritarian regimes, which often were supported by the U.S. governments. After decades of struggle and the fall of the dictatorships, the US Government has lost its credibility in Latin America. Brazil, Colombia and other countries in the region prefer to play a constructive role that would prevent a civil war in Venezuela. Colombia's Foreign Ministry said that all efforts to resolve Venezuela's crisis should be peaceful. Colombia proposed the idea of the Sustainable Development Goals and a final document was adopted by the United Nations.\n\nThe geography of Colombia is characterized by its six main natural regions that present their own unique characteristics, from the Andes mountain range region shared with Ecuador and Venezuela; the Pacific coastal region shared with Panama and Ecuador; the Caribbean coastal region shared with Venezuela and Panama; the \"Llanos\" (plains) shared with Venezuela; the Amazon Rainforest region shared with Venezuela, Brazil, Peru and Ecuador; to the insular area, comprising islands in both the Atlantic and Pacific oceans.\n\nColombia is bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru; it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean. It lies between latitudes 12°N and 4°S, and longitudes 67° and 79°W.\n\nPart of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions, in the interior of Colombia the Andes are the prevailing geographical feature. Most of Colombia's population centers are located in these interior highlands. Beyond the Colombian Massif (in the south-western departments of Cauca and Nariño) these are divided into three branches known as \"cordilleras\" (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena River valleys (to the west and east respectively) and including the cities of Medellín, Manizales, Pereira and Armenia; and the Cordillera Oriental, extending north east to the Guajira Peninsula and including Bogotá, Bucaramanga and Cúcuta.\n\nPeaks in the Cordillera Occidental exceed , and in the Cordillera Central and Cordillera Oriental they reach . At , Bogotá is the highest city of its size in the world.\n\nEast of the Andes lies the savanna of the \"Llanos\", part of the Orinoco River basin, and, in the far south east, the jungle of the Amazon rainforest. Together these lowlands comprise over half Colombia's territory, but they contain less than 6% of the population. To the north the Caribbean coast, home to 21.9% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Cristóbal Colón and Pico Simón Bolívar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serranía de Baudó mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.\n\nThe main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquetá. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.\n\nProtected areas and the \"National Park System\" cover an area of about and account for 12.77% of the Colombian territory. Compared to neighboring countries, rates of deforestation in Colombia are still relatively low. Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.\n\nThe climate of Colombia is characterized for being tropical presenting variations within six natural regions and depending on the altitude, temperature, humidity, winds and rainfall. The diversity of climate zones in Colombia is characterized for having tropical rainforests, savannas, steppes, deserts and mountain climate.\n\nMountain climate is one of the unique features of the Andes and other high altitude reliefs where climate is determined by elevation. Below in elevation is the warm altitudinal zone, where temperatures are above . About 82.5% of the country's total area lies in the warm altitudinal zone. The temperate climate altitudinal zone located between is characterized for presenting an average temperature ranging between . The cold climate is present between and the temperatures vary between . Beyond the cold land lie the alpine conditions of the forested zone and then the treeless grasslands of the páramos. Above , where temperatures are below freezing, the climate is glacial, a zone of permanent snow and ice.\n\nColombia is one of the megadiverse countries in biodiversity, ranking first in bird species. As for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, which is even more remarkable given that Colombia is considered a country of intermediate size. Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.\n\nColombia is the country in the planet more characterized by a high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world's mammals species, 14% of the amphibian species and 18% of the bird species of the world.\n\nColombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. Colombia is the country with more endemic species of butterflies, number 1 in terms of orchid species and approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 1,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.\n\nThe government of Colombia takes place within the framework of a presidential participatory democratic republic as established in the Constitution of 1991. In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.\n\nAs the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve four-year term (In 2015, Colombia's Congress approved the repeal of a 2004 constitutional amendment that eliminated the one-term limit for presidents). At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as \"corregimientos\" or \"comunas\". All regional elections are held one year and five months after the presidential election.\n\nThe legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate. The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts. Members of both houses are elected to serve four-year terms two months before the president, also by popular vote.\n\nThe judicial branch is headed by four high courts, consisting of the Supreme Court which deals with penal and civil matters, the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch. Colombia operates a system of civil law, which since 2005 has been applied through an adversarial system.\n\nDespite a number of controversies, the democratic security policy has ensured that former President Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009. However, having served two terms, he was constitutionally barred from seeking re-election in 2010. In the run-off elections on 20 June 2010 the former Minister of defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes. Santos won nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival Óscar Iván Zuluaga, who won 45%. Iván Duque won in the second round with 54% of the vote, against 42% for his left-wing rival, Gustavo Petro. His term as Colombia's president runs for four years beginning 7 August 2018.\n\nThe foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs. Colombia has diplomatic missions in all continents.\n\nColombia was one of the 4 founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries. Colombia is also a member of the United Nations, the World Trade Organization, the Organisation for Economic Co-operation and Development, the Organization of American States, the Organization of Ibero-American States, the Union of South American Nations and the Andean Community of Nations. Colombia is a global partner of NATO.\n\nThe executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 455,461 active military personnel. And in 2016 3.4% of the country's GDP went towards military expenditure, placing it 24th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil.\n\nThe Colombian military is divided into three branches: the National Army of Colombia; the Colombian Air Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the National Intelligence Directorate (DNI, in Spanish).\n\nThe National Army is formed by divisions, brigades, special brigades and special units; the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation and the Specific Command of San Andres y Providencia; and the Air Force by 15 air units. The National Police has a presence in all municipalities.\n\nColombia is divided into 32 departments and one capital district, which is treated as a department (Bogotá also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into \"corregimientos\" in rural areas and into \"comunas\" in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the \"corregimientos\" or \"comunas\".\n\nIn addition to the capital four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaupés and Vichada), special administrative divisions are employed, such as \"department \"corregimientos\"\", which are a hybrid of a municipality and a \"corregimiento\".\n\nHistorically an agrarian economy, Colombia urbanised rapidly in the 20th century, by the end of which just 15.8% of the workforce were employed in agriculture, generating just 6.6% of GDP; 19.6% of the workforce were employed in industry and 64.6% in services, responsible for 33.4% and 59.9% of GDP respectively. The country's economic production is dominated by its strong domestic demand. Consumption expenditure by households is the largest component of GDP.\n\nColombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery from that recession was long and painful. However, in recent years growth has been impressive, reaching 6.9% in 2007, one of the highest rates of growth in Latin America. According to International Monetary Fund estimates, in 2012 Colombia's GDP (PPP) was US$500 billion (28th in the world and third in South America).\n\nTotal government expenditures account for 27.9 percent of the domestic economy. External debt equals 39.9 percent of gross domestic product. A strong fiscal climate was reaffirmed by a boost in bond ratings. Annual inflation closed 2017 at 4.09% YoY (vs. 5.75% YoY in 2016). The average national unemployment rate in 2017 was 9.4%, although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%). Colombia has Free trade Zone (FTZ), such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.\n\nThe financial sector has grown favorably due to good liquidity in the economy, the growth of credit and the positive performance of the Colombian economy. The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities. Colombia is now one of only three economies with a perfect score on the strength of legal rights index, according to the World Bank.\n\nThe electricity production in Colombia comes mainly from renewable energy sources. 69.93% is obtained from the hydroelectric generation. Colombia's commitment to renewable energy was recognized in the 2014 \"Global Green Economy Index (GGEI)\", ranking among the top 10 nations in the world in terms of greening efficiency sectors.\n\nColombia is rich in natural resources, and its main exports include mineral fuels, oils, distillation products, fruit and other agricultural products, sugars and sugar confectionery, food products, plastics, precious stones, metals, forest products, chemical goods, pharmaceuticals, vehicles, electronic products, electrical equipments, perfumery and cosmetics, machinery, manufactured articles, textile and fabrics, clothing and footwear, glass and glassware, furniture, prefabricated buildings, military products, home and office material, construction equipment, software, among others. Principal trading partners are the United States, China, the European Union and some Latin American countries.\n\nNon-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements.\n\nIn 2017, the National Administrative Department of Statistics (DANE) reported that 26.9% of the population were living below the poverty line, of which 7.4% in \"extreme poverty\". The multidimensional poverty rate stands at 17.0 percent of the population. The Government has also been developing a process of financial inclusion within the country's most vulnerable population.\n\nRecent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs, Colombians with a net worth exceeding US $1 billion.\n\nThe contribution of Travel & Tourism to GDP was USD5,880.3bn (2.0% of total GDP) in 2016. Tourism generated 556,135 jobs (2.5% of total employment) in 2016. Foreign tourist visits were predicted to have risen from 0.6 million in 2007 to 4 million in 2017.\n\nColombia has more than 3,950 research groups in science and technology. iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions like Apps.co provide. Co-working spaces have arisen to serve as communities for startups large and small. Organizations such as the Corporation for Biological Research (CIB) for the support of young people interested in scientific work has been successfully developed in Colombia. The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.\n\nImportant inventions related to the medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electronics engineer Jorge Reynolds Pombo, invention of great importance for those who suffer from heart failure. Also invented in Colombia were the microkeratome and keratomileusis technique, which form the fundamental basis of what now is known as LASIK (one of the most important techniques for the correction of refractive errors of vision) and the Hakim valve for the treatment of Hydrocephalus, among others. Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military hardware, military robots, bombs, simulators and radar.\n\nSome leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the \"Paisa Mutation\" or a type of early-onset Alzheimer's, Rodolfo Llinás known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and Ángela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by the \"Paracoccidioides brasiliensis\", among other scientists.\n\nTransportation in Colombia is regulated within the functions of the Ministry of Transport and entities such as the National Roads Institute (INVÍAS) responsible for the Highways in Colombia, the Aerocivil, responsible for civil aviation and airports, the National Infrastructure Agency, in charge of concessions through public–private partnerships, for the design, construction, maintenance, operation, and administration of the transport infrastructure, the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy, among others and under the supervision of the Superintendency of Ports and Transport. The road network in Colombia has a length of about 215,000 km of which 23,000 are paved. Rail transportation in Colombia is dedicated almost entirely to freight shipments and the railway network has a length of 1,700 km of potentially active rails. Colombia has 3,960 kilometers of gas pipelines, 4,900 kilometers of oil pipelines, and 2,990 kilometers of refined-products pipelines.\n\nThe target of Colombia's government is to build 7,000 km of roads for the 2016–2020 period and reduce travel times by 30 per cent and transport costs by 20 per cent. A toll road concession programme will comprise 40 projects, and is part of a larger strategic goal to invest nearly $50bn in transport infrastructure, including: railway systems; making the Magdalena river navigable again; improving port facilities; as well as an expansion of Bogotá's airport.\n\nWith an estimated 49 million people in 2017, Colombia is the third-most populous country in Latin America, after Brazil and Mexico. At the beginning of the 20th century, Colombia's population was approximately 4 million. Since the early 1970s Colombia has experienced steady declines in its fertility, mortality, and population growth rates. The population growth rate for 2016 is estimated to be 0.9%. The total fertility rate was 1.9 births per woman in 2015. About 26.8% of the population were 15 years old or younger, 65.7% were between 15 and 64 years old, and 7.4% were over 65 years old. The proportion of older persons in the total population has begun to increase substantially. Colombia is projected to have a population of 50.2 million by 2020 and 55.3 million by 2050.\n\nThe population is concentrated in the Andean highlands and along the Caribbean coast, also the population densities are generally higher in the Andean region. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 6% of the population. Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to nearly 60% in 1973, and by 2014 the figure stood at 76%. The population of Bogotá alone has increased from just over 300,000 in 1938 to approximately 8 million today. In total seventy-two cities now have populations of 100,000 or more (2015). Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9 million people.\n\nThe life expectancy is 74.8 years in 2015 and infant mortality is 13.1 per thousand in 2016. In 2015, 94.58% of adults and 98.66% of youth are literate and the government spends about 4.49% of its GDP in education.\n\nColombia is ranked third in the world in the Happy Planet Index.\n\nMore than 99.2% of Colombians speak Spanish, also called Castilian; 65 Amerindian languages, two Creole languages, the Romani language and Colombian Sign Language are also spoken in the country. English has official status in the archipelago of San Andrés, Providencia and Santa Catalina.\n\nIncluding Spanish, a total of 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider to be varieties or dialects of the same language. Best estimates recorded 71 languages that are spoken in-country today—most of which belong to the Chibchan, Tucanoan, Bora–Witoto, Guajiboan, Arawakan, Cariban, Barbacoan, and Saliban language families. There are currently about 850,000 speakers of native languages.\n\nColombia is ethnically diverse, its people descending from the original native inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage. The demographic distribution reflects a pattern that is influenced by colonial history. Whites tend to live mainly in urban centers, like Bogotá, Medellín or Cali, and the burgeoning highland cities. The populations of the major cities also include mestizos. Mestizo \"campesinos\" (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades.\n\nThe 2005 census reported that the \"non-ethnic population\", consisting of whites and mestizos (those of mixed white European and Amerindian ancestry), constituted 86% of the national population. 10.6% is of African ancestry. Indigenous Amerindians comprise 3.4% of the population. 0.01% of the population are Roma. An extraofficial estimate considers that the 49% of the Colombian population is Mestizo or of mixed European and Amerindian ancestry, and that approximately 37% is White, mainly of Spanish lineage, but there is also a large population of Middle East descent; in some sectors of society there is a considerable input of Italian and German ancestry.\n\nMany of the Indigenous peoples experienced a reduction in population during the Spanish rule and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves (\"resguardos\") established for indigenous peoples occupy (27% of the country's total) and are inhabited by more than 800,000 people. Some of the largest indigenous groups are the Wayuu, the Paez, the Pastos, the Emberá and the Zenú. The departments of La Guajira, Cauca, Nariño, Córdoba and Sucre have the largest indigenous populations.\n\nThe Organización Nacional Indígena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.\n\nBlack Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Caribbean and Pacific coasts. The population of the department of Chocó, running along the northern portion of Colombia's Pacific coast, is over 80% black. British and Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.\n\nMany immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Arabs. There are also important communities of Chinese, Japanese, Romanis and Jews. There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela.\n\nThe National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%-79%) are Roman Catholic, while a significant minority (16.7%) adhere to Protestantism (primarily Evangelicalism). Some 4.7% of the population is atheist or agnostic, while 3.5% claim to believe in God but do not follow a specific religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and less than 1% adhere to other religions, such as Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Orthodox Catholic Church, and spiritual studies. The remaining people either did not respond or replied that they did not know. In addition to the above statistics, 35.9% of Colombians reported that they did not practice their faith actively.\n\nWhile Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom of religion and all religious faiths and churches are equally free before the law.\n\nColombia is a highly urbanized country. The largest cities in the country are Bogotá, with an estimated 8 million inhabitants, Medellín, with an estimated 2.5 million inhabitants, Cali, with an estimated 2.4 million inhabitants, and Barranquilla, with an estimated 1.2 million inhabitants. Cartagena highlights in number of inhabitants and the city of Bucaramanga is relevant in terms of metropolitan area population.\nColombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.\n\nMany national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.\n\nColombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the \"Legend of Yurupary\". In Spanish colonial times, notable writers include Juan de Castellanos (\"Elegías de varones ilustres de Indias\"), Hernando Domínguez Camargo and his epic poem to San Ignacio de Loyola, Pedro Simón, Juan Rodríguez Freyle (\"El Carnero\"), Lucas Fernández de Piedrahita, and the nun Francisca Josefa de Castillo, representative of mysticism.\n\nPost-independence literature linked to Romanticism highlighted Antonio Nariño, José Fernández Madrid, Camilo Torres Tenorio and Francisco Antonio Zea. In the second half of the nineteenth century and early twentieth century the literary genre known as \"costumbrismo\" became popular; great writers of this period were Tomás Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature). Within that period, authors such as José Asunción Silva, José Eustasio Rivera, León de Greiff, Porfirio Barba-Jacob and José María Vargas Vila developed the modernist movement. In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas. Candelario Obeso wrote the groundbreaking \"Cantos Populares de mi Tierra\" (1877), the first book of poetry by an Afro-Colombian author.\n\nBetween 1939 and 1940 seven books of poetry were published under the name \"Stone and Sky\" in the city of Bogotá that significantly impacted the country; they were edited by the poet Jorge Rojas. In the following decade, Gonzalo Arango founded the movement of \"nothingness\" in response to the violence of the time; he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando González Ochoa. During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel García Márquez and his magnum opus, \"One Hundred Years of Solitude\", Eduardo Caballero Calderón, Manuel Mejía Vallejo, and Álvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters. Other leading contemporary authors are Fernando Vallejo, William Ospina (Rómulo Gallegos Prize) and Germán Castro Caycedo.\n\nColombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000 BCE.\n\nThe earliest examples of gold craftsmanship have been attributed to the Tumaco people of the Pacific coast and date to around 325 BCE. Roughly between 200 BCE and 800 CE, the San Agustín culture, masters of stonecutting, entered its \"classical period\". They erected raised ceremonial centres, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphhic forms out of stone.\n\nColombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular baroque style was replaced with rococo when the Bourbons ascended to the Spanish crown. More recently, Colombian artists Pedro Nel Gómez and Santiago Martínez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco.\n\nSince the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio Gómez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmatic \"Naturaleza muerta en silencio\" (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obregón is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the Andean condor). Fernando Botero, Omar Rayo, Enrique Grau, Édgar Negret, David Manzur, Rodrigo Arenas Betancourt, Oscar Murillo, Doris Salcedo and Oscar Muñoz are some of the Colombian artists featured at the international level.\n\nThe Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend. During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.\n\nColombian photography was marked by the arrival of the daguerreotype. Jean-Baptiste Louis Gros was who brought the daguerreotype process to Colombia in 1841. The Piloto public library has Latin America's largest archive of negatives, containing 1.7 million antique photographs covering Colombia 1848 until 2005.\n\nThe Colombian press has promoted the work of the cartoonists. In recent decades, fanzines, internet and independent publishers have been fundamental to the growth of the comic in Colombia.\n\nThroughout the times, there have been a variety of architectural styles, from those of indigenous peoples to contemporary ones, passing through colonial (military and religious), Republican, transition and modern styles.\n\nAncient habitation areas, longhouses, crop terraces, roads as the Inca road system, cemeteries, hypogeums and necropolises are all part of the architectural heritage of indigenous peoples. Some prominent indigenous structures are the preceramic and ceramic archaeological site of Tequendama, Tierradentro (a park that contains the largest concentration of pre-Columbian monumental shaft tombs with side chambers), the largest collection of religious monuments and megalithic sculptures in South America, located in San Agustín, Huila, Lost city (an archaeological site with a series of terraces carved into the mountainside, a net of tiled roads and several circular plazas) and also stand out the large villages mainly built with stone, wood, cane and mud.\n\nArchitecture during the period of conquest and colonization is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian and Extremaduran, can be easily seen. When Europeans founded cities two things were making simultaneously: the dimensioning of geometrical space (town square, street), and the location of a tangible point of orientation. The construction of forts was common throughout the Caribbean and in some cities of the interior, because of the dangers that represented the English, French, and Dutch pirates and the hostile indigenous groups. Churches, chapels, schools, and hospitals belonging to religious orders cause a great urban impact. Baroque architecture is used in military buildings and public spaces. Marcelino Arroyo, Francisco José de Caldas and Domingo de Petrés were great representatives of neo-classical architecture.\n\nThe National Capitol is a great representative of romanticism. Wood is extensively used in doors, windows, railings and ceilings during the colonization of Antioquia. The Caribbean architecture acquires a strong Arabic influence. The Teatro Colón in Bogotá is a lavish example of architecture from the 19th century. The quintas houses with innovations in the volumetric conception are some of the best examples of the Republican architecture; the Republican action in the city focused on the design of three types of spaces: parks with forests, small urban parks and avenues and the Gothic style was most commonly used for the design of churches.\n\nDeco style, modern neoclassicism, eclecticism folklorist and art deco ornamental resources significantly influenced the architecture of Colombia, especially during the transition period. Modernism contributed with new construction technologies and new materials (steel, reinforced concrete, glass and synthetic materials) and the topology architecture and lightened slabs system also have a great influence. The most influential architects of the modern movement were Rogelio Salmona and Fernando Martínez Sanabria.\n\nThe contemporary architecture of Colombia is designed to give greater importance to the materials, this architecture takes into account the specific natural and artificial geographies and is also an architecture that appeals to the senses. The conservation of the architectural and urban heritage of Colombia has been promoted in recent years.\n\nColombia has a vibrant collage of talent that touches a full spectrum of rhythms. Musicians, composers, music producers and singers from Colombia are recognized internationally such as Shakira, Juanes, Carlos Vives and others. Colombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombia has a diverse and dynamic musical environment.\n\nGuillermo Uribe Holguín, an important cultural figure in the National Symphony Orchestra of Colombia, Luis Antonio Calvo and Blas Emilio Atehortúa are some of the greatest exponents of the art music. The Bogotá Philharmonic Orchestra is one of the most active orchestras in Colombia.\n\nCaribbean music has many vibrant rhythms, such as cumbia (it is played by the maracas, the drums, the gaitas and guacharaca), porro (it is a monotonous but joyful rhythm), mapalé (with its fast rhythm and constant clapping) and the \"vallenato\", which originated in the northern part of the Caribbean coast (the rhythm is mainly played by the caja, the guacharaca, and accordion).\n\nThe music from the Pacific coast, such as the currulao is characterized by its strong use of drums (instruments such as the native marimba, the conunos, the bass drum, the side drum and the cuatro guasas or tubular rattle). An important rhythm of the south region of the Pacific coast is the contradanza (it is used in dance shows, as a result of the striking colours of the costumes). Marimba music, traditional chants and dances from the Colombia South Pacific region are on UNESCO's Representative List of the Intangible Cultural Heritage of Humanity.\n\nImportant musical rhythms of the Andean Region are the danza (dance of Andean folklore arising from the transformation of the European contredance), the bambuco (it is played with guitar, tiple and mandolin, the rhythm is danced by couples), the pasillo (a rhythm inspired by the Austrian waltz and the Colombian \"danza\", the lyrics have been composed by well-known poets), the guabina (the tiple, the bandola and the requinto are the basic instruments), the sanjuanero (it originated in Tolima and Huila Departments, the rhythm is joyful and fast). Apart from these traditional rhythms, salsa music has spread throughout the country, and the city of Cali is considered by many salsa singers to be 'The New Salsa Capital of the World'.\n\nThe instruments that distinguish the music of the Eastern Plains are the harp, the cuatro (a type of four-stringed guitar) and maracas. Important rhythms of this region are the joropo (a fast rhythm and there is also tapping as a result of its flamenco ancestry) and the galeron (it is heard a lot while cowboys are working).\n\nThe music of the Amazon region is strongly influenced by the indigenous religious practices. Some of the musical instruments used are the manguaré (a musical instrument of ceremonial type, consisting of a pair of large cylindrical drums), the quena (melodic instrument), the rondador, the congas, bells, and different types of flutes.\n\nThe music of the Archipelago of San Andrés, Providencia and Santa Catalina is usually accompanied by a mandolin, a tub-bass, a jawbone, a guitar and maracas. Some popular archipelago rhythms are the Schottische, the Calypso, the Polka and the Mento.\n\nTheater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogotá is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world. Other important theater events are: The Festival of Puppet The Fanfare (Medellín), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture \"Cultural Invasion\" (Bogotá).\n\nAlthough the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003. Many film festivals take place in Colombia, but the two most important are the Cartagena Film Festival, which is the oldest film festival in Latin America, and the Bogotá Film Festival.\n\nSome important national circulation newspapers are \"El Tiempo\" and \"El Espectador\". Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.\n\nColombia has three major national radio networks: Radiodifusora Nacional de Colombia, a state-run national radio; Caracol Radio and RCN Radio, privately owned networks with hundreds of affiliates. There are other national networks, including Cadena Super, Todelar, and Colmundo. Many hundreds of radio stations are registered with the Ministry of Information Technologies and Communications.\n\nColombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood. Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, arazá, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit. Colombia is one of the world's largest consumers of fruit juices.\n\nAmong the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almojábanas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where kibbeh, suero, costeño cheese and carimañolas are also eaten. Representative side dishes are papas chorreadas (potatoes with cheese), remolachas rellenas con huevo duro (beets stuffed with hard-boiled egg) and arroz con coco (coconut rice). Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.\n\nRepresentative desserts are buñuelos, natillas, Maria Luisa cake, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de mango, roscón, milhoja, manjar blanco, dulce de feijoa, dulce de papayuela, torta de mojicón, and esponjado de curuba. Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style ají.\n\nSome representative beverages are coffee (Tinto), champús, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with water or milk).\n\nTejo is Colombia's national sport and is a team sport that involves launching projectiles to hit a target. But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa América, in which they set a new record of being undefeated, conceding no goals and winning each match. Colombia has been awarded \"mover of the year\" twice.\n\nColombia is a hub for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships. Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.\n\nBaseball is popular in the Caribbean, mainly in the cities Cartagena, Barranquilla and Santa Marta. Of those cities have come good players like: Orlando Cabrera, Édgar Rentería who was champion of the World Series in 1997 and 2010, and others who have played in Major League Baseball. Colombia was world amateur champion in 1947 and 1965.\n\nBoxing is one of the sports that more world champions has produced for Colombia.\nMotorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as BMX, judo, shooting sport, taekwondo, wrestling, high diving and athletics, also has a long tradition in weightlifting and bowling.\n\nThe overall life expectancy in Colombia at birth is 74.8 years (71.2 years for males and 78.4 years for females). Health standards in Colombia have improved very much since the 1980s, healthcare reforms have led to the massive improvements in the healthcare systems of the country. Although this new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012, health disparities persist.\n\nThrough health tourism, many people from over the world travel from their places of residence to other countries in search of medical treatment and the attractions in the countries visited. Colombia is projected as one of Latin America's main destinations in terms of health tourism due to the quality of its health care professionals, a good number of institutions devoted to health, and an immense inventory of natural and architectural sites. Cities such as Bogotá, Cali, Medellín and Bucaramanga are the most visited in cardiology procedures, neurology, dental treatments, stem cell therapy, ENT, ophthalmology and joint replacements because of the quality of medical treatment.\n\nA study conducted by \"América Economía\" magazine ranked 21 Colombian health care institutions among the top 44 in Latin America, amounting to 48 percent of the total. A cancer research and treatment centre was declared as a Project of National Strategic Interest.\n\nThe educational experience of many Colombian children begins with attendance at a preschool academy until age five (\"Educación preescolar\"). Basic education (\"Educación básica\") is compulsory by law. It has two stages: Primary basic education (\"Educación básica primaria\") which goes from first to fifth grade – children from six to ten years old, and Secondary basic education (\"Educación básica secundaria\"), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education (\"Educación media vocacional\") that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.\n\nAfter the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a \"bachiller\", because secondary basic school and middle education are traditionally considered together as a unit called \"bachillerato\" (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) in order to gain access to higher education (\"Educación superior\"). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.\n\n\"Bachilleres\" (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.\n\nPublic spending on education as a proportion of gross domestic product in 2015 was 4.49%. This represented 15.05% of total government expenditure. The primary and secondary gross enrolment ratios stood at 113.56% and 98.09% respectively. School-life expectancy was 14.42 years. A total of 94.58% of the population aged 15 and older were recorded as literate, including 98.66% of those aged 15–24.\n\n\nGeneral information\n\nGovernment\n\nCulture\n\nGeography\n"}
{"id": "2664708", "url": "https://en.wikipedia.org/wiki?curid=2664708", "title": "Delaware Bank", "text": "Delaware Bank\n\nThe Delaware Bank is an area of shallow water 20 miles off Galera Point on the east coast of Trinidad to the south of Tobago. The seafloor comes to within six fathoms (11 meters) of the surface, making the bank a hazard for shipping.\n"}
{"id": "2518907", "url": "https://en.wikipedia.org/wiki?curid=2518907", "title": "Drainage divide", "text": "Drainage divide\n\nA drainage divide, water divide, divide, ridgeline, watershed, or water parting is the line that separates neighbouring drainage basins. On rugged land, the divide lies along topographical ridges, and may be in the form of a single range of hills or mountains, known as a dividing range. On flat terrain, especially where the ground is marshy, the divide may be harder to discern.\n\nA valley floor divide is a low drainage divide that runs across a valley, sometimes created by deposition or stream capture.\n\nSince ridgelines are relatively easy to see and agree about, drainage divides are often natural borders defining political boundaries, as with the 18th century North America Royal Proclamation of 1763 that preceded the American Revolution.\n\nDrainage divides can be divided into three types:\n\nA valley-floor divide occurs on the bottom of a valley and arises as a result of subsequent depositions, such as scree, in a valley through which a river originally flowed continuously.\n\nExamples include the Kartitsch Saddle in the Gail valley in East Tyrol, which forms the watershed between the Drau and the Gail, and the divides in the \"Toblacher Feld\" between Innichen and Toblach in Italy, where the Drau empties into the Black Sea and the Rienz into the Adriatic.\n\nSettlements are often built on valley-floor divides in the Alps. Examples are Eben im Pongau, Kirchberg in Tirol and Waidring (In all of these, the village name indicates the pass and the watershed is even explicitly displayed in the coat of arms). Extremely low divides with heights of less than two metres are found on the North German Plain within the \"Urstromtäler\", for example, between Havel and Finow in the \"Eberswalde Urstromtal\". In marsh deltas such as the Okavango, the largest drainage area on earth, or in large lakes areas, such as the Finnish Lakeland, it is difficult to find a meaningful definition of a watershed. Another case is bifurcation, where the watershed is effectively in the river bed, a wetland or underground. The largest watershed of this type is the bifurcation of the Orinoco in the north of South America, whose main stream empties into the Caribbean, but which also drains into the South Atlantic via the Casiquiare canal and Amazon River.\n\nDrainage divides hinder river navigation. In pre-industrial times, water divides were crossed at portages. Later, canals connected adjoining drainage basins; a key problem in such canals is ensuring a sufficient water supply. Important examples are the Chicago Portage, connecting the Great Lakes and Mississippi by the Chicago Sanitary and Ship Canal, and the Canal des Deux Mers in France, connecting the Atlantic and the Mediterranean.\n\n"}
{"id": "50702", "url": "https://en.wikipedia.org/wiki?curid=50702", "title": "Environmental engineering", "text": "Environmental engineering\n\nEnvironmental engineering is the branch of engineering concerned with the application of scientific and engineering principles for protection of human populations from the effects of adverse environmental factors; protection of environments, both local and global, from potentially deleterious effects of natural and human activities; and improvement of environmental quality.\n\nEnvironmental engineering system can also be described as a branch of applied science and technology that addresses the issues of energy preservation, protection of assets and control of waste from human and animal activities. Furthermore, it is concerned with finding plausible solutions in the field of public health, such as waterborne diseases, implementing laws which promote adequate sanitation in urban, rural and recreational areas. It involves waste water management, air pollution control, recycling, waste disposal, radiation protection, industrial hygiene, animal agriculture, environmental sustainability, public health and environmental engineering law. It also includes studies on the environmental impact of proposed construction projects.\n\nEnvironmental engineers system study the effect of technological advances on the environment. To do so, they conduct studies on hazardous-waste management to evaluate the significance of such hazards, advise on treatment and containment, and develop regulations to prevent mishaps. Environmental engineers design municipal water supply and industrial wastewater treatment systems. They address local and worldwide environmental issues such as the effects of acid rain, global warming, ozone depletion, water pollution and air pollution from automobile exhausts and industrial sources.\n\nMany universities offer environmental engineering programs at either the department of civil engineering or the department of chemical engineering at engineering faculties. Environmental \"civil\" engineers focus on hydrology, water resources management, bioremediation, and water treatment plant design. Environmental \"chemical\" engineers, on the other hand, focus on environmental chemistry, advanced air and water treatment technologies and separation processes. Some subdivision of environmental engineering include natural resources engineering and agricultural engineering.\n\nMore engineers are obtaining specialized training in law (J.D.) and are utilizing their technical expertise in the practices of environmental engineering law.\n\nMost jurisdictions also impose licensing and registration requirements.\n\nEver since people first recognized that their health is related to the quality of their environment, they have applied principles to attempt to improve the quality of their environment. The ancient Indian Harappan civilization utilized early sewers in some cities more than 5000 years ago. More specifically, the Indus Valley Civilization (also called the Harappan civilization) had advanced control over the water in their society. The public work structures found at various sites in the area include wells, public baths, storage tanks, a drinking water system, and a city-wide sewage collection system. They also had an early version of a canal irrigation system that was needed for their large scale agriculture. The Romans constructed aqueducts to prevent drought and to create a clean, healthful water supply for the metropolis of Rome. In the 15th century, Bavaria created laws restricting the development and degradation of alpine country that constituted the region's water supply.\n\nThe field emerged as a separate environmental discipline during the middle third of the 20th century in response to widespread public concern about water and pollution and increasingly extensive environmental quality degradation. However, its roots extend back to early efforts in public health engineering. Modern environmental engineering began in London in the mid-19th century when Joseph Bazalgette designed the first major sewerage system that reduced the incidence of waterborne diseases such as cholera. The introduction of drinking water treatment and sewage treatment in industrialized countries reduced waterborne diseases from leading causes of death to rarities.\n\nIn many cases, as societies grew, actions that were intended to achieve benefits for those societies had longer-term impacts which reduced other environmental qualities. One example is the widespread application of the pesticide DDT to control agricultural pests in the years following World War II. While the agricultural benefits were outstanding and crop yields increased dramatically thus reducing world hunger substantially, and malaria was controlled better than it ever had been, numerous species were brought to the verge of extinction due to the impact of the DDT on their reproductive cycles. The story of DDT as vividly told in Rachel Carson's \"Silent Spring\" (1962) is considered to be the birth of the modern environmental movement and of the modern field of \"environmental engineering.\"\n\nConservation movements and laws restricting public actions that would harm the environment have been developed by various societies for millennia. Notable examples are the laws decreeing the construction of sewers in London and Paris in the 19th century and the creation of the U.S. national park system in the early 20th century.\n\nThe following topics typically make up a curriculum in environmental engineering:\nScientists have air pollution dispersion models to evaluate the concentration of a pollutant at a receptor or the impact on overall air quality from vehicle exhausts and industrial flue gas stack emissions. To some extent, this field overlaps the desire to decrease carbon dioxide and other greenhouse gas emissions from combustion processes.\nThey apply scientific and engineering principles to evaluate if there are likely to be any adverse impacts to water quality, air quality, habitat quality, flora and fauna, agricultural capacity, traffic impacts, social impacts, ecological impacts, noise impacts, visual (landscape) impacts, etc. If impacts are expected, they then develop mitigation measures to limit or prevent such impacts. An example of a mitigation measure would be the creation of wetlands in a nearby location to mitigate the filling in of wetlands necessary for a road development if it is not possible to reroute the road.\n\nIn the United States, the practice of environmental assessment was formally initiated on January 1, 1970, the effective date of the National Environmental Policy Act (NEPA). Since that time, more than 100 developing and developed nations either have planned specific analogous laws or have adopted procedure used elsewhere. NEPA is applicable to all federal agencies in the United States.\n\nEngineers evaluate the water balance within a watershed and determine the available water supply, the water needed for various needs in that watershed, the seasonal cycles of water movement through the watershed and they develop systems to store, treat, and convey water for various uses. Water is treated to achieve water quality objectives for the end uses. In the case of a potable water supply, water is treated to minimize the risk of infectious disease transmission, the risk of non-infectious illness, and to create a palatable water flavor. Water distribution systems are designed and built to provide adequate water pressure and flow rates to meet various end-user needs such as domestic use, fire suppression, and irrigation.\n\nThere are numerous wastewater treatment technologies. A wastewater treatment train can consist of a primary clarifier system to remove solid and floating materials, a secondary treatment system consisting of an aeration basin followed by flocculation and sedimentation or an activated sludge system and a secondary clarifier, a tertiary biological nitrogen removal system, and a final disinfection process. The aeration basin/activated sludge system removes organic material by growing bacteria (activated sludge). The secondary clarifier removes the activated sludge from the water. The tertiary system, although not always included due to costs, is becoming more prevalent to remove nitrogen and phosphorus and to disinfect the water before discharge to a surface water stream or ocean outfall.\n\nScientists have developed air pollution dispersion models to evaluate the concentration of a pollutant at a receptor or the impact on overall air quality from vehicle exhausts and industrial flue gas stack emissions. To some extent, this field overlaps the desire to decrease carbon dioxide and other greenhouse gas emissions from combustion processes.\n\nThe U.S. Environmental Protection Agency (EPA) is one of the many agencies that work with environmental engineers to solve key issues. An important component of EPA's mission is to protect and improve air, water, and overall environmental quality in order to avoid or mitigate the consequences of harmful effects.\n\nEcological engineering offers new alternatives for the management of agricultural systems that are more tailored to the ever-changing social and environmental necessities in these regions. This requires managing the complexity of agrosystems, while striving to mimic the functioning of natural ecosystems of West African drylands and taking advantage of traditional practices and local know-how resulting from a long process of adaptation to environmental constraints.\n\nCourses aimed at developing graduates with specific skills in environmental systems or environmental technology are becoming more common and fall into broad classes:\n\n\n\n"}
{"id": "66997", "url": "https://en.wikipedia.org/wiki?curid=66997", "title": "Epidemiology", "text": "Epidemiology\n\nEpidemiology is the study and analysis of the distribution (who, when, and where) and determinants of health and disease conditions in defined populations.\n\nIt is the cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.\n\nMajor areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.\n\n\"Epidemiology\", literally meaning \"the study of what is upon the people\", is derived , suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term \"epizoology\" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).\n\nThe distinction between \"epidemic\" and \"endemic\" was first drawn by Hippocrates, to distinguish between diseases that are \"visited upon\" a population (epidemic) from those that \"reside within\" a population (endemic). The term \"epidemiology\" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in \"Epidemiología Española\". Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.\n\nThe term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of everyone.\n\nThe Greek physician Hippocrates, known as the father of medicine, sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences. Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (air, fire, water and earth “atoms”). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine. He coined the terms \"endemic\" (for diseases usually found in some places but not in others) and \"epidemic\" (for diseases that are seen at some times but not others).\n\nIn the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that these very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen's miasma theory (poison gas in sick people). In 1543 he wrote a book \"De contagione et contagiosis morbis\", in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Antonie van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease.\n\nWu Youke (1582–1652) developed the concept that some diseases were caused by transmissible agents, which he called liqi (pestilential factors). His book Wenyi Lun (Treatise on Acute Epidemic Febrile Diseases) can be regarded as the main etiological work that brought forward the concept, ultimately attributed to Westerners, of germs as a cause of epidemic diseases (source: http://baike.baidu.com/view/143117.htm). His concepts are still considered in current scientific research in relation to Traditional Chinese Medicine studies (see: http://apps.who.int/medicinedocs/en/d/Js6170e/4.html).\n\nAnother pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated.\nJohn Graunt, a haberdasher and amateur statistician, published \"Natural and Political Observations ... upon the Bills of Mortality\" in 1662. In it, he analysed the mortality rolls in London before the Great Plague, presented one of the first life tables, and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.\n\nJohn Snow is famous for his investigations into the causes of the 19th century cholera epidemics, and is also known as the father of (modern) epidemiology. He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world. However, Snow's research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death.\n\nOther pioneers include Danish physician Peter Anton Schleisner, who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland. Another important pioneer was Hungarian physician Ignaz Semmelweis, who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill-received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister 'discovered' antiseptics in 1865 in light of the work of Louis Pasteur.\n\nIn the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross, Janet Lane-Claypon, Anderson Gray McKendrick, and others.\n\nAnother breakthrough was the 1954 publication of the results of a British Doctors Study, led by Richard Doll and Austin Bradford Hill, which lent very strong statistical support to the link between tobacco smoking and lung cancer.\n\nIn the late 20th century, with advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level, and disease was broadly named “molecular epidemiology”. Specifically, \"genetic epidemiology\" has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes. Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions.\n\nWhile most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease progression represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual (“the unique disease principle”), considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs. Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine, “molecular pathology” and “epidemiology” was integrated to create a new interdisciplinary field of “molecular pathological epidemiology” (MPE), defined as “epidemiology of molecular pathology and heterogeneity of disease”. In MPE, investigators analyze the relationships between; (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases. The concept and paradigm of MPE have become widespread in the 2010s.\n\nBy 2012 it was recognized that many pathogens' evolution is rapid enough to be highly relevant to epidemiology, and that therefore much could be gained from an interdisciplinary approach to infectious disease integrating epidemiology and molecular evolution to \"inform control strategies, or even patient treatment.\"\n\nEpidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive, analytic (aiming to further examine known associations or hypothesized relationships), and experimental (a term often equated with clinical or community trials of treatments and other interventions). In observational studies, nature is allowed to “take its course,\" as epidemiologists observe from the sidelines. Conversely, in experimental studies, the epidemiologist is the one in control of all of the factors entering a certain case study. Epidemiological studies are aimed, where possible, at revealing unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity. The identification of causal relationships between these exposures and outcomes is an important aspect of epidemiology. Modern epidemiologists use informatics as a tool.\n\nObservational studies have two components, descriptive and analytical. Descriptive observations pertain to the “who, what, where and when of health-related state occurrence”. However, analytical observations deal more with the ‘how’ of a health-related event. Experimental epidemiology contains three case types: randomized controlled trials (often used for new medicine or drug testing), field trials (conducted on those at a high risk of contracting a disease), and community trials (research on social originating diseases).\n\nThe term 'epidemiologic triad' is used to describe the intersection of \"Host\", \"Agent\", and \"Environment\" in analyzing an outbreak.\n\nCase-series may refer to the qualitative study of the experience of a single patient, or small group of patients with a similar diagnosis, or to a statistical factor with the potential to produce illness with periods when they are unexposed.\n\nThe former type of study is purely descriptive and cannot be used to make inferences about the general population of patients with that disease. These types of studies, in which an astute clinician identifies an unusual feature of a disease or a patient's history, may lead to a formulation of a new hypothesis. Using the data from the series, analytic studies could be done to investigate possible causal factors. These can include case-control studies or prospective studies. A case-control study would involve matching comparable controls without the disease to the cases in the series. A prospective study would involve following the case series over time to evaluate the disease's natural history.\n\nThe latter type, more formally described as self-controlled case-series studies, divide individual patient follow-up time into exposed and unexposed periods and use fixed-effects Poisson regression processes to compare the incidence rate of a given outcome between exposed and unexposed periods. This technique has been extensively used in the study of adverse reactions to vaccination and has been shown in some circumstances to provide statistical power comparable to that available in cohort studies.\n\nCase-control studies select subjects based on their disease status. It is a retrospective study. A group of individuals that are disease positive (the \"case\" group) is compared with a group of disease negative individuals (the \"control\" group). The control group should ideally come from the same population that gave rise to the cases. The case-control study looks back through time at potential exposures that both groups (cases and controls) may have encountered. A 2×2 table is constructed, displaying exposed cases (A), exposed controls (B), unexposed cases (C) and unexposed controls (D). The statistic generated to measure association is the odds ratio (OR), which is the ratio of the odds of exposure in the cases (A/C) to the odds of exposure in the controls (B/D), i.e. OR = (AD/BC).\n\nIf the OR is significantly greater than 1, then the conclusion is \"those with the disease are more likely to have been exposed,\" whereas if it is close to 1 then the exposure and disease are not likely associated. If the OR is far less than one, then this suggests that the exposure is a protective factor in the causation of the disease.\nCase-control studies are usually faster and more cost effective than cohort studies, but are sensitive to bias (such as recall bias and selection bias). The main challenge is to identify the appropriate control group; the distribution of exposure among the control group should be representative of the distribution in the population that gave rise to the cases. This can be achieved by drawing a random sample from the original population at risk. This has as a consequence that the control group can contain people with the disease under study when the disease has a high attack rate in a population.\n\nA major drawback for case control studies is that, in order to be considered to be statistically significant, the minimum number of cases required at the 95% confidence interval is related to the odds ratio by the equation:\n\nwhere N is the ratio of cases to controls.\nAs the odds ratio approached 1, approaches 0; rendering case control studies all but useless for low odds ratios. For instance, for an odds ratio of 1.5 and cases = controls, the table shown above would look like this:\n\nFor an odds ratio of 1.1:\n\nCohort studies select subjects based on their exposure status. The study subjects should be at risk of the outcome under investigation at the beginning of the cohort study; this usually means that they should be disease free when the cohort study starts. The cohort is followed through time to assess their later outcome status. An example of a cohort study would be the investigation of a cohort of smokers and non-smokers over time to estimate the incidence of lung cancer. The same 2×2 table is constructed as with the case control study. However, the point estimate generated is the relative risk (RR), which is the probability of disease for a person in the exposed group, \"P\" = \"A\" / (\"A\" + \"B\") over the probability of disease for a person in the unexposed group, \"P\" = \"C\" / (\"C\" + \"D\"), i.e. \"RR\" = \"P\" / \"P\".\n\nAs with the OR, a RR greater than 1 shows association, where the conclusion can be read \"those with the exposure were more likely to develop disease.\"\n\nProspective studies have many benefits over case control studies. The RR is a more powerful effect measure than the OR, as the OR is just an estimation of the RR, since true incidence cannot be calculated in a case control study where subjects are selected based on disease status. Temporality can be established in a prospective study, and confounders are more easily controlled for. However, they are more costly, and there is a greater chance of losing subjects to follow-up based on the long time period over which the cohort is followed.\n\nCohort studies also are limited by the same equation for number of cases as for cohort studies, but, if the base incidence rate in the study population is very low, the number of cases required is reduced by ½.\n\nAlthough epidemiology is sometimes viewed as a collection of statistical tools used to elucidate the associations of exposures to health outcomes, a deeper understanding of this science is that of discovering \"causal\" relationships.\n\n\"Correlation does not imply causation\" is a common theme for much of the epidemiological literature. For epidemiologists, the key is in the term inference. Correlation, or at least association between two variables, is a necessary but not sufficient criteria for inference that one variable causes the other. Epidemiologists use gathered data and a broad range of biomedical and psychosocial theories in an iterative way to generate or expand theory, to test hypotheses, and to make educated, informed assertions about which relationships are causal, and about exactly how they are causal.\n\nEpidemiologists emphasize that the \"one cause – one effect\" understanding is a simplistic mis-belief. Most outcomes, whether disease or death, are caused by a chain or web consisting of many component causes. Causes can be distinguished as necessary, sufficient or probabilistic conditions. If a necessary condition can be identified and controlled (e.g., antibodies to a disease agent, energy in an injury), the harmful outcome can be avoided (Robertson, 2015).\n\nIn 1965, Austin Bradford Hill proposed a series of considerations to help assess evidence of causation, which have come to be commonly known as the \"Bradford Hill criteria\". In contrast to the explicit intentions of their author, Hill's considerations are now sometimes taught as a checklist to be implemented for assessing causality. Hill himself said \"None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required \"sine qua non\".\"\n\n\nEpidemiological studies can only go to prove that an agent could have caused, but not that it did cause, an effect in any particular case:\n\n\"Epidemiology is concerned with the incidence of disease in populations and does not address the question of the cause of an individual's disease. This question, sometimes referred to as specific causation, is beyond the domain of the science of epidemiology. Epidemiology has its limits at the point where an inference is made that the relationship between an agent and a disease is causal (general causation) and where the magnitude of excess risk attributed to the agent has been determined; that is, epidemiology addresses whether an agent can cause a disease, not whether an agent did cause a specific plaintiff's disease.\"\n\nIn United States law, epidemiology alone cannot prove that a causal association does not exist in general. Conversely, it can be (and is in some circumstances) taken by US courts, in an individual case, to justify an inference that a causal association does exist, based upon a balance of probability.\n\nThe subdiscipline of forensic epidemiology is directed at the investigation of specific causation of disease or injury in individuals or groups of individuals in instances in which causation is disputed or is unclear, for presentation in legal settings.\n\nEpidemiological practice and the results of epidemiological analysis make a significant contribution to emerging population-based health management frameworks.\n\nPopulation-based health management encompasses the ability to:\n\n\nModern population-based health management is complex, requiring a multiple set of skills (medical, political, technological, mathematical etc.) of which epidemiological practice and analysis is a core component, that is unified with management science to provide efficient and effective health care and health guidance to a population. This task requires the forward looking ability of modern risk management approaches that transform health risk factors, incidence, prevalence and mortality statistics (derived from epidemiological analysis) into management metrics that not only guide how a health system responds to current population health issues, but also how a health system can be managed to better respond to future potential population health issues.\n\nExamples of organizations that use population-based health management that leverage the work and results of epidemiological practice include Canadian Strategy for Cancer Control, Health Canada Tobacco Control Programs, Rick Hansen Foundation, Canadian Tobacco Control Research Initiative.\n\nEach of these organizations use a population-based health management framework called Life at Risk that combines epidemiological quantitative analysis with demographics, health agency operational research and economics to perform:\n\n\nApplied epidemiology is the practice of using epidemiological methods to protect or improve the health of a population. Applied field epidemiology can include investigating communicable and non-communicable disease outbreaks, mortality and morbidity rates, and nutritional status, among other indicators of health, with the purpose of communicating the results to those who can implement appropriate policies or disease control measures.\n\nAs the surveillance and reporting of diseases and other health factors becomes increasingly difficult in humanitarian crisis situations, the methodologies used to report the data are compromised. One study found that less than half (42.4%) of nutrition surveys sampled from humanitarian contexts correctly calculated the prevalence of malnutrition and only one-third (35.3%) of the surveys met the criteria for quality. Among the mortality surveys, only 3.2% met the criteria for quality. As nutritional status and mortality rates help indicate the severity of a crisis, the tracking and reporting of these health factors is crucial.\n\nVital registries are usually the most effective ways to collect data, but in humanitarian contexts these registries can be non-existent, unreliable, or inaccessible. As such, mortality is often inaccurately measured using either prospective demographic surveillance or retrospective mortality surveys. Prospective demographic surveillance requires lots of manpower and is difficult to implement in a spread-out population. Retrospective morality surveys are prone to selection and reporting biases. Other methods are being developed, but are not common practice yet.\n\nDifferent fields in epidemiology have different levels of validity. One way to assess the validity of findings is the ratio of false-positives (claimed effects that are not correct) to false-negatives (studies which fail to support a true effect). To take the field of genetic epidemiology, candidate-gene studies produced over 100 false-positive findings for each false-negative. By contrast genome-wide association appear close to the reverse, with only one false positive for every 100 or more false-negatives. This ratio has improved over time in genetic epidemiology as the field has adopted stringent criteria. By contrast other epidemiological fields have not required such rigorous reporting and are much less reliable as a result.\n\nRandom error is the result of fluctuations around a true value because of sampling variability. Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.\n\nThere is random error in all sampling procedures. This is called sampling error.\n\nPrecision in epidemiological variables is a measure of random error. Precision is also inversely related to random error, so that to reduce random error is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.\n\nThere are two basic ways to reduce random error in an epidemiological study. The first is to increase the sample size of the study. In other words, add more subjects to your study. The second is to reduce the variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.\n\nNote, that if sample size or number of measurements are increased, or a more precise measuring tool is purchased, the costs of the study are usually increased. There is usually an uneasy balance between the need for adequate precision and the practical issue of study cost.\n\nA systematic error or bias occurs when there is a difference between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of systematic error is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).\n\nA mistake in coding that affects \"all\" responses for that particular question is another example of a systematic error.\n\nThe validity of a study is dependent on the degree of systematic error. Validity is usually separated into two components:\n\n\nSelection bias occurs when study subjects are selected or become part of the study as a result of a third, unmeasured variable which is associated with both the exposure and outcome of interest. For instance, it has repeatedly been noted that cigarette smokers and non smokers tend to differ in their study participation rates. (Sackett D cites the example of Seltzer et al., in which 85% of non smokers and 67% of smokers returned mailed questionnaires.) It is important to note that such a difference in response will not lead to bias if it is not also associated with a systematic difference in outcome between the two response groups.\n\nInformation bias is bias arising from systematic error in the assessment of a variable. An example of this is recall bias. A typical example is again provided by Sackett in his discussion of a study examining the effect of specific exposures on fetal health: \"in questioning mothers whose recent pregnancies had ended in fetal death or malformation (cases) and a matched group of mothers whose pregnancies ended normally (controls) it was found that 28% of the former, but only 20% of the latter, reported exposure to drugs which could not be substantiated either in earlier prospective\ninterviews or in other health records\". In this example, recall bias probably occurred as a result of women who had had miscarriages having an apparent tendency to better recall and therefore report previous exposures.\n\nConfounding has traditionally been defined as bias arising from the co-occurrence or mixing of effects of extraneous factors, referred to as confounders, with the main effect(s) of interest. A more recent definition of confounding invokes the notion of \"counterfactual\" effects. According to this view, when one observes an outcome of interest, say Y=1 (as opposed to Y=0), in a given population A which is entirely exposed (i.e. exposure \"X\" = 1 for every unit of the population) the risk of this event will be \"R\". The counterfactual or unobserved risk \"R\" corresponds to the risk which would have been observed if these same individuals had been unexposed (i.e. \"X\" = 0 for every unit of the population). The true effect of exposure therefore is: \"R\" − \"R\" (if one is interested in risk differences) or \"R\"/\"R\" (if one is interested in relative risk). Since the counterfactual risk \"R\" is unobservable we approximate it using a second population B and we actually measure the following relations: \"R\" − \"R\" or \"R\"/\"R\". In this situation, confounding occurs when \"R\" ≠ \"R\". (NB: Example assumes binary outcome and exposure variables.)\n\nSome epidemiologists prefer to think of confounding separately from common categorizations of bias since, unlike selection and information bias, confounding stems from real causal effects.\n\nTo date, few universities offer epidemiology as a course of study at the undergraduate level. One notable undergraduate program exists at Johns Hopkins University, where students who major in public health can take graduate level courses, including epidemiology, their senior year at the Bloomberg School of Public Health.\n\nAlthough epidemiologic research is conducted by individuals from diverse disciplines, including clinically trained professionals such as physicians, formal training is available through Masters or Doctoral programs including Master of Public Health (MPH), Master of Science of Epidemiology (MSc.), Doctor of Public Health (DrPH), Doctor of Pharmacy (PharmD), Doctor of Philosophy (PhD), Doctor of Science (ScD). Many other graduate programs, e.g., Doctor of Social Work (DSW), Doctor of Clinical Practice (DClinP), Doctor of Podiatric Medicine (DPM), Doctor of Veterinary Medicine (DVM), Doctor of Nursing Practice (DNP), Doctor of Physical Therapy (DPT), or for clinically trained physicians, Doctor of Medicine (MD) or Bachelor of Medicine and Surgery (MBBS or MBChB) and Doctor of Osteopathic Medicine (DO), include some training in epidemiologic research or related topics, but this training is generally substantially less than offered in training programs focused on epidemiology or public health. Reflecting the strong historical tie between epidemiology and medicine, formal training programs may be set in either schools of public health and medical schools.\n\nAs public health/health protection practitioners, epidemiologists work in a number of different settings. Some epidemiologists work 'in the field'; i.e., in the community, commonly in a public health/health protection service, and are often at the forefront of investigating and combating disease outbreaks. Others work for non-profit organizations, universities, hospitals and larger government entities such as state and local health departments, various Ministries of Health, Doctors without Borders, the Centers for Disease Control and Prevention (CDC), the Health Protection Agency, the World Health Organization (WHO), or the Public Health Agency of Canada. Epidemiologists can also work in for-profit organizations such as pharmaceutical and medical device companies in groups such as market research or clinical development.\n\n"}
{"id": "34140644", "url": "https://en.wikipedia.org/wiki?curid=34140644", "title": "Ewald Banse", "text": "Ewald Banse\n\nEwald Banse (born 23 May 1883 in Braunschweig – died 31 October 1953 in Braunschweig) was a German geographer.\n\nBanse was a professor at the technical college in his native city. Allied propaganda cited Banse's main work, \"Raum und Volk im Weltkriege\" (\"Space and People in the World War\") (1925), as proof of Germany's war lust. Banse advocated the union of all areas settled by Germans in a Great German Reich extending far beyond the 1914 frontiers. To achieve this he expressly demanded military action. The warrior was to be the carrier of the coming rule of \"Nordic nobility\".\n\n"}
{"id": "28771756", "url": "https://en.wikipedia.org/wiki?curid=28771756", "title": "GeoSMS", "text": "GeoSMS\n\nGeoSMS is a specification for geotagging SMS messages.\nIt works by embedding locations in the message text, where the locations are formatted as 'geo' URIs as defined in RFC 5870.\n\nIt was developed in 2010 by Matthew Kwan, a PhD Candidate at the RMIT School of Mathematical and Geospatial Sciences and should not be confused with the Open GeoSMS standard.\n\nA simple geotagged SMS might look like:\n\nwhich would contain the message I'm at the pub and a location with latitude 37.801631 degrees south, longitude 144.980294 degrees east, and an uncertainty of (+ or -) 10 metres.\n\nMessages using GeoSMS can also contain multiple locations, for example:\n\nwhich contains two locations.\n\nGeoSMS is used by the free Android application I Am Here (available through the Android Market) to send and receive geotagged SMS messages. It displays received messages using either a compass or map view. The GeoSMS specification is also being used to allow ships and cruising vessels to send position updates from an SMS-capable satellite phone, such as one of the recent models marketed by Iridium Communications or Globalstar.\n\nThe Open Geospatial Consortium also has an approved Open GeoSMS standard, published in 2011. This standard has been broadly implemented in Asia. The OGC Open GeoSMS standard was originally developed in Taiwan by ITRI in 2008 and submitted into the OGC in 2009.\n\n\n"}
{"id": "7732087", "url": "https://en.wikipedia.org/wiki?curid=7732087", "title": "Kenn Plateau", "text": "Kenn Plateau\n\nThe Kenn Plateau is a large piece of submerged continental crust off northeastern Australia that rifted from northeastern Australia about 63-52 mya, along with other nearby parts of the Zealandia continent.\n\n"}
{"id": "34559004", "url": "https://en.wikipedia.org/wiki?curid=34559004", "title": "List of Lepidoptera of French Polynesia", "text": "List of Lepidoptera of French Polynesia\n\nLepidoptera of French Polynesia consist of both the butterflies and moths recorded from French Polynesia.\n\nAccording to a recent estimate, there are a total of roughly 400 Lepidoptera species present on the islands of French Polynesia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33535240", "url": "https://en.wikipedia.org/wiki?curid=33535240", "title": "List of flag bearers for the Maldives at the Olympics", "text": "List of flag bearers for the Maldives at the Olympics\n\nThis is a list of flag bearers who have represented the Maldives at the Olympics.\n\nFlag bearers carry the national flag of their country at the opening ceremony of the Olympic Games.\n\n"}
{"id": "19402258", "url": "https://en.wikipedia.org/wiki?curid=19402258", "title": "List of former monarchies", "text": "List of former monarchies\n\nNote: entries in bold refer to groups of kingdoms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5840200", "url": "https://en.wikipedia.org/wiki?curid=5840200", "title": "List of honey plants", "text": "List of honey plants\n\nHoneybees usually collect nectar, pollen, or both from the following species of plants, which are called honey plants, for making honey.\n\n\n\n\n\n\n\n\nAll the plants of this family are found in the tropics and subtropics.\n\n\n\n\n\n\nAll the plants of this family are found mostly in the tropics or subtropics.\n\n\n\n\n\n\n\n\n\nAll the plants of this family are found only in the neotropics.\n\n\n\nfound in tropics or sub-tropics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll the plants of this family are found in the neotropics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll the plants of this family are found in the neotropics.\n\n\n\n"}
{"id": "7611805", "url": "https://en.wikipedia.org/wiki?curid=7611805", "title": "List of landscape gardens", "text": "List of landscape gardens\n\nThis a list of notable \"English\" landscape gardens:\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25558223", "url": "https://en.wikipedia.org/wiki?curid=25558223", "title": "List of people on the postage stamps of Abkhazia", "text": "List of people on the postage stamps of Abkhazia\n\nThis is a partial list of people featured on postage stamps of Abkhazia. These stamps are mainly issued for collectors. The dates given are the dates of issue.\n\n\nThere are also illegal stamps issued by \"opportunistic printing companies acting like Abkhazians\":\n\n"}
{"id": "41369246", "url": "https://en.wikipedia.org/wiki?curid=41369246", "title": "List of public art in Ørstedsparken", "text": "List of public art in Ørstedsparken\n\nThis is a list of public art in Ørstedsparken in Copenhagen, Denmark.\n\n"}
{"id": "1286466", "url": "https://en.wikipedia.org/wiki?curid=1286466", "title": "List of rulers of the Fon state of Savi Hweda", "text": "List of rulers of the Fon state of Savi Hweda\n\nTerritory located in present-day Benin.\n\nEnglish: \"Whydah\"; \n\n\n"}
{"id": "18376", "url": "https://en.wikipedia.org/wiki?curid=18376", "title": "Loran-C", "text": "Loran-C\n\nLoran-C was a \"hyperbolic\" radio navigation system which allowed a receiver to determine its position by listening to low frequency radio signals transmitted by fixed land-based radio beacons. Loran-C combined two different techniques to provide a signal that was both long-range and highly accurate, traits that had formerly been at odds. The downside was the expense of the equipment needed to interpret the signals, which meant that Loran-C was used primarily by militaries after it was first introduced in 1957.\n\nBy the 1970s the electronics needed to implement Loran-C had been dramatically reduced due to the introduction of solid state radio electronics, and especially the use of early microcontrollers to interpret the signal. Low-cost and easy-to-use Loran-C units became common from the late 1970s, especially in the early 1980s, leading to the earlier LORAN system being turned off in favour of installing more Loran-C stations around the world. Loran-C became one of the most common and widely used navigation systems for large areas of North America, Europe, Japan and the entire Atlantic and Pacific areas. The Soviet Union operated a nearly identical system, CHAYKA.\n\nThe introduction of civilian satellite navigation in the 1990s led to a very rapid drop-off in Loran-C use. Discussions about the future of Loran-C began in the 1990s, and several turn-off dates were announced and then cancelled. In 2010 the US and Canadian systems were shut down, along with shared Loran-C/CHAYKA stations with Russia. Several other chains remained active, and some had been upgraded for continued use. At the end of 2015, navigation chains in most of Europe were turned off. In December 2015 in the US, there was also renewed discussion of funding an eLoran system and NIST was offering to fund development of a microchip-sized eLoran receiver for distribution of timing signals.\n\nRecent U.S. legislation, such as the National Timing Resilience and Security Act of 2017 and other bills, has been introduced which may resurrect Loran.\n\nThe original LORAN was proposed by Alfred Lee Loomis at a meeting of the Microwave Committee. The US Army Air Corps was interested in the concept for aircraft navigation, and after some discussion they returned a requirement for a system offering accuracy of about at a range of , and a maximum range as great as for high-flying aircraft. The Microwave Committee, by this time organized into what would become the Radiation Laboratory, took up development as Project 3. During the initial meetings, a member of the UK liaison team, Taffy Bowen, mentioned that he was aware the British were also working on a similar concept, but had no information on its performance.\n\nThe development team, led by Loomis, made rapid progress on the transmitter design and tested several systems during 1940 before settling on a 3 MHz design. Extensive signal-strength measurements were made by mounting a conventional radio receiver in a station wagon and driving around the eastern states. However, the custom receiver design and its associated cathode ray tube displays proved to be a bigger problem. In spite of several efforts to design around the problem, instability in the display prevented accurate measurements.\n\nBy this time the team had become much more familiar with the British Gee system, and were aware of their related work on \"strobes\", a time base generator that produced well-positioned \"pips\" on the display that could be used for accurate measurement. They met with the Gee team in 1941, and immediately adopted this solution. But this meeting also revealed that Project 3 and Gee called for almost identical systems, with similar performance, range and accuracy. But Gee had already completed basic development and was entering into initial production, making Project 3 superfluous.\n\nIn response, the Project 3 team told the Army Air Force to adopt Gee, and realigned their own efforts to provide long-range navigation on the oceans. This led to US Navy interest, and a series of experiments quickly demonstrated that systems using the basic Gee concept but operating at a lower frequency around 2 MHz would offer reasonable accuracy on the order of a few miles over distances on the order of , at least at night when signals of this frequency range were able to skip off the ionosphere. Rapid development followed, and a system covering the western Atlantic was operational in 1943. Additional stations followed, first covering the European side of the Atlantic, and then a massive expansion in the Pacific. By the end of the war, there were 72 operational LORAN stations and as many as 75,000 receivers.\n\nIn 1958 the operation of the LORAN system was handed over to the US Coast Guard, which renamed the system \"Loran-A\", the lower-case name being introduced at that time.\n\nThere are two ways to implement the timing measurements needed for a hyperbolic navigation system, pulse timing systems like Gee and LORAN, and phase-timing systems like the Decca Navigator System.\n\nThe former requires sharp pulses of signal, and their accuracy is generally limited to how rapidly the pulses can be turned on and off, which is a function of the carrier frequency. There is an ambiguity in the signal; the same measurements can be valid at two locations relative to the broadcasters, but in normal operation, they are hundreds of kilometres apart, so one possibility can be eliminated.\n\nThe second system uses constant signals (\"continuous wave\") and takes measurements by comparing the phase of two signals. This system is easy to use even at very low frequencies. However, its signal is ambiguous over the distance of a wavelength, meaning there are hundreds of locations that will return the same signal. Decca referred to these ambiguous locations as \"cells\" in Decca. This demands some other navigation method to be used in conjunction to pick which cell the receiver is within, and then using the phase measurements to place the receiver accurately within the cell.\n\nNumerous efforts were made to provide some sort of secondary low-accuracy system that could be used with a phase-comparison system like Decca in order to resolve the ambiguity. Among the many methods were a directional broadcast systems known as POPI, and a variety of systems combining pulse-timing for low-accuracy navigation and then using phase-comparison for fine adjustment. Decca themselves had set aside one frequency, \"9f\", for testing this concept, but did not have the chance to do so until much later. Similar concepts were also used in the experimental Navarho system in the US.\n\nIt was known from the start of the LORAN project that the same CRT displays that showed the LORAN pulses could, when suitably magnified, also show the individual waves of the intermediate frequency. This meant that pulse-matching could be used to get a rough fix, and then the operator could gain additional timing accuracy by lining up the individual waves within the pulse, like Decca. This could either be used to greatly increase the accuracy of LORAN, or alternately, offer similar accuracy using much lower carrier frequencies, and thus greatly extend range. This would require the transmitter stations to be synchronized both in time and phase, but much of this problem had been solved by Decca engineers.\n\nThe long-range option was of considerable interest to the Coast Guard, who set up an experimental system known as LF LORAN in 1945. This operated at much lower frequencies than the original LORAN, at 180 kHz, and required very long balloon-borne antennas. Testing was carried out throughout the year, including several long-distance flights as far as Brazil. The experimental system was then sent to Canada where it was used during Operation Muskox in the Arctic. Accuracy was found to be at , a significant advance over LORAN. With the ending of Muskox it was decided to keep the system running under what became known as \"Operation Musk Calf\", run by a group consisting of the US Air Force, Royal Canadian Air Force, Royal Canadian Navy and Royal Corps of Signals. The system ran until September 1947.\n\nThis led to another major test series, this time by the newly formed USAF, known as Operation Beetle. Beetle was located in the far north, on the Canada-Alaska border, and used new guy-stayed steel towers, replacing the earlier system's balloon-lofted cable antennas. The system became operational in 1948 and ran for two years until February 1950. Unfortunately, the stations proved poorly sited, as the radio transmission over the permafrost was much shorter than expected and synchronization of the signals between the stations using groundwaves proved impossible. The tests also showed that the system was extremely difficult to use in practice; it was easy for the operator to select the wrong sections of the waveforms on their display, leading to significant real-world inaccuracy.\n\nIn 1946 the Rome Air Development Center sent out contracts for longer-ranged and more-accurate navigation systems that would be used for long-range bombing navigation. As the US Army Air Force was moving towards smaller crews, only three in the Boeing B-47 Stratojet for instance, a high degree of automation was desired. Two contracts were accepted; Sperry Gyroscope proposed the CYCLAN system (CYCLe matching LorAN) which was broadly similar to LF LORAN but with additional automation, and Sylvania proposed Whyn using continuous wave navigation like Decca, but with additional coding using frequency modulation. In spite of great efforts, Whyn could never be made to work, and was abandoned.\n\nCYCLAN operated by sending the same LF LORAN-like signals on two frequencies, LF LORAN's 180 kHz and again on 200 kHz. The associated equipment would look for a rising amplitude that indicated the start of the signal pulse, and then use sampling gates to extract the carrier phase. Using two receivers solved the problem of mis-aligning the pulses, because the phases would only align properly between the two copies of the signal when the same pulses were being compared. None of this was trivial; using the era's tube-based electronics, the experimental CYCLAN system filled much of a semi-trailer.\n\nCYCLAN proved highly successful, so much so that it became increasingly clear that the problems that led the engineers to use two frequencies were simply not as bad as expected. It appeared that a system using a single frequency would work just as well, given the right electronics. This was especially good news, as the 200 kHz frequency was interfering with existing broadcasts, and had to be moved to 160 kHz during testing.\n\nThrough this period the issue of radio spectrum use was becoming a major concern, and had led to international efforts to decide on a frequency band suitable for long-range navigation. This process eventually settled on the band from 90 to 100 kHz. CYCLAN appeared to suggest that accuracy at even lower frequencies was not a problem, and the only real concern was the expense of the equipment involved.\n\nThe success of the CYCLAN system led to a further contract with Sperry in 1952 for a new system with the twin goals of working in the 100 kHz range while being equally accurate, less complex and less expensive. These goals would normally be contradictory, but the CYCLAN system gave all involved the confidence that these could be met. The resulting system was known as Cytac.\n\nTo solve the complexity problem, a new circuit was developed to properly time the sampling of the signal. This consisted of a circuit to extract the envelope of the pulse, another to extract the derivative of the envelope, and finally another that subtracted the derivative from the envelope. The result of this final operation would become negative during a very specific and stable part of the rising edge of the pulse, and this zero-crossing was used to trigger a very short-time sampling gate. This system replaced the complex system of clocks used in CYCLAN. By simply measuring the time between the zero-crossings of the master and slave, pulse-timing was extracted.\n\nThe output of the envelope sampler was also sent to a phase-shifter that adjusted the output of a local clock that locked to the master carrier using a phase-locked loop. This retained the phase of the master signal long enough for the slave signal to arrive. Gating on the slave signal was then compared to this master signal in a phase detector, and a varying voltage was produced depending on the difference in phase. This voltage represented the fine-positioning measurement.\n\nThe system was generally successful during testing through 1953, but there were concerns raised about the signal power at long ranges, and the possibility of jamming. This led to further modifications of the basic signal. The first was to broadcast a series of pulses instead of just one, broadcasting more energy during a given time and improving the ability of the receivers to tune in a useful signal. They also added a fixed 45° phase shift to every pulse, so simple continuous-wave jamming signals could be identified and rejected.\n\nThe Cytac system underwent an enormous series of tests across the United States and offshore. Given the potential accuracy of the system, even minor changes to the groundwave synchronization were found to cause errors that could be eliminated - issues such as the number of rivers the signal crossed caused predictable delays that could be measured and then factored into navigation solutions. This led to a series of \"correction contours\" that could be added to the received signal to adjust for these concerns, and these were printed on the Cytac charts. Using prominent features on dams as target points, a series of tests demonstrated that the uncorrected signals provided accuracy on the order of 100 yards, while adding the correction contour adjustments reduced this to the order of ten yards.\n\nIt was at this moment that the US Air Force (having taken over these efforts while moving from the USAAF) dropped their interest in the project. Although the reasons are not well recorded, it appears the idea of a fully automated bombing system using radio aids was no longer considered possible. The AAF had been involved in missions covering about 1000 km (the distance from London to Berlin) and the Cytac system would work well at these ranges. But as the mission changed to trans-polar missions of 5,000 km or more, even Cytac did not offer the range and accuracy needed. They turned their attention to the use of inertial platforms and Doppler radar systems, cancelling work on Cytac as well as a competing system known as Navarho.\n\nAround this period the Navy began work on a similar system using combined pulse and phase comparison, but based on the existing LORAN frequency of 200 kHz. By this time the Navy had handed operational control of the LORAN system to the Coast Guard, and it was assumed the same arrangement would be true for any new system as well. Thus the Coast Guard was given the choice of naming the systems, and decided to rename the existing system Loran-A, and the new system Loran-B.\n\nWith Cytac fully developed and its test system on the US east coast mothballed, the Navy also decided to re-commission Cytac for tests in the long-range role. An extensive series of tests across the Atlantic were carried out by the USCGC \"Androscoggin\" (WHEC-68) starting in April 1956. Meanwhile, Loran-B proved to have serious problems keeping their transmitters in phase, and that work was abandoned. Minor changes were made to the Cytac systems to further simplify it, including a reduction in the pulse-chain spacing from 1200 to 1000 µs, the pulse rate changed to 20 pps to match the existing Loran-A system, and the phase-shifting between pulses to an alternating 0, 180 degree shift instead of 45 degrees at every pulse within the chain.\n\nThe result was Loran-C. Testing of the new system was intensive, and overwater flights around Bermuda demonstrated that 50% of fixes lay within a circle. This was a dramatic improvement over the original Loran-A, meeting the accuracy of the Gee system but at much greater range. The first chain were set up using the original experimental Cytac system, along with a second in the Mediterranean in 1957. Further chains covering the North Atlantic and large areas of the Pacific followed. At the time global charts were printed with shaded sections representing the area where a accurate fix could be obtained under most operational conditions.\n\nLoran-C had originally been designed to be highly automated, allowing the system to be operated more rapidly than the original LORAN's multi-minute measurement. It was also operated in \"chains\" of linked stations, allowing a fix to be made by simultaneously comparing two slaves to a single master. The downside of this approach was that the required electronic equipment, built using 1950s-era tube technology, was very large. Looking for companies with knowledge of seaborne, multi-channel phase-comparison electronics led, ironically, to Decca, who built the AN/SPN-31, the first widely used Loran-C receiver. The AN/SPN-31 weighed over and had 52 controls.\n\nAirborne units followed, and an adapted AN/SPN-31 was tested in an Avro Vulcan in 1963. By the mid-1960s, units with some transistorization were becoming more common, and a chain was set up in Viet Nam to support the US war efforts there. A number of commercial airline operators experimented with the system as well, using it for navigation on the great circle route between North America and Europe. However, inertial platforms ultimately became more common in this role.\n\nIn 1969, Decca sued the Navy for patent infringement, producing ample documentation of their work on the basic concept as early as 1944, along with the \"missing\" 9f frequency at 98 kHz that had been set aside for experiments using this system. Decca won the initial suit, but the judgement was overturned on appeal when the Navy claimed \"wartime expediency\".\n\nWhen Loran-C became widespread, the USAF once again became interested in using it as a guidance system. They proposed a new system layered on top of Loran-C, using it as the coarse guidance signal in much the same way that pulses were the coarse guidance and phase-comparison used for fine. To provide an extra-fine guidance signal, Loran-D interleaved another train of eight pulses immediately after the signals from one of the existing Loran-C stations, folding the two signals together. This technique became known as \"Supernumary Interpulse Modulation\" (SIM). These were broadcast from low-power portable transmitters, offering relatively short-range service of high accuracy.\n\nLoran-D was used only experimentally during war-games in the 1960s from a transmitter set in the UK. The system was also used in a limited fashion during the Vietnam War, combined with the Pave Spot laser designator system, a combination known as Pave Nail. Using mobile transmitters, the AN/ARN-92 LORAN navigation receiver could achieve accuracy on the order of , which the Spot system improved to about . The SIM concept became a system for sending additional data.\n\nAt about the same time, Motorola proposed a new system using pseudo-random pulse-chains. This mechanism ensures that no two chains within a given period (on the order of many seconds) will have the same pattern, making it easy to determine if the signal is a groundwave from a recent transmission or a multi-hop signal from a previous one. The system, Multi-User Tactical Navigation Systems (MUTNS) was used briefly but it was found that Loran-D met the same requirements but had the added advantage of being a standard Loran-C signal as well. Although MUTNS was unrelated to the Loran systems, it was sometimes referred to as Loran-F.\n\nIn spite of its many advantages, the high cost of implementing a Loran-C receiver made it uneconomical for many users. Additionally, as military users upgraded from Loran-A to Loran-C, large numbers of surplus Loran-A receivers were dumped on the market. This made Loran-A popular in spite of being less accurate and fairly difficult to operate. By the early 1970s the introduction of integrated circuits combining a complete radio receiver began to greatly reduce the complexity of Loran-A measurements, and fully automated units the size of a stereo receiver became common. For those users requiring higher accuracy, Decca had considerable success with their Decca Navigator system, and produced units that combined both receivers, using Loran to eliminate the ambiguities in Decca.\n\nThe same rapid development of microelectronics that made Loran-A so easy to operate worked equally well on the Loran-C signals, and the obvious desire to have a long-range system that could also provide enough accuracy for lake and harbour navigation led to the \"opening\" of the Loran-C system to public use in 1974. Civilian receivers quickly followed, and dual-system A/C receivers were also common for a time. The switch from A to C was extremely rapid, due largely to rapidly falling prices which led to many users' first receiver being Loran-C. By the late 1970s the Coast Guard decided to turn off Loran-A, in favour of adding additional Loran-C stations to cover gaps is its coverage. The original Loran-A network was shut down in 1979 and 1980, with a few units used in the Pacific for some time. Given the widespread availability of Loran-A charts, many Loran-C receivers included a system for converting coordinates between A and C units.\n\nOne of the reasons for Loran-C's opening to the public was the move from Loran to new forms of navigation, including INS, Transit and OMEGA, meant that the security of Loran was no longer as stringent as it was as a primary form of navigation. As these newer systems gave way to GPS through the 1980s and 90s, this process repeated itself, but this time the military was able to separate GPS's signals in such a way that it could provide both secure military and insecure civilian signals at the same time. GPS was more difficult to receive and decode, but by the 1990s the required electronics were already as small and inexpensive as Loran-C, leading to rapid adoption that has become largely universal.\n\nAlthough Loran-C was largely redundant by 2000, it has not universally disappeared due to a number of concerns. One is that the GPS system can be jammed through a variety of means; although the same is true of Loran-C, the transmitters are close-at-hand and can be adjusted if need be. More importantly, there are effects that might cause the GPS system to become unusable over wide areas, notably space weather events and potential EMP events. Loran, located entirely under the atmosphere, offers more resilience to these sorts of issues. There has been considerable debate about the relative merits of keeping the Loran-C system operational as a result of considerations like these.\n\nIn November 2009, the USCG announced that Loran-C is not needed by the U.S. for maritime navigation. This decision left the fate of LORAN and eLORAN in the U.S. to the Secretary of the Department of Homeland Security. Per a subsequent announcement, the U.S. Coast Guard, in accordance with the DHS Appropriations Act, terminated the transmission of all U.S. LORAN-C signals on 8 February 2010. On 1 August 2010 the U.S. transmission of the Russian American signal was terminated, and on 3 August 2010 all Canadian signals were shut down by the USCG and the CCG.\n\nThe European Union had decided that the potential security advantages of Loran are worthy not only of keeping the system operational, but upgrading it and adding new stations. This is part of the wider Eurofix system which combines GPS, Galileo and nine Loran stations into a single integrated system.\n\nHowever, in 2014, Norway and France both announced that all of their remaining transmitters, which make up a significant part of the Eurofix system, will be shut down on 31 December 2015. The two remaining transmitters in Europe (Anthorn, UK and Sylt, Germany) will no longer be able to sustain a positioning and navigation Loran service, with the result that the UK announced its trial eLoran service would be discontinued from the same date.\n\nIn conventional navigation, measuring one's location, or \"taking a fix\", is accomplished by taking two measurements against well known locations. In optical systems this is typically accomplished by measuring the angle to two landmarks, and then drawing lines on a nautical chart at those angles, producing an intersection that reveals the ship's location. Radio methods can also use the same concept with the aid of a radio direction finder, but due to the nature of radio propagation, such instruments are subject to significant errors, especially at night. More accurate radio navigation can be made using pulse timing or phase comparison techniques, which rely on the time-of-flight of the signals. In comparison to angle measurements, these remain fairly steady over time, and most of the effects that change these values are fixed objects like rivers and lakes that can be accounted for on charts.\n\nTiming systems can reveal the absolute distance to an object, as is the case in radar. The problem in the navigational case is that the receiver has to know when the original signal was sent. In theory, one could synchronize an accurate clock to the signal before leaving port, and then use that to compare the timing of the signal during the voyage. However, in the 1940s no suitable system was available that could hold an accurate signal over the time span of an operational mission.\n\nInstead, radio navigation systems adopted the \"multilateration\" concept. which is based on the difference in times (or phase) instead of the absolute time. The basic idea is that it is relatively easy to synchronize two ground stations, using a signal shared over a phone line for instance, so one can be sure that the signals received were sent at exactly the same time. They will not be received at exactly the same time, however, as the receiver will receive the signal from the closer station first. Timing the difference between two signals can be easily accomplished, first by physically measuring them on a cathode ray tube, or simple electronics in the case of phase comparison.\n\nThe difference in signal timing does not reveal the location by itself. Instead, it determines a series of locations where that timing is possible. For instance, if the two stations are 300 km apart and the receiver measures no difference in the two signals, that implies that the receiver is somewhere along a line equidistant between the two. If the signal from one is received exactly 100 µs, then the receiver is closer to one station than the other. Plotting all the locations where one station is 30 km closer than the other produces a curved line. Taking a fix is accomplished by making two such measurements with different pairs of stations, and then looking up both curves on a navigational chart. The curves are known as \"lines of position\" or LOP.\n\nIn practice, radio navigation systems normally use a \"chain\" of three or four stations, all synchronized to a \"master\" signal that is broadcast from one of the stations. The others, the \"secondaries\", are positioned so their LOPs cross at acute angles, which increases the accuracy of the fix. So for instance, a given chain might have four stations with the master in the center, allowing a receiver to pick the signals from two secondaries that are currently as close to right angles as possible given their current location. Modern systems, which know the locations of all the broadcasters, can automate which stations to pick.\n\nIn the case of LORAN, one station remains constant in each application of the principle, the \"primary\", being paired up separately with two other \"secondary\" stations. Given two secondary stations, the time difference (TD) between the primary and first secondary identifies one curve, and the time difference between the primary and second secondary identifies another curve, the intersections of which will determine a geographic point in relation to the position of the three stations. These curves are referred to as \"TD lines\".\n\nIn practice, LORAN is implemented in integrated regional arrays, or \"chains\", consisting of one \"primary\" station and at least two (but often more) \"secondary\" stations, with a uniform \"group repetition interval\" (GRI) defined in microseconds. The amount of time before transmitting the next set of pulses is defined by the distance between the start of transmission of primary to the next start of transmission of primary signal.\n\nThe secondary stations receive this pulse signal from the primary, then wait a preset number of milliseconds, known as the \"secondary coding delay\", to transmit a response signal. In a given chain, each secondary's coding delay is different, allowing for separate identification of each secondary's signal. (In practice, however, modern LORAN receivers do not rely on this for secondary identification.)\n\nEvery LORAN chain in the world uses a unique Group Repetition Interval, the number of which, when multiplied by ten, gives how many microseconds pass between pulses from a given station in the chain. (In practice, the delays in many, but not all, chains are multiples of 100 microseconds.) LORAN chains are often referred to by this designation (e.g., GRI 9960, the designation for the LORAN chain serving the Northeast United States).\n\nDue to the nature of hyperbolic curves, a particular combination of a primary and two secondary stations can possibly result in a \"grid\" where the grid lines intersect at shallow angles. For ideal positional accuracy, it is desirable to operate on a navigational grid where the grid lines are closer to right angles (orthogonal) to each other. As the receiver travels through a chain, a certain selection of secondaries whose TD lines initially formed a near-orthogonal grid can become a grid that is significantly skewed. As a result, the selection of one or both secondaries should be changed so that the TD lines of the new combination are closer to right angles. To allow this, nearly all chains provide at least three, and as many as five, secondaries.\n\nWhere available, common marine nautical charts include visible representations of TD lines at regular intervals over water areas. The TD lines representing a given primary-secondary pairing are printed with distinct colors, and note the specific time difference indicated by each line. On a nautical chart, the denotation for each Line of Position from a receiver, relative to axis and color, can be found at the bottom of the chart. The color on official charts for stations and the timed-lines of position follow no specific conformance for the purpose of the International Hydrographic Organization (IHO). However, local chart producers may color these in a specific conformance to their standard. Always consult the chart notes, administrations Chart1 reference, and information given on the chart for the most accurate information regarding surveys, datum, and reliability.\n\nThere are three major factors when considering signal delay and propagation in relation to LORAN-C:\n\nThe chart notes should indicate whether ASF corrections have been made (Canadian Hydrographic Service (CHS) charts, for example, include them). Otherwise, the appropriate correction factors must be obtained before use.\n\nDue to interference and propagation issues suffered from land features and artificial structures such as tall buildings, the accuracy of the LORAN signal can be degraded considerably in inland areas (see Limitations). As a result, nautical charts will not show TD lines in those areas, to prevent reliance on LORAN-C for navigation.\nTraditional LORAN receivers display the time difference between each pairing of the primary and one of the two selected secondary stations, which is then used to find the appropriate TD line on the chart. Modern LORAN receivers display latitude and longitude coordinates instead of time differences, and, with the advent of time difference comparison and electronics, provide improved accuracy and better position fixing, allowing the observer to plot their position on a nautical chart more easily. When using such coordinates, the datum used by the receiver (usually WGS84) must match that of the chart, or manual conversion calculations must be performed before the coordinates can be used.\n\nEach LORAN station is equipped with a suite of specialized equipment to generate the precisely timed signals used to modulate / drive the transmitting equipment. Up to three commercial cesium atomic clocks are used to generate 5 MHz and pulse per second (or 1 Hz) signals that are used by timing equipment to generate the various GRI-dependent drive signals for the transmitting equipment.\n\nWhile each U.S.-operated LORAN station is supposed to be synchronized to within 100 ns of UTC, the actual accuracy achieved as of 1994 was within 500 ns.\n\nLORAN-C transmitters operate at peak powers of 100–4,000 kilowatts, comparable to longwave broadcasting stations. Most use 190–220 metre tall mast radiators, insulated from ground. The masts are inductively lengthened and fed by a loading coil (see: electrical lengthening). A well known-example of a station using such an antenna is Rantum. Free-standing tower radiators in this height range are also used. Carolina Beach uses a free-standing antenna tower. Some LORAN-C transmitters with output powers of 1,000 kW and higher used supertall 412 metre mast radiators (see below). Other high power LORAN-C stations, like George, used four T-antennas mounted on four guyed masts arranged in a square.\n\nAll LORAN-C antennas are designed to radiate an omnidirectional pattern. Unlike longwave broadcasting stations, LORAN-C stations cannot use backup antennas because the exact position of the antenna is a part of the navigation calculation. The slightly different physical location of a backup antenna would produce Lines of Position different from those of the primary antenna.\n\nLORAN suffers from electronic effects of weather and the ionospheric effects of sunrise and sunset. The most accurate signal is the groundwave that follows the Earth's surface, ideally over seawater. At night the indirect skywave, bent back to the surface by the ionosphere, is a problem as multiple signals may arrive via different paths (multipath interference). The ionosphere's reaction to sunrise and sunset accounts for the particular disturbance during those periods. Magnetic storms have serious effects as with any radio based system.\n\nLORAN uses ground-based transmitters that only cover certain regions. Coverage is quite good in North America, Europe, and the Pacific Rim.\n\nThe absolute accuracy of LORAN-C varies from . Repeatable accuracy is much greater, typically from .\n\nLORAN Data Channel (LDC) is a project underway between the FAA and USCG to send low bit rate data using the LORAN system. Messages to be sent include station identification, absolute time, and position correction messages. In 2001, data similar to Wide Area Augmentation System (WAAS) GPS correction messages were sent as part of a test of the Alaskan LORAN chain. As of November 2005, test messages using LDC were being broadcast from several U.S. LORAN stations.\n\nIn recent years, LORAN-C has been used in Europe to send differential GPS and other messages, employing a similar method of transmission known as EUROFIX.\n\nA system called SPS (Saudi Positioning System), similar to EUROFIX, is in use in Saudi Arabia. GPS differential corrections and GPS integrity information are added to the LORAN signal. A combined GPS/LORAN receiver is used, and if a GPS fix is not available it automatically switches over to LORAN.\n\nAs LORAN systems are maintained and operated by governments, their continued existence is subject to public policy. With the evolution of other electronic navigation systems, such as satellite navigation systems, funding for existing systems is not always assured.\n\nCritics, who have called for the elimination of the system, state that the LORAN system has too few users, lacks cost-effectiveness, and that GNSS signals are superior to LORAN. Supporters of continued and improved LORAN operation note that LORAN uses a strong signal, which is difficult to jam, and that LORAN is an independent, dissimilar, and complementary system to other forms of electronic navigation, which helps ensure availability of navigation signals.\n\nOn 26 February 2009, the U.S. Office of Management and Budget released the first blueprint for the Financial Year 2010 budget. This document identified the LORAN-C system as “outdated” and supported its termination at an estimated savings of $36 million in 2010 and $190 million over five years. \n\nOn 21 April 2009 the U.S. Senate Committee on Commerce, Science and Transportation and the Committee on Homeland Security and Governmental Affairs released inputs to the FY 2010 Concurrent Budget Resolution with backing for the continued support for the LORAN system, acknowledging the investment already made in infrastructure upgrades and recognizing the studies performed and multi-departmental conclusion that eLORAN is the best backup to GPS.\n\nSenator Jay Rockefeller, Chairman of the Committee on Commerce, Science and Transportation, wrote that the committee recognized the priority in \"Maintaining LORAN-C while transitioning to eLORAN\" as means of enhancing the national security, marine safety and environmental protection missions of the Coast Guard.\n\nSenator Collins, the ranking member on the Committee on Homeland Security and Governmental Affairs wrote that the President's budget overview proposal to terminate the LORAN-C system is inconsistent with the recent investments, recognized studies and the mission of the U.S. Coast Guard. The committee also recognizes the $160 million investment already made toward upgrading the LORAN-C system to support the full deployment of eLORAN.\n\nFurther, the Committees also recognize the many studies which evaluated GPS backup systems and concluded both the need to back up GPS and identified eLORAN as the best and most viable backup. \"This proposal is inconsistent with the recently released (January 2009) Federal Radionavigation Plan (FRP), which was jointly prepared by DHS and the Departments of Defense (DOD) and Transportation (DOT). The FRP proposed the eLORAN program to serve as a Position, Navigation and Timing (PNT) backup to GPS (Global Positioning System).\"\n\nOn 7 May 2009, President Barack Obama proposed cutting funding (approx. $35 million/year) for LORAN, citing its redundancy alongside GPS. In regard to the pending Congressional bill, H.R. 2892, it was subsequently announced that \"[t]he Administration supports the Committee's aim to achieve an orderly termination through a phased decommissioning beginning in January 2010, and the requirement that certifications be provided to document that the LORAN-C termination will not impair maritime safety or the development of possible GPS backup capabilities or needs.\"\n\nAlso on 7 May 2009, the U.S. General Accounting Office (GAO), the investigative arm of Congress, released a report citing the very real potential for the GPS system to degrade or fail in light of program delays which have resulted in scheduled GPS satellite launches slipping by up to three years.\n\nOn 12 May 2009 the March 2007 Independent Assessment Team (IAT) report on LORAN was released to the public. In its report the ITA stated that it \"unanimously recommends that the U.S. government complete the eLORAN upgrade and commit to eLORAN as the national backup to GPS for 20 years.\" The release of the report followed an extensive Freedom Of Information Act (FOIA) battle waged by industry representatives against the federal government. Originally completed 20 March 2007 and presented to the co-sponsoring Department of Transportation and Department of Homeland Security (DHS) Executive Committees, the report carefully considered existing navigation systems, including GPS. The unanimous recommendation for keeping the LORAN system and upgrading to eLORAN was based on the team's conclusion that LORAN is operational, deployed and sufficiently accurate to supplement GPS. The team also concluded that the cost to decommission the LORAN system would exceed the cost of deploying eLORAN, thus negating any stated savings as offered by the Obama administration and revealing the vulnerability of the U.S. to GPS disruption.\n\nIn November 2009, the U.S. Coast Guard announced that the LORAN-C stations under its control would be closed down for budgetary reasons after 4 January 2010 provided the Secretary of the Department of Homeland Security certified that LORAN is not needed as a backup for GPS.\n\nOn 7 January 2010, Homeland Security published a notice of the permanent discontinuation of LORAN-C operation. Effective 2000 UTC 8 February 2010, the United States Coast Guard terminated all operation and broadcast of LORAN-C signals in the USA. The U.S. Coast Guard transmission of the Russian American CHAYKA signal was terminated on 1 August 2010. The transmission of Canadian LORAN-C signals was terminated on 3 August 2010.\n\nWith the potential vulnerability of GNSS systems, and their own propagation and reception limitations, renewed interest in LORAN applications and development has appeared. Enhanced LORAN, also known as eLORAN or E-LORAN, comprises an advancement in receiver design and transmission characteristics which increase the accuracy and usefulness of traditional LORAN. With reported accuracy as good as ± 8 meters, the system becomes competitive with unenhanced GPS. eLORAN also includes additional pulses which can transmit auxiliary data such as DGPS corrections. eLORAN receivers now use \"all in view\" reception, incorporating signals from all stations in range, not solely those from a single GRI, incorporating time signals and other data from up to 40 stations. These enhancements in LORAN make it adequate as a substitute for scenarios where GPS is unavailable or degraded. \nIn recent years the US Coast Guard has reported several episodes of GPS interference in the Black Sea. South Korea has claimed that North Korea has jammed GPS near the border, interfering with airplanes and ships. By 2018, the United States will build a new eLoran system as a complement to and backup for the GPS system. And the South Korean government has already pushed plans to have three eLoran beacons active by 2019, which is enough to provide accurate corrections for all shipments in the region if North Korea (or anyone else) tries to block GPS again.\n\nOn 31 May 2007, the UK Department for Transport (DfT), via the General Lighthouse Authorities (GLA), awarded a 15-year contract to provide a state-of-the-art enhanced LORAN (eLORAN) service to improve the safety of mariners in the UK and Western Europe. The service contract will operate in two phases, with development work and further focus for European agreement on eLORAN service provision from 2007 through 2010, and full operation of the eLORAN service from 2010 through 2022. The first eLORAN transmitter is situated at Anthorn radio station Cumbria, UK, and is operated by Babcock International (previously Babcock Communications).\n\neLORAN: The UK government has granted approval for seven differential eLoran ship-positioning technology stations to be built along the south and east coasts of the UK to help counter the threat of jamming of global positioning systems. They are set to reach initial operational capability by summer 2014. The General Lighthouse Authorities (GLAs) of the UK and Ireland announced October 31 the initial operational capability of UK maritime eLoran. Seven differential reference stations now provide additional position, navigation, and timing (PNT) information via low-frequency pulses to ships fitted with eLoran receivers. The service will help ensure they can navigate safely in the event of GPS failure in one of the busiest shipping regions in the world, with expected annual traffic of 200,000 vessels by 2020.\n\nDespite these plans, in light of the decision by France and Norway to cease Loran transmissions on 31 December 2015, the UK announced at the start of that month that its eLoran service would be discontinued on the same day.\n\nA list of LORAN-C transmitters. Stations with an antenna tower taller than 300 metres (984 feet) are shown in bold.\n\n\n\n"}
{"id": "2380581", "url": "https://en.wikipedia.org/wiki?curid=2380581", "title": "Lovecraft Country", "text": "Lovecraft Country\n\nLovecraft Country is a term coined by Keith Herber for the New England setting, combining real and fictitious locations, used by H. P. Lovecraft in many of his weird fiction stories, and later elaborated by other writers working in the Cthulhu Mythos. The term was popularized by Chaosium, the producers of the Lovecraftian role-playing game \"Call of Cthulhu\". Lovecraft scholar S. T. Joshi refers to the area as the \"Miskatonic region\", after its fictional river and university, while Lovecraft biographer Lin Carter calls it Miskatonic County, though Lovecraft indicates that at least some of his fictional towns were located in the real-life Essex County of Massachusetts.\n\nIn its 1998 supplement \"Dead Reckonings\", Chaosium defined Lovecraft Country as \"a land located in the northeast of Massachusetts. The most important portion stretches along the Miskatonic River valley, from Dunwich in the far west to where it enters the Atlantic Ocean between Arkham, Kingsport, and Martin's Beach.\" These locations, along with Innsmouth, are a list of the most significant locations in Lovecraft Country.\n\nSometimes the phrase is used in a more inclusive sense, encompassing not only northeastern Massachusetts but also the southern hills of Vermont (the setting of \"The Whisperer in Darkness\") as well as Lovecraft's hometown of Providence, Rhode Island, where he set such works as \"The Case of Charles Dexter Ward\" and \"The Haunter of the Dark\".\n\nLovecraft first used a New England setting in his 1920 short story \"The Terrible Old Man\", set in Kingsport. \"The Picture in the House\" (written later in 1920), is the first of his stories to mention both Arkham and the Miskatonic Valley. The story begins with something of a manifesto for why the New England countryside is a fitting backdrop for his horror stories:\n\n\"the true epicure of the terrible, to whom a new thrill of unutterable ghastliness is the chief end and justification of existence, esteem most of all the ancient, lonely farmhouses of backwoods New England; for there the dark elements of strength, solitude, grotesqueness, and ignorance combine to form the perfection of the hideous.\"\n\nIn a 1930 letter to Robert E. Howard, Lovecraft attempted to explain his fascination with New England as a setting for weird fiction: \"It is the night-black Massachusetts legendary which packs the really macabre 'kick'. Here is material for a really profound study in group neuroticism; for certainly, none can deny the existence of a profoundly morbid streak in the Puritan imagination.\" \n\nLovecraft first mentioned Arkham's Miskatonic University in \"Herbert West–Reanimator\", written in 1921-1922. He added Dunwich to his imaginary landscape in 1928's \"The Dunwich Horror\", and expanded it to include Innsmouth in 1931's \"The Shadow over Innsmouth\".\n\nOther Lovecraft stories that make use of Lovecraft Country settings include \"The Festival\", \"The Colour out of Space\", \"The Strange High House in the Mist\", \"The Dreams in the Witch House\", and \"The Thing on the Doorstep\".\n\nAugust Derleth, Lovecraft's friend and literary executor, discouraged other Cthulhu Mythos writers from setting their stories in Lovecraft's New England. But he himself attempted to fill in the blanks of the setting, particularly in his posthumous \"collaborations\" with Lovecraft — actually Derleth's stories based on fragments, notes or ideas that Lovecraft left behind after his death.\n\n\"The Lurker at the Threshold\" is set in Billington's Wood, a fictional forest north of Arkham, while \"Witches' Hollow\" takes place in the titular valley in the hills to the west of the town. The title of \"The Fisherman of Falcon Point\" refers to a promontory on the Atlantic coast south of Innsmouth. \"Wentworth's Day\" and \"The Horror from the Middle Span\" take place in the area north of Dunwich, while \"The Gable Window\" concerns a house on the Aylesbury Pike.\n\nBetween 1990 and 1998, Chaosium released a number of Lovecraft Country gamebooks for the \"Call of Cthulhu\" roleplaying game, series created by author/editor Keith Herber. Most were background supplements which codified descriptions of Lovecraft's named cities, but there were also a number of adventure books. These included:\n\n\nSome of the books have been rereleased in the 2000s (\"H.P. Lovecraft's Dunwich\", \"H.P. Lovecraft's Arkham\", and \"H.P. Lovecraft's Kingsport\").\n\nSkotos, an online game company, has licensed Chaosium's Lovecraft Country material. They have produced two games, \"Lovecraft Country: The Tomb of the Desert God\" and \"Lovecraft Country: Arkham by Night\", as well as a comic, \"Lovecraft Country: Return to Arkham\", written by Shannon Appelcline.\n\nIn 2008, following Chaosium's expanded licensing program for \"Call of Cthulhu\", Keith Herber and Tom Lynch established Miskatonic River Press. The publisher's first release, \"New Tales of the Miskatonic Valley\", which marked both Herber's return to \"Call of Cthulhu\" and Lovecraft Country was announced published in 2009.\n\nThe phrase \"Lovecraft Country\" is now used outside of the Cthulhu gaming community. \"Return to Lovecraft Country\" was a collection of short stories set in \"the New England of H.P. Lovecraft\", published by Triad Entertainments in 1996. The editor, Scott David Aniolowski, has also done editorial work for Chaosium. \"Eternal Lovecraft\", a short-story collection published by Golden Gryphon Press in 1998, has a section called \"Lovecraft Country\". \"Lovecraft Country\" was the title of a 2016 novel by Matt Ruff.\n\nThe phrase occurs in popular discussions of Lovecraft's connection to the region. The \"Harvard Law Record\" used the phrase in an October 20, 2005 article: \n\n"}
{"id": "13686670", "url": "https://en.wikipedia.org/wiki?curid=13686670", "title": "Milan metropolitan area", "text": "Milan metropolitan area\n\nThe Milan metropolitan area, also known as Grande Milano (\"Greater Milan\"), is the largest metropolitan area in Italy and the 54th largest in the world. The metropolitan area descripted in this article is strictly statistical and, contrary to the administrative Metropolitan City of Milan, a provincial-level municipality, does not imply any kind of administrative unity or function.\n\nGiven the absence of an official statistical definition for the metropolitan area of Milan, tracing precise boundaries is a somewhat slippery issue. However, during the last decade, a number of studies have been carried out on the subject by some authoritative institutions and scholars, notably the Organisation for Economic Co-operation and Development and numerous Italian sources that build a definition based on commuting fluxes and on the concentration of commercial, leisure and public utility services. A broad consensus exists upon a definition that includes the central Lombard provinces of Milan, Bergamo, Como, Lecco, Lodi, Monza and Brianza, Pavia, Varese and the Piedmontese Province of Novara, while some scholars include also the Province of Cremona and Brescia in Lombardy and the Emilian Province of Piacenza. The overall population under the narrowest definition is about 8.2 million over an area of about .\n\nThe following is a list of the twenty largest cities in the Milan metropolitan area as ranked by population.\n\n"}
{"id": "58258699", "url": "https://en.wikipedia.org/wiki?curid=58258699", "title": "Municipal road", "text": "Municipal road\n\nMunicipal road or municipal way is or was a category of roads which are owned and/or maintained by municipalities. Generally, transit sections of higher-class roads through the built-up area is counted as part of the higher-class road, not as a municipal road. Some countries use the term local road or local communication in similar sense.\n\n\n"}
{"id": "28735056", "url": "https://en.wikipedia.org/wiki?curid=28735056", "title": "North Island Fault System", "text": "North Island Fault System\n\nThe North Island Fault System or North Island Dextral Fault Belt is a set of southwest-northeast trending seismically-active faults in the North Island of New Zealand that carry most of the dextral (right lateral) strike-slip component of the oblique convergence of the Pacific Plate with the Australian Plate. They include the Wairarapa Fault and Wellington Fault to the southwest, the Ruahine and Mohaka Faults in the central section and the Waimana, Waiotahi, Whakatane and Waiohau Faults to the northeast. Most of the fault system consists of dextral strike-slip faults, although towards its northeastern end the trend swings to more S-N trend and the faults become mainly oblique normal in sense as the zone intersects with the Taupo rift zone. This fault zone accommodates up to 10 mm/yr of strike-slip displacement.\n\nThe North Island Fault System consists of eight main fault strands and many smaller related faults.\n\nThe more southeasterly branch of the Wellington Fault is known as the Mohaka fault. The fault splays to the north, onto the Waimana Fault, which itself branches into the Waiotahi and Waioeka Faults, the main segment of the Mohaka Fault eventually passes into the Whakatane Fault.\n\nAs the Wellington Fault branches near Woodville, the more northwesterly branch is known as the Ruahine Fault. Results from trenching over this fault suggest an earthquake recurrence interval of 400–500 years, with typical offsets in the range 3.0–5.5 m. At its northern end this fault becomes the Waiohau Fault.\n\nThe Waiohau Fault extends from the end of the Ruahine Fault north towards the Bay of Plenty. It lies roughly parallel with, and to the west of, the Whakatane, Waimana, and Waiotahi Faults, and to the east of the Taupo Rift. At its southern end it is a dextral strike-slip fault, becoming a normal dip-slip fault for the northern part of its length. The valley of the Rangitaiki River approximately follows the line of the fault. The fault is believed to be responsible for an earthquake in 1866 which was centred near Te Mahoe, east of Kawerau.\n\nThe Wairarapa Fault extends from near the coast just southwest of Lake Wairarapa, running along the lake's northwestern edge. The 1855 Wairarapa earthquake was caused by movement along this fault. The recurrence interval for large earthquakes on this fault is less than 2,000 years.\n\nThe Wellington Fault is a dextral strike-slip fault that runs from the Cook Strait on the southern coast of North Island up to near Woodville, where the fault branches into the Mohaka and Ruahine Faults. No historical earthquakes have been recorded along this fault although a significant event is estimated to have occurred within the last 1,000 years. The recurrence interval for large earthquakes on this fault is estimated to be less than 2,000 years. Three main segments have been identified, the Wellington-Hutt section, the Tararua section and the Pahiatua section.\n"}
{"id": "16279087", "url": "https://en.wikipedia.org/wiki?curid=16279087", "title": "Outline of Monaco", "text": "Outline of Monaco\n\nThe following outline is provided as an overview of and topical guide to Monaco:\n\nMonaco – small sovereign city-state located in Western Europe. Monaco lies on the northern coast of the Mediterranean and is surrounded by France. Monaco is often regarded as a tax haven, and many of its inhabitants are wealthy and from foreign countries (including France), although they are not a majority.\n\n\nGeography of Monaco\n\n\n\nDemographics of Monaco\n\n\nGovernment of Monaco\n\n\n\n\nForeign relations of Monaco\n\nThe Principality of Monaco is a member of:\n\n\nMilitary of Monaco\n\nHistory of Monaco\n\n\nHouse of Grimaldi\n\n\n\n\n\n \n\nCulture of Monaco\n\n\n\nSport in Monaco\n\n\n\n\nEconomy of Monaco\n\n\n\nHealth in Monaco\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "416255", "url": "https://en.wikipedia.org/wiki?curid=416255", "title": "Pausanias (geographer)", "text": "Pausanias (geographer)\n\nPausanias (; \"Pausanías\"; c. AD 110 – c. 180) was a Greek traveler and geographer of the second century AD, who lived in the time of Roman emperors Hadrian, Antoninus Pius, and Marcus Aurelius. He is famous for his \"Description of Greece\" (, \"Hellados Periegesis\"), a lengthy work that describes ancient Greece from his first-hand observations. This work provides crucial information for making links between classical literature and modern archaeology. Andrew Stewart assesses him as:\n\nPausanias was born in 110 AD into a Greek family and was probably a native of Lydia; he was certainly familiar with the western coast of Asia Minor, but his travels extended far beyond the limits of Ionia. Before visiting Greece, he had been to Antioch, Joppa, and Jerusalem, and to the banks of the River Jordan. In Egypt, he had seen the pyramids. While at the temple of Ammon, he had been shown the hymn once sent to that shrine by Pindar. In Macedonia, he appears to have seen the tomb said to be that of Orpheus in Libethra (modern Leivithra). Crossing over to Italy, he had seen something of the cities of Campania and of the wonders of Rome. He was one of the first known to write of seeing the ruins of Troy, Alexandria Troas, and Mycenae.\n\nPausanias' \"Description of Greece\" is in ten books, each dedicated to some portion of Greece. He begins his tour in Attica (), where the city of Athens and its demes dominate the discussion. Subsequent books describe Corinthia () (second book), Laconia () (third), Messenia () (fourth), Elis () (fifth and sixth), Achaea () (seventh), Arcadia () (eighth), Boetia () (ninth), Phocis () and Ozolian Locris () (tenth). The project is more than topographical; it is a cultural geography. Pausanias digresses from the description of architectural and artistic objects to review the mythological and historical underpinnings of the society that produced them. As a Greek writing under the auspices of the Roman empire, he was in an awkward cultural space, between the glories of the Greek past he was so keen to describe and the realities of a Greece beholden to Rome as a dominating imperial force. His work bears the marks of his attempt to navigate that space and establish an identity for Roman Greece.\n\nHe is not a naturalist by any means, although from time to time, he does comment on the physical realities of the Greek landscape. He notices the pine trees on the sandy coast of Elis, the deer and the wild boars in the oak woods of Phelloe, and the crows amid the giant oak trees of Alalcomenae. It is mainly in the last section that Pausanias touches on the products of nature, such as the wild strawberries of Helicon, the date palms of Aulis, and the olive oil of Tithorea, as well as the tortoises of Arcadia and the \"white blackbirds\" of Cyllene.\n\nPausanias is most at home in describing the religious art and architecture of Olympia and of Delphi. Yet, even in the most secluded regions of Greece, he is fascinated by all kinds of depictions of deities, holy relics, and many other sacred and mysterious objects. At Thebes he views the shields of those who died at the Battle of Leuctra, the ruins of the house of Pindar, and the statues of Hesiod, Arion, Thamyris, and Orpheus in the grove of the Muses on Helicon, as well as the portraits of Corinna at Tanagra and of Polybius in the cities of Arcadia.\n\nPausanias has the instincts of an antiquary. As his modern editor, Christian Habicht, has said,\nUnlike a Baedeker guide, in \"Periegesis\" Pausanias stops for a brief excursus on a point of ancient ritual or to tell an apposite myth, in a genre that would not become popular again until the early nineteenth century. In the topographical part of his work, Pausanias is fond of digressions on the wonders of nature, the signs that herald the approach of an earthquake, the phenomena of the tides, the ice-bound seas of the north, and the noonday sun that at the summer solstice, casts no shadow at Syene (Aswan). While he never doubts the existence of the deities and heroes, he sometimes criticizes the myths and legends relating to them. His descriptions of monuments of art are plain and unadorned. They bear the impression of reality, and their accuracy is confirmed by the extant remains. He is perfectly frank in his confessions of ignorance. When he quotes a book at second hand he takes pains to say so.\n\nThe work left faint traces in the known Greek corpus. \"It was not read\", Habicht relates; \"there is not a single mention of the author, not a single quotation from it, not a whisper before Stephanus Byzantius in the sixth century, and only two or three references to it throughout the Middle Ages.\" The only manuscripts of Pausanias are three fifteenth-century copies, full of errors and lacunae, which all appear to depend on a single manuscript that survived to be copied. Niccolò Niccoli had this archetype in Florence in 1418. At his death in 1437, it went to the library of San Marco, Florence, then it disappeared after 1500. \n\nUntil twentieth-century archaeologists concluded that Pausanias was a reliable guide to the sites they were excavating, Pausanias was largely dismissed by nineteenth- and early twentieth-century classicists of a purely literary bent: they tended to follow the usually authoritative Wilamowitz in regarding him as little more than a purveyor of second-hand accounts, who, it was suggested, had not visited most of the places he described. Habicht (1985) describes an episode in which Wilamowitz was led astray by his misreading of Pausanias in front of an august party of travellers in 1873, and attributes to it Wilamowitz's lifelong antipathy and distrust of Pausanias. Modern archaeological research, however, has tended to vindicate Pausanias.\n\n\n\n\n\n\n"}
{"id": "40432040", "url": "https://en.wikipedia.org/wiki?curid=40432040", "title": "Peter Anich", "text": "Peter Anich\n\nPeter Anich (1723–1766) was an Austrian cartographer and maker of mathematical instruments.\n\nHis works, particularly the 1772 published \"Atlas Tyrolensis\", are among the most accurate maps of their time. \n"}
{"id": "41883805", "url": "https://en.wikipedia.org/wiki?curid=41883805", "title": "Postal addresses in Gibraltar", "text": "Postal addresses in Gibraltar\n\nThe British Overseas Territory of Gibraltar has introduced the postcode GX11 1AA. This is pending the introduction of a postcode system similar to that used in the United Kingdom. This has been under consideration by the Government of Gibraltar in 2006. The postcode is not required for local mail.\n\nThe Royal Gibraltar Post Office has divided the territory into fourteen postal zones known as 'walks' or 'districts', each with a number or letter as well as a name, but these are for internal use and not encountered in addresses. They are separate from the Major Residential Areas, used for statistical purposes.\n"}
{"id": "657216", "url": "https://en.wikipedia.org/wiki?curid=657216", "title": "Regions of Europe", "text": "Regions of Europe\n\nEurope is often divided into regions based on geographical, cultural or historical criteria. Many European structures currently exist, some are cultural, economic, or political - examples include the Council of Europe, the European Broadcasting Union with the Eurovision Song Contest, and the European Olympic Committees with the European Games. Several transcontinental countries which border mainland Europe, are often included as belonging to a \"wider Europe\" including, Russia, Azerbaijan, Georgia, Kazakhstan, Cyprus, Armenia, Greenland, as well as the Special member state territories and the European Union.\n\nGroupings by compass directions are the hardest to define in Europe, since there are a few calculations of the midpoint of Europe (among other issues), and the pure geographical criteria of \"east\" and \"west\" are often confused with the political meaning these words acquired during the Cold War Era.\n\nThe modern physical geographic regions of Europe, include:\n\n\nEurope can be divided along many differing historical lines, normally corresponding to those parts that were inside or outside a particular cultural phenomenon, empire or political division. The areas varied at different times, and so it is arguable as to which were part of some common historical entity (e.g., were Germany or Britain part of Roman Europe as they were only partly and relatively briefly part of the Empire—or were the countries of the former communist Yugoslavia part of the Eastern Bloc, since it was not in the Warsaw Pact).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3870867", "url": "https://en.wikipedia.org/wiki?curid=3870867", "title": "Ribeira Afonso", "text": "Ribeira Afonso\n\nRibeira Afonso is a village on the eastern coast of São Tomé Island in São Tomé and Príncipe. Its population is 1,621 (2008 est.). It is 4 km southwest of Água Izé and 16 km south of the capital São Tomé.\n"}
{"id": "41626425", "url": "https://en.wikipedia.org/wiki?curid=41626425", "title": "Rooftopping", "text": "Rooftopping\n\nRooftopping sometimes called roofing refers to the unsecured ascent of rooftops, cranes, antennas, smokestacks, etc., usually illegally. Participants have been classified by psychologists as thrill seekers who thrive on high levels of stimulation and complexity, although other theories explaining their motivation exist. They usually take selfie photos or videos and panoramic photographs, placing an emphasis on being in an extremely dangerous but unique situation. It is chiefly an undertaking of younger people, many are seeking social-media exposure and followers, and it has been especially popular in Russia. Rooftoppers clandestinely access off-limits staircases, roof hatches, ladders, etc., and it incorporates some aspects of buildering. It is an offshoot of urban exploring, but is not universally condoned among urban explorers due to its high risk taking. Because it is often practiced in the pursuit of viral-ready videos and selfies, it tends to bring attention to urban exploring that can result in heightened security and greater restriction against access to desirable exploration venues.\n\nRooftoppers often use head-cams such as GoPro or other helmet cameras for videos.\n\nSome also use quadcopter drones for exploration and recording.\n\n\n"}
{"id": "51339588", "url": "https://en.wikipedia.org/wiki?curid=51339588", "title": "Russian Hydrographic Service", "text": "Russian Hydrographic Service\n\nThe Russian Hydrographic Service, full current official name Department of Navigation and Oceanography of the Ministry of Defence of the Russian Federation (), is Russia's hydrographic office, with responsibility to facilitate navigation, performing hydrographic surveys and publishing nautical charts.\n\nSince the Russian state is of such a vast size and nature that it includes many different seas, long and indented coastlines and a great number of islands, as well as a complex system of waterways and lakes, surveying has been an indispensable activity for the Russian Navy since its modernization at the time of Czar Peter the Great in the 17th century. The hydrographic service has been historically attached to the Russian Navy and the agents and supervisors of hydrographic works have been largely naval officers throughout its history.\n\nRussia is a member of the International Hydrographic Organization.\n\nDespite having undergone a number of name changes along its history, the main functions of the Hydrographic Service of the Russian Navy have been quite consistently the following:\n\nAt the time of Peter I hydrographic surveys were carried out following personal decrees of the emperor through the General admiral. Hydrographic tasks were always performed by Naval officers, who from 1724 onward began to work under instructions from the Admiralty Board.\n\nBy 1746 important matters concerning hydrography were entrusted to Fleet Captain Alexey Nagayev who compiled the first atlas of the Bering Sea, as well as of the Baltic Sea in 1752. Nagayev's charts were very detailed for its time and, despite a few shortcomings, his atlas of the Baltic Sea was republished in 1757, 1788, 1789 and 1795, serving Russian mariners for more than 50 years.\n\nIn 1777 the Admiralty Board founded the Russian Hydrographic Service, implementing a plan that marked the beginning of systematic drawing of nautical charts. In 1799 a committee for the dissemination of marine sciences and the improvement of the drawing of charts was created, and in 1807 the Russian Lighthouse Administration was established so that the lighthouse system in Russian shores, shoals and islands would follow an organized pattern and be provided with regular, state-controlled maintenance. The first director of this section was Leontiy Spafaryev.\n\nIn 1827 the special Office of the Hydrographer General was established. In the same year the Corps of Naval Navigators was founded, the chief of which was also a hydrographer. The first and only general of the newly-instituted body was hydrographer Admiral Gavril Andreevich Sarychev (in office 1827-31), after whose death the management of the office was transferred to the Chief of Naval Staff Prince A. S. Menshikov. The first and only director of the hydrographic depot was F. F. Schubert (in office 1827-37).\n\nIn 1837 the former institutions dealing with hydrography were abolished and all the management of the hydrographic section was transferred to the newly-instituted Russian Hydrographic Department, the directors of which were:\n\n\nThe Hydrographic Office engaged in the periodical publication of notes, devoted both to hydrographic information, as well as information on other sectors of naval affairs. The committee, established in 1799, published:\n\nIn 1885 the Russian Hydrographic Department was overhauled and renamed as 'Main Hydrographic Office' —Главное гидрографическое управление (ГГУ)— of the Admiralty. Its chief was the former director in charge of the lighthouses and navigation of the Baltic Sea, Vice Admiral R. Bazhenov, who was also the chairman of the Maritime Scientific Committee —Морского учёного комитета.\n\n1886 saw the establishment of the meteorological department. In 1891 the fields of drawing, engraving, lithography and printing were integrated into the maritime cartography section and from 1897 this section began successful experiments printing nautical charts using aluminum printing plates. In 1902 a new building with a photographic department, including a workshop introducing innovative photographic reproduction techniques, was built within the premises of the Main Hydrographic Office.\nBy 1904 the new techniques had been mastered by the staff and high-quality material began to be printed.\n\nThe duties of the Hydrographic Department at the time included:\n\nThe jurisdiction of the Main Hydrographic Office included the management of:\n\nThe period between 1885 and 1917 was characterized by full-scale construction and modernization of lighthouse equipment, compass technology and improvements in the printing of nautical charts.\nThe 1910-1915 Arctic Ocean Hydrographic Expedition, led by Rear Admiral Boris A. Vilkitsky on icebreakers Vaigach and Taimyr mapped the last blank areas of the northern coast of Eastern Siberia —which were the last unmapped coastal areas of Eurasia, and gathered as well a vast amount of oceanographic and meteorological data.\n\nThe heads of the Main Hydrographic Office were:\n\nAfter the initial period of instability that followed the 1917 Russian Revolution against the Imperial Government, the Hydrographic Office slowly returned to its duties as the situation calmed down and the Soviet takeover was completed. The work and dedication of hydrographers such as Konstantin Neupokoev, ensured that the service returned to normality. \nCzarist names and symbols were quickly removed, the former hydrographic institution being renamed as the 'Main Hydrographic Office of the Russian Republic' —Главное гидрографическое управление Российской Республики.\n\nBetween 1918 and 1922 hydrometeorological service units named Ubek (Убек) were established for the management of navigation safety measures. The Ubek regulated the particular hydrographic zone —in Arkhangelsk for example it was 'Ubek-North'— as part of the unified local authority directly responsible to the central Soviet government.\n\nIn 1924 the Hydrographic Office was renamed the 'Central Hydrographic Department of the USSR' and a set of new flags and pennants was issued. Barely two years later, in September 1926, the office underwent another name change; the new name was \"Hydrographic Department of the Office of Naval Forces of the Workers' and Peasants' Red Army\" (UVMS Hydrographic Department of the Red Army) —Гидрографический отдел Управления Военно-морских сил Рабоче-крестьянской Красной Армии. In 1927 the name was simplified to 'Hydrographic Office UVMS Red Army' —Гидрографическое управление УВМС РККА.\n\nEmperor Nicholas II Land first partially charted by Boris Vilkitsky in 1913, but still not fully surveyed when it was renamed \"Severnaya Zemlya\" by the Presidium of the Central Executive Committee of the USSR in 1926, was the last blank area on the vast map of the Soviet Union. Vilkitsky's expedition, which in 1913 sighted and surveyed a section of the eastern coastline of what he assumed was a single landmass, had to concentrate on the Siberian continental shore in order to prepare the way for the Northeast Passage and he had no means to make a comprehensive survey further north. Finally a hydrographic expedition of the Arctic Institute of the USSR led by Georgy Ushakov and Nikolay Urvantsev thoroughly surveyed the large Severnaya Zemlya archipelago in 1930–32, making it the last sizable territory on Earth to be put on the map.\n\nIn 1935 the Ubek were replaced by hydrographic offices subordinate to the commander of the fleets and flotillas. In 1937 the 'Hydrographic Office UVMS Red Army' was renamed the \"Hydrographic Office of the Workers' and Peasants' Navy\" —Гидрографическое управление Рабоче-крестьянского Военно-морского флота (РК ВМФ). The design of the flags and pennants used by the office also underwent changes.\n\nIn 1940 the Hydrographic Office was renamed 'Hydrographic Office of the Navy' —Гидрографическое управление ВМФ. The following year, owing to the dire situation of the Great Patriotic War (1941-1945), a Task Force in the Hydrographic Department of the Navy was formed in Moscow by the Chief of the General Staff in order to deal more effectively with the emergency matters caused by the war in Russian soil and waters.\n\nIn postwar times, after the initial period of reconstruction was over, the Soviet Navy began the challenging task of creating an oceanic fleet carrying nuclear weapons. The 1960-1970 decade saw the introduction and development of ballistic missile submarines as part of the strategical aims of the Soviet fleet. These submarines were designed to be able to navigate in all areas of the oceans, including in the Arctic Ocean under the ice cover. In the field of navigation, during this period radar reflectors, radio beacons and other modernized signal systems were introduced.\n\nThe technological leap of that decade would impose a radical change in the navigational, hydrographic and hydrometeorological support of the Navy. During this period the need for innovative and detailed survey and mapping of the Earth's geophysics, including gravity and magnetic fields, became of the utmost importance for the Hydrographic Service of the Soviet Union. At that time, using its survey vessels, the department spearheaded a comprehensive study of large areas of the Atlantic, Pacific, Indian, Arctic, as well as of the then little explored Southern Ocean off the coast of Antarctica. As a result, a huge volume of data on bottom topography, physical fields, and hydrophysical characteristics of the water masses, among other pioneering oceanographic information of the oceans, were collected.\n\nIn 1972, owing to Cold War dictated priorities, the Hydrographic Office of the Navy was overhauled and transformed into the 'General Directorate of Navigation and Oceanography of the Ministry of Defense of the USSR' —Главное управление навигации и океанографии Министерства обороны СССР (ГУНиО МО).\nThe 1970-1980 decade was marked by an increased international prominence of the 'General Directorate of Navigation and Oceanography of the Ministry of Defense'. Since then the Hydrographic Service of the USSR officially represented the interests of the Soviet Union —in the same manner that the organization that replaced it in later years would do for the Russian Federation— in the International Hydrographic Organization and the International Association of Lighthouse Authorities, as well as the interests of the Ministry of Defence in the Intergovernmental Oceanographic Commission of UNESCO.\n\nThe heads of the Hydrographic Service of the USSR were:\n\n\nIn the wake of the dissolution of the Soviet Union the 'General Directorate of Navigation and Oceanography of Defense of the USSR' would be placed under the Russian Federation, the legal successor of the USSR. In 1992 the service was renamed as the 'Main Directorate of Navigation and Oceanography of the Ministry of Defense of the Russian Federation' —Главное управление навигации и океанографии Министерства Обороны Российской Федерации (ГУНиО Минобороны России). Depending from their location, some of the vessels of the Soviet Hydrographic Service managed to be released and join the navies of the newly-formed republics after the breakup of the USSR. These ships were usually renamed and overhauled or transformed before being put into use, for example the GS-13 small hydrographic vessel that had been launched in 1986 in Soviet Lithuania became the Pereyaslav (U512) when it was made part of the Ukrainian Navy in November 1995.\n\nAlthough the basic designs were left unchanged, prominent Communist-era symbols —such as the hammer and sickle and the red star, were removed from the flags of the Russian Hydrographic Service and the blue and white Russian Navy Ensign was restored. In 2001 a new regulation introduced slight alterations in the symbols that had been adopted in 1992 following the fall of the USSR.\n\nIn 2006 the name of the Russian Hydrographic Service would be changed to 'Department of Navigation and Oceanography of the Ministry of Defence of the Russian Federation' —Управление навигации и океанографии Министерства Обороны Российской Федерации (УНиО Минобороны России), its current official name. Russian military presence in Arctic waters resumed in the summer of 2013 when Hydrographic Service vessels belonging to the Northern Fleet sailed to Rudolf Island in Franz Josef Land. The area had been neglected by the armed forces for a long time in the years that followed the fall of the USSR. \n\nThe heads of the Hydrographic Service of the Russian Federation in recent times have been:\n\nThe vessels operating for the Russian Hydrographic Service have been historically of two orders or categories. To the first category belong those vessels that were built specifically as survey ships, of which there are different classes, and to the second, other kind of vessels of the Russian Navy that, although not especially built for the purpose, have eventually engaged in survey operations. The latter usually undertook surveys for specific periods of time during their naval service, such as the a four-masted tall ship Kruzenshtern, which performed hydrographic surveys between 1961 and 1965.\n\n\n\n\n"}
{"id": "22650165", "url": "https://en.wikipedia.org/wiki?curid=22650165", "title": "Rõmuuta", "text": "Rõmuuta\n\nRõmuuta is a fictional place from Robert Vaidlo's influential children's book series. While the books' protagonists are from the drawn city of Kukeleegua, Rõmuuta is primarily inhabited by toys and ruled by marzipan figures in tinfoil.\n\nOfficially an island parandise with mandatory joy and free sugar-laden food for everybody, Rõmuuta practices a regime of oppression on both its inhabitants and visitors, and takes special steps to prevent visitors from leaving. For example, the landing quays of the island are made of sugar so, once the visitor arrival celebrations end, they will melt in the seawater — leaving boats tied to the quay to drift freely. Accordingly, Rõmuuta has been described as a subtle allegory for Soviet Union.\n\n"}
{"id": "55051296", "url": "https://en.wikipedia.org/wiki?curid=55051296", "title": "Siton Undae", "text": "Siton Undae\n\nSiton Undae is one of the largest and densest dune fields in the vicinity of Planum Boreum, the Martian northern polar ice-cap. It is named after one of the classical albedo features on Mars. Its name was officially approved by IAU on 20 March 2007. It extends from latitude 73.79°N to 77.5°N and from longitude 291.38°E to 301.4°E (43.98°W – 57.08°W). Its centre is located at latitude 75.55°N, longitude 297.28E (62.72°W), and has a diameter of .\n\nSiton Undae is part of a cluster of sand-seas (undae), which along with Hyperboreae, and Abalos Undae, overlay the lowlands of Vastitas Borealis. Siton Undae superposes the deepest basin of the northern region of Mars and contains amorphous silica-coated glass-rich dunes. It is theorised that the formation of Siton Undae may have occurred during early erosion incidents of the Planum Boreum cavi unit, and that Rupes Tenuis may also have been a sand source, although it is now depleted. Other dune fields sharing the same formation history include Olympia and Aspledon Undae.\n\nSiton Undae is the southernmost of the densest northern circumpolar dune fields and its presence indicates effective sand transport and accumulation from sand sources to the north and west. Siton Undae, along with Abalos, and Hyperboreae Undae, is also a tributary to less dense dune fields that continue all the way to the Martian prime meridian.\n\nSpectral analysis of the dunes of the circumpolar ergs, including Siton Undae, using the OMEGA instrument on board the European Mars Express orbiter, indicates that 80 to 90 percent of these sands are composed of volcanic glass produced by eruptions of volcanoes situated in Martian glaciers. These ratios of glass and crystalline material are similar to those obtained in Iceland from eruptions of volcanoes below the ice. It is also theorised that significant amounts of granular glass may have been transferred to Vastitas Borealis, and Siton Undae, by catastrophic floods originating from Chryse Planitia, Valles Marineris, Juventae Chasma, and the southern Acidalia region of Mars.\n\nThe dunes of Siton Undae contain amorphous silica-coated glass-rich sand. The lack of evidence for the presence of volcanoes in the region of Planum Boreum, as well as the absence of evidence of any large-scale melt deposits due to crater impact, suggests that these silica-coated deposits may have formed by alteration of the basaltic sands through acidic action. Further, it is theorised that acidic alteration of glassy deposits may have been a common conversion mechanism on Mars, especially at high latitudes.\n"}
{"id": "14420360", "url": "https://en.wikipedia.org/wiki?curid=14420360", "title": "Space Hijackers", "text": "Space Hijackers\n\nThe Space Hijackers is a group originating in the United Kingdom that defines itself as \"an international band of anarchitects who battle to save our streets, towns and cities from the evils of urban planners, architects, multinationals and other hoodlums\". \"Time Out\" magazine has described the group as \"an inventive and subversive group of London ‘Anarchitects’ who specialise in reclaiming public spaces – usually without permission.\"\n\nThe group's activities have included \"guerrilla benching\" — restoring benches that had been recently removed and bolting them to the ground — organising a midnight game of cricket in the centre of the City of London financial district, and satirising the glossy architects' drawings that are displayed on the perimeter of luxury apartments by depicting children’s playgrounds and other projects they believe to be actually desirable. Many of these activities aim to bring to people's attention the role which corporations play in society in a different light.\n\nThe Space Hijackers exist mainly to change the public's perception of spaces it regularly uses, mainly by staging unexpected events. Their explicit objective is \"to effect and change the physical space of architecture\", and, eschewing violent protest and other forms of transparent direct action, their methods aim \"to invade and re-brand corporate space\". They believe that the use of physical space is becoming more and more politicised, and thus in order to break apart from that politicisation, they stage events that are typically 'unusual' for that particular space, 'hijacking' it and hoping to change people's perception of the use of that space forever. They believe that increasing politicisation usually leads to increased subordination and discrimination and other forms of domination and control and so Space Hijackers claim to seek to break down and deconstruct society's notions of space. They seek to effect this by attempting to undermine the authority of the owner's \"text\", confusing and re-contextualising it and thereby making apparent the possibility of an alternative future. Protests tend to be non-violent and Space Hijacker projects usually involved a good sense of fun. Protest strategy includes involving passers-by.\n\nThe Space Hijackers have many contacts with other groups, and involve themselves with all sorts, including but not limited to Critical Mass, Indymedia, Rhythms of Resistance samba band, free media collective iconscious, Reverend Billy and the Church of Stop Shopping and evoLhypergrapHyCx. The Hijackers pulled several of these groups together in Anarchitecture Week 2005, a week of anti-building related activities in response to, and hosted at the same time as Architecture Week.\n\nFormed in 1999, their first major action was the Circle Line Party - a party on London Underground's Circle Line which attracted around 150 people armed with sound systems, disco lights and bars all disguised as luggage. \n\nProjects of the Space Hijackers have included the following:\n\n\nDuring the 2009 G-20 demonstrations in London, members of the Space Hijackers protest group drove their Alvis Saracen armoured personnel carrier (known to them as \"the tank\") into the City of London and parked it outside the Royal Bank of Scotland in Bishopsgate. The Saracen, which had been painted bright blue with black and white chequer stripes in a mock police livery, was equipped with CCTV. Eleven Hijackers were arrested and charged on two counts of impersonating police officers. On 27 January 2010, the Crown Prosecution Service dropped all charges against the Space Hijackers because there was \"not enough evidence to provide a realistic prospect of conviction\". The Space Hijackers won compensation from the police, but their names and biometric data were kept on file for 18 months, allegedly because of a dispute between the Metropolitan Police and the City of London Police.\n\n"}
{"id": "100175", "url": "https://en.wikipedia.org/wiki?curid=100175", "title": "Special Protection Area", "text": "Special Protection Area\n\nA Special Protection Area (SPA) is a designation under the European Union Directive on the Conservation of Wild Birds. Under the Directive, Member States of the European Union (EU) have a duty to safeguard the habitats of migratory birds and certain particularly threatened birds. Together with Special Areas of Conservation (SACs), the SPAs form a network of protected sites across the EU, called Natura 2000. Each SPA has an EU code – for example the North Norfolk Coast SPA has the code \"UK9009031\".\n\nAs at 21 September 2006, there were 252 classified SPAs and 12 proposed SPAs in England, Scotland, Wales and Northern Ireland.\n\nThe Conservation (Natural Habitats etc.) Regulations 1994 implement the terms of the Directive in Scotland, England and Wales. In Great Britain, SPAs (and SACs) designated on land or in the intertidal area are normally also notified as Sites of Special Scientific Interest (SSSIs), and in Northern Ireland as Areas of Special Scientific Interest (ASSIs). For example, the Broadland SPA in eastern England is a conglomeration of some 28 SSSIs. SPAs may extend below low tide into the sea, and for these areas SSSI notification is not possible. In Scotland, some SPAs have been classified without any underpinning designation by SSSI.\n\n\"Special Protection Areas\" for birds in Poland are called OSOPs (). As of 2005, there were 72 OSOP Areas designated as such.\n\nThe Castro Verde SPA extends into six municipalities of Baixo Alentejo Subregion: Aljustrel Municipality, Almodôvar Municipality, Beja Municipality, Castro Verde Municipality, Mértola Municipality and Ourique Municipality, a total area of 79,007 hectares (790 km ).\n\nThe Spanish term is \"ZEPA\".\nThere were 644 Spanish sites as at 2016.\n\nThe Czech Republic use the term \"Ptačí oblast\" (PO, bird area) for SPAs. There were declared 41 bird areas by the government directives between 2004 and 2009. They cover 9% of the state area.\n\n\n\n"}
{"id": "307200", "url": "https://en.wikipedia.org/wiki?curid=307200", "title": "Submergent coastline", "text": "Submergent coastline\n\nSubmergent coastlines are stretches along the coast that have been inundated by the sea by a relative rise in sea levels from either isostacy or eustacy.\n\nSubmergent coastline are the opposite of emergent coastlines, which have experienced a relative fall in sea levels.\n\nFeatures of a submergent coastline are drowned river valleys or rias and drowned glaciated valleys or fjords.\n\nEstuaries are often the drowned mouths of rivers.\n\nThe Western Coastal Plains of the Indian subcontinent are examples of submergent coastline. The ancient city of Dvārakā, which is mentioned in the great epic Mahabharata, is now under water. The coastline also forms the estuaries of the Narmada and the Tapti Rivers.\n"}
{"id": "5655147", "url": "https://en.wikipedia.org/wiki?curid=5655147", "title": "Telephone numbers in Montenegro", "text": "Telephone numbers in Montenegro\n\nThis is a list of dialing codes by town in Montenegro.\n\nUntil Montenegro gained independence from Serbia and Montenegro, the nation was accessed through the international dialing code +381. The new dialing code +382 was introduced after independence and the two codes were used in parallel until February 2007, when +382 nominally became the only acceptable code. As of 1 October 2007, +381 is used only for Serbia and +382 is the only acceptable code for Montenegro.\n\nArea codes have two digits after the initial '0' trunk prefix, and local numbers have six digits. The trunk prefix is omitted when calling from abroad.\n\nThe following code prefixes are used for network groups.\n\nThe old codes were used in parallel with the new codes until 1 October 2008:\n\nVoIP – Area code 78\n\nThe following special telephone numbers are valid across the country:\n\n"}
{"id": "52899852", "url": "https://en.wikipedia.org/wiki?curid=52899852", "title": "Urban areas in Finland", "text": "Urban areas in Finland\n\nAn urban area in Finland is defined as an inhabited area of at least 200 people and a maximum distance of between buildings. The Finnish term for this is a taajama (). Because of the strict definition of a \"taajama\", these areas exist both inside and outside of city and municipal borders.\n\nThe largest \"taajama\" in Finland is the Helsingin keskustaajama (\"Helsinki central urban area\") with approximately 1,200,000 inhibitants in 2014. It extends across Helsinki as well as 10 other municipalities in the Greater Helsinki area. The second largest is the Tampereen keskustaajama with almost 320,000 inhibitants in 2012.\n\nThe presence of \"taajama\" areas is used to regulate traffic, with a default of speed limit inside a \"taajama\" and outside. Each major road leading in or out of a \"taajama\" is marked with a road sign. \n\n"}
{"id": "16798540", "url": "https://en.wikipedia.org/wiki?curid=16798540", "title": "Vehicle registration plates of American Samoa", "text": "Vehicle registration plates of American Samoa\n\nThe U.S. unincorporated territory of American Samoa first required its residents to register their motor vehicles and display license plates in 1924.\n\nIn 1956, the United States, Canada and Mexico came to an agreement with the American Association of Motor Vehicle Administrators, the Automobile Manufacturers Association and the National Safety Council that standardized the size for license plates for vehicles (except those for motorcycles) at in height by in width, with standardized mounting holes. American Samoa adopted these standards in 1977.\n\n"}
