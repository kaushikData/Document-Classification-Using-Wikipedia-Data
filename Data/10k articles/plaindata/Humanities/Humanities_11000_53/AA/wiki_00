{"id": "5808498", "url": "https://en.wikipedia.org/wiki?curid=5808498", "title": "1819 in archaeology", "text": "1819 in archaeology\n\nThe year 1819 in archaeology involved some significant events.\n\n\n\n\n\n"}
{"id": "51239861", "url": "https://en.wikipedia.org/wiki?curid=51239861", "title": "2016 Hong Kong LegCo candidates' disqualification controversy", "text": "2016 Hong Kong LegCo candidates' disqualification controversy\n\nA controversy arose during the 2016 Legislative Council election in Hong Kong as the Electoral Affairs Commission (EAC) banned six potential localist candidates from running for the Legislative Council of Hong Kong (LegCo). The EAC carried out a new election measure to require all candidates to sign an additional \"confirmation form\" in the nomination to declare their understanding of Hong Kong being an inalienable part of China as stipulated in the Article 1, Article 12 and Article 159(4) of the Basic Law of Hong Kong.\n\nLocalist Hong Kong Indigenous's Edward Leung and pan-democrat League of Social Democrats (LSD) Avery Ng sought a judicial review but the court refused to immediately hear the judicial reviews. Leung subsequently signed the confirmation form but was asked by returning officers whether they would still advocate independence along with some other localist candidates including Civic Passion's Alvin Cheng and Hong Kong National Party's Chan Ho-tin.\n\nAfter the end of the nomination period, nominations of six localist candidates, Hong Kong National Party's Chan Ho-tin, Democratic Progressive Party's Yeung Ke-cheong, Nationalist Hong Kong's Nakade Hitsujiko, Conservative Party's Alice Lai Yee-man, Hong Kong Indigenous's Edward Leung and independent Chan Kwok-keung, were \"invalidated\", including Edward Leung which EAC returning officer Cora Ho Lai-sheung rejected Leung's nomination on the basis of she did not trust Leung \"genuinely changed his previous stance for independence.\"\n\nOn 14 July 2016, the Electoral Affairs Commission (EAC) announced its plan to require all candidates to sign an additional \"confirmation form\" in the nomination to declare their understanding of Hong Kong being an inalienable part of China as stipulated in the Article 1 of the Basic Law, Article 12 which stated that the Hong Kong Special Administrative Region (HKSAR) shall be a local administrative region of the People's Republic of China (PRC), which shall enjoy a high degree of autonomy and come directly under the Central People's Government, as well as Article 159(4) which stipulated that no amendment to the Basic Law shall contravene the established basic policies of the PRC regarding Hong Kong (i.e. Hong Kong should be a special administrative region of the PRC under the “one country, two systems” principle). Article 104 also required members of the Legislative Council to swear to uphold the Basic Law and swear allegiance to the Hong Kong Special Administrative Region before assuming office. \n\nAs many potential localist candidates are advocating or promoting Hong Kong independence, the EAC stated that “independence of the HKSAR” was inconsistent with the constitutional and legal status of the HKSAR as stipulated in the Basic Law, as well as the established basic policies of the PRC regarding Hong Kong. It also stated that returning officers were required to take into account all relevant information before deciding whether a nomination is valid according to and Electoral Affairs Commission (Electoral Procedure) (Legislative Council) Regulation 541D § 16 (the Regulation) and request the candidate to provide any other information the returning officer deems appropriate to satisfy him/her that the nomination is valid according to Sections 10 or 11 of the Regulation. Hong Kong Human Rights Monitor director Law Yuk-kai criticised the government's move as \"censorship of political ideas\" and a breach of freedom of thought.\n\nLocalist candidates reacted differently to the new measure. Civic Passion's Alvin Cheng signed the confirmation form when he submitted his nomination to run in the Hong Kong Island constituency. Civic Passion spokesman and New Territories West candidate Cheng Chung-tai justified the group's decision as a form of civil disobedience.\n\nEdward Leung of the pro-independence Hong Kong Indigenous who won over 66,000 votes in February's New Territories East by-election said he would not sign the form and would seek a judicial review. The pan-democrats also stated they would boycott the new election measure by not signing the additional form.\n\nOn 22 July, Edward Leung, who had not yet signed the confirmation form, received email from the EAC asking if he would still advocate Hong Kong independence after submitting the original nomination form stating he would uphold the Basic Law and pledge allegiance to the Hong Kong Special Administrative Region. Civic Passion’s Alvin Cheng and the Hong Kong National Party’s Chan Ho-tin both received similar emails on 25 July. Two other localist candidates, Nationalist Hong Kong's Nakade Hitsujiko and Conservative Party's Alice Lai Yee-man, received similar emails in the following days. Those questions were claimed to be a factor to determine the validity of their nominations.\n\nRepresented by Senior Counsel Martin Lee, Edward Leung and pan-democrat League of Social Democrats (LSD) chairman Avery Ng and general secretary Chan Tak-cheung filed a judicial review, arguing that the EAC had acted beyond its powers, and accuse the government of political censorship. On 27 July, High Court judge Justice Thomas Au Hing-cheung refused to immediately hear the judicial reviews, as he said he saw no urgency in dealing with the case before the end of the nomination period. After the court's decision, Leung agreed to sign the confirmation form.\n\nOn 30 July, Chan Ho-tin received an email from the EAC which said his nomination in New Territories West had been \"invalidated\" as he did not comply with the , since he had refused to sign the additional confirmation form.\n\nA day after, Yeung Ke-cheong of the localist Democratic Progressive Party, positioned second on a candidate list with Jonathan Ho Chi-kwong in Kowloon West was also invalidated as he, unlike Chan, explicitly rejected the Basic Law by not signing both the original and additional confirmation forms to pledge to uphold the Basic Law. Yeung said he would launch a judicial review.\n\nPro-independence candidate Nakade Hitsujiko for New Territories West became the third candidate to be disqualified on 1 August even though he had signed the new form. He had also previously run in 2015 District Council election.\n\nOn 2 August, three more localist candidates were disqualified, Conservative Party's Alice Lai Yee-man in Hong Kong Island, Hong Kong Indigenous' Edward Leung who ran in February and received more than 66,000 votes in the New Territories East by-election and independent Chan Kwok-keung in New Territories East while nominations of Clarence Ronald Leung Kam-shing and Yau Man-king on Chan's list were validated. In her letter, EAC returning officer Cora Ho Lai-sheung rejected Leung's nomination with the attachment of Leung’s Facebook posts, newspaper clippings and cited transcripts of his remarks at press conferences, and stated that although Leung had signed the forms, she did not believe that Leung \"genuinely changed his previous stance for independence.\"\n\nOn 3 August, all 30 Legal Subsector members of the 1,200-strong Election Committee, which is responsible for choosing Chief Executive of Hong Kong including former Hong Kong Bar Association chairmen Edward Chan King-sang SC and Philip Dykes SC questioned whether returning officers had the power to investigate the “genuineness” of candidates’ declarations and accordingly disqualify their candidacies. In the statement, it wrote that \"[the Section 40 of the Legislative Council Ordinance] does not give the returning officer any power to inquire into the so-called genuineness of the candidates’ declarations, let alone making a subjective and political decision to disqualify a candidate without following any due process on the purported ground that the candidate will not genuinely uphold the Basic Law.\" It also wrote that \"arbitrary and unlawful exercise of powers by government officials ... are most damaging to the rule of law in Hong Kong.\"\n\nHowever, Secretary for Justice Rimsky Yuen said officers did have the power to consider some evidence, as they had done in the past. He did not specify any past cases.\n\nOn 5 August, the Hong Kong independence advocates who were banned from the election launched a rally which was dubbed “first pro-independence rally in Hong Kong”. The rally drew about 12000 people. The pro-independence activists vowed they would press on with their cause and campaign for wider public support.\n\n"}
{"id": "53106462", "url": "https://en.wikipedia.org/wiki?curid=53106462", "title": "Access to public information in Croatia", "text": "Access to public information in Croatia\n\nAccess to public information and freedom of information (FOI) refer to the right of access to information held by public bodies also known as \"right to know\". Access to public information is considered of fundamental importance for the effective functioning of democratic systems, as it enhances governments' and public officials' accountability, boosting people participation and allowing their informed participation into public life. The fundamental premise of the right of access to public information is that the information held by governmental institutions is in principle public and may be concealed only on the basis of legitimate reasons which should be detailed in the law. \nIn the course of EU accession negotiations Croatia harmonised its media legislation to European standards. This process touched also legislation on access to public information which has been amended to reflect European and international standards.\nCroatia thus adopted its law on the Right of Access to Public Information in 2013, after a decade of advocacy, campaigns, and public discussions led by civil society organisations. However, despite the improvements of the legal framework regulating access to public information, problems remain in the implementation, especially for journalists willing to request and obtain information from the government.\nIn Croatia, access to public information has become a constitutional right with the 2010 amendments of the Constitution. It is regulated by the Law on the Right to Access Information adopted by the Croatian Parliament in 2013. The Law also regulates the re-use of information held by public authorities. The Law complies with the Directive 2003/98/EC of the European Parliament and of the Council on the re-use of public sector information and with Regulation 1049/2001 of the European Parliament and of the Council of 30 May 2001 regarding public access to European Parliament, Council and Commission documents.\nAccording to the Law, the right of access to information encompasses the right of the beneficiaries, i.e. any local or foreign natural person or legal entity, to seek and acquire information, as well as the obligation of public authorities to guarantee access to requested information, regardless of the request.\nThe Croatian law on Access to public information is in informed by four principles:\n\nPublic bodies are obliged to publish on the Internet the following relevant information: laws and other regulations in their field of activity, including draft proposals of laws; general acts and decisions affecting the interests of beneficiaries; annual plans, programmes, strategies and financial reports referring to the work of public authority bodies; information on budget, financing sources and subsidies; information on their internal organisation; notes and conclusions from official sessions; information on public procurement and tenders; information on the way of exercising rights of access to and re-use of information, including contact details of the Information Commissioner and the fee required to access and re-use of information.\nFor the purpose of ensuring the right of access to information, the Law establishes that public bodies are obliged to appoint an Information Commissioner, a special official in charge of resolving the issues emerging from the exercise of the right of access to information. Specifically, the Information Commissioner shall conduct the tasks connected to disclosure of public information in accordance with the Law, including providing the necessary assistance to applicants, improving the manner of processing, classification and safe-keeping of the information, and maintaining the Information Commissioner Register.\n\nPublic authority bodies may restrict access to information if:\nMoreover, restrictions may be operated when there can be reasonable doubts that disclosing the requested information might prevent the efficiency, independence or impartiality of ongoing proceedings or the execution of court orders and sentence.\nThe public authority body in charge of acting upon the request of access to information is obliged to conduct the Proportionality Interest Test in order to reach the decision about disclosure. Proportionality Test and Public Interest Test refer to the assessment of proportionality between reasons for granting access to information and reasons for imposing restrictions and granting access to information only when the public interest prevails. If, on the basis of the Test, the public interest prevails over the damage caused to other protected interests, the information shall be disclosed.\nPublic authorities are bound to grant access to information by timely publishing the information on their work in an accessible manner, i.e. on the webpage, or in the Official Gazette and the Central catalogue of officials documents of the Republic of Croatia, etc. Information can be provided directly, or in writing, or by giving insight into documents and making copies of the documents containing the requested information, or by delivering copies of the requested information.\nApplicants can submit the request either orally or in written form. The submitter of the request is not obliged to mention any reason for requesting access to information.\nAccess to public information does not require paying any administrative and court fees. The beneficiaries might only be called to pay for the actual costs of providing the information requested.\nAuthorities are obliged to issue their decisions within 15 days from the day of submitting a request. This deadline can be prolonged by other 15 days in case of complex requests (e.g. when the information must be sought outside the offices of the public authority concerned, when a single request contains a request for different information, or when the situation requires to conduct the Proportionality Test and Public Interest Test, etc.) \nIf the public authority does not hold the information, it is obliged to transfer the request to the body that might have it and notify the submitter thereof.\nAgainst the Decision taken by the public authority, the applicant may file a Complaint to the Commissioner within 15 days since the Decision has been delivered. \nNo complaint may be filed against the decision issued by the Commission, but an administrative dispute may be initiated before the High Administrative Court of the Republic of Croatia.\nThe Croatian law on access to public information regulates also the right to re-use information for commercial or non-commercial purposes.\n\nThe Croatian law on access to public information is quite advanced and in line with international standards and best practices. However, in the country a culture of secrecy persists. and the law, by itself, has not changed this and so far it has not raised the level of general transparency in Croatian society and institutions.\nOne of the main problem affecting the realization of the right of access to public information is the lack of adequate resources allocated to the office of the Commissioner. According to the 2015 Report on the implementation of the law presented to the Parliament, an increase in the resources for the functioning of the Commissioner’s Office if compared with previous years, they are still too limited to allow for the full application of the law. Over the course of 2015, the number of cases dealt with by the Commissioner’s Office has increased. However, the report showed that, due to a lack of staff and resources, not all the complaints received during 2015 have been solved by the Commissioner’s Office.\nAnother problem concerns the limited application of proactive disclosure, which - according to the Commissioner Anamarija Musa - together with re-use of information, a milestone of access to public information in the 21st century. Also, according to Commissioner Musa, the implementation of the law is particularly problematic at regional and local level and when it comes to requesting information to private entities providing a public service or to enterprises where the state holds the majority of capital shares. Moreover, Commissioner Musa deems particularly concerning the fact that access to public information requests are regularly ignored, so that the two thirds of the overall complaints received by the Commissioner are caused by this reason.\nIn Croatia, the web platform imamopravoznati.org, developed using the Alavetely software, has been launched to facilitate citizens' exercise of the right of access to public information. It allows to submit requests for public information to Croatian public authorities, and track their answers. Historic requests, any correspondence between the applicant and public authorities, are archived and publicly online.\n\n \n"}
{"id": "18935059", "url": "https://en.wikipedia.org/wiki?curid=18935059", "title": "Air Pirates", "text": "Air Pirates\n\nThe Air Pirates were a group of cartoonists who created two issues of an underground comic called \"Air Pirates Funnies\" in 1971, leading to a famous lawsuit by Walt Disney Productions. Founded by Dan O'Neill, the group also included Bobby London, Shary Flenniken, Gary Hallgren, and Ted Richards.\n\nThe original Air Pirates were a gang of Mickey Mouse antagonists of the 1930s; Dan O'Neill imagined Mickey Mouse to be a symbol of conformist hypocrisy in American culture, and therefore a ripe target for satire.\n\nThe lead stories in both issues of \"Air Pirates Funnies\" (published by Last Gasp in July & August 1971), created by O'Neill, London, and Hallgren, focused on Walt Disney characters, most notably from Floyd Gottfredson's Mickey Mouse newspaper strip, with the Disney characters engaging in adult behaviors such as sex and drug consumption. O'Neill insisted it would dilute the parody to change the names of the characters, so his adventurous mouse character was called \"Mickey\". Ted Richards took on the Big Bad Wolf and the Three Little Pigs, opening up a second wave of parody attacking Disney's appropriation of European (and American) folklore. In doing so, they infringed Disney's copyrights by using characters the company created without permission. On October 21, 1971, Disney filed a lawsuit against O'Neill, Hallgren, London and Richards (Flenniken had not contributed to the parody stories).\n\nThe nucleus of the \"Air Pirates\" collective began to form in late 1969-early 1970, when London met Richards at the office of the \"Berkeley Tribe\", an underground newspaper where both were staff cartoonists. (London later drew a highly fictionalized account of their experiences at the \"Tribe\" in his story \"Why Bobby Seale is Not Black\" in \"Merton of the Movement\" [Last Gasp's \"Cocoanut Comix\" imprint, Oct. 1972].) In 1970 London and Richards attended the Sky River Rock Festival near Portland, Oregon, and met Flenniken and O'Neill at the media booth, where Flenniken was producing a daily Sky River newsletter on a mimeograph machine. Before the festival was over the four of them produced a four-page tabloid comic, \"Sky River Funnies\", mostly drawn by London. O'Neill also met Seattle-based cartoonist Gary Hallgren at the festival.\n\nMeanwhile, O'Neill, who was producing the strip \"Odd Bodkins\" for the \"San Francisco Chronicle\", but was fearful of losing his copyright over it, decided on an odd tactic to regain control of his strip: \"he\" would engage in copyright infringement, which he reasoned would force the newspaper to surrender the strip's copyright back to him for fear of being sued. O'Neill worked 28 Walt Disney characters, including Mickey Mouse and Pluto, into the strip. In late November 1970, the \"Chronicle\" fired O'Neill for the final time and discontinued the strip.\n\nAfter the Sky River Rock Festival, Flenniken, Richards, and Hallgren returned to Seattle, where Flenniken created graphics for the Seattle Liberation Front's brief-lived underground newspaper, \"Sabot\". London went back to San Francisco with O'Neil and started working with him, contributing a \"basement\" strip to \"Odd Bodkins\". \n\nIn early 1971 O'Neill invited Flenniken, Richards, and Hallgren to San Francisco to form the Air Pirates collective. The Air Pirates lived together in a warehouse on Harrison Street in San Francisco, where London and Flenniken began a relationship that turned into a short-lived marriage.\n\nEach of the cartoonists shared a common interest in the styles of past masters of the comic strip, and — unrelated to their assault on Disney — in creating their stories for Air Pirates projects each set out to imitate the style of an old-time cartoonist:\n\nAfter the Pirates were established, Willy Murphy, Larry Todd and Gary King started hanging around the collective and contributing to their projects, missing the original \"Air Pirates Funnies\" but appearing in later Air Pirates comics.\n\nAccurately telling the story of Disney's lawsuit against the Air Pirates is difficult, due to the conflicting memories of the litigants; however, it is fair to say that all through the lawsuit, O'Neill was deviant. He was so eager to be sued by Disney that he had copies of \"Air Pirates Funnies\" am into a Disney board meeting by the son of a board member. On October 21, 1971, he got his wish as Disney filed a lawsuit against O'Neill, Hallgren, London and Richards (Flenniken had not contributed to the parody stories), alleging, among other things, copyright infringement, trademark infringement, and unfair competition. Disney later added Last Gasp publisher Ron Turner's name to the suit. The Pirates, in turn, claimed that the parody was fair use.\n\nThe initial decision by Judge Wollenberg in the California District Court, delivered on July 7, 1972, went against the Air Pirates, and O'Neill's lawyers appealed to the United States Court of Appeals for the Ninth Circuit. O'Neill suggested the other Pirates settle, and leave him to defend the case alone. Hallgren and Turner settled with Disney, but London and Richards decided to continue fighting. To raise money for the Air Pirates Defense Fund, O'Neill and other underground cartoonists sold original artwork — predominantly of Disney characters — at comic book conventions.\n\nDuring the legal proceedings and in violation of the temporary restraining order, the Air Pirates published some of the material intended for the third issue of \"Air Pirates Funnies\" in the comic \"The Tortoise and the Hare\" (Last Gasp, 1971), of which nearly 10,000 issues were soon confiscated under a court order. In 1975, Disney won a $200,000 preliminary judgement and another restraining order, which O'Neill defied by continuing to draw Disney parodies.\n\nThe case dragged on for several years. Finally, in 1978, the Ninth Circuit ruled against the Air Pirates 3-0 for copyright infringement, although they dismissed the trademark infringement claims. In 1979 the Supreme Court refused to hear an appeal. O'Neill later claimed that his plan in the Disney lawsuit was to lose, appeal, lose again, continue drawing his parodies, and eventually to force the courts to either allow him to continue or send him to jail.\n\nO'Neill's four-page Mickey Mouse story \"Communiqué #1 from the M.L.F.\" (Mouse Liberation Front) appeared in the magazine \"CoEvolution Quarterly\" #21 in 1979. Disney asked the court to hold O'Neill in contempt of court and have him prosecuted criminally, along with Stewart Brand, publisher of \"CoEvolution Quarterly\". By mid-1979, O'Neill recruited diverse artists for a \"secret\" artist's organization, The Mouse Liberation Front. An M.L.F. art show was displayed in New York City, Philadelphia and San Diego. With the help of sympathetic Disney employees, O'Neill delivered \"The M.L.F. Communiqué #2\" in person to the Disney studios, where he posed drawing Mickey Mouse at an animation table and allegedly smoked a joint in Walt Disney's office.\n\nIn 1980, weighing the unrecoverable $190,000 in damages and $2,000,000 in legal fees against O'Neill's continuing disregard for the court's decisions, Disney settled the case, dropping the contempt charges and promising not to enforce the judgment as long as the Pirates no longer infringed Disney's copyrights.\n\nIn Bob Levin's 2003 book \"The Pirates and The Mouse: Disney's War Against the Counterculture\", New York Law School professor Edward Samuels said, \"I was flabbergasted. He told me he had won the case. 'No, Dan,' I told him, 'You lost.' 'No, I won.' 'No, you lost.' To Dan O'Neill, not going to jail constituted victory.\" However, Samuels said of the Air Pirates, \"They set parody back twenty years.\"\n\n\nDuring the height of the Air Pirates \"moment\" (1971–1973), members of the collective were featured in other solo titles or anthologies:\n\n\n\n"}
{"id": "31604229", "url": "https://en.wikipedia.org/wiki?curid=31604229", "title": "Ancillary market", "text": "Ancillary market\n\nAncillary markets are non-theatrical markets for feature films, like home video, television, Pay Per View, VOD, Internet streaming, airlines and others.\n\nBefore television, studios played their films in theaters exclusively. However, in 1950 many studios began to sell all their pre-1948 features to television syndicators. The television syndicators then would use these films to fill in their programming schedules. Back in the 1960s, the first domestic ancillary market for feature films was created. NBC was the first to practice the market on September 23, 1961 by programming \"NBC Saturday Night at the Movies.\" After such success, ABC became the second network ever to program a series of prime time features in 1962. One of the other networks, CBS, followed and added a prime time feature program in 1965.\n\nToday, feature films opens in motion picture theaters to establish its box-office value. After that is established, it is then released to ancillary markets in a particular order as follows:\n\n\nThe sequence is to maximize the full economic potential of each market.\n\nHome video recorders were made public when Sony introduced the half-inch Betamax cassette in 1975. Following Betamax, the company JVC introduced the Video Home System (VHS). Marketed by RCA and manufactured by Matsushita, VHS soon became known as the video-cassette recorders (VCRs). VCRs, which gave the consumer the option of recording programs from television, were a new form of competition in the demanding consumer market. VCRs revenue contributed to the development of the ancillary market of video and DVD. By the 1980s, five million households owned VCRs. Major studios had not yet adapted to the new video technologies that were being developed for consumers. There were no anticipations of new markets or other opportunities to expand until an entrepreneur, Andre Blay, opened Hollywood film companies’ eyes. Blay wanted the license to transfer and sell their films on tape. After he succeeded and his approach was beneficial, film companies all around became a part of the video distribution. The film companies could not deny the fact that this new distribution would lead to a new revenue stream.\n\nAs the VHS market saturated, multiple media executives and manufacturers liked the idea of utilizing other home video technologies. In 1993, the film industry upgraded their technology with the creation of several new formats including the DVD, or digital video disc. Many manufacturers such as the Japanese (Hitachi, JVC, Matsushita, Mitsubishi, Pioneer, Sony and Toshiba) and the European (Philips and Thomson) collaborated to facilitate development of the DVD Forum. In March 1997, the US launch of the DVD systems went smoothly due to Hollywood's solidarity. Manufacturers and film studios decided to avoid making the same mistake of the VHS format battles and agreed upon a universal standard of cooperation. When first introduced in 1997, DVDs sold at the low price of $20, for which they offered high-quality image and extra special features. Consumers liked the advantages of the DVDs and soon surpassed VHS sales.\n\nThe first premium rate television services were Phonevision, Telemeter, SubscriberVision, among others. None of them were successful, until the launch of Home Box Office (HBO) in 1972, considered the first successful premium-rate subscription television service. Other services were launched: Z Channel, Showtime, The Movie Channel, Cinemax, Spotlight and Home Theater Network. Only Showtime, The Movie Channel and Cinemax survived through the 1980s. These premium rate services air features unedited, uncut, and commercial-free, the same way they were shown on theaters and/or home video.\n\nOut of the many ancillary markets out there, none were more effective and revenue rewarding than network television, and eventually syndication began. Amanda D. Lotz spoke on the radical change in her book titled \"Television Will Be Revolutionized (2008).\"\nShe mentions how time went on and as the post-network era developed, that the limited ways there were for medium to be distributed was eliminated. What was expanding, and evolving was network television as a whole.<Lotz 85> At first channels were few, and very limited. But soon came the multi-channel transition, then on with the creation of cable TV. As that later expended cable began to see the 'cash cow' in that and began selling shares to different networks in order to air their studio programs. Not only did Network Television open up ancillary markets for TV, but other markets as well. The VCR then became a hot commodity, because the consumer wanted the option of recording. That slowly transitioned to the DVD that took out the VCR, and finally DVR. Which seems to be just that, a VCR and a DVD combined. Network television took a gigantic step when it later allowed programs to have different showing dates, and even multiple air times. This offered different networks to still be able to ear revenue off of an older film that already has left the box office.\n\nAccording to McDonald and Wasko, Hollywood's interest in emerging medium of television dates from the 1920s and includes experiments with the developing technology, an alternative model of television as home theater, applications for television frequencies, as well as investments in broadcasting companies that were exploring television (Anderson, 1994; Hilmes, 1990; Wasko, 1994). The evidence suggests that studios clearly wanted to control the development and implementation of television technology. Securing that control required the assistance of the Federal Communication Commission, which regulated television's development by articulating technical standards and operating rules and through the FCC's exclusive right to license the use of television frequencies for experimentation or broadcast (McDonald and Wasko, p. 107).\n\nVideo games are a rapidly growing ancillary market for feature films. Video game over-all gross income has consistently surpassed that of movie sales since 2004. Because of the increased interest in video games many major media and communication companies have begun to show interest in video games as a way to market, brand, and advertise for their product. Some movies that have benefited from this are the \"Harry Potter\", \"Lord of the Rings\", and \"James Bond\" series. Because the video game industry has been able to outplay and outgross many movies, video game manufacturers are beginning to look in the direction of producing their own movies as well. Microsoft's producers of \"Halo\" have decided to pass Hollywood studios and produce their own movie.\n\nHollywood has shown interest in the video game industry as an ancillary market almost as far as video games have existed. The film industry's goal within the video game industry is to either seek control and therefore ownership of the video game market or to license products to video game producers. Once the video game profits are high, that is when Hollywood seeks control. On the other hand, when profits are down, that is when Hollywood offers the licensing. The collaboration had always been problematic since the film industry had been uncertain about the development of video games. Hollywood has seen video games as another promotional scheme for a film.\n\nHollywood has had interactions with the recorded music industry that dates back as far as the studio system itself. The interactions between the two include three main periods; 1927-1957: recorded music as a form of promotion, 1957-1977: recorded music as cross-promotion and ancillary revenue, and 1977-1997: recorded music as cross-promotion, ancillary revenue stream, and means of spreading risk.\nIn addition to music's promotional value, studios realized that, \"A chart success was an effective way of generating additional revenue for their companies, both in terms of publishing and performance royalties and, of course, in outright sales.\" Recorded music proved to be of great value as an ancillary market to the film industry.\n\n"}
{"id": "8150272", "url": "https://en.wikipedia.org/wiki?curid=8150272", "title": "Antisemitica", "text": "Antisemitica\n\nAntisemitica designates, in the fields of book collecting, and rare book dealing, the collection and distribution of books, pamphlets, serials, posters, and other printed literature, of an antisemitic nature. It is to be noted that antisemitica does not, generally, designate antisemitic activity, or antisemites themselves. In the United States, the freedom of the press does not limit the publication or distribution of antisemitic literature, and there are scholarly and historical interests in such material.\n\n\n"}
{"id": "3151382", "url": "https://en.wikipedia.org/wiki?curid=3151382", "title": "Archaeoastronomy and Stonehenge", "text": "Archaeoastronomy and Stonehenge\n\nThe prehistoric monument of Stonehenge has long been studied for its possible connections with ancient astronomy. The site is aligned in the direction of the sunrise of the summer solstice and the sunset of the winter solstice. Archaeoastronomers have made a range of further claims about the site’s connection to astronomy, its meaning, and its use.\n\nStonehenge has an opening in the henge earthwork facing northeast, and suggestions that particular significance was placed by its builders on the solstice and equinox points have followed. For example, the summer solstice sun rose close to the Heel Stone, and the sun’s first rays shone into the centre of the monument between the horseshoe arrangement. While it is possible that such an alignment could be coincidental, this astronomical orientation had been acknowledged since William Stukeley drew the site and first identified its axis along the midsummer sunrise in 1720.\n\nStukeley noticed that the Heel Stone was not precisely aligned on the sunrise. The drifting of the position of the sunrise due to the change in the obliquity of the ecliptic since the monument’s erection does not account for this imprecision. Recently, evidence has been found for a neighbour to the Heel Stone, no longer extant. The second stone may have instead been one side of a ‘solar corridor’ used to frame the sunrise.\n\nStukeley and the renowned astronomer Edmund Halley attempted what amounted to the first scientific attempt to date a prehistoric monument. Stukeley concluded the Stonehenge had been set up “by the use of a magnetic compass to lay out the works, the needle varying so much, at that time, from true north.” He attempted to calculate the change in magnetic variation between the observed and theoretical (ideal) Stonehenge sunrise, which he imagined would relate to the date of construction. Their calculations returned three dates, the earliest of which, 460 BC, was accepted by Stukeley. That was incorrect, but this early exercise in dating is a landmark in field archaeology.\n\nEarly efforts to date Stonehenge exploited changes in astronomical declinations and led to efforts such as H. Broome’s 1864 theory that the monument was built in 977 BC, when the star Sirius would have risen over Stonehenge’s Avenue. Sir Norman Lockyer proposed a date of 1680 BC based entirely on an incorrect sunrise azimuth for the Avenue, aligning it on a nearby Ordnance Survey trig point, a modern feature. Petrie preferred a later date of 730 AD. The relevant stones were leaning considerably during his survey, and it was not considered accurate.\n\nAn archaeoastronomy debate was triggered by the 1963 publication of \"Stonehenge Decoded\", by Gerald Hawkins an American astronomer. Hawkins claimed to observe numerous alignments, both lunar and solar. He argued that Stonehenge could have been used to predict eclipses. Hawkins’ book received wide publicity, in part because he used a computer in his calculations, then a novelty. Archaeologists were suspicious in the face of further contributions to the debate coming from British astronomer C. A. ‘Peter’ Newham and Sir Fred Hoyle, the famous Cambridge cosmologist, as well as by Alexander Thom, a retired professor of engineering, who had been studying stone circles for more than 20 years. Their theories have faced criticism in recent decades from Richard J. C. Atkinson and others who have suggested impracticalities in the ‘Stone Age calculator’ interpretation.\n\nGerald Hawkins’ work on Stonehenge was first published in \"Nature\" in 1963 following analyses he had carried out using the Harvard-Smithsonian IBM computer. Hawkins found not one or two alignments but dozens. He had studied 165 significant features at the monument and used the computer to check every alignment between them against every rising and setting point for the sun, moon, planets, and bright stars in the positions they would have been in 1500 BCE. Thirteen solar and eleven lunar correlations were very precise in relation to the early features at the site but precision was less for later features of the monument. Hawkins also proposed a method for using the Aubrey holes to predict lunar eclipses by moving markers from hole to hole. In 1965 Hawkins and J.B. White wrote \"Stonehenge Decoded\", which detailed his findings and proposed that the monument was a ‘Neolithic computer’.\n\nAtkinson replied with his article “Moonshine on Stonehenge” in \"Antiquity\" in 1966, pointing out that some of the pits which Hawkins had used for his sight lines were more likely to have been natural depressions, and that he had allowed a margin of error of up to 2 degrees in his alignments. Atkinson found that the probability of so many alignments being visible from 165 points to be close to 0.5 (or rather 50:50) rather that the “one in a million” possibility which Hawkins had claimed. That the Station Stones stood on top of the earlier Aubrey Holes meant that many of Hawkins’ alignments between the two features were illusory. The same article by Atkinson contains further criticisms of the interpretation of Aubrey Holes as astronomical markers, and of Fred Hoyle’s work.\n\nA question exists over whether the English climate would have permitted accurate observation of astronomical events. Modern researchers were looking for alignments with phenomena they already knew existed; the prehistoric users of the site did not have this advantage.\n\nIn 1966, C. A. ‘Peter’ Newham described an alignment for the equinoxes by drawing a line between one of the Station Stones with a posthole next to the Heel Stone. He also identified a lunar alignment; the long sides of the rectangle created by the four station stones matched the moon rise and moonset at the major standstill. Newham also suggested that the postholes near the entrance were used for observing the saros cycle.\n\nTwo of the Station Stones are damaged and although their positions would create an approximate rectangle, their date and thus their relationship with the other features at the site is uncertain. Stonehenge’s latitude ( 51° 10′ 44″ N ) is unusual in that only at this approximate latitude (within about 50 km) do the lunar and solar alignments mentioned above occur at right angles to one another. More than 50 km north or south of the latitude of Stonehenge, the station stones could not be set out as a rectangle.\n\nAlexander Thom had been examining stone circles since the 1950s in search of astronomical alignments and the megalithic yard. It was not until 1973 that he turned his attention to Stonehenge. Thom chose to ignore alignments between features within the monument, considering them to be too close together to be reliable. He looked for landscape features that could have marked lunar and solar events. However, one of Thom’s key sites – Peter’s Mound – turned out to be a twentieth-century rubbish dump.\n\nAlthough Stonehenge has become an increasingly popular destination during the summer solstice, with 20,000 people visiting in 2005, scholars have developed growing evidence that indicates prehistoric people visited the site only during the winter solstice. The only megalithic monuments in the British Isles to contain a clear, compelling solar alignment are Newgrange and Maeshowe, which both famously face the winter solstice sunrise.\n\nThe most recent evidence supporting the theory of winter visits includes bones and teeth from pigs which were slaughtered at nearby Durrington Walls. Their age at death indicating that they were slaughtered either in December or January every year. Mike Parker Pearson of the University of Sheffield has said, “We have no evidence that anyone was in the landscape in summer.”\n\n\n"}
{"id": "18452628", "url": "https://en.wikipedia.org/wiki?curid=18452628", "title": "Arts of Mankind", "text": "Arts of Mankind\n\nThe Arts of Mankind (in French \"L’Univers des formes\"), an ambitious series of art history survey books founded in 1960 for the French publisher Gallimard by André Malraux, who edited many of the volumes in collaboration with art historian Georges Salles. Over 40 volumes have appeared to date; roughly half have been translated into English, as follows:\n\n"}
{"id": "1851046", "url": "https://en.wikipedia.org/wiki?curid=1851046", "title": "Cartagena Manifesto", "text": "Cartagena Manifesto\n\nThe Cartagena Manifesto was written by Simón Bolívar during the Colombian and Venezuelan War of Independence, after the fall of the First Republic, explaining with great detail and precision what he believed to be the causes of this loss. It was written in Cartagena de Indias, on 15 December 1812. This is the first of Bolívar's public documents, which due to his later fame as the \"Liberator of five nations,\" have become quite well known. The document contained the conceptual framework of his new agenda, which he then acted out in the field.\n\nPrior to the document's publication, Bolívar had been an officer in the Venezuelan army. The First Republic, however, was defeated due to a number of movements that confronted and exploited each other such as the royalists who fought for the old order, the supporters of independence who fought for creole supremacy, and the \"pardos\", blacks, and slaves who fought for their liberation. The conflict was exacerbated by a number of factors such as the shortage of provisions and the effects of the 1812 Caracas earthquake, among others. Bolivar started acting on his own, leaving La Guaira on a Spanish ship. He briefly stayed in Curacao before finally arriving in Cartagena. he accepted a commission in the army of the United Provinces of New Granada (Colombia), which later granted him permission to lead a force to free Venezuela, in what became known as the Admirable Campaign.\n\nIn Cartagena Manifesto, Bolivar outlined a framework that would prevent New Granada from suffering the fate of Venezuela since the territory reproduced the prevailing pattern of colonial dissent from loyal juntas to independent governments. The political, economic, social, and natural causes which Bolívar mentioned included:\n\n\nBolivar advocated a strong central government and powerful executive to avoid infighting between city, state, and national authorities, which in his view created stalemate, dissipating funds and energy. He stated that \"the government must necessarily adjust itself, so to speak, to the context of the times, men, and circumstances in which it operates. If these are prosperous and serene, it has to be gentle and protective, but if they are calamitous and turbulent, it has to be severe and armed with a strength equal to the dangers.\" \n\n"}
{"id": "38612139", "url": "https://en.wikipedia.org/wiki?curid=38612139", "title": "Church reordering", "text": "Church reordering\n\nChurch reordering refers to the rearrangement and adaption of churches to accommodate changes in religious practice. More recently it has been used to describe the introduction of secular uses in under-used places of worship, while retaining their primary purpose as places of worship. \n\nTwenty-first century church church re-ordering involves the transformation of under-used places of worship by the introduction of secular uses, while retaining their primary purpose as places of worship. use the term to describe the latter.\n\nChurch conversions, where large under-used religious buildings have been transformed into other uses, are nothing new. The impressive Lincoln College Library in Oxford (formerly the city's baroque church of All Saints), the spacious OAP's Day Centre contained within the shell of York's Church of St Sampson, and the world famous music performance space at St John's, Smith Square in Westminster, are all English exemplars which pre-date the reordering movement.\n\nIn 1977, London's Victoria and Albert Museum mounted a major exhibition entitled 'Change or Decay', curated by its then Director Sir Roy Strong. Its central theme was that Britain had too many places of worship and that, because of their architectural significance, many needed to be saved from closure and demolition. An accompanying book, by Marcus Binney (co-founder of the conservation pressure group SAVE Britain's Heritage) and the architect Peter Burnham, illustrated the plight of many of these threatened church buildings, in an era when attrition was severe. Between 1970 and 2004 the CofE closed 1,630 of its churches; some 85 of these, which were listed buildings, were subsequently demolished.\n\nThe national attendance records maintained by the Church of England (CofE) show that fewer than 2% of the population are regular churchgoers. Church reordering advocates believe that broader community-based uses of under-used churches could turn the tide.\n\nThough the technique of church reordering has been embraced by many faiths, it is the Anglican Communion which leads the way. In England, there are just over 16,000 'active' church buildings, with 14,500 of them owned by the CofE, which as of 2013 spends £100M annually on their maintenance. Some 12,000 of these churches are 'listed' by English Heritage as being of architectural significance, with around 4,000 now classified as being Grade I (the highest category).\nOther European countries have tackled the problem of what to do with their unwanted places of worship rather more radically. In the Dutch town of Arnhem, the former church of St Joseph is now an indoor skatepark, while in Maastricht the city's 13th century has been converted (by the Amsterdam-based architects Merckx + Girod) into the award-winning Selexyz bookstore.\n\nDeconsecration - where a church building's religious use is officially removed - is a drastic step which many faiths are reluctant to take. In the five years 2007-12, 123 CofE church buildings were deconsecrated and put to other uses, residential being the largest (22%). A further 29 were demolished and six were transferred to the Churches Conservation Trust (CCT) for repair and preservation. Founded in 1968, the CCT currently cares for 341 English churches. Its reordering schemes range from the cultural centre in the King's Lynn, Norfolk church of St Nicholas, to Circomedia's award-winning training school for aspiring circus trapeze artists in the Bristol church of St Paul's.\n\nIn general, the Roman Catholic hierarchy is opposed to 'mixed-use' conversions, preferring to see its under-used consecrated buildings closed rather than becoming involved with non-worship activities. Nevertheless, several churches in Quebec have moved to mixed uses. Montreal's Church of the Gesù, while still an active church, is best known as a cultural centre and concert hall, while in rural Quebec, several churches have been partially repurposed, for example by converting the nave into community space but preserving the chancel behind a movable wall which can be opened for services. \n\nRichard Giles' \"Re-pitching the tent\" is now widely regarded as the definitive guide to reordering, while the American historian Katherine French's study of community life in medieval England (\"The People of the Parish\") reveals that activities other than conventional worship would frequently be encountered in many town churches. People might met to trade, complete business deals or consult lawyers. Scriveners were usually on hand, on the lookout for work. It is a return to this medieval concept of the church being 'community-based' which is at the heart of the reordering movement.\n\nIn Britain, national awards schemes and conferences have helped to promote the reordering movement. In 2008, the City of London's Mercers Livery Company launched its 'Village Church for Village Life' award, with a first prize of £10,000. The winner was the parish church of St Phillip & St James at Newton St Philip in Somerset. In October 2012, at Lambeth Palace (the London residence of the CofE's Leader, the Archbishop of Canterbury), a national conference entitled 'New work in churches' also focussed on church reordering.\n\nOne of the most successful church reordering schemes in England is the award-winning conversion of All Saints in the West Midlands cathedral city of Hereford, completed in 1999 and designed by local architect Rod Robinson. All Saints is the oldest parish church in Hereford (parts of its interior date back to the late-12th century), yet in 1991 it came perilously close to being closed and deconsecrated. The solution to the dwindling congregations and spiralling maintenance costs was to 'insert' a café at the west end of the nave, linked to a new mezzanine eating area 'suspended' above the south arcade, and to sacrifice the church's South Chapel, which is now available for commercial lettings such as arts and crafts exhibitions. Both the central nave and chancel, as well as the Lady Chapel, are retained as areas of worship. A new vestry, formed from curved sheets of frosted glass, completes the transformation. An indication of the success of this radical reordering scheme, which cost £1.7M, is that around 2,000 people pass through Café @ All Saints every week, and church worship attendances have quadrupled.\n\nThe Diocese of Hereford has played a leading role in propagating the ideas of church reordering right across Herefordshire, where 90% of the county's 425 churches are listed. A dedicated team within the Diocese Office has overseen or advised on 40 church reordering exercises, several of which have won national architectural awards. Among these are a community centre (shared by five neighbouring parishes) in the 12th-century church of St Andrew's at Bridge Sollers and St Peter's at Peterchurch, where a village lending library, a children's play area and a space for music recitals have all been seamlessly woven into the fabric of this little Norman church. The latter project won the prestigious national ACE/RIBA design award in 2012. Both conversions are the work of the Herefordshire practice Communion Design.\n\n\n"}
{"id": "20328201", "url": "https://en.wikipedia.org/wiki?curid=20328201", "title": "Confession album", "text": "Confession album\n\nThe confession album, or confession book, was a kind of autograph book popular in late nineteenth century Britain. Instead of leaving free room for invented or remembered poetry, it provided a formulaic catechism. The genre died out towards the end of the century, with occasional brief revivals in the twentieth century. The same kind of form is now found in the Dutch \"vriendenboek\" (\"friends book\") and German \"Freundschaftsbuch\" (\"friendship book\"), used by small children; and the questions that the confession album contained live on in the Proust Questionnaire often used for celebrity interviews.\n\nThe questions posed in a confession album varied from volume to volume. A typical set of questions, in a book from the 1860s, is given below along with the answers of one respondent (see picture, right). The same set of questions were presented to Queen Victoria's second son, Alfred, Duke of Saxe-Coburg and Gotha in 1873, to Marcel Proust around 1885 and to Claude Debussy in 1889.\n\nAmong the questions found in other albums, several seem designed to help courtship, sometimes with a reflection of changing relations between the sexes, for instance \"What is your opinion of the girl of the period?\", \" What is your opinion of the man of the period?\"\n\nThe origins of the confession album are unclear. Samantha Matthews notes the similarity to oracle games of the first half of the nineteenth century (she cites examples from 1810 to 1852). These interactive books, which bore titles such as \"The Young Lady's Oracle: A Fireside Amusement\", included questions that resemble those of the confession albums (for instance \"Which is your favourite flower?\", \"Which is your favourite historical character?\"), including sexually distinguished questions appropriate to courtship (for instance, \"What is the character of your lady love?\", \"What is the character of him you love?\").\n\nWhatever their origins, confession albums were an established form by the 1860s: Henry d'Ideville records using an album in 1861 (see Germany and France below); Karl Marx filled out answers to one in the spring of 1865 (his favourite colour was red); and Friedrich Engels answered another in 1868 (his idea of happiness was Château Margaux 1848). Early albums often had blank pages into which owners would paste the questionnaires; the questions on these could be preprinted or handwritten. One such album with handwritten questions, was that of Karl Marx's daughter Jenny, which contains entries dated from 1865 to 1870. For some of these, she sent the questions to friends, asking them to fill them out and return them. Her comments in a letter of November 1865 suggest that she regarded the genre as a novelty:\nI have a whole book filled up in that way and the answers are very amusing when compared with one another. These Confession books have put albums and stamp books quite into the shade. … I should much like to have answers. It is more interesting and amusing than a mere Autograph.\nBy the end of the decade, the printed and bound confession book had been introduced. The earliest currently known example with a printed publication date is \"Mental Photographs\", an album published in New York in 1869, which contained place for a photograph as well as the set of questions (a combination already found in Jenny Marx's album). The albums seem to have enjoyed their greatest popularity in the following decades, but to have become unfashionable by the early years of the twentieth century. At their height, confession albums were widespread enough that in 1883 Douglas Sladen could rely on readers' familiarity with the form to play on it with a poetical answer. He begins:\nMy favorite virtue, I confess, is chivalrous devotedness,\nMy favorite quality in man, the manful genius that can\nWith iron will and eye sublime, up to the heights of empire climb;\nAlthough in woman, as I think, gentleness is perfection's pink\n\nThis ubiquity was accompanied by a certain exasperation. In a novel of 1886, a character asks: \"\"A propos\" of the confessional, did any of you ever come under the torture of that modern Inquisition, the 'Confession Book?'\" Early twentieth century writers look back on the books as a long unfashionable genre. A writer in 1915 records:\nSo far as I can recollect it was voted a bore at the end of the 'seventies and, except in suburban homes, such as my own, was never referred to except with a yawn or a smile after the early eighties.\nThe \"Daily Chronicle\" in 1906 remarks: \"'If not yourself, who would you rather be?' was a favourite question of the confession album of the seventies.\" A. A. Milne (1882–1956) looked back in 1921 on the album as something from his childhood:\n\nThe confession-book, I suppose, has disappeared. It is twenty years since I have seen one. As a boy I told some inquisitive owner what was my favourite food (porridge, I fancy), my favourite hero in real life and in fiction, my favourite virtue in woman, and so forth.\nSome album producers saw World War I as an opportunity to revive the confession book. The new albums, unlike their Victorian predecessors, ignored questions appropriate to courtship. They were clearly intended to be filled out by soldiers, a point spelt out in the title of one: \"My Brave Friend's Confession Book\" (1915).\n\nAccording to Henry d'Ideville, he had an \"album-questionnaire\" in which he collected the responses of friends and acquaintances in 1861, when as a diplomat in Naples he took the answers of Urbano Rattazzi and Mme de Solms, which he reproduces. His account, written in 1872, explains the nature of the album, as though he did not expect readers to be familiar with it:\nHere is what the operation consists of: you pose a series of questions to the unfortunate subject of the interrogation and he has to answer them one after the other. – What poet, what painter, what occupation, what pleasure, what sensation, etc. etc. do you prefer? The answers are written down, then the person signs and dates his interrogation and that's the game over.\nBritish confession albums also circulated on the continent (see Questions above), and (perhaps in imitation of them) there were also questionnaire books in other languages. In Germany there were several, notably \"Erkenne Dich Selbst!\" (\"Know Yourself\"), which first appeared in 1878 and survived to around 1900, going through at least 22 impressions, produced by Friedrich Kirchner (1848–1900). The \"Leipziger Illustrirte Zeitung\", which belonged to the same publishers as Kirchner's album, set his questions to celebrities, a forerunner of the modern use as the Proust Questionnaire; facsimiles of these answers were in turn printed in the album. In France, two are known, both containing the word \"Confidences\" in the title to represent the English \"confessions\". Here too the \"Revue illustrée\" set confession album questions to the famous, including Zola and Verlaine.\n\nIn the Netherlands, where autograph books are very popular among young schoolchildren, publishers have recently started bringing out an alternative in the so-called \"vriendenboek\" or \"vriendjesboek\" (\"Friends book\"), which much like the confession album come preprinted with a set of questions about the respondents' hobbies, idols and wishes. Some children prefer the new format, which says more about the person answering, but most prefer the traditional autograph book, which leaves more room for their own creativity. The modern German \"Freundschaftsbuch\" (\"Friendship book\") has the same kind of preprinted questions and is typically aimed at the same age group.\n\nIn 1886, Marcel Proust, then a child of fourteen, filled out the answers to an English confession album, which bore the title \"Confessions. An Album to Record Thoughts, Feelings, &c.\" (his answers were auctioned in 2003 for $34,000). Interest in Proust led to a later revival of the questions as a kind of formulaic interview for celebrities, first by Léonce Peillard in France in the 1950s, later in French and American television and in the magazine of the German \"Frankfurter Allgemeine Zeitung\", starting in 1980, and in \"Vanity Fair\" since 1993, under the name of \"The Proust Questionnaire\", which disguises its unintellectual British origins. This use too had been anticipated by nineteenth century journals (see Germany and France).\n\nA slam book is a notebook (commonly the spiral-bound type) which is passed among children and teenagers. The keeper of the book starts by posing a question (which may be on any subject) and the book is then passed round for each contributor to fill in their own answer to the question\n\n\n\n"}
{"id": "343323", "url": "https://en.wikipedia.org/wiki?curid=343323", "title": "Copy editing", "text": "Copy editing\n\nCopy editing (also copyediting, sometimes abbreviated ce) is the process of reviewing and correcting written material to improve accuracy, readability, and fitness for its purpose, and to ensure that it is free of error, omission, inconsistency, and repetition. In the context of publication in print, copy editing is done before typesetting and again before proofreading, the final step in the editorial cycle.\n\nIn the United States and Canada, an editor who does this work is called a \"copy editor\". An organization's highest-ranking copy editor, or the supervising editor of a group of copy editors, may be known as the \"copy chief\", \"copy desk chief\", or \"news editor\". In book publishing in the United Kingdom and other parts of the world that follow British nomenclature, the term \"copy editor\" is used, but in newspaper and magazine publishing, the term is subeditor (or sub-editor), commonly shortened to \"sub\". The senior subeditor of a publication is frequently called the \"chief subeditor\". As the prefix \"sub\" suggests, copy editors typically have less authority than regular editors.\n\nIn the context of the Internet, online copy refers to the text content of web pages. Similar to print, online copy editing is the process of revising the raw or draft text of web pages and reworking it to make it ready for publication.\n\nCopy editing has three levels: light, medium, and heavy. Depending on the budget and scheduling of the publication, the publisher will let the copy editor know what level of editing to employ. The type of editing one chooses (light, medium, or heavy) will help the copy editor prioritize their efforts.\n\nWithin copy editing, there is mechanical editing and substantive editing: mechanical editing is the process of making a text or manuscript follow editorial or house style, keeping the preferred style and grammar rules of publication consistent across all content. It refers to editing in terms of spelling, punctuation, and correct usage of grammatical symbols, along with reviewing special elements like tables, charts, formatting footnotes, and endnotes. Content editing, also known as substantive editing, is the editing of material, including its structure and organization, to correct internal inconsistencies and discrepancies. Content editing may require heavy editing or rewriting as compared to mechanical editing.\n\nIn addition, copy editing may change punctuation, spelling and usage for a different country. For a Commonwealth readership, the American spelling of \"organize\" may be changed to \"organise\", and \"color\" changed to \"colour\".\n\nMechanical editing is the process of proofreading a piece of writing for consistency, either internally or in accordance with the publisher's house style. According to Einsohn, mechanical editors work with such things as the following:\n\nGilad also mentions the following:\n\nProper spelling and punctuation are subjective in some cases, where they must be left to the discretion of the copyeditor or the publisher. Most publishing firms use a widely recognized style manual such as \"The Chicago Manual of Style\" or \"The Associated Press Stylebook\". Companies that produce documents and reports but do not consider themselves publishers in the usual sense, tend to rely on in-house style guides or on the judgment of the copyeditor.\n\nThe goal of the copyeditor is to enforce inviolable rules while respecting personal stylistic preferences. This can be difficult, as some writers view grammatical corrections of the copyedited manuscript as a challenge to their intellectual ability or professional identity. For this reason, copy editors are encouraged to side with the author. If the author's preference is acceptable, it should be respected. This practice is complicated further by constantly evolving language conventions as recorded by books on grammar and usage. Additionally, the authors of such books often disagree.\n\nContent editing consists of reorganizing or restructuring the content of a document. This involves any inconsistent parts of the content as well as any variances. Copyeditors can either fix the content by rewriting it or heavily editing it. However, the copyeditor will often point out any difficult passages for the author to resolve on his or her own time.\n\nAlthough copyeditors are not responsible for factual correctness of the document, they can provide comments for the author on any information they know to be incorrect, such as year discrepancies or misleading ideas. This type of fact checking is acceptable for copyeditors that know the document's subject matter.\n\nThe copyeditor must also point out any biased language without infringing on the author's meaning. This includes material \"that might form the basis for a lawsuit alleging libel, invasion of privacy, or obscenity\". Some see censoring biased language as political correctness, so it is important the copyeditor distinguishes between the two. To do this, the copyeditor will permit intentional \"politically incorrect\" views and censor only marginalized, offensive, or exclusive language.\n\nMost manuscripts will require the copyeditor to correlate the parts within it. Copyeditors must carry out the following tasks in this process:\nSome manuscripts may require special cross-checking. For example, in a how-to text, a copyeditor might need to verify that the list of equipment or parts matches the instructions given within the text.\nTypecoding is the process of identifying which sections of the manuscript are not regular running text. These portions of text, known as elements, include the following:\n\nIt is the copyeditor's job to typecode (or make note of) all manuscript elements for the publication designer. Hard copy copyeditors are usually asked to pencil in the typecodes in the left margin of the manuscript. On-screen copyeditors may be asked to insert typecodes at the beginning and end of each element.\n\nFinally, if the manuscript contains long quotations from a published work that is still under copyright, the copyeditor should remind the author to acquire permission to reprint said quotations. The same goes for the reprinting of tables, charts, graphs, and illustrations that have appeared in print. Rules vary for the reproduction of unpublished materials (letters, diaries, etc.)\n\nThere are several basic procedures that every copyeditor must follow: copyeditors need a system for marking changes to the author's text (marking), a process for querying the author and the editorial coordinator (querying), a method for keeping track of editorial decisions (recordkeeping), and procedures for incorporating the author's review of the copyediting into a final manuscript or electronic files (cleanup). These systems were originally developed in an era before that of the computer, but over time these procedures were adapted to exist in a digital on-screen space.\n\nEach medium (in print and on screen) has its own affordances, and although a copyeditor may prefer one editing process over the other, copyeditors are practically required to use both techniques.\n\nTraditional markup copy editing, or hard-copy editing, is still important because screening tests for employment are administered in hard copy. Also, the author whose text the copy editor is editing may prefer hard-copy markup, and copy editors need to know traditional markup in case documents and materials cannot be exchanged electronically. When editing in hard-copy, all participating parties (the editor, author, typesetter, and proofreader) must understand the marks the copy editor makes, and therefore a universal marking system that signifies these changes exists. This is also why the copy editor should write legibly and neatly. Copy editors working hard-copy write their corrections in the text directly, leaving the margins for querying. Usually the copy editor is asked to write in a bright color, so the author and other parties can easily recognize the editor's changes.\n\nEvery year, more editing projects are being done on computer and fewer in print. Also, if there is a digital version of a text the copyeditor is editing, they can more easily search words, run spellcheckers, and generate clean copies of messy pages. The first thing copyeditors must do when editing on-screen is to copy the author's files, as the original document must be preserved. Each word processing program provides various options for how an editor's markups are shown on screen and on the printout. On-screen editing mainly differs from hard-copy editing in the fact that the copyeditor should edit more cleanly on-screen, refraining from saving parts of words, and be careful in maintaining proper line spacing.\n\nCopyeditors often need to query their authors in order to address questions, comments, or explanations: most of these can be done in the margins of the text, or the comment section when on-screen. The copyeditor must consider when to query and the length and tone of their queries, as querying too frequently or infrequently, cryptically, or sarcastically can result in a negative relationship between the copyeditor and the author.\n\nDepending on which publication a copyeditor is employed with, his or her goals may change, however there are a few constituencies that must always be served – the author (the person who wrote or compiled the manuscript), the publisher (the person or company that is paying to produce the material), and the readers (the audience for whom the material is being produced). These parties (in conjunction with the copyeditor) work to achieve the same goal, which is to produce an error free publication. The copyeditor strives to improve clarity, coherency, consistency, and correctness – otherwise known as the \"4 C's\". Each of these components serve the copyeditor's \"Cardinal C\", which is communication.\n\nThe advent of the printing press in the middle of the 15th century opened the doors to the first printing houses in Europe. Even after the invention of the printing press and on to today, the editor's job is to correct perceived mistakes. Within these printing houses, there were a variety of employees, one being correctors, or as it is referred to today, editors.\nThe biggest difference between monastic copyists and copyeditors is that copyeditors leave editions as suggestions that the original author can choose to reject. These printing houses established procedures for editing, preparing the text, and proofreading. Specialist correctors made sure texts were in accordance with the standards of the time.\nBefore the printing press, monastic copyists altered words or phrases they thought were odd, under the assumption that the copyist before them had made a mistake. This is what led to so much variety in standard texts like the Bible.\nAfter the globalization of the book from 1800 to 1970, the rise of American authors and editors came to fruition. One editor in particular, Maxwell Perkins, was sought out by writers such as Fitzgerald, Hemingway, and Wolfe because he greatly improved the work on these prominent authors with his editorial eye. Perkins was known editing, guiding, and befriending his writers – but the times were changing.\nIn the late 19th century, the role of an editor was to decide if a manuscript was good enough to be published. As time passed, the role of an editor and publisher became more distant. Although there was a newfound relationship between editors and authors, thoughtful editing did not end.\nCopyeditors were employed at various publishing houses, magazines, journals, and by private authors seeking revisions on their work. Some copyeditors were even employed by public relations and advertising firms who valued strong editing practices in their business.\nThe symbols used by copyeditors today are based on those that have been used by proofreaders since the beginnings of publishing, though they have undergone some changes over time. However, the exact beginnings of the copyediting language used today are unclear. Despite its long history, copyediting as a practice has not experienced any extreme upheaval other than the desktop publishing revolution of the 1980s. This phenomenon began as the result of a series of inventions that were released during the middle of this decade, and refers to the growth of technology usage in the field of copyediting. Namely, the development of the Macintosh computer, the desktop laser printer by Hewlett-Packard, and a software for desktop publishing called PageMaker created by Aldus (a company now under the control of Adobe) allowed the revolution to begin. By allowing both individuals and publishing agencies alike to cheaply and effectively begin to edit compositions entirely on-screen rather than by hand, desktop publishing revolution morphed copyediting into the practice it is today. Most copyeditors today rely on more modern WYSIWYG ('what you see is what you get') text processors such as Microsoft Word that are based on the original PageMaker to do their work.\nThere were a few events that led to changes within copyediting as a career. One of these, the successful strike of the editorial department of the Newark Ledger from November 17, 1934 to March 28, 1935, was \"the first major action of its kind by any local guild...[it] both confirmed the irreversibility of the guilds' movement away from the professional association idea and greatly accelerated that process\". Paired with another string of strikes led by The New York Newspaper Guild against a number of smaller newspapers in the summer of 1934, these actions served to shift the image of the editorial worker as a 'professional' to one as an average citizen. Another strike from the year 1934 was the strike at the Macaulay Company, reportedly the first ever strike to occur at a publishing firm. At the conclusion of the second Macaulay strike,which occurred three months after the first, the nationwide drive towards unionization had entered the publishing industry and was \"sweeping through all the major publishing houses\". As these events seemed to have the secondary result of lowering the status of editors across the various publishing fields, it could be said that they sparked the decline of copyeditors that can be seen across the publishing fields today.\nOwing to the rise of the Digital Age, the roles and responsibilities of a copyeditor have changed. For instance, beginning in 1990, copyeditors learned pagination electronically. They could now look at different pages of a text on multiple screens and easily edit on there, as opposed to pasting them by hand on a board. This technological advance also required that copyeditors learn new software such as Pagemaker, Quark Xpress, and now Adobe InDesign.\n\nModern copyeditors are often required to edit for digital as well as print versions of text. Digital copyediting requires copyeditors to understand RSS feeds, social media such as Twitter and Facebook, and Hyper Text Markup Language. What should be accounted for is that in this digital age, information is constantly being released which then leads to the decline in editing of the online versions. Editors of the website Buzzfeed commented that sometimes they \"simply can't get every post before it's published\".\nWhile copyeditors still do traditional tasks such as checking for facts, grammar, style, and writing headlines, some of their duties have been pushed aside to make way for technology. Some copyeditors now have to design page layouts and some even edit video content. Copyeditors are now sometimes referred to as \"copy/layout editors\" or \"producers/designers\".\n\nTraditionally, the copy editor would read a printed or written manuscript, manually marking it with editor's correction marks. At sizable newspapers, the main copy desk was often U-shaped; the copy desk chief sat in the \"slot\" (the center space of the U) and was known as the \"slot man\", while copy editors were arrayed around him or her on the outside of the U, known as the \"rim\". In the past, copy editors were sometimes known humorously as \"rim rats\". Chief copy editors are still sometimes called \"the slot\". But nowadays, the manuscript is more often read on a computer display and text corrections are entered directly.\n\nThe nearly universal adoption of computerized systems for editing and layout in newspapers and magazines has also led copy editors to become more involved in design and the technicalities of production. Technical knowledge is therefore sometimes considered as important as writing ability, though this is truer in journalism than it is in book publishing. Hank Glamann, co-founder of the American Copy Editors Society, made the following observation about ads for copy editor positions at American newspapers:\nWe want them to be skilled grammarians and wordsmiths and write bright and engaging headlines and must know Quark. But, often, when push comes to shove, we will let every single one of those requirements slide except the last one, because you have to know that in order to push the button at the appointed time.\n\nBesides an excellent command of language, copy-editors need broad general knowledge for spotting factual errors; good critical thinking skills in order to recognize inconsistencies or vagueness; interpersonal skills for dealing with writers, other editors and designers; attention to detail; and a sense of style. Also, they must establish priorities and balance a desire for perfection with the necessity to follow deadlines.\n\nMany copy editors have a college degree, often in journalism, the language the text is written in, or communications. In the United States, copy editing is often taught as a college journalism course, though its name varies. The courses often include news design and pagination.\n\nIn the United States, The Dow Jones Newspaper Fund sponsors internships that include two weeks of training. Also, the American Press Institute, the Poynter Institute, the University of North Carolina at Chapel Hill, UC San Diego Extension and conferences of the American Copy Editors Society offer mid-career training for newspaper copy editors and news editors (news copy desk supervisors).\n\nMost US newspapers and publishers give copy-editing job candidates an editing test or a tryout. These vary widely and can include general items such as acronyms, current events, math, punctuation, and skills such as the use of Associated Press style, headline writing, info graphics editing, and journalism ethics.\n\nIn both the US and the UK, there are no official bodies offering a single recognized qualification.\n\nIn the UK, several companies provide a range of courses unofficially recognized within the industry. Training may be on the job or through publishing courses, privately run seminars, or correspondence courses of the Society for Editors and Proofreaders. The National Council for the Training of Journalists also has a qualification for subeditors.\n\nBefore the digital era, copy-editors used to take a red pen to a piece of paper to point out errors and inconsistencies using a markup language made up of symbols universally known by copy-editors. The traditional copy editor was once defined as editing for grammar, spelling, punctuation and other mechanics of style.\nCopy editing symbols can no longer be used when editing digitally because they are not supported on digital platforms such as track changes. With more posting online and less printing on paper, this means current publishing processes are faster. Hard copy is no longer able to keep up with digital publishing. For a publisher to hire copy editors to print hard copy, make edits, and then make changes is no longer the most efficient process. The position of copy editors is at risk because time demands quicker results that can be done by automatic correction software that catches grammatical errors. Transferring the responsibility from human copy editors to digital software has been adopted by some publishing companies because it is available free of cost.\nProfessionals feared that the introduction of digital editing software would put an end to copyediting careers. Copy editors are still employed and needed for heavy editing, such as fact-checking and content organization, which software is not yet able to do. With grammar software and journalists that can edit, copy editors are seen as a luxury in publishing. The potential for a company to use editing software may also require the copy editor to only perform heavy editing and querying. Though the steps for copyediting are the same, the execution is what has been changed due to the introduction of digital environments.\n\nThe technological development of Cloud storage allows contemporary copy editors and writers to upload and share files across multiple devices. Online word processors such as Google Docs, Dropbox, Zoho, OpenGoo and Buzzword allow users to perform a number of tasks. Each processor has its advantages and disadvantages based on the users' preferences, but primarily allow users to share, edit and collaborate on documents. On Google Docs users can invite others via e-mail to view, comment and edit any file of their choosing. Those invited can view and edit the document together in real time. Unlike Google Docs whose files can only be shared through the web app, Dropbox shares from a desktop app. Dropbox users can share documents as links or as shared folders. Users can create shared folders and add others to the folder. Files in a shared folder will appear in the other user's Dropbox and all involved users receive notifications when edits are made to a file in the folder. Adobe's Buzzword allows users to share files, with the user's choice from varying levels of editing access, and includes a Version History feature which tracks changes made to documents and lets users revert to earlier versions. \nUseful in many word processors, a Track Changes feature allows users to make changes to a document and view them separately from the original document. In Microsoft Word users can choose whether to show or hide changes by clicking Track Changes under the Review ribbon. Those editing documents can leave comments by clicking wherever the user desires to leave a comment and clicking New Comment under the review ribbon or by highlighting text and clicking New Comment. Users can select the revision of specific users whom they have allowed to revise their work and choose which level of mark ups to view under the Show Markup dropdown menu in the Review ribbon. Users can also choose to accept or reject changes by clicking either Accept or Reject in the Review Ribbon.\n\nThe field of copy-editing is not obsolete. Teresa Schmedding, president of the American Copy Editors Society (ACES) and a deputy managing editor at the Daily Herald in Chicago, thinks that copyeditors are \"a natural fit\" for digital journalism and social media because though publishing has been made available to almost anyone, quality and credibility is brought to content only by copy editors. \nWhen editing a piece, copy editors now have to consider multimedia aspects of the story. The inclusion of video, images, SEO, and audio are just some of the components that are now created and included to digital publications by copy editors. Digital journalism has created many new roles for a copy editor, such as editing on the Web. Digital editing now requires copy editors to become familiar with search engine optimization, understanding HyperText Markup Language, Cascading Style Sheets, and RSS feeds. In addition to Web-based skills, contemporary copy editors must also obtain a larger skill set, having knowledge of and the ability to operate software such as Adobe Illustrator for generating graphics or Adobe Dreamweaver for designing web pages.\n\nOne of the problems with copy-editing is that it may slow the publication of text. With the digital publishing era came an increased demand for a fast turnover of information. Additional details such as color printing, page size, and layout are determined by the allotted budget. Web-based publications, such as BuzzFeed and Slate, do not have enough room in their budgets to keep a sufficient number of staff to edit their massive, daily rushes of content. Because of this, copy chief Emmy Favila says lower-priority posts are published without copy edits at Buzzfeed. Slate does not edit its blog posts before publication, but all of its news articles are copy edited before publication, say Slate copy chief Lowen Liu and deputy editor Julia Turner. \nIn response to such high demands for fast-produced content, some online publications have started publishing articles first and then editing later, a process known as back-editing. Editors prioritize stories to edit based on traffic and whether the content was originally reported for needing edits.\n\nReading material has become increasingly accessible to users with a wide range of disabilities. Carolyn Rude exemplifies such cases in alternatively replacing illustrations with text and audio translations for the visually impaired. Rude also suggests that web developers attempt to stick to print guidelines, such as \"clear and simple language and consistent terms and navigation devices\", especially when readers are looking at text in a second language.\n\nAs online resources rise in popularity, copy editors endeavor to meet the increase of digital consumerism to the best of their abilities, and such high competition has resulted in a gradually \"declining of quality in editing\", such as proofreading grammatical errors or fact checking. However, this doesn't mean the Internet has limited the scope of a copy editor's responsibilities or job opportunities. One of the most important advancements of the digital age is the advent of pagination, which gives copy editors more control over the construction and revisions of their content. Pagination is a convenient feature in programs such as \"Pagemaker, the Quark Xpress, and AdobeIndesign\". Despite the increasing number of programs, however, some copy editors believe their basic functions and duties haven't changed much. Other copy editors think the Internet has simplified the process of fact checking and online programs such as Facebook or Twitter have also expedited the process of information-gathering. Other digital skills, such as image selection and search-engine optimization, increase the visibility of search results, especially when searching for keywords in headlines.\n\nIn all likelihood, the Internet will continue to evolve, but this shouldn't hamper the overall importance of copy editing. Although it may be tempting to neglect proper revisions in favor of convenience, the credibility and quality of an editor's work should still be maintained, as there will always be updates in software and technology. As formats evolve, so too will the opportunities for journalists and other writers.\n\n\n"}
{"id": "4154464", "url": "https://en.wikipedia.org/wiki?curid=4154464", "title": "Dance squad", "text": "Dance squad\n\nA dance squad or dance team, sometimes called a pom squad or song team, is a team of participants that participates in competitive dance. In a routine, a squad will incorporate a specific dance style (i.e. hip hop, jazz, or lyrical), technical work (leaps, turns, kicks, splits, jumps), and, depending on the routine, pom-poms and/or cheers. A pom squad slightly differs from a regular dance squad in that it uses pom-poms in all its dance routines, whilst a regular dance squad may or may not do pom work in a dance routine. Dance teams are also popular in performance dance, especially at sporting events, most commonly performing during the pre-game and halftime periods (and, in a number of cases, on the sidelines) of football and basketball games.\n\nDance is a highly competitive activity. Youth/association, middle school, high school, collegiate, all-star, and professional teams, compete on local, regional, state, national, and international levels. Teams are judged on a number of criteria including form, team unison, showmanship, precision of motions, jumps, leaps, turns, choreography, enthusiasm, and, in the case of pom squads, visual use of poms-poms. Pom squads are like cheerleading or dance, but they use poms (pom-poms). Pom squads also use kicklines in their routines, after they set down their poms, or choose to hold them during the kickline. A kickline routine is a routine of kicks, which cheerleaders also use: high kicks, fan kicks, low kicks, and kicks that go to their waist. Pom squads regularly compete in competitions and perform at sporting events.\n\nDance squads emphasize precise, synchronized motions along with technical dance skills (such as jumps, turns, and leaps). Their routines encompass various styles of dance including the more usually incorporated hip hop, jazz, lyrical, and kickline styles, to the more unusually used styles like disco, rock and roll, and gospel. A key feature of the dance is the ability to change formations very smoothly.\n\nTraditional high school dance/pom squads include competition, performance dance, and promoting school spirit with dance. Dance/pom is usually a year-round sport, performing in competitions and at sporting events, most commonly football and basketball games. Some schools also have their dance team perform short sideline dances, and some dance teams also perform at school pep rallies.\n\nCollege dance squads are like traditional high school squads in that both include competition and performance dance, but there are many differences between the two. For example, a college squad will most likely dance on the sidelines at games or have a specific spot in the stands, whereas high schools usually reserve this activity for cheerleaders.\n\nThe U.S. All Star Federation governs all-star dance-pom squads.\n\nTryouts for all-star dance squads may be conducted in different ways. Some teams have only one tryout in the spring, whereas others may have a tryout in the spring and another in the fall. Some squads have year-round open tryouts where anyone can try out at any time during the season. The opportunity to compete in many large competitions attracts dancers to all-star programs. All-star dance teams can compete regionally, nationally, and even internationally.\n\nMost high schools in Texas have a precision dance/drill team, usually with 25-75 members. The traditional uniform for teams typically includes a white hat and white boots, with team officers wearing a solid white uniform while the line members wear school colors. Teams perform visual routines at football games, both in the stands during the game, and on the field at halftime. During the spring, teams often perform at basketball game halftimes, and compete in many different dance styles at competitions sponsored by dance and drill team companies. They often conclude the year with a spring show in late April or early May.\n\nTexas dance/drill teams are structured with a chain of command similar to the military including captains and lieutenants leading squads. Traditionally, Texas drill teams have been all female, but males have auditioned and been selected to teams in recent years.\n\nSeveral colleges in Texas also have dance teams. Well-known teams include the Kilgore College Rangerettes and the Tyler Junior College Apache Belles. A fierce but friendly rivalry between KC & TJC has existed since the Apache Belles were formed in 1947. The Rangerettes were the first college drill team created in 1939 by Miss Gussie Nell Davis.\n\nIn 1960, Barbara Tidwell, a former Kilgore College Rangerette, created the Strutters at Southwest Texas State University (now Texas State University), the first precision dance team created at a four-year university.\n\nIn Minnesota, competitive high school dance team is regulated under the Minnesota State High School League. The season begins after a two-week choreography period in October and ends after the state tournament in February each year. Team selection is led by the coaching staff in a tryout process individual to each participating school.\n\nTeams within this league are able to compete in one of three class divisions: A, AA, or AAA and in one or both of two categories: high kick or jazz. The high kick division requires a routine that ranges from 2:30 to 3:00 in length, contains 45-60 kicks performed by all members, and consists of up to 34 competing members. The jazz division has a range between 2:30 and 3:00 in length and may have up to 26 competing members. Music selection is done by the coaching staff and/or members of the team. Throughout the state, a wide variety of costume styles are worn to enhance the theme or mood of each routine.\n\nDuring the competition season, teams compete within their designated conference, at team invites, within designated sections, and may qualify to compete at the state tournament. Visit MSHSL dance team judging for more information on dance team scoring process. In addition to competitions, MSHSL dance teams also can perform at invitationals and school events including pep fests and basketball games.\n\nProfessional cheerleading incorporates a lot of pom dance styles, particularly in NFL Cheerleading and NBA Cheerleading.\n\nMany dance squads both in high school and college require everyone to attend a tryout. These are typically held in the spring or early summer, before most sports begin. There are many different aspects of a tryout. The first thing many tryouts do is go through basic dance techniques that will be used during the season. These include but are not limited to toe touches, fouetté turn combinations, kicks, and switch leaps. Other things that are many times included in a dance team tryout is the expectation that you can quickly master multiple short routines in different styles. Depending on what type of dance team the tryout is for will depend on what styles of dance you must know. NDA teams compete with routines that must incorporate jazz, hip hop, and pom styles, so many times you will learn a routine in each of these types of dance and then perform them shortly thereafter in front of a panel of judges.\n\nIn 1967, the legendary Dr. Kay Teer Crawford (1914-2001) founded Miss Dance Drill Team USA, which is historically verified as the first national dance team competition for precision dance teams, drill teams, and dance-sport teams in the United States. This event is recognized as the origin of the worldwide dance competition industry and hosts dance squads from elementary schools, secondary schools, and dance studios from across the United States. In 1981, Crawford started the world's first international dance/drill competition (Miss Dance Drill Team International World Championships) which has regularly hosted past international dance teams from the United States, Australia, Bulgaria, Canada, England, Germany, Japan, Korea, Lithuania, Mexico, New Zealand, Poland, Singapore, and South Africa. In 1991, Crawford founded the world's first national hip hop dance competition: National Street Dance USA. All events are held in the continental United States, with national events held in California each year. The international dance competition has been held in Japan, Australia, South Africa, and the United States.\n\nChampion Tours & Events, Inc. conducts competitions for secondary school and all-star dance teams. It holds national competitions in New York City at the College of Staten Island, in Los Angeles at the Mater Dei High School, and in Orlando, Florida at the University of Central Florida.\n\nThe Universal Dance Association, founded in 1980, holds a national championship for high school, college, and all-star dance teams at Walt Disney World Resort in Orlando, Florida. Approximately 300 high school, college, and all-star teams compete at the competition annually.\n\n"}
{"id": "4722084", "url": "https://en.wikipedia.org/wiki?curid=4722084", "title": "EMINTS", "text": "EMINTS\n\neMINTS is an educational program designed to train educators of children in the United States. The program's goals focus on technology in the classroom as well as social interaction and student research. The program began in the US state of Missouri. It was originally an acronym for \"enhancing Missouri's Instructional Networked Teaching Strategies\", but is now used in Utah, Illinois, Maine, Minnesota, Nevada, and Ohio as well.\n\nThe program is now being used with over 20,000 students in third to twelfth grade.\n\neMINTS professional development uses interactive group sessions and in-classroom coaching/mentoring to help teachers integrate technology into their teaching using an instructional model that supports high-quality lesson design, promotes inquiry-based learning, creates technology-rich learning environments, and builds community among students and teachers. These components comprise eMINTS's instructional model.\n\neMINTS professional development is available from eMINTS staff instructional specialists and through a “train-the-trainer” program called the eMINTS Affiliate Trainer program (formerly Professional Development for Educational Technology Specialists (PD4ETS).\n\n"}
{"id": "1619454", "url": "https://en.wikipedia.org/wiki?curid=1619454", "title": "Einar Haugen", "text": "Einar Haugen\n\nEinar Ingvald Haugen (; April 19, 1906 – June 20, 1994) was an American linguist, author and Professor at University of Wisconsin–Madison and Harvard University.\n\nHaugen was born in Sioux City, Iowa to Norwegians from the town of Oppdal in Norway. When he was a young child, the family moved back to Oppdal for a few years, but then returned to the United States. He attended Morningside College in Sioux City but transferred to St. Olaf College to study with Ole Edvart Rølvaag. He earned his B.A. in 1928 and immediately went on to graduate studies in linguistics at the University of Illinois at Urbana-Champaign, where he was awarded his Ph.D. in 1931.\n\nIn 1931 Haugen joined the faculty of the University of Wisconsin–Madison, where he stayed until 1962. He was made Victor S. Thomas Professor of Scandinavian and Linguistics at Harvard University in 1964, and stayed here until his retirement in 1975. Haugen served as president of the Linguistic Society of America, the American Dialect Society, and the Society for the Advancement of Scandinavian Study. Haugen was also a member of the Board of Editors of the Norwegian-American Historical Association.\nIn 1972 he was awarded an honorary degree, doctor philos. honoris causa, at the Norwegian Institute of Technology, later part of Norwegian University of Science and Technology.\n\nHaugen is credited for having pioneered the field of sociolinguistics and being a leading scholar within the field of Norwegian-American studies, including Old Norse studies. Perhaps his most important work was \"The Norwegian language in America; A study in bilingual behavior\" (). In addition to several important works within these fields, he wrote the authoritative work on the dialect of his ancestral home of Oppdal and a book entitled \"The Ecology of Language\", with which he pioneered a new field of linguistics later called Ecolinguistics.\nEinar Haugen also wrote \"Norwegian English Dictionary/Norsk engelsk ordbok\" ().\n\nHis last book was a biography of Norwegian virtuoso violinist Ole Bull co-written with his daughter, Camilla Cai.\n\nThe \"Einar and Eva Lund Haugen Memorial Scholarship\" has been established by the Norwegian-American Historical Association to honor Einar Haugen and his wife Eva Lund Haugen. Additionally, the Boston Chapter of the American-Scandinavian Foundation voted to establish the \"Einar and Eva Haugen Prize\". The prize is awarded annually to an undergraduate or graduate student for excellence in the field of Scandinavian languages and literature at Harvard University.\n\n\n\nLovoll, Odd S. \"The History of the Norwegian-American People\" (Minneapolis, MN: University of Minnesota Press. 1999)\n\n"}
{"id": "1787491", "url": "https://en.wikipedia.org/wiki?curid=1787491", "title": "English Plus", "text": "English Plus\n\nEnglish Plus is an American movement formed in reaction to the English-only movement. The intent was to promote greater acceptance of language diversity in the United States in order to encourage a broader American cultural development and more international perspectives. This would be achieved by encouraging education in English as well as secondary languages across the entire population, for immigrants and natives alike. This movement has been supported by language education professionals and minority language advocacy groups.\n\n\"English Plus\" resolutions have been passed in the U.S. states of New Mexico, Oregon, Rhode Island, and Washington.\n\nThe term \"English Plus\" originated in a 1985 letter to then-Secretary of Education William Bennett from the Spanish American League Against Discrimination.\n\n"}
{"id": "982249", "url": "https://en.wikipedia.org/wiki?curid=982249", "title": "Environmental archaeology", "text": "Environmental archaeology\n\nEnvironmental archaeology is a sub-field of archaeology and is the science of reconstructing the relationships between past societies and the environments they lived in. The field represents an archaeological-palaeoecological approach to studying the palaeoenvironment through the methods of human palaeoecology. Reconstructing past environments and past peoples' relationships and interactions with the landscapes they inhabited provides archaeologists with insights into the origin and evolution of anthropogenic environments, and prehistoric adaptations and economic practices.\n\nEnvironmental archaeology is commonly divided into three sub-fields: \n\nOther related fields include:\n\nEnvironmental archaeology often involves studying plant and animal remains in order to investigate which plant and animal species were present at the time of prehistoric habitations, and how past societies managed them. It may also involve studying the physical environment and how similar or different it was in the past compared to the present day. An important component of such analyses represents the study of site formation processes. This field is particularly useful when artifacts may be absent from an excavated or surveyed site, or in cases of earth movement, such as erosion, which may have buried artifacts and archaeological features. While specialist sub-fields, for example bioarchaeology or geomorphology, are defined by the materials they study, the term \"environmental\" is used as a general template in order to denote a general field of scientific inquiry that is applicable across time periods and geographical regions studied by archaeology as a whole.\n\nEnvironmental archaeology has emerged as a distinct discipline in the course of the last 50 years. In recent years it has grown rapidly in significance and is now an established component of most excavation projects. The field is multidisciplinary, and environmental archaeologists as well as palaeoecologists work side by side with archaeologists and anthropologists specialising in material culture studies in order to achieve a more holistic understanding of past human lifeways and people-environment interactions.\n\nA notable pioneer of environmental archaeology has been Karl Butzer.\n\nEach focus within environmental archaeology collects information about a different aspect of human relation with their surrounding environment. Together these components (along with methods from other fields) are combined to fully understand a past society’s lifestyle and interactions with their environment. Past aspects of land use, food production, tool use, and occupation patterns can all be established and the knowledge applied to current and future human-environment interactions.\n\n"}
{"id": "57702385", "url": "https://en.wikipedia.org/wiki?curid=57702385", "title": "Eskişehir Caricature Museum", "text": "Eskişehir Caricature Museum\n\nEskişehir Caricature Museum is a museum in Eskişehir, Turkey\nThe museum is in Odunpazarı ilçe (second level municipality) of Eskişehir at . It is in a neighborhood of museums.\n\nThe building is a residence house built in 1900s. It was restored and used as a museum of Anadolu University. Its total service area is . Each room of the house is dedicated to different displays such as, permanent display, temporary display, Eskişehir caricaturists’ display, portraits of famous caricaturists etc. There is also a library in the museum \n"}
{"id": "23531847", "url": "https://en.wikipedia.org/wiki?curid=23531847", "title": "Excusable negligence", "text": "Excusable negligence\n\nExcusable negligence is a paradoxical phrase, since if the failure to exercise reasonable care under the circumstances is excusable, there is no negligence. 38 Am J1st Negl § 12. As used in statutes authorizing the opening of a default and allowing a party to defend on the merits, the standard set by courts is slippery to define, but cases seem to agree that a reasonable excuse is sufficient, where it appears that the defense is meritorious and no substantial prejudice will result from setting aside the default.\n\n"}
{"id": "11008611", "url": "https://en.wikipedia.org/wiki?curid=11008611", "title": "Finnur Jónsson", "text": "Finnur Jónsson\n\nFinnur Jónsson (May 29, 1858 – March 30, 1934) was an Icelandic philologist and Professor of Nordic Philology at the University of Copenhagen. He made extensive contributions to the study of Old Norse literature.\nFinnur Jónsson was born at Akureyri in northern Iceland. He graduated from Menntaskólinn í Reykjavík in 1878 and went to Denmark for further studies at the University of Copenhagen. He received a doctorate in philology in 1884 with a dissertation on skaldic poetry. He became a docent at the University in 1887 and a professor in 1898, serving until 1928. After retiring he continued work on his subject with new publications until the year he died.\nHe was elected member of the Royal Society of Arts and Sciences in Gothenburg in 1905 and corresponding member of the Royal Swedish Academy of Letters, History and Antiquities in 1908.\n\nFinnur's principal area of study was Old Norse poetry. His three most important works are \"Den norsk-islandske skjaldedigtning\", an edition of the entire corpus of skaldic poetry in two parts - one which gives the text of the manuscripts with variants and one which gives a normalized text and a Danish translation. Another of Finnur's major works is \"Lexicon Poeticum\", a dictionary of Old Norse poetry, ostensibly an update of a work with the same name by Sveinbjörn Egilsson but in effect an original work. The third principal work is \"Den oldnorske og oldislandske litteraturs historie\", a detailed history of Old Norse literature.\n\nFinnur was an unusually prolific scholar, preparing editions of, among other works, numerous Icelanders' sagas, Kings' sagas, \"Rímur\" (along with a dictionary of \"rímur\") and the Eddas. A skilled polemicist, he defended his belief in the historical accuracy of the sagas and the antiquity of the Eddic poems in debates with other scholars.\n\n"}
{"id": "34248281", "url": "https://en.wikipedia.org/wiki?curid=34248281", "title": "Georgian calendar", "text": "Georgian calendar\n\nThe Georgian calendar () is the ancient or modern calendar of Georgia.\n\nThough Georgia now uses the modern Gregorian calendar, the old names for the months are still used.\n\n New Year in ancient Georgia started from September.\n\n"}
{"id": "11394021", "url": "https://en.wikipedia.org/wiki?curid=11394021", "title": "Hera Borghese", "text": "Hera Borghese\n\nThe Hera Borghese is a type of sculpture of Hera named after the owners of its archetype, the Borghese. One example is in the National Museum of Rome, whilst others are in the Palatine Antiquarium and at the Castello Aragonese Museum at Baiae. \n"}
{"id": "2138147", "url": "https://en.wikipedia.org/wiki?curid=2138147", "title": "History of the International Phonetic Alphabet", "text": "History of the International Phonetic Alphabet\n\nThe International Phonetic Alphabet was created soon after the International Phonetic Association was established in the late 19th century. It was intended as an international system of phonetic transcription for oral languages, originally for pedagogical purposes. The Association was established in Paris in 1886 by French and British language teachers led by Paul Passy. The prototype of the alphabet appeared in . The Association based their alphabet upon the Romic alphabet of Henry Sweet, which in turn was based on the Phonotypic Alphabet of Isaac Pitman and the Palæotype of Alexander John Ellis.\n\nThe alphabet has undergone a number of revisions during its history, the most significant being the one put forth at the Kiel Convention in 1989. Changes to the alphabet are proposed and discussed in the Association's organ, \"Journal of the International Phonetic Association\", previously known as \"Le Maître Phonétique\" and before that as \"The Phonetic Teacher\", and then put to a vote by the Association's Council.\n\nThe extensions to the IPA for disordered speech were created in 1990, with its first major revision approved in 2016.\n\nThe International Phonetic Association was founded in Paris in 1886 under the name \"Dhi Fonètik Tîtcerz' Asóciécon\" (The Phonetic Teachers' Association), a development of \"L'Association phonétique des professeurs d'Anglais\" (\"The English Teachers' Phonetic Association\"), to promote an international phonetic alphabet, designed primarily for English, French, and German, for use in schools to facilitate acquiring foreign pronunciation.\n\nOriginally the symbols had different phonetic values from language to language. For example, English was transcribed with and French with .\n\nAs of May and November of 1887, the alphabets were as follows:\n\nIn the August–September 1888 issue of its journal, the Phonetic Teachers' Association published a standardized alphabet intended for transcription of multiple languages, reflecting its members' consensus that only one set of alphabet ought to be used for all languages, along with a set of six principles:\n\nThe principles would govern all future development of the alphabet, with the exception of #5 and in some cases #2, until they were revised drastically in 1989. #6 has also been loosened, as diacritics have been admitted for limited purposes.\n\nThe devised alphabet was as follows. The letters marked with an asterisk were \"provisional shapes\", which were meant to be replaced \"when circumstances will allow\".\n\nDuring the 1890s, the alphabet was expanded to cover sounds of Arabic and other non-European languages which did not easily fit the Latin alphabet.\n\nThroughout the first half of the 1900s, the Association published a series of booklets outlining the specifications of the alphabet in several languages, the first being a French edition published in 1900. In the book, the chart appeared as follows:\n\nInitially, the charts were arranged with laryngeal sounds on the left and labial ones on the right, following the convention of Alexander Melville Bell's Visible Speech. Vowels and consonants were placed in a single chart, reflecting how sounds ranged in openness from stops (top) to open vowels (bottom). The voiced velar fricative was represented by (distinct from , which represents a plosive) since 1895 until it was replaced by in 1900. too would be replaced by in 1931.\n\nNot all symbols, especially those in the fricatives row which included both fricatives in modern terms and approximants, were self-explanatory and could only be discerned in the notes following the chart, which re-defined symbols using the orthographies of languages wherein the sounds they represent occur. For example:\n\n(θ) is the English hard \"th\", Spanish \"z\", Romaic θ, Icelandic þ; (ð) the English soft \"th\", Icelandic ð, Romaic δ. (ɹ) is the non-rolled \"r\" of Southern British, and can also be used for the simple \"r\" of Spanish and Portuguese ... (x) is found in German in \"ach\"; (ǥ), in \"wagen\", as often pronounced in the north of Germany. (ᴚ) is the Arabic \"kh\" as in \"khalifa\"; (ʁ) the Danish \"r\"; the Parisian \"r\" is intermediate between (ʀ) and (ʁ). (ʜ) and (ɦ) are the \"ha\" and \"he\" in Arabic.—(ᵷ) and (ʒ) are sounds in Circassian.\n\nNasalized vowels were marked with a tilde: , , etc. It was noted that may be used for \"any vowel of obscure and intermediate quality found in weak syllables\". A long sound was distinguished by trailing . Stress may be marked by before the stressed syllable as necessary. It was noted that in Swedish and Norwegian texts the sign was placed before the stressed syllable of words with \"the so-called compound tone\".\n\nA voiced sound was marked by and a voiceless one by . Retroflex consonants were marked by , as in . Arabic emphatic consonants were marked by : . Consonants accompanied by a glottal stop (ejectives) were marked by : . Tense and lax vowels were distinguished by acute and grave accents: \"naught\" , \"not\" . Non-syllabic vowels were marked by a breve, as in , and syllabic consonants by an acute below, as in . Following letters, stood for advanced tongue, for retracted tongue, for more open, for more close, for more rounded, and for more spread. It was also noted that a superscript letter may be used to indicate a tinge of that sound in the sound represented by the preceding letter, as in .\n\nIt was emphasized, however, that such details need not usually be repeated in transcription. The equivalent part of the 1904 English version said:\n\n[I]t must remain a general principle to \"leave out everything self-evident, and everything that can be explained once for all\". This allows us to dispense almost completely with the modifiers, and with a good many other signs, except in scientific works and in introductory explanations. We write English \"fill\" and French \"fil\" the same way ; yet the English vowel is 'wide' and the French 'narrow', and the English is formed much further back than the French. If we wanted to mark these differences, we should write English , French . But we need not do so: we know, once for all, that English short is always , and French always ; that English is always and French always .\n\nIn the 1904 \"Aim and Principles of the International Phonetic Association\", the first of its kind in English, the chart appeared as:\n\nIn comparison to the 1900 chart, the glottal stop appeared as a modifier letter rather than a full letter , and replaced . were removed from the chart and instead only mentioned as having \"been suggested for a Circassian dental hiss and its voiced correspondent\". Laryngeal consonants had also been moved around, reflecting little understanding about the mechanisms of laryngeal articulations at the time. and were defined as the Arabic and .\n\nIn the notes, the half-length symbol was now mentioned. It was noted that whispered sounds may be marked with a diacritical comma, as in . A syllabic consonant was now marked by a vertical bar, as in , rather than . It was noted, only in this edition, that \"shifted vowels\" may be indicated: for in-mixed or in-front, and for out-back.\n\nFollowing 1904, sets of specifications in French appeared in 1905 and 1908, with little to no changes. In 1912, the second English booklet appeared. For the first time, labial sounds were shown on the left and laryngeal ones on the right:\n\n, for the Czech fricative trill, and , replacing , were added, following their approval in 1909. Though not included in the chart, was mentioned as an optional symbol for the labiodental nasal. was still designated as the \"provisional\" symbol for the alveolar tap/flap. were defined as the Bantu sounds with \"tongue position of θ, ð, combined with strong lip-rounding\". were still included though not in the chart. was removed entirely.\n\nFor the first time, affricates, or [a]ssibilated' consonant groups, i. e. groups in which the two elements are so closely connected that the whole might be treated as a single sound\", were noted as able to be represented with a tie bar, as in . Palatalized consonants could be marked by a dot above the letter, as in , \"suggesting the connexion with the sounds i and j\".\n\nThe 1921 \"Écriture phonétique internationale\" introduced new symbols, some of which were never to be seen in any other booklet:\n\nThe book also mentioned symbols \"already commonly used in special works\", some of which \"have not yet been definitively adopted\":\n\n\nIt also introduced several new suprasegmental specifications:\n\n\nIt recommended the use of a circumflex for the Swedish grave accent, as in (\"the spirit\"). It was mentioned that some authors prefer in place of . Aspiration was marked as and stronger aspiration as .\n\nThe click symbols were conceived by Daniel Jones. In 1960, A. C. Gimson wrote to a colleague:\n\nPaul Passy recognized the need for symbols for the various clicks in the July–August 1914 number of \"Le Maître Phonétique\" and asked for suggestions. This number, however, was the last for some years because of the war. During this interval, Professor Daniel Jones himself invented the four symbols, in consultation with Paul Passy and they were all four printed in the pamphlet \"L'Écriture Phonétique Internationale\" published in 1921. The symbols were thus introduced in a somewhat unusual way, without the explicit consent of the whole Council of the Association. They were, however, generally accepted from then on, and, as you say, were used by Professor Doke in 1923. I have consulted Professor Jones in this matter, and he accepts responsibility for their invention, during the period of the First World War.\n\nThe 1921 book was the first in the series to mention the word \"phoneme\" (\"phonème\").\n\nIn April 1925, 12 linguists led by Otto Jespersen, including IPA Secretary Daniel Jones, attended a conference in Copenhagen and proposed specifications for a standardized system of phonetic notation. The proposals were largely dismissed by the members of the IPA Council. Nonetheless, the following additions recommended by the Conference were approved in 1927:\n\n\nIn 1928, the following symbols were adopted:\n\n\nIn the same year, for the rounded variety of and , replacing , were also approved.\n\nAn updated chart appeared as a supplement to \"Le Maître Phonétique\" in 1932.\n\nThe vowels were now arranged in a right-angled trapezium as opposed to an isosceles trapezium, reflecting Daniel Jones' development of the Cardinal Vowel theory. A practically identical chart—with the exception of —in German had appeared in . The substitution of for was approved in 1931.\n\nThe accompanying notes read:\n\n.—Palatalized consonants: , etc. Velarized or pharyngealized consonants: , etc. Ejective consonants (plosives with simultaneous glottal stop): , etc. Implosive voiced consonants: , etc. fricative trill. (labialized , or ). (labialized ). (clicks, Zulu \"c, q, x\"). (a sound between and ). (voiceless ). (lowered varieties of ). (a variety of ). (a vowel between and ).\n\nAffricates are normally represented by groups of two consonants (, etc.), but, when necessary, ligatures are used (, etc.), or the marks or ( or , etc.). may occasionally be used in place of . Aspirated plosives: , etc.\n\n.— (full length). (half length). (stress, placed at the beginning of the stressed syllable). (secondary stress). (high level pitch); (low level); (high rising); (low rising); (high falling); (low falling); (rise-fall); (fall-rise). See \"Écriture Phonétique Internationale\", p. 9.\n\n.— nasality. breath ( = breathed ). voice ( = ). slight aspiration following , etc. specially close vowel ( = a very close ). specially open vowel ( = a rather open ). labialization ( = labialized ). dental articulation ( = dental ). palatalization ( = ). tongue slightly raised. tongue slightly lowered. lips more rounded. lips more spread. Central vowels (= ), (= ), (= ), (= ), . (e.g. ) syllabic consonant. consonantal vowel. variety of resembling , etc.\nA new chart appeared in 1938, with a few modifications. was replaced by , which was approved earlier in the year with the compromise also acknowledged as an alternative. The use of tie bars was allowed for synchronous articulation in addition to affricates, as in for simultaneous and , which was approved in 1937. In the notes, the reference to , in regard to tonal notation was removed.\n\nA new chart appeared in 1947, reflecting minor developments up to the point. They were:\n\n\nThe word \"plosives\" in the description of ejectives and the qualifier \"slightly\" in the definitions of were removed.\n\nThe 1949 \"Principles of the International Phonetic Association\" was the last installment in the series until it was superseded by the \"Handbook of the IPA\" in 1999. It introduced some new specifications:\n\n\nNone of these specifications were inherited in the subsequent charts. was defined as an indicator of \"medium stress\".\n\nIn 1948, and were approved as typographic alternatives, while it was also acknowledged that may be used for a velar plosive and for an advanced one in narrow transcription of a language where it is preferable to distinguish the two, such as Russian. The 1949 \"Principles\" recommended this alternation of the symbols but did not mention their typographic equivalency in other languages. Nevertheless, the recommendation was hardly adopted, not even by , who used and .\n\nThe 1951 chart added as yet another alternative to an r-coloured , following its approval in 1950. Conceived by John S. Kenyon, the symbol was in itself a combination of and the hook for retroflex consonants approved by the IPA in 1927. Since its introduction in 1935, the symbol was widely adopted by American linguists and the IPA had been asked to recognize it as part of the alphabet.\n\nIn 1979, a revised chart appeared, incorporating the developments in the alphabet which were made earlier in the decade:\n\nThe following changes were approved in 1976:\n\n\nOn the same occasion, the following symbols were removed because they had \"fallen into disuse\":\n\n\nOn the other hand, for the close-mid central unrounded vowel, for the open-mid central rounded vowel, and for the open central unrounded vowel were proposed but rejected. The proposal of was based on . for the voiced palatal fricative and for creaky voice were proposed but the votes were inconclusive.\n\nIn the 1979 chart, , previously defined as \"lowered varieties of \", appeared slightly centered rather than simply midway between and as they did in the 1912 chart. , the predecessors to , were acknowledged as alternatives to under the section \"Other symbols\". appeared as the rounded counterpart to rather than between and .\n\nThe name of the column \"Dental and alveolar\" was changed to \"Dental, alveolar, or post-alveolar\". \"Pharyngeal\", \"trill\", \"tap or flap\", and \"approximant\" replaced \"\", \"rolled\", \"flapped\", and \"frictionless continuants\", respectively. , which were listed twice in both the fricative and frictionless continuant rows in the previous charts, now appeared as an approximant and a fricative, respectively, while the line between the rows was erased, indicating certain fricative symbols may represent approximants and vice versa, with the employment of the raised and lowered diacritics if necessary. , previously defined as \"voiceless \", was specified as a fricative. remained listed twice in the fricative and approximant rows. , previously defined merely as \"a sound between and \", was redefined as an alveolar lateral flap.\n\nBy the 1980s, phonetic theories had developed so much since the inception of the alphabet that the framework of it had become outdated. To resolve this, at the initiative of IPA President Peter Ladefoged, approximately 120 members of the IPA gathered at a convention held in Kiel, West Germany, in August 1989, to discuss revisions of both the alphabet and the principles it is founded upon. It was at this convention that it was decided that the \"Handbook of the IPA\" be written and published to supersede the 1949 \"Principles\".\n\nIn addition to the revisions of the alphabet, two workgroups were set up, one on computer coding of IPA symbols and computer representation of individual languages, and the other on pathological speech and voice quality. The former group concluded that each IPA symbol should be assigned a three-digit number for computer coding known as IPA Number, which was published in . The latter devised a set of recommendations for the transcription of disordered speech based on the IPA known as the Extensions to the International Phonetic Alphabet or extIPA, which was published in 1990 and adopted by the International Clinical Phonetics and Linguistics Association, which now maintains it, in 1994.\n\nA drastically renewed chart of the alphabet reflecting decisions made at the convention appeared later in the year. Additions were:\n\n\nTone, which had been indicated with an iconic line preceding the syllable or above or below the vowel, was now written one of two ways: with a similar iconic line following the syllable and anchored to a vertical bar, as in (Chao's tone letters), or with more abstract diacritics written over the vowel (acute = high, macron = mid, grave = low), which could be compounded with each other, as in .\n\nThe palato-alveolar column was removed and were listed alongside the postalveolars. appeared at the same horizontal position as the other alveolars rather than slightly more back as did in the previous charts. was specified as a trill rather than either a trill or flap. The alternative raised and lowered diacritics were eliminated in favour of , which could now be attached to consonants to denote fricative or approximant, as in . Diacritics for relative articulation placed next to, rather than below, a letter, namely , were no longer mentioned. The diacritic for no audible release was finally mentioned in the chart.\n\nAt the convention, proposals such as for a voiced labial–velar fricative, for a voiceless velar lateral fricative, for a voiced velar lateral fricative, for a voiceless palatal lateral fricative, for \"the 'hissing-hussing' fricatives of some Caucasian languages\", and for an open central unrounded vowel were discussed but dismissed.\n\nThe six principles set out in 1888 were replaced by a much longer text consisting of seven paragraphs. The first two paragraphs established the alphabet's purpose, namely to be \"a set of symbols for representing all the possible sounds of the world's languages\" and \"representing fine distinctions of sound quality, making the IPA well suited for use in all disciplines in which the representation of speech sounds is required\". The second paragraph also said, \" is a shorthand way of designating the intersection of the categories voiceless, bilabial, and plosive; is the intersection of the categories voiced, bilabial, and nasal; and so on\", refining the previous, less clearly defined principle #2 with the application of the distinctive feature theory. Discouragement of diacritics was relaxed, though recommending their use be limited to cases: \"(i) For denoting length, stress and pitch. (ii) For representing minute shades of sounds. (iii) When the introduction of a single, diacritic obviates the necessity for designing a number of new symbols (as, for instance, in the representation of nasalized vowels)\". The principles also adopted the recommendation of enclosing phonetic transcriptions in square brackets [ ] and phonemic ones in slashes / /, a practice that emerged in the 1940s. The principles were reprinted in the 1999 \"Handbook\".\n\nFollowing the 1989 revision, a number of proposals for revisions appeared in the \"Journal of the IPA\", which were submitted to the Council of the IPA. In 1993, the Council approved the following changes:\n\n\nOn the same occasion, it was reaffirmed that and are typographic alternatives.\n\nThe revised chart was now portrait-oriented. and were moved to the centerline of the vowel chart, indicating that they are not necessarily unrounded. The word \"voiced\" was removed from the definition for , now simply \"epiglottal plosive\". \"Other symbols\" and diacritics were slightly rearranged.\n\nIn 1996, it was announced that the symbol for the open-mid central rounded vowel in the 1993 chart, , was a typographical error and should be changed to , stating the latter was the symbol which \"J. C. Catford had in mind when he proposed the central vowel changes ... in 1990\", also citing and , who had . However, the symbol Catford proposed for the value in 1990 was in fact (a barred ), with an alternative being , but not . Errata for appeared in 1992, but the printed symbol was again and the errata even acknowledged that was included in , pointed out by David Abercrombie.\n\nIn the updated chart, the subsections were rearranged so that the left edge of the vowel chart appeared right beneath the palatal column, hinting at the palatal place of articulation for , as did in all pre-1989 charts, though the space did not allow the back vowels to appear beneath the velars. A tie bar placed below symbols, as in , was mentioned again. was now attached to the preceding letter, as in . A few illustrations in the chart were changed: was added for rhoticity, and were replaced with . The word \"etc.\" was dropped from the list of tones.\n\nThe 1999 \"Handbook of the International Phonetic Association\" was the first book outlining the specifications of the alphabet in 50 years, superseding the 1949 \"Principles of the IPA\". It consisted of just over 200 pages, four times as long as the \"Principles\". In addition to what was seen in the 1996 chart, which was reprinted in the front matter, the book included for mid central vowel release, for voiceless dental fricative release, and for voiceless velar fricative release as part of the official IPA in the \"Computer coding of IPA symbols\" section. It also said \"might be used\" for \"a secondary reduction of the lip opening accompanied by neither protrusion nor velar constriction\". It abandoned the 1949 \"Principles\" recommendation of alternating and for ordinary and advanced velar plosives, and acknowledged both shapes as acceptable variants.\n\nIn 2005, was added for the labiodental flap.\n\nIn 2011, it was proposed that be added to represent the open central unrounded vowel, but this was declined by the Council the following year.\n\nIn 2012, the IPA chart and its subparts were released under the Creative Commons Attribution-ShareAlike 3.0 Unported License.\n\nIn 2016, three versions of a revised chart dated 2015 were released online, each with the phonetic symbols rendered in a different typeface (IPA Kiel/LS Uni developed by Linguist's Software, Doulos SIL, and DejaVu Sans). No symbols were added or withdrawn, but some notes and the shapes of a few symbols were slightly modified. In particular, was replaced by , with a continuous, slanted stroke.\n\nIn 2018, another slightly modified chart in different fonts was released, this time also in TeX TIPA Roman developed by Rei Fukui, which was selected as best representing the IPA symbol set by the Association's Alphabet, Charts and Fonts committee, established the previous year.\n\n\n\n"}
{"id": "34559253", "url": "https://en.wikipedia.org/wiki?curid=34559253", "title": "Jacob Baden", "text": "Jacob Baden\n\nJacob Baden (4 May 1735 – 5 July 1804), was a Danish philologist, pedagogue, and critic. He was professor of rhetoric and the Latin language at University of Copenhagen in 1779. He was the first person to lecture on Danish grammar at the university between 1782 and 1783. He was the editor of the \"University Journal\" from 1793 to 1801.\nHe published a Danish grammar, Latin grammar, and also wrote an elementary Greek grammar in 1764. He produced a Danish grammar in German language in 1767. His Danish grammar is well recognized till today.\n\nHe was born at Vordingborg on 4 May 1735. His father, who was also called Jacob Baden, was rector of the local latin school. His mother Else Jacobine née From was a daughter of county manager (\"amtsforvalter\") From at ]]Antvorskov]]. He lost his father when he was 2 and was brought up by his mother. He enrolled at the University of Copenhagen at age 15. He was a resident of på Ehlers' Kollegium after passing his theological exams.\n\nHe later continued his studies at the University of Göttingen and University of Leipzig.\n\nHe married Charlotte Baden, a Danish writer in 1763. He died on 5 July 1804.\n\n\n"}
{"id": "52423730", "url": "https://en.wikipedia.org/wiki?curid=52423730", "title": "Jacqueline Suthren Hirst", "text": "Jacqueline Suthren Hirst\n\nJacqueline G. Suthren Hirst or Jackie Hirst is a Senior Lecturer in comparative religion and South Asian studies at Manchester University.\n\nShe has an MA and PhD from Cambridge University and is a qualified teacher, and taught religious education in a school for five years. She was a Senior Lecturer at Homerton College, training teachers to teach religious education, before moving to Manchester in 1994.\n\nShe has been a guest on BBC Radio 4's \"In Our Time\", in an episode first broadcast on 6 October 2016 on the topic of Lakshmi. \n\n\n\n"}
{"id": "24330902", "url": "https://en.wikipedia.org/wiki?curid=24330902", "title": "Junction Grammar", "text": "Junction Grammar\n\nJunction Grammar is a descriptive model of language developed during the 1960s by Dr. Eldon G. Lytle (1936–2010).\n\nJunction Grammar is based on the premise that the meaning of language can be described and precisely codified by the way language elements are joined together.\n\nThe model was used during the 1960s and 1970s in the attempt to create a functional computer-assisted translation system. It has also been used for linguistic analysis in the language instruction field.\n\nEarly generative grammars dealt with language from a \"syntactic\" perspective, i.e. as the problem presented by the task of creating rules able to combine words into well-formed (i.e., \"grammatical\") sentences. The rules used by these grammars were referred to as phrase-structure rules (P-rules). It was soon apparent, however, that a generative component composed solely of P-rules could not generate a wide variety of commonly occurring sentence types. In response to this dilemma, we find Harris proposing an explanation:\n\nChomsky’s model of syntax - transformational grammar -picked up on this line of reasoning and added a supplementary set of transformations (T-rules). T-rules effected combinations and permutations of words in step-wise fashion to fill in structural gaps where P-rules alone could not generate the sentences which Harris had pointed out as problems. The structural forms generated by P-rules alone were said to constitute ‘deep structure.’ ‘Surface structure’ was then derived transformationally by T-rules from the ‘kernel’ structures first generated by the operation of P-rules. In this way, Chomsky proposed to generate an infinite number of sentences using finite means (the closed sets of P-rules and T-rules). Syntax-based models of this vintage set semantics and phonology apart as linguistic processes to be approached separately.\n\nEnter from the sidelines under these circumstances \"junction grammar\" (JG), a model of natural language created by Eldon Lytle in the late ‘60s and early ‘70s. Junction grammar did not propose an amendment to Chomsky’s model of syntax, but purported to eliminate the need for transformations altogether through theoretical innovation and a novel design for generative grammars. Innovations fundamental to the new approach rejected common-place reliance on existing mathematics and formal language theory as tools for linguistic modeling and description \"in deference to the intuition of more fundamental structuring in the body and in natural language itself\" which appeared to provide a \"universal base for linguistic description\" - not only for natural language but also for the synthetic notation systems employed at the time for linguistic description. Implementation of the novelties in question entailed:\n\n\nIn sum, the junction grammar model of language (1) moved the base into a sub-verbal semantic domain, (2) added universally relevant linguistic operators to generation rules - thus, for example, solving the quandary of how to handle 'conjunction' - (3) incorporated auxiliary 'tracts' together with specialized data types for voice, audition, etc., and (4) added coding grammars to physically interface between tracts.\n\n\"The right-left, top-down representational format which forced everything into one big tree was dispensed with in favor of an ensemble of interfacing representations which employed as many data-specific descriptions as necessary to capture the functionality of the diverse neurobiological manifestations of linguistic behavior being dealt with.\" Because target structuring was generable directly by the powerful J-rule base component and its array of linguistic operators, structure-oriented transformations required to combine kernel sentences and/or otherwise manipulate constituent structure were no longer required. Transformations formulated to generate correct word order and otherwise massage surface strings were supplanted by coding algorithms/grammars in the JG model. It may be said by way of general comparison that, whereas Chomsky's model of syntax was by design \"derivative\" (speaking of its roots in existing forms of notation), \"derivational,\" and \"manipulative,\" the JG model was \"seminal\" (speaking of its formal novelty), \"modular,\" and \"transpositional.\" Despite polar differences, however, \"Chomsky's objective of generating an infinite number of sentences with finite means, remained firmly intact in JG, as did the presumption of the fundamental innateness of natural language in the normal speaker/hearer.\"\n\nThe base/junction rules (J-rules) of junction grammars are a set of algebraic formulas which generate for natural language what is akin to the Periodic Table of elements in chemistry, namely, an enumeration of well-formed linguistic structures sometimes referred to as the \"Periodic Chart of Constituent structures\" (PCCS). J-rules generate composite structures consisting of the labeled operands entering into them plus the relations established by operators, and assign a category to the composite produced, to wit: X ∘ Y ≐ Z. Composites thus generated become potential operands for subsequent operations. The resulting algebra may be represented in string form (e.g., infix, postfix, reverse polish) or graphed as a branching diagram (J-tree).\n\nThe universal operators utilized by these rules are \"subjunction\" (*), \"conjunction\" (&), and \"adjunction\" (+), plus subtypes required under particular circumstances, e.g., restrictive (.*) versus non-restrictive (=*=) under subjunction. Expressed in more familiar terms, subjunction joins modifiers and complements to their heads, conjunction bonds constituents of homogeneous category which are in some respect similar, and adjunction attaches relations and processes to their operands to form predicates, statements, and relational phrases. Supplemental operators effect the requirements of data management in mental modeling and conversational settings, corresponding in large part to the conventional classification of deixis.\n\nThe operands of the base are drawn from a dictionary of sememes (meaningful concepts) which are by definition non-lexical in JG and may be plausibly viewed as electromagnetic signatures in their neurobiological setting arising in connection with the formation of the mental models which provide the content and sensory linkages for their meanings.\n\nWhile the link between \"signified\" and \"signifier\" (as per Saussure) may be separately represented in a junction grammar, the interfacing between J-rule structuring and the coding extant in other components of the model is provided by context-sensitive coding grammars formulated as algorithms in an appropriate pattern matching language. For example, JG incorporates a lexical coding grammar consisting of \"lexical rules\" (L-rules) which encodes unordered sememic structuring as ordered lexical strings in a separate coding space. A subsequent coding provides either the distinct patterning of the voice contour, or the formatting and punctuation of printed material.\n\nWith the foregoing as a frame of reference, we draw renewed attention to significant differences between JG sentence analysis and conventional syntactic analysis. The more familiar syntax approach analyzes phrases and sentences in terms of outward ('surface') appearance, i.e. in terms of the words which they contain and how intuition groups them. Structural diagrams reflecting this method strive to depict constituent clusters in the word stream supplemented by labels, perhaps, or other information of focal interest to the analyst - some of it perhaps semantic. A change in word order requires that the diagram be changed.\n\nIn contrast, the JG approach, while taking note of the words, looks beyond them to the base constructions from which they have presumably been encoded, concentrating all the while on the semantic effects associated with the constituents and their structural nuances. JG diagrams (J-trees), therefore, are not directly representative of the word stream but, rather, of rational constructs in the neural mass having linkage with the words which we read or write. This means that a variety of structuring detail made explicit in J-trees is only implicit in the word stream. Conversely, it means that certain lexical detail made explicit in the word stream is only implicit in J-trees. For example, depending upon the ordering rules of the lexical coding grammar in play \"and\" the discourse context of the sentence - the same J-tree may yield alternative word orders in the lexical coding space.\n\nThe overall effect - as previously noted - is that, inasmuch as JG uses \"coding\" rather than \"derivation\" as a bridge between levels of representation, much that models of syntax have been preoccupied with in deriving \"surface structure\" from \"deep structure\" (movement, deletion, insertion, etc.) is taken over in JG by coding operations.\n\nThe first junction grammar was worked out by Eldon Lytle in connection with his Ph.D. dissertation during the late ‘60s, in which he constructed such a grammar for the analysis of structural derivation in Russian. That grammar relegated the data to four levels of representation, corresponding to:\n\n\nLytle employed one of the junction operators (subjunction) as a formal device to impose the properties of a governing category upon existing structure to obtain ‘derived’ forms (e.g., ‘transform-ation’ - Noun * Verb).\n\nEarly literature on JG was published in \"Junction Theory and Application\", the journal of the BYU \"Translation Sciences Institute\" (TSI), and/or the proceedings of the University’s \"Annual Linguistic Symposium\". More widely distributed overviews and analyses of junction theory first appeared in monograph form under Mouton’s \"Janua Linguarum Series\", followed by papers and research reports presented at LACUS forum proceedings and linguistic conferences abroad.\n\nMeanwhile, others tested the model for applicability to natural languages distanced from English. The latter studies applied junction analysis to particular languages of diverse families, including Finnish, Samoan, Korean, Japanese, and Russian. Chinese, French, German, Spanish, Portuguese were added to this list in due course (see below) . Following these studies, it was concluded that the J-rules comprising the syntacto-semantic base of the model proffered a pool of structural possibilities from which all natural languages draw, but that none necessarily uses them all nor the same subset.\n\nDuring the early ‘70s, at the urging and under the auspices of the Department of Defense, Lytle and a team of colleagues conducted research at BYU in computer-assisted, human-interactive translation in which junction grammars were subjected to formalization and applied to the problem of English-Russian translation. When positive results evinced proof of concept and a software prototype, funds from the private sector were invested to develop and test a one-to-many system based on the same translation model in expanded form - in particular, translations were synthesized from J-trees of English obtained via human interaction into Russian, Spanish, French, German, Portuguese, and Chinese. Just prior to his inauguration as president, Gerald Ford and an entourage of VIPS visited the development site to receive a briefing on the concept and progress of the work.\n\nAt the conclusion of this endeavor, an alpha model of the software translated a book submitted for test purposes and yielded translations in the cited target languages. While significant post-editing was required, suggesting - among other improvements - that a second, language-specific interaction may well improve the design, the utility of computers as a useful translation aid, both in standardizing the use of terminology and expediting the overall translation process had been demonstrated.\n\nJG adherents soon observed that the structural particulars of \"adjunction, conjunction,\" and \"subjunction\" are relevant beyond conventional linguistic structuring. More specifically, they lend themselves to such diverse structural scenarios as spousal interaction, departmental interaction in institutions, and a host of other real-world phenomena. In response to this observation, Lytle developed a method of phonological representation based on phonemic operands and junction operators. This method of representation was subsequently utilized as the basis for synthesizing speech contours which reflected the structure of the source text.\n\nJG proved to be classroom friendly, not only because its base structures were more intuitive, comprehensive, and explicit than traditional forms of diagramming, but also because their connection to overt forms of language were straightforward. Students quickly picked up on the challenge presented by the structural predictions of the PCCS and undertook to assist in verifying them. Among the memorable sentence types brought forward in class to challenge instructors and the Chart were:\n\n\nStudies were conducted to determine whether exposure to the JG method of diagramming was useful as a point of reference in teaching/learning foreign languages. Olson and Tuttle found that the answer was affirmative.\n\nSubsequent to his experience with computer-assisted translation and voice synthesis, Lytle undertook the development of JG-based educational software in the private sector. To facilitate his objectives he designed what is referred to as ‘JG Markup Notation’ and created a pattern matching language (JGPL) to complement it. This endeavor relied on the escalating power of micro-computers to field JG-powered applications able to provide constructive feedback to student writers and their teachers in a writing-lab context. The project culminated in a successful field test in a district of the public school system and ultimately a study conducted jointly with the Educational Testing Service (ETS) to evaluate the potential of such software to holistically score student written products. The joint conclusion:\n\n\"...computer analysis of student ways can provide a level of detail in feedback to students, teachers, and others that is not possible using human readers alone. This kind of feedback has important implications for instruction in English composition. Moreover, computer analysis can provide detailed feedback on many written products, even lengthy ones; a teacher of English will normally provide detailed feedback on only a few brief essays.\"\n\nIn more recent research, Millett has independently demonstrated the ability of the same JG-based software to evaluate the writing of ESL students of mixed nationalities. Owing to its demonstrated potential for educational assessment, overtures have been made to declare JGPL and its associated software applications - \"The WordMap Writing-Aids Software Ensemble\" - ‘open source’ so that it may serve as an expandable and adaptable public educational resource. This proposal is presently 'under advisement.' Meanwhile, to test the utility of JG-related writing assessment in an internet environment, the JGPL analysis engine has been made available free of charge for experimentation in an online writing-lab setting.\n\nThis tree was constructed in a tutorial setting equipped with experimental software for computer-aided JG diagramming. Note the explicit specification of junction operators on the squiggly lines connecting the node labels. This is one aspect of JG linguistic description which sets it apart from other models and from which its name derives. Conventional branching diagrams may also be used, but in any case the junction operator \"must\" be written between the nodes serving as its operands. \"Junction theory holds that linguistic structures written without junctions have no more meaning/substance than concatenations of algebraic operands written without any indication of the operations to be performed with them.\" Because of its algebraic format - and owing to its use of non-verbal formatives - JG base description has of late been dubbed \"MindMath.\"\n\n\nSome theoreticians have suggested that it would be productive to merge certain features of junction grammar with other models. Millett and Lonsdale, in fact, have proposed an expansion of Tree Adjoining Grammar (TAG) to create junction trees.\n\nIn planning for systematic expansion of the JG model of language, ‘elevator shafts’ were included in its original layout for the eventual incorporation of modules to deal with such phenomena as sensation, cognition, mental modeling, and communication - all of these being considered integral to the operation of language in its natural setting. Inasmuch as the addition of modules entails addition of data types and their dictionaries, reference was made in the literature to such future expansion as a function of `orders of specificity,' with each module or significant rule refinement ascending to a higher order, as it were.\n\nJG had - through its use of non-verbal data types - tacitly implemented Whorf’s expanded definition of language from its inception. To wit:\n\n...The linguistic order embraces all symbolism, all symbolic process, all process of reference and of logic...\n\nAt that point where formal development of the module for \"Level 1\" (the `real’ world) was contemplated, it became apparent that JG was graduating from a model managing the connection between MIND and EXPRESSION as a matter of linkage between familiar and readily representable data types (e.g., between the sememic data of the ‘mind tract’ and those required for the 'vocal tract') ... to a model which must now also make explicit the connection between MIND and REALITY in a general way among a profusion of data types systematically written upon by MIND in the ‘cold and dreary’ world itself. The fact of the matter was that even though \"Level 1\" had been occupying space in the schematic from its inception, with the exception of voice synthesis, no ‘order of specificity’ had as yet been implemented in JG to formalize the linkage between sememic data and forms of physical reality.\n\nThat the linkage existed was simply a matter of observation: Each time an instruction was given and carried out, for example, the structuring specified by the instruction was realized in the physical media to which the terms of the language referred. Ditto for the relation between recipes and cakes, blueprints and houses, constitutions and governments, etc. In each case, the linguistic relations became real, with the referents to which they applied representing themselves - a circumstance referred to in the parlance of JG as \"reflexive symbolism\". As the ramifications of this scenario were examined, it became apparent that the emergent ‘JG Upgrade Model’ was now making direct contact with Benjamin Whorf’s full conceptual model, which saw structuring in the world at large as a \"physical\" extension of the language in MIND, while writing and speech - by way of comparison - were \"rational\" extensions of language in MIND. To quote Whorf:\n\nHere then was the explanation for the relevance of JG’s fundamental constituent relations to external relationships and the reason why Lytle had been able to represent articulatory structuring with junction operations. There is indeed a linguistic linkage between MIND and REALITY ... and hence, theoretically, a 'linguistic' description for every structured phenomenon. This, perhaps, portends the ultimate ascent of linguistic science to a level of prominence previously envisioned only by Whorf. (We resume discussion of this eventuality below.)\n\nTo formally implement the linkage in question as a feature of the JG Upgrade Model, it was necessary to add notational symbolism able to describe the nature of the coding which transpires at the MIND-REALITY interface. To this end, symbols were introduced to signify the mental processes which are used to create instruction and to apply it, as well as the act of describing such processes. To this end, corresponding operators were added to the JG inventory for:\n\n\nOther symbolism was incorporated to signify authorship ( Kx ), as well as the language being employed (Lx ) vis-a-vis the \"language continuum\" (the global blend of ideolects, dialects, and languages), as well as the data types operative as media to be structured in the physical domain (Mx).\n\nThese notational addenda, when systematically organized and augmented with other pertinent formal devices, enable the linguist to write expressions for such diverse activities as mental modeling, conversation, putting one’s words into action, and use of the empirical method for discovery. Inasmuch as the system of classical junction rules can be derived directly from the new, more comprehensive symbolism, it is clear that the enhancements in question are a natural extension of the original model.\n\n"}
{"id": "5934430", "url": "https://en.wikipedia.org/wiki?curid=5934430", "title": "János Harmatta", "text": "János Harmatta\n\nJános Harmatta (2 October 1917 – 24 July 2004) was a Hungarian linguist. He deciphered the Parthian ostraca and papyri of Dura Europos and was the first to decipher a major Bactrian inscription.\n\nHe taught as a professor at the Hungarian Academy of Sciences.\n\n\n\n"}
{"id": "50341091", "url": "https://en.wikipedia.org/wiki?curid=50341091", "title": "Late Modernism (book)", "text": "Late Modernism (book)\n\nLate Modernism: Art, Culture, and Politics in Cold War America is a history book about late modernism as a historical period of aesthetics between modernism and postmodernism. The book was written by Robert Genter and published by University of Pennsylvania Press in 2010.\n\n"}
{"id": "3904796", "url": "https://en.wikipedia.org/wiki?curid=3904796", "title": "Lexical similarity", "text": "Lexical similarity\n\nIn linguistics, lexical similarity is a measure of the degree to which the word sets of two given languages are similar. A lexical similarity of 1 (or 100%) would mean a total overlap between vocabularies, whereas 0 means there are no common words.\n\nThere are different ways to define the lexical similarity and the results vary accordingly. For example, Ethnologue's method of calculation consists in comparing a standardized set of wordlists and counting those forms that show similarity in both form and meaning. Using such a method, English was evaluated to have a lexical similarity of 60% with German and 27% with French.\n\nLexical similarity can be used to evaluate the degree of genetic relationship between two languages. Percentages higher than 85% usually indicate that the two languages being compared are likely to be related dialects.\n\nThe lexical similarity is only one indication of the mutual intelligibility of the two languages, since the latter also depends on the degree of phonetical, morphological, and syntactical similarity. The variations due to differing wordlists weigh on this. For example, lexical similarity between French and English is considerable in lexical fields relating to culture, whereas their similarity is smaller as far as basic (function) words are concerned. Unlike mutual intelligibility, lexical similarity can only be symmetrical.\n\nThe table below shows some lexical similarity values for pairs of selected Romance, Germanic, and Slavic languages, as collected and published by Ethnologue.\n\n\"Notes:\"\n\n\n\n"}
{"id": "32919718", "url": "https://en.wikipedia.org/wiki?curid=32919718", "title": "List of World Heritage sites in Malaysia", "text": "List of World Heritage sites in Malaysia\n\nThe UNESCO (United Nations Educational, Scientific and Cultural Organization) has designated four World Heritage sites in Malaysia. The UNESCO World Heritage sites are places of importance to cultural or natural heritage as described in the UNESCO World Heritage Convention.\n\n, there are four site on the Tentative List for Malaysia:\n\n"}
{"id": "216931", "url": "https://en.wikipedia.org/wiki?curid=216931", "title": "List of anonymously published works", "text": "List of anonymously published works\n\nThroughout the history of literature, since the creation of bound texts in the forms of books and codices, various works have been published and written anonymously, often due to their political or controversial nature, or merely for the purposes of the privacy of their authors, among other reasons. This article provides a list of literary works published anonymously, either attributed to \"Anonymous\", or with no specific author's name given.\n\nNot included in this list are works which predate the advent of publishing and general attribution of authorship, such as ancient written inscriptions (such as hieroglyphic or pictographical, transcribed texts), certain historical folklore and myths of oral traditions now published as text, and reference or plain texts (letters, notes, graffiti) recovered archaeologically, which are otherwise unimportant to literary studies. Religious texts and grimoires, which are often written anonymously, may appear, along with works initially written anonymously whose authors are now known.\n\nThis list is ordered alphabetically by title.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42003318", "url": "https://en.wikipedia.org/wiki?curid=42003318", "title": "List of bad luck signs", "text": "List of bad luck signs\n\nBad luck is harmful, negative, or undesirable luck or fortune. This is a list of signs believed to be bring bad luck according to superstitions:\n\n"}
{"id": "10593222", "url": "https://en.wikipedia.org/wiki?curid=10593222", "title": "Livingstone–Stanley Monument", "text": "Livingstone–Stanley Monument\n\nThe Livingstone–Stanley Monument at Mugere in Burundi is 12 km south of the capital Bujumbura, overlooking Lake Tanganyika, and marks a location where explorer and missionary Dr David Livingstone and journalist and explorer Henry Morton Stanley visited and spent two nights on 25–27 November 1871. In French it is referred to as \"La Pierre de Livingstone et Stanley\". Some Burundians claim the location is where the famous first meeting of Livingstone and Stanley took place, at which the latter uttered the famous words \"Dr Livingstone, I presume?\".\n\nHowever that meeting actually took place in Ujiji in Tanzania on 10 November 1871 as clearly detailed in Stanley's book, \"How I Found Livingstone\". David Livingstone's journal also confirms Ujiji as the location, with an entry the day before the meeting reading \"At dawn, off and go to Ujiji\", a town he knew well. Livingstone then details meetings with several Arab residents of Ujiji including one who was supposed to be keeping his goods from his previous visit, before recording Stanley's arrival.\n\nFrom their writings, the visit to Mugere appears to be the one on 25–27 November which Livingstone and Stanley described as being one of the most hospitable they enjoyed. The date 25 November 1871 can be seen scratched on the rock. They had rested in Ujiji for six days, and then set off by canoe up the north-east shore of the lake to explore rivers which might flow out of the Lake Tanganyika. At the Mugere River they found the village of Chief Mukropeans to visit the area, their arrival was memorable, and it must be at some time later the event became confused in some people's minds as the first meeting between Livingstone and Stanley. A number of websites make this wrong claim.\n\n"}
{"id": "35845481", "url": "https://en.wikipedia.org/wiki?curid=35845481", "title": "Maal og Minne", "text": "Maal og Minne\n\nMaal og Minne (\"Language and Memory\") is a Norwegian academic journal of linguistics established in 1909 by Magnus Olsen. It covers research on Scandinavian languages, focusing mainly on language history and philology. It is a \"level 2\" journal in the Norwegian Scientific Index. The current editors-in-chief are Lars S. Vikør and Jon Gunnar Jørgensen.\n\nThe following persons are or have been editors of the journal:\n"}
{"id": "53518344", "url": "https://en.wikipedia.org/wiki?curid=53518344", "title": "Max &amp; Ivan", "text": "Max &amp; Ivan\n\nMax Olesker and Ivan Gonzalez are a British comedy duo known collectively as Max & Ivan. They are the creators, writers and stars of the BBC Radio 4 series \"The Casebook of Max & Ivan\" and Channel 4 Comedy Blap \"The Reunion.\" They also appear together as Ben & Jerry in BBC Two's W1A. The duo met at Royal Holloway, University of London where they produced the radio show (and later podcast) \"Max and Ivàn: Exposed\" for the college's Insanity Radio station. They co-founded London improvised comedy theatre The Free Association and perform live narrative sketch comedy across the world.\n\nIn episode 73 of The Comedian's Comedian with Stuart Goldsmith, Max & Ivan describe their live shows as narrative sketch comedy; writing character-led narratives that feature interweaving plot lines. They perform these multi-character shows with no costume changes and minimal use of props. \n\nComedy acts that Max & Ivan have cited as influences include The League of Gentlemen, Victoria Wood, Lucas and Walliams, Steve Coogan and the Marx Brothers.\n"}
{"id": "477264", "url": "https://en.wikipedia.org/wiki?curid=477264", "title": "Mr. Bill", "text": "Mr. Bill\n\nMr. Bill is a clay figurine clown star of a parody of children's shows, created by Walter Williams. Mr. Bill got its start on \"Saturday Night Live\" as a series Super 8 film sent in response to the show's request for home movies during the first season. Mr. Bill's first appearance occurred on the February 28, 1976, episode. After five submitted films, Williams became a full-time writer for the show in 1978, and would ultimately write more than 20 sketches based on Mr. Bill.\nEach Mr. Bill episode would start innocently enough but would quickly turn dangerous for Mr. Bill. Along with his dog, Spot, he would suffer various indignities inflicted by \"Mr. Hands,\" a man seen only as a pair of hands (originally performed by Vance DeGeneres).\n\nSometimes the abuse would ostensibly come from the mean Sluggo, another clay character. The violence would inevitably escalate, generally ending with Mr. Bill being crushed or dismembered while squealing in a high pitched voice, \"Ohhhh noooooooooooooo...\". The concept for Mr. Hands came from Williams' observation that children's cartoons in the 70s were so static, he expected the artist's hands to enter the screen at any moment and physically start moving the drawings around.\n\nInitial \"Saturday Night Live\" sketches featuring Mr. Bill were self-contained episodes with no direct continuity, with the earliest installments featuring higher pitched character voices. After Walter Williams joined \"SNL\"'s writing staff in 1978, Mr. Bill formally moved to New York at the start of the season. Later sketches saw Mr. Bill become aware of Mr. Hands and Sluggo's mistreatment, with the 1979-80 season harboring an extended story arc where Mr. Bill lost his home, sought psychiatric help, attempted to get Mr Hands and Sluggo arrested, and was ultimately thrown into prison.\n\nThough Williams left \"Saturday Night Live\" after that season, Mr. Bill returned for a Christmas short film in December 1980, as well as the sixth-season finale, where guest Chevy Chase found Mr. Bill in a garbage can. The last Mr. Bill sketch on \"SNL\" aired early in the 1981-1982 season, where Mr. Bill moved to Los Angeles. After \"SNL\", Mr. Bill has subsequently appeared on numerous other television programs and advertisements, including regular new sketches on the USA Network series \"Night Flight\" in the 1980s.\n\n\n\nThe character's popularity spawned the 1986 live-action Showtime television film \"Mr. Bill's Real Life Adventures\", with Peter Scolari as Mr. Bill.\n\nA new Mr. Bill short film entitled \"Mr. Bill Goes To Washington\" premiered in theaters in 1993, preceding the movie \"Ernest Rides Again\". The short, which sees Mr. Bill elected as President of the United States, was also featured on the \"Ernest Rides Again\" home video release.\n\nTwo new Mr. Bill home videos were released in the mid-1990s featuring new content, including 1996's \"Ohh Nooo!!! It's Mr. Bill's 20th Anniversary\", and 1997 straight-to-video \"Ho Ho Noooooo!!! It's Mr Bill's Christmas Special!\", the latter featuring a guest appearance by former SNL contributor Don Novello as Father Guido Sarducci.\n\n\n"}
{"id": "50897484", "url": "https://en.wikipedia.org/wiki?curid=50897484", "title": "Muthurangam Govt. Arts College", "text": "Muthurangam Govt. Arts College\n\nMuthurangam Govt. Arts College (MGAC) is located in Bagayam, Vellore, Tamil Nadu, India. The college is affiliated with Thiruvalluvar University. This college offers different courses in arts, commerce and science.\n\nMuthurangam Govt. Arts College was inaugurated on 1965 by then Chief Minister Hon'able. Backthavachalam.\n\n\nCourse offered : \n\n\n\n"}
{"id": "22204931", "url": "https://en.wikipedia.org/wiki?curid=22204931", "title": "Natalia O'Shea", "text": "Natalia O'Shea\n\nNatalia Andreyevna O'Shea (, née Nikolayeva, , known as Hellawes; born 3 September 1976) is a Russian harpist, singer-songwriter, linguist and band leader of \"Melnitsa\" (folk-rock), \"Clann Lir\" (traditional Celtic folk) and \"Romanesque\" (folk). Earlier she took part in the \"Till Eulenspiegel\" project (folk), for which she was a vocalist, author and co-author of many songs.\n\nO'Shea is a linguist, and expert in Indo-European languages especially Celtic languages. She is also a PhD of philological science and was an instructor in Lomonosov Moscow State University. Earlier she worked in Trinity College in Dublin (Ireland). Natalia has been living and working in Ireland (Dublin) and in Switzerland (Geneva) since 2004, but periodically she returns in Russia to take part in concerts of \"Melnitsa\", \"Clann Lir\" or in solo concerts.\n\nO'Shea has been appearing on the stage since 1998. She is one of the most popular folk-rock singers on the Russian scene and cult author-performer in the youth (and especially in student) midst of Moscow, Saint-Petersburg and other cities. Also she is a very popular singer for the players of \"role-playing game movement\".\n\nThe Hellawes's songs have spread through the Internet and have won recognition of listeners long ago in many cities of Russia and outside.\n\nNikolayeva married James Cornelius O'Shea, an Irish citizen, who was a member of staff at the Irish Embassy in Moscow on the 21st of August 2004, and on 22 July 2008 Natalia O'Shea gave birth to a daughter, Nina Caitríona O'Shea, in Geneva (Switzerland). 15 April 2011 she gave birth to their second daughter, Úna Tamar.\n\n\n\n"}
{"id": "56026131", "url": "https://en.wikipedia.org/wiki?curid=56026131", "title": "National Prize for Humanities and Social Sciences (Chile)", "text": "National Prize for Humanities and Social Sciences (Chile)\n\nThe National Prize for Humanities and Social Sciences () was created in Chile in 1992 under Law 19169. It is granted \"to the humanist, scientist, or academic, who has distinguished himself for his contribution in the field of Human Sciences\" (Article 8 of the aforementioned law). The history field has its own National Award.\n\nThe prize, which is awarded every two years, consists of a diploma, the sum of 6,576,457 pesos () which is adjusted every year, according to the previous year's consumer price index, and a pension of 20 (approximately US$1,600).\n\nIt is part of the National Prize of Chile, awarded by the President of the Republic.\n\n"}
{"id": "10304397", "url": "https://en.wikipedia.org/wiki?curid=10304397", "title": "Neo-Sovietism", "text": "Neo-Sovietism\n\nNeo-Sovietism is the Soviet Union-style of policy decisions in some Post-Soviet states, as well as a political movement of reviving the Soviet Union in the modern world or to reviving specific aspects of Soviet life based on the nostalgia for the Soviet Union. Some commentators have said that current Russian President Vladimir Putin holds many neo-Soviet views, especially concerning law and order and military strategic defense.\n\nAccording to Pamela Druckerman of \"The New York Times\", an element of Neo-Sovietism is that \"the government manages civil society, political life and the media.\" \n\nAccording to Mathew Kaminski of The Wall Street Journal, in includes efforts by Putin to express the glory of the Soviet Union in order to generate support for a \"revived Great Russian power in the future\" by bringing back memories of various Russian accomplishments that legitimatized Soviet dominance, including the Soviet victory against Nazi Germany. Kaminski continues on by saying that Neo-Sovietism \"offers up Russian jingoism stripped bare of Marxist internationalist pretenses\" and uses it to scare Russia's neighbours and to generate Russian patriotism and anti-Americanism.\n\nAndrew Meier of the Los Angeles Times in 2008 listed three points that laid out Neo-Sovietism and how modern Russia resembles the Soviet Union: First was that Russia was a land of doublespeak. Meier claims that Russia has deliberately distorted words and facts on various subjects, particularly regarding the Russo-Georgian War at the time by claiming that the United States instigated the conflict and that Georgia was committing genocide in South Ossetia. Second was that Russia was willing to enhance its power by any means possible, including harsh repression of its own citizens with examples being Mikhail Khodorkovsky and the Mothers of Beslan. Lastly, Russia remains a land in which \"fear of the state -- and its suffocating reach -- prevails\" by introducing numerous laws that limit free expression and promote propaganda.\n\n\n\n"}
{"id": "37495", "url": "https://en.wikipedia.org/wiki?curid=37495", "title": "Noun", "text": "Noun\n\nA noun (from Latin \"nōmen\", literally meaning \"name\") is a word that functions as the name of some specific thing or set of things, such as living creatures, objects, places, actions, qualities, states of existence, or ideas. Linguistically, a noun is a member of a large, open part of speech whose members can occur as the main word in the subject of a clause, the object of a verb, or the object of a preposition.\n\nLexical categories (parts of speech) are defined in terms of the ways in which their members combine with other kinds of expressions. The syntactic rules for nouns differ from language to language. In English, nouns are those words which can occur with articles and attributive adjectives and can function as the head of a noun phrase.\n\nWord classes (parts of speech) were described by Sanskrit grammarians from at least the 5th century BC. In Yāska's \"Nirukta\", the noun (\"nāma\") is one of the four main categories of words defined.\n\nThe Ancient Greek equivalent was \"ónoma\" (ὄνομα), referred to by Plato in the \"Cratylus\" dialog, and later listed as one of the eight parts of speech in \"The Art of Grammar\", attributed to Dionysius Thrax (2nd century BC). The term used in Latin grammar was \"nōmen\". All of these terms for \"noun\" were also words meaning \"name\". The English word \"noun\" is derived from the Latin term, through the Anglo-Norman \"noun\".\n\nThe word classes were defined partly by the grammatical forms that they take. In Sanskrit, Greek and Latin, for example, nouns are categorized by gender and inflected for case and number. Because adjectives share these three grammatical categories, adjectives are placed in the same class as nouns.\n\nSimilarly, the Latin \"nōmen\" includes both nouns (substantives) and adjectives, as originally did the English word \"noun\", the two types being distinguished as \"nouns substantive\" and \"nouns adjective\" (or \"substantive nouns\" and \"adjective nouns\", or short \"substantives\" and \"adjectives\"). (The word \"nominal\" is now sometimes used to denote a class that includes both nouns and adjectives.)\n\nMany European languages use a cognate of the word \"substantive\" as the basic term for noun (for example, Spanish \"sustantivo\", \"noun\"). Nouns in the dictionaries of such languages are demarked by the abbreviation \"s.\" or \"sb.\" instead of \"n.\", which may be used for proper nouns or neuter nouns instead. In English, some modern authors use the word \"substantive\" to refer to a class that includes both nouns (single words) and noun phrases (multiword units, also called noun equivalents). It can also be used as a counterpart to \"attributive\" when distinguishing between a noun being used as the head (main word) of a noun phrase and a noun being used as a noun adjunct. For example, the noun \"knee\" can be said to be used substantively in \"my knee hurts\", but attributively in \"the patient needed knee replacement\".\n\nNouns have sometimes been defined in terms of the grammatical categories to which they are subject (classed by gender, inflected for case and number). Such definitions tend to be language-specific, since nouns do not have the same categories in all languages.\n\nNouns are frequently defined, particularly in informal contexts, in terms of their semantic properties (their meanings). Nouns are described as words that refer to a \"person\", \"place\", \"thing\", \"event\", \"substance\", \"quality\", \"quantity\", etc. However this type of definition has been criticized by contemporary linguists as being uninformative.\n\nThere have been offered several examples of English-language nouns which do not have any reference: \"drought\", \"enjoyment\", \"finesse\", \"behalf\" (as found in \"on behalf of\"), \"dint\" (\"in dint of\"), and \"sake\" (\"for the sake of\"). Moreover, there may be a relationship similar to reference in the case of other parts of speech: the verbs \"to rain\" or \"to mother\"; many adjectives, like \"red\"; and there is little difference between the adverb \"gleefully\" and the noun-based phrase \"with glee\".\n\nThere are placeholder names, such as the legal fiction \"reasonable person\" (whose existence is not in question), an experimental \"artifact\", or personifications such as \"gremlin\".\n\nLinguists often prefer to define nouns (and other lexical categories) in terms of their formal properties. These include morphological information, such as what prefixes or suffixes they take, and also their syntax – how they combine with other words and expressions of particular types. Such definitions may nonetheless still be language-specific, since syntax as well as morphology varies between languages. For example, in English it might be noted that nouns are words that can co-occur with definite articles (as stated at the start of this article), but this would not apply in Russian, which has no definite articles.\n\nThere have been several attempts, sometimes controversial, to produce a stricter definition of nouns on a semantic basis. Some of these are referenced in the section below.\n\nIn some languages, genders are assigned to nouns, such as masculine, feminine and neuter. The gender of a noun (as well as its number and case, where applicable) will often entail agreement in words that modify or are related to it. For example, in French, the singular form of the definite article is \"le\" with masculine nouns and \"la\" with feminines; adjectives and certain verb forms also change (with the addition of with feminines). Grammatical gender often correlates with the form of the noun and the inflection pattern it follows; for example, in both Italian and Russian most nouns ending are feminine. Gender can also correlate with the sex of the noun's referent, particularly in the case of nouns denoting people (and sometimes animals). Nouns arguably do not have gender in Modern English, although many of them denote people or animals of a specific sex (or \"social gender\"), and pronouns that refer to nouns must take the appropriate gender for that noun. (The girl lost her spectacles.)\n\nA \"proper noun\" or \"proper name\" is a noun representing unique entities (such as \"India\", \"Pegasus\", \"Jupiter\", \"Kaumarya saurav\", \"Confucius\", or \"Pequod\"), as distinguished from common nouns which describe a class of entities (such as \"country\", \"animal\", \"planet\", \"person\" or \"ship\").\n\n\"Count nouns\" or \"countable nouns\" are common nouns that can take a plural, can combine with numerals or counting quantifiers (e.g., \"one\", \"two\", \"several\", \"every\", \"most\"), and can take an indefinite article such as \"a\" or \"an\" (in languages which have such articles). Examples of count nouns are \"chair\", \"nose\", and \"occasion\".\n\n\"Mass nouns\" or \"uncountable\" (or \"non-count\") \"nouns\" differ from count nouns in precisely that respect: they cannot take plurals or combine with number words or the above type of quantifiers. For example, it is not possible to refer to \"a furniture\" or \"three furnitures\". This is true even though the pieces of furniture comprising \"furniture\" could be counted. Thus the distinction between mass and count nouns should not be made in terms of what sorts of things the nouns refer to, but rather in terms of how the nouns \"present\" these entities.\n\nMany nouns have both countable and uncountable uses; for example, \"soda\" is countable in \"give me three sodas\", but uncountable in \"he likes soda\".\n\n\"Collective nouns\" are nouns that – even when they are inflected for the singular – refer to \"groups\" consisting of more than one individual or entity. Examples include \"committee\", \"government\", and \"police\". In English these nouns may be followed by a singular or a plural verb and referred to by a singular or plural pronoun, the singular being generally preferred when referring to the body as a unit and the plural often being preferred, especially in British English, when emphasizing the individual members. Examples of acceptable and unacceptable use given by Gowers in \"Plain Words\" include:\n\n\"Concrete nouns\" refer to physical entities that can, in principle at least \"(i.e. different schools of philosophy and sciences may question the assumption, but, for the most part, people agree to the existence of something. E.g. a rock, a tree, universe)\", be observed by at least one of the senses (for instance, \"chair\", \"apple\", \"Janet\" or \"atom\"). \"Abstract nouns\", on the other hand, refer to abstract objects; that is, ideas or concepts (such as \"justice\" or \"hatred\"). While this distinction is sometimes exclusive, some nouns have multiple senses, including both concrete and abstract ones: consider, for example, the noun \"art\", which usually refers to a concept (e.g., \"Art is an important element of human culture.\") but which can refer to a specific artwork in certain contexts (e.g., \"I put my daughter's art up on the fridge.\")\n\nSome abstract nouns developed etymologically by figurative extension from literal roots. These include \"drawback\", \"fraction\", \"holdout\" and \"uptake\". Similarly, some nouns have both abstract and concrete senses, with the latter having developed by figurative extension from the former. These include \"view\", \"filter\", \"structure\" and \"key\".\n\nIn English, many abstract nouns are formed by adding a suffix (\"-ness\", \"-ity\", \"-ion\") to adjectives or verbs. Examples are \"happiness\" (from the adjective \"happy\"), \"circulation\" (from the verb \"circulate\") and \"serenity\" (from the adjective \"serene\").\n\nSome languages refer to nouns differently, depending on how ownership is being given for the given noun. This can be broken into two categories: alienable and inalienable. An alienable noun is something that does not belong to a person indefinitely. Inalienable nouns, on the other hand, refer to something that is possessed definitely. Examples of alienable nouns would be a tree or a shirt or roads. Examples of inalienable nouns would be a father or shadow or hair.\n\nThe Pingelapese language uses a distinction between nouns. There are several classifier forms: The first is for objects which tend to be pretty large in size and not being a favorite possession (tree or shirt), and the second is for small, controllable, favorite objects like dogs, books or spears. A third form would be set aside for food objects like bananas, oranges or fish. Drinks like water or coconut liquor also have classifier forms. A fifth classifier would be designated for things that are to be chewed but not fully consumed. The only example of this was from the book \"Papers in Kosraean and Ponapeic\": the fruit, pandanus, is chewed for the sweet/bitter juice, but what remains after consuming the juice discarded. The 6th classifier forms are set aside for ways of transportation (bikes, canoes, and boats). The last two classifiers are designated for land and houses.\n\nA noun phrase is a phrase based on a noun, pronoun, or other noun-like word (nominal) optionally accompanied by modifiers such as determiners and adjectives. A noun phrase functions within a clause or sentence in a role such as that of subject, object, or complement of a verb or preposition. For example, in the sentence \"The black cat sat on a dear friend of mine\", the noun phrase \"the black cat\" serves as the subject, and the noun phrase \"a dear friend of mine\" serves as the complement of the preposition \"on\".\n\nNouns and noun phrases can typically be replaced by pronouns, such as \"he\", \"it\", \"which\", and \"those\", in order to avoid repetition or explicit identification, or for other reasons. For example, in the sentence \"Gareth thought that he was weird\", the word \"he\" is a pronoun standing in place of the person's name. The word \"one\" can replace parts of noun phrases, and it sometimes stands in for a noun. An example is given below:\n\nBut \"one\" can also stand in for larger parts of a noun phrase. For example, in the following example, \"one\" can stand in for \"new car\".\n\nNominalization is a process whereby a word that belongs to another part of speech comes to be used as a noun.\nIn French and Spanish, for example, adjectives frequently act as nouns referring to people who have the characteristics denoted by the adjective. This sometimes happens in English as well, as in the following examples:\n\n\n\n\nFor definitions of nouns based on the concept of \"identity criteria\":\n\nFor more on identity criteria:\n\nFor the concept that nouns are \"prototypically referential\":\n\nFor an attempt to relate the concepts of identity criteria and prototypical referentiality:\n\nUnderstanding nouns in the context of WordNet:\n\n"}
{"id": "22484742", "url": "https://en.wikipedia.org/wiki?curid=22484742", "title": "Obadiah Rich", "text": "Obadiah Rich\n\nObadiah Rich (November 25, 1777 – January 20, 1850) was an American diplomat, bibliophile and bibliographer specializing the history of Latin America. He was credited with making the field of Americana a recognized field of scholarship by the bibliographer Nicholas Trübner.\n\nObadiah Rich was born on Cape Cod, at Truro, Massachusetts, on November 25, 1777. He was the son of Captain Obadiah Rich (1758–1805) who commanded the brig \"Intrepid\" in the American Revolutionary War and his first wife Salome Lombard (1761–1807). He was the older brother of the botanist William Rich.\n\nObadiah Rich was elected to the Massachusetts Historical Society at the early age of 22, and helped found the Anthology Society in 1804, which later became the Boston Athenæum. President James Madison appointed him American consul in Valencia, Spain, in 1816. He was consul in Madrid from 1823, and was working full-time in the book trade in London by 1830. He was again resident in Madrid and in Mahón on the island of Menorca between 1834 and 1835. He was elected a member of the American Antiquarian Society in 1834.\n\nWhile in Spain, Rich compiled an extensive collection of ancient Spanish and Latin American books and manuscripts, and was part of the circle of Latin America historians and scholars that included George Ticknor, William H. Prescott, and Washington Irving, who researched his 1828 biography of Christopher Columbus while staying with Rich in Madrid.\n\nRich wrote \"A Catalog of Books relating principally to America, arranged under the Years in which they were printed, 1500-1700\" (London, 1832); \"Miscellaneous Catalog of Books in all Languages\" (1834), and \"Bibliotheca Americana Nova, or a Catalog of Books in Various Languages, relating to America, printed since the Year 1700\" (2 vols., London and New York, Volume I, 1835 and Volume II, 1846).\n\nHe died in London in 1850.\n\nRich's books eventually were acquired by Edward G. Allen of London, and dispersed. A substantial portion were acquired by the American bibliophile James Lenox in 1848 who subsequently donated them to the New York Public Library in 1897. The Obadiah Rich Collection is now housed in the Library’s Manuscripts and Archives Division.\n\nThis collection contains hundreds of original manuscripts and transcriptions of manuscripts covering the period from Christopher Columbus's first voyage of 1492 to the last years of the colonial period. The collection contains papers on New Spain (Mexico), Peru and the other Spanish colonies and Brazil. The collection is documented in the catalog \"Colonial Latin American Manuscripts and Transcripts in the Obadiah Rich Collection\". Among his notable pieces is the only known copy of the first printing of Columbus's announcement of his discovery (Barcelona, 1493) and \"The Brief and Most Concise Christian Doctrine in the Mexican Language\" of Juan de Zumárraga, first bishop of Mexico. This volume is considered to be the first book printed in the Americas. It was printed in an imported press in the Casa de las Campanas in Mexico City in 1543, nearly one hundred years before the first book was printed in the English colonies.\n\n\n"}
{"id": "34518962", "url": "https://en.wikipedia.org/wiki?curid=34518962", "title": "Paolo and Francesca da Rimini", "text": "Paolo and Francesca da Rimini\n\nPaolo and Francesca da Rimini is a watercolour by English artist and poet Dante Gabriel Rossetti, painted in 1855 and currently housed at Tate Britain.\n\nRossetti's real name was Charles Gabriel Dante Rossetti, but his admiration for the great Florentine poet led him to change it to Dante Gabriel Rossetti, and he proceeded to sign all his work so. In the specific, the very subject of this painting is taken from Dante Aligheri's \"Inferno\", Canto V – it is a small watercolour triptych executed in the archaic, medievalising style of this period in Rossetti's art, and was never painted in oil. Although the artist had been sketching the subject for many years, the watercolour took him just one week to complete. The buyer was the writer and critic John Ruskin. The drawing is simple and the colours generally muted. Only Francesca's long golden hair looks forward to the more sensuous creatures of Rossetti's later works. The picture was originally planned as a triptych in oil, with the same scenes as in the watercolour, but with the lovers kissing as the central motif.\n\nFrancesca was the sister-in-law of Paolo Malatesta, and both were married, but they fell in love. Their tragic adulterous story was told by Dante in his \"Divine Comedy\", Canto V of the \"Inferno\", and was a popular subject with Victorian artists and sculptors, especially with followers of the Pre-Raphaelite ideology, and with other writers.\n\nThe triptych has several inscriptions taken from Canto V, with Rossetti bringing the story to life by writing relevant quotations in the original Italian around the edge of the composition. Its three parts read from left to right. The left-hand panel shows the adulterous kiss that condemns the lovers: staying faithful to Dante's poem, Rossetti depicts them reading about the Arthurian knight Sir Lancelot who also suffered for his forbidden love (his figure can be seen on the book's open page, dressed, like Paolo, in red and blue). The scene illustrates the following lines from Dante's text:\nThe central panel depicts two of Rossetti's literary heroes crowned with laurel: the Roman poet Virgil and the much-revered Dante himself – they regard with concern the two lovers on the right, who appear to float like wraiths in each other's arms, amid the flames of hell. Their adulterous relationship uncovered, they were murdered by Francesca's husband and Paolo's brother, Sigismondo Malatesta, and banished to the second circle of hell.\n\nIn the final panel of the triptych, the lovers are being blown about violently with the wind, as described by Dante's verses: \n\n\n\n"}
{"id": "12793759", "url": "https://en.wikipedia.org/wiki?curid=12793759", "title": "Philosophy of Mathematics Education Journal", "text": "Philosophy of Mathematics Education Journal\n\nThe Philosophy of Mathematics Education Journal is a peer-reviewed open-access academic journal published and edited by Paul Ernest (University of Exeter). It publishes articles relevant to the philosophy of mathematics education, a subfield of mathematics education that often draws in issues from the philosophy of mathematics. The journal includes articles, graduate student assignments, theses, and other pertinent resources.\n\nSpecial issues of the journal have focussed on\n\n"}
{"id": "278439", "url": "https://en.wikipedia.org/wiki?curid=278439", "title": "Power vacuum", "text": "Power vacuum\n\nIn political science and political history, the term power vacuum, also known as a power void, is an analogy between a physical vacuum, to the political condition \"when someone has lost control of something and no one has replaced them.\" The situation can occur when a government has no identifiable central power or authority. The physical analogy suggests that in a power vacuum, other forces will tend to \"rush in\" to fill the vacuum as soon as it is created, perhaps in the form of an armed militia or insurgents, military coup, warlord or dictator. The term is also often used in organized crime when a crime family becomes vulnerable to competition.\n\nHereditary or statutory order of succession or effective succession planning are orderly ways to resolve questions of succession to positions of power. When such methods are unavailable, such as in failed dictatorships or civil wars, a power vacuum arises, which prompts a power struggle entailing political competition, violence, or (usually) both. A power vacuum can also occur after a constitutional crisis in which large portions of the government resign or are removed, creating unclear succession.\n\nHistoric examples include the death of Alexander the Great, the 11th century power vacuum in the middle east which allowed the Seljuks to take over, the defeat of France in the Franco-Prussian War, the death of Vladimir Lenin, and the decrease in power of Great Britain and France in the Middle East after the Suez Crisis.\n\nDuring the course of the Ming treasure voyages (1405-1433), the Chinese Ming empire was the dominant political and military force within the Indian Ocean. However, in 1433, the Chinese government withdrew their treasure fleet and thus left a large void within the Indian Ocean.\n\nWhen in 2003 the United States led a coalition to oust Saddam Hussein in the Iraq War, the absence of an all-out Iraqi opposition force at war with government forces meant that once the Ba'ath Party was removed, no local figures were on hand to immediately assume the now-vacant administerial posts. For this reason, Paul Bremer was appointed by the United States government as the interim head of state to oversee the transition.\n\nIn other western-led interventions such as in Kosovo (1999) and Libya (2011) where the initial claim of justification in each case was a humanitarian matter, there had been active opposition fighting on the ground to oust the relevant governments (in the case of Kosovo, this meant removal of state forces from the desired territory rather than ousting the government itself). Subsequently, successor entities were immediately effective in Libya and Kosovo.\n\n"}
{"id": "55103546", "url": "https://en.wikipedia.org/wiki?curid=55103546", "title": "Sexism in medicine", "text": "Sexism in medicine\n\nSexism in medicine involves discriminating against patients, physicians, medical students and medical-school staff on the basis of gender. Most sexism is directed against women. The discrimination can involve comments on women's appearance; the imposition of an aggressive \"macho\" culture; verbal abuse; bullying; sexual assault; being passed over for promotion; and being denied treatment. Assessing the differences in pain treatment and the report of pain between men and women is complicated by the possibility that men and women perceive or experience pain differently. Men in nursing are often subjected to sexist treatment.\n\nFemale patients are often treated differently from men. Women have been described in studies and in narratives as emotional and hysterical. Historically, women's health has been called \"bikini medicine\". In addition, some physicians assume that women should be assessed and receive identical treatments as men. Narratives include the reporting that women's complaints are considered exaggerated and may be assumed to be invalid. Because of this women are often subsequently are referred to psychiatrists for treatment. The tendency of treating pain in women with antidepressants exposes the women to developing side effects to medication that they might not even need. The report of medical concerns by women are more likely to be discounted, misdiagnosed, ignored and assumed to be psychosomatic.\nOne observer has stated that, \"different forms of female suffering are minimized, mocked, coaxed into silence.\"\nThere are those that disagree with this characterization.\n\nClinicians are not as likely to assess women for substance abuse as often as they assess men. They also tend to miss signs of substance addiction in women. Women are not as likely as men to be assessed for alcohol abuse. Out of those women who are found to have an alcohol problem, they were found to be less likely to be referred for treatment. Those women in the childbearing years are prescribed more prescription medications than men. It is generally more common for women to be prescribed antipsychotics and opioids. \n\nWomen report feeling like they were 'silly' by male physicians but female physicians were more sensitive and preferred. In a study of multiple men, women, and married couples, it was observed that men’s complaints about physical health were evaluated more in depth than women’s.\n\nSex-selective abortion is the medical procedure or treatment that terminates a pregnancy when the baby is an undesired gender. The abortion of female fetuses is most common in areas where the culture values male children over females. \nSex selective abortion has been heavily utilized in numerous Asian countries. A British medical journal stated: \"Compared with the normal ratio of about 95 girls being born per 100 boys (which is what we observe in Europe and North America), Singapore and Taiwan have 92, South Korea 88, and China a mere 86 girls born per 100 boys.\"\n\nMost clinical trials published before 1988 included no women and so many older medications on the market were never evaluated for their effects and side effects on women. The physiology of male sex differentiation is described as \"well studied, whereas the pathways that regulate female sexual differentiation remain incompletely defined\".\n\nIn the 1950s and 1960s \"women's health' was mostly considered only as reproductive health, and women who were capable of bearing children were excluded from clinical trials to avoid any risk to a potential fetus. Additionally, the thalidomide tragedy led the FDA to issue regulations in 1977 recommending that women should be excluded from participating in Phase I and Phase II studies in the US. The approach to women shifted from paternalistic protection to access in the early 1980s as AIDS activists like ACT UP and women's groups challenged ways that drugs were developed. The NIH responded with policy changes in 1986, but a Government Accountability Office report in 1990 found that women were still being excluded from clinical research. That report, the appointment of Bernadine Healy as the first woman to lead the NIH, and the realization that important clinical trials had excluded women led to the creation of the Women's Health Initiative at the NIH and to the federal legislation, the 1993 National Institutes of Health Revitalization Act, which mandated that women and minorities be included in NIH-funded research. The initial large studies on the use of low-dose aspirin to prevent heart attacks that were published in the 1970s and 1980s are often cited as examples of clinical trials that included only men, but from which people drew general conclusions that did not hold true for women. In 1993 the FDA reversed its 1977 guidance, and included in the new guidance a statement that the former restriction was “rigid and paternalistic, leaving virtually no room for the exercise of judgment by responsible research subjects, physician investigators, and investigational review boards (IRBs)”.\n\nThe National Academy of Medicine published a report called \"Women and Health Research: Ethical and Legal Issues of Including Women in Clinical Studies\" in 1994 and another report in 2001 called \"Exploring the Biological Contributions to Human Health: Does Sex Matter?” which each urged including women in clinical trials and running analyses on subpopulations by sex.\n\nA 2005 review by the International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use found that regulation in the US, Europe, and Japan required that clinical trials should reflect the population to whom an intervention will be given, and found that clinical trials that had been submitted to agencies were generally complying with those regulations.\n\nA review of NIH-funded studies (not necessarily submitted to regulatory agencies) published between 1995 and 2010 found that they had an \"average enrollment of 37% (±6% standard deviation [SD]) women, at an increasing rate over the years. Only 28% of the publications either made some reference to sex/gender-specific results in the text or provided detailed results including sex/gender-specific estimates of effect or tests of interaction.\"\n\nThe FDA published a study of the 30 sets of clinical trial data submitted after 2011, and found that for all of them, information by sex was available in public documents, and that almost all of them included subanalyses by sex.\n\nAs of 2015, recruiting women to participate in clinical trials remained a challenge.\n\nIn 2018 the US FDA released draft guidelines for inclusion of pregnant women in clinical trials.\n\nElizabeth Blackwell became the first woman to graduate from a Western medical school in 1849. To raise awareness of the importance of women physicians, Physician Moms Group and Medelita founded February 3rd as National Women Physicians Day in 2016.\n\nFemale clinicians have experienced sexual assault. 30% of female clinicians have reported instances in which they were the victim of sexual harassment. Sexual harassment is common amongst younger clinicians when they come in contact with male clinicians in power who have more seniority over them. Due to their sense of power over their coworkers and employees, they feel empowered to commit acts of sexual assault. When victims of their abuse remain silent, they allow such acts to persist in medical workplaces. In many cases, the women that come forward about being assaulted have a hard time finding jobs afterwards because they are considered “troublemakers” rather than victims. This often makes it difficult for the victims to find other jobs in the medical field. Human Relations also tends to protect their company and its employees they consider assets before protecting those who are their victims. This discourages women from speaking up for fear that their jobs may be in jeopardy and that their claims will not be believed. Future female employees then suffer the consequences of their silence because the cycle of misconduct continues to occur. \n\nIn addition to falling victim to sexual assault in the workplace, female surgeons have also been found to fall victim to the wage gap. Females were reported to have lower salaries than male surgeons. In a study conducted in 1990, male clinicians were making a mean earnings of $155,400, while female clinicians were making a mean earnings of $109,900; about $45,500 less than their male counterparts. As of 2016, female physicians have statistically been found to make about $18,677 less than male physicians. Disparities between male and female surgeons has also been blamed upon not being as qualified as men to commit to leadership roles that earn them higher salaries. Yet women are just as willing as men to accept positions of leadership when they are equally qualified. In many cases, women clinicians are equal to men at leadership tasks. Other clinicians have expressed that they believe women in medicine are less committed to their careers and women are less effective as leaders.\n\nMoreover, female clinicians have also experienced barriers within finding the support to balance both working and having or maintaining a family. It has been reported that females are more likely to return back to work after having a child part-time as opposed to full-time because they lack the support they receive by both their employers and society as a whole. It was found that the percentage of female clinicians working part-time in either a hospital setting or a general physician’s office after having a child is much higher than the percentage of these women working full time after having a child (92.7%, 96.3% 59.2%, 76.5% respectively). Not only is there a difference within the percentage of females working part time and full time, there is also a relatively large difference within the percentages of female clinicians returning to their job in the hospital or in a general physicians office. Unfortunately, women are less likely to return back full time if having worked in the hospital because those shifts require longer hours and a lot of help that they may not be receiving.\n\nFurthermore, female physician narratives have described instances of sexism. Female physicians are often mistaken for nurses by patients. Patients have also been reported to have less trust in their physician if they are female and instead ask for a second opinion from a male physician. Women physicians, on the other hand, have also been found to partake in sexist actions. Female clinicians often treat women patients differently than they do men. Women physicians were found to admit less female patients to intensive care units because they were proactive in treating them in the emergency room, rendering their admittance to more intense care units unnecessary.\n\nMen often decide to become nurses for self-actualization or survival needs, or simply because their original plans did not work out. However, there are a handful of men who decide to become nurses and start their studies with that goal in mind. Unfortunately, when men enter the field of nursing, they encounter many barriers that limit their choice of specialty. They run the risk of being labeled and stereotyped. These gender biases and role stereotyping occur because of the fact that nursing facilities are often composed mainly of women. Nursing tends to be identified with feminine style of care.\n\nMales only make up 9% of nurses. Stereotyping of men is related to nursing being considered a profession for women. Men tend to face two common stereotypes when it comes to being nurses. The first being the stereotype that male nurses are gay since they are in a “feminine occupation.” The other common stereotype is that men are generally hypersexual and that this will inhibit them from being able to provide intimate care to women in nonsexual ways.\n\nIssues regarding sexism in/against male clinicians are harder to describe except possibly by example. Male nurses report:\n\n\nOther questions are often asked of male nurses such as 'why did you go into nursing'? Or they are asked if they are gay failed medical school and became a nurse because it was easier. Sometimes a male nurse can be asked if he is nurse so that he can see undressed women. In some instances male nurses were assumed to be the 'muscle' for other female nurses. Nursing supervisors tended to ask patients if it was alright to assign a male nurse to provide care. Male nurses have reported bias directed toward them during their studies. They experienced anxiety, insomnia, anger, and trepidation in anticipation of being treated poorly.\n\nAnother difficulty that male nurses face is that they are passed over for work with female patients, or they are not allowed on birthing or gynecological units. This is concerning due to the fact that male doctors are completely welcome in these situations. In addition, male nurses find that they are pushed toward tasks that are stereotypically consistent with their gender role. Some of these might include heavy lifting, administrative roles, or psychiatric nursing. \n\nWomen are underrepresented in leadership positions in academic medicine. Women and men begin their medical careers at similar rates but they do not advance at the same rate. Studies indicate a systematic bias that has resulted in relatively fewer appointments to academic chairs. Thirty-two percent of associate professors at medical schools are women, 32% of associate professors are women, 20% of full professors are women, 14% of department chairs are women, and 11% of deans of medical schools are women. \n\nA factor that impedes women’s opportunities for advancement in academic medicine is a “stereotype-based cognitive bias.” There are two forms of this. The first type is related to clear personal beliefs about women, such as believing that women are less committed to their careers than men and believing that women are worse leaders than men. The second type is implicit bias, which is harder to see because the biases are harder to see, but they still influence one’s judgment and actions towards women. Although implicit gender bias still plays a role, explicit bias in academic medicine has significantly decreased during the past half century in the United States as a result of Title IX getting passed. Implicit bias has had little to no improvement. Cultural stereotypes characterize women as “communal,” such as kind, dependent, and nurturing, but characterize women as lacking “agentric” traits, such as logical, independent, and strong, which are typically used as a male stereotype. These stereotypes make it difficult for women to achieve in the workforce, specifically in medicine, science, and in leadership. While men are associated with “agentic” traits and women are not, this can lead to women feeling that their work is less valued and they typically receive fewer nominations for opportunities that can advance their career. It has also been found that gender stereotypes play a role in socializing students towards their specialties. For example, women are more likely to go into communal specialties, including family medicine, pediatrics, and internal medicine, while men are more likely to go into surgery, research, and be the chair of a position. If women to go into specialties dominated by males, they typically have lower statuses. Residency is the first time the medical students, or new physicians, get to be in a leadership role. Men who are too communal can be accused of being “wimpy” or “soft” whereas women who are too agentic can be accused of being “bossy” or “domineering.” \n\nThese stereotypes are due to the lack of gender awareness and role models. Female medical students have reported sexual harassment and discrimination. This is of concern because these obstacles affect \"the professional identity formation and specialty choice.\" Personality differences exist between male and female surgical students. Fewer women choose to specialize in surgery. The lack of female role models may discourage some from choosing a surgical career.\n\nA study by the National Medical Foundation found that 60% of women have reported that gender has had an effect on their educational experience whereas only 25% of males have reported that gender has had an effect on their educational experience. Women said they felt as though they had to be twice as good to be treated equal to men. Additionally, 30.7% of women reported overcoming fear and failure whereas only 19.4% of males reported overcoming fear and failure in education.\n\nOne response to bias against women academics has been to conduct training for faculty and students to recognize bias and change their habits.\n\nCommunal specialties, which women are more likely to go into, often have a lower pay than the specialties in which men typically go into. Women have been found to have a larger representation than men in lower-paying specialties, such as pediatrics and men lave a larger representation in higher-paying specialties, such as cardiology and surgery. In New York State between 1999 and 2008, the average starting salary for men was $187,385 whereas the mean starting salary for women was $158,727. In 2001, it was found that male physicians earned roughly around 41% more than their female colleagues. As of 2017, an updated version then found that the percentage had dropped to roughly around 27.7%. That is roughly around a 100,000 dollar difference in salary per year. However, interestingly enough, women who work in radiology are the only women who make more than their male colleagues and the difference is only about 2,000 dollars. A study published in 2005 found that women physicians in the US had an annual earning gap of 11% if they were married, 14% if they had one child, and 22% if they had more than one child. Women typically had household obligations that affected their ability to work as much as men and therefore led to a trade-off of higher earnings for family-friendly jobs.\n\nDuring the Salem Witch Trials in the late 1600s, women were disproportionately accused of witchcraft due to induced seizures induced by mold. Despite ergotism affecting both male and female populations, young females were more likely to be tried and killed for witchcraft.\n\nDuring the late 1800s, physicians (predominately male) described physical ailments of women as 'hysteria'. In 1948 some women volunteered to take part in an experiment designed to quantify pain in laboring women. During their labor, their hands were burned.\n\nIn a 1979 observational study, 104 women and men gave responses to their health in 5 areas: “back pain, headaches, dizziness, chest pain, and fatigue. When receiving these complaints, it was seen that doctors gave extensive checkups to men more often than women with similar complaints, supporting that female patients tend to be taken less seriously than their male counterparts with regard to receiving medical illnesses. \n\nIn 1990, the National Institutes of Health recognized the disparities in research of disease in men and when. At this time the Office of Research on Women’s Health was created. A large part of its purpose is to raise awareness of sex affects disease and treatments. In 1991 and 1992 recognition that a 'glass ceiling' existed which prevented from female clinicians from being promoted. In 1994 the FDA created an Office of Women’s Health by congressional mandate.\n\nAccording to a study done in 2003, it can be seen that the numbers of women in medicine have increased significantly. This trend continues into today. In the United States, there has actually been a “progressive decrease in male applicants to medicine and a substantial rise in female applicants.” Gender difference have been found in the motivations for applying to medical school. Studies suggest that “male applicants are more motivated by financial, prestige, scientific and technical issues, whereas female applicants stress more ‘person orientated’ humanistic and altruistic reasons.” Gender differences have also been found in “attitudes toward health promotion.” In addition, male and female clinicians are likely to use different styles of communication. Male doctors were found to be more likely to “speak in an authoritative manner, give direct commands to patients, interrupt more, are perceived as more imposing and presumptuous, spend less time with patients, make fewer positive statements and smile and nod less.” Some studies have found that female doctors “provide more intensive therapeutic milieu that could lead to more open exchange and comprehensive diagnosis and treatment.” In addition, females have been found to take more precautionary measures and give more tests than men are. \n\nThere is also a connection between gender roles in the medical field and family pressures. A study was done to determine how doctors combine their working lives with having a normal family life. This study analyzes three different strategies used by men and women in order to cope with managing a normal family life and a work-heavy career. The three different types of strategies that men and women use are “career dominant, segregated, and accommodated.” When it comes to the career dominant strategy, about 15% of women and 3% of men adopt this strategy. This strategy “implies a continuous, full time career and a reduced family life- living single or divorced and childless as a consequence of the career.” The segregated strategy is composed of 55% of women and 85% of men, and it “implies a continuous, full time career with family roles organized so as to enable more time to be devoted to the career.” And lastly, the accommodating strategy is adopted by 30% of women and 12% of men. This strategy “implies that work involvement has been reduced in some way to allow more time for family roles.” As can be seen by these statistics, men are more likely than women to devote more time to their job as opposed to their family.\n\n\n"}
{"id": "51557587", "url": "https://en.wikipedia.org/wiki?curid=51557587", "title": "The Conservative Case for Trump", "text": "The Conservative Case for Trump\n\nThe Conservative Case for Trump is a 2016 book by Phyllis Schlafly, a movement conservative best known for helping to defeat the Equal Rights Amendment in the 1970s. It was her final book, being published posthumously. The book was published in September 2016, following the nomination of Donald Trump for President of the United States, and explains in detail Schlafly's rationale for viewing Trump a serious conservative and a better candidate than Hillary Clinton.\n\nThe book, co-written by Ed Martin and Brett M. Decker, describes Trump's anti-establishment stances on the following issues:\n\n\nOne argument the book makes is that Republicans who do not support Trump would wind up helping to elect Hillary Clinton instead of him. It also argues that Republicans and independents ought to \"unify behind his candidacy\". (See Buckley Rule.)\n\nThe book's publication followed Schlafly's tussle with the board of the Eagle Forum after she unilaterally endorsed Trump's presidency.\n"}
{"id": "52680815", "url": "https://en.wikipedia.org/wiki?curid=52680815", "title": "The Politics of Social Change in the Middle East and North Africa", "text": "The Politics of Social Change in the Middle East and North Africa\n\nThe Politics of Social Change in the Middle East and North Africa is a 1963 book by Manfred Halpern. For years it was \"the only academic treatment of Islamism,\" and served as \"the basic text\" on the politics of the Arab world for a generation of students. \n\n\"Politics of Social change\" was written at the behest of the RAND Corporation and published by Princeton University Press. It was widely reviewed and went through 6 printings.\n\nHalpern describes the Muslim Brotherhood and similar Islamist movements as a new kind of \"Neo-Islamic Totalitarianism\" best understood as a uniquely Islamic form of fascism. He argued that it is distinctive in drawing on a deeply Islamic apocalyptic tradition whereby at times of crises \"an apocalyptic vision of spiritual and political redemption\" comes to the fore. That modernity and urbanization deprived enormous numbers of people of livelihoods, and attracting many to faith and to an ideological rejection of material goods, while the literacy produced by urban life and modernity enable them to access religious texts. And that Islamic fascism drew these seekers to charismatic leaders who offered \"an intoxicating sense of nihilism\" in an atmosphere in which leaders made martyrdom into a spiritual goal and followers were \"sent to death as robots\" with the \"illusion of dying as martyrs\".\nHe described the Muslim Brotherhood as aiming to unify all Muslims under a new caliphate.\n\nFrédéric Volpi argues that Halpern's work represents a mix of mid-Cold War and orientalism. \n"}
{"id": "13214335", "url": "https://en.wikipedia.org/wiki?curid=13214335", "title": "Triangle Arts Trust", "text": "Triangle Arts Trust\n\nThe Triangle Network (formally known as the Triangle Arts Trust) is an international arts organization that brings together artists from different countries to explore new ideas and expand the boundaries of their practice. Triangle was initiated through a series of artists' workshops providing an uninterrupted period of two weeks where 20–25 artists from diverse cultural backgrounds engage with each other, to explore new ideas and expand the boundaries of their practice. Now the Triangle Network coalesces grassroots arts organisations around the world (many of which were initiated as workshops while others grew independently), in order that artists' mobility, international cultural exchange and capacity building objectives can be shared.\n\nThe Triangle Network is registered as a charitable organization in the UK.\n\nThe Triangle Network was established in 1982 by Robert Loder and Anthony Caro, and is organised as a network of artists, visual art organisations, and artists-led workshops. It currently is active in over 30 countries. Each centre within the Network is independent and set up to respond to local needs. The object of the workshops is \"to counterbalance the tendency of the Western art world to put the emphasis on the object and its marketing rather than on the creative process itself\".\nDavid Elliott was appointed to Chair the Board, succeeding Robert Loder who retired in 2009. Loder remained a Trustee of the organization until 2012.\n\n\n\n\n\n\n\n\n"}
