{"id": "29809352", "url": "https://en.wikipedia.org/wiki?curid=29809352", "title": "An Analysis of the Laws of England", "text": "An Analysis of the Laws of England\n\nAn Analysis of the Laws of England is a legal treatise by British legal professor William Blackstone. It was first published by the Clarendon Press in 1756. A Fellow of All Souls, Oxford, and a lecturer there, on 3 July 1753 Blackstone announced his intentions to give a set of lectures on the common law — the first lectures of that sort in the world. A prospectus was issued on 23 June 1753, and with a class of approximately 20 students, the first lecture series was completed by July 1754. Despite Blackstone's limited oratory skills and a speaking style described by Jeremy Bentham as \"formal, precise and affected\", Blackstone's lectures were warmly appreciated. The second and third series were far more popular, partially due to his then unusual use of printed handouts and lists of suggested reading. These show Blackstone's attempts to reduce English law to a logical system, with the division of subjects later being the basis for his \"Commentaries\". The lecture series brought him £116, £226 and £111 a year respectively from 1753 to 1755 — a total of £ in 2018 terms. Seeing the success of this publication, Blackstone was induced to write \"An Analysis of the Laws of England\", a 200-page introduction to English law, which was first published in 1756 by the Clarendon Press.\n\n\"Analysis\" begins with a summary of the ways that English law had been subdivided until that time. Blackstone examined the methods of Ranulf de Glanvill, Henry de Bracton and Matthew Hale, concluding that Hale's method was superior to the others. As such, Hale's distribution \"hath therefore been principally followed\" [by Blackstone in \"An Analysis . . \"], albeit with some amendments. The treatise is \"a marked advance on any previous introduction to English law . . including constitutional, civil and criminal law, public and private law, substantive law and procedure, as well as some introductory jurisprudential content\". The initial print run of 1,000 copies almost immediately sold out, leading to the printing of three additional 1,000-book lots over the next three years, which also sold out. A fifth edition was published in 1762, and a sixth, edited to take into account Blackstone's \"Commentaries on the Laws of England\", in 1771. Many of the later editions were prefaced with copies of Blackstone's \"A Discourse on the Study of the Law\", first published in 1758. Because of the success of the \"Commentaries\", Prest remarks that \"relatively little scholarly attention has been paid to this work\"; at the time, however, it was hailed as \"an elegant performance...calculated to facilitate this branch of knowledge\".\n\n"}
{"id": "48338624", "url": "https://en.wikipedia.org/wiki?curid=48338624", "title": "Anthony Musson", "text": "Anthony Musson\n\nAnthony Musson is professor of legal history at the University of Exeter. Musson is a barrister of the Middle Temple and a fellow of the Royal Historical Society and the Society of Antiquaries of London.\n\n"}
{"id": "2649781", "url": "https://en.wikipedia.org/wiki?curid=2649781", "title": "Asian immigration to the United States", "text": "Asian immigration to the United States\n\nAsian immigration to the United States refers to immigration to the United States from throughout the continent of Asia, including East Asia, Southeast Asia, South Asia, Central Asia, and the Middle East. Asian-origin populations have historically been in the territory that would become the United States since the 16th century. A first major wave of Asian immigration occurred in the late 19th century, primarily in Hawaii and the West Coast. Asian Americans experienced exclusion by law from the United States between 1880 and 1965, and were largely prohibited from naturalization until the 1940s. Since the Immigration and Nationality Act of 1965, a new wave of new immigrants to the United States in 2010 were from Asia.\n\nThe first Asian-origin people known to arrive in North America after the beginning of the European colonization were a group of Filipinos known as \"Luzonians\" or Luzon Indians who were part of the crew and landing party of the Spanish galleon \"Nuestra Señora de Buena Esperanza\". The ship set sail from Manila and landed in Morro Bay in what is now the California coast on 17 October 1587, as part of the Galleon Trade between the Spanish East Indies (the colonial name for what would become the Philippines) and New Spain (Spain's colonies in North America). More Filipino sailors arrived along the California coast when both places were part of the Spanish Empire. By 1763, \"Manila men\" or \"Tagalas\" had established a settlement called St. Malo on the outskirts of New Orleans, Louisiana. Indians have been documented in Colonial America as early as 1775. With the establishment of the Old China Trade in the late 18th century, a handful of Chinese merchants were recorded as residing in the United States by 1815.\n\nBy the 1830s, East Asian groups had begun immigrating to Hawaii, where American capitalists and missionaries had established plantations and settlements. Originating primarily from China, Japan, Korea, and the Philippines, these early migrants were predominantly contract workers who labored on plantations. With the annexation of Hawaii by the United States in 1893, a large population of Asians lived in a US territory and more would continue to immigrate. As American capitalists established sugar cane plantations in Hawaii in the 19th century, they turned, through organizations such as the Royal Hawaiian Agricultural Society, to the Chinese as a source of cheap labor as early as the 1830s, with the first formal contract laborers arriving in 1852. Resistance from plantation laborers protesting low wages and tensions between various native and immigrant groups encouraged plantation owners to import more labor from different Asian countries to keep wages low. Between 1885 and 1924, \"some 30,000 Japanese had gone to [Hawaii] as \"kan'yaku imin\", or government-sponsored contract laborers.\" Between 1894 and 1924, roughly 170,000 Japanese immigrants went to Hawaii as private contract laborers, family members of existing immigrants, and merchants. Taking refuge from Japanese imperialism and growing poverty and famine in Korea, and encouraged by Christian missionaries, thousands of Koreans migrated to Hawaii in the early 1900s. Filipinos, who were American colonial subjects after 1898, migrated by the \"tens of thousands\" to Hawaii in the early 1900s.\n\nThe first major wave of Asian immigration to the continental United States occurred primarily on the West Coast during the California Gold Rush, starting in the 1850s. Whereas Chinese immigrants numbered less than 400 in 1848, there were 25,000 by 1852. Most Chinese immigrants in California, which they called \"Gam Saan\" (\"Gold Mountain\"), were also from Guangdong province; they sought sanctuary from conflicts such as the Opium Wars and ensuing economic instability, and hoped to earn wealth to send back to their families. As in Hawaii, many capitalists in California and elsewhere (including as far as North Adams, Massachusetts) sought Asian immigrants to fill an increasing demand for labor in gold mines, factories, and on the Transcontinental Railroad. Some plantation owners in the South sought Chinese labor as a cheap means to replace the free labor of slavery. Chinese laborers generally arrived in California with the help of brokers in Hong Kong and other ports under the credit-ticket system, where they would pay back money loaned from brokers with their wages upon arrival. In addition to laborers, merchants also migrated from China, opening businesses and stores, including those that would form the beginnings of Chinatowns.\n\nJapanese, Korean, and South Asian immigrants also arrived in the continental United States starting from late 1800s and onwards to fill demands for labor. Japanese immigrants were primarily farmers facing economic upheaval during the Meiji Restoration; they began to migrate in large numbers to the continental United States (having already been migrating to Hawaii since 1885) in the 1890s, after Chinese exclusion (see below). By 1924, 180,000 Japanese immigrants had gone to the mainland. Filipino migration to North America continued in this period, with reports of \"Manila men\" in early gold camps in Mariposa County, California in the late 1840s. The 1880 census counted 105,465 Chinese and 145 Japanese, indicating that Asian immigration to the continent by this point consisted primarily of Chinese immigrants, overwhelmingly present in California.\n\nIn the 1860s and 1870s, nativist hostility to the presence of Asian laborers in the continental United States grew and intensified, with the formation of organizations such as the Asiatic Exclusion League. East Asian immigrants, particularly Chinese Americans who composed the majority of the population on the mainland, were seen as the \"yellow peril\" and suffered violence and discrimination. Lynchings of Chinese were common, and large-scale attacks also occurred, most prominently the Rock Springs massacre in which a mob of white miners killed nearly 30 Chinese immigrants. In 1875, Congress passed the Page Act, the first restrictive immigration law. This law identified forced laborers from Asia as well as Asian women who would potentially engage in prostitution as \"undesirable\" persons, who would henceforth be barred from entering the United States. In practice, the law was enforced to institute a near-complete exclusion of Chinese women from the United States, preventing male laborers from bringing their families with or after them.\n\nThe Chinese Exclusion Act of 1882 prohibited virtually all immigration from China, the first immigration law to do so on the basis of race or national origin. Minor exceptions were made for select merchants, diplomats, and students. The law also prevented Chinese immigrants from naturalizing as U.S. citizens. The Geary Act of 1892 further \"required Chinese to register and secure a certificate as proof of their right to be in the United States\" if they sought to leave and reenter the United States, with imprisonment or deportation as potential penalties. Although racial discrimination intensified in the exclusion era, Chinese immigrants fought to defend their existing rights and continued to pursue voting rights and citizenship. The children of these immigrants began to develop \"a sense of themselves as having a distinct identity as Chinese Americans.\"\n\nInitially, Japanese and South Asian laborers filled the demand that could not be met by new Chinese immigrants. The 1900 census counted 24,326 Japanese residents, a sharp increase, and 89,863 Chinese residents. The first South Asian immigrants landed in the United States in 1907, and were predominantly Punjabi Sikh farmers. As immigration restrictions specific to South Asians would begin two years later and against Asians generally eight years after that, \"[a]ltogether only sixty-four hundred came to America\" during this period. Like the Chinese and Japanese immigrants of the time, these South Asians were predominantly men. South Asian migrants also arrived on the East Coast, although to a lesser extent in the late 19th and early 20th centuries, predominantly Bengali Muslims who worked as craftsmen and merchants, selling 'exotic' goods such as embroidered silks and rugs. The 1910 census, the first to count South Asians, recorded that there were 2,545 \"Hindus\" in the United States.\n\nAnti-Asian hostility against these both older and newer Asian immigrant groups continued, becoming explosive in events such as the Pacific Coast race riots of 1907 in San Francisco, California; Bellingham, Washington; and Vancouver, British Columbia, Canada. The San Francisco riot was led by anti-Japanese activist, rebelling with violence in order to receive segregated schools for Caucasian and Japanese students. In the Bellingham riots, a mob of 400-500 white men attacked the homes of hundreds of South Asian immigrants, beating them and driving them out of town, with over 400 South Asians held in \"protective custody\" by local authorities. Along with geopolitical factors, these events encourage the United States to pursue the 1907 Gentleman's Agreement with Japan, wherein the Japanese government agreed to prohibit emigration to the United States and the latter's government agreed to impose less restrictions on Japanese immigrants. In practice, this meant that Japanese immigrants were barred unless they had previously acquired property or were immediate relatives of existing immigrants. While overall Japanese immigration was sharply curtailed, the family reunification provision allowed for the gender gap among Japanese Americans to be reduced significantly (including through \"picture brides\"). As Koreans were Japanese colonial subjects at the time and could be issued Japanese passports, many Korean women also immigrated as family members and \"picture brides\".\n\nAfter the Spanish–American War ended with the Treaty of Paris in 1898, the United States replaced Spain as the colonial ruler of the Philippines. As Filipinos become colonial subjects of the United States, they also became US nationals. As American colonial subjects, Filipinos were considered US nationals and thus were not initially subject to exclusion laws. Many Filipinos came as agricultural laborers to fill demands once answered by Chinese and Japanese immigration, with migration patterns to Hawaii extending to the mainland starting from the 1920s. The US government also initially sponsored select Filipino students, known as \"pensionados\", to attend US colleges and universities. However, in 1934, the Tydings–McDuffie Act, which promised independence to the Philippines by 1945, also sharply curtailed Filipino immigration with a quota of 50 immigrants per year.\n\nThe prohibitions of Chinese and Japanese immigration were consolidated and the exclusion was expanded to Asia as a whole in the Asiatic Barred Zone Act of 1917, which prohibited all immigration from a zone that encompassed the Middle East, Central Asia, South Asia (then-British India), and Southeast Asia. The Immigration Act of 1924 introduced national origin quotas for the entire Eastern Hemisphere, and barred the immigration of \"aliens ineligible for citizenship.\" This introduced a period of near complete exclusion of Asian immigration to the United States. There were some key exceptions to this broad exclusion: in addition to continuing Filipino immigration due to their status as US nationals until 1934, Asian immigrants continued to immigrate to Hawaii, which was a US territory and therefore not subject to the same immigration laws until it achieved statehood in 1959. Many Chinese had also immigrated to Puerto Rico after 1882, which would become a US territory in 1898 and remains one today.\n\nAfter exclusion, existing Chinese immigrants were further excluded from agricultural labor by racial hostility, and as jobs in railroad construction declined, they increasingly moved into self-employment as laundry workers, store and restaurant owners, traders, merchants, and wage laborers; and they congregated in Chinatowns established in California and across the country.\n\nOf the various Asian immigrant groups present in the United States after broad exclusion was introduced in 1917 and 1924, the South Asian population had the most severe gender gap. This led to many of the Punjabi Sikhs in California at the time to marry women of Mexican descent, avoiding anti-miscegenation laws and racial prejudice that prevented them from marrying into white communities.\n\nTwo important Supreme Court cases in the exclusion era determined the citizenship status of Asian Americans. In 1922, the Court ruled in \"Takao Ozawa v. United States\" that ethnic Japanese were not Caucasian, and therefore did not meet the \"free white persons\" requirement to naturalize according to the Naturalization Act of 1790. A few months later in 1923, the Court ruled in \"United States v. Bhagat Singh Thind\" that while Indians were considered Caucasian by contemporary racial anthropology, they were not seen as \"white\" in the common understanding, and were therefore ineligible for naturalization. Whereas \"United States vs. Wong Kim Ark\" had determined that all persons born in the United States, including Asian Americans, were citizens, these cases confirmed that foreign-born Asian immigrants were legally excluded from naturalized citizenship on the basis of race.\n\nDuring this period, Asian immigrants continued to face racial discrimination. In addition to first-generation immigrants whose permanent ineligibility for citizenship curtailed their civil and political rights, second-generation Asian Americans (who formally had birthright citizenship) continued to face segregation in schools, employment discrimination, and prohibitions on property and business ownership. The most severe discrimination against Asian Americans occurred during the height of the World War II, when 110,000 to 120,000 Japanese Americans (primarily on the West Coast) were incarcerated in internment camps between 1942–1946. While roughly a third of those interned were \"issei\" (first-generation immigrants) who were ineligible for citizenship, the vast majority were \"nisei\" or \"sansei\" (second- and third-generation) who were citizens by birth.\n\nAfter the Second World War, immigration policy in the United States began to undergo significant changes. In 1943, the Magnuson Act ended 62 years of Chinese exclusion, providing for a quota of 105 persons to immigrate each year, and permitting the Chinese present in the United States to become naturalized citizens. Despite these provisions, the Act consolidated the prohibition of property or business ownership by Chinese Americans. In 1946, the Luce–Celler Act allowed Filipino and Indian nationals to naturalize and provided for a quota of 100 persons to immigrate from each country. Many Asian Americans (including future congressman Dalip Singh Saund) had been campaigning for such a law for decades. Under the act, upon attaining citizenship, immigrants would be able to own property (a right not afforded to Chinese immigrants in the Magnuson Act) and petition for family from their nation of origin.\n\nThis wave of reform eventually led to the McCarran–Walter Act of 1952, which repealed the remnants of the \"free white persons\" restriction of the Naturalization Act of 1790, permitting Asian and other non-white immigrants to become naturalized citizens. However, this Act retained the quota system that effectively banned nearly all immigration from Asia, except for small annual quotas. Its primary exception to the quota system was family reunification provisions for US citizens, which allowed both relatives of longstanding Asian American families and those who had married American soldiers during World War II and the Korean War (also known as \"war brides\") to immigrate. The McCarran–Walter Act also introduced some labor qualifications for the first time, and allowed the government to bar the entry of or deport immigrants suspected of engaging in \"subversive activities\", such as membership in a Communist Party.\n\nAfter the enactment of the 1965 Immigration Act, Asian American demographics changed rapidly. This act replaced exclusionary immigration rules of the 1924 Immigration Act and its predecessors, which effectively excluded \"undesirable\" immigrants, including most Asians. The 1965 rules set across-the-board immigration quotas for each country. It opened US borders to immigration from Asia for the first time in nearly half a century.\n\nSenator Hiram Fong (R-HI) answered questions concerning the possible change in our cultural pattern by an influx of Asians: \n\"Asians represent six-tenths of 1 percent of the population of the United States ... with respect to Japan, we estimate that there will be a total for the first 5 years of some 5,391 ... the people from that part of the world will never reach 1 percent of the population .. .Our cultural pattern will never be changed as far as America is concerned.\" (U.S. Senate, Subcommittee on Immigration and Naturalization of the Committee on the Judiciary, Washington, D.C., Feb. 10, 1965, pp.71, 119.)<br>Note: From 1966 to 1970, 19,399 immigrants came from Japan, more than three times Sen. Fong's estimate. Immigration from Asia as a whole has totaled 5,627,576 from 1966 to 1993. 6.8% of the American population is currently of Asian birth or heritage.]\nImmigration of Asian Americans were also affected by U.S. war involvement from the 1940s to the 1970s. In the wake of World War II, immigration preferences favored family reunification. This may have helped attract highly skilled workers to meet American workforce deficiencies. Another instance related to World War II was the Luce–Celler Act of 1946, which helped immigrants from India and the Philippines.\n\nThe end of the Korean War and Vietnam War and the \"Secret Wars\" in Southeast Asia brought a new wave of Asian American immigration, as people from Korea, Vietnam, Laos, and Cambodia arrived. Some of the new immigrants were war brides, who were soon joined by their families. Others, like the Southeast Asians, were either highly skilled and educated, or part of subsequent waves of refugees seeking asylum. Some factors contributing to the growth of sub-groups such as South Asians and mainland Chinese were higher family sizes, higher use of family-reunification visas, and higher numbers of technically skilled workers entering on H-1 and H-1B visas.\n\nEthnic Chinese immigration to the United States since 1965 has been aided by the fact that the United States maintains separate quotas for Mainland China, Taiwan, and Hong Kong. During the late 1960s and early and mid-1970s, Chinese immigration into the United States came almost exclusively from Taiwan creating the Taiwanese American subgroup. A smaller number of immigrants from Hong Kong arrived as college and graduate students. Immigration from Mainland China was almost non-existent until 1977, when the PRC removed restrictions on emigration leading to immigration of college students and professionals. These recent groups of Chinese tended to cluster in suburban areas and to avoid urban Chinatowns.\n\nOne notable suburban Chinatown was Monterey Park. While it was a predominantly White middle-class community in the 1970s, the demographics quickly changed with the incoming Chinese population. The emergence of Chinese-Americans in Monterey Park could be credited to the efforts of the Chinese realtor Frederic Hsieh. He began investing in abandoned properties in Monterey Park in order to gain the interest of wealthy Chinese in Taiwan. He broadcast his plans back in Taiwan and Hong Kong. He aggressively marketed his project as the new mecca of Chinese Americans: in his own words, \"Chinese Beverly Hills\". Due to political unrest in Asia, there was a lot of interest in overseas investment for Monterey Park from wealthy Chinese in Taiwan. \n\nThe contrasts between Japanese Americans and South Asian Americans are emblematic of the dramatic changes since the immigration reforms. Japanese Americans are among the most widely recognized of Asian American sub-groups during the 20th century. At its peak in 1970, there were nearly 600,000 Japanese Americans, making it the largest sub-group, but historically the greatest period of immigration was generations past. Today, given relatively low rates of births and immigration, Japanese Americans are only the sixth-largest Asian American group. In 2000, there were between 800,000 and 1.2 million Japanese Americans (depending on whether multi-ethnic responses are included). The Japanese Americans have the highest rates of native-born, citizenship, and assimilation into American values and customs.\n\nBefore 1990, there were slightly fewer South Asians in the U.S. than Japanese Americans. By 2000, Indian Americans nearly doubled in population to become the third largest group of Asian Americans, with increasing visibility in high-tech communities such as the Silicon Valley and the Seattle area. Indian Americans have some of the highest rates of academic achievement among American ethnic groups. Most immigrants speak English and are highly educated. South Asians are increasingly accepted by most Asian organizations as another significant Asian group. Currently, Chinese, Indians, and Filipinos are the three largest Asian ethnic groups immigrating to the United States.\n\n\n\n\n"}
{"id": "193806", "url": "https://en.wikipedia.org/wiki?curid=193806", "title": "Atlantic Charter", "text": "Atlantic Charter\n\nThe Atlantic Charter was a pivotal policy statement issued during World War II on 14 August 1941 which defined the Allied goals for the post-war world. The leaders of the United Kingdom and the United States drafted the work and all the Allies of World War II later confirmed it. The Charter stated the ideal goals of the war: no territorial aggrandizement; no territorial changes made against the wishes of the people (self-determination); restoration of self-government to those deprived of it; reduction of trade restrictions; global cooperation to secure better economic and social conditions for all; freedom from fear and want; freedom of the seas; and abandonment of the use of force, as well as disarmament of aggressor nations. Adherents of the Atlantic Charter signed the \"Declaration by United Nations\" on 1 January 1942, which became the basis for the modern United Nations.\n\nThe Atlantic Charter set goals for the postwar world and inspired many of the international agreements that followed the war. The General Agreement on Tariffs and Trade (GATT), the postwar independence of European colonies, and much more are derived from the Atlantic Charter.\n\nUS President Franklin D. Roosevelt and British Prime Minister Winston Churchill drafted the Atlantic Charter at the Atlantic Conference (codenamed \"Riviera\") in Placentia Bay, Newfoundland. They issued it as a joint declaration on 14 August 1941 at Naval Station Argentia although the United States would not officially enter the War until four months later. The policy was issued as a \"statement\"; as such there was no formal, legal document entitled \"The Atlantic Charter\". It detailed the goals and aims of the Allied powers concerning the war and the postwar world.\n\nMany of the ideas of the Charter came from an ideology of Anglo-American internationalism that sought British and American cooperation for the cause of international security. Roosevelt's attempts to tie Britain to concrete war aims and Churchill's desperation to bind the US to the war effort helped provide motivations for the meeting which produced the Atlantic Charter. It was assumed at the time that Britain and America would have an equal role to play in any postwar international organization that would be based on the principles of the Atlantic Charter.\n\nChurchill and Roosevelt began communicating in 1939; this was the first of their 11 wartime meetings. Both men traveled in secret; Roosevelt was on a ten-day fishing trip. On 9 August 1941, the British battleship HMS \"Prince of Wales\" steamed into Placentia Bay, with Churchill on board, and met the American heavy cruiser USS \"Augusta\", where Roosevelt and members of his staff were waiting. On first meeting, Churchill and Roosevelt were silent for a moment until Churchill said \"At long last, Mr. President\", to which Roosevelt replied \"Glad to have you aboard, Mr. Churchill\". Churchill then delivered to the president a letter from King George VI and made an official statement which, despite two attempts, the movie sound crew present failed to record.\n\nThe Atlantic Charter made clear that the United States was supporting the United Kingdom in the war. Both the US and UK wanted to present their unity, regarding their mutual principles and hopes for a peaceful postwar world and the policies they agreed to follow once the Nazis had been defeated. A fundamental aim was to focus on the peace that would follow, and not specific American involvement and war strategy, although American involvement appeared increasingly likely.\n\nThe eight principal points of the Charter were:\n\nAlthough Clause Three clearly states that all peoples have the right to decide their form of government, it fails to say what changes are necessary in both social and economic terms, so as to achieve freedom and peace.\n\nClause Four, with respect to international trade, consciously emphasized that both \"victor [and] vanquished\" would be given market access \"on equal terms.\" This was a repudiation of the punitive trade relations that were established within Europe after World War I, as exemplified by the Paris Economy Pact.\n\nOnly two clauses expressly discuss national, social, and economic conditions necessary after the war, despite this significance.\n\nWhen it was released to the public, the Charter was titled \"Joint Declaration by the President and the Prime Minister\" and was generally known as the \"Joint Declaration\". The Labour Party newspaper \"Daily Herald\" coined the name \"Atlantic Charter\", but Churchill used it in Parliament on 24 August 1941, and it has since been generally adopted.\n\nNo signed version ever existed. The document was threshed out through several drafts and the final agreed text was telegraphed to London and Washington. President Roosevelt gave Congress the Charter's content on 21 August 1941. He said later, \"There isn't any copy of the Atlantic Charter, so far as I know. I haven't got one. The British haven't got one. The nearest thing you will get is the [message of the] radio operator on \"Augusta\" and \"Prince of Wales\". That's the nearest thing you will come to it ... There was no formal document.\"\n\nThe British War Cabinet replied with its approval and a similar acceptance was telegraphed from Washington. During this process, an error crept into the London text, but this was subsequently corrected. The account in Churchill's \"The Second World War\" concludes \"A number of verbal alterations were agreed, and the document was then in its final shape\", and makes no mention of any signing or ceremony. In Churchill's account of the Yalta Conference he quotes Roosevelt saying of the unwritten British constitution that \"it was like the Atlantic Charter – the document did not exist, yet all the world knew about it. Among his papers he had found one copy signed by himself and me, but strange to say both signatures were in his own handwriting.\"\n\nThe Allied nations and leading organisations quickly and widely endorsed the Charter. At the subsequent meeting of the Inter-Allied Council in London on 24 September 1941, the governments in exile of Belgium, Czechoslovakia, Greece, Luxembourg, the Netherlands, Norway, Poland, and Yugoslavia, as well as the Soviet Union, and representatives of the Free French Forces, unanimously adopted adherence to the common principles of policy set forth in the Atlantic Charter. On 1 January 1942, a larger group of nations, who adhered to the principles of the Atlantic Charter, issued a joint Declaration by United Nations stressing their solidarity in the defense against Hitlerism.\n\nThe Axis powers interpreted these diplomatic agreements as a potential alliance against them. In Tokyo, the Atlantic Charter rallied support for the militarists in the Japanese government, who pushed for a more aggressive approach against the US and Britain.\n\nThe British dropped millions of flysheets over Germany to allay fears of a punitive peace that would destroy the German state. The text cited the Charter as the authoritative statement of the joint commitment of Great Britain and the US \"not to admit any economical discrimination of those defeated\" and promised that \"Germany and the other states can again achieve enduring peace and prosperity.\"\n\nThe most striking feature of the discussion was that an agreement had been made between a range of countries that held diverse opinions, who were accepting that internal policies were relevant to the international problem. The agreement proved to be one of the first steps towards the formation of the United Nations.\n\nThe problems came not from Germany and Japan, but from those of the allies that had empires and which resisted self-determination—especially the United Kingdom, the Soviet Union and the Netherlands. Initially it appears that Roosevelt and Churchill had agreed that the third point of Charter was not going to apply to Africa and Asia. However Roosevelt's speechwriter Robert E. Sherwood noted that \"it was not long before the people of India, Burma, Malaya, and Indonesia were beginning to ask if the Atlantic Charter extended also to the Pacific and to Asia in general.\" With a war that could only be won with the help of these allies, Roosevelt's solution was to put some pressure on Britain but to postpone until after the war the issue of self-determination of the colonies.\n\nPublic opinion in Britain and the Commonwealth was delighted with the principles of the meetings but disappointed that the US was not entering the war. Churchill admitted that he had hoped the US would decide to commit itself.\n\nThe acknowledgement that all people had a right to self-determination gave hope to independence leaders in British colonies.\n\nThe Americans were insistent that the charter was to acknowledge that the war was being fought to ensure self-determination. The British were forced to agree to these aims but in a September 1941 speech, Churchill stated that the Charter was only meant to apply to states under German occupation, and certainly not to the countries who formed part of the British Empire.\n\nChurchill rejected its universal applicability when it came to the self-determination of subject nations such as British India. Mahatma Gandhi in 1942 wrote to President Roosevelt: \"I venture to think that the Allied declaration that the Allies are fighting to make the world safe for the freedom of the individual and for democracy sounds hollow so long as India and for that matter Africa are exploited by Great Britain...\" While self-determination was Roosevelt's guiding principle, he was reluctant to place pressure on the British in regard to India and other colonial possessions as they were fighting for their lives in a war in which the United States was not a participant. Gandhi refused to help either the British or the American war effort against Germany and Japan in any way, and Roosevelt chose to back Churchill. India was already contributing significantly to the war effort, sending over 2.5 million men (the largest volunteer force in the world at the time) to fight for the Allies, mostly in West Asia and North Africa.\n\nChurchill was unhappy with the inclusion of references to peoples' right to \"self-determination\" and stated that he considered the Charter an \"interim and partial statement of war aims designed to reassure all countries of our righteous purpose and not the complete structure which we should build after the victory.\" An office of the Polish Government in Exile wrote to warn Władysław Sikorski that if the Charter was implemented with regard to national self-determination, it would make the desired Polish annexation of Danzig, East Prussia and parts of German Silesia impossible, which led the Poles to approach Britain asking for a flexible interpretation of the Charter.\n\nDuring the war Churchill argued for an interpretation of the charter in order to allow the Soviet Union to continue to control the Baltic states, an interpretation rejected by the US until March 1944. Lord Beaverbrook warned that the Atlantic Charter \"would be a menace to our [Britain's] own safety as well as to that of the Soviet Union.\" The US refused to recognise the Soviet takeover of the Baltics, but did not press the issue against Stalin when he was fighting the Germans. Roosevelt planned to raise the Baltic issue after the war, but he died in April 1945, before fighting had ended in Europe.\n\n\n\n\n"}
{"id": "36224332", "url": "https://en.wikipedia.org/wiki?curid=36224332", "title": "Awza'i", "text": "Awza'i\n\nThe Awza'i () madhhab was one of the schools of Fiqh, the Islamic jurisprudence, or religious law within Sunni Islam in the 8th century. Its Imam was Abd al-Rahman al-Awza'i.\n\nSome posit that the reason why the people of Morocco took up the madhhab of Imam Malik, is that the people of Morocco and al-Andalus were originally upon the madhhab of al-Awza’i. However since the Umayyad conquest of the region and the Berber revolt, Morocco and western Algeria were following Kharijites schools which were adopted by the ruling dynasties such as the Maghrawa, the emirate of Toudgha and the Ibadhi Rustamid dynasty. And, with the exception of Tunisia and al-Andalus, the Maliki school only became established in the region after the rise of the Almoravid dynasty.\n\nThen during the rule of Al-Hakam I, the official fatwas were changed and given according to the opinion of Malik ibn Anas and the people of al-Madina. This was due to the opinion and preference of al-Hakam due to some political benefits he saw and they differ about the actual reason, which still remains unclear. Most hold that it was due to the scholars of al-Andalus travelling to Medina, then when they returned they spoke of the excellence of Malik, his wide knowledge and great station, so they honoured him and preferred his Madhhab. Others say that Imam Malik asked some of the people of al-Andalus about the rule in their region and they described it to him and Malik was very pleased by it since the Abbasids in that time did not rule in a manner that was agreeable. So, Imam Malik said to the person who told him, ‘We ask Allaah to enlighten our sacred precincts with your rule.’ This was transmitted to the ruler of al-Andalus, who already knew of the knowledge, excellence and piety of Malik; so he led the people to accept his Madhhab and ordered that the madhhab of al-Awza’i be abandoned. Later, the kings of Morocco and the west agreed that the rulings and actions should be according to the preferences of Ibn al-Qaasim al-`Utaqi (a famous student of Malik) only.\n\nIn Syria, the Madhhab of al-Awza’i remained the main school of thought until the 10th century, when Abu Zar’ah Muhammed ibn Uthman of the Shafi’ee Madhhab was appointed judge of Damascus. Abu Zar’ah began the practice of giving a prize of 100 dinars to any student who memorized the book, Mukhtasr al-Muzanee (the basics of Shafi’ee fiqh). Naturally, the practice caused the Shafi’ee Madhhab to spread rapidly in Syria, until none of al-Awza’i’s followers remained until the 11th century.\n"}
{"id": "2139927", "url": "https://en.wikipedia.org/wiki?curid=2139927", "title": "Bermuda II Agreement", "text": "Bermuda II Agreement\n\nBermuda II was a bilateral air transport agreement between the governments of the United Kingdom and the United States signed on 23 July 1977 as a renegotiation of the original 1946 Bermuda air services agreement. A new \"open skies\" agreement was signed by the United States and the European Union (EU) (of which the United Kingdom is part) on 30 April 2007 and came into effect on 30 March 2008, thus replacing Bermuda II.\n\nThe original 1946 Bermuda agreement took its name from the island where UK and US transport officials met to negotiate a new, inter-governmental air services agreement between Britain and the United States. That agreement, which was (relative to modern agreements) highly restrictive at the insistence of the British negotiators who feared that \"giving in\" to US demands for a \"free-for-all\" would lead to the then financially superior US airlines' total domination of the global air transport industry, was the world's first bilateral air services agreement. It became a blueprint for all subsequent air services agreements.\n\nBermuda II was revised several times since its signing, most recently in 1995. Although Bermuda II was much less restrictive than the original Bermuda agreement it replaced, it was widely regarded as a highly restrictive agreement that contrasted with the principle of \"open skies\" against the background of continuing liberalization of the legal framework governing the air transport industry in various parts of the world.\n\nIn July 1976, Edmund Dell, the then new UK Secretary of State for Trade, renounced the original Bermuda Agreement of 1946 and initiated bilateral negotiations with his US counterparts on a new air services agreement, which resulted in the Bermuda II treaty of 1977.\n\nThe reason for this was that there was no provision in the original 1946 Bermuda agreement that would have allowed British Caledonian (BCal), then the UK's foremost wholly privately owned, independent international scheduled airline, to use the licences the UK Civil Aviation Authority (CAA) had awarded it in 1972 to begin daily scheduled services from its London Gatwick base to Houston and Atlanta. (These cities were not nominated as \"gateway cities\" in the original Bermuda agreement.) In addition, there was no provision in the original Bermuda agreement that would have allowed Laker Airways to use the licence the UK's Air Transport Licensing Board (ATLB), the CAA's predecessor, had awarded it the same year to commence a daily \"Skytrain\" operation between London Stansted and New York.\n\nUnder the original 1977 version of Bermuda II, British Airways, Pan American World Airways and Trans World Airlines were the only carriers allowed to operate flights between London Heathrow and the United States.\n\nAir India, El Al, Iran Air and Kuwait Airways were permitted to continue exercising their so-called \"fifth freedom\" traffic rights from Heathrow to John F. Kennedy Airport (JFK), which they had already enjoyed under the original Bermuda agreement. (Both El Al and Iran Air stopped exercising these rights. The former decided that it made better economic sense to fly non-stop between Tel Aviv and New York. The latter's US traffic rights were withdrawn in the aftermath of the 1979 Iranian hostage crisis.) Similarly, Air New Zealand was allowed to continue using its fifth freedom rights between London and Los Angeles.\n\nThe extensive fifth freedom rights US carriers used to enjoy from the UK to other European countries were restricted to a few routes from London Heathrow to what used to be West Germany (including West Berlin) in the days prior to German reunification. In the early '90s, United Airlines used to fly between Heathrow, Berlin, Hamburg and Munich (United had acquired these traffic rights along with Pan Am's transatlantic rights to/from Heathrow for US$1billion in 1990). A few years earlier, TWA flew between London and Brussels but, unlike United, did not have traffic rights to carry local traffic between the two cities.\n\nAmerican and British regulatory authorities needed to approve every airline's capacity and pricing ahead of each operating season. Each country could refuse traffic rights to a carrier it was not satisfied with, particularly with regard to ownership and/or control. Only a specified number of US \"gateway cities\" could be served by both UK and US carriers from London Heathrow as well as London Gatwick.\n\nOnly the following US gateway cities could be served non-stop from Heathrow: Baltimore, Boston, Chicago O'Hare, Detroit, Los Angeles, Miami, New York JFK, Newark, Anchorage, Minneapolis/St. Paul, Philadelphia, San Francisco, Seattle, and Washington-Dulles. Anchorage and Minneapolis/St. Paul, held dormant authorities to use Heathrow as their London terminal, grandfathered from use during the original Bermuda Agreement; but could only be operated non-stop by a British carrier. As such, Anchorage remained dormant during the latter years of Bermuda II, and Minneapolis featured service only to London Gatwick, as its operating carrier (Northwest Airlines) did not have the authority to operate into Heathrow.\n\nOther airports in the United States had to be served from Gatwick rather than Heathrow: these eventually included Atlanta, Cincinnati, Cleveland, New Orleans, Pittsburgh, Dallas/Fort Worth, Houston, Las Vegas, Orlando, Tampa, Raleigh/Durham, St. Louis and Charlotte.\n\nA provision existed that allowed any Heathrow-authorised British carrier operating as the sole carrier between London Gatwick and any US city to switch that service to Heathrow as long as the incumbent airline had demonstrated its ability to carry a minimum of 300,000 non-stop passengers (both transfer and point-to-point) between that city and London, and vice versa, over a period of two consecutive calendar years. Using this method, BA was able to transfer Denver, San Diego and Phoenix services from Gatwick to Heathrow.\n\nBermuda II continued and expanded the principle of \"dual designation\", i.e. the right to designate two UK airlines as well as two US carriers as \"flag carriers\" on the same routes, which already existed on the London–New York and London–Los Angeles routes under the original Bermuda treaty.\n\nUnder the new agreement, BCal had its licences to commence scheduled services from its Gatwick base to both Houston and Atlanta confirmed and was designated as the UK's exclusive flag carrier on both routes. It also obtained a licence and sole UK flag carrier status to commence scheduled services from Gatwick to Dallas/Fort Worth. In addition, BCal obtained a licence and sole UK flag carrier status to commence scheduled all cargo flights between Gatwick and Houston – including an optional stop-over at Manchester or Prestwick in either direction.\n\nThe UK Government chose to designate Laker Airways rather than BCal as the second UK flag carrier to New York to enable that airline to inaugurate its long-planned \"Skytrain\" operation on that route.\n\nMoreover, both sides agreed to continue dual designation, i.e. designating two UK flag carriers as well as two US flag carriers, on the London–New York and London–Los Angeles routes. The principle of dual designation was extended to another two high-volume routes. The UK side chose to designate a second carrier on London–Miami, while the US side chose London–Boston for the same purpose. This meant that a second British airline was permitted to commence scheduled services on the former route, whereas another American carrier could do the same on the latter route. The UK government chose to designate Laker Airways as the second UK flag carrier on the Los Angeles and Miami routes, whilst the US government decided to designate Northwest as the second US flag carrier on London–Boston. (Pan Am and TWA continued in their role as the two designated US flag carriers between London and New York as well as London and Los Angeles respectively.)\nDuring the Bermuda II negotiations, the UK side succeeded in having inserted into the new air services agreement a clause stating that Gatwick – rather than Heathrow – was to be nominated as the designated US flag carrier's London gateway airport whenever BCal was going to be the sole designated UK flag carrier on the same route. This clause was meant to support the growth of BCal's scheduled operation at Gatwick as well as to redress the competitive imbalance between it and its much bigger, more powerful rivals.\n\nThe UK side furthermore succeeded in negotiating a three-year \"exclusivity\" period for the incumbent operator on any new route with their US counterparts.\n\nFor Gatwick-based BCal this meant that it did not have to face any competitor that was using Heathrow, a more accessible airport with a bigger catchment area and a far greater number of passengers connecting between flights, on any of the new routes it was planning to launch to the US. It also meant that it had any new route to the US completely to itself for the first three years of operation, which most airline industry analysts reckon is sufficiently long for a brand-new scheduled air service to become profitable.\n\nAt British insistence Bermuda II furthermore contained clauses that made it illegal for any airline operating scheduled flights between the UK and the US to resort to predatory pricing or capacity dumping. Air fares were only approved if they reflected the actual cost of providing these services. Similarly, capacity increases were sanctioned on a reciprocal basis only. The reason for insisting on the inclusion of these provisions in the Bermuda II agreement was to prevent the much bigger, better financed and commercially far more aggressive US carriers from undercutting BCal with \"loss-leading\" fares cross-subsidised with profits those carriers' vast domestic networks generated as well as to stop them from \"marginalising\" the UK carrier by adding capacity far in excess of what the market could sustain.\n\nBermuda II also provided routing restrictions for flights between the United States and various British overseas colonies including Bermuda, various Caribbean islands and Hong Kong.\n\nBoth sides agreed to automatically nominate Gatwick as the gateway airport for London for any London–US route that did not already exist under the original 1946 Bermuda agreement. When all available routes between London Heathrow/Gatwick and the US were taken, any carrier wishing to start a new route to a US gateway city not served from either of London's two main airports at the time of application for route authority needed to drop another route. In addition, any such change could only become effective when there was unanimous agreement between both the UK and US governments. Failure of both nations' governments to agree to such changes prevented the introduction of additional non-stop flights, including between London and Honolulu, Portland (OR), and Salt Lake City. British Airways did successfully gain approval in 1982 to operate nonstop to New Orleans from Gatwick, as an intermediate stop on its L-1011 service to Mexico City. This gateway was later omitted as the performance capability of newer 747s allowed the airline to operate the round trip route nonstop, despite the Mexican city's high altitude.\n\nIn the wake of the bankruptcies of TWA and Pan Am, the carriers authorized to operate Heathrow routes were replaced by British Airways and Virgin Atlantic on the UK side and American Airlines and United Airlines on the US side.\n\nPan Am had previously sold its Heathrow traffic rights to United Airlines, but British negotiators initially stated that they would not allow United to receive the transferred route authority citing Bermuda II's specific designation of Pan Am; they furthermore stated that United was not a successor airline because it was not assuming ownership of Pan Am. \n\nVirgin Atlantic's access rights to Heathrow under Bermuda II derived from the fact that the UK was not using its entitlement to nominate a second carrier to match the two US carriers' presence at London's premier airport. The UK Government therefore took advantage of the abolition of the London [Air] Traffic Distribution Rules, which had confined Virgin's London operations to Gatwick, as well as of the US Government's intention to have American and United replace TWA and Pan Am as the designated US flag carriers at Heathrow to help Virgin establish a presence at that airport as well. \n\nThese access restrictions were also the reason BA (as BCal's legal heir between London and Houston, Dallas, and Atlanta) and American (as Braniff's legal heir between Dallas and London) were compelled to continue using Gatwick as their UK gateway for all non-stop scheduled operations between London and Houston, Dallas, and Atlanta as long as Bermuda II remained in force.\n\nCode sharing also became possible under the 1991 agreement. The US later approved Continental Airlines to fly to London Heathrow, but British refusal to endorse the US position prevented Continental from exercising this route authority. However, Continental succeeded in obtaining UK permission to enter into a codeshare agreement with Virgin Atlantic, which placed Continental's flight numbers in addition to Virgin's on some of the latter's Heathrow and Gatwick flights. \n\nFare and route restrictions governing all scheduled air services serving airports other than London Heathrow or London Gatwick were lifted. (This partial liberalisation came about as a UK concession to the US to help BA gain approval for its code-share alliance with US Air. As a result, access restrictions that originally covered all London airports were lifted at Luton and Stansted. This, in turn, enabled the now defunct \"new generation\", all-business class carriers such as Eos, Maxjet and Silverjet to enter the lucrative London–New York business travel market by choosing Stansted and Luton rather than Heathrow or Gatwick as their UK departure/arrival airports). Continental Airlines also took advantage of this liberalisation by starting service to a number of important regional UK airports, including Bristol, Birmingham, Manchester, Edinburgh, Glasgow and Belfast. Continental furthermore introduced non-stop service to Stansted from Newark in 2001, but this was withdrawn in the industry downturn after the September 11 terrorist attacks.\n\nFlights between the United States and Hong Kong were removed from the scope of Bermuda II in 1997, when sovereignty over Hong Kong was transferred to the People's Republic of China.\n\nLiberalisation of the Bermuda II agreement was the declared intention of both countries since 1995. However, bilateral negotiations between the UK and the US were unsuccessful.\n\nSubsequently, matters were complicated by the European Court of Justice's judgment to declare all bilateral agreements between individual EU member states and the US illegal. Such agreements were deemed to violate the EU's Common Aviation Market. \n\nThe main sticking point that had prevented the conclusion of a new, transatlantic \"Common Aviation Area\" agreement between the EU and the US was that the UK and most other European countries viewed the US version of \"open skies\" as too restrictive. The US \"open skies\" template denied foreign airlines \"cabotage\" rights, i.e. the right to operate wholly within the US domestic market without entering into a code-share agreement with a US carrier. It also denied foreign airlines the right to acquire stakes in their US counterparts with the intention of exercising boardroom control.\n\nOn March 2, 2007 a draft agreement was reached by negotiators from the European Commission and the US that proposed to drop Bermuda II's restrictions preventing US flag carriers, other than American and United, from flying to Heathrow. This new Air Transport Agreement between the EU and the US was approved unanimously by the EU Transport Council on 22 March 2007 and replaced Bermuda II with effect from 30 March 2008. It also paved the way for either country to allow airlines headquartered in other EU countries to enter the UK-US air transport market. On 3 October 2007, Britain concluded its first fully liberal open skies agreement with Singapore, allowing Singapore Airlines to fly completely unrestricted from any point in the United Kingdom, including Heathrow, to any other destination, including the United States and domestic destinations, effective 30 March 2008.\n\n"}
{"id": "1230380", "url": "https://en.wikipedia.org/wiki?curid=1230380", "title": "Black populism", "text": "Black populism\n\nFollowing the collapse of Reconstruction, African Americans created a broad-based independent political movement in the South: Black Populism.\n\nBetween 1886 and 1898 Black farmers, sharecroppers, and agrarian laborers organized their communities to combat the rising tide of Jim Crow laws. As Black Populism asserted itself and grew into a regional force, it met fierce resistance from the white planter and business elite that, through the Democratic Party and its affiliated network of courts, militias, sheriffs, and newspapers, maintained tight control of the region. Violence against Black Populism was organized through the Ku Klux Klan, among other white terrorist organizations designed to halt or reverse the advance of black civil and political rights.\n\nDespite opposition, Black Populists carried out a wide range of activities:\n\n\nBlack Populism found early expression in various agrarian organizations, including the Colored Agricultural Wheels, the southern Knights of Labor, the Cooperative Workers of America, and the Colored Farmers' Alliance. However, facing the limitations in attempting to implement their reforms absent of engaging the electoral process, Black Populists helped to launch the People’s Party and used the then left-of-centre Republican Party in fusion campaigns. (Today though, after the Republican Party moved to the right, and the Democratic Party in the South was abandoned by the White Populist Dixiecrats who had opposed integration in the 1960s, most African Americans who vote cast ballots for Democratic Party candidates).\n\nBy the late 1890s, under relentless attack – propaganda campaigns warning of a “second Reconstruction” and “Negro rule,” physical intimidation, violence, and assassinations of leaders and foot soldiers – the movement was crushed. A key figure in the attack on Black Populism was Ben Tillman, the leader of South Carolina's white farmers' movement. As realistic politicians, the Southern Populist knew that they had only two possible alternatives in the fight against the ruling Bourbon Democrats. They must choose between trying to win the Negro votes or working to eliminate it entirely. The Tillman group in South Carolina sought the latter method. They were completely reactionary on the Negro question and stood with the Bourbons in disregarding the principles of the Fifteenth Amendment. Elsewhere the populists sought to win Negro votes, either through fusion with the Republican minority or through the raising of issues with a broad appeal to the Negro farmers. It was no accident that in the South the third-party movement was strongest in those states where it sought not only black votes but active black support.\n\nThe notion that African Americans had somehow betrayed populism would haunt the Georgia People's Party from the very beginning. Populists had realized the political importance of blacks. Of the state's forty thousand Republicans voters, a considerable majority were former bondsmen. If the white votes were to split, they might decide the outcome of any state election. But therein lay a predicament. How were Populists to court the black votes without losing the whites? How were they to keep whites from supporting the \"negro party\"? An attempt had to be made to win over blacks. It was a risky scheme, but it contained a degree of precedent in state politics. In the 1870s and 1880s, democrats and independents had sometimes used the same device when the white votes splits. In those days many whites were willing to allow African American men the ballot, especially when it could be sometimes bought for so little. \n\nBlack populism was destroyed, marking the end of organized political resistance to the return of white supremacy in the South in the late 19th century. Nevertheless, black populism stood as the largest independent political uprising in the South until the modern Civil Rights movement.\n\n"}
{"id": "3407129", "url": "https://en.wikipedia.org/wiki?curid=3407129", "title": "British Pipeline Agency", "text": "British Pipeline Agency\n\nBritish Pipeline Agency Ltd (BPA) is a joint venture, established in 1969, between BP Oil UK and Shell UK. The company operates the UK oil pipeline network of oil pipelines which transport petroleum products around the UK.\nBPA replaced the pipeline operations group of Shell Mex & BP in the late 1960s and took over the role as the majority operator on the Government Pipeline and Storage System. They continued as the major operator of the GPSS under reimbursable and fully indemnified contracts until the 1990s when those contracts were ended and instead the GPSS was instead operated by a number of companies under competitively tendered term contracts. \n\n"}
{"id": "42259363", "url": "https://en.wikipedia.org/wiki?curid=42259363", "title": "Customs4U", "text": "Customs4U\n\nCustoms4u is a custom online video portal featuring models from various countries who create customized, personalized video clips for customers and fans. The company was established in 2013 and is headquartered in London, England.\n\nThe service allows performers and models to own their own video content and control their own profits, rather than only working with studios.\n\nCustoms4U has established partnerships with several adult industry talent agencies and production studios, which have set up white-label websites for the adult performers they have under contract.\n\n"}
{"id": "11661", "url": "https://en.wikipedia.org/wiki?curid=11661", "title": "False Claims Act", "text": "False Claims Act\n\nThe False Claims Act, also called the \"Lincoln Law\" is an American federal law that imposes liability on persons and companies (typically federal contractors) who defraud governmental programs. It is the federal Government's primary litigation tool in combating fraud against the Government. The law includes a \"qui tam\" provision that allows people who are not affiliated with the government, called \"relators\" under the law, to file actions on behalf of the government (informally called \"whistleblowing\" especially when the relator is employed by the organization accused in the suit). Persons filing under the Act stand to receive a portion (usually about 15–25 percent) of any recovered damages. As of 2012, over 70 percent of all federal Government FCA actions were initiated by whistleblowers. Claims under the law have typically involved health care, military, or other government spending programs, and dominate the list of largest pharmaceutical settlements. The government recovered $38.9 billion under the False Claims Act between 1987 and 2013 and of this amount, $27.2 billion or 70% was from qui tam cases brought by relators.\n\n\"Qui tam\" laws have history dating back to the Middle Ages in England. In 1318, King Edward II offered one third of the penalty to the relator when the relator successfully sued government officials who moonlighted as wine merchants. The Maintenance and Embracery Act 1540 of Henry VIII provided that common informers could sue for certain forms of interference with the course of justice in legal proceedings that were concerned with the title to land. This act is still in force today in the Republic of Ireland, although in 1967 it was extinguished in England. The idea of a common informer bringing suit for damages to the Commonwealth was later brought to Massachusetts, where \"penalties for fraud in the sale of bread [are] to be distributed one third to inspector who discovered the fraud and the remainder for the benefit of the town where the offense occurred.\" Other statutes can be found on the colonial law books of Connecticut, New York, Virginia and South Carolina.\n\nThe American Civil War (1861–1865) was marked by fraud on all levels, both in the Union north and the Confederate south. During the war, unscrupulous contractors sold the Union Army decrepit horses and mules in ill health, faulty rifles and ammunition, and rancid rations and provisions, among other unscrupulous actions. In response, Congress passed the False Claims Act on March 2, 1863, . Because it was passed under the administration of President Abraham Lincoln, the False Claims Act is often referred to as the \"Lincoln Law\".\n\nImportantly, a reward was offered in what is called the \"qui tam\" provision, which permits citizens to sue on behalf of the government and be paid a percentage of the recovery. \"Qui tam\" is an abbreviated form of the Latin legal phrase \"qui tam pro domino rege quam pro se ipso in hac parte sequitur\" (\"he who brings a case on behalf of our lord the King, as well as for himself\") In a \"qui tam\" action, the citizen filing suit is called a \"relator\". As an exception to the general legal rule of standing, courts have held that \"qui tam\" relators are \"partially assigned\" a portion of the government's legal injury, thereby allowing relators to proceed with their suits.\n\nU.S. Senator Jacob M. Howard, who sponsored the legislation, justified giving rewards to whistle blowers, many of whom had engaged in unethical activities themselves. He said, \"I have based the [\"qui tam\" provision] upon the old-fashioned idea of holding out a temptation, and ‘setting a rogue to catch a rogue,’ which is the safest and most expeditious way I have ever discovered of bringing rogues to justice.\"\n\nIn the massive military spending leading up to and during World War II, the US Attorney General relied on criminal provisions of the law to deal with fraud, rather than using the FCA. As a result, attorneys would wait for the Department of Justice to file criminal cases and then immediately file civil suits under the FCA, a practice decried as \"parasitic\" at the time. Congress moved to abolish the FCA but at the last minute decided instead to reduce the relator's share of the recovered proceeds.\n\nThe law was again amended in 1986, again due to issues with military spending. Under President Ronald Reagan's military buildup, reports of massive fraud among military contractors had become major news, and Congress acted to strengthen the FCA.\n\nThe law has always primarily been used against defense contractors but by the late 1990s, health care fraud began to receive more focus, accounting for approximately 40% of recoveries by 2008 \"Franklin v. Parke-Davis\", filed in 1996, was the first case to apply the FCA to fraud committed against the government, due to bills submitted for payment by Medicaid/Medicare for treatments that those programs do not pay for as they are not FDA-approved or otherwise listed on a government formulary. FCA cases in the field of health care are often related to off-label marketing of drugs by drug companies, which is illegal under a different law, the Federal Food, Drug, and Cosmetic Act; the intersection occurs when off-label marketing leads to prescriptions being filled and bills for those prescriptions being submitted to Medicare/Medicaid.\n\nAs of 2012, over 70 percent of all federal FCA actions were initiated by whistleblowers. The government recovered $38.9 billion under the False Claims Act between 1987 and 2013 and of this amount, $27.2 billion or 70% was from \"qui tam\" cases brought by relators. In 2014, whistleblowers filed over 700 False Claims Act lawsuits. In 2016, the Department of Justice had its third highest annual recovery in False Claims Act history, obtaining more than $4.7 billion in settlements and judgments from civil cases involving fraud and false claims against the government. Since 2009 alone, the federal government has recovered $31.3 billion in False Claims Act settlements and judgments.\n\nThe Act establishes liability when any person or entity improperly receives from or avoids payment to the Federal government (tax fraud is excepted). The Act prohibits: \n\nCertain claims are not actionable, including:\n\nThere are unique procedural requirements in False Claims Act cases. For example:\n\nIn addition, the FCA contains an anti-retaliation provision, which allows a relator to recover, in addition to his award for reporting fraud, double damages plus attorney fees for any acts of retaliation for reporting fraud against the Government. This provision specifically provides relators with a personal claim of double damages for harm suffered and reinstatement.\n\nUnder the False Claims Act, the Department of Justice is authorized to pay rewards to those who report fraud against the federal government and are not convicted of a crime related to the fraud, in an amount of between 15 and 25 (but up to 30 percent in some cases) of what it recovers based upon the whistleblower's report. The relator's share is determined based on the FCA itself, legislative history, Department of Justice guidelines released in 1997, and court decisions.\n\n(False Claims Act Amendments \n\n\nOn May 20, 2009, the Fraud Enforcement and Recovery Act of 2009 (FERA) was signed into law. It includes the most significant amendments to the FCA since the 1986 amendments. FERA enacted the following changes:\n\n\nWith this revision, the FCA now prohibits knowingly (changes are in bold):\n\n\nOn March 23, 2010, the Patient Protection and Affordable Care Act (also referred to as the health reform bill or PPACA) was signed into law by President Barack Obama. The Affordable Care Act made further amendments to the False Claims Act, including:\n\n\nThe False Claims Act has a detailed process for making a claim under the Act. Mere complaints to the government agency are insufficient to bring claims under the Act. A complaint (lawsuit)\nmust be filed in U.S. District Court (federal court) in camera (under seal). After an investigation by the Department of Justice within 60 days, or frequently several months after an extension is granted, the Department of Justice decides whether it will pursue the case.\n\nIf the case is pursued, the amount of the reward is less than if the Department of Justice decides not to pursue the case and the plaintiff/relator continues the lawsuit himself. However, the success rate is higher in cases that the Department of Justice decides to pursue.\n\nTechnically, the government has several options in handing cases. These include: \n\nIn practice, there are two other options for the Department of Justice:\nThere is case law where claims may be prejudiced if disclosure of the alleged unlawful act has been reported in the press, if complaints were filed to an agency instead of filing a lawsuit, or if the person filing a claim under the act is not the first person to do so. Individual states in the U.S. have different laws regarding whistleblowing involving state governments.\n\nThe U.S. Internal Revenue Service (IRS) takes the position that, for Federal income tax purposes, \"qui tam\" payments to a relator under FCA are ordinary income and not capital gains. The IRS position was challenged by a relator in the case of \"Alderson v. United States\" and, in 2012, the U.S. Court of Appeals for the Ninth Circuit upheld the IRS' stance. As of 2013, this remained the only circuit court decision on tax treatment of these payments.\n\nIn a 2000 case, \"Vermont Agency of Natural Resources v. United States ex rel. Stevens\", 529 U.S. 765 (2000), the United States Supreme Court held that a private individual may not bring suit in federal court on behalf of the United States against a State (or state agency) under the FCA. In \"Stevens\", the Supreme Court also endorsed the \"partial assignment\" approach to \"qui tam\" relator standing to sue, which had previously been articulated by the Ninth Circuit Federal Court of Appeals and is an exception to the general legal rule for standing.\n\nIn a 2007 case, \"Rockwell International Corp. v. United States\", the United States Supreme Court considered several issues relating to the \"original source\" exception to the FCA's public-disclosure bar. The Court held that (1) the original source requirement of the FCA provision setting for the original-source exception to the public-disclosure bar on federal-court jurisdiction is jurisdictional; (2) the statutory phrase \"information on which the allegations are based\" refers to the relator's allegations and not the publicly disclosed allegations; the terms \"allegations\" is not limited to the allegations in the original complaint, but includes, at a minimum, the allegations in the original complaint as amended; (3) relator's knowledge with respect to the pondcrete fell short of the direct and independent knowledge of the information on which the allegations are based required for him to qualify as an original source; and (4) the government's intervention did not provide an independent basis of jurisdiction with respect to the relator.\n\nIn a 2008 case, \"Allison Engine Co. v. United States ex rel. Sanders\", the United States Supreme Court considered whether a false claim had to be presented directly to the Federal government, or if it merely needed to be paid with government money, such as a false claim by a subcontractor to a prime contractor. The Court found that the claim need not be presented directly to the government, but that the false statement must be made with the intention that it will be relied upon by the government in paying, or approving payment of, a claim. The Fraud Enforcement and Recovery Act of 2009 reversed the Court's decision and made the types of fraud to which the False Claims Act applies more explicit.\n\nIn a 2009 case, \"United States ex rel. Eisenstein v. City of New York\", the United States Supreme Court considered whether, when the government declines to intervene or otherwise actively participate in a \"qui tam\" action under the False Claims Act, the United States is a \"party\" to the suit for purposes of Federal Rule of Appellate Procedure 4(a)(1)(A) (which requires that a notice of appeal in a federal civil action generally be filed within 30 days after entry of a judgment or order from which the appeal is taken). The Court held that when the United States has declined to intervene in a privately initiated FCA action, it is not a \"party\" for FRAP 4 purposes, and therefore, petitioner's appeal filed after 30 days was untimely.\n\nIn a 2016 case, \"Universal Health Services, Inc. v. United States ex rel. Escobar\",the United States Supreme Court sought to clarify the standard for materiality under the FCA. The court unanimously upheld the implied certification theory of FCA liability and strengthened the FCA’s materiality requirement.\n\nAs of 2014, thirty states and the District of Columbia have also created false-claims statutes to protect their publicly funded programs from fraud by including qui tam provisions, which enables them to recover money at state level. Some of these state False Claims Act statutes provide similar protections to those of the federal law, while others limit recovery to claims of fraud related to the Medicaid program.\n\nThe California False Claims Act was enacted in 1987, but lay relatively dormant until the early 1990s, when public entities, frustrated by what they viewed as a barrage of unjustified and unmeritorious claims, began to employ the False Claims Act as a defensive measure.\n\nIn Australia, there have been calls since 2011 for legislation modeled on the False Claims Act and for their application to the tobacco industry and carbon pricing schemes\n\nIn October 2013, the UK Government announced that it is considering the case for financially incentivising individuals reporting fraud in economic crime cases by private sector organisations, in an approach much like the US False Claims Act. The 'Serious and Organised Crime Strategy' paper released by the UK's Secretary of State for the Home Department sets out how that government plans to take action to prevent serious and organised crime and strengthen protections against and responses to it. The paper asserts that serious and organised crime costs the UK more than £24 billion a year. In the context of anti-corruption, the paper acknowledges that there is a need to not only target serious and organised criminals but also support those who seek to help identify and disrupt serious and organised criminality. Three UK agencies, the Department for Business, Innovation & Skills, the Ministry of Justice and the Home Office have been tasked with considering the case for a US-style False Claims Act in the UK.\n\nUnder Rule 9(b) of the Federal Rules of Civil Procedure, allegations of fraud or mistake must be pleaded with particularity. All appeals courts to have address the issue of whether Rule 9(b) pleading standards apply to qui tam actions have held that the heightened standard applies. The Fifth Circuit, the Sixth Circuit, the Seventh Circuit, the Eighth Circuit, the Tenth Circuit, and the Eleventh Circuit have all found that plaintiffs must allege specific false claims.\n\nIn 2010, the First Circuit decision in \"U.S. ex rel. Duxbury v. Ortho Biotech Prods., L.P.\"(2009) and the Eleventh Circuit ruling in \"U.S. ex rel. Hopper v. Solvay Pharms., Inc.\"(2009) were both appealed to the U.S. Supreme Court. The Court denied \"certiorari\" for both cases, however, declining to resolve the divergent appeals court decisions.\n\nIn 2009, the American Civil Liberties Union (ACLU), Government Accountability Project (GAP) and OMB Watch filed suit against the Department of Justice challenging the constitutionality of the \"seal provisions\" of the FCA that require the whistleblower and the court to keep lawsuits confidential for at least 60 days. The plaintiffs argued that the requirements infringe the First Amendment rights of the public and the whistleblower, and that they violate the division of powers, since courts are not free to release the documents until the executive branch acts. The government moved for dismissal, and the district court granted that motion in 2009. The plaintiffs appealed, and in 2011 their appeal was denied.\n\nIn 2010, a subsidiary of Johnson & Johnson agreed to pay over $81 million in civil and criminal penalties to resolve allegations in a FCA suit filed by two whistleblowers. The suit alleged that Ortho-McNeil-Janssen Pharmaceuticals, Inc. (OMJPI) acted improperly concerning the marketing, promotion and sale of the anti-convulsant drug Topamax. Specifically, the suit alleged that OMJPI \"illegally marketed Topamax by, among other things, promoting the sale and use of Topamax for a variety of psychiatric conditions other than those for which its use was approved by the Food and Drug Administration, (i.e., \"off-label\" uses).\" It also states that \"certain of these uses were not medically accepted indications for which State Medicaid programs provided coverage\" and that as a result \"OMJPI knowingly caused false or fraudulent claims for Topamax to be submitted to, or caused purchase by, certain federally funded healthcare programs.\n\nIn response to a complaint from whistleblower Jerry H. Brown II, the US Government filed suit against Maersk for overcharging for shipments to US forces fighting in Iraq and Afghanistan. In a settlement announced on 3 January 2012, the company agreed to pay $31.9 million in fines and interest, but made no admission of wrongdoing. Brown was entitled to $3.6 million of the settlement.\n\nIn 2014, CareFusion paid $40.1 million to settle allegations of violating the False Claims Act by promoting off label use of its products in the case United States ex rel. Kirk v. CareFusion et al., No. 10-2492. The government alleged that CareFusion promoted the sale of its drug ChloraPrep for uses that were not approved by the FDA. ChloraPrep is the commercial name under which CareFusion produced the drug chlorhexidine, used to clean the skin before surgery. In 2017, this case was called into question and was under review by the DOJ because the lead attorney for the DOJ serving as Assistant Attorney General in the case, Jeffery Wertkin, was arrested by the FBI on January 31, 2017 for allegedly attempting to sell a copy of a complaint in a secret whistleblower suit that was under seal.\n\n\n"}
{"id": "12210455", "url": "https://en.wikipedia.org/wiki?curid=12210455", "title": "Federal Convention (German Confederation)", "text": "Federal Convention (German Confederation)\n\nThe Federal Convention (or \"Confederate Diet\" ) was the only central institution of the German Confederation from 1815 until 1848, and from 1850 until 1866. The Federal Assembly had its seat in the \"Palais Thurn und Taxis\" in Frankfurt. It was organized as a permanent congress of envoys.\n\nThe German Confederation and its Federal Assembly came into existence as a result of the Congress of Vienna in 1815 after the defeat of Napoleon. The original task was to create a new constitutional structure for Germany after the dissolution of the Holy Roman Empire eight years before. The princes of the German states wanted to keep their sovereignty, therefore the German Confederation was created as a loose confederation of independent monarchist states, but included four free cities as well. The founding act was the \"German Federal Act\" of June 8, 1815 (German: \"Deutsche Bundesakte\"), which was part of the treaty of the Congress of Vienna.\n\nThe Federal Assembly was created as a permanent congress of envoys of all member states, which replaced the former imperial central power of the Holy Roman Empire. The Federal Assembly took its seat at the \"Palais Thurn und Taxis\" in Frankfurt, where it met once a week after November 5, 1816.\n\nThe Federal Assembly was presided over by the Austrian delegate and consisted of two executive bodies: the inner council and the plenary session. Its members were not elected, neither by popular vote nor by state parliaments (which even didn't exist in some member states), but had been appointed by the state governments or by the state's prince.\n\nThe inner council consisted of 17 envoys (one seat each for the 11 larger states, 5 seats for the 23 smaller states and one seat for the four free cities). The inner council determined the legislative agenda and decided which issues should be discussed by the plenary session. Decisions of the inner circle initially required an absolute majority, but in 1822 unanimous consent was required for all decisions to have force.\n\nThe plenary session had 69 seats, according roughly to the state's sizes. The plenary session was involved especially in decisions regarding constitutional changes, which initially required a majority of 2/3 of the plenary session but was also changed to unanimous consent.\n\nThe decisions of the Federal Assembly had been mandatory for the member states, but the execution of those decisions remained under the control of each member state. As well, the member states remained fully sovereign regarding customs, police, and military.\n\nUntil the March Revolution of 1848 and again since 1851 the Federal Assembly of the German Confederation was the main instrument of the reactionary forces of Germany to suppress democracy, liberalism and nationalism. For example, during 1835/36, the Federal Assembly decreed rules for censorship, which banned the works of Heinrich Heine and other authors in all states of the German Confederation.\n\nAfter the March Revolution of 1848, the Federal Assembly of the German Confederation was challenged by the newly formed National Assembly, which began its sittings in Frankfurt on 18 May 1848. On 28 June, the National Assembly decided to created a provisional government for all of Germany prior to the creation of a Constitution. On 29 June, they elected Archduke John of Austria to be the Regent of the Provisional Central Power.\n\nAt noon on 12 July 1848, the Federal Assembly handed over its responsibilities to the Regent and formally dissolved itself. The act lent legitimacy and, at least in theory, legally binding authority to the new office. However, the Regent refused to employ his powers and remained passive during this period. The National Assembly lost prestige and was closed on 19 June 1849. The Regent resigned his office on 20 December 1849, though not before transferring all responsibilities of the provisional government to Austria and Prussia on 30 September.\n\nPrussia spent the next year challenging Austria's claims to supremacy in Germany, but on 30 November 1850 the Punctuation of Olmütz forced Prussia to abandon its proposal to alter Germany's political composition in its favor. By that time, all of the states in Germany had suppressed their Constitutions, popularly elected parliaments, and democratic clubs, thus erasing all work of the revolution. On 30 May 1851, the old Confederate Diet was reopened in the Thurn and Taxis Palace.\n\nThe Federal Assembly was dissolved after the Austro-Prussian War of 1866, the terms being dictated by the Peace of Prague on 23 August 1866. Although the North German Confederation was legally not the successor of the German Confederation, the new Federal Council (Bundesrat) could be seen as a kind of replacement for the Federal Assembly.\n\n"}
{"id": "198681", "url": "https://en.wikipedia.org/wiki?curid=198681", "title": "Federalism in China", "text": "Federalism in China\n\nChinese federalism refers to political theories which argue that China's central government should share sovereignty with regional entities, under a form of federalism. Such proposals were made in the early twentieth century, in connection with the end of the Qing dynasty; as well as recently, with a view to providing checks against the power of the central government, as well as settling the relationship between the People's Republic of China, Taiwan, Hong Kong, Macau, and other potential political entities.\n\nWu Bangguo, who used to be the official number two in China's leadership structure, said in 2011 there will be no federal system in China. \"There would be no separation of powers between the different branches of government and no federal system\" he said. \"It is possible that the state could sink into the abyss of internal disorder.\"\n\nThe Revive China Society, founded in November 1894 by Sun Yat-Sen, was among the first to suggest that a future Chinese government should be established on federal lines—a feeling expressed in the organisation's oath, \"Expel the northern barbarians, revive Zhonghua, and establish a unified (\"hézhòng\") government\". The term \"hézhòng\", literally meaning \"many unified as one\", refers to a federal structure such as the United States of America.\n\nDuring the Xinhai Revolution (1911), fourteen provinces proclaimed independence from the Qing dynasty and reunited as the Republic of China. But when the Guizhou Provincial Consultative Council proclaimed independence, they asked to build the Great Han Federal Democratic Republic. Prior to January 1912, one semi-official translation of the country's new name used by revolutional Shanghai Military Government was the United Provinces of China. Sun Yat-sen's title in 1912 was \"President of the Provisional Government of the United Provinces of China\". Chinese federalists from this period often used \"United Provinces\" instead of \"Federation\" or \"United States\" because \"states\" suggested a more independent arrangement than \"provinces.\" In other words, they wished to avoid the impression that federalism implied separatism.\n\nProposals for a federal Chinese state were first advanced in the 1920s, but these proved unpopular. These often used the phrase United Autonomous Provinces as the name of the intended system. Hunan was the center of this movement. The young Mao Zedong even advocated the formation of a \"Republic of Hunan\" during that period. But many intellectuals, including Sun Yat-sen, argued that these proposals would limit the ability of China to fight off external invasion and would legitimize the rule of warlords.\n\nAfter Chinese Communists established the Chinese Soviet Republic in Jiangxi, they aimed at a political system modeled after the union republics of the Soviet Union. According to their plans, China was to be a Soviet federal republic with several autonomous republics (such as Mongolia, Turkestan, and Tibet). During the period of the Long March they established a small autonomous republic for Tibetans in Sichuan. In Shaanxi, however, they changed their nationality policy, abandoning their plan to establish autonomous republics (as in the Soviet Union) in favor of autonomous regions. The first of these to be created was Inner Mongolia in 1947.\n\nWhen the People's Republic of China was founded in 1949, it was divided into six semi-independent greater administrative areas. The central government was transferred from the People's Government of North China and just controlled northern China and Inner Mongolia. Other greater administrative areas had more autonomy. This autonomy had ended completely by 1954.\n\nMore recently, some economists have argued that during the process of Chinese economic reform that the People's Republic has evolved into a de facto federal state in which provinces have wide discretion to implement policy goals which are set by the PRC central government and in which provinces and localities actively compete with each other in order to advance economically.\nAccording to a 2004 study conducted by Bo Zhiyue, Chair of the Department of International Studies at the St. John Fisher College, provincial authorities have greater institutional power than central institutions. Bo concluded that after the 16th Party Congress, due to personnel transfers between the provinces and the centre, the central authorities emerged more powerful, but was still shy of outpowering the provincial authorities on his power index score.\n\nCharter 08, one of the authors of which is human rights activist and Nobel Peace Prize winner Liu Xiaobo, calls for the establishment of a Chinese \"Federal Republic\". The relevant proposal states:\n\n\"A Federated Republic. A democratic China should seek to act as a responsible major power contributing toward peace and development in the Asian Pacific region by approaching others in a spirit of equality and fairness. In Hong Kong and Macao, we should support the freedoms that already exist. With respect to Taiwan, we should declare our commitment to the principles of freedom and democracy and then, negotiating as equals and ready to compromise, seek a formula for peaceful unification. We should approach disputes in the national-minority areas of China with an open mind, seeking ways to find a workable framework within which all ethnic and religious groups can flourish. We should aim ultimately at a federation of democratic communities of China.\"\n\nAs of late 2010, Charter 08 has already been signed by more than 10,000 people both inside and outside China.\n\nA Federal Republic of China is a proposed future federal republic encompassing mainland China, Macau, Hong Kong, and Taiwan. This \"Third Republic\" (following on from the Republic of China and the People's Republic of China) is proposed by supporters of the Tibet independence movement, although it would not in effect create an independent Tibet. Yan Jiaqi, writing for the Tibetan government in exile, has written that:\n\"It would be a federation with the characteristics of a confederation. Federal China would consist of two kinds of republics: 'loose republics' such as Taiwan, Hong Kong, Macao, Tibet, Inner Mongolia and Xinjiang: and 'close republics' consisting the rest of China.\"\n\nAccording to Yan:\n\n\"They would differ from the existing federal countries in their defence, taxation and legal systems\"\n\nThis model, however, in which the close republics would have an arrangement based on the United States of America, and the loose republics more on the European Union, is not agreed upon by all advocates of a Federal Republic.\n\nAnother concept is that of a United China or a United States of China. First devised in the early 1920s by Chen Jiongming, it was modeled closely after the United States of America. Given the political, social and linguistic realities of China in the warlord period, Chen Jiongming believed that a federalist approach was the only feasible way to eventually establish a united, democratic republic. Beginning with Guangdong as a model state, he wanted to organize a \"United States of China in the manner of the American experience\" through negotiation with federalists from all parts of the country (New York Times June 27, 1922).\n\nThis usage was popularized after Chinese president Jiang Zemin in 2001 made a comment that a united China can adopt a new national name and flag. Large economic ties between China and Taiwan have also motivated the occasional informal use of the term to describe a united China.\n\nThe introduction of Special Economic Zones since the 1980s have led to the development of several distinct regional economies within the People's Republic of China, such as the Pearl River Delta, Yangtze River Delta, and the Bohai Rim. Several of these regions have economies the size of small developed nations. Some scholars who use the term United States of China argue that during the process of Chinese economic reform the People's Republic has evolved into a de facto federal state in which these economic regions have wide discretion to implement policy goals which are set by the PRC central government and in which provinces and localities actively compete with each other in order to advance economically.\n\nThe concept of a United Republics of China first appeared in the fantastic \"Outline of (the) Post-War New World Map\". Published in Philadelphia in early 1942, this map - created by Maurice Gomberg - shows a proposal to re-arrange the world after an Allied victory against the Axis forces. In the map the United Republics of China (URC) includes most parts of present-day China, Korea, the erstwhile French colony of Indochina (now Vietnam, Laos and Cambodia), Thailand and Malaya. Otherwise, North Manchuria and Mongolia belong to the USSR; Taiwan and Hainan become territories of the USA.\n\nIn 2004, Lin Chong-Pin, former deputy Minister of Defense of the ROC, said that a think tank in Beijing or Shanghai gave a proposal for United Republics of China. None of this proposal has become public. But in the same years the officials and think-tanks of the PRC have often shown an interest in the 1964 merger of mainland Tanganyika and the archipelago of Zanzibar to form the United Republic of Tanzania. As Zanzibar has its own president, government, parliament, autonomy, etc. and the president of Zanzibar served as the vice-president of Tanzania until the first Multiparty elections in 1995, Tanzania may have provided an examplar of Deng Xiaoping's proposals for \"One country, two systems\" in China in the 1980s.\n\nIn 2011, Li Yi-hu, director of Institute of International Politics, University of Peking, said that Tanzania and Zanzibar, the model of \"One country, two constitutions\", could be referring to \"One China, two constitutions\". In February 2011, \"China Review News\" published an article about the Tanzanian style of Chinese reunification.\n\n\n"}
{"id": "14611843", "url": "https://en.wikipedia.org/wiki?curid=14611843", "title": "Green library", "text": "Green library\n\nA green library is designed to minimize negative impact on the natural environment and maximize indoor environmental quality by means of careful site selection, use of natural construction materials and biodegradable products, conservation of resources (water, energy, paper), and responsible waste disposal (recycling, etc.). In new construction and library renovation, sustainability is increasingly achieved through Leadership in Energy and Environmental Design (LEED) certification, a rating system developed and administered by the U.S. Green Building Council (USGBC).\n\nGreen libraries are a part of the larger green building movement. Also known as sustainable libraries, green libraries are being built all over the world, with many high-profile projects bringing the concept into the mainstream. Along with library 2.0, green design is an emerging trend, defining the library of the 21st century. Many view the library as having a unique role in the green building movement due to its altruistic mission, public and pedagogical nature, and the fact that new libraries are usually high profile, community driven projects.\n\nThere are many ways to define a green library, but there are a number of central themes that run through all of them, including, minimizing the negative impact the building will have on the local environment, and if possible having a positive impact. Reducing the use of water and energy by designing in a way that maximizes the use of natural and renewable resources. Integrating actual greenery and vegetation into the building and site design; Preferably, using drought resistant and/or native vegetation. And, maintaining high standards of indoor air quality to help ensure the health of the people who inhabit the building.\n\nDespite the fact that there are many paths to sustainable design, the emergence of the trend has created a demand for quantifiability. In the United States, the non-profit organization the United States Green Building Council (USGBC) developed the Leadership in Energy and Environmental Design (LEED) rating system in the year 2000. Their point based rating has a total of 100 base points possible, and buildings can be categorized as certified (40 points), silver (50), gold (60), or platinum (80+). LEED uses five different categories to judge a building's sustainability; 1) site location, 2) water conservation, 3)energy efficiency, 4) materials, 5) indoor air quality, and a bonus category for innovation and design. libraries accounted for 16% of all LEED projects (Brown, 2003).\n\nSustainable library design is strongly tied to the overall green building movement, but libraries have specific needs that present some extra challenges for green builders.\n\nThe biggest challenge is balancing the sometimes conflicting needs of the patrons and the materials. One of the central themes of the library's mission is to preserve knowledge, so that it can be passed on to future generations. For over a thousand years books have been the dominant way to do that. While the internet has become the information medium of choice for many, books still play a very important role in the preservation of knowledge. In order to be preserved, books must be kept away from extreme temperatures, moisture, and sunlight. In contrast, many individuals find sunlight to be the most enjoyable light for reading. Sunlight also plays a major role in green design, because it can be used to reduce the reliance on artificial lighting. For a long time, libraries needed to protect the collection from the damaging ultra-violet rays of the sun. New developments in glass technology over the past ten years have given designers more flexibility in their ability to place collections (Mcabe, 2003).\n\nAnother, often overlooked, challenge the library presents is the weight of the books. A common strategy in green design is to raise the floors to increase circulation, but the weight of the stacks can be an impediment to this strategy. To deal with this challenge, many designers have resorted to zoning the library into designated areas, so these strategies can be enacted in certain areas, and alternatives can be used in others.\nLibraries need to be built flexibly, in order to make room for expansions in size and in wiring capabilities. Library buildings are long term investments into the community, so when designing them architects need to be looking 50 or 100 years into the future. These obstacles by no means present insurmountable challenges to green libraries. The special needs of the library just need to be taken into consideration from the beginning of the project.\n\nGreen design is an integrated process. No one aspect of a building's architecture makes it green architecture. Without proper integration from the earliest moments of the planning phase, redundancies can occur, eliminating many of the potential benefits of sustainable design. Good sustainable design capitalizes on the synergistic relationships that occur between the various design elements. LEED groups these elements into five categories. Buildings can be designed in a way in which, good design in one category helps another category fulfill its goal.\n\nBefore building can start, a site must be chosen. The selection of the site has a large impact on how ecologically friendly the library will be. LEED has a number of guidelines to help the site selection process. There are a number of questions to consider that will help guide the site selection process, including, what kind of impact will construction have on the local environment, will there be erosion, what can be done with storm runoff, and is the site already green? Also, the library should be located in a densely populated area, near a number of other service related buildings. People should be able to reach the building via public transportation and the parking lots should give priority parking to those driving energy efficient automobiles. The heat island effect can be reduced by shading hard surfaces, putting them underground, or by implementing a vegetative roof (LEED, 2005)\n\nThere are many different ways for libraries to conserve water. A number of them rely on proper site selection. If a site is selected properly strategies can be used to capture rainwater runoff to be used in irrigation. Another strategy is to use low flow fixtures, and waterless urinals.\n\nEnergy efficiency is considered by many to be the most important category in becoming sustainable. In the LEED rating system it is the heaviest weighted of all the categories. Energy efficient design is in many ways a return to passive design principles that evolved over thousands of years, until the advent of air conditioning and cheap energy made those strategies appear to be unnecessary. After air conditioning became widely available, buildings were designed to eliminate influences of the outside environment. Lamis illustrates this point in \"Greening the Library\" when he compares two libraries built near the turn of the 20th century, the New York Public Library and the Boston Public Library; to two more recently built libraries, the Chicago Public Library and the Phoenix Public Library. The two older libraries have interior spaces that are narrow, so they can be reached by natural light and air. Whereas the two more modern libraries have large floor plans, with interior spaces far removed from the outside environment. Making them more dependent on artificial systems of temperature control.\n\nAs environmental awareness increases, as well as the cost of fossil fuels needed to operate giant heating, air conditioning, and ventilation ([HVAC]) systems, building designers are beginning to recognize that the outside environment cannot be ignored, and should be taken advantage of. What 21st-century designers are beginning to do is implement ancient passive design principles, while taking advantage of the most advanced technology available.\n\nThe passive strategies vary according to location, but they are always implemented to capitalize on the natural elements, mostly wind and sun, to manage the temperature and to provide ventilation and light. Active strategies are more technologically advanced solutions that include using various forms of renewable energy resources and using sensors to adjust lighting. Using photovoltaic cells that turn sunlight into energy is becoming an increasingly popular way to reduce energy dependence. In order to fully maximize energy efficiency and comfort, libraries are combining passive and active strategies.\n\nIt is believed that up to 40% of landfill space is filled with construction waste material. The primary responsibility in selecting materials for the library is to contribute as little waste as possible. Another responsibility is to choose materials that can be produced without causing too much damage to the natural environment. In order to fulfill the first responsibility, post-industrial and post-consumer recycled materials are being used. When purchasing materials claiming to be made from recycled goods it is important to investigate what their claims mean. It is a common marketing practice to exaggerate how green a product is by using misleading statements. Also, materials should be chosen that are going to be able to be reused or recycled 50–100 years down the road when the library building has reached the end of its useful life (Tseng, 2007). As non-renewable resources decrease, reusing and recycling are going to become increasingly necessary in the future.\n\nIt is also important to consider where materials are coming from: Resources have emerged to help guide the material selection process, such as the Forest Stewardship Council (FSC). They rate and certify wood based on a number of factors regarding how it was produced; rights of indigenous peoples, environmental impact, workers rights, efficiency, management, and conservation (FSC, 1996). Another material option is using quickly renewable materials such as bamboo in place of wood whenever possible. The widening availability of green building materials, along with the development of non-profit watchdog groups are two important factors in the greening of 21st-century library buildings.\n\nAlong with energy inefficiency, poor air quality has been another side-effect of the post air conditioning building design. Because most modern buildings are temperature controlled, they are designed to be airtight. The lack of ventilation can not only make buildings expensive to cool, it also traps harmful toxins that can do serious damage to people's respiratory systems. Toxins come from a variety of sources. Materials that make up the library, including paints and carpeting, have volatile organic compounds (VOC's), which produce a ground-level ozone after reacting with sunlight and nitrogen. The carbon dioxide that people breathe into the atmosphere is another toxin. To improve air quality, materials can be bought that have a low VOC content, and CO monitors can be installed to ensure that CO levels remain at a safe level. On average, people spend about 90% of their time indoors. Therefore, green buildings need to be designed in a way in which the air gets recycled, and does not stay stagnant. A green library is not just about taking care of the environment, it is about taking care of the health and well-being of those who work in it and patronize it.\n\nGreen libraries combine the needs of a library, sustainable design, and real cost savings in energy consumption (Brown, 2003). The main goal of green buildings is to develop and use sustainable energy-efficient resources in construction, maintenance, and overall life of the structure. Libraries considering green design will often look at the Leadership in Energy and Environmental Design (LEED) rating system. Brown (2003) identifies the following green design elements, which can be incorporated into libraries:\n\n•Community collaboration – makes sure that community assets are efficiently used and helps to maintain public support\n\n•Daylight – pair daylight with artificial lighting to reduce energy costs\n\n•Green materials – use renewable materials like wood, linoleum, bamboo, and cork\n\n•Green roofs\n\n•Raised floor systems\n\n•Energy efficiency\n\n•Natural ventilation\n\n•Green power and renewable energy\n\n•Indoor environmental quality\n\nFirst, libraries have been expanding the scope of their mission statements, to include working for the betterment of mankind. Second, technology is no longer a barrier. Third, it is great for the image of the library. Finally, sustainability offers the library a degree of independence, because cost of maintenance goes down, as does reliance on the volatile fossil fuels market.\n\nAll libraries have a mission statement, and spoken or unspoken, libraries are here to improve the condition of mankind. An institution can no longer, in good faith aim to improve the human condition while contributing to the destruction of the future: Buildings produce about 40% of the dangerous greenhouse gasses emitted into the atmosphere (Anisko & Willoughby, 2006). The environmental debate has evolved. The fact that humans are having a negative impact on the environment is no longer seriously questioned. Now, two questions shape the debate: What is our responsibility to fix it, and what can we do to fix it? Individuals and private organizations have a right to find their personal answers to those questions, but libraries are an investment in the future of our society. Libraries have a responsibility to not contribute to the destruction of the environment, to educate the people regarding our current situation, and empower them to make a difference. Libraries are discovering that their green building gives them a great opportunity to educate the citizenry (Tseng, 2007). As libraries continue to take a more progressive stance on improving the human condition, sustainability will have to be a central theme.\n\nThe availability of the technology and knowledge to build green buildings has passed a tipping point. Green buildings are constructed all over the world in every sector of the economy; residential, commercial, non-profit, government, etc. Another breakthrough is the diversity of green technology. There is an abundance of options, so any green builder has the ability to capitalize on the local natural resources available, and customize the building to most efficiently operate in the local environment. Along with the advancement of technology, the increasing awareness of environmental issues decrease the burden on the green builder. With the development of organizations like the USGBC and the FSC, green builders have information resources available to them. These organizations offer measurable levels of achievement to strive for, along with acting as watchdogs to help prevent the exaggeration of green credentials or \"green-washing.\" With these advances, sustainable construction is no longer a utopian fantasy, but is simply becoming the way good buildings are being built.\n\nThe library is undergoing an identity transformation. It is struggling to stay relevant, as a vocal minority predicts its demise. While its image as an outdated institution is not entirely deserved, it is trying to assert itself as an irreplaceable part of the community, that plans on being an assertive force for good in the 21st century. Green design helps it do that three different ways. 1) A sustainable building makes a statement that the library is investing in the future of the community. 2) Sustainable buildings are smartly designed, aesthetically pleasing, and are powered by state-of-the-art technology. When people see these emerald marvels they will no longer be able to maintain false stereotypes regarding libraries as anachronistic relics from an analog age. 3) More and more people take environmentalism seriously, so a green image is a good image. The public awareness on this issue is only going to increase. Libraries want the public to believe that they are still relevant, and that their mission is to better humankind. Many have decided that a green library is a physical manifestation of their mission statement, and it provides an image of how libraries want to be seen in the 21st century.\n\nAs publicly funded institutions, libraries are constantly battling with budget issues. Swings in the economy can affect the tax dollars coming into the library, as well as new legislation. Sustainable design offers libraries a way to reduce maintenance and energy costs, providing them with a degree of independence. Thanks to computer modeling software, building planning can be done more efficiently than in the past. Precise estimations on quantity of building materials can prevent waste and save money. Simulations can also be done to predict how big of an HVAC system the library needs. Solar 5.5 is a computer program that builds a 3-D model of the library's energy performance, and then plugs in various passive and Active Design strategies to see what kind of effect they would have on each other to maximize the energy savings and cost of the building; it has saved some California libraries up to 46% of the energy cost compared to meeting minimum state requirements (Boyden & Weiner, 2000).\n\nOne of the most important features of green design is a shift from the reliance on depleting fossil fuels to renewable energy resources. The independence from fossil fuels will save the library large sums of money, and it will relish its independence if prices continue to rise.\n\nMoney will also be saved by having higher morale, health, and productivity from employees. The architectural firm Heschong Mahone conducted a study that indicated students perform 25% better on standardized tests when in classrooms lit naturally. High levels of CO can decreases performance as well. Savings can also be increased, because there are governmental incentives to capitalize on, and some utility companies offer incentives too (Boyden & Weiner, 2001).\n\nBecause of the long-term nature of the library, green design is potentially less expensive than standard design, as heavy up-front costs often pay for themselves, waste is reduced, efficiency is increased, and energy and water are conserved. Evolving libraries of the 21st century are integrating sustainable practices, because it is becoming the most cost-effective way to do things.\n\nIn the 2000s (decade) a number of high-profile green libraries have been built in the U.S. and in the rest of the world.\n\nThe Fayetteville Public Library designed by Meyer, Scherer and Rockcastle, Ltd. in Minneapolis opened in October 2004. The library, \"Library Journal's\" 2005 Library of the Year, was the first building in Arkansas to register with the U.S. Green Building Council and achieved the silver LEED designation in 2006. To earn this designation the library employed many green-design techniques. The library was built on an empty lot a few blocks away from the city's bustling square, making it a textbook infill project. During construction, any trees removed were harvested and used for furniture or donated to local parks. Throughout the project, almost 99% of the construction waste was recycled or reused. More than 65% of the materials used to build the library were made within of the city. By incorporating a green roof and using alternative roofing materials, the design team reduced air temperature as much as 20 degrees. Water collected on the roof is reused for landscape irrigation. The library's green roof saves about $4,000 a year in energy savings. The building's reading spaces and circulation desks were situated to take advantage of the natural sunlight without over-working the building's air conditioners, reducing energy costs by 25% and the overall building's energy consumption by 30%. Sunlight streams through 75% of the building's public spaces.\n\nThe Seattle Central Library designed by Rem Koolhaas opened in May 2004. It employs a number of innovative techniques to achieve the status of a green library. It is located in a dense urban area, accessible by public transportation. Rainwater runoff is stored in a 40,000 gallon tank, and used to irrigate the landscape. It has triple glazed glass, used to reduce heat buildup. Seventy-five percent of the demolition and construction waste was recycled. Many other green strategies were employed that can be read in more detail here: SPL's green strategies\n\nThe Singapore National Library has been called the greenest building on the planet. Designed by Ken Yeang, it opened in July 2005. It is designed using light shelves that allow the light to filter into the library, without having any harsh effects. During the moments that the sun is either to bright or not bright enough, sensors are programmed to dim or brighten the lights, and raise and lower the shades to maximize comfort and reduce costs (Anisko & Willoughby, 2006)\n\nThe Central Branch of the Minneapolis Public Library System was designed by Cesar Pelli, and it opened in May 2006. It has a green roof. The green roof is planted with vegetation that does well in Minnesota's harsh climate, and it reduces rainwater runoff, reduces the building's heating and cooling load, reduces the buildings heat island effect, and adds green space to the downtown cityscape (MPL, 2006).\n\nThe Joe and Joan Martin Center is the first public building in Charlotte and Mecklenburg County certified by the US Green Building Council. In 2006, ImaginOn was awarded LEED certification at the silver level .\n\nThe Children's Museum of Pittsburgh underwent extensive expansion and renovation in 2004 using sustainable techniques and guiding principles thereby earning silver LEED-certification, one of largest museums in the country to receive this designation, and the first children’s museum in America to do so. For more detailed information, see The Green Museum\n\nOpened in August 2005, UC Merced's Kolligian Library was awarded Gold Leeds Certification in 2007. The glass-and-concrete building uses 42% less water and 50% less energy than comparable buildings. The building's carpet contains 37% recycled content, while its acoustical ceiling tiles contain 66% recycled content that includes telephone books and newspapers. Nearly 30% of the materials used to construct the building were manufactured locally, resulting in significant transportation and energy savings.\n\nThe list of green libraries is growing all the time. For an up-to-date information on green libraries and green library projects in the U.S. and Canada see green libraries.\n\n"}
{"id": "9223854", "url": "https://en.wikipedia.org/wiki?curid=9223854", "title": "Greenhouse debt", "text": "Greenhouse debt\n\nGreenhouse debt is the measure to which an individual person, incorporated association, business enterprise, government instrumentality or / [and] (per Neb., USA) geographic community exceeds its permitted greenhouse footprint and contributes greenhouse gases that contribute to global warming and climate change.\n\nThe concept makes no sense without a clear numerical value for the permitted greenhouse footprint. It is not clear what this value is.\n\nFriends of the Earth and similar organisations put forward the concept to define specifically the environmental harm caused by developed countries' past and present policies. Some governments, at least the Australian Labor leadership, have a tendency to accept such a line of reasoning.\n\nThe greenhouse debt assessment thus forms an ecological footprint analysis but can be used separately. Taken conjointly with a 'water debt' analysis and an ecological impact assessment, greenhouse debt analysis is basic to giving individuals, organisations, governments and communities an understanding of the effects they are having on Gaia, life, and global warming. \n\nEnsuring that the greenhouse debt is zero is essential towards achieving ecologically sustainable development or a sustainable retreat. Any greenhouse debt incurred will contribute to making life harder for future generations of humans and non-human lifeforms.\n\nThere are three possible consequences that occur as a result of a greenhouse debt.\n\n\n"}
{"id": "38947262", "url": "https://en.wikipedia.org/wiki?curid=38947262", "title": "Hog reeve", "text": "Hog reeve\n\nA hog reeve or hogreeve, hog-reeve, hog constable is a Colonial New England term for a person charted with the prevention or appraising of damages by stray swine. Wandering domestic pigs were a problem to the community, due to the amount of damage they could do to gardens and crops by rooting.\n\nThe owners of hogs were responsible for yoking and placing rings in their noses. If the hogs got loose and became a nuisance in the community, one or more of the men assigned as a hog reeve would be responsible for capturing and impounding the animal. If the animal did not have a ring in its nose, then the reeve was responsible for performing the necessary chore for the owner, who could legally be charged a small fee for the service. There were punishments and fines established for failing to yoke hogs and to control animals. In an 1865 Act of the General Assembly of Prince Edward Island, Canada, owners were required to pay 4 shillings per head to reclaim their animals, and hogreeves were permitted to sell unclaimed swine at public auction 48 hours after written notices were posted in public places in that district. \nIn New England, the owner of a stray hog in the town of Chelsea, Massachusetts would be charged 10 shillings per hog.\n\nThe office originated in Anglo-Saxon England, when hogreeves would be stationed at the doors of cathedrals during services to prevent swine from entering the church. The office of hogreeve was among the earliest elected offices to exist in colonial North America; the earliest rights specifically granted to towns in New England concerned the herding of swine, cattle, and the regulation of fences and common fields. The field driver held similar duties but was not restricted to swine. The hogreeve was a historical municipal official in Nova Scotia and Prince Edward Island in Canada; it was also listed as an elected office in early New Hampshire township records. \nIn Massachusetts, towns could vote to stop enforcement of the state law against letting swine run loose; many towns did so, leaving their hog reeves with nothing to do. As a result, it became a joke to elect a man hog reeve during the first year of his marriage.\n\n\n"}
{"id": "38455", "url": "https://en.wikipedia.org/wiki?curid=38455", "title": "Imre Lakatos", "text": "Imre Lakatos\n\nImre Lakatos (, ; ; November 9, 1922 – February 2, 1974) was a Hungarian philosopher of mathematics and science, known for his thesis of the fallibility of mathematics and its 'methodology of proofs and refutations' in its pre-axiomatic stages of development, and also for introducing the concept of the 'research programme' in his methodology of scientific research programmes.\n\nLakatos was born Imre (Avrum) Lipschitz to a Jewish family in Debrecen, Hungary in 1922. He received a degree in mathematics, physics, and philosophy from the University of Debrecen in 1944. In March 1944 the Germans invaded Hungary and Lakatos along with Éva Révész, his then-girlfriend and subsequent wife, formed soon after that event a Marxist resistance group. In May of that year, the group was joined by Éva Izsák, a 19-year-old Jewish antifascist activist. Lakatos, considering that there was a risk that she would be captured and forced to betray them, decided that her duty to the group was to commit suicide. Subsequently, a member of the group took her to Debrecen and gave her cyanide.\n\nDuring the occupation, Lakatos avoided Nazi persecution of Jews by changing his name to Imre Molnár. His mother and grandmother died in Auschwitz. He changed his surname once again to \"Lakatos\" (Locksmith) in honor of Géza Lakatos.\n\nAfter the war, from 1947, he worked as a senior official in the Hungarian ministry of education. He also continued his education with a PhD at Debrecen University awarded in 1948, and also attended György Lukács's weekly Wednesday afternoon private seminars. He also studied at the Moscow State University under the supervision of Sofya Yanovskaya in 1949. When he returned, however, he found himself on the losing side of internal arguments within the Hungarian communist party and was imprisoned on charges of revisionism from 1950 to 1953. More of Lakatos' activities in Hungary after World War II have recently become known. In fact, Lakatos was a hardline Stalinist and, despite his young age, had an important role between 1945 and 1950 (his own arrest and jailing) in building up the Communist rule, especially in cultural life and the academia, in Hungary. Preceding his fleeing to Vienna he confessed he has worked as an informer of State Protection Authority.\n\nAfter his release, Lakatos returned to academic life, doing mathematical research and translating George Pólya's \"How to Solve It\" into Hungarian. Still nominally a communist, his political views had shifted markedly and he was involved with at least one dissident student group in the lead-up to the 1956 Hungarian Revolution.\n\nAfter the Soviet Union invaded Hungary in November 1956, Lakatos fled to Vienna, and later reached England. He received a PhD in philosophy in 1961 from the University of Cambridge; his thesis advisor was R. B. Braithwaite. The book \"Proofs and Refutations: The Logic of Mathematical Discovery\", published after his death, is based on this work.\n\nLakatos never obtained British citizenship.\nIn 1960, he was appointed to a position in the London School of Economics, where he wrote on the philosophy of mathematics and the philosophy of science. The LSE philosophy of science department at that time included Karl Popper, Joseph Agassi and J. O. Wisdom. It was Agassi who first introduced Lakatos to Popper under the rubric of his applying a fallibilist methodology of conjectures and refutations to mathematics in his Cambridge PhD thesis.\n\nWith co-editor Alan Musgrave, he edited the often cited \"Criticism and the Growth of Knowledge\", the \"Proceedings\" of the International Colloquium in the Philosophy of Science, London, 1965. Published in 1970, the 1965 Colloquium included well-known speakers delivering papers in response to Thomas Kuhn's \"The Structure of Scientific Revolutions\".\n\nLakatos remained at the London School of Economics until his sudden death in 1974 of a heart attack at the age of 51. The Lakatos Award was set up by the school in his memory.\n\nIn January 1971, he became editor of the \"British Journal for the Philosophy of Science\", which J. O. Wisdom had built up before departing in 1965, and he continued as editor until his death in 1974, after which it was then edited jointly for many years by his LSE colleagues John W. N. Watkins and John Worrall, Lakatos's ex-research assistant.\n\nHis last LSE lectures in scientific method in Lent Term 1973 along with parts of his correspondence with his friend and critic Paul Feyerabend have been published in \"For and Against Method\" ().\n\nLakatos and his colleague Spiro Latsis organized an international conference devoted entirely to historical case studies in Lakatos's methodology of research programmes in physical sciences and economics, to be held in Greece in 1974, and which still went ahead following Lakatos's death in February 1974. These case studies in such as Einstein's relativity programme, Fresnel's wave theory of light and neoclassical economics, were published by Cambridge University Press in two separate volumes in 1976, one devoted to physical sciences and Lakatos's general programme for rewriting the history of science, with a concluding critique by his great friend Paul Feyerabend, and the other devoted to economics.\n\nLakatos' philosophy of mathematics was inspired by both Hegel's and Marx's dialectic, by Karl Popper's theory of knowledge, and by the work of mathematician George Pólya.\n\nThe 1976 book \"Proofs and Refutations\" is based on the first three chapters of his four chapter 1961 doctoral thesis \"Essays in the logic of mathematical discovery\". But its first chapter is Lakatos's own revision of its chapter 1 that was first published as \"Proofs and Refutations\" in four parts in 1963–4 in \"The British Journal for the Philosophy of Science\". It is largely taken up by a fictional dialogue set in a mathematics class. The students are attempting to prove the formula for the Euler characteristic in algebraic topology, which is a theorem about the properties of polyhedra, namely that for all polyhedra the number of their \"V\"ertices minus the number of their \"E\"dges plus the number of their \"F\"aces is 2:  (\"V – E + F = 2\"). The dialogue is meant to represent the actual series of attempted proofs which mathematicians historically offered for the conjecture, only to be repeatedly refuted by counterexamples. Often the students paraphrase famous mathematicians such as Cauchy, as noted in Lakatos's extensive footnotes.\n\nLakatos termed the polyhedral counter examples to Euler's formula \"monsters\" and distinguished three ways of handling these objects:\nFirstly, \"monster-barring\", by which means the theorem in question could not be applied to such objects. Secondly, \"monster-adjustment\" whereby by making a re-appraisal of the \"monster\" it could be \"made\" to obey the proposed theorem. Thirdly, \"exception handling\", a further distinct process. These distinct strategies have been taken up in qualitative physics, where the terminology of \"monsters\" has been applied to apparent counter-examples, and the techniques of \"monster-barring\" and \"monster-adjustment\" recognized as approaches to the refinement of the analysis of a physical issue.\n\nWhat Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true.)\n\nLakatos proposed an account of mathematical knowledge based on the idea of heuristics. In \"Proofs and Refutations\" the concept of 'heuristic' was not well developed, although Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs, and sometimes called his philosophy 'quasi-empiricism'.\n\nHowever, he also conceived of the mathematical community as carrying on a kind of dialectic to decide which mathematical proofs are valid and which are not. Therefore, he fundamentally disagreed with the 'formalist' conception of proof which prevailed in Frege's and Russell's logicism, which defines proof simply in terms of \"formal\" validity.\n\nOn its first publication as a paper in \"The British Journal for the Philosophy of Science\" in 1963–4, \"Proofs and Refutations\" became highly influential on new work in the philosophy of mathematics, although few agreed with Lakatos' strong disapproval of formal proof. Before his death he had been planning to return to the philosophy of mathematics and apply his theory of research programmes to it. Lakatos, Worrall and Zahar use Poincaré (1893) to answer one of the major problems perceived by critics, namely that the pattern of mathematical research depicted in \"Proofs and Refutations\" does not faithfully represent most of the actual activity of contemporary mathematicians.\n\nIn a 1966 text published as (Lakatos 1978), Lakatos re-examines the history of the calculus, with special regard to Augustin-Louis Cauchy and the concept of uniform convergence, in the light of non-standard analysis. Lakatos is concerned that historians of mathematics should not judge the evolution of mathematics in terms of currently fashionable theories. As an illustration, he examines Cauchy's proof that the sum of a series of continuous functions is itself continuous. Lakatos is critical of those who would see Cauchy's proof, with its failure to make explicit a suitable convergence hypothesis, merely as an inadequate approach to Weierstrassian analysis. Lakatos sees in such an approach a failure to realize that Cauchy's concept of the continuum differed from currently dominant views.\n\nLakatos's second major contribution to the philosophy of science was his model of the 'research programme', which he formulated in an attempt to resolve the perceived conflict between Popper's falsificationism and the revolutionary structure of science described by Kuhn. Popper's standard of falsificationism was widely taken to imply that a theory should be abandoned as soon as any evidence appears to challenge it, while Kuhn's descriptions of scientific activity were taken to imply that science was most constructive when it upheld a system of popular, or 'normal', theories, despite anomalies. Lakatos' model of the research programme aims to combine Popper's adherence to empirical validity with Kuhn's appreciation for conventional consistency.\n\nA Lakatosian research programme is based on a \"hard core\" of theoretical assumptions that cannot be abandoned or altered without abandoning the programme altogether. More modest and specific theories that are formulated in order to explain evidence that threatens the 'hard core' are termed \"auxiliary hypotheses\". Auxiliary hypotheses are considered expendable by the adherents of the research programme—they may be altered or abandoned as empirical discoveries require in order to 'protect' the 'hard core'. Whereas Popper was generally read as hostile toward such \"ad hoc\" theoretical amendments, Lakatos argued that they can be \"progressive\", i.e. productive, when they enhance the programme's explanatory and/or predictive power, and that they are at least permissible until some better system of theories is devised and the research programme is replaced entirely. The difference between a \"progressive\" and a \"degenerative\" research programme lies, for Lakatos, in whether the recent changes to its auxiliary hypotheses have achieved this greater explanatory/predictive power or whether they have been made simply out of the necessity of offering some response in the face of new and troublesome evidence. A degenerative research programme indicates that a new and more progressive system of theories should be sought to replace the currently prevailing one, but until such a system of theories can be conceived of and agreed upon, abandonment of the current one would only further weaken our explanatory power and was therefore unacceptable for Lakatos. Lakatos's primary example of a research programme that had been successful in its time and then progressively replaced is that founded by Isaac Newton, with his three laws of motion forming the 'hard core'.\n\nThe Lakatosian research programme deliberately provides a framework within which research can be conducted on the basis of 'first principles' (the 'hard core') which are shared by those involved in the research programme and accepted for the purpose of that research without further proof or debate. In this regard, it is similar to Kuhn's notion of a paradigm. Lakatos sought to replace Kuhn's paradigm, guided by an irrational 'psychology of discovery', with a research programme no less coherent or consistent yet guided by Popper's objectively valid logic of discovery.\n\nLakatos was following Pierre Duhem's idea that one can always protect a cherished theory (or part of one) from hostile evidence by redirecting the criticism toward other theories or parts thereof. (See \"Confirmation holism\" and Duhem–Quine thesis). This aspect of falsification had been acknowledged by Popper.\n\nPopper's theory, falsificationism, proposed that scientists put forward theories and that nature 'shouts NO' in the form of an inconsistent observation. According to Popper, it is irrational for scientists to maintain their theories in the face of Nature's rejection, as Kuhn had described them doing. For Lakatos, however, \"It is not that we propose a theory and Nature may shout NO; rather, we propose a maze of theories, and nature may shout INCONSISTENT\". The continued adherence to a programme's 'hard core', augmented with adaptable auxiliary hypotheses, reflects Lakatos's less strict standard of falsificationism.\n\nLakatos saw himself as merely extending Popper's ideas, which changed over time and were interpreted by many in conflicting ways. In his 1968 paper \"Criticism and the Methodology of Scientific Research Programmes\", Lakatos contrasted \"Popper0\", the \"naive falsificationist\" who demanded unconditional rejection of any theory in the face of any anomaly (an interpretation Lakatos saw as erroneous but that he nevertheless referred to often); \"Popper1\", the more nuanced and conservatively interpreted philosopher; and \"Popper2\", the \"sophisticated methodological falsificationist\" that Lakatos claims is the logical extension of the correctly interpreted ideas of \"Popper1\" (and who is therefore essentially Lakatos himself). It is, therefore, very difficult to determine which ideas and arguments concerning the research programme should be credited to whom.\n\nWhile Lakatos dubbed his theory \"sophisticated methodological falsificationism\", it is not \"methodological\" in the strict sense of asserting universal methodological rules by which all scientific research must abide. Rather, it is methodological only in that theories are only abandoned according to a methodical progression from worse theories to better theories—a stipulation overlooked by what Lakatos terms \"dogmatic falsificationism\". Methodological assertions in the strict sense, pertaining to which methods are valid and which are invalid, are, themselves, contained within the research programmes that choose to adhere to them, and should be judged according to whether the research programmes that adhere to them prove progressive or degenerative. Lakatos divided these 'methodological rules' within a research programme into its 'negative heuristics', i.e., what research methods and approaches to avoid, and its 'positive heuristics', i.e., what research methods and approaches to prefer. While the 'negative heuristic' protects the hard core, the 'positive heuristic' directs the modification of the hard core and auxiliary hypotheses in a general direction.\n\nLakatos claimed that not all changes of the auxiliary hypotheses of a research programme (which he calls 'problem shifts') are equally productive or acceptable. He took the view that these 'problem shifts' should be evaluated not just by their ability to defend the 'hard core' by explaining apparent anomalies, but also by their ability to produce new facts, in the form of predictions or additional explanations. Adjustments that accomplish nothing more than the maintenance of the 'hard core' mark the research programme as degenerative.\n\nLakatos' model provides for the possibility of a research programme that is not only continued in the presence of troublesome anomalies but that remains progressive despite them. For Lakatos, it is essentially necessary to continue on with a theory that we basically know cannot be completely true, and it is even possible to make scientific progress in doing so, as long as we remain receptive to a better research programme that may eventually be conceived of. In this sense, it is, for Lakatos, an acknowledged misnomer to refer to 'falsification' or 'refutation', when it is not the truth or falsity of a theory that is solely determining whether we consider it 'falsified', but also the availability of a \"less false\" theory. A theory cannot be rightfully 'falsified', according to Lakatos, until it is superseded by a better (i.e. more progressive) research programme. This is what he says is happening in the historical periods Kuhn describes as revolutions and what makes them rational as opposed to mere leaps of faith or periods of deranged social psychology, as Kuhn argued.\n\nAccording to the demarcation criterion of pseudoscience proposed by Lakatos, a theory is pseudoscientific if it fails to make any novel predictions of previously unknown phenomena or its predictions were mostly falsified, in contrast with scientific theories, which predict novel fact(s). Progressive scientific theories are those which have their novel facts confirmed and degenerate scientific theories, which can degenerate so much they become pseudo-science, are those whose predictions of novel facts are refuted. As he put it:\n\n\"A given fact is explained scientifically only if a new fact is predicted with it...The idea of growth and the concept of empirical character are soldered into one.\" See pages 34–5 of \"The Methodology of Scientific Research Programmes\", 1978.\n\nLakatos's own key examples of pseudoscience were Ptolemaic astronomy, Immanuel Velikovsky's planetary cosmogony, Freudian psychoanalysis, 20th century \"Soviet\" Marxism, Lysenko's biology, Niels Bohr's Quantum Mechanics post-1924, astrology, psychiatry, sociology, neoclassical economics, and Darwin's theory.\n\nIn his 1973 LSE Scientific Method Lecture 1 he also claimed that \"nobody to date has yet found a demarcation criterion according to which Darwin can be described as scientific\".\n\nAlmost 20 years after Lakatos's 1973 challenge to the scientificity of Darwin, in her 1991 \"The Ant and the Peacock\", LSE lecturer and ex-colleague of Lakatos, Helena Cronin, attempted to establish that Darwinian theory was empirically scientific in respect of at least being supported by evidence of likeness in the diversity of life forms in the world, explained by descent with modification. She wrote that our usual idea of corroboration as requiring the successful prediction of novel facts...Darwinian theory was not strong on temporally novel predictions. ... however familiar the evidence and whatever role it played in the construction of the theory, it still confirms the theory.\n\nIn his 1970 paper \"History of Science and Its Rational Reconstructions\" Lakatos proposed a dialectical historiographical meta-method for evaluating different theories of scientific method, namely by means of their comparative success in explaining the actual history of science and scientific revolutions on the one hand, whilst on the other providing a historiographical framework for rationally reconstructing the history of science as anything more than merely inconsequential rambling. The paper started with his now renowned dictum \"Philosophy of science without history of science is empty; history of science without philosophy of science is blind.\"\n\nHowever, neither Lakatos himself nor his collaborators ever completed the first part of this dictum by showing that in any scientific revolution the great majority of the relevant scientific community converted just when Lakatos's criterion – one programme successfully predicting some novel facts whilst its competitor degenerated – was satisfied. Indeed, for the historical case studies in his 1968 paper \"Criticism and the Methodology of Scientific Research Programmes\" he had openly admitted as much, commenting 'In this paper it is not my purpose to go on seriously to the second stage of comparing rational reconstructions with actual history for any lack of historicity.'\n\nPaul Feyerabend argued that Lakatos's methodology was not a methodology at all, but merely \"words that \"sound\" like the elements of a methodology.\" He argued that Lakatos's methodology was no different in practice from epistemological anarchism, Feyerabend's own position. He wrote in \"Science in a Free Society\" (after Lakatos's death) that: Lakatos realized and admitted that the existing standards of rationality, standards of logic included, were too restrictive and would have hindered science had they been applied with determination. He therefore permitted the scientist to violate them (he admits that science is not \"rational\" in the sense of \"these\" standards). However, he demanded that research programmes show certain features \"in the long run\" — they must be progressive... I have argued that this demand no longer restricts scientific practice. Any development agrees with it. Lakatos and Feyerabend planned to produce a joint work in which Lakatos would develop a rationalist description of science and Feyerabend would attack it. The correspondence between Lakatos and Feyerabend, where the two discussed the project, has since been reproduced, with commentary, by Matteo Motterlini.\n\n\n\n\n\n"}
{"id": "24106629", "url": "https://en.wikipedia.org/wiki?curid=24106629", "title": "Kids for cash scandal", "text": "Kids for cash scandal\n\nThe \"kids for cash\" scandal centered on judicial kickbacks to two judges at the Luzerne County Court of Common Pleas in Wilkes-Barre, Pennsylvania. In 2008, judges Michael Conahan and Mark Ciavarella were accused of accepting money in return for imposing harsh adjudications on juveniles to increase occupancy at for-profit detention centers.\n\nCiavarella disposed thousands of children to extended stays in youth centers for offenses as trivial as mocking an assistant principal on Myspace or trespassing in a vacant building. After a judge rejected an initial plea agreement in 2009, a federal grand jury returned a 48-count indictment. In 2010, Conahan pleaded guilty to one count of racketeering conspiracy and was sentenced to 17.5 years in federal prison. Ciavarella opted to go to trial the following year. He was convicted on 12 of 39 counts and sentenced to 28 years in federal prison.\n\nIn the wake of the scandal, the Supreme Court of Pennsylvania overturned hundreds of adjudications of delinquency in Luzerne County. The Juvenile Law Center filed a class action lawsuit against the judges and numerous other parties, and the Pennsylvania state legislature created a commission to investigate juvenile justice problems in the county.\n\nThe Pennsylvania state Judicial Conduct Board received four complaints about Conahan between 2004 and 2008, but later admitted it failed to investigate any of them, nor had it sought documentation regarding the cases involved. The FBI was tipped off about Conahan and nepotism in the county courts in 2006. An additional investigation into improper sentencing in Luzerne County began early in 2007 as a result of requests for assistance from several youths received by the Philadelphia-based Juvenile Law Center. Attorneys from the center determined that several hundred cases were tried without the defendants receiving proper counsel. In April 2008, the Center petitioned the Pennsylvania Supreme Court seeking relief for alleged violation of the youths' civil rights. The application of relief was later denied, then reconsidered in January 2009 when charges of corruption against the judges surfaced.\n\nThe Federal Bureau of Investigation and the Internal Revenue Service also investigated the two judges, although the exact dates and scope of the investigations by the two federal agencies were not made public. Part of the investigation was revealed to have occurred during disciplinary hearings over the conduct of another former Luzerne County judge, Ann H. Lokuta. Lokuta was brought before the Judicial Conduct Board of Pennsylvania in November 2006 to answer charges of using court workers to do her personal bidding, openly displaying bias against some attorneys arguing before her, and publicly berating staff to cause mental distress. The board ruled against Lokuta in November 2008, and she was removed from the bench. During the course of the disciplinary hearings, Lokuta accused Conahan of bullying behavior and charged that he was behind a conspiracy to have her removed. Lokuta aided the federal investigation into the \"kids for cash\" scheme prior to the determination of the disciplinary board, and a stay order was issued in March 2009 by the state Supreme Court in light of the ongoing corruption investigations, halting Lokuta's removal and the election that was to be held in May to replace her. During the Lokuta hearing, Conahan testified that there were no social relationships amongst the county judges. However, Judge Michael Toole, who was later convicted of case fixing, as well as another judge, had each stayed at a Florida condo jointly owned by Conahan and Ciavarella. The state Supreme Court decided to uphold Lokuta's removal from the bench in January 2011, finding she had received a fair trial, regardless of Conahan's testimony. It also ordered the expungements of the records of 2,401 of those juveniles who were affected.\n\nA statement from the office of the United States Attorney for the Middle District of Pennsylvania outlined the charges against the two judges on January 26, 2009. The charges outlined in the information described actions between 2000 and 2007 by both judges to assist in the construction and population of private juvenile facilities operated by the two Pennsylvania Child Care companies, acting in an official capacity in favor of the private facilities over the facility operated by Luzerne County.\n\nThe U.S. Attorney charged that in 2002 Conahan, who at the time was President Judge of the court, used his authority to remove funding for the county-operated facility. The judges were alleged to have received \"millions of dollars\" in payments for the completion of a binding agreement between the court and the private facilities, co-owned by attorney Robert Powell, to use their services and the subsequent closing of the county facility. The methods used to conceal the payments involved several parties and transactions which resulted in allegations of tax evasion against the two. Ciavarella and Conahan were also charged with \"Ordering juveniles to be sent to these facilities in which the judges had a financial interest even when Juvenile Probation Officers did not recommend placement,\" according to the statement.\n\nOriginal, negotiated plea agreements called for both judges to serve up to seven years in prison, pay fines and restitution, and accept responsibility for the crimes. However, on July 30, 2009, Judge Edwin M. Kosik of Federal District Court in nearby Scranton rejected the plea agreement, citing \"post-guilty plea conduct and expressions from the defendants\" that he ruled did not satisfy the terms of the agreement. Kosik wrote that Conahan and Ciavarella continued to deny their crimes even in the face of overwhelming evidence, and therefore did not merit sentences that were well below federal sentencing guidelines. Attorneys for the two judges brought a motion requesting reconsideration of the judge's rejection of the plea agreement. The motion was denied on August 24, 2009, and Ciavarella and Conahan subsequently withdrew their guilty pleas, an action which eventually resulted in a jury trial for Ciavarella and additional charges against the former judges.\n\nFollowing the plea withdrawals, on September 9, 2009, a federal grand jury in Harrisburg, Pennsylvania returned a 48-count indictment against Ciavarella and Conahan including racketeering, fraud, money laundering, extortion, bribery, and federal tax violations. Both judges were arraigned on the charges on September 15, 2009. Ciavarella and Conahan entered pleas of not guilty on all counts and remained free on $1 million in bail, despite federal prosecutors' contentions that their bail should be raised. Prosecutors argued the judges' bail should have been higher, since they faced the possibility of substantially more prison time and there had been evidence of attempts made to shield assets.\n\nRobert Powell, an attorney and co-owner of the two juvenile facilities at the heart of the scandal, pleaded guilty on July 1, 2009, to failing to report a felony and being an accessory to tax evasion conspiracy, in connection with $770,000 in kickbacks he paid to Ciavarella and Conahan in exchange for facilitating the development of his juvenile detention centers. The Pennsylvania Supreme Court temporarily suspended Powell's law license on September 1, citing his criminal conviction.\n\nRobert Mericle, the prominent real estate developer who built the two juvenile facilities, pleaded guilty on September 3, 2009, to failing to disclose a felony. Mericle had failed to tell a grand jury he had paid $2.1 million to Ciavarella and Conahan as a finder's fee. As part of his plea, Mericle agreed to pay $2.15 million to fund local children's health and welfare programs. Mericle faced up to three years in prison and a $250,000 maximum fine. Mericle was released from federal custody in 2015 after serving a one-year sentence.\n\nSandra Brulo, the former Deputy Director of Forensic Services for the Luzerne County Juvenile Probation Office, agreed to plead guilty in March 2009 to federal obstruction of justice. Those charges stemmed from actions Brulo took after she became aware she had been named in the federal civil action. Brulo backdated her recommendation of a placement she made concerning a juvenile defendant in September 2007, and changed her original recommendation of placement to probation.\n\nOn February 18, 2011, following a trial, a federal jury convicted Ciavarella on 12 of the 39 remaining counts he faced including racketeering, a crime in which prosecutors said the former judge used children \"as pawns to enrich himself.\" In convicting Ciavarella of racketeering, the jury agreed with prosecutors that he and Conahan had taken an illegal payment of nearly $1 million from a youth center's builder, then hid the money.\n\nThe panel of six men and six women also found Ciavarella guilty of \"honest services mail fraud\" and of being a tax cheat, for failing to list that money and more on his annual public financial-disclosure forms and on four years of tax returns. In addition, they found him guilty of conspiring to launder money. The jurors acquitted Ciavarella of extortion and bribery in connection with $1.9 million that prosecutors said the judges extracted from the builder and owner of two youth centers, including allegations that Ciavarella shared the proceeds of FedEx boxes that were stuffed with tens of thousands of dollars in cash.\n\nFollowing Ciavarella's conviction, he and his lawyer appeared on the steps of the courthouse to give an impromptu press conference. The press conference was interrupted by Sandy Fonzo, whose son Edward Kenzakoski committed suicide after Ciavarella adjudicated him to placement, despite Kenzakoski's first-time offender status.\n\nOn August 11, 2011, Ciavarella was sentenced to 28 years in federal prison as a result of his conviction. He is currently being held at the Federal Correctional Institution, Ashland, a low-security federal prison in eastern Kentucky. He is scheduled for release in 2035, when he will be 85 years old. Ciavarella appealed his conviction to the Third Circuit and it was rejected on July 25, 2013.\n\nHowever, Ciavarella has continued to appeal, contending that the Supreme Court's decision in the case of ex-Governor of Virginia Robert McDonnell narrowed the scope of the honest services fraud statute. On January 9, 2018, federal judge Christopher C. Conner threw out Civarella's convictions for racketeering, conspiracy to commit racketeering, and conspiracy to commit money laundering. Conner upheld Civarella's contention that his attorneys failed to raise statute of limitations claims on those charges. He ordered a new trial on those counts, but allowed the honest services fraud convictions to stand.\nOn September 23, 2011, Conahan was sentenced to 17 and a half years in federal prison after pleading guilty to one count of racketeering conspiracy. , he is currently being held at the security facility at the Federal Correctional Institution, Miami in Florida. He is scheduled for release in 2026, when he will be 74 years old.\n\nOn November 4, 2011, Powell was sentenced to 18 months in federal prison after pleading guilty to failing to report a felony and being an accessory to tax conspiracy. He was incarcerated at the Federal Prison Camp, Pensacola, a low-security facility in Florida, and was released from a halfway house on April 16, 2013. On August 10, 2015, a judge approved a $4.75M settlement agreement for Powell, for having paid kickbacks related to the adjudication of at least 2,400 juveniles.\n\nRobert Mericle's sentencing in connection with his guilty plea for failing to report a felony was delayed pending his anticipated testimony in the bribery trial of former Congressman and Pennsylvania State Senator Raphael Musto. On November 23, 2010, a federal grand jury had issued a six-count indictment against Musto, charging him with accepting more than $28,000 from an unnamed company and individual in exchange for his help in obtaining grants and funding. Musto denied any wrongdoing.\nAfter Musto died on April 24, 2014, Mericle was sentenced the next day to one year in prison by Senior District Judge Edwin M. Kosik. Mericle was ordered to pay a $250,000 fine and to report to the Federal Bureau of Prisons on May 14, 2014. He was released on May 29, 2015.\n\nActing under a rarely used power established in 1722 and reserved for extraordinary circumstances, known as \"King's Bench jurisdiction\", the Pennsylvania Supreme Court appointed a special master on February 11, 2009, to review all juvenile cases handled by Ciavarella. Senior Judge Arthur Grim of the Berks County Court of Common Pleas was appointed special master and returned his findings in an interim report dated March 11, 2009. Grim recommended that all adjudications handed down by Civarella from 2003 to 2008 be vacated, and that the affected juveniles' records be expunged. He concluded that due to Civarella's disregard for the juveniles' constitutional rights, as well as the kickbacks, no one who appeared before Civarella in that period had a truly impartial hearing. On March 26, 2009, the Supreme Court approved Grim's recommendations and ruled that Ciavarella had violated the constitutional rights of thousands of juveniles, and initially hundreds of juvenile adjudications were ordered overturned.\n\nA class action lawsuit was filed by the Juvenile Law Center on behalf of the juveniles who were adjudicated delinquent by Ciavarella despite not being represented by counsel or advised of their rights. Besides naming Ciavarella and Conahan, the suit seeks damages under the civil portion of the Racketeer Influenced and Corrupt Organizations Act (RICO) against the judges' spouses and business associates, shell companies, youth center officials, and Luzerne County. Three other federal lawsuits filed on behalf of the victims have been consolidated with the Juvenile Law Center lawsuit into a master class action. An amended master complaint was filed on August 28, 2009.\n\nIn June 2010, an injunction was filed on behalf of PA Child Care, Western PA Child Care, and Mid Atlantic Youth Services, the companies that provided treatment programs at the youth centers, to prevent the ordered destruction of thousands of juvenile records on the grounds the records are needed for the defense's case.\n\nOn July 8, 2013, a three-judge panel of the Third Circuit Court of Appeals ruled in the case of Markel International Insurance Company v. Robert J. Powell, (his business partner) Gregory Zappala, et al., that the insurance company had no obligation to defend or indemnify the individuals or corporations involved, leaving the defendants liable for adverse judgments.\n\nIn the aftermath of the federal charges and defendant pleas, the Pennsylvania General Assembly moved to create a commission to investigate the entire set of circumstances surrounding the miscarriage of justice in Luzerne County. Sponsored by Representative Todd Eachus of Butler Township in Luzerne County, House Bill 1648 established the Interbranch Commission on Juvenile Justice in July 2009. The commission comprises 11 members, appointed from each branch of government in Pennsylvania, with four members chosen by the judiciary, four by the legislature and three by the governor.\n\nIn signing the legislation on August 7, 2009, Governor Ed Rendell castigated Ciavarella and Conahan, saying they \"violated the rights of as many as 6000 young people by denying them basic rights to counsel and handing down outrageously excessive sentences. The lives of these young people and their families were changed forever.\" Scheduled to meet a minimum of once per month, the commission was organized to investigate the actions of and damages caused by the two judges and review the state of the Luzerne County courts left in the wake of their tenures. The commission was given power of subpoena and was required to complete its work and report its recommendations and findings to the three branches of state government by May 31, 2010.\n\nThe scandal was featured in \"\", the 2009 documentary by Michael Moore. A full-length documentary covering the scandal entitled \"Kids for Cash\" was released in February 2014, and has been favorably reviewed. In April 2014, CNBC's crime documentary series \"American Greed\" reported on the case.\n\nThe scandal has also led to several portrayals in fictional works. The \"\" episode \"\", the \"\" episode \"Crossroads\", and an episode of \"The Good Wife\" featured corrupt judges sending children to private detention centers. An episode of \"Cold Case\" called \"Jurisprudence\" is loosely based on this event. In 2015, crime fiction novelist Ace Atkins, using the nom de plume of the late Robert B. Parker, wrote \"Kickback\", which borrows heavily from this case, transposing it into a Boston suburb instead. An episode of \"Billions\" called \"Quality of Life\" has a subplot inspired by the scandal in which a New York City judge sentences youths harshly for financial gain. The novel \"Corrupted\" by Lisa Scottoline includes the scandal, portraying characters in the book as victims of the scandal.\n\nThe scandal was featured in an April 2018 episode of \"Swindled\", a podcast dealing with white-collar crime.\n\nThe 2017 drama-thriller film \"The Archer\" is loosely based on the scandal and is set in the California and Utah mountains instead of Pennsylvania.\n\n\n"}
{"id": "55443257", "url": "https://en.wikipedia.org/wiki?curid=55443257", "title": "Kocowa", "text": "Kocowa\n\nKocowa is an online video streaming website headquartered in Los Angeles as a joint venture between the 3 major Korean Broadcast Networks KBS (Korean Broadcasting System), MBC (Munhwa Broadcasting Corporation) and SBS (Seoul Broadcasting System) to provide Korean TV programs outside South Korea.\n\nA subsidiary of KCP (Korea Content Platform) the name Kocowa stands for Korean Content Wave launched on July 2017 to become the centralized content broker for existing platforms Viki, OnDemandKorea, and DramaFever\n\nThree major Korean Broadcast Networks KBS (Korean Broadcasting System), MBC (Munhwa Broadcasting Corporation) and SBS (Seoul Broadcasting System) consolidated its efforts to distribute content globally due to the rising demand for Korean content across the globe. \nIn July 2017, Kocowa launched its streaming services accessible via PC, mobile and tablet with plans to expand its streaming services to Smart TVs.\n\nKocowa streams a variety of Korean programs ranging from TV series such as School 2017 and The King in Love, variety shows Running Man and Infinite Challenge, and K-pop music stages including Inkigayo and Music Bank World Tour up to six hours after being broadcast in Korea. \nTo differ from its platform partners Kocowa plans to provide perks and access to exclusive content including the 2017 World Tour of popular K-POP artist G-Dragon.\nUnlike Viki's subtitling technology that crowd-sources subtitle translations under a Creative Commons license Kocowa's content is translated and subtitled by a certified translation team. Content will be translated initially in English and exclusively consumed in the U.S. market with plans to expand to Spanish and French speaking countries.\n\nKocowa is a subscription based service offering a variety of plans from $0.99 per day, $6.99 per month or $69.99 annually that provides full access to its programing without ads within six hours after being broadcast in Korea. Those who opt for the advertising-based plan can access new content after 24 hours of its release. \nIn August 2017, Viki announced the partnership with Kocowa which allows its Viki Pass Plus subscribers to have additional access to Kocowa contents without separate subscription on Kocowa\n"}
{"id": "14837954", "url": "https://en.wikipedia.org/wiki?curid=14837954", "title": "Kyiv Student Journal of International Law", "text": "Kyiv Student Journal of International Law\n\nThe Kyiv Student Journal of International Law (KSJIL) or Jus Gentium (Russian: \"Киевский студенческий журнал международного права\"; Ukrainian: \"Київський студентський журнал міжнародного права\") is an online quarterly journal focusing on issues of public and private international law. It was created by international law students at Kiev National University's Institute of International Relations in 2005. The journal accepts material from all international law students at the university; students also prepare each issue for publication, supervised by Ukrainian legal scholars. Articles written in English or Russian are occasionally included, but most are written in Ukrainian. The editors state they hope to publish more English- and Russian-language articles in the future by recruiting more international students to the institute.\n\n"}
{"id": "4077197", "url": "https://en.wikipedia.org/wiki?curid=4077197", "title": "Leo Mechelin", "text": "Leo Mechelin\n\nLeopold (Leo) Henrik Stanislaus Mechelin (24 November 1839 in Hamina, Finland – 26 January 1914 in Helsinki, Finland) was a Finnish professor, statesman, senator and liberal reformer. A leading defender of the autonomy of the Grand Duchy of Finland, and of the rights of women and minorities, Mechelin's 1905–1908 government\n(\"Mechelin's Senate\") made Finland the first nation in the world with the universal right to vote and to be elected. During his period in office the freedom of expression, the press, and of assembly were introduced.\n\nHe also founded the Liberal Party of Finland (1880–1885), wrote its program, was one of the founders of the Union Bank of Finland 1862 (now part of Nordea Bank) and co-founded the Nokia Company (1871) with Fredrik Idestam, was the first chairman of the town council of Helsinki (1875–1876 and 1892–1899) and an internationally respected expert on politology and member of peace movement. Emperor Alexander II ennobled Mechelin 1876.\n\nMechelin led the passive resistance in Finland during the first period of oppression (1899–1905) until and even after his banishment (1903), from which officials had to let him return as a member of parliament (House of Nobles) 1904, welcomed by a celebrating crowd of 10,000 people. In a secret meeting of the Kagaali, Mechelin had written a petition against the draft of Finns to the Russian army, which collected almost 500,000 signatures. His coalition, the Constitutionals, managed to end the draft through boycott.\n\nBorn in Hamina in 1839, the son of Gustaf Johan Mechelin and Amanda Gustava Costiander, Leo Mechelin studied at the University of Helsinki, gaining his Bachelor's and Master's degree's in Philosophy in 1860, a bachelor's degree in law in 1864, and a License and Doctorate in 1873.\n\nAs professor of jurisdiction and politology 1874-82, Mechelin had argued that the tsars were bound by the old constitutional laws from the time of the Swedish rule of Finland (before 1809), and hence affirmed that Finland was a separate, constitutional state, which the tsar could only rule by law, whereas in Russia he had absolute power. During the periods of oppression, the tsar tried to impose unconstitutional laws, which Mechelin opposed. The unrests in Russia and Finland (1905) finally compelled the tsar to comply with the November Manifesto written by Mechelin. This allowed Mechelin to form a government (1905–1908) and to transform Finland into what was in many respects the first liberal democracy (e.g., in New Zealand women already had the right to vote but not to be voted; in Australia only white people had those rights) in 1906. In 1907, the first universal elections to the one-chamber parliament (\"eduskunta\") were held, and 19 of its 200 first members were women. However, the constitutionals of all parties did not obtain the majority of seats, and the tsar realised that he could carry on with the oppression, starting the second period of oppression (1908–1917). After Mechelin's death (in 1914), the two revolutions in Russia allowed Finland to declare its independence (1917) and Mechelin's younger co-workers were able to complete his work.\n\nNokia, once a world-leading mobile phone corporation, was founded by Mechelin and his student days' roommate Fredrik Idestam as a forestry company. Later Mechelin's wish to expand into the electricity business was at first thwarted by Idestam's opposition, but Mechelin managed to convince most shareholders of his plans and became the company chairman (1898–1914), thus being able to realise his visions.\n\nMechelin was also active in civil society and President of the current University of Art and Design Helsinki and of the Finnish Art Society. As a politician he was always highly respected among all parties and citizens, although after the dissolution of the Liberal Party (1885) he never joined any other party.\n\nMechelin demanded peaceful nonviolent resistance and did not bend even during hard times.\n\n\n"}
{"id": "18290", "url": "https://en.wikipedia.org/wiki?curid=18290", "title": "Light-emitting diode", "text": "Light-emitting diode\n\nA light-emitting diode (LED) is a two-lead semiconductor light source. It is a p–n junction diode that emits light when activated. When a suitable current is applied to the leads, electrons are able to recombine with electron holes within the device, releasing energy in the form of photons. This effect is called electroluminescence, and the color of the light (corresponding to the energy of the photon) is determined by the energy band gap of the semiconductor. LEDs are typically small (less than 1 mm) and integrated optical components may be used to shape the radiation pattern.\n\nAppearing as practical electronic components in 1962, the earliest LEDs emitted low-intensity infrared light.\nInfrared LEDs are still frequently used as transmitting elements in remote-control circuits, such as those in remote controls for a wide variety of consumer electronics.\nThe first visible-light LEDs were of low intensity and limited to red. Modern LEDs are available across the visible, ultraviolet, and infrared wavelengths, with very high brightness.\n\nEarly LEDs were often used as indicator lamps for electronic devices, replacing small incandescent bulbs. They were soon packaged into numeric readouts in the form of seven-segment displays and were commonly seen in digital clocks. Recent developments have produced LEDs suitable for environmental and task lighting. LEDs have led to new displays and sensors, while their high switching rates are useful in advanced communications technology.\n\nLEDs have many advantages over incandescent light sources, including lower energy consumption, longer lifetime, improved physical robustness, smaller size, and faster switching. Light-emitting diodes are used in applications as diverse as aviation lighting, automotive headlamps, advertising, general lighting, traffic signals, camera flashes, lighted wallpaper and medical devices. They are also significantly more energy efficient and, arguably, have fewer environmental concerns linked to their disposal.\n\nUnlike a laser, the color of light emitted from an LED is neither coherent nor monochromatic, but the spectrum is narrow with respect to human vision, and for most purposes the light from a simple diode element can be regarded as functionally monochromatic.\n\nElectroluminescence as a phenomenon was discovered in 1907 by the British experimenter H. J. Round of Marconi Labs, using a crystal of silicon carbide and a cat's-whisker detector.\nRussian inventor Oleg Losev reported creation of the first LED in 1927. His research was distributed in Soviet, German and British scientific journals, but no practical use was made of the discovery for several decades.\nIn 1936, Georges Destriau observed that electroluminescence could be produced when zinc sulphide (ZnS) powder is suspended in an insulator and an alternating electrical field is applied to it. In his publications, Destriau often referred to luminescence as Losev-Light. Destriau worked in the laboratories of Madame Marie Curie, also an early pioneer in the field of luminescence with research on radium.\n\nKurt Lehovec, Carl Accardo, and Edward Jamgochian explained these first light-emitting diodes in 1951 using an apparatus employing SiC crystals with a current source of battery or pulse generator and with a comparison to a variant, pure, crystal in 1953.\n\nRubin Braunstein of the Radio Corporation of America reported on infrared emission from gallium arsenide (GaAs) and other semiconductor alloys in 1955. Braunstein observed infrared emission generated by simple diode structures using gallium antimonide (GaSb), GaAs, indium phosphide (InP), and silicon-germanium (SiGe) alloys at room temperature and at 77 Kelvin.\n\nIn 1957, Braunstein further demonstrated that the rudimentary devices could be used for non-radio communication across a short distance. As noted by Kroemer Braunstein \"…had set up a simple optical communications link: Music emerging from a record player was used via suitable electronics to modulate the forward current of a GaAs diode. The emitted light was detected by a PbS diode some distance away. This signal was fed into an audio amplifier and played back by a loudspeaker. Intercepting the beam stopped the music. We had a great deal of fun playing with this setup.\" This setup presaged the use of LEDs for optical communication applications.\n\nIn September 1961, while working at Texas Instruments in Dallas, Texas, James R. Biard and Gary Pittman discovered near-infrared (900 nm) light emission from a tunnel diode they had constructed on a GaAs substrate. By October 1961, they had demonstrated efficient light emission and signal coupling between a GaAs p-n junction light emitter and an electrically-isolated semiconductor photodetector. On August 8, 1962, Biard and Pittman filed a patent titled \"Semiconductor Radiant Diode\" based on their findings, which described a zinc diffused p–n junction LED with a spaced cathode contact to allow for efficient emission of infrared light under forward bias. After establishing the priority of their work based on engineering notebooks predating submissions from G.E. Labs, RCA Research Labs, IBM Research Labs, Bell Labs, and Lincoln Lab at MIT, the U.S. patent office issued the two inventors the patent for the GaAs infrared (IR) light-emitting diode (U.S. Patent US3293513), the first practical LED. Immediately after filing the patent, Texas Instruments (TI) began a project to manufacture infrared diodes. In October 1962, TI announced the first commercial LED product (the SNX-100), which employed a pure GaAs crystal to emit a 890 nm light output. In October 1963, TI announced the first commercial hemispherical LED, the SNX-110.\n\nThe first visible-spectrum (red) LED was developed in 1962 by Nick Holonyak, Jr. while working at General Electric. Holonyak first reported his LED in the journal \"Applied Physics Letters\" on December 1, 1962.\nM. George Craford, a former graduate student of Holonyak, invented the first yellow LED and improved the brightness of red and red-orange LEDs by a factor of ten in 1972. In 1976, T. P. Pearsall created the first high-brightness, high-efficiency LEDs for optical fiber telecommunications by inventing new semiconductor materials specifically adapted to optical fiber transmission wavelengths.\n\nThe first commercial LEDs were commonly used as replacements for incandescent and neon indicator lamps, and in seven-segment displays, first in expensive equipment such as laboratory and electronics test equipment, then later in such appliances as TVs, radios, telephones, calculators, as well as watches (see list of signal uses).\nUntil 1968, visible and infrared LEDs were extremely costly, in the order of US$200 per unit, and so had little practical use.\nThe Monsanto Company was the first organization to mass-produce visible LEDs, using gallium arsenide phosphide (GaAsP) in 1968 to produce red LEDs suitable for indicators. Hewlett-Packard (HP) introduced LEDs in 1968, initially using GaAsP supplied by Monsanto. These red LEDs were bright enough only for use as indicators, as the light output was not enough to illuminate an area. Readouts in calculators were so small that plastic lenses were built over each digit to make them legible. Later, other colors became widely available and appeared in appliances and equipment. In the 1970s commercially successful LED devices at less than five cents each were produced by Fairchild Optoelectronics. These devices employed compound semiconductor chips fabricated with the planar process invented by Dr. Jean Hoerni at Fairchild Semiconductor. The combination of planar processing for chip fabrication and innovative packaging methods enabled the team at Fairchild led by optoelectronics pioneer Thomas Brandt to achieve the needed cost reductions. LED producers continue to use these methods.\n\nMost LEDs were made in the very common 5 mm T1¾ and 3 mm T1 packages, but with rising power output, it has grown increasingly necessary to shed excess heat to maintain reliability, so more complex packages have been adapted for efficient heat dissipation. Packages for state-of-the-art high-power LEDs bear little resemblance to early LEDs.\n\nBlue LEDs were first developed by Herbert Paul Maruska at RCA in 1972 using gallium nitride (GaN) on a sapphire substrate. SiC-types were first commercially sold in the United States by Cree in 1989. However, neither of these initial blue LEDs were very bright.\n\nThe first high-brightness blue LED was demonstrated by Shuji Nakamura of Nichia Corporation in 1994 and was based on InGaN. In parallel, Isamu Akasaki and Hiroshi Amano in Nagoya were working on developing the important GaN nucleation on sapphire substrates and the demonstration of p-type doping of GaN. Nakamura, Akasaki, and Amano were awarded the 2014 Nobel prize in physics for their work. In 1995, Alberto Barbieri at the Cardiff University Laboratory (GB) investigated the efficiency and reliability of high-brightness LEDs and demonstrated a \"transparent contact\" LED using indium tin oxide (ITO) on (AlGaInP/GaAs).\n\nIn 2001 and 2002, processes for growing gallium nitride (GaN) LEDs on silicon were successfully demonstrated. In January 2012, Osram demonstrated high-power InGaN LEDs grown on silicon substrates commercially, and GaN-on-silicon LEDs are in production at Plessey Semiconductors. As of 2017, some manufacturers are using SiC as the substrate for LED production, but sapphire is more common, as it has the most similar properties to that of gallium nitride, reducing the need for patterning the sapphire wafer. (Patterned wafers are known as epi wafers.) Samsung, the University of Cambridge, and Toshiba are performing research into GaN on Si(licon) LEDs. Toshiba has stopped research, possibly due to low yields. Some opt towards epitaxy, which is difficult on silicon, while others, like the University of Cambridge, opt towards a multi layer structure, in order to reduce (crystal) lattice mismatch and different thermal expansion ratios, in order to avoid cracking of the led chip at high temperatures (e.g. during manufacturing), reduce heat generation and increase luminous efficiency. Epitaxy (or patterned sapphire) can be carried out with Nanoimprint lithography.\n\nEven though white light can be created using individual red, green and blue LEDs, perhaps using a single SMD (Surface Mount Device) or through-hole RGB LED, this setup results in poor color rendering or CRI, due to the fact that virtually only 3 wavelengths of light are being emitted, so the attainment of high efficiency in blue LEDs was quickly followed by the development of the first white LED. In this device a :Ce (known as \"YAG\") cerium doped phosphor coating on the emitter absorbs some of the blue emission and produces yellow light through fluorescence. The combination of that yellow with remaining blue light appears white to the eye. However, using different phosphors (fluorescent materials) it also became possible to instead produce green and red light through fluorescence. The resulting mixture of red, green and blue is not only perceived by humans as white light but is superior for illumination in terms of color rendering, whereas one cannot appreciate the color of red or green objects illuminated only by the yellow (and remaining blue) wavelengths from the YAG phosphor.\nThe first white LEDs were expensive and inefficient. However, the light output of LEDs has increased exponentially, with a doubling occurring approximately every 36 months since the 1960s (similar to Moore's law). The latest research and development has been propagated by Japanese manufacturers such as Panasonic, Nichia, etc. and later by Korean and Chinese factories and investment such as: Samsung, Solstice, Kingsun, and countless others. This trend is generally attributed to the parallel development of other semiconductor technologies and advances in optics and materials science and has been called Haitz's law after Dr. Roland Haitz.\n\nLight output and efficiency of blue and near-ultraviolet LEDs rose as the cost of reliable devices fell. This led to relatively high-power white-light LEDs for illumination, which are replacing incandescent and fluorescent lighting.\n\nExperimental white LEDs have been demonstrated to produce 303 lumens per watt of electricity (lm/w); some can last up to 100,000 hours. However, commercially available LEDs have an efficiency of up to 223 lm/w. Compared to incandescent bulbs, this is not only a huge increase in electrical efficiency but – over time – a similar or lower cost per bulb. The LED chip is encapsulated inside a small, plastic, white mold. It can be encapsulated using resin, silicone, or epoxy containing (powdered) Cerium doped YAG phosphor. After allowing the solvents to evaporate, the LEDs are often tested, and placed on tapes for SMT placement equipment for use in LED light bulb production. Encapsulation is performed after probing, dicing, die transfer from wafer to package, and wire bonding or flip chip mounting, perhaps using Indium tin oxide, a transparent electrical conductor. In this case, the bond wire(s) are attached to the ITO film that has been deposited in the LEDs.\n\nThe YAG phosphor in the LED's encapsulation compound is why white LEDs look yellow when off. Some LED light bulbs use a single plastic cover with YAG phosphor (remote phosphor) for several blue LEDs instead of using a diffuser and single chip white LEDs. Remote phosphor LED light bulbs may have behind the plastic cover a white plastic reflector. Others shape the remote phosphor as a dome, or sphere, and place it atop a single PCB containing blue LEDs; this assembly may be behind a frosted glass or plastic cover. The PCB is often installed atop a pillar, which is lined with white plastic.\n\nA P-N junction can convert absorbed light energy into a proportional electric current. The same process is reversed here (i.e. the P-N junction emits light when electrical energy is applied to it). This phenomenon is generally called electroluminescence, which can be defined as the emission of light from a semiconductor under the influence of an electric field. The charge carriers recombine in a forward-biased P-N junction as the electrons cross from the N-region and recombine with the holes existing in the P-region. Free electrons are in the conduction band of energy levels, while holes are in the valence energy band. Thus the energy level of the holes is less than the energy levels of the electrons. Some portion of the energy must be dissipated to recombine the electrons and the holes. This energy is emitted in the form of heat and light.\n\nThe electrons dissipate energy in the form of heat for silicon and germanium diodes but in gallium arsenide phosphide (GaAsP) and gallium phosphide (GaP) semiconductors, the electrons dissipate energy by emitting photons. If the semiconductor is translucent, the junction becomes the source of light as it is emitted, thus becoming a light-emitting diode. However, when the junction is reverse biased, the LED produces no light and—if the potential is great enough, the device is damaged.\n\nThe LED consists of a chip of semiconducting material doped with impurities to create a \"p-n junction\". As in other diodes, current flows easily from the p-side, or anode, to the n-side, or cathode, but not in the reverse direction. Charge-carriers—electrons and holes—flow into the junction from electrodes with different voltages. When an electron meets a hole, it falls into a lower energy level and releases energy in the form of a photon.\n\nThe wavelength of the light emitted, and thus its color, depends on the band gap energy of the materials forming the \"p-n junction\". In silicon or germanium diodes, the electrons and holes usually recombine by a \"non-radiative transition\", which produces no optical emission, because these are indirect band gap materials. The materials used for the LED have a direct band gap with energies corresponding to near-infrared, visible, or near-ultraviolet light.\n\nLED development began with infrared and red devices made with gallium arsenide. Advances in materials science have enabled making devices with ever-shorter wavelengths, emitting light in a variety of colors.\n\nLEDs are usually built on an n-type substrate, with an electrode attached to the p-type layer deposited on its surface. P-type substrates, while less common, occur as well. Many commercial LEDs, especially GaN/InGaN, also use sapphire substrate.\n\nBare uncoated semiconductors such as silicon exhibit a very high refractive index relative to open air, which prevents passage of photons arriving at sharp angles relative to the air-contacting surface of the semiconductor due to total internal reflection. This property affects both the light-emission efficiency of LEDs as well as the light-absorption efficiency of photovoltaic cells. The refractive index of silicon is set at 3.96 (at 590 nm), while air's refractive index is set at 1.0002926.\n\nIn general, a flat-surface uncoated LED semiconductor chip emits light only perpendicular to the semiconductor's surface, and a few degrees to the side, in a cone shape referred to as the \"light cone\", \"cone of light\", or the \"escape cone\". The maximum angle of incidence is referred to as the critical angle. When the critical angle is exceeded, photons no longer escape the semiconductor but are, instead, reflected internally inside the semiconductor crystal as if it were a mirror.\n\nInternal reflections can escape through other crystalline faces if the incidence angle is low enough and the crystal is sufficiently transparent to not re-absorb the photon emission. But for a simple square LED with 90-degree angled surfaces on all sides, the faces all act as equal angle mirrors. In this case, most of the light can not escape and is lost as waste heat in the crystal.\n\nA convoluted chip surface with angled facets similar to a jewel or fresnel lens can increase light output by distributing light perpendicular to the chip surface and far to the sides of the photon emission point.\n\nThe ideal shape of a semiconductor with maximum light output would be a microsphere with the photon emission occurring at the exact center, with electrodes penetrating to the center to contact at the emission point. All light rays emanating from the center would be perpendicular to the entire surface of the sphere, resulting in no internal reflections. A hemispherical semiconductor would also work, with the flat back-surface serving as a mirror to back-scattered photons.\n\nAfter the doping of the wafer, it is usually cut apart into individual dies. Each die is commonly called a chip.\n\nMany LED semiconductor chips are encapsulated or potted in clear or colored molded solid plastic. The plastic encapsulation has three purposes:\nThe third feature helps to boost the light emission from the semiconductor by acting as a diffusing lens, emitting light at a much higher angle of incidence from the light cone than the bare chip would alone.\n\nTypical indicator LEDs are designed to operate with no more than 30–60 milliwatts (mW) of electrical power. Around 1999, Philips Lumileds introduced power LEDs capable of continuous use at one watt. These LEDs used much larger semiconductor die sizes to handle the large power inputs. Also, the semiconductor dies were mounted onto metal slugs to allow for greater heat dissipation from the LED die.\n\nOne of the key advantages of LED-based lighting sources is high luminous efficacy. White LEDs quickly matched and overtook the efficacy of standard incandescent lighting systems. In 2002, Lumileds made five-watt LEDs available with luminous efficacy of 18–22 lumens per watt (lm/W). For comparison, a conventional incandescent light bulb of 60–100 watts emits around 15 lm/W, and standard fluorescent lights emit up to 100 lm/W.\n\n, Philips had achieved the following efficacies for each color. The efficiency values show the physics – light power out per electrical power in. The lumen-per-watt efficacy value includes characteristics of the human eye and is derived using the luminosity function.\n\nIn September 2003, a new type of blue LED was demonstrated by Cree. This produced a commercially packaged white light giving 65 lm/W at 20 mA, becoming the brightest white LED commercially available at the time, and more than four times as efficient as standard incandescents. In 2006, they demonstrated a prototype with a record white LED luminous efficacy of 131 lm/W at 20 mA. Nichia Corporation has developed a white LED with luminous efficacy of 150 lm/W at a forward current of 20 mA. Cree's XLamp XM-L LEDs, commercially available in 2011, produce 100 lm/W at their full power of 10 W, and up to 160 lm/W at around 2 W input power. In 2012, Cree announced a white LED giving 254 lm/W, and 303 lm/W in March 2014.\nPractical general lighting needs high-power LEDs, of one watt or more. Typical operating currents for such devices begin at 350 mA.\n\nThese efficiencies are for the light-emitting diode only, held at low temperature in a lab. Since LEDs installed in real fixtures operate at higher temperature and with driver losses, real-world efficiencies are much lower. United States Department of Energy (DOE) testing of commercial LED lamps designed to replace incandescent lamps or CFLs showed that average efficacy was still about 46 lm/W in 2009 (tested performance ranged from 17 lm/W to 79 lm/W).\n\nEfficiency droop is the decrease in luminous efficiency of LEDs as the electric current increases above tens of milliamperes.\n\nThis effect was initially thought related to elevated temperatures. Scientists proved the opposite is true: though the life of an LED is shortened, the efficiency droop is less severe at elevated temperatures. The mechanism causing efficiency droop was identified in 2007 as Auger recombination, which was taken with mixed reaction. In 2013, a study confirmed Auger recombination as the cause of efficiency droop.\n\nIn addition to being less efficient, operating LEDs at higher electric currents creates higher heat levels, which can compromise LED lifetime. Because of this increased heat at higher currents, high-brightness LEDs have an industry standard of operating at only 350 mA, which is a compromise between light output, efficiency, and longevity.\n\nInstead of increasing current levels, luminance is usually increased by combining multiple LEDs in one bulb. Solving the problem of efficiency droop would mean that household LED light bulbs would need fewer LEDs, which would significantly reduce costs.\n\nResearchers at the U.S. Naval Research Laboratory have found a way to lessen the efficiency droop. They found that the droop arises from non-radiative Auger recombination of the injected carriers. They created quantum wells with a soft confinement potential to lessen the non-radiative Auger processes.\n\nResearchers at Taiwan National Central University and Epistar Corp are developing a way to lessen the efficiency droop by using ceramic aluminium nitride (AlN) substrates, which are more thermally conductive than the commercially used sapphire. The higher thermal conductivity reduces self-heating effects.\n\nSolid-state devices such as LEDs are subject to very limited wear and tear if operated at low currents and at low temperatures. Typical lifetimes quoted are 25,000 to 100,000 hours, but heat and current settings can extend or shorten this time significantly. One way to cool down the LED and use the energy efficiently, is to blink the LED in order to gain biomass for example in microalgae cultivation. It is important to note that these projections are based on a standard test that may not accelerate all the potential mechanisms that can induce failures in LEDs.\n\nThe most common symptom of LED (and diode laser) failure is the gradual lowering of light output and loss of efficiency. Sudden failures, although rare, can also occur. Early red LEDs were notable for their short service life. With the development of high-power LEDs, the devices are subjected to higher junction temperatures and higher current densities than traditional devices. This causes stress on the material and may cause early light-output degradation. To quantify useful lifetime in a standardized manner, some suggest using L70 or L50, which are runtimes (typically in thousands of hours) at which a given LED reaches 70% and 50% of initial light output, respectively.\n\nWhereas in most previous sources of light (incandescent lamps, discharge lamps, and those that burn combustible fuel, e.g. candles and oil lamps) the light results from heat, LEDs only operate if they are kept cool enough. The manufacturer commonly specifies a maximum junction temperature of 125 or 150 °C, and lower temperatures are advisable in the interests of long life. At these temperatures, relatively little heat is lost by radiation, which means that the light beam generated by an LED is cool.\n\nThe waste heat in a high-power LED (which as of 2015 can be less than half the power that it consumes) is conveyed by conduction through the substrate and package of the LED to a heat sink, which gives up the heat to the ambient air by convection. Careful thermal design is, therefore, essential, taking into account the thermal resistances of the LED’s package, the heat sink and the interface between the two. Medium-power LEDs are often designed to solder directly to a printed circuit board that contains a thermally conductive metal layer. (Often, it is a PCB with a white solder mask (white pcb) and a ~1mm thick aluminum backing.) High-power LEDs are packaged in large-area ceramic packages that attach to a metal heat sink—the interface being a material with high thermal conductivity (thermal grease, phase-change material, thermally conductive pad, or thermal adhesive).\n\nIf an LED-based lamp is installed in an unventilated luminaire, or a luminaire is located in an environment that does not have free air circulation, the LED is likely to overheat, resulting in reduced life or early catastrophic failure. Thermal design is often based on an ambient temperature of . LEDs used in outdoor applications, such as traffic signals or in-pavement signal lights, and in climates where the temperature within the light fixture becomes very high, could experience reduced output or even failure.\n\nSince LED efficacy is higher at low temperatures, LED technology is well suited for supermarket freezer lighting. Because LEDs produce less waste heat than incandescent lamps, freezer tube lighting use can save on refrigeration costs as well. However, they may be more susceptible to frost and snow buildup than incandescent lamps, so some LED lighting systems have been designed with an added heating circuit. Additionally, research has developed heat sink technologies that transfer heat produced within the junction to appropriate areas of the light fixture.\n\nConventional LEDs are made from a variety of inorganic semiconductor materials. The following table shows the available colors with wavelength range, voltage drop, and material:\n\nThe first blue-violet LED using magnesium-doped gallium nitride was made at Stanford University in 1972 by Herb Maruska and Wally Rhines, doctoral students in materials science and engineering. At the time Maruska was on leave from RCA Laboratories, where he collaborated with Jacques Pankove on related work. In 1971, the year after Maruska left for Stanford, his RCA colleagues Pankove and Ed Miller demonstrated the first blue electroluminescence from zinc-doped gallium nitride, though the subsequent device Pankove and Miller built, the first actual gallium nitride light-emitting diode, emitted green light. In 1974 the U.S. Patent Office awarded Maruska, Rhines and Stanford professor David Stevenson a patent for their work in 1972 (U.S. Patent US3819974 A) and today, magnesium-doping of gallium nitride remains the basis for all commercial blue LEDs and laser diodes. In the early 1970s, these devices were too dim for practical use, and research into gallium nitride devices slowed. In August 1989, Cree introduced the first commercially available blue LED based on the indirect bandgap semiconductor, silicon carbide (SiC). SiC LEDs had very low efficiency, no more than about 0.03%, but did emit in the blue portion of the visible light spectrum.\n\nIn the late 1980s, key breakthroughs in GaN epitaxial growth and p-type doping ushered in the modern era of GaN-based optoelectronic devices. Building upon this foundation, Theodore Moustakas at Boston University patented a method for producing high-brightness blue LEDs using a new two-step process. Two years later, in 1993, high-brightness blue LEDs were demonstrated again by Shuji Nakamura of Nichia Corporation using a gallium nitride growth process similar to Moustakas's. Both Moustakas and Nakamura were issued separate patents, which confused the issue of who was the original inventor (partly because although Moustakas invented his first, Nakamura filed first). This new development revolutionized LED lighting, making high-power blue light sources practical, leading to the development of technologies like Blu-ray, as well as allowing the bright high-resolution screens of modern tablets and phones.\n\nNakamura was awarded the 2006 Millennium Technology Prize for his invention.\nNakamura, Hiroshi Amano and Isamu Akasaki were awarded the Nobel Prize in Physics in 2014 for the invention of the blue LED. In 2015, a US court ruled that three companies (i.e. the litigants who had not previously settled out of court) that had licensed Nakamura's patents for production in the United States had infringed Moustakas's prior patent, and ordered them to pay licensing fees of not less than 13 million USD.\n\nBy the late 1990s, blue LEDs became widely available. They have an active region consisting of one or more InGaN quantum wells sandwiched between thicker layers of GaN, called cladding layers. By varying the relative In/Ga fraction in the InGaN quantum wells, the light emission can in theory be varied from violet to amber. Aluminium gallium nitride (AlGaN) of varying Al/Ga fraction can be used to manufacture the cladding and quantum well layers for ultraviolet LEDs, but these devices have not yet reached the level of efficiency and technological maturity of InGaN/GaN blue/green devices. If un-alloyed GaN is used in this case to form the active quantum well layers, the device emits near-ultraviolet light with a peak wavelength centred around 365 nm. Green LEDs manufactured from the InGaN/GaN system are far more efficient and brighter than green LEDs produced with non-nitride material systems, but practical devices still exhibit efficiency too low for high-brightness applications.\n\nWith nitrides containing aluminium, most often AlGaN and AlGaInN, even shorter wavelengths are achievable. Ultraviolet LEDs in a range of wavelengths are becoming available on the market. Near-UV emitters at wavelengths around 375–395 nm are already cheap and often encountered, for example, as black light lamp replacements for inspection of anti-counterfeiting UV watermarks in some documents and paper currencies. Shorter-wavelength diodes, while substantially more expensive, are commercially available for wavelengths down to 240 nm. As the photosensitivity of microorganisms approximately matches the absorption spectrum of DNA, with a peak at about 260 nm, UV LED emitting at 250–270 nm are expected in prospective disinfection and sterilization devices. Recent research has shown that commercially available UVA LEDs (365 nm) are already effective disinfection and sterilization devices.\nUV-C wavelengths were obtained in laboratories using aluminium nitride (210 nm), boron nitride (215 nm) and diamond (235 nm).\n\nRGB LEDs consist of one red, one green, and one blue LED. By independently adjusting each of the three, RGB LEDs are capable of producing a wide color gamut. Unlike dedicated-color LEDs, however, these obviously do not produce pure wavelengths. Moreover, such modules as commercially available are often not optimized for smooth color mixing.\n\nThere are two primary ways of producing white light-emitting diodes (WLEDs), LEDs that generate high-intensity white light. One is to use individual LEDs that emit three primary colors—red, green, and blue—and then mix all the colors to form white light. The other is to use a phosphor material to convert monochromatic light from a blue or UV LED to broad-spectrum white light, much in the same way a fluorescent light bulb works. The 'whiteness' of the light produced is essentially engineered to suit the human eye. The yellow phosphor in LEDs is cerium-doped YAG crystal, in powder form and may be suspended in plastic, epoxy, synthetic rubber or silicone. Touching \nbare LEDs using synthetic rubber or silicone can damage the delicate wire bonds from LED to package or the LED itself, so they may be protected by a plastic or glass cover, such as in the LED flash in smartphones. It is this YAG phosphor which causes white LEDs without diffusers (such as those used in LED lamps and bulbs) to look yellow when off.\n\nThere are three main methods of mixing colors to produce white light from an LED:\n\nBecause of metamerism, it is possible to have quite different spectra that appear white. However, the appearance of objects illuminated by that light may vary as the spectrum varies; this is the issue of color rendition, quite separate from color temperature, where a really orange or cyan object could appear with the wrong color and much darker as the LED or phosphor does not emit the wavelength it reflects. The best color rendition CFL and LEDs use a mix of phosphors, resulting in less efficiency but better quality of light. Though incandescent halogen lamps have a more orange color temperature, they are still the best easily available artificial light sources in terms of color rendition.\n\nWhite light can be formed by mixing differently colored lights; the most common method is to use red, green, and blue (RGB). Hence the method is called multicolor white LEDs (sometimes referred to as RGB LEDs). Because these need electronic circuits to control the blending and diffusion of different colors, and because the individual color LEDs typically have slightly different emission patterns (leading to variation of the color depending on direction) even if they are made as a single unit, these are seldom used to produce white lighting. Nonetheless, this method has many applications because of the flexibility of mixing different colors, and in principle, this mechanism also has higher quantum efficiency in producing white light.\n\nThere are several types of multicolor white LEDs: di-, tri-, and tetrachromatic white LEDs. Several key factors that play among these different methods include color stability, color rendering capability, and luminous efficacy. Often, higher efficiency means lower color rendering, presenting a trade-off between the luminous efficacy and color rendering. For example, the dichromatic white LEDs have the best luminous efficacy (120 lm/W), but the lowest color rendering capability. However, although tetrachromatic white LEDs have excellent color rendering capability, they often have poor luminous efficacy. Trichromatic white LEDs are in between, having both good luminous efficacy (>70 lm/W) and fair color rendering capability.\n\nOne of the challenges is the development of more efficient green LEDs. The theoretical maximum for green LEDs is 683 lumens per watt but as of 2010 few green LEDs exceed even 100 lumens per watt. The blue and red LEDs approach their theoretical limits.\n\nMulticolor LEDs offer not merely another means to form white light but a new means to form light of different colors. Most perceivable colors can be formed by mixing different amounts of three primary colors. This allows precise dynamic color control. As more effort is devoted to investigating this method, multicolor LEDs should have profound influence on the fundamental method that we use to produce and control light color. However, before this type of LED can play a role on the market, several technical problems must be solved. These include that this type of LED's emission power decays exponentially with rising temperature,\nresulting in a substantial change in color stability. Such problems inhibit and may preclude industrial use. Thus, many new package designs aimed at solving this problem have been proposed and their results are now being reproduced by researchers and scientists. However multicolor LEDs without phosphors can never provide good quality lighting because each LED is a narrow band source (see graph). LEDs without phosphor while a poorer solution for general lighting are the best solution for displays, either backlight of LCD, or direct LED based pixels.\n\nCorrelated color temperature (CCT) dimming for LED technology is regarded as a difficult task since LED binning, age and temperature drift effects of LEDs change the actual color value output. Feedback loop systems are used for example with color sensors, to actively monitor and control the color output of multiple color mixing LEDs.\n\nThis method involves coating LEDs of one color (mostly blue LEDs made of InGaN) with phosphors of different colors to form white light; the resultant LEDs are called phosphor-based or phosphor-converted white LEDs (pcLEDs). A fraction of the blue light undergoes the Stokes shift, which transforms it from shorter wavelengths to longer. Depending on the original LED's color, various color phosphors are used. Using several phosphor layers of distinct colors broadens the emitted spectrum, effectively raising the color rendering index (CRI).\n\nPhosphor-based LEDs have efficiency losses due to heat loss from the Stokes shift and also other phosphor-related issues. Their luminous efficacies compared to normal LEDs depend on the spectral distribution of the resultant light output and the original wavelength of the LED itself. For example, the luminous efficacy of a typical YAG yellow phosphor based white LED ranges from 3 to 5 times the luminous efficacy of the original blue LED because of the human eye's greater sensitivity to yellow than to blue (as modeled in the luminosity function). Due to the simplicity of manufacturing, the phosphor method is still the most popular method for making high-intensity white LEDs. The design and production of a light source or light fixture using a monochrome emitter with phosphor conversion is simpler and cheaper than a complex RGB system, and the majority of high-intensity white LEDs presently on the market are manufactured using phosphor light conversion.\n\nAmong the challenges being faced to improve the efficiency of LED-based white light sources is the development of more efficient phosphors. As of 2010, the most efficient yellow phosphor is still the YAG phosphor, with less than 10% Stokes shift loss. Losses attributable to internal optical losses due to re-absorption in the LED chip and in the LED packaging itself account typically for another 10% to 30% of efficiency loss. Currently, in the area of phosphor LED development, much effort is being spent on optimizing these devices to higher light output and higher operation temperatures. For instance, the efficiency can be raised by adapting better package design or by using a more suitable type of phosphor. Conformal coating process is frequently used to address the issue of varying phosphor thickness.\n\nSome phosphor-based white LEDs encapsulate InGaN blue LEDs inside phosphor-coated epoxy. Alternatively, the LED might be paired with a remote phosphor, a preformed polycarbonate piece coated with the phosphor material. Remote phosphors provide more diffuse light, which is desirable for many applications. Remote phosphor designs are also more tolerant of variations in the LED emissions spectrum. A common yellow phosphor material is cerium-doped yttrium aluminium garnet (Ce:YAG).\n\nWhite LEDs can also be made by coating near-ultraviolet (NUV) LEDs with a mixture of high-efficiency europium-based phosphors that emit red and blue, plus copper and aluminium-doped zinc sulfide (ZnS:Cu, Al) that emits green. This is a method analogous to the way fluorescent lamps work. This method is less efficient than blue LEDs with YAG:Ce phosphor, as the Stokes shift is larger, so more energy is converted to heat, but yields light with better spectral characteristics, which render color better. Due to the higher radiative output of the ultraviolet LEDs than of the blue ones, both methods offer comparable brightness. A concern is that UV light may leak from a malfunctioning light source and cause harm to human eyes or skin.\n\nAnother method used to produce experimental white light LEDs used no phosphors at all and was based on homoepitaxially grown zinc selenide (ZnSe) on a ZnSe substrate that simultaneously emitted blue light from its active region and yellow light from the substrate.\n\nA new style of wafers composed of gallium-nitride-on-silicon (GaN-on-Si) is being used to produce white LEDs using 200-mm silicon wafers. This avoids the typical costly sapphire substrate in relatively small 100- or 150-mm wafer sizes. The sapphire apparatus must be coupled with a mirror-like collector to reflect light that would otherwise be wasted. It is predicted that by 2020, 40% of all GaN LEDs will be made with GaN-on-Si. Manufacturing large sapphire material is difficult, while large silicon material is cheaper and more abundant. LED companies shifting from using sapphire to silicon should be a minimal investment.\n\nIn an organic light-emitting diode (OLED), the electroluminescent material composing the emissive layer of the diode is an organic compound. The organic material is electrically conductive due to the delocalization of pi electrons caused by conjugation over all or part of the molecule, and the material therefore functions as an organic semiconductor. The organic materials can be small organic molecules in a crystalline phase, or polymers.\n\nThe potential advantages of OLEDs include thin, low-cost displays with a low driving voltage, wide viewing angle, and high contrast and color gamut. Polymer LEDs have the added benefit of printable and flexible displays. OLEDs have been used to make visual displays for portable electronic devices such as cellphones, digital cameras, and MP3 players while possible future uses include lighting and televisions.\n\nQuantum dots (QD) are semiconductor nanocrystals with optical properties that let their emission color be tuned from the visible into the infrared spectrum. This allows quantum dot LEDs to create almost any color on the CIE diagram. This provides more color options and better color rendering than white LEDs since the emission spectrum is much narrower, characteristic of quantum confined states.\n\nThere are two types of schemes for QD excitation. One uses photo excitation with a primary light source LED (typically blue or UV LEDs are used). The other is direct electrical excitation first demonstrated by Alivisatos et al.\n\nOne example of the photo-excitation scheme is a method developed by Michael Bowers, at Vanderbilt University in Nashville, involving coating a blue LED with quantum dots that glow white in response to the blue light from the LED. This method emits a warm, yellowish-white light similar to that made by incandescent light bulbs. Quantum dots are also being considered for use in white light-emitting diodes in liquid crystal display (LCD) televisions.\n\nIn February 2011 scientists at PlasmaChem GmbH were able to synthesize quantum dots for LED applications and build a light converter on their basis, which was able to efficiently convert light from blue to any other color for many hundred hours. Such QDs can be used to emit visible or near infrared light of any wavelength being excited by light with a shorter wavelength.\n\nThe structure of QD-LEDs used for the electrical-excitation scheme is similar to basic design of OLEDs. A layer of quantum dots is sandwiched between layers of electron-transporting and hole-transporting materials. An applied electric field causes electrons and holes to move into the quantum dot layer and recombine forming an exciton that excites a QD. This scheme is commonly studied for quantum dot display. The tunability of emission wavelengths and narrow bandwidth is also beneficial as excitation sources for fluorescence imaging. Fluorescence near-field scanning optical microscopy\n(NSOM) utilizing an integrated QD-LED has been demonstrated.\n\nIn February 2008, a luminous efficacy of 300 lumens of visible light per watt of radiation (not per electrical watt) and warm-light emission was achieved by using nanocrystals.\n\nThe main types of LEDs are miniature, high-power devices, and custom designs such as alphanumeric or multicolor.\n\nThese are mostly single-die LEDs used as indicators, and they come in various sizes from 2 mm to 8 mm, through-hole and surface mount packages. They usually do not use a separate heat sink. Typical current ratings range from around 1 mA to above 20 mA. The small size sets a natural upper boundary on power consumption due to heat caused by the high current density and need for a heat sink. Often daisy chained as used in LED tapes.\n\nCommon package shapes include round, with a domed or flat top, rectangular with a flat top (as used in bar-graph displays), and triangular or square with a flat top.\nThe encapsulation may also be clear or tinted to improve contrast and viewing angle.\n\nResearchers at the University of Washington have invented the thinnest LED. It is made of two-dimensional (2-D) flexible materials. It is three atoms thick, which is 10 to 20 times thinner than three-dimensional (3-D) LEDs and is also 10,000 times smaller than the thickness of a human hair. These 2-D LEDs are going to make it possible to create smaller, more energy-efficient lighting, optical communication and nano lasers.\n\nThere are three main categories of miniature single die LEDs:\n\n5 V and 12 V LEDs are ordinary miniature LEDs that incorporate a suitable series resistor for direct connection to a 5 V or 12 V supply.\n\nHigh-power LEDs (HP-LEDs) or high-output LEDs (HO-LEDs) can be driven at currents from hundreds of mA to more than an ampere, compared with the tens of mA for other LEDs. Some can emit over a thousand lumens. LED power densities up to 300 W/cm have been achieved. Since overheating is destructive, the HP-LEDs must be mounted on a heat sink to allow for heat dissipation. If the heat from an HP-LED is not removed, the device fails in seconds. One HP-LED can often replace an incandescent bulb in a flashlight, or be set in an array to form a powerful LED lamp.\n\nSome well-known HP-LEDs in this category are the Nichia 19 series, Lumileds Rebel Led, Osram Opto Semiconductors Golden Dragon, and Cree X-lamp. As of September 2009, some HP-LEDs manufactured by Cree now exceed 105 lm/W.\n\nExamples for Haitz's law—which predicts an exponential rise in light output and efficacy of LEDs over time—are the CREE XP-G series LED, which achieved 105 lm/W in 2009 and the Nichia 19 series with a typical efficacy of 140 lm/W, released in 2010.\n\nLEDs developed by Seoul Semiconductor can operate on AC power without a DC converter. For each half-cycle, part of the LED emits light and part is dark, and this is reversed during the next half-cycle. The efficacy of this type of HP-LED is typically 40 lm/W. A large number of LED elements in series may be able to operate directly from line voltage. In 2009, Seoul Semiconductor released a high DC voltage LED, named as 'Acrich MJT', capable of being driven from AC power with a simple controlling circuit. The low-power dissipation of these LEDs affords them more flexibility than the original AC LED design.\n\nFlashing LEDs are used as attention seeking indicators without requiring external electronics. Flashing LEDs resemble standard LEDs but they contain an integrated multivibrator circuit that causes the LED to flash with a typical period of one second. In diffused lens LEDs, this circuit is visible as a small black dot. Most flashing LEDs emit light of one color, but more sophisticated devices can flash between multiple colors and even fade through a color sequence using RGB color mixing.\n\nBi-color LEDs contain two different LED emitters in one case. There are two types of these. One type consists of two dies connected to the same two leads antiparallel to each other. Current flow in one direction emits one color, and current in the opposite direction emits the other color. The other type consists of two dies with separate leads for both dies and another lead for common anode or cathode so that they can be controlled independently. The most common bi-color combination is red/traditional green, however, other available combinations include amber/traditional green, red/pure green, red/blue, and blue/pure green.\n\nTri-color LEDs contain three different LED emitters in one case. Each emitter is connected to a separate lead so they can be controlled independently. A four-lead arrangement is typical with one common lead (anode or cathode) and an additional lead for each color.\n\nRGB LEDs are tri-color LEDs with red, green, and blue emitters, in general using a four-wire connection with one common lead (anode or cathode). Others, however, have only two leads (positive and negative) and have a built-in electronic control unit.\n\nDecorative-multicolor LEDs incorporate several emitters of different colors supplied by only two lead-out wires. Colors are switched internally by varying the supply voltage.\n\nAlphanumeric LEDs are available in seven-segment, starburst, and dot-matrix format. Seven-segment displays handle all numbers and a limited set of letters. Starburst displays can display all letters. Dot-matrix displays typically use 5x7 pixels per character. Seven-segment LED displays were in widespread use in the 1970s and 1980s, but rising use of liquid crystal displays, with their lower power needs and greater display flexibility, has reduced the popularity of numeric and alphanumeric LED displays.\n\nDigital RGB Addressable LEDs are RGB LEDs that contain their own \"smart\" control electronics. In addition to power and ground, these provide connections for data-in, data-out, and sometimes a clock or strobe signal. These are connected in a daisy chain, with the data in of the first LED sourced by a microprocessor, which can control the brightness and color of each LED independently of the others. They are used where a combination of maximum control and minimum visible electronics are needed such as strings for Christmas and LED matrices. Some even have refresh rates in the kHz range, allowing for basic video applications. These devices are also known by their part number (WS2812 being the most common) or a brand name such as NeoPixel\n\nAn LED filament consists of multiple LED chips connected in series on a common longitudinal substrate that forms a thin rod reminiscent of a traditional incandescent filament. These are being used as a low-cost decorative alternative for traditional light bulbs that are being phased out in many countries. The filaments require a rather high voltage to light to nominal brightness, allowing them to work efficiently and simply with mains voltages. Often a simple rectifier and capacitive current limiting are employed to create a low-cost replacement for a traditional light bulb without the complexity of the low voltage, high current converter that single die LEDs need. Usually, they are packaged in a sealed enclosure with a shape similar to lamps they were designed to replace (e.g. a bulb) and filled with inert nitrogen or carbon dioxide gas to remove heat efficiently.\n\nThe current–voltage characteristic of an LED is similar to other diodes, in that the current is dependent exponentially on the voltage (see Shockley diode equation). This means that a small change in voltage can cause a large change in current. If the applied voltage exceeds the LED's forward voltage drop by a small amount, the current rating may be exceeded by a large amount, potentially damaging or destroying the LED. The typical solution is to use constant-current power supplies to keep the current below the LED's maximum current rating. Since most common power sources (batteries, mains) are constant-voltage sources, most LED fixtures must include a power converter, at least a current-limiting resistor.\nHowever, the high resistance of three-volt coin cells combined with the high differential resistance of nitride-based LEDs makes it possible to power such an LED from such a coin cell without an external resistor.\n\nAs with all diodes, current flows easily from p-type to n-type material.\nHowever, no current flows and no light is emitted if a small voltage is applied in the reverse direction. If the reverse voltage grows large enough to exceed the breakdown voltage, a large current flows and the LED may be damaged. If the reverse current is sufficiently limited to avoid damage, the reverse-conducting LED is a useful noise diode.\n\nThe vast majority of devices containing LEDs are \"safe under all conditions of normal use\", and so are classified as \"Class 1 LED product\"/\"LED Klasse 1\". At present, only a few LEDs—extremely bright LEDs that also have a tightly focused viewing angle of 8° or less—could, in theory, cause temporary blindness, and so are classified as \"Class 2\".\nThe opinion of the French Agency for Food, Environmental and Occupational Health & Safety (ANSES) of 2010, on the health issues concerning LEDs, suggested banning public use of lamps in the moderate Risk Group 2, especially those with a high blue component, in places frequented by children.\n\nIn general, laser safety regulations—and the \"Class 1\", \"Class 2\", etc. system—also apply to LEDs.\n\nWhile LEDs have the advantage over fluorescent lamps that they do not contain mercury, they may contain other hazardous metals such as lead and arsenic. Regarding the toxicity of LEDs when treated as waste, a study published in 2011 stated: \"According to federal standards, LEDs are not hazardous except for low-intensity red LEDs, which leached Pb [lead] at levels exceeding regulatory limits (186 mg/L; regulatory limit: 5). However, according to California regulations, excessive levels of copper (up to 3892 mg/kg; limit: 2500), lead (up to 8103 mg/kg; limit: 1000), nickel (up to 4797 mg/kg; limit: 2000), or silver (up to 721 mg/kg; limit: 500) render all except low-intensity yellow LEDs hazardous.\"\n\nIn 2016 a statement of the American Medical Association (AMA) concerning the possible influence of blueish street lighting on the sleep-wake cycle of city-dwellers led to some controversy.\nSo far high-pressure sodium lamps (HPS) with an orange light spectrum were the most efficient light sources commonly used in street-lighting.\nNow many modern street lamps are equipped with Indium gallium nitride LEDs (InGaN). These are even more efficient and mostly emit blue-rich light with a higher \"correlated color temperature (CCT)\". Since light with a high CCT resembles daylight it is thought that this might have an effect on the normal circadian physiology by suppressing melatonin production in the human body. There have been no relevant studies as yet and critics claim exposure levels are not high enough to have a noticeable effect.\n\n\n\nLED uses fall into four major categories:\n\n\nThe low energy consumption, low maintenance and small size of LEDs has led to uses as status indicators and displays on a variety of equipment and installations. Large-area LED displays are used as stadium displays, dynamic decorative displays, and dynamic message signs on freeways. Thin, lightweight message displays are used at airports and railway stations, and as destination displays for trains, buses, trams, and ferries.\n\nOne-color light is well suited for traffic lights and signals, exit signs, emergency vehicle lighting, ships' navigation lights or lanterns (chromacity and luminance standards being set under the Convention on the International Regulations for Preventing Collisions at Sea 1972, Annex I and the CIE) and LED-based Christmas lights. In cold climates, LED traffic lights may remain snow-covered. Red or yellow LEDs are used in indicator and alphanumeric displays in environments where night vision must be retained: aircraft cockpits, submarine and ship bridges, astronomy observatories, and in the field, e.g. night time animal watching and military field use.\n\nBecause of their long life, fast switching times, and visibility in broad daylight due to their high output and focus, LEDs have been used in brake lights for cars' high-mounted brake lights, trucks, and buses, and in turn signals for some time. However, many vehicles now use LEDs for their rear light clusters. The use in brakes improves safety, due to a great reduction in the time needed to light fully, or faster rise time, up to 0.5 second faster than an incandescent bulb. This gives drivers behind more time to react. In a dual intensity circuit (rear markers and brakes) if the LEDs are not pulsed at a fast enough frequency, they can create a phantom array, where ghost images of the LED appear if the eyes quickly scan across the array. White LED headlamps are beginning to appear. Using LEDs has styling advantages because LEDs can form much thinner lights than incandescent lamps with parabolic reflectors.\n\nDue to the relative cheapness of low output LEDs, they are also used in many temporary uses such as glowsticks, throwies, and the photonic textile Lumalive. Artists have also used LEDs for LED art.\n\nWeather and all-hazards radio receivers with Specific Area Message Encoding (SAME) have three LEDs: red for warnings, orange for watches, and yellow for advisories and statements whenever issued.\n\nWith the development of high-efficiency and high-power LEDs, it has become possible to use LEDs in lighting and illumination. To encourage the shift to LED lamps and other high-efficiency lighting, the US Department of Energy has created the L Prize competition. The Philips Lighting North America LED bulb won the first competition on August 3, 2011, after successfully completing 18 months of intensive field, lab, and product testing.\n\nLEDs are used as street lights and in other architectural lighting. The mechanical robustness and long lifetime are used in automotive lighting on cars, motorcycles, and bicycle lights. LED light emission may be efficiently controlled by using nonimaging optics principles.\n\nLED street lights are employed on poles and in parking garages. In 2007, the Italian village of Torraca was the first place to convert its entire illumination system to LEDs.\n\nLEDs are used in aviation lighting. Airbus has used LED lighting in its Airbus A320 Enhanced since 2007, and Boeing uses LED lighting in the 787 and 737 Sky Interior. LEDs are also being used now in airport and heliport lighting. LED airport fixtures currently include medium-intensity runway lights, runway centerline lights, taxiway centerline and edge lights, guidance signs, and obstruction lighting.\n\nLEDs are also used as a light source for DLP projectors, and to backlight LCD televisions (referred to as LED TVs) and laptop displays. RGB LEDs raise the color gamut by as much as 45%. Screens for TV and computer displays can be made thinner using LEDs for backlighting.\n\nThe lack of IR or heat radiation makes LEDs ideal for stage lights using banks of RGB LEDs that can easily change color and decrease heating from traditional stage lighting, as well as medical lighting where IR-radiation can be harmful. In energy conservation, the lower heat output of LEDs also means air conditioning (cooling) systems have less heat in need of disposal.\n\nLEDs are small, durable and need little power, so they are used in handheld devices such as flashlights. LED strobe lights or camera flashes operate at a safe, low voltage, instead of the 250+ volts commonly found in xenon flashlamp-based lighting. This is especially useful in cameras on mobile phones, where space is at a premium and bulky voltage-raising circuitry is undesirable.\n\nLEDs are used for infrared illumination in night vision uses including security cameras. A ring of LEDs around a video camera, aimed forward into a retroreflective background, allows chroma keying in video productions.\n\nLEDs are used in mining operations, as cap lamps to provide light for miners. Research has been done to improve LEDs for mining, to reduce glare and to increase illumination, reducing risk of injury to the miners.\n\nLEDs are now used commonly in all market areas from commercial to home use: standard lighting, AV, stage, theatrical, architectural, and public installations, and wherever artificial light is used.\n\nLEDs are increasingly finding uses in medical and educational applications, for example as mood enhancement, and new technologies such as AmBX, exploiting LED versatility. NASA has even sponsored research for the use of LEDs to promote health for astronauts.\n\nLight can be used to transmit data and analog signals. For example, lighting white LEDs can be used in systems assisting people to navigate in closed spaces while searching necessary rooms or objects.\n\nAssistive listening devices in many theaters and similar spaces use arrays of infrared LEDs to send sound to listeners' receivers. Light-emitting diodes (as well as semiconductor lasers) are used to send data over many types of fiber optic cable, from digital audio over TOSLINK cables to the very high bandwidth fiber links that form the Internet backbone. For some time, computers were commonly equipped with IrDA interfaces, which allowed them to send and receive data to nearby machines via infrared.\n\nBecause LEDs can cycle on and off millions of times per second, very high data bandwidth can be achieved.\n\nEfficient lighting is needed for sustainable architecture. In 2009, US Department of Energy testing results on LED lamps showed an average efficacy of 35 lm/W, below that of typical CFLs, and as low as 9 lm/W, worse than standard incandescent bulbs. A typical 13-watt LED lamp emitted 450 to 650 lumens, which is equivalent to a standard 40-watt incandescent bulb.\n\nHowever, as of 2011, there are LED bulbs available as efficient as 150 lm/W and even inexpensive low-end models typically exceed 50 lm/W, so that a 6-watt LED could achieve the same results as a standard 40-watt incandescent bulb. The latter has an expected lifespan of 1,000 hours, whereas an LED can continue to operate with reduced efficiency for more than 50,000 hours.\n\nSee the chart below for a comparison of common light types:\nIn the US, one kilowatt-hour (3.6 MJ) of electricity currently causes an average of emission. Assuming the average light bulb is on for 10 hours a day, a 40-watt bulb causes of emission per year. The 6-watt LED equivalent only causes of over the same time span. A building’s carbon footprint from lighting can, therefore, be reduced by 85% by exchanging all incandescent bulbs for new LEDs—if a building previously used only incandescent bulbs.\n\nIn practice, most buildings that use a lot of lighting use fluorescent lighting, which has 22% luminous efficiency compared with 5% for filaments, so changing to LED lighting would still give a 34% reduction in electrical power use and carbon emissions.\n\nThe reduction in carbon emissions depends on the source of electricity. Nuclear power in the United States produced 19.2% of electricity in 2011, so reducing electricity consumption in the U.S. reduces carbon emissions more than in France (75% nuclear electricity) or Norway (almost entirely hydroelectric).\n\nReplacing lights that spend the most time lit results in the most savings, so LED lights in infrequently used locations bring a smaller return on investment.\n\nMachine vision systems, also known as Computer vision systems, often require bright and homogeneous illumination, so features of interest are easier to process. LEDs are often used to achieve this type of purpose, and this is likely to remain one of their major uses until their price drops low enough to make signaling and illumination uses more economically feasible.\n\nBarcode scanners are the most common example of machine vision applications, and many of those scanners use low-cost products red LEDs instead of laser-based LEDs. Optical computer mice are an example of LEDs in machine vision, as it is used to provide an even light source on the surface for the miniature camera within the mouse. LEDs constitute a nearly ideal light source for machine vision systems for several reasons:\n\nThe light from LEDs can be modulated very quickly so they are used extensively in optical fiber and free space optics communications. This includes remote controls, such as for TVs, VCRs, and LED Computers, where infrared LEDs are often used. Opto-isolators use an LED combined with a photodiode or phototransistor to provide a signal path with electrical isolation between two circuits. This is especially useful in medical equipment where the signals from a low-voltage sensor circuit (usually battery-powered) in contact with a living organism must be electrically isolated from any possible electrical failure in a recording or monitoring device operating at potentially dangerous voltages. An optoisolator also lets information be transferred between circuits that don't share a common ground potential.\n\nMany sensor systems rely on light as the signal source. LEDs are often ideal as a light source due to the requirements of the sensors. LEDs are used as motion sensors, for example in optical computer mice. The Nintendo Wii's sensor bar uses infrared LEDs. Pulse oximeters use them for measuring oxygen saturation. Some flatbed scanners use arrays of RGB LEDs rather than the typical cold-cathode fluorescent lamp as the light source. Having independent control of three illuminated colors allows the scanner to calibrate itself for more accurate color balance, and there is no need for warm-up. Further, its sensors only need be monochromatic, since at any one time the page being scanned is only lit by one color of light. Since LEDs can also be used as photodiodes, they can be used for both photo emission and detection. This could be used, for example, in a touchscreen that registers reflected light from a finger or stylus. Many materials and biological systems are sensitive to, or dependent on, light. Grow lights use LEDs to increase photosynthesis in plants, and bacteria and viruses can be removed from water and other substances using UV LEDs for sterilization.\n\nLEDs have also been used as a medium-quality voltage reference in electronic circuits. The forward voltage drop (e.g. about 1.7 V for a normal red LED) can be used instead of a Zener diode in low-voltage regulators. Red LEDs have the flattest I/V curve above the knee. Nitride-based LEDs have a fairly steep I/V curve and are useless for this purpose. Although LED forward voltage is far more current-dependent than a Zener diode, Zener diodes with breakdown voltages below 3 V are not widely available.\n\nThe progressive miniaturization of low-voltage lighting technology, such as LEDs and OLEDs, suitable to incorporate into low-thickness materials has fostered experimentation in combining light sources and wall covering surfaces for interior walls. The new possibilities offered by these developments have prompted some designers and companies, such as Meystyle, Ingo Maurer, Lomox and Samsung, to research and develop proprietary LED wallpaper technologies, some of which are currently available for commercial purchase. Other solutions mainly exist as prototypes or are in the process of being further refined.\n\n"}
{"id": "39138120", "url": "https://en.wikipedia.org/wiki?curid=39138120", "title": "List of law schools in Turkey", "text": "List of law schools in Turkey\n\nThis is a list of law schools in Turkey.\n\n"}
{"id": "1190441", "url": "https://en.wikipedia.org/wiki?curid=1190441", "title": "London Declaration concerning the Laws of Naval War", "text": "London Declaration concerning the Laws of Naval War\n\nThe London Declaration concerning the Laws of Naval War is an international code of maritime law, especially as it relates to wartime activities, proposed in 1909 at the London Naval Conference by the leading European naval powers, the United States and Japan, after a multinational conference that occurred in 1908 in London. The declaration largely reiterated existing law, but dealt with many controversial points, including blockades, contraband and prize, and showed greater regard to the rights of neutral entities.\n\nThe declaration was signed by most of the great powers of the day: Austria-Hungary, France, Germany, Italy, Japan, Russia, the United Kingdom, and the United States. (It was also signed by the Netherlands and Spain.)\nHowever, no state ever ratified the declaration and consequently it never came into force. The United States insisted that the belligerent nations fighting in World War I abide by the Declaration, while the British and Germans ignored it.\n\nThe British geostrategist and naval historian Sir Julian Corbett argued strongly against the provisions of the Declaration which sought to outlaw 'general capture' of enemy commerce on the high seas during wartime. In his earlier 1907 essay 'The Capture of Private Property at Sea', he argued that the curtailment of the Royal Navy's right to seize enemy shipping would have a detrimental impact on Britain's ability to wage economic warfare against a continental enemy; economic warfare being the single most important function of the Navy, in his view. The arguments he set out gained currency within the Navy and British government, and would eventually prevail with Britain's decision not to ratify the Declaration and the successful waging of maritime economic warfare, including 'general capture', against Germany during the First World War.\n\n\n"}
{"id": "13677155", "url": "https://en.wikipedia.org/wiki?curid=13677155", "title": "M. C. Mehta v. Kamal Nath", "text": "M. C. Mehta v. Kamal Nath\n\nM. C. Mehta v. Kamal Nath was a landmark case in Indian environmental law. In the case, the Supreme Court of India held that the public trust doctrine applied in India.\n\nThe \"Indian Express\" published an article reporting that Span Motels Private Limited, which owns Span Resorts, had floated another ambitious venture, Span Club. The family of Indian politician Kamal Nath has direct links with this company. \nThe club was built after encroaching upon 27.12 bighas of land, including substantial forestland, in 1990. The land was later regularised and leased out to the company on 11 April, 1994. \n\nThe regularisation was done when Nath was Minister of Environment and Forests. This encroachment led to the swelling of the Beas River, and the swollen river changed its course and engulfed the Span Club and the adjoining lawns, washing it away. For almost five months now, the Span Resorts management has been moving bulldozers and earth movers to turn the course of the Beas for a second time.\n\nA worrying thought was that of the river eating into the mountains, leading to landslides which were an occasional occurrence in that area. In September, these caused floods in the Beas and property estimated to be worth Rs. 105 crore was destroyed.\nThe Government of India, Ministry of Environment and Forests by the letter dated 24.11.1993, addressed to the Secretary, Forest, Government of Himachal Pradesh, Shimla conveyed its prior approval in terms of Section 2 of the Forest (Conservation) Act, 1980 for leasing to the Motel 27 bighas and 12 biswas of forest land adjoining to the land already on lease with the Motel.\nAn expert committee formed to assess the situation of the area arrived at the following conclusion;\n\n\"The river is presently in a highly unstable regime after the extraordinary floods of 1995, and it is difficult to predict its behaviour if another high flood occur in the near future. A long-term planning for flood control in the Kullu Valley needs to be taken up immediately with the advice of an organisation having expertise in the field, and permanent measures shall be taken to protect the area so that recurrence of such a heavy flood is mitigated permanently\".\n\nHowever, it could be easily ascertained from the facts that the Motel had made various constructions on the surrounding area and on the banks of the river.\n\nThe forest lands which have been given on lease to the Motel by the State Governments are situated at the bank of the river Beas. Beas is a young and dynamic river. The river is fast-flowing, carrying large boulders, at the time of flood. When water velocity is not sufficient to carry the boulders, these are deposited in the channel often blocking the flow of water. Under such circumstances the river stream changes its course, remaining within the valley but swinging from one bank to the other. The right bank of the river Beas where the motel is located mostly comes under forest, the left bank consists of plateaus, having steep banks facing the river, where fruit orchards and cereal cultivation are predominant. The area being ecologically fragile and full of scenic beauty should not have been permitted to be converted into private ownership and for commercial gains.\n\nThe notion that the public has a right to expect certain lands and natural areas to retain their natural characteristic is finding its way into the law of the land. The ancient Roman Empire developed a legal theory known as the \"Doctrine of the Public Trust\". It was founded on the ideas that certain common properties such as rivers, sea-shore, forests and the air were held by Government in trusteeship for the free and unimpeded use of the general public. Under the Roman Law these resources were either owned by no one (Res Nullious) or by everyone in common (Res Communious).\n\nUnder the English common law, however, the Sovereign could own these resources but the ownership was limited in nature, the Crown could not grant these properties to private owners if the effect was to interfere with the public interests in navigation of fishing. Resources that were suitable for these uses were deemed to be held in trust by the Crown for the benefit of the public.\nThe Public Trust Doctrine primarily rests on the principle that certain resources like air, sea, waters and the forests have such a great importance to the people as a whole that it would be wholly unjustified to make them a subject of private ownership. The said resources being a gift of nature. They should be made freely available to everyone irrespective of the status in life. The doctrine enjoins upon the Government to protect the resources for the enjoyment of the general public rather than to permit their use for private ownership or commercial purposes.\nThree types of restrictions on governmental authority are often thought to be imposed by the public trust: first, the property subject to the trust must not only be used for a public purpose, but it must be held available for use by the general public; second, the property may not be sold, even for a fair cash equivalent; and third, the property must be maintained for particular types of uses.\n\nSupreme Court of California said in the Mono Lake case,\n“….the public trust is more than an affirmation of state power to use public property for public purposes. It is an affirmation of the duty of the state to protect the people's common heritage of streams, lakes, marshlands and tidelands, surrendering that right of protection only in rare cases when the abandonment of that right is consistent with the purposes of the trust...\"\nOur legal system-based on English Common Law - includes the public trust doctrine as part of its jurisprudence. The State is the trustee of all natural resources which are by nature meant for public use and enjoyment. The State as a trustee is under a legal duty to protect the natural resources. These resources meant for public use cannot be converted into private ownership.\n\nThe esthetic use and the pristine glory of the natural resources, the environment and the eco-systems of our country cannot be permitted to be eroded for private, commercial or any other use unless the courts find it necessary in good faith, for the public good and in public interest to encroach upon the said resources.\n\nThe Court said that they had no hesitation in holding that the Himachal Pradesh Government committed patent breach of public trust by leasing the ecologically fragile land to the Motel management. Both the lease - transactions are in patent breach of the trust held by the State Government. The second lease granted in the year 1994 was virtually of the land which is a part of riverbed. Even the board in its report has recommended deleasing of the said area.\n\nThe public trust doctrine, as discussed by the Court in this judgment was a part of the law of the land.\nThe prior approval granted by the Government of India, Ministry of Environment and Forest and the lease-deed dated 11.04.1994 in favour of the Motel were quashed. The lease granted to the Motel by the said lease-deed in respect of 27 bighas and 12 biswas of area, is cancelled and set aside. The Himachal Pradesh Government shall take over the area and restore it to its original-natural conditions.\nThe Motel shall pay compensation by way of cost for the restitution of the environment and ecology of the area. The pollution caused by various constitutions made by the Motel in the riverbed and the banks on the river Beas have to be removed and reversed.\n\n"}
{"id": "57952201", "url": "https://en.wikipedia.org/wiki?curid=57952201", "title": "Ministry of Justice (Somaliland)", "text": "Ministry of Justice (Somaliland)\n\nThe Ministry of Justice and Judiciary Affairs of Somaliland administers the court system (with the exception of the Supreme Court) and has the authority to hire court personnel, allocate funds, and train, discipline or dismiss judicial officers. According to Articles 7 and 38 of the Organisation of the Judiciary Law, the ministry even compiles a panel of assessors on an annual basis for the regional courts. Additionally, the ministry is a member of the Judicial Commission. More so, the objectives of the ministry are as follows per Article 105 of the Constitution:\n\n\n\n"}
{"id": "57888644", "url": "https://en.wikipedia.org/wiki?curid=57888644", "title": "Ministry of Justice and Legal Affairs (Solomon Islands)", "text": "Ministry of Justice and Legal Affairs (Solomon Islands)\n\nThe Ministry of Justice and Legal Affairs of Solomon Islands is a department of the government of the Solomon Islands.\n\n\n\n\n\n\n"}
{"id": "45454159", "url": "https://en.wikipedia.org/wiki?curid=45454159", "title": "Morrison v Upper Hutt City Council", "text": "Morrison v Upper Hutt City Council\n\nMorrison v Upper Hutt City Council [1998] 2 NZLR 331, [1998] NZRMA 224 is a cited case in New Zealand regarding negilgence claims against the government\n\nAfter first granting Morrison a building consent to build a new town house, the UHCC later withdrew the consent after discovering it had been issued outside of the District Plan.\n\nMorrison appealed successfully to the Environment Court, and subsequently sought damages from the council.\n\nThe court ruled that a council had no liability for making what is a quasi judicial decision.\n"}
{"id": "3156313", "url": "https://en.wikipedia.org/wiki?curid=3156313", "title": "Open research", "text": "Open research\n\nOpen research is research conducted in the spirit of free and open-source software. Much like open-source schemes that are built around a source code that is made public, the central theme of open research is to make clear accounts of the methodology freely available via the internet, along with any data or results extracted or derived from them. This permits a massively distributed collaboration, and one in which anyone may participate at any level of the project.\n\nEspecially if the research is scientific in nature, it is frequently referred to as open science. \"Open research\" can also include social sciences, the humanities, mathematics, engineering and medicine.\n\nImportant distinctions exist between different types of open projects.\n\nProjects that provide open data but don't offer open collaboration are referred to as \"open access\" rather than open research. Providing open data is a necessary but not sufficient condition for open research, because although the data may be used by anyone, there is no requirement for subsequent research to take place openly. For example, though there have been many calls for more open collaborative research in drug discovery and the open deposition of large amounts of data, there are very few active, openly collaborative projects in this area.\n\nCrowdsourcing projects that recruit large numbers of participants to carry out small tasks which are then assembled into a larger project outcome have delivered significant research outcomes, but these projects are distinct from those in which participants are able to influence the overall direction of the research, or in which participants are expected to have creative input into the science behind the project.\n\nMost open research is conducted within existing research groups. Primary research data are posted which can be added to, or interpreted by, anyone who has the necessary expertise and who can therefore join the collaborative effort. Thus the \"end product\" of the project (which may still be subject to future expansion or modification) arises from many contributions across multiple research groups, rather than the effort of one group or individual. Open research is therefore distinct from open access in that the output of open research is prone to change with time.\n\nUnlike open access, true open research must demonstrate live, online collaboration. Project websites that demonstrate this capability have started to become available.\n\nIssues with copyright are dealt with by using either standard copyright (where applicable), releasing the content into the Public domain or by releasing the content under licenses such as one of the Creative Commons licenses or one of the GNU General Public Licenses.\n\nIn 2005, several examples arose in the area of the search for new/improved medical treatments of Neglected Diseases.\n\nScience and engineering research to support the creation of open-source appropriate technology for sustainable development has long used open research principles. Open source research for sustainable development is now becoming formalized with open access for literature reviews, research methods, data, results and summaries for laypeople.\n\nWiki-based examples include: Appropedia, , Citizendium, Scholarpedia.\n\nWhile first attempts towards opening research were primarily aimed at opening areas such as scientific data, methodologies, software and publications, now increasingly other artifacts of the scientific workflow are also tackled, such as scientific meta-data and funding ideas.\n\nIn 2013, open research became more mainstream with web based platforms such as figshare continuing to grow in terms of users and publicly available outputs.\n\nThe Transparency and Openness Promotion (TOP) Committee met in 2014 to address one key element of the incentive systems: journals' procedures and policies for publication. The committee consisted of disciplinary leaders, journal editors, funding agency representatives, and disciplinary experts largely from the social and behavioral sciences. By developing shared standards for open practices across journals, the committee said it hopes to translate scientific norms and values into concrete actions and change the current incentive structures to drive researchers' behavior toward more openness. The committee said it sought to produce guidelines that (a) focus on the commonalities across disciplines, and that (b) define what aspects of the research process should be made available to the community to evaluate, critique, reuse, and extend. The committee added that the guidelines aim to help improve journal policies in order to help transparency, openness, and reproducibility \"become more evident in daily practice and ultimately improve the public trust in science, and science itself.\"\n\n"}
{"id": "10964348", "url": "https://en.wikipedia.org/wiki?curid=10964348", "title": "Penal notice", "text": "Penal notice\n\nIn civil procedure a penal notice is a warning endorsed on a court order, notifying the recipient that he or she is liable to committal to prison or to pay a fine for breach of the order. In the case of a company or corporation, their assets may be seized.\n\n\n"}
{"id": "23099288", "url": "https://en.wikipedia.org/wiki?curid=23099288", "title": "Policy jury", "text": "Policy jury\n\nIn Canada a policy jury or citizen jury is a body of people convened to render a decision or advice on a matter of public policy. It is similar to juries used in modern court trial except that the subject of its deliberation is a matter of public policy, rather than law. The concept of the policy jury is closely connected with deliberative democracy or participatory models of democratic governance, and is similar to a deliberative opinion poll.\n\nIn some cases, policy juries are composed of randomly selected members of a given population. Citizens participating in a policy jury engage in a comprehensive learning and deliberation processes before settling on a conclusion or set of recommendations.\n\nPolicy juries have been used in Canada. Citizens’ Assemblies on Electoral Reform convened in British Columbia in 2004 and Ontario in 2006 used policy juries to address alternative electoral systems. Three of Ontario’s Local Health Integration Networks (LHIN) have referred their Integrated Health Service Plans (IHSP) for 2010-2013 to policy juries for advice and refinement. LHINs referring their IHSPs to policy juries include the South East LHIN, Central LHIN and Mississauga Halton LHIN.\n\n\n"}
{"id": "34245192", "url": "https://en.wikipedia.org/wiki?curid=34245192", "title": "Political demography", "text": "Political demography\n\nPolitical demography is the study of how population change affects politics. Population change is driven by classic demographic mechanisms – birth, death, age structure, and migration. \n\nHowever, in political demography, there is always scope for assimilation as well as boundary and identity change, which can redraw the boundaries of populations in a way that is not possible with biological populations. Typically, political-demographic projections can account for both demographic factors and transitions caused by social change. A notable leader in the area of sub-state population projection is the World Population Program of the International Institute of Applied Systems Analysis (IIASA) in Laxenburg, Austria. \n\nSome of the issues which are studied in the context of political demography are: surges of young people in the developing world, significantly increasing aging in the developed world, and the impact of increasing urbanization. Political demographers study issues like population growth in a political context. A population's growth is impacted by the relative balance of variables like mortality, fertility and immigration. \n\nMany of the present world's most powerful nations are aging quickly, largely as a result of major decreases in fertility rates and major increases in life expectancies. As the labor pools in these nations shrink, and spending on the elderly increases, their economies are likely to slow down. By 2050, the workforce in Japan and Russia is predicted to decrease by more than 30 percent, while the German workforce is expected to decline by 25 percent by that year. The governments of these countries have made financial commitments to the elderly in their populations which will consume huge percentages of their national GDP. For example, based on current numbers, more than 25% of the national GDPs of Japan, France and Germany will be consumed by these commitments by 2040.\n\nDifferential reproductive success is the mechanism through which evolution takes place. For much of human history this occurred through migrations and wars of conquest, with disease and mortality through famine and war affecting the power of empires, tribes and city-states. Differential fertility also played a part, though typically reflected resource availability rather than cultural factors. Though culture has largely usurped this role, some claim that differential demography continues to affect cultural and political evolution.\n\nThe demographic transition from the late eighteenth century onwards opened up the possibility that significant change could occur within and between political units. Though the writings of Polybius and Cicero in classical times bemoaned the low fertility of the patrician elite as against their more fecund barbarian competitors, differential fertility has probably only recently emerged as a central aspect of political demography. \n\nThis has come about due to medical advances which have lowered infant mortality while conquest migrations have faded as a factor in world history. Differences in immunity levels to infectious diseases between populations also play no major role in our age of modern medicine and widespread exposure to a common disease pool. \n\nIt is not so much the trajectory of demographic transition that counts as the fact that it has become more intense and uneven in the late twentieth century as it has spread into the developing world. Uneven transitions lend themselves to differential growth rates between contending groups. These changes are in turn, magnified by democratization, which entrenches majority rule and privileges the power of numbers in politics as never before.\n\nIndeed, in many new democracies riven by ethnic and religious conflicts, elections are akin to censuses while groups seek to 'win the census'. Ethnic parties struggle to increase their constituencies through pronatalism ('wombfare'), oppose family planning, and contest census and election results.\n\nOne branch of political demography examines how differences in population growth between nation-states, religions, ethnic groups and civilizations affects the balance of power between these political actors. For instance, Ethiopia is projected to have a larger population than Russia in 2020, and while there were 3.5 Europeans per African in 1900, there will be four Africans for each European in 2050. Population has always counted for national power to some degree and it is unlikely that these changes will leave the world system unaffected. \n\nThe same dynamic can be witnessed within countries due to differential ethnic population growth. Irish Catholics in Northern Ireland increased their share of the population through higher birthrates and the momentum of a youthful age structure from 35 to nearly 50 percent of the total between 1965 and 2011. Similar changes, also affected by in- and out-migration, have taken place in, amongst others, the United States (Hispanics), Israel-Palestine (Jews and Arabs), Kosovo (Albanians), Lebanon (Shia, with decline of Christians) and Nagorno-Karabakh (Armenians). \n\nIn the US, the growth of Hispanics and Asians, and Hispanics' youthful age profile as against whites, has the potential to tilt more states away from the Republican Party. On the other hand, the fertility advantage of conservative over liberal white voters is significant and rising, thus the Republicans are poised to win a larger share of the white vote - especially over the very long run of 50 to 100 years. \n\nAccording to London-based scholar Eric Kaufmann, the high birth rates of religious fundamentalists as against seculars and moderates has contributed to an increase in religious fundamentalism and decrease of moderate religion within religious groups, as in Israel, the US and the Muslim Middle East. Kaufmann, armed with empirical from a number of countries, also posits that this will be further bolstered by the higher retention rates of religious fundamentalists, with individuals in religiously fundamentalist households less likely to become religiously non-observant than others. See also .\n\nA second avenue of inquiry considers age structures: be these 'youth bulges' or aging populations. Young populations are associated with a ratio of dependents to producers: a high proportion of the population under age 16 puts pressure on resources. A 'youth bulge' of those in the 16-30 bracket creates a different set of problems.\n\nA large population of adolescents entering the labor force and electorate strains at the seams of the economy and polity, which were designed for smaller populations. This creates unemployment and alienation unless new opportunities are created quickly enough - in which case a 'demographic dividend' accrues because productive workers outweigh young and elderly dependents. Yet the 16-30 age range is associated with risk-taking, especially among males.\n\nIn general, youth bulges in developing countries are associated with higher unemployment and, as a result, a heightened risk of violence and political instability. For Cincotta and Doces (2011), the transition to more mature age structures is almost a sine qua non for democratization.\n\nPopulation aging presents the obverse effect: older populations are less risk-taking and less prone to violence and instability. However, like those under-16, they place great strain on the social safety net, especially in countries committed to old-age provision and high-quality medical care. \n\nSome observers believe that the advent of a much older planet, courtesy of below-replacement fertility in Europe, North America, China and much of the rest of Asia and Latin America, will produce a 'geriatric peace'. Others are concerned that population aging will bankrupt the welfare state and handicap western liberal democracies' ability to project power abroad to defend their interests. A more cautious climate could also herald slower economic growth, less entrepreneurship and reduced productivity in mature democracies. \n\nIt is also possible that ageing leads to lower inflation as elderly people are both inflation averse and politically powerful. However, some argue that older people in the developed world have much higher productivity, human capital and better health than their counterparts in developing countries, so the economic effects of population aging will be largely mitigated.\n\nOther areas in political demography address the political impact of skewed sex ratios (typically caused by female infanticide or neglect), urbanization, global migration, and the links between population, environment and conflict\n\nThe study of political demography is in its early stages and can be traced back to the works of figures such as Jack Goldstone, whom is often considered to be the father of Political Demography. Since 2000 the subject has drawn the attention of policymakers and journalists and is now emerging as an academic subfield. Panels on political demography appear at demography conferences such as the Population Association of America (PAA) and European Association for Population Studies (EAPS). There is now a political demography section at the International Studies Association. A number of important international conferences have also taken place since 2006 on the subject.\n\n\n"}
{"id": "42626271", "url": "https://en.wikipedia.org/wiki?curid=42626271", "title": "Privilege revocation (law)", "text": "Privilege revocation (law)\n\nIn law the general term privilege revocation is often used when discussing some paper, such as a drivers licence, being voided after a (negative) condition is met by the holder.\n\n"}
{"id": "55548438", "url": "https://en.wikipedia.org/wiki?curid=55548438", "title": "Qualified website authentication certificate", "text": "Qualified website authentication certificate\n\nA qualified website authentication certificate (QWAC certificate) is a qualified digital certificate under the trust services defined in the eIDAS Regulation.\n\nAn ENISA report proposed six strategies and twelve recommended actions as an escalated approach that targets the most important aspects detected to be critical for improving the website authentication market in Europe and successfully introducing qualified website authentication certificates as a means to increase transparency in this market.\n\nThere are different types of website authentication certificates: Domain Validated (DV), Organization Validated (OV) and Extended Validation (EV).\nAnother distinction that can be made among types of website authentication certificates relates to the number of domains that are secured by the certificate: Single domain, wildcard, multi domain. Extended Validation certificates offer the highest quality in terms of assurance of the identity of a certificate owner among exiting types of certificate in the market. Typically the use of an EV certificate is indicated by a green color – but this varies by browser.\n\nThe eIDAS Regulation has taken into account that there is an established market with its own industrial standardization efforts. The objective is not to create a disruption with existing initiatives and to optimize the effort for qualified providers to align both with the EU regulations and with the existing market standards.\n\nIn the eIDAS Regulation trust services are defined as electronic services, normally provided by trust service providers (TSPs), which consist of electronic signatures, electronic seals, electronic time stamps, electronic registered delivery services and website authentication.\n\nIn essence, the eIDAS Regulation provides a framework to promote:\n\nWebsite authentication certificates are one of the five trust service defined in the eIDAS Regulation. Article 45 sets the requirement for trust service providers issuing qualified website authentication certificates of being qualified, which implies that all requirements for qualified trust service providers (QTSPs) described in the previous section will be applicable. Annex IV defines the content of qualified certificates for website authentication:\n"}
{"id": "373335", "url": "https://en.wikipedia.org/wiki?curid=373335", "title": "Quebec Charter of Human Rights and Freedoms", "text": "Quebec Charter of Human Rights and Freedoms\n\nThe Charter of Human Rights and Freedoms () is a statutory bill of rights and human rights code passed by the National Assembly of Quebec on June 27, 1975. It received Royal Assent from Lieutenant Governor Hugues Lapointe, coming into effect on June 28, 1976.\n\nIntroduced by the Liberal government of Robert Bourassa, the charter followed extensive preparatory work that began under the Union Nationale government of Daniel Johnson. The charter ranks among other quasi-constitutional Quebec laws, such as the Charter of the French Language and the Act respecting Access to documents held by public bodies and the Protection of personal information. Having precedence over all provincial legislation (including the latter), the Charter of Human Rights and Freedoms stands at the pinnacle of Quebec's legal system. Only the Constitution of Canada, including the Canadian Charter of Rights and Freedoms, enjoys priority over the Quebec charter.\n\nThe Charter of Human Rights and Freedoms consists of six parts:\n\n\nThe Charter of Human Rights and Freedoms is unique among Canadian (and North American) human rights documents in that it covers not only the fundamental (civil and political) human rights, but also a number of important social and economic rights. The protections contained in the charter are inspired by the Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights and the International Covenant on Economic, Social and Cultural Rights. Furthermore, the list of prohibited grounds of discrimination included in the Quebec charter is extensive; a total of fourteen prohibited grounds are enumerated, including race, colour, ethnic or national origin, sex, pregnancy and age. \"Social condition\" has been a prohibited ground of discrimination since the charter came into force. Discrimination based on sexual orientation has been prohibited since 1977; with that change, Quebec became the first jurisdiction larger than a city or county to prohibit anti-gay discrimination. In 2016, gender identity or expression was added to the Quebec Charter.\n\nAn illicit violation of the charter, whether by a private party or by the provincial Crown, may give rise to a cease-and-desist order and to compensation for damages. Punitive damages may be awarded in case of an intentional and unlawful violation.\n\nThe Charter of Human Rights and Freedoms is called quasi-constitutional because, according to section 52, no provision of any other act passed by the Quebec National Assembly may derogate from sections 1 to 38, unless such act expressly states that it applies despite the charter. A total impossibility to adopt derogating laws could be considered incompatible with parliamentary sovereignty, a fundamental principle in political systems following the British tradition; however, Canada, of which Quebec is a province, has a tradition of constitutional supremacy. Its Constitution, which includes the Canadian Charter of Rights and Freedoms, is supreme, binding the federal parliament and the legislative assemblies of Canada's provinces and territories.\n\nThe Quebec charter's supremacy under its section 52 applies to the following categories of rights: fundamental rights and freedoms (the right to life, free speech, freedom of religion, the right to privacy, etc.); the right to equality; political rights; and judicial rights. Economic and social rights do not enjoy supremacy but, according to the Supreme Court of Canada in the 2002 case of Gosselin v. Quebec (Attorney General), failure to respect such a right may give rise to a judicial declaration of violation.\n\nThe charter provides for a specific machinery in cases of discrimination (or exploitation of an elderly person or person with a disability). Instead of introducing litigation in court, victims of such a violation may file a complaint with the Human Rights and Youth Rights Commission. The commission will investigate the matter and attempt to foster a settlement between the parties. It may recommend corrective measures. If those are not followed, the commission may introduce litigation before a court (usually, but not necessarily, the Human Rights Tribunal). Victims will be represented free of charge by the commission.\n\nThe Quebec charter does not apply to federally regulated activities in Quebec. Those are subject to the Canadian Charter of Rights and Freedoms and/or the Canadian Human Rights Act.\n\nNotable cases decided under the Charter include:\n\n\n"}
{"id": "44803060", "url": "https://en.wikipedia.org/wiki?curid=44803060", "title": "Rivers State Ministry of Justice", "text": "Rivers State Ministry of Justice\n\nThe Rivers State Ministry of Justice is a ministry of the Government of Rivers State responsible for handling and coordinating matters relating to judicial administration. The headquarters of the ministry is located at the State Secretariat in Port Harcourt. The current Attorney-General and Commissioner of Justice is Emmanuel C. Aguma, appointed in 2015.\n\n\n"}
{"id": "556323", "url": "https://en.wikipedia.org/wiki?curid=556323", "title": "Settlement (litigation)", "text": "Settlement (litigation)\n\nIn law, a settlement is a resolution between disputing parties about a legal case, reached either before or after court action begins. The term \"settlement\" also has other meanings in the context of law. Structured settlements provide for future periodic payments, instead of a one time cash payment.\n\nA settlement, as well as dealing with the dispute between the parties is a contract between those parties, and is one possible (and common) result when parties sue (or contemplate so doing) each other in civil proceedings. The plaintiff(s) and defendant(s) identified in the lawsuit can end the dispute between themselves without a trial.\n\nThe contract is based upon the bargain that a party forgoes its ability to sue (if it has not sued already), or to continue with the claim (if the plaintiff has sued), in return for the certainty written into the settlement. The courts will enforce the settlement: if it is breached, the party in default could be sued for breach of that contract. In some jurisdictions, the party in default could also face the original action being restored.\n\nThe settlement of the lawsuit defines legal requirements of the parties, and is often put in force by an order of the court after a joint stipulation by the parties. In other situations (as where the claims have been satisfied by the payment of a certain sum of money) the plaintiff and defendant can simply file a notice that the case has been dismissed.\n\nThe majority of cases are decided by a settlement. Both sides (regardless of relative monetary resources) often have a strong incentive to settle to avoid the costs (such as legal fees, finding expert witnesses, etc.), the time and the stress associated with a trial, particularly where a trial by jury is available. Generally, one side or the other will make a settlement offer early in litigation. The parties may hold (and indeed, the court may require) a settlement conference, at which they attempt to reach such a settlement.\n\nIn controversial cases, it may be written into a settlement that both sides keep its contents and all other information relevant to the case confidential, and/or that one of the parties (usually the one being sued) does not, by agreeing to the settlement, admit to any fault or wrongdoing in the underlying issue.\n\nA \"global settlement\" is one employed where suits have been filed, or charges brought, in multiple jurisdictions and is defined as \"a legal agreement that addresses or compromises both civil claims and criminal charges against a corporation or other large entity\". Examples of a global settlement include the Tobacco Master Settlement Agreement between the attorneys general of 46 U.S. states and the four major U.S. tobacco companies in 1999. Another example is within the Global Analyst Research Settlements.\n\nUsually, lawsuits end in a settlement, with an empirical analysis finding that less than 2% of cases end with a trial, 90% of torts settle, and around 50% of other civil cases settle. Generally, when a settlement is reached in the U.S., it will be submitted to the court to be \"rolled into a court order.\" This is done so that the court which was initially assigned the case may retain jurisdiction over it. The court is then free to modify its order as necessary to achieve justice in the case, and a party that breaches the settlement may be held in contempt of court, rather than facing only a civil claim for the breach. A party may request that the case be dismissed with prejudice to ensure that the lawsuit cannot be resubmitted.\n\nUnder Federal Rule of Evidence 408, settlement negotiations cannot usually be introduced as evidence at trial, and many state rules of evidence have similar rules modeled after it.\n\nMost settlements are confidential. In these cases, the court order may refer to another document which is not disclosed, but which may be revealed to prove a breach of the settlement. Confidentiality is not possible in class action cases in the United States, where all settlements are subject to approval by the court pursuant to Rule 23 of the Federal Rules of Civil Procedure and counterpart rules adopted in most states.\n\nIn some cases, confidential settlements are requested in discovery. Federal courts can issue protective orders preventing the release, but the party seeking to prevent disclosure must show that harm or prejudice would result from the disclosure. In specific states such as California, however the burden is on the party seeking release of the confidential settlement.\n\nThe confidentiality of settlements is controversial as it allows damaging actions to remain secret, as occurred in the Catholic sexual abuse scandal. In response, some states have passed laws which limit confidentiality. For example, in 1990 Florida passed a 'Sunshine in Litigation' law which limits confidentiality from concealing public hazards. Washington state, Texas, Arkansas, and Louisiana have laws limiting confidentiality as well, although judicial interpretation has weakened the application of these types of laws. In the U.S. Congress, a similar federal Sunshine in Litigation Act has been proposed but not passed in 2009, 2011, 2014, and 2015. Confidentiality agreements which keep secrets from regulators about violations is probably unenforceable, but a specific carveout granting regulators access is usually not included.\n\nIn England and Wales, if the matter is already before the courts, except in a case where the claim is to be dismissed outright and the Claimant agrees to pay the Defendant's costs, the matter is usually dealt with by a consent order, signed by the legal representatives of both parties and approved by the judge.\n\nTo get around the issue of confidentiality referred to above, a standard procedure consent order, known as a Tomlin Order is entered into. The order itself contains an agreement that the claim is stayed and no further action can be taken in court (except for referring a dispute in the implementation of the order to court, which is allowed). The order also deals with payment of costs, and payments of money out of court if any money is held by the court (as these are matters which must be dealt with by Court Order). However, the actual terms of the settlement are dealt with in a 'schedule' to the order, which can remain confidential. Breach of the schedule can be dealt with as breach of contract, or breach of the consent order.\n\nIn Israel, which is a common law jurisdiction, settlements almost always are submitted to the court, for two reasons: (a) only by submitting the settlement to the court can the litigants control whether the court will order one or more parties to pay costs, and (b) the plaintiff (claimant) usually prefers for the settlement to be given the effect of a judgment.\n\nIn criminal matters, the closest parallel to a settlement is a plea bargain, although this differs in several important respects, particularly the ability of the presiding judge to reject the terms of a settlement.\n\n\n"}
{"id": "3898691", "url": "https://en.wikipedia.org/wiki?curid=3898691", "title": "Steve Jackson Games, Inc. v. United States Secret Service", "text": "Steve Jackson Games, Inc. v. United States Secret Service\n\nSteve Jackson Games, Inc. v. United States Secret Service, 816 F. Supp. 432 (W.D. Tex. 1993), was a lawsuit arising from a 1990 raid by the United States Secret Service on the headquarters of Steve Jackson Games (SJG) in Austin, Texas. The raid, along with the Secret Service's unrelated Operation Sundevil, was influential in the founding of the Electronic Frontier Foundation.\n\nIn October 1988, Bell South became aware that a proprietary document relating to its 9-1-1 system had been posted on a bulletin board system (BBS) in Illinois. This was reported to the Secret Service in July 1989. In February 1990 Secret Service found that the document had been posted on the \"Phoenix\" BBS in Austin, Texas, which was operated by Loyd Blankenship, who was at the time employed by SJG and moderator of the company's own BBS, \"Illuminati\". The Secret Service believed there was probable cause to search computers belonging to Blankenship and his employer, and a search warrant was issued on February 28.\n\nThe Secret Service executed the warrant on SJG on 1 March 1990. Three SJG computers were seized, along with over 300 floppy disks. Among these was the master copy of \"GURPS Cyberpunk\", a role-playing game written by Blankenship which SJG was about to release. The \"Illuminati\" server included private personal emails to and from SJG employees. The material was returned in June 1990.\n\nSJG sued the Secret Service for damages arising from loss of revenue while the computers were in its custody. Steve Jackson and three other employees also sued for invasion of privacy, claiming the seizures were illegal under the Privacy Protection Act of 1980, Electronic Communications Privacy Act and Stored Communications Act. Loyd Blankenship was not party to the suits. The case came to trial in 1993 in the Western Texas District Court. SJG was represented by the Austin firm of George, Donaldson & Ford, while the lead counsel was Pete Kennedy. SJG won two out of the three counts and was awarded $50,000 in statutory damages and $250,000 in attorney's fees. No compensatory damages were awarded. The judge said that Steve Jackson had little involvement in SJG at the time of the raid, and the company was close to Chapter 11 bankruptcy, and that Jackson's renewed involvement in the wake of the raid had turned the company's fortunes around. The judge reprimanded the Secret Service, calling their warrant preparation \"sloppy\", suggesting that they needed \"better education\" regarding relevant statutes, and finding that they had no basis to suspect SJG of any wrongdoing. The third count dealing with interception of e-mail was upheld in October 1994 by the Fifth Circuit Court of Appeals. The Electronic Frontier Foundation was an amicus curiae at the appeal.\n\nAlthough the raid was not a part of Operation Sundevil, this law enforcement effort, which spanned two years, has a tarnished image owing to a lack of successful prosecutions and questionable procedures. The \"GURPS Cyberpunk\" book lists \"Unsolicited Comments: The United States Secret Service\" on its credits page.\n\n"}
{"id": "41478251", "url": "https://en.wikipedia.org/wiki?curid=41478251", "title": "Sugimura &amp; Partners", "text": "Sugimura &amp; Partners\n\nSugimura was founded in 1923 by patent attorney and engineer Nobuchika Sugimura. The firm specializes in providing quality legal services in the acquisition, protection and consultation of Japanese IP rights, and is repeatedly ranked among the top Japanese patent and trademark firms. The company has offices in Japan and the United States, as well as professional relationships with associate law firms located in more than 80 countries. In 2017, the firm changed its name from Sugimura International Patent and Trademark Attorneys to Sugimura & Partners. Today, Kenji Sugimura. Principal Patent Attorney, and Koji Sugimura, Principal Attorney at Law are leading the firm.\n\nIt is headquartered in the Kasumigaseki Common Gate West Tower in Kasumigaseki, Chiyoda, Tokyo, Japan, and has a representative office in Palo Alto, California, U.S.A.\n\nSugimura is repeatedly ranked among top Japanese patent and trademark law firms.\n\nThe firm is consistently ranked on the World IP Survey. The firm has been ranked in the patent prosecution and trademark prosecution categories.\n\nWorld Trademark Review recognizes the firm as a leading trademark professional in Japan.\n\nIntellectual Asset Management recognizes the firm as a leading patent professional in Japan.\n\n\"Asian Legal Business\" recognizes the firm as a leading domestic patent firm.\n\nThe firm appears in the Legal 100 of Acquisition International magazine, a vanity award\n\n\nPatent attorneys from Sugimura have featured in or written various articles about IP industries, and have been invited as speakers at various instituesa and IP meetings.\n\n"}
{"id": "39066034", "url": "https://en.wikipedia.org/wiki?curid=39066034", "title": "Sunil Kumar Lahoria", "text": "Sunil Kumar Lahoria\n\nSunil Kumar Lahoria (about 1961 – February 2013 Navi Mumbai) was a builder from Mumbra, Thane district, India who was killed in February, 2013 after reporting about illegal buildings. This occurred about a year and a half before the collapse of an illegal building in Thane on 4 April 2013, killing 74 people.\n\nIn 2011, Lahoria informed municipal and state authorities that within Navi Mumbai there were more than 600 illegal buildings. He also filed a writ petition in the Bombay High Court. Construction of the buildings also violated floor space index (FSI) rules, including development of additional storeys.\n\n21 towers on Palm Beach Road were included in the count of 600 illegal buildings. According to Navi Mumbai Municipal Corporation's (NMMC) town planning officer, Prakash Thakur: one of the towers is being investigated by the Maharashtra Coastal Zone Management Authority and action has been taken on 20 towers.\n\nIn addition to the complaints to governmental authorities and the court, he also questioned CIDCO and Navi Mumbai Municipal Corporation's (NMMC) role in allowing the construction to occur. \n\nStarting the latter half of 2011, about six months before Lahoria's murder, he received death threats from builders with criminal connections. Although protection was requested of the Navi Mumbai police, he never received it.\n\nIn February 2013, Lahoria sustained five bullet wounds and a slash to his hand and neck outside of S K Brothers Builders & Developers' Office, his real estate company in Navi Mumbai. The two men who shot him wore private security guard uniforms, one of whom knocked a coconut into his head after he had fallen to the ground. Intending to perform a mock funeral, the men had 'agarbattis' and a garland with them\n\nHaving seen the incident, two workers in Lahoria's office chased the men. They were able to catch one of the men, a hitman named Venkatesh Shettiyar, who was placed in police custody. The victim's son, Sunny Kumar, claimed that the names of \"two prominent developers\" had been given by the captured gunman; on the other hand, the gunman told police that he committed the murder because he had not been paid a large commission. \n\nAnurag Garg and Suresh Bijlani, both still escaping incarceration, are suspects in his murder.\n\nLahoria's son, Sandeep Kumar has continued his father's crusade to mitigate illegal buildings. He has also written to the state Director General of Police and the Prime Minister's Office about the handling of his father's murder investigation. He called out two specific Navi Mumbai senior policemen and questioned the easy treatment of the suspects, including not seizing their passports. Of his father, he said:\nMy father paid a very heavy price for exposing irregularities that took place with the active involvement of government officers. He made use of the Right to Information Act to deal with corruption, but he was killed.\n\n"}
{"id": "43900780", "url": "https://en.wikipedia.org/wiki?curid=43900780", "title": "Taxation in Uruguay", "text": "Taxation in Uruguay\n\nTaxes in Uruguay are collected by the General Taxation Directorate (, DGI).\n\nA major tax reform bill came into force on 1 July 2007 with the number 18083. Nevertheless, something important remained: Uruguay applies the source principle, with investments located and activities performed outside Uruguay remaining untouched.\n\n"}
{"id": "22256927", "url": "https://en.wikipedia.org/wiki?curid=22256927", "title": "Telecoms Package", "text": "Telecoms Package\n\nThe Telecoms Package was the review of the European Union Telecommunications Framework from 2007 – 2009. The objective of the review was to update the EU Telecoms Framework of 2002 and to create a common set of regulations for the telecoms industry across all 27 EU member states. The review consisted of a package of directives addressing the regulation of service provision, access, interconnection, users' contractual rights and users' privacy, as well as a Regulation creating a new European regulatory body (BEREC).\n\nThe update to the telecoms regulations was needed to address the growth of broadband Internet. It was intended merely to address structural regulation and competitive issues concerning the broadband providers and the provision of spectrum. The Telecoms Package created a new pan-European agency called Body of European Regulators of Electronic Communications (BEREC) overseeing telecoms regulation in the Member States. It provided for Member States to set minimum quality of service levels for broadband network transmission. It harmonised European contractual rights for telephone and Internet subscribers. These rights included the ability to switch telephone operators within 24 hours of giving notice, and retaining the phone number. Broadband and phone providers are obligated to limit the contract term to 12 months. Subscribers are to be notified of data privacy breaches.\n\nThe Telecoms Package became subject to several political controversies, including disputes over the provision of access to infrastructure by dominant broadband providers. However, the most significant controversies concerned copyright and net neutrality.\n\nThe controversy over copyright arose because of an attempt to put in amendments mandating Internet Service Providers to enforce copyright. It was argued that these amendments sought to implement a three strikes regime. There was a public political argument over this matter. The debate eventually centred on one single counter-amendment, known as Amendment 138. The outcome was that the Package was forced to go to three readings in the European Parliament, and a compromise amendment was drafted, with the agreement of the three European institutions – Parliament, Commission and Council. This compromise amendment is sometimes now known as the 'Freedom provision'.\n\nThe net neutrality controversy arose out of changes made to transparency requirements for broadband providers, where, it was argued, those changes could permit the providers to alter quality of service or favour or discriminate against other players.\n\nThe Telecoms Package is known in German as \"Telekom-Paket\", in French as \"Paquet Telecom\", in Spanish as \"Paquete Telecom\", and in Swedish as \"Telekompaketet\".\n\nThe legislation that comprises the Telecoms Package, as published in the European Union Official Journal is:\n\nDirective 2009/140/EC (Framework, Authorisation and Access directives)\n\nDirective 2009/136/EC(Universal services and E-privacy directives)\n\nRegulation No 1211/2009 establishing the Body of European Regulators for Electronic Communications (BEREC)\n\nThe Telecoms Package was presented by Viviane Reding, the European Commissioner for Information Society, to the European Parliament in Strasbourg 13 November 2007.\n\nThe draft legislation that was presented to the European Parliament was:\n\nProposal for a directive of the European Parliament and of the Council amending Directives 2002/21/EC on a common regulatory framework for electronic communications networks and services, 2002/19/EC on access to, and interconnection of, electronic communications networks and services, and 2002/20/EC on the authorisation of electronic communications networks and services\n\nProposal for a directive of the European Parliament and of the Council amending Directive 2002/22/EC on universal service and users' rights relating to electronic communications networks, Directive 2002/58/EC concerning the processing of personal data and the protection of privacy in the electronic communications sector and Regulation (EC) No 2006/2004 on consumer protection cooperation\n\nProposal for a regulation of the European Parliament and of the Council establishing the European Electronic Communications Market Authority\n\nThe Telecoms Package went through three readings in the European Parliament. The First Reading concluded on 24 September 2008. The Second Reading concluded on 5 May 2009. The Third Reading, also known as the Conciliation process, concluded at midnight on 5 November 2009.\n\nThe entire Package was finally adopted by a majority vote in the European Parliament on 24 November 2009. This was however, a legal technicality. The critical policy issues had already been decided during the three Readings.\n\nThe Telecoms Package entered into European law on 18 December 2009 (the date on which it was published in the Official Journal), after which member states had 18 months to implement its provisions in national law.\n\nThe Telecoms Package was a complex piece of legislation. It was intended to update many aspects of telecoms regulation. It combined earlier directives from 2002 into two new bundles. The Framework, Access and Authorisation directives from 2002, were put into one new directive. The Universal Services and e-Privacy directives, also from 2002, were bundled together into another new directive.\n\nIn the European Commission's draft of 13 November 2007, there were two amendments that attempted to insert support for copyright, notably that EU Member States should mandate their broadband providers to co-operate with rights-holders and favouring a 'three strikes' or graduated response regime. These two amendments were Annex 1, point 19 of the Authorisation directive and Amendment 20.6 of the Universal Services directive. They sparked a major political controversy over the enforcement of copyright on the Internet.\n\nThe copyright controversy became public during the first reading of European Parliament. It came to dominate the political debate, and was the subject of a vocal activist campaign led by La Quadrature du Net. It was only resolved during the Third Reading, when the European Parliament drafted a new provision that reminded Member State governments of their obligations under the European Convention of Human Rights, notably the right to due process.\n\nThe famous (or infamous) Amendment 138 was tabled to highlight the problem of copyright and with the aim of stopping a three strikes regime being legitimated in European Union legislation.\n\nAmendment 138 was an amendment tabled to the Framework directive, that sought to mandate a judicicial ruling in cases where Internet access would be cut off. It was deliberately framed to target other proposals for copyright measures – the so-called '3 strikes'.\nThe text of amendment 138 was:\n\n“ applying the principle that no restriction may be imposed on the fundamental rights and freedoms of end-users, without a prior ruling by the judicial authorities, notably in accordance with Article 11 of the Charter of Fundamental Rights of the European Union on freedom of expression and information, save when public security is threatened in which case the ruling may be subsequent.”\n\nAmendment 138 was adopted by the European Parliament in the First Reading plenary vote on 24 September 2008. This created an inter-institutional stand-off between the Parliament on the one hand, and the Commission and the Council of Ministers, on the other.\n\nIn the Second Reading, on 5 May 2009, the European Parliament again voted for Amendment 138.\n\nIn the Third Reading, the only issue under discussion was Amendment 138 and how to handle the copyright issue. A compromise provision was finally agreed by all three EU institutions at midnight on 4 November 2009. This provision is Article 1.3a of the Framework directive. It is sometimes known as the 'Freedom Provision'.\n\nThe text of Article 1.3a (the so-called \"Freedom Provision\") is:\n\"3a. Measures taken by Member States regarding end-users access’ to, or use of, services and applications through electronic communications networks shall respect the fundamental rights and freedoms of natural persons, as guaranteed by the European Convention for the Protection of Human Rights and Fundamental Freedoms\nand general principles of Community law.\n\nAny of these measures regarding end-users’ access to, or use of, services and applications through electronic communications\nnetworks liable to restrict those fundamental rights or freedoms may only be imposed if they are appropriate, proportionate and necessary within a democratic society, and their implementation shall be subject to adequate procedural safeguards in conformity with the European Convention for the Protection of Human Rights and Fundamental Freedoms and with general principles of Community law, including effective judicial protection and due process. Accordingly, these measures may only be taken with due respect for the principle of the presumption of innocence and the right to privacy. A prior, fair and impartial procedure shall be guaranteed, including the right to be heard of the person or persons concerned, subject to the need for appropriate conditions and procedural arrangements in duly substantiated cases of urgency in conformity with the European Convention for the Protection of Human Rights and Fundamental Freedoms. The right to effective and timely judicial review shall be guaranteed.\"\n\nThe Telecoms Package contained provisions that concerned net neutrality. These provisions related to transparency of information supplied by network operators and Internet Service Providers (ISPs) to their subscribers. They can be found in Article 20 and 21 of the Universal Services directive.\nThese two Articles were subject to considerable lobbying by the telecoms network operators, who wanted to retain the flexibility to run the networks to suit their business requirements.\n\nSome of their demands were criticised by citizens advocacy groups\n, who argued that certain proposed amendments would allow the broadband operators to use discriminatory forms of traffic management. The outcome was a strange wording in the text:\n\n\"inform subscribers of any change to conditions limiting access to and/or use of services and applications, where such conditions are permitted under national law in accordance with Community law\";\n\nThe Telecoms Package was a target for lobbying by American telecoms companies, notably AT&T and Verizon, seeking to get the ability to use sophisticated traffic management techniques on broadband networks embedded into European law. Filip Svab, chairman of the \"Telecoms Working Group\" of the Council of the European Union, which was responsible for drafting the Council's changes to the Telecoms Package on the second reading, left Brussels for a new job with AT&T (External Affairs Director).\n\n\n"}
{"id": "17932591", "url": "https://en.wikipedia.org/wiki?curid=17932591", "title": "Torrentz", "text": "Torrentz\n\nTorrentz was a Finland-based metasearch engine for BitTorrent, run by an individual known as Flippy. It indexed torrents from various major torrent websites, and offered compilations of various trackers per torrent that were not necessarily present in the default .torrent file, so that when a tracker was down, other trackers could do the work. It was the second most popular torrent website in 2012.\n\nTorrentz's user interface was simple; it had only a user menu and a search panel. Users were not required to register before searching files.\n\nTo perform a search, users would simply type in a string of keywords within the autocomplete-enabled search field and execute the search by either pressing the search button in the UI or the enter key on the keyboard. From there, a list of matching torrent files were displayed on the screen for the user to choose from. This list could be filtered by age (one day, three days, one week or one month) and by \"safety & quality\" (\"any\", \"good\" or \"verified\"). The \"good\" filter was applied by default, and \"verified\" was reserved for torrents uploaded by well-known groups.\n\nSelecting a torrent from the search results list would take the user to another page listing the websites currently hosting the specified torrent (with which users would download files). As Torrentz used meta-search engines, users would be redirected to other torrent sites to download content (commonly KickassTorrents, which was considered safe to use).\nIn November 2008, scammers using fake papers attempted to take over the torrentz.com domain. As a backup, the site administrator set up the domain torrentz.eu. After December 18, 2010, torrentz.eu became the site's default domain, due to the domain name seizures carried out by US authorities on various torrent websites.\n\nIn 2013, Paramount Pictures sent a DMCA claim to Google to remove the Torrentz homepage and two other pages from its search engine. Torrentz counter-claimed on this request, claiming that the links did not infringe any copyright policies.\n\nOn 26 May 2014, Torrentz had its domain name suspended without a court order following a request from the Police Intellectual Property Crime Unit. A day later, the suspension of torrentz.eu was lifted. The website had three alternative domains (.me, .ch and .in) and hoped to move the .eu domain to a new registrar. All of these domains have since been blocked within the UK by Sky.\n\nOn August 5, 2016, Torrentz was shut down by its operators with the message \"Torrentz will always love you. Farewell.\" displayed; it operated for over 13 years.\n\nIn August 2016, a few days after Torrentz shut down, an unofficial clone of Torrentz - Torrentz2.eu - was launched that initially indexed 60 million torrents. Later that month, another unofficial clone - torrentzeu.to - was launched that initially indexed 30 million torrents.\n\n\n"}
{"id": "5120263", "url": "https://en.wikipedia.org/wiki?curid=5120263", "title": "Treaty of Simulambuco", "text": "Treaty of Simulambuco\n\nThe Treaty of Simulambuco was signed in 1885 by representatives of the Portuguese government and officials in the N'Goyo Kingdom. The agreement was drafted and signed in response to the Treaty of Berlin, which was an agreement between the colonizing European powers about how to divide up Africa. The long-established Portuguese, not wanting to miss out on the Scramble for Africa involving territories near its own old possessions, began to colonize deeper than the numerous trading ports it had controlled on the African coast since the early 16th century. In contrast to the violent struggles between the Portuguese and some native peoples in Mozambique, the colonization of Cabinda was peaceful.\n\nPortugal first claimed sovereignty over Cabinda in the February 1885 Treaty of Simulambuco, which gave Cabinda the status of a protectorate of the Portuguese Crown under the request of “the princes and governors of Cabinda”. Article 1 of the treaty, states, “the princes and chiefs and their successors declare, voluntarily, their recognition of Portuguese sovereignty, placing under the protectorate of this nation all the territories by them governed”. Article 2, which is often used in separatist arguments, goes even further: “Portugal is obliged to maintain the integrity of the territories placed under its protection.” The treaty was signed between the emissaries of the Portuguese Crown and the princes and notables of Cabinda, giving rise to three territories within the Portuguese protectorate of Cabinda: Cacongo, Loango and Ngoio.\n\nCabinda was incorporated into the Portuguese Empire separately from its larger southern neighbour Angola even though, at the time, the two were separated merely by the Congo River and a strip of land of Democratic Republic of Congo. In 2005, Cabindans celebrated the 120th anniversary of the treaty, to the annoyance of Angolan officials, who view the treaty as running counter to their claim that the territory is an exclave. This dispute over the treaty has led to an ongoing separatist conflict.\n\n"}
{"id": "3252212", "url": "https://en.wikipedia.org/wiki?curid=3252212", "title": "Voluntary dismissal", "text": "Voluntary dismissal\n\nVoluntary dismissal is termination of a lawsuit by voluntary request of the plaintiff (the party who originally filed the lawsuit). A voluntary dismissal with prejudice (meaning the plaintiff is permanently barred from further litigating the same subject matter) is the modern descendant of the common law procedure known as retraxit.\n\nIn the United States, voluntary dismissal in Federal court is subject to Rule 41(a) of the Federal Rules of Civil Procedure. Rule 41(a)'s full text can be found below. Simply stated, Rule 41(a) allows the plaintiff to make a dismissal as long as the defendant has not filed an answer or filed a motion for summary judgment. \n\nIf the defendant has taken such action, dismissal is only proper under two circumstances:\n\na. all defendants stipulate to dismissal; or\n\nb. the judge overseeing the case rules for the case to be dismissed\n\nOnce the case has been voluntarily dismissed, if it is brought to court again a dismissal in this second case will mean the case can never again be brought back to court.\n\nIf the defendant has a counterclaim, the case can only be dismissed if the counterclaim can still stand as its own case. \n\nFull text of Rule 41 (a) of the Federal Rules of Civil Procedure:\n\n(a) Voluntary Dismissal: Effect Thereof\n"}
{"id": "18411352", "url": "https://en.wikipedia.org/wiki?curid=18411352", "title": "Waltons Stores (Interstate) Ltd v Maher", "text": "Waltons Stores (Interstate) Ltd v Maher\n\nWaltons Stores (Interstate) Ltd v Maher, is a leading case in Australian contract law. The Australian High Court decided that estoppel, in certain circumstances could be a cause of action.\n\nMaher owned some property with buildings on it in Nowra. He was negotiating with a department store company called Waltons Stores (at the time controlled by the Bond Corporation) for a lease of the land. They wanted an existing building to be demolished and a new one erected.\n\nIn reliance on representations made before a contract was completed, Maher demolished the building and started to erect a new one. But the contract never came to completion because Waltons Stores did not sign the lease as Maher had yelled at them and become hostile towards them. Waltons told their solicitors to slow the deal while they did further investigations as to whether the transaction would be good business, but allowed Maher to remain under the impression that the deal would be completed.\n\nThe High Court held that to avoid detriment through Waltons' unconscionable behaviour, Waltons was estopped from denying the contract. Whilst the mere exercise of legal right not to exchange contracts was not unconscionable, there were two additional elements which made Waltons' conduct unconscionable: a) element of urgency, b) Maher executed and forwarded on 11/11 and assumed execution by Walton was a formality. The award (though very similar to an expectation interest, as if it were a contract that was enforced) was only meant to cover reliance. Because Maher had acted to his detriment, in reliance on the encouragement of Walton Stores, which had acted unconscionably, equity would intervene.\n\nMason CJ and Wilson J said the following\n\nBrennan J, Deane J and Gaudron J gave concurring judgments.\n\n"}
