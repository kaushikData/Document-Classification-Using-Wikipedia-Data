{"id": "1753261", "url": "https://en.wikipedia.org/wiki?curid=1753261", "title": "A Biographical Dictionary of Civil Engineers", "text": "A Biographical Dictionary of Civil Engineers\n\nA Biographical Dictionary of Civil Engineers in Great Britain and Ireland discusses the lives of the people who were concerned with building harbours and lighthouses, undertook fen drainage and improved river navigations, built canals, roads, bridges and early railways, and provided water supply facilities. Volume One, published in 2002, covers the years from 1500 to 1830, while Volume Two, published in 2008, covers 1830 to 1890. The principal editor of the first volume was Professor A. W. Skempton, and the entries were written by a number of specialist historians.\n\nAn 18-page introduction in the first volume discusses the practice of civil engineering from 1500-1830. The work concludes with appendices discussing wages, costs and inflation, a chronology of major civil engineering works, and indices of places and names. Volume Two's introduction discusses the practice of civil engineering from 1830-1890.\n\n\n"}
{"id": "1681976", "url": "https://en.wikipedia.org/wiki?curid=1681976", "title": "Avery Dennison", "text": "Avery Dennison\n\nAvery Dennison Corporation is a global manufacturer and distributor of pressure-sensitive adhesive materials (such as self-adhesive labels), apparel branding labels and tags, RFID inlays, and specialty medical products. The company is a member of the Fortune 500 and is headquartered in Glendale, California.\n\nThe company was founded in Los Angeles, California, in 1935 as Kum Kleen Products, a partnership of Mr. and Mrs. Ray Stanton Avery. The name was changed to Avery Adhesives in 1937. In 1946, the company was incorporated as Avery Adhesive Label Corp., and the name was subsequently changed to Avery Adhesive Products, Inc. in 1958, and to Avery Products Corporation in 1964. The name was changed again to Avery International Corporation in 1976, and it became Avery Dennison after the company merged with the Dennison Manufacturing Company in 1990.\n\nThe Dennison Manufacturing Company was founded by Andrew Dennison and his son Aaron Lufkin Dennison, residents of Brunswick. Maine, in 1844, as a jewelry- and watch-box manufacturing company located in the Dunlap Block of Maine Street. Five years later Aaron turned the Dennison Manufacturing Company over to his younger brother, Eliphalet Whorf Dennison, who took over and developed the company into a significant-size industrial enterprise. It was in 1898 that the business moved to a location in Framingham, Massachusetts. (Aaron Dennison went on to co-found the Waltham Watch Company in 1850, a leader in the American system of watch manufacturing using interchangeable parts.)\n\nAvery Dennison created a separate division for office products such as binders, file labels and name badges in 1982. The division and its products, sold under the Avery brand and logo, contrasted with the company’s larger materials division in that its products were finished (“converted”) materials, and they were aimed at consumers as well as businesses. Over the next 30 years, the division grew, as personal computing created a market for printable media both at home and at work. However, with the rise of email and the decline in conventional mail, the office products market as a whole began to decline. On July 1, 2013, Avery Dennison completed the sale of Office and Consumer Products and a second business, Designed and Engineered Solutions, to CCL Industries. CCL purchased the Avery office products brand along with the business. Avery Dennison retains its full name, history, brand and logo.\n\nThe company is headquartered in Glendale, California. It ranked number 435 on the 2015 Fortune 500 list with total sales of $7.0 billion. Its main lines of business are pressure-sensitive materials (contributing 73% of 2015 revenues) and retail branding and information solutions (contributing 26% of 2015 revenues). The company operates in more than 50 countries and employs 25,000 people worldwide. Its first overseas subsidiary was established in the Netherlands in 1955.\n\nThe company's operations are organized into three business units:\n\nThe Pressure-Sensitive Materials segment manufactures and sells pressure-sensitive roll-label materials, films for graphic applications, reflective highway-safety products, a variety of specialized tapes, performance polymers, and extruded films.\n\nPrinters take its label materials, which are manufactured in large rolls, and convert them into finished labels by printing, cutting and applying them to packaging for consumer goods such as shampoo, beverages and pharmaceuticals.\n\nThe Retail Branding and Information Solutions segment designs, manufactures, and sells various branding and information management products and solutions for apparel and general retail, including tickets, graphic, barcode and radio-frequency identification (RFID) tags, labels and inserts, woven and printed labels, external embellishments, price management systems, a variety of fasteners, and related supplies and equipment.\n\nThe information management products that the business provides, including its RFID-based systems, help apparel and other retailers to manage inventory both within stores and across supply chains that today extend for thousands of miles. The ability to accurately identify inventory and replenish it can help improve the shopping experience, as goods are less likely to be out of stock.\n\nThe company also operates Vancive Medical Technologies, which provides products such as wearable sensors, barrier films, wound dressings and a variety of tapes and securement products for the healthcare industry. Results for Vancive Medical Technologies are reported in Other specialty converting businesses. Vancive Medical Technologies products are sold to medical products and device manufacturers.\n\nIn 2012, Avery Dennison and 3M Company agreed to settle patent and antitrust litigation between the parties. The litigation began when 3M alleged that Avery Dennison infringed 3M's patents related to retroreflective sheeting used for road signs and other highway and transportation products and requested an injunction to prevent Avery Dennison from selling its OmniCube retroreflective product. The court denied 3M's request, and after Avery Dennison brought claims of its own against 3M for patent infringement and antitrust violations, the parties agreed to dismiss three pending cases.\n"}
{"id": "25276638", "url": "https://en.wikipedia.org/wiki?curid=25276638", "title": "Ball transfer unit", "text": "Ball transfer unit\n\nBall transfer units are omnidirectional load-bearing spherical balls mounted inside a restraining fixture. They are identical in principle to a computer trackball (pointing device). Typically the design involves a single large ball supported by smaller ball bearings. \n\nThey are commonly used in an inverted ball up position where objects are quickly moved across an array of units, known as a ball transfer table, a type of conveyor system. This permits manual transfer to and from machines and between different sections of another conveyor system. They are used in airports for luggage delivery, or in industry as part of manufacturing systems. Prior to the invention of the ball transfer unit, first patented by Autoset Production Ltd in 1958, these applications were solved by the use of inverted casters. However, casters recognise a trail, meaning that the wheels had to align before directional change could be achieved.\n\nBall transfer units can also be used in a non-inverted ball down position as a type of caster, however this use is restricted by load-bearing limitations and the type of floor. Manufacturers have addressed this problem with ball transfer units incorporating recirculating ball principles, however the inverted position is still the most common application and the least problematic.\n\n"}
{"id": "31961218", "url": "https://en.wikipedia.org/wiki?curid=31961218", "title": "Blind mate connector", "text": "Blind mate connector\n\nA blind mate connector is differentiated from other types of connectors by the mating action that happens via a sliding or snapping action which can be accomplished without wrenches or other tools. They have self-aligning features which allows a small misalignment when mating.\n\nElectrical blind mate connectors provide power or signal and are distinguished from other connectors in that they do not feature a rigid mechanical retention mechanism belonging to the interface itself, such as a threaded coupling nut on an SMA connector. They are typically used in a multi-pin arrangement between racks and panels, daughtercard to backplane, or similar applications where the connector is not mated by itself, but rather by the action of inserting the entire unit or module.\n\nBlind mate RF connectors are generally intended to be used without wrenches or other tools, but can come packaged in a multiport configuration that includes mounting hardware or provided as inserts in a circular connector such as a MIL-DTL-38999. Some examples of blind mate RF interfaces include BMA (OSP), BMMA (OSSP), SMP, SMPM, SMPS, and Planar Crown connectors.\n\nCertain styles of blind mate RF connectors feature a detent or undercut to allow for the mating connector to snap into, improving retention. These styles are typically used when the mate is a cable connector and might see a significant force that would normally demate the connection.\n\nOther configurations feature a smooth bore into which the mate simply slides to connect. These arrangements are more commonly found in board to board connections, where they are mated with a female to female \"bullet\" adapter, and no significant force is present to demate the connection.\n\nKey features of hermetic, push-on and blind-mate RF connectors are listed below:\n\nOptical signals can also be connected using blind mate connectors providing the capability to link fibre optics on a plug-in card to an optical backplane, or through an optical midplane\n\n"}
{"id": "1358686", "url": "https://en.wikipedia.org/wiki?curid=1358686", "title": "Bomb tower", "text": "Bomb tower\n\nA bomb tower is a lightly constructed tower, often 100 to 700 feet (30 to 210 meters) high, built to hold a nuclear weapon for an above ground nuclear test. The tower holds the bomb for the purpose of the investigation of its destructive effects (such as burst height and distance with given explosive yield) and for the adjustment of measuring instruments, such as high-speed cameras. Normally, the bomb tower disintegrates completely on detonation due to the enormous heat of the explosion.\n"}
{"id": "2724567", "url": "https://en.wikipedia.org/wiki?curid=2724567", "title": "Bone crusher", "text": "Bone crusher\n\nA bone crusher is a device regularly used for crushing animal bones. Bones obtained during slaughter are cleaned, boiled in water and dried for several months. After that, they are suitable for crushing with the special machine into a relatively dry gritty powder which is used as fertilizer.\n\nThe machine, shown in the picture, is powered by a water wheel. It contains eight S-shaped pairs of cams that raise the crushers alternately and let them fall into material to be crushed. The simple transmission increases the rotation speed of the crusher wheel to 21 rpm from the water wheel speed of about 7 rpm.\n\nBone meal has been used since about 1790 as a fertilizer supplement to ordinary farmyard manure. From about 1880 onwards it was supplanted by chemical fertilizers.\n\n"}
{"id": "44212433", "url": "https://en.wikipedia.org/wiki?curid=44212433", "title": "Carl A. Wiley", "text": "Carl A. Wiley\n\nCarl Atwood Wiley (December 30, 1918 – April 21, 1985) was an American mathematician and engineer. He is most widely known as the originator of the solar sail concept as well as the inventor of synthetic aperture radar.\n\nWiley's research work began at the Air Force Aircraft Radiation Lab at Wright Field in 1941. In 1942 he discovered the piezoelectricity of Barium titanate, for which he later was awarded a patent. In 1949 he went to work for as the engineer-in-charge of Goodyear Aerophysics. It was during this time he invented synthetic aperture radar in 1951, patented as \"Pulsed Doppler Radar Methods and Means,\" #3,196,436. That same year Wiley posited the idea of solar sails in a science fiction story published in \"Astounding Science Fiction\" magazine entitled \"Clipper Ships of Space\" (originally titled \"Are the Clipper Ships gone forever?\"). Wiley wrote his story under the \"nom de plume\" of Russel Saunders, an in-group reference to Russel–Saunders coupling not unlike J.J. Coupling, itself a reference to angular momentum. Seven years later Richard L. Garwin developed the first technical specifications for a solar sail. Robert L. Forward credited Wiley for the genesis of the idea in Forward's 1990 patent. Wiley's research and manuscripts for the story are now housed in the Eaton collection. In 1953 he left Goodyear to found his own company, Wiley Electronics in Phoenix, Arizona until it was bought out in 1962. Following that Wiley worked for North American Aviation and its successor, Rockwell International where he worked on various radar projects including LOCO, SINCO, VOLPHASE, and VOLFRE. In 1978 he went to work at Hughes Aircraft Company where he eventually retired as a chief scientist in the technology division of Hughes' Space and Communications Group. In 1985, IEEE awarded Wiley their Pioneer Award.\n"}
{"id": "698858", "url": "https://en.wikipedia.org/wiki?curid=698858", "title": "Carton", "text": "Carton\n\nA carton is a box or container usually made of paperboard and sometimes of corrugated fiberboard.\nMany types of cartons are used in packaging. Sometimes a carton is also called a box.\n\nA carton is a type of packaging suitable for food, pharmaceuticals, hardware, and many other types of products. Folding cartons are usually combined into a tube at the manufacturer and shipped flat (knocked down) to the packager. Tray styles have a solid bottom and are often shipped as flat blanks and assembled by the packager. Some also are self-erecting. High-speed equipment is available to set up, load, and close the cartons.\n\nEgg cartons or trays are designed to protect whole eggs while in transit. Traditionally, these have been made of molded pulp. This uses recycled newsprint which is molded into a shape which protects the eggs. More recently, egg cartons have also been made from expanded polystyrene and PET.\n\nCartons for liquids can be fabricated from laminates of liquid packaging board, foil, and polyethylene. Most are based on either Tetra Pak or SIG Combibloc systems. One option is to have the printed laminate supplied on a roll. The carton is cut, scored, and formed at the packager. A second option is to have the pre-assembled tubes delivered to the packager for completion and filling. These are suited for aseptic processing and are used for milk, soup, juice, etc. Paperboard-based cartons are lighter compared to a similarly-sized steel can, but are more difficult to recycle. Some open-loop recycling operations pelletize or flatten ground-up cartons for use in building materials; closed-loop recycling is possible by separating the layers before processing, though some recyclers only recycle the cardboard fibers.\n\nGable top cartons are often used for liquid products such as milk, juice, etc. These use polyethylene-coated paperboard or other liquid packaging board and sometimes a foil laminate. Most are opened by pushing open the gables at the top back and pulling the top (spout) out. Some have fitments to assist in opening and eating the contents.\n\nThe history of the carton goes as far back as 1879 in a Brooklyn, New York, factory operated by Robert Gair. A die-ruled, cut, and scored paperboard into a single impression of a folded carton. By 1896, the National Biscuit Company was the first to use cartons to package crackers.\n\nThe next development of folded paper used to construct cartons are mentioned by Dr. Winslow of Seattle, Washington, in 1908 who claimed that paper milk containers were commercially sold in San Francisco and Los Angeles as early as 1906. The inventor of this carton was G.W. Maxwell. However, it was in 1915 that John Van Wormer of Toledo, Ohio, was granted the first patent for the first \"paper bottle,\" which was the first folded blank box for holding milk. He called it the \"Pure-Pak.\" The milk carton was original in the sense that it could be folded, glued, filled with milk, and sealed at a dairy farm. The early 1960s brought many automated systems to help with production of repeatable processes. It was at that time that an engineer from Detroit, Michigan, named Mario Lepore developed a machine to fold and seal a gable top paper carton. \n\nAn early American packaging industry pioneer was the Kieckhefer Container Company, which was run by John W. Kieckhefer. The company excelled in the use of fibre shipping containers, which especially included the paper milk carton. In 1957, through an exchange of stock, the Kieckhefer Container Co. holdings were merged with the Weyerhauser Timber Company of Tacoma, Washington.\n\nAlthough quite often shaped like a cuboid, it is not uncommon to find cartons lacking right angles and straight edges, as in squrounds used for ice cream.\n\nTetrahedrons and other shapes are available. Cartons with a hexagonal or octagonal cross sections are sometimes used for specialty items.\n\nCartons can be made from many materials: paperboard, duplex, white kraft, recycled and many more various plastics, or a composite. Some are \"food grade\" for direct contact with foods. Many cartons are made out of a single piece of paperboard. Depending on the need, this paperboard can be waxed or coated with polyethylene to form a moisture barrier. This may serve to contain a liquid product or keep a powder dry.\n\nIn art history, the carton (pronounced the French way) was a drawing on heavy pasteboard or paperboard, used as life-size design for the manufacture in an atelier of a valuable tapestry, such as a gobelin. During the weaving it hung behind the tapestry in the making, a time-consuming process thus in a creative sense simplified to 'mechanical' painting-by-numbers.\n\nAs these were extremely valuable, often commanded by the very richest art-buyers, including princes who hung them in their palaces and even took them on their travels as prestigious displays of wealth, often with a visual message, especially the world-famous Flemish ateliers were deemed worthy to have cartons made by some of the greatest graphic artists of the time, including such celebrated painters as Rubens.\n\nIn the 1980s, milk cartons in the United States often printed photos of missing children with the hope that someone would recognize the photograph and provide information to police.\n\n\"Carton-pierre\" was a material used for the making of raised ornaments for wall and ceiling decoration. It is composed of the pulp of paper mixed with whiting (ground calcium carbonate) and glue, this being forced into plaster moulds backed with paper, and then removed to a drying room to harden. It is much stronger and lighter than common plaster-of-Paris ornaments, and is not so liable to chip or break if struck with anything.\n\n\n\n"}
{"id": "503252", "url": "https://en.wikipedia.org/wiki?curid=503252", "title": "Collapse action", "text": "Collapse action\n\nCollapse action is a device behaviour that snaps a switch into place, usually using a bistable element. When flipping a light switch, strain on one spring increases until it flips position, pulling down the switch. Collapse action allows one to remove their hand from the switch without risk of it falling to the down position, as the force needed to overcome the resistance is too great. The action also does not exert force in the lower position, avoiding a spontaneous rise to the up position.\n\n"}
{"id": "2092835", "url": "https://en.wikipedia.org/wiki?curid=2092835", "title": "Comb drive", "text": "Comb drive\n\nComb-drives are actuators, often used as linear actuators electrostatic forces that act between two electrically conductive combs. Comb drive actuators typically operate at the micro- or nanometer scale and are generally manufactured by bulk micromachining or surface micromachining a silicon wafer substrate.\n\nThe attractive electrostatic forces are created when a voltage is applied between the static and moving combs causing them to be drawn together. The force developed by the actuator is proportional to the change in capacitance between the two combs, increasing with driving voltage, the number of comb teeth, and the gap between the teeth. The combs are arranged so that they never touch (because then there would be no voltage difference). Typically the teeth are arranged so that they can slide past one another until each tooth occupies the slot in the opposite comb.\n\nRestoring springs, levers, and crankshafts can be added if the motor's linear operation is to be converted to rotation or other motions.\n\nThe force can be derived by first starting with the energy stored in a capacitor and then differentiating in the direction of the force. The energy in a capacitor is given by:\n\nformula_1\n\nformula_2\n\nUsing the capacitance for a parallel plate capacitor, the force is:\n\nformula_3\n\nformula_4 = applied electric potential, \nformula_5 = relative permittivity of dielectric, \nformula_6 = permittivity of free space (8.85 pF/m),\nformula_7 = total number of fingers on both sides of electrodes, \nformula_8 = thickness in the out of plane direction of the electrodes, \nformula_9 = gap between electrodes.\n\n• rows of interlocking teeth\n• half fixed\n• half part of movable assembly\n• electrically isolated\n• electrostatic attraction/ repulsion\n– CMOS drive voltage\n• many teeth increased force\n– typically 10μm long and strong\n\nComb drives cannot scale to large gap distances (equivalently actuation distance), since development of effective forces at large gaps distances would require high voltages—therefore limited by electrical breakdown. More importantly, limitations imposed by gap distance limits the actuation distance.\n\n"}
{"id": "273543", "url": "https://en.wikipedia.org/wiki?curid=273543", "title": "Consumer Technology Association", "text": "Consumer Technology Association\n\nThe Consumer Technology Association (CTA), is a standards and trade organization representing more than 2,200 consumer technology companies in the United States. CTA works to influence public policy, holds events such as CES and CES Asia, conducts market research, and helps its members and regulators implement technical standards. CTA is led by President and CEO Gary Shapiro.\n\nCES is a major technology trade show held each January in Las Vegas. The CTA-sponsored show typically hosts previews of products and new product announcements.\n\nThe first CES was held in 1967 in New York City. It was a spinoff from the Chicago Music Show, which until then had served as the main event for exhibiting consumer electronics. The event had 17,500 attendees and over 100 exhibitors; the kickoff speaker was Motorola chairman Bob Galvin. From 1978 to 1994, CES was held twice each year: once in January in Las Vegas known as \"Winter Consumer Electronics Show\" (WCES) and once in June in Chicago, known as \"Summer Consumer Electronics Show\". In 1998, the show changed to a once-a-year format with Las Vegas as the location. CES is one of the largest and longest-running trade shows held in Las Vegas, taking up to 17 days to set up, run and break down.\n\nCES Asia is owned and produced by the International CES (Shanghai) Exhibition Co. Ltd., a wholly foreign-owned enterprise by the Consumer Technology Association (CTA), and is co-produced by Shanghai Intex Exhibition Co., Ltd (Shanghai Intex). Special co-organizers for CES Asia are the China Chamber of Commerce for Import and Export of Machinery and Electronic Products (CCCME) and the China Electronic Chamber of Commerce (CECC). CES Asia serves as a platform for both Chinese and American companies to introduce new products into the Asian marketplace\n\nShapiro is president and CEO of CTA. Shapiro has worked for CTA since 1979, when he was still a law student. Shapiro is also chairman of the Home Recording Rights Coalition. As chairman of the coalition, Shapiro has testified often before Congress and has helped ensure the growth of the video rental market, VCRs, home computers, and audio- recording equipment, including MP3 technology. Shapiro is also the author of the best- selling book, \"The Comeback: How Innovation Will Restore the American Dream\".\n\nShapiro holds a law degree from the Georgetown University Law Center. He is also a Phi Beta Kappa graduate of the Binghamton University, where he majored in economics and psychology. Shapiro was an associate at the law firm of Squire, Sanders and Dempsey. He also worked as a legislative aide on Capitol Hill.\n\nDavid Hagan currently serves as Chairman of the Board.\n\n\"i3,\" CTA's flagship magazine, is published six times a year and focuses on innovation in technology, policy and business as well as the entrepreneurs, industry leaders and startups that grow the consumer technology industry. The magazine has a circulation of 38,600 (print and digital) and has won a number of awards including the Eddie Award, Full Issue (N/D 2017), FOLIO, Top 25 Tabbie Best Issue Award (J/A 2017).\n\nThe CTA Corporate Report is published every year and covers CTA's accomplishments and assesses trends relevant to the consumer electronics industry. The Corporate Report won a Platinum Award from the League of American Communications Professionals.\n\n\"Digital America\" is the CTA's annual comprehensive report on the state of the consumer electronics industry in the United States. \"Digital America\" includes market research, analysis of new and existing technology, industry history, and other detailed information.\n\nCTA has several awards programs for industry leaders, inventors, products, and technologies.\n\nSince 1976, the Innovations Design and Engineering Awards has given consumer technology manufacturers and developers an opportunity to have their newest products judged by a panel of designers, engineers and members of the trade press. The winning products are then showcased each year at CES, also produced by CTA.\n\nTo recognize the contributions of the \"true pioneers\" of the consumer electronics industry, CTA created the Consumer Technology Hall of Fame, first launched at the 2000 International CES. Each year a new class of inventors, engineers, business leaders, retailers and journalists is inducted.\n\nCTAPAC is CTA's political action committee. About CTAPAC, CTA says, \"The Consumer Technology Association (CTA)™ Political Action Committee (CTAPAC) protects your freedom to build and sell consumer technology products.\" With regard to CEAPAC's funding, CEA says, \"CTAPAC is funded solely through voluntary, personal contributions from the executive and administrative personnel of CTA's corporate members — people like you. Corporate contributions are prohibited under federal law.\"\n\nIn 2009, CTA established a charitable foundation dedicated to providing seniors and the disabled with technology in order to enhance their quality of life. Selfhelp Community Services, an eldercare service organization, in New York City received the first grant issued by what was originally called the CEA Foundation. The grant was dedicated to reducing social isolation and providing better access to community services among homebound seniors using computer and internet technology.\n\nThe CTA maintains a Hall of Fame, to which notable contributors to the field of consumer electronics are named.\n\nCTA originally started as the Radio Manufacturers Association (RMA) in 1924. In 1950, it changed its name to Radio-Television Manufacturers Association (RTMA). In 1953, it changed its name to Radio-Electronics-Television Manufacturers Association (RETMA). It was then the Electronic Industries Association (EIA) from 1957 to 1998, when it became the Electronic Industries Alliance. In 1995, EIA's Consumer Electronics Group (CEG) became the Consumer Electronics Manufacturers Association (CEMA). In 1999, President Gary Shapiro announced the trade group's name change from CEMA to the Consumer Electronics Association (CEA) and became an independent sector of the Electronic Industries Alliance (EIA). The name of CEA was changed to Consumer Technology Association (CTA) in November 2015.\n"}
{"id": "21395468", "url": "https://en.wikipedia.org/wiki?curid=21395468", "title": "Consumer relationship system", "text": "Consumer relationship system\n\nConsumer relationship systems (CRS) are specialized customer relationship management (CRM) software applications that are used to handle a company's dealings with its customers.\n\nCurrent consumer relationship systems integrate the software with telephone and call recording systems as well as with corporate systems for input and reporting. Customers can provide input from the company's website directly into the CRS. These systems are popular because they can deliver the 'voice of the consumer' that contributes to product quality improvement and that ultimately increases corporate profits.\n\nConsumer relationship systems that provide automated support as well as advanced systems may have artificial intelligence (AI) interfaces that can extract and analyse data collected, or handle basic questions and complaints.\n\nThe first CRS was developed in the 1980s. In 1981 Michael Wilke and Robert Thornton founded Wilke/Thornton, Inc in Columbus, Ohio, to develop new CRS software.\n\n"}
{"id": "26955741", "url": "https://en.wikipedia.org/wiki?curid=26955741", "title": "Continuous availability", "text": "Continuous availability\n\nContinuous Availability is an approach to computer system and application design that protects users against downtime, whatever the cause and ensures that users remain connected to their documents, data files and business applications. Continuous availability describes the information technology methods to ensure business continuity. \n\nIn early days of computing, availability was not considered business critical. With the increasing use of mobile computing, global access to online business transactions and business-to-business communication, continuous availability is increasingly important based on the need to support customer access to information systems.\n\nSolutions to continuous availability exists in different forms and implementations depending on the software and hardware manufacturer. The goal of the discipline is to reduce the user or business application downtime, which can have a severe impact on business operations. Inevitably, such downtime can lead to loss of productivity, loss of revenue, customer dissatisfaction and ultimately can damage a company's reputation.\n\nThe terms high availability, continuous operation, and continuous availability are generally used to express how available a system is. The following is a definition of each of these terms.\n\nHigh availability\nrefers to the ability to avoid unplanned outages by eliminating single points of failure.\nThis is a measure of the reliability of the hardware, operating system, middleware, and database manager software. \nAnother measure of high availability is the ability to minimize the effect of an unplanned outage by masking the outage from the end users. \nThis can be accomplished by providing redundancy or quickly restarting failed components.\n\nAvailability is usually expressed as a percentage of uptime in a given year:\n\nWhen defining such a percentage it needs to be specified if it applies to the hardware, the IT infrastructure or the business application on top.\n\nContinuous operation refers to the ability to avoid planned outages. \nFor continuous operation there must be ways to perform necessary administrative work, like hardware and software maintenance while the business application remains available to the end users. This is accomplished by providing multiple servers and switching end users to an available server at times when one server is made unavailable. \nNote that a system running in continuous operation is not necessarily operating with high availability because an excessive number of unplanned outages could compromise this.\n\nContinuous availability combines the characteristics of high availability and continuous operation to provide the ability to keep the business application running without any noticeable downtime.\n\nPlanned outages are deliberate and are scheduled at a convenient time. These involve such activities as:\n- Hardware installation or maintenance\n- Software maintenance or upgrades of the operating system, the middleware, the database server or the business application\n- Database administration such as offline backup, or offline reorganization\n\nUnplanned outages are unexpected outages that are caused by the failure of any system component. \nThey include hardware failures, software issues, or people and process issues.\n\nVarious commercially viable examples exist for hardware/software implementations. These include:\n\n\n"}
{"id": "38972489", "url": "https://en.wikipedia.org/wiki?curid=38972489", "title": "Corps of Mining Engineers", "text": "Corps of Mining Engineers\n\nThe Corps of Mining Engineers was a militarized organization founded in Imperial Russia on 1 January 1834 to manage mining and oil extraction.\n\nYegor Kankrin, the Minister of Finance was the first head of the organization, with Major General Konstantin Chevkin acting as Chief of staff. \n"}
{"id": "4448948", "url": "https://en.wikipedia.org/wiki?curid=4448948", "title": "DSSP (imaging)", "text": "DSSP (imaging)\n\nDSSP stands for \"digital shape sampling\" and \"processing\". It is an alternative and often preferred way of describing \"reverse engineering\" software and hardware. The term originated in a 2005 Society of Manufacturing Engineers' \"Blue Book\" on the topic, which referenced numerous suppliers of both scanning hardware and processing software.\n\nDSSP employs various 3D scanning methods, including laser scanners, to acquire thousands to millions of points on the surface of a form and then software from a variety of suppliers to convert the resulting \"point cloud\" into forms useful for inspection, computer-aided design, visualization and other applications. It may also employ volumetric methods of scanning, such as digital tomography.\n\nSome common applications include CAI (computer-aided inspection), creation of 3D CAD models from scanned data, medical applications, 3D imaging for Web 2.0 applications, and the restoration of culturally significant artifacts; as well as conventional reverse engineering for creating replacement parts.\n\nThe term 'reverse engineering' itself has acquired some notoriety when the technology has been used to copy others' designs.\n\nThe term 'laser scanning' has also been used somewhat interchangeably for DSSP. However, there are two problems with the term as a broad description of the field. First, it is only one of many alternative scanning technologies. Second, it misses the essential role of processing software in converting point cloud data into useful forms.\n\nIn some ways, DSSP is a 3D analog to DSP (digital signal processing) in that the software attempts to extract a clear and accurate 3D image from point data that may include noise. The notion of 'shape sampling' embedded in the term also acknowledges that, as in many measurement processes, the accuracy of the 3D data will depend upon the number and accuracy of points sampled.\n\nThe speed and accuracy of both scanners to acquire data and software algorithms to extract useful data has dramatically increased in recent years. The amount of data capturing capability has also increased many fold, due to the advances in the camera technology and faster , more powerful computers. As some the limitations of the technology are eliminated and costs reduced, more uses are appearing.\n"}
{"id": "9246776", "url": "https://en.wikipedia.org/wiki?curid=9246776", "title": "Edison, the Man", "text": "Edison, the Man\n\nEdison, the Man is a 1940 biographical film depicting the life of inventor Thomas Edison, who was played by Spencer Tracy. Hugo Butler and Dore Schary were nominated for the Academy Award for Best Writing, Original Story for their work on this film. However, much of the film's script fictionalizes or exaggerates the real events of Edison's life.\n\nThe film was the second of a complementary pair of Edison biopics released by Metro-Goldwyn-Mayer in 1940. \"Young Tom Edison\" starring Mickey Rooney was released first and told the story of Edison's youth.\n\nIn 1869, anxious to be more than a tramp telegraph operator, Edison (Spencer Tracy) travels to New York at the prompting of an old friend, Bunt Cavatt (Lynne Overman). He goes to work for Mr. Els (Henry Travers). He tries to persuade financier Mr. Taggart (Gene Lockhart) to fund the development of his inventions, but Taggart has no interest in financing “green electrical workers”. However, General Powell (Charles Coburn), the president of Western Union, does.\n\nEdison eventually sells an invention to Taggart and Powell for $40,000, enabling him to get married and open his own “invention factory” at Menlo Park. In the next few years, he perfects the phonograph with his devoted staff.\n\nTrouble arises when Bunt brags to reporters that Edison has invented the electric light. Since he hasn't yet, he is condemned by the scientific community (encouraged by Taggart, whose gas stocks are threatened by the announcement). Edison “leaves science behind”, and with a Herculean trial-and-error effort, finally succeeds in inventing a practical electric light. His subsequent plans to light New York are again hindered by Taggart, who arranges it so that Edison is only given six months to complete the entire task. Nevertheless, Edison finishes the job just in time.\n\n\n“I’m an inventor. I can’t be told what to do. I’ve got to do the things I want to do. I work with ideas, visionary things. Nobody—not even I—knows how useful they’re going to be or how profitable until I had a chance to work them out in my own way.”\n\n“You think you’re nothing but wood and metal and glass. But you’re not: you’re dreams and hard work and heart. You’d better not disappoint us.”\n\n“It’s not the money wrapped up in the laboratory, it’s the lives wrapped up in the laboratory. It’s come to mean everything that I ever set out to do. It means a weekly paycheck for all my men. It means home, shelter, clothing, and food for lots of families.”\n\n“He hasn’t got a darn thing but I like to hear him talk that way.”\n\nBosley Crowther of \"The New York Times\" praised Tracy's performance for bringing \"human and vital substantiality\" to the role, but criticized the film for its historical inaccuracies: \"When Metro deliberately distorts certain important details in Edison's career and boldly invents others—even though it were done with the sanction of his family—the question arises as to whether this creation is intended to be a reliable portrait of the great inventor or just another fellow who looks something like him. Frankly, we think it wiser to regard it in the second light.\" \"Variety\" called the film \"a top-bracket picture from Metro that takes its place among the more important biographical contributions by the screen.\" \"Harrison's Reports\" wrote: \"As in \"Young Tom Edison\", this offers good entertainment for both young and old, in spite of the fact that the action is not particularly fast-moving. In a way it is even better than the first picture, for the older Edison is more interesting, and the work he accomplishes is far more exciting.\" \"Film Daily\" called it \"one of the truly memorable pictures of the year\" and predicted it would \"command most serious consideration\" when it came time to vote for the Academy Awards. John Mosher of \"The New Yorker\" wrote that even though the story of Edison's career was \"not really screen material ... more than one might think, its interest mounts with the advance of the picture and some actual excitement is achieved at last.\"\n\nThe film is recognized by American Film Institute in these lists:\n\nAccording to MGM records the film earned $1,152,000 in the US and Canada and $635,000 elsewhere resulting in a profit of $143,000.\n\n"}
{"id": "4385464", "url": "https://en.wikipedia.org/wiki?curid=4385464", "title": "Electronic tuner", "text": "Electronic tuner\n\nIn music, an electronic tuner is a device that detects and displays the pitch of musical notes played on a musical instrument. \"Pitch\" is the highness or lowness of a musical note, which is typically measured in Hertz. Simple tuners indicate—typically with an analog needle-dial, LEDs, or an LCD screen—whether a pitch is lower, higher, or equal to the desired pitch. In the 2010s, software applications can turn a smartphone, tablet, or personal computer into a tuner. More complex and expensive tuners indicate pitch more precisely. Tuners vary in size from units that fit in a pocket to 19\" rack-mount units. Instrument technicians and piano tuners typically use more expensive, accurate tuners.\n\nThe simplest tuners detect and display tuning only for a single pitch—often \"A\" or \"E\"—or for a small number of pitches, such as the six used in the standard tuning of a guitar (E,A,D,G,B,E). More complex tuners offer chromatic tuning for all 12 pitches of the equally tempered octave. Some electronic tuners offer additional features, such as pitch calibration, temperament options, the sounding of a desired pitch through an amplifier plus speaker, and adjustable \"read-time\" settings that affect how long the tuner takes to measure the pitch of the note.\n\nAmong the most accurate tuning devices, strobe tuners work differently than regular electronic tuners. They are stroboscopes that flicker a light at the same frequency as the note. The light shines on a wheel that spins at a precise speed. The interaction of the light and regularly-spaced marks on the wheel creates a stroboscopic effect that makes the marks for a particular pitch appear to stand still when the pitch is in tune. These can tune instruments and audio devices more accurately than most non-strobe tuners. However, mechanical strobe units are expensive and delicate, and their moving parts require periodic servicing, so they are used mainly in applications that require higher precision, such as by professional instrument makers and repair experts.\n\nRegular electronic tuners contain either an input jack for electric instruments (usually a 1/4\" patch cord input), a microphone, or a clip-on sensor (e.g., a piezoelectric pickup) or some combination of these inputs. Pitch detection circuitry drives some type of display (an analog needle, an LCD simulated image of a needle, LED lights, or a spinning translucent disk illuminated by a strobing backlight). Some tuners have an output, or through-put, so the tuner can connect 'in-line' from an electric instrument to an instrument amplifier or mixing console. Small tuners are usually battery powered. Many battery powered tuners also have a jack for an optional AC power supply.\nMost musical instruments generate a fairly complex waveform. It contains a number of harmonic partials, including the fundamental frequency (which a typical listener perceives as the pitch of the note) and additional \"harmonics\" (also called \"partials\" or \"overtones\"). Each instrument produces different ratios of harmonics, which is what makes notes of the same pitch played on different instruments (e.g., an A 440 Hz note played on oboe, violin or electric guitar) sound different. As well, this waveform constantly changes. This means that for non-strobe tuners to be accurate, the tuner must process a number of cycles and use the pitch average to drive its display. Background noise from other musicians or harmonic overtones from the musical instrument can impede the electronic tuner from \"locking\" onto the input frequency. This is why the needle or display on regular electronic tuners tends to waver when a pitch is played. Small movements of the needle, or LED, usually represent a tuning error of 1 cent. Typical accuracy of these types of tuners is around +/- 3 cents. Some inexpensive LED tuners may drift by as much as +/- 9 cents.\n\n\"Clip-on\" tuners typically attach to instruments with a spring-loaded clip that has a built-in contact microphone. Clipped onto a guitar headstock or violin scroll, these sense pitch even in loud environments, for example when other people are tuning.\n\nSome guitar tuners fit into the instrument itself. Typical of these are the Sabine AX3000 and the \"NTune\" device. The NTune consists of a switching potentiometer, a wiring harness, illuminated plastic display disc, a circuit board and a battery holder. The unit installs in place of an electric guitar's existing volume knob control. The unit functions as a regular volume knob when not in tuner mode. To operate the tuner, the player pulls the volume knob up. The tuner disconnects the guitar's output so the tuning process is not amplified. The lights on the illuminated ring, under the volume knob, indicate the note being tuned. When the note is in tune a green \"in tune\" indicator light illuminates. After tuning is complete the musician pushes the volume knob back down, disconnecting the tuner from the circuit and re-connecting the pickups to the output jack.\n\nGibson guitars released a guitar model in 2008 called the \"Robot Guitar\"—a customized version of either the Les Paul or SG model. The guitar is fitted with a special tailpiece with in-built sensors that pick up the frequency of the strings. An illuminated control knob selects different tunings. Motorized tuning machines on the headstock automatically tune the guitar. In \"intonation\" mode, the device displays how much adjustment the bridge requires with a system of flashing LEDs on the control knob.\n\nThe first automated guitar tuner was invented by JD Richard in 1982 while studying Electrical Engineering at the University of New Brunswick, New Brunswick Canada. This tuner was based on phase-locked-looped feedback design that listened to the frequency of the string and turned a stepper motor (with a 400/1 gear ratio) attached to the tuning peg of the guitar. This first design never went into production although the thesis paper can still be obtained at the university. (Ref: UNB, Canada, Department of Electrical Engineering, Paper \"Automated Guitar Tuner by J.D. Steven Richard - April 1982\"; Supervisor Dr. J.P. Burgess)\n\nA needle, LCD or regular LED type tuner uses a microprocessor to measure the average period of the waveform. It uses that information to drive the needle or array of lights. When the musician plays a single note, the tuner senses the pitch. The tuner then displays the pitch in relation to the desired pitch, and indicates whether the input pitch is lower, higher, or equal to the desired pitch. With needle displays, the note is in tune when the needle is in a 90° vertical position, with leftward or rightward deviations indicating that the note is flat or sharp, respectively. Tuners with a needle are often supplied with a backlight, so that the display can be read on a darkened stage.\n\nFor block LED or LCD display tuners, markings on the readout drift left if the note is flat and right if the note is sharp from the desired pitch. If the input frequency is matched to the desired pitch frequency the LEDs are steady in the middle and an 'in tune' reading is given.\n\nSome LCDs mimic needle tuners with a needle graphic that moves in the same way as a genuine needle tuner. Somewhat misleadingly, many LED displays have a 'strobe mode' that mimics strobe tuners by scrolling the flashing of the LEDs cyclically to simulate the display of a true strobe. However, these are all just display options. The way a regular tuner 'hears' and compares the input note to a desired pitch is exactly the same, with no change in accuracy. For more on how strobe tuners work see the dedicated section.\n\nThe least expensive models only detect and display a small number of pitches, often those pitches that are required to tune a given instrument (e.g., E, A, D, G, B, E of standard guitar tuning). While this type of tuner is useful for bands that only use stringed instruments such as guitar and electric bass, it is not that useful for tuning brass or woodwind instruments. Tuners at the next price point offer chromatic tuning, the ability to detect and assess all the pitches in the chromatic scale (e.g., C, C, D, D, etc.). Chromatic tuners can be used for B and E brass instruments, such as saxophones and horns. Many models have circuitry that automatically detects which pitch is being played, and then compares it against the correct pitch. Less expensive models require the musician to specify the target pitch via a switch or slider. Most low- and mid-priced electronic tuners only allow tuning to an equal temperament scale.\n\nElectric guitar and electric bass players who perform concerts may use electronic tuners built into an effects pedal, often called a \"stomp box\". These tuners have a rugged metal or heavy-duty plastic housing and a foot-operated switch to toggle between the tuner and a bypass mode. Professional guitarists may use a more expensive version of the LED tuner mounted in a rack-mount case with a larger range of LEDs for more accurate pitch display. Many models let the user select reference pitches other than A440. On many electronic tuners, the user can select a different note—useful for, for example, dropping a guitar's tuning to a lower pitch (e.g., Dropped tuning). Some models are adjustable standards other than A=440. This is useful to some Baroque musicians who play period instruments at lower reference pitches—such as A=435. Some higher-priced electronic tuners support tuning to a range of different temperaments—a feature useful to some guitarists and harpsichord players.\n\nSome expensive tuners also include an on-board speaker that can sound notes, either to facilitate tuning by ear or to act as a pitch reference point for intonation practice. Some expensive tuners provide an adjustable read time that controls at what time interval the circuitry assesses pitch. The combination of all the above features makes some tuners preferable for tuning instruments in an orchestra. These are sometimes called \"orchestral tuners\".\n\nA clip-on tuner clips on to an instrument—such as onto the headstock of a guitar or the bell of a trombone. A vibration sensor built into the clip transmits the instrument vibrations to the tuning circuitry. The absence of a microphone makes these tuners immune to background noise, so musicians can tune in noisy environments, including while other musicians are tuning. The clip-on tuner was invented in 1995 by Mark Wilson of OnBoard Research Corporation, which marketed it as The Intellitouch Tuner Model PT1.\n\nMany chromatic and guitar tuner apps are available for Android and iOS smartphones. Many are free to download and install.\n\nStrobe tuners (the popular term for stroboscopic tuners) are the most accurate type of tuner. There are three types of strobe tuners: the mechanical rotating disk strobe tuner, an LED array strobe in place of the rotating disk, and \"virtual strobe\" tuners with LCDs or ones that work on personal computers. A strobe tuner shows the difference between a reference frequency and the musical note being played. Even the slightest difference between the two shows up as a rotating motion in the strobe display. The accuracy of the tuner is only limited by the internal frequency generator. The strobe tuner detects the pitch either from a TRS input jack or a built-in or external microphone connected to the tuner.\n\nThe first strobe tuner dates back to 1936 and was originally made by the Conn company; it was called the Stroboconn and was produced for approximately 40 years. However, these strobes are now mainly collector pieces. They had 12 strobe discs, driven by one motor. The gearing between discs was a very close approximation to the 12th root of two ratio. This tuner had an electrically driven temperature-compensated tuning fork; the electrical output of this fork was amplified to run the motor. The fork had sliding weights, an adjustment knob, and a dial to show the position of the weights. These weights permitted setting it to different reference frequencies (such as A = 435 Hz), although over a relatively narrow range, perhaps a whole tone. When set at A = 440 Hz the tuning fork produced a 55 Hz signal, which drove the four-pole 1650 RPM synchronous motor to which the A disc was mounted. (The other discs were all gear-driven off of this one.) Incoming audio was amplified to feed a long neon tube common to all 12 discs. Wind instrument repair people liked this tuner because it needed no adjustment to show different notes. Anyone who had to move this tuner around was less inclined to like it because of its size and weight: two record-player-sized cases of 30-40 pounds each.\nThe best known brand in strobe tuner technology is Peterson Tuners who in 1967 marketed their first strobe tuner, the \"Model 400\". Other companies, such as Sonic Research, TC Electronic, and Planet Waves, sell highly accurate LED-based true strobe tuners. Other LED tuners have a 'strobe mode' that emulates the appearance of a strobe. However, the accuracy of these tuners in strobe mode, while sufficient for most tuning, is no better than in any other mode, as they use the same technique as any basic tuner to measure frequency, only displaying it in a way that imitates a strobe tuner.\n\nMechanical strobe tuners have a series of lamps or LEDs powered by amplified audio from the instrument; they flash (or strobe) at the same frequency as the input signal. For instance an 'A' played on a guitar's 6th string at the 5th fret has the frequency of 110 Hz when in tune. An 'A' played on the 1st string at the 5th fret vibrates at 440 Hz. As such, the lamps would flash either 110 or 440 times per second in the above examples. In front of these flashing lights is a motor-driven, translucent printed disc with rings of alternating transparent and opaque sectors.\n\nThis disc rotates at a fixed specific speed, set by the user. Each disc rotation speed is set to a particular frequency of the desired note. If the note being played (and making the lamps behind the disc flash) is at exactly the same frequency as the spinning of the disc, then the disc appears to be static (due to the persistence of vision) from the strobing effect. If the note is out of tune then the pattern appears to be moving as the light flashing and the disc rotation are out of sync from each other. The more out of tune the played note is, the faster the pattern seems to be moving, although in reality it always spins at the same speed for a given note. Many good turntables for vinyl disc records have stroboscopic patterns lit by the incoming AC power (mains). The power frequency, either 50 or 60 Hz, serves as the reference, although commercial power frequency sometimes changes slightly (a few tenths of a percent) with varying load. Unless reference and measured quantity are interchanged, the operating principle is the same; the turntable speed is adjusted to stop drifting of the pattern.\nAs the disc has multiple bands, each with different spacings, each band can be read for different partials within one note. As such, extremely fine tuning can be obtained, because the user can tune to a particular partial within a given note. This is impossible on regular needle, LCD or LED tuners. The strobe system is about 30 times more accurate than a quality electronic tuner, being accurate to 1/10 of a cent. Advertisements for the Sonic Research LED strobe claim that it is calibrated to ± 0.0017 cents and guaranteed to maintain an accuracy of ± 0.02 cents or 1/50 of a cent.\n\nStrobe units can often be calibrated for many tunings and preset temperaments and allow for custom temperament programming, stretched tuning, \"sweetened\" temperament tunings and Buzz Feiten tuning modifications. Due to their accuracy and ability to display partials even on instruments with a very short \"voice\" (e.g., notes of short duration), strobe tuners can perform tuning tasks that would be very difficult, if not impossible, for needle-type tuners. For instance, needle/LED display type tuners cannot track the signal to identify a tone of the Caribbean steelpan (often nicknamed the \"steeldrum\") due to its very short \"voice\". A tuner needs to be able to detect the first few partials for tuning such an instrument, which means that only a strobe tuner can be used for steelpan tuning. This is also true of the comb teeth used in mechanical musical instruments like Music Boxes and the like. In such cases a technician has to physically remove metal from the tooth to reach the desired note. The metal teeth only resonates briefly when plucked. Great accuracy is required as once the metal is cut or filed away, the lost material cannot be replaced. As such, the strobe-type tuners are the unit of choice for such tasks. Tuners with an accuracy of better than 0.2 cent are required for guitar intonation tuning.\n\nOne of the most expensive strobe tuners is the Peterson \"Strobe Center\", which has twelve separate mechanical strobe displays; one for each pitch of the equally tempered octave. This unit (about $3,500 US) can tune multiple notes of a sound or chord, displaying each note's overtone sub-structure simultaneously. This gives an overall picture of tuning within a sound, note or chord that is not possible with most other tuning devices. (The TC Electronic Polytune can display the pitch accuracy of up to six pre-selected notes.) It is often used for tuning complex instruments and sound sources, or difficult-to-tune instruments where the technician requires a very accurate and complete aural picture of an instrument's output. For instance, when tuning musical bells, this model displays several of the bell's partials (hum, second partial, tierce, quint and nominal/naming note) as well as the prime, and each of their partials, on separate displays. The unit is heavy and fragile, and requires a regular maintenance schedule. Each of the twelve displays requires periodic re-calibration. It can be used to teach students about note substructures, which show on the separate strobing displays.\n\nMechanical disc strobe tuners are expensive, bulky, delicate, and require periodic maintenance (keeping the motor that spins the disc at the correct speed, replacing the strobing LED backlight, etc.). For many, a mechanical strobe tuner is simply not practical for one or all of the above reasons. To address these issues, in 2001 Peterson Tuners added a line of non-mechanical electronic strobe tuners that have LCD dot-matrix displays mimicking a mechanical strobe disc display, giving a stroboscopic effect. In 2004 Peterson made a model of LCD strobe in a sturdy floor based \"stomp box\" for live on-stage use. Virtual strobe tuners are as accurate as standard mechanical disc strobe tuners. However, there are limitations to the virtual system compared to the disc strobes. Virtual strobes display fewer bands to read note information, and do not pick up harmonic partials like a disc strobe. Rather, each band on a virtual strobe represents octaves of the fundamental. A disc strobe provides \"one band correspondence\"—each band displays a particular frequency of the note being played. On the virtual strobe system, each band combines a few close frequencies for easier reading on the LCD. This is still extremely accurate for intoning and tuning most instruments—but, as of this writing, no virtual strobe tuner provides detailed information on partials.\n\nSonic Research and Planet Waves both released a true-strobe with a bank of LEDs arranged in a circle that gives a strobing effect based upon the frequency of the input note. Both LCD and LED display true strobes do not require mechanical servicing and are much cheaper than the mechanical types. As such, they are a popular option for musicians who want the accuracy of a strobe without the high cost and the maintenance requirements. However, LED strobe displays offer no information about the harmonic structure of a note, unlike LCD types, which do offer four bands of consolidated information.\nPeterson released a PC-based virtual strobe tuner in 2008 called \"StroboSoft\". This computer software package has all the features of a virtual strobe, such as user-programmable temperaments and tunings. To use this tuner, a musician must have a computer next to the instrument to be tuned. An alternative is the PC-based strobe tuner TB Strobe Tuner with fewer functions.\nIn 2009 Peterson Tuners released a VirtualStrobe tuner as an end-user application add-on for Apple's iPhone and iPod Touch where the application is bought cheaply as a download and installed. There exists a special 1/4\" TRS jack adapter for connecting an electric instrument to the iPhone, a notable achievement in strobe tuner technology, which has made such tuning widely available. In order to use it, however, a compatible iPod or iPhone must already be on hand.\n\nAs both mechanical and electronic strobes are still more expensive and arguably more difficult to use in order to achieve the desired results than ordinary tuners, their use is usually limited to those whose business it is accurately to intone and tune pianos, harps, and early instruments (e. g. harpsichords) on a regular basis: luthiers, instrument restorers and technicians – and instrument enthusiasts. These tuners make the intonation process more precise.\n\nIn classical music, there is a longstanding tradition to tune \"by ear\", by adjusting the pitch of instruments to a reference pitch. In an orchestra, the oboe player gives a 440 Hz \"A\", and the different instrument sections tune to this note. In chamber music, either one of the woodwind players gives an \"A\", or if none is present, one of the string players, usually the first violinist, bows his or her open \"A\" string. If an orchestra is accompanying a piano concerto, the first oboist takes the \"A\" from the piano and then plays this pitch for the rest of the orchestra.\n\nDespite this tradition of tuning by ear, electronic tuners are still widely used in classical music. In orchestras the oboist often uses a high-end electronic tuner to ensure that her/his \"A\" is correct. As well, other brass or woodwind players may use electronic tuners to ensure that their instruments are correctly tuned. Classical performers also use tuners off-stage for practice purposes or to check their tuning (or, with the further aid of a speaker, to practice ear training). Electronic tuners are also used in opera orchestras for offstage trumpet effects. In offstage trumpet effects, trumpet players performs a melody from the backstage or from a hallway behind the stage, creating a haunting, muted effect. Since trumpet players cannot hear the orchestra, they cannot know whether or not their notes are in tune with the rest of the ensemble; to resolve this problem, some trumpet players use a high-end, sensitive tuner so that they can monitor the pitch of their notes.\n\nPiano tuners, harp makers and the builders and restorers of early instruments, e.g. harpsichords, use high-end tuners to assist with their tuning and instrument building. Even piano tuners who work mostly \"by ear\" may use an electronic tuner to tune just a first key on the piano, e. g. the a' to 440 Hz, after which they proceed by means of octaves, approximate fifths and approximate fourths to tune the others. (In the twelve-tone equal temperament system dominant in classical and Western music, all intervals except the octave are slightly \"mistuned\" or compromised compared to more consonant just intervals.) They may also use electronic tuners to get a very out-of-tune piano roughly in pitch, after which point they tune by ear. Electronic tuning devices for keyboard instruments are for various reasons generally much more complex and therefore expensive than in the case of other widely used instruments.\n\nIn popular music, amateur and professional bands from styles as varied as country and heavy metal use electronic tuners to ensure that the guitars and electric bass are correctly tuned. In popular music genres such as rock music, there is a great deal of stage volume due to the use of drums and guitar amplifiers, so it can be difficult to tune \"by ear\". Electronic tuners are helpful aids at jam sessions where a number of players are sharing the stage, because it helps all of the players to have their instruments tuned to the same pitch, even if they have come to the session halfway through. Tuners are helpful with acoustic instruments, because they are more affected by temperature and humidity changes. An acoustic guitar or upright bass that is perfectly in tune backstage can change in pitch under the heat of the stage lights and from the humidity from thousands of audience members.\n\nTuners are used by guitar technicians who are hired by rock and pop bands to ensure that all of the band's instruments are ready to play at all times. Guitar technicians (often called guitar techs) tune all of the instruments (electric guitars, electric basses, acoustic guitars, mandolins, etc.) before the show, after they are played, and before they are used onstage. Guitar techs also retune instruments throughout the show. Whereas amateur musicians typically use a relatively inexpensive quartz tuner, guitar technicians typically use expensive, high-end tuners such as strobe tuners. Most strobe tuners, counter-intuitively, also use quartz crystal oscillators as time references, although the responses are processed differently by the different units.\n\nStrobe tuners are used in the tuning of bells, which require accurate tuning of many partials. The removal of metal from various parts of the bell shape is by a tuning lathe, and once too much metal has been removed it cannot be reversed. Hence accurate approach to the desired tuning partial is essential to prevent overshoot.\n\n"}
{"id": "26550202", "url": "https://en.wikipedia.org/wiki?curid=26550202", "title": "FIXatdl", "text": "FIXatdl\n\nFIX Algorithmic Trading Definition Language, better known as FIXatdl, is a standard for the exchange of meta-information required to enable algorithmic trading activity within the financial markets. It works in tandem with the Financial Information eXchange (FIX) protocol which is the lingua franca of electronic trading in the securities market.\n\nPrior to the mid-nineties, virtually all trading of securities was transacted over the phone, but with the advent of FIX, trading moved steadily over to electronic means. The FIX protocol is used to communicate between sell-side and the buy-side Order Management Systems (OMS) to exchange orders and order execution information without human intervention, using standardised messages and workflows that are defined by the protocol.\nInitially, sell-side firms only provided access to their 'trading desks' via FIX, which meant that once an order arrived at the sell-side broker, it was handled by a human trader, at least at the start of its lifecycle. Subsequently, sell-side firms started to offer direct access via FIX to the exchanges/markets they were members of; this is known as direct market access (DMA). At this time, many sell-side firms had their own proprietary systems to trade automatically in the market, using algorithmic trading strategies, and over time they began to see that offering access to these trading strategies to the buy-side was a way to attract business and increase revenue.\n\nWhilst FIX is an extensible protocol, there were two challenges that arose as a result of sell-side firms offering access to their algorithmic trading strategies via FIX. The first was that each sell-side strategy had its own parameters that had to be included as part of the order, so every firm ended up requiring a different set of fields (known in FIX as \"tags\") to be included in the FIX message. This made life very difficult for the buy-side, and more particularly for their suppliers as adding new algorithms to their trading systems and managing all the different combinations of tags became a significant overhead for their development operations.\n\nThe second issue for the market was that each sell-side firm had a specific way they wanted their algorithms to be displayed on the buy-side OMS, with controls in the user interface arranged logically for easy order entry. Again this proved a challenge for the buy-side systems vendors, as each new screen for each sell-side broker required dedicated development and testing effort.\n\nTo tackle these issues, FIX Protocol Limited established the Algorithmic Trading Working Group in Q3 2004. The initial focus of the group was to solve the first of these issues, which it did by defining a new group of fields, the StrategyParametersGrp, made up of FIX tags 957 through 960 – these tags were formally introduced with the release of FIX 5.0 in Q4 2006. By allowing sell-side firms to include their proprietary fields in a repeating name-value pair structure, there was no requirement for OMS vendors to define specific FIX message structures for each sell-side trading destination.\n\nThis solution was not broadly adopted, in part because of the limited penetration of FIX 5.0 and in part due to the fact that firms already had working implementations in the market place that they were unwilling to change without good cause. Perhaps more importantly, it failed to solve what was the more substantial issue for the market, the complexity for the buy-side vendors resulting from lack of standardisation.\n\nThe idea of using an XML structure to describe the presentation of algorithm user interfaces and their accompanying parameters was firstly suggested within the working group by Daniel Clayden, then of JP Morgan Chase in a 2005 forum posting. Members of the working group developed this idea during 2006 and in January 2007 invited broader industry participation at a workshop to review their ideas. A specification was eventually produced and this began beta testing in July 2007. This specification became FIXatdl 1.0 which was approved by the FPL Global Technical Committee (GTC) on March 28, 2008.\n\nDespite some initial enthusiasm, overall Version 1.0 had a lacklustre reception by the marketplace. Some vendors saw an opportunity to provide services around the standard, such as ULLINK with their algorithm publication and management and tool UL AMS but whilst the major OMS vendors were irritated by the overhead of implementing new broker algorithms, they had grown to enjoy the revenue that they could command from both their customers and from brokers keen to get their algorithms onto buy-side desks.\n\nAlthough Version 1.0 was a major step forward, it had some significant limitations. In particular, the definition of the data to be transmitted and its presentation on the user interface were tightly bound together, limiting the flexibility sell-side brokers had in defining their algorithms. The 1.0 specification also afforded insufficient control in terms of user interface layouts.\nThe working group set out to address these limitations in what was to become Version 1.1 on the specification. The first major change was to split the definition of the data content from the presentation, defining what is referred to as a separate \"Data Contract\" made up of the algorithm parameters, their data types and supporting information such as minimum and maximum values. A separate section of the XML document is then concerned with the layout of the user interface, what controls to use for each parameter and where to place them on the screen. An XSD schema is provided to ensure that FIXatdl files are valid and well-formed.\n\nFIXatdl Version 1.1 was preliminarily approved by the GTC on February 9, 2010, when it entered a public comment period, and then finally approved on March 3, 2010. The specification was formally introduced to the market at the FPL’s Europe Middle East and Africa conference on March 23, 2010.\n\nA FIXatdl document can contain one or more strategy definitions. Within a strategy definition, there are four main sections as follows:\nFIXatdl documents should validate against the set of XSD schema provided by FPL. These schema are organised into the following four categories:\n\nVersion 1.1 supports 14 different user interface controls, which can be grouped as follows:\nControls are laid out using a hierarchy of panels (referred to as StrategyPanels), each of which can be horizontal or vertical in orientation. The picture on the right shows how the XML elements refer to the individual panels within a given layout.\n\nUnlike the previous version, Version 1.1 looks set to be widely accepted and adopted by the securities industry. Even at the end of 2009, there were already firms using the 1.1 standard, despite its pre-release status. Examples of firms supporting the FIXatdl standard include:\nThere are also open source Java and .NET implementations, atdl4j and Atdl4net respectively, which are both Version 1.1 compliant.\n\nThe question has often been asked, why doesn’t FIXatdl use an off-the-shelf user interface standard, such as Mozilla’s XUL, Microsoft’s Windows Presentation Foundation or Apache Flex? This is a valid question, but it seems that the authors of the specification wanted to maintain complete platform independence and adopting any one platform would risk damaging this proposition. Whilst lacking the degree of sophistication of some of these platforms, the current specification provides an acceptable degree of control in terms of user interface layout without being unduly restrictive. It remains to be seen how this design choice will pan out, and it does seem likely that further refinement of this part of the specification will be needed as adoption grows.\n\n\n"}
{"id": "1729000", "url": "https://en.wikipedia.org/wiki?curid=1729000", "title": "FOX-7", "text": "FOX-7\n\nFOX-7 or 1,1-diamino-2,2-dinitroethene (DADNE) is an insensitive high explosive compound. It was first synthesized in 1998 by the Swedish National Defence Research Institute (FOA).\n\nFOX-7 is similar to the insensitive chemical compound TATB, which is a benzene ring compound with three amino and three nitro groups. FOX-7 has a two-carbon backbone rather than a benzene ring, but the amino and nitro groups have similar effects in both cases according to published reports on sensitivity and chemical decay processes of FOX-7. FOX-7 is today produced by Eureco Bofors AB in Sweden.\n\nIts explosive properties appear extremely favorable; in addition to its insensitive properties, the detonation velocity of mixtures of 80% FOX-7 plus binders is as high as Composition B, and nearly pure FOX-7 based plastic bonded explosives are slightly superior to RDX. FOX-7 has been calculated to have a detonation velocity of 8,870 m/s.\n\nDue to its small-scale production, the cost of FOX-7 is relatively high. However, the production is based on commercial starting material and the synthesis is uncomplicated. The price is therefore predicted to fall as production scale increases. There is no current full scale use of FOX-7, but it is being tested at several military research centers. The need for less sensitive munitions is the most important driver for testing FOX-7.\n\n"}
{"id": "20810258", "url": "https://en.wikipedia.org/wiki?curid=20810258", "title": "Glossary of winemaking terms", "text": "Glossary of winemaking terms\n\nThis glossary of winemaking terms lists some of terms and definitions involved in making wine, fruit wine, and mead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossflow filtration: A highspeed form of microfiltration that has the wine flow across a membrane filter rather than through it.\n\n\n\n\n\n\nCuve Close Alternative name for the Charmat method of sparkling wine production.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "40736452", "url": "https://en.wikipedia.org/wiki?curid=40736452", "title": "Glynwood", "text": "Glynwood\n\nGlynwood is a nonprofit organization in Cold Spring, New York. Its mission is to help save farming in the Northeast, particularly the Hudson Valley, by strengthening farm communities and regional food systems. Located in the Hudson Valley, the Glynwood Center strives to get small- and mid-sized farmers to thrive, \"on the land and in the marketplace\".\n\nA non-profit organization, Glynwood finds it important to have a healthier regional food system, help the economy, conserve the natural environment, and promote a sense of place. They make it their mission to ensure that farming thrives in the Hudson Valley by developing and implementing core programs that enhance the viability of farming in our region. Currently, Glynwood's vision is \"a Hudson Valley defined by food: where farmers prosper, food entrepreneurs succeed, residents are nourished and visitors are inspired.\"\n\nGlynwood works their own farmland, testing, innovating and teaching techniques that demonstrate the economic viability of sustainable farming practices. Glynwood is composed of three core programs in addition to its farm that fulfill their mission to promote sustainable farming. A few projects they have are \"The Cider Project\", dedicated to preserving the apple orchards in the Hudson Valley by promoting the production of hard cider and apple spirits, the Hudson Valley Farm Business Incubator, in New Paltz, New York, and \"Keep Farming Program\", devoted to empowering communities to save farming, and others.\n\nIn 1929, financier George Walbridge Perkins, Jr. and his family purchased land located in the Hudson Highlands now called Glynwood Farm. Mr. Perkins was a noted conservationist and his father George Walbridge Perkins was one of the creators of the Palisades Interstate Park Commission, which to date has conserved over 100,000 acres of parklands and historic sites. The name of the farm uses a combination of Mr. and Mrs. Perkins' first names, George and Lynn. Mr. Perkins' father's involvement in the Palisades Interstate Park Commission inspired the deep thought and conservation ethic that made Glynwood Farm more than just a farm.\n\nAfter the death of the owners, their family preserved Glynwood Farm, making it the site of a \"not-for-profit organization dedicated to rural conservation\". In 1995, the Open Space Institute purchased Glynwood Farm, a 957-acre estate, in an effort to preserve one of the last untouched woodlands in the lower Hudson River Valley. In this $6.25 million purchase, more than 700 acres of the purchase were added to the adjacent Clarence Fahnestock State Park, completing the effort by the Open Space Institute to \"expand the protected lands in the Hudson Highlands\" a mountain range bisected by the Hudson River. The purchase was made with a grant provided by the Lila Acheson and DeWitt Wallace Fund for the Hudson Highlands through a trust set up in 1983 to preserve unspoiled parts of the Hudson Valley by the founders of Reader's Digest. The 230-acre core of the estate, of cow pastures, farms and houses, remained a farm and became Glynwood Center.\n\nIn 1997, the Glynwood Center adopted the Countryside Exchange Program, a program introduced by the Countryside Institute. This program brought together international teams of volunteer professionals to work with communities on their most important issues, typically those that center on conservation and economic development. Communities would apply to participate in the Countryside Exchange and were chosen based on their depth of interest within the community, existing leadership and leadership skills, and the diversity of community members supporting the application. Once selected, the community would form a Local Organizing Committee (LOC) which would the implement the Exchange Team's suggestions based on their weekly report which consisted of its observations and ideas. The Countryside Exchanges, although not used anymore, still continues to provide the intellectual underpinnings for Glynwood's mission and programs.\n\nGlynwood serves as place where community leaders can seek information, support, and training on sustainable farming. Glynwood consists of different programs to promote their mission to \"save farming by strengthening farm communities and regional food systems\", the farm and a think tank that seeks innovative ways to sustainable farming.\n\nGlynwood's farm is located at its headquarters in Cold Spring, New York where they work with vegetable crops and livestock. The farm serves as a way for Glynwood to convey their mission by \"testing, innovation, and demonstrating sustainable practices, while also growing food\" for their community. Glynwood does this through their use of Conservation Goat Grazing and energy efficient farming. Conservation Goat Grazing is an innovative practice used by the farm which uses goats to eat any invasive shrub like the Multiflora Rose in effort to provide their sheep and cattle with usable pasture. Glynwood Farm also practices energy efficient farming through the use of an innovative radiant heating system in their greenhouse, electric tractors, and draft horses that help eliminate air pollution. Apart from practicing energy efficient farming, the Glynwood Farm also offers an apprentice program through the Collaborative Regional Alliance for Farmer Training that educates and provides a hands on experience for individuals interested in farming. The Glynwood farm sells their products through CSA shares, a farm store located on site, and online.\n\nGlynwood is composed of different core programs apart from the farm that fulfill their mission to promote sustainable farming. The \"Keep Farming\" program is a program aimed at helping communities strengthen their farm economy, protect their farmland and open space and prepare these communities for their future in farming. Glynwood's \"Keep Farming\" initiative has successfully helped communities in New York like Northampton. The Cider Project is a program in which Glynwood aims to increase profits, enhance production, and expand markets for apple orchards in the Hudson Valley. Through the Cider Project program, Glynwood is preserving apple orchards in the Hudson Valley by promoting hard cider and apple spirits. Glynwood launched Cider Week in effort to not only work directly with the apple producers but also restaurants and bars. Another program Glynwood offers is the Harvest Awards. Through this program Glynwood presents awards to \"individuals and organizations from across the country that do an exceptional job of supporting local and regional agriculture and increases access to fresh, healthy food.\"\n\n"}
{"id": "45636234", "url": "https://en.wikipedia.org/wiki?curid=45636234", "title": "Grain hopper trailer", "text": "Grain hopper trailer\n\nA grain hopper trailer is a trailer pulled by a semi tractor and used to haul bulk commodity products (such as grain). These trailers are used extensively throughout the United States to transport agricultural products as well as any other commodity that can be hauled in bulk and loaded and unloaded through the trailer.\n\nGrain hopper trailers typically feature a rolling tarp on the top of the trailer, to enable easy loading of the product to be transported, and also offer protection during transport. They utilize two hoppers on the base of the trailer to unload to product, one servicing the rear half of the trailer, the other the front half. The output of these hoppers is usually controlled by a sliding plate on the base of the hopper, controlled by a user-operated crank handle.\n\nA grain hopper trailer is used in conjunction with various other pieces of agricultural machinery to complete the harvest of a field. Combine harvesters or similar unload the harvest into grain carts, which in turn unload \"their\" load into a grain hopper trailer, for long distance transportation. The trailers are then unloaded into low-profile grain augers for transportation to, and long-term storage in, grain bins, also known as silo's.\n"}
{"id": "37419576", "url": "https://en.wikipedia.org/wiki?curid=37419576", "title": "History of videotelephony", "text": "History of videotelephony\n\nThe history of videotelephony covers the historical development of several technologies which enable the use of live video in addition to voice telecommunications. The concept of videotelephony was first popularized in the late 1870s in both the United States and Europe, although the basic sciences to permit its very earliest trials would take nearly a half century to be discovered. This was first embodied in the device which came to be known as the video telephone, or videophone, and it evolved from intensive research and experimentation in several telecommunication fields, notably electrical telegraphy, telephony, radio, and television.\n\nThe development of the crucial video technology first started in the latter half of the 1920s in the United Kingdom and the United States, spurred notably by John Logie Baird and AT&T's Bell Labs. This occurred in part, at least with AT&T, to serve as an adjunct supplementing the use of the telephone. A number of organizations believed that videotelephony would be superior to plain voice communications. However video technology was to be deployed in analog television broadcasting long before it could become practical—or popular—for videophones.\n\nVideotelephony developed in parallel with conventional voice telephone systems from the mid-to-late 20th century. Very expensive videoconferencing systems rapidly evolved throughout the 1980s and 1990s from proprietary equipment, software and network requirements to standards-based technologies that were readily available to the general public at a reasonable cost. Only in the late 20th century with the advent of powerful video codecs combined with high-speed Internet broadband and ISDN service did videotelephony become a practical technology for regular use.\n\nWith the rapid improvements and popularity of the Internet, videotelephony has become widespread through the deployment of video-enabled mobile phones, plus videoconferencing and computer webcams which utilize Internet telephony. In the upper echelons of government, business and commerce, telepresence technology, an advanced form of videoconferencing, has helped reduce the need to travel.\n\nBarely two years after the telephone was first patented in the United States in 1876 by Dr. Alexander Graham Bell, an early concept of a combined videophone and wide-screen television called a \"telephonoscope\" was conceptualized in the popular periodicals of the day. It was also mentioned in various early science fiction works such as \"Le Vingtième siècle. La vie électrique\" (The 20th century. The electrical life) and other works written by Albert Robida, and was also sketched in various cartoons by George du Maurier as a fictional invention of Thomas Edison. One such sketch was published on December 9, 1878 in \"Punch\" magazine.\n\nThe term 'telectroscope' was also used in 1878 by French writer and publisher Louis Figuier, to popularize an invention wrongly interpreted as real and incorrectly ascribed to Dr. Bell, possibly after his Volta Laboratory discreetly deposited a sealed container of a Graphophone phonograph at the Smithsonian Institution for safekeeping. Written under the pseudonym \"Electrician\", one article earlier claimed that \"an eminent scientist\" had invented a device whereby objects or people anywhere in the world \"...could be seen anywhere by anybody\". The device, among other functions, would allow merchants to transmit pictures of their wares to their customers, and the contents of museum collections to be made available to scholars in distant cities...\" In the era prior to the advent of broadcasting, electrical \"seeing\" devices were conceived as adjuncts to the telephone, thus creating the concept of a videophone.\n\nFraudulent reports of 'amazing' advances in video telephones would be publicized as early as 1880 and would reoccur every few years, such as the episode of 'Dr. Sylvestre' of Paris who claimed in 1902 to have invented a powerful (and inexpensive) video telephone, termed a 'spectograph', the intellectual property rights he believed were worth $5,000,000. After reviewing his claim Dr. Bell denounced the supposed invention as a \"fairy tale\", and publicly commented on the charlatans promoting bogus inventions for financial gain or self-promotion.\n\nHowever Dr. Alexander Graham Bell personally thought that videotelephony was achievable even though his contributions to its advancement were incidental. In April 1891, Dr. Bell actually did record conceptual notes on an 'electrical radiophone', which discussed the possibility of \"seeing by electricity\" using devices that employed tellurium or selenium imaging components. Bell wrote, decades prior to the invention of the image dissector:\n\nBell went on to later predict that: \"...the day would come when the man at the telephone would be able to see the distant person to whom he was speaking.\" The discoveries in physics, chemistry and materials science underlying video technology would not be in place until the mid-1920s, first being utilized in electromechanical television. More practical 'all-electronic' video and television would not emerge until 1939, but would then suffer several more years of delays before gaining popularity due to the onset and effects of World War II.\n\nThe compound name 'videophone' slowly entered into general usage after 1950, although 'video telephone' likely entered the lexicon earlier after \"video\" was coined in 1935. Prior to that time there appeared to be no standard terms for 'video telephone', with expressions such as 'sight-sound television system', 'visual radio' and nearly 20 others (in English) being used to describe the marriage of telegraph, telephone, television and radio technologies employed in early experiments.\n\nAmong the technological precursors to the videophone were telegraphic image transmitters created by several companies, such as the wirephoto used by Western Union, and the \"teleostereograph\" developed by AT&T's Bell Labs, which were forerunners of today's fax (facsimile) machines. Such early image transmitters were themselves based on previous work by Ernest Hummel and others in the 19th century. By 1927 AT&T had created its earliest electromechanical television-videophone called the \"ikonophone\" (from Greek: 'image-sound'), which operated at 18 frames per second and occupied half a room full of equipment cabinets. An early U.S. test in 1927 had their then-Commerce Secretary Herbert Hoover address an audience in New York City from Washington, D.C.; although the audio portion was two-way, the video portion was one-way with only those in New York being able to see Hoover.\n\nBy 1930, AT&T's 'two-way television-telephone' system was in full-scale experimental use. The Bell Labs' Manhattan facility devoted years of research to it during the 1930s, led by Dr. Herbert Ives along with his team of more than 200 scientists, engineers and technicians, intending to develop it for both telecommunication and broadcast entertainment purposes.\n\nThere were also other public demonstrations of \"two-way television-telephone\" systems during this period by inventors and entrepreneurs who sought to compete with AT&T, although none appeared capable of dealing with the technical issues of signal compression that Bell Labs would eventually resolve. Signal compression, and its later sibling data compression were fundamental to the issue of transmitting the very large bandwidth of low-resolution black and white video through the very limited capacity of low-speed copper PSTN telephone lines (higher resolution colour videophones would require even far greater capabilities). After the Second World War, Bell Labs resumed its efforts during the 1950s and 1960s, eventually leading to AT&T's Picturephone.\n\nIn early 1936 the world's first public video telephone service, Germany's \"Gegensehn-Fernsprechanlagen\" (visual telephone system), was developed by , who headed the development department at the \"Fernseh-AG\", a technical combine for television broadcasting technology. It was opened by the German \"Reichspost\" (post office) between Berlin and Leipzig, utilizing broadband coaxial cable to cover the distance of approximately 160 km (100 miles).\n\nSchubert's system was based on Gunter Krawinkel's earlier research of the late-1920s that he displayed at the \"1929 Internationale Funkausstellung Berlin\" (Berlin International Radio Exposition). Schubet's higher performance system employed mechanical television scanning and 20 cm (8 inch) square displays with a resolution of 180 lines (initially 150 lines), transmitting some 40,000 pixels per frame at 25 frames per second.\n\nThe system's opening was inaugurated by the Minister of Posts Paul von Eltz-Rübenach in Berlin on March 1, 1936, who viewed and spoke with Leipzig's chief burgomaster. It employed a Nipkow disk flying-spot scanner for its transmitter, and a cathode ray display tube with (an initial) resolution of 150 lines at its receiving-end videophone booth. The same coaxial cables were also used to distribute television programming.\n\nIn the initial service trial, broadband coaxial cable lines initially linked Berlin to Leipzig. After a period of experimentation the system entered public use and was soon extended another 160 km (100 miles) from Berlin to Hamburg, and then in July 1938 from Leipzig to Nuremberg and Munich. The system eventually operated with more than 1,000 km (620 miles) of coaxial cable transmission lines. The videophones were integrated within large public videophone booths, with two booths provided per city. Calls between Berlin and Leipzig cost RM3½, approximately one sixth of a British pound sterling, or about one-fifteenth of the average weekly wage. The video telephone equipment used in Berlin was designed and built by the German Post Office Laboratory. Videophone equipment used in other German cities were developed by Fernseh A.G., partly owned by Baird Television Ltd. of the U.K., inventors of the world's first functional television. During its life the German system underwent further development and testing, resulting in higher resolutions and a conversion to an all-electronic camera tube transmission system to replace its mechanical Nipkow scanning disc. While the system's image quality was primitive by modern standards, it was deemed impressive in contemporary reports of the era, with users able to clearly discern the hands on wristwatches.\n\nThe special public videophone service was offered to the general public, which had to visit special post office \"Fernsehsprechstellen\" (video telephone booths, from \"far sight speech place\") simultaneously in their respective cities, but which at the same time also had Nazi political and propagandistic overtones similar to the broadcasting of the 1936 Olympic Games in Berlin. The German post office announced ambitious plans to extend their public videophone network to Cologne, Frankfurt and Vienna, Austria, but expansion plans were discontinued in 1939 with the start of the Second World War. After Germany subsequently became fully engaged in the war its public videophone system was closed in 1940, with its expensive inter-city broadband cables converted to telegraphic message traffic and broadcast television service.\n\nA similar commercial post office system was also created in France during the late-1930s. The Deutsche Bundespost postal service would decades later develop and deploy its (Broadband Integrated Glass-Fiber Optical Network) videotelephony network from 1981 to 1988, serving several large German cities, and also created one of Europe's first public switched broadband services in 1989.\n\nIn the United States AT&T's Bell Labs conducted extensive research and development of videophones, eventually leading to public demonstrations of its trademarked Picturephone product and service in the 1960s. Its large Manhattan experimental laboratory devoted years of technical research during the 1930s, led by Dr. Herbert Ives along with his team of more than 200 scientists, engineers and technicians. The Bell Labs early experimental model of 1930 had transmitted uncompressed video through multiple phone lines, a highly impractical and expensive method unsuitable for commercial use.\n\nDuring the mid-1950s, its laboratory work had produced another early test prototype capable of transmitting still images every two seconds over regular analog PSTN telephone lines. The images were captured by the Picturephone's compact Vidicon camera and then transferred to a storage tube or magnetic drum for transmission over regular phone lines at two-second intervals to the receiving unit, which displayed them on a small cathode-ray television tube. AT&T had earlier promoted its experimental video for telephone service at the 1939 New York World's Fair.\n\nThe more advanced Picturephone Mod I's early promotion included public evaluation displays at Disneyland and the 1964 New York World's Fair, with the first transcontinental videocall between the two venues made on April 20, 1964. The first Picturephone 'Mod I' (Model No. 1) demonstration units used small oval housings on swivel stands, intended to stand on desks. Similar AT&T Picturephone units were also featured at the Telephone Pavilion (also called the \"Bell Telephone Pavilion\") at Expo 67, an International World's Fair held in Montreal, Canada in 1967. Demonstration units were available at the fairs for the public to test, with fairgoers permitted to make videophone calls to volunteer recipients at other locations.\n\nThe United States would not see its first public videophone booths until 1964, when AT&T installed their earliest commercial videophone units, the Picturephone \"Mod I\", in booths that were set up in New York's Grand Central Terminal, Washington D.C., and Chicago. Its system was the result of decades of research and development at Bell Labs, its principal supplier, Western Electric, plus other researchers working under contract to the Bell Labs. However the use of reservation time slots and their cost of US$16 (Washington, D.C. to New York) to $27 (New York to Chicago) (equivalent to $118 to $200 in 2012 dollars) for a three-minute call at the public videophone booths greatly limited their appeal resulting in their closure by 1968.\n\nPicturephones were also later installed in the offices of Westinghouse and Alcoa in Pittsburgh, as well as other technology companies in that city. AT&T's commercial service in Pittsburgh started on June 30, 1970 with 38 Picturephones at eight companies, at a lease rate of $160 per month ($947/month in 2012 dollars) for the service, which provided for 30 minutes of videocalling time per month, with extra minutes costing 25 cents each.\n\nColor on AT&T's Picturephone was not employed with their early models. These Picturephone units packaged Plumbicon cameras and small CRT displays within their housings. The cameras were located atop their screens to help users see eye to eye. Later generation display screens were larger than in the original demonstration units, approximately six inches (15 cm) square in a roughly cubical cabinet.\n\nThe Picturephone's video bandwidth was 1 MHz with a vertical scan rate of 30 Hz, horizontal scan rate of 8 kHz, and about 250 visible scan lines. The equipment included a speakerphone (hands free telephone), with an added box to control picture transmission. Each Picturephone line used three twisted pairs of ordinary telephone cable, two pairs for video and one for audio and signaling. Cable amplifiers were spaced about a mile apart (1.6 kilometres) with built-in six-band adjustable equalization filters. For distances of more than a few miles, the signal was digitized at 2 MHz and 3 bits per sample DPCM, and transmitted on a T-2 carrier.\n\nThe original Picturephone system used contemporary crossbar and multi-frequency operation. Lines and trunks were six wire, one pair each way for video and one pair two way for audio. MF address signaling on the audio pair was supplemented by a Video Supervisory Signal (VSS) looping around on the video quad to ensure continuity. More complex protocols were later adopted for conferencing.\n\nTo deploy Picturephone service, new wideband crossbar switches were designed and installed into the Bell System's 5XB switch offices, this being the most widespread of the relatively modern kind. Hundreds of technicians attended schools to learn to operate the Cable Equalizer Test Set and other equipment, and to install Picturephones.\n\nAt the time of its first launch, AT&T foresaw a hundred thousand Picturephones in use across the Bell System by 1975. However, by the end of July 1974, only five Picturephones were being leased in Pittsburgh, and U.S.-wide there were only a few hundred, mostly in Chicago. Unrelated difficulties at New York Telephone also slowed AT&T's efforts, and few customers signed up for the service in either city. At its peak Picturephone service had only about 500 subscribers, with the service fading away through the 1970s.\n\nAT&T's initial Picturephone 'Mod I' (Model No. 1) and then its upgraded 'Mod II' programs, were a continuation of its many years of prior research during the 1920s, 1930s, late 1940s and 1950s. Both Picturephone programs, like their experimental AT&T predecessors, were researched principally at its Bell Labs, formally spanned some 15 years and consumed more than US$500 million, eventually meeting with commercial failure. AT&T concluded that its early Picturephone was a \"concept looking for a market\" and discontinued its 'Mod II' service in the late 1970s.\n\nAT&T would later market its VideoPhone 2500 to the general public from 1992 to 1995 with prices starting at US$1,500 (approximately $ in current dollars) and later dropping to $1,000 ($ in current dollars), marketed by its Global VideoPhone Systems unit. The VideoPhone 2500 was designed to provide low-frame rate compressed color video on ordinary Plain Old Telephone Service (POTS) lines, circumventing the significantly higher cost ADSL telephone service lines used by several other videoconferencing manufacturers. It was limited by analog phone line connection speeds of about 19 Kilobits per second, the video portion being 11,200 bit/s, and with a maximum frame rate of 10 frames per second, but typically much slower, as low as a third of a video frame per second. The VideoPhone 2500 used proprietary technology protocols, including AT&T's Global VideoPhone Standard (GVS). Again, AT&T met with very little commercial success, selling only about 30,000 units, mainly outside the United States.\n\nDespite AT&T's various videophone products meeting with commercial failure, they were widely viewed as technical successes which expanded the limits of the telecommunications sciences in several areas. Its videotelephony programs were critically acclaimed for their technical brilliance and even the novel uses they experimented with. The research and development programs conducted by Bell Labs were highly notable for their beyond-the-state-of-the-art results produced in materials science, advanced telecommunications, microelectronics and information technologies.\n\nAT&T's published research additionally helped pave the way for other companies to later enter the field of videoconferencing. The company's videophones also generated significant media coverage in science journals, the general news media and in popular culture. The image of a futuristic AT&T videophone being casually used in the science fiction film , became iconic of both the movie and, arguably, the public's general view of the future.\n\nBeginning in the late 1960s, several countries worldwide sought to compete with AT&T's advanced development of its Picturephone service in the United States. However such projects were research and capital intensive, and fraught with difficulties in being deployed commercially. This time period also saw the research, development and commercial roll-out of what would become powerful video compression and decompression software codecs, which would eventually lead to low cost videotelephony in the early 2000s.\n\nFrance's post office telecommunications branch had earlier set up a commercial videophone system similar to the German \"Reichspost\" public videophone system of the late 1930s. In 1972 the defense and electronics manufacturer Matra was one of three French companies that sought to develop advanced videophones in the early 1970s, spurred by the AT&T's Picturephone in the United States. Initial plans by Matra included the deployment of 25 units to France's Centre national d'études des télécommunications (CNET of France Télécom) for their internal use. CNET intended to guide its initial use towards the business sector, to be later followed by personal home usage. Its estimated unit cost in 1971 was the equivalent of £325, with a monthly usage subscription charge of £3.35.\n\nStudies of applications of videotelephony were conducted by CNET in France in 1972, with its first commercial applications for videophones appearing in 1984. The delay was due to the problem of insufficient bandwidth, with 2 Mb per second being required for transmitting both video and audio signals. The problem was solved worldwide by the creation of software for data encoding and compression via video coding and decoding algorithms, also known as codecs.\n\nIn Japan the \"Lumaphone\" was developed and marketed by Mitsubishi in 1985. The project was originally started by the Ataritel division of the Atari Video Game Company in 1983 under the direction of Atari's Steve Bristow. Atari then sold its division to Mitsubishi Electric in 1984. The Lumaphone was marketed by Mitsubishi Electric of America in 1986 as the Luma LU-1000, costing US$1,500,\n\nMitsubishi also marketed its lower-cost VisiTel LU-500 image phone in 1988 costing about US$400, aimed at the consumer market. It came with reduced capabilities but had with a larger black and white display. Other Japanese electronic manufacturers marketed similar image transfer phones during the late-1980s, including Sony's PCT-15 (US$500), and two models from Panasonic, its WG-R2 (US$450) and its KX-TV10 (US$500).\n\nMuch later the Kyocera Corporation, an electronics manufacturer based in Kyoto, conducted a two-year development campaign from 1997 to 1999 that resulted in the release of the VP-210 VisualPhone, the world's first mobile colour videophone that also doubled as a camera phone for still photos. The camera phone was the same size as similar contemporary mobile phones, but sported a large camera lens and a 5 cm (2 inch) colour TFT display capable of displaying 65,000 colors, and was able to process two video frames per second. The 155 gram (5.5 oz.) camera could also take 20 photos and convey them by e-mail, with the camera phone retailing at the time for 40,000 yen, about US$325 in 1999.\n\nThe VP-210 was released in May 1999 and used its single front-facing 110,000-pixel camera to send two images per second through Japan's PHS mobile phone network system. Although its frame rate was crude and its memory is considered tiny in the present day, the phone was viewed as \"revolutionary\" at the time of its release.\n\nThe Kyocera project was initiated at their Yokohama research and development center by Kazumi Saburi, one of their section managers. His explanation for the project was \"Around that time, cellular handsets with enabled voice and SMS communication capabilities were considered to be just one among many personal communication tools. One day a simple idea hit us - 'What if we were able to enjoy talking with the intended person watching his/her face on the display?' We were certain that such a device would make cell phone communications much more convenient and enjoyable.\"\n\nSaburi also stated that their R&D section had \"nourished [the idea] for several years before\" they received project approval from their top management which had encourage such forward-thinking research, because they \"also believed that such a product would improve Kyocera's brand image.\" Their research showed that a \"cell phone with a camera and color display provided a completely new value for users, It could be used as a phone, a camera and a photo album\".\n\nTechnical challenges handled by about a dozen engineers at Kyocera over the two year development period included the camera module's placement within the phone at a time when electronic components had not been fully reduced in size, as well as increasing its data transmission rate. After its release the mobile video-camera phone was commercially successful, spawning several other competitors such as the DDI Pocket, and one from Vodafone K.K.\n\nIn Sweden, electronics maker Ericsson began developing a videophone in the mid-1960s, intending to market it to government, institutions, businesses and industry, but not to consumers due to AT&T's lack of success in that market segment. Tests were conducted in Stockholm, including trial communications in banking. Ultimately Ericsson chose not to proceed with further production.\n\nIn 1970 the British General Post Office had 16 demonstration models of its Viewphone built, meant to be the equivalent to AT&T's Picturephone. Their initial attempt at a first generation commercial videophone later led to the British Telecom Relate 2000, which was released for sale in 1993, costing between £400-£500 each. The Relate 2000 featured a flip-up colour LCD display screen operating at a nominal rate of 8 video frames per second, which could be depressed to 3-4 frames per second if the PSTN bandwidth was limited. In the era prior to low-cost, high-speed broadband service, its video quality was found to be generally poor by the public with images shifting jerkily between frames, due to British phone lines that generally provided less than 3.4 kHz of bandwidth. British Telecom had initially expected the device, manufactured by Marconi Electronics, to sell at a rate of 10,000 per year, but its actual sales were minimal. Its second generation videophone thus also proved to be commercially unsuccessful, similar to AT&T's VideoPhone 2500 of the same time period.\n\nSignificant improvements in video call quality of service for the deaf occurred in the United States in 2003 when Sorenson Media Inc. (formerly Sorenson Vision), a video compression software coding company, developed its VP-100 model stand-alone videophone specifically for the deaf community. It was designed to output its video to the user's television in order to lower the cost of acquisition, and to offer remote control and a powerful video compression codec for unequaled video quality and ease of use with a video relay service (VRS). Favourable reviews quickly led to its popular usage at educational facilities for the deaf, and from there to the greater deaf community.\n\nCoupled with similar high-quality videophones introduced by other electronics manufacturers, the availability of high speed Internet, and sponsored video relay services authorized by the U.S. Federal Communications Commission in 2002, VRS services for the deaf underwent rapid growth in that country.\n\n\n\n"}
{"id": "2893300", "url": "https://en.wikipedia.org/wiki?curid=2893300", "title": "Horizontal scan rate", "text": "Horizontal scan rate\n\nHorizontal scan rate, or horizontal frequency, usually expressed in kilohertz, is the frequency at which a CRT moves the electron beam from the left side of the display to the right and back, and therefore describes the number of horizontal lines displayed per second. CRT timings include some horizontal scans before the visible display, after it, and during the travel from bottom to top (known as vertical back porch, vertical front porch, and vertical sync width, respectively, and collectively known as vertical blank time), so the horizontal scan rate does not directly correlate to visible display lines, unless the unseen lines are also known, but it can still be used to approximate the display lines, as the total blank time is usually a small but significant portion of the total lines.\n\nIt is usually the most limiting factor of a CRT display. This limit is due to how quickly the electromagnetic deflection system can reverse the current flowing in the horizontal deflection coil in order to move the electron beam from one side of the display to the other. Reversing the current more quickly requires higher voltages, which requires more expensive electrical components. The percentage of time allowed by the standard was chosen to allow practical TV receiver designs.\n\nCompare vertical scan rate (refresh rate), which indicates how often the electron beam is moved from the bottom of the display to the top. Given the horizontal scan rate, the refresh rate can be approximated by dividing the horizontal scan rate by the number of horizontal lines multiplied by 1.05 (since about 5% of the time it takes to scan the screen is spent moving the electron beam back to the top). For instance, a monitor with a horizontal scanning frequency of 96 kHz at a resolution of 1280 x 1024 has a refresh rate of 96,000 / (1024 x 1.05) ≈ 89 Hz (rounded down).\n\nIn analog television systems the horizontal frequency is between 15.625 kHz and 15.750 kHz.\n\n"}
{"id": "28023991", "url": "https://en.wikipedia.org/wiki?curid=28023991", "title": "Hospital bed", "text": "Hospital bed\n\nA hospital bed or hospital cot is a bed specially designed for hospitalized patients or others in need of some form of health care. These beds have special features both for the comfort and well-being of the patient and for the convenience of health care workers. Common features include adjustable height for the entire bed, the head, and the feet, adjustable side rails, and electronic buttons to operate both the bed and other nearby electronic devices.\n\nHospital beds and other similar types of beds such as nursing care beds are used not only in hospitals, but in other health care facilities and settings, such as nursing homes, assisted living facilities, outpatient clinics, and in home health care.\n\nWhile the term \"hospital bed\" can refer to the actual bed, the term \"bed\" is also used to describe the amount of space in a health care facility, as the capacity for the number of patients at the facility is measured in available \"beds.\"\n\nBeds with adjustable side rails first appeared in Britain some time between 1815 and 1825.\n\nIn 1874 the mattress company Andrew Wuest and Son, Cincinnati, Ohio, registered a patent for a type of mattress frame with a hinged head that could be elevated, a predecessor of the modern day hospital bed.\n\nThe modern 3-segment adjustable hospital bed was invented by Willis Dew Gatch, chair of the Department of Surgery at the Indiana University School of Medicine, in the early 20th century. This type of bed is sometimes referred to as the Gatch Bed.\n\nThe modern push-button hospital bed was invented in 1945, and it originally included a built-in toilet in hopes of eliminating the bedpan.\n\nWheels enable easy movement of the bed, either within parts of the facility in which they are located, or within the room. Sometimes movement of the bed a few inches to a few feet may be necessary in patient care.\n\nWheels are lockable. For safety, wheels can be locked when transferring the patient in or out of the bed.\n\nBeds can be raised and lowered at the head, feet, and their entire height. While on older beds this is done with cranks usually found at the foot of the bed, on modern beds this feature is electronic.\n\nToday, while a fully electric bed has many features that are electronic, a \"semi-electric bed\" has two motors, one to raise the head, and the other to raise the foot.\n\nRaising the head (known as a Fowler's position) can provide some benefits to the patient, the staff, or both. The Fowler's position is used for sitting the patient upright for feeding or certain other activities, or in some patients, can ease breathing, or may be beneficial to the patient for other reasons.\n\nRaising the feet can help ease movement of the patient toward the headboard and may also be necessary for certain conditions.\n\nRaising and lowering the height of the bed can help bring the bed to a comfortable level for the patient to get in and out of bed, or for caregivers to work with the patient.\n\nBeds have side rails that can be raised or lowered. These rails, which serve as protection for the patient and sometimes can make the patient feel more secure, can also include the buttons used for their operation by staff and patients to move the bed, call the nurse, or even control the television.\n\nThere are a variety of different types of side rails to serve different purposes. While some are simply to prevent patient falls, others have equipment that can aid the patient themself without physically confining the patient to bed.\n\nSide rails, if not built properly, can be of risk for patient entrapment. In the United States, more than 300 deaths were reported as a result of this between 1985 and 2004. As a result, the Food and Drug Administration has set guidelines regarding the safety of side rails.\n\nIn some cases, use of the rails may require a physician's order (depending on local laws and the policies of the facility where they are used) as rails may be considered a form of medical restraint.\n\nSome advanced beds are equipped with columns which help tilt the bed to 15-30 degrees on each side. Such tilting can help prevent pressure ulcers for the patient, and help caregivers to do their daily tasks with less of a risk of back injuries.\n\nMany modern hospital beds are able to feature a bed exit alarm whereby a pressure pad on or in the mattress arms an audible alert when a weight such as a patient is placed on it, and activating the full alarm once this weight is removed. This is helpful to hospital staff or caregivers monitoring any number of patients from a distance (such as a nurse's station) as the alarm will trigger in the event of a patient (especially the elderly or memory impaired) falling out of the bed or wandering off unsupervised. This alarm can be emitted solely from the bed itself or connected to the nurse call bell/light or hospital phone/paging system.\nAlso some beds can feature a multi-zone bed exit alarm which can alert the staff when the patient start moving in the bed and before the actual exit which is necessary for some cases.\n\nIn the event of the bed occupant suddenly requiring cardiopulmonary resuscitation, some hospital beds offer a CPR function in the form of a button or lever which when activated flatten the bed platform and put it in lowest height and deflates and flattens the bed's air mattress (if installed) creating a flat hard surface necessary for effective CPR administration.\n\nMany specialist hospital beds are also produced in order to effectively treat different injuries. These include standing beds, turning beds and legacy beds. These are usually used to treat back and spinal injuries as well as severe trauma.\n\nA hospital bed can cost over $1000.00 USD; on average with different costs associated with completely manual functions, 2-motor functions and fully electric 3-motor functions (whole bed going up and down). Other costs are associated with bariatric heavy duty models that also offer extra width.\n\nHospital beds can make a patient's spine more rounded because a patient who sits up a lot, such as when watching television, tends to slip down.\nSome of the category A bed manufacturers are providing their beds with built-in function which act as anti-slip. LINET is providing Ergoframe while others got different names.\n\nDuring the 1980s, patient safety had been a concern with hospital beds.\n\nIn 1982, a 3-year-old Milwaukee girl hospitalized for pneumonia was killed when crushed by a mechanical hospital bed.\n\nIn 1983, an 11-year-old Illinois boy was strangled to death by a hospital bed.\n\n"}
{"id": "5599520", "url": "https://en.wikipedia.org/wiki?curid=5599520", "title": "House R 128", "text": "House R 128\n\nHouse R 128 (Sobek House) is a modernist single-family house in Stuttgart, Germany, built by architect Werner Sobek in 1999/2000.\nThe house features a modular and recyclable design, is completely glazed and has no interior dividing walls.\nIt is computerized and meets its own energy requirements completely.\n\nThe name of the house is derived from its location at Römerstrasse 128, which is a small and steep piece of land at the edge of the vale of Stuttgart. When the house was built, it was paid a great amount of attention in the architectural world. The building is shaped like a cube, has four levels and is wrapped by a glass shield. All components can be segregated for recycling. There are no walls or closed rooms (apart from the bathrooms) and only few pieces of furniture. The transparency is supposed to create the impression that one lives outdoors exposed to nature. Owing to its passive solar architecture with triple-glazed walls, the house needs no energy for heating.\nSeasonal temperature shifts are balanced by a seasonal thermal energy store.\nElectricity is generated by solar cells.\nEvery item in the house is computer-controlled.\n\n"}
{"id": "16809925", "url": "https://en.wikipedia.org/wiki?curid=16809925", "title": "Hydraulically activated pipeline pigging", "text": "Hydraulically activated pipeline pigging\n\nHydraulically activated pipeline pigging (HAPP) is a pigging technology applied for pipeline cleaning. The basic principle is that a pressure drop is created over a by-passable pig held back against a pipeline's fluid flow. The pipeline fluid passing through the pigs cleaning head is accelerated by this pressure drop, forming strong cleaning jets. These jets are directed onto the inner wall in front of the pig, removing all kinds of deposits.\n\nPipeline pigs are devices that are inserted into and travel throughout the length of a pipeline driven by the product flow. They were originally developed to remove deposits which could obstruct or retard flow through a pipeline (Fig. 1). Today pigs are used during all phases in the life of a pipeline for many different reasons.\n\nPigs used today can be divided into three categories (Fig. 2):\nGenerally for cleaning pigs, the cleaning force applied is the mechanical force between the pipe inner wall and the cleaning pig itself. This force is determined by the pig travel speed as well as by the hardness and shape of the cleaning edge: The faster the pig, the higher the cleaning impact on the deposits, but at the same time only the surface of the debris is scratched away. Therefore several, sometimes many, pig runs are required to clean a pipeline.\n\nHydraulically activated pigs apply high pressure liquid jets either supplied by high pressure hoses (depended) or made available by the kinetic energy locally available. Depended hydraulically activated pigs are limited in reach due to the hose which needs to be inserted into the pipeline and guides the cleaning head.\n\nA hydraulically activated pig consists of three units (Fig. 3): a brake unit, a seal unit and the cleaning head.\n\nAll units have openings that allow the entire fluid flow through the pipeline to bypass. The brake unit ensures that a hydraulically activated pig is held back against the fluid flow in the pipeline. The fluid pushes against the following seal unit, which channels it into the openings of the cleaning head. The seal unit and cleaning head restrict the flow, resulting in a pressure difference across the pig. Thus, the fluid is accelerated in the cleaning head's nozzles, creating extremely powerful liquid jets. These jets are directed onto the pipe inner wall to remove any kind of deposits.\n\nThe brake unit ensures that the travel speed of the pig is much slower than the fluid velocity, thus allowing it to entirely remove deposits from the pipe wall before it travels across the cleaned surface. The deposits removed are immediately flushed down the pipeline with the main jet of the cleaning head which is directed to the middle of the pipeline. With all deposits removed from the pipe wall and transported downstream by the fluid flow there remains no risk of the pig getting stuck in debris accumulated in front of it.\n\n\n"}
{"id": "4040188", "url": "https://en.wikipedia.org/wiki?curid=4040188", "title": "Hyperaccumulator", "text": "Hyperaccumulator\n\nA hyperaccumulator is a plant capable of growing in soils with very high concentrations of metals, absorbing these metals through their roots, and concentrating extremely high levels of metals in their tissues. The metals are concentrated at levels that are toxic to closely related species not adapted to growing on the metalliferous soils. Compared to non-hyperaccumulating species, hyperaccumulator roots extract the metal from the soil at a higher rate, transfer it more quickly to their shoots, and store large amounts in leaves and roots. The ability to hyperaccumulate toxic metals compared to related species has been shown to be due to differential gene expression and regulation of the same genes in both plants. Over 500 species of flowering plants have been identified as having the ability to hyperaccumulate metals in their tissues.\n\nHyperaccumulating plants hold interest for their ability to extract metals from the soils of contaminated sites (phytoremediation) to return the ecosystem to a less toxic state. The plants also hold potential to be used to mine metals from soils with very high concentrations (phytomining) by growing the plants then harvesting them for the metals in their tissues.\n\nThe genetic advantage of hyperaccumulation of metals may be that the toxic levels of heavy metals in leaves deter herbivores or increase the toxicity of other anti-herbivory metabolites.\n\nSeveral gene families are involved in the processes of hyperaccumulation including upregulation of absorption and sequestration of heavy metal metals. These hyperaccumulation genes (HA genes) are found in over 450 plant species, including the model organisms \"Arabidopsis\" and Brassicaceae. The expression of such genes is used to determine whether a species is capable of hyperaccumulation. Expression of HA genes provides the plant with capacity to uptake and sequester metals such as As, Co, Fe, Cu, Cd, Pb, Hg, Se, Mn, Zn, Mo and Ni in 100–1000x the concentration found in sister species or populations.\n\nThe capacity for hyperaccumulation is dependent on two major factors: environmental exposure and expression of members of the ZIP gene family.\nAlthough experiments have shown that the hyperaccumulation is partially dependent on environmental exposure (i.e. only plants exposed to a metal are observed with high concentrations of that metal), hyperaccumulation is ultimately dependent on the presence and upregulation of genes involved with that process. It has been shown that hyperaccumulation capacities can be inherited in \"Thlaspi caerulescens\" (Brassicaceae) and others. As there is wide variety among hyperaccumulating species that span across different plant families, it is likely that HA genes were ecotypically selected for. \nIn most hyperaccumulating plants, the main mechanism for metal transport are the proteins coded by genes in the ZIP family, however other families such as the HMA, MATE, YSL and MTP families have also been observed to be involved. The ZIP gene family is a novel, plant-specific gene family that encodes Cd, Mn, Fe and Zn transporters. The ZIP family plays a role in supplying Zn to metalloproteins.\n\nIn one study on \"Arabidopsis\", it was found that the metallophyte \"Arabidopsis halleri\" expressed a member of the ZIP family that was not expressed in a non-metallophytic sister species. This gene was an iron regulated transporter (IRT-protein) that encoded several primary transporters involved with cellular uptake of cations above the concentration gradient. When this gene was transformed into yeast, hyperaccumulation was observed. This suggests that overexpression of ZIP family genes that encode cation transporters is a characteristic genetic feature of hyperaccumulation. \nAnother gene family that has been observed ubiquitously in hyperaccumulators are the ZTP and ZNT families. A study on T. caerulescens identified the ZTP family as a plant specific family with high sequence similarity to other zinc transporter4. Both the ZTP and ZNT families, like the ZIP family, are zinc transporters. It has been observed in hyperaccumulating species, that these genes, specifically ZNT1 and ZNT2 alleles are chronically overexpressed.\n\nWhile the exact mechanism by which these genes facilitate hyperaccumulation is not yet characterized, expression patterns correlate heavily with individual hyperaccumulation capacity and metal exposure, suggesting these gene families play a regulatory role. As the presence and expression zinc transporter gene families are highly prevalent in hyperaccumulators, the capacity to accumulate a wide range of heavy metals is likely due to an inability of the zinc transporters to discriminate against certain metal ions. The response of the plants to hyperaccumulation of any metal also supports this theory as it has been observed that AhHMHA3 is expressed in hyperaccumulating individuals. AhHMHA3 has been identified to be expressed in response to and aid of Zn detoxification. In another study, using metallophytic and non-metallophytic \"Arabidopsis\" populations, back crosses indicated pleiotropy between Cd and Zn tolerances. This response suggests that plants are unable to detect specific metals, and that hyperaccumulation is likely a result of an overexpressed Zn transportation system.\n\nThe overall effect of these expression patterns has been hypothesized to assist in plant defense systems. In one hypothesis, \"the elemental defense hypothesis\", provided by Poschenrieder, it is suggested that the expression of these genes assist in antiherbivory or pathogen defenses by making tissues toxic to organisms attempting to feed on that plant. Another hypothesis, \"the joint hypothesis\", provided by Boyd, suggests that expression of these genes assists in systemic defense.\n\n"}
{"id": "19174035", "url": "https://en.wikipedia.org/wiki?curid=19174035", "title": "IGG Software", "text": "IGG Software\n\nIGG Software is a Putney, Vermont-based software company that specializes in Mac OS X and iOS applications for personal finance management.\n\nIGG's iBank application, a personal finance product and Quicken competitor,\nwas runner-up for the Best Mac OS X Leopard Application in the 2007 Apple Design Awards.\n\niBank 3, was written to run exclusively in Mac OS 10.5 (Leopard). iBank 2 was developed specifically for the previous Mac OS, 10.4 (Tiger). iBank 3 leverages various aspects of the Mac OS, including Cover Flow, Quick Look, Core Animation and more. It also integrates with other Apple Inc. products, such as syncing with an iPhone via MobileMe.\n\niBank 4 was updated to be compatible with Mac OS X 10.7.\n\nSome iBank 4 features included Direct Connect (downloads from online accounts), iPhone sync, MobileMe compatibility, interactive charts, multiple currencies, a budget monitor, check printing, loan and portfolio management, smart accounts, TurboTax exporting, and Quicken importing.\n\niBank 5 was released November 19, 2013, introducing several new features such as online billpay, Direct Access and Improvements to budgeting. iBank 5 saw the release of six major upgrades to the product through iBank 5.6.4 which included the addition of iBank Cloud Sync.\n\nBanktivity 5 released January 28, 2016. After nearly 13 years IGG retired the iBank name, announcing Banktivity as the successor of the IGG’s personal finance software line.\n\nBanktivity 6: initial public release on April 25, 2017 \n\nList of personal finance software\n"}
{"id": "43199301", "url": "https://en.wikipedia.org/wiki?curid=43199301", "title": "Institute of Refrigeration", "text": "Institute of Refrigeration\n\nThe Institute of Refrigeration is an organisation in the UK that supports the refrigeration and air-conditioning industry.\n\nThe Institute was formed in 1899 as the Cold Storage and Ice Association, the first national society in the world for the refrigeration industry. The Institute's first president was Alan Egerton, 3rd Baron Egerton. It became the IOR in 1944 when professional membership was introduced (when certain qualifications had to be obtained). A division of the organisation, the Air Conditioning and Heat Pump Institute, was launched in 2009. In 2010 the Institute launched a short video explaining the opportunities for careers in the refrigeration industry under the title Careers in Cooling. This uses interviews with a wide range of young people working in different aspects of refrigeration and air-conditioning to explain what a rewarding career it can be. A webpage was also set up to support the video.\n\nThe Institute of Refrigeration is governed by an Executive Council which comprises the President, the President-Elect, the Immediate Past-President, the Honorary Treasurer and six elected members.\n\nThe current council comprises\n\nThe work of the Institute is carried out by committees, including\n\n\nIt has branches covering:\n\nThe purpose of the Institute of Refrigeration is outlined in the Institute's constitution as follows:\n\na) The general advancement of refrigeration in all its applications, in relation both to the perfection of its methods, and to the extension of its services to the community.\n\nb) To promote means for communication between members and their interchange of views.\n\nc) To encourage invention and research in all matters relating to the science and practice of refrigeration.\n\nd) To promote a sustainable approach to all aspects of refrigeration system design and operation\n\ne) To co-operate with educational institutions for the furtherance of education in the science and practice of refrigeration.\n\nf) To hold meetings of the Institute for reading and discussing papers dealing with refrigeration and allied subjects.\n\ng) To publish and distribute the proceedings or reports of the Institute.\n\nh) To do all other things, incidental or conducive to the attainment of the above objects or any of them.\n\nThe Institute hosts monthly meetings when a technical paper on a topical subject is presented. The current programme of meetings can be found on the Institute's website IOR. The Institute also hosts a black-tie dinner in London, usually on the third Thursday in February. In previous years the Institute dinner was held in the Great Room of the Grosvenor House Hotel, but declining numbers in recent years prompted a move in 2009 to the Grand Connaught Rooms in Covent Garden and then in 2012 to the Grange St Pauls Hotel. The Institute publishes an annual set of Technical Proceedings on CD-rom as well as Safety Alerts, Guidance Notes and Good Practice Guides for Technicians. It has also hosted International Scientific Conventions.\n\nThere are four main grades of membership; Associate, Technician, Member and Fellow. Membership grade is based on relevant qualifications and experience in the industry and is determined by application to the membership committee. The Institute is affiliated with the UK Engineering Council and has many overseas members.\n\n\n"}
{"id": "3532369", "url": "https://en.wikipedia.org/wiki?curid=3532369", "title": "Knitting machine", "text": "Knitting machine\n\nA knitting machine is a device used to create knitted fabrics in a semi or fully automated fashion.\n\nThere are numerous types of knitting machines, ranging from simple spool or board templates with no moving parts to highly complex mechanisms controlled by electronics. All, however, produce various types of knitted fabrics, usually either flat or tubular, and of varying degrees of complexity. Pattern stitches can be selected by hand manipulation of the needles, or with push-buttons and dials, mechanical punch cards, or electronic pattern reading devices and computers.\n\nEarly flat bed stocking frames had low carbon steel bearded needles where the tips were reflexed and could be depressed onto a hollow closing the loop. The needles were supported on a \"needle bar\" that passed back and forth, to and from the operator. The beards were simultaneously depressed by a \"presser bar\".\n\nThis basic process can still be recognised in all machines, but it has been refined as new technologies have become available.\n\nA few simple devices permit knitting without needles for toy or hobby purposes. The simplest of these is spool knitting, followed by knitting boards or knitting looms, which consist of two rows of pins mounted in two parallel rows approximately apart. Yarn is wound around the pins; various patterns of winding produce different textured knitting. A needle or special tool is then used to transfer the loops of yarn from around the pins, either off the pins or to other pins, to produce the knitting. Knitting boards can produce complex designs. Other semi-mechanical knitting devices are available.\n\nTo produce larger and more complex knitted items, such as garments, domestic and industrial machines, with either flat or circular beds, producing rectangular or tubular fabrics, respectively, are needed. Double bed machines have two flat beds facing each other, in order to produce purl and plain rib fabrics plus a variety of multi patterns. Ribbing attachments can be added to single bed machines to achieve a similar result.\n\nLate 20th century domestic/studio models typically use up to 200 latch-hook needles to hold the stitches in fine, standard, mid-gauge or bulky gauge needle. A carriage or cam box is passed across the bed of needles causing the needle movements required to produce each next stitch. By means of various selection methods, e.g. punch cards, particular needles can be caused to travel by alternate pathways through the cam box. Thus needles will knit or not, and the unknitted yarn portions will lie under (slip stitch) or over the needle or be held in the needle hook (tuck stitch). Needles can be placed in holding position to allow short row shaping. In the most modern machines, punchcards have been replaced by computer control.\nAutomatic patterning machines can knit two-colour Fair Isle patterns automatically, and have machine stitch patterning features such as plating and knitweaving. Plating refers to knitting with two strands of yarn that are held in such a way that one is in front of the other. Plated effects can be particularly striking in a ribbed fabric. Knitweaving refers to a technique in which a separate piece of yarn, often heavier than the knitted fabric, is carried along and caught between stitches to produce an effect like weaving. With knitwoven fabric, the purl side (usually the wrong side) is the right side of the fabric. The fine and standard gauge models have the option of a lace carriage, where stitches can be transferred from one needle to the next. The yarn passes through a tensioning mechanism and down through the knit carriage, which feeds the yarn to the needles as they knit.\n\nDomestic knitting machines use the \"weft\" knitting method which produces a fabric similar to hand knitting. Knitting proceeds more quickly than in hand knitting, where (usually two) straight needles are held in the hand and each stitch is manipulated individually across the row. Knitting machines work an entire row of loops in a single movement.\n\nThe fabric produced using a knitting machine is of a more even texture than hand-knitted fabric, which is particularly noticeable on large areas of plain stockinette stitch, and can be an advantage. Some stitch patterns (e.g., tuck stitches) are much easier to produce with a knitting machine. Others (e.g. garter stitch) can also be produced with machine knitting but can take a little longer but still much faster than hand knitting. The standard gauge 200-needle machine can knit the finest yarns up to a good sport-weight, while the heavier yarns knit better on a mid-gauge or bulky knitting machine.\n\nMachine knitting saves a considerable amount of time but does require learning to operate the machines correctly. Most if not all hand knitting patterns can be worked up on a machine, either identically or in a similar design. Hand knitting patterns are designed to \"flip\" the fabric on every row so that the knitter consistently uses the dominant hand. However, machine knitting is consistently knit with the fabric facing the same way. Flat Bed machines knit back and forth and Circular machines knit continuously in the round.\n\n\n"}
{"id": "50677", "url": "https://en.wikipedia.org/wiki?curid=50677", "title": "Laser printing", "text": "Laser printing\n\nLaser printing is an electrostatic digital printing process. It produces high-quality text and graphics (and moderate-quality photographs) by repeatedly passing a laser beam back and forth over a negatively charged cylinder called a \"drum\" to define a differentially charged image. The drum then selectively collects electrically charged powdered ink (toner), and transfers the image to paper, which is then heated in order to permanently fuse the text, imagery, or both. As with digital photocopiers, laser printers employ a xerographic printing process. However, laser printing differs from analog photocopiers in that the image is produced by the direct scanning of the medium across the printer's photoreceptor. This enables laser printing to copy images more quickly than most photocopiers.\n\nInvented at Xerox PARC in the 1970s, laser printers were introduced for the office and then home markets in subsequent years by IBM, Canon, Xerox, Apple, Hewlett-Packard and many others. Over the decades, quality and speed have increased as price has fallen, and the once cutting-edge printing devices are now ubiquitous.\n\nIn the 1960s, the Xerox Corporation held a dominant position in the photocopier market. In 1969, Gary Starkweather, who worked in Xerox's product development department, had the idea of using a laser beam to \"draw\" an image of what was to be copied directly onto the copier drum. After transferring to the recently formed Palo Alto Research Center (Xerox PARC) in 1971, Starkweather adapted a Xerox 7000 copier to create SLOT (Scanned Laser Output Terminal). In 1972, Starkweather worked with Butler Lampson and Ronald Rider to add a control system and character generator, resulting in a printer called EARS (Ethernet, Alto Research character generator, Scanned laser output terminal)—which later became the Xerox 9700 laser printer.\n\n\n\n\n\n\nA laser beam (typically, an aluminium gallium arsenide (AlGaAs) semiconductor laser) projects an image of the page to be printed onto an electrically charged, selenium-coated, rotating, cylindrical drum (or, more commonly in subsequent versions, a drum called an organic photoconductor made of N-vinylcarbazole, an organic monomer). Photoconductivity allows the charged electrons to fall away from the areas exposed to light. Powdered ink (toner) particles are then electrostatically attracted to the charged areas of the drum that have not been laser-beamed. The drum then transfers the image onto paper (which is passed through the machine) by direct contact. Finally the paper is passed onto a finisher, which uses intense heat to instantly fuse the toner/image onto the paper.\n\nThere are typically seven steps involved in the process:\n\nThe document to be printed is encoded in a page description language such as PostScript, Printer Command Language (PCL), or Open XML Paper Specification (OpenXPS). The raster image processor converts the page description into a bitmap which is stored in the printer's raster memory. Each horizontal strip of dots across the page is known as a raster line or scan line.\n\nLaser printing differs from other printing technologies in that each page is always rendered in a single continuous process without any pausing in the middle, while other technologies like inkjet can pause every few lines. To avoid a buffer underrun (where the laser reaches a point on the page before it has the dots to draw there), a laser printer typically needs enough raster memory to hold the bitmap image of an entire page.\n\nMemory requirements increase with the square of the dots per inch, so 600 dpi requires a minimum of 4 megabytes for monochrome, and 16 megabytes for color (still at 600 dpi). For fully graphical output using a page description language, a minimum of 1 megabyte of memory is needed to store an entire monochrome letter/A4 sized page of dots at 300 dpi. At 300 dpi, there are 90,000 dots per square inch (300 dots per linear inch). A typical 8.5 × 11 sheet of paper has margins, reducing the printable area to , or 84 square inches. 84 sq/in × 90,000 dots per sq/in = 7,560,000 dots. 1 megabyte = 1,048,576 bytes, or 8,388,608 bits, which is just large enough to hold the entire page at 300 dpi, leaving about 100 kilobytes to spare for use by the raster image processor.\n\nIn a color printer, each of the four CMYK toner layers is stored as a separate bitmap, and all four layers are typically preprocessed before printing begins, so a minimum of 4 megabytes is needed for a full-color letter-size page at 300 dpi.\n\nDuring the 1980s, memory chips were still very expensive, which is why entry-level laser printers in that era always came with four-digit suggested retail prices in US dollars. Memory prices later plunged, and 1200 dpi printers have been widely available in the consumer market since 2008. 2400 dpi electrophotographic printing plate makers, essentially laser printers that print on plastic sheets, are also available.\n\nIn older printers, a corona wire positioned parallel to the drum or, in more recent printers, a primary charge roller, projects an electrostatic charge onto the photoreceptor (otherwise named the photo conductor unit), a revolving photosensitive drum or belt, which is capable of holding an electrostatic charge on its surface while it is in the dark.\n\nAn AC bias voltage is applied to the primary charge roller to remove any residual charges left by previous images. The roller will also apply a DC bias on the drum surface to ensure a uniform negative potential.\n\nNumerous patents describe the photosensitive drum coating as a silicon sandwich with a photocharging layer, a charge leakage barrier layer, as well as a surface layer. One version uses amorphous silicon containing hydrogen as the light receiving layer, Boron nitride as a charge leakage barrier layer, as well as a surface layer of doped silicon, notably silicon with oxygen or nitrogen which at sufficient concentration resembles machining silicon nitride.\n\nA laser printer uses a laser because lasers are able to form highly focused, precise, and intense beams of light, especially over the short distances inside of a printer. The laser is aimed at a rotating polygonal mirror which directs the light beam through a system of lenses and mirrors onto the photoreceptor drum, writing pixels at rates up to sixty five million times per second. The drum continues to rotate during the sweep, and the angle of sweep is canted very slightly to compensate for this motion. The stream of rasterized data held in the printer's memory rapidly turns the laser on and off as it sweeps.\n\nThe laser beam neutralizes (or reverses) the charge on the surface of the drum, leaving a static electric negative image on the drum's surface which will repel the negatively charged toner particles. The areas on the drum which were struck by the laser, however, momentarily have no charge, and the toner being pressed against the drum by the toner-coated developer roll in the next step moves from the roll's rubber surface to the charged portions of the surface of the drum.\n\nSome non-laser printers (LED printers) use an array of light-emitting diodes spanning the width of the page to generate an image, rather than using a laser. \"Exposing\" is also known as \"writing\" in some documentation.\n\nAs the drums rotate, toner is continuously applied in a 15-micron-thick layer to the \"developer roll\". The surface of the photoreceptor with the latent image is exposed to the toner-covered developer roll.\n\nToner consists of fine particles of dry plastic powder mixed with carbon black or coloring agents. The toner particles are given a negative charge inside the toner cartridge, and as they emerge onto the developer drum they are electrostatically attracted to the photoreceptor's latent image (the areas on the surface of the drum which had been struck by the laser). Because negative charges repel each other, the negatively charged toner particles will not adhere to the drum where the negative charge (imparted previously by the charge roller) remains.\n\nA sheet of paper is then rolled under the photoreceptor drum, which has been coated with a pattern of toner particles in the exact places where the laser struck it moments before. The toner particles have a very weak attraction to both the drum and the paper, but the bond to the drum is weaker and the particles transfer once again, this time from the drum's surface to the paper's surface. Some machines also use a positively charged \"transfer roller\" on the back side of the paper to help pull the negatively charged toner from the photoreceptor drum to the paper.\n\nThe paper passes through rollers in the fuser assembly, where temperatures up to and pressure are used to permanently bond the toner to the paper. One roller is usually a hollow tube (heat roller) and the other is a rubber backed roller (pressure roller). A radiant heat lamp is suspended in the centre of the hollow tube, and its infrared energy uniformly heats the roller from the inside. For proper bonding of the toner, the fuser roller must be uniformly hot.\n\nSome printers use a very thin flexible metal foil roller, so there is less thermal mass to be heated and the fuser can more quickly reach operating temperature. If paper moves through the fuser more slowly, there is more roller contact time for the toner to melt, and the fuser can operate at a lower temperature. Smaller, inexpensive laser printers typically print slowly, due to this energy-saving design, compared to large high speed printers where paper moves more rapidly through a high-temperature fuser with a very short contact time.\n\nAs the drum completes a revolution, it is exposed to an electrically neutral soft plastic blade which cleans any remaining toner from the photoreceptor drum and deposits it into a waste reservoir. A charge roller then re-establishes a uniform negative charge on the surface of the now clean drum, readying it to be struck again by the laser.\n\nOnce the raster image generation is complete, all steps of the printing process can occur one after the other in rapid succession. This permits the use of a very small and compact unit, where the photoreceptor is charged, rotates a few degrees and is scanned, rotates a few more degrees and is developed, and so forth. The entire process can be completed before the drum completes one revolution.\n\nDifferent printers implement these steps in distinct ways. LED printers use a linear array of light-emitting diodes to \"write\" the light on the drum. The toner is based on either wax or plastic, so that when the paper passes through the fuser assembly, the particles of toner melt. The paper may or may not be oppositely charged. The fuser can be an infrared oven, a heated pressure roller, or (on some very fast, expensive printers) a xenon flash lamp. The warmup process that a laser printer goes through when power is initially applied to the printer consists mainly of heating the fuser element.\n\nThe mechanism inside a laser printer is somewhat delicate and, once damaged, often impossible to repair. The drum in particular is a critical component: it must not be left exposed to ambient light for more than a few hours, as light is what causes it to lose its charge and will eventually wear it out. Anything that interferes with the operation of the laser such as a scrap of torn paper may prevent the laser from discharging some portion of the drum, causing those areas to appear as white vertical streaks. If the neutral wiper blade fails to remove residual toner from the drum's surface, that toner may circulate on the drum a second time, causing smears on the printed page with each revolution. If the charge roller becomes damaged or does not have enough power, it may fail to adequately negatively charge the surface of the drum, allowing the drum to pick up excessive toner on the next revolution from the developer roll and causing a repeated but fainter image from the previous revolution to appear down the page.\n\nIf the toner doctor blade does not ensure that a smooth, even layer of toner is applied to the developer roll, the resulting printout may have white streaks from this in places where the blade has scraped off too much toner. Alternatively if the blade allows too much toner to remain on the developer roll, the toner particles might come loose as the roll turns, precipitate onto the paper below, and become bonded to the paper during the fusing process. This will result in a general darkening of the printed page in broad vertical stripes with very soft edges.\n\nIf the fuser roller does not reach a high enough temperature or if the ambient humidity is too high, the toner will not fuse well to the paper and may flake off after printing. If the fuser is too hot, the plastic component of the toner may smear, causing the printed text to look like it is wet or smudged, or may cause the melted toner to soak through the paper to the back side.\n\nDifferent manufacturers claim that their toners are specifically developed for their printers, and that other toner formulations may not match the original specifications in terms of either tendency to accept a negative charge, to move to the discharged areas of the photoreceptor drum from the developer roll, to fuse appropriately to the paper, or to come off the drum cleanly in each revolution.\n\nAs with most electronic devices, the cost of laser printers has fallen markedly over the years. In 1984, the HP LaserJet sold for $3500, had trouble with even small, low resolution graphics, and weighed . , low-end monochrome laser printers can sell for less than $75. These printers tend to lack onboard processing and rely on the host computer to generate a raster image, but outperform the 1984 LaserJet in nearly all situations.\n\nLaser printer speed can vary widely, and depends on many factors, including the graphic intensity of the job being processed. The fastest models can print over 200 monochrome pages per minute (12,000 pages per hour). The fastest color laser printers can print over 100 pages per minute (6000 pages per hour). Very high-speed laser printers are used for mass mailings of personalized documents, such as credit card or utility bills, and are competing with lithography in some commercial applications.\n\nThe cost of this technology depends on a combination of factors, including the cost of paper, toner, drum replacement, as well as the replacement of other items such as the fuser assembly and transfer assembly. Often printers with soft plastic drums can have a very high cost of ownership that does not become apparent until the drum requires replacement.\n\nDuplex printing (printing on both sides of the paper) can halve paper costs and reduce filing volumes. Formerly only available on high-end printers, duplexers are now common on mid-range office printers, though not all printers can accommodate a duplexing unit. Duplexing can also give a slower page-printing speed, because of the longer paper path.\n\nIn a commercial environment such as an office, it is becoming increasingly common for businesses to use external software that increases the performance and efficiency of laser printers in the workplace. Software can be used to set rules dictating how employees interact with printers, such as setting limits on how many pages can be printed per day, limiting usage of color ink, and flagging jobs that appear to be wasteful. \n\nColor laser printers use colored toner (dry ink), typically cyan, magenta, yellow, and black (CMYK). While monochrome printers only use one laser scanner assembly, color printers often have two or more.\n\nColor printing adds complexity to the printing process because very slight misalignments known as registration errors can occur between printing each color, causing unintended color fringing, blurring, or light/dark streaking along the edges of colored regions. To permit a high registration accuracy, some color laser printers use a large rotating belt called a \"transfer belt\". The transfer belt passes in front of all the toner cartridges and each of the toner layers are precisely applied to the belt. The combined layers are then applied to the paper in a uniform single step.\n\nColor printers usually have a higher cost per page than monochrome printers (even if printing monochrome-only pages).\n\nManufacturers use a similar business model for both low-cost color laser printers and inkjet printers: the printers are sold cheaply while replacement toners and inks are relatively expensive. Color laser printers are much faster than inkjet printers and their running cost per page is usually slightly less. The print quality of color lasers is limited by their resolution (typically 600–1200 dpi) and their use of just four color toners. They often have trouble printing large areas of the same or subtle gradations of color. Inkjet printers designed for printing photos can produce much higher quality color images.\n\nAn in depth comparison of inkjet and laser printers suggest that laser printers are the ideal choice for a high quality, volume printer, while inkjet printers tend to focus on large-format printers and household units. Laser printers offer more precise edging and in-depth mono-chromatic color, but tend to cost more than a traditional inkjet printer.\n\nMany modern color laser printers mark printouts by a nearly invisible dot raster, for the purpose of traceability. The dots are yellow and about in size, with a raster of about . This is purportedly the result of a deal between the US government and printer manufacturers to help track counterfeiters. The dots encode data such as printing date, time, and printer serial number in binary-coded decimal on every sheet of paper printed, which allows pieces of paper to be traced by the manufacturer to identify the place of purchase, and sometimes the buyer.\n\nDigital rights advocacy groups such as the Electronic Frontier Foundation are concerned about this erosion of the privacy and anonymity of those who print.\n\nSimilar to inkjet printers, toner cartridges may contain smart chips that reduce the number of pages that can be printed with it (reducing the amount of usable ink or toner in the cartridge to sometimes only 50%), in an effort to increase sales of the toner cartridges. Besides being more expensive to the consumer, this technique also increases waste, and thus increases pressure on the environment. For these toner cartridges (as with inkjet cartridges), reset devices can be used to override the limitation set by the smart chip. Also, for some printers, online walk-throughs have been posted to demonstrate how to use up all the ink in the cartridge. These chips offer no benefit to the end consumer—all laser printers originally used an optical mechanism to assess the amount of remaining toner in the cartridge rather than using a chip to electrically count the number of printed pages, and the chip's only function was as an alternate method to decrease the cartridge's usable life.\n\nToner particles are formulated to have electrostatic properties and can develop static electric charges when they rub against other particles, objects, or the interiors of transport systems and vacuum hoses. Static discharge from charged toner particles can ignite combustible particles in a vacuum cleaner bag or create a small dust explosion if sufficient toner is airborne. Toner particles are so fine that they are poorly filtered by conventional household vacuum cleaner filter bags and blow through the motor or back into the room.\n\nIf toner spills into the laser printer, a special type of vacuum cleaner with an electrically conductive hose and a high-efficiency (HEPA) filter may be needed for effective cleaning. These specialized tools are called \"ESD-safe\" (Electrostatic Discharge-safe) or \"toner vacuums\".\n\nAs a normal part of the printing process, the high voltages inside the printer can produce a corona discharge that generates a small amount of ionized oxygen and nitrogen, which react to form ozone and nitrogen oxides. In larger commercial printers and copiers, an activated carbon filter in the air exhaust stream breaks down these noxious gases to prevent pollution of the office environment.\n\nHowever, some ozone escapes the filtering process in commercial printers, and ozone filters are not used at all in most smaller consumer printers. When a laser printer or copier is operated for a long period of time in a small, poorly ventilated space, these gases can build up to levels at which the odor of ozone or irritation may be noticed. A potential for creating a health hazard is theoretically possible in extreme cases.\n\nAccording to a 2012 study conducted in Queensland, Australia, some printers emit sub-micrometre particles which some suspect may be associated with respiratory diseases. Of 63 printers evaluated in the Queensland University of Technology study, 17 of the strongest emitters were made by HP and one by Toshiba. The machine population studied, however, was only those machines already in place in the building and was thus biased toward specific manufacturers. The authors noted that particle emissions varied substantially even among the same model of machine. According to Professor Morawska of the Queensland University of Technology, one printer emitted as many particles as a burning cigarette:\n\nMuhle et al. (1991) reported that the responses to chronically inhaled copying toner, a plastic dust pigmented with carbon black, titanium dioxide and silica were also similar qualitatively to titanium dioxide and diesel exhaust.\n\nIn December 2011, the Australian government agency Safe Work Australia reviewed existing research and concluded that \"no epidemiology studies directly associating laser printer emissions with adverse health outcomes were located\" and that several assessments conclude that \"risk of direct toxicity and health effects from exposure to laser printer emissions is negligible\". The review also observes that, because the emissions have been shown to be volatile or semi-volatile organic compounds, \"it would be logical to expect possible health effects to be more related to the chemical nature of the aerosol rather than the physical character of the ‘particulate’ since such emissions are unlikely to be or remain as ‘particulates’ after they come into contact with respiratory tissue\".\n\nAfter the 2010 cargo plane bomb plot, in which shipments of laser printers with explosive-filled toner cartridges were discovered on separate cargo airplanes, the US Transportation Security Administration prohibited pass-through passengers from carrying toner or ink cartridges weighing over on inbound flights, in both carry-on and checked luggage. \"PC Magazine\" noted that the ban would not impact most travelers, as the majority of cartridges do not exceed the proscribed weight.\n\n\n"}
{"id": "43336896", "url": "https://en.wikipedia.org/wiki?curid=43336896", "title": "List of Ireland mobile virtual network operators", "text": "List of Ireland mobile virtual network operators\n\nMobile virtual network operators (MVNOs) in Ireland lease wireless telephone and data spectrum from major carriers such as Vodafone,eirMobile , and Three for resale.\n"}
{"id": "13625222", "url": "https://en.wikipedia.org/wiki?curid=13625222", "title": "MULTICOM", "text": "MULTICOM\n\nIn U.S. and Canadian aviation, MULTICOM is a frequency allocation used as a Common Traffic Advisory Frequency (CTAF) by aircraft near airports where no air traffic control is available. Frequency allocations vary from region to region.\n\nDespite the use of uppercase letters, MULTICOM is not an abbreviation or acronym.\n\nIn the United States, there is one MULTICOM frequency: 122.9 MHz. (See AIM table 4-1-2 or AIM table 4-1-1) At uncontrolled airports without a UNICOM, pilots are to self-announce on the MULTICOM frequency.\n\nIn Australia, there is one MULTICOM frequency: 126.7 MHz.\n\nIn Brazil, there is one MULTICOM frequency: 123.45 MHz.\n\n"}
{"id": "19237796", "url": "https://en.wikipedia.org/wiki?curid=19237796", "title": "Managed facilities-based voice network", "text": "Managed facilities-based voice network\n\nA managed facilities-based voice network (MFVN) is a physical network owned and operated by a voice service provider that delivers traditional telephone service via a loop start analog telephone interface. MFVNs are interconnected with the public switched telephone network (PSTN) and provide dialtone to end users. Historically, this was provided by equipment at Bell company central offices, however today's MFVNs can include a combination of access network (last mile network of copper, coaxial cable, or fiber optics), customer premises equipment (CPE), network switches and routers, network management systems, voice call servers, and gateways to the larger PSTN.\n\nMFVN providers include cable operators and telephone companies, but do not include Internet based providers such as Vonage, Magic Jack, and others that use the public internet to carry calls.\n\nMFVN providers:\n\nThe term MFVN was introduced in 2007 by various telephony user organizations and stakeholders who rely on telephone service to provide security and life safety services. The concern of these organizations and stakeholders was the reliability of new telephone technology and services. This new technology was based on packet voice technology, or the Voice over Internet Protocol, which was not well understood. These organizations and stakeholders increasingly realized that they could no longer simply assume that phone service would be reliable enough, because it was increasingly being delivered in various ways, even by traditional providers. Clear performance requirements were needed to define when a phone line was suitable for security and life safety services.\n\nThis issue was not new, as analog copper based networks had been transitioning to digital telephony technology for 25 years (via fiber buildout by telephone companies), and to IP technology methods for the last 10 years (via broadband buildout by telco, cable, and competitive local exchange carriers). What was new was that copper based analog phone service was not even an option anymore in many areas, as it was being completely replaced by digital and IP based phone service.\n\nStarting in the early part of the 2000s, IP based voice services began being offered by non-traditional providers such as cable television service providers and Internet voice service providers. The demand for these services grew due to competitive pricing and value added services not offered by the traditional telephone providers. The use of these non-traditional telephone methods for security and life safety communications was not well understood, so use was discouraged and in some cases not allowed by local authorities. There was no distinction between voice services provided over the \"best-effort\" Internet and voice services provided over managed facilities. It became clear that only managed facilities based providers could assure reliability end to end. Only facilities based providers could monitor and maintain the expected quality of service (call quality, operation during power failure, wiring procedures that guaranteed pre-emption of existing calls for emergency calls, and local disaster recovery capabilities).\n\nIn 2007, the concept of the Managed Facilities-based Voice Network was introduced by non-traditional telephone providers as a way to think about the PSTN as a collection of managed networks, rather than as a single, monolithic entity.\n\nThe National Fire Protection Association incorporated this concept into the latest fire code, NFPA 72 2010, which is now the basis for determining whether a given phone line is an acceptable method for fire alarm signaling transmission from a protected premises to a supervising central monitoring station. Local authorities, such as fire inspectors, now no longer need to make these determinations on an individual case basis.\n\nStates have begun to recognize and accept the use of MFVN. In Florida, it has been adopted by statute, whereby all qualified MFVNs are now allowed for fire alarm monitoring. \n\nThe following diagram is a high level view of the different types of MFVNs vs non-MFVNs compared side-by-side. They are the non-MFVN Internet VoIP, Plain Old Telephone Service MFVN (POTS), MFVN Cable, MFVN DSL, and MFVN Fiber.\n\n"}
{"id": "56143789", "url": "https://en.wikipedia.org/wiki?curid=56143789", "title": "MasterClass", "text": "MasterClass\n\nMasterClass is a San Francisco-based online education platform. Students can access tutorials and lectures pre-recorded by experts in various fields.\n\nThe concept was conceived by David Rogier and Aaron Rasmussen.\n\nIn addition to the video classes, class workbook and homework are also part of the course. In the case of actor Dustin Hoffman's course, the \"interactive assignments\" involved the student acting with other students, either in person or over Skype.\n\nThe general consensus among critics is that while the classes do not teach technical skills to improve proficiency in the craft, they provide insight into the grueling nature of artistic pursuits and striving for perfection while inspiring a love of the craft.\n\n\"The Verge\" noted that while the subscriptions last a lifetime, each course has little replay value, and so it is challenging to keep the student learning within the platform once they have viewed the video classes. Reviewopedia noted that while there are many educational tutorial platforms, this has a point of difference as having teachers that are worldwide recognised in their fields. \"Deadline\" noted that the classes are not interactive, which is a symptom of online courses in general.\n\nKevin Spacey's and Dustin Hoffman's acting courses were removed in late 2017 after multiple sexual assault allegations had come out against both actors.\n"}
{"id": "40714668", "url": "https://en.wikipedia.org/wiki?curid=40714668", "title": "Microwave analog signal processing", "text": "Microwave analog signal processing\n\nReal-time Analog Signal Processing (R-ASP), as an alternative to DSP-based processing, might be defined as the manipulation of signals in their pristine analog form and in real time to realize specific operations enabling microwave or millimeter-wave and terahertz applications.\n\nThe exploding demand for higher spectral efficiency in radio has spurred a renewed interest in analog real-time components and systems beyond conventional purely digital signal processing techniques. Although they are unrivaled at low microwave frequencies, due to their high flexibility, compact size, low cost and strong reliability, digital devices suffer of major issues, such as poor performance, high cost of A/D and D/A converters and excessive power consumption, at higher microwave and millimeter-wave frequencies. At such frequencies, analog devices and related real-time or analog signal processing (ASP) systems, which manipulate broadband signals in the time domain, may be far preferable, as they offer the benefits of lower complexity and higher speed, which may offer unprecedented solutions in the major areas of radio engineering, including communications, but also radars, sensors, instrumentation and imaging. This new technology might be seen as microwave and millimeter-wave counterpart of ultra-fast optics signal processing, and has been recently enabled by a wide range of novel phasers, that are components following arbitrary group delay versus frequency responses.\n"}
{"id": "25208936", "url": "https://en.wikipedia.org/wiki?curid=25208936", "title": "Mobile technology", "text": "Mobile technology\n\nMobile technology is the technology used for cellular communication. Mobile code-division multiple access (CDMA) technology has evolved rapidly over the past few years. Since the start of this millennium, a standard mobile device has gone from being no more than a simple two-way pager to being a mobile phone, GPS navigation device, an embedded web browser and instant messaging client, and a handheld gaming console. Many experts believe that the future of computer technology rests in mobile computing with wireless networking. Mobile computing by way of tablet computers are becoming more popular. Tablets are available on the 3G and 4G networks.\n\nIn the early 1980s, 1G was introduced as voice-only communication via \"brick phones\". Later in 1991, the development of 2G introduced Short Message Service (SMS) and Multimedia Messaging Service (MMS) capabilities, allowing picture messages to be sent and received between phones. In 1998, 3G was introduced to provide faster data-transmission speeds to support video calling and internet access. 4G was released in 2008 to support more demanding services such as gaming services, HD mobile TV, video conferencing, and 3D TV. 5G technology has been planned for the upcoming future.\n\n4G is the current mainstream cellular service offered to cell phone users, performance roughly 10 times faster than 3G service. One of the most important features in the 4G mobile networks is the domination of high-speed packet transmissions or burst traffic in the channels. The same codes used in the 2G-3G networks are applied to 4G mobile or wireless networks, the detection of very short bursts will be a serious problem due to their very poor partial correlation properties. Recent study has indicated that traditional multilayer network architecture based on the Open Systems Interconnection (OSI) model may not be well suited for 4G mobile network, where transactions of short packets will be the major part of the traffic in the channels. As the packets from different mobiles carry completely different channel characteristics, the receiver should execute all necessary algorithms, such as channel estimation, interactions with all upper layers and so on, within a very short period of time.\n\nMany types of mobile operating systems (OS) are available for smartphones, including Android, BlackBerry OS, webOS, iOS, Symbian, Windows Mobile Professional (touch screen), Windows Mobile Standard (non-touch screen), and Bada. The most popular are the Apple iPhone, and the newest: Android. Android, a mobile OS developed by Google, is the first completely open-source mobile OS, meaning that it is free to any cell phone mobile network.\n\nSince 2008 customizable OSs allow the user to download apps like games, GPS, utilities, and other tools. Users can also create their own apps and publish them, e.g. to Apple's App Store. The Palm Pre using webOS has functionality over the Internet and can support Internet-based programming languages such as Cascading Style Sheets (CSS), HTML, and JavaScript. The Research In Motion (RIM) BlackBerry is a smartphone with a multimedia player and third-party software installation. The Windows Mobile Professional Smartphones (Pocket PC or Windows Mobile PDA) are like personal digital assistants (PDA) and have touchscreen abilities. The Windows Mobile Standard does not have a touch screen but uses a trackball, touchpad, or rockers.\n\nThere will be a hit to file sharing, the normal web surfer would want to look at a new web page every minute or so at 100 kbs a page loads quickly. Because of the changes to the security of wireless networks users will be unable to do huge file transfers because service providers want to reduce channel use. AT&T claimed that they would ban any of their users that they caught using peer-to-peer (P2P) file sharing applications on their 3G network. It then became apparent that it would keep any of their users from using their iTunes programs. The users would then be forced to find a Wi-Fi hotspot to be able to download files. The limits of wireless networking will not be cured by 4G, as there are too many fundamental differences between wireless networking and other means of Internet access. If wireless vendors do not realize these differences and bandwidth limits, future wireless customers will find themselves disappointed and the market may suffer setbacks.\n\nIncreasing mobile technology use has changed how the modern family interacts with one another through technology. With the rise of mobile devices, families are becoming increasingly \"on-the-move\", and spend less time in physical contact with one another. However, this trend does not mean that families are no longer interacting with each other, but rather have evolved into a more digitized variant. A study has shown that the modern family actually learns better with usage of mobile media, and children are more willing to cooperate with their parents via a digital medium than a more direct approach. For example, family members can share information from articles or online videos via mobile devices and thus stay connected with one another during a busy day.\n\nThis trend is not without controversy, however. Many parents of elementary school-age children express concern and sometimes disapproval of heavy mobile technology use. Parents may feel that excessive usage of such technologies distracts children from \"un-plugged\" bonding experiences, and many express safety concerns about children using mobile media. While parents may have many concerns are, they are not necessarily anti-technology. In fact, many parents express approval of mobile technology usage if their children can learn something from the session. for example, through art or music tutorials on YouTube.\n\nThe next generation of smartphones will be context-aware, taking advantage of the growing availability of embedded physical sensors and data exchange abilities. One of the main features applying to this is that phones will start keeping track of users' personal data, and adapt to anticipate the information will need. All-new applications will come out with the new phones, one of which is an X-ray device that reveals information about any location at which the phone is pointed. Companies are developing software to take advantage of more accurate location-sensing data. This has been described as making the phone a virtual mouse able to click the real world. An example would be pointing the phone's camera at a building while having the live feed open, and the phone will show text with the image of the building, and save its location for use in the future.\n\nOmnitouch is a device via which apps can be viewed and used on a hand, arm, wall, desk, or any other everyday surface. The device uses a sensor touch interface, which enables the user to access all the functions through the use of the touch of a finger. It was developed at Carnegie Mellon University. This device uses a projector and camera worn on the user's shoulder, with no controls other than the user's fingers.\n"}
{"id": "27393959", "url": "https://en.wikipedia.org/wiki?curid=27393959", "title": "Nocton Dairies controversy", "text": "Nocton Dairies controversy\n\nNocton Dairies is a British company which was formed by Devon farmer and cheese-maker Peter Willes and Lancashire milk producer David Barnes in order to construct an 8,100-cow dairy at Nocton Heath in Lincolnshire, objectors to which claimed that it would have been the largest in Western Europe.\n\nA planning application to North Kesteven District Council was made on 17 December 2009, but after concerns raised by the Environment Agency, was withdrawn on 15 April 2010 having already aroused considerable reaction in the media.\n\nAfter a premature report that the company had resubmitted its plans in August 2010, a revised application was submitted on 17 November 2010 for a 3,770-cow dairy, the reduced size being intended to address some concerns.\n\nWith the new application lodged, public concern was raised again and opposition became increasingly vociferous. On 16 February 2011, the company withdrew its second planning application and announced that it was abandoning its plans. A statement released by Nocton Dairies cited the objections of the Environment Agency as the sole reason and raised concerns that facts had been twisted on animal welfare matters. It added: \"The concept we have been proposing is a sound one. We challenge other farmers to pick up the baton and see where these concepts can take them.\" Just hours after Nocton Dairies' shock announcement, officers at North Kesteven District Council took the unusual step of making a public statement that they had been minded to recommend refusal of the application, on six grounds, namely:\n\n\nMeanwhile, two separate but associated plans supporting the dairy were still active; a pipeline for the transportation of the digestate to be produced and a water storage reservoir intended for either water for dairy cows or for better management of water resources on the arable land, should the application for the dairy farm be refused. Nocton Dairies did not withdraw either of these plans. In February 2011 North Kesteven District Council refused planning permission for the pipeline and the following month it refused permission for the reservoir, irrespective of its dual functionality.\n\nNocton Dairies' initial application aroused much opposition, including an Early Day Motion signed by 172 MPs in the House of Commons on 8 March 2010; it was labelled by media reports at the time as a 'battery' farm for cows, despite reports of a growing number of similar (albeit smaller) indoor-based systems for dairy cows already successfully operating in the UK and Nocton Dairies' explanations that their housing plans mirrored the open housing systems in which all British cows were already kept for the winter months.\nAfter concerns were voiced over the potential for pollution of the water aquifer, smells, animal welfare, disease control, security, transport issues and property blight surrounding the site, some local people formed a campaign group named CAFFO. A Number 10 e-petition, calling for a public inquiry into the development, was signed by 1,234 people in three weeks before the web site was closed for the duration of the 2010 UK election period. However, a later Government response emphasised the rigors of the planning process and current UK legislation ensuring high welfare standards.\n\nAnimal welfare charities and campaigners, and vegan and vegetarian groups also joined to support the case against the dairy. A Facebook group set up by Viva! calling for a halt to construction attracted over 7,500 members. WSPA (now called WAP) launched a campaign in September 2010 in anticipation of the resubmission of Nocton Dairies' proposal, featuring celebrities including Twiggy, Andrew Sachs, Chrissie Hynde, Jenny Seagrove and a large number of soap stars, and attracted over 25,000 pledges from people around the world that 'factory milk from battery cows' would not be used in their cuppas. The results of an Ipsos MORI survey released at the launch showed that 61% of those questioned said they would never buy milk produced in large-scale indoor dairy sheds. Compassion in World Farming's 'Cows belong in fields' campaign was launched late 2010, and the CPRE also campaigned on the issue. As well as this, a campaign was set up through site 38 Degrees who submitted a petition of over 50,000 signatures to the district council in January 2011 on the basis that the farm was cruel and would put other farmers out of business. 38 Degrees also singled out neighbouring farmers who had been keen to use the 'digestate' (left from the cow manure after anaerobic digestion had taken place) on their arable land as a more sustainable and natural source of fertilisers and to replace essential organic matter. These farmers pulled out of their agreements due to fear of reprisals after being named in adverts which urged the public to target them directly.\n\nHowever, a letter sent by the Farm Animal Welfare Council to government ministers stated that cow welfare need not be compromised in large dairy units, a message echoed by the RSPCA, which, despite not permitting year-round housing for dairy cows within its Freedom Foods standards, said it didn't believe 'big was necessarily bad' and in fact could offer welfare benefits if implemented correctly. Other debates range around the potential for a large dairy such as this to improve food security and opportunities to reduce the carbon footprint of milk production through better efficiency and the adoption of technology such as anaerobic digestion. More recently the Government has published its Foresight report on Food and Farming, and the dairy's developers have asserted that their plans would help address the report's conclusions that farming needs to produce more food using fewer resources while tackling climate change.\n\nDuring the consultation period, the Council reported as many as 14,000 objections had been lodged with PETA claiming responsibility for at least 6,000 CIWF 5,000, and other animal and vegan groups claiming many more; this was substantiated by an extensive social media campaign orchestrated by these groups against the proposal. However, this was countered by growing dairy industry support for the Nocton proposal, illustrated in submissions from the National Farmers Union (England and Wales) (NFU), the Country Land and Business Association, and Dairy UK, representing processors and farmers. Dairy UK warned: \"Reject Nocton and UK dairy will suffer.\"\n\nSupermarket chains Sainsbury, Tesco, Waitrose and Marks & Spencer, as well as online food retailer Ocado, all indicated they did not intend to buy milk from 'super-dairies', while Morrisons and American-owned Asda seemed to support them. According to \"The Independent\", Morrisons said they would consider buying from the farm, while Asda said they refused to answer such a \"hypothetical\" question.\n\nIn letters to a Parliamentary group in March 2010, Tesco and Sainsbury stressed their commitment to animal welfare and stated that they had no plans to buy milk from Nocton. In another letter, Waitrose's managing director Mark Price stated, \"a dairy farm of the size proposed would not fit with the Waitrose way of doing business, and I have to say that I am anxious that it represents the first step along the way towards a highly-industrialised, US approach to farming\". On 17 November 2010, Marks & Spencer declared, \"M&S does not buy milk from 'super-dairy' farms and we are committed to our current pool of dedicated dairy farms.\" Jason Gissing, co-founder of Ocado, said in a letter published on the Ocado web site in December 2010, \"Rest assured, Ocado will not be milking it with Nocton.\".\n\nHowever, Morrisons again stressed its willingness to consider buying the farm's milk at the NFU conference in February 2011, saying the supermarket was open-minded about purchasing milk from Nocton-style dairies. And despite the stand taken by these other supermarkets, their words were called into question when it was revealed in an industry newsletter that a number of them already willingly take supply from larger indoor-based UK dairy farms.\n\nThe consortium of opponents of the dairy - Vegetarian International Voice for Animals, The Soil Association, CPRE, Compassion in World Farming, Friends of the Earth, WSPA and local campaign group CAFFO - expressed delight that the plans had been withdrawn when the news was announced on 16 February 2011. WSPA UK's director Suzi Morris said: \"This is fantastic news and greatly welcomed. This is a victory for consumers, dairy farmers and of course the cows within it and we can't forget the Lincolnshire community which has had a narrow escape.\"\n\nThe industry reaction was somewhat different. The news coincided with the end of the NFU conference where NFU president Peter Kendall, Agriculture Minister Jim Paice and food critic Jay Rayner were among those defending the concept of large scale farming. The news was greeted with concern that the dairy industry would find it hard to meet future challenges if it could not evolve and develop. Mansel Raymond, chair of the NFU's dairy board, said: \"It is disappointing that the application has been withdrawn. Any planned investment in the dairy industry is a positive step. Nocton was an imaginative and innovative proposal, and I firmly believe that there remains a place in Britain for this type of investment if we are to meet the growing demand for food.” The Royal Association of British Dairy Farmers also expressed its disappointment at the withdrawal. The British Cattle Veterinary Association, which leads the industry in managing the health and welfare of dairy cows in the UK, also took the steps of dismissing claims that Nocton Dairies would have been a \"cow prison\", saying its developers demonstrated a commitment to good welfare.\n\nWhether or not this or indeed any so called 'super-dairy' is ever built, this controversy is likely to have a lasting impact on Britain's future dairy policy. Supporters claim large scale farming offers opportunities to meet food security and climate change challenges of the future. Research by opponents counter this hypothesis with an argument that the economics of the system are unsustainable in a report titled: 'Weighing the economics of dairy farms'; however, the figures used in this report have also been heavily criticised by the industry's leading providers of economics data DairyCo, part of the Agriculture and Horticulture Development Board, saying that the different systems are not being compared on a like for like basis and are ‘astonishingly naïve’. However, there are concerns that any similar proposal, if successful, would set a precedent for the development of large-scale farming systems more commonly associated with the US where such units are known as CAFOs - although with the definition of a dairy CAFO being over 750 cows, DairyCo, the dairy industry's levy and advisory body, says there are already at least 12 such farms operating successfully and without issues - as proven by the lack of media interest in these farms - in England.\n\nComments made by at the 2011 NFU conference by food critic Jay Rayner during a panel discussion entitled ‘Is modern agriculture palatable?’ pointed to the industry being at fault. Mr Rayner told farmers that perception that the public would disapprove of such a development had been given fuel by a lack of positive PR. “The industry has to look at how it communicates to the media, not just to the industry and government. Basically it needs to work out a way to kill those page three Daily Mail stories which misrepresent what agriculture is. There is a failure of imagination. You need to employ some PR people to communicate the realities of agricultural production in the 21st century.”\n\nEither way, the fact that many farmers are prosecuted by the Environment Agency for pollution or waste offences as they struggle to adhere to constantly tightening regulation, demonstrates that environmental legislation is crucial to the protection of the land where any such development is placed. Following the withdrawal and refusal of Nocton Dairies' plans, one of the directors, Peter Willes, had to pay over £23,000 when he accepted responsibility for three environmental offences, two of which related to pollution of water courses. Mr Willes' previous environmental offences were also outlined in a widely criticised Daily Mail article on mega dairies, which drew unrepresentative comparisons against US practices long banned in the UK and Europe.\n\nAll involved also learned that social media plays an important part in 21st century campaigning as while they had no direct impact on the withdrawal of the application, thousands of supporters were gained via the range of sites named earlier. However, it also became clear that as well as factual information, a number of myths and untruths were also being propagated over social media - one led to a petition with 15,000 signatures being withdrawn as it falsely claimed hormones and tail docking would form part of the plan, prompting concerns that support generated in this way might contain little substance. Campaigners also learned that developers will both take issue with their efforts and will threaten legal action where they feel inaccuracies have been portrayed; Nocton Dairies' referral of local group CAFFO to the Advertising Standards Authority (United Kingdom) was 'informally resolved' when CAFFO notified the ASA that the leaflet had already been distributed and there were no immediate plans to publish another, stopping the investigation in its tracks. At a later date, another large scale farm developer, Midland Pig Producers, threatened legal action against the Soil Association because of unsubstantiated and libellous allegations.\n\n"}
{"id": "10792673", "url": "https://en.wikipedia.org/wiki?curid=10792673", "title": "Norfolk cases", "text": "Norfolk cases\n\nNorfolk leather cases were luggage made in the 1940s and 1950s for business and domestic applications. They came in a range of sizes from suitcases to a much smaller attaché case. Norfolk cases were made of moulded leather that resulted in cases with no cut or sewn corners.\n"}
{"id": "174814", "url": "https://en.wikipedia.org/wiki?curid=174814", "title": "Phaistos Disc", "text": "Phaistos Disc\n\nThe Phaistos Disc (also spelled Phaistos Disk, Phaestos Disc) is a disk of fired clay from the Minoan palace of Phaistos on the island of Crete, possibly dating to the middle or late Minoan Bronze Age (second millennium B.C.). The disk is about 15 cm (5.9 in) in diameter and covered on both sides with a spiral of stamped symbols. Its purpose and meaning, and even its original geographical place of manufacture, remain disputed, making it one of the most famous mysteries of archaeology. This unique object is now on display at the archaeological museum of Heraklion.\n\nThe disc was discovered in 1908 by the Italian archaeologist Luigi Pernier in the Minoan palace-site of Phaistos, and features 241 tokens, comprising 45 distinct signs, which were apparently made by pressing hieroglyphic \"seals\" into a disc of soft clay, in a clockwise sequence spiraling toward the center of the disk.\n\nThe Phaistos Disc captured the imagination of amateur and professional archaeologists, and many attempts have been made to decipher the code behind the disc's signs. While it is not clear that it is a script, most attempted decipherments assume that it is; most additionally assume a syllabary, others an alphabet or logography. Attempts at decipherment are generally thought to be unlikely to succeed unless more examples of the signs are found, as it is generally agreed that there is not enough context available for a meaningful analysis.\n\nAlthough the Phaistos Disc is generally accepted as authentic by archaeologists, a few scholars believe that the disc is a forgery or a hoax.\n\nThe Phaistos Disc was discovered in the Minoan palace-site of Phaistos, near Hagia Triada, on the south coast of Crete; specifically the disc was found in the basement of room 8 in building 101 of a group of buildings to the northeast of the main palace. This grouping of four rooms also served as a formal entry into the palace complex. Italian archaeologist Luigi Pernier recovered the intact \"dish\", about in diameter and uniformly slightly more than in thickness, on 3 July 1908 during his excavation of the first Minoan palace.\n\nIt was found in the main cell of an underground \"temple depository\". These basement cells, only accessible from above, were neatly covered with a layer of fine plaster. Their content was poor in precious artifacts, but rich in black earth and ashes, mixed with burnt bovine bones. In the northern part of the main cell, in the same black layer, a few inches south-east of the disc and about above the floor, Linear A tablet \"PH 1\" was also found. The site apparently collapsed as a result of an earthquake, possibly linked with the eruption of the Santorini volcano that affected large parts of the Mediterranean region during the mid second millennium B.C.\n\nThe Phaistos Disc is generally accepted as authentic by archaeologists. The assumption of authenticity is based on the excavation records by Luigi Pernier. This assumption is supported by the later discovery of the Arkalochori Axe with similar but not identical glyphs.\n\nThe possibility that the disc is a 1908 forgery or hoax has been raised by two scholars. According to a report in \"The Times\" the date of manufacture has never been established by thermoluminescence. In his 2008 review, Robinson does not endorse the forgery arguments, but argues that \"a thermoluminescence test for the Phaistos Disc is imperative. It will either confirm that new finds are worth hunting for, or it will stop scholars from wasting their effort.\"\n\nA gold signet ring from Knossos (the Mavro Spilio ring), found in 1926, contains a Linear A inscription developed in a field defined by a spiral—similar to the Phaistos Disc. A sealing found in 1955 shows the only known parallel to sign 21 (𐇤, the “comb”) of the Phaistos disc. This is considered as evidence that the Phaistos Disc is a genuine Minoan artifact.\n\nYves Duhoux (1977) dates the disc to between 1850 B.C. and 1600 B.C. (MMIII) on the basis of Luigi Pernier's report, which says that the Disc was in a Middle Minoan undisturbed context. Jeppesen (1963) dates it to after 1400 (LMII-III). Doubting the viability of Pernier's report, Louis Godart (1990) resigns himself to admitting that archaeologically, the disc may be dated to anywhere in Middle or Late Minoan times (MMI-LMIII, a period spanning most of the second millennium B.C.). J. Best suggests a date in the first half of the fourteenth century B.C. (LMIIIA) based on his dating of tablet PH 1.\n\nThe inscription was apparently made by pressing hieroglyphic \"seals\" into the soft clay, in a clockwise sequence spiraling toward the center of the disk. It was then fired at high temperature. The unique character of the Phaistos Disc stems from the fact that the entire text was inscribed in this way, reproducing a body of text with reusable characters.\n\nThe German typesetter and linguist Herbert Brekle, in his article \"The typographic principle\" in the \"Gutenberg-Jahrbuch\", argues that the Phaistos Disc is an early document of movable type printing, since it meets the essential criterion of typographic printing, that of type identity:\n\nAs a medieval example for the same technique he goes on to cite the Prüfening dedicatory inscription.\n\nIn his work on decipherment, Benjamin Schwartz also refers to the Phaistos Disc as \"the first movable type\".\n\nIn his popular science book \"Guns, Germs and Steel\", Jared Diamond describes the disc as an example of a technological advancement that did not become widespread because it was made at the wrong time in history, and contrasts this with Gutenberg's printing press.\n\nThere are 242 tokens on the disc, comprising 45 distinct signs. Many of these 45 signs represent easily identifiable everyday things. In addition to these, there is a small diagonal line that occurs underneath the final sign in a group a total of 18 times. The disc shows traces of corrections made by the scribe in several places. The 45 symbols were numbered by Arthur Evans from 01 to 45, and this numbering has become the conventional reference used by most researchers. Some symbols have been compared with Linear A characters by Nahm, Timm, and others. Other scholars (J. Best, S. Davis) have pointed to similar resemblances with the Anatolian hieroglyphs, or with Egyptian hieroglyphs (A. Cuny). In the table below, the character \"names\" as given by Louis Godart (1995) are given in upper case; where other description or elaboration applies, they are given in lower case.\n\nThe frequency distribution of the Phaistos Disc signs is:\n\nThe nine \"hapaxes\", i.e. occurring just once, are 04 (A5), 05 (B3), 11 (A13), 15 (B8), 17 (A24), 30 (B27), 42 (B9), 43 (B4), 44 (A7). Of the eight twice-occurring symbols, four (03, 21, 28, 41) occur on side A only, three (09, 16, 20) on side B only, and only one (14) on both sides.\n\nThere are a number of signs marked with an oblique stroke; the strokes are not imprinted but carved by hand, and are attached to the first or last sign of a \"word\", depending on the direction of reading chosen. Their meaning is a matter of discussion. One hypothesis, supported by Evans, Duhoux, Ohlenroth and others, is that they were used to subdivide the text into paragraphs, but alternative meanings have been offered by other scholars.\n\nEvans, at one point, published an assertion that the disc had been written, and should be read, from the center out; because it would have been easiest to place the inscription first and then size the disc to fit the text. There is general agreement that he was wrong, and Evans himself later changed his mind: the inscription was made, and should be read, from the outside in toward the centre. The centres of the spirals are not in the centre of the disc, and some of the symbols near the centre are crowded, as though the maker was cramped for space. One pair of symbols are set top-to-bottom, so it is hard to tell what order they should be in. Except in the cramped section, when there are overstrikes, the inner symbol overlies the outer symbol. Jean Faucounau has proposed a reconstruction of the scribe's movements, which would also require an inward direction; Yves Duhoux says that any outward reading may be discarded. Despite this consensus, there are still a few such attempted decipherments (See Phaistos Disc decipherment claims).\n\nIn addition to the question of the directionality of the text on the disc itself, different viewpoints are held as to how the Phaistos Disc characters should be displayed when transcribed into text. The disc itself probably has right-to-left directionality, if reading proceeds from the outside to the centre; this means that the reading direction is into the faces of the people and animals, as it is in Egyptian and Anatolian. Phaistos Disc characters are shown with left-to-right directionality in this article, with the glyphs mirrored compared to their orientation on the disc; which is also the typical practice for edited Egyptian and Anatolian hieroglyphic text.\n\nThe following is a rendering of the Phaistos Disc inscription in Unicode characters (the text will only be displayed correctly if a font that supports Unicode Phaistos Disc characters, such as Noto Sans Symbols or Everson Mono is installed):\n\nSide A\n\nSide B\n\nThere are 61 \"words\", 31 on side A and 30 on side B (numbered A1 to A31 and B1 to B30, outside to inside), here read outside-to-inside (putting the \"plumed head\" signs word-initially and the strokes word-finally). The shortest words are two symbols in length, the longest seven symbols. The strokes are here transcribed as diagonal strokes (/). The transcription begins at the vertical line of five dots, circling the rim of the disc once, clockwise (13 words on A, 12 words on B) before spiralling toward the center (18 more words on each side). There is one word-final effaced sign at A8, which Godart (1995:101) notes as resembling sign 3 or 20; or less probably 8 or 44. Evans considered side A as the front side, but technical arguments have since been forwarded favouring side B as the front side.\n\nThe signs in the transcription below appear in left-to-right orientation, and the reader may read into the faces of the human and animal figures (as one reads Egyptian and Anatolian hieroglyphs):\n\nIn numerical transcription:\n\nSide A:\n\nSide B:\n\nThe \"plumed head\" (02) only ever occurs word-initially, in 13 instances followed by the \"shield\" (12, which in some instances also occurs word-finally). Six words occur twice each:\nThe three-word sequence 02-27-25-10-23-18 28-01/ 02-12-31-26/ occurs twice (A14-16, A20-22). 02-12-31-26/ recurs for a third time (A19). Four more words occur twice each, 02-12-27-27-35-37-21 (A17, A29), 10-03-38 (A28, A31), 22-29-36-07-08/ (B21, B26) and 29-45-07/ (A3, B20).\n\nAs noted above, corrections have been made. Signs have been erased and over-printed by other signs.\n\nGodart (1995:99-107) describes these corrections, by word.\nThey occur in the following words: A1 (signs 02-12-13-01), A4 (29-29-34) together with A5 (02-12-04), A8 (12), A10 (02-41-19?-35), A12 (12), A16 (12-31-26?), A17 (second 27?), A29 (second 27?), B1 (12-22), B3 (37?), B4 (22-25 imprinted over the same), B10 (07?-24?-40?), B13 (beside 29?). Question marks indicate uncertainty about that particular sign being the result of a correction.\n\nAlso, the borders of word B28 has been widened to make room for sign 02. See Duhoux (1977:34-35) and Godart (1995:107).\n\nThe two signs 27 (Hide) in word A29 are rotated 180 degrees compared with all other occurrences of this sign: \"head down\" versus \"head up\". This rotation might be motivated by lack of space in A29; see Duhoux (1977), p.24 (section 6A-8). \n\nThe rotations of the signs 29 (Cat) and 31 (Eagle) have no lack of space. Defining the sign 29 in words B19, B20 and B21 as \"head to the right\", we have: head down in B29; head to the left in A3 and B15; head up in B18 and B26; head in between up and left in B13; head in between right and down in A4 (twice). The direction of the head of sign 31 is as follows: to the right in A16, up in A9 and A25, and to the left in A22.\n\nThe sign 02 (Plumed head) in word A29 is 90 degrees rotated to the right compared with all other occurrences of this sign. This might well be due to lack of space; the word is crowded and messy, with the sign 12 (Shield) pushed aside. \n\nThe two occurrences of sign 28 (Bull's leg) are not rotated compared with each other. Rather, the way this sign is shown in the literature (including Unicode), with the foot down, is rotated compared with the sign on the disc, with the foot up. \n\nIf one assumes the rotations are completely randomly distributed, then the probability that they end up in only two (or three) signs is very small. This suggests that these rotations might be deliberate.\n\nThere are several occurrences on side A where the same sign is at two places near each other in adjacent windings of the spiral. For example, consider the Plumed Head (sign 02) in word A1 and the Plumed Head in word A14. Three patterns of such occurrences have been identified. A computer analysis of one of them (involving most of the Plumed Head signs on side A) has been performed with the conclusion that the probability of this pattern being coincidental is small. The existence of the two other patterns further decreases the probability of coincidence.\n\nSeveral occurrences are caused by a correction. Also, the orientation of the signs seems to be relevant: the two Hides (sign 27) in word A29 are upside down, with the \"heads\" pointing to the Hide sign in the adjacent winding in word A23.\n\nIf this is indeed not coincidental then the inscription is not a one-dimensional text. Of course this does not give us a decipherment; rather it narrows down the potential decipherments.\n\nA great deal of speculation developed around the disc during the twentieth century. The Phaistos Disc captured the imagination of amateur archeologists. Many attempts have been made to decipher the code behind the disc's signs. Historically, almost anything has been proposed, including prayers, a narrative or an adventure story, a \"psalterion\", a call to arms, a board game, and a geometric theorem. Some of the more fanciful interpretations of its meaning are classic examples of pseudoarchaeology.\n\nMost linguistic interpretations assume a syllabary, based on the proportion of 45 symbols in a text of 241 tokens typical for that type of script; some assume a syllabary with interspersed logographic symbols, a property of every known syllabary of the Ancient Near East (Linear B as well as cuneiform and hieroglyphic writing). There are, however, also alphabetic and purely logographical interpretations.\n\nWhile enthusiasts still believe the mystery can be solved, scholarly attempts at decipherment are thought to be unlikely to succeed unless more examples of the signs turn up somewhere, as it is generally thought that there is not enough context available for meaningful analysis. Any decipherment without external confirmation, such as successful comparison to other inscriptions, is unlikely to be accepted as conclusive.\n\nThere are a few main theories about the origin of the signs. For the first few decades after its discovery most scholars argued strongly against the local origin of the artifact. Evans (1909:24f.) wrote that\nGlotz (1925:381) claimed that the clay was not from Crete. Ipsen (1929:15) concluded that the Disc was certainly from somewhere on the Aegean. Because of its differences from Linear A or B, Ipsen found it tempting to assume, like Evans, a non-Cretan origin for the Disc. He observes, however, that since Linear A was a common Aegean script such an assumption will not resolve the problem of multiplicity.\n\nThe Arkalochori Axe and other finds have made Cretan origin more popular: female images with pendulous breasts have also been found at Malia and Phaistos. (Godart 1995:125). Duhoux asserts the Cretan \"provenance\" of the disc; in his review of current research, Trauth (1990:154) concludes that \"Crete as [the] source of the Disc can no longer be called into question.\" Andrew Robinson (2008), in a review in \"Nature\", wrote \"Most scholars today, including Duhoux, think it a plausible working hypothesis that the disc was made in Crete.\"\n\nIpsen (1929:11) also speaks against an entirely independent origin of the scripts, arguing that its inventors did not leap from no knowledge of writing to a syllabic script with these elegant signs. He goes on to cite Hieroglyphic Luwian as a \"perfect parallel\" (Ipsen 1929:17) of an original script inspired under the direct influence of other scripts (its symbol values inspired by cuneiform, its shapes by Egyptian hieroglyphs)\n\nSchwartz (1956:108) asserts a genetic relationship between the Phaistos Disc script and the Cretan linear scripts.\n\nAmong the known scripts, there are three main candidates for being related to the Disc's script, all of them partly syllabic, partly logographic: Linear A, Anatolian hieroglyphs and Egyptian hieroglyphs. More remote possibilities are comparison with the Phoenician abjad or the Byblos syllabary.\n\nSome signs are close enough to both Linear A and Linear B that they may have the same phonetic values, as 12 = \"qe\", 43 = \"ta\", or 31 = \"ku\". But this opinion is not shared by all specialists of the Aegean Scripts.\nA recent systematic comparison with Linear A is that of Torsten Timm, 2004. Based on the Linear A character distribution patterns collected by Facchetti Timm concludes that the language of the Disc inscription is the same as the language of Linear A. Timm identifies 20 of the 45 characters with Linear signs, assigning Linear B phonetic values to 16.\n\nAchterberg et al. (2004) present a systematic comparison with Anatolian hieroglyphs, resulting in a full decipherment claim (see below). In particular, they consider the stroke symbol cognate to the Luwian \"r(a/i)\" symbol, but assign it the value \"-ti\". The stroke on A3 is identified as the personal name determinative. 01 is compared to the logogram \"SARU\", a walking man or walking legs in Luwian. 02 is compared to word-initial \"a\", a head with a crown in Luwian. The \"bow\" 11 is identified as the logogram \"sol suus\", the winged sun known from Luwian royal seals. The \"shield\" 12 is compared to the near identical Luwian logogram \"TURPI\" \"bread\" and assigned the value \"tu\". 39 they read as the \"thunderbolt\", logogram of Tarhunt, in Luwian a W-shaped hieroglyph.\n\nThe decipherment claims listed are categorized into linguistic decipherments, identifying the language of the inscription, and non-linguistic decipherments. A purely logographical reading is not linguistic in the strict sense: while it may reveal the meaning of the inscription, it will not allow for the identification of the underlying language.\n\n\n\nA set of 46 Phaistos Disc characters, comprising 45 signs and one combining oblique stroke, have been encoded in Unicode since April 2008 (Unicode version 5.1). They are assigned to the range 101D0–101FF in Plane 1 (the Supplementary Multilingual Plane). Phaistos Disc characters were encoded with strong left-to-right directionality, and so in code charts and text (such as elsewhere on this page) the glyphs are mirrored from the way they appear on the disc itself.\n\nSide A of the Phaistos disc is used as the logo of FORTH, one of the largest research centers in Greece.\n\n\n\nThis list contains off-line accounts of various decipherments.\n\n"}
{"id": "46728635", "url": "https://en.wikipedia.org/wiki?curid=46728635", "title": "RV Fridge", "text": "RV Fridge\n\nRecreational vehicles (RVs) are designed to be self-sufficient, making an on-board refrigerator a highly desirable appliance. However, RVs are subject to conditions that make the design of their refrigerators quite challenging. The main issues are vibrations, accelerations, the energy required to drive the refrigeration process, and the often off-level attitude of the vehicle, especially while in transit. An absorption-type refrigerator overcomes most of these design challenges, and therefore has been popular in RVs since the 1950s.\n\nFollowing is a block diagram and a simplified drawing of a typical absorption type RV Fridge. The block diagram is a simplification of the fridge cycle which shows the key components combined with the fluid flow within the circuit. The simplified fluids schematic more closely resembles the actual refrigeration cycle so that the reader can draw a connection between the components on their fridge and what is occurring within the cooling unit pressure vessel.\n\nThe first absorption refrigeration system was patented by Ferdinand Carre in 1859. This refrigeration system had mechanical pumps and throttling valves which change the pressures in the refrigeration cycle.\n\nThe RV Fridge is considered a single pressure absorption refrigeration (SPAR) system because the entire system is at the same total pressure. Because there are no pumps or compressors it is easy to sustain these fridges in an RV as long as propane (LP gas) is available. The SPAR was patented by Baltzar von Platen and Carl Munters in 1923.\n\nThe block diagram shows the flow of fluids (Q) along with the flow direction arrows. Also, the heat input (endothermic process) and heat rejection (exothermic process) for the respective components has been identified.\nThe fluid flow is as follows:\n\nQ: The yellow line is the holding tank mixture of liquid water, ammonia, and a rust inhibitor.\n\nQ: The blue line is the weak solution. The weak solution preferably consists of only water and the rust inhibitor as a result of the distillation process.\n\nQ: The pink line is pure ammonia gas (anhydrous ammonia). The heat input in the fired boiler causes the liquid ammonia within Q to boil (go through a phase change).\n\nQ: The green line is ammonia liquid. The condenser cools the ammonia gas (Q) to produce the working fluid which removes the heat from the refrigerated compartment.\n\nQ: The pink line is once again ammonia gas. The ammonia gas is produced by evaporation of the ammonia liquid (Q) within the evaporator. The evaporation of the liquid ammonia is due to Dalton's law of partial pressures\nThe pressure vessel which is commonly referenced as a cooling unit in the RV industry has all of the air evacuated and a hydrogen charge of approximately 350psi (~2.4MPa).\n\nQ + Q → Q: In the absorber coil the circuit is completed. Q is reconstituted due to the fact that ammonia has an affinity for water. The weak solution (Q) absorbs the ammonia gas thereby returning the ammonia to a liquid state to reenter the boiler and start the process over.\n\nThe RV fridge simplified fluids schematic more closely resembles the actual cooling unit. (The same color schemes have been used for the fluid flow as the block diagram.)\n\nThe RV Fridge boiler is the heart of the refrigeration process. It performs two main tasks; one is to separate the ammonia from the water using the process of distillation. The second task that the boiler performs is to pump the fluids which maintains the flow Qx within the cooling unit. The RV Fridge boiler is considered a watertube type boiler which has the advantage of low weight and a high circulation rate which is desirable for mobile applications.\n\nQ Holding Tank to Percolator Tube\n\nStarting at the holding tank the fluids Q1 are delivered to the boiler through the inner annular tube. Heat is applied and the ammonia changes phase from liquid to gas. The ammonia gas rises up the percolator tube (pump tube) forcing the remaining water that has been depleted of ammonia up the pump tube at the same time.\n\nQ Weak Solution\n\nAt the top of the percolator tube the water Q2 drops down by the force of gravity into the outer annular tube. This water passes by the heat input to further deplete the flow Q2 of any remaining ammonia. In addition, the flow Q2 also provides preheating (regenerator) for flow Q1 as Q1 moves from the holding tank to the boiler.\nQ Generation of Refrigerant\n\nFrom the top of the percolator tube the anhydrous ammonia rises due to its density. The entire boiler assembly and percolator tube are also called a generator because the refrigerant is generated here.\n\nThe ammonia accumulator is a bell shaped chamber. The bottom of the chamber is open to the boiler and the top is closed with the percolator tube passing through. The accumulator fills with ammonia gas from the bottom, this in turn forces out any liquids from within the accumulator. The ammonia accumulator serves two important startup functions as follows:\n\nOne is to provide an area where the ammonia bubbles can gather when the heat source is started. This allows large bubbles to rise up the percolator tube upon startup rather than allowing the ammonia in the boiler to rise up the percolator tube without pumping the resulting liquid water.\n\nThe second function is to provide an ammonia reserve. When the heat is turned off at the boiler, as the boiler cools the same effect can occur as described above. If the ammonia is depleted from the boiler upon cool down the process cannot restart upon startup. Thus, when the boiler temperature lowers, the ammonia gas trapped in the accumulator will return into solution reconstituting Q1 for a strong ammonia solution in the boiler for an easy startup.\n\nThe controls for the RV Fridge center around controlling the boiler heat source. For the most part the control is binary. If refrigeration is called for, one of the boiler heat sources is turned on. When the refrigerated space is cool enough the boiler heat source is turned off.\n\nRV Fridge controls have not changed much in function since 1923 when Kelvinator introduced the first refrigerator control with automatic temperature control. Please see next section Boiler Safety for modern RV Fridge controls.\n\nOne issue that is faced by any heated pressure vessel is how to prevent catastrophic rupture. This is why the typical RV Fridge has a pressure relief device which is commonly called a PRV. The RV Fridge PRV is very limited because it is a fusible plug. It is reported that the fusible plug will only vent the pressure vessel in the event of a fire. Unfortunately the fusible plug does not prevent a majority of fridge safety issues. The RV fridge fusible plug is a retroactive device which responds to a major failure mode that results in a fire.\n\nOne of the first major RV Fridge control changes occurred when the Absorption Refrigeration Protective RV (ARPrv) Controller was invented. The ARPrv control principle of operation is based on the same physics that result in a constant temperature for the boiling of water. The ARP Control simply measures the latent heat of vaporization of ammonia. The ARP detects when Q2 is not flowing by the rise in temperature due to ammonia not being returned to the boiler. When ammonia does not return to the boiler, the sodium chromate is destroyed by excess heat. Once the sodium chromate is destroyed, intergranular corrosion, which leads to stress corrosion cracking, will eventually result in a cooling unit rupture. This discovery has solved most of the operator error issues such as off-level operation of the fridge. The ARPrv controller is a proactive automatic boiler control that limits both thermal stress on the boiler and stress on the working fluids. In addition, by limiting the temperature at the boiler, the pressure in the cooling unit is controlled which in turn increases safety and longevity of the RV Fridge.\n\nThe RV Fridge condenser cools and liquefies the pure ammonia gas Q3 that rises up from the boiler. The liquefied ammonia Q4 flows from the condenser to the evaporator where the actual refrigeration takes place.\n\nThe RV Fridge condenser needs two requirements meet in order for it to sustain the refrigeration cycle. The first is the condenser environment temperature must remain below the condensation temperature of ammonia at the system pressure, otherwise the ammonia will remain in a gaseous state. Therefore, the condenser requires proper air circulation. The second factor for proper operation is that the condenser is level such that the liquid ammonia will flow into the evaporator. When the fridge is operated off-level the ammonia will pool in the condenser. This pooling results in Q4 flow cessation and thus stops the refrigeration process because the ammonia cannot enter the evaporator for cooling of the fridge. In addition, the pooling of ammonia in the condenser prevents the ammonia from returning to the boiler causing complete cessation of the boiler function. When the boiler function stops overheating of the boiler follows.\n\nThe RV Fridge evaporator is where cooling of the refrigerated cabinet occurs. Basically all refrigeration process use evaporation for cooling. In the absorption type RV Fridge liquid ammonia Q4 enters from the condenser and flows down the evaporator. Within the evaporator the liquid ammonia comes in contact with the assistant gas which is generally hydrogen. Due to Dalton's law of partial pressures the liquid ammonia Q4 evaporates thereby absorbing heat from within the refrigerated space. Because the ammonia gas Q5 is heavier than the assistant gas, the ammonia gas flows out the bottom of the evaporator.\n\nThe evaporator is located within the foam cabinet in an RV Fridge. When one looks into either the freezer or refrigerated cabinets the back wall will generally have an aluminum plate which may have fins for increasing surface area for the absorption of heat from the refrigerated space. Depending on the type and construction of the RV Fridge, as a rule of thumb, the top half of the evaporator is in thermal contact with the freezer section and the lower half is in contact with the refrigerated section of the cabinet.\n\nDue to the construction of the evaporator there are two failure modes. The first is not a failure of the evaporator but rather the consequence of a failing boiler assembly. As an example, if the fridge has been operated off-level and the boiler is allowed to overheat, the volume of liquid ammonia Q4 is reduced. The reduced ammonia flow Q4 results in the ammonia evaporating before reaching the lower half of the evaporator. As a result, the freezer may be very cold, while the refrigerated space is too warm to prevent food spoilage.\n\nThe second mode of failure is due to the evaporator not being sealed properly. Often screws hold the evaporator into place from the inside of the refrigerator. If the screw heads are not sealed, water from inside the cabinet will flow by capillary action into the sealed evaporator space within the foam. Also, the back side of the evaporator has to be sealed so that ambient air cannot infiltrate the sealed evaporator space. When ambient air is allowed to come into contact with the evaporator, condensation results. The result for both of these issues is that water is trapped against the steel evaporator tubing, forming a galvanic cell which will rust a hole in the evaporator, resulting in premature fridge failure.\n\nThe absorber or absorber coils is where the fluids are recombined to complete the refrigeration fluids circuit. The ammonia gas Q5 drops down from the evaporator into the absorber where it comes into contact with the weak solution Q2 (water) flowing into the top of the absorber coils. When the ammonia vapor Q5 comes into contact with the weak solution liquid Q2, the weak solution absorbs the ammonia gas due to ammonias affinity for water. The absorption of ammonia gas into the water solution is an exothermic process, thus heat needs to be rejected from the absorber coils. The absorber coil is where the heat from within the fridge is rejected into the surroundings.\n\nThe absorber itself is not subject to failure due to corrosion or stress when constructed properly. The absorber can be reduced in capacity if the temperatures get too high in the cooling unit compartment. High temperatures slow down the rate at which ammonia gas is reabsorbed (Solubility of Ammonia in Water), therefore this can result in poor cooling within the refrigerated space. A simple test to see if the ammonia is completing the circuit is to feel the absorber coil. If the absorber coil does not heat within an hour of turning on the fridge it is possible that the ammonia is not being produced by the boiler or the ammonia is not completing the circuit. Besides the fridge cabinet temperature, knowing the RV Fridge boiler temperature and the absorber temperature gives one the information to know if the RV Fridge is functioning properly.\n"}
{"id": "23671111", "url": "https://en.wikipedia.org/wiki?curid=23671111", "title": "Reindustrialization", "text": "Reindustrialization\n\nReindustrialization is the economic, social, and political process of organizing national resources for the purpose of re-establishing industries. The process proceeds as a result of a need to reinvigorate national economies.\n\nChina, India and South-East Asia were industrial powerhouse for major parts of human history. These countries and regions suffered great loss of industrial production due to colonization during 18th-20th centuries. After many decades of their independence, these countries have started reindustrializing themselves. In last three decades, share of these countries in global industrial output has increased manifold.\nIn context of declining share of OECD in world GDP and outsourcing of manufacturing and services, reindustrialization is considered as a contrasts to deindustrialization, the process under which industry, especially manufacturing, is relocated outside of a country's borders, and seeks to reverse that trend.\n\nNo longer the preserve of BRICs or South-East Asian countries, the notion of reindustrialization seems to be making inroads in the political discourse of populist policy makers in the developed economies of Western Europe and North America: notably France and the United States - where the rise of Trumponomics may potentially challenge some of the free trade tenets of the neoliberal \"Washington Consensus\".\n\n\n\n"}
{"id": "19989102", "url": "https://en.wikipedia.org/wiki?curid=19989102", "title": "Skol Company", "text": "Skol Company\n\nThe Skol Company produced Skol antiseptic for sunburn and Skol sunglasses from the 1920s through the mid-1940s. Based in New York City, their products were available in the United States and Canada. George Gallowhur was president of the business. He developed Skol suntan lotion in the Austrian Alps in the 1920s. He also introduced Skat insect repellent.\n\nIn April 1938, the firm signed a five-year contract with the J. Walter Thompson Company for car-card advertising.\nThe following month the Skol Company launched a nationwide campaign to promote Skol sunglasses, utilizing newspapers throughout the United\nStates.\n\nOn November 6, 1946, the Skol Company merged with the Gallowhur Chemical Company. Gallowhur maintained his titles as president and treasurer. The Skol business was\nsold to the J.B. Williams Company in 1948. \n\nGallowhur died at the age of 69 at the Miami Heart Institute in Miami, Florida, in March 1974.\n\n"}
{"id": "22082236", "url": "https://en.wikipedia.org/wiki?curid=22082236", "title": "Society for Risk Analysis", "text": "Society for Risk Analysis\n\nThe Society for Risk Analysis (SRA) is a learned society providing an open forum for anyone interested in risk analysis. It seeks to: provide an avenue for individuals (from different disciplines from various corners of the globe) to discuss ideas and information as well as methodologies for analyzing risk and solving related issues and problems; promote understanding and encourage collaboration among professionals and organisations relevant to analyzing risk and providing solutions; disseminate and promote risk (methods) knowledge and their applications in research and education; and serve its members in developing and furthering their careers in risk analysis.\n\nIn early 1979, Robert B. Cumming recognized the growing need for risk researchers and practitioners to publish their work in a dedicated scientific journal. This led to the formation of an organization to support such a journal, with the certificate of incorporation for the SRA made official on August 28, 1980. The first issue of \"Risk Analysis\" appeared in March 1981.\n\nSRA broadly defines risk analysis to include risk assessment, risk characterization, risk communication, risk management, and policy relating to risk. The society's interests include: risk perception, risks to human health and the environment, both built and natural; threats from physical, chemical, and biological agents and from a variety of human activities as well as natural events; and risks of concern to individuals, to public and private sector organizations, and to society at various geographic scales. SRA publishes a glossary of definitions of key terms related to risk and fundamental principles for high quality risk analysis.\n\nSince the first awards were given in 1984, SRA continues to value outstanding contributions to the society and to the field of risk analysis. The following are its most-sought annual awards:\n\nDistinguished Achievement Award. This award is given to any individual for his or her extraordinary achievement in science or public policy relevant to the field of risk analysis.\n\nRichard J Burk Outstanding Service Award. A recognition to the society's member for extraordinary service to SRA.\n\nOutstanding Practitioner Award. Award alternately given to a member in public and private practice for an outstanding practice in the field of risk analysis.\n\nChauncey Starr Distinguished Young Risk Analyst Award. For an outstanding risk-analysis-related achievement in science or public policy by SRA's member (of 40 years of age or below) and exceptional promise for continued contributions to risk analysis.\n\nDistinguished Educator Award. Awarded to an outstanding teacher, author or mentor for substantial training of new experts in risk analysis.\n\nThe following are the past presidents of the society:\n\nSRA functions primarily through its specialty groups\n, which cover a wide range of risk analysis fields.\n"}
{"id": "19524205", "url": "https://en.wikipedia.org/wiki?curid=19524205", "title": "Springfields", "text": "Springfields\n\nSpringfields is a nuclear fuel production installation in Salwick, near Preston in Lancashire, England (). The site is currently operated by Springfields Fuels Limited, under the management of Westinghouse Electric UK Limited, on a 150-year lease from the Nuclear Decommissioning Authority. Since its conversion from a munitions factory in 1946, it has previously been operated and managed by a number of different organisations including the United Kingdom Atomic Energy Authority and British Nuclear Fuels. Fuel products are produced for the UK’s nuclear power stations and for international customers.\n\nThe site has been making nuclear fuels since the mid-1940s. The site is notable for being the first nuclear plant in the world to produce fuel for a commercial power station (Calder Hall).\n\nThe four main activities carried out on the site are:\n\nManufacture is scheduled to continue until 2023. Decommissioning activities have so far resulted in 87 buildings on the site having been fully demolished.\n"}
{"id": "19331901", "url": "https://en.wikipedia.org/wiki?curid=19331901", "title": "Swim diaper", "text": "Swim diaper\n\nA swim diaper is a diaper that is made for those who are incontinent, usually babies or young children, which is worn underneath a bathing suit, or as a bathing suit. Swim diapers can be reusable and disposable. They are not intended to be absorbent. Typically, it is assumed that a swim diaper should be absorbent, or contain urine, like a regular diaper. However, the purpose of a swim diaper is only to contain solid waste; the lack of absorbency prevents the swim diaper from swelling up with water.\n\nOften reusable swim diapers are lined with a fiber which encourages the solid waste to cling to the fiber without an absorbency layer. A snug fit in the legs and waist are key to function. Brands such as Splash About and The Honest Co use tightly knit polyester or neoprene as their material. The disadvantages of a reusable swim diaper is that they must be washed to be reused. On the other hand, a disposable swim diaper is only partially biodegradable and repeated purchases may cost more than reuse. One brand of disposable swim diapers is Little Swimmers, marketed under the Kimberly-Clark Huggies brand. Procter & Gamble produces the rival brand Pampers Splashers. Both are sold in three sizes: small (16–26 lb or 7–12 kg), medium (24–34 lb or 11–15 kg) and large (over 32 lb or 14 kg+). Due to their design for swimwear, they are not as absorbent and not intended for regular diapering.\n\nSome public pools require swim diapers for use by young children and the incontinent out of hygiene concerns. For the same reason, other pools do not allow swim diapers at all. Sick children who are not potty-trained and do not wear swim diapers may be responsible for the transmission of e. coli from fecal matter.\n\nWhen not used properly, or when using inferior products, health experts caution that swim diapers may not protect pool water against communicable diseases, such as norovirus.\n"}
{"id": "239427", "url": "https://en.wikipedia.org/wiki?curid=239427", "title": "Table of handgun and rifle cartridges", "text": "Table of handgun and rifle cartridges\n\nTable of selected pistol/submachine gun and rifle/machine gun cartridges by common name. Data values are the highest found for the cartridge, and might not occur in the same load (e.g. the highest muzzle energy might not be in the same load as the highest muzzle velocity, since the bullet weights can differ between loads).\n\n\n\n"}
{"id": "58457", "url": "https://en.wikipedia.org/wiki?curid=58457", "title": "Timeline of agriculture and food technology", "text": "Timeline of agriculture and food technology\n\n\n\n\n\n\n\n"}
{"id": "6289623", "url": "https://en.wikipedia.org/wiki?curid=6289623", "title": "UWA Telerobot", "text": "UWA Telerobot\n\nThe UWA telerobot is a teleoperable robot belonging to the school of mechanical and civil engineering at the University of Western Australia.\nThe UWA telerobot is a historic landmark for the Internet and The University of Western Australia. It was the first telerobot device made available for general use on the Internet in 1994. The UWA telerobot was originally developed as part of a PhD thesis by Kenneth Taylor and was the subject of a later PhD by Barney Dalton.\n\nThe first robot on the Internet, a plastic toy robot with only 2 degrees of freedom, was placed online by a team under Ken Goldberg at the University of Southern California only three weeks before the UWA team released their website. The USC robot only lasted for seven months. The UWA robot is still online today, although the original robot was replaced in 1996 and the robot is no longer available for unrestricted public access, though interested parties can request permission.\n\nThe current UWA telerobot is an ABB IRB1400 model 6 DOF serial chain robot fitted with a pneumatic gripper attachment. The robot runs on a standard ABB S4 Robot Controller linked to a Linux server and which in turn communicates with a second server running ABB's RobComm software and a National Instruments Labview application that was custom written for the task by James Trevelyan with assistance from Perth-based Icon Technologies and students. The robot forms part of the UWA telelabs project.\n\nThe Telerobot has undergone many changes to its control structure over time. Originally controlled via static html web pages using CGI, work by Dalton saw the introduction of an augmented reality Java-based interface that met with limited success. Control is currently by way of a downloadable LabVIEW client application that incorporates real-time video streaming, with access control provided by the Telelabs system.\n\nThe robot continues to be the basis for research and group projects undertaken by Mechatronics Engineering students and staff at UWA, Primarily involving the addition of new features or capabilities to the system. The robot is also used as a teaching aid for a course in mechanisms and multibody systems run by Karol Miller.\n\n"}
