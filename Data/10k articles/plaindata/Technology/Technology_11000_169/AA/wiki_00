{"id": "147845", "url": "https://en.wikipedia.org/wiki?curid=147845", "title": "Application-specific integrated circuit", "text": "Application-specific integrated circuit\n\nAn Application-Specific Integrated Circuit (ASIC ) is an integrated circuit (IC) customized for a particular use, rather than intended for general-purpose use. For example, a chip designed to run in a digital voice recorder or a high-efficiency Bitcoin miner is an ASIC. Application-specific standard products (ASSPs) are intermediate between ASICs and industry standard integrated circuits like the 7400 series or the 4000 series.\n\nAs feature sizes have shrunk and design tools improved over the years, the maximum complexity (and hence functionality) possible in an ASIC has grown from 5,000 logic gates to over 100 million. Modern ASICs often include entire microprocessors, memory blocks including ROM, RAM, EEPROM, flash memory and other large building blocks. Such an ASIC is often termed a SoC (system-on-chip). Designers of digital ASICs often use a hardware description language (HDL), such as Verilog or VHDL, to describe the functionality of ASICs.\n\nField-programmable gate arrays (FPGA) are the modern-day technology for building a breadboard or prototype from standard parts; programmable logic blocks and programmable interconnects allow the same FPGA to be used in many different applications. For smaller designs or lower production volumes, FPGAs may be more cost effective than an ASIC design, even in production. The non-recurring engineering (NRE) cost of an ASIC can run into the millions of dollars. Therefore, device manufacturers typically prefer FPGAs for prototyping and devices with low production volume and ASICs for very large production volumes where NRE costs can be amortized across many devices. \n\nThe initial ASICs used gate array technology. An early successful commercial application was the gate array circuitry found in the low-end 8-bit ZX81 and ZX Spectrum personal computers, introduced in 1981 and 1982. These were used by Sinclair Research (UK) essentially as a low-cost I/O solution aimed at handling the computer's graphics.\n\nCustomization occurred by varying a metal interconnect mask. Gate arrays had complexities of up to a few thousand gates; this is now called mid-scale integration. Later versions became more generalized, with different base dies customized by both metal and polysilicon layers. Some base dies also include random-access memory (RAM) elements.\n\nIn the mid-1980s, a designer would choose an ASIC manufacturer and implement their design using the design tools available from the manufacturer. While third-party design tools were available, there was not an effective link from the third-party design tools to the layout and actual semiconductor process performance characteristics of the various ASIC manufacturers. Most designers used factory-specific tools to complete the implementation of their designs. A solution to this problem, which also yielded a much higher density device, was the implementation of standard cells. Every ASIC manufacturer could create functional blocks with known electrical characteristics, such as propagation delay, capacitance and inductance, that could also be represented in third-party tools. Standard-cell design is the utilization of these functional blocks to achieve very high gate density and good electrical performance. Standard-cell design is intermediate between and in terms of its non-recurring engineering and recurring component costs as well as performance and speed of development (including time to market).\n\nBy the late 1990s, logic synthesis tools became available. Such tools could compile HDL descriptions into a gate-level netlist. Standard-cell integrated circuits (ICs) are designed in the following conceptual stages referred to as electronics design flow, although these stages overlap significantly in practice:\n\nThese steps, implemented with a level of skill common in the industry, almost always produce a final device that correctly implements the original design, unless flaws are later introduced by the physical fabrication process.\n\nThe design steps, also called design flow, are also common to standard product design. The significant difference is that standard-cell design uses the manufacturer's cell libraries that have been used in potentially hundreds of other design implementations and therefore are of much lower risk than full custom design. Standard cells produce a design density that is cost effective, and they can also integrate IP cores and static random-access memory (SRAM) effectively, unlike gate arrays.\n\nGate array design is a manufacturing method in which diffused layers, each consisting transistors and other active devices, are predefined and electronics wafers containing such devices are \"held in stock\" or unconnected prior to the metallization stage of the fabrication process. The physical design process defines the interconnections of these layers for the final device. For most ASIC manufacturers, this consists of between two to nine metal layers with each layer running perpendicular to the one below it. Non-recurring engineering costs are much lower than full custom designs, as photolithographic masks are required only for the metal layers. Production cycles are much shorter, as metallization is a comparatively quick process; thereby accelerating time to market. \n\nGate-array ASICs are always a compromise between rapid design and performance as mapping a given design onto what a manufacturer held as a stock wafer never gives 100% circuit utilization. Often difficulties in routing the interconnect require migration onto a larger array device with consequent increase in the piece part price. These difficulties are often a result of the layout EDA software used to develop the interconnect.\n\nPure, logic-only gate-array design is rarely implemented by circuit designers today, having been almost entirely replaced by field-programmable devices. Most prominent of such devices are field-programmable gate arrays (FPGAs) which can be programmed by the user and thus offer minimal tooling charges non-recurring engineering, only marginally increased piece part cost, and comparable performance. \n\nToday, gate arrays are evolving into structured ASICs that consist of a large IP core like a CPU, digital signal processor units, peripherals, standard interfaces, integrated memories, SRAM, and a block of reconfigurable, uncommitted logic. This shift is largely because ASIC devices are capable of integrating large blocks of system functionality, and systems on a chip (SoCs) require glue logic, communications subsystems (such as networks on chip), peripherals and other components rather than only functional units and basic interconnection. \n\nIn their frequent usages in the field, the terms \"gate array\" and \"semi-custom\" are synonymous when referring to ASICs. Process engineers more commonly use the term \"semi-custom\", while \"gate-array\" is more commonly used by logic (or gate-level) designers.\n\nBy contrast, full-custom ASIC design defines all the photolithographic layers of the device. Full-custom design is used for both ASIC design and for standard product design.\n\nThe benefits of full-custom design include reduced area (and therefore recurring component cost), performance improvements, and also the ability to integrate analog components and other pre-designed—and thus fully verified—components, such as microprocessor cores, that form a system on a chip. \n\nThe disadvantages of full-custom design can include increased manufacturing and design time, increased non-recurring engineering costs, more complexity in the computer-aided design (CAD) and electronic design automation systems, and a much higher skill requirement on the part of the design team.\n\nFor digital-only designs, however, \"standard-cell\" cell libraries, together with modern CAD systems, can offer considerable performance/cost benefits with low risk. Automated layout tools are quick and easy to use and also offer the possibility to \"hand-tweak\" or manually optimize any performance-limiting aspect of the design.\n\nThis is designed by using basic logic gates, circuits or layout specially for a design.\n\n\"Structured ASIC design\" (also referred to as \"platform ASIC design\") is a relatively new trend in the semiconductor industry, resulting in some variation in its definition. However, the basic premise of a structured ASIC is that both manufacturing cycle time and design cycle time are reduced compared to cell-based ASIC, by virtue of there being pre-defined metal layers (thus reducing manufacturing time) and pre-characterization of what is on the silicon (thus reducing design cycle time). \n\nOne definition states thatIn a \"structured ASIC\" design, the logic mask-layers of a device are predefined by the ASIC vendor (or in some cases by a third party). Design differentiation and customization is achieved by creating custom metal layers that create custom connections between predefined lower-layer logic elements. \"Structured ASIC\" technology is seen as bridging the gap between field-programmable gate arrays and \"standard-cell\" ASIC designs. Because only a small number of chip layers must be custom-produced, \"structured ASIC\" designs have much smaller non-recurring expenditures (NRE) than \"standard-cell\" or \"full-custom\" chips, which require that a full mask set be produced for every design.This is effectively the same definition as a gate array. What distinguishes a structured ASIC from a gate array is that in a gate array, the predefined metal layers serve to make manufacturing turnaround faster. In a structured ASIC, the use of predefined metallization is primarily to reduce cost of the mask sets as well as making the design cycle time significantly shorter. \n\nFor example, in a cell-based or gate-array design the user must often design power, clock, and test structures themselves. By contrast, these are predefined in most structured ASICs and therefore can save time and expense for the designer compared to gate-array based designs. Likewise, the design tools used for structured ASIC can be substantially lower cost and easier (faster) to use than cell-based tools, because they do not have to perform all the functions that cell-based tools do. In some cases, the structured ASIC vendor requires that customized tools for their device (e.g., custom physical synthesis) be used, also allowing for the design to be brought into manufacturing more quickly.\n\nCell libraries of logical primitives are usually provided by the device manufacturer as part of the service. Although they will incur no additional cost, their release will be covered by the terms of a non-disclosure agreement (NDA) and they will be regarded as intellectual property by the manufacturer. Usually their physical design will be pre-defined so they could be termed \"hard macros\".\n\nWhat most engineers understand as \"intellectual property\" are IP cores, designs purchased from a third-party as sub-components of a larger ASIC. They may be provided in the form of a hardware description language (often termed a \"soft macro\"), or as a fully routed design that could be printed directly onto an ASIC's mask (often termed a \"hard macro\"). Many organizations now sell such pre-designed cores — CPUs, Ethernet, USB or telephone interfaces — and larger organizations may have an entire department or division to produce cores for the rest of the organization. The company ARM (Advanced RISC Machines) \"only\" sells IP cores, making it a fabless manufacturer.\n\nIndeed, the wide range of functions now available in structured ASIC design is a result of the phenomenal improvement in electronics in the late 1990s and early 2000s; as a core takes a lot of time and investment to create, its re-use and further development cuts product cycle times dramatically and creates better products. Additionally, open-source hardware organizations such as OpenCores are collecting free IP cores, paralleling the open-source software movement in hardware design.\n\nSoft macros are often process-independent (i.e. they can be fabricated on a wide range of manufacturing processes and different manufacturers). Hard macros are process-limited and usually further design effort must be invested to migrate (port) to a different process or manufacturer.\n\nSome manufacturers offer multi-project wafers (MPW) as a method of obtaining low cost prototypes. Often called shuttles, these MPW, containing several designs, run at regular, scheduled intervals on a \"cut and go\" basis, usually with very little liability on the part of the manufacturer. The contract involves the assembly and packaging of a handful of devices. The service usually involves the supply of a physical design database (i.e. masking information or pattern generation (PG) tape). The manufacturer is often referred to as a \"silicon foundry\" due to the low involvement it has in the process.\n\nAn application specific standard product or ASSP is an integrated circuit that implements a specific function that appeals to a wide market. As opposed to ASICs that combine a collection of functions and are designed by or for one customer, ASSPs are available as off-the-shelf components. ASSPs are used in all industries, from automotive to communications. As a general rule, if you can find a design in a data book, then it is probably not an ASIC, but there are some exceptions.\n\nFor example, two ICs that might or might not be considered ASICs are a controller chip for a PC and a chip for a modem. Both of these examples are specific to an application (which is typical of an ASIC) but are sold to many different system vendors (which is typical of standard parts). ASICs such as these are sometimes called Application-Specific Standard Products (ASSPs).\n\nExamples of ASSPs are encoding/decoding chip, standalone USB interface chip, etc.\n\nIEEE used to publish an ASSP magazine, which was renamed to IEEE Signal Processing Magazine in 1990.\n\n\n"}
{"id": "13771799", "url": "https://en.wikipedia.org/wiki?curid=13771799", "title": "Audio over IP", "text": "Audio over IP\n\nAudio over IP (AoIP) is the distribution of digital audio across an IP network such as the Internet. It is being used increasingly to provide high-quality audio feeds over long distances. The application is also known as audio contribution over IP (ACIP) in reference to the programming contributions made by field reporters and remote events. Audio quality and latency are key issues for contribution links.\n\nIn the past, these links have made use of ISDN services but these have become increasingly difficult or expensive to obtain in some parts of Europe and elsewhere. On March 28, 2013 Verizon announced it would no longer be taking new orders for ISDN service in the Northeastern United States starting on May 18 of that year. Though they still service existing active installations.\n\nMany proprietary systems came into existence for transporting high-quality audio over IP based on Transmission Control Protocol (TCP), User Datagram Protocol (UDP) or Real-time Transport Protocol (RTP). An interoperable standard for audio over IP using RTP now exists.\n\nWithin a single building or music venue, audio over Ethernet (AoE) is more likely to be used instead, avoiding audio data compression and, in some cases, IP encapsulation.\n\nThe European Broadcasting Union (EBU) together with many equipment manufacturers defined a common framework for audio contribution over IP in order to achieve interoperability between products. The framework defines RTP as a common protocol and media payload type formats according to IETF definitions. Session Initiation Protocol (SIP) is used as signalling for call setup and control. The recommendation is currently published as EBU Tech 3326-2007.\n\nMore advanced audio codecs are capable of sending audio over unmanaged IP networks like the internet using automated jitter buffering, forward error correction and error concealment to minimise latency and maximise packet streaming stability in live broadcast situations over unmanaged IP networks.\n\nIn the face of IPv4 address exhaustion, IPv6 capability ensures codecs are capable of connecting over new Internet infrastructure. IPv6 infrastructure is being widely deployed to deliver a virtually inexhaustible supply of IP addresses. IPv6 addressing makes it much easier for broadcast codecs to connect to each other directly and perform flexible multi-point connections over IP.\n\nThe BBC began using audio contribution over IP in Scotland as part of the BBC Pacific Quay development in Glasgow. A similar system has been installed in the Regions of England and will be installed in Wales and Northern Ireland. The audio packets are sent using UDP over the BBC’s Layer-3 network. To reduce the chance that the audio is corrupted, quality of service (QoS) is set to ensure that the packets are given priority over other network traffic. The platforms used are the WorldNet Oslo for multiple channel contribution and distribution with the WorldCast Horizon deployed in stereo drop-off locations.\n\nAudio over IP is even used for large sport events. More than 1000 Barix IP Audio Codecs have been recently used to network the various venues of the Commonwealth Games hosted in India.\n\nCodecs such as the Tieline i-Mix G3 have been used since 2004 at the Olympic Games for live sports broadcasting. These codecs also have the ability to send audio over wireless IP, i.e. 3G and Wi-Fi, as well as other audio transports like POTS, ISDN, satellite and X.21, and have been used at UEFA and World Cup Soccer tournaments.\n\nUltra portable audio over IP codecs are also available as a smartphone applications to send high fidelity broadcast quality audio from remote sites to studios. Applications such as Report-IT Live for iPhone can send bidirectional 15 kHz quality audio live with automated jitter buffering, forward error correction and error concealment. They can also send 20 kHz quality audio recordings from the phone to a studio via FTP.\n\nThe New Jersey Transit Corporation distributes messages to their customers using audio over IP equipment, and IP audio technology is used in modern train carriages for customer information and messaging now.\n\nAudio over IP is also used in scientific applications, such as the Neumayer Station in Antarctica, where Barix IP Audio encoders digitize and stream the complete audio spectrum captured by hydrophones under water to the Alfred Wegener Institute for Polar and Marine Research in Germany.\n\n\n"}
{"id": "490068", "url": "https://en.wikipedia.org/wiki?curid=490068", "title": "Backhoe loader", "text": "Backhoe loader\n\nA backhoe loader, also called a loader backhoe, digger in layman's terms, or colloquially shortened to backhoe within the industry, is a heavy equipment vehicle that consists of a tractor like unit fitted with a loader-style shovel/bucket on the front and a backhoe on the back. Due to its (relatively) small size and versatility, backhoe loaders are very common in urban engineering and small construction projects (such as building a small house, fixing urban roads, etc.) as well as developing countries. This type of machine is similar to and derived from what is now known as a TLB (Tractor-Loader-Backhoe), which is to say, an agricultural tractor fitted with a front loader and rear backhoe attachment.\n\nThe true development of the backhoe actually began in 1947 by the inventors that started the Wain-Roy Corporation of Hubbardston, Massachusetts. In 1947 Wain-Roy Corporation developed and tested the first actual backhoes. In April 1948 Wain-Roy Corporation sold the very first all hydraulic backhoes, mounted to a Ford Model 8N tractor, to the Connecticut Light and Power Company for the sum of $705.\n\nEvolving in parallel to development in the U.S., backhoes were first produced in the UK in 1953 by JCB, but it was just a prototype. The world's first backhoe loader with factory warranty was introduced in the U.S. by J.I. Case in 1957. Their Model 320 was the world's first serial backhoe loader. Although based on a tractor, a backhoe loader was and is almost never called a \"tractor\" when both the loader and the backhoe are permanently attached. Backhoe loaders are also not generally used for towing and usually do not have a power take-off (PTO) as often this is used to drive the hydraulic pump operating the attachments. When the backhoe is permanently attached, the machine usually has a seat that can swivel to the rear to face the hoe controls. Removable backhoe attachments almost always have a separate seat on the attachment itself.\nIn Britain and Ireland they are commonly referred to simply as JCBs; they are comparably called \"Shinrai\" in India due to the company being the major supplier. In the United States, they are often referred to as \"backhoes\", although the term 'backhoe' only refers to one component. In Russia they are referred as excavator-loaders.\n\nIn 1970, Hy-Dynamic, now a division of Bucyrus-Erie, manufacturer of the Dynahoe, was the first company to incorporate a four-wheel drive system into their backhoe loaders, allowing these models to go over almost any terrain with little difficulty. Since the backhoe was invented, several companies such as Caterpillar and John Deere have changed the backhoe's back arm to be slightly curved like that of an excavator, which can allow more maneuverability.\n\nBackhoe loaders are very common and can be used for a wide variety of tasks: construction, small demolitions, light transportation of building materials, powering building equipment, digging holes/excavation, landscaping, breaking asphalt, and paving roads. Often, the backhoe bucket can also be replaced with powered attachments such as a breaker, grapple, auger, or a stump grinder. Enhanced articulation of attachments can be achieved with intermediate attachments such as the tiltrotator. Many backhoes feature quick coupler (quick-attach) mounting systems and auxiliary hydraulic circuits for simplified attachment mounting, increasing the machine's utilization on the job site. Some loader buckets have a retractable bottom or \"clamshell\", enabling it to empty its load more quickly and efficiently. Retractable-bottom loader buckets are also often used for grading and scraping. The front assembly may be a removable attachment or permanently mounted.\n\nBecause digging while on tires intrinsically causes the machine to rock, and the swinging weight of the backhoe could cause the vehicle to tip, most backhoe loaders use hydraulic outriggers or stabilizers at the rear when digging and lower the loader bucket for additional stability. This means that the bucket must be raised and the outriggers retracted when the vehicle needs to change positions, reducing efficiency. For this reason many companies offer miniature tracked excavators, which sacrifice the loader function and ability to be driven from site to site, for increased digging efficiency.\n\nTheir relatively small frame and precise control make backhoe-loaders very useful and common in urban engineering projects such as construction and repairs in areas too small for larger equipment. Their versatility and compact size makes them one of the most popular urban construction vehicles. For larger projects, a tracked excavator is generally used.\n\nIn recent years, small compact tractors have become very popular with private homeowners. Subcompact tractors, the size between a compact tractor and lawn tractor, are also often sold in backhoe loader setup, sometimes with a belly-mounted mower also included. These tractors offer private homeowners the ability to perform minor excavation projects.\n\n\n"}
{"id": "58990693", "url": "https://en.wikipedia.org/wiki?curid=58990693", "title": "Beckhoff group", "text": "Beckhoff group\n\nBeckhoff (Beckhoff-Gruppe) with headquarters in Verl, NRW, Germany is a family-owned global provider of PC-based automation systems. \nApplications of Beckhoff technology range from CNC-controlled tools, to building automation/intelligent buildings and Internet of Things (IoT) systems.\nUS headquarters is in Savage, Minnesota (Minneapolis area).\n\nBeckhoff has 3 business units:\n\nGlobal revenue created by Beckhoff Automation's 3,900 employees was 810 million Euros in 2017.\n\nBeckhoff products include:\n\n"}
{"id": "34808927", "url": "https://en.wikipedia.org/wiki?curid=34808927", "title": "Bioproducts engineering", "text": "Bioproducts engineering\n\nBioproducts engineering or bioprocess engineering refers to engineering of bio-products from renewable bioresources. This pertains to the design and development of processes and technologies for the sustainable manufacture of bioproducts (materials, chemicals and energy) from renewable biological resources.\n\nBioproducts engineers harness the molecular building blocks of renewable resources to design, develop and manufacture environmentally friendly industrial and consumer products. From biofuels, renewable energy, and bioplastics to paper products and \"green\" building materials such as bio-based composites, Bioproducts engineers are developing sustainable solutions to meet the world's growing materials and energy demand. Conventional bioproducts and emerging bioproducts are two broad categories used to categorize bioproducts. Examples of conventional bio-based products include building materials, pulp and paper, and forest products. Examples of emerging bioproducts or biobased products include biofuels, bioenergy, starch-based and cellulose-based ethanol, bio-based adhesives, biochemicals, biodegradable plastics, etc. Bioproducts Engineers play a major role in the design and development of \"green\" products including biofuels, bioenergy, biodegradable plastics, biocomposites, building materials, paper and chemicals. Bioproducts engineers also develop energy efficient, environmentally friendly manufacturing processes for these products as well as effective end-use applications. Bioproducts engineers play a critical role in a sustainable 21st century bio-economy by using renewable resources to design, develop, and manufacture the products we use every day. The career outlook for bioproducts engineers is very bright with employment opportunities in a broad range of industries, including pulp and paper, alternative energy, renewable plastics, and other fiber, forest products, building materials and chemical-based industries.\n\nCommonly referred to as bioprocess engineering, bioprocess engineering is a specialization of biotechnology, biological engineering, chemical engineering or of agricultural engineering. It deals with the design and development of equipment and processes for the manufacturing of products such as food, feed, pharmaceuticals, nutraceuticals, chemicals, and polymers and paper from biological materials. Bioprocees engineering is a conglomerate of mathematics, biology and industrial design, and consists of various spectrums like designing of fermentors, study of fermentors (mode of operations etc.). It also deals with studying various biotechnological processes used in industries for large scale production of biological product for optimization of yield in the end product and the quality of end product. Bio process engineering may include the work of mechanical, electrical and industrial engineers to apply principles of their disciplines to processes based on using living cells or sub component of such cells.\n\n\n\n"}
{"id": "17904603", "url": "https://en.wikipedia.org/wiki?curid=17904603", "title": "Bipack color", "text": "Bipack color\n\nIn bipack color photography for motion pictures, two strips of black-and-white 35 mm film, running through the camera emulsion to emulsion, are used to record two regions of the color spectrum, for the purpose of ultimately printing the images, in complementary colors, superimposed on one strip of film. The result is a multicolored projection print that reproduces a useful but limited range of color by the subtractive color method. Bipack processes became commercially practical in the early 1910s when Kodak introduced duplitized film print stock, which facilitated making two-color prints.\n\nBipack photography was, from about 1935 to 1950, the most economical means of 35 mm natural color cinematography available, used when color was wanted but the budget could not bear the much higher cost of three-strip Technicolor or the less well-known alternative three-color processes sometimes available outside the US. After 1950, when economical \"monopack\" color negative and print stocks such as Eastmancolor and Ansco Color were introduced, the use of bipack photography and printing rapidly declined. By 1955 all two-color motion picture processes were commercially extinct in the US.\n\nBipack and three-element tripack sandwiches of plates and films were used in some early color processes for still photography, the field in which the concept originated.\n\n Bipack color refers to the type of camera load that is used for the effect. Bipack photography refers to two strips running through the camera at once, for the purpose of recording two different spectra of light, generally.\n\nColor photography begins with any standard camera. Special magazines or adapters must be provided to accommodate two separate rolls of film. Two films are loaded, passing through the photographing aperture with the emulsions towards each other. The front film is orthochromatic, to record the blue-green portion of the picture. On the surface of its emulsion is a red-dye layer equivalent to a Wratten 23A filter. The rear film is panchromatic, and being photographed through the red coating of the front film, records only the red-orange components of the picture. No filtering is necessary either for exterior or interior photography, as all necessary color corrections are made by adjusting the development of the two negatives during printing.\n\nSince the image must be focused on the plane of contact of the two negatives used, lenses and focusing screens used in bipack photography would be readjusted to throw the plane of focus .006\" behind that of the standard black-and-white plane.\n\nCare would be taken to avoid photographing objects of purple, lavender or pink coloring, as bipack color generally cannot reproduce these colors in printing.\n\nAfter processing the two negatives, the red and cyan records were printed separately on a single strip of Eastman or DuPont duplitized stock. Since the red negative was reversed in camera (that is, its emulsion away from the lens), there was no optical printing required to focus the image, and thus contact printing on both emulsions took place. Both sides were toned by floating each side in a tank with the complementary colors (cyan for the side exposed with the red negative and vice versa) using toning chemicals or through dye mordanting.\n\nOver the years, a great number of bipack color processors existed, largely due to the lack of holding patents on processing in this method. These systems included:\n\n\nIn addition, Consolidated Film also owned the Trucolor color system, which was shot as bipack color, but processed with special duplitized stock produced by the Eastman Kodak company that carried a dye-coupler.\n\nFor the 1948 Summer Olympics in London, the Technicolor Corporation devised a bipack color filming process – dubbed \"Technichrome\" – whereby hundreds of hours of film documented the Olympics in color, without having to ship expensive and heavy Technicolor cameras to London.\n\n"}
{"id": "45516432", "url": "https://en.wikipedia.org/wiki?curid=45516432", "title": "Boston Society of Civil Engineers", "text": "Boston Society of Civil Engineers\n\nThe Boston Society of Civil Engineers, now the \"Boston Society of Civil Engineers Section of the American Society of Civil Engineers\" was established in 1848. It claims to be the oldest engineering society in the United States by four years, and has over 4,000 members.\n\nAccording to the ASCE 150th anniversary publication in 2002, the BSCE founding members, including James Laurie met at the \"U.S. Hotel\" in Boston on April 26, 1848, four years before the founding of the ASCEA. Laurie became the first president if the ASCEA (which originally included Architects in its name) in 1852. From 1855 - 57, when the American Institute of Architects was formed out of the AFSCEA, and until 1867, that organization did little.\n\nIn 1867, the AFSCEA found its five shares of the New York Central Railroad on which enough dividends had accrued and went back into business.\n"}
{"id": "422752", "url": "https://en.wikipedia.org/wiki?curid=422752", "title": "CA Technologies", "text": "CA Technologies\n\nCA Technologies, formerly known as Computer Associates International, Inc. and CA, Inc., is an American multinational corporation headquartered in New York City. CA was acquired by Broadcom, Inc. on November 5, 2018.\n\nCA is ranked as one of the largest independent software corporations in the world. The company creates systems software and applications software that runs in mainframe, distributed computing, virtual machine, cloud computing environments, mobile devices and the Internet of Things (IoT).\nThe company had been a provider of anti-virus and Internet security commercial software programs for personal computers. Today, it is primarily known for its business-to-business (B2B) software with a product portfolio focused on Agile software development, DevOps and Computer Security Software spanning across a wide range of environments such as mainframe, distributed computing, cloud computing and mobile devices. CA Technologies states that its computer software products are used by \"a majority of the Fortune Global 500 companies, government organizations, educational institutions, and thousands of other companies in diverse industries worldwide.\" \n\nCA Technologies posted $4.2 billion in revenue for fiscal year 2018 (ending March 31, 2018) and maintains offices in more than 40 countries. The company employs approximately 11,300 people (March 31, 2018). CA holds more than 1,500 patents worldwide, and has more than 900 patent applications pending. It was headquartered on Long Island for most of its history, at first Jericho, NY and Garden City in Nassau County, then Suffolk County for 22 years in Islandia before moving to Manhattan in June 2014. It was once the second-largest software company in the United States.\n\nThe company avoided indictment for involvement in the 35 day month accounting scandal by reaching a deferred-prosecution agreement with the government in 2004 in which it agreed to pay $225 million in shareholder restitution.\n\nAs of 2018 the company markets nearly 200 software products. Some of the best known are ACF2, TopSecret (security), Datacom (database), Easytrieve (report generator), IDMS (database management), InterTest (debugging), Librarian, Panvalet (library management), and TLMS (tape library management).\n\nThe company was established by Charles Wang and Russell Artzt in 1976.\n\nUnder regulatory pressure in 1969, IBM announced its decision to unbundle the sale of computer hardware from its software and support services; \"i.e.\", mainframe computers from computer programs, etc. The decision opened new markets to competition in the nascent software industry. Wang and Artzt developed several products for that market. In 1976, they obtained exclusive North American distribution rights for CA-Sort, a sort/merge/copy and data management utility software program. Wang and Artzt established a new venture in partnership with Sam Goodner and Max Sevcik of Swiss Company Computer Associates, which they named Computer Associates International, and went to market with CA-Sort, along with their original products. \n\nCA-SORT's sorting algorithms and the product's performance (mostly on the DOS/VS computing platform), combined with modest pricing, led to rapid growth in the North American market. \n\nThroughout the decade, the company grew rapidly via several acquisitions.\n\nIn 1987, CA's stock began trading on the New York Stock Exchange using the ticker symbol \"CA\" following its time on the NASDAQ (1981-1987) using the stock symbol \"CASI\". As the decade ended, CA became the first software company after Microsoft to exceed $1 billion in sales. By 1991, it had $1.4 billion in sales.\n\nEarly in the decade, CA was forced to address criticism of the company as well as a sharp decline in its stock price, which fell more than 50% during 1990. The ensuing changes included a push into foreign markets (Japan, Canada, Africa, Latin America), reform in how the company charged its customers for software maintenance, and improved compatibility with products from other vendors such as Hewlett-Packard (HP), Apple Computer, and Digital Equipment Corporation (DEC). In 1994, CA acquired the ASK Group and continued to offer the Ingres database management system under a variety of brand names.\n\nCA continued its expansion through acquisitions. Legent Corporation was acquired for $1.78 billion in 1995, at that time the biggest-ever acquisition in the software industry, and Cheyenne Software for $1.2 billion in 1996. CA executed the software industry's then-largest acquisition ($3.5 billion) via Platinum Technology International in 1999.\n\nBy 2000, CA had acquired about 200 companies. At this time, the U.S. Department of Justice limited CA's acquisitions. The company refinanced large amounts of debt, and a proxy battle between the board and shareholders. There were also questions regarding executive compensation, accounting methods, and insider-trading by its then CEO and chairman, Sanjay Kumar.\n\nIn 2000, Sanjay Kumar replaced Charles Wang as Chief Executive Officer. In 2002, Kumar became Chairman of Computer Associates' board of directors. In 2006, he was sentenced to 12 years in prison and fined $8 million for his role in a massive accounting fraud at Computer Associates.\n\nCA started the India Technology Centre in Hyderabad on December 10, 2003. In 2004, CA appointed ex-IBMer John Swainson as CEO, who held the position until retirement at the end of 2009.\n\nIn 2010, the company acquired eight companies to support its cloud computing strategy: 3Tera, Nimsoft, NetQoS, Oblicore, Cassatt, 4Base Technology, Arcot Systems, and Hyperformix. In 2011, CA acquired ITKO for $330 million. Two years later, it acquired app deployment and management company Nolio for approximately $40 million, as well as Layer7.\n\nOn January 7, 2013, CA Technologies announced that Michael P. Gregoire would be a member of the board and new chief executive officer. In June 2014, CA Technologies moved its headquarters, without an announcement, from Islandia in Suffolk County, to 520 Madison Avenue in New York City.\n\nIn 2015, the company made four acquisitions, including Rally software for $480 million, Unifyalm, Gridtools, Idmlogic and Xceedium.\n\nIn 2016, CA acquired Blazemeter and Automic, and Veracode and Runscope in 2017.\n\nOn August 8, 2018, CEO Mike Gregoire was elected as chairman of CA Technologies board of directors, replacing retiring chairman Art Weinbach.\n\nOn July 11, 2018, Broadcom announced it would acquire CA Technologies for $18.9 billion in cash, pending regulatory approval. On November 5, 2018 Broadcom announced that it had completed the acquisition of CA Technologies. After the acquisition, Broadcom laid off former CA Technology workers in Silicon Valley. It also laid off 262 former CA Technology employees in Islandia, and some in Manhattan.\n\nIn 1992, the company was sued by Electronic Data Systems (EDS), a CA customer. EDS accused CA of breach of contract, misuse of copyright and violations of anti-trust laws. CA filed a counter-claim, also alleging breach of contract, including copyright infringement and misappropriation of trade secrets. The companies reached a settlement in 1996. \n\nIn 1998, an unsuccessful and hostile takeover bid by CA for computer consulting firm Computer Sciences Corporation (CSC) prompted a bribery suit by CSC's chairman Van Honeycutt against CA's founder and then CEO, Charles Wang.\n\nIn 1999, Wang received the largest bonus in history at that time from a public company. The receipt of a $670 million stock grant that dated to the vesting of a 1995 stock option occurred while the company faced a slowdown in European markets and an economic slump in Asia, both of which had affected CA's earnings and stock price. In total, the company took a $675 million after-tax charge for $1.1 billion in payouts to Wang and other top CA executives.\n\nIn 2000 a shareholder-based class-action lawsuit accused CA of misstating more than $500 million in revenue in its 1998 and 1999 fiscal years in order to artificially inflate its stock price. An investigation by the Securities and Exchange Commission (SEC) followed, resulting in charges against the company and some of its former top executives. The SEC alleged that from 1998 to 2000, CA routinely kept its books open to include quarterly revenue from contracts executed after the quarter ended in order to meet Wall Street analysts’ expectations. The company reached a settlement with the SEC and Department of Justice in 2004, agreeing to pay $225 million in restitution to shareholders and to reform its corporate governance and financial accounting controls. Eight CA executives pleaded guilty to fraud charges – most notably, former CEO and chairman Sanjay Kumar, who received a 12-year prison sentence for orchestrating the scandal. The company subsequently made sweeping changes through virtually all of its senior leadership positions.\n\nIn 2010, CA was listed among the greenest companies by Newsweek's Green rankings. CA has been named a component of the Dow Jones Sustainability Indexes (DJSI) for seven years, from 2012 to 2018. In 2015 and 2016, CA was ranked as one of America's Greenest companies by Newsweek. \n\nIn 2017, the company scored an A- from CDP, the world's most comprehensive rating of companies leading on environmental action, for environmental performance and disclosure.\n\nAccording to a corporate sustainability report released by the company in 2018, CA reduced its Greenhouse Gas Footprint by more than 35% since 2006. It received the Climate Leadership Award in Excellence in GHG Management in 2018, and was included in Barron's 100 Most Sustainable Companies in 2018 as well.\n\nIn February 2018, CA was named one of the World's Most Ethical Companies by Ethisphere Institute for the third consecutive year.\n\nCA Technologies was named one of the best companies for multicultural women by Working Mother Magazine for four consecutive years, from 2015 to 2018 as well as one of the 100 Best Companies from 2015 to 2017. The company was also awarded 4.3 of 5 stars by InHerSight as one of the Top 10 IT Companies for Women in 2017. In 2015 and 2016, Fatherly.com ranked CA as one of the Best Places to Work for New Dads.\n\nIn 2018, CA was named a NAFE top company for executive women. CA was also included in the Bloomberg Gender-Equality Index (GEI) in 2018.\n\nIn 2018, for the fourth consecutive year, the Human Rights Campaign Foundation ranked CA as one of the Best Places to Work for LGBT Equality.\n\nCA CEO Mike Gregoire is a signatory of the CEO Action for Diversity and Inclusion pledge.\n\nFor four consecutive years, 2015-2018, CA was named by Computerworld as one of the Best Places to Work in IT. In 2017, it was named to the Forbes list of America's Best Employers and recognized with a STAR Award for Leadership and Innovation by the Technology Services Industry Association (TSIA).\n\nIn 2018, CA was named to the Thomson Reuters World's Top 100 Technology companies and for six consecutive years has been the recipient of the NorthFace ScoreBoard Award from Customer Relationship Management Institute (CRMI).\n\nCA has a long history of acquisitions in the software industry; some of the largest are listed below.\n"}
{"id": "1257929", "url": "https://en.wikipedia.org/wiki?curid=1257929", "title": "CHEMKIN", "text": "CHEMKIN\n\nCHEMKIN is a proprietary software tool for solving complex chemical kinetics problems. It is used worldwide in the combustion, chemical processing, microelectronics and automotive industries, and also in atmospheric science. It was originally developed at Sandia National Laboratories and is now developed by a US company, Reaction Design.\n\nCHEMKIN solves thousands of reaction combinations to develop a comprehensive understanding of a particular process, which might involve multiple chemical species, concentration ranges, and gas temperatures.\n\nChemical kinetics simulation software allows for a more time-efficient investigation of a potential new process compared to direct laboratory investigation.\n\nOne important driver for the development and use of CHEMKIN is the reduction of pollutants, such as NOx. As these pollutants become more tightly regulated through agreements by agencies such as the United States Environmental Protection Agency and the California Air Resource Board (CARB), researchers are making increasing use of simulation technology.\n\nOne limitation of CHEMKIN is that it assumes the reaction vessel has a relatively simple geometry, whereas sometimes this is not the case. For that reason, a related program called KINetics is often used in conjunction with Computational Fluid Dynamics tools. CFD programs are better able to account for geometric complexity, at the expense of being more limited in their treatment of the underlying chemistry of the reactive process being studied.\n\nReaction Design was acquired by ANSYS in 2014 so Chemkin and related products are now available through ANSYS. \n\n\n"}
{"id": "51341771", "url": "https://en.wikipedia.org/wiki?curid=51341771", "title": "CM Telecom", "text": "CM Telecom\n\nCM (formerly called CM Telecom) is a mobile services company based in the Netherlands. The company was formed in 1999, and provides a number of different software solutions for direct messaging, , ecommerce payments and digital identification. In 2006, it was ranked by Deloitte as one of the fastest growing Dutch tech companies.\n\nIn recent years, CM has acquired a number of messaging and media startups. CM is the main sponsor of football club NAC Breda.\n\nCM was founded in February 1999 by Jeroen van Glabbeek and Gilbert Gooijers. In the company's first year, it was known as ClubMessage B.V. The company's main product at the time GroupText, used by event organisers to distribute information to clubbers and event attendees. The software was predominantly used across the Benelux by large nightlife venues to promote their nights and DJ events.\n\nAs the company grew, they began to operate in other event sectors, such as music festivals. During the next couple of years, the company focused on the expansion of its SMS messaging service, and created and patented the software MailText. In recent years, CM has diversified into the mobile payments market, with the launch of CM Payments Worldwide.\n\nIn early 2015 it was announced that the company would be opening offices in both Paris and London. During the same year, CM received coverage on The Next Web for their growing innovations in media messaging. Their solution for managing push message campaigns, where companies or app developers can measure and drive interaction with their users. Firstly push messages are sent using the CM solution. Customers can then be automatically contacted by SMS if the push notification goes unread.\n\nIn March 2016, CM acquired GlobalMessaging, which was based in Peterborough, England. Around the same time, the company also acquired the mobile app developer, .\n\nIn July 2017, CM took over payment institution Payments from its American owner Ingram Micro.. In the same month, the company revealed a new visual identity, logo and name change.\n\nAs part of the instant messaging market, CM has predominantly focused on backend solutions. The mobile services company and others similar have been at the forefront of mobile messaging solutions over the last decade, and the rise of what is often referred to as the messaging economy.\n\nCM has become known for their work in the market of hybrid messaging. This type of messaging means that customers can be contacted using a variety of messaging formats. It allows the sender to contact its customers through more than one messaging format.\n\nIn 2016, they launched a real-time analytics tool for mobile. The web tool provides insights in messaging traffic and conversions to customers.\n\n"}
{"id": "6473308", "url": "https://en.wikipedia.org/wiki?curid=6473308", "title": "Catalyst poisoning", "text": "Catalyst poisoning\n\nCatalyst poisoning refers to the partial or total deactivation of a catalyst. Poisoning is caused by chemical compounds. Although usually undesirable, poisoning may be helpful when it results in improved selectivity. For example, Lindlar's catalyst is poisoned so that it selectively catalyzes the reduction of alkynes. On the other hand Lead from leaded gasoline deactivates catalytic converters.\n\nPoisoning is one of several mechanisms for the deactivation of a catalyst. Other mechanisms include sintering, coking, and solid-state transformation.\n\nPoisoning often involves compounds that bond chemically to the active surface sites of a catalyst. Poisoning decreases the number of catalytic sites or the fraction of the total surface area that has the capability of promoting reaction always decreases, and the average distance that a reactant molecule must diffuse through the pore structure before undergoing reaction may increase. Poisoned sites can no longer accelerate the reaction with which the catalyst was supposed to catalyze. Large scale production of substances such as ammonia in the Haber–Bosch process include steps to remove potential poisons from the product stream.\n\nWhen the poisoning reaction rate is slow relative to the rate of diffusion, the poison will be evenly distributed throughout the catalyst and will result in homogeneous poisoning of the catalyst. Conversely, if the reaction rate is fast compared to the rate of diffusion, a poisoned shell will form on the exterior layers of the catalyst, a situation known as \"pore-mouth\" poisoning, and the rate of catalytic reaction may become limited by the rate of diffusion through the inactive shell.\n\nOrganic functional groups and inorganic anions often have the ability to strongly adsorb to metal surfaces, i.e. they are poisons. Common catalyst poisons include the following: carbon monoxide, halide, cyanide, sulfide, sulfite, and phosphite and organic molecules such as nitriles, nitros, oximes and nitrogen-containing heterocycles. Agents vary their catalytic properties because of the nature of the transition metal.\n\nIf the catalyst and reaction conditions are indicative of a low effectiveness factor, selective poisoning may be observed, which is a phenomenon where poisoning of only a small fraction of the catalyst surface gives a disproportionately large drop in activity. Mathematical models exist which describe the cases where the interaction of the poisoning process with the influence of the intraparticle diffusion on the rates of the primary and poisoning reactions leads to an interesting relations between observed catalytic activity and the fraction of surface poisoned.\n\nBy combining a material balance over a differential element of pore length and the Thiele modulus, the equation is found:\n\nwhere \"η\" is the effectiveness factor of the poisoned surface and \"h\" is the Thiele modulus for the poisoned case.\n\nWhen the ratio of the reaction rate for the poisoned pore to the unpoisoned pore is considered, the following equation can be found:\n\nwhere \"F\" is the ratio of rates of poisoned and unpoisoned pores, \"h\" is the Thiele modulus for the unpoisoned case, and \"α\" is the fraction of the surface that is poisoned.\n\nThe above equation simplifies depending on the value of \"h\". When \"h\" is small, meaning that the surface is available, the equation becomes:\n\nThis represents the \"classical case\" of nonselective poisoning where the fraction of the activity remaining is equal to the fraction of the unpoisoned surface remaining.\n\nWhen \"h\" is very large, it becomes:\n\nIn this case, the catalyst effectiveness factors are considerably less than unity, and the effects of the portion of the poison adsorbed near the closed end of the pore are not as apparent as when \"h\" is small.\n\nDelving further into the mathematical relationships of selective poisoning, or \"Pore-Mouth\" poisoning, looking at the steady-state conditions, the rate of diffusion of the reactant through the poisoned region is equal to the rate of reaction. The rate of diffusion is given by:\n\nAnd the rate of reaction within a pore is given by:\n\nThrough further manipulation and substitution, the fraction of the catalyst surface available for reaction can be obtained from the ratio of the poisoned reaction rate to the unpoisoned reaction rate:\n\nor\n\nwhere, as before, \"h\" is the Thiele modulus for the unpoisoned case, and \"α\" is the fraction of the surface that is poisoned.\n\nUsually, catalyst poisoning is undesirable as it leads to a loss of usefulness of expensive noble metals or their complexes. However, poisoning of catalysts can be used to improve selectivity of reactions. Poisoning can allow for selective intermediates to be isolated and final products with desirable stereochemistry to be achieved.\n\nPoisoning of palladium and platinum catalysts has been extensively researched. As a rule of thumb, platinum (as Adams' catalyst, platinum oxide finely divided on carbon) is less susceptible. Common poisons for these two metals are sulfur and nitrogen-heterocycles like pyridine and quinoline.\n\nIn some cases, a highly active catalyst can lead to undesirable secondary reactions with the desired product. In some of these cases, the addition of a small amount of a catalyst poison increases the yield of the desired product by lower the catalyst activity. For example, in the classical \"Rosenmund reduction\" of an acyl chloride to the corresponding aldehyde, the palladium catalyst (over barium sulfate or calcium carbonate) is intentionally poisoned by the addition of sulfur or quinoline in order to lower the catalyst activity and thereby prevent further reduction of the aldehyde product to yield a primary alcohol. In the case of Lindlar's catalyst, palladium is poisoned with a lead salt to allow reduction of an alkyne to the corresponding alkene while preventing reduction of the alkene product to the corresponding alkane.\n\nIn the purification of crude petroleum products the process of hydrodesulfurization is utilized. Thiol containing hydrocarbons, such as thiophene, are reduced using H in order to produce HS and different length chains of hydrocarbons. Common catalyst used are tungsten and molybdenum sulfide particles. By adding cobalt and nickel nuclei to either edge s or partially incorporating them into the crystal lattice structure can create more efficient catalyst. The synthesis of the catalyst creates a supported hybrid that prevents poisoning of the cobalt nuclei that may be unstable in the mono-nuclear form.\n\nA catalytic converter for an automobile can be poisoned if the vehicle is operated on gasoline containing lead additives. Fuel cells running on hydrogen must use very pure reactants, free of sulfur and carbon compounds.\n\nRaney nickel catalysts have reduced activity when it is in combination with mild steel. The loss in activity of the catalyst can be overcome by having a lining of epoxy or other substances.\n\n"}
{"id": "1012777", "url": "https://en.wikipedia.org/wiki?curid=1012777", "title": "Circuit bending", "text": "Circuit bending\n\nCircuit bending is the creative, chance-based customization of the circuits within electronic devices such as low-voltage, battery-powered guitar effects, children's toys and digital synthesizers to create new musical or visual instruments and sound generators.\n\nEmphasizing spontaneity and randomness, the techniques of circuit bending have been commonly associated with noise music, though many more conventional contemporary musicians and musical groups have been known to experiment with \"bent\" instruments. Circuit bending usually involves dismantling the machine and adding components such as switches and potentiometers that alter the circuit.\n\nCircuit bending is experimenting with second-hand electronics in a DIY fashion. Inexpensive keyboards, drum machines, and electronic children's toys (not necessarily designed for music production) are commonly used. Typically, this is done with battery-powered devices. Random modifications to devices plugged into the wall can result in fire or electrocution.\n\nAesthetic value, immediate usability and highly randomized results are often factors in the process of successfully \"bending\" electronics. Although the history of electronic music is often associated with unconventional sonic results, innovators like Robert Moog and Léon Theremin were electrical engineers, and were typically more concerned with the consistency of their instruments. In contrast, circuit bending is typified by inconsistencies in instruments built in an unscientific manner. While many pre-fitted circuit bent machines are sold on auction sites such as eBay, this somewhat contravenes the intention of most practitioners. Machines bent to a repeated configuration are more analogous to the well known practice of \"mods\", such as the Devilfish mod for the Roland TB-303, the famous Speak & Spell toys or various Analogman or Pedaldoc guitar pedal circuit modifications.\n\nCircuit bending an audio device typically involves removing the rear panel of the device and connecting any two circuit locations with a \"jumper\" wire, sending current from one part of the circuit into another. Results are monitored through either the device's internal speaker or by connecting an amplifier to the speaker output. If an interesting effect is achieved, this connection would be marked for future reference or kept active by either soldering a new connection or bridging it with crocodile clips. Often other components are inserted at these points such as pushbuttons or switches, to turn the effect on or off; or components such as resistors or capacitors, to change the quality of the audio output. This is repeated on a trial and error basis. Other components added into the circuit can give the performer more expressiveness, such as potentiometers, photoresistors (for reaction to light) and pressure sensors.\nThe simplest input, and the one most identified with circuit bending, is the body contact, where the performer's touch causes the circuit to change the sound. Often metal knobs, plates, screws or studs are wired to these circuit points to give easier access to these points from the outside the case of the device.\nSince creative experimentation is a key element to the practice of circuit bending, there is always a possibility that short circuiting may yield undesirable results, including component failure. In particular, connecting the power supply or a capacitor directly to a computer chip lead can destroy the chip and make the device inoperable. Before beginning to do circuit bending, a person should learn the basic risk factors about working with electrical and electronic products, including how to identify capacitors (which can give a person a serious shock due to the electrical charge that they store), and how to avoid risks with AC power. For safety reasons, a circuit bender should have a few basic electronics tools, such as a multimeter (an electronic testing device which measures voltage, resistance and other factors). It is advised that beginner circuit benders should \"never\" \"bend\" any device that gets its power from mains electricity (household AC power), as this would carry a serious risk of electrocution. Circuit bending can also be carried out in interactive electronic audio games. People modify their electronic games to enhance the quality of recordings used for fan-made projects or to change the speed of the game which results in a pitch change. This makes the gameplay easier, especially if the game gets impossibly fast. Adding a knob or a switch to change the pitch of the game can lead to some disadvantages which include the game can change its pitch slightly when its lights are turned on, and it can cause the batteries to drain out quickly on high speeds.\n\nAlthough similar methods were previously used by other musicians and engineers, this method of music creation is believed to have been pioneered by Reed Ghazala in the 1960s. Ghazala's experience with circuit-bending began in 1966 when a toy transistor amplifier, by chance, shorted-out against a metal object in his desk drawer, resulting in a stream of unusual sounds. While Ghazala says that he was not the first circuit bender, he coined the term Circuit Bending and whole-heartedly promoted the proliferation of the concept and practice through his writings and internet site, earning him the title \"Father of Circuit Bending\".\n\nSerge Tcherepnin, designer of the Serge modular synthesizers, discussed his early experiments in the 1950s with the transistor radio, in which he found sensitive circuit points in those simple electronic devices and brought them out to \"body contacts\" on the plastic chassis. Prior to Mark's and Reed's experiments other pioneers also explored the body-contact idea, one of the earliest being Thaddeus Cahill (1897) whose telharmonium, it is reported, was also touch-sensitive.\n\nSince 1984, Swiss duo Voice Crack created music by manipulating common electronic devices in a practice they termed \"cracked everyday electronics\".\n\nThe city of Chicago is host to a longtime community of circuit bending innovation, including the performance/teaching duo Roth Mobot who introduced such techniques as the voltage starving Waldeck Interruptor and were nominated for Prix Ars Electronica's Digital Communities award in 2008. Roth Mobot's Patrick McCarthy hosted a long-running circuit bending class at the Old Town School of Folk Music, and currently runs the Museum of Science and Industry's Fab Lab, incorporating digital fabrication techniques alongside creative circuitry experimentation. Other prominent local innovators include Chicago Art Institute professor Nicolas Collins, author of the influential text Handmade Electronic Music, as well as Alex Inglizian, chief engineer at Experimental Sound Studio.\n\n\nAlexandre Marino Fernandez, Fernando Iazzetta, Circuit-Bending and DIY Culture\n\n"}
{"id": "51529056", "url": "https://en.wikipedia.org/wiki?curid=51529056", "title": "Coalesse", "text": "Coalesse\n\nCoalesse is a United States-based furniture company founded in 2008. It is a division of Steelcase and creates products with the goal of encouraging collaboration. Coalesse is headquartered in Grand Rapids, Michigan with their design headquarters in San Francisco and their main showroom located in the Merchandise Mart in Chicago.\n\nCoalesse launched in June 2008 at the NeoCon World’s Trade Fair in Chicago. It is a combination of Steelcase’s Brayton, Metro, and Vecta brands, but also includes pieces made by Carl Hansen & Son, Walter Knoll AG & Co., Viccarbe, and PP Mobler. Their original brand focus was on designing office and home furniture that appeals to what the company called “live/work lifestyles, but later shifted their focus away from creating home-oriented pieces toward creating more work-oriented pieces.”\n\nIn 2012, it was announced that Coalesse was planning to expand their business operations into Europe.\n\nCoalesse has a 4,000 square foot in-house design studio and a team of seven to eight internal designers, with many partners worldwide, such as:\n\nCoalesse has worked with companies such as Campbell Ewald, Scape, DirecTV, Grand Valley State University, Tolleson, and Quicken Loans to design some of their office spaces.\n\nSome of Coalesse’s products have received awards at the NeoCon World’s Trade Fair:\n\nIn 2016, Coalesse’s LessThanFive Chair was included in an exhibit at Grand Hornu in Belgium that showcased the work of designer Michael Young.\n\nHerman Miller\n\nKnoll\n\nVitra\n\nHaworth\n\nIzzy+\n\nNucraft\n\nKI (Krueger International)\n\nOkamura Corporation\n\nTeknion\n"}
{"id": "5638", "url": "https://en.wikipedia.org/wiki?curid=5638", "title": "Combustion", "text": "Combustion\n\nCombustion, or burning, is a high-temperature exothermic redox chemical reaction between a fuel (the reductant) and an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke. Combustion in a fire produces a flame, and the heat produced can make combustion self-sustaining. Combustion is often a complicated sequence of elementary radical reactions. Solid fuels, such as wood and coal, first undergo endothermic pyrolysis to produce gaseous fuels whose combustion then supplies the heat required to produce more of them. Combustion is often hot enough that incandescent light in the form of either glowing or a flame is produced. A simple example can be seen in the combustion of hydrogen and oxygen into water vapor, a reaction commonly used to fuel rocket engines. This reaction releases 242 kJ/mol of heat and reduces the enthalpy accordingly (at constant temperature and pressure):\n\nCombustion of an organic fuel in air is always exothermic because the double bond in O is much weaker than other double bonds or pairs of single bonds, and therefore the formation of the stronger bonds in the combustion products and results in the release of energy. The bond energies in the fuel play only a minor role, since they are similar to those in the combustion products; e.g., the sum of the bond energies of CH is nearly the same as that of . The heat of combustion is approximately -418 kJ per mole of O used up in the combustion reaction, and can be estimated from the elemental composition of the fuel.\n\nUncatalyzed combustion in air requires fairly high temperatures. Complete combustion is stoichiometric with respect to the fuel, where there is no remaining fuel, and ideally, no remaining oxidant. Thermodynamically, the chemical equilibrium of combustion in air is overwhelmingly on the side of the products. However, complete combustion is almost impossible to achieve, since the chemical equilibrium is not necessarily reached, or may contain unburnt products such as carbon monoxide, hydrogen and even carbon (soot or ash). Thus, the produced smoke is usually toxic and contains unburned or partially oxidized products. Any combustion at high temperatures in atmospheric air, which is 78 percent nitrogen, will also create small amounts of several nitrogen oxides, commonly referred to as , since the combustion of nitrogen is thermodynamically favored at high, but not low temperatures. Since combustion is rarely clean, flue gas cleaning or catalytic converters may be required by law.\n\nFires occur naturally, ignited by lightning strikes or by volcanic products. Combustion (fire) was the first controlled chemical reaction discovered by humans, in the form of campfires and bonfires, and continues to be the main method to produce energy for humanity. Usually, the fuel is carbon, hydrocarbons or more complicated mixtures such as wood that contains partially oxidized hydrocarbons. The thermal energy produced from combustion of either fossil fuels such as coal or oil, or from renewable fuels such as firewood, is harvested for diverse uses such as cooking, production of electricity or industrial or domestic heating. Combustion is also currently the only reaction used to power rockets. Combustion is also used to destroy (incinerate) waste, both nonhazardous and hazardous.\n\nOxidants for combustion have high oxidation potential and include atmospheric or pure oxygen, chlorine, fluorine, chlorine trifluoride, nitrous oxide and nitric acid. For instance, hydrogen burns in chlorine to form hydrogen chloride with the liberation of heat and light characteristic of combustion. Although usually not catalyzed, combustion can be catalyzed by platinum or vanadium, as in the contact process.\n\nIn complete combustion, the reactant burns in oxygen, and produces a limited number of products. When a hydrocarbon burns in oxygen, the reaction will primarily yield carbon dioxide and water. When elements are burned, the products are primarily the most common oxides. Carbon will yield carbon dioxide, sulfur will yield sulfur dioxide, and iron will yield iron(III) oxide. Nitrogen is not considered to be a combustible substance when oxygen is the oxidant, but small amounts of various nitrogen oxides (commonly designated species) form when the air is the oxidant.\n\nCombustion is not necessarily favorable to the maximum degree of oxidation, and it can be temperature-dependent. For example, sulfur trioxide is not produced quantitatively by the combustion of sulfur. NOx species appear in significant amounts above about , and more is produced at higher temperatures. The amount of NOx is also a function of oxygen excess.\n\nIn most industrial applications and in fires, air is the source of oxygen (). In the air, each mole of oxygen is mixed with approximately of nitrogen. Nitrogen does not take part in combustion, but at high temperatures some nitrogen will be converted to (mostly , with much smaller amounts of ). On the other hand, when there is insufficient oxygen to completely combust the fuel, some fuel carbon is converted to carbon monoxide and some of the hydrogen remains unreacted. A more complete set of equations for the combustion of a hydrocarbon in the air, therefore, requires an additional calculation for the distribution of oxygen between the carbon and hydrogen in the fuel.\n\nThe amount of air required for complete combustion to take place is known as theoretical air. However, in practice, the air used is 2-3x that of theoretical air.\n\nIncomplete combustion will occur when there is not enough oxygen to allow the fuel to react completely to produce carbon dioxide and water. It also happens when the combustion is quenched by a heat sink, such as a solid surface or flame trap. Same as complete combustion, water is produced by incomplete combustion. However, carbon, carbon monoxide, and/or hydroxide are the products instead of carbon dioxide.\n\nFor most fuels, such as diesel oil, coal or wood, pyrolysis occurs before combustion. In incomplete combustion, products of pyrolysis remain unburnt and contaminate the smoke with noxious particulate matter and gases. Partially oxidized compounds are also a concern; partial oxidation of ethanol can produce harmful acetaldehyde, and carbon can produce toxic carbon monoxide.\n\nThe quality of combustion can be improved by the designs of combustion devices, such as burners and internal combustion engines. Further improvements are achievable by catalytic after-burning devices (such as catalytic converters) or by the simple partial return of the exhaust gases into the combustion process. Such devices are required by environmental legislation for cars in most countries and may be necessary to enable large combustion devices, such as thermal power stations, to reach legal emission standards.\n\nThe degree of combustion can be measured and analyzed with test equipment. HVAC contractors, firemen and engineers use combustion analyzers to test the efficiency of a burner during the combustion process. In addition, the efficiency of an internal combustion engine can be measured in this way, and some U.S. states and local municipalities use combustion analysis to define and rate the efficiency of vehicles on the road today.\n\nCarbon monoxide is one of the products from incomplete combustion. Carbon is released in the normal incomplete combustion reaction, forming soot and dust. Since carbon monoxide is considered as a poisonous gas, complete combustion is more preferable, as carbon monoxide may also lead to respiratory troubles when breathed since it takes the place of oxygen and combines with hemoglobin.\n\nThese oxides combine with water and oxygen in the atmosphere, creating nitric acid and sulfuric acids, which return to Earth's surface as acid deposition, or \"acid rain.\" Acid deposition harms aquatic organisms and kills trees. Due to its formation of certain nutrients which are less available to plants such as calcium and phosphorus, it reduces the productivity of ecosystem and farms. An additional problem associated with nitrogen oxides is that they, along with hydrocarbon pollutants, contribute to the formation of tropospheric ozone, a major component of smog.\n\nBreathing carbon monoxide causes headache, dizziness, vomiting, and nausea. If carbon monoxide levels are high enough, humans become unconscious or die. Exposure to moderate and high levels of carbon monoxide over long periods of time are positively correlation with risk of heart disease. People who survive severe CO poisoning may suffer long-term health problems. Carbon monoxide from air is absorbed in the lungs which then binds with hemoglobin in human's red blood cells. This would reduce the capacity of red blood cells to carry oxygen throughout the body.\n\nSmouldering is the slow, low-temperature, flameless form of combustion, sustained by the heat evolved when oxygen directly attacks the surface of a condensed-phase fuel. It is a typically incomplete combustion reaction. Solid materials that can sustain a smouldering reaction include coal, cellulose, wood, cotton, tobacco, peat, duff, humus, synthetic foams, charring polymers (including polyurethane foam) and dust. Common examples of smoldering phenomena are the initiation of residential fires on upholstered furniture by weak heat sources (e.g., a cigarette, a short-circuited wire) and the persistent combustion of biomass behind the flaming fronts of wildfires.\n\nRapid combustion is a form of combustion, otherwise known as a fire, in which large amounts of heat and light energy are released, which often results in a flame. This is used in a form of machinery such as internal combustion engines and in thermobaric weapons. Such a combustion is frequently called an explosion, though for an internal combustion engine this is inaccurate. An internal combustion engine nominally operates on a controlled rapid burn. When the fuel-air mixture in an internal combustion engine explodes, that is known as detonation.\n\nSpontaneous combustion is a type of combustion which occurs by self-heating (increase in temperature due to exothermic internal reactions), followed by thermal runaway (self-heating which rapidly accelerates to high temperatures) and finally, ignition.\nFor example, phosphorus self-ignites at room temperature without the application of heat. Organic materials undergoing bacterial composting can generate enough heat to reach the point of combustion.\n\nCombustion resulting in a turbulent flame is the most used for industrial application (e.g. gas turbines, gasoline engines, etc.) because the turbulence helps the mixing process between the fuel and oxidizer.\n\nThe term 'micro' gravity refers to a gravitational state that is 'low' (i.e., 'micro' in the sense of 'small' and not necessarily a millionth of Earth's normal gravity) such that the influence of buoyancy on physical processes may be considered small relative to other flow processes that would be present at normal gravity. In such an environment, the thermal and flow transport dynamics can behave quite differently than in normal gravity conditions (e.g., a candle's flame takes the shape of a sphere.). Microgravity combustion research contributes to the understanding of a wide variety of aspects that are relevant to both the environment of a spacecraft (e.g., fire dynamics relevant to crew safety on the International Space Station) and terrestrial (Earth-based) conditions (e.g., droplet combustion dynamics to assist developing new fuel blends for improved combustion, materials fabrication processes, thermal management of electronic systems, multiphase flow boiling dynamics, and many others).\n\nCombustion processes which happen in very small volumes are considered micro-combustion. The high surface-to-volume ratio increases specific heat loss. Quenching distance plays a vital role in stabilizing the flame in such combustion chambers.\n\nGenerally, the chemical equation for stoichiometric combustion of a hydrocarbon in oxygen is:\nwhere formula_1.\n\nFor example, the stoichiometric burning of propane in oxygen is:\n\nIf the stoichiometric combustion takes place using air as the oxygen source, the nitrogen present in the air (Atmosphere of Earth) can be added to the equation (although it does not react) to show the stoichiometric composition of the fuel in air and the composition of the resultant flue gas. Note that treating all non-oxygen components in air as nitrogen gives a 'nitrogen' to oxygen ratio of 3.77, i.e. (100% - O2%) / O2% where O2% is 20.95% vol:\nwhere formula_3.\n\nFor example, the stoichiometric combustion of propane (<chem>C3H8</chem>) in air is:\nThe stoichiometric composition of propane in air is 1 / (1 + 5 + 18.87) = 4.02% vol\n\nVarious other substances begin to appear in significant amounts in combustion products when the flame temperature is above about . When excess air is used, nitrogen may oxidize to and, to a much lesser extent, to . forms by disproportionation of , and and form by disproportionation of .\n\nFor example, when of propane is burned with of air (120% of the stoichiometric amount), the combustion products contain 3.3% . At , the equilibrium combustion products contain 0.03% and 0.002% . At , the combustion products contain 0.17% , 0.05% , 0.01% , and 0.004% .\n\nDiesel engines are run with an excess of oxygen to combust small particles that tend to form with only a stoichiometric amount of oxygen, necessarily producing nitrogen oxide emissions. Both the United States and European Union enforce limits to vehicle nitrogen oxide emissions, which necessitate the use of special catalytic converters or treatment of the exhaust with urea (see Diesel exhaust fluid).\n\nThe incomplete (partial) combustion of a hydrocarbon with oxygen produces a gas mixture containing mainly , , , and . Such gas mixtures are commonly prepared for use as protective atmospheres for the heat-treatment of metals and for gas carburizing. The general reaction equation for incomplete combustion of one mole of a hydrocarbon in oxygen is:\n\nWhen \"z\" falls below roughly 50% of the stoichiometric value, can become an important combustion product; when \"z\" falls below roughly 35% of the stoichiometric value, elemental carbon may become stable.\n\nThe products of incomplete combustion can be calculated with the aid of a material balance, together with the assumption that the combustion products reach equilibrium. For example, in the combustion of one mole of propane () with four moles of , seven moles of combustion gas are formed, and \"z\" is 80% of the stoichiometric value. The three elemental balance equations are:\n\nThese three equations are insufficient in themselves to calculate the combustion gas composition.\nHowever, at the equilibrium position, the water-gas shift reaction gives another equation:\n\nFor example, at the value of \"K\" is 0.728. Solving, the combustion gas consists of 42.4% , 29.0% , 14.7% , and 13.9% . Carbon becomes a stable phase at and pressure when z is less than 30% of the stoichiometric value, at which point the combustion products contain more than 98% and and about 0.5% .\n\nSubstances or materials which undergo combustion are called fuels. The most common examples are natural gas, propane, kerosene, diesel, petrol, charcoal, coal, wood, etc.\n\nCombustion of a liquid fuel in an oxidizing atmosphere actually happens in the gas phase. It is the vapor that burns, not the liquid. Therefore, a liquid will normally catch fire only above a certain temperature: its flash point. The flash point of a liquid fuel is the lowest temperature at which it can form an ignitable mix with air. It is the minimum temperature at which there is enough evaporated fuel in the air to start combustion.\n\nCombustion of gaseous fuels may occur through one of four distinctive types of burning: diffusion flame, premixed flame, autoignitive reaction front, or as a detonation. The type of burning that actually occurs depends on the degree to which the fuel and oxidizer are mixed prior to heating: for example, a diffusion flame is formed if the fuel and oxidizer are separated initially, whereas a premixed flame is formed otherwise. Similarly, the type of burning also depends on the pressure: a detonation, for example, is an autoignitive reaction front coupled to a strong shock wave giving it its characteristic high-pressure peak and high detonation velocity.\n\nThe act of combustion consists of three relatively distinct but overlapping phases:\n\nEfficient process heating requires recovery of the largest possible part of a fuel’s heat of combustion into the material being processed. There are many avenues of loss in the operation of a heating process. Typically, the dominant loss is sensible heat leaving with the offgas (i.e., the flue gas). The temperature and quantity of offgas indicates its heat content (enthalpy), so keeping its quantity low minimizes heat loss.\n\nIn a perfect furnace, the combustion air flow would be matched to the fuel flow to give each fuel molecule the exact amount of oxygen needed to cause complete combustion. However, in the real world, combustion does not proceed in a perfect manner. Unburned fuel (usually and ) discharged from the system represents a heating value loss (as well as a safety hazard). Since combustibles are undesirable in the offgas, while the presence of unreacted oxygen there presents minimal safety and environmental concerns, the first principle of combustion management is to provide more oxygen than is theoretically needed to ensure that all the fuel burns. For methane () combustion, for example, slightly more than two molecules of oxygen are required.\n\nThe second principle of combustion management, however, is to not use too much oxygen. The correct amount of oxygen requires three types of measurement: first, active control of air and fuel flow; second, offgas oxygen measurement; and third, measurement of offgas combustibles. For each heating process, there exists an optimum condition of minimal offgas heat loss with acceptable levels of combustibles concentration. Minimizing excess oxygen pays an additional benefit: for a given offgas temperature, the NOx level is lowest when excess oxygen is kept lowest.\n\nAdherence to these two principles is furthered by making material and heat balances on the combustion process. The material balance directly relates the air/fuel ratio to the percentage of in the combustion gas. The heat balance relates the heat available for the charge to the overall net heat produced by fuel combustion. Additional material and heat balances can be made to quantify the thermal advantage from preheating the combustion air, or enriching it in oxygen.\n\nCombustion in oxygen is a chain reaction in which many distinct radical intermediates participate. The high energy required for initiation is explained by the unusual structure of the dioxygen molecule. The lowest-energy configuration of the dioxygen molecule is a stable, relatively unreactive diradical in a triplet spin state. Bonding can be described with three bonding electron pairs and two antibonding electrons, whose spins are aligned, such that the molecule has nonzero total angular momentum. Most fuels, on the other hand, are in a singlet state, with paired spins and zero total angular momentum. Interaction between the two is quantum mechanically a \"forbidden transition\", i.e. possible with a very low probability. To initiate combustion, energy is required to force dioxygen into a spin-paired state, or singlet oxygen. This intermediate is extremely reactive. The energy is supplied as heat, and the reaction then produces additional heat, which allows it to continue.\n\nCombustion of hydrocarbons is thought to be initiated by hydrogen atom abstraction (not proton abstraction) from the fuel to oxygen, to give a hydroperoxide radical (HOO). This reacts further to give hydroperoxides, which break up to give hydroxyl radicals. There are a great variety of these processes that produce fuel radicals and oxidizing radicals. Oxidizing species include singlet oxygen, hydroxyl, monatomic oxygen, and hydroperoxyl. Such intermediates are short-lived and cannot be isolated. However, non-radical intermediates are stable and are produced in incomplete combustion. An example is acetaldehyde produced in the combustion of ethanol. An intermediate in the combustion of carbon and hydrocarbons, carbon monoxide, is of special importance because it is a poisonous gas, but also economically useful for the production of syngas.\n\nSolid and heavy liquid fuels also undergo a great number of pyrolysis reactions that give more easily oxidized, gaseous fuels. These reactions are endothermic and require constant energy input from the ongoing combustion reactions. A lack of oxygen or other poorly designed conditions result in these noxious and carcinogenic pyrolysis products being emitted as thick, black smoke.\n\nThe rate of combustion is the amount of a material that undergoes combustion over a period of time. It can be expressed in grams per second (g/s) or kilograms per second (kg/s).\n\nDetailed descriptions of combustion processes, from the chemical kinetics perspective, requires the formulation of large and intricate webs of elementary reactions. For instance, combustion of hydrocarbon fuels typically involve hundreds of chemical species reacting according to thousands of reactions.\n\nInclusion of such mechanisms within computational flow solvers still represents a pretty challenging task mainly in two aspects. First, the number of degrees of freedom (proportional to the number of chemical species) can be dramatically large; second, the source term due to reactions introduces a disparate number of time scales which makes the whole dynamical system stiff. As a result, the direct numerical simulation of turbulent reactive flows with heavy fuels soon becomes intractable even for modern supercomputers.\n\nTherefore, a plethora of methodologies has been devised for reducing the complexity of combustion mechanisms without resorting to high detail level. Examples are provided by:\n\nThe kinetic modelling may be explored for insight into the reaction mechanisms of thermal decomposition in the combustion of different materials by using for instance Thermogravimetric analysis.\n\nAssuming perfect combustion conditions, such as complete combustion under adiabatic conditions (i.e., no heat loss or gain), the adiabatic combustion temperature can be determined. The formula that yields this temperature is based on the first law of thermodynamics and takes note of the fact that the heat of combustion is used entirely for heating the fuel, the combustion air or oxygen, and the combustion product gases (commonly referred to as the \"flue gas\").\n\nIn the case of fossil fuels burnt in air, the combustion temperature depends on all of the following:\n\nThe adiabatic combustion temperature (also known as the \"adiabatic flame temperature\") increases for higher heating values and inlet air and fuel temperatures and for stoichiometric air ratios approaching one.\n\nMost commonly, the adiabatic combustion temperatures for coals are around (for inlet air and fuel at ambient temperatures and for formula_10), around for oil and for natural gas.\n\nIn industrial fired heaters, power station steam generators, and large gas-fired turbines, the more common way of expressing the usage of more than the stoichiometric combustion air is \"percent excess combustion air\". For example, excess combustion air of 15 percent means that 15 percent more than the required stoichiometric air is being used.\n\nCombustion instabilities are typically violent pressure oscillations in a combustion chamber. These pressure oscillations can be as high as 180 dB, and long-term exposure to these cyclic pressure and thermal loads reduce the life of engine components. In rockets, such as the F1 used in the Saturn V program, instabilities led to massive damage to the combustion chamber and surrounding components. This problem was solved by re-designing the fuel injector. In liquid jet engines, the droplet size and distribution can be used to attenuate the instabilities. Combustion instabilities are a major concern in ground-based gas turbine engines because of NOx emissions. The tendency is to run lean, an equivalence ratio less than 1, to reduce the combustion temperature and thus reduce the NOx emissions; however, running the combustion lean makes it very susceptible to combustion instability.\n\nThe Rayleigh Criterion is the basis for analysis of thermoacoustic combustion instability and is evaluated using the Rayleigh Index over one cycle of instability\n\nwhere q' is the heat release rate perturbation and p' is the pressure fluctuation.\nWhen the heat release oscillations are in phase with the pressure oscillations, the Rayleigh Index is positive and the magnitude of the thermo acoustic instability is maximised. On the other hand, if the Rayleigh Index is negative, then thermoacoustic damping occurs. The Rayleigh Criterion implies that a thermoacoustic instability can be optimally controlled by having heat release oscillations 180 degrees out of phase with pressure oscillations at the same frequency. This minimizes the Rayleigh Index.\n\n\n\n\n"}
{"id": "23572891", "url": "https://en.wikipedia.org/wiki?curid=23572891", "title": "Computer-aided inspection", "text": "Computer-aided inspection\n\nComputer-aided inspection (CAI) is the use of computer-based software tools that assist quality engineers, machinists and inspectors in manufacturing product components. Its primary purpose is to create a faster production process and components with more precise dimensions and material consistency. CAI is a software tool that makes it possible to inspect physical models using computer-aided design (CAD) programs. CAM creates real life versions of components designed within a software package. CAM was first used in 1971 for car body design and tooling.\n"}
{"id": "7057338", "url": "https://en.wikipedia.org/wiki?curid=7057338", "title": "Computer Press Association", "text": "Computer Press Association\n\nFounded in 1983, the Computer Press Association (CPA) was established to promote excellence in the field of computer journalism. The association was composed of working editors, writers, producers, and freelancers who covered issues related to computers and technology. The CPA conducted the annual Computer Press Awards, which was the preeminent editorial awards of the computer and technology media. The CPA Awards honored outstanding examples in print, broadcast and electronic media. Awards were given for print publications, such as PC Magazine; online news media, such as Newsbytes News Network (both were multiple winners); individual columns and features by well-known journalists such as Steven Levy (author of “”); broadcast awards such as “Best Radio Program”; as well as book awards in categories such as Best Product Specific Book. CPA President Jeff Yablon (1994-1996) developed an updated code of ethics for technology journalists that was adopted by many major trade show groups, most notably Bruno Blenheim. The Computer Press Association disbanded in 2000.\n\n"}
{"id": "25388707", "url": "https://en.wikipedia.org/wiki?curid=25388707", "title": "Convia", "text": "Convia\n\nConvia, Inc., based in Buffalo Grove, Illinois, is an American manufacturer of components which provide an integrated energy management platform that allows for the control and metering of lighting, plug-loads and HVAC. It is notable as one of the first companies to deliver and control power while at the same time monitoring energy and adapting its use in real-time.\n\nIn the late 1990s, Herman Miller, Inc., Convia’s parent company, realized that they could not create truly flexible environments until the infrastructure of the building became more flexible. They decided that if a building infrastructure embraced technology then the applications, including systems furniture, could also take advantage of that infrastructure and become more intelligent. The need for intelligent infrastructure led Herman Miller to partner with a leading technology think tank called Applied Minds in Glendale, California and their founder Danny Hillis. Danny Hillis is considered a pioneer of the parallel computing industry and is the lead designer of Convia. Convia was launched in 2004.\n\nIn 2009, Herman Miller, Inc., and Legrand North America, an innovative manufacturer of electrical and network infrastructure solutions, announced a strategic alliance designed to broaden the reach of energy management strategies to fuel the adoption of flexible, sustainable spaces, ultimately reducing real estate and building operating costs while improving worker productivity. Under the terms of the agreement, technology from Herman Miller’s Convia, Inc. subsidiary is embedded into Wiremold wire and cable management systems from Legrand. These include modular power and lighting distribution systems, floor boxes, poke-thru devices and architectural columns, which provide flexible, accessible power distribution to building owners and managers. Convia technology integrates a facility's power delivery and other infrastructure and technology applications, including lighting, HVAC, and occupancy and daylight harvesting sensors into an energy efficient, easy-to manage platform. Under the new alliance, “Convia-enabled” Wiremold systems enhanced this capability by adding control and monitoring of office plug loads (the amount of energy drawn by devices from an electrical outlet) and lighting loads.\n\nAlso in 2009, recognizing that workers spend approximately 50 percent of their time away from their desks, Convia developed the Energy Manager for Herman Miller to bring energy management strategies to the workstation. The system connects building power with the modular power in a cluster of Herman Miller workstations—including Vivo interiors, My Studio Environments, Ethospace system, Prospects, and Action Office system.\n\n\n"}
{"id": "30171206", "url": "https://en.wikipedia.org/wiki?curid=30171206", "title": "Digital Cinema Open System Alliance", "text": "Digital Cinema Open System Alliance\n\nThe Digital Cinema Open System Alliance (DCOSA) is an industry group committed to promoting interoperability between digital cinema equipment from different vendors.\n\nDCOSA\n\nDolby Laboratories, MikroM, USL Inc., and XDC Collaborate on Interface Specifications to Lower Development Costs, to Increase Flexibility, and to Support Innovation for the Digital Cinema Industry\nDigital Cinema Open System Alliance Announces Launch of Interoperability Effort\n\nVideo Technology Blog article: Digital Cinema Open System Alliance\n"}
{"id": "9258009", "url": "https://en.wikipedia.org/wiki?curid=9258009", "title": "Digital media player", "text": "Digital media player\n\nA digital media player (DMP) is a home entertainment consumer electronics device that can connect to a home network to stream digital media such as music, photos or digital video. Digital media players can stream files from a personal computer, network-attached storage or another networked media server, to play the media on a television or video projector display for home cinema. Most digital media players utilize a 10-foot user interface, and many are navigated via a remote control. Some digital media players also have smart TV features, such as allowing users to stream media such as digital versions of movies and TV shows from the Internet or streaming services.\n\nDigital media players were first introduced in 2000. In the 2010s, the main difference between most digital media players and modern set-top boxes was the obligation to have a TV tuner. Set-top boxes generally contain at least one TV tuner and are as such capable of receiving broadcasting signals from cable television, satellite television, over-the-air television or IPTV.\n\nIn the 2010s, with the popularity of portable media players and digital cameras, as well as fast Internet download speeds and relatively cheap mass storage, many people came into possession of large collections of digital media files that cannot be played on a conventional analog HiFi without connecting a computer to an amplifier or television. The means to play these files on a network-connected digital media player that is permanently connected to a television is seen as a convenience. The rapid growth in the availability of online content has made it easier for consumers to use these devices and obtain content. YouTube, for instance, is a common plug-in available on most networked devices. Netflix has also struck deals with many consumer-electronics makers to make their interface available in the device's menus, for their streaming subscribers. This symbiotic relationship between Netflix and consumer electronics makers has helped propel Netflix to become the largest subscription video service in the U.S., using up to 20% of U.S. bandwidth at peak times.\n\nMedia players are often designed for compactness and affordability, and tend to have small or non-existent hardware displays other than simple LED lights to indicate whether the device is powered on. Interface navigation on the television is usually done with an infrared remote control, while more-advanced digital media players come with high-performance remote controls which allow control of the interface using integrated touch sensors. Some remotes also include accelerometers for air mouse features which allow basic motion gaming. Most digital media player devices are unable to play physical audio or video media directly, and instead require a user to convert these media into playable digital files using a separate computer and software. They are also usually incapable of recording audio or video. In the 2010s, it is also common to find digital media player functionality integrated into other consumer-electronics appliances, such as DVD players, set-top boxes, smart TVs, or even video game consoles.\n\nDigital media players are also commonly referred to as a \"digital media extender\", \"digital media streamer\", \"digital media hub\", \"digital media adapter\", or \"digital media receiver\" (which should not be confused with \"AV Receiver\" that are also called \"Digital Media Renderer\").\n\nDigital media player manufacturers use a variety of names to describe their devices. Some more commonly used alternative names include:\n\nBy November 2000, an audio-only digital media player was demonstrated by a company called SimpleDevices, which was awarded two patents covering this invention in 2006. Developed under the SimpleFi name by Motorola in late 2001, the design was based on a Cirrus Arm-7 processor and the wireless HomeRF networking standard which pre-dated 802.11b in the residential markets. Other early market entrants in 2001 included the Turtle Beach AudioTron, Rio Receiver and SliMP3 digital media players. An early version of a video-capable digital media player was presented by F.C. Jeng et al. in the International Conf. on Consumer Electronics in 2002. It included a network interface card, a media processor for audio and video decoding, an analog video encoder (for video playback to a TV), an audio digital to analog converter for audio playback, and an IR (infrared receiver) for remote-control-interface.\n\nA concept of a digital media player was also introduced by Intel in 2002 at the Intel Developer Forum as part of their “Extended Wireless PC Initiative.\" Intel’s digital media player was based on an Xscale PXA210 processor and supported 802.11b wireless networking. Intel was among the first to use the Linux embedded operating system and UPnP technology for its digital media player. Networked audio and DVD players were among the first consumer devices to integrate digital media player functionality. Examples include the Philips Streamium-range of products that allowed for remote streaming of audio, the GoVideo D2730 Networked DVD player which integrated DVD playback with the capability to stream Rhapsody audio from a PC, and the Buffalo LinkTheater which combined a DVD player with a digital media player. More recently, the Xbox 360 gaming console from Microsoft was among the first gaming devices that integrated a digital media player. With the Xbox 360, Microsoft also introduced the concept of a Windows Media Center Extender, which allows users to access the Media center capabilities of a PC remotely, through a home network. More recently, Linksys, D-Link, and HP introduced the latest generation of digital media players that support 720p and 1080p high resolution video playback and may integrate both Windows Extender and traditional digital media player functionality.\n\nA digital media player can connect to the home network using either a wireless (IEEE 802.11a, b, g, and n) or wired Ethernet connection. Digital media players includes a user interface that allows users to navigate through their digital media library, search for, and play back media files. Some digital media players only handle music; some handle music and pictures; some handle music, pictures, and video; while others go further to allow internet browsing or controlling Live TV from a PC with a TV tuner.\n\nSome other capabilities which are accomplished by digital media players include:\n\nIn the 2010s, there are stand-alone digital media players on the market from AC Ryan, Asus, Apple (e.g., Apple TV), NetGear (e.g., NTV and NeoTV models), Dune, iOmega, Logitech, Pivos Group, Micca, Sybas (Popcorn Hour), Amkette EvoTV, D-Link, EZfetch, Android TV, Pinnacle, Xtreamer, and Roku, just to name a few. The models change frequently, so it is advisable to visit their web sites for current model names.\n\nThese devices come with low power consumption processors or SoC (System on Chip) and are most commonly either based on MIPS or ARM architecture processors combined with integrated DSP GPU in a SoC (or MPSoC) package. They also include RAM-memory and some type of built-in type of non-volatile computer memory (Flash memory).\n\n\"HD media player\" or \"HDD media player\" (\"HDMP\") is a generic term used for a category of consumer product that combines digital media player with a hard drive (HD) enclosure with all the hardware and software for playing audio, video and photos to a television. All these can play computer-based media files to a television without the need for a separate computer or network connection, and some can even be used as a conventional external hard-drive. These types of digital media players are sometimes sold as empty shells to allow the user to fit their own choice of hard drive (some can manage unlimited hard disk capacity and other only a certain capacity, i.e. 1TB, 2TB, 3TB, or 4TB), and the same model is sometimes sold with or without an internal hard drive already fitted.\n\nDigital media players can usually play H.264 (SD and HD), MPEG-4 Part 2 (SD and HD), MPEG-1, MPEG-2 .mpg, MPEG-2 .TS, VOB and ISO images video, with PCM, MP3 and AC3 audio tracks. They can also display images (such as JPEG and PNG) and play music files (such as FLAC, MP3 and Ogg).\n\nWhile most media players have traditionally been running proprietary or open source software frameworks versions based Linux as their operating systems, many newer network connected media players are based on the Android platform which gives them an advantage in terms of applications and games from the Google Play store. Even without Android some digital media players still have the ability to run applications (sometimes available via an 'app store' digital distribution platform), interactive on-demand media, personalized communications, and social networking features\n\nThere are two ways to connect an extender to its central media center or HTPC server - wired, or wireless. A wireless connection can be established between the media extender and its central media center. On the downside, interference may cause a \"less than optimal\" connection and cause network congestion, resulting in stuttering sound, missing frames from video, and other anomalies. It is recommended that an 802.11a or better be used, and over as short of a distance as possible.\nWhile early digital media players used proprietary communication protocols to interface with media servers, today most digital media players either use standard-based protocols such SMB/CIFS/SAMBA or NFS, or rely on some version of UPnP (Universal Plug and Play) and DLNA (Digital Living Network Alliance) standards. DLNA-compliant digital media players and Media Servers is meant to guarantee a minimum set of functionality and proper interoperability among digital media players and servers regardless of the manufacturer, but unfortunately not every manufacturer follows the standards perfectly which can lead to incompatibility.\n\nSome digital media players will only connect to specific media server software installed on a PC to stream music, pictures and recorded or live TV originating from the computer. Apple iTunes can, for example, be used this way with the Apple TV hardware that connects to a TV. Apple has developed a tightly integrated device and content management ecosystem with their iTunes Store, personal computers, iOS devices, and the AppleTV digital media receiver. The most recent version of the AppleTV, at $99, has lost the hard-drive that was included in its predecessor and fully depends on either streaming internet content, or another computer on the home network for media.\n\nTelevision connection is usually done via; composite, SCART, Component, HDMI video, with Optical Audio (TOSLINK/SPDIF), and connect to the local network and broadband internet using either a wired Ethernet or a wireless wifi connection, and some also have built-in Bluetooth support for remotes and game-pads or joysticks. Some players come with USB (USB 2.0 or USB 3.0) ports which allow local media content playback.\n\nThe convergence of content, technology, and broadband access allows consumers to stream television shows and movies to their high-definition television in competition with pay television providers. The research company SNL Kagan expects 12 million households, roughly 10%, to go without cable, satellite or telco video service by 2015 using Over The Top services. This represents a new trend in the broadcast television industry, as the list of options for watching movies and TV over the Internet grows at a rapid pace. Research also shows that even as traditional television service providers are trimming their customer base, they are adding Broadband Internet customers. Nearly 76.6 million U.S. households get broadband from leading cable and telephone companies, although only a portion have sufficient speeds to support quality video steaming. Convergence devices for home entertainment will likely play a much larger role in the future of broadcast television, effectively shifting traditional revenue streams while providing consumers with more options.\n\nAccording to a report from the researcher NPD In-Stat, only about 12 million U.S. households have their either Web-capable TVs or digital media players connected to the Internet, although In-Stat estimates about 25 million U.S. TV households own a set with the built-in network capability. Also, In-Stat predicts that 100 million homes in North America and western Europe will own digital media players and television sets that blend traditional programs with Internet content by 2016.\n\nSince at least 2015, dealers have marketed digital media players, often running the Android operating system and branded as being \"fully-loaded\", that are promoted as offering free streaming access to copyrighted media content, including films and television programs, as well as live feeds of television channels. These players are commonly bundled with the open source media player software Kodi, which is in turn pre-loaded with plug-ins enabling access to services streaming this content without the permission of their respective copyright holders. These \"fully-loaded\" set-top boxes are often sold through online marketplaces such as Amazon.com and eBay, as well as local retailers. The spread of these players has been attributed to their ease of use (with user experiences similar to legal subscription services such as Netflix) and low cost, as well as the content that is offered from the services pre-installed on the boxes.\n\n\"Fully-loaded\" set-top boxes have been subject to legal controversies, especially noting that their user experiences made them accessible to end-users who may not always realize that they are actually streaming pirated content. In the United Kingdom, the Federation Against Copyright Theft (FACT) has taken court actions on behalf of rightsholders against those who market digital media players pre-loaded with access to copyrighted content. In January 2017, an individual seller plead not guilty to charges of marketing and distributing devices that circumvent technological protection measures. In March 2017, the High Court of Justice ruled that BT Group, Sky plc, TalkTalk, and Virgin Media must block servers that had been used on such set-top boxes to illegally stream Premier League football games. Later in the month, Amazon UK banned the sale of \"certain media players\" that had been pre-loaded with software to illegally stream copyrighted content. On 26 April 2017, the European Court of Justice ruled that the distribution of set-top boxes with access to unauthorized streams of copyrighted works violated the exclusive rights to communicate them to the public. In September 2017, a British seller of such boxes pled guilty to violations of the Copyright, Designs and Patents Act for selling devices that can circumvent effective technical protection measures.\n\nIn Canada, it was initially believed that these set-top boxes fell within a legal grey area, as the transient nature of streaming content did not necessarily mean that the content was being downloaded in violation of Canadian copyright law. However, on 1 June 2016, a consortium of Canadian media companies (BCE Inc., Rogers Communications, and Videotron) obtained a temporary federal injunction against five retailers of Android-based set-top boxes, alleging that their continued sale were causing \"irreparable harm\" to their television businesses, and that the devices' primary purpose were to facilitate copyright infringement. The court rejected an argument by one of the defendants, who stated that they were only marketing a hardware device with publicly available software, ruling that the defendants were \"deliberately encourag[ing] consumers and potential clients to circumvent authorized ways of accessing content.\" 11 additional defendants were subsequently added to the suit. The lawyer of one of the defendants argued that retailers should not be responsible for the actions of their users, as any type of computing device could theoretically be used for legal or illegal purposes. In April 2017, the Federal Court of Appeal blocked an appeal requesting that the injunction be lifted pending the outcome of the case.\n\nAlthough the software is free to use, the developers of Kodi have not endorsed any add-on or Kodi-powered device intended for facilitating copyright infringement. Nathan Betzen, president of the XBMC Foundation (the non-profit organization which oversees the development of the Kodi software), argued that the reputation of Kodi had been harmed by third-party retailers who \"make a quick buck modifying Kodi, installing broken piracy add-ons, advertising that Kodi lets you watch free movies and TV, and then vanishing when the user buys the box and finds out that the add-on they were sold on was a crummy, constantly breaking mess.\" Betzen stated that the XBMC Foundation was willing to enforce its trademarks against those who use them to promote Kodi-based products which facilitate copyright infringement.\n\nFollowing a lawsuit by Dish Network against TVAddons, a website that offered streaming add-ons that were often used with Kodi and on such devices, in June 2017, the group shut down its add-ons and website. A technology analyst speculated that the service could eventually re-appear under a different name in the future, as have torrent trackers. In June, the service's operator was also sued by the Bell/Rogers/Videotron consortium for inducing copyright infringement.\n\nIn June 2017, Televisa was granted a court order banning the sale of all Roku products in Mexico, as it was alleged that third-parties had been operating subscription television services for the devices that contain unlicensed content. The content is streamed through unofficial apps that are added to the devices through hacking. Roku objected to the allegations, stating that these services were not certified by the company or part of its official Channels platform, whose terms of service require that they have rights to stream the content that they offer. Roku also stated that it actively cooperates with reports of channels that infringe copyrights.\n\nIn May 2018, the Federal Communications Commission sent letters to the CEOs of Amazon.com and eBay, asking for their help in removing such devices from their marketplaces. The letter cited malware risks, fraudulent use of FCC certification marks, and how their distribution through major online marketplaces may incorrectly suggest that they are legal and legitimate products.\n\n"}
{"id": "23562920", "url": "https://en.wikipedia.org/wiki?curid=23562920", "title": "DynaVox", "text": "DynaVox\n\nTobii Dynavox (formerly DynaVox Mayer-Johnson) is a U.S.-based developer, manufacturer and distributor of speech generating devices headquartered in Pittsburgh, Pennsylvania. The company was formed in 1983 and has since become the leading provider of speech communication devices and symbol-adapted special education software used to assist individuals in overcoming speech, language and learning challenges. DynaVox Mayer-Johnson's stated mission is to enable children and adults to reach their educational potential and experience a greater quality of life by maximizing each person’s ability to communicate and learn. The company's best-known products include the Maestro and EyeMax System communication solutions, and the Boardmaker Software Family, a suite of tools that allow educational curriculum and activities to be adapted to meet a range of student learning, cognitive and physical needs.\n\nFounded as Sentient Systems Technology, Inc. in 1983, the company’s first product was the EyeTyper. Created as a student project at Carnegie Mellon University to help a young woman with cerebral palsy to communicate, the EyeTyper allowed individuals spell messages with their eyes. These messages were then “spoken” by a computerized voice.\n\nThe EyeTyper made it possible for some people with cerebral palsy to communicate effectively. Killiany formed the company with CMU professor Mark Friedman and business partner Tilden Bennett. The patent for this technology was sold to the US Navy and all revenue went back into further development of communication applications that could be used by more people with significant speech disabilities. In 1991 the first DynaVox branded product was released. The DynaVox was the first speech-generating device to feature touchscreen technology.\n\nThe company changed its name to DynaVox Systems Inc. when was acquired by Sunrise Medical Inc. in 1998., before being spun out again several years later. In 2004 DynaVox acquired Enkidu Research Inc. and Mayer-Johnson. In 2009 DynaVox Mayer-Johnson acquired BlinkTwice, and incorporated that company's product, the Tango AAC device, into the DynaVox product line. DynaVox acquired Eye Response Technologies in January 2010.\n\nThe company floated on the stock market in 2010, although as of September 2011, its stock had fallen by 29.8% year in the last year In 2012, after continued falling stock prices, Dynavox was warned by Nasdaq that if shares continued to be valued below $1 the company would be dropped from the exchange. \"The company intends to consider available options to resolve the noncompliance with the minimum bid price requirement,\" DynaVox said in a recent SEC filing. \"No determination regarding the Company's response has been made at this time.\" In a separate and unrelated matter, DynaVox said in a separate SEC filing that JoAnn A. Reed would not seek re-election to the company's Board of Directors when her term expires on Dec. 5, 2012. On April 10, 2013, Dynavox announced that it received notification on April 5, 2013 that The NASDAQ Stock Market LLC (\"NASDAQ\") had determined to delist the Company's Class A common stock from the NASDAQ Global Select Market, effective with the open of business on April 16, 2013. As previously disclosed, on October 2, 2012, the Company received a notice from NASDAQ indicating it was not in compliance with NASDAQ's $1.00 minimum bid price requirement. The delisting is the result of the Company's failure to regain compliance with this requirement. The Company did not appeal the NASDAQ staff's determination.\nThe Company has been advised by OTC Markets Group Inc. that its Class A common stock will be immediately eligible for trading on the OTCQB marketplace effective with the open of business on April 16, 2013. The Company's Class A common stock will continue to trade under the symbol DVOX.\n\nAs of April 2014, the company found itself grappling with restructuring its assets and debts estimated at between $10 million and $50 million. DynaVox Intermediate LLC, became the focus of a threatened collateral sale by JEC Capital Partners LLC and Fondren Management LP.\n\nJEC and Fondren bought DynaVox's secured debt March 2014, the culmination of a process that saw the troubled Pittsburgh company searching for a deal or refinancing to bail it out of financial trouble.\n\nApril 8, 2014 DynaVox Systems, LLC (\"DynaVox\"), announced on their website clarifications regarding the Chapter 11 filings made by certain related entities and then on April 09th began giving away a free app to SLP's - “DynaVox Compass for Professionals gives SLPs free access to the DynaVox Compass app to use on their own iPad or Windows tablet, along with a comprehensive set of training and support tools, to help them better evaluate and serve their clients,”\n\nOn May 22, 2014, it was announced that competitor Tobii acquired DynaVox, and integrated them into Tobii's integrated Assisted Technology division. The company was renamed Tobii Dynavox LLC. The US division remains in Pittsburgh.\n\nDynaVox devices assist individuals who are unable to communicate reliably with their own voices due to cognitive, language and physical impairments. According to the 2005 US disability census, cognitive illnesses alone affect over 16 million people over the age of 15, while 35 million American’s were described as having a severe disability.\n\nThe devices produced by DynaVox closely resemble touch screen tablet computers. DynaVox uses a Microsoft Windows-based configuration as a platform to run proprietary InterAACt communiciation software.\n\nDynaVox devices, like almost all Speech generating device, use a hierarchical set of pages that contain different vocabulary for different contexts or situations. DynaVox's particular system of organising these pages is called 'InterAACT'.\n\nDynavox produce a range of speech generating devices, including the very small 'Xpress', and the 'Tango', which was designed particularly for children. Its newest device is called the Maestro, which, like the older V+ and VMax+ and M, provides options for multiple communication channels including cell phone, text messaging and e-mail. The larger Vmax is typically selected for those using wheelchairs (the device can be mounted) or for individuals with visual impairments. All Dynavox devices can be accessed in a variety of ways to compensate for limited physical abilities. The M is physically similar to the V+ and provides a limited number of prerecorded messages, and is often used by individuals with limited or no literacy skills.\n\nDynaVox also produced the EyeMax, which is an accessory which allows someone with limited or no fine motor skills to use their eyes to control their device. Augie Nieto, founder of Life Fitness, member of DynaVox's board of directors, and the former \"King of Fitness\", was diagnosed with ALS, and has slowly lost movement in most of his body and his ability to speak. Nieto has been able to continue communicating with his family, friends, and the world using the EyeMax technology. The EyeMax Systems' camera tracks the person’s eye movements on the screen. The user makes a selection by blinking or pausing on the desired vocabulary. Nieto says, “I can type up to 20 words per minute with the DynaVox” using only his eyes.\n"}
{"id": "21687161", "url": "https://en.wikipedia.org/wiki?curid=21687161", "title": "Dynamic trimming", "text": "Dynamic trimming\n\nA Dynamic trimming system operates seagoing vessels to achieve minimum water resistance under all circumstances. It is based on multidimensional analysis of real-time data collected on vessel attitude (trim).\n\nDynamic trimming automates data retrieval from sensor networks, for vessel management software applications. The core of the method is a multidimensional analysis model, which continuously calculates the key forces affecting the vessel attitude.\n\nThe system helps officers ensure that their vessels are operated efficiently. The key metrics are graphically displayed to facilitate decision-making.\nThe system aims to continuously optimize vessel trim, thus minimizing water resistance and reducing fuel consumption.\n"}
{"id": "9449198", "url": "https://en.wikipedia.org/wiki?curid=9449198", "title": "EDay (online retail)", "text": "EDay (online retail)\n\neDay was a term previously used for the peak sales day for the online retail sector in the United States by web analytics firm Coremetrics.\n\n"}
{"id": "15235042", "url": "https://en.wikipedia.org/wiki?curid=15235042", "title": "FED-STD-209E", "text": "FED-STD-209E\n\nFED-STD-209 E \"Airborne Particulate Cleanliness Classes in Cleanrooms and Cleanzones\" was a federal standard concerning classification of air cleanliness, intended for use in environments like clean rooms. The standard based its classifications on the measurement of airborne particles.\n\nThe standard was canceled on November 29, 2001 by the United States General Services Administration (GSA). The document was superseded by standards written for the International Organization for Standardization (ISO).\n\n"}
{"id": "10826", "url": "https://en.wikipedia.org/wiki?curid=10826", "title": "Fax", "text": "Fax\n\nFax (short for facsimile), sometimes called telecopying or telefax (the latter short for telefacsimile), is the telephonic transmission of scanned printed material (both text and images), normally to a telephone number connected to a printer or other output device. The original document is scanned with a fax machine (or a telecopier), which processes the contents (text or images) as a single fixed graphic image, converting it into a bitmap, and then transmitting it through the telephone system in the form of audio-frequency tones. The receiving fax machine interprets the tones and reconstructs the image, printing a paper copy. Early systems used direct conversions of image darkness to audio tone in a continuous or analog manner. Since the 1980s, most machines modulate the transmitted audio frequencies using a digital representation of the page which is compressed to quickly transmit areas which are all-white or all-black.\n\nScottish inventor Alexander Bain worked on chemical mechanical fax type devices and in 1846 was able to reproduce graphic signs in laboratory experiments. He received British patent 9745 on May 27, 1843 for his \"Electric Printing Telegraph\".\n\nFrederick Bakewell made several improvements on Bain's design and demonstrated a telefax machine. The Pantelegraph was invented by the Italian physicist Giovanni Caselli. He introduced the first commercial telefax service between Paris and Lyon in 1865, some 11 years before the invention of the telephone.\n\nIn 1880, English inventor Shelford Bidwell constructed the \"scanning phototelegraph\" that was the first telefax machine to scan any two-dimensional original, not requiring manual plotting or drawing. Around 1900, German physicist Arthur Korn invented the \"\", widespread in continental Europe especially, since a widely noticed transmission of a wanted-person photograph from Paris to London in 1908, used until the wider distribution of the radiofax. Its main competitors were the \"Bélinographe\" by Édouard Belin first, then since the 1930s the \"Hellschreiber\", invented in 1929 by German inventor Rudolf Hell, a pioneer in mechanical image scanning and transmission.\n\nThe 1888 invention of the telautograph by Elisha Gray marked a further development in fax technology, allowing users to send signatures over long distances, thus allowing the verification of identification or ownership over long distances.\n\nOn May 19, 1924, scientists of the AT&T Corporation \"by a new process of transmitting pictures by electricity\" sent 15 photographs by telephone from Cleveland to New York City, such photos being suitable for newspaper reproduction. Previously, photographs had been sent over the radio using this process.\n\nThe Western Union \"Deskfax\" fax machine, announced in 1948, was a compact machine that fit comfortably on a desktop, using special spark printer paper.\n\nAs a designer for the Radio Corporation of America (RCA), in 1924, Richard H. Ranger invented the wireless photoradiogram, or transoceanic radio facsimile, the forerunner of today’s \"fax\" machines. A photograph of President Calvin Coolidge sent from New York to London on November 29, 1924, became the first photo picture reproduced by transoceanic radio facsimile. Commercial use of Ranger’s product began two years later. Also in 1924, Herbert E. Ives of AT&T transmitted and reconstructed the first color facsimile, a natural-color photograph of silent film star Rudolph Valentino in period costume, using red, green and blue color separations.\n\nBeginning in the late 1930s, the Finch Facsimile system was used to transmit a \"radio newspaper\" to private homes via commercial AM radio stations and ordinary radio receivers equipped with Finch's printer, which used thermal paper. Sensing a new and potentially golden opportunity, competitors soon entered the field, but the printer and special paper were expensive luxuries, AM radio transmission was very slow and vulnerable to static, and the newspaper was too small. After more than ten years of repeated attempts by Finch and others to establish such a service as a viable business, the public, apparently quite content with its cheaper and much more substantial home-delivered daily newspapers, and with conventional spoken radio bulletins to provide any \"hot\" news, still showed only a passing curiosity about the new medium.\n\nBy the late 1940s, radiofax receivers were sufficiently miniaturized to be fitted beneath the dashboard of Western Union's \"Telecar\" telegram delivery vehicles.\n\nIn the 1960s, the United States Army transmitted the first photograph via satellite facsimile to Puerto Rico from the Deal Test Site using the Courier satellite.\n\nRadio fax is still in limited use today for transmitting weather charts and information to ships at sea. Also, it is also widely used within the medical field to transmit confidential patient information. \n\nIn 1964, Xerox Corporation introduced (and patented) what many consider to be the first commercialized version of the modern fax machine, under the name (LDX) or Long Distance Xerography. This model was superseded two years later with a unit that would truly set the standard for fax machines for years to come. Up until this point facsimile machines were very expensive and hard to operate. In 1966, Xerox released the Magnafax Telecopiers, a smaller, 46-pound facsimile machine. This unit was far easier to operate and could be connected to any standard telephone line. This machine was capable of transmitting a letter-sized document in about six minutes. The first sub-minute, digital fax machine was developed by Dacom, which built on digital data compression technology originally developed at Lockheed for satellite communication.\n\nBy the late 1970s, many companies around the world (especially Japanese firms) had entered the fax market. Very shortly after this, a new wave of more compact, faster and efficient fax machines would hit the market. Xerox continued to refine the fax machine for years after their ground-breaking first machine. In later years it would be combined with copier equipment to create the hybrid machines we have today that copy, scan and fax. Some of the lesser known capabilities of the Xerox fax technologies included their Ethernet enabled Fax Services on their 8000 workstations in the early 1980s.\n\nPrior to the introduction of the ubiquitous fax machine, one of the first being the Exxon Qwip in the mid-1970s, facsimile machines worked by optical scanning of a document or drawing spinning on a drum. The reflected light, varying in intensity according to the light and dark areas of the document, was focused on a photocell so that the current in a circuit varied with the amount of light. This current was used to control a tone generator (a modulator), the current determining the frequency of the tone produced. This audio tone was then transmitted using an acoustic coupler (a speaker, in this case) attached to the microphone of a common telephone handset. At the receiving end, a handset’s speaker was attached to an acoustic coupler (a microphone), and a demodulator converted the varying tone into a variable current that controlled the mechanical movement of a pen or pencil to reproduce the image on a blank sheet of paper on an identical drum rotating at the same rate.\n\nIn 1985, Hank Magnuski, founder of GammaLink, produced the first computer fax board, called GammaFax. Such boards could provide voice telephony via Analog Expansion Bus.\n\nAlthough businesses usually maintain some kind of fax capability, the technology has faced increasing competition from Internet-based alternatives. In some countries, because electronic signatures on contracts are not yet recognized by law, while faxed contracts with copies of signatures are, fax machines enjoy continuing support in business. In Japan, faxes are still used extensively for cultural and graphemic reasons and are available for sending to both domestic and international recipients from over 81% of all convenience stores nationwide. Convenience-store fax machines commonly print the slightly re-sized content of the sent fax in the electronic confirmation-slip, in A4 paper size.\n\nIn many corporate environments, freestanding fax machines have been replaced by fax servers and other computerized systems capable of receiving and storing incoming faxes electronically, and then routing them to users on paper or via an email (which may be secured). Such systems have the advantage of reducing costs by eliminating unnecessary printouts and reducing the number of inbound analog phone lines needed by an office.\n\nThe once ubiquitous fax machine has also begun to disappear from the small office and home office environments. Remotely hosted fax-server services are widely available from VoIP and e-mail providers allowing users to send and receive faxes using their existing e-mail accounts without the need for any hardware or dedicated fax lines. Personal computers have also long been able to handle incoming and outgoing faxes using analog modems or ISDN, eliminating the need for a stand-alone fax machine. These solutions are often ideally suited for users who only very occasionally need to use fax services. In July 2017 the United Kingdom's National Health Service was said to be the world's largest purchaser of fax machines because the digital revolution has largely bypassed it. In June 2018 the Labour Party said that the NHS had at least 11,620 fax machines in operation.\n\nThere are several indicators of fax capabilities: group, class, data transmission rate, and conformance with ITU-T (formerly CCITT) recommendations. Since the 1968 Carterphone decision, most fax machines have been designed to connect to standard PSTN lines and telephone numbers.\n\nGroup 1 and 2 faxes are sent in the same manner as a frame of analog television, with each scanned line transmitted as a continuous analog signal. Horizontal resolution depended upon the quality of the scanner, transmission line, and the printer. Analog fax machines are obsolete and no longer manufactured. ITU-T Recommendations T.2 and T.3 were withdrawn as obsolete in July 1996.\n\n\nA major breakthrough in the development of the modern facsimile system was the result of digital technology, where the analog signal from scanners was digitized and then compressed, resulting in the ability to transmit high rates of data across standard phone lines. The first digital fax machine was the Dacom Rapidfax first sold in late 1960s, which incorporated digital data compression technology developed by Lockheed for transmission of images from satellites.\n\nGroup 3 and 4 faxes are digital formats, and take advantage of digital compression methods to greatly reduce transmission times.\n\n\nFax Over IP (FoIP) can transmit and receive pre-digitized documents at near realtime speeds using ITU-T recommendation T.38 to send digitised images over an IP network using JPEG compression. T.38 is designed to work with VoIP services and often supported by analog telephone adapters used by legacy fax machines that need to connect through a VoIP service. Scanned documents are limited to the amount of time the user takes to load the document in a scanner and for the device to process a digital file. The resolution can vary from as little as 150 DPI to 9600 DPI or more. This type of faxing is not related to the e-mail to fax service that still uses fax modems at least one way.\n\nComputer modems are often designated by a particular fax class, which indicates how much processing is offloaded from the computer's CPU to the fax modem.\n\n\nSeveral different telephone line modulation techniques are used by fax machines. They are negotiated during the fax-modem handshake, and the fax devices will use the highest data rate that both fax devices support, usually a minimum of 14.4 kbit/s for Group 3 fax.\n\nNote that \"Super Group 3\" faxes use V.34bis modulation that allows a data rate of up to 33.6 kbit/s.\n\nAs well as specifying the resolution (and allowable physical size of the image being faxed), the ITU-T T.4 recommendation specifies two compression methods for decreasing the amount of data that needs to be transmitted between the fax machines to transfer the image. The two methods defined in T.4 are:\nAn additional method is specified in T.6:\nLater, other compression techniques were added as options to ITU-T recommendation T.30, such as the more efficient JBIG (T.82, T.85) for bi-level content, and JPEG (T.81), T.43, MRC (T.44), and T.45 for grayscale, palette, and colour content. Fax machines can negotiate at the start of the T.30 session to use the best technique implemented on both sides.\n\nModified Huffman (MH), specified in T.4 as the one-dimensional coding scheme, is a codebook-based run-length encoding scheme optimised to efficiently compress whitespace. As most faxes consist mostly of white space, this minimises the transmission time of most faxes. Each line scanned is compressed independently of its predecessor and successor.\n\nModified READ (MR), specified as an optional two-dimensional coding scheme in T.4, encodes the first scanned line using MH. The next line is compared to the first, the differences determined, and then the differences are encoded and transmitted. This is effective as most lines differ little from their predecessor. This is not continued to the end of the fax transmission, but only for a limited number of lines until the process is reset and a new 'first line' encoded with MH is produced. This limited number of lines is to prevent errors propagating throughout the whole fax, as the standard does not provide for error-correction. MR is an optional facility, and some fax machines do not use MR in order to minimise the amount of computation required by the machine. The limited number of lines is two for 'Standard' resolution faxes, and four for 'Fine' resolution faxes.\n\nThe ITU-T T.6 recommendation adds a further compression type of Modified Modified READ (MMR), which simply allows for a greater number of lines to be coded by MR than in T.4. This is because T.6 makes the assumption that the transmission is over a circuit with a low number of line errors such as digital ISDN. In this case, there is no maximum number of lines for which the differences are encoded.\n\nIn 1999, ITU-T recommendation T.30 added JBIG (ITU-T T.82) as another lossless bi-level compression algorithm, or more precisely a \"fax profile\" subset of JBIG (ITU-T T.85). JBIG-compressed pages result in 20% to 50% faster transmission than MMR-compressed pages, and up to 30-times faster transmission if the page includes halftone images.\n\nJBIG performs adaptive compression, that is both the encoder and decoder collect statistical information about the transmitted image from the pixels transmitted so far, in order to predict the probability for each next pixel being either black or white. For each new pixel, JBIG looks at ten nearby, previously transmitted pixels. It counts, how often in the past the next pixel has been black or white in the same neighborhood, and estimates from that the probability distribution of the next pixel. This is fed into an arithmetic coder, which adds only a small fraction of a bit to the output sequence if the more probable pixel is then encountered.\n\nThe ITU-T T.85 \"fax profile\" constrains some optional features of the full JBIG standard, such that codecs do not have to keep data about more than the last three pixel rows of an image in memory at any time. This allows the streaming of \"endless\" images, where the height of the image may not be known until the last row is transmitted.\n\nITU-T T.30 allows fax machines to negotiate one of two options of the T.85 \"fax profile\":\n\nA proprietary compression scheme employed on Panasonic fax machines is Matsushita Whiteline Skip (MWS). It can be overlaid on the other compression schemes, but is operative only when two Panasonic machines are communicating with one another. This system detects the blank scanned areas between lines of text, and then compresses several blank scan lines into the data space of a single character. (JBIG implements a similar technique called \"typical prediction\", if header flag TPBON is set to 1.)\n\nGroup 3 fax machines transfer one or a few printed or handwritten pages per minute in black-and-white (bitonal) at a resolution of 204×98 (normal) or 204×196 (fine) dots per square inch. The transfer rate is 14.4 kbit/s or higher for modems and some fax machines, but fax machines support speeds beginning with 2400 bit/s and typically operate at 9600 bit/s. The transferred image formats are called ITU-T (formerly CCITT) fax group 3 or 4. Group 3 faxes have the suffix codice_1 and the MIME type image/g3fax.\n\nThe most basic fax mode transfers in black and white only. The original page is scanned in a resolution of 1728 pixels/line and 1145 lines/page (for A4). The resulting raw data is compressed using a modified Huffman code optimized for written text, achieving average compression factors of around 20. Typically a page needs 10 s for transmission, instead of about 3 minutes for the same uncompressed raw data of 1728×1145 bits at a speed of 9600 bit/s. The compression method uses a Huffman codebook for run lengths of black and white runs in a single scanned line, and it can also use the fact that two adjacent scanlines are usually quite similar, saving bandwidth by encoding only the differences.\n\nFax classes denote the way fax programs interact with fax hardware. Available classes include Class 1, Class 2, Class 2.0 and 2.1, and Intel CAS. Many modems support at least class 1 and often either Class 2 or Class 2.0. Which is preferable to use depends on factors such as hardware, software, modem firmware, and expected use.\n\nFax machines from the 1970s to the 1990s often used direct thermal printers with rolls of thermal paper as their printing technology, but since the mid-1990s there has been a transition towards plain-paper faxes: thermal transfer printers, inkjet printers and laser printers.\n\nOne of the advantages of inkjet printing is that inkjets can affordably print in color; therefore, many of the inkjet-based fax machines claim to have color fax capability. There is a standard called ITU-T30e (formally ITU-T Recommendation T.30 Annex E ) for faxing in color; however, it is not widely supported, so many of the color fax machines can only fax in color to machines from the same manufacturer.\n\nStroke speed in facsimile systems is the rate at which a fixed line perpendicular to the direction of scanning is crossed in one direction by a scanning or recording spot. Stroke speed is usually expressed as a number of strokes per minute. When the fax system scans in both directions, the stroke speed is twice this number. In most conventional 20th century mechanical systems, the stroke speed is equivalent to drum speed.\n\nAs a precaution, thermal fax paper is typically not accepted in archives or as documentary evidence in some courts of law unless photocopied. This is because the image-forming coating is eradicable and brittle, and it tends to detach from the medium after a long time in storage.\n\nOne popular alternative is to subscribe to an Internet fax service, allowing users to send and receive faxes from their personal computers using an existing email account. No software, fax server or fax machine is needed. Faxes are received as attached TIFF or PDF files, or in proprietary formats that require the use of the service provider's software. Faxes can be sent or retrieved from anywhere at any time that a user can get Internet access. Some services offer secure faxing to comply with stringent HIPAA and Gramm–Leach–Bliley Act requirements to keep medical information and financial information private and secure. Utilizing a fax service provider does not require paper, a dedicated fax line, or consumable resources.\n\nAnother alternative to a physical fax machine is to make use of computer software which allows people to send and receive faxes using their own computers, utilizing fax servers and unified messaging. A virtual (email) fax can be printed out and then signed and scanned back to computer before being emailed. Also the sender can attach a digital signature to the document file.\n\nWith the surging popularity of mobile phones, virtual fax machines can now be downloaded as applications for Android and iOS. These applications make use of the phone's internal camera to scan fax documents for upload or they can import from various cloud services.\n\n\n\n"}
{"id": "50642426", "url": "https://en.wikipedia.org/wiki?curid=50642426", "title": "Fintech awards", "text": "Fintech awards\n\nFinTech Awards are several award ceremonies, most of them unrelated, which are held all over the world to recognize excellence in financial technology as assessed by public vote and panels of judges. As a rule, there are many categories and prizes. Hundreds of companies, mostly startups, participate as candidates.\n\nAs of 2016, major fintech awards ceremonies are held in India, Australia, Canada, Italy, Luxembourg, the Netherlands, South Africa, Switzerland, United Kingdom, and United States. Two ceremonies are not national but continental: the African FinTech Awards and Conference, and the European Fintech Awards.\n\nThe first \"African FinTech Awards and Conference\" will be held within the Finance Indaba Africa conference and expo in Johannesburg in October 2016. It will gather Africa's leading FinTech entrepreneurs, bankers, investors and advisors.\n\nThe 1st \"Annual Australian FinTech Awards\" will be held on 23 June 2016. The awards will be given in thirteen categories. The project, product, research paper, campaign, or activity that is entered must have been created, developed or released in the 18 months before the awards to be a valid entry.\n\nFounded by Glen Frost, the founder of FinTech Summit, and sponsored by Ashurst, the awards celebrate the achievements of the people and businesses comprising the fintech sector. \n\nThe Canadian FinTech Awards were created in 2014 by the Digital Finance Institute, a prominent global think tank for FinTech created in 2013, that has helped build the Canadian FinTech ecosystem. The first Canadian FinTech Awards were held on October 19 2015, in Toronto as part of the Digital Finance Institute's Annual National Conference in Toronto.\n\nThe \"European FinTech Awards\" is an organization that aims to disrupt traditional financial intuitions by providing a platform for fintech entrepreneurs to collaborate with one another. Like the Dutch and African awards, the European awards are the result of an initiative of Alex van Groningen BV, a Dutch financial publisher.\n\nThe awards start with the \"European FinTech 100\", a selection of innovative companies with ground-breaking ideas and technologies. The wide batch is chosen by the vote of more than 55,000 fintech enthusiasts from all over Europe (as of 2016). Then a panel of fintech experts reduces the selection to one hundred companies. The one hundred, in turn, are the pool from which the finalists are chosen. The finalists have the opportunity to pitch during the conference.\n\nThe first European FinTech Awards & Conference was held in Amsterdam on 14 April 2016. 413 companies from 34 European countries were nominated. 30% of all nominees were from United Kingdom.\n\nBehaviosec was the winner of the award for the Best European FinTech Company 2016. Winners in the 9 sub-categories were: Monese (Challenger Banks), Funding Circle (Alternative Finance), Ebury (Payments), Wikifolio (PFM), Everledger (Blockchain), Kreditech (Financial Inclusion), Knip (InsurTech), Backbase (Innovative Banking Software), and Behaviosec (Risk, Intelligence & Security).\n\nFinTech Awards is an event organized by the India FinTech Forum along with several startup accelerators. The winners are decided by a panel of industry stalwarts. The event focuses on actual working product demonstrations to encourage innovation and entrepreneurship.\n\nThe \"Italian Fintech Awards\" originated from the Grand Prix for Italian startups, held in 2014. The Grand Prix and the Fintech Awards, both organized by CheBanca!, have been geared towards the best Italian fintech startups. The 2016 edition had a call for submission till 31 March. 12 finalists were chosen among the candidates.\n\nThe winner is awarded 25,000 euros, personalized workshops with startup coaches, and a visit to a startup bootcamp in London. In 2016, the co-winners of the ceremony in Milan were Ovalmoney and eXrade.\n\nThe first Fintech Lion Awards in Luxembourg, where held on 21 June 2016. Among the candidates, 15 start-ups from Luxembourg were chosen to pitch their projects at the semi-finals in May. Ten of them were chosen for the finals, which will be held under the patronage of Prime Minister Xavier Bettel.\n\nThe first \"Dutch Fintech Awards\" took place at the headquarters of host ABN AMRO in Amsterdam in 2015. A 40-member jury gave awards in seven categories. The awards were attended by some 400 national and international fintech investors, companies, experts and executives.\n\nIn 2015, Adyen was the overall winner, receiving the award for a lasting breakthrough in the current payment industry. The winners in particular categories were: AcceptEmail (Payments), Five degrees (Banking IT), Bux (Personal Finance), Bitonic (Bitcoin), Sparkholder (Intelligence & Analytics), Symbid (SME).\n\nThe call for submissions for the first \"Swiss Fintech Award\" was published in October 2015. Any company founded by a Swiss citizen or headquartered in Switzerland could participate. A jury of 16 experts chose the top ten enterprises that would be trained in boot camps. The second round was a speed dating event with the jury and award sponsors in early February 2016. The three finalists pitched their ideas at the financial and economic forum \"Fintech 2016\" on 31 March 2016. The winner won a cash prize and a stay at the Accenture Fintech Innovation Lab in London.\n\nThe \"UK Fintech Awards\" ceremony was held in London on 13 April 2016. Sponsored by Bobsguide, the awards recognized achievements across the global financial technology industry.\n\nIn the United States, there are two major award ceremonies.\n\nThe \"Benzinga Fintech Awards\" (\"BZ Awards\") take place in New York. Their purpose is to \"showcase the companies with the most impressive technology, who are paving the future in financial services and capital markets.\" There are 18 awards. In 2016, more than 25,000 people voted for their favorites among 223 companies from around the globe.\n\nThe \"Efma Fintech Awards\" highlight the best in class fintech solutions in several categories, with a panel of experts voting on the best solutions. In 2016, the winners were announced at Efma’s Distribution Summit on 14 April 2016.\n"}
{"id": "1074761", "url": "https://en.wikipedia.org/wiki?curid=1074761", "title": "George Kistiakowsky", "text": "George Kistiakowsky\n\nGeorge Bogdanovich Kistiakowsky (November 18, 1900 – December 7, 1982) (, ) was a Ukrainian-American physical chemistry professor at Harvard who participated in the Manhattan Project and later served as President Dwight D. Eisenhower's Science Advisor.\n\nBorn in Kiev in the old Russian Empire, into \"an old Ukrainian Cossack family which was part of the intellectual elite in pre-revolutionary Russia\", Kistiakowsky fled his homeland during the Russian Civil War. He made his way to Germany, where he earned his PhD in physical chemistry under the supervision of Max Bodenstein at the University of Berlin. He emigrated to the United States in 1926, where he joined the faculty of Harvard University in 1930, and became a citizen in 1933.\n\nDuring World War II, Kistiakowsky was the head of the National Defense Research Committee (NDRC) section responsible for the development of explosives, and the technical director of the Explosives Research Laboratory (ERL), where he oversaw the development of new explosives, including RDX and HMX. He was involved in research into the hydrodynamic theory of explosions, and the development of shaped charges. In October 1943, he was brought into the Manhattan Project as a consultant. He was soon placed in charge of X Division, which was responsible for the development of the explosive lenses necessary for an implosion-type nuclear weapon. In July 1945, he watched as the first one was detonated in the Trinity test. A few weeks later another Fat Man implosion-type weapon was dropped on Nagasaki.\n\nFrom 1962 to 1965, Kistiakowsky chaired the National Academy of Sciences's Committee on Science, Engineering, and Public Policy (COSEPUP), and was its vice president from 1965 to 1973. He severed his connections with the government in protest against the war in Vietnam, and became active in an antiwar organization, the Council for a Livable World, becoming its chairman in 1977.\n\nGeorge Bogdanovich Kistiakowsky was born in Kiev, in the Kiev Governorate of the Russian Empire (now part of Ukraine), on November 18, 1900. George's grandfather Oleksandr Fedorovych Kistiakovsky was a professor of law and an attorney of the Russian Empire who specialized in criminal law. His father Bogdan Kistiakovsky was Professor of Legal Philosophy at the University of Kiev, and was elected a member of the National Academy of Sciences of Ukraine in 1919. Kistiakowsky's mother was Maria Berendshtam, and he had a brother, Alexander. George's uncle Ihor Kistiakovsky was the Minister of Internal Affairs of the Ukrainian State.\n\nKistiakowsky attended private schools in Kiev and Moscow until the Russian Revolution broke out in 1917. He then joined the anti-Communist White Army. In 1920 he escaped from Russia in a commandeered French ship. After spending time in Turkey and Yugoslavia, he made his way to Germany, where he enrolled at the University of Berlin later that year. In 1925, he earned his PhD in physical chemistry under the supervision of Max Bodenstein, writing his thesis on the photochemical decomposition of chlorine monoxide and ozone. He then became Bodenstein's graduate assistant. His first two published papers were elaborations of his thesis, co-written with Bodenstein.\n\nIn 1926, Kistiakowsky traveled to the United States as an International Education Board fellow. Hugh Stott Taylor, another student of Bodenstein, accepted Bodenstein's assessment of Kistiakowsky, and gave him a place at Princeton University. That year, Kistiakowsky married a Swedish woman, Hildegard Moebius. In 1928, they had a daughter, Vera, who later became a Professor Emerita of Physics at Massachusetts Institute of Technology. When Kistiakowsky's two-year fellowship ran out in 1927, he received a Research Associate and DuPont Fellowship. On October 25, 1928, he became an associate professor at Princeton. Taylor and Kistiakowsky published a series of papers together. Encouraged by Taylor, Kistiakowsky also published an American Chemical Society monograph on photochemical processes.\n\nIn 1930, Kistiakowsky joined the faculty of Harvard University, an affiliation that continued throughout his career. At Harvard, his research interests were in thermodynamics, spectroscopy, and chemical kinetics. He became increasingly involved in consulting for the government and industry. He became an associate professor again, this time at Harvard in 1933. That year he also became an American citizen. In 1938, he became the Abbott and James Lawrence Professor of Chemistry.\n\nForeseeing an expanded role for science in World War II, which the United States had not yet joined, President Franklin D. Roosevelt created the National Defense Research Committee (NDRC) on June 27, 1940, with Vannevar Bush as its chairman. James B. Conant, the President of Harvard, was appointed head of Division B, which was responsible for bombs, fuels, gases and chemicals. He appointed Kistiakowsky to head its Section A-1, which was concerned with explosives. In June 1941, the NDRC was absorbed into the Office of Scientific Research and Development (OSRD). Bush became Chairman of the OSRD, Conant succeeded him as Chairman of the NDRC, and Kistiakowsky became head of Section B. In a reorganization in December 1942, Division B was broken up, and he became head of Division 8, which was responsible for explosives and propellants, remaining in this position until February 1944.\n\nKistiakowsky was unhappy with the state of American knowledge of explosives and propellants. Conant established the Explosives Research Laboratory (ERL) near the laboratories of the Bureau of Mines in Bruceton, Pennsylvania in October 1940, and Kistiakowsky initially supervised its activities, making occasional visits; but Conant did not formally appoint him as its Technical Director until the spring of 1941. Although initially hampered by a shortage of facilities, the ERL grew from five staff in 1941 to a wartime peak of 162 full-time laboratory staff in 1945. An important field of research was RDX. This powerful explosive had been developed by the British before the war. The challenge was to develop an industrial process that could produce it on a large scale. RDX was also mixed with TNT to produce Composition B, which was widely used in various munitions, and torpex, which was used in torpedoes and depth charges. Pilot plants were in operation by May 1942, and large-scale production followed in 1943.\n\nIn response to a special request for an explosive that could be smuggled through Japanese checkpoints by Chinese guerrillas, Kistakowsky mixed HMX, a non-toxic explosive produced as a by-product of the RDX process, with flour to create \"Aunt Jemima\", after a brand of pancake flour. This was an edible explosive, which could pass for regular flour, and even be used in cooking.\n\nIn addition to research into synthetic explosives like RDX and HMX, the ERL investigated the properties of detonations and shock waves. This was initiated as a pure research project, without obvious or immediate applications. Kistiakowsky visited England in 1941 and again in 1942, where he met with British experts, including William Penney and Geoffrey Taylor. When Kistiakowsky and Edgar Bright Wilson, Jr., surveyed the existing state of knowledge, they found several areas that warranted further investigation. Kistiakowsky began to look into the Chapman–Jouguet model, which describes the way the shock wave created by a detonation propagates.\n\nAt this time, the efficacy of the Chapman–Jouguet model was still in doubt, and it was the subject of studies by John von Neumann at the Princeton Institute for Advanced Study. Kistiakowsky realized that the deviations from hydrodynamic theory were the result of the speed of the chemical reactions themselves. To control the reaction, calculations down to the microsecond level were needed. Section 8 was drawn into the investigation of shaped charges, whose mechanism was explained by Taylor and James Tuck in 1943.\n\nAt the Manhattan Project's Los Alamos Laboratory, research into implosion had been proceeding under Seth Neddermeyer, but his division had worked with cylinders and small charges, and had only produced objects that looked like rocks. Their research was accorded a low priority, owing to expectations that a gun-type nuclear weapon design would work for both uranium-235 and plutonium, and implosion technology would not be required.\n\nIn September 1943, the Los Alamos Laboratory's director, Robert Oppenheimer, arranged for von Neumann to visit Los Alamos and investigate implosion with a fresh set of eyes. After reviewing Neddermeyer's studies, and discussing the matter with Edward Teller, von Neumann suggested the use of high explosive in shaped charges to implode a sphere, which he showed could not only result in a faster assembly of fissile material than was possible with the gun method, but which could greatly reduce the amount of material required. The prospect of more efficient nuclear weapons impressed Oppenheimer, Teller and Hans Bethe, but they decided that an expert on explosives was required. Kistiakowsky's name was immediately suggested, and he was brought into the project as a consultant in October 1943.\nKistiakowsky was initially reluctant to come, \"partly because\", he later explained, \"I didn't think the bomb would be ready in time and I was interested in helping to win the war\". At Los Alamos, he began reorganizing the implosion effort. He introduced techniques such as photography and X-Rays to study the behavior of shaped charges. The former had been extensively employed by the ERL, while the latter had been described in papers by Tuck, who also suggested using three-dimensional explosive lenses. As with other aspects of the Manhattan Project, research into explosive lenses followed multiple lines of inquiry simultaneously because, as Kistiakowsky noted, it was \"impossible to predict which of these basic techniques will be the more successful.\"\n\nKistiakowsky brought with him to Los Alamos a detailed knowledge of all the studies into shaped charges, of explosives like Composition B, and of the procedures used at the ERL in 1942 and 1943. Increasingly, the ERL itself would be drawn into the implosion effort; its deputy director Duncan MacDougall also took charge of the Manhattan Project's Project Q. Kistiakowsky replaced Neddermeyer as head of E (for explosives) Division in February 1944.\n\nThe implosion program acquired a new urgency after Emilio Segrè's group at Los Alamos verified that plutonium produced in the nuclear reactors contained plutonium-240, which made it unsuitable for use in a gun-type weapon. A series of crisis meetings in July 1944 concluded that the only prospect for a working plutonium weapon was implosion. In August, Oppenheimer reorganized the entire laboratory to concentrate on it. A new explosives group, X Division, was created under Kistiakowsky to develop the lenses.\n\nUnder Kistiakowsky's leadership, X-Division designed the complex explosive lenses needed to compress the fissile plutonium pit. These employed two explosives with significantly different velocities of detonation in order to produce the required wave form. Kistiakowsky chose Baratol for the slow explosive. After experimenting with various fast explosives, X-Division settled on Composition B. Work on molding the explosives into the right shape continued into 1945. The lenses needed to be flawless, and techniques for casting Composition B and Baratol had to be developed. The ERL managed to accomplish this by devising a procedure for preparing Baratol in a form that was easy to cast. In March 1945, Kistiakowsky became part of the Cowpuncher Committee, so-called because it rode herd on the implosion effort. On July 16, 1945, Kistiakowsky watched as the first device was detonated in the Trinity test. A few weeks later a Fat Man implosion-type nuclear weapon was dropped on Nagasaki.\n\nAlong with his work on implosion, Kistiakowsky contributed to skiing in Los Alamos by using rings of explosives to fell trees for a ski slope — leading to the establishment of Sawyer's Hill Ski Tow Association. He divorced Hildegard in 1942 and married Irma E. Shuler in 1945. They were divorced in 1962, and he married Elaine Mahoney.\n\nIn 1957, during the Eisenhower Administration, Kistiakowsky was appointed to the President's Science Advisory Committee, and succeeded James R. Killian as chairman in 1959. He directed the Office of Science and Technology Policy from 1959 to 1961, when he was succeeded by Jerome B. Wiesner.\nIn 1958, Kistiakowsky suggested to President Eisenhower that inspection of foreign military facilities was not sufficient to control their nuclear weapons. He cited the difficulty in monitoring missile submarines, and proposed that the arms control strategy focus on disarmament rather than inspections. In January 1960, as part of arms control planning and negotiation, he suggested the \"threshold concept\". Under this proposal, all nuclear tests above the level of seismic detection technology would be forbidden. After such an agreement, the US and USSR would work jointly to improve detection technology, revising the permissible test yield downward as techniques improved. This example of the \"national means of technical verification\", a euphemism for sensitive intelligence collection used in arms control, would provide safeguards, without raising the on-site inspection requirement to a level unacceptable to the Soviets. The US introduced the threshold concept to the Soviets at the Geneva arms control conference in January 1960 and the Soviets, in March, responded favorably, suggesting a threshold of a given seismic magnitude. Talks broke down as a result of the U-2 Crisis of 1960 in May.\n\nAt the same time as the early nuclear arms control work, the Chairman of the Joint Chiefs of Staff, General Nathan F. Twining, sent a memorandum, in August 1959, to the Secretary of Defense, Neil McElroy, which suggested that the Strategic Air Command (SAC) formally be assigned responsibility to prepare the national nuclear target list, and a single plan for nuclear operations. Up to that point, the Army, Navy, and Air Force had done their own target planning. This had led to the same objectives being targeted multiple times by the different services. The separate service plans were not mutually supporting as in, for example, the Navy destroying an air defense facility on the route of an Air Force bomber going to a deeper target. While Twining had sent the memo to McElroy, the members of the Joint Chiefs of Staff disagreed on the policy during early 1960. Thomas Gates, who succeeded McElroy, asked President Dwight D. Eisenhower to decide the policy.\n\nEisenhower said he would not \"leave his successor with the monstrosity\" of the uncoordinated and un-integrated forces that then existed. In early November 1960, he sent Kistiakowsky to SAC Headquarters in Omaha to evaluate its war plans. Initially, Kistiakowsky was not given access, and Eisenhower sent him back, with a much stronger set of orders for SAC officers to cooperate. Kistiakowsky's report, presented on November 29, described uncoordinated plans with huge numbers of targets, many of which would be attacked by multiple forces, resulting in overkill. Eisenhower was shocked by the plans, and focused not just on the creation of the Single Integrated Operational Plan (SIOP), but on the entire process of picking targets, generating requirements, and planning for nuclear war operations.\n\nBetween his work for the Manhattan Project and his White House service, and again after he left the White House, Kistiakowsky was a professor of physical chemistry at Harvard. When asked to teach a freshmen class in 1957, he turned to Hubert Alyea, whose lecture style had impressed him. Alyea sent him some 700 index cards containing details of lecture demonstrations. Aside from the cards, Kistiakowsky never prepared the demonstrations. He later recalled: He retired from Harvard as professor emeritus in 1972.\n\nFrom 1962 to 1965, Kistiakowsky chaired the National Academy of Science's Committee on Science, Engineering, and Public Policy (COSEPUP), and was its vice president from 1965 to 1973. He received several awards over the years, including the Department of the Air Force Decoration for Exceptional Civilian Service in 1957. He was awarded the Medal for Merit by President Truman, the Medal of Freedom by President Eisenhower in 1961, and the National Medal of Science by President Lyndon Johnson in 1967. He was also a recipient the Charles Lathrop Parsons Award for public service from the American Chemical Society in 1961, Priestley Medal from the American Chemical Society in 1972, and the Franklin Medal from Harvard.\n\nIn later years, Kistiakowsky was active in an antiwar organization, the Council for a Livable World. He severed his connections with the government in protest against the US involvement in the Vietnam War. In 1977, he assumed the chairmanship of the Council, campaigning against nuclear proliferation. He died of cancer in Cambridge, Massachusetts, on December 17, 1982. His body was cremated, and his ashes scattered near his summer home on Cape Cod, Massachusetts. His papers are in the Harvard University archives.\n\n\n\n"}
{"id": "12822", "url": "https://en.wikipedia.org/wiki?curid=12822", "title": "Greek fire", "text": "Greek fire\n\nGreek fire was an incendiary weapon used by the Eastern Roman (Byzantine) Empire that was first developed . The Byzantines typically used it in naval battles to great effect, as it could continue burning while floating on water. It provided a technological advantage and was responsible for many key Byzantine military victories, most notably the salvation of Constantinople from two Arab sieges, thus securing the Empire's survival.\n\nThe impression made by Greek fire on the western European Crusaders was such that the name was applied to any sort of incendiary weapon, including those used by Arabs, the Chinese, and the Mongols. However, these were different mixtures and not the same formula as the Byzantine Greek fire, which was a closely guarded state secret. Byzantines also used pressurized nozzles or \"siphōns\" to project the liquid onto the enemy, in a manner resembling a modern flamethrower.\n\nAlthough usage of the term \"Greek fire\" has been general in English and most other languages since the Crusades, original Byzantine sources called the substance a variety of names, such as \"sea fire\" (Medieval Greek: πῦρ θαλάσσιον \"pŷr thalássion\"), \"Roman fire\" ( \"pŷr rhōmaïkón\"), \"war fire\" ( \"polemikòn pŷr\"), \"liquid fire\" ( \"hygròn pŷr\"), \"sticky fire\" ( \"pŷr kollētikón\") or \"manufactured fire\" ( \"pŷr skeuastón\").\n\nThe composition of Greek fire remains a matter of speculation and debate, with various proposals including combinations of pine resin, naphtha, quicklime, calcium phosphide, sulfur, or niter.\nIn Titus Livy's history of Rome, priestesses of Bacchus are said to have dipped fire into the water, which did not extinguish, \"for it was sulphur mixed with lime.\"\n\nIncendiary and flaming weapons were used in warfare for centuries prior to the invention of Greek fire. They included a number of sulfur-, petroleum-, and bitumen-based mixtures. Incendiary arrows and pots containing combustible substances surrounded by caltrops or spikes, or launched by catapults, were used as early as the 9th century BC by the Assyrians and were extensively used in the Greco-Roman world as well. Furthermore, Thucydides mentions that in the siege of Delium in 424 BC a long tube on wheels was used which blew flames forward using a large bellows. The Roman author Julius Africanus, writing in the 3rd Century AD, records a mixture that ignited from adequate heat and intense sunlight, used in grenades or night attacks:Automatic fire also by the following formula. This is the recipe: take equal amounts of sulphur, rock salt, ashes, thunder stone, and pyrite and pound fine in a black mortar at midday sun. Also in equal amounts of each ingredient mix together black mulberry resin and Zakynthian asphault, the latter in a liquid form and free-flowing, resulting in a product that is sooty colored. Then add to the asphalt the tiniest amount of quicklime. But because the sun is at its zenith, one must pound it carefully and protect the face, for it will ignite suddenly. When it catches fire, one should seal it in some sort of copper receptacle; in this way you will have it available in a box, without exposing it to the sun. If you should wish to ignite enemy armaments, you will smear it on in the evening, either on the armaments or some other object, but in secret; when the sun comes up, everything will be burnt up.In naval warfare, the Eastern Roman Emperor Anastasius I (r. 491–518) is recorded by chronicler John Malalas to have been advised by a philosopher from Athens called Proclus to use sulfur to burn the ships of Vitalianus. Greek fire proper, however, was developed in c. 672 and is ascribed by the chronicler Theophanes to Kallinikos (Latinized Callinicus), an architect from Heliopolis in the former province of Phoenice, by then overrun by the Muslim conquests:\n\nThe accuracy and exact chronology of this account is open to question: Theophanes reports the use of fire-carrying and \"siphōn\"-equipped ships by the Byzantines a couple of years before the supposed arrival of Kallinikos at Constantinople. If this is not due to chronological confusion of the events of the siege, it may suggest that Kallinikos merely introduced an improved version of an established weapon. The historian James Partington further thinks it likely that Greek fire was not in fact the creation of any single person but \"invented by chemists in Constantinople who had inherited the discoveries of the Alexandrian chemical school.\" Indeed, the 11th-century chronicler George Kedrenos records that Kallinikos came from Heliopolis in Egypt, but most scholars reject this as an error. Kedrenos also records the story, considered rather implausible by modern scholars, that Kallinikos' descendants, a family called \"Lampros\", \"brilliant,\" kept the secret of the fire's manufacture and continued doing so to Kedrenos' time.\n\nKallinikos' development of Greek fire came at a critical moment in the Byzantine Empire's history: weakened by its long wars with Sassanid Persia, the Byzantines had been unable to effectively resist the onslaught of the Muslim conquests. Within a generation, Syria, Palestine, and Egypt had fallen to the Arabs, who in set out to conquer the imperial capital of Constantinople. Greek fire was used to great effect against the Muslim fleets, helping to repel the Muslims at the first and second Arab sieges of the city. Records of its use in later naval battles against the Saracens are more sporadic, but it did secure a number of victories, especially in the phase of Byzantine expansion in the late 9th and early 10th centuries. Utilisation of the substance was prominent in Byzantine civil wars, chiefly the revolt of the thematic fleets in 727 and the large-scale rebellion led by Thomas the Slav in 821–823. In both cases, the rebel fleets were defeated by the Constantinopolitan Imperial Fleet through the use of Greek fire. The Byzantines also used the weapon to devastating effect against the various Rus' raids on the Bosporus, especially those of 941 and 1043, as well as during the Bulgarian war of 970–971, when the fire-carrying Byzantine ships blockaded the Danube.\n\nThe importance placed on Greek fire during the Empire's struggle against the Arabs would lead to its discovery being ascribed to divine intervention. The Emperor Constantine Porphyrogennetos (r. 945–959), in his book \"De Administrando Imperio\", admonishes his son and heir, Romanos II (r. 959–963), to never reveal the secrets of its composition, as it was \"shown and revealed by an angel to the great and holy first Christian emperor Constantine\" and that the angel bound him \"not to prepare this fire but for Christians, and only in the imperial city.\" As a warning, he adds that one official, who was bribed into handing some of it over to the Empire's enemies, was struck down by a \"flame from heaven\" as he was about to enter a church. As the latter incident demonstrates, the Byzantines could not avoid capture of their precious secret weapon: the Arabs captured at least one fireship intact in 827, and the Bulgars captured several \"siphōns\" and much of the substance itself in 812/814. This, however, was apparently not enough to allow their enemies to copy it (see below). The Arabs, for instance, employed a variety of incendiary substances similar to the Byzantine weapon, but they were never able to copy the Byzantine method of deployment by \"siphōn\", and used catapults and grenades instead.\n\nGreek fire continued to be mentioned during the 12th century, and Anna Komnene gives a vivid description of its use in a naval battle against the Pisans in 1099. However, although the use of hastily improvised fireships is mentioned during the 1203 siege of Constantinople by the Fourth Crusade, no report confirms the use of the actual Greek fire. This might be because of the general disarmament of the Empire in the 20 years leading up to the sacking, or because the Byzantines had lost access to the areas where the primary ingredients were to be found, or even perhaps because the secret had been lost over time.\n\nRecords of a 13th-century event in which \"Greek fire\" was used by the Saracens against the Crusaders can be read through the Memoirs of the Lord of Joinville during the Seventh Crusade. One description of the memoir says \"the tail of fire that trailed behind it was as big as a great spear; and it made such a noise as it came, that it sounded like the thunder of heaven. It looked like a dragon flying through the air. Such a bright light did it cast, that one could see all over the camp as though it were day, by reason of the great mass of fire, and the brilliance of the light that it shed.\"\n\nIn the 19th century, it is reported that an Armenian by the name of Kavafian approached the government of the Ottoman Empire with a new type of Greek fire he claimed to have developed. Kavafian refused to reveal its composition when asked by the government, insisting that he be placed in command of its use during naval engagements. Not long after this, he was poisoned by imperial authorities, without their ever having found out his secret.\n\nAs Constantine Porphyrogennetos' warnings show, the ingredients and the processes of manufacture and deployment of Greek fire were carefully guarded military secrets. So strict was the secrecy that the composition of Greek fire was lost forever and remains a source of speculation. Consequently, the \"mystery\" of the formula has long dominated the research into Greek fire. Despite this almost exclusive focus, however, Greek fire is best understood as a complete weapon system of many components, all of which were needed to operate together to render it effective. This comprised not only the formula of its composition, but also the specialized dromon ships that carried it into battle, the device used to prepare the substance by heating and pressurizing it, the \"siphōn\" projecting it, and the special training of the \"siphōnarioi\" who used it. Knowledge of the whole system was highly compartmentalised, with operators and technicians aware of the secrets of only one component, ensuring that no enemy could gain knowledge of it in its entirety. This accounts for the fact that when the Bulgarians took Mesembria and Debeltos in 814, they captured 36 \"siphōns\" and even quantities of the substance itself, but were unable to make any use of them.\n\nThe information available on Greek fire is exclusively indirect, based on references in the Byzantine military manuals and a number of secondary historical sources such as Anna Komnene and Western European chroniclers, which are often inaccurate. In her \"Alexiad\", Anna Komnene provides a description of an incendiary weapon, which was used by the Byzantine garrison of Dyrrhachium in 1108 against the Normans. It is often regarded as an at least partial \"recipe\" for Greek fire:\nThis fire is made by the following arts. From the pine and the certain such evergreen trees inflammable resin is collected. This is rubbed with sulfur and put into tubes of reed, and is blown by men using it with violent and continuous breath. Then in this manner it meets the fire on the tip and catches light and falls like a fiery whirlwind on the faces of the enemies. At the same time, the reports by Western chroniclers of the famed \"ignis graecus\" are largely unreliable, since they apply the name to any and all sorts of incendiary substances.\n\nIn attempting to reconstruct the Greek fire system, the concrete evidence, as it emerges from the contemporary literary references, provides the following characteristics:\n\nThe first and, for a long time, most popular theory regarding the composition of Greek fire held that its chief ingredient was saltpeter, making it an early form of gunpowder. This argument was based on the \"thunder and smoke\" description, as well as on the distance the flame could be projected from the \"siphōn\", which suggested an explosive discharge. From the times of Isaac Vossius, several scholars adhered to this position, most notably the so-called \"French school\" during the 19th century, which included chemist Marcellin Berthelot. This view has been rejected since, as saltpeter does not appear to have been used in warfare in Europe or the Middle East before the 13th century, and is absent from the accounts of the Muslim writers—the foremost chemists of the early medieval world—before the same period. In addition, the nature of the proposed mixture would have been radically different from the \"siphōn\"-projected substance described by Byzantine sources.\n\nA second view, based on the fact that Greek fire was inextinguishable by water (some sources suggest that water intensified the flames) suggested that its destructive power was the result of the explosive reaction between water and quicklime. Although quicklime was certainly known and used by the Byzantines and the Arabs in warfare, the theory is refuted by literary and empirical evidence. A quicklime-based substance would have to come in contact with water to ignite, while Emperor Leo's \"Tactica\" indicate that Greek fire was often poured directly on the decks of enemy ships, although admittedly, decks were kept wet due to lack of sealants. Likewise, Leo describes the use of grenades, which further reinforces the view that contact with water was not necessary for the substance's ignition. Furthermore, C. Zenghelis pointed out that, based on experiments, the actual result of the water–quicklime reaction would be negligible in the open sea. Another similar proposition suggested that Kallinikos had in fact discovered calcium phosphide, which can be made by boiling bones in urine within a sealed vessel. On contact with water, calcium phosphide releases phosphine, which ignites spontaneously. However, extensive experiments with it also failed to reproduce the described intensity of Greek fire.\n\nAlthough the presence of either quicklime or saltpeter in the mixture cannot be entirely excluded, they were consequently not the primary ingredient. Most modern scholars agree that Greek fire was based on petroleum, either crude or refined; comparable to modern napalm. The Byzantines had easy access to crude oil from the naturally occurring wells around the Black Sea (e.g., the wells around Tmutorakan noted by Constantine Porphyrogennetos) or in various locations throughout the Middle East. An alternate name for Greek fire was \"Median fire\" (), and the 6th-century historian Procopius records that crude oil, called \"naphtha\" (in Greek: \"naphtha\", from Old Persian \"naft\") by the Persians, was known to the Greeks as \"Median oil\" (). This seems to corroborate the use of naphtha as a basic ingredient of Greek fire. Naphtha was also used by the Abbasids in the 9th century, with special troops, the \"naffāṭūn\", who wore thick protective suits and used small copper vessels containing burning oil, which they threw onto the enemy troops. There is also a surviving 9th century Latin text, preserved at Wolfenbüttel in Germany, which mentions the ingredients of what appears to be Greek fire and the operation of the \"siphōn\"s used to project it. Although the text contains some inaccuracies, it clearly identifies the main component as naphtha. Resins were probably added as a thickener (the \"Praecepta Militaria\" refer to the substance as , \"sticky fire\"), and to increase the duration and intensity of the flame. A modern theoretical concoction included the use of pine tar and animal fat along with other ingredients.\n\nA 12th century treatise prepared by Mardi bin Ali al-Tarsusi for Saladin records an Arab version of Greek fire, called \"naft\", which also had a petroleum base, with sulfur and various resins added. Any direct relation with the Byzantine formula is unlikely. An Italian recipe from the 16th century has been recorded for recreational use; it includes coal from a willow tree, alcohol, incense, sulfur, wool and camphor as well as two undetermined components (burning salt and \"pegola\"); the concoction was guaranteed to \"burn under water\" and to be \"beautiful.\"\n\nThe chief method of deployment of Greek fire, which sets it apart from similar substances, was its projection through a tube (\"siphōn\"), for use aboard ships or in sieges. Portable projectors (\"cheirosiphōnes\", χειροσίφωνες) were also invented, reputedly by Emperor Leo VI. The Byzantine military manuals also mention that jars (\"chytrai\" or \"tzykalia\") filled with Greek fire and caltrops wrapped with tow and soaked in the substance were thrown by catapults, while pivoting cranes (\"gerania\") were employed to pour it upon enemy ships. The \"cheirosiphōnes\" especially were prescribed for use at land and in sieges, both against siege machines and against defenders on the walls, by several 10th-century military authors, and their use is depicted in the \"Poliorcetica\" of Hero of Byzantium. The Byzantine dromons usually had a \"siphōn\" installed on their prow under the forecastle, but additional devices could also on occasion be placed elsewhere on the ship. Thus in 941, when the Byzantines were facing the vastly more numerous Rus' fleet, \"siphōn\"s were placed also amidships and even astern.\n\nThe use of tubular projectors (σίφων, \"siphōn\") is amply attested in the contemporary sources. Anna Komnene gives this account of beast-shaped Greek fire projectors being mounted to the bow of warships:\nAs he [the Emperor Alexios I] knew that the Pisans were skilled in sea warfare and dreaded a battle with them, on the prow of each ship he had a head fixed of a lion or other land-animal, made in brass or iron with the mouth open and then gilded over, so that their mere aspect was terrifying. And the fire which was to be directed against the enemy through tubes he made to pass through the mouths of the beasts, so that it seemed as if the lions and the other similar monsters were vomiting the fire.\n\nSome sources provide more information on the composition and function of the whole mechanism. The Wolfenbüttel manuscript in particular provides the following description:\n...having built a furnace right at the front of the ship, they set on it a copper vessel full of these things, having put fire underneath. And one of them, having made a bronze tube similar to that which the rustics call a \"squitiatoria\", \"squirt,\" with which boys play, they spray [it] at the enemy.\n\nAnother, possibly first-hand, account of the use of Greek fire comes from the 11th-century \"Yngvars saga víðförla\", in which the Viking Ingvar the Far-Travelled faces ships equipped with Greek fire weapons:\n[They] began blowing with smiths’ bellows at a furnace in which there was fire and there came from it a great din. There stood there also a brass [or bronze] tube and from it flew much fire against one ship, and it burned up in a short time so that all of it became white ashes...\nThe account, albeit embellished, corresponds with many of the characteristics of Greek fire known from other sources, such as a loud roar that accompanied its discharge. These two texts are also the only two sources that explicitly mention that the substance was heated over a furnace before being discharged; although the validity of this information is open to question, modern reconstructions have relied upon them.\nBased on these descriptions and the Byzantine sources, John Haldon and Maurice Byrne designed a hypothetical apparatus as consisting of three main components: a bronze pump, which was used to pressurize the oil; a brazier, used to heat the oil (πρόπυρον, \"propyron\", \"pre-heater\"); and the nozzle, which was covered in bronze and mounted on a swivel (στρεπτόν, \"strepton\"). The brazier, burning a match of linen or flax that produced intense heat and the characteristic thick smoke, was used to heat oil and the other ingredients in an airtight tank above it, a process that also helped to dissolve the resins into a fluid mixture. The substance was pressurized by the heat and the usage of a force pump. After it had reached the proper pressure, a valve connecting the tank with the swivel was opened and the mixture was discharged from its end, being ignited at its mouth by some source of flame. The intense heat of the flame made necessary the presence of heat shields made of iron (βουκόλια, \"boukolia\"), which are attested in the fleet inventories.\n\nThe process of operating Haldon and Byrne's design was fraught with danger, as the mounting pressure could easily make the heated oil tank explode, a flaw which was not recorded as a problem with the historical fire weapon. In the experiments conducted by Haldon in 2002 for the episode \"Fireship\" of the television series \"Machines Times Forgot\", even modern welding techniques failed to secure adequate insulation of the bronze tank under pressure. This led to the relocation of the pressure pump between the tank and the nozzle. The full-scale device built on this basis established the effectiveness of the mechanism's design, even with the simple materials and techniques available to the Byzantines. The experiment used crude oil mixed with wood resins, and achieved a flame temperature of over and an effective range of up to .\n\nThe portable \"cheirosiphōn\" (\"hand-\"siphōn\"\"), the earliest analogue to a modern flamethrower, is extensively attested in the military documents of the 10th century, and recommended for use in both sea and land. They first appear in the \"Tactica\" of emperor Leo VI the Wise, who claims to have invented them. Subsequent authors continued to refer to the \"cheirosiphōnes\", especially for use against siege towers, although Nikephoros II Phokas also advises their use in field armies, with the aim of disrupting the enemy formation. Although both Leo VI and Nikephoros Phokas claim that the substance used in the \"cheirosiphōnes\" was the same as in the static devices used on ships, Haldon and Byrne consider that the former were manifestly different from their larger cousins, and theorize that the device was fundamentally different, \"a simple syringe [that] squirted both liquid fire (presumably unignited) and noxious juices to repel enemy troops.\" The illustrations of Hero's \"Poliorcetica\" show the \"cheirosiphōn\" also throwing the ignited substance, but such illustrations were \"narrative\" in intent, so that showing the device throwing the ignited substance may have seemed preferable to trying to clearly illustrate throwing the unignited substance and then igniting it.\n\nIn its earliest form, Greek fire was hurled onto enemy forces by firing a burning cloth-wrapped ball, perhaps containing a flask, using a form of light catapult, most probably a seaborne variant of the Roman light catapult or onager. These were capable of hurling light loads, around , a distance of .\n\nAlthough the destructiveness of Greek fire is indisputable, it did not make the Byzantine navy invincible. It was not, in the words of naval historian John Pryor, a \"ship-killer\" comparable to the naval ram, which, by then, had fallen out of use. While Greek fire remained a potent weapon, its limitations were significant when compared to more traditional forms of artillery: in its \"siphōn\"-deployed version, it had a limited range, and it could be used safely only in a calm sea and with favourable wind conditions.\n\nThe Muslim navies eventually adapted themselves to it by staying out of its effective range and devising methods of protection such as felt or hides soaked in vinegar.\n\nIn C. J. Sansom's historical mystery novel \"Dark Fire\", Thomas Cromwell sends the lawyer Matthew Shardlake to recover the secret of Greek fire, following its discovery in the library of a dissolved London monastery.\n\nIn Michael Crichton's sci-fi novel \"Timeline\", Professor Edward Johnston is stuck in the past in 14th century Europe, and claims to have knowledge of Greek fire.\n\nIn Mika Waltari's novel \"The Dark Angel\", some old men who are the last ones who know the secret of Greek fire are mentioned as present in the last Christian services held in Hagia Sophia before the Fall of Constantinople. The narrator is told that in the event of the city's fall, they will be killed so as to keep the secret from the Turks.\n\nIn George R. R. Martin's fantasy series of novels \"A Song of Ice and Fire\", and its adaptation \"Game of Thrones\", Wildfire is similar to Greek fire, it was used in naval battles as it could remain lit on water and it was an extremely secretive recipe and was protected immensely.\n\n\n"}
{"id": "3477804", "url": "https://en.wikipedia.org/wiki?curid=3477804", "title": "Hankook Hewlett-Packard", "text": "Hankook Hewlett-Packard\n\nHankook Hewlett-Packard (hangul:한국휴렛팩커드) is a Korean company that produces computers. Hankook is headquartered in Yeoeuido-dong Yeongdeungpo-gu, Seoul, South Korea, and was established in 1982 by Samsung Hewlett-Packard. (hangul:삼성휴렛팩커드) in its shareholder meet to Samsung Electronics and Hewlett-Packard in 1982. part indefendent at Samsung Electronics in 1992 of computer divisions. The company's current CEO is Choi Joon Keun. (최준근)\nHangul means Korean\n\n"}
{"id": "1950831", "url": "https://en.wikipedia.org/wiki?curid=1950831", "title": "Hobcart", "text": "Hobcart\n\nA hobcart was a type of mobility device designed in the late 1960s by Dr. Steven Perry of Albrighton, Shropshire, UK. In his practice he had two young children, both of whom had spina bifida. He considered that the wheelchairs the children were provided with were liable to set them apart from other children of their age, so set about designing a mobility device, that would look like a go-kart. The end result was the hobcart, which when it was first made, was produced at a local borstal. The idea behind this was to try to provide the inmates of the borstal the opportunity to be involved in a project which they could see was doing some good. Hobcarts were still being made into the 1980s.\n\n"}
{"id": "162183", "url": "https://en.wikipedia.org/wiki?curid=162183", "title": "IEEE-488", "text": "IEEE-488\n\nIEEE 488 is a short-range digital communications 8-bit parallel multi-master interface bus specification. IEEE 488 was created as HP-IB (Hewlett-Packard Interface Bus) and is commonly called GPIB (General Purpose Interface Bus). It has been the subject of several standards.\n\nAlthough originally created in the late 1960s to connect together automated test equipment, it also had some success during the 1970s and 1980s as a peripheral bus for early microcomputers, notably the Commodore PET. Newer standards have largely replaced IEEE 488 for computer use, but it still sees some use in the test equipment field.\n\nIn the late 1960s, Hewlett-Packard (HP) manufactured various automated test and measurement instruments, such as digital multimeters and logic analyzers. They developed the \"HP Interface Bus (HP-IB)\" to enable easier interconnection between instruments and controllers (computers and other instruments).\n\nThe bus was relatively easy to implement using the technology at the time, using a simple parallel bus and several individual control lines. For example, the HP 59501 Power Supply Programmer and HP 59306A Relay Actuator were both relatively simple HP-IB peripherals implemented only in TTL, using no microprocessor.\n\nHP licensed the HP-IB patents for a nominal fee to other manufacturers. It became known as the General Purpose Interface Bus (GPIB), and became a de facto standard for automated and industrial instrument control. As GPIB became popular, it was formalized by various standards organizations.\n\nIn 1975, the IEEE standardized the bus as \"Standard Digital Interface for Programmable Instrumentation\", IEEE 488; it was revised in 1978 (producing IEEE 488-1978). The standard was revised in 1987, and redesignated as IEEE 488.1 (IEEE 488.1-1987). These standards formalized the mechanical, electrical, and basic protocol parameters of GPIB, but said nothing about the format of commands or data.\n\nIn 1987, IEEE introduced \"Standard Codes, Formats, Protocols, and Common Commands\", IEEE 488.2. It was revised in 1992. IEEE 488.2 provided for basic syntax and format conventions, as well as device-independent commands, data structures, error protocols, and the like. IEEE 488.2 built on IEEE 488.1 without superseding it; equipment can conform to IEEE 488.1 without following IEEE 488.2.\n\nWhile IEEE 488.1 defined the hardware and IEEE 488.2 defined the protocol, there was still no standard for instrument-specific commands. Commands to control the same class of instrument, \"e.g.\", multimeters, would vary between manufacturers and even models.\n\nThe United States Air Force, and later Hewlett-Packard, recognized this problem. In 1989, HP developed their TML language which was the forerunner to Standard Commands for Programmable Instrumentation (SCPI). SCPI was introduced as an industry standard in 1990. SCPI added standard generic commands, and a series of instrument classes with corresponding class-specific commands. SCPI mandated the IEEE 488.2 syntax, but allowed other (non-IEEE 488.1) physical transports.\n\nThe IEC developed their own standards in parallel with the IEEE, with IEC 60625-1 and IEC 60625-2 (IEC 625), later replaced by IEC 60488.\n\nNational Instruments introduced a backward-compatible extension to IEEE 488.1, originally known as HS-488. It increased the maximum data rate to 8 Mbyte/s, although the rate decreases as more devices are connected to the bus. This was incorporated into the standard in 2003 (IEEE 488.1-2003), over HP's objections.\n\nIn 2004, the IEEE and IEC combined their respective standards into a \"Dual Logo\" IEEE/IEC standard IEC 60488-1, \"Standard for Higher Performance Protocol for the Standard Digital Interface for Programmable Instrumentation - Part 1: General\", replaces IEEE 488.1/IEC 60625-1, and IEC 60488-2,\"Part 2: Codes, Formats, Protocols and Common Commands\", replaces IEEE 488.2/IEC 60625-2.\n\nIEEE 488 is an 8-bit, electrically parallel bus. The bus employs sixteen signal lines — eight used for bi-directional data transfer, three for handshake, and five for bus management — plus eight ground return lines.\n\nEvery device on the bus has a unique 5-bit primary address, in the range from 0 to 30 (31 total possible addresses).\n\nThe standard allows up to 15 devices to share a single physical bus of up to 20 meters total cable length. The physical topology can be linear or star (forked). Active extenders allow longer buses, with up to 31 devices theoretically possible on a logical bus.\n\nControl and data transfer functions are logically separated; a controller can address one device as a \"talker\" and one or more devices as \"listeners\" without having to participate in the data transfer. It is possible for multiple controllers to share the same bus; but only one can be the \"Controller In Charge\" at a time.\n\nIn the original protocol, transfers use an interlocked, three-wire \"ready–valid–accepted\" handshake. The maximum data rate is about one megabyte per second. The later HS-488 extension relaxes the handshake requirements, allowing up to 8 Mbyte/s. The slowest participating device determines the speed of the bus.\n\nIEEE 488 specifies a 24-pin Amphenol-designed micro ribbon connector. Micro ribbon connectors have a D-shaped metal shell, but are larger than D-subminiature connectors. They are sometimes called \"Centronics connectors\" after the 36-pin micro ribbon connector Centronics used for their printers.\n\nOne unusual feature of IEEE 488 connectors is they commonly use a \"double-headed\" design, with male on one side, and female on the other. This allows stacking connectors for easy daisy-chaining. Mechanical considerations limit the number of stacked connectors to four or fewer, although a workaround involving physically supporting the connectors may be able to get around this.\n\nThey are held in place by screws, either UTS (now largely obsolete) or metric M3.5×0.6 threads. Early versions of the standard suggested that metric screws should be blackened to avoid confusion with the incompatible UTS threads. However, by the 1987 revision this was no longer considered necessary because of the prevalence of metric threads.\n\nThe IEC 60625 standard prescribes the use of 25-pin D-subminiature connectors (the same as used for the parallel port on IBM-PCs). This connector did not gain significant market acceptance against the established 24-pin connector.\n\nMore information see Tektronix.\n\nHP's designers did not specifically plan for IEEE 488 to be a peripheral interface for general-purpose computers; the focus was on instrumentation. But when HP's early microcomputers needed an interface for peripherals (disk drives, tape drives, printers, plotters, etc.), HP-IB was readily available and easily adapted to the purpose.\n\nHP computer products which used HP-IB included the HP series 80, HP 9800 series, the HP 2100 series, and the HP 3000 series. HP computer peripherals which did not utilize the RS-232 communication interface often used HP-IB including disc systems like the HP 7935. Some of HP's advanced pocket calculators of the 1980s, such as the HP-41 and HP-71B series, also had IEEE 488 capabilities, via an optional HP-IL/HP-IB interface module.\n\nOther manufacturers adopted GPIB for their computers as well, such as with the Tektronix 405x line.\n\nThe Commodore PET (introduced 1977) range of personal computers connected their peripherals using the IEEE 488 bus, but with a non-standard card edge connector. Commodore's following 8-bit machines utilized a serial bus whose protocol was based on IEEE 488. Commodore marketed an IEEE 488 cartridge for the VIC-20 and the Commodore 64 (Reverse-engineered schematics for Commodore C64 IEEE interface). Several third party suppliers of Commodore 64 peripherals made a cartridge for the C64 that provided an IEEE 488-derived interface on a card edge connector similar to that of the PET series.\n\nEventually, faster, more complete standards such as SCSI superseded IEEE 488 for peripheral access.\n\nElectrically, IEEE 488 used a hardware interface that could be implemented with some discrete logic or with a microcontroller. The hardware interface enabled devices made by different manufacturers to communicate with a single host. Since each device generated the asynchronous handshaking signals required by the bus protocol, slow and fast devices could be mixed on one bus. The data transfer is relatively slow, so transmission line issues such as impedance matching and line termination are ignored. There was no requirement for galvanic isolation between the bus and devices, which created the possibility of ground loops causing extra noise and loss of data.\n\nPhysically, the IEEE 488 connectors and cabling were rugged and held in place by screws. While physically large and sturdy connectors were an advantage in industrial or laboratory set ups, the size and cost of the connectors was a liability in applications such as personal computers.\n\nAlthough the electrical and physical interfaces were well defined, there was not an initial standard command set. Devices from different manufacturers might use different commands for the same function. Some aspects of the command protocol standards were not standardized until Standard Commands for Programmable Instruments (SCPI) in 1990. Implementation options (e.g. end of transmission handling) can complicate interoperability in pre-IEEE 488.2 devices.\n\nMore recent standards such as USB, FireWire, and Ethernet take advantage of declining costs of interface electronics to implement more complex standards providing higher bandwidth. The multi-conductor (parallel data) connectors and shielded cable were inherently more costly than the connectors and cabling that could be used with serial data transfer standards such as RS-232, RS-485, USB, FireWire or Ethernet. Very few mass-market personal computers or peripherals (such as printers or scanners) implemented IEEE 488.\n\n\n"}
{"id": "22732748", "url": "https://en.wikipedia.org/wiki?curid=22732748", "title": "IEEE Consumer Electronics Society", "text": "IEEE Consumer Electronics Society\n\nThe IEEE Consumer Electronics Society (CE Society or CESoc) is a professional society of the IEEE that focuses on the \"theory and practice of Electronic Engineering and of the allied arts and sciences with respect to the field of Consumer Electronics and the maintenance of a high professional standing among its members\". Wahab Almuhtadi is the current president of the IEEE CE society.\n\nIn 1983, the IEEE Broadcast, Cable and Consumer Electronics Society was broken apart into two societies, the IEEE Consumer Electronics Society and the IEEE Broadcast Technology Society.\n\nThe field of interest of the IEEE Consumer Electronics Society is engineering and research aspects of the theory, design, construction, manufacture or end use of mass market electronics, systems, software and services for consumers.\n\nThe IEEE CE Society produces a number of publications, including the following:\n\nIEEE Consumer Electronics Magazine is an award-winning periodical which has received multiple awards: \n\nThe IEEE Consumer Electronics Society sponsors a number of internationally recognized conferences that focus on the consumer electronics field of interest.\n\n\nThe IEEE Consumer Electronics Society is a member of two IEEE councils:\n\nThe IEEE Consumer Electronics Society currently has 20 chapters spread all over the world:\n\n\nThe IEEE Consumer Electronics Society awards the following IEEE distinctions:\n"}
{"id": "23266113", "url": "https://en.wikipedia.org/wiki?curid=23266113", "title": "IEEE Ernst Weber Engineering Leadership Recognition", "text": "IEEE Ernst Weber Engineering Leadership Recognition\n\nThe IEEE Ernst Weber Engineering Leadership Recognition, now called the IEEE Ernst Weber Managerial Leadership Award, was established by the Board of Directors of the Institute of Electrical and Electronics Engineers (IEEE) in 1985. This award is presented \"for exceptional managerial leadership in the fields of interest to the IEEE\". Recipients of this award will receive a certificate and a crystal sculpture.\n\nThe award was initially called the IEEE Engineering Leadership Recognition Award, but in 1996 renamed to honor Ernst Weber. He was the first President of the IEEE and one of the founders of the U.S. National Academy of Engineering.\n\nThe following people have received the IEEE Ernst Weber Engineering Leadership Recognition:\n"}
{"id": "19873235", "url": "https://en.wikipedia.org/wiki?curid=19873235", "title": "Institute of Electronic Music and Acoustics", "text": "Institute of Electronic Music and Acoustics\n\nThe Institute of Electronic Music and Acoustics (IEM) is a multidisciplinary research center within the University of Music and Performing Arts, Graz, (Austria).\n\nResearch activities are concentrated mainly in digital signal processing, audio engineering, and psycho acoustics, e.g. projects in analysis and syntheses of sound and loudness perception.\n\n\nThe IEM is very active in the field of Artistic Research.\nAs part of the 'Doctor Artium' program of the University of Music and Performing Arts Graz, several art-based doctorates have been supervised at the IEM. \nAlso, since 2011 IEM staff members have won seven PEEK grands for various artistic research projects, each with a run-time of three years. The PEEK program is a highly competitive research program for artistic research in Austria, and is funded by the Austrian Science Fund (FWF).\n\nAt IEM, compositions students are trained in musical acoustics, sound synthesis, algorithmic composition, and real-time systems. \nThe institute has established an audio engineering curriculum in collaboration with the Graz University of Technology. The most important aspects of these courses is to bring technology and artistic creativity closer together.\n\nThe IEM offers a 3 year bachelor and a 2 year master program in Computer Music.\nThe main focusses of this three-year program – the first of its kind in Austria – are:\n\nAs these areas strongly overlap in the computer music practice, the presentation of a broad range of expertise is at the forefront of studying. Furthermore, the students get a comprehensive understanding of the history, theory and aesthetics of computer music.\n\nSome of the offered courses are taking place in collaboration with the \"Institute of Composition, Music Theory and Conducting\" (Institute 1) of the \"Graz University of Music and Dramatic Arts\"\n\nBachelor/Master study in cooperation with the Graz University of Technology\n\nBachelor/Master study in cooperation with the University of Graz\n\nIEM provides technology and know-how to composers and musicians in the creation and realization of their works. \nSince 1990, IEM has collaborated with guest artists (including Olga Neuwirth, Peter Ablinger, Bernhard Lang) in the production and performance of more than 80 new compositions. In this process, many international partnerships have been established.\n\nSince 2014 the IEM is also offering an Artist-In-Residence program, which enables the selected artist to work at the facilities of the IEM for a duration of 5 months. The selected Artists-In-Residence are:\n\n\nThe IEM publishes the series \"Contributions to Electronic Music\".\n\nThe IEM is involved in several Open Source projects.\nIt is currently hosting the main community portal and the mailing-lists for Pure Data\n\n\n"}
{"id": "48139617", "url": "https://en.wikipedia.org/wiki?curid=48139617", "title": "Interstellar Probe (1999)", "text": "Interstellar Probe (1999)\n\nInterstellar Probe is the name of a 1999 space probe concept by NASA intended to travel out 200 AU in 15 years. This 1999 study by Jet Propulsion Laboratory (JPL) is noted for its circular 400 meters diameter, solar sail as a propulsion method (1 g/m) combined with a 0.25 AU flyby of the Sun to achieve higher solar light pressure, after which the sail is jettisoned at 5 AU distance from the Sun. \n\nSolar sails work by converting the energy in light into a momentum on the spacecraft, thus propelling the spacecraft. Felix Tisserand noted the effect of light pressure on comet tails in the 1800s.\n\nThe study by the NASA Jet Propulsion Laboratory, proposed using a solar sail to accelerate a spacecraft to reach the interstellar medium. It was planned to reach as far as 200 AU within 10 years at a speed of 14 AU/year (about 70 km/s, and function up to 400+ AU. A critical technology for the mission is a large 1 g/m solar sail. \nIn the following years there were additional studies, including the Innovative Interstellar Explorer (published 2003), which focused on a design using RTGs powering an ion engine rather than a solar sail. Another project in this field for advanced spaceflight during this period was the Breakthrough Propulsion Physics Program which ran from 1996 through 2002.\n\nLater examples of solar sail-propelled spacecraft include IKAROS, Nanosail-D2, and LightSail. Near-Earth Asteroid Scout is a planned light sail-propelled mission. For comparison, the LightSail spacecraft uses a sail 5 micron in thickness, whereas they predict a sail with 1 micron thickness would be needed for interstellar travel.\n\nThe probe would use an advanced radioisotope thermoelectric generator (RTG) for electrical power, Ka band radio for communication with Earth, a Delta 2 rocket for Earth launch, and a 25 kg instrument package using 20 watts.\n\n\n"}
{"id": "937847", "url": "https://en.wikipedia.org/wiki?curid=937847", "title": "Khepera mobile robot", "text": "Khepera mobile robot\n\nThe Khepera is a small (5.5 cm) differential wheeled mobile robot that was developed at the LAMI laboratory of Prof. Jean-Daniel Nicoud at EPFL (Lausanne, Switzerland) in the mid '90s. It was developed by Edo. Franzi, Francesco Mondada, André Guignard and others.\n\nSmall, fast, and architectured around a Motorola 68331, it has served researchers for 10 years, widely used by over 500 universities worldwide.\n\nThe Khepera was sold to a thousand research labs and featured on the cover of the 31 August 2000 issue of Nature. It appeared again in a 2003 article .\n\nA Google scholar search with \"khepera mobile robots\" returns 4800 hits . The Khepera helped in the emergence of evolutionary robotics .\n\n\nSeveral extension turrets exist for the Khepera, including:\n\n\n"}
{"id": "25407989", "url": "https://en.wikipedia.org/wiki?curid=25407989", "title": "L Prize", "text": "L Prize\n\nThe L Prize (aka the Bright Tomorrow Lighting Prize) is a competition run by the United States Department of Energy aimed to \"spur lighting manufacturers to develop high-quality, high-efficiency solid-state lighting products to replace the common incandescent light bulb\".\n\nThe competition, launched in May 2008 at Lightfair, offers two prizes for the replacement of two types of bulb, an A19 60-watt incandescent light bulb and a PAR 38 halogen incandescent bulb. The prize fund for the 60 W replacement is up to a maximum of US$10 million and for the PAR 38 up to US$5 million.\nThere is a third category, yet to be publicly defined, called the 21st-century lamp.\n\nThe competition has set out various qualifying requirements for the replacement bulbs summarized in the table below:\n\nIt was announced on 3 August 2011, that the winner of the 60 W replacement bulb competition was a bulb made by Philips. The winning bulb was a LED lamp using less than 10 watts and emitting the equivalent amount of light as a traditional 60-watt incandescent bulb. That amounts to an 83% energy savings. It was announced that Philips would be given the US$10 million cash prize. The bulb was released commercially in February 2012 through several online sellers. The widespread launch at retail stores, however, was not until Earth Day, April 22. Although the subsidized price was expected to be $22 in the first year, $15 in the second and $8 in the third, the bulb was initially selling for $50–$60 (without rebates) as of July 2012. As of March, 2013 Home Depot began offering the bulbs for $15 in stores. Many stores sold out, and according to Philips customer service the L-prize bulb has been discontinued.\n\nThe winning bulb is able to achieve all of the required specifications by using red and blue LEDs that excite yellow phosphors, which emits the required color of light. This is in contrast to most other LED lights, which use white LEDs to emit light that lacks all of the required characteristics for the L-Prize.\n\nThe rules for the PAR 38 lightbulb competition were retooled in July, 2012, keeping the same main specifications. As of June 13, 2014, the competition has been suspended.\n\nOn August 1, 2011, Cree announced that they had created a bulb that would exceed the DOE specifications for a 21st-century lamp. It emitted 1,300 lumens at 152 lumens per watt with a CRI of 91.2 and a color temperature of 2800 K. They also stated at the time that they would not be bringing the bulb to market. As of March, 2013 they had not brought the bulb or any bulb like it to market.\n\n"}
{"id": "496925", "url": "https://en.wikipedia.org/wiki?curid=496925", "title": "List of eating utensils", "text": "List of eating utensils\n\n\nIn some cultures, such as Ethiopian cuisine and Indian dining, breads or hands are used in place of utensils.\n\n\"Fun Dip\" is a type of candy in the United States, where a solid candy \"dipping stick\" is used to convey flavored sugar to the eater's mouth. The dipper is first licked to provide moisture, and then dredged through a small pouch with the flavored sugar, so that the sugar sticks to the dipping stick.\n\nSome single-serve ice cream is sold with a flat wooden spade, often erroneously called a \"spoon\", to lift the product to one's mouth.\n\nPrepackaged tuna salad or cracker snacks may contain a flat plastic spade for similar purposes.\n\n\nOver time, these utensils were combined in various ways in attempts to make eating more convenient or to reduce the total number of utensils required.\n\nSome utensils are useful only for specific foods.\n\n"}
{"id": "6026011", "url": "https://en.wikipedia.org/wiki?curid=6026011", "title": "London Decca", "text": "London Decca\n\nLondon Decca is a manufacturer of phonograph tonearms and pick-up cartridges. The London Decca cartridges may be unique in that they do not employ a \"proper\" cantilever, neither are they \"moving magnet\" (MM) nor \"moving coil\" (MC) designs. These products are critically acclaimed for their sonic detail.\n\n\n"}
{"id": "1047828", "url": "https://en.wikipedia.org/wiki?curid=1047828", "title": "Meal powder", "text": "Meal powder\n\nMeal powder is the fine dust left over when black powder (gunpowder) is corned and screened to separate it into different grain sizes. It is used extensively in various pyrotechnic procedures, usually to prime other compositions. It can also be used in many fireworks to add power and substantially increasing the height of the firework. It is also used in most firearm ammunition.\n\n'Mill meal' powder is a mixture of potassium nitrate, charcoal and sulfur in the correct proportions (75% potassium nitrate:15% charcoal:10% sulfur) which has been ball-milled to mix it intimately. It is used in the same way as commercial meal powder or can be pressed and corned to produce true black powder.\n\nTo make meal powder or black powder, a chemist mixes the ingredients by mass, never by volume. These ingredients are processed in a ball mill, basically a spinning drum with non-sparking ceramic or lead balls. The more time left in the mill, the more \"explosive\" the powder will be. One main reason to ball mill as opposed to other methods is because it presses sulfur and KNO into the porous charcoal.\n\n\n"}
{"id": "16637212", "url": "https://en.wikipedia.org/wiki?curid=16637212", "title": "Multilateral export control regime", "text": "Multilateral export control regime\n\nA Multilateral Export Control Regime is an international body that states use to organize their national export control systems. For a chart of national membership in different regimes, see the SIPRI Yearbook chapter on \"Transfer controls\"\n\nThere are currently four such regimes:\n\nFollowing 30 countries are members of all of the above four regimes:\n\n\nFollowing 13 countries are members of at least three of these regimes:\n\n\nFollowing two countries are members of at least two of these regimes:\n\n\nFollowing four countries are members of at least one of these regimes:\n\n"}
{"id": "52724403", "url": "https://en.wikipedia.org/wiki?curid=52724403", "title": "MySensors", "text": "MySensors\n\nMySensors is a free and open source DIY (do-it yourself) software framework for wireless IoT (Internet of Things) devices allowing devices to communicate using radio transmitters. The library was originally developed for the Arduino platform.\n\nThe MySensors devices create a virtual radio network of nodes that automatically forms a self healing mesh like structure. Each node can relay messages for other nodes to cover greater distances using simple short range transceivers. Each node can have several sensors or actuators attached and can interact with other nodes in the network.\n\nThe radio network can consist of up to 254 nodes where one node can act as a gateway to the internet or a home automation controller. The controller adds functionality to the radio network such as id assignment and time awareness.\n\nThe framework can natively be run on the following platforms and micro controllers. \n\nMySensors supports over-the-air communication using the following transceivers:\nWired communication over:\n\nThe wireless communication can be signed using truncated HMAC-SHA256 either through hardware with Atmel ATSHA204A or compatible software emulation and optionally encrypted. The implementation is timing neutral with whitened random numbers, attack detection-and-lockout and protects against timing attacks, replay attacks and man in the middle attacks.\n\nThe firmware of a MySensor node can be updated over the air using a few different bootloader options:\n\n\n"}
{"id": "4995589", "url": "https://en.wikipedia.org/wiki?curid=4995589", "title": "NFPA 921", "text": "NFPA 921\n\nNFPA 921, \"Guide for Fire and Explosion Investigations\", is a peer reviewed document that is published by the National Fire Protection Association (NFPA). Its purpose is \"to establish guidelines and recommendations for the safe and systematic investigation or analysis of fire and explosion incidents\" (section 1.2.1). Familiarity with NFPA 921 is strongly recommended by National Association of Fire Investigators and the International Association of Arson Investigators (IAAI). NFPA 921 forms a large basis of the information which a professional fire investigator must know to pass the various Fire Investigator Certification (NAFI and IAAI) examinations.\n\nWhile not every recommendation in NFPA 921 will apply to any particular fire or explosion investigation, the document itself recommends that if a particular fire investigator does not apply certain sections to an investigation where they are called-for, the investigator must be prepared to justify the exclusion.\n\nThis standard was long resisted in legal circles, with civil and criminal arson investigations leading to convictions later proven wrongful, such as that of Cameron Todd Willingham.\n\n"}
{"id": "6110795", "url": "https://en.wikipedia.org/wiki?curid=6110795", "title": "Passive cooling", "text": "Passive cooling\n\nPassive cooling is a building design approach that focuses on heat gain control and heat dissipation in a building in order to improve the indoor thermal comfort with low or no energy consumption. This approach works either by preventing heat from entering the interior (heat gain prevention) or by removing heat from the building (natural cooling). Natural cooling utilizes on-site energy, available from the natural environment, combined with the architectural design of building components (e.g. building envelope), rather than mechanical systems to dissipate heat. Therefore, natural cooling depends not only on the architectural design of the building but on how the site's natural resources are used as heat sinks (i.e. everything that absorbs or dissipates heat). Examples of on-site heat sinks are the upper atmosphere (night sky), the outdoor air (wind), and the earth/soil.\n\nPassive cooling covers all natural processes and techniques of heat dissipation and modulation without the use of energy. Some authors consider that minor and simple mechanical systems (e.g. pumps and economizers) can be integrated in passive cooling techniques, as long they are used to enhance the effectiveness of the natural cooling process. Such applications are also called ‘hybrid cooling systems’. The techniques for passive cooling can be grouped in two main categories:\n\n\nProtection from or prevention of heat gains encompasses all the design techniques that minimizes the impact of solar heat gains through the building’s envelope and of internal heat gains that is generated inside the building due occupancy and equipment. It includes the following design techniques:\n\n\nThe modulation and heat dissipation techniques rely on natural heat sinks to store and remove the internal heat gains. Examples of natural sinks are night sky, earth soil, and building mass. Therefore, passive cooling techniques that use heat sinks can act to either modulate heat gain with thermal mass or dissipate heat through natural cooling strategies.\n\nVentilation as a natural cooling strategy uses the physical properties of air to remove heat or provide cooling to occupants. In select cases, ventilation can be used to cool the building structure, which subsequently may serve as a heat sink.\n\n\n\nOne specific application of natural ventilation is night flushing.\n\nNight flushing (also known as night ventilation, night cooling, night purging, or nocturnal convective cooling) is a passive or semi-passive cooling strategy that requires increased air movement at night to cool the structural elements of a building. Unlike free cooling, which assists in chilling water, night flushing cools down the thermal mass. To execute night flushing, the building envelope typically stays closed during the day, causing excess heat gains to be stored in the building's thermal mass. The building structure acts as a sink through the day and absorbs heat gains from occupants, equipment, solar radiation, and conduction through walls, roofs, and ceilings. At night, when the outside air is cooler and not too humid, the envelope is opened, allowing cooler air to pass through the building so the stored heat can be dissipated by convection. This process reduces the temperature of the indoor air and of the building's thermal mass, allowing convective, conductive, and radiant cooling to take place during the day when the building is occupied. Night flushing is most effective in climates with a large diurnal swing, i.e. a large difference between the daily maximum and minimum outdoor temperature. For optimal performance, the nighttime outdoor air temperature should fall below the daytime comfort zone limits of and 60% relative humidity. For the night flushing strategy to be effective at reducing indoor temperature and energy usage, the thermal mass must be sized sufficiently and distributed over a wide enough surface area to absorb the space's daily heat gains. Also, the total air change rate must be high enough to remove the internal heat gains from the space at night.\nThere are three ways night flushing can be achieved in a building:\nThere are numerous benefits to using night flushing as a cooling strategy for buildings, including improved comfort and a shift in peak energy load. Energy is most expensive during the day. By implementing night flushing, the usage of mechanical ventilation is reduced during the day, leading to energy and money savings.\n\nThere are also a number of limitations to using night flushing, such as usability, security, reduced indoor air quality, and poor room acoustics. For natural night flushing, the process of manually opening and closing windows every day can be tiresome, especially in the presence of insect screens. This problem can be eased with automated windows or ventilation louvers, such as in the Manitoba Hydro Place. Natural night flushing also requires windows to be open at night when the building is most likely unoccupied, which can raise security issues. If outdoor air is polluted, night flushing can expose occupants to harmful conditions inside the building. In loud city locations, the opening of windows can create poor acoustical conditions inside the building.\n\nAll objects constantly emit and absorb radiant energy. An object will cool by radiation if the net flow is outward, which is the case during the night. At night, the long-wave radiation from the clear sky is less than the long-wave infrared radiation emitted from a building, thus there is a net flow to the sky. Since the roof provides the greatest surface visible to the night sky, designing the roof to act as a radiator is an effective strategy. There are two types of radiative cooling strategies that utilize the roof surface: direct and indirect:\n\nThis design relies on the evaporative process of water to cool the incoming air while simultaneously increasing the relative humidity. A saturated filter is placed at the supply inlet so the natural process of evaporation can cool the supply air. Apart from the energy to drive the fans, water is the only other resource required to provide conditioning to indoor spaces. The effectiveness of evaporative cooling is largely dependent on the humidity of the outside air; dryer air produces more cooling. A study of field performance results in Kuwait revealed that power requirements for an evaporative cooler are approximately 75% less than the power requirements for a conventional packaged unit air-conditioner. As for interior comfort, a study found that evaporative cooling reduced inside air temperature by 9.6 °C compared to outdoor temperature. An innovative passive system uses evaporating water to cool the roof so that major portion of solar heat does not come inside.\n\nEarth coupling uses the moderate and consistent temperature of the soil to act as a heat sink to cool a building through conduction. This passive cooling strategy is most effective when earth temperatures are cooler than ambient air temperature, such as in hot climates.\n\n"}
{"id": "30943670", "url": "https://en.wikipedia.org/wiki?curid=30943670", "title": "Power–delay product", "text": "Power–delay product\n\nIn digital electronics, the power–delay product (PDP) is a figure of merit correlated with the energy efficiency of a logic gate or logic family. Also known as switching energy, it is the product of power consumption \"P\" (averaged over a switching event) times the input–output delay or duration of the switching event \"D\". It has the dimension of energy and measures the energy consumed per switching event.\n\nIn a CMOS circuit the switching energy and thus the PDP for a 0-to-1-to-0 computation cycle is C·V. Therefore, lowering the supply voltage V lowers the PDP.\n\nEnergy-efficient circuits with a low PDP may also be performing very slowly, thus energy–delay product (EDP), the product of \"E\" and \"D\" (or \"P\" and \"D\"), is sometimes a more preferable metric.\n\nIn CMOS circuits the delay is inversely proportional to the supply voltage V and hence EDP is proportional to V. Consequently, lowering V also benefits EDP.\n\n\n"}
{"id": "42744699", "url": "https://en.wikipedia.org/wiki?curid=42744699", "title": "Salvinia effect", "text": "Salvinia effect\n\nThe Salvinia effect describes the permanent stabilization of an air layer upon a hierarchically structured surface submerged in water. Based on biological models (e.g. the floating ferns \"Salvinia\", backswimmer \"Notonecta\"), biomimetic Salvinia-surfaces are used as drag reducing coatings (up to 30% reduction were previously measured on the first prototypes. When applied to a ship hull, the coating would allow the boat to float on an air-layer; reducing energy consumption and emissions. Such surfaces require an extremely water repellent super-hydrophobic surface and an elastic hairy structure in the millimeter range to entrap air while submerged. The Salvinia effect was discovered by the biologist and botanist Wilhelm Barthlott (University of Bonn) and his colleagues and has been investigated on several plants and animals since 2002. Publications and patents were published between 2006 and 2016. The best biological models are the floating ferns (\"Salvinia)\" with highly sophisticated hierarchically structured hairy surfaces, and the back swimmers (e.g.\"Notonecta)\" with a complex double structure of hairs (setae) and microvilli (microtrichia). Three of the ten known Salvinia species show a paradoxical chemical heterogenity: hydrophilic hair tips, in addition to the super-hydrophobic plant surface, further stabilizing the air layer.\n\nImmersed in water, extremely water repellent (super-hydrophobic), structured surfaces trap air between the structures and this air-layer is maintained for a period of time. A silvery shine, due to the reflection of light at the interface of air and water, is visible on the submerged surfaces.\n\nLong lasting air layers also occur in aquatic arthropods which breathe via a physical gill (plastron) e. g. the water spider (\"Argyroneta\") and the saucer bug (\"Aphelocheirus\") Air layers are presumably also conducive to the reduction of friction in fast moving animals under water, as is the case for the back swimmer \"Notonecta\".\n\nThe best known examples for long term air retention under water are the floating ferns of genus \"Salvinia\". About ten species of very diverse sizes are found in lentic water in all warmer regions of the earth, one widely spread species (\"S. natans\") found in temperate climates can be even found in Central Europe. The ability to retain air is presumably a survival technique for these plants. The upper side of the floating leaves is highly water repellent and possesses highly complex and species-specific very distinctive hairs. Some species present multicellular free-standing hairs of 0.3–3 mm length (e. g. \"S.\" \"cucullata\") while on others, two hairs are connected at the tips (e. g. \"S. oblongifolia\"). S. minima and S. natans have four free standing hairs connected at a single base. The Giant Salvinia (\"S. molesta\"\"),\" as well as \"S. auriculata,\" and other closely related species, display the most complex hairs: four hairs grow on a shared shaft; they are connected at their tips. These structures resemble microscopic eggbeaters and are therefore referred to as “eggbeater trichomes”. The entire leaf surface, including the hairs, is covered with nanoscale wax crystals which are the reason for the water repellent properties of the surfaces. These leaf surfaces are therefore a classical example of a “hierarchical structuring“.\n\nThe egg-beater hairs of \"Salvinia molesta\" and closely related species (e. g. Salvinia \"auriculata\") show an additional remarkable property. The four cells at the tip of each hair (the anchor cells), as opposed to the rest of the hair, are free of wax and therefore hydrophilic; in effect, wettable islands surrounded by a super-hydrophobic surface. This chemical heterogeneity, the \"Salvinia\" paradox, enables a pinning of the air water interface to the plant and increases the pressure and longtime stability of the air layer.\n\nThe air retaining surface of the floating fern does not lead to a reduction in friction. The ecological extremely adaptable Giant Salvinia (S. molesta) is one of the most important invasive plants in all tropical and subtropical regions of the earth and is the cause of economic as well as ecological problems. Its growth rate might be the highest of all vascular plants. In the tropics and under optimal conditions, \"S. molesta\" can double its biomass within four days. The Salvinia effect, described here, most likely plays an essential role in its ecological success; the multilayered floating plant mats presumably maintain their function of gas exchange within the air-layer.\n\nThe Salvinia effect defines surfaces which are able to permantently keep relatively thick air layers as a result of their hydrophobic chemistry, in combination with a complex architecture in nano- and microscopic dimensions.\n\nThis phenomenon was discovered during a systematic research on aquatic plants and animals by Wilhelm Barthlott and his colleagues at the University of Bonn between 2002 and 2007. Five criteria have been defined, they enable the existence of stable air layers under water and as of 2009 define the Salvinia effect: (1) hydrophobic surfaces chemistry in combination with (2) nanoscalic structures generate superhydrophobicity, (3) microscopic hierarchical structures ranging from a few mirco- to several millimeters with (4) undercuts and (5) elastic properties. Elasticity appears to be important for the compression of the air-layer in dynamic hydrostatic conditions. An additional optimizing criterion is the chemical heterogeneity of the hydrophilic tips (Salvinia Paradox). This is a prime example of a hierarchical structuring on several levels.\n\nIn plants and animals, air retaining salvinia effect surfaces are always fragmented in small compartments with a length of 0.5 to 8 cm and the borders are sealed against loss of air by particular microstructures. Compartments with sealed edges are also important for technical applications.\n\nThe working principle is illustrated in for the Giant Salvinia. The leaves of \"S. molesta\" are capable of keeping an air layer on its surfaces for a long time when submerged in water. If a leaf is pulled under water, the leaf surface shows a silvery shine. The distinctive feature of \"S. molesta\" lies in the long term stability. While the air layer on most hydrophobic surfaces vanishes shortly after submerging, \"S. molesta\" is able to stabilize the air for several days to several weeks. The time span is thereby just limited by the lifetime of the leaf.\n\nThe high stability is a consequence of a seemingly paradoxical combination of a superhydrophobic (extremely water repellent) surface with hydrophilic (water attractive) patches on the tips of the structures.\n\nWhen submerged under water, no water can penetrate the room between the hairs due to the hydrophobic character of the surfaces. However, the water is pinned to the tip of each hair by the four wax free (hydrophilic) end cells. This fixation results in a stabilization of the air layer under water. The principle is shown in the figure.\n\nTwo submerged, air retaining surfaces are schematically shown: on the left hand side: a hydrophobic surface. On the right hand side: a hydrophobic surface with hydrophilic tips.\n\nIf negative pressure is applied, a bubble is quickly formed on the purely hydrophobic surfaces (left) stretching over several structures. With increasing negative pressure the bubble grows and can detach from the surface. The air bubble rises to the surface and the air layer decreases until it vanishes completely.\n\nIn case of the surface with hydrophilic anchor cells (right) the water is pinned to the tips of every structure by the hydrophilic patch on top. These linkages allow the formation of a bubble stretching over several structures; bubble release is suppressed because several links have to be broken first. This results in a higher energy input for the bubble formation. Therefore an increased negative pressure is needed to form a bubble able to detach from the surface and rise upwards.\n\nUnderwater air retaining surfaces are of great interest for technical applications. If a transfer of the effect to a technical surface is successful, ship hulls could be coated with this surface to reduce friction between ship and water resulting in less fuel consumption, fuel costs and reduction of its negative environmental impact (antifouling effect by the air layer). In 2007 first test boats already achieved a ten percent friction reduction and the principle was subsequently patented. By now scientists assume a friction reduction of over 30%.\n\nThe underlying principle is schematically shown in a figure. Two flow profiles of laminar flow in water over a solid surface and water flowing over an air retaining surface are compared here.\n\nIf water flows over a smooth solid surface, the velocity at the surface is zero due to the friction between water and surface molecules. If an air layer is situated between the solid surface and the water the velocity is higher than zero. The lower viscosity of air (55 times lower than the viscosity of water) reduces the transmission of friction forces by the same factor.\n\nResearchers are currently working on the development of a biomimetic, permanently air retaining surface modeled on \"S. molesta\" to reduce friction on ships. Salvinia-Effect surfaces have been proven to quickly and efficiently adsorb oil and can be used for oil-water separation applications \n\n\n"}
{"id": "780166", "url": "https://en.wikipedia.org/wiki?curid=780166", "title": "Sanitary napkin", "text": "Sanitary napkin\n\nA sanitary napkin, sanitary towel, sanitary pad, menstrual pad, or pad is an absorbent item worn in the underwear by women who are menstruating, bleeding after giving birth, recovering from gynecologic surgery, experiencing a miscarriage or abortion, or in any other situation where it is necessary to absorb a flow of blood from the vagina.\n\nA menstrual pad is a type of feminine hygiene product that is worn externally, unlike tampons and menstrual cups which are worn inside the vagina. The frequency a woman will need to change her pad will vary depending on whether her menstrual flow is heavy or light. \n\nMenstrual pads are made from a range of materials, differing depending on style, country of origin, and brand. US brands include Kotex, Always, Equate, and Stayfree. Lil-lets are available in several other English-speaking countries. \n\nThese pads are not to be confused with generally higher absorbency incontinence pads, which are worn by people who have urinary incontinence problems or experience stress incontinence. Although menstrual pads are not made for this use, some people use them for this purpose.\n\nAlthough producers are generally reluctant to reveal the exact composition of their products, the main materials will usually be bleached rayon (cellulose made from wood pulp), cotton and plastics. In addition, fragrance and antibacterial agents can be included. The plastic parts are the backsheet and polymer powder as an additional powerful absorbent (superabsorbent polymers) that turns into a gel when moistened. Procter & Gamble are advertising a proprietary material called infinicel as the core of their napkins.\nIn general, the layering is as follows: \"an absorbent core material placed between a flexible liquid-pervious topsheet and a liquid-impervious plastic backsheet that has an adhesive on the outside for attaching the napkin to an undergarment\". As is the case with disposable tampons and diapers recycling is difficult and rarely done due to cost reasons although proof-of-principle solution appear to exist. When not dumped in a landfill where the non-biodegradable parts may persist for thousands of years, conventional hygiene products can at best be \"thermally recycled\" (incinerated). Environmentally more friendly solutions can be found in 100% cotton pads.\n\nThere are several different types of disposable menstrual pads:\n\n\nThe shape, absorbency and lengths may vary depending on manufacturer, but usually range from the short slender panty liner to the larger and longer overnight. Long pads are offered for extra protection or for larger people whose undergarments might not be completely protected by regular length pads, and also for overnight use.\n\nOther options are often offered in a manufacturer's line of pads, such as wings or tabs that wrap around the sides of underwear to add additional leak protection and help secure the pad in place. Deodorant is also added to some pads, which is designed to cover menstrual odor with a light fragrance. There are even panty liners specifically designed to be worn with a thong/G-string.\n\nAlternatively, some people use a washable or reusable cloth menstrual pad. These are made from a number of types of fabric — most often cotton flannel, or hemp (which is highly absorbent and not as bulky as cotton). Most styles have wings that secure around the underpants, but some are just held in place (without wings) between the body and the underpants. Some (particularly the older styles) are available in belted styles. Washable menstrual pads do not need to be disposed of after use and therefore offer a more economical alternative. Reusable menstrual pads can be found on a number of websites, or are made at home (instructions are available online). They have become a popular alternative because they are allergen- and perfume-free, and can be more comfortable for people who suffer from irritations from using disposable pads.\n\nCloth menstrual pads made a comeback around the 1970s, with their popularity increasing in the late 80s and early 90s. Reasons women choose to switch to cloth menstrual pads include comfort, savings over time, environmental impact and health reasons.\n\nThere are many styles of cloth menstrual pads available today, ranging from pantyliners to overnight pads. Popular styles of cloth menstrual pads include all-in-one, or AIO pads, in which the absorbent layer is sewn inside the pad, 'inserts on top' style pads, which have absorbent layers that can be secured on top of the pad as needed, envelope or pocket style pads, which have absorbent layers that can be inserted inside the pad as needed, and a foldable style, in which the pad folds around the absorbent layers. Cloth menstrual pads can have waterproof lining, which provides more leak protection but may also be less breathable.\n\nMenstrual pads are worn to absorb menstrual discharge (and thereby protect clothing and furnishings). They are usually individually wrapped so they are easier and more discreet to carry in a purse or bag. This wrapper may be used to wrap the soiled pads before disposing of them in appropriate receptacles. Some people prefer to wrap the pads with toilet paper instead of (or as well as) using the wrapper, which, often being made of slick plastic with a small tape tab, may not adequately stick. Menstrual pads of any type should not be flushed down the toilet as they can cause blockages. In developed countries, public toilets almost always include a purpose-made receptacle in which to place soiled pads. In first aid, they make excellent dressings for heavy bleeding due to their high absorbency if gauze is unavailable or inadequate.\n\nMany people who experience urinary incontinence use menstrual pads to manage bladder leaks. However, since menstrual pads are designed to absorb menstrual flow, they are not as effective in absorbing urinary leaks; incontinence pads are designed for this purpose.\n\nIf a woman does not have sanitary napkins on hand, she might use toilet paper as a temporary substitution.\n\nThrough the ages women have used different forms of menstrual protection. Menstrual pads have been mentioned as early as the 10th century, in the Suda, where Hypatia, who lived in the 4th century AD, was said to have thrown one of her used menstrual rags at an admirer in an attempt to discourage him. Women often used strips of folded old cloth (rags) to absorb their menstrual flow, which is why the term \"on the rag\" is used to refer to menstruation.\n\nDisposable menstrual pads grew from a Benjamin Franklin invention created to help stop wounded soldiers from bleeding, but appear to have been first commercially available from around 1888 with the Southall's pad. The first commercially available American disposable napkins were Lister's Towels created by Johnson & Johnson in 1888. Disposable pads had their start with nurses using their wood pulp bandages to absorb their menstrual flow, creating a pad that was made from easily obtainable materials and inexpensive enough to throw away after use. Kotex's first advertisement for products made with this wood pulp (Cellucotton) appeared in January 1921. Several of the first disposable pad manufacturers were also manufacturers of bandages, which could give an indication of what these products were like.\n\nUntil disposable sanitary pads were created, cloth or reusable pads were widely used to collect menstrual blood. Women often used a variety of home-made menstrual pads which they crafted from various fabrics, or other absorbent materials, to collect menstrual blood. Even after disposable pads were commercially available, for several years they were too expensive for many women to afford. When they could be afforded, women were allowed to place money in a box so that they would not have to speak to the clerk and take a box of Kotex pads from the counter themselves. It took several years for disposable menstrual pads to become commonplace. However, they are now used nearly exclusively in most of the industrialized world.\n\nThe first of the disposable pads were generally in the form of a cotton wool or similar fibrous rectangle covered with an absorbent liner. The liner ends were extended front and back so as to fit through loops in a special girdle or belt worn beneath undergarments. This design was notorious for slipping either forward or back of the intended position.\n\nLater an adhesive strip was placed on the bottom of the pad for attachment to the crotch of the panties, and this became a favoured method with women. The belted sanitary napkin quickly disappeared during the early 1980s.\n\nThe ergonomic design and materials used to make pads also changed through the 1980s to today. With earlier materials not being as absorbent and effective, and early pads being up to two centimetres thick, leaks were a major problem. Some variations introduced were quilting of the lining, adding \"wings\" and reducing the thickness of the pad by utilising products such as sphagnum and polyacrylate superabsorbent gels derived from petroleum. The materials used to manufacture most pads are derived from the petroleum industry and forestry. The absorbent core, made from chlorine bleached wood pulp, could be reduced to make slimmer products with the addition of polyacrylate gels which sucks up the liquid quickly and holds it in a suspension under pressure. The remaining materials are mostly derived from the petroleum industry, the cover stock used is polypropylene non woven, with the leakproof barrier made from polyethylene film.\n\nIn developing countries, makeshift pads are still used to collect menstrual blood as they are cheaper. Rags, soil, and mud are also reportedly used for collecting menstrual flow by women who cannot afford the more expensive disposable pads or tampons.\n\nIn order to meet the need for achieving an inexpensive solution to reduce unsanitary and unhygienic practices in countries like India, Arunachalam Muruganantham from rural Coimbatore in the southern state of Tamil Nadu, India developed and patented a machine which could manufacture low-cost sanitary pads for less than a third of the former cost.\n\nNapkins, especially reusable ones, may be visible on full body scanners.\n\n\n"}
{"id": "106001", "url": "https://en.wikipedia.org/wiki?curid=106001", "title": "Soil pH", "text": "Soil pH\n\nSoil pH is a measure of the acidity or basicity (alkalinity) of a soil. pH is defined as the negative logarithm (base 10) of the activity of hydronium ions ( or, more precisely, ) in a solution. In soils, it is measured in a slurry of soil mixed with water (or a salt solution, such as 0.01 M ), and normally falls between 3 and 10, with 7 being neutral. Acid soils have a pH below 7 and alkaline soils have a pH above 7. Ultra-acidic soils (pH < 3.5) and very strongly alkaline soils (pH > 9) are rare.\n\nSoil pH is considered a master variable in soils as it affects many chemical processes. It specifically affects plant nutrient availability by controlling the chemical forms of the different nutrients and influencing the chemical reactions they undergo. The optimum pH range for most plants is between 5.5 and 7.5; however, many plants have adapted to thrive at pH values outside this range.\n\nThe United States Department of Agriculture Natural Resources Conservation Service classifies soil pH ranges as follows:\n\nMethods of determining pH include:\n\nPrecise, repeatable measures of soil pH are required for scientific research and monitoring. This generally entails laboratory analysis using a standard protocol; an example of such a protocol is that in the USDA Soil Survey Field and Laboratory Methods Manual. In this document the three-page protocol for soil pH measurement includes the following sections: Application; Summary of Method; Interferences; Safety; Equipment; Reagents; and Procedure.\n\nThe pH of a natural soil depends on the mineral composition of the parent material of the soil, and the weathering reactions undergone by that parent material. In warm, humid environments, soil acidification occurs over time as the products of weathering are leached by water moving laterally or downwards through the soil. In dry climates, however, soil weathering and leaching are less intense and soil pH is often neutral or alkaline.\n\nMany processes contribute to soil acidification. These include: \n\nTotal soil alkalinity increases with:\n\nThe accumulation of alkalinity in a soil (as carbonates and bicarbonates of Na, K, Ca and Mg) occurs when there is insufficient water flowing through the soils to leach soluble salts. This may be due to arid conditions, or poor internal soil drainage; in these situations most of the water that enters the soil is transpired (taken up by plants) or evaporates, rather than flowing through the soil.\n\nThe soil pH usually increases when the total alkalinity increases, but the balance of the added cations also has a marked effect on the soil pH. For example, increasing the amount of sodium in an alkaline soil tends to induce dissolution of calcium carbonate, which increases the pH. Calcareous soils may vary in pH from 7.0 to 9.5, depending on the degree to which or dominate the soluble cations.\n\nPlants grown in acid soils can experience a variety of stresses including aluminium (Al), hydrogen (H), and/or manganese (Mn) toxicity, as well as nutrient deficiencies of calcium (Ca) and magnesium (Mg).\n\nAluminium toxicity is the most widespread problem in acid soils. Aluminium is present in all soils, but dissolved Al is toxic to plants; Al is most soluble at low pH; above pH 5.0, there is little Al in soluble form in most soils. Aluminium is not a plant nutrient, and as such, is not actively taken up by the plants, but enters plant roots passively through osmosis. Aluminium inhibits root growth; lateral roots and root tips become thickened and roots lack fine branching; root tips may turn brown. In the root, the initial effect of Al is the inhibition of the expansion of the cells of the rhizodermis, leading to their rupture; thereafter it is known to interfere with many physiological processes including the uptake and transport of calcium and other essential nutrients, cell division, cell wall formation, and enzyme activity.\n\nProton (H ion) stress can also limit plant growth. The proton pump, H-ATPase, of the plasmalemma of root cells works to maintain the near-neutral pH of their cytoplasm. A high proton activity (pH within the range 3.0–4.0 for most plant species) in the external growth medium overcomes the capacity of the cell to maintain the cytoplasmic pH and growth shuts down.\n\nIn soils with a high content of manganese-containing minerals, Mn toxicity can become a problem at pH 5.6 and lower. Manganese, like aluminium, becomes increasingly soluble as pH drops, and Mn toxicity symptoms can be seen at pH levels below 5.6. Manganese is an essential plant nutrient, so plants transport Mn into leaves. Classic symptoms of Mn toxicity are crinkling or cupping of leaves.\n\nSoil pH affects the availability of some plant nutrients:\n\nAs discussed above, aluminium toxicity has direct effects on plant growth; however, by limiting root growth, it also reduces the availability of plant nutrients. Because roots are damaged, nutrient uptake is reduced, and deficiencies of the macronutrients (nitrogen, phosphorus, potassium, calcium and magnesium) are frequently encountered in very strongly acidic to ultra-acidic soils (pH<5.0).\n\nMolybdenum availability is increased at higher pH; this is because the molybdate ion is more strongly sorbed by clay particles at lower pH.\n\nZinc, iron, copper and manganese show decreased availability at higher pH (increased sorbtion at higher pH).\n\nThe effect of pH on phosphorus availability varies considerably, depending on soil conditions and the crop in question. The prevailing view in the 1940s and 1950s was that P availability was maximized near neutrality (soil pH 6.5–7.5), and decreased at higher and lower pH. Interactions of phosphorus with pH in the moderately to slightly acidic range (pH 5.5–6.5) are, however, far more complex than is suggested by this view. Laboratory tests, glasshouse trials and field trials have indicated that increases in pH within this range may increase, decrease, or have no effect on P availability to plants.\n\nStrongly alkaline soils are sodic and dispersive, with slow infiltration, low hydraulic conductivity and poor available water capacity. Plant growth is severely restricted because aeration is poor when the soil is wet; in dry conditions, plant-available water is rapidly depleted and the soils become hard and cloddy (high soil strength).\n\nMany strongly acidic soils, on the other hand, have strong aggregation, good internal drainage, and good water-holding characteristics. However, for many plant species, aluminium toxicity severely limits root growth, and moisture stress can occur even when the soil is relatively moist.\n\nIn general terms, different plant species are adapted to soils of different pH ranges. For many species, the suitable soil pH range is fairly well known. Online databases of plant characteristics, such \"USDA PLANTS\" and \"Plants for a Future\" can be used to look up the suitable soil pH range of a wide range of plants. Documents like \"Ellenberg's indicator values for British plants\" can also be consulted.\n\nHowever, a plant may be intolerant of a particular pH in some soils as a result of a particular mechanism, and that mechanism may not apply in other soils. For example, a soil low in molybdenum may not be suitable for soybean plants at pH 5.5, but soils with sufficient molybdenum allow optimal growth at that pH. Similarly, some calcifuges (plants intolerant of high-pH soils) can tolerate calcareous soils if sufficient phosphorus is supplied. Another confounding factor is that different varieties of the same species often have different suitable soil pH ranges. Plant breeders can use this to breed varieties that can tolerate conditions that are otherwise considered unsuitable for that species – examples are projects to breed aluminium-tolerant and manganese-tolerant varieties of cereal crops for food production in strongly acidic soils.\n\nThe table below gives suitable soil pH ranges for some widely cultivated plants as found in the \"USDA PLANTS Database\". Some species (like \"Pinus radiata\" and \"Opuntia ficus-indica\") tolerate only a narrow range in soil pH, whereas others (such as \"Vetiveria zizanioides\") tolerate a very wide pH range.\n\nFinely ground agricultural lime is often applied to acid soils to increase soil pH (liming). The amount of limestone or chalk needed to change pH is determined by the mesh size of the lime (how finely it is ground) and the buffering capacity of the soil. A high mesh size (60 mesh = 0.25 mm; 100 mesh = 0.149 mm) indicates a finely ground lime that will react quickly with soil acidity. The buffering capacity of a soil depends on the clay content of the soil, the type of clay, and the amount of organic matter present, and may be related to the soil cation exchange capacity. Soils with high clay content will have a higher buffering capacity than soils with little clay, and soils with high organic matter will have a higher buffering capacity than those with low organic matter. Soils with higher buffering capacity require a greater amount of lime to achieve an equivalent change in pH.\n\nAmendments other than agricultural lime that can be used to increase the pH of soil include wood ash, industrial calcium oxide (burnt lime), magnesium oxide, basic slag (calcium silicate), and oyster shells. These products increase the pH of soils through various acid-base reactions. Calcium silicate neutralizes active acidity in the soil by reacting with H ions to form monosilicic acid (HSiO), a neutral solute.\n\nThe pH of an alkaline soil can be reduced by adding acidifying agents or acidic organic materials. Elemental sulfur (90–99% S) has been used at application rates of 300–500 kg/ha – it slowly oxidizes in soil to form sulfuric acid. Acidifying fertilizers, such as ammonium sulfate, ammonium nitrate and urea, can help to reduce the pH of a soil because ammonium oxidises to form nitric acid. Acidifying organic materials include peat or sphagnum peat moss.\n\nHowever, in high-pH soils with a high calcium carbonate content (more than 2%), it can be very costly and/or ineffective to attempt to reduce the pH with acids. In such cases, it is often more efficient to add phosphorus, iron, manganese, copper and/or zinc instead, because deficiencies of these nutrients are the most common reasons for poor plant growth in calcareous soils.\n\n\n"}
{"id": "4110846", "url": "https://en.wikipedia.org/wiki?curid=4110846", "title": "Telecommand", "text": "Telecommand\n\nA telecommand is a command sent to control a remote system or systems not directly connected (e.g. via wires) to the place from which the telecommand is sent. The word is derived from \"tele\" = remote (Greek), and \"command\" = to entrust/order (Latin). Systems that need remote measurement and reporting of information of interest to the system designer or operator require the counterpart of telecommand, telemetry. The \"telecommand\" can be done in real time or not depending on the circumstances (in space, delay may be of days), as was the case of Marsokhod.\n\n\nFor a Telecommand (TC) to be effective, it must be compiled into a pre-arranged format (which may follow a standard structure), modulated onto a carrier wave which is then transmitted with adequate power to the remote system. The remote system will then demodulate the digital signal from the carrier, decode the TC, and execute it. Transmission of the carrier wave can be by ultrasound, infra-red or other electromagnetic means.\n\nInfrared light makes up the invisible section of the electromagnetic spectrum. This light, also classified as heat, transmits signals between the transmitter and receiver of the remote system. Telecommand systems usually include a physical remote, which contains four key parts: buttons, integrated circuit, button contacts, and a light-emitting diode. When the buttons on a remote are pressed they touch and close their corresponding contacts below them within the remote. This completes the necessary circuit on the circuit board along with a change in electrical resistance, which is detected by the integrated circuit. Based on the change in electrical resistance, the integrated circuit distinguishes which button was pushed and sends a corresponding binary code to the light-emitting diode (LED) usually located at the front of the remote. To transfer the information from the remote to the receiver, the LED turns the electrical signals into an invisible beam of infrared light that corresponds with the binary code and sends this light to the receiver. The receiver then detects the light signal via a photodiode and it is transformed into an electrical signal for the command and is sent to the receiver’s integrated circuit/microprocessor to process and complete the command. The strength of the transmitting LED can vary and determines the required positioning accuracy of the remote in relevance to the receiver. Infrared remotes have a maximum range of approximately 30 feet and require the remote control or transmitter and receiver to be within a line of sight.\n\nUltrasonic is a technology used more frequently in the past for telecommand. Inventor Robert Adler is known for inventing the remote control which did not require batteries and used ultrasonic technology. There are four aluminum rods inside the transmitter that produce high frequency sounds when they are hit at one end. Each rod is a different length, which enables them to produce varying sound pitches, which control the receiving unit. This technology was widely used but had certain issues such as dogs being bothered by the high frequency sounds.\n\nOften the smaller new remote controlled airplanes and helicopters are incorrectly advertised as radio controlled devices (see Radio control) but they are either controlled via infra-red transmission or electromagnetically guided. Both of these systems are part of the telecommand area.\n\nTo prevent unauthorised access to the remote system, TC encryption may be employed. Secret sharing may be used.\n\n"}
{"id": "4764824", "url": "https://en.wikipedia.org/wiki?curid=4764824", "title": "Trinitrotriazine", "text": "Trinitrotriazine\n\nTrinitrotriazine, or 2,4,6-trinitro-1,3,5-triazine, is a theoretical explosive compound. Synthesis of this compound has been elusive despite its simple structure, as conventional nitration of triazine becomes increasingly more difficult as more nitro groups are added. A successful route would more likely proceed by trimerisation of nitryl cyanide. The precursor, nitryl cyanide, was first synthesized by Rahm et al in 2014.\n\nTrinitrotriazine has a perfect oxygen balance, potentially making it a very powerful explosive, though calculations predict it would be fairly unstable and inferior to the related compound 3,6-dinitro-1,2,4,5-tetrazine. \n\n"}
{"id": "12031361", "url": "https://en.wikipedia.org/wiki?curid=12031361", "title": "Walkability", "text": "Walkability\n\nWalkability is a measure of how friendly an area is to walking. Walkability has health, environmental, and economic benefits. Factors influencing walkability include the presence or absence and quality of footpaths, sidewalks or other pedestrian rights-of-way, traffic and road conditions, land use patterns, building accessibility, and safety, among others. Walkability is an important concept in sustainable urban design.\n\nOne proposed definition for walkability is: \"The extent to which the built environment is friendly to the presence of people living, shopping, visiting, enjoying or spending time in an area\". Factors affecting walkability include, but are not limited to:\nMajor infrastructural factors include access to mass transit, presence and quality of footpaths, buffers to moving traffic (planter strips, on-street parking or bike lanes) and pedestrian crossings, aesthetics, nearby local destinations, air quality, shade or sun in appropriate seasons, street furniture, traffic volume and speed. and wind conditions.\n\nWalkability is also examined based on the surrounding built environment. Reid Ewing and Robert Cervero's five D's of the built environment—density, diversity, design, destination accessibility, and distance to transit—heavily influence an area's walkability. Combinations of these factors influence an individual's decision to walk.\n\nBefore cars and bicycles were mass-produced, walking was the main way to travel. It was the only way to get from place to place for much of human history. In the 1930s, economic growth led to increased automobile manufacturing. Cars were also becoming more affordable, leading to the rise of the automobile during the Post–World War II economic expansion. The detrimental effects of automobile emissions soon led to public concern over pollution. Alternatives, including improved public transportation and walking infrastructure, have attracted more attention from planners and policymakers.\n\nWalkability indices have been found to correlate with both Body Mass Index (BMI) and physical activity of local populations. Physical activity can prevent chronic diseases, such as cardiovascular disease, diabetes, hypertension, obesity, depression, and osteoporosis. Thus for instance, an increase in neighborhood Walk Score has linked with both better Cardio metabolic risk profiles and a decreased risk of heart-attacks. The World Cancer Research Fund and American Institute for Cancer Research released a report that new developments should be designed to encourage walking, on the grounds that walking contributes to a reduction of cancer. A further justification for walkability is founded upon evolutionary and philosophical grounds, contending that gait is important to the cerebral development in humans.\n\nDue to discrepancies between residents' health in inner city neighborhoods and suburban neighborhoods with similar walkability measures, further research is needed to find additional built environment factors in walkability indices.\n\nOne of most important benefits of walkability is the decrease of the automobile footprint in the community. Carbon emissions can be reduced if more people choose to walk rather than drive or use public transportation. The benefits of less emissions include improved health conditions and quality of life, less smog, and less of a contribution to global climate change.\n\nWalkability has also been found to have many economic benefits, including accessibility, cost savings both to individuals and to the public, increased efficiency of land use, increased livability, economic benefits from improved public health, and economic development, among others. The benefits of walkability are best guaranteed if the entire system of public corridors is walkable - not limited to certain specialized routes. More sidewalks and increased walkability can promote tourism and increase property value.\n\nIn recent years, the demand for housing in a walkable urban context has increased. The term \"Missing Middle Housing\" as coined by Daniel Parolek of Opticos Design, Inc., refers to multi-unit housing types (such as duplexes, fourplexes, bungalow courts, and mansion apartments not bigger than a large house), which are integrated throughout most walkable Pre-1940s neighborhoods, but became much less common after World War II, hence the term \"missing.\" These housing types are often integrated into blocks with primarily single-family homes, to provide diverse housing choices and generate enough density to support transit and locally-serving commercial amenities.\n\nAuto-focused street design diminish walking and needed \"eyes on the street\" provided by the steady presence of people in an area. Walkability increases social interaction, mixing of populations, average number of friends and associates where people live, reduced crime (with more people walking and watching over neighborhoods, open space and main streets), increased sense of pride, and increased volunteerism.\n\nSocioeconomic factors contribute to willingness to choose walking over driving. Income, age, race, ethnicity, education, household status, and having children in a household all influence walking travel.\n\nMany communities have embraced pedestrian mobility as an alternative to older building practices that favor automobiles. Reasons for this shift include a belief that dependency on automobiles is ecologically unsustainable. Automobile-oriented environments engender dangerous conditions to both motorists and pedestrians, and are generally bereft of aesthetics. A tool that some American cities, like Cincinnati, OH, are employing to improve walkability is a type of zoning called Form-based coding.\n\nThere are several ways to make a community more walkable:\n\nOne way of assessing and measuring walkability is to undertake a walking audit. An established and widely used walking audit tool is PERS (Pedestrian Environment Review System) which has been used extensively in the UK.\n\nA simple way to determine the walkability of a block, corridor or neighborhood is to count the number of people walking, lingering and engaging in optional activities within a space. This process is a vast improvement upon pedestrian level of service (LOS) indicators, recommended within the Highway Capacity Manual. However it may not translate well to non-Western locations where the idea of \"optional\" activities may be different. In any case, the diversity of people, and especially the presence of children, seniors and people with disabilities, denotes the quality, completeness and health of a walkable space.\n\nA number of commercial walkability scores also exist:\n\nA newly developing concept is the transit time map (sometimes called a transit shed map), which is a type of isochrone map. These are maps (often online and interactive) that display the areas of a metropolis which can be reached from a given starting point, in a given amount of travel time. Such maps are useful for evaluating how well-connected a given address is to other possible urban destinations, or conversely, how large a territory can quickly get to a given address. The calculation of transit time maps is computationally intensive, and considerable work is being done on more efficient algorithms for quickly producing such maps. To be useful, the production of a transit time map must take into consideration detailed transit schedules, service frequency, time of day, and day of week.\n\n\n"}
