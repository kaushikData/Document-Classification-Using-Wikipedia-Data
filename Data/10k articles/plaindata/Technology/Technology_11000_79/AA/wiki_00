{"id": "6271995", "url": "https://en.wikipedia.org/wiki?curid=6271995", "title": "AN/TPS-72", "text": "AN/TPS-72\n\nAN/TPS-72 is a planar array, E\\F band air search radar based on AN/TPS-43 and produced by Westinghouse.\n\n\n"}
{"id": "4513097", "url": "https://en.wikipedia.org/wiki?curid=4513097", "title": "Autoconform", "text": "Autoconform\n\nAutoconform is the video editing post production process where an online editing system combines a timecode based edit decision list (EDL) created from an offline editing system with the original video and audio source material to produce a version of the edited video which is a high quality (usually broadcast quality) analogue of the programme produced in the offline editing system.\n\nThis process can be compared with word processing, where the edits required to make a document are made on a PC, and draft copies are printed out on a low quality printer. When the creative process has been completed, the final version can be sent to a professional printer who can reproduce the document at full quality.\n"}
{"id": "22008603", "url": "https://en.wikipedia.org/wiki?curid=22008603", "title": "Back boiler", "text": "Back boiler\n\nA back boiler is a device which is fitted to a residential heating stove or open fireplace to enable it to provide both room heat and domestic hot water or central heating. The device is a water filled heat exchanger enclosed at the rear of the burning chamber with a hot water output at the top of the chamber and a cold water feed at the bottom.\n\nThe back boiler is typically used with a gravity feed circuit to the hot water cylinder, with a vent or overflow to prevent excess pressure build up. It can also be connected to a series of radiators to provide central heating but requires an electrical pump to be fitted to circulate the hot water.\n\nA back boiler can improve the efficiency of a stove by acting as a heat-sink and can also act as a method of extracting additional heat from the flue system which would otherwise have been lost.\nManufacturers of stoves with back boilers quote efficiency figures of up to 80% on oil-fed models.\n\nBack boilers require that the hot water or central heating system be able to disperse all of the heat captured from the fire, otherwise boiling or overheating of the water can occur. Back boilers are strictly regulated in Ireland and the UK following explosions relating to incorrectly serviced or installed back boilers.\n\n"}
{"id": "12066363", "url": "https://en.wikipedia.org/wiki?curid=12066363", "title": "Bergen Academy of Art and Design", "text": "Bergen Academy of Art and Design\n\nBergen Academy of Art and Design () or KHiB is one of the two independent institutions of higher learning in the visual arts and design in Norway. It is located in Bergen, Norway.\nArt education has long traditions in Bergen, as the first school of art was established there in 1772, modelled on the Academy of Art in Copenhagen. The present college, Bergen Academy of Art and Design, is a fairly new institution which in 1996 merged two former institutions; \"Vestlandets kunstakademi\" which had been founded in 1972 and \"Statens høgskole for kunsthåndverk og design\" which is dated to 1909.\nThe institution is spread across several buildings at two campuses. A new building is under construction that is designed by the architectural group Snøhetta after winning an International competition in 2005. Planned for 2009, the concept was redrawn to meet budget demands from the Norwegian Ministry of Education and Research. It then took until 2013 before the Norwegian Government came through with the necessary funding for the project. The Academy will take the new building into use after its completion in the autumn of 2017.\nThe Academy has a library on campus located in Vaskerelven 8. It offers books and periodicals on the subjects of art and design. Although primarily for students and staff, the library also welcomes the public.\n\nBergen Academy of Art and Design is located in five buildings in the center of Bergen. The main building and reception area is in Strømgaten 1, just across from the Bergen Railway Station. In the vicinity of the main building (around the pedestrian street Marken) the Department of Design is located at Kong Oscarsgate 62 and share a building in Marken 37 with the Department of Fine Art. The Department of Fine Art also has two more locations in buildings at Vaskerelven 8 and C. Sundtsgate 53.\n\nSince a restructuring of the institution's departments in 2012, the academy have consisted of two departments: Department of Fine Art and Department of Design. In the study year of 2015–16 there are approximately 350 students and around 100 staff members. In addition a range of continuing and further education courses are available.\n\nStudents can take a three-year Bachelor degree or a two-year Master degree in the following areas:\n\nThe department is divided into two subject areas, Visual Communication and Furniture and Spatial Design /Interior Architecture.\n\nThe department has eight specializations: painting and drawing, sculpture and installation, ceramics and clay, time-based art/performance, new media (audio/video), textiles, printmaking, and photography.\n\n"}
{"id": "29404759", "url": "https://en.wikipedia.org/wiki?curid=29404759", "title": "Biodegradable electronics", "text": "Biodegradable electronics\n\nBiodegradable electronics are electronic circuits and devices with a limited lifetime owing to their tendency to biodegrade. Such devices are proposed to represent useful medical implant, and temporary communication sensors.\n\nOrganic electronic devices as compostable material platforms have been fabricated on aluminum foil and paper to accommodate these expanded functionalities. In one embodiment of this idea, paper films were utilized as a combination substrate and gate dielectric for use with pentacene-based active layers. This idea was expanded upon to create complete circuits using foldable paper-based substrates.\n\nSilk coatings could underpin an electronic devices because it melts away when the device is no longer needed. One test device, a heating circuit powered by beaming radio waves at it, was implanted under the skin of a rat with a wound. After the wound had healed, the implant simply melts away. The US military research agency DARPA funded research on building a tiny dissolving camera with this silk coating for use as a disposable spy camera.\n"}
{"id": "44789336", "url": "https://en.wikipedia.org/wiki?curid=44789336", "title": "Blackman's theorem", "text": "Blackman's theorem\n\nBlackman's theorem is a general procedure for calculating the change in an impedance due to feedback in a circuit. It was published by Ralph Beebe Blackman in 1943, was connected to signal-flow analysis by John Choma, and was made popular in the extra element theorem by R. D. Middlebrook and the asymptotic gain model of Solomon Rosenstark. Blackman's approach leads to the formula for the impedance \"Z\" between two selected terminals of a negative feedback amplifier as Blackman's formula:\nwhere \"Z\" = impedance with the feedback disabled, \"T\" = loop transmission with a small-signal short across the selected terminal pair, and \"T\" = loop transmission with an open circuit across the terminal pair. The loop transmission also is referred to as the return ratio. Blackman's formula can be compared with Middlebrook's result for the input impedance \"Z\" of a circuit based upon the extra-element theorem:\n\nwhere:\n\nBlackman's formula also can be compared with Choma's signal-flow result:\n\nwhere formula_11 is the value of formula_12 under the condition that a selected parameter \"P\" is set to zero, return ratio formula_13 is evaluated with zero excitation and formula_14 is formula_13 for the case of short-circuited source resistance. As with the extra-element result, differences are in the perspective leading to the formula.\n\n\n"}
{"id": "2033735", "url": "https://en.wikipedia.org/wiki?curid=2033735", "title": "Brassica juncea", "text": "Brassica juncea\n\nBrassica juncea, commonly brown mustard, Chinese mustard, Indian mustard, leaf mustard, Oriental mustard and vegetable mustard, is a species of mustard plant. One subvariety is \"southern giant curled mustard\", which resembles a headless cabbage such as kale, but with a distinct horseradish or mustard flavor. It is also known as \"green mustard cabbage\".\n\n\"Brassica juncea\" cultivars can be divided into four major subgroups: integrifolia, juncea, napiformis, and tsatsai.\n\nIn 100 grams, cooked mustard greens provide 26 calories and are a rich source (20% or more of the Daily Value) of vitamins A, C and K which is especially high as a multiple of its Daily Value. Mustard greens are a moderate source of vitamin E and calcium. Greens are 92% water, 4.5% carbohydrates, 2.6% protein and 0.5% fat (table).\n\nThe leaves, seeds, and stems of this mustard variety are edible. The plant appears in some form in African, Bangladeshi, Chinese, Italian, Indian, Japanese, Nepali, Pakistani, Korean, and African-American (soul food) cuisines. Cultivars of \"B. juncea\" are grown for their greens, and for the production of oilseed. The mustard condiment made from the seeds of the \"B. juncea\" is called brown mustard and is considered to be spicier than yellow mustard.\n\nBecause it may contain erucic acid, a potential toxin, mustard oil is restricted from import as a vegetable oil into the United States. Essential oil of mustard, however, is accepted as GRAS (Generally Recognized as Safe). But in Russia, this is the main species grown for the production of mustard oil. It is widely used in canning, baking and margarine production in Russia, and the majority of Russian table mustard is also made from \"B. juncea\".\n\nThe leaves are used in African cooking, and all plant parts are used in Nepali cuisine, particularly in the mountain regions of Nepal, as well as in the Punjab cuisine of India and Pakistan, where a dish called \"sarson da saag\" (mustard greens) is prepared. \"B. juncea\" subsp. \"tatsai\", which has a particularly thick stem, is used to make the Nepali pickle called \"achar\", and the Chinese pickle \"zha cai\".\n\nThe Gorkhas of Darjeeling, Sikkim and Nepal prepare pork with mustard greens (also called \"rayo\" in Nepali). It is usually eaten with relish and steamed rice, but can also be eaten with \"roti\" (griddle breads). In Nepal it is also a common practice to cook these greens with meat of all sorts specially goat meat; which is normally prepared in a pressure cooker with minimal use of spices to focus on the flavour of the greens and dry chillies. \"Brassica juncea\" (especially the seeds) is more pungent than greens from the closely related \"Brassica oleracea\" (kale, broccoli, and collard greens), and is frequently mixed with these milder greens in a dish of \"mixed greens\".\n\nChinese and Japanese cuisines also make use of mustard greens. In Japanese cuisine, it is known as \"takana\" and often pickled for use as filling in onigiri or as a condiment. Many varieties of \"B. juncea\" cultivars are used, including \"zha cai\", \"mizuna\", \"takana\" (var. \"integrifolia\"), \"juk gai choy\", and \"xuelihong\". Asian mustard greens are most often stir-fried or pickled. A Southeast Asian dish called \"asam gai choy\" or \"kiam chai boey\" is often made with leftovers from a large meal. It involves stewing mustard greens with tamarind, dried chillies and leftover meat on the bone. Brassica juncea is also known as \"gai choi\", \"siu gai choi\", \"xaio jie cai\", baby mustard, Chinese leaf mustard or \"mostaza\".\n\nVegetable growers sometimes grow mustard as a green manure. Its main purpose is to act as a mulch, covering the soil to suppress weeds between crops. If grown as a green manure, the mustard plants are cut down at the base when sufficiently grown, and left to wither on the surface, continuing to act as a mulch until the next crop is due for sowing, when the mustard is dug in. In the UK, mustard sown in summer and autumn is cut down starting in October. April sowings can be cut down in June, keeping the ground clear for summer-sown crops. One of the disadvantages of using mustard as a green manure is its propensity to harbor club root.\n\nThis mustard plant is used in phytoremediation to remove heavy metals, such as lead, from the soil in hazardous waste sites because it has a higher tolerance for these substances and stores the heavy metals in its cells. In particular, \"Brassica juncea\" was particularly effective at removing cadmium from soil. The process of removing heavy metals ends when the plant is harvested and properly discarded. Phytoremediation has been shown to be cheaper and easier than traditional methods for heavy metal reduction in soils. In addition, it has the effect of reducing soil erosion, reducing cross-site contamination.\n\n\n\n"}
{"id": "9634115", "url": "https://en.wikipedia.org/wiki?curid=9634115", "title": "Business informatics", "text": "Business informatics\n\nBusiness informatics (BI) or organizational informatics is a discipline combining information technology (IT), informatics and management concepts. The BI discipline was created in Germany, from the concept of Wirtschaftsinformatik. It is an established academic discipline including bachelor, master, diploma and PhD programs in Austria, Belgium, France, Germany, Ireland, The Netherlands, Russia, Sweden, Switzerland, Turkey and is establishing in an increasing number of other countries as well as Australia, Malaysia, Poland or Mexico. BI integrates core elements from the disciplines of business administration, information systems and computer science into one field.\n\nBI shows similarities to information systems (IS), which is a well established discipline originating from North America. However, there are a few differences that make business informatics a unique own discipline:\n\nInformation systems (IS) focuses on empirically explaining phenomena of the real world. IS has been said to have an \"explanation-oriented\" focus in contrast to the \"solution-oriented\" focus that dominates BI. IS researchers make an effort to explain phenomena of acceptance and influence of IT in organizations and the society applying an empirical approach. In order to do that usually qualitative and quantitative empirical studies are conducted and evaluated. In contrast to that, BI researchers mainly focus on the creation of IT solutions for challenges they have observed or assumed.\n\nTight integration between research and teaching following the Humboldtian ideal is another goal in business informatics. Insights gained in actual research projects become part of the curricula quite fast because most researchers are also lecturers at the same time. The pace of scientific and technological progress in BI is quite rapid, therefore subjects taught are under permanent reconsideration and revision. In its evolution, the BI discipline is fairly young. Therefore, significant hurdles have to be overcome in order to further establish its vision.\n\n"}
{"id": "765313", "url": "https://en.wikipedia.org/wiki?curid=765313", "title": "CHILL", "text": "CHILL\n\nIn computing, CHILL (an acronym for CCITT High Level Language) is a procedural programming language designed for use in telecommunication switches (the hardware used inside telephone exchanges). The language is still used for legacy systems in some telecommunication companies and for signal box programming.\n\nThe CHILL language is similar in size and complexity to the original Ada language. The first specification of the CHILL language was published in 1980, a few years before Ada.\n\nITU provides a standard CHILL compiler.\nA free CHILL compiler was bundled with GCC up to version 2.95, however, was removed from later versions. An object-oriented version, called Object CHILL, was developed also.\n\nITU is responsible for the CHILL standard, known as ITU-T Rec. Z.200. The equivalent ISO standard is ISO/IEC 9496:2003. (The text of the two documents is the same). In late 1999 CCITT stopped maintaining the CHILL standard.\n\nCHILL was used in systems of Alcatel System 12 and Siemens EWSD, for example.\n\n\n"}
{"id": "4653959", "url": "https://en.wikipedia.org/wiki?curid=4653959", "title": "Cheetah Marketing", "text": "Cheetah Marketing\n\nCheetah Marketing was a United Kingdom-based company that produced electronic music-related hardware products and software for home computer systems during the 1980s. They later changed their name to Cheetah International Ltd.\n\nBased in Cardiff, Cheetah was run by two brothers, Howard and Michael Jacobson, but owned by Cannon Street Investments. The company was closed in 1993 when the UK recession badly hit the share price of its owners. After this Chris Wright and Nick Owen bought the music products division and formed Soundscape Digital Technology Ltd. The joysticks and other computer peripheral products division went to another company in the Cannon Street group.\n\nThe company originally produced joysticks like the infrared R.A.T. for the Commodore 64 and Sinclair ZX Spectrum computers and later branched out into music peripherals and stand-alone musical equipment for price conscious home users.\n\nAmong their offerings were the SpecDrum (a sample-based drum machine), a Cheetah Sound Sampler, a Cheetah Midi Interface, and in the later, 8-bit/16-bit drum machines, music sequencer, and a range of music keyboards (including polyphonic analog / digital synthesizers and rack mount modules).\n\nJoysticks and peripherals included the Cheetah 125, Cheetah 125 Plus, Mach 1, and an infrared joypad.\n\nCheetah's range of music products expanded quickly during the 1980s when they began to work with external designers. Among these were Chris Wright, who later founded Soundscape Digital Technology, Ian Jannaway, who later founded Novation Digital Music Systems and Mike Lynch, who later founded Autonomy Corporation.\n\nCheetah also distributed the Gamate handheld console in the UK.\n\n"}
{"id": "2272298", "url": "https://en.wikipedia.org/wiki?curid=2272298", "title": "Coa de jima", "text": "Coa de jima\n\nA coa de jima or coa (\"hoe for harvesting\", \"hoe\") is a specialized tool for harvesting agaves. \n\nIt is a long, machete-like round-ended knife on a long wooden handle used by a jimador to cut the leaves off an agave being harvested and to cut the agave from its roots. The core (or \"heart\") left, called \"piña\" (\"pineapple\"), is used for the production of mezcal, sotol or tequila. \n\nThe shape of the coa is adapted for the efficiency of carrying out these operations.\n"}
{"id": "1384972", "url": "https://en.wikipedia.org/wiki?curid=1384972", "title": "David Greenglass", "text": "David Greenglass\n\nDavid Greenglass (March 2, 1922 – July 1, 2014) was an atomic spy for the Soviet Union who worked on the Manhattan Project. He was briefly stationed at the Clinton Engineer Works uranium enrichment facility at Oak Ridge, Tennessee, and then worked at the Los Alamos laboratory in New Mexico from August 1944 until February 1946. \n\nHe provided testimony that helped convict his sister and brother-in-law Ethel and Julius Rosenberg, who were executed for their spying activity. Greenglass served nine and a half years in prison.\n\nGreenglass was born in 1922 in the Lower East Side of Manhattan in New York City. His parents, Barnet and Tessie, were Jewish immigrants from Russia and Austria, respectively. He attended Haaren High School, and graduated in 1940. He attended Brooklyn Polytechnic Institute but did not graduate.\n\nGreenglass married Ruth Printz in 1942, when she was 18 years old. The two joined the Young Communist League shortly before Greenglass entered the U.S. Army in April 1943. They had a son and a daughter. He worked as a machinist at Fort Ord, California, and then at the Mississippi Ordnance Plant in Jackson, Mississippi. In July 1944, Greenglass was assigned to the secret Manhattan Project, the wartime project to develop the first atomic weapons. He was first stationed at the Clinton Engineer Works uranium enrichment facility at Oak Ridge, Tennessee, but was there for less than two weeks. In August 1944 he was sent to the Los Alamos laboratory in New Mexico. In order to pass his security clearance, he disguised or omitted details of his communist associations, and had friends write glowing references.\n\nJulius Rosenberg, who had married Greenglass' sister, Ethel, in 1939, had become an agent for the Soviet Union (USSR), working under Alexander Feklisov. In September 1944, Feklisov suggested to Rosenberg that he should consider recruiting his brother-in-law, David Greenglass, and his wife. On September 21, 1944, Feklisov reported to Moscow: \"They are young, intelligent, capable, and politically developed people, strongly believing in the cause of communism and wishing to do their best to help our country as much as possible. They are undoubtedly devoted to us.\" David wrote to his wife, Ruth: \"My darling, I most certainly will be glad to be part of the community project [espionage] that Julius and his friends [the Russians] have in mind.\"\n\nAfter Julius Rosenberg recommended his sister-in-law Ruth Greenglass to his NKVD superiors for the use of her apartment as a safe house for photography, the NKVD realized that David was working on the Manhattan Project. He was then recruited into Soviet espionage by Ruth at Rosenberg's behest in November 1944. Greenglass began to pass nuclear secrets to the USSR via the courier Harry Gold, and more directly with a Soviet official in New York City. \n\nAccording to the Venona project intercepts decrypted by the National Security Agency between 1944 and some time in the 1970s, Greenglass and his wife Ruth were given code names. David was codenamed \"KALIBR\" (\"calibre\") and Ruth \"OSA\" (\"wasp\"). \n\nGreenglass turned down requests from the Los Alamos Laboratory (and Rosenberg) to work on the Operation Crossroads nuclear tests at Bikini Atol because he wanted to be with Ruth. He was honorably discharged from the Army on February 29, 1946. Greenglass returned to Manhattan, where, with his brother Bernie, and Julius Rosenberg, he ran a small machine shop known as G & R Engineering.\n\nOn February 14, 1950, Ruth, who was pregnant with their second child, came too close to the gas heater in their Lower East Side apartment, and her nightgown caught on fire. Greenglass extinguished the blaze, but she suffered severe burns. She was taken to Gouverneur Hospital for skin grafts. He suffered second degree burns to his right hand. He was already aware that the UK and US intelligence agencies had discovered that a Los Alamos theoretical physicist, Klaus Fuchs, had spied for the USSR during the war. \n\nThrough Fuchs' confession, they found that one of his American contacts had been a man named Harry Gold from Brooklyn, New York. Gold had passed Fuchs' information on to a Soviet agent, performing the role of courier, and Anatoli Yakovlev would then pass the information on to his controllers in the USSR. Through Gold, the FBI's trail led to Greenglass and the Rosenbergs, who had allegedly also used Gold as a courier. When Fuchs was first captured, Julius allegedly gave the Greenglasses $5,000 to finance an escape to Mexico. Instead, they went to the Catskills and used the money to seek legal advice.\n\nDavid Greenglass was arrested by the FBI for espionage in June 1950 and quickly implicated Julius Rosenberg. He explicitly denied his sister Ethel's involvement when he testified before a grand jury in August 1950. In February 1951, weeks before the trial, he changed his testimony to claim that Ethel had typed up his notes. He testified against his sister and her husband in court in 1951 as part of an immunity agreement. In exchange for that testimony, the government allowed Ruth to stay with their two children. She was named a co-conspirator, but was never indicted. Greenglass told the court, \"I had a kind of hero worship there with Julius Rosenberg and I did not want my hero to fail ...\"\nDuring subsequent testimony in 1951, Greenglass related in detail the secrets he passed on to the Soviet Union. He falsely attributed passing the cross-section drawing of the Atom Bomb to the Soviets to Julius and he also acknowledged passing other sketches through Gold. He described his work on the molds into which were poured the component of the explosive lenses of the Fat Man bombs used for the Trinity nuclear test and in the bombing of Nagasaki. At first this was a matter of difficulty for the prosecution, who wanted Greenglass to testify in open court about the secrets he had given—something which would by definition make them no longer \"secret\".\n\nThe Atomic Energy Commission decided that the implosion concept could be declassified for the trial, and limited all discussion to the weapons used in World War II (fearing that Greenglass may have seen prototypes for future weapons while at Los Alamos). As a result of a surprise defense motion that all testimony about the alleged \"secret of the atomic bomb\" be impounded, Federal Judge Irving Kaufman at first made all spectators and news reporters leave the room when Greenglass began testifying about his \"secrets\".\nTen minutes later, Judge Kaufman invited the news reporters back in, asking them to use their discretion in reporting on Greenglass's testimony. The Rosenbergs' defense attorney, Emanuel H. Bloch, attempted to convince the jury that his clients were concerned about issues of national security, but failed. Greenglass' testimony, later seen to be crude and in the words of many scientists who examined it \"worthless\", remained sealed until 1966. He also testified that Rosenberg had stolen and given to the Russians a proximity fuze.\n\nHowever, Aleksander Feklisov also claimed that Julius Rosenberg supplied him with a whole proximity fuze, which would corroborate at least this part of Greenglass' testimony. During the trial, Bloch claimed Greenglass wanted revenge for the machine shop business failure. Bloch attempted to discredit Greenglass' character and testimony. At Greenglass' sentencing hearing, his attorney O. John Rogge repeatedly told the court his client deserved \"a pat on the back\" for his testimony and argued that a light sentence, no more than five years, would encourage others to follow his example. Greenglass was sentenced to 15 years in prison. He was released after nine and a half years and reunited with his wife.\n\nIn March 1953, three months before the Rosenbergs' executions, he wrote a letter for his attorney to deliver to President Eisenhower asking for their sentences to be commuted to prison terms so that they would have an opportunity to confess. He wrote: \"if these two die, I shall live the rest of my life with a very dark shadow on my conscience\". He described his own testimony as \"an act of contrition for the wrong I had done my country, my family and myself\" and explained how he now viewed its consequences: \"Here I had to take the choice of hurting someone dear to me, and I took it deliberately. I could not believe that this would be the outcome. May God in His mercy change that awful sentence.\" That same month he admitted he had stolen a few ounces of uranium-238 from a bomb laboratory at Los Alamos years before and had tossed it into the East River in 1950 after he first denied having stolen it.\n\nAfter his release in 1960, Greenglass and his family lived in New York City under an assumed name. For some years they lived on 228th Street in Laurelton, Queens, New York. In 1996, Greenglass recanted his sworn testimony in an interview with \"The New York Times\" reporter Sam Roberts and stated he had lied under oath about the extent of his sister's involvement in the spying plot in order to protect his wife. At the trial, Greenglass had testified that Ethel Rosenberg typed his notes to give to the Russians. However, in the Roberts interview, he stated, \"I frankly think my wife did the typing, but I don't remember ... My wife is more important to me than my sister. Or my mother or my father, O.K.? And she was the mother of my children.\" When Roberts asked Greenglass if he would have done anything differently, he replied, \"Never.\" \n\nThe role of Ethel Rosenberg in her husband's espionage ring remains a matter of dispute.\n\nIn 2008, when a group of academic historians sought the release of the transcripts of the grand jury proceedings that indicted the Rosenbergs, Greenglass objected to the government's release of his testimony. U.S. District Judge Alvin Hellerstein declined to order the release of the testimony of Greenglass and other surviving witnesses who withheld their consent or could not be located. \n\nThe grand jury testimony was finally released in July 2015. Greenglass never mentioned involvement by his sister in Rosenberg's delivery of atomic secrets to the Russians.\n\nDavid Greenglass died on July 1, 2014. He was predeceased by his wife, Ruth, who died on April 7, 2008. His death was not publicly announced by his family and was only discovered on October 14, 2014, when \"The New York Times\" called the nursing home where he had been living under an assumed name.\n\n\n\n"}
{"id": "12686162", "url": "https://en.wikipedia.org/wiki?curid=12686162", "title": "Deepwater Horizon", "text": "Deepwater Horizon\n\nDeepwater Horizon was an ultra-deepwater, dynamically positioned, semi-submersible offshore drilling rig owned by Transocean. Built in 2001 in South Korea by Hyundai Heavy Industries, the rig was commissioned by R&B Falcon (a later asset of Transocean), registered in Majuro, and leased to BP from 2001 until September 2013. In September 2009, the rig drilled the deepest oil well in history at a vertical depth of and measured depth of in the Tiber Oil Field at Keathley Canyon block 102, approximately southeast of Houston, in of water.\n\nOn 20 April 2010, while drilling at the Macondo Prospect, an uncontrollable blowout caused an explosion on the rig that killed 11 crewmen and ignited a fireball visible from away. The fire was inextinguishable and, two days later, on 22 April, the \"Horizon\" sank, leaving the well gushing at the seabed and causing the largest oil spill in U.S. waters.\n\n\"Deepwater Horizon\" was a fifth-generation, RBS-8D design (i.e. model type), deepwater, dynamically positioned, column-stabilized, semi-submersible mobile offshore drilling unit, designed to drill subsea wells for oil exploration and production using an , blowout preventer, and a outside diameter marine riser.\n\n\"Deepwater Horizon\" was the second semi-submersible rig constructed of a class of two, although \"Deepwater Nautilus\", its predecessor, is not dynamically positioned. The rig was and capable of operating in waters up to deep, to a maximum drill depth of . In 2010 it was one of approximately 200 deepwater offshore rigs capable of drilling in waters deeper than . Its American Bureau of Shipping (ABS) class notations were \"A1, Column Stabilized Drilling Unit, AMS, ACCU, DPS-3\".\n\nIn 2002, the rig was upgraded with \"e-drill\", a drill monitoring system whereby technical personnel based in Houston, Texas, received real-time drilling data from the rig and transmitted maintenance and troubleshooting information.\n\nAdvanced systems played a key role in the rig's operation, from pressure and drill monitoring technology, to automated shutoff systems and modelling systems for cementing. The OptiCem cement modelling system, used by Halliburton in April 2010, played a crucial part in cement slurry mix and support decisions. These decisions became a focus for investigations into the explosion on the rig that month.\n\n\"Deepwater Horizon\" was built for R&B Falcon (which later became part of Transocean) by Hyundai Heavy Industries in Ulsan, South Korea. Construction started in December 1998, the keel was laid on 21 March 2000, and the rig was delivered on 23 February 2001, after the acquisition of R&B Falcon by Transocean. Until 29 December 2004 the rig was registered in the Republic of Panama.\n\nTransocean, through its Steinhausen, Switzerland subsidiary Triton Asset Leasing GmbH, operated the rig under the Marshallese flag of convenience. The rig was leased to BP on a 3-year contract for deployment in the Gulf of Mexico following construction. The lease was renewed in 2004 for a year, 2005 for 5 years, and 2009 for 3 years covering 2010 to 2013. The last contract was worth $544 million, or $496,800 a day, for a \"bare rig\", with crew, gear and support vessels estimated to cost the same.\n\nAccording to R&B Falcon's filings to SEC in 2001, the transfer document between R&B Falcon and Transocean was dated 17 August 2001, and the rig was specified as \"official registration number of 29273-PEXT-1, IMO number of 8764597, with gross tonnage of 32,588 and net tonnage of 9,778\" and the transfer value as . , the rig was insured for covering the replacement cost and wreckage removal.\n\n\"Deepwater Horizon\" worked on wells in the Atlantis (BP 56%, BHP Billiton 44%) and Thunder Horse (BP 75%, ExxonMobil 25%) oil fields. It was described at times as a \"lucky\" and \"celebrated\" rig, and in 2007 was still described as \"one of the most powerful rigs in the world\". In 2006 it discovered oil in the Kaskida oil field, and in 2009 the \"giant\" Tiber field. The well in the Tiber field has a vertical depth of and a measured depth of , below of water. The well was the deepest oil well in the world, and more than further below the seabed than the rig's official drilling specification stated on the company's fleet list.\n\nIn February 2010, \"Deepwater Horizon\" commenced drilling an exploratory well at the Macondo Prospect (Mississippi Canyon Block 252), about off the southeast coast of Louisiana, at a water depth of approximately . The Macondo prospect exploration rights were acquired by BP in 2009, with the prospect jointly owned by BP (65%), Anadarko (25%) and MOEX Offshore 2007 (10%). \"Deepwater Horizon\" was still working on the Macondo site on 20 April 2010, when a violent explosion occurred leading to destruction of the rig and resulting oil spill. The well was in the final stages of completion at the time; its cement casing was injected and hardening, and the rig was due to move shortly to its next role as a semi-permanent production platform at the Tiber site followed by a return to the Kaskida field; squatting semi-permanently over an oil dome 50 miles off the coast of Louisiana. The exploratory work was described as \"concluded\" and permission had already been requested from MMS to terminate operations at the Macondo site.\n\nDuring its operational lifetime, the rig was actively in operation for 93% of its working life (2,896 of 3,131 days). The remainder partly relates to time spent between sites.\n\nThe Minerals Management Service (renamed on 18 June 2010 to the Bureau of Ocean Energy Management, Regulation and Enforcement, or Bureau of Ocean Energy (BOE)) is the regulatory and inspecting body for offshore oil drilling and rigs in the United States of America. According to an \"Associated Press\" investigation, certain safety documentation and emergency procedure information, including documentation for the exact incident that later occurred, was absent. The exact number of required monthly inspections performed varied over time; the inspections were carried out as required for the first 40 months, but after that around 25% of inspections were omitted, although the investigation notes this is partly expected, since there are circumstances such as weather and movement which preclude an inspection. Reports of the last three inspections for 2010 were provided under Freedom of Information legislation. Each of these inspections had taken two hours or less.\n\nDuring its lifetime the rig received 5 citations for non-compliance, 4 of which were in 2002 (safety, including the blowout preventer) and the other in 2003 (pollution). A sixth citation in 2007 related to non-grounded electrical equipment was later withdrawn when the equipment was determined to be compliant with regulations. Overall the \"Deepwater Horizon\" safety record was \"strong\" according to a drilling consultant reviewing the information. In 2009 the Minerals Management Service \"herald[ed] the Deepwater Horizon as an industry model for safety\". According to AP's investigation \"its record was so exemplary, according to MMS officials, that the rig was never on inspectors' informal 'watch list' for problem rigs\".\n\nAt CDT on 20 April 2010, during the final phases of drilling the exploratory well at Macondo, a geyser of seawater erupted from the marine riser onto the rig, shooting into the air. This was soon followed by the eruption of a slushy combination of drilling mud, methane gas, and water. The gas component of the slushy material quickly transitioned into a fully gaseous state and then ignited into a series of explosions and then a firestorm. An attempt was made to activate the blowout preventer, but it failed. The final defense to prevent an oil spill, a device known as a blind shear ram, was activated but failed to plug the well.\n\nAt the time of the explosion, there were 126 crew on board; seven were employees of BP, 79 of Transocean, there were also employees of various other companies involved in the operation of the rig, including Anadarko, Halliburton and M-I SWACO. Eleven workers were presumed killed in the initial explosion. The rig was evacuated, with injured workers airlifted to medical facilities. After approximately 36 hours, \"Deepwater Horizon\" sank on 22 April 2010. The remains of the rig were located resting on the seafloor approximately deep at that location, and about (quarter of a mile) northwest of the well.\n\nThe resultant oil spill continued until 15 July when it was closed by a cap. Relief wells were used to permanently seal the well, which was declared \"effectively dead\" on 19 September 2010. \nTransocean received an early partial insurance settlement for total loss of the \"Deepwater Horizon\" of around 5 May 2010. Financial analysts noted that the insurance recovery was likely to outweigh the value of the rig (although not necessarily its replacement value) and any liabilities the latter estimated at up to .\n\nLitigation, ultimate roll call of damage, and the scope of final insurance recovery were all unknown , with analysts reporting that the aftermath was of unprecedented scale and complexity compared to previous disasters which themselves took many years to unfold and resolve. A July 2010 analysis by the \"Financial Times\" on the aftermath cited legal sources as saying that \"at some point the scale of the litigation becomes so large that it really is novel\", that \"the situation is likely to be complicated further because the variety of probable cases means it will be hard to aggregate them into so-called class actions\" and that there was \"no way to put this in historical context because we have never faced anything like this before\". As with the Exxon Valdez disaster, litigation was being discussed in terms of a 20-year timescale.\n\nIn January 2013, Transocean agreed to pay US$1.4 billion for violations of the US Clean Water Act. BP had earlier agreed to pay $2.4 billion but faced additional penalties that could range from $5 billion to $20 billion. In September 2014, Halliburton agreed to settle a large percentage of legal claims against them by paying $1.1 billion into a trust by way of three installments over two years. On 4 September 2014, U.S. District Judge Carl Barbier ruled BP was guilty of gross negligence and willful misconduct under the Clean Water Act (CWA). He described BP's actions as \"reckless,\" while he said Transocean's and Halliburton's actions were \"negligent.\" He apportioned 67% of the blame for the spill to BP, 30% to Transocean, and 3% to Halliburton. BP issued a statement strongly disagreeing with the finding, and saying the court's decision would be appealed.\n\nOn 8 December 2014, The US Supreme Court rejected BP's legal challenge to a compensation deal over the 2010 Gulf of Mexico oil spill. The settlement agreement had no cap, but BP initially estimated that it would pay roughly $7.8bn (£6.1bn) to compensate victims.\n\n\n"}
{"id": "14538761", "url": "https://en.wikipedia.org/wiki?curid=14538761", "title": "Disease diffusion mapping", "text": "Disease diffusion mapping\n\nDisease diffusion occurs when a disease is transmitted to a new location. It implies that a disease spreads, or pours out, from a central source. The idea of showing the spread of disease using a diffusion pattern is relatively modern, compared to earlier methods of mapping disease, which are still used today. According to Rytokonen, the goals of disease mapping are: 1) to describe the spatial variation in disease incidence to formulate an etiological hypothesis; 2) to identify areas of high risk in order to increase prevention; and 3) to provide a map of disease risk for a region for better risk preparedness.\n\nTorsten Hägerstrand’s early work on “waves of innovation” is the basis that many medical cartographers and geographers use for mapping spatial diffusion (1968). The diffusion of disease can be described in four patterns: expansion diffusion, contagious diffusion, hierarchal diffusion and relocation diffusion. Cromley and McLafferty also mention network diffusion and mixed diffusion.\n\nThe diffusion of infectious disease tends to occur in a ‘wave’ fashion, spreading from a central source. Pyle mentions barriers that pose a resistance towards a wave of diffusion, which include but are not limited to: physiographic features (i.e. mountains, water bodies), political boundaries, linguistic barriers, and with diseases, a barrier could be differing control programs. The diffusion of disease can be identified as a normal distribution over time and translated into an S-shaped curve to show the phases of disease diffusion. The phases are: Infusion (25th percentile), Inflection (50th percentile), Saturation (75th percentile), and Waning to the upper limits.\n\n\nThe value of mapping and Geographic Information Systems (GIS) is becoming better known to public health professionals to help link disease control to prevention efforts, which can aid in developing better immunization programs. GIS is an excellent tool used to identify spatial patterns and core areas of disease transmission. Disease maps can distinguish the low and high risk areas, as well as highlight “physical and/or socio-cultural” factors that contribute to the causation of disease. Understanding how a disease spreads gives health officials a better understanding of how to better serve the public.\n\n4. Rytkönen, Mika JP. “Not All Maps are Equal: GIS and Spatial Analysis in Epidemiology.” International Journal of Circumpolar Health 63:1, 2004: pp. 11 Available: http://www.circumpolarhealthjournal.net/index.php/ijch/article/viewFile/17642/20108\n\n"}
{"id": "49537551", "url": "https://en.wikipedia.org/wiki?curid=49537551", "title": "DuPont Fabros Technology", "text": "DuPont Fabros Technology\n\nDuPont Fabros Technology, Inc. (DFT) was a real estate investment trust that invested in carrier-neutral data centers and provided colocation and peering services. In 2017, the company was acquired by Digital Realty.\n\nAs of December 31, 2016, the company owned 11 operating data center facilities comprising over 3.3 million net rentable square feet. Eight of the properties were in Northern Virginia, two were in Elk Grove Village, Illinois, and one was in Santa Clara, California.\n\nThe company leased space to companies, on a wholesale level, in which such companies rented space to build their own data centers.\n\nThe company had 32 customers and derived 92% of its revenue from its 15 largest customers. The company's largest customers included Microsoft (25.4% of revenue), Facebook (20.2% of revenue), Rackspace (9.0% of revenue), and Yahoo! (6.0% of revenue).\n\nThe company was co-founded by Lammot J. du Pont, an analyst for JPMorgan Chase and Hossein Fateh, a real estate developer in the Washington metropolitan area.The company sought to acquire data centers that belonged to defunct internet service providers.\n\nIn 2004, the company's predecessor acquired 5 data centers from Savvis for $52 million in a leaseback transaction.\n\nIn 2005, the company's predecessor acquired a 230,000 square foot data center from AOL for $58.5 million.\n\nOn March 2, 2007, the company was incorporated as a real estate investment trust.\n\nIn October 2007, the company became a public company via an initial public offering that raised $640 million, the 7th largest initial public offering of a real estate investment trust at that time.\n\nIn early 2008, the company halted construction projects due to a lack of financing.\n\nIn 2009, the company was named as the fastest growing company in the Washington metro area by American City Business Journals.\n\nIn February 2011, Mohammed Mark Amin resigned from the board of directors and was replaced by John T. Roberts Jr.\n\nIn 2012, Hossein Fateh, the chief executive officer of the company, forgone his $450,000 salary in exchange for use of the company jet.\n\nIn 2012, the company reported that the volume of leasing was the largest in company history.\n\nIn May 2012, Mohammed Mark Amin, formerly a director of the company, was accused by the U.S. Securities and Exchange Commission of making a $618,000 profit as a result of insider trading in the company's securities.\n\nIn September 2014, the company opened a new data center in Ashburn, Virginia.\n\nIn February 2015, Christopher P. Eldredge was named chief executive officer of the company.\n\nIn March 2015, the company won the Brill Award For Data Center Design issued by Uptime Institute.\n\nIn March 2016, the company acquired a 46.7 acre parcel of land in Hillsboro, Oregon for $11.2 million.\n\nIn June 2016, the company sold a 38-acre data center in New Jersey to Quality Technology Services for $125 million.\n\nIn October 2016, the company acquired the former printing plant of the \"Toronto Star\" for C$54.25 million, with plans to convert it to a data center.\n\nIn May 2017, the company acquired a 56.5-acre undeveloped site in Mesa, Arizona with plans to construct a data center campus.\n\nIn September 2017, the company was acquired by Digital Realty.\n"}
{"id": "20078826", "url": "https://en.wikipedia.org/wiki?curid=20078826", "title": "Expansion tube", "text": "Expansion tube\n\nAn expansion tube is a type of impulse facility that is conceptually similar to a shock tube with a secondary diaphragm, an expansion section, a test section, and a dump tank where the endwall would be located in a shock tube. It is typically used to produce high enthalpy flows for high speed aerodynamic flow and aerodynamic heating and atmospheric reentry testing.\n"}
{"id": "32093678", "url": "https://en.wikipedia.org/wiki?curid=32093678", "title": "First-order reliability method", "text": "First-order reliability method\n\nThe first-order reliability method, (FORM), is a semi-probabilistic reliability analysis method devised to evaluate the reliability of a system.\n\nThe accuracy of the method can be improved by averaging over many samples, which is known as Line Sampling.\n\n\n"}
{"id": "56456244", "url": "https://en.wikipedia.org/wiki?curid=56456244", "title": "Furniture Museum", "text": "Furniture Museum\n\nThe Furniture Museum (), formerly known as the \"Norsk Møbelfaglig Senter\" 'Norwegian Furniture Design Center', is a museum and documentation center for the furniture industry in Norway. The museum is located in Aure in the municipality of Sykkylven, and it is part of the Sunnmøre Museum Foundation. The museum has a permanent display presenting the history of the furniture industry.\n\n"}
{"id": "212513", "url": "https://en.wikipedia.org/wiki?curid=212513", "title": "Good agricultural practice", "text": "Good agricultural practice\n\nGood agricultural practice (GAP) are specific methods which, when applied to agriculture, create food for consumers or further processing that is safe and wholesome. While there are numerous competing definitions of what methods constitute good agricultural practice there are several broadly accepted schemes that producers can adhere to.\n\nThe Food and Agricultural Organization of the United Nations (FAO) uses good agricultural practice as a collection of principles to apply for on-farm production and post-production processes, resulting in safe and healthy food and non-food agricultural products, while taking into account economical, social and environmental sustainability.\n\nGAPs may be applied to a wide range of farming systems and at different scales. They are applied through sustainable agricultural methods, \n\nGAPs require maintaining a common database on integrated production techniques for each of the major agro-ecological area (see ecoregion), thus to collect, analyze and disseminate information of good practices in relevant geographical contexts.\n\nThe United States Department of Agriculture Agricultural Marketing Service currently operates an audit/certification program to verify that farms use good agricultural practice and/or good handling practice. This is a voluntary program typically utilized by growers and packers to satisfy contractual requirements with retail and food service buyers. The program was implemented in 2002 after the New Jersey Department of Agriculture petitioned USDA-AMS to implement an audit based program to verify conformance to the 1998 Food & Drug Administration publication entitled, \"Guide to Minimize Microbial Food Safety Hazards for Fresh Fruits and Vegetables.\"\n\nThe program has been updated several times since 2002, and includes additional certification programs such as commodity specific audit programs for mushrooms, tomatoes, leafy greens, and cantaloupes. In 2009, USDA-AMS participated in the GAPs Harmonization Initiative which \"harmonized\" 14 of the major North American GAP audit standards, which in 2011 resulted in the release and implementation of the Produce GAPs Harmonized Food Safety Standard.\n\nDemand for agricultural crops is expected to double as the world's population reaches 9.1 billion by 2050. Increasing the quantity and quality of food in response to growing demand will require increased agricultural\n. Good agricultural practices, often in combination with effective input use, are one of the best ways to increase smallholder productivity. Many agribusinesses are building sustainable supply chains to increase production and improve quality.\n\n\n\n\n\n\n\n"}
{"id": "55664545", "url": "https://en.wikipedia.org/wiki?curid=55664545", "title": "History of computer hardware in Bulgaria", "text": "History of computer hardware in Bulgaria\n\nThis article describes the history of computer hardware in Bulgaria. At its peak, Bulgaria supplied 40% of the computers in COMECON. The electronics industry employed 300,000 workers, and it generated 8 billion rubles a year. Since the democratic changes in 1989 and the subsequent chaotic political and economic conditions, the once blooming Bulgarian computer industry almost completely disintegrated.\n\nIn the 1980s, Bulgaria manufactured computers according to an agreement within the COMECON:\n\nIZOT series and ES EVM series (abbreviation from Edinnaya Sistema Elektronno Vichislitelnih Machin, or Unified Computer System — created in 1969 by USSR, Bulgaria, Hungary, GDR, Poland and Czechoslovakia).\n\n\nFor example, the Pravetz-8M featured two processors (primary: Bulgarian-made clone of 6502, designated SM630 at 1.018 MHz, secondary: Z80A at 4 MHz), 64 KB DRAM and 16 KB EPROM.\n\nThe largest computer factory was some from Sofia, in Pravetz. Another big facility was the plant \"Electronika\" in Sofia. Smaller plants throughout the country produced monitors and peripherals, notably DZU (\"Diskovi Zapametyavashti Ustroistva\" — Disk Memory Devices) — Stara Zagora made hard disks for mainframes and personal computers.\n\n\n"}
{"id": "15756535", "url": "https://en.wikipedia.org/wiki?curid=15756535", "title": "Hydrodynamic separator", "text": "Hydrodynamic separator\n\nHydrodynamic separators (HDS) are stormwater management devices that use cyclonic separation to control water pollution. They are designed as flow-through structures with a settling or separation unit to remove sediment and other pollutants. HDS are considered structural best management practices (BMPs), and are used to treat and pre-treat stormwater runoff.\n\nHDS systems use the physics of flowing water to remove a variety of pollutants and are characterized by an internal structure that either creates a swirling vortex or plunges the water into the main sump. Along with supplemental features to reduce velocity, an HDS system is designed to separate floatables (trash, debris and oil) and settleable particles, like sediment, from stormwater. HDS systems are not effective for the removal of very fine solids or dissolved pollutants. The systems are also subject to scour and sediment washout during large storm events, e.g. a 10-year storm.\n\nA number of factors are relevant in selecting a hydrodynamic separator product for a site.\n\nHDS systems should be sized based on treatment objectives including desired level of pollutant removal, drainage basin characteristics, climate of the region, and particle size to be targeted. Performance is also sensitive to water temperature, i.e. season. Care must be taken to avoid routing excess flow through the device and compromising performance. Each vendor’s product has different pollutant removal rates that should be evaluated before selecting the system.\n\nThe TAPE and TARP programs are evaluation programs sponsored by several state agencies in the U.S. These programs include lab and field testing and provide specific sizing criteria for hydrodynamic separation systems.\n\nCurrently, the EWRI-ASCE and the ASTM International are developing comprehensive verification guidelines and standard test methods for assessing the performance of these devices.\n\nHDS systems are not maintenance-intensive, when compared with land-based BMP’s. Each manufactured system is different, therefore maintenance and inspection requirements should be looked at closely when purchasing an HDS system. Vacuum trucks are typically used for maintenance, so unobstructed access to accumulated pollutants for removal is critical.\n\nCosts for HDS systems depend on site-specific conditions such as land characteristics, amount of runoff to be treated, system depth and performance requirements. Be aware that not all HDS systems are alike in treatment performance, and basing a decision solely on the installation and operating cost of a system may compromise system performance and the environment. Long-term maintenance costs should also be considered with overall costs when purchasing or selecting a stormwater BMP as initial installation and operating costs may not reflect the long-term investment needed to maintain the system.\n\nAccording to the U.S. Environmental Protection Agency (EPA), “Using structural BMPs that can be placed underground and are design to withstand site specific soil, groundwater and traffic loading conditions provide valuable savings in land area compared to conventional volume-based stormwater treatment practices such as ponds, wetlands, and swales.” HDS systems may be ideal for areas where land is not readily available and/or tight retrofits are needed as they are installed underground.\n\nAs stormwater regulations become increasingly stringent, many states and municipalities have developed criteria to govern the use and sizing of HDS systems, and publish lists that identify acceptable HDS systems. Other jurisdictions evaluate the applicability of HDS on a site-specific basis. It is increasingly common to use HDS as the first component of a treatment train, a combination of BMPs in series, to remove coarse solids and floatable pollutants that can rapidly clog other BMPs thus prolonging their maintenance cycle.\n\n"}
{"id": "18381067", "url": "https://en.wikipedia.org/wiki?curid=18381067", "title": "KONČAR Group", "text": "KONČAR Group\n\nKONČAR – Elektroindustrija d.d. is an electrical, transport and energy company based in Zagreb, Croatia.\n\nListed on the Zagreb Stock Exchange, the company consists of some 20 dependent companies, employing a staff of 4,000. Annual sales account for €400 million, of which half are exports. During recent years KONČAR has delivered its products and plants to more than 100 countries across all continents.\n\nIn 2012 the most important KONČAR export market was Germany (11% of exports), followed by Sweden, Czech Republic, Bosnia and Herzegovina, Finland, Turkey, Serbia, Austria, Slovenia, Kosovo and Russia.\n\nKONČAR dates from 1921, when a modest but at the time highly significant manufacture of electrical motors commenced in Zagreb. The company is named after World War II resistance fighter Rade Končar. Company is listed on Zagreb Stock Exchange being constituent of CROBEX. The largest shareholder (28.17%) of company is Hrvatska poštanska banka (through Kapitalni fond).\n\nKONČAR - Catering Equipment Ltd. is an industrial company that operates within the KONČAR Group. The company was founded in 1946 and specializes in the design, manufacture, installation and servicing of professional catering equipment.\n\nKONČAR - Catering Equipment Ltd company offers a wide range of professional catering products based on \"turnkey system\".\n\nProducts include: cooking equipment, neutral equipment, refrigeration equipment of various sizes and purposes, professional dishwashers, special equipment for hospital kitchens, marine equipment, special equipment, Free-Flow System, etc.\n\nThe production lines are located in an industrial facility organized under the ISO 9001:2008 quality management system. All products are manufactured in accordance with relevant guidelines and standards of Republic of Croatia and European Union.\n\n"}
{"id": "38748108", "url": "https://en.wikipedia.org/wiki?curid=38748108", "title": "Kamakura Corporation", "text": "Kamakura Corporation\n\nKamakura Corporation is a global financial software company headquartered in Honolulu, Hawaii. It specializes in software and data for risk management for banking, insurance and investment businesses.\n\nThe company was founded in 1990 by its current CEO and Chairman Dr. Donald R. van Deventer, and as of 2018 Kamakura had served more than 330 clients in 47 countries. Cornell professor Robert A. Jarrow, co-creator of the Heath–Jarrow–Morton framework for pricing interest rate derivatives and the reduced form Jarrow–Turnbull credit risk models employed for pricing credit derivatives, serve as the company's Managing Director of Research.\n\nThe company has two primary products. Kamakura Risk Manager (KRM), an enterprise risk management system integrating credit risk management including IFRS 9 and CECL, market risk management, asset liability management, Basel II and Basel III and other capital allocation technologies, transfer pricing, and performance measurement. Kamakura Risk Information Services (KRIS) is a risk portal providing data for quantitative credit risk measures such as default probabilities, bond spreads, implied spreads and implied ratings for corporate, sovereign and bank counterparties. It also allows users to stress portfolios through Macro Factor Sensitivities and Portfolio Management tools. The Kamakura Troubled Company index measures the percentage of 39,000 public firms in 68 countries that have an annualized one- month default risk of over one percent. In January 2018 the company released its Troubled Bank Index.\n\n\n\n\n"}
{"id": "44424894", "url": "https://en.wikipedia.org/wiki?curid=44424894", "title": "Khurpa", "text": "Khurpa\n\nA khurpa is a short handled cutting tool with a flat blade used for digging soil and weeding in small gardens or vegetable farms. It is commonly use in small farms or in ridges or rows of vegetables to hoewing or earth up the weeds. It is used to cut branches of trees. It is traditionally used while in a squatting posture. Khurpa is word of Nepali language. \n\nIt is in Punjab and other areas in India for various processes like tilling, bed preparation, weeding and digging at small scale.\n"}
{"id": "423830", "url": "https://en.wikipedia.org/wiki?curid=423830", "title": "Konica Minolta", "text": "Konica Minolta\n\nKonica Minolta was formed by a merger between Japanese imaging firms Konica and Minolta, announced on January 7, 2003 with the corporate structure completing the re-organization in October 2003. Different group companies, such as the operations in the headquarters and national operating companies, began the process around the same time, however the exact dates vary for each group company.\n\nKonica Minolta uses a \"Globe Mark\" logo that is similar, however not identical to the logo of the former Minolta company. It also uses the same corporate slogan as the former Minolta company—\"The Essentials of Imaging\".\n\nOn January 19, 2006 the company announced that it was quitting the camera business due to high financial losses. SLR camera service operations were handed over to Sony starting on March 31, 2006 and Sony has continued development of cameras that are compatible with Minolta autofocus lenses. Originally, in the negotiations, Konica Minolta wanted a cooperation with Sony in camera equipment production rather than a sell-out deal, but Sony vehemently refused, saying that it would either acquire everything or leave everything that had to do with the camera equipment sector of KM. Subsequently, Konica Minolta withdrew from the photo business on September 30, 2006. Three thousand seven hundred employees were laid off.\n\nKonica Minolta closed down their photo imaging division in March 2007. The color film, color paper, photo chemical and digital mini-lab machine divisions have ceased operations. Dai Nippon Printing purchased Konica's Odawara factory, with plans to continue to produce paper under Dai Nippon's brand. CPAC acquired the Konica chemical factory.\n\nKonica expanded its business presence and currently sells its products in the Americas, Asia Pacific, Europe, Middle East, and Africa.\n\nMinolta had been a competitor in the 35 mm SLR market since the development of the manual-focus (MF) SRT and other models in the mid-1960s. Minolta positioned most of its cameras to compete in the amateur market, though it did produce a very high quality MF SLR in the XD-11. Minolta's last MF SLR cameras were the X370 and X700. Shanghai Optical Co. (Seagull) purchased tools and production plant from Minolta at different times, making some X300 series for Minolta branding, and continues to release MD mount film SLRs compatible with the old system under the Seagull name.\n\nUntil the sale of Konica Minolta's Photo Imaging unit to Sony in 2006, Konica Minolta produced the former Minolta range of 35 mm autofocus single-lens reflex cameras, variously named \"Minolta Maxxum\" in North America, \"Minolta Dynax\" in Europe, and \"Minolta Alpha\" in Japan and the rest of Asia. This range was introduced in 1985 with the Minolta Maxxum 7000, and culminated with the professional (1997) later made in a titanium body (9Ti) and technically advanced 7 (1999). The final Minolta 35 mm SLR AF cameras were the Maxxum 50 and 70 (Dynax 40 and 60), built in China.\n\nKonica Minolta had a line of digital point and shoot cameras to compete in the digital photography market. Their Dimage line (originally styled as Dimâge, later as DiMAGE) included digital cameras and imaging software as well as film scanners.\n\nThey created a new category of \"SLR-like\" cameras with the introduction of the DiMAGE 7 and DiMAGE 5. These cameras mixed many of the features of a traditional SLR camera with the special abilities of a digital camera. They had a mechanical zoom ring and electronic focus ring on the lens barrel and used an electronic viewfinder (EVF) showing 100 per cent of the lens view. They added many high level features such as a histogram and made the cameras TTL-compatible with Minolta's final generation of flashes for film SLRs. The controls were designed to be used by people familiar with SLR cameras, however the manual zoom auto-focus lens was not interchangeable. The model 5 had a 1/1.8-inch sensor with 3.3 megapixels, and the fixed zoom was equal to a 35–250 mm (relative to 24×36mm format). The Dimage 7, later 7i, 7Hi and A1 had 5-megapixel sensors for which the same lens provided 28–200 mm equivalent coverage. The later A2 and A200 increased the sensor resolution to 8 megapixels.\nThe Dimage 5 and 7 original models were more sensitive to infrared light than later models, which incorporated more aggressive IR sensor filters, so have become popular for infrared photography.\n\nThe A1/A2/A200 integrated a sensor-based, piezoelectrically actuated anti-camera-shake system. Before the closure of the Photo Imaging unit, the Dimâge lineup included the long-zoom Z line, the E/G lines (the G series finally incorporating former Konica models), the thin/light X line, and the advanced A line.\n\nThe DiMAGE G500 was a five-megapixel compact digital camera manufactured by Konica Minolta in 2003. It comes in a stainless steel case, 3x zoom lens with a retractable barrel, and dual Secure Digital and MagicGate card slots, the camera has a 1.3-second startup time.\n\nMinolta made some early forays into digital SLRs with the RD-175 in 1995 and the Minolta Dimâge RD 3000 in 1999 but were the last of the large camera manufacturers to launch a successful digital SLR camera using a current 35 mm AF mount in late 2004. The RD-175 was based on the Maxxum/Dynax 505si 35 mm film SLR and used three different ½-inch CCD image sensors—two for green and one for red and blue—supplied with images by a light splitting mechanism using prisms mounted behind the lens. The RD 3000 used Minolta V-mount APS format lenses and again used multiple CCDs—this time two 1.5 MP ½-inch sensors stitched to give a 2.7 MP output image.\nIt was not until late 2004 (after the merger with Konica) that they launched the Dynax/Maxxum/α 7D, a digital SLR based on the very successful Dynax/Maxxum 7 35 mm SLR body. The unique feature of this camera is that it features an in-body Anti-Shake system to compensate for camera shake. However, by 2004 Canon and Nikon had a whole range of digital SLR cameras and many serious photographers had already switched, thus leading Konica Minolta to withdraw from the market and transfer assets to Sony. The only two Konica Minolta digital SLRs to reach production before the company's withdrawal were the Dynax/Maxxum 7D and the Dynax/Maxxum 5D (which is an entry-level model that shared the 7D's sensor and Anti-Shake technology).\n\nIn early 2006 Sony announced its Sony α (Alpha) line of digital SLRs, (based on Konica Minolta technology) and stated they were scheduled to launch production in the summer of 2006. The Sony Alpha 100, announced on June 6, 2006, is generally agreed to have been a Konica Minolta design based on the 5D with minimal Sony input. The range of 21 Sony lenses announced at that time also included only revisions of earlier Minolta designs, or models which had been in development, rebadged and with minor cosmetic changes. The Sony Alpha DSLR range has remained compatible with all Minolta AF system lenses, and most accessories, from 1985 onwards. \n\nIn 2000 Minolta announced the introduction of Super Sonic Motor (SSM) focusing to a limited number of new lenses. This dispensed with a mechanical drive between camera and lens, but only SLRs made from 1999 onwards (the Dynax/Maxxum 7 and later) were compatible, the professional Dynax 9 requiring a factory upgrade to operate. Sony announced a program in 2008 to fit more future lenses with SSM and these designs may, therefore, not be compatible with 1985–1999 SLR bodies.\n\nFor some time after the merger between Konica and Minolta, both product lines continued to be sold, while research and development efforts were underway to create new products.\nThe first Konica Minolta badged products were almost entirely \"Konica\" or \"Minolta\" products however, as they were the next generation products being produced by both companies before the merger. These products included MFPs such as the Konica Minolta bizhub C350 (a \"Minolta\" design, also badged as the Konica 8022 and Minolta CF2203), and Konica Minolta 7235 (A \"Konica\" design).\n\nSuccessive models included greater integration between the two sets of technologies, and current products such as the bizhub C451 (pictured below in this article) contain many technologies from both histories. Some products such as the bizhub 501 are more noticeably an engine design from one company rather the other, however the system itself, including operation, features and RIP technologies are i\n\nAs the printer operations of the former Konica company were limited to \"printer models\" of MFP models, or re-badged printers from other manufacturers, while the printer operations of the former Minolta company were strong since the purchase of QMS (completed in 2000 after increasing influence and shareholding by Minolta), printer operations were initially not affected greatly by the Konica Minolta merger. In the 1980s QMS made the KISS laser printer, the most inexpensive then available at $1995.\n\nDue to the increased complexity of both MFP and printer devices, Konica Minolta increased technology sharing between the two lines of products. In many regions, this has led to the integration of the Printer products company into the Business equipment products company.\n\nKonica Minolta has spun off business units into separate companies.\n\nKonica Minolta Business Technologies Division engages in ventures in the fields of Technology (Office Mulitfunction, Print Production, 3D Printers, Wide Format etc), IT Services (IT consulting service solution) and Information Management (Graphic Communications, Automated Workflow Solutions, Business Process Automation etc).\n\nOffering document solutions for evolving office environments focused on colour, digitalisation, high-speed and networking.\n\nLocation of head office: Tokyo, Number of employees: Approx. 19,600\nWorldwide headquarters are also located in: Germany (Konica Minolta Europe), USA (Konica Minolta Business Solutions USA), New Zealand (Konica Minolta Business Solutions New Zealand), Australia (Konica Minolta Australia) and China (Konica Minolta China). These headquarters are responsible for sales and support of the Konica Minolta companies in each country within their region, including distributors and the dealer networks.\nMain products: MFPs, Copiers, printers, facsimile machines, microfilm systems and related supplies.\n\nPursuing advanced imaging markets Konica Minolta's digital multi-functional peripherals (MFPs), branded the \"bizhub\" series, are equipped with multiple functions (copying, printing, faxing, scanning), and can integrate into any corporate network environments. They allow users to consolidate the administration of office equipment connected to a network by using a series of network management software programs and even to manage and share both scanned data and computer-generated data.\n\nAdvanced generation of compact, lightweight and high-performance color laser printers. The market for color laser printers continues to expand, fuelled by the rapid shift of business documents from monochrome to color. Konica Minolta's color laser printers—branded the \"Magicolor\" series and using toner technology inherited from QMS/Qume include what was then the world's smallest and lightest color laser printer with 2400 dpi photographic quality, the Magicolor 2430DL of 2005. This printer also offered direct output from digital cameras using PictBridge and EXIFII Print Order Management technology, via USB. The Magicolor series covers from entry level home/office models like the 2430s successors, to large print stations for corporate environments.\n\nAs of May 2007 Printing Solutions (Europe) business was merged with Konica Minolta Business Solutions (Europe) as part of radical reforms within the company.\n\nKonica Minolta Opto, Inc. develops optical components, units, and systems.\n\nKonica Minolta Medical & Graphic, Inc. is involved in the manufacturing, sale, and related services of film and processing equipment for medical and graphic imaging. The company is located in Grand Rapids, MI, and manufactures and distributes both conventional and digital graphic arts supplies including: analog and digital films, graphics arts papers, conventional and CTP printing plates, processing chemicals, film and plate processors, imagesetters, platesetters, digital color proofers and software. The company serves the printing and publishing, corporate communications and newspaper industries.\n\nKonica Minolta Sensing offer products, software, and services utilizing light control and measurement technology within four main product areas: Color Measurement, Display Measurement, 3D measurement and Medical Measurement.\n\n\nOffering Digital Radiography, Ultrasound Imaging, Healthcare IT Solutions and Service-based solutions to hospitals, imaging centers, clinics and private practices. Headquartered in Wayne, NJ with facilities in NC, IL, FL and Brazil.\n\nKonica Minolta Healthcare Americas, Inc., formerly known as Konica Minolta Medical Imaging USA, Inc., is a business unit of Konica Minolta, Inc., with global headquarters in Tokyo, Japan. Konica Minolta Healthcare provides product and service based solutions across the USA, Canada and Latin America.\n\nIn 2012, Konica Minolta bought the Japanese operations of FedEx Kinko's. The deal consisted of the sale of 61 printing offices across Japan. Subsequently in 2013, Konica Minolta bought FedEx Kinko's operations in South Korea. The Kinko's operations in both countries were later rebranded to remove a reference to FedEx, but retained the Kinko's name.\n\nIn Japan, the Kinko's stores in Kyushu, Chugoku and Shikoku regions are continued to be operated by GA Creous, a subsidiary of General Asahi.\n\nKonica Minolta's sponsorships include:\n\n\n\n"}
{"id": "54812230", "url": "https://en.wikipedia.org/wiki?curid=54812230", "title": "Laser 50", "text": "Laser 50\n\nThe Laser 50 is an educational portable computer that ran the BASIC programming language released in 1984.\n\nThe Laser 50 used a Zilog Z80 central processing unit running at 3.5 MHz, 2 kB to 18 kB of RAM, a 12 kB ROM, and a 80x7 dots LCD screen.\n"}
{"id": "1203608", "url": "https://en.wikipedia.org/wiki?curid=1203608", "title": "Lawrence S. Coben", "text": "Lawrence S. Coben\n\nLawrence S. \"Larry\" Coben (born 1958) is an archaeologist who founded the Sustainable Preservation Initiative. He serves as Chairman of the Board of NRG Energy, the Fortune 200 integrated electricity company\n\nCoben is a consulting scholar and archaeologist at the University of Pennsylvania Museum of Archaeology and Anthropology and the founder and Executive Director of the Sustainable Preservation Initiative . His most recent work focuses on Inca imperial strategy and the archaeology of performance. He directs a multidisciplinary archaeological project in the Canete Valley of Peru and was director of a project at the monumental site of \"Incallajta\" in Bolivia. With Takeshi Inomata, he co-authored the book \"Archaeology of Performance: Theater, Power and Community\". Richard Schechner described this work as \"an important work integrating performance theory, forensics, and classical archaeology to describe and analyze not a \"dead past\" but pasts that continue to operate as rich repositories of living behaviors.\" Coben has published articles on the Inca, archaeological site museums, and the role of performance and spectacle in ancient society. Coben recently delivered a TED talk about alleviating poverty, empowering women and saving cultural heritage entitled \"Build Futures, Save Pasts\".\n\nHe also runs the aforementioned Sustainable Preservation Initiative (\"SPI). SPI preserves the world's cultural heritage by providing sustainable economic opportunities to poor communities where endangered archaeological sites are located. SPI believes the best way to preserve cultural heritage is creating or supporting locally owned businesses whose success is tied to that preservation. SPI's grants provide a TWO for ONE benefit: they provide transformative economic opportunities for local residents while saving sites for future generations to study and enjoy. SPI's paradigm, designed by Coben, has been suggested by the Milken Institute as an optimal solution to preserve and develop Israel's cultural heritage\n\nCoben is an expert member of the ICOMOS International Scientific Committee on Archaeological Heritage Management (\"ICAHM\"). He is Chairman of ICAHM's Nominations Assistance Committee and Vice Chairman of its Standards Board. He was recently named to the jury of the prestigious Cotsen Prize in Archaeology\n\nIn addition to his academic work in archaeology, Coben has started and run numerous energy companies. He is Chairman of the Board of NRG Energy, the Fortune 200 integrated electricity supplier. He is founder and CEO of Tremisis Energy Corporation and was CEO of Tremisis's two eponymous publicly traded affiliates. Coben serves on the Board of Freshpet, and was an Advisory Partner of Morgan Stanley Infrastructure Partners. Coben was one of the founders of Catalyst Energy Corporation, one of the nation's first alternative energy companies. Catalyst was #1 on the Inc. Magazine Fastest Growing Public Company List for the years 1982-1986. He served as chief executive officer of the New York Stock Exchange traded Bolivian Power Company, Ltd., Bolivia's largest private integrated electric generator and distributor. He was also a director of Prisma Energy and the Chilean utility SAESA, among other companies.\n\nCoben is also an advisor to several politicians and groups on energy policy. Ambassador Dick Swett and he wrote the national energy policy for Senator Joseph Lieberman's 2004 presidential campaign. He is a member of the Department of Homeland Security's Sustainability and Efficiency Task Force and Cleantech and Green Business for Obama.\n\nCoben writes the Larry Coben energy policy and cultural heritage blog for the Huffington Post, and his own blog Energizing America, both of which discuss major energy policy and cultural heritage issues and comment on related news from around the globe.\n\nCoben holds a BA in Economics from Yale University, a JD from Harvard Law School, and a Ph.D in Anthropology (Archaeology) from the University of Pennsylvania.\n\n\n"}
{"id": "1088297", "url": "https://en.wikipedia.org/wiki?curid=1088297", "title": "List of objects at Lagrangian points", "text": "List of objects at Lagrangian points\n\nThis is a list of known objects which occupy, have occupied, or are planned to occupy any of the five Lagrangian points of two-body systems in space.\n is the Lagrangian point located approximately 1.5 million km from Earth towards the Sun.\n\n\n\n is the Lagrangian point located approximately 1.5 million km from Earth in the direction opposite the Sun.\n\n\n\n\n is the Sun–Earth Lagrangian point located on the side of the Sun opposite Earth, slightly outside the Earth's orbit.\n is the Sun–Earth Lagrangian point located close to the Earth's orbit 60° ahead of Earth.\n\n is the Sun–Earth Lagrangian point located close to the Earth's orbit 60° behind Earth.\n\n\n\n\n\n\n\nAsteroids in the and Sun–Mars Lagrangian points are sometimes called \"Mars trojans,\" with a lower-case t, as \"Trojan asteroid\" was originally defined as a term for Lagrangian asteroids of Jupiter. They may also be called \"Mars Lagrangian asteroids.\"\n\nSource: Minor Planet Center \n\nAsteroids in the and Sun–Jupiter Lagrangian points are known as \"Jupiter Trojan asteroids\" or simply \"Trojan asteroids\".\n\n\n\n\n<div id=\"Neptune trojan\">\nMinor planets in the and Sun–Neptune Lagrangian points are called Neptune trojans, with a lower-case \"t\", as \"Trojan asteroid\" was originally defined as a term for Lagrangian asteroids of Jupiter.\n\n\n\nSource: Minor Planet Center \n\n"}
{"id": "47230389", "url": "https://en.wikipedia.org/wiki?curid=47230389", "title": "Lowry Solutions", "text": "Lowry Solutions\n\nLowry Solutions provides RFID services, barcode and wireless networking services. Lowry also provides bar coding equipment, automatic label applicators, software, custom labels, ribbons and supplies.\n\nHeadquartered in Brighton, MI, it is partnered with companies such as Zebra, Honeywell, Motion Computing, Panasonic and Paragon Labeling.\n\nIn 1974, Richard Lowry founded a business called Lowry & Associates. The company served as a Manufacturers' Rep Firm selling for Intel. Some of the items in its portfolio included single board computers, memory cards, power supplies, analog to digital I/O and Intel microprocessor development systems. Most of the computers in those days were used for test and measurement applications in R&D and manufacturing environments. Lowry & Associates had the exclusive regional rights to Intel systems products at that time.\n\nSpartan Stores in Grand Rapids, MI, was Lowry's first bar code sale. The company saw what Lowry's team was doing in the digital printing graphics environment and was asked by Spartan if they could provide them with a bar code printing solution. At the time, Lowry & Associates was reselling Printronix printers.\n\nBy 1986, Lowry sales grew to $19 million covering a four-state region. $11 million was in sales of computer peripherals and $8 million was in AIDC products and services. At that time, Lowry decided to spend more time and money on growing the AIDC side of the business. And, by 1990, Lowry Computer Products was making $19 million per year in just AIDC sales.\n\nBetween the years 1994 and 1996, Lowry grew from a regional company to national sales coverage by acquiring 4 regional VAR companies; two of those VAR Company's also had label conversion manufacturing capability. In 1996, Lowry consolidated all manufacturing operations into their 40,000 square foot plant in White Bear Lake, Minnesota.\n\nIn 1996 Lowry began manufacturing a line of label applicators and automated print and apply systems which they still sell today under the Paragon Brand name. They offer their customers a choice of Zebra, Sato and Datamax print engines. Paragon units are installed on a global basis for some of the largest Fortune 500 companies such as P&G, 3M, and many others.\n\nIn January 2014, Lowry Computers became Lowry Solutions.\n\nIn May 2015, Lowry Solutions was awarded the AIT-V Contract. It was one of three companies that will provide automatic identification technology data communications, software, hardware, documentation and associated services under a $181 million indefinite-delivery/indefinite quantity contract.\n"}
{"id": "17603472", "url": "https://en.wikipedia.org/wiki?curid=17603472", "title": "Lò trấu", "text": "Lò trấu\n\nThe lò trấu (\"rice husk stove\") is a type of versatile fuel burning cook stove used in Vietnam since the 1950s. \"Lò trấu\" comes from lò (stove) and trấu (rice husk). A kitchen with this kind of stove is a bếp trấu, \"husk kitchen.\"\n\nThe timeline of the development of the lò trấu is unclear, however it is known that the Lo Trau has been in use in Vietnam at least since the 1950s. The fixed version Lo Trau stove is thought to be strongly related to the \"Castrol stove\" design of the architect François de Cuvilliés in 1735 and similar European designs in the 1830s, with flue pipes connected to the chimney, oven holes into which concentric iron rings on which the pots were placed. Depending on the size of the pot or the heat needed, one could remove the inner rings. A recent innovation is the portable Lo Trau. Its compact design and efficient operation has been the target of a number of studies.\nRecently it has been distributed in the Negros area of the Philippines in The Southern Negros Sustainable Agriculture Demonstration Projects in the 1990s. This relatively late uptake of apparently long-used innovation means the potential of rice husk as fuel has been overlooked by many for a long time, as well as the obscurity of the Lo Trau designs to the wider world.\n\nThe stove is started by burning easily ignited material such as bundle of coconut leaves, newspaper partially embedded in the rice husks or other fuels in the combustion chamber area. After the fire is established, the updraft air from the stove designs will quickly sustain and magnify the combustion. The rate of combustion is regulated by removing appropriate amount of ash from under the combustion area with a poker, thus enable more fuel to enter.\n\nContrary to the name, the Lo Trau is quite versatile. It can burn wood chips, saw dust, small branches, small logs (with reduced heat controlling ability) and leaves.\n\nOpen fire has four major disadvantages: It is dangerous, it produces much smoke, soot blackens the cookware, and the heat efficiency is poor. The enclosed design of the Lo Trau means complete combustion of the fuel, better use of the heat that it generated and thus reduce the fuel consumption by its furnace-like burning generated by the updraft through the chimney. Due to its furnace-like operation, most fuel will burned thoroughly into fine ashes with almost no waste. \"In situ\" test findings showed that it took only 5 minutes to boil water using an approximate 180g of rice husk. This figure is impressive when compared to liquefied petroleum gas (LPG), fuel wood, and charcoal stoves, which required 5, 15, and 20 minutes respectively to boil the same amount of water. It should also be noted that at the present time both wood and charcoal fires are frequently ignited using kerosene, a cost that would be eliminated with the rice husk stove.\n\n\n"}
{"id": "1121651", "url": "https://en.wikipedia.org/wiki?curid=1121651", "title": "Man and Technics", "text": "Man and Technics\n\nMan and Technology: A Contribution to a Philosophy of Life is a short book by Oswald Spengler, in which the author discusses a critique of technology and industrialism. It was published as Der Mensch und die Technik in Munich in 1931.\n\nThe principal idea in the work is that many of the Western world's great achievements may soon become spectacles for our descendants to marvel at, as we do with the pyramids of Egypt or the baths of Rome. Spengler especially pointed to the tendency of Western technology to spread to hostile \"Colored races\" which would then use the weapons against the West. In Spengler's view, western culture will be destroyed from within by materialism, and destroyed by others through economic competition and warfare. \n\n, and \n\n"}
{"id": "8931952", "url": "https://en.wikipedia.org/wiki?curid=8931952", "title": "Manufacturing test requirement design specification", "text": "Manufacturing test requirement design specification\n\nManufacturing test requirement design specification (MTRD) is a document which specifies how testing is to be implemented on a new or modified manufactured product. The specification includes which tests will be applied to different build processes, the percentage of coverage for each stage of those build process, and how data on errors and problems will be gathered and processed. It would be expected to refer heavily to the organisation's Quality Management System (QMS). \n\nThe MTRD is usually developed by the manufacturing test engineers and presented to the program managers. The schedules and deliverables are then reviewed by engineering, quality and operations teams and the finalized document signed off.\n\n"}
{"id": "10568670", "url": "https://en.wikipedia.org/wiki?curid=10568670", "title": "Ministry of Communications and Information", "text": "Ministry of Communications and Information\n\nThe Ministry of Communications and Information (Abbreviation: MCI; ; Chinese: ) is a ministry of the Government of Singapore. It is in charge of information and communications technology, the media and design sectors, public libraries, as well as the Government's information and public communication policies.\n\nOn 5 June 1959, the Ministry of Culture came into being with the swearing-in and appointments of ministers of the new Government of Singapore. On 1 February 1980, the Broadcasting Division of the Ministry of Culture became a statutory board, the Singapore Broadcasting Corporation.\n\n1985 saw the dissolution of the Ministry of Culture. Its Information Division came under the new Ministry of Communications and Information (MCI). Its arts promotion component was assimilated into the Ministry of Community Development (MCD) as the Cultural Affairs Division.\n\nFive years later, on 28 November 1990, the Information Division of the MCI and the Cultural Affairs Division of MCD, together with other associated departments and statutory boards, reunited to form the Ministry of Information and the Arts (MITA).\n\nOn 1 September 1991, the Festival of Arts Secretariat, Singapore Cultural Foundation, the Arts Division of MITA, and the National Theatre Trust merged to form the National Arts Council (NAC).\n\nOn 1 October 1994, the Singapore Broadcasting Authority (SBA) was formed as a statutory board under MITA to oversee and promote the broadcasting industry in Singapore.\n\nOn 23 November 2001, the information and communications technology (ICT) functions under the Ministry of Communications and Information Technology came under MITA. The expanded Ministry was renamed the Ministry of Information, Communications and the Arts, but retained the acronym MITA. In that year, Infocomm Development Authority (IDA) became one of MITA's statutory boards.\n\nOn 1 January 2003, the Singapore Broadcasting Authority, Singapore Films Commission and Films and Publications Department (previously under the MITA headquarters) merged to form the Media Development Authority (MDA). On 13 August 2004, the Ministry's acronym was changed from \"MITA\" to \"MICA\".\n\nOn 1 November 2012, MICA was renamed the Ministry of Communications and Information (MCI). The move followed the restructuring of two previous ministries – MICA and the Ministry of Community Development, Youth and Sports (MCYS) – into MCI, the Ministry of Culture, Community and Youth (MCCY) and the Ministry of Social and Family Development (MSF). REACH (Reaching Everyone for Active Citizenry @ Home) was assimilated into MCI while the resilience, arts and heritage portfolios became part of MCCY. MCI now oversees the development of the information and communications technology, media and design sectors, public libraries, and the Government’s information and public communication policies.\n\nOn 18 January 2016, MCI announced that the Infocomm Development Authority of Singapore (IDA) and the Media Development Authority (MDA) will be restructured into two new entities: The Infocommunications Media Development Authority (IMDA) and the Government Technology Organisation (GTO), in the second half of 2016.\n\nMCI has 2 statutory boards:\n\n\nMCI also manages: \n\n\n"}
{"id": "48621885", "url": "https://en.wikipedia.org/wiki?curid=48621885", "title": "NDC Netzler &amp; Dahlgren Co AB", "text": "NDC Netzler &amp; Dahlgren Co AB\n\nNetzler & Dahlgren Co AB, or NDC for short, was a Swedish company founded in 1962 by Göran Netzler and Anders Dahlgren. The initial business idea was to build customized electronics equipment. Over the years, NDC evolved into a technology platform provider (navigation, hardware, software) for AGV (automated guided vehicle) builders. Danaher acquired NDC in 2001.\n\nNDC began producing specially built electronics equipment on a small scale for a variety of industries, for example, manufacturing and marine. The number of owners grew in the 1960s to include Ingvar Bergström, Arne Nilsson and Jan Jutander.\n\nAt the beginning of the 1970s, NDC had some 30 employees and started to attract big companies such as Getinge, Tetra Pak and Volvo. The first AGV project was for the Volvo Kalmar Assembly plant in 1972. NDC was involved in producing the AGV prototypes but was considered too small to take on the total project. They were, however, commissioned to deliver all the drives. At that time, the Volvo plant was a revolution in the automotive industry, introducing a totally new way of working.\n\nIn 1976, Tetra Pak installed twenty AGVs in their plant in Lund, Sweden. NDC provided the electronics, Tellus the mechanics and ErgoData the control system. The project was a success, and Tetra Pak began to introduce AGVs in their plants worldwide. NDC was involved in many AGV projects for Tetra Pak during the 1970s and 1980s, and saw its chance to expand the business into something more.\n\nIn the early 1980s, NDC made the strategically important decision to focus on creating a generic controls platform for AGV builders. Driven by the development of computer technology, it was now possible for NDC to offer the flexibility that AGV builders needed to create customized driverless vehicles. The new NDC business model was born: “Generic technology to be applied by others.”\n\nThe first partner was the French company Lamson Saunier Duval, and the number of partners quickly grew into two digits. NDC spent a lot of time educating partners on all the possibilities of generic technology.\n\nThe new business model paved the way for international expansion with NDC subsidiaries in Italy, the Czech Republic and the United States. The American NDC subsidiary, NDC Automation Inc., had its own subsidiary in Australia. NDC Automation Inc. went public on March 28, 1990, and subsequently changed its name to Transbotics in 2001.\n\nLaser technology took off in many industrial applications in the 1980s, and NDC started to explore the potential. In 1991, NDC introduced laser technology as AGV navigation for a Tetra Pak factory in Singapore. Tetra Pak appreciated the ease-of-change to new working patterns with laser navigation and began a worldwide plant upgrade to laser navigation. NDC named their navigation technology Lazerway.\n\nIn the 1990s, AGVs increased in complexity. By focusing on being a technology platform provider (navigation, hardware, software), NDC could stay strong in an AGV industry that had split into new categories such as mobile robots.\n\nAfter the Danaher acquisition in 2001, NDC was named Danaher Motion and later on transformed into the Kollmorgen brand. The NDC brand name was kept but is now the name of the technology platform: NDC Solutions.\n\nAs of July 2016, Kollmorgen is part of Fortive Corporation.\n"}
{"id": "26469223", "url": "https://en.wikipedia.org/wiki?curid=26469223", "title": "Ocean Traveler", "text": "Ocean Traveler\n\nOcean Traveler was a drilling platform built in the United States and used in the Gulf of Mexico. In 1966, it was transferred to Esso for the first exploration wells on the Norwegian continental shelf in the North Sea, following the Dutch discovery of Groningen gas field in 1959. On 16 July 1966, the platform did a limited discovery in the North Sea, in a block that much later became the discovery place of the Balder Field.\n\nOcean Traveler had big problems with the weather conditions in the North Sea, the weather being much tougher in the North Sea than the areas off the southern coast of the United States. These experiences laid the foundation for the sister platforms, especially the Norwegian-built \"Ocean Viking\", which had a significantly strengthened structure.\n\n"}
{"id": "46902608", "url": "https://en.wikipedia.org/wiki?curid=46902608", "title": "Outlit", "text": "Outlit\n\nOutlit is an online pay-per-view journalism and digital media virtual marketplace founded in 2014 in Arlington, Virginia. Outlit is advertisement-free for registered users and has full, native content from over 40 newspapers and magazines. Content is available by source or topic and all units of content come with a free preview. Full content can be accessed for free or through a system of micropayments. Journalists and bloggers may also publish content to sell directly to end users. Also called the \"iTunes of News,\" the company claims it is the largest collection of newspaper, magazine, and blog sources in a marketplace format in English. \n\nThe company's CEO and co-founder is Lucien Zeigler, a media consultant and economist. Zeigler has said that Outlit was founded in response to declining revenues for publishers and the prevalence of advertising in news media, as well as the \"inefficiency of the subscription business model\" for readers.\n\nOutlit is backed by an undisclosed angel investment secured in March 2015. \n"}
{"id": "10171759", "url": "https://en.wikipedia.org/wiki?curid=10171759", "title": "Promoting adversaries", "text": "Promoting adversaries\n\nPromoting adversaries refers to an unofficiated self-organizing tactical relationship between opposing organizations, in which both opposing sides benefit by attacking each other. The relationship typically relies on either side never fully defeating the other, because the whole time their 'conflict' helps both sides (while each side also simultaneously takes occasional 'acceptable' losses). Promoting adversaries requires neither side to be finally defeated throughout the relationship, because both sides actually prefer the relationship to continue and thus both sides to keep existing and fighting each other.\n\nPromoting adversaries works within a tendency where those opposed are increasingly polarized. When the tactic is used, it has the effect of making those involved in the relationship even more extreme than they were to begin with. Fundamentalist groups become more fanatical ... and nations, agencies, militaries & political parties become more repressive and authoritarian -- as the promoting adversaries drags on.\n\nIt is argued by some that the modern day guerrilla tactic of suicide bombing emerged from conditions in which one or more \"promoting adversaries\" relationships developed.\n\nIn George Orwell's book \"Nineteen Eighty-Four\", with typical or worse than typical command-economy style technological and industrial incompetence, the three remaining superpowers left in the world use high-intensity conventional total war against each other indefinitely. What reason, if any, they have for doing so is not quite clear. It could simply be for conquest and realpolitik in international relations. At first it appears to be for some sort of propaganda purpose, using the desperation and nationalism of the war to preserve some trace of persuasiveness of the propaganda in favor of their policies. However, that is not necessarily their actual strategy, because they have deliberately made their propaganda even more utterly unconvincing than it already is, and making sure that the population is constantly exposed to thorough and proficient, though still entirely obvious historical revision, and are exposed to introductions to Goldberg's political views for the sake of testing the population, making sure to provoke any potential dissidents into dissenting so that potential dissidents can be caught. The superpowers consider using nuclear weapons against each other's cities, not for retaliation, but for conquest, but decide to wait until they are \"ready\", which likely means that they do not yet consider it acceptable to finally destroy each other, perhaps because they are promoting adversaries.\n\nThis tactic is dynamically similar to certain publicity techniques, and so can be used by individuals and products seeking to gain/concentrate power or wealth as well.\n\nSome examples include:\n\nAs well as aspects of manufactured conflict for ratings purposes on many \"reality\" shows on TV...\n\n\"Promoting Adversaries\" has also been parodied most recently by Stephen Colbert on his show, The Colbert Report, in which Stephen's brand of Ben & Jerry's ice cream (\"AmeriCone Dream\") is pitted against Willie Nelson's brand of Ben & Jerry's ice cream (\"Country Peach Cobbler\"). Of course, this public 'conflict' generates advertising for both products, which are owned by the same company.\n\n\"Promoting adversaries\" is a similar concept to the term frenemy.\n\n"}
{"id": "3455617", "url": "https://en.wikipedia.org/wiki?curid=3455617", "title": "Rapid transit technology", "text": "Rapid transit technology\n\nRapid transit technology is technology used for public, mass rapid transit. Such transit is commonly known as a \"Metro\" or \"Subway\", and it has undergone significant changes in the years since the Metropolitan Railway opened publicly in London in 1863.\n\nSome urban rail lines are built to a loading gauge as large as that of main-line railways; others are built to smaller and have tunnels that restrict the size and sometimes the shape of the train compartments. One example is the London Underground which has acquired the informal term \"tube train\" due to its cylindrical cabin shape.\n\nThere are lines that use light rail rolling stock, such as trams or streetcars, that are simply routed into a tunnel, onto a viaduct, or other type of grade-separated alignment—for all or part of their route (as in Philadelphia—where the route is shared with full-size heavy-rail trains). Platforms at stations on these routes are apt to be built to accommodate various train sizes and not be optimal for any one size, accounting for a sizable \"gap\" between the train and the platform.\n\nIn many cities, such as Berlin and Boston, lines using different sizes and types of vehicles are organized into a single unified system. Although these are not often connected by track, in cases when it is necessary, rolling stock among different types of vehicles is compatible for non-revenue transfers or other purposes.\n\nAlthough initially the trains of what is now the London Underground were drawn by steam engines, virtually all metro trains, both now and historically, use electric power and are built to run as multiple units. Most trains ride on steel wheels running on two steel rails, as in a conventional railway, although some use other methods. Power for the trains, referred to as traction power, is commonly supplied by means of a single live third rail (as in New York). The current powering the trains is generally in the range of 600 to 750 volts, although some systems, such as the ones in London and Milan, use two live rails, one positive and one negative. The practice of sending power through rails on the ground is mainly due to the limited overhead clearance of tunnels, which physically prevents the use of overhead wires.\n\nThe use of overhead wires allows higher power supply voltages to be used. Although overhead wires are more likely to be used on metro systems without many tunnels, an example of which is the Shanghai Metro, overhead wires are employed on some systems that are predominantly underground, as in Barcelona, Fukuoka, Madrid, and Shijiazhuang. Lines that utilize street running while on the surface (such as Boston's Green Line) tend to derive power completely from overhead wires, both while traveling in a tunnel in the central city and at street level in the suburban areas, as a third rail would make the track unsafe for road vehicles, especially at level crossings (which are found on some lines of the Chicago 'L'). There are transit lines that make use of both rail and overhead power, with vehicles able to switch between the two (an example of this being in Rotterdam and formerly in Chicago).\n\nThe electric power is generally DC rather than AC, even though this requires large rectifiers. DC motors were formerly more efficient for railway applications, and once a DC system is in place, converting it to AC is generally considered infeasible.\n\nMost rapid transit systems use conventional standard gauge railway track. Since tracks in subway tunnels are not exposed to rain, snow, or other forms of precipitation, they are often fixed directly to the floor rather than resting on ballast, such as normal railway tracks (Amsterdam being an exception). Sections of the light rail system in San Diego, California operate on former railroad rights of way that were acquired by the local transit agency.\n\nAn alternate technology, using rubber tires on narrow concrete or steel roll ways, was pioneered on certain lines of the Paris Métro, and the first completely new system to use it was in Montreal, Canada. On most of these networks, additional horizontal wheels are required for guidance, and a conventional track is often provided in case of flat tires and for switching. There are also some rubber-tired systems that use a central guide rail, such as the Sapporo Municipal Subway and the NeoVal system in Rennes, France. Advocates of this system note that it is much quieter than conventional steel-wheeled trains, and allows for greater inclines given the increased traction of the rubber tires.\n\nSome cities with steep hills incorporate mountain railway technologies in their metros. One of the lines of the Lyon Metro includes a section of rack (cog) railway, while the Carmelit, in Haifa, is an underground funicular.\n\nFor elevated lines, another alternative is the monorail, which can be built either as supported or straddle-beam monorails (with a single rail below the train, including the Tokyo Monorail and certain Chongqing Rail Transit lines), or as a suspended monorail, where the train body hangs below the wheels and rail (a notable example being the Wuppertal Schwebebahn). While monorails have never gained wide acceptance outside Japan, there are some which are widely used. These include the Seattle Monorail, which consists of one short line dating from the World's Fair of 1962, which local voters have decided against expanding. Another recently built line operates in Las Vegas. An older system, also one of the first monorail systems in the United States, was installed in 1959 at Disneyland in California and connects the amusement park to a nearby hotel. The designer of the system, Walt Disney, once offered to build a similar system between Anaheim and Los Angeles.\n\nIn the early days of underground railways, at least two staff members were needed to operated each train: one or more attendants (also called \"conductor\" or \"guard\") to operate the doors or gates, as well as a driver (also called the \"engineer\" or \"motorman\"). The introduction of powered doors around 1920 permitted crew sizes to be reduced, and trains in many cities are now operated by a single person. Where the operator would not be able to see the whole side of the train to tell whether the doors can be safely closed, mirrors or closed-circuit TV monitors are often provided for that purpose.\nA replacement system for human drivers became available in the 1960s, with the advancement of computerized technologies for automatic train control and, later, automatic train operation (ATO). ATO could start a train, accelerate to the correct speed, and stop automatically in the correct position at the railway platform at the next station, while taking into account the information that a human driver would obtain from lineside or cab signals. The first metro line to use this technology in its entirety was London's Victoria line, opened in 1968. In normal operation, a crew member sits in the driver's position at the front, but is only responsible for closing the doors at each station. By pressing two \"start\" buttons the train would then move automatically to the next station. This style of \"semi-automatic train operation\" (STO), known technically as \"Grade of Automation (GoA) 2\", has become widespread, especially on newly built lines like the BART network in the San Francisco Bay Area.\n\nA variant of ATO, \"driverless train operation\" (DTO) or technically \"GoA 3\", is seen on some systems, as in London's Docklands Light Railway, which opened in 1987. Here, a \"passenger service agent\" (formerly called \"train captain\") would ride with the passengers rather than sit at the front as a driver would, but would have the same responsibilities as a driver in a GoA 2 system. This technology could allow trains to operate completely automatically with no crew, just as most elevators do. When the initially increasing costs for automation began to decrease, this became a financially attractive option for employers. At the same time, countervailing arguments stated that in an emergency situation, a crew member on board the train would have possibly been able to prevent the emergency in the first place, drive a partially failed train to the next station, assist with an evacuation if needed, or call for the correct emergency services (police, fire, or ambulance) and help direct them to the location where the emergency occurred. In some cities, the same reasons are used to justify a crew of two rather than one; one person drives from the front of the train, while the other operates the doors from a position farther back, and is more conveniently able to assist passengers in the rear cars. An example of the presence of a driver purely due to union opposition is the Scarborough RT line in Toronto.\n\nCompletely unmanned trains, or \"unattended train operation\" (UTO) or technically \"GoA 4\", are more accepted on newer systems where there are no existing crews to be displaced, and especially on light metro (medium-capacity) lines. One of the first such systems was the VAL (\"véhicule automatique léger\" or \"automated light vehicle\"), first used in 1983 on the Lille Metro in France. Additional VAL lines have been built in other cities (such as Toulouse, also in France, and Turin in Italy). Another system that uses unmanned trains is Bombardier's Innovia Metro, originally developed by the Urban Transportation Development Corporation as the Intermediate Capacity Transit System (ICTS). It was later used on the SkyTrain in Vancouver, British Columbia, which carries no crew members, and the Kelana Jaya Line in Kuala Lumpur, Malaysia. (The Scarborough RT uses identical trains.)\n\nSystems which use automatic trains also commonly employ full-height platform screen doors (PSDs, alternatively called platform edge doors or PEDs) or half-height automatic platform gates in order to improve safety and ensure passenger confidence, but this is not universal, as networks like Nuremberg do not, using infrared sensors instead to detect obstacles on the track. Conversely, some lines which retain drivers or manual train operation nevertheless use PEDs, notably London's Jubilee Line Extension and upcoming Crossrail heavy rail line. The first network to install PSDs on an already operational system was Hong Kong's MTR, followed by the Singapore MRT. Rapid transit systems in the United States do not use PEDs, except the monorail in Las Vegas and the under-construction Honolulu Rail Transit line, where their hot climates would make an uncontrolled outdoor waiting environment uncomfortable for passengers.\n\nAs for larger trains, the Paris Métro has human drivers on most lines but runs automated trains on its newest line, Line 14, which opened in 1998. The older Line 1 was subsequently converted to unattended operation by 2012, and it is expected that Line 4 will follow by 2019. The North East MRT Line in Singapore, which opened in 2003, is the world's first fully automated underground urban heavy-rail line. The MTR Disneyland Resort Line is also automated, along with trains on the future South Island Line.\n\nUrban people mover systems also tend to use fully automated and unstaffed trains, though to a much smaller scale. This is used in the LRT lines of Singapore and Macau, along with the Metromover in Miami, Florida and the Jacksonville Skyway.\n\nThe construction of an underground metro is an expensive project and is often carried out over a number of years. There are several different methods of building underground lines.\n\nIn one common method, known as cut-and-cover (used in the first New York City subway line), the city streets are excavated and a tunnel structure strong enough to support the road above is built in the trench, which is then filled in and the roadway rebuilt. This method (used for most of the underground parts of the São Paulo Metro and Guadalajara light rail system, for example) often involves extensive relocation of utilities commonly buried not far below street level – particularly power and telephone wiring, water and gas mains, and sewers. This relocation must be done carefully, as according to documentaries from the National Geographic Society, one of the causes of the April 22, 1992, explosions in Guadalajara was a mislocated water pipeline. The structures are typically made of concrete, perhaps with structural columns of steel; in the oldest systems, brick, and cast iron were used. Cut-and-cover construction can take so long that it is often necessary to build a temporary roadbed while construction is going on underneath, in order to avoid closing main streets for long periods of time. In Toronto, a temporary surface on Yonge Street supported cars and streetcar tracks for several years while the first segment of the Yonge subway was built.\n\nSome American cities, like Newark, Cincinnati and Rochester, were initially built around canals. When the canals were replaced by railways, the builders were able to bury a subway in the disused canal's trench, without rerouting other utilities, or acquiring right-of-way piecemeal.\n\nAnother usual type of tunneling method is called bored tunneling. Here, construction starts with a vertical shaft from which tunnels are horizontally dug, often with a tunneling shield, thus avoiding almost any disturbance to existing streets, buildings, and utilities. But problems with ground water are more likely, and tunneling through native bedrock may require blasting. (The first city to extensively use deep tunneling was London, where a thick sedimentary layer of clay largely avoids both problems.) The confined space in the tunnel also limits the machinery that can be used, but specialized tunnel-boring machines are now available to overcome this challenge.\nOne disadvantage with this, however, is that the cost of tunneling is much higher than building cut-and-cover systems, at-grade or elevated. Early tunneling machines could not make tunnels large enough for conventional railway equipment, necessitating special low, round trains, such as are still used by most of the London Underground, which cannot install air conditioning on most of its lines because the amount of empty space between the trains and tunnel walls is so small. Other lines were built with cut-and-cover and have since been equipped with air-conditioned trains.\n\nThe deepest metro system in the world was built in St. Petersburg, Russia where in the marshland, stable soil starts more than deep. Above that level, the soil mostly consists of water-bearing finely dispersed sand. Because of this, only three stations out of nearly 60 are built near ground level and three more above the ground. Some stations and tunnels lie as deep as below the surface. However, the location of the world's deepest station is not clear. Usually, the vertical distance between the ground level and the rail is used to represent the depth. Among the possible candidates are:\n\n\nOne advantage of deep tunnels is that they can dip in a basin-like profile between stations, without incurring the significant extra costs associated with digging near ground level. This technique, also referred to as putting stations \"on humps\", allows gravity to assist the trains as they accelerate from one station and brake at the next. It was used as early as 1890 on parts of the City and South London Railway and has been used many times since, particularly in Montreal.\n\nThe West Island Line, an extension of the MTR Island Line serving western Hong Kong Island, opened in 2015, has two stations (Sai Ying Pun and HKU) situated over below ground level, to serve passengers on the Mid-levels. They have several entrances/exits equipped with high-speed lifts, instead of escalators. These kinds of exits have existed in many London Underground stations and other stations in former Soviet Union nations.\n"}
{"id": "982231", "url": "https://en.wikipedia.org/wiki?curid=982231", "title": "Rubble trench foundation", "text": "Rubble trench foundation\n\nThe rubble trench foundation, an ancient construction approach popularized by architect Frank Lloyd Wright, is a type of foundation that uses loose stone or rubble to minimize the use of concrete and improve drainage. It is considered more environmentally friendly than other types of foundation because cement manufacturing requires the use of enormous amounts of energy. However, some soil environments are not suitable for this kind of foundation; particularly expansive or poor load-bearing (< 1 ton/sf) soils. A rubble trench foundation with a concrete grade beam is not recommended for earthquake prone areas.\n\nA foundation must bear the structural loads imposed upon it and allow proper drainage of ground water to prevent expansion or weakening of soils and frost heaving. While the far more common concrete foundation requires separate measures to ensure good soil drainage, the rubble trench foundation serves both foundation functions at once.\n\nTo construct a rubble trench foundation a narrow trench is dug down below the frost line. The bottom of the trench would ideally be gently sloped to an outlet. Drainage tile, graded 1\":8' to daylight, is then placed at the bottom of the trench in a bed of washed stone protected by filter fabric. The trench is then filled with either screened stone (typically 1-1/2\") or recycled rubble. A steel-reinforced concrete grade beam may be poured at the surface to provide ground clearance for the structure.\n\nIf an insulated slab is to be poured inside the grade beam, then the outer surface of the grade beam and the rubble trench should be insulated with rigid XPS foam board, which must be protected above grade from mechanical and UV degradation.\n\nThe rubble-trench foundation is a relatively simple, inexpensive, and environment-friendly alternative to a conventional foundation, but may require an engineer's approval if building officials are not familiar with it. Frank Lloyd Wright used them successfully for more than 50 years in the first half of the 20th century, and there is a revival of this style of foundation with the increased interest in green building.\n"}
{"id": "10186231", "url": "https://en.wikipedia.org/wiki?curid=10186231", "title": "S-TEC Corporation", "text": "S-TEC Corporation\n\nS-TEC Corporation is a United States corporation that was founded in 1978 and is headquartered in Mineral Wells, Texas. It manufactures flight control systems for the General Aviation aftermarket and for a number of original equipment manufacturers. S-TEC is the leader in the general aviation autopilot market for small- and mid-sized planes.\n\nGenesys Aerosystems has a number of forward-fit autopilot customers including Pilatus, Indonesian Aerospace, Epic Aircraft, and Aviat Aircraft. Their latest autopilot, the S-TEC 3100, has earned Supplemental Type Certificate (STC) on over 100 aircraft models .\n\nMeggitt acquired S-TEC and S-TEC Unmanned Technologies (SUTI) for $24 million in 2000. (DRS Technologies purchased the UAV business from Meggitt in 2002 but later closed the Mineral Wells facility.)\n\nS-TEC was purchased for $38 million by Cobham plc in 2008.\n\nIn April 2014, Cobham sold Chelton Flight Systems and S-TEC Corporation to Genesys Aerosystems.\n\n<ul>\n\n\n"}
{"id": "192092", "url": "https://en.wikipedia.org/wiki?curid=192092", "title": "Self-hosting", "text": "Self-hosting\n\nSelf-hosting is the use of a computer program as part of the toolchain or operating system that produces new versions of that same program—for example, a that can compile its own source code. Self-hosting software is commonplace on personal computers and larger systems. Other programs that are typically self-hosting include kernels, assemblers, command-line interpreters and revision control software.\n\nIf a system is so new that no software has been written for it, then software is developed on another self-hosting system, often using a cross compiler, and placed on a storage device that the new system can read. Development continues this way until the new system can reliably host its own development. Writing new software development tools without using another host system is rare.\n\nThe first self-hosting compiler (excluding assemblers) was written for Lisp by Hart and Levin at MIT in 1962. They wrote a Lisp compiler in Lisp, testing it inside an existing Lisp interpreter. Once they had improved the compiler to the point where it could compile its own source code, it was self-hosting.\n\nThis technique is only possible when an interpreter already exists for the very same language that is to be compiled. It borrows directly from the notion of running a program on itself as input, which is also used in various proofs in theoretical computer science, such as the proof that the halting problem is undecidable.\n\nKen Thompson started development on Unix in 1968 by writing and compiling programs on the GE-635 and carrying them over to the PDP-7 for testing. After the initial Unix kernel, a command interpreter, an editor, an assembler, and a few utilities were completed, the Unix operating system was self-hosting - programs could be written and tested on the PDP-7 itself.\n\nDevelopment of the Linux kernel was initially hosted on a Minix system. When sufficient packages, like GCC, GNU bash and other utilities are ported over, developers can work on new versions of Linux kernel based on older versions of itself (like building kernel 3.21 on a machine running kernel 3.18). This procedure can also be used when building a new Linux distribution from scratch.\n\nMany programming languages have self-hosted implementations: compilers that are both in and for the same language. Such languages include Ada, BASIC, C, C++, C#, CoffeeScript, Dylan, F#, FASM, Forth, Gambas, Go, Haskell, HolyC, Java, Lisp, Modula-2, OCaml, Oberon, Pascal, Python, Rust, Scala, Smalltalk, TypeScript, Vala, and Visual Basic.\n\nIn some of these cases, the initial implementation was not self-hosted, but rather, written in another language (or even in machine language); in other cases, the initial implementation was developed using bootstrapping.\n\nIn the context of website management and online publishing, the term \"self hosting\" is used to describe the practice of running and maintaining a website using a private web server. The concept of self hosting is mostly relevant in situations where a webmaster has a clear and immediate \"hosted\" alternative, however the term can be applied in other situations. A hosted solution is any service whereby an external provider is relied upon to provide a fully managed service, which might include adequate server space, on-demand support, and regular software updates.\n\nThere is often confusion as to what constitutes \"self hosted\" or \"hosted\". The key distinction lies in the amount of control a webmaster has over the web property. A good example of the self hosting vs. hosted dichotomy would be WordPress, a widely used, free and open-source content management system (CMS). A webmaster could choose to use either WordPress.com, which is a largely free service maintained by the company Automattic, or alternatively download the underlying WordPress software from WordPress.org. The latter option would enable the webmaster to install the WordPress software manually on his or her own private web server, whether that server is leased from a web hosting provider or set up in house. In summary, WordPress.com can serve as a hosted alternative, whereas WordPress.org could constitute as self hosting.\n\nSome services like Shopify could also constitute as \"hosted\" services. Shopify is an example of a SaaS-based ecommerce platform, which offers customers a fully managed service, including proprietary website-building software, on-demand support, and server space/hosting. Since the Shopify software is proprietary and not open source, there is no \"self hosting\" alternative.\n\n"}
{"id": "3663653", "url": "https://en.wikipedia.org/wiki?curid=3663653", "title": "Self-service software", "text": "Self-service software\n\nSelf-service software is a subset within the knowledge management software category and which contains a range of software that specializes in the way information, process rules and logic are collected, framed within an organized taxonomy, and accessed through decision support interviews. Self-service software allows people to secure answers to their inquiries and/or needs through an automated interview fashion instead of traditional search approaches.\n\nSelf-service software allows authors (typically subject-matter experts) to readily automate the deployment of, the timeliness of, and compliance around a variety of processes with which they are involved in communicating without having to physically address the questions, needs, and solicitations of end-users who are inquiring about the particular process being automated. \n\nSelf-service software primarily addresses closed-loop inquiries whereby the author emulates a variety of known (finite) questions and related (known) responses on hand or required steps that must be addressed to derive and deliver a final answer or directive. Often the author using such software codifies such known processes and steps then generates (publishes) end-user facing applications which can encompass a variety of code bases and platforms.\n\nSelf-service software is sometimes referred to decision support software and even expert systems. It is typically categorized as a subtopic within the knowledge management software category. Self-service software allows individuals and companies alike to tailor and address customer support, technical support and employee support inquiries and needs in an on-demand fashion where the person with a question (need) can interface with the author's generated application via a computer, a handheld device, a kiosk, register, or other machine type to secure their answers as if they were directly interacting (talking to) the author.\n\nSome self-service software is able to handle automatic execution of processes. An approval process can also be added to the workflow. For instance, to give managers the possibility to keep track of the cost related to the ordered services by employees.\n\n"}
{"id": "20594810", "url": "https://en.wikipedia.org/wiki?curid=20594810", "title": "Solid oxide electrolyser cell", "text": "Solid oxide electrolyser cell\n\nA solid oxide electrolyzer cell (SOEC) is a solid oxide fuel cell that runs in regenerative mode to achieve the electrolysis of water (and/or carbon dioxide) by using a solid oxide, or ceramic, electrolyte to produce hydrogen gas (and/or carbon monoxide) and oxygen.\nThe production of pure hydrogen is compelling because it is a clean fuel that can be stored easily, thus making it a potential alternative to batteries, which have a low storage capacity and create high amounts of waste materials. Electrolysis is currently the most promising method of hydrogen production from water due to high efficiency of conversion and relatively low required energy input when compared to thermochemical and photocatalytic methods.\n\nSolid oxide electrolyzer cells operate at temperatures which allow high-temperature electrolysis to occur, typically between 500 and 850 °C. These operating temperatures are similar to those conditions for an SOFC. The net cell reaction yields hydrogen and oxygen gases. The reactions for one mole of water are shown below, with oxidation of water occurring at the anode and reduction of water occurring at the cathode.\n\nAnode: O → 1/2O + 2e\n\nCathode: HO + 2e → H + O\n\nNet Reaction: HO → H + 1/2O\n\nElectrolysis of water at 298 K (25 °C) requires 285.83 kJ of energy per mole in order to occur, and the reaction is increasingly endothermic with increasing temperature. However, the energy demand may be reduced due to the Joule heating of an electrolysis cell, which may be utilized in the water splitting process at high temperatures. Research is ongoing to add heat from external heat sources such as concentrating solar thermal collectors and geothermal sources.\n\nThe general function of the electrolyse cell is to split water in the form of steam into pure H and O. Steam is fed into the porous cathode. When a voltage is applied, the steam moves to the cathode-electrolyte interface and is reduced to form pure H and oxygen ions. The hydrogen gas then diffuses back up through the cathode and is collected at its surface as hydrogen fuel, while the oxygen ions are conducted through the dense electrolyte. The electrolyte must be dense enough that the steam and hydrogen gas cannot diffuse through and lead to the recombination of the H and O. At the electrolyte-anode interface, the oxygen ions are oxidized to form pure oxygen gas, which is collected at the surface of the anode.\n\nSolid oxide electrolyzer cells follow the same construction of a solid-oxide fuel cell, consisting of a fuel electrode (cathode), an oxygen electrode (anode) and a solid-oxide electrolyte.\n\nThe most common electrolyte, again similar to solid-oxide fuel cells, is a dense ionic conductor consisting of ZrO doped with 8 mol % Y2O3 (also knows as YSZ). Zirconia dioxide is used because of its high strength, high melting temperature (approximately 2700 °C) and excellent corrosion resistance. YO is added to mitigate the phase transition from the tetragonal to the monoclinic phase on rapid cooling, which can lead to cracks and decrease the conductive properties of the electrolyte by causing scattering. Some other common choices for SOEC are Scandia stabilized zirconia (ScSZ), ceria based electrolytes or lanthanum gallate materials. Despite the material similarity to solid oxide fuel cells, the operating conditions are different, leading to issues such as high steam concentrations at the fuel electrode and high oxygen partial pressures at the electrolyte/oxygen electrode interface. A recent study found that periodic cycling a cell between electrolyzer and fuel cell modes reduced the oxygen partial pressure build up and drastically increased the lifetime of the electrolyzer cell.\n\nThe most common fuel electrode material is a Ni doped YSZ, however, high steam partial pressures and low hydrogen partial pressures at the Ni-YSZ interface caused oxidation of the nickel and lead to irreversible degradation. Perovskite-type lanthanum strontium manganese (LSM) is also commonly used as a cathode material. Recent studies have found that doping LSM with scandium to form LSMS promotes mobility of oxide ions in the cathode, increasing reduction kinetics at the interface with the electrolyte and thus leading to higher performance at low temperatures than traditional LSM cells. However, further development of the sintering process parameters is required to prevent precipitation of scandium oxide into the LSM lattice. These precipitate particles are problematic because they can impede electron and ion conduction. In particular, the processing temperature and concentration of scandium in the LSM latice are being researched to optimize the properties of the LSMS cathode. New materials are being researched such as lanthanum strontium manganese chromate (LSCM), which has proven to be more stable under electrolysis conditions. LSCM has high redox stability, which is crucial especially at the interface with the electrolyte. Scandium-doped LCSM (LSCMS) is also being researched as a cathode material due to its high ionic conductivity. However, the rare-earth element introduces a significant materials cost and was found to cause a slight decrease in overall mixed conductivity. Nonetheless, LCSMS materials have demonstrated high efficiency at temperatures as low as 700 °C.\n\nLanthanum strontium manganate (LSM) is the most common oxygen electrode material. LSM offers high performance under electrolysis conditions due to generation of oxygen vacancies under anodic polarization that aid oxygen diffusion. In addition, impregnating LSM electrode with GDC nanoparticles was found to increase cell lifetime by preventing delamination at the electrode/electrolyte interface. The exact mechanism by how this happen needs to be explore further. In a 2010 study, it was found that neodymium nickelate as an anode material provided 1.7 times the current density of typical LSM anodes when integrated into a commercial SOEC and operated at 700 °C, and approximately 4 times the current density when operated at 800 °C. The increased performance is postulated to be due to higher \"overstoichimoetry\" of oxygen in the neodymium nickelate, making it a successful conductor of both ions and electrons.\n\nAdvantages of solid oxide-based regenerative fuel cells include high efficiencies, as they are not limited by Carnot efficiency.\nAdditional advantages include long-term stability, fuel flexibility, low emissions, and low operating costs. However, the greatest disadvantage is the high operating temperature, which results in long start-up times and break-in times. The high operating temperature also leads to mechanical compatibility issues such as thermal expansion mismatch and chemical stability issues such as diffusion between layers of material in the cell\n\nIn principle, the process of any fuel cell could be reversed, due to the inherent reversibility of chemical reactions.\nHowever, a given fuel cell is usually optimized for operating in one mode and may not be built in such a way that it can be operated in reverse. Fuel cells operated backwards may not make very efficient systems unless they are constructed to do so such as in the case of solid oxide electrolyzer cells, high pressure electrolyzers, unitized regenerative fuel cells and regenerative fuel cells. However, current research is being conducted to investigate systems in which a solid oxide cell may be run in either direction efficiently.\n\nFuel cells operated in electrolysis mode have been observed to degrade primarily due to anode delamination from the electrolyte. The delamination is a result of high oxygen partial pressure build up at the electrolyte-anode interface. Pores in the electrolyte-anode material act to confine high oxygen partial pressures inducing stress concentration in the surrounding material. The maximum stress induced can be expressed in terms of the internal oxygen pressure using the following equation from fracture mechanics: \n\nwhere c is the length of the crack or pore and formula_2 is the radius of curvature of the crack or pore. If formula_3 exceeds the theoretical strength of the material, the crack will propagate, macroscopically resulting in delamination.\n\nVirkar et al. created a model to calculate the internal oxygen partial pressure from the oxygen partial pressure exposed to the electrodes and the electrolyte resistive properties..The internal pressure of oxygen at the electrolyte- anode interface was modelled as:\n\nwhere formula_6 is the oxygen partial pressure exposed to the oxygen electrode (anode), formula_7 is the area specific electronic resistance at the anode interface, formula_8 is the area specific ionic resistance at the anode interface, formula_9 is the applied voltage, formula_10 is the Nernst potential, formula_11 and formula_12 are the overall electronic and ionic area specific resistances respectively, and formula_13 and formula_14 are the electric potentials at the anode surface and the anode electrolyte interface respectively. \n\nIn electrolysis mode formula_13>formula_14 and formula_9>formula_10. Whether formula_19 is greater than formula_6 is dictated by whether (formula_13-formula_14 ) or formula_23 is greater than formula_24 . The internal oxygen partial pressure is minimized by increasing the electronic resistance at the anode interface and decreasing the ionic resistance at anode interface. \n\nDelamination of the anode from the electrolyte increases the resistance of the cell and necessitates higher operating voltages in order to maintain a stable current. Higher applied voltages increases the internal oxygen partial pressure, further exacerbating the degradation.\n\nSOECs have possible application in fuel production, carbon dioxide recycling, and chemicals synthesis. In addition to the production of hydrogen and oxygen, an SOEC could be used to create syngas by electrolyzing water vapor and carbon dioxide.\nThis conversion could be useful for energy generation and energy storage applications.\n\nMIT will test the method on the Mars 2020 rover mission as a means to produce oxygen for both human sustenance and liquid oxygen rocket propellant.\n\n\n"}
{"id": "31285614", "url": "https://en.wikipedia.org/wiki?curid=31285614", "title": "Sonic soot blowers", "text": "Sonic soot blowers\n\nSonic soot blowers offer a cost-effective and non-destructive means of preventing ash and particulate build-up within the power generation industry. They use high energy – low frequency sound waves that provide 360° particulate de-bonding and at a speed in excess of 344 metres per second. Because they employ non-destructive sound waves, unlike steam soot blowers they eliminate any concerns over corrosion, erosion or mechanical damage and do not produce an effluent stream.\n\nThe sonic soot blower can in some ways be compared to a musical reed instrument such as an oboe, where the ‘base tone’ is created by blowing air over a reed and then converting this ‘base tone’ into a particular high or low note, depending on how far the sound wave has to travel along inside the body of the instrument.\n\nThe sonic soot blower operates in the same manner, the ‘base tone’ being produced by passing compressed air into a wave generator which houses a titanium diaphragm causing it to oscillate rapidly. This ‘base tone’ is then converted into a range of selected frequencies ranging from 350 Hz down to 60 Hz by the design and length of the horn section, producing the desired sound frequency at a sound level approaching 200 dB. The sonic soot blower is usually ‘sounded’ for a few seconds at intervals of between 3 and 10 minutes. This ‘sounding’ pattern is normally controlled via the plant’s PLC. However, it may also be operated by such means as a SCADA system, individual timers on each solenoid valve or via a manual ball valve.\n\nSonic soot blowers are normally constructed from fabricated, 316 grade stainless steel as opposed to some sonic horns which are manufactured from heavy cast iron. For installations in harsher environments, such as high temperature or acidic gas streams, other types of stainless steels are used such as 310, 316 and 825.\n\nSonic soot blowers create a rapid series of very powerful sound induced pressure fluctuations which when transmitted into the ash or particulate, cause them to de-bond from other particles and from the heat transfer surface to which they are bonded and so carried away in the gas stream. This prevents the ash from building up and sintering onto the boiler tubes thus significantly reducing thermal efficiency. This is in contrast to the operating principles of steam soot blowers which are usually only employed at most once every eight hours by which time the ash has built up and baked hard onto the heat transfer surfaces. The steam soot blower then tries to blast away his hard deposit, usually only from the leading edge of the steam tubes.\n\nSonic soot blowers are a proven alternative to conventional steam soot blowers in power generation plants which burn a range of fossil fuels and other waste fuels including biofuels. Depending on the application and boiler plant design, sonic soot blowers usually totally replace existing high maintenance steam soot blowers whether of the retractable or rotary type. In a few cases, sonic soot blowers can be used to supplement steam soot blowers.\nSonic soot blower cleaning technologies can be applied in superheaters, generating sections, economizers, and airheaters as well as downstream equipment such as electrostatic precipitators, baghouse filters and fans.\n\nThe main advantages of sonic soot blowers over steam soot blowers are:-\n\n• Elimination of opacity spikes due to more regular, more efficient cleaning\n• No structural damage to tube bundles or boiler structure\n• Elimination of tube corrosion and erosion problems\n• 360° cleaning of all tube surfaces – not harsh leading edge tube cleaning as with steam soot blowers\n• Prevention of ash build up and sintering on steam tubes due to sonic soot blowers regular operation\n• Extremely low maintenance or operational costs\n• Eco-friendly – helps to combat global climate change and the effect of global warming\n\n"}
{"id": "24946015", "url": "https://en.wikipedia.org/wiki?curid=24946015", "title": "Subtitle editor", "text": "Subtitle editor\n\nA subtitle editor is a type of software used to create and edit subtitles to be superimposed over, and synchronized with, video. Such editors usually provide video preview, easy entering/editing of text, start, and end times, and control over text formatting and positioning. Subtitle editors are available as standalone applications, as components of many video editing software suites, and as web applications.\n\nIn television, subtitles are used for \"clarification, translation, services for the deaf, as well as identifying places or people in the news.\" In movies, subtitles are mainly used for translations from foreign languages. \nSubtitles are frequently used to provide informative details regarding the action on screen, such as the names and titles of interview subjects, a discussion topic change, to spell out web URLs or email addresses, to assist understanding speakers who mumble, details in instructional videos such as recipe ingredients, and humorous effects such as commentary or contradictory captions.\n\nEach subtitle, during editing, consists of text, a start time, an end time or duration, and optional text styling and positioning.\n\nTime is measured either in video frames, in milliseconds, or in hours:minutes:seconds.frames/milliseconds. The exact format and separator characters(colon, period, comma, etc.) are determined by the subtitle format chosen. A capable subtitle editor can convert one time measurement system into another without error.\n\nA subtitle editing tool will typically have these features:\n\nEditors used by broadcasters may have additional features:\nSee the general description of subtitles for further description.\n\nNot all editors are equally flexible in terms of video formats, subtitle formats, or ease of use.\n\n\n\n\n"}
{"id": "1579998", "url": "https://en.wikipedia.org/wiki?curid=1579998", "title": "Temperature control", "text": "Temperature control\n\nTemperature control is a process in which change of temperature of a space (and objects collectively there within) is measured or otherwise detected, and the passage of heat energy into or out of the space is adjusted to achieve a desired average temperature. \n\nAir-conditioners, space-heaters, refrigerators, water-heaters, etc. are examples of devices that perform temperature control. These are often broadly classified as Thermostatically Controlled Loads (TCLs). \n\nA home thermostat is an example of a closed control loop: It constantly assesses the current room temperature and controls a heater and/or air conditioner to increase or decrease the temperature according to user-defined setting(s). A simple (low-cost, cheap) thermostat merely switches the heater or air conditioner either on or off, and temporary overshoot and undershoot of the desired average temperature must be expected. A more expensive thermostat varies the amount of heat or cooling provided by the heater or cooler, depending on the difference between the required temperature (the \"setpoint\") and the actual temperature. This minimizes over/undershoot. This method is called Proportional control. Further enhancements using the accumulated error signal (Integral) and the rate at which the error is changing (Derivative) are used to form more complex PID Controllers which is the form usually seen in industry.\n\nAn object's or space's temperature increases when heat energy moves into it, increasing the average kinetic energy of its atoms, e.g., of things and air in a room. Heat energy leaving an object or space lowers its temperature. Heat flows from one place to another (always from a higher temperature to a lower one) by one or more of three processes: conduction, convection and radiation. In conduction, energy is passed from one atom to another by direct contact. In convection, heat energy moves by conduction into some movable fluid (such as air or water) and the fluid moves from one place to another, carrying the heat with it. At some point the heat energy in the fluid is usually transferred to some other object by means conduction again. The movement of the fluid can be driven by negative-buoyancy, as when cooler (and therefore denser) air drops and thus upwardly displaces warmer (less-dense) air (natural convection), or by fans or pumps (forced convection). In radiation, the heated atoms make electromagnetic emissions absorbed by remote other atoms, whether nearby or at astronomical distance. For example, the Sun radiates heat as both invisible and visible electromagnetic energy. What we know as \"light\" is but a narrow region of the electromagnetic spectrum.\n\nIf, in a place or thing, more energy is received than is lost, its temperature increases. If the amount of energy coming in and going out are exactly the same, the temperature stays constant—there is thermal balance, or thermal equilibrium.\n\n"}
{"id": "50685400", "url": "https://en.wikipedia.org/wiki?curid=50685400", "title": "Uganda Coffee Development Authority", "text": "Uganda Coffee Development Authority\n\nThe Uganda Coffee Development Authority (UCDA) is a government agency mandated to \"promote and oversee the coffee industry by supporting research, promoting production, controlling the quality and improving the marketing of coffee\" in the country.\n\nThe headquarters of the agency are located at Coffee House, 35 Jinja Road, in the central business district of Kampala, the capital and largest city in the country. The coordinates of UCDA headquarters are:0°18'53.0\"N, 32°35'28.0\"E (Latitude:0.314735; Longitude:32.591106).\n\nUCDA was established in 1991, by government decree. The law was later modifies by parliament in 1994. It is administered under the Uganda Ministry of Agriculture, Animal Industry and Fisheries. The agency periodically releases production and price data.\n\n\n"}
{"id": "2698594", "url": "https://en.wikipedia.org/wiki?curid=2698594", "title": "Unbundling", "text": "Unbundling\n\nUnbundling is a neologism to describe how the ubiquity of mobile devices, Internet connectivity, consumer web technologies, social media and information access in the 21st century is affecting older institutions (education, broadcasting, newspapers, games, shopping, etc.) by \"break[ing] up the packages they \nonce offered (possibly even for free), providing particular parts of them at a scale and cost unmatchable by the old order.\" Unbundling has been called \"the great disruptor\".\n\n\"Unbundling\" most basically means simply the \"process of breaking apart something into smaller parts.\" In the context of mergers and acquisitions, unbundling refers to the \"process by which a large company with several different lines of business retains one or more core businesses and sells off the remaining assets, product/service lines, divisions or subsidiaries.\"\n\n\n\n"}
{"id": "2417303", "url": "https://en.wikipedia.org/wiki?curid=2417303", "title": "Walt Mossberg", "text": "Walt Mossberg\n\nWalter S. Mossberg (born March 27, 1947) is an American journalist. He is widely credited with pioneering the modern, consumer-focused, technology review and commentary.\n\nFrom 1991 through 2013, he was the principal technology columnist for \"The Wall Street Journal\". He also co-founded \"AllThingsD\", \"Recode\" and the D and Code Conferences. From 2015 to 2017, Mossberg was Executive Editor of \"The Verge\" and Editor-at-Large of \"Recode\", web sites owned by Vox Media. Mossberg wrote a weekly column for both and also had a weekly podcast, Ctrl-Walt-Delete. Mossberg was also co-executive producer of the annual Code Conference. He retired in July 2017.\n\nDow Jones announced on September 19, 2013, that Mossberg would leave \"The Wall Street Journal\" as part of the breakup with AllThingsD by the end of the year. AllThingsD was a technology conference and web site owned by Dow Jones but created and operated by Mossberg and Kara Swisher. Along with other reporters from AllThingsD, Mossberg and Swisher started a new media site called \"Recode\" in 2014, which was acquired by Vox Media in 2015.\n\nIn April 2017, Mossberg announced his plans to retire. He serves on the board of The News Literacy Project and is writing a book for St. Martin’s Press.\n\nMossberg, a native of Warwick, Rhode Island, is a graduate of Pilgrim High School, Brandeis University and the Columbia University Graduate School of Journalism.\n\nMossberg was a reporter and editor at \"The Wall Street Journal\" from 1970 until the end of 2013. He was based in the \"Journal's\" Washington, D.C., office, where he spent 18 years covering national and international affairs before turning his attention to technology. Mossberg's \"Personal Technology\" column appeared every Thursday from 1991 through 2013. He also edited the \"Digital Solution\" column each Wednesday (authored by his colleague, Katherine Boehret), and wrote the \"Mossberg's Mailbox\" column on Thursdays. He appeared weekly on CNBC, and in web videos, and was on numerous times a guest on the Charlie Rose Show, airing on PBS stations.\n\nIn 1999, Mossberg became the first technology writer to receive the Loeb award for Commentary. In 2001, he won the World Technology Award for Media and Journalism and received an honorary Doctorate of Law from the University of Rhode Island. Mossberg is widely regarded as one of the most influential writers on information technology. In 2004, in a lengthy profile, \"Wired\" called him \"The Kingmaker\", saying \"few reviewers have held so much power to shape an industry's successes and failures.\" A 2007 profile in the \"New Yorker\" was entitled \"Everyone Listens to Walter Mossberg\" and declared him \"someone whose judgment can ratify years of effort or sink the show.\"\n\nIn 2017, he received the Loeb Award's Lifetime Achievement Award.\n\nIn partnership with his fellow \"Journal\" columnist Kara Swisher, Mossberg created, produced and hosted the \"Journal's\" annual \"All Things Digital\" conference in Carlsbad, California, in which top technology leaders, such as Bill Gates, Steve Jobs and Elon Musk, appeared on stage without prepared remarks, or slides, and were interviewed by the two columnists. That conference concept continues today in the form of their Code Conference. Mossberg and Swisher also co-edited the All Things Digital web site, which included his columns, her blog and other posts.\n\nOn May 30, 2007, Mossberg and Swisher conducted a historic, unrehearsed, joint onstage interview with Steve Jobs and Bill Gates. The next month, Mossberg was one of only four journalists provided with advance access to the first iPhone in order to review it.\n\nIn September, 2013, by mutual agreement, Dow Jones & Co. and Mossberg and Swisher announced they would not renew the contract with \"AllThingsD\", and that Mossberg would be leaving \"The Wall Street Journal\" by the end of the year.\n\nOn January 2, 2014, Mossberg and Swisher launched \"Recode\", a tech website. The website was acquired by Vox Media in May 2015 in an all-stock deal.\n\nOn April 7, 2017, Mossberg announced his planned retirement, which occurred on July 3 of that year. \"It just seems like the right time to step away,\" Mossberg wrote in \"Recode\". \"I’m ready for something new.\" His final column was published on May 25, 2017. His final Code Conference was May 30–June 1, and his retirement podcast, performed live in New York, was on June 9.\n\n"}
{"id": "39115171", "url": "https://en.wikipedia.org/wiki?curid=39115171", "title": "Áslaug Magnúsdóttir", "text": "Áslaug Magnúsdóttir\n\nÁslaug Magnúsdóttir is an Icelandic businesswoman and entrepreneur. She is the co-founder and former chief executive officer of Moda Operandi and a co-founder of TSM Capital. She has been dubbed \"fashion's fairy godmother\" by \"Vogue\" and one of the 100 Most Creative People in Business by \"Fast Company\".\n\nA Fulbright scholar, Áslaug holds an MBA from Harvard Business School, an LL.M from Duke University School of Law and a Candidate of Law degree from the University of Iceland.\n\nÁslaug Magnúsdóttir was born in Reykjavík, Iceland, to parents Magnús Sigurðsson and Rakel Valdimarsdóttir. She has one brother, Sigurður R. Magnússon. Áslaug grew up in Los Angeles and Reykjavík and attended college at the University of Iceland, where she graduated in 1993 with a degree in law. She received a Fulbright scholarship and returned to the U.S. to attend the LL.M program at Duke University. She then earned an MBA from Harvard Business School in 2000, the first Icelandic woman to do so.\n\nÁslaug lives in New York with her husband Gabriel Levy. She was previously married to Gunnar Thoroddsen, a lawyer and former CEO of Landsbanki bank in Luxembourg. Gunnar is the grandson of Iceland's former Prime Minister Gunnar Thoroddsen, and the great grandson of Iceland's second President, Ásgeir Ásgeirsson. Together they have a son, Gunnar A. Thoroddsen.\n\nÁslaug began her career in Iceland, where as a student she headed a modeling agency and was chairman of the National Ballet Company of Iceland, and then became a corporate and tax attorney at Deloitte. After completing her graduate studies, she moved to London, where she was an Engagement Manager at McKinsey & Company.\n\nÁslaug first became involved in the fashion industry while helping to run a modeling agency during college. She started her professional career in fashion at Baugur Group, an investment company focused on the fashion and retail sectors. During her time at Baugur, Aslaug spearheaded a number of investments in early-stage fashion brands.\n\nIn 2006, Áslaug relocated from London to New York and joined Marvin Traub Associates as a Vice President. In 2007, she partnered with Traub, the former CEO of Bloomingdale's, to form an investment company, TSM Capital, which invested in designer brands such as Rachel Roy and Matthew Williamson. She later worked with actor and jewelry designer Waris Ahluwalia as President of his label, House of Waris Fine Jewelry. In 2009, Áslaug joined Gilt Groupe to head merchandising for Gilt Noir.\n\nIn 2009, Áslaug came up with the idea for Moda Operandi, a \"pre-tailer\" that lets consumers pre-order items directly from the runway. She shared the concept with Lauren Santo Domingo, and, together, they co-founded the company.\n\nÁslaug left Moda Operandi in May 2013 after raising $36 million from venture capital firms. With Matthew Pavelle, Gabriel Levy and Cleo Davis-Urman, she co-founded Tinker Tailor, a business that let consumers customize designer clothing or design their own. Tinker Tailor ceased operations in 2015.\n\n\n\n\n\n\n\n"}
