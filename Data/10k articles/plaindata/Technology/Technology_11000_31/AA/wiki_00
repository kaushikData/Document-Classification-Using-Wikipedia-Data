{"id": "9251623", "url": "https://en.wikipedia.org/wiki?curid=9251623", "title": "360-degree video", "text": "360-degree video\n\n\"360-degree videos, also known as immersive videos or spherical videos\"', are video recordings where a view in every direction is recorded at the same time, shot using an omnidirectional camera or a collection of cameras. During playback on normal flat display the viewer has control of the viewing direction like a panorama. It can also be played on a displays or projectors arranged in a sphere or some part of a sphere.\n\n360-degree video is typically recorded using either a special rig of multiple cameras, or using a dedicated camera that contains multiple camera lenses embedded into the device, and filming overlapping angles simultaneously. Through a method known as video stitching, this separate footage is merged into one spherical video piece, and the color and contrast of each shot is calibrated to be consistent with the others. This process is done either by the camera itself, or using specialized software such as Mistika VR or Kolor AVP that can analyze common visuals and audio to synchronize and link the different camera feeds together. Generally, the only area that cannot be viewed is the view toward the camera support.\n\n360-degree video is typically formatted in an equirectangular projection and is either monoscopic, with one image directed to both eyes, or stereoscopic, viewed as two distinct images directed individually to each eye for a 3D effect. Due to this projection and stitching, equirectangular video exhibits a lower quality in the middle of the image than at the top and bottom. Spherical videos are frequently in curvilinear perspective with a fisheye effect. The heavy barrel distortion often requires rectilinear correction before applications in detection, tracking or navigation.\n\nSpecialized omnidirectional cameras and rigs have been developed for the purpose of filming 360-degree video, including rigs such as GoPro's Omni and Odyssey (which consist of multiple action cameras installed within a frame), and contained cameras like the HumanEyes Vuze and Nokia OZO, There have also been handheld dual-lens cameras such as the Ricoh Theta S, Samsung Gear 360, Garmin VIRB 360, and the Kogeto Dot 360—a panoramic camera lens accessory developed for the iPhone 4, 4S, and Samsung Galaxy Nexus.\n\n360-degree videos are typically viewed via personal computers, mobile devices such as smartphones, or dedicated head-mounted displays. Users can pan around the video by clicking and dragging. On smartphones, internal sensors such as the gyroscope can also be used to pan the video based on the orientation of the device. Taking advantage of this behavior, stereoscope-style enclosures for smartphones (such as Google Cardboard viewers and the Samsung Gear VR) can be used to view 360-degree videos in an immersive format similar to virtual reality. The phone display is viewed through lenses contained within the enclosure, as opposed to virtual reality headsets that contain their own dedicated displays.\n\nIn March 2015, YouTube launched support for publishing and viewing 360-degree videos, with playback on its website and its Android mobile apps. Parent company Google also announced that it would collaborate with camera manufacturers to make it easier for creators to upload 360-degree content recorded with their products to YouTube. However, in 2017, Google and YouTube began to promote an alternative stereoscopic video format known as VR180, which is limited to a 180-degree field of view, but is promoted as being more accessible to produce than 360-degree video, and allowing more depth to be maintained by not subjecting the video to equirectangular projection \n\nFacebook (parent company of VR headset maker Oculus VR) followed suit by adding 360-degree video support in September 2015, and subsequently unveiled reference designs for its own 360-degree camera systems known as Facebook Surround 360. Facebook announced in March 2017 that more than 1 million 360-degree videos had been uploaded to Facebook to date. Vimeo also launched 360-degree video support in March 2017.\n\nGoogle Cardboard, which is typically distributed in the form of do-it-yourself kits consisting of low-cost materials and components, has been credited with helping virtual reality become more readily available to the general public, and helping boost the adoption of 360-degree video by publishers, such as mainstream journalists and media brands.\n\nThe use of the term \"virtual reality\" to describe 360-degree video has been disputed, as VR typically refers to interactive experiences wherein the viewer's motions can be tracked to allow real-time interactions within a virtual environment, with orientation and position tracking. In 360-degree video, the locations of viewers are fixed, viewers are limited to the angles captured by the cameras, and cannot interact with the environment. The non-dynamic nature of video also means that rendering techniques cannot be used to reduce the risk of motion sickness.\n\n"}
{"id": "3256517", "url": "https://en.wikipedia.org/wiki?curid=3256517", "title": "AN/ALR-67 Radar Warning Receiver", "text": "AN/ALR-67 Radar Warning Receiver\n\nThe AN/ALR-67 Radar Warning Receiver is designed to warn an aircraft's crew of potentially hostile radar activity. It is an airborne threat warning and countermeasures control system built to be successor to the United States Navy's AN/ALR-45. Northrop Grumman Corporation's Electronic Systems sector (Rolling Meadows, Illinois) was the main contractor for the AN/ALR-67(V) and (V)2. Raytheon Electronic Warfare Systems (Goleta, California) was the main contractor for the AN/ALR-67(V)3. \n\nThe AN/ALR-67 countermeasures warning and control system is the standard threat warning system for tactical aircraft and was specifically designed for the A-6E/SWIP, AV-8B, F-14B, F-14D and F/A-18. The system detects, identifies and displays radars and radar-guided weapon systems in the C to J frequency range (about 0.5 to 20 GHz). The system also coordinates its operation with onboard fire-control radars, datalinks, jammers, missile detection systems and anti-radiation missiles.\n\nThe AN/ALR-67(V)2 comprises the following units: \n\nThe AN/ALR-67(V)2 in turn has been given a significant enhancement in capability, through Engineering Change Procedure ECP-510 to the AN/ALR-67E(V)2 standard. The AN/ALR-67E(V)2 provides additional enhancements including a 10-fold improvement in detection ranges when in the presence of a wingman's radar signals; it also incorporates Inertial guidance system (INS) stabilisation for accurate display in high \"g\" manoeuvres and during high roll maneuvers. \n\nThe designation ALR-67B(V)2 is in connection with the ALR-67 systems fitted to Spain's EF-18 and Canadian's CF-18 aircraft. \n\nOver 1,600 AN/ALR-67(V) and AN/ALR-67(V)2 systems have been sold. The AN/ALR-67(V) has been supplied to the U.S. Navy and U.S. Marine Corps and the air forces of Australia, Canada, Finland, Kuwait, Malaysia, Spain and Switzerland.\n\nThe AN/ALR-67(V)3 is commonly referred to as the Advanced Special Receiver (ASR) set. The receiver electronics unit has been upgraded to a fully channelized digital architecture with dual 32-bit processors, yet with an overall reduction in system size and weight. The Azimuth Display Indicator (ADI) is a 3 in (76.2 mm) diameter CRT or LCD cockpit display, carried over from the AN/ALR-67(V)2, used to show intercepted threats. The AN/ALR-67(V)3 also forms part of the electronic countermeasures programme, including an interface to the ALE-50 Towed Decoy System.\n\nIn August 1999, Raytheon was awarded an initial contract for full-rate production of the AN/ALR-67(V)3 for the U.S. Navy F/A-18E/F Super Hornet, totalling 34 complete installations, together with 40 spare quadrant receivers and five countermeasures receivers. These were delivered during 2001/02. Further production contracts followed, with the latest in April 2005, where Raytheon received its seventh production contract for 42 systems totalling US$ 44 million. This contract brought total orders to 284 receivers plus spares.\n\nIt was reported on 13 September 2006 that the Australian Defence minister has accepted a recommendation to stop development of the ALR-2002 for the F/A-18, the RAAF will most likely install the ALR-67V(3) instead.\n\nOn 3 August 2007, Deagel.com reported that the Defense Security Cooperation Agency notified Congress of a possible Foreign Military Sale to Canada of AN/ALR-67(V)3 radar warning receivers as well as associated equipment and services. The total value, if all options are exercised, could be as high as $209 million. This notice of a potential sale is required by law; it does not mean that the sale has been concluded.\n\nA variant of this system, designated AN/ALR-67(V)4, was included in a proposed upgrade for the F-14A but was never implemented due to the retirement of those aircraft in the United States.\n"}
{"id": "1937803", "url": "https://en.wikipedia.org/wiki?curid=1937803", "title": "AN/SLQ-32 Electronic Warfare Suite", "text": "AN/SLQ-32 Electronic Warfare Suite\n\nThe AN/SLQ-32 is a shipboard electronic warfare suite built by the Raytheon Company of Goleta, California and The Hughes Aircraft Company. It is currently the primary electronic warfare system in use by U.S. Navy ships (as of 2017).\n\nReferred to by its operators as the \"slick-32\". The SLQ-32 was originally conceived in the 1970s to augment the AN/WLR-1, which had been in service since the early 1960s. It was later determined to save costs to replace the various WLR-1 series suites with the SLQ-32 as a stand alone system. As originally designed, the SLQ-32 was produced in three variants, the (V)1, (V)2 and (V)3. Later in its service life, two additional versions were built, the (V)4 and (V)5. The Air Transport Rack sized processors were supplied by ROLM Mil-Spec Computers in San Jose, CA.\n\nAll versions of the SLQ-32, with the exception of the (V)4, are interfaced with the MK36 Decoy Launching System, able to launch chaff and infrared decoys under the control of the SLQ-32. The number and arrangement of MK36 launchers installed depends on the size of the ship, ranging from two launchers on a small combatant to as many as ten on an aircraft carrier. A growing number of systems are being upgraded to incorporate the multi-national MK-53 Nulka system.\n\nThe original modular design was intended to allow upgrades of the system from one variant to the next by simply installing additional equipment as required. Starting in the early 1990s, a program was begun to upgrade all SLQ-32s in the U.S. fleet. Most (V)1 systems were upgraded to (V)2, and most (V)2 systems were upgraded to (V)3. This was normally carried out during a major ship overhaul.\n\nThe initial procurement process was built around a “design to price” concept in which the final delivery cost per system was fixed in the contract. The SLQ-32 was designed to support the protection of ships against anti-ship missiles in an open sea environment. After initial deployment of the system, naval roles began to change requiring ships to operate much closer to shore in denser signal environments. This change in roles required changes to the SLQ-32 systems which were added over time. With experience gained working with the SLQ-32, coupled with improvements to the hardware and software, technicians and operators gradually overcame the initial problems. The SLQ-32 is now the mainstay of surface electronic warfare in the U.S. Navy and U.S. Coast Guard's WMEC Class Ships.\n\nIn 1996, a program called the \"Advanced Integrated Electronic Warfare System\" (AIEWS) was begun to develop a replacement for the SLQ-32. Designated the AN/SLY-2, AIEWS reached the prototype stage by 1999, but funding was withdrawn in April 2002 due to ballooning costs and constant delays in the projects development. It has since been replaced with \"Surface Electronic Warfare Improvement Program\" (SEWIP), which will replace the existing SLQ-32 hardware and technology in an evolutionary fashion. \nSEWIP Block 2 upgrades were first installed on Burke-class destroyers in 2014, with full-rate production scheduled for mid-2015. Block 2 improved detection capabilities; better jamming is planned from 2017, but the 2013 sequestration cuts may push this date back a year.\n\nSEWIP Block 2 was tested on in December 2014.\n\n\n"}
{"id": "197037", "url": "https://en.wikipedia.org/wiki?curid=197037", "title": "Artificial organ", "text": "Artificial organ\n\nAn artificial organ is an engineered device or tissue that is implanted or integrated into a human — interfacing with living tissue — to replace a natural organ, to duplicate or augment a specific function or functions so the patient may return to a normal life as soon as possible. The replaced function does not have to be related to life support, but it often is. For example, replacement bones and joints, such as those found in hip replacements, could also be considered artificial organs.\n\nImplied by definition, is that the device must not be continuously tethered to a stationary power supply or other stationary resources such as filters or chemical processing units. (Periodic rapid recharging of batteries, refilling of chemicals, and/or cleaning/replacing of filters would exclude a device from being called an artificial organ.) Thus, a dialysis machine, while a very successful and critically important life support device that almost completely replaces the duties of a kidney, is not an artificial organ.\n\nConstructing and installing artificial organs, an extremely research-intensive and expensive process initially, may entail many years of ongoing maintenance services not needed by a natural organ.:\n\n\nThe use of any artificial organ by humans is almost always preceded by extensive experiments with animals. Initial testing in humans is frequently limited to those either already facing death or who have exhausted every other treatment possibility.\n\nArtificial arms and legs, or prosthetics, are intended to restore a degree of normal function to amputees. Mechanical devices that allow amputees to walk again or continue to use two hands have probably been in use since ancient times, the most notable one being the simple peg leg. Since then, the development of artificial limbs has progressed rapidly. New plastics and other materials, such as carbon fiber have allowed artificial limbs to become stronger and lighter, limiting the amount of extra energy necessary to operate the limb. Additional materials have allowed artificial limbs to look much more realistic. Prostheses can roughly be categorized as upper- and lower-extremity and can take many shapes and sizes.\n\nNew advances in artificial limbs include additional levels of integration with the human body. Electrodes can be placed into nervous tissue, and the body can be trained to control the prosthesis. This technology has been used in both animals and humans. The prosthetic can be controlled by the brain using a direct implant or implant into various muscles.\n\nThe two main methods for replacing bladder function involve either redirecting urine flow or replacing the bladder \"in situ\". Standard methods for replacing the bladder involve fashioning a bladder-like pouch from intestinal tissue. As of 2017 methods to grow bladders using stem cells had been attempted in clinical research but this procedure was not part of medicine.\n\nNeural prostheses are a series of devices that can substitute a motor, sensory or cognitive modality that might have been damaged as a result of an injury or a disease.\n\nNeurostimulators, including deep brain stimulators, send electrical impulses to the brain in order to treat neurological and movement disorders, including Parkinson's disease, epilepsy, treatment resistant depression, and other conditions such as urinary incontinence. Rather than replacing existing neural networks to restore function, these devices often serve by disrupting the output of existing malfunctioning nerve centers to eliminate symptoms.\n\nTo treat erectile dysfunction, both corpora cavernosa can be irreversibly surgically replaced with manually inflatable penile implants. This is a drastic therapeutic surgery meant only for men who suffer from complete impotence who have resisted all other treatment approaches. An implanted pump in the (groin) or (scrotum) can be manipulated by hand to fill these artificial cylinders, normally sized to be direct replacements for the natural corpora cavernosa, from an implanted reservoir in order to achieve an erection.\n\nIn cases when a person is profoundly deaf or severely hard of hearing in both ears, a cochlear implant may be surgically implanted. Cochlear implants bypass most of the peripheral auditory system to provide a sense of sound via a microphone and some electronics that reside outside the skin, generally behind the ear. The external components transmit a signal to an array of electrodes placed in the cochlea, which in turn stimulates the cochlear nerve.\n\nIn the case of an outer ear trauma, a craniofacial prosthesis may be necessary.\n\nThe most successful function-replacing artificial eye so far is actually an external miniature digital camera with a remote unidirectional electronic interface implanted on the retina, optic nerve, or other related locations inside the brain. The present state of the art yields only partial functionality, such as recognizing levels of brightness, swatches of color, and/or basic geometric shapes, proving the concept's potential.\n\nVarious researchers have demonstrated that the retina performs strategic image preprocessing for the brain. The problem of creating a completely functional artificial electronic eye is even more complex. Advances towards tackling the complexity of the artificial connection to the retina, optic nerve, or related brain areas, combined with ongoing advances in computer science, are expected to dramatically improve the performance of this technology.\n\nCardiovascular-related artificial organs are implanted in cases where the heart, its valves, or another part of the circulatory system is in disorder. The artificial heart is typically used to bridge the time to heart transplantation, or to permanently replace the heart in case heart transplantation is impossible. Artificial pacemakers represent another cardiovascular device that can be implanted to either intermittently augment (defibrillator mode), continuously augment, or completely bypass the natural living cardiac pacemaker as needed. Ventricular assist devices are another alternative, acting as mechanical circulatory devices that partially or completely replace the function of a failing heart, without the removal of the heart itself.\n\nBesides these, lab-grown hearts and 3D bioprinted hearts are also being researched. Currently, scientists are limited in their ability to grow and print hearts due to difficulties in getting blood vessels and lab-made tissues to function cohesively.\n\nHepaLife is developing a bioartificial liver device intended for the treatment of liver failure using stem cells. The artificial liver is designed to serve as a supportive device, either allowing the liver to regenerate upon failure, or to bridge the patient's liver functions until transplant is available. It is only made possible by the fact that it uses real liver cells (hepatocytes), and even then, it is not a permanent substitute.\n\nResearchers from Japan found that a mixture of human liver precursor cells (differentiated from human induced pluripotent stem cells [iPSCs]) and two other cell types can spontaneously form three-dimensional structures dubbed “liver buds.”\n\nWith some almost fully functional, artificial lungs promise to be a great success in the near future. An Ann Arbor company MC3 is currently working on this type of medical device.\n\nExtracorporeal membrane oxygenation (ECMO) can be used to take significant load off of the native lung tissue and heart. In ECMO, a one or more catheters are placed into the patient and a pump is used to flow blood over hollow membrane fibers, which exchange oxygen and carbon dioxide with the blood. Similar to ECMO, Extracorporeal CO2 Removal (ECCO2R) has a similar set-up, but mainly benefits the patient through carbon dioxide removal, rather than oxygenation, with the goal of allowing the lungs to relax and heal.\n\nReproductive age patients who develop cancer often receive chemotherapy or radiation therapy, which damages oocytes and leads to early menopause. An artificial human ovary has been developed at Brown University with self-assembled microtissues created using novel 3-D petri dish technology. In a study funded and conducted by the NIH in 2017, scientists were successful in printing 3-D ovaries and implanting them in sterile mice. In the future, scientists hope to replicate this in larger animals as well as humans. The artificial ovary will be used for the purpose of in vitro maturation of immature oocytes and the development of a system to study the effect of environmental toxins on folliculogenesis.\n\nAn artificial pancreas is used to substitute endocrine functionality of a healthy pancreas for diabetic and other patients who require it. It can be used to improve insulin replacement therapy until glycemic control is practically normal as evident by the avoidance of the complications of hyperglycemia, and it can also ease the burden of therapy for the insulin-dependent. Approaches include using an insulin pump under closed loop control, developing a bio-artificial pancreas consisting of a biocompatible sheet of encapsulated beta cells, or using gene therapy.\n\nAn implantable machine that performs the function of a thymus does not exist. However, researchers have been able to grow a thymus from reprogrammed fibroblasts. They expressed hope that the approach could one day replace or supplement neonatal thymus transplantation.\n\nThe field of artificial tracheas went through a period of high interest and excitement with the work of Paolo Macchiarini at the Karolinska Institute and elsewhere from 2008 to around 2014, with front-page coverage in newspapers and on television. Concerns were raised about his work in 2014 and by 2016 he had been fired and high level management at Karolinska had been dismissed, including people involved in the Nobel Prize.\n\nAs of 2017 engineering a trachea—a hollow tube lined with cells—had proved more challenging then originally thought; challenges include the difficult clinical situation of people who present as clinical candidates, who generally have been through multiple procedures already; creating an implant that can become fully developed and integrate with host while withstanding respiratory forces, as well as the rotational and longitudinal movement the trachea undergoes.\n\nIt is also possible to construct and install an artificial organ to give its possessor abilities that are not naturally occurring. Research is proceeding in areas of vision, memory, and information processing. Some current research focuses on restoring short-term memory in accident victims and long-term memory in dementia patients.\n\nOne area of success was achieved when Kevin Warwick carried out a series of experiments extending his nervous system over the internet to control a robotic hand and the first direct electronic communication between the nervous systems of two humans.\n\nThis might also include the existing practice of implanting subcutaneous chips for identification and location purposes (ex. RFID tags).\n\nOrgan chips are devices containing hollow microvessels filled with cells simulating tissue and/or organs as a microfluidic system that can provide key chemical and electrical signal information.\n\nThis information can create various applications such as creating \"human in vitro models\" for both healthy and diseased organs, drug advancements in toxicity screening as well as replacing animal testing.\n\nUsing 3D cell culture techniques enables scientists to recreate the complex extracellular matrix, ECM, found in in vivo to mimic human response to drugs and human diseases.\nOrgans on chips are used to reduce the failure rate in new drug development; microengineering these allows for a microenvironment to be modeled as an organ.\n\n\n\n"}
{"id": "41133512", "url": "https://en.wikipedia.org/wiki?curid=41133512", "title": "Bidirectional current", "text": "Bidirectional current\n\nA bidirectional current is one which both charges and discharges at once.\n\nComplicated systems which have integrated recharging capability sometimes resort to using bidirectional currents, as in Laptops or other systems. Monitoring of a bidirectional current is required for a laptop to report the battery level and charging status. Components are available for this purpose.\n\n"}
{"id": "40044449", "url": "https://en.wikipedia.org/wiki?curid=40044449", "title": "Cannery Workers and Farm Laborers Union, Local 7", "text": "Cannery Workers and Farm Laborers Union, Local 7\n\nThe Cannery Workers and Farm Laborers Union, Local 7 was the first Filipino-led union in the United States.\n\nFounded in 1933 as the Cannery Workers and Farm Laborers Union, Local 18257 of the American Federation of Labor (AFL), it represented Alaska salmon cannery workers and farm workers. In 1937, the union became Cannery Workers and Farm Laborers Union, Local 7 of the United Cannery, Agricultural, Packing, and Allied Workers of America. In 1945, Local 7 became affiliated with the Food, Tobacco, Agricultural, and Allied Workers. In 1951 the union became Local 37 of International Longshore and Warehouse Union, and around 1987 it became Region 37 of IBU/ILWU. The membership historically was Filipino American cannery workers.\n\nThe Cannery Workers' and Farm Laborers' Union was organized June 19, 1933 in Seattle to represent the primarily Filipino-American laborers who worked in the Alaska salmon canneries. Filipino Alaskeros first appeared in the canneries around 1911. In the 1920s as exclusionary immigration laws went into effect, they replaced the Japanese, who had replaced the Chinese in the canneries. Workers were recruited through labor contractors who were paid to provide a work crew for the summer canning season. The contractor paid workers wages and other expenses. This system led to many abuses and harsh working conditions from which grew the movement toward unionization.\n\nThe CWFLU, under the leadership of its first President, Virgil Duyungan, was chartered as Local 19257 by the American Federation of Labor in 1933. On December 1, 1936 an agent of a labor contractor murdered Duyungan and Secretary Aurelio Simon. Despite this setback, the union was able to win a hiring hall and end the contract labor system in 1937. After Duyungan's death, Conrad Espe, A Norwegian American labor organizer, took the leading role in the union. Under the leadership of Duyungan and Espe, the CWFLU made numerous attempts to organize farm workers during the winter months. Farm Division organizers attempted to organize workers in Yakima, Kent, Everett, Bainbridge Island and the White River area, but were often met with harsh opposition from local officials and vigilantes.\n\nLocal 18257 came into conflict with the AFL, in 1937 when the parent body, attempting to separate the union along racial lines, recognized a Japanese local organized by Clarence Arai. Local 18257 successfully retained negotiation rights and dispatched its workers in 1937 despite pickets set up by the rival group. Bitterness toward the AFL resulted from the incidents and led to a November 4 vote by the Seattle, Portland, and San Francisco locals to affiliate with the newly formed United Cannery, Agricultural, Packing, and Allied Workers of America-CIO (UCAPAWA). In Seattle, Local 18257 became UCAPAWA, Local 7 and in San Francisco and Portland Cannery Workers unions also joined UCAPAWA Opponents of re-affiliation, led by John Ayamo and called the \"defeated candidates party,\" received the old 18257 charter and challenged Local 7 for the right to represent cannery workers. On May 4, 1938 the issue was settled in Local 7's favor in a National Labor Relations Board (NLRB) supervised election. The industry representative, Canned Salmon Industry Inc., subsequently recognized the victorious union. Ayamo later formed another AFL union, the Alaska Fish Cannery Workers, under the jurisdiction of the Seafarers International Union. In 1937 also, the CWFLU merged with a rival, the Filipino Protective Association. I.R. Cabatit was president of the union during the period of rivalry with the AFL. When he was succeeded by Trinidad Rojo in 1939, the CWFLU, Local 7 was on the verge of bankruptcy. It was discovered that officers had been selling membership cards, misappropriating funds and neglecting their duties. Rojo cut expenses and returned the union to a sound financial footing.\n\n\n"}
{"id": "31834230", "url": "https://en.wikipedia.org/wiki?curid=31834230", "title": "Deimos Imaging", "text": "Deimos Imaging\n\nDeimos Imaging is a Spanish company which operates a complete Remote Sensing system. The system comprises the satellites Deimos-1 and Deimos-2, the ground stations at Boecillo near Valladolid and Puertollano near Ciudad Real, the reception hardware hosted at KSAT, Svalbard, and Inuvik and kiruna at SSC and an image processing laboratory with generation of agricultural and environment products also at Boecillo.\n\nDeimos-1 satellite has been operational since its launch on 29 July 2009 and the company has been commercializing imagery ever since. The goal of the company is to provide imagery data globally at the highest quality standards. 19 June 2014, Deimos-2 was launched as well.\n\n"}
{"id": "25717498", "url": "https://en.wikipedia.org/wiki?curid=25717498", "title": "Desktop and mobile Architecture for System Hardware", "text": "Desktop and mobile Architecture for System Hardware\n\nDesktop and mobile Architecture for System Hardware (DASH) is a Distributed Management Task Force (DMTF) standard.\n\nIn April 2007 the Desktop and Mobile Working Group (DMWG) of the DMTF started work on an implementation requirements specification (DSP0232). Version, DASH 1.1, was published in December 2007 and became a DMTF standard in June 2009.\n\nIn-service and out-of-service systems can be managed, with manageability aligned between the modes, independent of operating system state. Both HTTP and HTTPS management ports are supported: TCP ports 623 and 664, respectively, for connections from remote management consoles to DASH out-of-band management access points (MAP).\n\nThe DMTF Common Information Model (CIM) schema defines the supported DASH management data and operations. There are 28 CIM profiles supported in the DASH 1.1 specification.\n\nDASH uses the DMTF's Web Services for Management (WS-Management) protocol for communication of CIM objects and services.\n\nThe web services expose a common set of operations for system management:\n\nDiscovery of access points is a two phase process:\n\nDASH management protocols layers include:\n\nThe DMTF CIM Profiles supported by the DASH 1.1 specification:\n\nDASH is designed for desktop and mobile computer systems; a related DMTF standard for management of server computer systems id the Systems Management Architecture for Server Hardware (SMASH), with a similar set of CIM Profiles.\n\nIntel Active Management Technology is a compliant implementation of DASH.\n\n\n"}
{"id": "34434308", "url": "https://en.wikipedia.org/wiki?curid=34434308", "title": "Diffused lighting camouflage", "text": "Diffused lighting camouflage\n\nDiffused lighting camouflage was a form of active camouflage using counter-illumination to enable a ship to match its background, the night sky, that was tested by the Royal Canadian Navy on corvettes during World War II. The principle was discovered by a Canadian professor, Edmund Godfrey Burr, in 1940. It attracted interest because it could help to hide ships from submarines in the Battle of the Atlantic, and the research project began early in 1941. The Royal Navy and the US Navy carried out further equipment development and trials between 1941 and 1943.\n\nThe concept behind diffused lighting camouflage was to project light on to the sides of a ship, to make its brightness match its background. Projectors were mounted on temporary supports attached to the hull and the prototype was developed to include automatic control of brightness using a photocell. The concept was never put into production, though the Canadian prototypes did briefly see service. The Canadian ideas were adapted by the US Air Force in its Yehudi lights project.\n\nDiffused lighting camouflage was explored by the Royal Canadian Navy (RCN) and tested at sea on corvettes during World War II, and later in the armed forces of the UK and the US.\n\nAn equivalent strategy, known to zoologists as counter-illumination, is used by many marine organisms, notably cephalopods including the midwater squid, \"Abralia veranyi\". The underside is covered with small photophores, organs that produce light. The squid varies the intensity of the light according to the brightness of the sea surface far above, providing effective camouflage by lighting out the animal's shadow.\n\nIn 1940, Edmund Godfrey Burr, a Canadian professor at McGill University, serendipitously stumbled on the principle of counter-illumination, or as he called it \"diffused-lighting camouflage\". Burr had been tasked by Canada's National Research Council (NRC) to evaluate night observation instruments. With these, he found that aircraft flying without navigation lights remained readily visible as silhouettes against the night sky, which was never completely black. Burr wondered if he could camouflage planes by somehow reducing this difference in brightness. One night in December 1940, Burr saw a plane coming in to land over snow suddenly vanish: light reflected from the snow had illuminated the underside of the plane just enough to cancel out the difference in brightness, camouflaging the plane perfectly. Burr informed the NRC, who told the RCN. They realized that the technique could help to hide ships from German submarines in the Battle of the Atlantic. Before the introduction of centimetre radar, submarines with their small profile could see convoy ships before they were themselves seen. Diffused lighting camouflage might, the RCN believed, redress the balance.\n\nBurr was quickly called to Canada's Naval Services Headquarters to discuss how to apply diffused lighting camouflage. Simple tests in the laboratory served as proof of concept. In January 1941, sea trials began on the new corvette HMCS \"Cobalt\". She was fitted with ordinary light projectors—neither designed for robustness, nor waterproofed—on temporary supports on one side of the hull; brightness was controlled manually. The trial was sufficiently promising for a better prototype to be developed.\n\nThe second version, with blue-green filters over the projectors, was trialled on board the corvette HMCS \"Chambly\" in May 1941. This gave better results as the filters removed the reddish bias to the lamps when at low intensity (lower colour temperature). The supports too were retractable, so the delicate projectors could be stowed away for protection when not in use. This second version reduced \"Chambly's\" visibility by 50% in most conditions, and sometimes by as much as 75%. This was enough to justify development of a more robust version.\n\nThe third version featured a photocell to measure the brightness of the night sky and the ship's side; the projectors' brightness was automatically controlled to balance out the difference. It was tested in September 1941 on the corvette HMCS \"Kamloops\".\n\nParallel trials of the Canadian diffused lighting equipment were carried out in March 1941 by the Royal Navy on the corvette HMS \"Trillium\" in the Clyde approaches.\nThe Admiralty report on the \"Trillium\" trials stated that \"Under certain weather conditions, the Canadian trials, in spite of the crude equipment used, gave highly satisfactory results. The experience gained during the present trials indicated that in various other types of weather this same equipment gave a much less conclusive indication of its value\", and described the technical difficulties that any future version would face.\nThe Admiralty informed the prime minister, Winston Churchill, at the end of that month, stating that the \"results seem quite promising\".\nChurchill replied the next day, suggesting that \"Surely all this business should be pressed forward on a broader front than the one ship?\"\nAccordingly, in April 1941 the Admiralty ordered further development work for \"full scale trials\". The British General Electric Company developed a manually operated diffused lighting system, which was trialled on the ocean boarding vessel HMS \"Largs\" and the light cruiser HMS \"Penelope\". \nThe \"Largs\" surface observation trials were conducted between 25 January and 6 February 1942; air observation trials, using Hudson bombers, took place on the nights of 4/5 February and 25/26 March 1942. They found an average reduction in the range at which the ship could be seen at night from another ship of around 25% using binoculars, 33% using the naked eye; the results from the air were less conclusive. The best case was on the exceptionally clear moonless night of 29/30 January 1942, when \"Largs\" could be seen from a surface ship with the naked eye at unlighted, but only with her diffused lighting, a 57% reduction.\nBy June 1942, Royal Navy commanders considered that camouflage was largely unnecessary, given that the enemy would be using RDF and submarine hydrophones. In April 1943, the Admiralty decided that diffused lighting was impractical, and development was halted, though discussions continued with the Canadian Navy.\n\nThe US Navy trialled an automatic system made by General Electric of New York on the supply ship USS \"Hamul\", but halted research in 1942. The US Navy sent its control system and diffused lighting fittings to Canada's NRC, which installed it on the corvettes HMCS \"Edmundston\" and HMCS \"Rimouski\" in 1943 and carried out further prototyping.\n\nBoth \"Edmundston\" and \"Rimouski\" were fitted with about 60 light projectors: those on the hull were on retractable supports; those on the superstructure were on fixed supports. Each ship's diffused lighting system was tested systematically in St Margaret's Bay, and then trialled when actually escorting Atlantic convoys in 1943. Experimentally, the diffused lighting reduced the ships' visibility by up to 70%, but at sea the electrical equipment proved too delicate, and frequently malfunctioned. Worse, the system was slow to respond to changes in background lighting, and the Canadian Navy considered the lighting too green.\n\nIn September 1943, \"Rimouski\", using her diffused lighting system, but also some navigation lights, approached in the Baie des Chaleurs. The intention was to make Rimouski appear as \"a small and inoffensive ship\" in an operation to trap the submarine, and this appears to have worked as the U-boat did not detect her. However the attack failed, as a wrong signal sent from shore alerted the submarine's commander, Kapitänleutnant Schauenburg; \"U-536\" dived and escaped. \n\nFollowing Allied victory in the Battle of the Atlantic – through long-range aircraft, radar, code decryption, and better escort tactics – the need to camouflage ships from submarines greatly decreased, and diffused lighting research became a low priority. The work was halted when the war ended.\n\nBecause submarines at the surface could see the dark shape of an attacking aircraft against the night sky, the principle of diffused lighting camouflage also applied to aircraft. However, British researchers found that the amount of electrical power required to camouflage an aircraft's underside in daylight was prohibitive, while externally mounted light projectors disturbed the aircraft's aerodynamics.\n\nAn American version, \"Yehudi\", using lamps mounted in the aircraft's nose and the leading edges of the wings, was trialled in B-24 Liberators, Avenger torpedo bombers and a Navy glide bomb from 1943 to 1945. By directing the light forwards towards an observer (rather than towards the aircraft's skin), the system provided effective counter-illumination camouflage with an affordable use of energy, more like that of marine animals than the Canadian diffused lighting approach. But the system never entered active service, as radar became the principal means of detecting aircraft.\n\n\n\n"}
{"id": "242495", "url": "https://en.wikipedia.org/wiki?curid=242495", "title": "Diffusion (business)", "text": "Diffusion (business)\n\nDiffusion is the process by which a new idea or new product is accepted by the market. The rate of diffusion is the speed with which the new idea spreads from one consumer to the next. Adoption (the reciprocal process as viewed from a consumer perspective rather than distributor) is similar to diffusion except that it deals with the psychological processes an individual goes through, rather than an aggregate market process. In economics it is more often named \"technological change\".\n\nThere are several theories that purport to explain the mechanics of diffusion:\n\nAccording to Everett M. Rogers, the rate of diffusion is influenced by:\n\nThere are several types of diffusion rate models:\n\n\n"}
{"id": "47747479", "url": "https://en.wikipedia.org/wiki?curid=47747479", "title": "Domain-validated certificate", "text": "Domain-validated certificate\n\nA domain-validated certificate (DV) is an X.509 digital certificate typically used for Transport Layer Security (TLS) where the domain name of the applicant has been validated by proving some control over a DNS domain.\n\nThe sole criterion for a domain-validated certificate is proof of control over whois records, DNS records file, email or web hosting account of a domain. Typically control over a domain is determined using one of the following:\n\n\nA domain-validated certificate is distinct from an Extended Validation Certificate in that this is the only requirement for issuing the certificate. In particular, domain-validated certificates do not assure that any particular legal entity is connected to the certificate, even if the domain name may imply a particular legal entity controls the domain.\n\nMost web browsers may show a lock (often in grey, rather than the green lock typically used for an Extended Validation Certificate) and a DNS domain name. A legal entity is never displayed, as domain-validated certificates do not include a legal entity in their subject.\n\n\nAs the low assurance requirements allow domain-validated certificates to be issued quickly without requiring human intervention, domain-validated certificates have a number of unique characteristics:\n"}
{"id": "1021673", "url": "https://en.wikipedia.org/wiki?curid=1021673", "title": "Environmentally friendly", "text": "Environmentally friendly\n\nEnvironmentally friendly or environment-friendly, (also referred to as eco-friendly, nature-friendly, and green) are sustainability and marketing terms referring to goods and services, laws, guidelines and policies that claim reduced, minimal, or no harm upon ecosystems or the environment. Companies use these ambiguous terms to promote goods and services, sometimes with additional, more specific certifications, such as ecolabels. Their overuse can be referred to as greenwashing.\n\nThe International Organization for Standardization has developed ISO 14020 and ISO 14024 to establish principles and procedures for environmental labels and declarations that certifiers and eco-labellers should follow. In particular, these standards relate to the avoidance of financial conflicts of interest, the use of sound scientific methods and accepted test procedures, and openness and transparency in the setting of standards.\n\nProducts located in members of the European Union can use the EU's Eco-label pending the EU's approval. EMAS is another EU label that signifies whether an organization management is green as opposed to the product. Germany also uses the Blue Angel, based on Germany's standard.\n\nIn the United States, environmental marketing claims require caution. Ambiguous titles such as \"environmentally friendly\" can be confusing without a specific definition; some regulators are providing guidance. The United States Environmental Protection Agency has deemed some ecolabels misleading in determining whether a product is truly \"green\".\n\nIn Canada, one label is that of the Environmental Choice Program. Created in 1988, only products approved by the program are allowed to display the label.\n\nThe Energy Rating Label is a Type III label that provides information on \"energy service per unit of energy consumption\". It was first created in 1986, but negotiations led to a redesign in 2000.\n\nThe environmentally friendly trends are marketed with a different color association, using the color blue for clean air and clean water, as opposed to green in western cultures. Japanese and Korean built hybrid vehicles use the color blue instead of green all throughout the vehicle, and use the word \"blue\" indiscriminately.\n\nEnergy Star is a program with a primary goal of increasing energy efficiency and indirectly decreasing greenhouse gas emissions. Energy Star has different sections for different nations or areas, including the United States, the European Union and Australia. The program, which was founded in the United States, also exists in Canada, Japan, New Zealand, and Taiwan.\n"}
{"id": "2102443", "url": "https://en.wikipedia.org/wiki?curid=2102443", "title": "Export Administration Regulations", "text": "Export Administration Regulations\n\nThe Export Administration Regulations (EAR) are a set of regulations found at 15 C.F.R. § 730 \"et seq\". They are administered by the Bureau of Industry and Security, which is part of the US Commerce Department. In general, the EAR govern whether a person may export a thing from the U.S., reexport the thing from a foreign country, or transfer a thing from one person to another in a foreign country. The EAR apply to physical things (sometimes referred to as \"commodities\") as well as technology and software.\n\nThe EAR have very broad application. With only the exceptions noted below, the EAR apply to the following categories of things:\n\n\nThe EAR do not apply, however, to the following:\n\n\nThe EAR contain a list called the Commerce Control List (CCL). The CCL is a limited list of items within the scope of the EAR which merit particular attention because they could potentially have a military use in addition to a commercial use. CCL-listed items are therefore often referred to as \"dual use.\" The CCL, however, is not an exhaustive list of things that are within the scope of the EAR; to the contrary, the overwhelming majority of things that fall within the scope of the EAR are not listed on the CCL; instead, they are given the designation \"EAR 99.\"\n\nItems that are listed on the CCL are organized according to alpha-numeric designations called \"Export Control Classification Numbers\" (ECCNs).\n\nThe EAR contain a list of rules called the 10 General Prohibitions, which provide as follows:\n\nYou may not, without a license or License Exception, export any item subject to the EAR to another country or reexport any item of U.S.-origin if each of the following is true:\n\n(i) The item is controlled for a reason indicated in the applicable Export Control Classification Number (ECCN), and\n\n(ii) Export to the country of destination requires a license for the control reason as indicated on the Country Chart at part 738 of the EAR. \n\nYou may not, without a license or license exception, reexport or export from abroad foreign-made commodities that incorporate controlled U.S.-origin commodities, foreign-made commodities that are “bundled” with controlled U.S.-origin software, foreign-made software that is commingled with controlled U.S.-origin software, or foreign-made technology that is commingled with controlled U.S.-origin technology if such items require a license according to any of the provisions in the EAR and incorporate or are commingled with more than a \"de minimis\" amount of controlled U.S. content, as defined in §734.4 of the EAR concerning the scope of the EAR.\n\nGeneral Prohibition 3 applies to certain items that are produced outside of the U.S. and that are the \"direct product\" of U.S. technology or software, or they are developed from a plat which is the \"direct product\" of U.S. technology or software.\n\nUnder General Prohibition 3, you may not, without a license or license exception, reexport any item that meets the direct product test to a destination in Country Group D:1, E:1, or E:2 (See supplement no.1 to part 740 of the EAR). Additionally, you may not, without a license or license exception, reexport or export from abroad any ECCN 0A919 commodities (foreign-made military commodities) that meet the direct product test to a destination in Country Group D:1, D:3, D:4, D:5, E:1, or E:2.\n\nYou may not take any action that is prohibited by a denial order issued under 15 CFR Part 766. These orders prohibit many actions in addition to direct exports by the person denied export privileges, including some transfers within a single country, either in the United States or abroad, by other persons. You are responsible for ensuring that any of your transactions in which a person who is denied export privileges is involved do not violate the terms of the order. Orders denying export privileges are published in the Federal Register when they are issued and are the legally controlling documents in accordance with their terms. BIS also maintains compilations of persons denied export privileges on its Web site at \"<nowiki>http://www.bis.doc.gov</nowiki>.\" BIS may, on an exceptional basis, authorize activity otherwise prohibited by a denial order. See 15 CFR §764.3(a)(2).\n\nYou may not, without a license, knowingly export or reexport any item subject to the EAR to an end-user or end-use that is prohibited by part 744 of the EAR.\n\nYou may not, without a license or License Exception authorized under part 746, export or reexport any item subject to the EAR to any of the following countries:\n\n\nU.S. persons may not perform certain activities relating to nuclear explosive devices, missiles, and chemical or biological weapons, as set out in 15 CFR § 744.6.\n\nUnder General Prohibition 8, if you export or reexport an item, it may not pass through any of the following countries without a license : \n\n\nYou may not violate terms or conditions of a license or of a License Exception issued under or made a part of the EAR, and you may not violate any order issued under or made a part of the EAR. \n\nYou may not sell, transfer, export, reexport, finance, order, buy, remove, conceal, store, use, loan, dispose of, transport, forward, or otherwise service, in whole or in part, any item subject to the EAR and exported or to be exported with knowledge that a violation of the Export Administration Regulations, the Export Administration Act or any order, license, License Exception, or other authorization issued thereunder has occurred, is about to occur, or is intended to occur in connection with the item. Nor may you rely upon any license or License Exception after notice to you of the suspension or revocation of that license or exception.\n\nWith a few exceptions, the EAR define \"export\" to mean:\n\n\nOnce a thing has been exported from the United States to a foreign country, the EAR define \"reexport\" to mean a subsequent export of the thing from the first foreign country to a second foreign country, and to any subsequent export after that.\n\nThe EAR define \"technology\" to mean information necessary for the “development,” “production,” “use,” operation, installation, maintenance, repair, overhaul, or refurbishing (or other terms specified in ECCNs on the CCL that control “technology”) of an item.\n\n“Technology” may be in any tangible or intangible form, such as written or oral communications, blueprints, drawings, photographs, plans, diagrams, models, formulae, tables, engineering designs and specifications, computer-aided design files, manuals or documentation, electronic media or information revealed through visual inspection.\n\n\n"}
{"id": "14525752", "url": "https://en.wikipedia.org/wiki?curid=14525752", "title": "Free standard", "text": "Free standard\n\nA free standard or libre standard is a standard whose specification is publicly available. The concept of Free/Libre standards emerged in the software industry as a reaction against closed \"de facto\" \"standards\" which served to reinforce monopolies.\nUsers of a free standard have the same four freedoms associated with free software, and the freedom to participate in its development process. The standardisation process typically requires a complete free software reference implementation, which demonstrates that it is implementable and renders it usable. A libre standard is not patent-encumbered.\n\nThe Free Standards Group, for example, developed standards and released them under the GNU Free Documentation License with no cover texts or invariant sections. Reference implementations and test suites, etc. were released as Free software.\n\nSimilar processes are now followed by the various \"open\" standards bodies, the word \"open\" having been popularised by the \"open source\" movement in order to engage powerful industry players\n\n\n"}
{"id": "21591987", "url": "https://en.wikipedia.org/wiki?curid=21591987", "title": "Green Valley Industrial Park", "text": "Green Valley Industrial Park\n\nGreen Valley Industrial Park is a major industrial park located on the north coast of Honduras, Central America. The park is about and is operating as a free trade zone.\n\nGreen Valley Industrial Park is home for several companies focusing on the textile and apparel industry. Companies include Anvil Knits Honduras(AKH), Ceiba Textiles, Roman Knit Honduras, Pride Chemicals, FCI, Premier Narrow Fabrics, Leomar and Simtex International.\n\nThe park is about 90 minutes from Puerto Cortes, the largest port in Central America.\n\nPublic transport is available from nearby cities, special bus services provide transport from major cities like San Pedro Sula.\n\n<br>\n"}
{"id": "34061920", "url": "https://en.wikipedia.org/wiki?curid=34061920", "title": "Heliatek", "text": "Heliatek\n\nThe company Heliatek was spun off in July 2006 from the Technical University of Dresden (IAPP) and the University of Ulm. The company’s founding brought together internationally renowned expertise in the fields of organic optoelectronics and organic oligomer synthesis. Among related fields of operation, the company wants to be instrumental in establishing environmentally friendly solar energy as a widespread, commonplace technology.\n\nIn 2011 the company was recognized, by an audience other than professionals in the field, for winning the German Future Prize. The World Economic Forum awarded the firm as a Technology Pioneer in 2015.\n"}
{"id": "58630953", "url": "https://en.wikipedia.org/wiki?curid=58630953", "title": "Host cell protein", "text": "Host cell protein\n\nHost cell proteins (HCPs) are process-related impurities, expressed by the host cell used for production of biopharmaceutical proteins. During the purification process, the majority of the HCPs are removed (>99%), but small HCP amounts remain in the distributed products, such as monoclonal antibodies (mAbs), antibody-drug-conjugates (ADCs), therapeutic proteins, vaccines, and other protein-based biopharmaceuticals.\n\nNational regulatory authorizations, such as FDA and EMA, require that biopharmaceuticals must be analysed and purified to reduce HCPs to an acceptable level. The HCP acceptance level is evaluated case-by-case and depends on multiple factors including; dose, frequency of drug administration, type of drug and severity of disease. Analysis of HCPs is not simple, since the HCP mixture consists of a large number of protein species, which are unique to the specific host and not related to the intended recombinant protein.\n\nThe acceptance level of HCPs has commonly been in the range 1-100 ppm (1-100 ng/mg product) due to the detection limit of the established analytical methods. Even with these trace levels of HCPs in the final product reaching the patient, it is unknown if specific HCPs might affect protein stability or immunogenicity in the patient. If the stability is affected, durability of the active substance in a biopharmaceutical could decrease. It is also possible that the effect of the protein might be higher or lower than intended. The degree of immunogenicity on a long-term basis is practically impossible to determine and thus might be a relatively severe threat to the patient’s health.\n\nIt is crucial to characterise the HCP population in biopharmaceuticals due to the potential safety risk of introducing foreign proteins into the human immune system. With commonly applied host cell systems such as \"E. coli\", yeast, the mouse myeloma cell line (NS0) and Chinese Hamster Ovary (CHO), the genetic differences between the host system and the human body are many.\n\nIt is well established that a higher difference to human proteins increases the risk of immunogenicity and thus, a higher level of HCPs is suggested to elicit a more pronounced immune response. Several studies have linked a reduction in HCPs to a decline in specific inflammatory cytokines. Other HCPs may be very similar to a human protein and may induce an immune response with cross reactivity against the human protein or the drug substance protein. The exact consequences of HCPs for the individual patient is uncertain and difficult to determine with the current analytical methods applied in biopharmaceutical approval.\n\nDuring the production process several factors, including the genes of the host cell, the way of product expression and the purification steps, influence the HCP composition and abundance. Several studies report that HCPs often are co-purified along with the product itself by interacting with the recombinant protein. Therefore, the requirements for analytic instruments are extremely high and must be developed further to analyse the entire HCP population more thoroughly in a biopharmaceutical product.\n\nEnzyme Linked Immunosorbent Assay (ELISA) is the most commonly applied method for HCP analysis, mainly because it has been the only method with the required sensitivity to detect the low levels of HCPs. Even though the developmental process requires a significant period of work and several research animals, the analysis is rapidly performed and interpreted. However, there are several limitations associated with ELISA for HCP analysis. The HCP quantification relies mainly on the quantity and affinity of anti-HCP antibodies for detection of the HCP antigens. Anti-HCP antibody pools cannot cover the entire HCP population and weakly immunogenic proteins are impossible to detect, since equivalent antibodies are not generated in the process. It is apparent that HCP-ELISAs are insufficient alone for analysis of the HCP population, and therefore orthogonal methods providing complementary information are needed.\n\nFor a thorough evaluation on the risk of HCPs in biopharmaceuticals and for proper quality control of the manufacturing, it is of the essence that all HCPs are identified and quantified during the production process and in the final product. A suitable orthogonal method is ideally able to:\n\n\nA method, which fulfills these requirements and emerges as the primary orthogonal method to ELISA, is mass spectrometry (MS). The main advantage of MS is the ability to identify the individual proteins of low abundance, when MS is coupled to liquid chromatography (LC-MS).\n\nRecently, the MS method has been further improved through the method SWATH LC-MS. SWATH is a data independent acquisition (DIA) form of mass spectrometry, where the mass range is partitioned in small mass windows, which are then analysed with tandem MS (MS/MS). The key advantages are the reproducibility for both individual HCP identification and absolute quantification by applying internal protein standards.\n\n"}
{"id": "45454328", "url": "https://en.wikipedia.org/wiki?curid=45454328", "title": "Indigenous architecture", "text": "Indigenous architecture\n\nThe recent field of Indigenous Architecture refers to the study and practice of architecture of, for and by Indigenous people. It is a field of study and practice in the United States, Australia, New Zealand, Canada, Arctic area of Sápmi and many other countries where Indigenous people have a built tradition or aspire translate or to have their cultures translated in the built environment. This sometimes has been extended to include landscape architecture and other designs for the built environment.\n\nThe traditional or vernacular architecture of Australian Aboriginal and Torres Strait Islander people varied to meet the lifestyle, social organisation, family size, cultural and climatic needs and resources available to each community.\n\nThe types of forms varied from dome frameworks made of cane through spinifex-clad arc-shaped structures, to tripod and triangular shelters and elongated, egg-shaped, stone-based structures with a timber frame to pole and platform constructions. Annual base camp structures, whether dome houses in the rainforests of Queensland and Tasmania or stone-based houses in south-eastern Australia, were often designed for use over many years by the same family groups. Different language groups had differing names for structures. These included humpy, gunyah (or gunya), goondie, wiltja and wurley (or wurlie).\n\nUntil the 20th century, it was assumed that Aborigines lacked permanent buildings, likely because early contacts by Europeans with Indigenous populations led them to misinterpret Aboriginal ways of life. Labelling Aboriginal communities as 'nomadic' allowed early settlers to justify the takeover of traditional lands claiming that they were not inhabited by permanent residents.\n\nStone engineering was sometimes utilised by a number of Indigenous language groups. Examples of Aboriginal stone structures come from Western Victoria’s Gunditjmara peoples These builders took utilised basalt rocks around Lake Condah to erect housing and complicated systems of stone weirs, fish and eel traps and gates in water courses creeks. The lava-stone homes had circular stone walls over a metre high and topped with a dome roof made of earth or sod cladding. Evidence of sophisticated stone engineering has been found in other parts of Australia. As late as 1894, a group of around 500 people still lived in houses near Bessibelle that were constructed out of stone with sod cladding on a timber-framed dome. Nineteenth Century observers also reported flat slab slate-type stone housing in South Australia’s north-east corner. These dome-shaped homes were built on heavy limbs and used clay to fill in the gaps. In New South Wales’ Warringah area, stone shelters were constructed in an elongated egg shape and packed with clay to keep the interior dry.\n\nHousing for Indigenous people living in many parts of Australia has been characterised by an acute shortage of dwellings, poor quality construction, and housing stock ill-suited to Indigenous lifestyles and preferences. Rapid population growth, shorter lifetimes for housing stock and rising construction costs have meant that efforts to limit overcrowding and provide healthy living environments for Indigenous people have been difficult for governments to achieve. Indigenous housing design and research is a specialised field within housing studies.\nThere have been two main approaches to the design of Indigenous housing in Australia - Health and Culture.\n\nThe cultural design model attempts incorporate understandings of differences in Aboriginal cultural norms into housing design. There a large body of knowledge on Indigenous housing in Australia that promotes the provision and design of housing that supports Indigenous residents’ socio-spatial needs, domilicary behaviours, cultural values and aspirations. The culturally specific needs for Indigenous housing have been identified as major factors in the success of housing and failing to recognise the varying and diverse cultural housing needs of Indigenous peoples have been cited as the reasons for Aboriginal housing failures by academics for a number of decades. Western style housing imposes conditions on Indigenous residents that may hinder the practice of cultural norms. If adjusting to living in a particular house strains relationships, then severe stress on the occupants may result. Ross noted, \"Inappropriate housing and town planning have the capacity to disrupt social organisation, the mechanisms for maintaining smooth social relations, and support networks.\" There are a range of cultural factors which are discussed in the literature. These include designing housing to accommodate aspects of customary behaviour such as avoidance behaviours, household group structures, sleeping and eating behaviours, cultural constructs of crowding and privacy, and responses to death. All of the literature indicates that each housing design should be approached independently to recognise the many Indigenous cultures with varying customs and practices that exist across Australia.\n\nThe health approach to housing design developed as housing is an important factor affecting the health of Aboriginal and Torres Strait Islander people. Substandard and poorly maintained housing along with non-functioning infrastructure can create serious health risks. The 'Housing for Health' approach developed from observations of the housing factors affecting Aboriginal peoples' health into a methodology for measuring, rating and fixing 'household hardware' deemed essential for health. The approach is based on nine 'healthy housing principles' which are the: \n\nDefining what is 'Indigenous architecture' in the contemporary context is a debate in some spheres. Many researchers and practitioners generally agree that Indigenous architectural projects are those which are designed for Indigenous clients or projects that imbue Aboriginality through consultation Aboriginal involvement. This latter category may include projects which are designed primarily for non-Indigenous users. Notwithstanding the definition, a range of projects have been designed for, by or with Indigenous users. The application of evidence-based research and consultation has led to museums, courts, cultural centres, keeping houses, prisons, schools and a range of other institutional and residential buildings being designed to meet the varying and differing needs and aspirations of Indigenous users.\n\nNotable Projects include:\nIndigenous architecture of the 21st century has been enhanced by university-trained Indigenous architects, landscape architects and other design professionals who have incorporated different aspects of traditional Indigenous cultural references and symbolism, fused architecture with ethnoarchitectural styles and pursued various approaches to the questions of identity and architecture.\n\n\n\nThe original indigenous people of Canada developed complex building traditions thousands of years before the arrival of the first Europeans. Canada contained five broad cultural regions, defined by common climatic, geographical and ecological characteristics. Each region gave rise to distinctive building forms which reflected these conditions, as well as the available building materials, means of livelihood, and social and spiritual values of the resident peoples.\n\nA striking feature of traditional Canadian architecture was the consistent integrity between structural forms and cultural values. The wigwam, (otherwise known as 'wickiup' or 'wetu), tipi and snow house were building-forms perfectly suited to their environments and to the requirements of mobile hunting and gathering cultures. The longhouse, pit house and plank house were diverse responses to the need for more permanent building forms.\n\nThe semi-nomadic peoples of the Maritimes, Quebec, and Northern Ontario, such as the Mi'kmaq, Cree, and Algonquin generally lived in wigwams '. The wood framed structures, covered with an outer layer of bark, reeds, or woven mats; usually in a cone shape, although sometimes a dome. The groups changed locations every few weeks or months. They would take the outer layer of the structure with them, and leave the heavy wood frame in place. The frame could be reused if the group returned to the location at a later date.\n\nFurther south, in what is today Southern Ontario and Quebec the Iroquois society lived in permanent agricultural settlements holding several hundred to several thousand people. The standard form of housing was the long house. These were large structures, several times longer than they were wide holding a large number of people. They were built with a frame of saplings or branches, covered with a layer of bark or woven mats.\n\nOn the Prairies the standard form of life was a nomadic one, with the people often moving to a new location each day to follow the bison herds. Housing thus had to be portable, and the tipi was developed. The tipi consisted of a thin wooden frame and an outer covering of animal hides. The structures could be quickly erected, and were light enough to transport long distances.\n\nIn the Interior of British Columbia the standard form of home was the semi-permanent pit house, thousands of relics of which, known as quiggly holes are scattered across the Interior landscape. These were structures shaped like an upturned bowl, placed on top of a pit. The bowl, made of wood, would be covered with an insulating layer of earth. The house would be entered by climbing down a ladder at the centre of the roof. \n\nSome of the best architectural designs were made by settled people along the North American west coast. People like the Haida used advanced carpentry and joinery skills to construct large houses of red cedar planks. These were large square, solidly built houses. One advanced design was the six beam house, named for the number of beams that supported the roof, where the front of each house would be decorated with a heraldric pole that would be sometimes be brightly painted with artistic designs.\n\nIn the far north, where wood was scarce and solid shelter essential for survival, several unique and innovative architectural styles were developed. One of the most famous is the igloo, a domed structure made of snow, which was quite warm. In the summer months, when the igloos melted, tents made of seal skin, or other hides, were used. The Thule adopted a design similar to the pit houses of the BC interior, but because of the lack of wood they instead used whale bones for the frame.\n\nIn addition to meeting the primary need for shelter, structures functioned as integral expressions of their occupants' spiritual beliefs and cultural values. In all five regions, dwellings performed dual roles - providing both shelter and a tangible means of linking mankind with the universe. Building-forms were often seen as metaphorical models of the cosmos, and as such they frequently assumed powerful spiritual qualities which helped define the cultural identity of the group.\n\nThe sweat lodge is a hut, typically dome-shaped and made with natural materials, used by Indigenous peoples of the Americas for ceremonial steam baths and prayer. There are several styles of structures used in different cultures; these include a domed or oblong hut similar to a wickiup, a permanent structure made of wood and earth, or even a simple hole dug into the ground and covered with planks or tree trunks. Stones are typically heated and then water poured over them to create steam. In ceremonial usage, these ritual actions are accompanied by traditional prayers and songs.\n\nAs many more settlers arrived in Canada, indigenous peoples were strongly motivated to relocate to newly created reserves, where the Canadian government encouraged Aboriginal people to build permanent houses and adopt farming in place of their traditional hunting and trapping. Not familiar to this sedentary lifestyle, many of these people continued to using their traditional hunting grounds, but when much of southern Canada was settled in the late 1800s and early 1900s, this practice ceased ending their nomadic way of life. After World War II indigenous people were relative non-participants in the housing and economic boom Canada. Most remained on remote rural reserves often in crowded dwellings that mostly lacked basic amenities. As health services on Aboriginal reserves increased during the 1950s and 1960s, life expectancy greatly improved including dramatic drop in the infant mortality, though this may have exacerbated the existing overcrowding problem.\n\nSince the 1960s the living conditions in on-reserve housing in Canada has not improved significantly. Overcrowding remains a serious problem in many communities. Many houses are in serious need of repair and others still lack basic amenities. Poor conditions of housing on reservations has contributed to many Aboriginal people leaving reserves and migrating to urban areas of Canada, causing issues with homelessness, child poverty, tenancy, and transience.\n\nNotable projects include:\n\n\n\nKanak cultures developed in the New Caledonia archipelago over a period of three thousand years. Today, France governs New Caledonia but has not developed a national culture. The Kanak claim for independence is upheld by a culture thought of as national by the indigenous population. Kanaks have settled over all the islands officially indicated by France as New Caledonia and Dependencies. The archipelago includes the principal island, Grande Terre, Belep Islands to the north and Isle of Pines to the south. It is bordered on the east by the Loyalty Islands, consisting of three coral atolls (Mare, Lifou, and Ouvea).\n\nKanak society is organised around clans, which are both social and spatial units. The clan could initially be made up of people related through a common ancestor, comprising several families. There can be between fifty and several hundred people in a clan. This basic definition of the clan has become modified over the years due to historical situations and places involving wars, disagreements, new arrivals etc. The clan structure, therefore, evolved as new people arrived and were given a place and a role in the social organisation of the clan, or through clan members leaving to join other clans.\n\nTraditionally a village is set up in the following manner. The Chief's hut (called La Grande Case) lies at the end of a long and wide central walkway which is used for gathering and performing ceremonies. The Chief's younger brother lives in a hut at the other end. The rest of the village lives in huts along the central walkway, which is lined with auracarias or palms. Trees lined the alleys which were used as shady gathering places. For Kanak people, space is divided between premises reserved for important men and other residences placed closer to the women and children. Kanak people generally avoided being alone in empty spaces.\n\nThe inside of a Grande Case is dominated by the central pole (made out of houp wood), which holds up the roof and the rooftop spear, the flèche faîtière. Along the walls are various posts which are carved to represent ancestors. The door is flanked by two carved door posts (called Katana), who were the “sentinels who reported the arrival of strangers”. There also is a carved door step. The rooftop spear has three main parts: the spear facing up, which prevents bad spirits coming down onto ancestor. The face, which represents the ancestor. The spear on the bottom which keep bad spirits coming up to ancestor.\n\nThe flèche faîtière or a carved rooftop spear, spire or finial is the home of ancestral spirits and is characterized by three major components. The ancestor is symbolized by a flat, crowned face in the centre of the spear. The ancestor's voice is symbolized by a long, rounded pole that is run through by conch shells. The symbolic connection of the clan, through the chief, is a base, which is planted into the case's central pole. Sharply pointed wood pieces fan out from either end of the central area, symbolically preventing bad spirits from being able to reach the ancestor. It evokes, beyond a particular ancestor, the community of ancestors.<ref name=\"musees.org/louvre\"></ref> and represents the ancestral spirits, symbolic of transition between the world of the dead and the world of the living.\n\nThe arrow or the spear normally has a needle at the end to insert threaded shells from bottom to top; one of the shells contains arrangements to ensure protection of the house and the country. During wars enemies attacked this symbolic finial. After the death of a Kanak chief, the flèche faîtière is removed and his family takes it to their home. Though it is allowed to be used again, as a sign of respect, it is normally kept at burial grounds of noted citizens or at the mounds of abandoned grand houses.\n\nThe form of the buildings varied from island to island, but were generally round in plan and conical in the vertical elevation. The traditional hut features represent the organization and lifestyle of the occupants. The hut is the endogenous Kanak architectural element and built entirely of plant material taken from the surrounding forest reserve. Consequently, from one area to another, the materials used are different. Inside the hut, a hearth is built on the floor between the entrance and the centre pole that defines a collective living space covered with pandanus leaf (ixoe) woven mats, and a mattress of coconut leaves (behno). The round hut is the translation of physical and material into Kanak cultures and social relations within the clan.\n\nContemporary Kanak society has several layers of customary authority, from the 4,000-5,000 family-based clans to the eight customary areas (\"aires coutumières\") that make up the territory. Clans are led by clan chiefs and constitute 341 tribes, each headed by a tribal chief. The tribes are further grouped into 57 customary chiefdoms (\"chefferies\"), each headed by a head chief, and forming the administrative subdivisions of the customary areas.\n\nThe Jean-Marie Tjibaou Cultural Centre () designed by Italian architect Renzo Piano and opened in 1998 is the icon of the Kanak culture and contemporary Kanak architecture.\nThe Centre was constructed on the narrow Tinu Peninsula, approximately northeast of the centre of Nouméa, the capital of New Caledonia, celebrates the vernacular Kanak culture, amidst much political controversy over the independent status sought by Kanaks from French rule. It was named after Jean-Marie Tjibaou, the leader of the independence movement who was assassinated in 1989 and who had a vision of establishing a cultural centre which blended the linguistic and artistic heritage of the Kanak people.\n\nThe Kanak building traditions and the resources of modern international architecture were blended by Piano. The formal curved axial layout, long on the top of the ridge, contains ten large conical cases or pavilions (all of different dimensions) patterned on the traditional Kanak Grand Hut design. The building is surrounded by landscaping which is also inspired by traditional Kanak design elements. Marie Claude Tjibaou, widow of Jean Marie Tjibaou and current leader of the Agency for the Development of Kanak Culture (ADCK), observed: \"We, the Kanaks, see it as a culmination of a long struggle for the recognition of our identity; on the French Government’s part it is a powerful gesture of restitution.\"\n\nThe building plans, spread over an area of of the museum, were conceived to incorporate the link between the landscape and the built structures in the Kanak traditions. The people had been removed from their natural landscape and habitat of mountains and valleys and any plan proposed for the art centre had to reflect this aspect. Thus, the planning aimed at a unique building which would be, as the architect Piano stated, \"to create a symbol and ...a cultural centre devoted to Kanak civilization, the place that would represent them to foreigners that would pass on their memory to their grand children\". The model as finally built evolved after much debate in organized 'Building Workshops' in which Piano’s associate, Paul Vincent and Alban Bensa, an anthropologist of repute on Kanak culture were also involved.\n\nThe Kanak villages planning principles where made the houses in groups with the Chief’s house at the end of an open public alley formed by other buildings clustered along on both sides was adopted in the Cultural Centre planned by Piano and his associates. An important concept that evolved after deliberations in the 'building workshops, after Piano won the competition for building the art centre, also involved \"landscaping ideas\" to be created around each building. To this end, an interpretative landscape path was conceived and implemented around each building with series of vegetative cover avenues along the path that surrounded the building, but separated it from the lagoon. This landscape setting appealed to the Kanak people when the centre was inaugurated. Even the approach to the buildings from the paths catered to the local practices of walking for three quarters of the path to get to the entrance to the Cases. One critic of the building observed: \"It was very intelligent to use the landscape to introduce the building. This is the way the Kanak people can understand\".\n\nThe centre comprises an interconnected series of ten stylised grandes cases (chiefs' huts), which form three villages (covering an area of 6060 square metres). These huts have an exposed stainless-steel structure and are constructed of iroko, an African rot-resistant timber which has faded over time to reveal a silver patina evocative of the coconut palms that populate the coastline of New Caledonia. The Jean-Marie Tjibaou Cultural Centre draws materially and conceptually on its geopolitical environment, so that despite being situated on the outskirts of the capital city, it draws influence from the diverse Kanak communities residing elsewhere across Kanaky. The circling pathway that leads from the car park to the centre's entrance is lined with plants from various regions of Kanaky. Together, these represent the myth of the creation of the first human: the founding hero, Téâ Kanaké. Signifying the collaborative design process, the path and centre are organically interconnected so it is difficult to discern any discrete edges existing between the building and gardens. Similarly, the soaring huts appear unfinished as they open outward to the sky, projecting the architect's image of Kanak culture as flexible, diasporic, progressive and resistant to containment by traditional museological spaces.\n\nOther important architectural projects have included the construction of the Mwâ Ka, 12m totem pole, topped by a grande case (chief's hut) complete with flèche faîtière standing in a landscaped square opposite Musée de Nouvelle-Calédonie. Mwâ Ka means the house of mankind - in other words, a house where discussions are held. Its carvings are divided into eight cylindrical sections representing the eight customary regions of New Caledonia. Mounted on a concrete double-hulled pirogue, the Mwâ Ka symbolises the mast but also the central post of a case. At the back of the pirogue a wooden helmsman steers the ever forwards. The square's flowerbed arrangements depicting stars and moons are symbolic of navigation. The Mwâ Ka was conceived by the Kanak community to commemorate 24 September, the anniversary of the French annexation of New Caledonia in 1853. Initially a day of mourning, the creation of the Mwâ Ka (inaugurated in 2005) symbolised the end of the mourning period thus giving the date a new significance. The erection of the Mwâ Ka was a way of burying past suffering related to French colonisation and turning a painful anniversary into a day for celebrating Kanak identity and the new multi-ethnic identity of Kanaky.\n\nThe first known dwellings of the ancestors of Māori were based on houses from their Polynesian homelands (Māori are known to have migrated from eastern Polynesia no later than 850 A.D.). The Polynesians found they needed warmth and protection from a climate markedly different from the warm and humid tropical Polynesian islands. The early colonisers soon modified their construction techniques to suit the colder climate. Many traditional island building techniques were retained, using new materials: raupo reed, toetoe grass, aka vines and native timbers: totara, pukatea and manuka. Archeological evidence suggests that the design of Moa-hunter sleeping houses (850-1350 AD) was similar to that of houses found in Tahiti and eastern Polynesia. These were rectangular, round, oval, or 'boat-shaped' semi-permanent dwellings.\n\nThese buildings were semi-permanent, as people moved around looking for food sources. Houses had wooden frames covered in reeds or leaves, with mats on earth floors. To help people keep warm, houses were small, with low doors, earth insulation and a fire inside. The standard building in a Māori settlement was a simple sleeping \"whare puni\" (house/hut) about 2 metres x 3 metres with a low roof, an earth floor, no window and a single low doorway. Heating was provided by a small open fire in winter. There was no chimney. Materials used in construction varied between areas, but raupo reeds, flax and totara bark shingles for the roof were common. Similar small \"whare\", but with interior drains, were used to store kumara on sloping racks. Around the 15th century communities became bigger and more settled. People built wharepuni – sleeping houses with room for several families, and a front porch. Other buildings included pātaka (storehouses), sometimes decorated with carvings, and kāuta (cooking houses).\n\nThe classic phase (1350-1769) which was characterized by a more developed tribal society expressing itself clearly in wood carving and architecture. The most spectacular building type was the whare-whakairo, or carved\nmeeting house. This building was the focus of social and symbolic Maori assemblies, and made visible a long tribal\nhistory. The wall slabs depicted warriors, chiefs and explorers. The painted rafter patterns and tututuku panels\ndemonstrated the Maori love for land, forest and river. The whare-whakairo was a colourful synthesis of carved\narchitecture, expressing reverence for ancestors and love of nature. In the classic period, a higher proportion of \"whare\" were located inside \"pa\" than was the case after contact with Europeans. A chief's \"whare\" was similar but larger—often with full headroom in the centre, a small window and a partly enclosed front porch. In times of conflict the chief lived in a \"whare\" on the \"tihi\" or summit of a hill pa. In colder areas, such as in the North Island central plateau, it was common for \"whare\" to be partly sunk into the ground for better insulation.\nNgāti Porou ancestor, Ruatepupuke is said to have established the tradition of whare whakairo (carved meeting houses) on the East Coast. Whare whakairo are often named after ancestors and considered to embody that person. The house is seen as an outstretched body, and can be addressed like a living being. A wharenui (literally 'big house' alternatively known as\"meeting houses\", \"whare rūnanga\" or \"whare whakairo\" (literally \"carved house\") is a communal house generally situated as the focal point of a marae. The present style of wharenui originated in the early to middle nineteenth century. The houses are often carved inside and out with stylised images of the iwi's ancestors, with the style used for the carvings varying from iwi to iwi. The houses always have names, sometimes the name of an ancestor or sometimes a figure from Māori mythology. While a meeting house is considered sacred, it is not a church or house of worship, but religious rituals may take place in front of or inside a meeting house. On most marae, no food may be taken into the meeting house.\n\nFood was not cooked in the sleeping \"whare\" but in the open or under a \"kauta\" (lean-to). Saplings with branches and foliage removed were used to store and dry item such as fishing nets or cloaks. Valuable items were stored in pole-mounted storage shelters called \"pataka\". Other constructions were large racks for drying split fish.\n\nThe \"marae\" was the central place of the village where culture can be celebrated and intertribal obligations can be met and customs can be explored and debated, where family occasions such as birthdays can be held, and where important ceremonies, such as welcoming visitors or farewelling the dead (\"tangihanga\"), can be performed.\n\nThe building often symbolises an ancestor of the wharenui's tribe. So different parts of the building refer to body parts of that ancestor:\n\n\nOther important components of the wharenui include:\n\n\nRau Hoskins defines Māori architecture as anything that involves a Māori client with a Māori focus. “I think traditionally Māori architecture has been confined to marae architecture and sometimes churches, and now Māori architecture manifests across all environments, so we have Māori immersion schools, Māori medical centres and health clinics, Māori tourism ventures, and papa kāinga or domestic Māori villages. So the opportunities that exist now are very diverse. The kaupapa (purpose or reason) for the building and client’s aspirations are the key to how the architecture manifests.”\n\nFrom the 1960s, marae complexes were built in urban areas. In contemporary context these generally comprise a group of buildings around an open space, that frequently host events such as weddings, funerals, church services and other large gatherings, with traditional protocol and etiquette usually observed. They also serve as the base of one or sometimes several hapū. The \"marae\" is still \"wāhi tapu\", a 'sacred place' which carries cultural meaning. They included buildings such as wharepaku (toilets) and whare ora (health centres). Meeting houses were still one large space with a porch and one door and window in front. In the 1980s marae began to be built in prisons, schools and universities.\n\nNotable projects include:\n\n\n\nSápmi is the term for Sámi (also Saami) traditional lands. The Sámi people are the Indigenous people of the northern part of the Scandinavian Peninsula and large parts of the Kola Peninsula, which encompasses parts of far northern Norway, Sweden, Finland, and Russia, and the border area between south and middle Sweden and Norway. The Sámi are the only Indigenous people of Scandinavia recognized and protected under the international conventions of indigenous people, and the northernmost Indigenous people of Europe. Sámi ancestral lands span an area of approximately 388,350 km (150,000 sq. mi.) across the Nordic countries.\n\nThere are a number of Sámi ethnoarchitectural forms; including the lavvu, goahti, the Finnish laavu.The differences between the goahti and the lavvu can be seen when looking at the top of structures. A lavvu will have its poles coming together, while the goahti will have its poles separate and not coming together. The turf version of the goahti will have the canvas replaced with wood resting on the structure covered with birch bark then peat to provide a durable construction.\n\nLavvu (or , , , , , , and ) is a structure built by the Sámi of northern Scandinavia. It has a design similar to a Native American tipi but is less vertical and more stable in high winds. It enables the indigenous cultures of the treeless plains of northern Scandinavia and the high arctic of Eurasia to follow their reindeer herds. It is still used as a temporary shelter by the Sámi, and increasingly by other people for camping. \n\nThere are several historical references that describe the lavvu structure (also called a \"kota\", or a variation on this name) used by the Sami. These structures have the following in common:\n\n\nNo historical record has come to light that describes the Sami using a single-pole structure claimed to be a lavvu, or any other Scandinavian variant name for the structure. The definition and description of this structure has been fairly consistent since the 17th century and possibly many centuries earlier.\n\nA goahti (also \"gábma\", \"gåhte\", \"gåhtie\" and \"gåetie\", Norwegian: \"gamme\", Finnish: \"kota\", Swedish: \"kåta\"), is a Sami hut or tent of three types of covering: fabric, peat moss or timber. The fabric-covered goahti looks very similar to a Sami lavvu, but often constructed slightly larger. In its tent version the goahti is also called a 'curved pole' lavvu, or a 'bread box' lavvu as the shape is more elongated while the lavvu is in a circular shape.\n\nThe interior construction of the poles is thus: 1) four curved poles ( long), 2) one straight center pole ( long), and 3) approximately a dozen straight wall-poles ( long). All the pole sizes can vary considerably.\n\nThe four curved poles curve to about a 130° angle. Two of these poles have a hole drilled into them at one end, with those ends being joined together by the long center pole that is inserted by the described poles. The other two curved poles are also joined at the other end of the long pole. When this structure is set up, a four-legged stand is formed with the long pole at the top and center of the structure. With the four-legged structure standing up to about five to eight feet in height, approximately ten or twelve straight \"wall-poles\" are laid up against the structure. The goahti covering, today made usually of canvas, is laid up against the structure and tied down. There can be more than one covering that covers the structure.\n\nThe Sámi Parliament building was designed by the architects Stein Halvorsen & Christian Sundby, who won the Norwegian government's call for projects in 1995, and inaugurated in 2005. The government called for a building such that “the Sámi Parliament appears dignified” and “reflects Sámi architecture.”\n\nNotable Projects include:\n\nThe architecture of Samoa is characterised by openness, with the design mirroring the culture and life of the Samoan people who inhabit the Samoa Islands. Architectural concepts are incorporated into Samoan proverbs, oratory and metaphors, as well as linking to other art forms in Samoa, such as boat building and tattooing. The spaces outside and inside of traditional Samoan architecture are part of cultural form, ceremony and ritual. Fale is the Samoan word for all types of houses, from small to large. In general, traditional Samoan architecture is characterized by an oval or circular shape, with wooden posts holding up a domed roof. There are no walls. The base of the architecture is a skeleton frame. Before European arrival and the availability of Western materials, a Samoan fale did not use any metal in its construction.\n\nThe fale is lashed and tied together with a plaited sennit rope called \"afa\", handmade from dried coconut fibre. The \"afa\" is woven tight in complex patterns around the wooden frame, and binds the entire construction together. \"Afa\" is made from the husk of certain varieties of coconuts with long fibres, particularly the \"niu'afa\" (\"afa\" palm). The husks are soaked in fresh water to soften the interfibrous portion. The husks from mature nuts must be soaked from four to five weeks, or perhaps even longer, and very mature fibre is best soaked in salt water, but the green husk from a special variety of coconut is ready in four or five days. Soaking is considered to improve the quality of the fibre. Old men or women then beat the husk with a mallet on a wooden anvil to separate the fibres, which, after a further washing to remove interfibrous material, are tied together in bundles and dried in the sun. When this stage is completed, the fibres are manufactured into sennit by plaiting, a task usually done by elderly men or \"matai\", and performed at their leisure. This usually involves them seated on the ground rolling the dried fibre strands against their bare thigh by hand, until heavier strands are formed. These long, thin strands are then woven together into a three-ply plait, often in long lengths, that is the finished sennit. The sennit is then coiled in bundles or wound tightly in very neat cylindrical rolls.\n\nMaking enough lengths of \"afa\" for an entire house can take months of work. The construction of an ordinary traditional \"fale\" is estimated to use 30,000 to 50,000 feet of \"afa\". The lashing construction of the Samoan \"fale\" is one of the great architectural achievements of Polynesia. A similar lashing technique was also used in traditional boat building, where planks of wood were 'sewn' together in parts. \"Afa\" has many other uses in Samoan material culture, including ceremonial items, such as the \"fue\" fly whisk, a symbol of orator status. This lashing technique was also used in other parts of Polynesia, such case the \"magimagi\" of Fiji.\nThe form of a \"fale\", especially the large meeting houses, creates both physical and invisible spatial areas, which are clearly understood in Samoan custom, and dictate areas of social interaction. The use and function of the \"fale\" is closely linked to the Samoan system of social organisation, especially the \"Fa'amatai\" chiefly system.\n\nThose gathered at a formal gathering or \"fono\" are always seated cross-legged on mats on the floor, around the \"fale\", facing each other with an open space in the middle. The interior directions of a \"fale\", east, west, north and south, as well as the positions of the posts, affect the seating positions of chiefs according to rank, the place where orators (host or visiting party) must stand to speak or the side of the house where guests and visitors enter and are seated. The space also defines the position where the 'ava makers (\"aumaga\") in the Samoa 'ava ceremony are seated and the open area for the presentation and exchanging of cultural items such as the \"'ie toga\" fine mats.\n\nThe front of a Samoan house is that part that faces the main thoroughfare or road through the village. The floor is quartered, and each section is named: \"Tala luma\" is the front side section, \"tala tua\" the back section, and \"tala\", the two end or side sections. The middle posts, termed \"matua tala\" are reserved for the leading chiefs and the side posts on the front section, termed \"pou o le pepe\" are occupied by the orators. The posts at the back of the house, \"talatua\", indicate the positions maintained by the 'ava makers and others serving the gathering.\n\nThe immediate area exterior of the \"fale\" is usually kept clear, and is either a grassy lawn or sandy area if the village is by the sea. The open area in front of the large meeting houses, facing the main thoroughfare or road in a village, is called the \"malae\", and is an important outdoor area for larger gatherings and ceremonial interaction. The word \"fale\" is also constructed with other words to denote social groupings or rank, such as the \"faleiva\" (house of nine) orator group in certain districts. The term is also used to describe certain buildings and their functions. The word for hospital is \"falema'i\", \"house of the ill\".\n\nThe simplest types of \"fale\" are called \"faleo'o\", which have become popular as ecofriendly and low-budget beach accommodations in local tourism. Every family complex in Samoa has a \"fale tele\", the meeting house, \"big house\". The site on which the house is built is called \"tulaga fale\" (place to stand).\n\nDiagrams of \"fale\" showing architectural parts of a traditional house in the Samoan language; from \"An Account of Samoan History up to 1918\" by Samoan historian Teo Tuvale.\n\nThe builders in Samoan architecture were also the architects, and they belonged to an exclusive ancient guild of master builders, \"Tufuga fau fale\". The Samoan word \"tufuga\" denotes the status of master craftsmen who have achieved the highest rank in skill and knowledge in a particular traditional art form. The words \"fau-fale\" means \"house builder\". There were \"Tufuga\" of navigation (\"Tufuga fau va'a\") and Samoan tattooing (\"Tufuga ta tatau\").\nContracting the services of a \"Tufuga fau fale\" required negotiations and cultural custom.\n\nThe fale tele (big house), the most important house, is usually round in shape, and serves as a meeting house for chief council meetings, family gatherings, funerals or chief title investitures. The \"fale tele\" is always situated at the front of all other houses in an extended family complex. The houses behind it serve as living quarters, with an outdoor cooking area at the rear of the compound. At the front is an open area, called a \"malae\". The \"malae\", (similar to \"marae\" concept in Māori and other Polynesian cultures), is usually a well-kept, grassy lawn or sandy area. The \"malae\" is an important cultural space where interactions between visitors and hosts or outdoor formal gatherings take place.\n\nThe open characteristics of Samoan architecture are also mirrored in the overall pattern of house sites in a village, where all \"fale tele\" are situated prominently at the fore of all other dwellings in the village, and sometimes form a semicircle, usually facing seawards. In modern times, with the decline of traditional architecture and the availability of western building materials, the shape of the \"fale tele\" has become rectangular, though the spatial areas in custom and ceremony remain the same.\n\nTraditionally, the afolau (long house), a longer \"fale\" shaped like a stretched oval, served as the dwelling house or guest house.\n\nThe faleo'o (small house), traditionally long in shape, was really an addition to the main house. It is not so well constructed and is situated always at the back of the main dwelling. In modern times, the term is also used for any type of small and simple \"fale\", which is not the main house of dwelling. Popular as a \"grass hut\" or beach fale in village tourism, many are raised about a meter off the ground on stilts, sometimes with an iron roof. In a village, families build a \"faleo'o\" beside the main house or by the sea for resting during the heat of the day or as an extra sleeping space at night if there are guests.\n\nThe tunoa (cook house) is a flimsy structure, small in size, and not really to be considered as a house. In modern times, the cook house, called the \"umukuka\", is at the rear of the family compound, where all the cooking is carried out in an earth oven, \"umu\", and pots over the fire. In most villages, the \"umukuka\" is really a simple open shed made with a few posts with an iron roof to protect the cooking area from the weather.\n\nConstruction of a \"fale\", especially the large and important \"fale tele\", often involves the whole extended family and help from their village community.\n\nThe \"Tufuga fai fale\" oversees the entire building project. Before construction, the family prepares the building site. Lava, coral, sand or stone materials are usually used for this purpose. The \"Tufuga\", his assistants (\"autufuga\") and men from the family cut the timber from the forest. The main supporting posts, erected first, vary in number, size and length depending on the shape and dimensions of the house. Usually they are between 16 and 25 feet in length and six to 12 inches in diameter, and are buried about four feet in the ground. The term for these posts is \"poutu\" (standing posts); they are erected in the middle of the house, forming central pillars.\n\nAttached to the \"poutu\" are cross pieces of wood of a substantial size called \"so'a\". The \"so'a\" extend from the \"poutu\" to the outside circumference of the \"fale\" and their ends are fastened to further supporting pieces called \"la'au fa'alava\".\n\nThe \"la'au fa'alava\", placed horizontally, are attached at their ends to wide strips of wood continuing from the \"faulalo\" to the \"auau\". These wide strips are called \"ivi'ivi\". The \"faulalo\" is a tubular piece (or pieces) of wood about four inches in diameter running around the circumference of the house at the lower extremity of the roof, and is supported on the \"poulalo\". The \"auau\" is one or more pieces of wood of substantial size resting on the top of the \"poutu\". At a distance of about two feet between each are circular pieces of wood running around the house and extending from the \"faulalo\" to the top of the building. They are similar to the \"faulalo\".\n\nThe \"poulalo\" are spaced about three to four feet apart and are sunk about two feet in the ground. They average three to four inches in diameter, and extend about five feet above the floor of the \"fale\". The height of the \"poulalo\" above the floor determines the height of the lower extremity of the roof from the ground.\n\nOn the framework are attached innumerable \"aso\", thin strips of timber (about half an inch by a quarter by 12 to 25 feet in length). They extend from the \"faulalo\" to the \"ivi'ivi\", and are spaced from one to two inches apart. Attached to these strips at right angles are further strips, \"paeaso\", the same size as \"aso.\" As a result, the roof of the \"fale\" is divided into an enormous number of small squares.\nMost of the timber is grown in forests on family land. The timber was cut in the forest and carried to the building site in the village. The heavy work involved the builder's assistants, members of the family and help from the village community. The main posts were from the breadfruit tree (\"ulu\"), or \"ifi lele\" or \"pou muli\" if this wood was not available. The long principal rafters had to be flexible, so coconut wood (\"niu\") was always selected. The breadfruit tree was also used for other parts of the main framework.\n\nIn general, the timbers most frequently used in the construction of Samoan houses are:-\nPosts (\"poutu\" and \"poulalo\"): \"ifi lele\", \"pou muli\", \"asi\", \"ulu\", \"talia\", \"launini'u\" and \"aloalovao\".\n\"Fau\": \"ulu\", \"fau\", \"niu\", and \"uagani\"\n\"Aso\" and \"paeso\": \"niuvao\", \"ulu\", \"matomo\" and \"olomea\"\nThe \"auau\" and \"talitali\" use \"ulu\" and the \"so'a' used both \"ulu\" and \"niu\".\nThe completed, domed framework is covered with thatch (\"lau\" leaves), which is made by the women. The best quality of thatch is made with the dry leaves of the sugarcane. If sugarcane leaf was not available, the palm leaves of the coconut tree was used in the same manner. The long, dry leaves are twisted over a three-foot length of \"lafo\", which are then fastened by a thin strip of the frond of the coconut being threaded through the leaves close up to the \"lafo\" stem. These sections of thatch are fastened to the outside of the framework of the \"fale\" beginning at the bottom and working up to the apex. They are overlapped, so each section advances the thatching about three inches. This means there is really a double layer of thatch covering the whole house. The sections are fastened to the \"aso\" at each end by \"afa\".\n\nProvided the best quality of thatch is used and it has been truly laid, it will last about seven years. On an ordinary dwelling house, about 3000 sections of thatch are laid. Protection from sun, wind or rain, as well as from prying eyes, was achieved by suspending from the \"fau\" running round the house several of a sort of drop-down Venetian blind, called \"pola.\" The fronds of the coconut tree are plaited into a kind of mat about a foot wide and three feet long. A sufficient number of \"pola\" to reach from the ground to the top of the \"poulalo\" are fastened together with \"afa\" and are tied up or let down as the occasion demands. Usually, one string of these mats covers the space between two \"poulalo\" and so on round the house. They do not last for long, but being quickly made, are soon replaced. They afford ample protection from the elements, and it being possible to let them down in sections; seldom is the whole house is closed up.\n\nThe natural foundations of a \"fale\" site are coral, sand, and lava, with sometimes a few inches of soil in some localities. Drainage is therefore good. The top layers of the flooring are smooth pebbles and stones. When occupied, the house floors are usually covered or partially covered with native mats.\nIn Samoan mythology, an explanation of why Samoan houses are round is explained in a story about the god Tagaloa, also known as Tagaloalagi (Tagaloa of the Heavens).\n\nFollowing is the story, as told by Samoan historian Te'o Tuvale in \"An Account of Samoan History up to 1918\".\n\n\nIn Old Fiji, the architecture of villages was simple and practical to meet the physical and social need and to provide communal safety. The houses were square in shape and with pyramid like shaped roofs, and the walls and roof were thatched and various plants of practical use were planted nearby, each village having a meeting house and a Spirit house. The spirit house was elevated on a pyramid like base built with large stones and earth, again a square building with an elongated pyramid like roof with various scented flora planted nearby.\n\nThe houses of Chiefs were of similar design and would be set higher than his subjects houses but instead of an elongated roof would have similar roof to those of his subjects homes but of course on a larger scale.\n\nWith the introduction of communities from Asia aspects of their cultural architecture are now evident in urban and rural areas of Fiji's two main Islands Viti Levu and Vanua Levu. A village structure shares similarities today but built with modern materials and spirit houses (Bure Kalou) have been replaced by churches of varying design.\n\nThe urban landscape of early Colonial Fiji was reminiscent of most British colonies of the 19th and 20th century in tropical regions of the world, while some of this architecture remains, the urban landscape is evolving in leaps and bounds with various modern aspects of architecture and design becoming more and more evident in the business, industrial and domestic sector, the rural areas are evolving at a much slower rate.\n\nWithin the body of Hawai'ian architecture are various subsets of styles; each are considered typical of particular historical periods. The earliest form of Hawaiian architecture originates from what is called ancient Hawaii—designs employed in the construction of village shelters from the simple shacks of outcasts and slaves, huts for the fishermen and canoe builders along the beachfronts, the shelters of the working class \"makaainana\", the elaborate and sacred \"heiau\" of \"kahuna\" and the palatial thatched homes on raised basalt foundation of the \"alii\". The way a simple grass shack was constructed in ancient Hawaii was telling of who lived in a particular home. The patterns in which dried plants and lumber were fashioned together could identify caste, skill and trade, profession and wealth. Hawaiian architecture previous to the arrival of British explorer Captain James Cook used symbolism to identify religious value of the inhabitants of certain structures. Feather standards called \"kahili\" and \"koa\" adorned with \"kapa\" cloth and crossed at the entrance of certain homes called \"puloulou\" indicated places of \"alii\" (nobility caste). \"Kii\" enclosed within basalt walls indicated the homes of \"kahuna\" (priestly caste).\n\nThe Bahay Kubo, Kamalig, or Nipa Hut, is a type of stilt house indigenous to most of the lowland cultures of the Philippines. It often serves as an icon of broader Filipino culture, or, more specifically, Filipino rural culture.\nAlthough there is no strict definition of the Bahay Kubo and styles of construction vary throughout the Philippine archipelago, similar conditions in Philippine lowland areas have led to numerous characteristics \"typical\" of examples of Bahay Kubo.\n\nWith few exceptions arising only in modern times, most Bahay Kubo are raised on stilts such that the living area has to be accessed through ladders. This naturally divides the bahay kubo into three areas: the actual living area in the middle, the area beneath it (referred to in Tagalog as the \"\"Silong\"), and the roof space (\"Bubungan\" in Tagalog), which may or may not be separated from the living area by a ceiling (\"Kisame\"\" in Tagalog).\n\nThe traditional roof shape of the Bahay Kubo is tall and steeply pitched, ending in long eaves. A tall roof created space above the living area through which warm air could rise, giving the Bahay Kubo a natural cooling effect even during the hot summer season. The steep pitch allowed water to flow down quickly at the height of the monsoon season while the long eaves gave people a limited space to move about around the house's exterior whenever it rained. The steep pitch of the roofs are often used to explain why many Bahay Kubo survived the ash fall from the Mt. Pinatubo eruption, when more ’modern’ houses notoriously collapsed from the weight of the ash.\n\nRaised up on hardwood stilts which serve as the main posts of the house, Bahay Kubo have a \"Silong\" (the Tagalog word also means \"shadow\") area under the living space for a number of reasons, the most important of which are to create a buffer area for rising waters during floods, and to prevent pests such as rats from getting up to the living area. This section of the house is often used for storage, and sometimes for raising farm animals, and thus may or may not be fenced off.\n\nThe main living area of the Bahay Kubo is designed to let in as much fresh air and natural light as possible. Smaller Bahay Kubo will often have bamboo slat floors which allow cool air to flow into the living space from the silong below (in which case the Silong is not usually used for items which produce strong smells), and the particular Bahay Kubo may be built without a \"kisame\" (ceiling) so that hot air can rise straight into the large area just beneath the roof, and out through strategically placed vents there.\n\nThe walls are always of light material such as wood, bamboo rods, or bamboo mats called \"sawali.\" As such, they tend to also let some coolness flow naturally through them during hot times, and keep warmth in during the cold wet season. The cube shape distinctive of the Bahay Kubo arises from the fact that it is easiest to pre-build the walls and then attach them to the wooden stilt-posts that serve as the corners of the house. The construction of a Bahay Kubo is therefore usually modular, with the wooden stilts established first, a floor frame built next, then wall frames, and finally, the roof.\n\nIn addition, Bahay kubo are typically built with large windows, to let in more air and natural light. The most traditional are large awning windows, held open by a wooden rod). Sliding windows are also common, made either with plain wood or with wooden Capiz shell frames which allow some light to enter the living area even with the windows closed. In more recent decades inexpensive jalousie windows also became commonly used. In larger examples, the large upper windows may be augmented with smaller windows called \"Ventanillas\" (Spanish for \"little window) underneath\", which can be opened to let in additional air on especially hot days.\n\nSome (but not all) Bahay Kubo, especially one built for long-term residence, feature a \"Batalan\" \"wet area\" distinct from other sections of the house - usually jutting out somewhat from one of the walls. Sometimes at the same level as the living area and sometimes at ground level, the Batalan can contain any combination of cooking and dishwashing area, bathing area, and in some cases, a lavatory.\n\nThe walls of the living area are made of light materials - with posts, walls, and floors typically made of wood or bamboo and other light materials. Topped by a thatched roof, often made out of nipa, anahaw or some other locally plentiful plant. The Filipino term \"Bahay Kubo\" literally means \"cube house\", describing the shape of the dwelling. The term \"Nipa Hut\", introduced during the Philippines' American colonial era, refers to the nipa or anahaw thatching material often used for the roofs.\n\nNipa huts were the native houses of the indigenous people of the Philippines before the Spaniards arrived. They are still used today, especially in rural areas. Different architectural designs are present among the different ethnolinguistic groups in the country, although all of them conform to being stilt houses, similar to those found in neighboring countries such as Indonesia, Malaysia, and other countries of Southeast Asia. The advent of the Spanish Colonial era introduced the idea of building more permanent communities with the Church and Government Center as a focal point. This new community setup made construction using heavier, more permanent materials desirable. Finding European construction styles impractical given local conditions, both Spanish and Filipino builders quickly adapted the characteristics of the Bahay Kubo and applied it to Antillean houses locally known as \"Bahay na Bato\" (Literally \"stone house\" in Tagalog).\n\n"}
{"id": "5159915", "url": "https://en.wikipedia.org/wiki?curid=5159915", "title": "Intermec", "text": "Intermec\n\nIntermec is a manufacturer and supplier of automated identification and data capture equipment, including barcode scanners, barcode printers, mobile computers, RFID systems, voice recognition systems, and life cycle services.\n\nIntermec holds patents in RFID (Radio Frequency Identification) and customers include 75 percent of Fortune 500 companies and 60 percent of Fortune 100 companies. Intermec was traded on the New York Stock Exchange.\n\nOn December 10, 2012, Intermec announced it agreed to be acquired by Honeywell International Inc. in an all-cash transaction valued at approximately $600 million. The Merger was approved by Intermec’s stockholders on March 19, 2013 and received regulatory approval from the European Commission on June 14, 2013. FTC clearance was announced on September 13, 2013.\n\nOn September 17, 2013, Honeywell announced the completion of the acquisition of Intermec. Intermec will be integrated with Honeywell Scanning & Mobility, within the Honeywell Automation and Control Solutions (ACS) business.\n\nThe majority of Intermec's business comes from automating supply chain operations in manufacturing, warehouse and distribution, retail, transportation and logistics, direct store delivery and field service sectors. Their product lines include:\n\n\n"}
{"id": "16017967", "url": "https://en.wikipedia.org/wiki?curid=16017967", "title": "Japanese cryptology from the 1500s to Meiji", "text": "Japanese cryptology from the 1500s to Meiji\n\nThe cipher system that Uesugi used is basically a simple substitution usually known as a Polybius square or “checkerboard.” The i-ro-ha alphabet contains forty-eight letters, so a seven-by-seven square is used, with one of the cells left blank. The rows and columns are labeled with a number or a letter. In the table below, the numbers start in the top left, as does the i-ro-ha alphabet. In practice these could start in any corner.\n\nTo encipher, find the plaintext letter in the square and replace it with the number of that row and column. So using the square above, kougeki becomes 55 43 53 63 or 55 34 35 36 if the correspondents decided ahead of time on column-row order. The problem of what to do in the case of letters such as “ga,” “de,” and “pe” that do not appear in the i-ro-ha alphabet is avoided by using the base form of the letter instead – as above where “kougeki” becomes \"koukeki.\" Technically, this is a serious flaw because some messages may have two or more equally valid decipherments. To avoid this the encipherer may have had to rephrase messages.\n\nThe column and row headers do not have to be numbers. One common variation is to use letters. This was common in European cryptography and is found in the Uesugi cipher as well. However, the Japanese cipher had a twist that never seems to have been used in the West: using the last 14 letters of the Iroha poem to fill in the row and column headers. The table shown below gives an example of this, using “tsurenakumieshiakinoyufukure”.\n\nThis system of using a “checkerboard” to convert an alphabet into numbers or letters was described by Polybius over 2000 years ago. There are three main advantages to this system. First, converting letters into numbers allows for various mathematical transformations which are not possible or not as easy with letters – super-enciphering for example. Second, the checkerboard system reduces the total number of characters. Whether converting to numbers or letters, the Polybius square reduces 25 English letters to five characters. Uesugi's square reduces to seven. This reduction makes crytanalysis slightly more difficult than simple one-to-one substitution. Another benefit of the reduction in the number of letters is that it reduces the chance of error in communicating the message. The letters of the German ADGFX system in World War I were chosen because in morse code they are quite distinct and thus it was unlikely that an error in the morse code transmission would accidentally turn one letter into another. This would have been important for a sengoku daimyō, for instance, if he experimented with sending coded messages over long distances by torches, flags, poles, or similar system.\n\nFinally, although the checkerboard system doubles the length of messages, breaking each plaintext letter into two ciphertext letters allows for separate transformations on each of the halves. However, this does not seem to have been used much in American or European cryptology and Japanese cryptologists apparently did not use it at all.\n\nIt is not known how or even if Uesugi actually used the seven-by-seven checkerboard system. The scarcity of evidence makes it impossible to draw any firm conclusions but tentatively it seems that senkoku period daimyō did not have much use for cryptology. Of course it is possible that they did have their “black chambers” and that those chambers were shrouded in such secrecy that no hint of their existence escaped. This seems unlikely however. Several daimyō compiled codes of conduct or books of advice on governing for their offspring. Had cryptology been an important factor in the success of such men, they might be expected to pass that advantage along to their successor. The fact that they did not do so, in writing at least, does not prove anything but, in light of the other evidence – and lack of it – does make the existence of black chambers of the European sort seem unlikely.\n\nThe history of cryptology in Japan shows two things. First, the fact that substitution ciphers existed makes the failure of the Japanese to improve on the substitution cipher or to invent the transposition cipher much harder to explain. Second, the lack of a strong cryptographic tradition suggests – almost requires – a correspondingly weak cryptanalytic tradition. In fact there seems to be no cryptanalysis in Japanese history before the late 19th century.\n\nTBA\n\nDavid Kahn identifies World War I as a major turning point for institutional cryptology. Before the war, breaking codes was an individual endeavor – one person wresting with the messages until one of due them broke. After the war, successful cryptology against major nation states required large-scale organization.\n\nJapanese cryptology does not seem to have been affected at all by the Great War. The government continued using insecure codes of the sort they had been using since the Meiji Restoration. As a result, in 1921 Japanese diplomacy was unable to gain its preferred result at the Washington Naval Conference, ending with the least position Japan was willing to accept. Weak codes were the primary cause of that result, as the American delegation had the Japanese secret communications available.\n\nThe American “Black Chamber” under Herbert O. Yardley broke Japanese diplomatic codes in 1919 – less than a year after starting operations – and the Black Chamber cryptanalysts were still reading Japanese diplomatic traffic in 1921 when the Washington Naval Conference took place. Thanks to Yardley's book The American Black Chamber, the failure of Japanese cryptography at the Conference is well known. Yardley's book gives a valuable look into the quality of the codes employed by the Japanese government in the years leading up to, and during, the Conference and thus is worth looking at in some detail.\n\nJudging from Yardley's description of the codes he and his cryptanalysts broke, Japanese codes in 1919 were weak and barely deserved to be called “codes”. He might have exaggerated the difficulty of breaking the Japanese codes – British codebreakers thought Japanese codes at that time were so weak you almost didn't need a cryptanalyst.\n\nThe two-letter code Japanese diplomats were using in 1919 consisted of two English-letter groups. This allows for a maximum of 676 (26*26) groups. That is far too small for a diplomatic code in 1819 much less 1919. Worse, the Japanese cryptographers did not use all of the available groups because Yardley says that the groups were either vowel-consonant or consonant-vowel, with “y” counting as both. If Yardley is correct about this, it means that the Japanese cryptographers limited themselves to only 252 of the 676 possible groups. After using anywhere from 54 to 100 groups for the kana and ten groups for the numbers zero to nine, there were at most 188 unassigned code groups remaining.\n\nYardley made his original break into the code by realizing that codice_1 was codice_2 (Ireland independence). The doubled codice_3 suggests the codice_4 of codice_5. This guess is confirmed when he discovers that the recovered groups codice_6 work elsewhere for codice_7 (Germany).\n\nThe initial break into the code is further confirmed when codice_8 makes sense as codice_9 (stop). This is exactly how one breaks a simple substitution cipher --- letter frequencies and repetitions in the text suggest possible plaintext letters. The cryptanalyst plugs in those letters and sees what yields meaningful text and what does not. Meaningful text suggests new letters to try and the cryptanalyst starts the cycle over again.\n\nAs can be seen from the description of Yardley's original break into the code, groups were assigned to kana like “do” and “bo” which in Japanese are not part of the regular alphabet but are created from other kana by adding pronunciation marks. Providing for these non-alphabet kana would require at least another 25 and possibly as many as 60 more code groups – hence the range given above for code groups for the kana – leaving only about 150 groups for words, phrases, and names. French cryptanalysts were making and breaking bigger, better codes in the 18th century. One suspects the Japanese language gave Yardley more trouble than the code itself did.\n\nThus the Japanese diplomatic code in use in 1919 was extremely weak and fundamentally flawed: a diplomatic code that does not contain code groups for common geopolitical names and phrases but requires them to be spelled out cannot be considered strong. Spelling out “stop” is further evidence that the code was not well designed. Even if the Japanese cryptographers devoted their 188 groups to the 188 most common phrases, the fact that they only had 188 groups to work with meant that most of their encoded messages would actually be simple-substitution enciphered messages of the sort that people had been solving for hundreds of years.\n\nAccording to Yardley, the Japanese codes his Black Chamber broke in 1919 were improved by a Polish cipher expert about a year later. His exact words are [italics in original]:\n\nYardley was right about a Polish expert visiting Japan but he was mistaken about the timing. The Japanese army did bring in a Polish expert, Jan Kowalefsky, but he did not arrive in Japan until September 1924. If Japanese codes improved significantly between 1919 and 1924, as Yardley claims, the improvements were the work of Japanese cryptologists.\n\nA possibility that is ripe for further research, is that Japanese cryptologists studied one or more of the books on codes and ciphers that were occasionally published in Europe and America. For example, Parker Hitt's 1916 book Manual for the Solution of Military Ciphers was hugely popular, selling around 16,000 copies in America. Also, Japanese military attachés might have been aware that Winston Churchill, in his 1923 The World Crisis, admitted that Britain had read German naval messages during World War I.\n\nIt is possible that Yardley is simply wrong and Japanese codes did not improve significantly between 1919 and 1924. Kahn found that one improvement Yardley mentions – three letter code groups mixed in with two letter groups – was not actually present in the Japanese telegram that Yardley claimed it was.\n\nJapanese cryptographers supposedly improved their codes through sectioning – breaking the message into parts and rearranging them prior to encoding. This buries stereotypical openings and closings, which makes it harder for cryptanalysts to make initial breaks into a code by guessing at probable words. The technique is known as bisecting, Russian copulation, trisecting, tetrasecting, etc. depending on how many pieces the text is broken into. Sectioning was not a new or revolutionary technique in the 1910s.\n\nIf, as Yardley claims, some Japanese codes did have as many as 25,000 code groups at the time of the Washington Naval Conference, it would indicate a healthy appreciation of cryptological realities. Cryptographers have long known that bigger codes are better – all else being equal, a 25,000 group code is stronger than a 2,500 group code. In fact, many commercial code books as far back as the 1850s had 50,000 groups – but governments were often reluctant to pay for the production of large codebooks. This limited the size and thus strength of government and military codes for many years. To be fair, the secure production, storage, and distribution of codebooks is not easy nor is it cheap.\n\nHowever, it seems unlikely that the Japanese government was using codebooks with 25,000 groups in the early 1920s. Jumping from the weak code used for the Washington Naval Conference to a book code of 25,000 in just a few years seems too fast, especially without some external indication that their codes had been compromised. Further, as shown below, even in 1926 the Army's top cryptologist was developing a cipher system that had only about 2,500 groups and those were actually just 10 charts of about 250 groups each.\n\nThus, the situation between the Washington Naval Conference and the mid-1920s was not that of a Polish officer helping to make Japanese codes much more secure. Rather, Japanese cryptographers were working to bring their codes up to the level of other major governments.\n\nThe Polish cipher expert, Jan Kowalefsky, might not have helped improve Japanese codes before the Washington Naval Conference but he did have a strong influence on Japanese cryptography between the conference and World War II. He trained what seems to be the first generation of professional Japanese cryptographers.\n\nJapanese authors have identified two events that influenced the Japanese army's decision to invite a foreigner to improve their cryptology.\n\nThe first was an incident during the Siberian Intervention. The Japanese army came into possession of some Soviet diplomatic correspondence, but their cryptanalysts were unable decipher the messages. Someone suggested asking the Polish military to try cryptanalyzing them. It took the Poles less than a week to break the code and read the messages.\n\nThe second event also involved a failure to decipher intercepts. Starting in 1923, the Army began intercepting European and American diplomatic radio communications. Interception was difficult but the task of deciphering intercepted messages proved too much for the Army cryptanalysts.\n\nThese two failures convinced the leaders of the Japanese army that they needed some outside help and for geopolitical reasons, they decided to turn to the Polish military. Poland had fought the Soviet Union in 1920 and the Japanese believed the Poles would be receptive to the idea of teaching someone on the Soviet Union's opposite flank how to read Soviet codes.\n\nThe Japanese Army could not have asked for more distinguished teachers. Polish cryptanalysts would later break early versions of the German Enigma machine in 1932 and their work jump-started the French and British efforts to break later, more complicated, Enigma machines. In the 1920s and 1930s it is accurate to say that Polish cryptanalysts were some of the best in the world.\n\nThe arrangements were made and on 7 September 1924, Captain Jan Kowalefsky arrived in Yokohama. Kowalefsky taught a three-month joint Army-Navy course to at least seven officers: four from the Army and three from the Navy.\n\nWhen the course finished, someone suggested that the novice cryptologists get some practical experience working with the Polish cryptologists in Poland. The Japanese students would go to Poland with their teacher. Arrangements were made and a study-abroad program of sorts was started. Five officers left for Poland with Kowalefsky late in 1924 (Taishō 13). They spent a year working in the Polish Army's Bureau of Ciphers before returning to Japan and taking up positions in the Japanese Army Cipher Department.\n\nTakagawa and Hiyama both assert that each year for about the next fourteen (until Shōwa 14) years, two Japanese Army officers traveled to Warsaw for a year of cryptological training. Neither Smith nor Budiansky mentions Kowalefsky or anything about Japanese officers studying in Poland. Yardley mentions the “Polish expert” working for the Army but gets the timing wrong. In English, only Kahn actually gives this expert a name and provides some more details.\n\nKahn writes that Kowalefsky had been in Japan from about 1920, when he was supposedly helping improve Japanese codes, and was still there in 1925 to teach at a new Navy code school. That is, Kahn has Kowalefsky working for the Navy, not the Army. Japanese sources make it clear that both Army and Navy officers attended Kowalefsky's three-month course, so some confusion is possible. However, Yardley wrote, correctly, that Kowalefsky worked for the Army but was wrong about the year since he claimed that the Polish expert had arrived in 1920. Yardley's error might explain why Kahn had Kowalefsky arriving in the wrong year but nothing in Yardley suggests that Kowalefsky ever worked for the Navy.\n\nAlthough they do mention Kowalefsky (if not by name) neither Kahn nor Yardley mentions anything about Japanese cryptologists training in Poland or even Kowalefsky returning home. Thus, probably the most widely read English books on cryptological history are possibly missing a large and important part of the development of professional cryptology in Japan – if the Japanese sources are correct. If the Japanese sources for this history can be confirmed, it would be an important addition to the understanding of Japanese cryptology leading up to World War II. Polish cryptanalysts were very good and if they tutored the Japanese for almost fifteen years, it makes the Japanese failure to break most of the Allied codes during the war much more puzzling.\n\nHyakutake Harukichi was among the first group of Japanese officers to study in Poland and on his return was made the chief of the code section of the third department of the army general staff. This was in 1926. Naturally enough, one of his first concerns was strengthening Army codes. He started by designing a new system to replace a four-letter code used by military attachés that had been in use since around 1918. The replacement was the two-letter, ten-chart code that Yardley mentions but mistakenly attributes to Kowalefsky in about 1920. Yardley gives the following description of Hyakutake's new system and its effectiveness:\n\nYardley also describes the Japanese system of sectioning their messages but does not make it clear if this applies to the two-letter, ten-chart code. Takagawa's description of Hyakutake's code does not mention any sectioning but otherwise closely matches Yardley's account. It is possible then that sectioning was not a part of Hyakutake's new system. Which code systems involved sectioning and when the systems were used is not clear. Michael Smith mentions in \"The Emperor's Codes\" that British codebreakers were surprised by the appearance of sectioning in Japanese codes around 1937. The British had been reading some Japanese codes since at least as far back as the Washington Naval Conference. If they did not see sectioning in Army codes until 1937, in which code did Yardley see sectioning during his time at America's Black Chamber? Further research is necessary to answer that question.\n\nIt is clear from Yardley's description that Hyakutake's new system was not very effective. The system used 10 charts, each with 26 rows and columns labeled from codice_10 to codice_11. This gives 626 two-letter code groups. Most words and phrases will not be in the code and must be spelled out in kana. In this respect it is similar to, but larger than, the first Japanese code that Yardley broke in 1919. The difference is that this time however there were ten codes instead of just one.\nBasically, Hyakutake created a poly-code system where the code changes every few words. This is just a code version of a polyalphabetic substitution cipher. Polyalphabetic ciphers use several different enciphering alphabets and change between them at some interval, usually after every letter. The strength of a polyalphabetic cipher comes from how many alphabets it uses to encipher, how often it switches between them, and how it switches between them (at random or following some pattern for example). The Vigenere is probably the most famous example of a polyalphabetic substitution cipher. The famous cipher machines of World War II encipher in a polyalphabetic system. Their strength came from the enormous number of well-mixed alphabets that they used and the fairly random way of switching between them.\n\nWith a bit of luck, experienced cryptanalysts have been able to break polyalphabetic ciphers for centuries. From the late 19th century they did not even need luck --- Auguste Kerckhoffs published a general solution for polyalphabetic ciphers in 1883 in his book \"La Cryptographie militaire\".\n\nSo although Hyakutake's new code system was original, the fundamental idea underlying the system was well known, as were its weaknesses. With only 626 code groups, it is more cipher than code. As mentioned above, the ten different code charts just make it a polyalphabetic cipher --- one with only ten \"alphabets.\" Methods like Kerckhoffs' superimposition can be used to convert several polyalphabetically encoded messages into ten monoalphabetically encoded message chucks. Chunks which are very easily solved. It is not surprising that the members of Yardley's Black Chamber broke the code in a few months.\n\nThe use of ten charts may have been an illusory complication --- rather than improve the security of the code, it probably made the code weaker. If, instead of ten different code groups for 626 terms, Hyakutake had used the ten charts (with slight modification to make each group unique) to provide code groups for closer to six thousand terms, the code would have been much stronger.\n\nIncluding more terms means that fewer have to be spelled out in kana --- which is the whole point of using a code. Further, the reduction in duplication allows more flexibility in assigning homophones. Instead of ten groups for each letter, word, or phrase, each could receive homophones based on its frequency of occurrence. For example, the cryptographer can assign an appropriately large number of homophones to high-frequency letters and words like \"n,\" \"shi,\" and \"owari\" and only one or two code groups to lower frequency elements.\n\nLikewise, if code groups were used to indicate a switch to a new chart, this could also have weakened the code unnecessarily. In fact, Yardley specifically mentions it as making the codes easier to cryptanalyze. Generally speaking, substitution systems switch alphabets as often as possibly because that provides the best security. Their strength lies in how many alphabets they use and how randomly they switch between them.\n\nSo switching charts after every couple of words is not as secure as switching after every word. Also important for security is how the cryptographer switches between the charts. If Hyakutake's system required the code clerk to switch codes charts pseudo-randomly, that would provide more security than requiring a set sequence of changes. This is more important if the charts are derived from one another in some predictable manner. If, for example, the plaintext codice_12 is codice_13 on chart 1, codice_14 on chart 2, and codice_15 on chart 3, then switching between the charts in order will pose much less difficulty for the cryptanalyst than using the charts in a more random order.\n\nRegular polyalphabetic substitution ciphers often rely on code words to determine alphabet changes. Each letters of the code work references a different alphabet. With the ten charts of Hyakutake's system, a code number would be easy to use for pseudo-random changes --- \"301934859762\" means encode the first word or phrase with the third table, the second word or phrase with the tenth (zeroth) table, etc. The thirteenth word or phrase would be encoded with the third table again. Of course to give maximum security this code number needs to be changed frequently.\n\nUnfortunately, there is no information on how tables were changed except for Yardley's vague \"until all ten had been used in the encoding of a single message,\" quoted above. This unfortunately says nothing of the order the charts are used in.\n\nHara Hisashi became head of the code section of the Seventh Division sometime after 1932 and was later transferred to the Third Section of the Army General Staff. Sometime between then and 1940, Hara devised a system that used a pseudo-random number additive to superencipher the three number code the Army already had in service.\n\nNeither Takagawa nor Hiyama provide details about when this three-number code system was adopted for Army communications. A three-number code has a maximum of 10³, or 1000 groups — which is still too small for a strategic code and a far cry from the 25,000 that Yardley claims some Japanese codes had in the 1920s. However, it was a two-part code — an important improvement.\n\nCode books contain two lists --- one of code groups and one of plaintext letters, words, and phrases. Someone encoding a message looks up the words in the plaintext list and substitutes the corresponding code group. Obviously it is important for that person's sanity that the plaintext be in some sort of order so words can be looked up easily. Since the system is similar for decoding --- look up the code group and substitute the plaintext --- it is equally important to have the code groups in order as well. With a one-part code, both lists are in alphabetical (or numerical) order. This means that you can encode and decode using the same book.\n\nIt also makes it easier for the enemy to break the code because once they realize they are dealing with a one-part code, they can use known groups to draw conclusions about unknown groups. For example, if the enemy knows that codice_16 is codice_17 and codice_18 is codice_19, they will know that codice_20 cannot be codice_21.\n\nA two-part code mixes the lists, making the code stronger by avoiding the problem described above. The drawback is that you now need two books. One, for encoding, has the plaintext in order to make encoding easy and the other, for decoding, has the code groups in order. Hence the name \"two-part\" code. The increase in security usually outweighs the increase in size and extra security concerns.\nAntoine Rossignol invented the two-part code around 1650 or so. The idea could hardly be considered new or secret by the 20th century, so again it is surprising to see Japanese cryptographers taking so long to begin using a common cryptographic method.\n\nThe \"one-time pad\" system is only cipher system that is totally secure. It uses random numbers to encode the plaintext. If the numbers are truly random and the encoder never reuses those numbers, the encoded message cannot be broken. Fortunately for cryptologists, random numbers are very difficult to come up with and creating, distributing, and managing pads for more than a handful of correspondents is beyond the capabilities of even most governments.\n\nUsing random numbers for cryptography was first done around 1917 for securing teleprinter communications. It proved unfeasible for the reasons mentioned above. By the mid-1920s however, the German government was using one-time pads for diplomatic correspondence. They had learned their lessons from World War I and were determined not to let it happen again.\n\nHara devised a system that used random numbers to superencipher Japanese army codes. Possibly because of the logistical difficulties inherent in the one-time pad system, Hara's system used tables of pseudo-random numbers. The encipherer had to indicate where in the table he (or much less likely at the time, she) did this by hiding the row and column headers from the table in the message.\n\nThis system is not new. Diplomats and armies started superenciphering with additives sometime during or soon after the First World War and by the 1920s it was common. German diplomats in Paris were using, shortly after the First World War, a codebook of 100,000 groups superenciphered \"twice\" from a book of 60,000 additive groups! It would be very surprising if after five to ten years of training with the Poles, Japanese Army cryptologists were not already familiar with superenciphering with additive tables.\n\nSuperencipherment is fairly strong. It can be, and was, broken, but it is very hard to do. With the exception of the one-time pad, which will keep its secrets until the end of time, any code or cipher can be broken. All that is required is sufficient material. All that can be expected of a code or cipher system is that by the time the enemy breaks it, the information in the message is no longer useful. This is just a cryptographic fact of life.\n\nHara's pseudo-random code system, like every additive system other than the one-time pad, can be broken. Eventually someone, somewhere will use overlapping parts of the additive charts. The first thing the cryptanalyst does is identify where in the message the starting point of the chart (the \"indicator\") is hidden --- this allows the messages that are enciphered with the same sections of the number charts to be lined up and the additives stripped off.\n\nPerhaps realizing the gap between theory and practice, Hara devised a small system for generating pseudo-random numbers that could be used by units whose charts were outdated and which could not be supplied with new ones. This suggests that the cryptographers had real world experience with cryptology under battlefield conditions.\n\nThe system is simple, as it no doubt was intended to be. It requires a small chart of random numbers. Instead of using the numbers as additives, the encipherer uses two or more of them to create a much longer number. That number is then used to superencipher the message. The figure below shows how this is done.\n\nWhen the numbers are added, any tens units are dropped. Thus 8 + 9 = 7. If the encipherer uses a six-digit number and a five-digit number, the resulting pseudo-random number will repeat after 30 digits. Hiyama gives an example of this system using a seven-digit and a five-digit number, which repeats after 35 digits.\n\nThis pseudo-random number system is much weaker than the usual system of superencipherment but as an emergency backup system it would have been adequate and certainly better than using a transposition or simple substitution cipher. Like any other cipher system, breaking a pseudo-random number system just requires a sufficient amount of intercepted ciphertext.\n\nHyakutake's two-letter, ten-chart system was exceedingly weak. It might have made a decent tactical field code --- it is simple to use, requires only the paper charts and a pencil, and is easily changed. As a code for military attachés around the globe, however, Hyakutake's system was much too weak. It was basically a slightly improved version of the Foreign Ministry's two-letter code that Yardley broke in 1919 and possibly not as strong as the four-letter code it replaced.\n\nKahn, Smith, and Budiansky all make it clear that superenciphering and using pseudo-random additives were nothing new even in the 1920s --- Kahn says that enciphered code was \"the customary method for diplomatic communications.\" A system using random numbers to superencipher messages was not revolutionary in the 1930s.\n\nThus, Hara's system was not new and does not seem to have been any better than similar systems long in use in other countries. Nevertheless, devising and implementing the Army's system was an important accomplishment and it is possible that Hara was responsible for it. A topic for further research would be why this system was chosen instead of machine ciphers. Was the random number system chosen for non-cryptological reasons? Were the Army cryptanalysts good enough to understand that random numbers were more secure, when used correctly, than cipher machines?\n\nThere were several books available that hint at ways to break cipher machines. William Friedman's \"The Index of Coincidence and Its Applications to Cryptography\" was revolutionary; the addition of advanced mathematical, especially statistical, methods to the cryptological toolkit made traditional cryptographic systems obsolete and machine systems breakable. So it is possible that the Japanese cryptanalysts knew that cipher machines were, in theory at least, breakable.\n\nThe Polish military realized early on that machine enciphering would change the science of cryptology and from 1929 employed mathematicians to work on cryptanalysis. However, as the goal of Japanese-Polish cryptographic cooperation was to train the Japanese side to break Russian codes, there would have been no need for the Polish cryptologists to reveal methods of breaking machines the Russians were not using. Teaching the Japanese the latest and greatest methods would not be of any use against Russian codes and would only risk the Germans finding out and changing their codes. The Poles thus had a strong incentive to teach the Japanese just as much as they needed to know.\n\nThe Japanese army was aware of machine systems; at the Hague in 1926, a Japanese military attaché saw a demonstration of the Model B1 cipher machine from Aktiebolaget Cryptograph. In fact, in the early 1930s, both the Japanese Navy and the Foreign Ministry switched to machine systems for their most secret messages. The fact that those systems seem to have been developed in Japan suggests that there were knowledgeable cryptographers in Japan. Which suggests that perhaps there were other, non-cryptographic reasons why the Army continued to use chart and book based systems. Perhaps further research into the cultural and institutional aspects of inter-war cryptology in Japan could uncover those reasons.\n\nSeveral curious facts stand out in this cursory overview of Japanese cryptological history. One is that the Japanese government did not bring in an outside expert to help with their codes until 1924. Considering all the other \"gaikokujin oyatoi\" (hired foreigners) brought in to assist with \"modernization\" in the Meiji period, it is striking that such an important field as cryptology would be ignored.\n\nThis suggests that the Japanese government in the first decades of the 20th century did not really understand the importance of cryptology for protecting communications. Such an attitude would hardly have been limited to Japan in the 1910s or 1920s --- despite their success at the Washington Naval Conference, and later public chastisement by Yardley, American codes remained weak right up to the early 1940s. However, even America, thanks to its ties to Europe, had a cryptological history and a reserve of talented people who understood the problems and the solutions. Japan does not seem to have had anyone like Yardley, much less a William Friedmann.\n\nThe Japanese Army cryptologists, despite training with the Polish military for over ten years, originally developed substandard codes. Hara's system shows significant improvement and demonstrates an understanding of cryptography at least the same level as practiced by other major world powers in the early 1940s.\n\n"}
{"id": "30132416", "url": "https://en.wikipedia.org/wiki?curid=30132416", "title": "Kaniv Hydroelectric Power Plant", "text": "Kaniv Hydroelectric Power Plant\n\nKaniv Hydroelectric Power Plant is a hydroelectricity generating complex on the Dnieper River in Kaniv, Ukraine. It is operated by the Ukrhydroenego that is part of the state company Energy Company of Ukraine.\n\nThe power plant was projected by the Ukrainian department of project-research institute \"UkrHydroProject\" of S.Zhuk. The construction was conducted by several specialized in this field companies. During the construction also there was built a small settlement for the power plant's working personnel.\n\nTurbines for the plant were produced by the Kharkiv Factory \"\", generators - Kharkiv Factory \"Elektrovazhmash\".\n\nThe dam has a lock to allow a water travel along the Dnieper river. It is a single stage, a single chamber lock. The lock's size is .\n\nIn 1997 the power plant started to be renovated. The first stage lasted until June 2002, while the second one - 2017. The first stage was financed through the IBRD, while for the second World Bank invested $106 million with addition of the Ukrainian government budget.\n\n\n \n"}
{"id": "14535827", "url": "https://en.wikipedia.org/wiki?curid=14535827", "title": "List of Allis-Chalmers engines", "text": "List of Allis-Chalmers engines\n\nThis is a list of internal combustion engines produced by the former Allis-Chalmers Corporation Engine Division for use in their lines of tractors, combine harvesters, other agricultural equipment, engine-generators, and other industrial plant.\n\nAllis-Chalmers purchased the Buda Engine Co. in 1953 and took over their well-established line of products. Since Buda was merged entirely into A-C as part of their new Engine Division, its operations became known simply as the \"Harvey plant\" and all of its production after 1953 was under the Allis-Chalmers name.\n\nThe very earliest A-C tractors, up to the mid-1930s, used engines built by outside suppliers (LeRoi, Midwest, Waukesha and Continental were common). Those engines are not included in this list. In a later reversal of this practice, the Engine Division eventually served as a third-party supplier to other makers of farm and industrial machinery, most notably Cockshutt and LeRoi.\n\nAllis-Chalmers (and Buda) produced heavy-duty engine designs that were built to handle a variety of fuel types (generally gasoline, diesel fuel, or liquefied propane gas). The types of fuel each engine could burn are listed where appropriate; further information on fuel types for each engine can be found in the individual engine articles.\n\n\n\n\n\n\n"}
{"id": "3987613", "url": "https://en.wikipedia.org/wiki?curid=3987613", "title": "List of Heckler &amp; Koch products", "text": "List of Heckler &amp; Koch products\n\nThis is a list of all of the weapon products made by Heckler & Koch, a German weapons defence manufacturer with subsidiaries all over the world. It includes fully developed, experimental and military products, as well as those produced under license.\n\n\n\n\n\n\n\n\n"}
{"id": "27572189", "url": "https://en.wikipedia.org/wiki?curid=27572189", "title": "List of weapons of the Spanish–American War", "text": "List of weapons of the Spanish–American War\n\nThis is a list of weapons of the Spanish–American War. The Spanish–American War was a conflict in 1898 between Spain and the United States, effectively the result of American intervention in the ongoing Cuban War of Independence.\n\nRifles\n\nRevolvers\n\nShotguns\n\nMachineguns\n\nCannons\n\nMelee\n\nRifles\n\nPistols\n\nMachine guns\n\nCannons\n\nMelee\n\n"}
{"id": "47140358", "url": "https://en.wikipedia.org/wiki?curid=47140358", "title": "London dial", "text": "London dial\n\nA London dial in the broadest sense can mean any sundial that is set for 51°30′ N, but more specifically refers to a engraved brass horizontal sundial with a distinctive design.\nLondon dials were originally engraved by scientific instrument makers. The trade was heavily protected by the system of craft guilds. \n\nA gnomon or style is set to point at the celestial north pole, the shadow of the sun is thrown onto the dial plate and will appear at the same position each day of the year, and this position can be calculated using trigonometry, or drawn using geometric construction. \nIn the world of sundials some of the technical terms use an old form of language, so the angle to which the style is set is called the style height. The style height is identical to the geographical latitude, and in London this was 51 degrees 30 minutes or 51.50 degrees, which roughly corresponds with Westminster Bridge.\n\nThe gnomon has thickness, and thus two shadow throwing edges (the styles) one for the morning and one for the afternoon, there is a gap left on the dialplate the width of the gnomon.\n\nA sundial displays the local apparent time, and watches that use mean time or average time will always be at a slight variance. This difference of time can be calculated and displayed on the dial – again using old language it is known as the equation of time.\nA London dials show hour lines, some show half-hour markers, the quarter hours and some divisions representing half-quarters (7 and a half minutes). The equation of time is shown as a three rings, showing the words \"WATCH SLOWER, WATCH FASTER\", the months and dates and the minutes of time of variance.\n\nThere are many methods to do this: the one published by Leybourn in 1652 is still popular. Though the one by Dom Francois Bedos de Celles in 1790 is more widely known.\n\n\nA horizontal dial, takes the equiangular hour lines of a equatorial dial and projects them onto an plane oblique to the style, the markup simulates this transformation.\n\nThe Dom Francois Bedos de Celles method (1760) otherwise known as the Waugh method (1973) \n\n\"Caveat: These diagrams have not been tested for accuracy. The further north the wider the dial becomes.\"\n\n\n"}
{"id": "17537", "url": "https://en.wikipedia.org/wiki?curid=17537", "title": "Lysergic acid diethylamide", "text": "Lysergic acid diethylamide\n\nLysergic acid diethylamide (LSD), also known as acid, is a hallucinogenic drug. Effects typically include altered thoughts, feelings, and awareness of one's surroundings. Many users see or hear things that do not exist. Dilated pupils, increased blood pressure, and increased body temperature are typical. Effects typically begin within half an hour and can last for up to 12 hours. It is used mainly as a recreational drug and for spiritual reasons. \nWhile LSD does not appear to be addictive, tolerance with use of increasing doses may occur. Adverse psychiatric reactions such as anxiety, paranoia, and delusions are possible. Distressing flashbacks, a condition called hallucinogen persisting perception disorder, may occur despite no further use. Death as a result of LSD is very rare, though occasionally occurs via accidents. The effects of LSD are believed to occur as a result of alterations in the serotonin system. As little as 20 micrograms can produce an effect. In pure form LSD is clear or white in color, has no smell, and is crystalline. It breaks down with exposure to ultraviolet light.\nIn the United States, as of 2017, about 10% of people have used LSD at some point in their life, while 0.7% have used it in the last year. It was most popular in the 1960s to 1980s. LSD is typically either swallowed or held under the tongue. It is most often sold on blotter paper and less commonly as tablets or in gelatin squares. There are no known treatments for addiction, if it occurs. \nLSD was first made by Albert Hofmann in 1938 from lysergic acid, a chemical from the fungus ergot. Hofmann discovered its hallucinogenic properties in 1943. In the 1950s, the Central Intelligence Agency (CIA) believed the drug might be useful for mind control so tested it on people, some without their knowledge, in a program called MKUltra. LSD was sold as a medication for research purposes under the trade-name Delysid in the 1950s and 1960s. It was listed as a schedule 1 controlled substance by the United Nations in 1971. It currently has no approved medical use. In Europe, as of 2011, the typical cost of a dose was between 4.50 and 25 Euro.\n\nLSD is commonly used as a recreational drug.\n\nLSD is considered an entheogen because it can catalyze intense spiritual experiences, during which users may feel they have come into contact with a greater spiritual or cosmic order. Users sometimes report out of body experiences. In 1966, Timothy Leary established the League for Spiritual Discovery with LSD as its sacrament. Stanislav Grof has written that religious and mystical experiences observed during LSD sessions appear to be phenomenologically indistinguishable from similar descriptions in the sacred scriptures of the great religions of the world and the texts of ancient civilizations.\n\nLSD currently has no approved uses in medicine. A meta analysis concluded that a single dose was effective at reducing alcohol consumption in alcoholism. LSD has also been studied in depression, anxiety, and drug dependence, with positive preliminary results.\n\nLSD can cause pupil dilation, reduced appetite, and wakefulness. Other physical reactions to LSD are highly variable and nonspecific, some of which may be secondary to the psychological effects of LSD. Among the reported symptoms are numbness, weakness, nausea, hypothermia or hyperthermia, elevated blood sugar, goose bumps, heart rate increase, jaw clenching, perspiration, saliva production, mucus production, hyperreflexia, and tremors.\n\nThe most common immediate psychological effects of LSD are visual hallucinations and illusions (colloquially known as \"trips\"), which can vary greatly depending on how much is used and how the brain responds. Trips usually start within 20–30 minutes of taking LSD by mouth (less if snorted or taken intravenously), peak three to four hours after ingestion, and last up to 12 hours. Negative experiences, referred to as \"bad trips\", produce intense negative emotions, such as irrational fears and anxiety, panic attacks, paranoia, rapid mood swings, intrusive thoughts of hopelessness, wanting to harm others, and suicidal ideation. It is impossible to predict when a bad trip will occur. Good trips are stimulating and pleasurable, and typically involve feeling as if one is floating, disconnected from reality, feelings of joy or euphoria (sometimes called a \"rush\"), decreased inhibitions, and the belief that one has extreme mental clarity or superpowers.\n\nSome sensory effects may include an experience of radiant colors, objects and surfaces appearing to ripple or \"breathe\", colored patterns behind the closed eyelids (eidetic imagery), an altered sense of time (time seems to be stretching, repeating itself, changing speed or stopping), crawling geometric patterns overlaying walls and other objects, and morphing objects. Some users, including Albert Hofmann, report a strong metallic taste for the duration of the effects.\n\nLSD causes an animated sensory experience of senses, emotions, memories, time, and awareness for 6 to 14 hours, depending on dosage and tolerance. Generally beginning within 30 to 90 minutes after ingestion, the user may experience anything from subtle changes in perception to overwhelming cognitive shifts. Changes in auditory and visual perception are typical. Visual effects include the illusion of movement of static surfaces (\"walls breathing\"), after image-like trails of moving objects (\"tracers\"), the appearance of moving colored geometric patterns (especially with closed eyes), an intensification of colors and brightness (\"sparkling\"), new textures on objects, blurred vision, and shape suggestibility. Some users report that the inanimate world appears to animate in an inexplicable way; for instance, objects that are static in three dimensions can seem to be moving relative to one or more additional spatial dimensions. Many of the basic visual effects resemble the phosphenes seen after applying pressure to the eye and have also been studied under the name \"form constants\". The auditory effects of LSD may include echo-like distortions of sounds, changes in ability to discern concurrent auditory stimuli, and a general intensification of the experience of music. Higher doses often cause intense and fundamental distortions of sensory perception such as synaesthesia, the experience of additional spatial or temporal dimensions, and temporary dissociation.\n\nOf the 20 drugs ranked according to individual and societal harm by David Nutt, LSD was third to last, approximately 1/10th as harmful as alcohol. The most significant adverse effect was impairment of mental functioning while intoxicated.\n\nLSD may trigger panic attacks or feelings of extreme anxiety, known familiarly as a \"bad trip.\" Review studies suggest that LSD likely plays a role in precipitating the onset of acute psychosis in previously healthy individuals with an increased likelihood in individuals who have a family history of schizophrenia. There is evidence that people with severe mental illnesses like schizophrenia have a higher likelihood of experiencing adverse effects from taking LSD.\n\nWhile publicly available documents indicate that the CIA and Department of Defense have discontinued research into the use of LSD as a means of mind control, research from the 1960s suggests that both mentally ill and healthy people are more suggestible while under its influence.\n\n\"flashbacks\" are a reported psychological phenomenon in which an individual experiences an episode of some of LSD's subjective effects after the drug has worn off, \"persisting for months or years after hallucinogen use\".\n\nA diagnosable condition called hallucinogen persisting perception disorder has been defined to describe intermittent or chronic flashbacks that cause distress or impairment in life and work, and are caused only by prior hallucinogen use and not some other condition.\n\nThe mutagenic potential of LSD is unclear. Overall, the evidence seems to point to limited or no effect at commonly used doses. Empirical studies showed no evidence of teratogenic or mutagenic effects from use of LSD in man.\n\nTolerance to LSD builds up over consistent use and cross-tolerance has been demonstrated between LSD, mescaline\nand psilocybin.\nThis tolerance is probably caused by downregulation of 5-HT receptors in the brain and diminishes a few days after cessation of use.\n\nLSD is not addictive. Experimental evidence has demonstrated that LSD use does not yield positive reinforcement in either human or animal subjects.\n\nAs of 2008 there were no documented fatalities attributed directly to an LSD overdose. Despite this several behavioral fatalities and suicides have occurred due to LSD. Eight individuals who accidentally consumed very high amounts by mistaking LSD for cocaine developed comatose states, hyperthermia, vomiting, gastric bleeding, and respiratory problems–however, all survived with supportive care.\n\nReassurance in a calm, safe environment is beneficial. Agitation can be safely addressed with benzodiazepines such as lorazepam or diazepam. Neuroleptics such as haloperidol are recommended against because they may have adverse effects. LSD is rapidly absorbed, so activated charcoal and emptying of the stomach will be of little benefit, unless done within 30–60 minutes of ingesting an overdose of LSD. Sedation or physical restraint is rarely required, and excessive restraint may cause complications such as hyperthermia (over-heating) or rhabdomyolysis.\n\nResearch suggests that massive doses are not lethal, but do typically require supportive care, which may include endotracheal intubation or respiratory support. It is recommended that high blood pressure, tachycardia (rapid heart-beat), and hyperthermia, if present, are treated symptomatically, and that low blood pressure is treated initially with fluids and then with pressors if necessary. Intravenous administration of anticoagulants, vasodilators, and sympatholytics may be useful with massive doses.\n\nMost serotonergic psychedelics are not significantly dopaminergic, and LSD is therefore atypical in this regard. The agonism of the D receptor by LSD may contribute to its psychoactive effects in humans.\n\nLSD binds to most serotonin receptor subtypes except for the 5-HT and 5-HT receptors. However, most of these receptors are affected at too low affinity to be sufficiently activated by the brain concentration of approximately 10–20 nM. In humans, recreational doses of LSD can affect 5-HT (K=1.1nM), 5-HT (K=2.9nM), 5-HT (K=4.9nM), 5-HT (K=23nM), 5-HT (K=9nM [in cloned rat tissues]), and 5-HT receptors (K=2.3nM). 5-HT receptors, which are not present in humans, also have a high affinity for LSD. The psychedelic effects of LSD are attributed to cross-activation of 5-HT receptor heteromers. Many but not all 5-HT agonists are psychedelics and 5-HT antagonists block the psychedelic activity of LSD. LSD exhibits functional selectivity at the 5-HT and 5HT receptors in that it activates the signal transduction enzyme phospholipase A2 instead of activating the enzyme phospholipase C as the endogenous ligand serotonin does. Exactly how LSD produces its effects is unknown, but it is thought that it works by increasing glutamate release in the cerebral cortex and therefore excitation in this area, specifically in layers IV and V. LSD, like many other drugs of recreational use, has been shown to activate DARPP-32-related pathways. The drug enhances dopamine D receptor protomer recognition and signaling of D–5-HT receptor complexes, which may contribute to its psychotic effects.\n\nThe crystal structure of LSD bound in its active state to a serotonin receptor, specifically the 5-HT receptor, has recently (2017) been elucidated for the first time. The LSD-bound 5-HT receptor is regarded as an excellent model system for the 5-HT receptor and the structure of the LSD-bound 5-HT receptor was used in the study as a template to determine the structural features necessary for the activity of LSD at the 5-HT receptor. The diethylamide moiety of LSD was found to be a key component for its activity, which is in accordance with the fact that the related lysergamide lysergic acid amide (LSA) is far less hallucinogenic in comparison. LSD was found to stay bound to both the 5-HT and 5-HT receptors for an exceptionally long amount of time, which may be responsible for its long duration of action in spite of its relatively short terminal half-life. The extracellular loop 2 leucine 209 residue of the 5-HT2B receptor forms a 'lid' over LSD that appears to trap it in the receptor, and this was implicated in the potency and functional selectivity of LSD and its very slow dissociation rate from the 5-HT receptors.\n\nThe effects of LSD normally last between 6 and 12 hours depending on dosage, tolerance, body weight, and age. The Sandoz prospectus for \"Delysid\" warned: \"intermittent disturbances of affect may occasionally persist for several days.\" Contrary to early reports and common belief, LSD effects do not last longer than the amount of time significant levels of the drug are present in the blood. Aghajanian and Bing (1964) found LSD had an elimination half-life of only 175 minutes (about 3 hours). However, using more accurate techniques, Papac and Foltz (1990) reported that 1 µg/kg oral LSD given to a single male volunteer had an apparent plasma half-life of 5.1 hours, with a peak plasma concentration of 5 ng/mL at 3 hours post-dose.\n\nThe pharmacokinetics of LSD were not properly determined until 2015, which is not surprising for a drug with the kind of low-μg potency that LSD possesses. In a sample of 16 healthy subjects, a single mid-range 200 μg oral dose of LSD was found to produce mean maximal concentrations of 4.5 ng/mL at a median of 1.5 hours (range 0.5–4 hours) post-administration. After attainment of peak levels, concentrations of LSD decreased following first-order kinetics with a terminal half-life of 3.6 hours for up to 12 hours and then with slower elimination with a terminal half-life of 8.9 hours thereafter. The effects of the dose of LSD given lasted for up to 12 hours and were closely correlated with the concentrations of LSD present in circulation over time, with no acute tolerance observed. Only 1% of the drug was eliminated in urine unchanged whereas 13% was eliminated as the major metabolite 2-oxo-3-hydroxy-LSD (O-H-LSD) within 24 hours. O-H-LSD is formed by cytochrome P450 enzymes, although the specific enzymes involved are unknown, and it does not appear to be known whether O-H-LSD is pharmacologically active or not. The oral bioavailability of LSD was crudely estimated as approximately 71% using previous data on intravenous administration of LSD. The sample was equally divided between male and female subjects and there were no significant sex differences observed in the pharmacokinetics of LSD.\n\nLSD is a chiral compound with two stereocenters at the carbon atoms C-5 and C-8, so that theoretically four different optical isomers of LSD could exist. LSD, also called (+)--LSD, has the absolute configuration (5\"R\",8\"R\"). The C-5 isomers of lysergamides do not exist in nature and are not formed during the synthesis from \"d\"-lysergic acid. Retrosynthetically, the C-5 stereocenter could be analysed as having the same configuration of the alpha carbon of the naturally occurring amino acid L-tryptophan, the precursor to all biosynthetic ergoline compounds.\n\nHowever, LSD and iso-LSD, the two C-8 isomers, rapidly interconvert in the presence of bases, as the alpha proton is acidic and can be deprotonated and reprotonated. Non-psychoactive iso-LSD which has formed during the synthesis can be separated by chromatography and can be isomerized to LSD.\n\nPure salts of LSD are triboluminescent, emitting small flashes of white light when shaken in the dark. LSD is strongly fluorescent and will glow bluish-white under UV light.\n\nLSD is an ergoline derivative. It is commonly synthesized by reacting diethylamine with an activated form of lysergic acid. Activating reagents include phosphoryl chloride and peptide coupling reagents. Lysergic acid is made by alkaline hydrolysis of lysergamides like ergotamine, a substance usually derived from the ergot fungus on agar plate; or, theoretically possible, but impractical and uncommon, from ergine (lysergic acid amide, LSA) extracted from morning glory seeds. Lysergic acid can also be produced synthetically, eliminating the need for ergotamines.\n\nA single dose of LSD may be between 40 and 500 micrograms—an amount roughly equal to one-tenth the mass of a grain of sand. Threshold effects can be felt with as little as 25 micrograms of LSD. Dosages of LSD are measured in micrograms (µg), or millionths of a gram. By comparison, dosages of most drugs, both recreational and medicinal, are measured in milligrams (mg), or thousandths of a gram. For example, an active dose of mescaline, roughly , has effects comparable to 100 µg or less of LSD.\n\nIn the mid-1960s, the most important black market LSD manufacturer (Owsley Stanley) distributed acid at a standard concentration of 270 µg, while street samples of the 1970s contained 30 to 300 µg. By the 1980s, the amount had reduced to between 100 and 125 µg, dropping more in the 1990s to the 20–80 µg range, and even more in the 2000s (decade).\n\n\"LSD,\" writes the chemist Alexander Shulgin, \"is an unusually fragile molecule… As a salt, in water, cold, and free from air and light exposure, it is stable indefinitely.\"\n\nLSD has two labile protons at the tertiary stereogenic C5 and C8 positions, rendering these centres prone to epimerisation. The C8 proton is more labile due to the electron-withdrawing carboxamide attachment, but removal of the chiral proton at the C5 position (which was once also an alpha proton of the parent molecule tryptophan) is assisted by the inductively withdrawing nitrogen and pi electron delocalisation with the indole ring.\n\nLSD also has enamine-type reactivity because of the electron-donating effects of the indole ring. Because of this, chlorine destroys LSD molecules on contact; even though chlorinated tap water contains only a slight amount of chlorine, the small quantity of compound typical to an LSD solution will likely be eliminated when dissolved in tap water. The double bond between the 8-position and the aromatic ring, being conjugated with the indole ring, is susceptible to nucleophilic attacks by water or alcohol, especially in the presence of light. LSD often converts to \"lumi-LSD\", which is inactive in human beings.\n\nA controlled study was undertaken to determine the stability of LSD in pooled urine samples.\nThe concentrations of LSD in urine samples were followed over time at various temperatures, in different types of storage containers, at various exposures to different wavelengths of light, and at varying pH values. These studies demonstrated no significant loss in LSD concentration at 25 °C for up to four weeks. After four weeks of incubation, a 30% loss in LSD concentration at 37 °C and up to a 40% at 45 °C were observed. Urine fortified with LSD and stored in amber glass or nontransparent polyethylene containers showed no change in concentration under any light conditions. Stability of LSD in transparent containers under light was dependent on the distance between the light source and the samples, the wavelength of light, exposure time, and the intensity of light. After prolonged exposure to heat in alkaline pH conditions, 10 to 15% of the parent LSD epimerized to iso-LSD. Under acidic conditions, less than 5% of the LSD was converted to iso-LSD. It was also demonstrated that trace amounts of metal ions in buffer or urine could catalyze the decomposition of LSD and that this process can be avoided by the addition of EDTA.\n\nLSD may be quantified in urine as part of a drug abuse testing program, in plasma or serum to confirm a diagnosis of poisoning in hospitalized victims or in whole blood to assist in a forensic investigation of a traffic or other criminal violation or a case of sudden death. Both the parent drug and its major metabolite are unstable in biofluids when exposed to light, heat or alkaline conditions and therefore specimens are protected from light, stored at the lowest possible temperature and analyzed quickly to minimize losses.\n\nThe apparent plasma half life of LSD is considered to be around 5.1 hours with peak plasma concentrations occurring 3 hours after administration.\n\nLSD was first synthesized on November 16, 1938 by Swiss chemist Albert Hofmann at the Sandoz Laboratories in Basel, Switzerland as part of a large research program searching for medically useful ergot alkaloid derivatives. LSD's psychedelic properties were discovered 5 years later when Hofmann himself accidentally ingested an unknown quantity of the chemical. The first intentional ingestion of LSD occurred on April 19, 1943, when Hofmann ingested 250 µg of LSD. He said this would be a threshold dose based on the dosages of other ergot alkaloids. Hofmann found the effects to be much stronger than he anticipated. Sandoz Laboratories introduced LSD as a psychiatric drug in 1947 and marketed LSD as a psychiatric panacea, hailing it \"as a cure for everything from schizophrenia to criminal behavior, ‘sexual perversions,’ and alcoholism.”\nBeginning in the 1950s, the US Central Intelligence Agency began a research program code named Project MKULTRA. Experiments included administering LSD to CIA employees, military personnel, doctors, other government agents, prostitutes, mentally ill patients, and members of the general public in order to study their reactions, usually without the subjects' knowledge. The project was revealed in the US congressional Rockefeller Commission report in 1975.\n\nIn 1963, the Sandoz patents expired on LSD. Several figures, including Aldous Huxley, Timothy Leary, and Al Hubbard, began to advocate the consumption of LSD. LSD became central to the counterculture of the 1960s. In the early 1960s the use of LSD and other hallucinogens was advocated by new proponents of consciousness expansion such as Leary, Huxley, Alan Watts and Arthur Koestler, and according to L. R. Veysey they profoundly influenced the thinking of the new generation of youth.\n\nOn October 24, 1968, possession of LSD was made illegal in the United States. The last FDA approved study of LSD in patients ended in 1980, while a study in healthy volunteers was made in the late 1980s. Legally approved and regulated psychiatric use of LSD continued in Switzerland until 1993.\n\nBy the mid-1960s, the youth countercultures in California, particularly in San Francisco, had adopted the use of hallucinogenic drugs, with the first major underground LSD factory established by Owsley Stanley. From 1964, the Merry Pranksters, a loose group that developed around novelist Ken Kesey, sponsored the Acid Tests, a series of events primarily staged in or near San Francisco, involving the taking of LSD (supplied by Stanley), accompanied by light shows, film projection and discordant, improvised music known as the \"psychedelic symphony\". The Pranksters helped popularize LSD use, through their road trips across America in a psychedelically-decorated converted school bus, which involved distributing the drug and meeting with major figures of the beat movement, and through publications about their activities such as Tom Wolfe's \"The Electric Kool-Aid Acid Test\" (1968).\n\nIn San Francisco's Haight-Ashbury neighborhood, brothers Ron and Jay Thelin opened the Psychedelic Shop in January 1966. The Thelins' store is regarded as the first ever head shop. The Thelins opened the store to promote safe use of LSD, which was then still legal in California. The Psychedelic Shop helped to further popularize LSD in the Haight and to make the neighborhood the unofficial capital of the hippie counterculture in the United States. Ron Thelin was also involved in organizing the Love Pageant rally, a protest held in Golden Gate park to protest California's newly adopted ban on LSD in October 1966. At the rally, hundreds of attendees took acid in unison. Although the Psychedelic Shop closed after barely a year-and-a-half in business, its role in popularizing LSD was considerable.\n\nA similar and connected nexus of LSD use in the creative arts developed around the same time in London. A key figure in this phenomenon in the UK was British academic Michael Hollingshead, who first tried LSD in America in 1961 while he was the Executive Secretary for the Institute of British-American Cultural Exchange. After being given a large quantity of pure Sandoz LSD (which was still legal at the time) and experiencing his first \"trip\", Hollingshead contacted Aldous Huxley, who suggested that he get in touch with Harvard academic Timothy Leary, and over the next few years, in concert with Leary and Richard Alpert, Hollingshead played a major role in their famous LSD research at Millbrook before moving to New York City, where he conducted his own LSD experiments. In 1965 Hollingshead returned to the UK and founded the World Psychedelic Center in Chelsea, London. \n\nIn both music and art, the influence of LSD was soon being more widely seen and heard thanks to the bands that participated in the Acid Tests and related events, including the Grateful Dead, Jefferson Airplane and Big Brother and the Holding Company, and through the inventive poster and album art of San Francisco-based artists like Rick Griffin, Victor Moscoso, Bonnie MacLean, Stanley Mouse & Alton Kelley, and Wes Wilson, meant to evoke the visual experience of an LSD trip. LSD had a strong influence on the Grateful Dead and the culture of \"Deadheads\".\n\nAmong the many famous people in the UK that Michael Hollingshead is reputed to have introduced to LSD are artist and Hipgnosis founder Storm Thorgerson, and musicians Donovan, Keith Richards, Paul McCartney, John Lennon, and George Harrison. Although establishment concern about the new drug led to it being declared an illegal drug by the Home Secretary in 1966, LSD was soon being used widely in the upper echelons of the British art and music scene, including members of the Beatles, the Rolling Stones, the Moody Blues, the Small Faces, Pink Floyd, Jimi Hendrix and others, and the products of these experiences were soon being both heard and seen by the public with singles like The Small Faces' \"Itchycoo Park\" and LPs like the Beatles' \"Sgt Pepper's Lonely Hearts Club Band\" and Cream's \"Disraeli Gears\", which featured music that showed the obvious influence of the musicians' recent psychedelic excursions, and which were packaged in elaborately-designed album covers that featured vividly-coloured psychedelic artwork by artists like Peter Blake, Martin Sharp, Hapshash and the Coloured Coat (Nigel Waymouth and Michael English) and art/music collective \"The Fool\".\n\nIn the 1960s, musicians from psychedelic music and psychedelic rock bands began to refer (at first indirectly, and later explicitly) to the drug and attempted to recreate or reflect the experience of taking LSD in their music. A number of features are often included in psychedelic music. Exotic instrumentation, with a particular fondness for the sitar and tabla are common. Electric guitars are used to create feedback, and are played through wah wah and fuzzbox effect pedals. Elaborate studio effects are often used, such as backwards tapes, panning, phasing, long delay loops, and extreme reverb. In the 1960s there was a use of primitive electronic instruments such as early synthesizers and the theremin. Later forms of electronic psychedelia also employed repetitive computer-generated beats. Songs allegedly referring to LSD include John Prine's \"Illegal Smile\" and The Beatles' song \"Lucy in the Sky with Diamonds\", although the authors of the latter song repeatedly denied this claim.\n\nIn modern times, LSD has had a prominent influence on artists such as Keith Haring, electronic dance music, and the jam band Phish.\n\nThe United Nations Convention on Psychotropic Substances (adopted in 1971) requires the signing parties to prohibit LSD. Hence, it is illegal in all countries that were parties to the convention, including the United States, Australia, New Zealand, and most of Europe. However, enforcement of those laws varies from country to country. Medical and scientific research with LSD in humans is permitted under the 1971 UN Convention.\n\nLSD is a Schedule 9 prohibited substance in Australia under the Poisons Standard (February 2017). A Schedule 9 substance is defined as a substance which may be abused or misused, the manufacture, possession, sale or use of which should be prohibited by law except when required for medical or scientific research, or for analytical, teaching or training purposes with approval of Commonwealth and/or State or Territory Health Authorities.\n\nIn Western Australia section 9 of the Misuse of Drugs Act 1981 provides for summary trial before a magistrate for possession of less than 0.004g; section 11 provides rebuttable presumptions of intent to sell or supply if the quantity is 0.002g or more, or of possession for the purpose of trafficking if 0.01g.\n\nIn Canada, LSD is a controlled substance under Schedule III of the Controlled Drugs and Substances Act. Every person who seeks to obtain the substance, without disclosing authorization to obtain such substances 30 days before obtaining another prescription from a practitioner, is guilty of an indictable offense and liable to imprisonment for a term not exceeding 3 years. Possession for purpose of trafficking is an indictable offense punishable by imprisonment for 10 years.\n\nIn the United Kingdom, LSD is a Schedule 1 Class 'A' drug. This means it has no recognized legitimate uses and possession of the drug without a license is punishable with 7 years' imprisonment and/or an unlimited fine, and trafficking is punishable with life imprisonment and an unlimited fine (\"see main article on drug punishments Misuse of Drugs Act 1971).\"\n\nIn 2000, after consultation with members of the Royal College of Psychiatrists' Faculty of Substance Misuse, the UK Police Foundation issued the Runciman Report which recommended \"the transfer of LSD from Class A to Class B\".\n\nIn November 2009, the UK Transform Drug Policy Foundation released in the House of Commons a guidebook to the legal regulation of drugs, \"After the War on Drugs: Blueprint for Regulation\", which details options for regulated distribution and sale of LSD and other psychedelics.\n\nLSD is Schedule I in the United States, according to the Controlled Substances Act of 1970. This means LSD is illegal to manufacture, buy, possess, process, or distribute without a license from the Drug Enforcement Administration (DEA). By classifying LSD as a Schedule I substance, the DEA holds that LSD meets the following three criteria: it is deemed to have a high potential for abuse; it has no legitimate medical use in treatment; and there is a lack of accepted safety for its use under medical supervision. There are no documented deaths from chemical toxicity; most LSD deaths are a result of behavioral toxicity.\n\nThere can also be substantial discrepancies between the amount of chemical LSD that one possesses and the amount of possession with which one can be charged in the US. This is because LSD is almost always present in a medium (e.g. blotter or neutral liquid), and the amount that can be considered with respect to sentencing is the total mass of the drug and its medium. This discrepancy was the subject of 1995 United States Supreme Court case, \"Neal v. United States\".\n\nLysergic acid and lysergic acid amide, LSD precursors, are both classified in Schedule III of the Controlled Substances Act. Ergotamine tartrate, a precursor to lysergic acid, is regulated under the Chemical Diversion and Trafficking Act.\n\nIn April 2009, the Mexican Congress approved changes in the General Health Law that decriminalized the possession of illegal drugs for immediate consumption and personal use, allowing a person to possess a moderate amount of LSD. The only restriction is that people in possession of drugs should not be within a 300-meter radius of schools, police departments, or correctional facilities. Marijuana, along with cocaine, opium, heroin, and other drugs were also decriminalized; their possession is not considered a crime as long as the dose does not exceed the limit established in the General Health Law. Many question this, as cocaine is as synthesised as heroin, and both are produced as extracts from plants. The law establishes very low amount thresholds and strictly defines personal dosage. For those arrested with more than the threshold allowed by the law this can result in heavy prison sentences, as they will be assumed to be small traffickers even if there are no other indications that the amount was meant for selling.\n\nIn the Czech Republic, until 31 December 1998 only drug possession \"for other person\" (i.e. intent to sell) was criminal (apart from production, importation, exportation, offering or mediation, which was and remains criminal) while possession for personal use remained legal.\n\nOn 1 January 1999, an amendment of the Criminal Code, which was necessitated in order to align the Czech drug rules with the Single Convention on Narcotic Drugs, became effective, criminalizing possession of \"amount larger than small\" also for personal use (Art. 187a of the Criminal Code) while possession of small amounts for personal use became a misdemeanor.\n\nThe judicial practice came to the conclusion that the \"amount larger than small\"\" must be five to ten times larger (depending on drug) than a usual single dose of an average consumer.\n\nUnder the Regulation No. 467/2009 Coll, possession of less than 5 doses of LSD was to be considered smaller than large for the purposes of the Criminal Code and was to be treated as a misdemeanor subject to a fine equal to a parking ticket.\n\nAccording to the 2008 Constitution of Ecuador, in its Article 364, the Ecuadorian state does not see drug consumption as a crime but only as a health concern. Since June 2013 the State drugs regulatory office CONSEP has published a table which establishes maximum quantities carried by persons so as to be considered in legal possession and that person as not a seller of drugs. The \"CONSEP established, at their latest general meeting, that the 0.020 milligrams of LSD shal be considered the maximum consumer amount.\n\nAn active dose of LSD is very minute, allowing a large number of doses to be synthesized from a comparatively small amount of raw material. Twenty five kilograms of precursor ergotamine tartrate can produce 5–6 kg of pure crystalline LSD; this corresponds to 100 million doses. Because the masses involved are so small, concealing and transporting illicit LSD is much easier than smuggling cocaine, cannabis, or other illegal drugs.\n\nManufacturing LSD requires laboratory equipment and experience in the field of organic chemistry. It takes two to three days to produce 30 to 100 grams of pure compound. It is believed that LSD is not usually produced in large quantities, but rather in a series of small batches. This technique minimizes the loss of precursor chemicals in case a step does not work as expected.\n\nLSD is produced in crystalline form and then mixed with excipients or redissolved for production in ingestible forms. Liquid solution is either distributed in small vials or, more commonly, sprayed onto or soaked into a distribution medium. Historically, LSD solutions were first sold on sugar cubes, but practical considerations forced a change to tablet form. Appearing in 1968 as an orange tablet measuring about 6 mm across, \"Orange Sunshine\" acid was the first largely available form of LSD after its possession was made illegal. Tim Scully, a prominent chemist, made some of these tablets, but said that most \"Sunshine\" in the USA came by way of Ronald Stark, who imported approximately thirty-five million doses from Europe.\n\nOver a period of time, tablet dimensions, weight, shape and concentration of LSD evolved from large (4.5–8.1 mm diameter), heavyweight (≥150 mg), round, high concentration (90–350 µg/tab) dosage units to small (2.0–3.5 mm diameter) lightweight (as low as 4.7 mg/tab), variously shaped, lower concentration (12–85 µg/tab, average range 30–40 µg/tab) dosage units. LSD tablet shapes have included cylinders, cones, stars, spacecraft, and heart shapes. The smallest tablets became known as \"Microdots\".\n\nAfter tablets came \"computer acid\" or \"blotter paper LSD\", typically made by dipping a preprinted sheet of blotting paper into an LSD/water/alcohol solution. More than 200 types of LSD tablets have been encountered since 1969 and more than 350 blotter paper designs have been observed since 1975. About the same time as blotter paper LSD came \"Windowpane\" (AKA \"Clearlight\"), which contained LSD inside a thin gelatin square a quarter of an inch (6 mm) across. LSD has been sold under a wide variety of often short-lived and regionally restricted street names including Acid, Trips, Uncle Sid, Blotter, Lucy, Alice and doses, as well as names that reflect the designs on the sheets of blotter paper. Authorities have encountered the drug in other forms—including powder or crystal, and capsule.\n\nLSD manufacturers and traffickers in the United States can be categorized into two groups: A few large-scale producers, and an equally limited number of small, clandestine chemists, consisting of independent producers who, operating on a comparatively limited scale, can be found throughout the country. As a group, independent producers are of less concern to the Drug Enforcement Administration than the larger groups because their product reaches only local markets.\n\nMany LSD dealers and chemists describe a religious or humanitarian purpose that motivates their illicit activity. Nicholas Schou's book \"Orange Sunshine: The Brotherhood of Eternal Love and Its Quest to Spread Peace, Love, and Acid to the World\" describes one such group, the Brotherhood of Eternal Love. The group was a major American LSD trafficking group in the late 1960s and early 1970s.\n\nIn the second half of the 20th century, dealers and chemists loosely associated with the Grateful Dead like Owsley Stanley, Nicholas Sand, Karen Horning, Sarah Maltzer, \"Dealer McDope,\" and Leonard Pickard played an essential role in distributing LSD.\n\nSince 2005, law enforcement in the United States and elsewhere has seized several chemicals and combinations of chemicals in blotter paper which were sold as LSD mimics, including DOB, a mixture of DOC and DOI, 25I-NBOMe, and a mixture of DOC and DOB. Street users of LSD are often under the impression that blotter paper which is actively hallucinogenic can only be LSD because that is the only chemical with low enough doses to fit on a small square of blotter paper. While it is true that LSD requires lower doses than most other hallucinogens, blotter paper is capable of absorbing a much larger amount of material. The DEA performed a chromatographic analysis of blotter paper containing 2C-C which showed that the paper contained a much greater concentration of the active chemical than typical LSD doses, although the exact quantity was not determined. Blotter LSD mimics can have relatively small dose squares; a sample of blotter paper containing DOC seized by Concord, California police had dose markings approximately 6 mm apart. Several deaths have been attributed to 25I-NBOMe.\n\nA number of organizations—including the Beckley Foundation, MAPS, Heffter Research Institute and the Albert Hofmann Foundation—exist to fund, encourage and coordinate research into the medicinal and spiritual uses of LSD and related psychedelics. New clinical LSD experiments in humans started in 2009 for the first time in 35 years. As it is illegal in many areas of the world, potential medical uses are difficult to study.\n\nIn 2001 the United States Drug Enforcement Administration stated that LSD \"produces no aphrodisiac effects, does not increase creativity, has no lasting positive effect in treating alcoholics or criminals, does not produce a 'model psychosis', and does not generate immediate personality change.\" More recently, experimental uses of LSD have included the treatment of alcoholism and pain and cluster headache relief.\n\nIn the 1950s and 1960s LSD was used in psychiatry to enhance psychotherapy known as psychedelic therapy. Some psychiatrists believed LSD was especially useful at helping patients to \"unblock\" repressed subconscious material through other psychotherapeutic methods, and also for treating alcoholism. One study concluded, \"The root of the therapeutic value of the LSD experience is its potential for producing self-acceptance and self-surrender,\" presumably by forcing the user to face issues and problems in that individual's psyche.\n\nTwo recent reviews concluded that conclusions drawn from most of these early trials are unreliable due to serious methodological flaws. These include the absence of adequate control groups, lack of followup, and vague criteria for therapeutic outcome. In many cases studies failed to convincingly demonstrate whether the drug or the therapeutic interaction was responsible for any beneficial effects.\n\nIn recent years organizations like the Multidisciplinary Association for Psychedelic Studies have renewed clinical research of LSD.\n\nIn the 1950s and 1960s, some psychiatrists (e.g. Oscar Janiger) explored the potential effect of LSD on creativity. Experimental studies attempted to measure the effect of LSD on creative activity and aesthetic appreciation.\n\nSince 2008 there has been ongoing research into using LSD to alleviate anxiety for terminally ill cancer patients coping with their impending deaths.\n\nA 2012 meta-analysis found evidence that a single dose of LSD in conjunction with various alcoholism treatment programs was associated with a decrease in alcohol abuse, lasting for several months, but no effect was seen at one year. Adverse events included seizure, moderate confusion and agitation, nausea, vomiting, and acting in a bizarre fashion.\n\nLSD has been used as a treatment for cluster headaches with positive results in some small studies.\n\nSome notable individuals have commented publicly on their experiences with LSD. Some of these comments date from the era when it was legally available in the US and Europe for non-medical uses, and others pertain to psychiatric treatment in the 1950s and 1960s. Still others describe experiences with illegal LSD, obtained for philosophic, artistic, therapeutic, spiritual, or recreational purposes.\n\n\n\n\n"}
{"id": "19377", "url": "https://en.wikipedia.org/wiki?curid=19377", "title": "Microelectronics", "text": "Microelectronics\n\nMicroelectronics is a subfield of electronics. As the name suggests, microelectronics relates to the study and manufacture (or microfabrication) of very small electronic designs and components. Usually, but not always, this means micrometre-scale or smaller. These devices are typically made from semiconductor materials. Many components of normal electronic design are available in a microelectronic equivalent. These include transistors, capacitors, inductors, resistors, diodes and (naturally) insulators and conductors can all be found in microelectronic devices. Unique wiring techniques such as wire bonding are also often used in microelectronics because of the unusually small size of the components, leads and pads. This technique requires specialized equipment and is expensive.\n\nDigital integrated circuits (ICs) consist mostly of transistors. Analog circuits commonly contain resistors and capacitors as well. Inductors are used in some high frequency analog circuits, but tend to occupy larger chip area due to their lower reactance at low frequencies. Gyrators can replace them in many applications.\n\nAs techniques have improved, the scale of microelectronic components has continued to decrease. At smaller scales, the relative impact of intrinsic circuit properties such as interconnections may become more significant. These are called parasitic effects, and the goal of the microelectronics design engineer is to find ways to compensate for or to minimize these effects, while delivering smaller, faster, and cheaper devices.\n\nToday, microelectronics design is largely aided by Electronic Design Automation software.\n\n\n"}
{"id": "3089078", "url": "https://en.wikipedia.org/wiki?curid=3089078", "title": "Micros Systems", "text": "Micros Systems\n\nMicros Systems, Inc.; now owned by Oracle Corporation and renamed Oracle Hospitality (one of the global business business Units at Oracle Corporation), was headquartered in Columbia, Maryland, United States (as is the business unit it still based there). The company manufactured and sold computer hardware, software, and services for the restaurant point of sale, hotel, hospitality, sports and entertainment venues, casinos, cruise lines, specialty retail markets and other similar markets. Analyst estimates cited in 2003 put Micros' market share at about 35% of the restaurant point-of-sale business.\n\nOn June 23, 2014, Oracle Corporation announced its intent to purchase Micros Systems for $68 per share in cash for a total value of approximately $5.3 billion.\n\nThe company was incorporated in 1977 as Picos Manufacturing, Inc. and changed its name to Micros Systems, Inc. in 1978. Micros Systems, Inc. was headquartered in Columbia, Maryland. The name Micros is an acronym for Modular Integrated Cash Register Operating Systems.\n\nOpera is the Micros property management system used in many large hotel chains, such as Four Seasons Hotels and Resorts, Travelodge Hotels UK, Crown Resorts, Hyatt Hotels and Resorts, Rydges Hotels and Resorts, Oberoi Hotels & Resorts,Jupiter Hotels, Marriott Hotels, Starwood Hotels and Resorts, Resorts and Suites, Radisson Hotels and Resorts (subsidiary of Carlson Companies), the InterContinental Hotels Group and the Thistle Hotels.\n\nWith the purchase of the company Torex from Cerberus Capital Management, Micros acquired MiRetail Hub, a Workflow application written using Windows Workflow Foundation and designed for the Retail market.\n"}
{"id": "20091428", "url": "https://en.wikipedia.org/wiki?curid=20091428", "title": "NJIT Steel Bridge Team", "text": "NJIT Steel Bridge Team\n\nNJIT Steel Bridge Team is a team within the New Jersey Institute of Technology's ASCE chapter). It consists of undergraduate students who are attending at NJIT, majoring in civil engineering, and also members of ASCE. Every year, the team competes against other schools in a steel bridge competition.\n\nEvery year, the team has a few fund raising events, which are very crucial for the competition because the team needs to have proper finance in order to order parts and fabricate them. Besides fund raising, sponsors from other corporates and companies are very important too.\n\nA lot of time the team members get together and have outside activities such as hiking, paint ball, go cart, or whatever anyone wants to do. The team also does many productive activities such as visiting high schools and talking about civil engineering and the steel bridge competition. \n\nThere is at least a meeting every week talking about the process of the team and keeping members informed. Meetings are always held during common hours. Sometimes pizzas and drinks are served.\n\nThe objective of the competition is to design a light bridge yet strong and economically and assembly it fast with as few team members as possible. \n\nThe competition has 3 processes: Design & testing, which students do that themselves using programs, knowledges they learned from classes, and sometimes help from professors and alumni; Fabrication, which is when students grind, weld, and fit the parts together; and finally Assembly, which is when students put the parts together to achieve the designed bridge. \n\nThere are 6 categories in scoring: Display (how the bridge look), construction speed (time management), construction economy (low cost to build), lightness, stiffness (aggregate deflection), and structural efficiency (a formula used to calculate this based on weight and deflection).\n\nThe competition occurs every year. It is hosted at different places every time. This year, 2008, the regional competition will be hosted at Polytechnic Institute of NYU, New York City, on Saturday April 4, 2009; and the national competition will be hosted at University of Nevada, Las Vegas on May 22 and May 23. \n\nIf there are 1-4 teams competing in the region, the best team will proceed to the national competition\n\nIf there are 5-10 teams competing in the region, the 2 best teams will proceed to the national competition\n\nIf there are more than 10 teams competing in the region, the 3 best teams will proceed to the national competition\n\nThe price for winning the national competition is $2,500\n\n2012 National Competition at Clemson, SC\n15th Place Overall - Highest in History\n9th Place in Construction Speed\n\n2012 Regional Competition\n1st Place Overall\n1st Place Economy\n1st Place Stiffness\n1st Place Build Time\n2010 National Competition (at Purdue University)\n\n2010 Regional Competition (Regional competition score-sheet)\n\n2009 National Competition (at University of Nevada, Las Vegas)\n\n2009 Regional Competition (Regional competition score-sheet)\n\n2008 National Competition (at University of Florida)\n\n2008 Regional Competition\n\n2007 National Competition\n2007 Regional Competition\n\n2006 National Competition\n<br>\n2006 Regional Competition\n<br>\n2005 National Competition\n<br>\n2005 Regional Competition\n<br>\n· Second Place Overall\n<br>\n· First Place Efficiency\n<br>\n· First Place Aesthetics\n<br>\n· First Place Stiffness\n<br>\n2004 National Competition\n<br>\n2004 Regional Competition\n<br>\n2003 National Competition\n<br>\n2003 Regional Competition\n<br>\n· First Place Overall\n<br>\n· First Place Lightness\n<br>\n· First Place Efficiency\n<br>\n· First Place Economy\n<br>\n· First Place Stiffness\n<br>\n· First Place Construction Speed\n<br>\n2002 National Competition\n<br>\n2002 Regional Competition\n<br>\n· First Place Overall\n<br>\n· First Place Efficiency\n<br>\n· First Place Stiffness\n<br>\n2001 National Competition\n<br>\n2001 Regional Competition\n<br>\n· Second Place Overall\n<br>\n· First Place Efficiency\n<br>\n· First Place Aesthetics\n\n\n\n\n"}
{"id": "2046416", "url": "https://en.wikipedia.org/wiki?curid=2046416", "title": "Nuclear fuel", "text": "Nuclear fuel\n\nNuclear fuel is material used in nuclear power stations to produce heat to power turbines. Heat is created when nuclear fuel undergoes nuclear fission.\n\nMost nuclear fuels contain heavy fissile actinide elements that are capable of undergoing and sustaining nuclear fission. The three most relevant fissile isotopes are Uranium-233, Uranium-235 and Plutonium-239. When the unstable nuclei of these atoms are hit by a slow-moving neutron, they split, creating two daughter nuclei and two or three more neutrons. These neutrons then go on to split more nuclei. This creates a self-sustaining chain reaction that is controlled in a nuclear reactor, or uncontrolled in a nuclear weapon.\n\nThe processes involved in mining, refining, purifying, using, and disposing of nuclear fuel are collectively known as the nuclear fuel cycle. \n\nNot all types of nuclear fuels create power from nuclear fission; plutonium-238 and some other elements are used to produce small amounts of nuclear power by radioactive decay in radioisotope thermoelectric generators and other types of atomic batteries. \n\nNuclear fuel has the highest energy density of all practical fuel sources.\n\nFor fission reactors, the fuel (typically based on uranium) is usually based on the metal oxide; the oxides are used rather than the metals themselves because the oxide melting point is much higher than that of the metal and because it cannot burn, being already in the oxidized state.\n\nUranium dioxide is a black semiconducting solid. It can be made by reacting uranyl nitrate with a base (ammonia) to form a solid (ammonium uranate). It is heated (calcined) to form UO that can then be converted by heating in an argon / hydrogen mixture (700 °C) to form UO. The UO is then mixed with an organic binder and pressed into pellets, these pellets are then fired at a much higher temperature (in H/Ar) to sinter the solid. The aim is to form a dense solid which has few pores.\n\nThe thermal conductivity of uranium dioxide is very low compared with that of zirconium metal, and it goes down as the temperature goes up.\n\nCorrosion of uranium dioxide in water is controlled by similar electrochemical processes to the galvanic corrosion of a metal surface.\n\nMixed oxide, or MOX fuel, is a blend of plutonium and natural or depleted uranium which behaves similarly (though not identically) to the enriched uranium feed for which most nuclear reactors were designed. MOX fuel is an alternative to low enriched uranium (LEU) fuel used in the light water reactors which predominate nuclear power generation.\n\nSome concern has been expressed that used MOX cores will introduce new disposal challenges, though MOX is itself a means to dispose of surplus plutonium by transmutation.\n\nReprocessing of commercial nuclear fuel to make MOX was done in the Sellafield MOX Plant (England). As of 2015, MOX fuel is made in France (see Marcoule Nuclear Site), and to a lesser extent in Russia (see Mining and Chemical Combine), India and Japan. China plans to develop fast breeder reactors (see CEFR) and reprocessing.\n\nThe Global Nuclear Energy Partnership, was a U.S. proposal in the George W. Bush Administration to form an international partnership to see spent nuclear fuel reprocessed in a way that renders the plutonium in it usable for nuclear fuel but not for nuclear weapons. Reprocessing of spent commercial-reactor nuclear fuel has not been permitted in the United States due to nonproliferation considerations. All of the other reprocessing nations have long had nuclear weapons from military-focused \"research\"-reactor fuels except for Japan. Normally, with the fuel being changed every three years or so, about half of the Pu-239 is 'burned' in the reactor, providing about one third of the total energy. It behaves like U-235 and its fission releases a similar amount of energy. The higher the burn-up, the more plutonium in the spent fuel, but the lower the fraction of fissile plutonium. Typically about one percent of the used fuel discharged from a reactor is plutonium, and some two thirds of this is fissile (c. 50% Pu-239, 15% Pu-241). Worldwide, some 70 tonnes of plutonium contained in used fuel is removed when refueling reactors each year.\n\nMetal fuels have the advantage of a much higher heat conductivity than oxide fuels but cannot survive equally high temperatures. Metal fuels have a long history of use, stretching from the Clementine reactor in 1946 to many test and research reactors. Metal fuels have the potential for the highest fissile atom density. Metal fuels are normally alloyed, but some metal fuels have been made with pure uranium metal. Uranium alloys that have been used include uranium aluminum, uranium zirconium, uranium silicon, uranium molybdenum, and uranium zirconium hydride. Any of the aforementioned fuels can be made with plutonium and other actinides as part of a closed nuclear fuel cycle. Metal fuels have been used in water reactors and liquid metal fast breeder reactors, such as EBR-II.\n\nTRIGA fuel is used in TRIGA (Training, Research, Isotopes, General Atomics) reactors.\nThe TRIGA reactor uses UZrH fuel, which has a prompt negative fuel temperature coefficient of reactivity, meaning that as the temperature of the core increases, the reactivity decreases—so it is highly unlikely for a meltdown to occur. Most cores that use this fuel are \"high leakage\" cores where the excess leaked neutrons can be utilized for research. TRIGA fuel was originally designed to use highly enriched uranium, however in 1978 the U.S. Department of Energy launched its Reduced Enrichment for Research Test Reactors program, which promoted reactor conversion to low-enriched uranium fuel. A total of 35 TRIGA reactors have been installed at locations across the USA. A further 35 reactors have been installed in other countries.\n\nIn a fast neutron reactor, the minor actinides produced by neutron capture of uranium and plutonium can be used as fuel. Metal actinide fuel is typically an alloy of zirconium, uranium, plutonium, and minor actinides. It can be made inherently safe as thermal expansion of the metal alloy will increase neutron leakage.\n\nMolten plutonium, alloyed with other metals to lower its melting point and encapsulated in tantalum, was tested in two experimental reactors, LAMPRE I and LAMPRE II, at LANL in the 1960s. \"LAMPRE experienced three separate fuel failures during operation.\"\n\nCeramic fuels other than oxides have the advantage of high heat conductivities and melting points, but they are more prone to swelling than oxide fuels and are not understood as well.\n\nThis is often the fuel of choice for reactor designs that NASA produces, one advantage is that UN has a better thermal conductivity than UO. Uranium nitride has a very high melting point. This fuel has the disadvantage that unless N was used (in place of the more common N) that a large amount of C would be generated from the nitrogen by the (n,p) reaction. As the nitrogen required for such a fuel would be so expensive it is likely that the fuel would have to be reprocessed by pyroprocessing to enable the N to be recovered. It is likely that if the fuel was processed and dissolved in nitric acid that the nitrogen enriched with N would be diluted with the common N.\n\nMuch of what is known about uranium carbide is in the form of pin-type fuel elements for liquid metal fast reactors during their intense study during the 1960s and 1970s. However, recently there has been a revived interest in uranium carbide in the form of plate fuel and most notably, micro fuel particles (such as TRISO particles).\n\nThe high thermal conductivity and high melting point makes uranium carbide an attractive fuel. In addition, because of the absence of oxygen in this fuel (during the course of irradiation, excess gas pressure can build from the formation of O or other gases) as well as the ability to complement a ceramic coating (a ceramic-ceramic interface has structural and chemical advantages), uranium carbide could be the ideal fuel candidate for certain Generation IV reactors such as the gas-cooled fast reactor.\n\nLiquid fuels are liquids containing dissolved nuclear fuel and have been shown to offer numerous operational advantages compared to traditional solid fuel approaches.\nLiquid-fuel reactors offer significant safety advantages due to their inherently stable \"self-adjusting\" reactor dynamics. This provides two major benefits:\n- virtually eliminating the possibility of a run-away reactor meltdown,\n- providing an automatic load-following capability which is well suited to electricity generation and high-temperature industrial heat applications.\n\nAnother major advantage of the liquid core is its ability to be drained rapidly into a passively safe dump-tank. This advantage was conclusively demonstrated repeatedly as part of a weekly shutdown procedure during the highly successful 4 year Molten Salt Reactor Experiment.\n\nAnother huge advantage of the liquid core is its ability to release xenon gas which normally acts as a neutron absorber and causes structural occlusions in solid fuel elements (leading to the early replacement of solid fuel rods with over 98% of the nuclear fuel unburned, including many long-lived actinides). In contrast, Molten Salt Reactors (MSR) are capable of retaining the fuel mixture for significantly extended periods, which not only increases fuel efficiency dramatically but also incinerates the vast majority of its own waste as part of the normal operational characteristics.\n\nMolten salt fuels have nuclear fuel dissolved directly in the molten salt coolant. Molten salt-fueled reactors, such as the liquid fluoride thorium reactor (LFTR), are different from molten salt-cooled reactors that do not dissolve nuclear fuel in the coolant.\n\nMolten salt fuels were used in the LFTR known as the Molten Salt Reactor Experiment, as well as other liquid core reactor experiments. The liquid fuel for the molten salt reactor was a mixture of lithium, beryllium, thorium and uranium fluorides: LiF-BeF-ThF-UF (72-16-12-0.4 mol%). It had a peak operating temperature of 705 °C in the experiment, but could have operated at much higher temperatures since the boiling point of the molten salt was in excess of 1400 °C.\n\nThe aqueous homogeneous reactors (AHRs) use a solution of uranyl sulfate or other uranium salt in water. Historically, AHRs have all been small research reactors, not large power reactors. An AHR known as the Medical Isotope Production System is being considered for production of medical isotopes.\n\nUranium dioxide (UO) powder is compacted to cylindrical pellets and sintered at high temperatures to produce ceramic nuclear fuel pellets with a high density and well defined physical properties and chemical composition. A grinding process is used to achieve a uniform cylindrical geometry with narrow tolerances. Such fuel pellets are then stacked and filled into the metallic tubes. The metal used for the tubes depends on the design of the reactor. Stainless steel was used in the past, but most reactors now use a zirconium alloy which, in addition to being highly corrosion-resistant, has low neutron absorption. The tubes containing the fuel pellets are sealed: these tubes are called fuel rods. The finished fuel rods are grouped into fuel assemblies that are used to build up the core of a power reactor.\n\nCladding is the outer layer of the fuel rods, standing between the coolant and the nuclear fuel. It is made of a corrosion-resistant material with low absorption cross section for thermal neutrons, usually Zircaloy or steel in modern constructions, or magnesium with small amount of aluminium and other metals for the now-obsolete Magnox reactors. Cladding prevents radioactive fission fragments from escaping the fuel into the coolant and contaminating it.\n\nPressurized water reactor (PWR) fuel consists of cylindrical rods put into bundles. A uranium oxide ceramic is formed into pellets and inserted into Zircaloy tubes that are bundled together. The Zircaloy tubes are about 1 cm in diameter, and the fuel cladding gap is filled with helium gas to improve the conduction of heat from the fuel to the cladding. There are about 179-264 fuel rods per fuel bundle and about 121 to 193 fuel bundles are loaded into a reactor core. Generally, the fuel bundles consist of fuel rods bundled 14×14 to 17×17. PWR fuel bundles are about 4 meters long. In PWR fuel bundles, control rods are inserted through the top directly into the fuel bundle. The fuel bundles usually are enriched several percent in U. The uranium oxide is dried before inserting into the tubes to try to eliminate moisture in the ceramic fuel that can lead to corrosion and hydrogen embrittlement. The Zircaloy tubes are pressurized with helium to try to minimize pellet-cladding interaction which can lead to fuel rod failure over long periods.\n\nIn boiling water reactors (BWR), the fuel is similar to PWR fuel except that the bundles are \"canned\". That is, there is a thin tube surrounding each bundle. This is primarily done to prevent local density variations from affecting neutronics and thermal hydraulics of the reactor core. In modern BWR fuel bundles, there are either 91, 92, or 96 fuel rods per assembly depending on the manufacturer. A range between 368 assemblies for the smallest and 800 assemblies for the largest U.S. BWR forms the reactor core. Each BWR fuel rod is backfilled with helium to a pressure of about three atmospheres (300 kPa).\n\nCANDU fuel bundles are about a half meter long and 10 cm in diameter. They consist of sintered (UO) pellets in zirconium alloy tubes, welded to zirconium alloy end plates. Each bundle is roughly 20 kg, and a typical core loading is on the order of 4500-6500 bundles, depending on the design. Modern types typically have 37 identical fuel pins radially arranged about the long axis of the bundle, but in the past several different configurations and numbers of pins have been used. The CANFLEX bundle has 43 fuel elements, with two element sizes. It is also about 10 cm (4 inches) in diameter, 0.5 m (20 in) long and weighs about 20 kg (44 lb) and replaces the 37-pin standard bundle. It has been designed specifically to increase fuel performance by utilizing two different pin diameters. Current CANDU designs do not need enriched uranium to achieve criticality (due to their more efficient heavy water moderator), however, some newer concepts call for low enrichment to help reduce the size of the reactors.\n\nVarious other nuclear fuel forms find use in specific applications, but lack the widespread use of those found in BWRs, PWRs, and CANDU power plants. Many of these fuel forms are only found in research reactors, or have military applications.\n\nMagnox reactors are pressurised, carbon dioxide–cooled, graphite-moderated reactors using natural uranium (i.e. unenriched) as fuel and Magnox alloy as fuel cladding. Working pressure varies from 6.9 to 19.35 bar for the steel pressure vessels, and the two reinforced concrete designs operated at 24.8 and 27 bar. Magnox alloy consists mainly of magnesium with small amounts of aluminium and other metals—used in cladding unenriched uranium metal fuel with a non-oxidising covering to contain fission products. Magnox is short for Magnesium non-oxidising. This material has the advantage of a low neutron capture cross-section, but has two major disadvantages:\n\n\nMagnox fuel incorporated cooling fins to provide maximum heat transfer despite low operating temperatures, making it expensive to produce. While the use of uranium metal rather than oxide made reprocessing more straightforward and therefore cheaper, the need to reprocess fuel a short time after removal from the reactor meant that the fission product hazard was severe. Expensive remote handling facilities were required to address this danger.\n\nTristructural-isotropic (TRISO) fuel is a type of micro fuel particle. It consists of a fuel kernel composed of UO (sometimes UC or UCO) in the center, coated with four layers of three isotropic materials. The four layers are a porous buffer layer made of carbon, followed by a dense inner layer of pyrolytic carbon (PyC), followed by a ceramic layer of SiC to retain fission products at elevated temperatures and to give the TRISO particle more structural integrity, followed by a dense outer layer of PyC. TRISO fuel particles are designed not to crack due to the stresses from processes (such as differential thermal expansion or fission gas pressure) at temperatures up to and beyond 1600 °C, and therefore can contain the fuel in the worst of accident scenarios in a properly designed reactor. Two such reactor designs are the pebble-bed reactor (PBR), in which thousands of TRISO fuel particles are dispersed into graphite pebbles, and the prismatic-block gas-cooled reactor (such as the GT-MHR), in which the TRISO fuel particles are fabricated into compacts and placed in a graphite block matrix. Both of these reactor designs are high temperature gas reactors (HTGRs). These are also the basic reactor designs of very-high-temperature reactors (VHTRs), one of the six classes of reactor designs in the Generation IV initiative that is attempting to reach even higher HTGR outlet temperatures.\n\nTRISO fuel particles were originally developed in the United Kingdom as part of the Dragon reactor project. The inclusion of the SiC as diffusion barrier was first suggested by D. T. Livey. The first nuclear reactor to use TRISO fuels was the Dragon reactor and the first powerplant was the THTR-300. Currently, TRISO fuel compacts are being used in the experimental reactors, the HTR-10 in China, and the High-temperature engineering test reactor in Japan. Spherical fuel elements utilizing a TRISO particle with a UO and UC solid solution kernel are being used in the Xe-100 in the United States.\n\nIn QUADRISO particles a burnable neutron poison (europium oxide or erbium oxide or carbide) layer surrounds the fuel kernel of ordinary TRISO particles to better manage the excess of reactivity. If the core is equipped both with TRISO and QUADRISO fuels, at beginning of life neutrons do not reach the fuel of the QUADRISO particles because they are stopped by the burnable poison. During irradiation, the poison depletes and more neutrons are able to stream into the fuel kernel of QUADRISO particles inducing fission reactions. This mechanism compensates for the normal depletion of fissile material during fuel burnup. In the generalized QUADRISO fuel concept the poison can eventually be mixed with the fuel kernel or the outer pyrocarbon. The QUADRISO concept has been conceived at Argonne National Laboratory.\n\nRBMK reactor fuel was used in Soviet-designed and built RBMK-type reactors. This is a low-enriched uranium oxide fuel. The fuel elements in an RBMK are 3 m long each, and two of these sit back-to-back on each fuel channel, pressure tube. Reprocessed uranium from Russian VVER reactor spent fuel is used to fabricate RBMK fuel. Following the Chernobyl accident, the enrichment of fuel was changed from 2.0% to 2.4%, to compensate for control rod modifications and the introduction of additional absorbers.\n\nCerMet fuel consists of ceramic fuel particles (usually uranium oxide) embedded in a metal matrix. It is hypothesized that this type of fuel is what is used in United States Navy reactors. This fuel has high heat transport characteristics and can withstand a large amount of expansion.\n\nPlate-type fuel has fallen out of favor over the years. Plate-type fuel is commonly composed of enriched uranium sandwiched between metal cladding. Plate-type fuel is used in several research reactors where a high neutron flux is desired, for uses such as material irradiation studies or isotope production, without the high temperatures seen in ceramic, cylindrical fuel. It is currently used in the Advanced Test Reactor (ATR) at Idaho National Laboratory, and the nuclear research reactor at the University of Massachusetts Lowell Radiation Laboratory.\n\nSodium-bonded fuel consists of fuel that has liquid sodium in the gap between the fuel slug (or pellet) and the cladding. This fuel type is often used for sodium-cooled liquid metal fast reactors. It has been used in EBR-I, EBR-II, and the FFTF. The fuel slug may be metallic or ceramic. The sodium bonding is used to reduce the temperature of the fuel.\n\nUsed nuclear fuel is a complex mixture of the fission products, uranium, plutonium, and the transplutonium metals. In fuel which has been used at high temperature in power reactors it is common for the fuel to be \"heterogeneous\"; often the fuel will contain nanoparticles of platinum group metals such as palladium. Also the fuel may well have cracked, swollen, and been heated close to its melting point. Despite the fact that the used fuel can be cracked, it is very insoluble in water, and is able to retain the vast majority of the actinides and fission products within the uranium dioxide crystal lattice.\n\nTwo main modes of release exist, the fission products can be vaporised or small particles of the fuel can be dispersed.\n\nPost-Irradiation Examination (PIE) is the study of used nuclear materials such as nuclear fuel. It has several purposes. It is known that by examination of used fuel that the failure modes which occur during normal use (and the manner in which the fuel will behave during an accident) can be studied. In addition information is gained which enables the users of fuel to assure themselves of its quality and it also assists in the development of new fuels. After major accidents the core (or what is left of it) is normally subject to PIE to find out what happened. One site where PIE is done is the ITU which is the EU centre for the study of highly radioactive materials.\n\nMaterials in a high-radiation environment (such as a reactor) can undergo unique behaviors such as swelling and non-thermal creep. If there are nuclear reactions within the material (such as what happens in the fuel), the stoichiometry will also change slowly over time. These behaviors can lead to new material properties, cracking, and fission gas release.\n\nThe thermal conductivity of uranium dioxide is low; it is affected by porosity and burn-up. The burn-up results in fission products being dissolved in the lattice (such as lanthanides), the precipitation of fission products such as palladium, the formation of fission gas bubbles due to fission products such as xenon and krypton and radiation damage of the lattice. The low thermal conductivity can lead to overheating of the center part of the pellets during use. The porosity results in a decrease in both the thermal conductivity of the fuel and the swelling which occurs during use.\n\nAccording to the International Nuclear Safety Center the thermal conductivity of uranium dioxide can be predicted under different conditions by a series of equations.\n\nThe bulk density of the fuel can be related to the thermal conductivity\n\nWhere \"ρ\" is the bulk density of the fuel and \"ρ\" is the theoretical density of the uranium dioxide.\n\nThen the thermal conductivity of the porous phase (\"K\") is related to the conductivity of the perfect phase (\"K\", no porosity) by the following equation. Note that \"s\" is a term for the shape factor of the holes.\n\nRather than measuring the thermal conductivity using the traditional methods such as Lees' disk, the Forbes' method, or Searle's bar, it is common to use Laser Flash Analysis where a small disc of fuel is placed in a furnace. After being heated to the required temperature one side of the disc is illuminated with a laser pulse, the time required for the heat wave to flow through the disc, the density of the disc, and the thickness of the disk can then be used to calculate and determine the thermal conductivity.\n\n\nIf \"t\" is defined as the time required for the non illuminated surface to experience half its final temperature rise then.\n\n\nFor details see \n\nThe terms atomic battery, nuclear battery and radioisotope battery are used interchangeably to describe a device which uses the radioactive decay to generate electricity. These systems use radioisotopes that produce low energy beta particles or sometimes alpha particles of varying energies. Low energy beta particles are needed to prevent the production of high energy penetrating bremsstrahlung radiation that would require heavy shielding. Radioisotopes such as plutonium-238, curium-242, curium-244 and strontium-90 have been used. Tritium, nickel-63, promethium-147, and technetium-99 have been tested.\n\nThere are two main categories of atomic batteries: thermal and non-thermal. The non-thermal atomic batteries, which have many different designs, exploit charged alpha and beta particles. These designs include the direct charging generators, betavoltaics, the optoelectric nuclear battery, and the radioisotope piezoelectric generator. The thermal atomic batteries on the other hand, convert the heat from the radioactive decay to electricity. These designs include thermionic converter, thermophotovoltaic cells, alkali-metal thermal to electric converter, and the most common design, the radioisotope thermoelectric generator.\n\nA \"radioisotope thermoelectric generator\" (RTG) is a simple electrical generator which converts heat into electricity from a radioisotope using an array of thermocouples.\n\n\"Radioisotope heater units\" normally provide about 1 watt of heat each, derived from the decay of a few grams of plutonium-238. This heat is given off continuously for several decades.\n\nTheir function is to provide highly localised heating of sensitive equipment (such as electronics in outer space). The Cassini–Huygens orbiter to Saturn contains 82 of these units (in addition to its 3 main RTGs for power generation). The Huygens probe to Titan contains 35 devices.\n\nFusion fuels include deuterium (H) and tritium (H) as well as helium-3 (He). Many other elements can be fused together, but the larger electrical charge of their nuclei means that much higher temperatures are required. Only the fusion of the lightest elements is seriously considered as a future energy source. Fusion of the lightest atom, H hydrogen, as is done in the Sun and stars, has also not been considered practical on Earth. Although the energy density of fusion fuel is even higher than fission fuel, and fusion reactions sustained for a few minutes have been achieved, utilizing fusion fuel as a net energy source remains only a theoretical possibility.\n\nDeuterium and tritium are both considered first-generation fusion fuels; they are the easiest to fuse, because the electrical charge on their nuclei is the lowest of all elements. The three most commonly cited nuclear reactions that could be used to generate energy are:\n\nSecond-generation fuels require either higher confinement temperatures or longer confinement time than those required of first-generation fusion fuels, but generate fewer neutrons. Neutrons are an unwanted byproduct of fusion reactions in an energy generation context, because they are absorbed by the walls of a fusion chamber, making them radioactive. They cannot be confined by magnetic fields, because they are not electrically charged. This group consists of deuterium and helium-3. The products are all charged particles, but there may be significant side reactions leading to the production of neutrons.\n\nThird-generation fusion fuels produce only charged particles in the primary reactions, and side reactions are relatively unimportant. Since a very small amount of neutrons is produced, there would be little induced radioactivity in the walls of the fusion chamber. This is often seen as the end goal of fusion research. He has the highest Maxwellian reactivity of any 3rd generation fusion fuel. However, there are no significant natural sources of this substance on Earth.\n\nAnother potential aneutronic fusion reaction is the proton-boron reaction:\n\nUnder reasonable assumptions, side reactions will result in about 0.1% of the fusion power being carried by neutrons. With 123 keV, the optimum temperature for this reaction is nearly ten times higher than that for the pure hydrogen reactions, the energy confinement must be 500 times better than that required for the D-T reaction, and the power density will be 2500 times lower than for D-T.\n\n\n\n\n\n\n\n\n\n"}
{"id": "15894895", "url": "https://en.wikipedia.org/wiki?curid=15894895", "title": "Paila", "text": "Paila\n\nA paila is a type of cookware that in several Spanish-speaking South American countries refers to a large shallow metal pan or earthenware bowl which oftentimes is also used as a serving plate for the foods prepared in it.\n\nDishes served in clay pailas are often prepared in the paila itself by way of baking in an oven.\n\nBy extension, the word \"paila\" is also used for the dishes that are eating from it, such as Paila marina and Paila de huevo. An advantage of the clay paila is that clay retains heat well and keeps foods warm.\n\nIn addition to being used to prepare the traditional Ecuadorian pork fritada, the large shallow and heavy copper paila which is used in Ecuador is also used throughout the country and in the department of Nariño in Colombia as an \"ice pail.\" This is performed by placing the bowl on ice and adding ingredients such as fruit, which is stirred to form a variety of ice creams and sorbets.\n\nIn gastronomy, Chilean and Peruvian clay pans are used to cook cornbread and other specialties such as pan or \"pan marina.\"\n\nIn Bolivia, especially in the Cochabamba region, paila is used to cook chicharrón (pork cracklings).\n\nThe \"paila\" also refers to a Latin percussion instrument, also called \"timbal\" or \"timbaleta\". It is composed of two metal cylindrical drums, with a patch on the upper parts. It is usually accompanied by bells and woodblocks. It is frequently used by salsa bands.\n\nIn Chile the word paila refers to something vulgar. It is basically used to poke fun at people with large or very pronounced ears.\n\nIn Colombia the word is used to express that something is bad, ugly or undesirable.\n\n"}
{"id": "4568281", "url": "https://en.wikipedia.org/wiki?curid=4568281", "title": "Pile driver", "text": "Pile driver\n\nA pile driver is a device used to drive piles (poles) into soil to provide foundation support for buildings or other structures. The term is also used in reference to members of the construction crew that work with pile-driving rigs.\n\nOne type of pile driver uses a weight placed between guides so that it can slide vertically. It is placed above a pile (pole). The weight is raised, which may involve the use of hydraulics, steam, diesel, or manual labour. When the weight reaches its highest point it is released, and hits the pile, driving it into the ground.\n\nThere are a number of claims to the invention of the pile driver. A mechanically sound drawing of a pile driver appeared as early as 1475 in Francesco di Giorgio Martini's treatise \"Trattato di Architectura\". Also, several other prominent inventors — James Nasmyth (son of Alexander Nasmyth), who invented a steam-powered pile driver in 1845, watchmaker James Valoué, Count Giovan Battista Gazzola, and Leonardo da Vinci — have all been credited with inventing the device. However, there is evidence that a comparable device was used in the construction of Crannogs at Oakbank and Loch Tay in Scotland as early as 5000 years ago. In 1801 John Rennie came up with a steam piledriver in Britain. Otis Tufts is credited with inventing the steam pile driver in the United States.\n\nAncient pile driving equipment used human or animal labor to lift weights, usually by means of pulleys, then dropping the weight onto the upper end of the pile. Modern piledriving equipment uses various methods to raise the weight and guide the pile.\n\nA modern diesel pile hammer is a large two-stroke diesel engine. The weight is the piston, and the apparatus which connects to the top of the pile is the cylinder. Piledriving is started by raising the weight; usually a cable from the crane holding the pile driver — This draws air into the cylinder. Diesel fuel is injected into the cylinder. The weight is dropped, using a quick-release. The weight of the piston compresses the air/fuel mixture, heating it to the ignition point of diesel fuel. The mixture ignites, transferring the energy of the falling weight to the pile head, and driving the weight up. The rising weight draws in fresh air, and the cycle continues until the fuel is depleted or is halted by the crew.\n\nFrom an army manual on pile driving hammers:\nThe initial start-up of the hammer requires that the piston (ram) be raised to a point where the trip automatically releases the piston, allowing it to fall. As the piston falls, it activates the fuel pump, which discharges a metered amount of fuel into the ball pan of the impact block. The falling piston blocks the exhaust ports, and compression of fuel trapped in the cylinder begins. The compressed air exerts a pre-load force to hold the impact block firmly against the drive cap and pile. At the bottom of the compression stroke, the piston strikes the impact block, atomizing the fuel and starting the pile on its downward movement. In the instant after the piston strikes, the atomized fuel ignites, and the resulting explosion exerts a greater force on the already moving pile, driving it further into the ground. The reaction of the explosion rebounding from the resistance of the pile drives the piston upward. As the piston rises, the exhaust ports open, releasing the exhaust gases to the atmosphere. After the piston stops its upward movement, it again falls by gravity to start another cycle.\n\nThe vertical travel lead, referred to as \"VTL\" system, was first developed and patented by C.W. Bermingham in the 1970s. This lead system was developed in response to the fundamental limitations of either a fixed lead or swinging lead system. The fixed lead system is well suited to level job sites with few obstructions and has the advantage of faster positioning of the lead. The hanging lead is very adaptable to different elevations and batter piles but takes much longer to position. The Vertical Travel Lead was developed to combine the fast and accurate positioning of fixed leads, with the ability to adjust the height of the lead base up or down. The VTL lead is connected to the boom by a sliding connection, which allows the lead to be elevated or lowered below grade. The VTL system has become the industry standard in Canada, US Railway Construction, and many parts of the USA. Vertical Travel Leads come in two main forms: Spud and Box Lead types. Box leads are very common in the Southern United States and Spud Leads are common in the Northern United States, Canada and Europe.\n\nA hydraulic hammer is a modern type of piling hammer used instead of diesel and air hammers for driving steel pipe, precast concrete, and timber piles. Hydraulic hammers are more environmentally acceptable than older, less efficient hammers as they generate less noise and pollutants. In many cases the dominant noise is caused by the impact of the hammer on the pile, or the impacts between components of the hammer, so that the resulting noise level can be similar to diesel hammers.\n\nSpecialty equipment which installs piles using hydraulic rams to press piles into the ground. This system is preferred where vibration is a concern. There are press attachments that can adapt to conventional pile driving rigs to press 2 pairs of sheet piles simultaneously. Other types of press equipment sit atop existing sheet piles and grip previously driven piles. This system allows for greater press-in and extraction force to be used since more reaction force is developed. The reaction-based machines operate at only 69 dB at 23 ft allowing for installation and extraction of piles in close proximity to sensitive areas where traditional methods may threaten the stability of existing structures.\n\nSuch equipment and methods are specified in portions of the internal drainage system in the New Orleans area after Hurricane Katrina, as well as projects where noise, vibration and access are a concern.\n\nVibratory pile hammers contain a system of counter-rotating eccentric weights, powered by hydraulic motors, and designed so that horizontal vibrations cancel out, while vertical vibrations are transmitted into the pile. The pile driving machine positioned over the pile with an excavator or crane, and is fastened to the pile by a clamp and/or bolts. Vibratory hammers can drive or extract a pile. Extraction is commonly used to recover steel \"H\" piles used in temporary foundation shoring. Hydraulic fluid is supplied to the driver by a diesel engine-powered pump mounted in a trailer or van, and connected to the driver head via hoses. When the pile driver is connected to a dragline excavator, it is powered by the excavator's diesel engine. Vibratory pile drivers are often chosen to mitigate noise, as when the construction is near residences or office buildings, or when there is insufficient vertical clearance to permit use of a conventional pile hammer (for example when retrofitting additional piles to a bridge column or abutment footing). Hammers are available with several different vibration rates, ranging from 1200 vibrations per minute to 2400 VPM. The vibration rate chosen is influenced by soil conditions and other factors, such as power requirements and equipment cost.\n\nA piling rig is a construction machine for piling in foundation engineering. It is mainly applied to drill in sandy soil, clay, silty clay, etc. and widely used cast-in-place piles, diaphragm walls, foundation reinforcement and other foundation projects. Its rated power of engine is around 108–450 kW, output torque 60–400 kN•m, maximum pile diameter 1.5–4 m, maximum pile depth 60–90 m. It can meet construction requirement of kinds of foundation engineering projects. It generally applies hydraulic crawler chassis, automatic lift box-type mast, telescopic drill pipe, auto vertical adjustor, depth indicator, etc. Its operation applies hydraulic pilot control, load sensor, thus is easy to control. Main and auxiliary hoisting can meet different requirements in construction sites. The rig can be used on piling construction in dry (short screw), wet soil (rotary bucket) and rock (core drill) by equipping with drilling tool, and can be equipped with long screw drill, diaphragm wall grab, vibratory hammer, etc., to achieve many functions. It is mainly used in foundation engineering of municipal construction, expressway, bridge, industrial and civil buildings, diaphragm wall, water conservancy project, and slope protection.\n\n\nThe underwater sound pressure caused by pile-driving may be deleterious to nearby fish. State and local regulatory agencies manage environment issues associated with pile-driving.\n\n\n"}
{"id": "38886873", "url": "https://en.wikipedia.org/wiki?curid=38886873", "title": "Pin mill", "text": "Pin mill\n\nA pin mill is a mill that comminutes materials by the action of pins that repeatedly move past each other. Much like a kitchen blender, it breaks up substances through repeated impact.\n\nThe mill is a type of vertical shaft impactor mill and consists of two rotating disks with pins embedded on one face. The disks are arrayed parallel to each other so that the pins of one disk face those of the other. The substance to be homogenized is fed into the space between the disks and either one or both disks are rotated at high speeds.\n\nPin mills can be used on both dry substances and liquid suspensions.\n\nPin mills are commonly use in the manufacture of pharmaceuticals, as they can achieve particle sizes as low as a few micrometers. However, heat generated by friction can sometimes be a concern.\n\nA manual form of the mill is commonly used to grind marijuana to a useful consistency.\n"}
{"id": "9394145", "url": "https://en.wikipedia.org/wiki?curid=9394145", "title": "Prescott Wright", "text": "Prescott Wright\n\nPrescott J. Wright (May 8, 1935 in Bronx, New York – December 28,\n2006 in Albuquerque, New Mexico) was best known as the longtime producer and film distributor of the annual touring programs of animated films from around the world known as the International Tournée of Animation. In addition, he was one of the founding directors of the Ottawa International Animated Film Festival in Canada, which began in 1976 and which is now held annually, as well as being instrumental in fostering the art of animated films throughout his working life.\n\nPrescott Wright was raised in the Bronx, was stationed in the Army at Fort Ord, and went to Monterey Peninsula College. The school had a film series run by Phil Chamberlin and Pres became active with it, running the projectors and helping in other ways. His resume says he was president of the film society and jazz club. In 1963, Chamberlin recommended Prescott, who had recently married, for a job at Brandon Films (also known as Western Cinema Guild) in San Francisco as an assistant and, when the manager left, Prescott became head of the San Francisco office. Brandon was a major 16mm film distributor of American and foreign features and shorts.\n\nIn 1969 he moved to Los Angeles to work at the American Film Institute (AFI). They sent him to New York City in January 1970 to market films produced with AFI grants. In a letter to a friend dated December 1970 he wrote, \"By September I had sold about $50 grand worth of films and was applying for my own iron lung. There were some good $$ deals with some major companies if I would stay in New York.\"\n\nIn 1971 he returned to college. He received a BA in communication and the visual arts in 1973 from what was then called California State University, San Francisco and an MA in Film in 1977 from the renamed San Francisco State University. He was a teaching assistant and then a part-time instructor at SF State from 1972–1980. He also taught an extension course in film for the University of California at Berkeley in 1975.\n\nTaking up an offer to take on distribution of the \"Tourney (\"sic\") of Animation\" (as it was initially written), he acquired eight films from KQED-TV and began FilmWright, his small film distribution company, to late 1970.\n\nAbout 1966 several members of ASIFA-Hollywood (Bill Scott, Bill Littlejohn, Les Goldman and June Foray) decided to put together an international animation program to be shown at the Los Angeles County Museum of Art. Since it was almost impossible to see quality animation in the US at that time, Prescott became active with the group when he joined the AFI in 1969 and, having worked previously in film distribution, he was asked to head the project when they decided to show the program in other cities. Under his guidance, the program became known as the \"International Tournee of Animation\" and, in late 1970 or early 1971, he began to book the program at the San Francisco Museum of Modern Art, university campuses and other cultural institutions around this country.\n\nFilmWright offered animators an interesting and generous contract when Wright produced the Tournee. As producer, the company received 50% of the gross while the remaining 50% was split among the artists. About half of the money going to the animators was split evenly between each filmmaker and the remaining amount was split based the length of each short film. This meant that a very short film got slightly less than one which was a minute or two longer.\n\nIn the 1970s Gary Meyer, who ran the U.C. Theater in Berkeley, California and became a co-founder Landmark Theatres, convinced Prescott to expand his distribution of the Tournee of Animation to theaters:\n\nAuthor and animation historian Jerry Beck noted:\n\nBetween 1975 and 1985, Prescott was on the board of directors of ASIFA, the International Animated Film Association, which had over 30 chapters around the world. Until the Iron Curtain collapsed, ASIFA helped bridge the gap between East and West by helping animators from Eastern Europe attend festivals in the West, to visit studios in the Europe and North America, and to show their films in-person. Pres, David Ehrlich, Howard Beckerman, Charles Samu, John Halas, John Hubley and others worked hard to further international relations and to arrange for these screenings in cities with ASIFA chapters.\n\nHe was also a founder of ASIFA-San Francisco in 1975. Thanks to his guidance, the chapter grew over the years, and it is known for presenting great programs and having a very informative newsletter.\n\nPrescott served as an advisor to major animation festivals around the world, and regularly attended the ASIFA-sponsored festivals in Annecy, France, and Zagreb, (then in Yugoslavia).\n\nHe was a founder and the first International Director of the Ottawa International Animation Festival in 1976 and served in that capacity in 1978, 1980, 1982, and also in 1992. In 2004, the Ottawa festival made him an Honorary President of the festival, but he was unable to attend the event due to his health problems. ASIFA-Hollywood has also honored him with the \"June Foray Award\" for benevolent and significant contributions to the art of animation at the “Annie Awards”.\n\nPres served a year as director of the Denver International Film Festival (1980/81) and, returning to San Francisco, became the producer of \"The Animators\", a series of TV programs made in 1982 for KQED-TV in San Francisco which featured Bay Area talent such as Jeff Hale, Bud Luckey, Rudy Zamora, Sally Cruickshank and Marcy Page.\n\nOrganized by ASIFA-Hollywood during the Los Angeles Summer Olympic Games of 1984, Prescott was involved, as Director, with the creation and management of the \"Olympiad of Animation\", which was shown in Los Angeles at the Academy of Motion Picture Arts and Sciences' Samuel Goldwyn Theater. For one program, the organizers polled one hundred animation professionals around the world to determine which animated films were regarded as the greatest of all time; over 30 of the films were shown. He was proud of the fact that he brought the project in under budget.\n\nHe also was involved with many other cultural events over the years. These included serving as a member of the Film Arts Foundation board of directors in San Francisco from the late 1960s (and as President of the Board from 1978 to 1979), and as Treasurer of the Society for Animation Studies in the late 1980s and early 1990s (with Harvey Deneroff, the founder and first president of that association).\n\nIn 1990, Prescott worked for Disney's Feature Division as a \"Creative Staffing Specialist\". He spent 4- months traveling to international animation festivals as a spotter and recruiter of animation talent. He planned to travel to festivals in Hiroshima, Annecy, Zagreb and also in Russia.\n\nAfter Disney, he worked in both the Philippines and Southern India as an instructor and festival director for emerging animation studios. While he was in India he programmed and managed the first \"Week with the Masters\" for Toonz India, an emerging animation studio at Trivandrum in October 1999.\n\n\n"}
{"id": "5396663", "url": "https://en.wikipedia.org/wiki?curid=5396663", "title": "Pruning shears", "text": "Pruning shears\n\nPruning shears, also called hand pruners (in American English), or secateurs, are a type of scissors for use on plants. They are strong enough to prune hard branches of trees and shrubs, sometimes up to two centimetres thick. They are used in gardening, arboriculture, farming, flower arranging, and nature conservation, where fine-scale habitat management is required. \n\nLoppers are a larger, two-handed, long-handled version for branches thicker than pruning shears can cut.\n\nCutting plants as part of gardening dates to antiquity in both Europe and East Asian topiary, with specialized scissors used for Chinese penjing and its offshoots – Japanese bonsai and Vietnamese Hòn Non-Bộ – for over a thousand years.\n\nIn modern Europe, scissors only used for gardening work have existed since 1819, when the French aristocrat Antoine Francois Bertrand de Molleville was listed in \"Bon Jardinier\", as the first inventor of secateurs. During the late 1890s, secateurs were sold all over Europe and the US. Today secateurs are widely used by professional and semiprofessional gardeners, vintners and fruit farmers.\n\nThe world's first anvil pruners were developed and produced in 1923 by Walther Schröder in Kiel, Germany. The pruners were given the product name \"Original LÖWE\" and were distributed internationally as far back as 1925. Other companies are producing anvil pruners, include Bahco, Edma, Felco, Fiskars Gardena and Wolf Garten.\n\nThere are three different blade designs for pruning shears: \"anvil\", \"bypass\" and \"parrot-beak\".\nAnvil pruners have only one blade, which closes onto a flat surface; unlike bypass blades it can be sharpened from both sides and remains reliable when slightly blunt. Anvil pruners are useful for cutting thick branches; one can bite into the stem from one direction, swing the handle around and bite further through narrowed wood from another direction. The anvil is made of a material softer than the blade, so that the blade is not damaged when it meets the anvil. Suitable materials for the anvil are plastic, aluminum, zinc, brass, or bronze alloys. The blades are made from hardened carbon or chromium steels. The hardness of the blades is generally between 54 and 58 HRC. On an anvil pruner, proper cutting is assured even if the blade swerves slightly to the left or right during cutting. As long as the blade meets the anvil at the end of the cut and fits tightly against it, the material is separated. For this reason, the blades of anvil pruners can be ground thinner than those on bypass pruners. The LÖWE principle – a drawing cut made against a fixed support – combines a drawing cut with a pushing cut. This is possible because the blade lever and base lever are connected by an eccentric bearing. When the pruners are open, the blade is longer than the anvil thanks to the eccentric bearing. When the pruners close, the blade draws back slightly while it pushes through the material. This reduces the cutting force needed to make a cut still further.\n\nBypass pruners usually work exactly like a pair of scissors, with two blades \"passing by\" each other to make the cut. At least one of the blades will be curved: a convex upper blade with either a concave or straight lower one. Some bypass designs have only one blade, the lower jaw being broad (like an anvil) but passing the upper jaw. The ratchet pruner, which can handle stems thicker than two centimeters, fits in this category. \n\nParrot-beak pruners consist of two concave passing blades, which trap the stem between them to make the cut. These are suitable only for narrower stems.\n\nSecateurs have short handles and are operated with one hand. A spring between the handles causes the jaws to open again after closing. When not in use, the jaws may be held closed by a safety catch or by a loop holding the handles together. Some types are designed for right-handed or left-handed use only, and some incorporate a rotating handle to reduce friction and minimize hand stress during repetitive use. There are also longer versions called telescopic pruners, which are adjustable for long-reach and operate by means of a rod system inside of a telescoping pole between the handles and the blades. An early version of these was known as an averruncator.\n\nThere are two different types of blades for pruning shears: Stainless steel and carbon steel.\n\nIn addition there are pruning shears that have titanium coating.\n\nStainless steel have a high corrosion resistance, due to the protective chromium oxide layer that covers the steel surface after heat treatment. On the other hand, they are not durable for long.\n\nCarbon steel has a higher carbon content, which gives the steel a lower melting point, more malleability and durability, and better heat distribution. The disadvantages are the quick corrosion and staining.\n\nTitanium coated blades offer a balance between durability, sharpness and anti-corrosion.\nTitanium is stronger, has higher corrosion resistance, and has about half the density (weight) of steel. \nThe titanium coating helps strengthen the blade and prevent corrosion, and after being sharpened a few times it will expose the steel edge underneath, giving you the best edge with higher strength and resistance to corrosion over the length of the blade. \nThe titanium coating is recognized by the gold colored blade as opposed to the typical silver colored steel blade \n\n"}
{"id": "11958900", "url": "https://en.wikipedia.org/wiki?curid=11958900", "title": "RechargeIT", "text": "RechargeIT\n\nRechargeIT is one of five initiatives within Google.org, the charitable arm of Google, created with the aim to reduce CO emissions, cut oil use, and stabilize the electrical grid by accelerating the adoption of plug-in electric vehicles.\n\nThe RechargeIT initiative was unveiled in June 2007. As part of the program Google.org awarded US$1 million in grants and announced plans for a US$10 million request for proposals to fund development, adoption and commercialization of plug-in hybrids, fully electric cars and related vehicle-to-grid (V2G) technology. As part of the program Google established a partnership with Pacific Gas and Electric Co. to develop software for energy management.\nTogether with the announcement of the initiative, Google also announced that it had switched on the solar panel installation at its Mountain View, California headquarters in order to help the company reduce its environmental footprint and also to power its plug-ins with clean solar electricity. At 1.6 megawatts the project became the largest solar installation at that time on any corporate campus in the U.S. and one of the largest on any corporate site in the world. \nThe actual rollout of the initiative took place in January 2008.\n\nBy early 2010 Google's Mountain View campus had 100 available charging stations for its share-use fleet of converted plug-in hybrids available to its employees through a free carsharing program and for those employees who drive to work in their Tesla Roadster (2008) electric cars. Solar panels are used to generate the electricity, and this pilot program is being monitored on a daily basis and performance results are published on RechargeIT's website.\n\nIn addition to the data collected for two years when the converted plug-ins were driven by Google employees, RechargeIT set up a controlled test using three conventional gasoline vehicles, two regular hybrids and two plug-in converted Ford Escape Hybrid and Toyota Prius. The results of the seven-week driving experiment for the converted Prius plug-in showed an average fuel economy of across all trips, and for city trips, the maximum reached for any of the driving conditions tested. A summary of the results of the seven-week driving experiment are the following\n\nIn order to reduce the carbon footprint of its employees' commute and based on the results of the RechargeIT pilot, the company expanded its corporate carsharing program to create Google GFleet and also introduced shuttle buses powered with biodiesel. In addition, more charging stations were deployed at the Googleplex for employees owning plug-in electric vehicles.\n\nThe initial GFleet was made of the converted plug-in hybrids from the RechargeIT initiative, and by mid-2011 Nissan Leafs and Chevrolet Volts were added, expanding the carsharing corporate fleet to more than 30 plug-in electric vehicles. In December 2011, the first production Ford Focus Electric was delivered to Google and incorporated into the GFleet. By early 2012, Honda Fit EVs and Mitsubishi i-MiEVs have also been added to the GFleet. The Fit EV was incorporated as part of Honda's field testing program of its upcoming electric car. Through the partnership Google will analyze the vehicle environmental performance including CO reduction, energy consumption and overall energy cost.\n\nEmployees who use the biodiesel shuttle system to commute to work at Mountain View, have the GFleet vehicles available for their errands, off-site meetings, and emergencies. Employees can also use GBikes, Google's on-campus bike fleet. As of June 2011, a total of 71 Level 2 chargers were added to the existing 150 Level 1 chargers, bringing the Googleplex total capacity to more than 200 chargers, and another 250 new ones are scheduled to be installed. Google's goal is to electrify 5 percent of the parking spaces—all over campus, free of charge to its employees.\n\nDaily, up to a third of Bay Area employees take the shuttle to work. The corporate coach fleet exceeds the United States Environmental Protection Agency's 2010 bus emission standards. The buses run on 5% biodiesel and are fitted with filtration systems that eliminate many harmful emissions, including nitrogen oxide. Google is testing solar panels on some to power air circulation, so that shuttles can turn off their engines while they wait for passengers, thus reducing fuel use and emissions. As of mid-2011, Google estimated that its Gfleet and biodiesel shuttle system resulted in net annual savings of more than 5,400 tonnes of , the equivalent of taking over 2,000 cars per day off the road, or avoiding 14 million vehicle miles every year.\n\n\n"}
{"id": "34679677", "url": "https://en.wikipedia.org/wiki?curid=34679677", "title": "Rockbreaker", "text": "Rockbreaker\n\nA rockbreaker is a machine designed to manipulate large rocks, including reducing large rocks into smaller rocks. They are typically used in the mining industry to remove oversize rocks that are too large or too hard to be reduced in size by a crusher. Rockbreakers consist of two major components, a hydraulic hammer (used to break rocks) and a boom (the arm). There are two major types of rock breakers, mobile and stationary - typically placed on a pedestal or slew frame.\n\nIn 2008, researchers from the CSIRO implemented remote-operation functionality for a Transmin rockbreaker located at Rio Tinto's West Angelas iron ore mine from Perth, over 1000 km away.\n\nIn 2011, Transmin developed the first commercially available automation system for pedestal rockbreakers. The system was first installed at Newcrest's Ridgeway Deeps gold mine providing collision avoidance and remote operation functionality.\n\n"}
{"id": "3813953", "url": "https://en.wikipedia.org/wiki?curid=3813953", "title": "Rustproofing", "text": "Rustproofing\n\nRustproofing is the prevention or delay of rusting of iron and steel objects, or the permanent protection against corrosion. Typically, the protection is achieved by a process of surface finishing or treatment. Depending on mechanical wear or environmental conditions, the degradation may not be stopped completely, unless the process is periodically repeated. The term is particularly used in the automobile industry.\n\nIn the factory, car bodies are protected with special chemical formulations, typically phosphate conversion coatings. Some firms galvanize part or all of their car bodies before the primer coat of paint is applied. If a car is body-on-frame, then the frame (chassis) and its must also be rustproofed. Paint is the final part of the rustproofing barrier between the body shell (apart from on the underside) and the atmosphere. On the underside, an underseal rubberized or PVC-based coating is sprayed on. These products will be breached eventually and can lead to unseen corrosion that spreads underneath the underseal. Old 1960s and 1970s rubberized underseal can become brittle on older cars and is particularly liable to this.\n\nAftermarket kits are available to apply rustproofing compounds both to external surfaces and inside enclosed sections, for example sills/rocker panels (see monocoque), through either existing or specially drilled holes. The compounds are usually wax-based and can be applied by aerosol can, brush, low pressure pump up spray (which has a reputation for mixed results), and Shutz compressor fed spray gun which gives best results but is the most messy - both the sprayers come with extension pipes for enclosed body sections. Shutz spray guns are much simpler and cheaper than top coat spray guns.\n\nAn alternative for sills/rocker panels, is to block drain holes and simply fill them up with wax and then drain most of it out (the excess can be stored and reused), leaving a complete coating inside. Anti-rust wax like phosphoric acid based rust killers/neutralizers can also be painted on already rusted areas. Loose or thick rust must be removed before anti-rust wax like Waxoyl or a similar product is used. \n\nStructural rust (affecting structural components which must withstand considerable forces) should be cut back to good metal and new metal welded in, or the affected part should be completely replaced. Wax may not penetrate spot-welded seams or thick rust effectively. A penetrating anti-rust product like WD-40 followed by anti-rust wax can be more effective. Application is easier in hot weather rather than cold because even when pre-heated, the products are viscous and don't flow and penetrate well on cold metal.\n\nAftermarket \"underseals\" can also be applied. They are particularly useful in high-impact areas like wheel arches. There are two types - drying and non-drying. The hardening and drying products are also known as \"Shutz\" and \"Anti Stone Chip\" with similar potential problems to the original factory underseals. These are available in black, white, grey and red colors and can be overpainted. These are best used for the area below the bumpers on cars that have painted metal body work in that location, rather than modern plastic deep bumpers. The bitumen based products do not dry and harden, so they cannot become brittle, like the confusingly named \"Underbody Seal with added Waxoyl\" made by Hammerite, which can be supplied in a Shutz type cartridge labelled \"Shutz\" for use with a Shutz compressor fed gun. Mercedes bodyshops use a similar product supplied by Mercedes-Benz. There are many manufacturers of similar products at varying prices, these are regularly group tested and reviewed in the classic car magazine press.\n\nThe non drying types contain anti-rust chemicals similar to those in anti-rust waxes. Petroleum-based rust-inhibitors provide several benefits, including the ability to creep over metal, covering missed areas. Additionally, a petroleum, solvent-free rust inhibitor remains on the metal surface, sealing it from rust-accelerating water and oxygen. Other benefits of petroleum-based rust protection include the self-healing properties that come naturally to oils, which helps undercoatings to resist abrasion caused by road sand and other debris. The disadvantage of using a petroleum-based coating is the film left over on surfaces, rendering these products too messy for top side exterior application, and unsafe in areas where it can be slipped on. They also cannot be painted.\n\nThere are electronic \"rustproofing\" technologies claimed to prevent corrosion by \"pushing\" electrons into the car body, to limit the combination of oxygen and iron to form rust. The loss of electrons in paint is also claimed to be the cause of “paint oxidisation” and the electronic system is also supposed to protect the paint. However, there is no peer reviewed scientific testing and validation supporting the use of these devices and corrosion control professionals find they do not work. \n\nThe rate at which vehicles corrode is dependent upon:\n\n\nStainless steel, also known as \"inox steel\" does not stain, corrode, or rust as easily as ordinary steel. Pierre Berthier, a Frenchman, was the first to notice the rust-resistant properties of mixing chromium with alloys in 1821, which led to new metal treating and metallurgy processes and eventually the creation of usable stainless steel. DeLorean cars had innovative fiberglass body structure with a steel backbone chassis, along with external brushed stainless-steel body panels.\n\nSome cars have been made from aluminum, which may be more corrosion resistant than steel when exposed to water, but not salt or certain other chemicals.\n"}
{"id": "450268", "url": "https://en.wikipedia.org/wiki?curid=450268", "title": "Stationary engine", "text": "Stationary engine\n\nA stationary engine is an engine whose framework does not move. They are used to drive immobile equipment, such as pumps, generators, mills or factory machinery. The term usually refers to large immobile reciprocating engines, principally stationary steam engines and, to some extent, stationary internal combustion engines. Other large immobile power sources, such as steam turbines, gas turbines, and large electric motors, are categorized separately. \n\nStationary engines were once widespread in the era when each factory or mill generated its own power, and power transmission was mechanical (via line shafts, belts, gear trains, and clutches). Applications for stationary engines have declined since electrification has become widespread; most industrial uses today draw electricity from an electrical grid and distribute it to various individual electric motors instead. \n\nEngines that operate in one place, but can be moved to another place for later operation, are called portable engines. Although stationary engines and portable engines are both \"stationary\" (not moving) while running, preferred usage (for clarity's sake) reserves the term \"stationary engine\" to the permanently immobile type, and \"portable engine\" to the mobile type. \n\n\nBefore mains electricity and the formation of nationwide power grids, stationary engines were widely used for small-scale electricity generation. Whilst large power stations in cities used steam turbines or high-speed reciprocating steam engines, in rural areas petrol/gasoline, paraffin/kerosene or fuel oil powered internal combustion engines were cheaper to buy, install and operate, since they could be started and stopped quickly to meet demand, left running unattended for long periods of time and did not require a large dedicated engineering staff to operate and maintain. Due to their simplicity and economy, hot bulb engines were popular for high-power applications until the diesel engine took their place from the 1920s. Smaller units were generally powered by spark-ignition engines, which were cheaper to buy and required less space to install. \n\nMost engines of the late-19th and early-20th centuries ran at speeds too low to drive a dynamo or alternator directly. As with other equipment, the generator was driven off the engine's flywheel by a broad flat belt. The pulley on the generator was much smaller than the flywheel, providing the required 'gearing up' effect. Later spark-ignition engines developed from the 1920s could be directly coupled. \n\nUp to the 1930s most rural houses in Europe and North America needed their own generating equipment if electric light was fitted. Engines would often be installed in a dedicated 'engine house', which was usually an outbuilding separate from the main house to reduce the interference from the engine noise. The engine house would contain the engine, the generator, the necessary switchgear and fuses, as well as the engine's fuel supply and usually a dedicated workshop space with equipment to service and repair the engine. Wealthy households could afford to employ a dedicated engineer to maintain the equipment, but as the demand for electricity spread to smaller homes, manufacturers produced engines that required less maintenance and that did not need specialist training to operate. \n\nSuch generator sets were also used in industrial complexes and public buildings- anywhere where electricity was required but mains electricity was not available. \n\nMost countries in the Western world completed large-scale rural electrification in the years following World War II, making individual generating plants obsolete for front-line use. However, even in countries with a reliable mains supply, many buildings are still fitted with modern diesel generators for emergency use, such as hospitals and pumping stations. This network of generators often forms a crucial part of the national electricity system's strategy for coping with periods of high demand.\n\nThe development of water supply and sewage removal systems required the provision of many pumping stations. In these, some form of stationary engine (steam-powered for earlier installations) is used to drive one or more pumps, although electric motors are more conventionally used nowadays.\n\nFor canals, a distinct area of application concerned the powering of boat lifts and inclined planes. Where possible these would be arranged to utilise water and gravity in a balanced system, but in some cases additional power input was required from a stationary engine for the system to work. The vast majority of these were constructed (and in many cases, demolished again) before steam engines were supplanted by internal combustion alternatives.\n\nIndustrial railways in quarries and mines made use of cable railways based on the inclined plane idea, and certain early passenger railways in the UK were planned with lengths of cable-haulage to overcome severe gradients.\n\nFor the first proper railway, the Liverpool and Manchester of 1830, it was not clear whether locomotive traction would work, and the railway was designed with steep 1 in 100 gradients concentrated on either side of Rainhill, just in case. Had cable haulage been necessary, then inconvenient and time-consuming shunting would obviously have been required to attach and detach the cables. The Rainhill gradients proved not to be a problem, and in the event, locomotive traction was determined to be a new technology with great potential for further development.\n\nThe steeper 1 in 50 grades from Liverpool down to the docks were operated by cable traction for several decades until locomotives improved. Cable haulage continued to be used where gradients were even steeper.\n\nCable haulage did prove viable where the gradients were exceptionally steep, such as the 1 in 8 gradients of the Cromford and High Peak Railway opened in 1830. Cable railways generally have two tracks with loaded wagons on one track partially balanced by empty wagons on the other, to minimise fuel costs for the stationary engine. Various kinds of rack railway were developed to overcome the lack of friction of conventional locomotives on steep gradients.\n\nThese early installations of stationary engines would all have been steam-powered initially.\n\n\nMany steam rallies, like the Great Dorset Steam Fair, include an exhibit section for internal combustion stationary engines for which purpose the definition is usually extended to include any engine which was not intended primarily for the propulsion of a vehicle. Thus many are in fact portable engines, either from new or having been converted by mounting on a wheeled trolley for ease of transport and may also include such things as marine or airborne auxiliary power units and engines removed from equipment such as motor mowers. These engines have been restored by private individuals and often are exhibited in operation, powering water pumps, electric generators, hand tools, and the like.\n\nIn the UK there are few museums where visitors can see stationary engines in operation. Many museums have one or more engines but only a few specialise in the internal combustion stationary engines. Among these are the Internal Fire - Museum of Power, in Wales, and the Anson Engine Museum in Cheshire. The Amberley Working Museum in West Sussex also has a number of engines, as does Kew Bridge Steam Museum in London.\n\n\n"}
{"id": "56283411", "url": "https://en.wikipedia.org/wiki?curid=56283411", "title": "Telcom (Ireland)", "text": "Telcom (Ireland)\n\nTelcom is an Irish telecommunications company founded in 1999 and operating in the business-to-business market. They offer a range of communications services, are a ComReg registered carrier for voice and data and operate as an independent Internet Service Provider. Telcom is a registered member of the Internet Neutral Exchange (INEX).\n\nTelcom was founded in 1999 in Dublin by Liam Tully, initially providing phone systems for offices, then expanding to include data and internet services.\n\nIn 2015 Telcom invested €1 million into new network infrastructure, creating a 10Gb core network. This allowed them to expand their business fibre broadband service, offering speeds of up to 1 Gb/s to customers with zero contention rates. This upgrade was completed as a partnership between Telcom and Agile Networks, a fellow Dublin-based company.\n\nTelcom provides various services to businesses, including:\nClients using Telcom's services include Fyffes, Hertz, Maxol, Philips, Savills, Scotiabank, DZ Bank and the Higher Education Authority.\n\n\n"}
{"id": "786716", "url": "https://en.wikipedia.org/wiki?curid=786716", "title": "The Dickson Experimental Sound Film", "text": "The Dickson Experimental Sound Film\n\nThe Dickson Experimental Sound Film is a film made by William Dickson in late 1894 or early 1895. It is the first known film with live-recorded sound and appears to be the first motion picture made for the Kinetophone, the proto-sound-film system developed by Dickson and Thomas Edison. (The Kinetophone, consisting of a Kinetoscope accompanied by a cylinder-playing phonograph, was not a true sound-film system, for there was no attempt to synchronize picture and sound throughout playback.) The film was produced at the \"Black Maria\", Edison's New Jersey film studio. There is no evidence that it was ever exhibited in its original format. Newly digitized and restored, it is the only surviving Kinetophone film with live-recorded sound.\n\nThe film features Dickson playing a violin into a recording horn for an off-camera wax cylinder. The melody is from a barcarolle, \"Song of the Cabin Boy\", from \"Les Cloches de Corneville\" (literally \"The Bells of Corneville\"; presented in English-speaking countries as \"The Chimes of Normandy\"), a light opera composed by Robert Planquette in 1877. In front of Dickson, two men dance to the music. In the final seconds, a fourth man briefly crosses from left to right behind the horn. The running time of the restored film is seventeen seconds; the accompanying cylinder contains approximately two minutes of sound, including twenty-three seconds of violin music, encompassing the film's soundtrack. After its restoration in 2000, \"The Dickson Experimental Sound Film\" was selected for inclusion in the United States National Film Registry.\n\nA soundless 35mm nitrate print of the movie, described as precisely forty feet long, was acquired by the Museum of Modern Art and transferred to safety film in 1942. Thomas A. Edison, Incorporated donated the Edison Laboratory to the U.S. National Park Service in 1956. The soundtrack was inventoried at the Edison National Historic Site in the early 1960s, when a wax cylinder in a metal canister labeled \"Dickson—Violin by W.K.L. Dixon with Kineto\" was found in the music room of the Edison laboratory. In 1964, researchers opened the canister only to find that the cylinder was broken in two; that year, as well, all nitrate film materials remaining at the facility were removed to the Library of Congress for conservation. Among the filmstrips was a print that the Library of Congress catalogued as \"Dickson Violin\". According to Patrick Loughney, the library's film and TV curator, this print is \"thirty-nine feet and fourteen frames [two frames short of 40 feet].\"\n\nThe connection between film and cylinder was not made until 1998, when Loughney and Edison NHS sound recordings curator Jerry Fabris arranged for the cylinder to be repaired and its contents recovered at the Rodgers and Hammerstein Archive of Recorded Sound in New York. A new reel-to-reel master was created, allowing for fidelity reproduction onto digital audio tape. As the library was not equipped to synchronize the recovered soundtrack with the film element, producer and restoration specialist Rick Schmidlin suggested that award-winning film editor Walter Murch be enlisted on the project (the two had worked together on the 1998 restoration of Orson Welles's \"Touch of Evil\"). Murch was given the short piece of film and the two minutes of sound recovered from the cylinder to work with. By digitally converting the film and editing the media together on an Avid system, Murch synchronized the visual and audio elements.\n\nOn the cylinder, before the camera starts rolling, a man's voice can be heard to say, \"Are the rest of you ready? Go ahead!\" This extra sound is included on the version of the film that was distributed in the early 2000s. However, since filming had not yet begun when the words were uttered, this cannot be claimed as the first incidence of the spoken word on film.\n\nOne question that remains unanswered is how the eventual running time of just over 17 seconds was arrived at. Per the curatorial reports, the 35-mm prints have a standard 16 frames per foot of film— plus 14 frames thus equals a total of 638 frames. Murch describes the film as having been shot at 40 frames per second (fps); Loughney describes it as 46 fps. At 40 fps, 638 frames would run 15.95 seconds, which should be the maximum length of the restored film if all other reports are correct; as Loughney notes, at 46 fps, the film would last 13.86 seconds. If the latter figure is correct, as many as 9 seconds of film are missing from both extant prints if the entire violin performance was filmed. On the basis of his own tests of eighteen Kinetoscope films, scholar Gordon Hendricks argued that no Kinetoscope films were shot at 46 fps, making the speed of 40 fps reported by Murch more likely. Yet there is still a difference of more than a second between the maximum potential running time at that speed and the actual duration of the film as digitized by Murch. That 17-second running time works out to an average camera speed of approximately 37.5 fps, a significant difference from Murch's report.\n\nIn his book \"The Celluloid Closet\" (1981), film historian Vito Russo discusses the film, claiming, without attribution, that it was titled \"The Gay Brothers\". Russo's unsupported naming of the film has been adopted widely online and in at least three books, and his assertions that the film's content is homosexual are frequently echoed. In addition to there being no evidence for the title Russo gives the film, in fact the word \"gay\" was not generally used as a synonym for \"homosexual\" at the time the film was made. There is also no evidence that Dickson intended to present the men—presumably employees of the Edison studio—as a romantic couple. Given the lyrics of the song Dickson plays, which describes life at sea without women, it is more plausible that he intended a joke about the virtually all-male environment of the Black Maria. It was also quite common in the 19th century for men to dance with men without homosexual overtones being perceived; all-male \"stag dances,\" for instance, were a standard part of life in the 19th century U.S. Army and were even part of the curriculum at West Point. Still, this may be seen as one of the earliest examples of same-sex imagery in the cinema. An excerpt of the film is included in the documentary based on Russo's book, also titled \"The Celluloid Closet\" (1995).\n\n\n\n\n"}
{"id": "56886798", "url": "https://en.wikipedia.org/wiki?curid=56886798", "title": "The Future of Work and Death", "text": "The Future of Work and Death\n\nThe Future of Work and Death is a 2016 documentary by Sean Blacknell and Wayne Walsh about the growth of exponential technology.\n\nThe film showed at several film festivals including Raindance Film Festival, International Film Festival Rotterdam, Academia Film Olomouc and CPH:DOX.\n\nIn May 2017 it received an official screening at the European Commission. It was distributed by First Run Features and Journeyman Pictures and was released on iTunes and On-demand on May 9, 2017.\n\nThe documentary explores how imminent technologies could begin to significantly redefine our lives. The first part of the film focuses on how advanced automation, AI and technological singularity could be achievable in the next 30 years and how job obsolescence could consequently occur. The film then examines the possibility of extreme longevity and mind uploading and what the socio-political repercussions of these innovations could be.\n\n\n"}
{"id": "10046948", "url": "https://en.wikipedia.org/wiki?curid=10046948", "title": "Tiffany Shlain", "text": "Tiffany Shlain\n\nTiffany Shlain (born April 8, 1970) is an American filmmaker, author, and public speaker. Regarded as an internet pioneer, Shlain is the founder of the Webby Awards and the co-founder of the International Academy of Digital Arts and Sciences.\n\nShlain was raised in Mill Valley, California, the daughter of Leonard Shlain, a surgeon, author, and inventor, and Carol Lewis Jaffe, a psychologist. In high school, intrigued by technology and communications, Shlain wrote a proposal called \"Uniting Nations in Telecommunications & Software (UNITAS),\" which envisioned students in enemy countries communicating over personal computers and via modems. From this proposal, she was invited to be a student ambassador through the People to People program, and traveled to the Soviet Union in 1988.\n\nWhile a student at UC Berkeley, Shlain produced and directed \"Hunter & Pandora\", an experimental film which won the university's Eisner Award, the highest award in art. In 1992, she earned a BA in Interdisciplinary Studies, and was selected as a valedictory speaker for her graduating class.\n\nShlain additionally studied organizational change at the Harvard Business School Executive Education program and film production at New York University's Sight & Sound program. She was a 2006-2007 Henry Crown Fellow of the Aspen Institute.\n\nIn 1996, Shlain founded the Webby Awards, an annual event which the \"New York Times\" described as the \"Oscars of the Web.\" In 1998, she co-founded The International Academy of Digital Arts & Sciences. The Webbys had hosts that included Alan Cumming, and appearances by Al Gore, Prince, and Thomas Friedman. Shlain appeared on \"Good Morning America\" as the program's on-air internet expert from 2000 – 2003.\n\nIn 2002, Shlain combined her background in technology and the web with her earlier work as a filmmaker, and directed, produced and co-wrote \"Life, Liberty, and the Pursuit of Happiness\", a documentary about reproductive rights in America. The film premiered at the 2003 Sundance Film Festival and was used nationally by Planned Parenthood to mark the 30 year anniversary of Roe v. Wade.\n\nIn 2005, Shlain decided to pursue filmmaking full-time; she subsequently sold the Webby Awards and founded the San Francisco film studio, the Moxie Institute. Shlain's next documentary, \"The Tribe\", co-written with her husband, Ken Goldberg, explored American Jewish identity through the history of the Barbie Doll. \"The Tribe\", which also premiered at the Sundance Film Festival, was the first documentary short to become #1 on iTunes.\n\nIn 2011, her first feature documentary, \",\" premiered at the Sundance Film Festival. Examining personal connections in relation to global conditions - and the potential of what can happen with so many people online — the film ran in theaters and on television, and was subsequently released on digital platforms. The winner of a Tribeca Film Festival's Disruptive Innovation Award, in addition to other significant awards, \"Connected\" was selected by the United States Department of State and the University of Southern California for the 2012 American Filmmaker Showcase. In 2013, the Margaret Herrick Library at the Academy of Motion Picture Arts and Sciences acquired the film's script for their permanent collection.\n\nIn 2011, she introduced the concept of \"Cloud Filmmaking\" with a series of shorts produced through cloud-based collaborative filmmaking. The first film in the series, \"A Declaration of Interdependence\", was released Sept 2011; the second film, \"Engage\", debuted in early 2012. Later that same year, both a 10-minute film and a best-selling TED Book, called \"Brain Power: From Neurons to Networks\" were released. Exploring new research on how to best grow children's brains and the global brain of the internet, \"Brain Power\" premiered in November 2012 at The California Academy of Sciences. It was selected by the US State Department as a part of the 2013 American Film Showcase and was screened at embassies in the Middle East in November 2013. Shlain discussed cloud filmmaking as the keynote speaker at the Tribeca Film Festival's 2013 Interactive Day where she delivered her \"Cloud Filmmaking Manifesto.\"\n\nIn 2013, Shlain co-founded the nonprofit Let it Ripple: Mobile Films for Global Change, and continued making cloud films. The next film in the series was \"The Science of Character\". To premiere the film, Shlain and her co-workers founded Character Day, where schools and organizations around the world would all premiere the film and discuss ideas around character development on the same day in a simultaneous online video conversation. There were over 1500 events in 31 countries. For the second annual Character Day, they premiered \"The Adaptable Mind\", which explores skills needed in the 21st century, and \"The Making of a Mensch\", about the science of character through the Jewish teachings of Mussar, interpreted through a modern-day lens. This second annual Character Day had over 6700 events in 41 countries. The US State Department selected \"The Adaptable Mind\" to be part of their 2016-2017 American Film Showcase. The 4th annual Character Day happened on Sept 13th and had over 133,000 events in all 50 states and over 100 countries. The 5th Annual Character Day on September 26, 2018 had over 210,000 events worldwide.\n\nShlain created two seasons of the AOL original series \"The Future Starts Here,\" which includes episodes entitled \"Technology Shabbats\", \"Motherhood Remix\", \"10 Stages of The Creative Process\", \"The Future of Our Species\", \"Why We Love Robots\", co-directed with her husband Ken Goldberg, and \"A Case for Optimism.\" The series, which began airing on AOL in 2013 was nominated for an Emmy Award in the News & Documentary for New Approaches: Arts, Lifestyle & Culture in 2014, and has since been viewed more than 40 million times.\n\nShlain lectures globally on filmmaking, the Internet's influence on society, and the future, and has spoken at TEDWomen and TEDMED. She delivered the keynote address for UC Berkeley's commencement ceremony in May 2010; the speech was included on NPR's list of \"The Best Commencement Speeches, Ever.\"\n\nShlain directed a film on women and power that was released through Refinery29's \"Shatterbox Anthology\". Released on October 27, 2016, it is called \"50/50: Rethinking the Past, Present, and Future of Women + Power,\" and explores the 10,000-year history of women. In addition, on May 10, 2017, in support of 50/50 Day: Gender Equality, 11,000 events took place around the world, all linked by the internet. These gatherings of people of all ages at organizations, companies, schools, museums, libraries, and homes screened this film, listened to speakers such as the former presidents of Iceland, Vigdis Finnbogadóttir, and Malawi, Joyce Banda, as well as Oscar-nominated filmmaker Ava DuVernay, comedian and activist Margaret Cho, activist Dolores Huerta, actor and activist Eva Longoria, among others and included a live global Q&A that promoted the sharing of ideas about how to initiate gender-balance in the world. The 2nd annual 50/50 Day on April 26, 2018 had over 36,000 events with speakers Jada Pinkett Smith, Kimberlé Williams Crenshaw, Matt McGorry, and more.\n\nShlain was asked to expand the script of the 50/50 film into a chapter on American women's struggle for justice into the book The Good Fight. She directed a film for Planned Parenthood that launched their Unstoppable art initiative called The Unstoppable Manifesto that Sia (musician) did the music for. She was also asked to be one of the 100 contributors for The Albert Einstein Foundation's new book Genius: 100 Visions of the Future. Other contributors included Supreme Court Justice Ruth Bader Ginsberg, Deepak Chopra, US songstress Barbra Streisand, Liberian President Ellen Johnson Sirleaf and Architect Frank Gehry.\n\nHer film studio Let it Ripple just held the 5th Annual Character Day with 210,000 registered events worldwide. Speakers over the years have included Angela Duckworth, Moby, Sharon Salzberg, Ken Robinson, and Krista Tippett.\n\nShe is currently writing a book about her family's decade-long ritual of turning off screen one day each week for what they call their Technology Shabbats. The book will be published by Simon & Schuster fall 2019.\n\nShlain has served on the boards of The Commonwealth Club of California, The Institute for the Future, Berkeley Center for New Media, among others. In 2017, she joined the Leadership Board of The Center on Media and Child Health at Harvard's Boston Children's Hospital. In 2018, she joined the advisory board of Wait til 8th (grade) for smartphones pledge. Through cloud filmmaking via her non-profit Let It Ripple, Shlain has made and donated over 3000 free customized films for schools and nonprofits.\n\nShlain lives in Marin County, Northern California, with husband Ken Goldberg, with whom she frequently collaborates on art installations and other projects. They have two daughters, Odessa and Blooma, and have received significant media attention based on the family's weekly observance for what she and her family call their \"Technology Shabbat,\" which they have observed since 2010.\n\nShlain has a brother, Jordan Shlain; a sister, artist Kimberly Brooks; and brother-in-law, Albert Brooks. Her sister-in-law is Professor Adele Goldberg. Following her father's death, Shlain and her siblings worked together to edit the manuscript of his final book, \"Leonardo's Brain: Understanding Da Vinci's Creative Genius\".[2][3][15]\n\n"}
{"id": "7325366", "url": "https://en.wikipedia.org/wiki?curid=7325366", "title": "Timeline of telescope technology", "text": "Timeline of telescope technology\n\nThe following timeline lists the significant events in the invention and development of the telescope.\n\n\n\n\n\n\n\n\n"}
{"id": "47769", "url": "https://en.wikipedia.org/wiki?curid=47769", "title": "Transistor–transistor logic", "text": "Transistor–transistor logic\n\nTransistor–transistor logic (TTL) is a logic family built from bipolar junction transistors. Its name signifies that transistors perform both the logic function (the first \"transistor\") and the amplifying function (the second \"transistor\"); it is the same naming convention used in resistor–transistor logic (RTL) and diode–transistor logic (DTL).\n\nTTL integrated circuits (ICs) were widely used in applications such as computers, industrial controls, test equipment and instrumentation, consumer electronics, and synthesizers. Sometimes TTL-compatible logic levels are not associated directly with TTL integrated circuits, for example as a label on the inputs and outputs of electronic instruments.\n\nAfter their introduction in integrated circuit form in 1963 by Sylvania, TTL integrated circuits were manufactured by several semiconductor companies. The 7400 series by Texas Instruments became particularly popular. TTL manufacturers offered a wide range of logic gates, flip-flops, counters, and other circuits. Variations of the original TTL circuit design offered higher speed or lower power dissipation to allow design optimization. TTL devices were originally made in ceramic and plastic dual-in-line (DIP) packages, and flat-pack form. TTL chips are now also made in surface-mount packages.\n\nTTL became the foundation of computers and other digital electronics. Even after larger scale integrated circuits made multiple-circuit-board processors obsolete, TTL devices still found extensive use as the glue logic interfacing between more densely integrated components. \n\nTTL was invented in 1961 by James L. Buie of TRW, which declared it, \"particularly suited to the newly developing integrated circuit design technology.\" The original name for TTL was \"transistor-coupled transistor logic\" (TCTL). The first commercial integrated-circuit TTL devices were manufactured by Sylvania in 1963, called the Sylvania Universal High-Level Logic family (SUHL). The Sylvania parts were used in the controls of the Phoenix missile. TTL became popular with electronic systems designers after Texas Instruments introduced the 5400 series of ICs, with military temperature range, in 1964 and the later 7400 series, specified over a narrower range and with inexpensive plastic packages, in 1966.\n\nThe Texas Instruments 7400 family became an industry standard. Compatible parts were made by Motorola, AMD, Fairchild, Intel, Intersil, Signetics, Mullard, Siemens, SGS-Thomson, Rifa, National Semiconductor, and many other companies, even in the Eastern Bloc (Soviet Union, GDR, Poland, Czechoslovakia, Hungary, Romania - for details see 7400 series). Not only did others make compatible TTL parts, but compatible parts were made using many other circuit technologies as well. At least one manufacturer, IBM, produced non-compatible TTL circuits for its own use; IBM used the technology in the IBM System/38, IBM 4300, and IBM 3081.\n\nThe term \"TTL\" is applied to many successive generations of bipolar logic, with gradual improvements in speed and power consumption over about two decades. The most recently introduced family 74Fxx is still sold today, and was widely used into the late 90s. 74AS/ALS Advanced Schottky was introduced in 1985. As of 2008, Texas Instruments continues to supply the more general-purpose chips in numerous obsolete technology families, albeit at increased prices. Typically, TTL chips integrate no more than a few hundred transistors each. Functions within a single package generally range from a few logic gates to a microprocessor bit-slice. TTL also became important because its low cost made digital techniques economically practical for tasks previously done by analog methods.\n\nThe Kenbak-1, ancestor of the first personal computers, used TTL for its CPU instead of a microprocessor chip, which was not available in 1971. The Datapoint 2200 from 1970 used TTL components for its CPU and was the basis for the 8008 and later the x86 instruction set. The 1973 Xerox Alto and 1981 Star workstations, which introduced the graphical user interface, used TTL circuits integrated at the level of Arithmetic logic units (ALUs) and bitslices, respectively. Most computers used TTL-compatible \"glue logic\" between larger chips well into the 1990s. Until the advent of programmable logic, discrete bipolar logic was used to prototype and emulate microarchitectures under development.\n\nTTL inputs are the emitters of bipolar transistors. In the case of NAND inputs, the inputs are the emitters of multiple-emitter transistors, functionally equivalent to multiple transistors where the bases and collectors are tied together. The output is buffered by a common emitter amplifier.\n\nInputs both logical ones. When all the inputs are held at high voltage, the base–emitter junctions of the multiple-emitter transistor are reverse-biased. Unlike DTL, a small “collector” current (approximately 10µA) is drawn by each of the inputs. This is because the transistor is in reverse-active mode. An approximately constant current flows from the positive rail, through the resistor and into the base of the multiple emitter transistor. This current passes through the base–emitter junction of the output transistor, allowing it to conduct and pulling the output voltage low (logical zero).\n\nAn input logical zero. Note that the base–collector junction of the multiple-emitter transistor and the base–emitter junction of the output transistor are in series between the bottom of the resistor and ground. If one input voltage becomes zero, the corresponding base–emitter junction of the multiple-emitter transistor is in parallel with these two junctions. A phenomenon called current steering means that when two voltage-stable elements with different threshold voltages are connected in parallel, the current flows through the path with the smaller threshold voltage. That is, current flows out of this input and into the zero (low) voltage source. As a result, no current flows through the base of the output transistor, causing it to stop conducting and the output voltage becomes high (logical one). During the transition the input transistor is briefly in its active region; so it draws a large current away from the base of the output transistor and thus quickly discharges its base. This is a critical advantage of TTL over DTL that speeds up the transition over a diode input structure.\n\nThe main disadvantage of TTL with a simple output stage is the relatively high output resistance at output logical \"1\" that is completely determined by the output collector resistor. It limits the number of inputs that can be connected (the fanout). Some advantage of the simple output stage is the high voltage level (up to V) of the output logical \"1\" when the output is not loaded.\n\nA common variation omits the collector resistor of the output transistor, making an open-collector output. This allows the designer to fabricate logic by connecting the open-collector outputs of several logic gates together and providing a single external pull-up resistor. If any of the logic gates becomes logic low (transistor conducting), the combined output will be low. Examples of this type of gate are the 7401 and 7403 series. Open-collector outputs of some gates have a higher maximum voltage, such as 15V for the 7426, useful when driving other than TTL loads.\n\nTo solve the problem with the high output resistance of the simple output stage the second schematic adds to this a \"totem-pole\" (\"push–pull\") output. It consists of the two n-p-n transistors V and V, the \"lifting\" diode V and the current-limiting resistor R (see the figure on the right). It is driven by applying the same \"current steering\" idea as above.\n\nWhen V is \"off\", V is \"off\" as well and V operates in active region as a voltage follower producing high output voltage (logical \"1\").\n\nWhen V is \"on\", it activates V, driving low voltage (logical \"0\") to the output. Again there is a current-steering effect: the series combination of V's C-E junction and V's B-E junction is in parallel with the series of V B-E, V's anode-cathode junction, and V C-E. The second series combination has the higher threshold voltage, so no current flows through it, i.e. V base current is deprived. Transistor V turns \"off\" and it does not impact on the output.\n\nIn the middle of the transition, the resistor R limits the current flowing directly through the series connected transistor V, diode V and transistor V that are all conducting. It also limits the output current in the case of output logical \"1\" and short connection to the ground. The strength of the gate may be increased without proportionally affecting the power consumption by removing the pull-up and pull-down resistors from the output stage.\n\nThe main advantage of TTL with a \"totem-pole\" output stage is the low output resistance at output logical \"1\". It is determined by the upper output transistor V operating in active region as an emitter follower. The resistor R does not increase the output resistance since it is connected in the V collector and its influence is compensated by the negative feedback. A disadvantage of the \"totem-pole\" output stage is the decreased voltage level (no more than 3.5 V) of the output logical \"1\" (even if the output is unloaded). The reason of this reduction are the voltage drops across the V base–emitter and V anode–cathode junctions.\n\nLike DTL, TTL is a \"current-sinking logic\" since a current must be drawn from inputs to bring them to a logic 0 level. At low input voltage, the TTL input sources current which must be absorbed by the previous stage. The maximum value of this input current is about 1.6 mA for a standard TTL gate. The input source has to be low-resistive enough (<500 Ω) so that the flowing current creates only a negligible voltage drop (<0.4 V) across it, for the input to be considered as a logical \"0\" (with a 0.4 V \"noise margin\", see below). The output stage of the most common TTL gates is specified to function correctly when driving up to 10 standard input stages (a fanout of 10). TTL inputs are sometimes simply left floating to provide a logical \"1\", though this usage is not recommended.\n\nStandard TTL circuits operate with a 5-volt power supply. A TTL input signal is defined as \"low\" when between 0 V and 0.8 V with respect to the ground terminal, and \"high\" when between 2 V and V (5 V), and if a voltage signal ranging between 0.8 V and 2.0 V is sent into the input of a TTL gate, there is no certain response from the gate and therefore it is considered \"uncertain\" (precise logic levels vary slightly between sub-types and by temperature). TTL outputs are typically restricted to narrower limits of between 0.0 V and 0.4 V for a \"low\" and between 2.4 V and V for a \"high\", providing at least 0.4 V of noise immunity. Standardization of the TTL levels is so ubiquitous that complex circuit boards often contain TTL chips made by many different manufacturers selected for availability and cost, compatibility being assured; two circuit board units off the same assembly line on different successive days or weeks might have a different mix of brands of chips in the same positions on the board; repair is possible with chips manufactured years (sometimes over a decade) later than original components. Within usefully broad limits, logic gates can be treated as ideal Boolean devices without concern for electrical limitations. The 0.4V noise margins are adequate because of the low output impedance of the driver stage, that is, a large amount of noise power superimposed on the output is needed to drive an input into an undefined region.\n\nIn some cases (e.g., when the output of a TTL logic gate needs to be used for driving the input of a CMOS gate), the voltage level of the \"totem-pole\" output stage at output logical \"1\" can be increased closer to V by connecting an external resistor between the V collector and the positive rail. It pulls up the V cathode and cuts-off the diode. However, this technique actually converts the sophisticated \"totem-pole\" output into a simple output stage having significant output resistance when driving a high level (determined by the external resistor).\n\nLike most integrated circuits of the period 1963–1990, commercial TTL devices are usually packaged in dual in-line packages (DIPs), usually with 14 to 24 pins, for through-hole or socket mounting. The DIPs were usually made of epoxy plastic (PDIP) for commercial-grade parts or of ceramic (CDIP) for military-grade parts.\n\nBeam-lead chip dies without packages were made for assembly into larger arrays as hybrid integrated circuits. Parts for military and aerospace applications were packaged in flatpacks, a form of surface-mount package, with leads suitable for welding or soldering to printed circuit boards. Today, many TTL-compatible devices are available in surface-mount packages, which are available in a wider array of types than through-hole packages.\n\nTTL is particularly well suited to bipolar integrated circuits because additional inputs to a gate merely required additional emitters on a shared base region of the input transistor. If individually packaged transistors were used, the cost of all the transistors would discourage one from using such an input structure. But in an integrated circuit, the additional emitters for extra gate inputs add only a small area.\n\nAt least one computer manufacturer, IBM, built its own flip chip integrated circuits with TTL; these chips were mounted on ceramic multi-chip modules.\n\nTTL devices consume substantially more power than equivalent CMOS devices at rest, but power consumption does not increase with clock speed as rapidly as for CMOS devices. Compared to contemporary ECL circuits, TTL uses less power and has easier design rules but is substantially slower. Designers can combine ECL and TTL devices in the same system to achieve best overall performance and economy, but level-shifting devices are required between the two logic families. TTL is less sensitive to damage from electrostatic discharge than early CMOS devices.\n\nDue to the output structure of TTL devices, the output impedance is asymmetrical between the high and low state, making them unsuitable for driving transmission lines. This drawback is usually overcome by buffering the outputs with special line-driver devices where signals need to be sent through cables. ECL, by virtue of its symmetric low-impedance output structure, does not have this drawback.\n\nThe TTL \"totem-pole\" output structure often has a momentary overlap when both the upper and lower transistors are conducting, resulting in a substantial pulse of current drawn from the power supply. These pulses can couple in unexpected ways between multiple integrated circuit packages, resulting in reduced noise margin and lower performance. TTL systems usually have a decoupling capacitor for every one or two IC packages, so that a current pulse from one TTL chip does not momentarily reduce the supply voltage to another.\n\nSeveral manufacturers now supply CMOS logic equivalents with TTL-compatible input and output levels, usually bearing part numbers similar to the equivalent TTL component and with the same pinouts. For example, the 74HCT00 series provides many drop-in replacements for bipolar 7400 series parts, but uses CMOS technology.\n\nSuccessive generations of technology produced compatible parts with improved power consumption or switching speed, or both. Although vendors uniformly marketed these various product lines as TTL with Schottky diodes, some of the underlying circuits, such as used in the LS family, could rather be considered DTL.\n\nVariations of and successors to the basic TTL family, which has a typical gate propagation delay of 10ns and a power dissipation of 10 mW per gate, for a power–delay product (PDP) or switching energy of about 100 pJ, include:\n\n\nMost manufacturers offer commercial and extended temperature ranges: for example Texas Instruments 7400 series parts are rated from 0 to 70 °C, and 5400 series devices over the military-specification temperature range of −55 to +125 °C.\n\nSpecial quality levels and high-reliability parts are available for military and aerospace applications.\n\nRadiation-hardened devices (for example from the SNJ54 series) are offered for space applications.\n\nBefore the advent of VLSI devices, TTL integrated circuits were a standard method of construction for the processors of mini-computer and mainframe processors; such as the DEC VAX and Data General Eclipse, and for equipment such as machine tool numerical controls, printers and video display terminals. As microprocessors became more functional, TTL devices became important for \"glue logic\" applications, such as fast bus drivers on a motherboard, which tie together the function blocks realized in VLSI elements.\n\nWhile originally designed to handle logic-level digital signals, a TTL inverter can be biased as an analog amplifier. Connecting a resistor between the output and the input biases the TTL element as a negative feedback amplifier. Such amplifiers may be useful to convert analog signals to the digital domain but would not ordinarily be used where analog amplification is the primary purpose. TTL inverters can also be used in crystal oscillators where their analog amplification ability is significant.\n\nA TTL gate may operate inadvertently as an analog amplifier if the input is connected to a slowly changing input signal that traverses the unspecified region from 0.8 V to 2 V. The output can be erratic when the input is in this range. A slowly changing input like this can also cause excess power dissipation in the output circuit. If such an analog input must be used, there are specialized TTL parts with Schmitt trigger inputs available that will reliably convert the analog input to a digital value, effectively operating as a one bit A to D converter.\n\n\n"}
{"id": "14291150", "url": "https://en.wikipedia.org/wiki?curid=14291150", "title": "Unified Power Format", "text": "Unified Power Format\n\nUnified Power Format (UPF) is the popular name of the Institute of Electrical and Electronics Engineers (IEEE) standard for specifying power intent in power optimization of electronic design automation. The IEEE 1801-2009 release of the standard was based on a donation from the Accellera organization. The current release is IEEE 1801-2015.\n\nA Unified Power Format technical committee was formed by the Accellera organization, chaired by Stephen Bailey of Mentor Graphics.\nAs a reaction to the Power Forward Initiative the group was proposed in July 2006 and met on September 13, 2006.\nIt submitted its first draft in January 2007, and a version 1.0 was approved to be published on February 26, 2007.\nJoe Daniels was technical editor.\n\nFiles written to this standard annotate an electric design with the power and power control intent of that design. Elements of that annotation include:\nThe standard describes extensions to the Tool Command Language (Tcl): commands and arguments for anotating a design hierarchy which has been read into a tool.\nSemantics for inferring additional elements in the design from the intent are provided in the standard.\nDigital designers, IP Block providers, Physical Designers, and Verification engineers make use of this standard language to communicate their design intent and implementation with respect to the variable power of an electronic system.\n\nThe Design Automation Standards Committee (DASC) of the IEEE Standards Association sponsored working group 1801, with the project authorization approved on May 7, 2007.\nGoals included:\nThe IEEE group was initially called the \"Low Power Study Group\". Proposed standards have the letter \"P\" in front of them (such as P1801), which is removed and replaced with a dash and year when the standard is ratified.\nAccelera's UPF 1.0 was donated to the IEEE as a basis of this standard in June 2006.\n\nAfter reviewing 14 drafts, on March 27, 2009, the \"Standard for Design and Verification of Low Power Integrated Circuits\" was published as IEEE Std 1801-2009. It is sometimes called UPF 2.0.\nBailey was also chairman of the IEEE group.\nAnother notable supporter of the standard was Synopsys.\nA follow-on project planned to develop a list of frequently asked questions (FAQ) about the specification.\n\n\n"}
{"id": "12042059", "url": "https://en.wikipedia.org/wiki?curid=12042059", "title": "V-hull", "text": "V-hull\n\nThe V-hull is a type of vehicle armor design used on wheeled armored personnel carriers (APC), infantry mobility vehicles, infantry fighting vehicles (IFV) and MRAPs. The design originated in the 1970s with vehicles such as the iconic Casspir used extensively during the South African Border War, Leopard security vehicle used in the Rhodesian Bush War and South African armored vehicle company Land Systems OMC's Ratel IFV and Buffel.\n\nThe purpose of V-hulls is to increase vehicle and crew survivability by deflecting an upward directed blast from a landmine (or Improvised Explosive Device) away from the vehicle, while also presenting a sloped armor face. By presenting its armor at an angle, it increases the amount of material a ballistic projectile must pass through in order to penetrate the vehicle, and increases the chance of deflection.\n\nV-hulls are incorporated in armored vehicle designs in several different ways. Many vehicles, such as the BAE Systems RG-33 incorporate the V-hull into a monocoque chassis, while others, such as the ATF Dingo and International MaxxPro use a body-on-frame chassis, with an armored V-hull crew compartment, and an additional V or semicircular shaped piece protecting the driveline. Others, such as the Cougar H have a V-hull crew compartment, and allow the driveline and suspension components to be sacrificed in an attack, while maintaining the safety of the crew.\n\n"}
{"id": "9890699", "url": "https://en.wikipedia.org/wiki?curid=9890699", "title": "Video standards converter", "text": "Video standards converter\n\nA video standards converter is a video device that converts NTSC to PAL and/or PAL to NTSC.\n\nThe PAL TV signals may be transcoded to or from SECAM.\n\nVideo standards converters are primarily used so television shows can be viewed in nations with different video standards.\n\nWith the use of high-definition television, new digital video standards converters came on the market. Some were down converters only, HDTV to PAL or NTSC. Others could both up and down convert: HDTV to standard definition: PAL or NTSC and vice versa. \n\nConverters are needed because NTSC uses 30 frames (pictures) per second and PAL uses 25 frames per second.\nFirst video standards converters were analog. That is a special professional video camera that used a video camera tube would be pointed at a cathode ray tube video monitor. Both the Camera and the monitor could be switched to either NTSC or PAL, to convert both ways. Robert Bosch GmbH's Fernseh Division made a large three rack analog video standards converter. These were the high end converters of the 1960s and 1970s. Image Transform in Universal City, CA used the Fernseh converter and in the 1980s made their own a custom digital converter. This was also a larger 3 rack device. As digital memory size became larger in smaller packages converters became the size of a microwave oven. Today one can buy a very small converter for home use.\n\n"}
{"id": "25881569", "url": "https://en.wikipedia.org/wiki?curid=25881569", "title": "Xpeak", "text": "Xpeak\n\nXpeak is a standard for device management, based on XML and platform agnostic, initially focused on financial applications but not restricted to it. It serves the same purpose that other APIs like CEN/XFS and J/XFS but is not restricted to one operating system or language, since it works in a client/server model using XML in a way to homogenise the communication between the application and the device services. Its flexibility allows different parts of the whole business to be implemented in different languages, having the application and the various devices, some implemented in Java, other in C++ and still others in the device's firmware.\n\nIt was designed based on the experiences had with CEN/XFS, J/XFS and JavaPOS, but instead of using a standards organization it uses and open source model to develop the architecture and tools used by the project, like its base the complete open source software solution named Xpeaker. This way it can be updated, quickly and openly, by the users themselves, using the Internet as the means of communication rather than meetings requiring a physical presence.\n\nXpeak follows the open source model and participation in the project is totally free, but it is moderated by the R&D Open Source Foundation, participated in equal parts by Sun Microsystems and Intecna. The initial code contribution is the responsibility of Cashware, one of the leading companies in devices connectivity through the use of standards (CEN/XFS and J/XFS).\n\nXpeaker is a collection of software projects, integrally developed by Cashware, with the philosophy of Open Source and commercialized under a dual license the XPEAKER PUBLIC LICENSE and a commercial licence.\nXpeaker includes the following elements:\n\nAn Eclipse Plugin which permits: \n\nMade up of:\n\nHigh level API for access to Xpeak Services.\nXpeaking permits access to said services from different programming languages (Java, C, C++, C#, Pascal)\n\n"}
